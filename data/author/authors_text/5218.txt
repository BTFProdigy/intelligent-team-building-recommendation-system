Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 177?184
Manchester, August 2008
Pedagogically Useful Extractive Summaries for Science Education 
Sebastian de la Chica, Faisal Ahmad, James H. Martin, Tamara Sumner 
Institute of Cognitive Science 
Department of Computer Science 
University of Colorado at Boulder 
sebastian.delachica, faisal.ahmad, james.martin, 
tamara.sumner@colorado.edu 
 
 
 Abstract 
This paper describes the design and 
evaluation of an extractive summarizer 
for educational science content called 
COGENT. COGENT extends MEAD 
based on strategies elicited from an em-
pirical study with science domain and in-
structional design experts. COGENT 
identifies sentences containing pedagogi-
cally relevant concepts for a specific sci-
ence domain. The algorithms pursue a 
hybrid approach integrating both domain 
independent bottom-up sentence scoring 
features and domain-aware top-down fea-
tures. Evaluation results indicate that 
COGENT outperforms existing summar-
izers and generates summaries that 
closely resemble those generated by hu-
man experts. COGENT concept invento-
ries appear to also support the computa-
tional identification of student miscon-
ceptions about earthquakes and plate tec-
tonics. 
1 Introduction 
Multidocument summarization (MDS) research 
efforts have resulted in significant advancements 
in algorithm and system design (Mani, 2001). 
Many of these efforts have focused on summariz-
ing news articles, but not significantly explored 
the research issues arising from processing edu-
cational content to support pedagogical applica-
tions. This paper describes our research into the 
application of MDS techniques to educational 
                                                 
 
 ? 2008. Licensed under the Creative Commons At-
tribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
science content to generate pedagogically useful 
summaries. 
Knowledge maps are graphical representations 
of domain information laid out as networks of 
nodes containing rich concept descriptions inter-
connected using a fixed set of relationship types 
(Holley and Dansereau, 1984). Knowledge maps 
are a variant of the concept maps used to capture, 
assess, and track student knowledge in education 
research (Novak and Gowin, 1984). Learning 
research indicates that knowledge maps may be 
useful cognitive scaffolds, helping users lacking 
domain expertise to understand the macro-level 
structure of an information space (O'Donnell et 
al., 2002). Knowledge maps have emerged as an 
effective representation to generate conceptual 
browsers that help students navigate educational 
digital libraries, such as the Digital Library for 
Earth System Education (DLESE.org) (Butcher 
et al, 2006). In addition, knowledge maps have 
proven useful for domain and instructional ex-
perts to capture domain knowledge from digital 
library resources and to analyze student under-
standing for the purposes of providing formative 
assessments (Ahmad et al, 2007).  
Knowledge maps have proven useful both as 
representations of knowledge for assessment 
purposes and as learning resources for presenta-
tion to students. However, domain knowledge 
map construction by experts is an expensive 
knowledge engineering activity. In this paper, we 
describe our progress towards the automated 
generation of pedagogically useful extractive 
summaries from educational texts about a sci-
ence domain. In the context of automated knowl-
edge map generation, summary sentences corre-
spond to concepts. While the detection of rela-
tionships between concepts is also part of our 
overall research agenda, this paper focuses solely 
on concept identification using MDS techniques. 
The remainder of this paper is organized as fol-
177
lows. First, we review related work in the areas 
of automated concept extraction from texts and 
extractive summarization. We then describe the 
empirical study we have conducted to understand 
how domain and instructional design experts 
identify pedagogically important science con-
cepts in educational digital library resources. 
Next, we provide a detailed description of the 
algorithms we have designed based on expert 
strategies elicited from our empirical study. We 
then present and discuss our evaluation results 
using automated summarization metrics and hu-
man judgments. Finally, we present our conclu-
sions and future work in this area. 
2 Related Work 
Our work is informed by efforts to automate the 
acquisition of ontology concepts from text. On-
toLearn (Navigli and Velardi, 2004) extracts do-
main terminology from a collection of texts using 
a syntactic parse to identify candidate terms that 
are filtered based on domain relevance and con-
nected using a semantic interpretation based on 
word sense disambiguation. The newly identified 
concepts and relationships are used to update an 
existing ontology. Knowledge Puzzle focuses on 
n-grams to produce candidate terms filtered 
based on term frequency in the input documents 
and on the number of relationships associated 
with a given term (Zouaq et al, 2007). This ap-
proach leverages pattern extraction techniques to 
identify concepts and relationships. While these 
approaches produce ontologies useful for compu-
tational purposes, the identified concepts are of a 
very fine granularity and therefore may yield 
graphs not suitable for identifying student mis-
conceptions or for presentation back to the stu-
dent. Clustering by committee has also been used 
to discover concepts from a text by grouping 
terms into conceptually related clusters (Lin and 
Pantel, 2002). The resulting clusters appear to be 
tightly related, but operate at a very fine level of 
granularity. Our approach focuses on sentences 
as units of knowledge to produce concise repre-
sentations that may be useful both as computa-
tional objects and as learning resources to present 
back to the student. Therefore, extractive sum-
marization research also informs our work. 
Topic representation and topic themes have 
been used to explore promising MDS techniques 
(Harabagiu and Lacatusu, 2005). Recent efforts 
in graph-based MDS have integrated sentence 
affinity, information richness and diversity pen-
alties to produce very promising results (Wan 
and Yang, 2006). Finally, MEAD is a widely 
used multi-document summarization and evalua-
tion platform (Radev et al, 2000). MEAD re-
search efforts have resulted in significant contri-
butions to support the development of summari-
zation applications (Radev et al, 2000). While 
all these systems have produced promising re-
sults in automated evaluations, none have di-
rectly targeted educational content as input or the 
generation of pedagogically useful summaries. 
We are directly building upon MEAD due its 
focus on sentence extraction and its high degree 
of modularization. 
3 Empirical Study 
We have conducted a study to capture how hu-
man experts construct and use knowledge maps. 
In this 10-month study, we examined how ex-
perts created knowledge maps from educational 
digital libraries and how they used the maps to 
assess student work and provide personalized 
feedback. 
In this paper, we are focusing on the knowl-
edge map construction aspects of the study. Four 
geology and instructional design experts collabo-
ratively selected 20 resources from DLESE to 
construct a domain knowledge map on earth-
quakes and plates tectonics for high school age 
learners. The experts independently created 
knowledge maps of individual resources which 
they collaboratively merged into the final domain 
knowledge map in a one-day workshop. The re-
sulting domain knowledge map consisted of 564 
nodes containing domain concepts and 578 rela-
tionships. The concepts consist of 7,846 words, 
or 5% of the total number of words in the origi-
nal resources. Figure 1 shows a fragment of the 
domain knowledge map created by our experts. 
 
 
Figure 1. Fragment of domain  knowledge map 
created by domain and instructional experts 
 
Experts created nodes containing concepts of 
varying granularity, including nouns, noun 
phrases, partial sentences, single sentences, and 
178
multiple sentences. Our analysis of this domain 
knowledge map indicates that experts relied on 
copying-and-pasting (58%) and paraphrasing 
(37%) to create most domain concepts. Only 5% 
of the nodes could not be traced directly to the 
original resources. 
Experts used relationship types in a Zipf-like 
distribution with the top 10 relationship types 
accounting for 64% of all relationships. The top 
2 relationship types each accounted for more 
than 10% of all relationships: elaborations (19% 
or 110 links) and examples (14% or 78 links). 
We have established the completeness of this 
domain knowledge map by asking a domain ex-
pert to assess its content coverage of nationally-
recognized educational goals on earthquakes and 
plate tectonics for high school age learners using 
the American Association for the Advancement 
of Science (AAAS) Benchmarks (Project 2061, 
1993). The results indicate adequate content cov-
erage of the relevant AAAS Benchmarks achieved 
through 82 of the concepts (15%) with the re-
maining 482 concepts (85%) providing very de-
tailed elaborations of the associated learning 
goals. 
Qualitative analysis of the verbal protocols 
captured during the study indicates that all ex-
perts used external sources to construct the do-
main knowledge map. Experts made references 
to their own knowledge (e.g., ?I know that??), 
to content learned or taught in geology courses, 
to other resources used in the study, and to the 
National Science Education Standards (NSES), a 
comprehensive collection of nationally-
recognized science learning goals for K-12 stu-
dents (National Research Council, 1996). 
We have examined sentence extraction agree-
ment between experts using a kappa measure that 
accounts for prevalence of judgments and con-
flicting biases amongst experts, called PABA-
kappa (Byrt et al, 1993). The average PABA-
kappa value of 0.62 indicates that our experts 
substantially agree on sentence extraction from 
digital library resources. While this study was 
not designed as an annotation project to support 
summarization evaluation, this level of agree-
ment indicates that the concepts selected by the 
experts may serve as the reference summary to 
evaluate the performance of our summarizer. 
4 Summarizer for Science Education 
Creating a knowledge map from a collection of 
input texts involves identifying sentences con-
taining important domain concepts, linking con-
cepts, and labeling those links. This paper fo-
cuses solely on identifying and extracting peda-
gogically relevant sentences as domain concepts. 
We have designed and implemented an extrac-
tive summarizer for educational science content, 
called COGENT, based on MEAD version 3.11 
(Radev et al, 2000). COGENT processes a col-
lection of educational digital library resources by 
first preprocessing each resource using Tidy 
(tidy.sourceforge.net) to fix improperly format-
ted HTML code. COGENT then merges multiple 
web pages into a single HTML document and 
extracts the contents of each resource into a plain 
text file. We have extended MEAD with sen-
tence scoring features based on domain content, 
document structure, and sentence length. 
4.1 Domain Content 
We have designed two sentence-scoring features 
that aim to capture the domain content relevance 
of each sentence: the educational standards 
feature and the gazetteer feature. 
We have developed a feature that models how 
human experts used external sources to identify 
and extract concepts. The educational standards 
feature uses the textual description of the 
relevant AAAS Benchmarks on earthquakes and 
plate tectonics for high-school age learners and 
the associated NSES. Each sentence receives a 
score based on its similarity to the text contents 
of the learning goals and educational standards 
computed using a TFIDF (Term Frequency-
Inverse Document Frequency) approach  (Salton 
and Buckley, 1988). We have used KinoSearch, 
a Perl implementation of the Lucene search 
engine (lucene.apache.org), to create an index 
that includes the AAAS Benchmarks learning 
goal description (boosted by 2), subject (boosted 
by 8), and keywords (boosted by 2), plus the text 
of the associated national standards (not 
boosted). Sentence scores are based on the 
similarity scores generated by KinoSearch in 
response to a query consisting of the sentence 
text. 
To account for the large number of examples 
used by the experts in the domain knowledge 
map (14% of all links), we have developed a 
feature that reflects the number and relevance of 
the geographical names in each sentence. Earth 
science examples often refer to names of 
geographical places, including geological 
formations on the planet. The gazetteer feature 
leverages the Alexandria Digital Library (ADL) 
Gazetteer service (Hill, 2000) to check whether 
named entities identified in each sentence match 
179
entries in the ADL Gazetteer. A gazetteer is a 
georeferencing resource containing information 
about locations and place-names, including 
latitude and longitude as well as type information 
about the corresponding geographical feature. 
Each sentence receives a score based on a TFIDF 
approach where the TF is the number of times a 
particular location name appears in the sentence 
and the IDF is the inverse of the count of 
gazetteer entries matching the location name. If 
the ADL Gazetteer returns a large number of 
results for a given place-name, it means there are 
many geographical locations identified by that 
name. Our assumption is that unique names may 
be more pedagogically relevant. For example, 
Ohio receives an IDF score of 0.0625 because 
the ADL Gazetteer contains 16 entries so named, 
while the Mid-Atlantic Ridge, the distinctive 
underwater mountain range dividing the Atlantic 
Ocean, receives a score of 1.0 as it appears only 
once. 
4.2 Document Structure 
Based on the intuition that the HTML structure 
of a web site reflects content relevancy, we have 
developed the hypertext feature. The hypertext 
feature assigns a higher score to sentences con-
tained under higher level HTML headings.  
  
Heading Bonus 
H1 1/1 = 1.00 
H2 1/2 = 0.50 
H3 1/3 = 0.33 
H4 1/4 = 0.25 
H5 1/5 = 0.20 
H6 1/6 = 0.17 
Table 1. Hypertext feature heading bonus 
 
Within a given heading level, the hypertext 
feature assigns a higher score to sentences that 
appear earlier within that level based on both 
relative paragraph order within the heading and 
relative sentence position within each paragraph. 
The equation used to compute the hypertext 
score for a sentence is  
44
_1 * _1 * _  _ nosentnoparbonusheadingscorehypertext =  
where heading_bonus is obtained from Table 1, 
par_no is the paragraph number within the head-
ing, and sent_no is the sentence number within 
the paragraph. We use the 4 1 x   function to at-
tenuate the contributions to the feature score of 
later paragraphs and sentences. Initially, we used 
the same function MEAD uses to modulate its 
position feature ( 2 1 x ), but initial experimenta-
tion indicated this function decayed too rapidly, 
resulting in later sentences being over-penalized. 
4.3 Sentence Length 
To promote the extraction of sentences contain-
ing scientific concepts, we have developed the 
content word density feature. This feature makes 
a cut-off decision based on the ratio of content 
words to function words in a sentence. The con-
tent word density feature uses a pre-populated 
list of function words (a stopword list) to calcu-
late the ratio of content to function words within 
each sentence, keeping sentences that meet or 
exceed the ratio of 50%. This cut-off value im-
plies that the extracted sentences contain rela-
tively more content words than function words. 
4.4 Sentence Scoring and Selection 
We compute the final score of each sentence by 
adding the scores obtained for the MEAD default 
configuration features (centroid and position) to 
the scores for the COGENT features (educational 
standards, gazetteer, and hypertext). After the 
sentences have been sorted according to their 
cumulative scores, we keep sentences that pass 
the cut-off constraints, including the MEAD 
length feature equal or greater than 9 and CO-
GENT content word density equal or greater than 
50%. We use the MEAD cosine re-ranker to 
eliminate redundant sentences based on a cutoff 
similarity value of 0.7. Since human experts used 
only 5% of the total word count in the resources, 
we have configured MEAD to use a 5% word 
compression rate. 
5 Evaluation 
We have evaluated COGENT by processing the 
20 digital library resources used in the empirical 
study and comparing its output against the con-
cepts identified by the experts. 
5.1 Quality 
To assess the quality of the generated summaries, 
we have examined three configurations: Random, 
Default, and COGENT. The Random configura-
tion extracts a random collection of sentences 
from the input texts. The Default configuration 
uses the MEAD default centroid, position and 
length (cut-off value of 9) sentence scoring fea-
tures. Finally, the COGENT configuration in-
cludes the MEAD default features and the CO-
GENT features. The Default and COGENT con-
figurations use the MEAD cosine function with a 
threshold of 0.7 to eliminate redundant sen-
180
tences. All three configurations use a word com-
pression factor of 5% resulting in summaries of 
very similar length. 
For this evaluation, we leverage ROUGE (Lin 
and Hovy, 2003) to address the relative quality of 
the generated summaries based on common n-
gram counts and longest common subsequence 
(LCS). We report on ROUGE-1 (unigrams), 
ROUGE-2 (bigrams), ROUGE W-1.2 (weighted 
LCS), and ROUGE-S* (skip bigrams) as they 
appear to correlate well with human judgments 
for longer multi-document summaries, particu-
larly ROUGE-1 (Lin, 2004). Table 2 shows the 
results of this ROUGE-based evaluation includ-
ing recall (R), precision (P), and balanced f-
measure (F). 
 
  Random Default COGENT 
R 0.4855 0.4976 0.6073 
P 0.5026 0.5688 0.6034 R-1 
F 0.4939 0.5308 0.6054 
R 0.0972 0.1321 0.1907 
P 0.1006 0.1510 0.1895 R-2 
F 0.0989 0.1409 0.1901 
R 0.0929 0.0951 0.1185 
P 0.1533 0.1733 0.1877 R-W-1.2 
F 0.1157 0.1228 0.1453 
R 0.2481 0.2620 0.3820 
P 0.2657 0.3424 0.3772 R-S* 
F 0.2566 0.2969 0.3796 
Table 2. Quality evaluation results (5% word 
compression) 
 
COGENT consistently outperforms the Ran-
dom and Default baselines based on all four re-
ported ROUGE measures. Given that much of 
the original research efforts on MEAD have cen-
tered on news articles, this result is not surpris-
ing. Pedagogical content, such as the educational 
digital library resources used in our work, differs 
in rhetorical intent, structure and terminology 
from the news articles leveraged by the MEAD 
researchers. However, the COGENT features 
described here are complementary to the default 
MEAD configuration. COGENT can best be 
characterized as a hybrid MDS, integrating bot-
tom-up (centroid, position, length, hypertext, and 
content word density) and top-down (educational 
standards and gazetteer) sentence scoring fea-
tures. This hybrid approach reflects our findings 
from observing expert behaviors for identifying 
concepts from educational digital library re-
sources. We believe the overall improvement in 
quality scores may be due to the COGENT fea-
tures targeting different dimensions of what con-
stitutes a pedagogically effective summary than 
the default MEAD features. 
To characterize the COGENT summary con-
tents, one of our research team members manu-
ally generated a summary corresponding to the 
best case for an extractive summarizer. This Best 
Case summary comprises the sentences from the 
digital library resources that align to the concepts 
selected by the human experts in our empirical 
study. Since the experts created concepts of vary-
ing granularity, this alignment produces the list 
of sentences that the experts would have pro-
duced if they had only selected single sentences 
to create concepts for their domain knowledge 
map. This summary comprises 621 sentences 
consisting of 13,116 words, or about a 9% word 
compression. 
For this aspect of the evaluation, we have used 
ROUGE-L, an LCS metric computed using 
ROUGE. The ROUGE-L computation examines 
the union LCS between each reference sentence 
and all the sentences in the candidate summary. 
We believe this metric may be well-suited to re-
flect the degree of linguistic surface structure 
similarity between summaries. We postulate that 
ROUGE-L may be able to account for the explic-
itly copy-pasted concepts and to detect the more 
subtle similarities with paraphrased concepts in 
the expert-generated domain knowledge map. 
We have also used the content-based evaluation 
capabilities of MEAD to report on a cosine 
measure to capture similarity between the candi-
date summaries and the reference. Table 3 shows 
the results of this aspect of the evaluation includ-
ing recall (R), precision (P), and balanced f-
measure (F). 
 
  Random 
(5%) 
Default 
(5%) 
COGENT 
(5%) 
Best Case 
(9%) 
R 0.4814 0.4919 0.6021 0.9669 
P 0.4982 0.5623 0.5982 0.6256 R-L 
F 0.4897 0.5248 0.6001 0.7597 
Cosine 0.5382 0.6748 0.8325 0.9323 
Table 3. Content-based evaluation results (word 
compression in parentheses) 
 
COGENT consistently outperforms the Ran-
dom and Default baselines on both the ROUGE-
L and cosine measures. Given the cosine value of 
0.8325, it appears COGENT extracts sentences 
containing similar terms  in very similar fre-
quency distribution as the experts. 
The ROUGE-L scores also consistently indi-
cate that the COGENT summary may be closer 
to the reference summary in relative word order-
181
ing than either the Random or Default configura-
tions. However, the scores for the Best Case 
summary reveal two interesting points. First, the 
ROUGE-L recall score for COGENT (R=0. 
6021) is lower than that obtained by the Best 
Case summary (R=0.9669), meaning our sum-
marizer appears to be extracting different sen-
tences than those selected by the experts. Given 
the high cosine similarity with the reference 
summary (0.8325), we hypothesize that CO-
GENT may be selecting sentences that cover 
very similar concepts to those selected by the 
experts only expressed differently. Second, we 
would have expected the ROUGE-L precision 
score for the Best Case configuration to be closer 
to 1.0. Instead, the Best Case precision score is 
0.6256, only a minor improvement over CO-
GENT (P=0.5982). Since the sentences in the 
Best Case summary come directly from the digi-
tal library resources, we hypothesize that experts 
may have used extensive linguistic transforma-
tions for paraphrased concepts, resulting in struc-
tures that ROUGE-L could not identify as simi-
lar. 
Given the difference in word compression for 
the Best Case summary, we have performed an 
incremental analysis using the ROUGE-L meas-
ure shown in Figure 2.  
ROUGE-L COGENT Evaluation
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0 5 10 15 20 25 30
MEAD Word Percent Compression
Recall Precision F-Measure
 
Figure 2. COGENT ROUGE-L results at differ-
ent word compression rates 
 
This graph shows improved COGENT per-
formance in ROUGE-L recall as the length of the 
summary increases, while both precision and f-
measure degrade. COGENT can match the recall 
scores of the Best Case summary (R=0.9669) by 
making the generated summary longer (30% 
word compression rate or 32,619 words), but the 
precision would suffer a sizeable decay 
(P=0.1558). For educational applications, more 
comprehensive concept inventories (longer 
summaries) may be better suited for computa-
tional purposes, such as pedagogical reasoning 
about student understanding, while more succinct 
inventories (shorter summaries) may be more 
appropriate for display to the student. 
5.2 Pedagogical Utility 
We have evaluated COGENT?s pedagogical util-
ity in the context of computationally identifying 
student scientific misconceptions. We have de-
veloped algorithms that reliably detect incorrect 
statements in student essays by comparing an 
expert-created domain knowledge map to an ex-
pert-created knowledge map of an essay. These 
algorithms use textual entailment techniques 
based on a shallow linguistic analysis of knowl-
edge map concepts to identify sentences that con-
tradict concepts in the domain knowledge map. 
Initial evaluation results indicate that these algo-
rithms identify incorrect statements nearly as 
adeptly as human experts. 
 
 Manual 
Expert  
Agreement 
Expert  
Knowledge 
Maps 
COGENT  
Concept  
Inventory 
Recall 0.69 0.87 0.93 
Precision 0.69 0.57 0.57 
F-Measure 0.69 0.68 0.69 
Table 4. Incorrect statement identification 
evaluation results 
 
As shown in Table 4, the algorithms detect 
87% of all incorrect statements identified by ex-
perts and 57% of the reported incorrect state-
ments agree with human judgments on the same 
task. By comparison, experts show 69% overlap 
on average along both dimensions. Introducing 
the COGENT concept inventory in place of the 
expert-created domain knowledge map improves 
recall performance, as the algorithms return 93% 
of all incorrect statements reported by the ex-
perts, while preserving 57% precision. These 
results indicate that the generated summary cov-
ers the necessary pedagogical concepts to com-
putationally identify student scientific miscon-
ceptions. 
Informal sampling of the sentences selected by 
COGENT shows the following three important 
science concepts receiving the highest scores:  
1. Earthquakes are the result of forces deep 
within the Earth's interior that continuously 
affect the surface of the Earth. 
2. Scientists believed that the movement of the 
Earth's plates bends and squeezes the rocks at 
the edges of the plates. 
3. In particular, four major scientific develop-
ments spurred the formulation of the plate-
182
tectonics theory: (1) demonstration of the 
ruggedness and youth of the ocean floor; (2) 
confirmation of repeated reversals of the 
Earth magnetic field in the geologic past; (3) 
emergence of the seafloor-spreading hypothe-
sis and associated recycling of oceanic crust; 
and (4) precise documentation that the world's 
earthquake and volcanic activity is concen-
trated along oceanic trenches and submarine 
mountain ranges. 
For a more rigorous analysis of the pedagogi-
cal utility of the COGENT concepts, we asked an 
instructional expert with domain expertise in ge-
ology to evaluate the 326 sentences returned by 
COGENT. The expert used a 5-point Likert scale 
to judge whether each concept would be peda-
gogically useful in the context of a concept in-
ventory on earthquakes and plate tectonics 
knowledge for high school age learners. The ex-
pert agreed or strongly agreed that 60% of the 
sentences would be pedagogically useful, with 
30% of the sentences being potentially useful and 
only 10% of the sentences being judged as not 
useful. These results indicate that COGENT ap-
pears to perform quite well at identifying sen-
tences that contain information relevant for 
learning about the domain. 
We have also completed an ablation study to 
identify the relative contribution of the COGENT 
features to the quality of the summary. We have 
focused on the cosine metric to capture the over-
all similarity between the COGENT concept in-
ventory and the concepts from the expert-created 
knowledge map. 
 
Features Cosine 
All Features 0.8325 
(Gazetteer) 0.5545 
(Hypertext) 0.5575 
(Educational Standards) 0.8083 
(Content Word Density) 0.8271 
 
Table 5. Feature ablation evaluation results for 
COGENT 
 
Table 5 shows the cosine similarity between 
the concept inventory generated after taking the 
feature shown in parentheses out of the summar-
izer. The results are ordered from low-to-high 
such that the feature contributing the most to the 
all-features cosine score appears at the top of the 
table. Removing either the gazetteer or the hy-
pertext feature causes the largest drops in simi-
larity indicating the importance of the use of ex-
amples and the relevance of document structure 
for the quality of the COGENT-generated sum-
mary. Meanwhile both the educational standards 
and content word density appear to provide mod-
est but useful improvements to the quality of the 
COGENT summary. 
Given that our algorithms have only been 
evaluated on the topic of earthquakes and plate 
tectonics for high school age learners, COGENT 
may be limited in its ability to transcend domains 
due to its reliance on two domain-aware sentence 
scoring features: educational standards and gaz-
etteer. However, the educational standards fea-
ture may be applicable across other science top-
ics because the AAAS Benchmarks and NSES 
provide very thorough and detailed coverage of a 
wide range of topics for the Science, Technol-
ogy, Engineering, and Math disciplines for 
grades K-12. Only the gazetteer feature would 
need to be replaced, especially given its signifi-
cant contribution to the quality of the generated 
summary as indicated by the results of the abla-
tion study. We believe these results highlight the 
need to generalize our approach, perhaps using a 
classifier for identifying examples in educational 
texts without resorting to overly domain-specific 
language resources, such as the ADL Gazetteer. 
Overall, the evaluation results indicate that our 
approach holds promise for effectively identify-
ing concepts for inclusion in the construction of a 
pedagogically useful domain knowledge map 
from educational science content. 
6 Conclusions and Future Work 
In this paper, we have presented a multi-
document summarization system, COGENT, that 
integrates bottom-up and top-down sentence 
scoring features to identify pedagogically rele-
vant concepts from educational digital library 
resources. Our results indicate that COGENT 
generates concept inventories that resemble those 
identified by experts and outperforms existing 
multi-document summarization systems. We 
have also used the COGENT concept inventory 
as input to our  misconception identification al-
gorithms and the evaluation results indicate the 
algorithms perform as well as when using an ex-
pert-created domain knowledge map. In the con-
text of generating domain knowledge maps, our 
next step is to explore how machine learning 
techniques may be employed to connect concepts 
with links. 
Automating the process of creating inventories 
of important pedagogical concepts represents an 
important step towards creating scalable intelli-
183
gent learning and tutoring systems. We hope our 
progress in this direction may contribute to in-
crease the interest within the computational lin-
guistics research community in novel educational 
technology research. 
Acknowledgments 
This research is funded in part by the National 
Science Foundation under NSF IIS/ALT Award 
0537194. Any opinions, findings, and conclu-
sions or recommendations expressed in this ma-
terial are those of the author(s) and do not neces-
sarily reflect the views of the NSF. 
References 
Ahmad, F., de la Chica, S., Butcher, K., Sumner, T. 
and Martin, J.H. (2007, June 17-23). Towards 
automatic conceptual personalization tools. In Pro-
ceedings of the 7th ACM/IEEE-CS Joint Confer-
ence on Digital Libraries, (Vancouver, British Co-
lumbia, Canada, 2007), pages 452 - 461. 
Butcher, K.R., Bhushan, S. and Sumner, T. (2006). 
Multimedia displays for conceptual discovery: in-
formation seeking with strand maps. ACM Multi-
media Systems, 11 (3), pages 236-248. 
Byrt, T., Bishop, J. and Carlin, J.B. (1993). Bias, 
prevalence, and kappa. Journal of Clinical Epide-
miology, 46 (5), pages 423-429. 
Harabagiu, S. and Lacatusu, F. (2005, August 15-19). 
Topic themes for multi-document summarization. 
In Proceedings of the 28th Annual International 
ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, (Salvador, Brazil, 
2005), pages 202-209. 
Hardy, H., Shimizu, N., Strzalkowski, T., Ting, L., 
Wise, G.B. and Zhang, X. (2002). Summarizing 
large document sets using concept-based cluster-
ing. In Proceedings of the Human Language Tech-
nology Conference 2002, (San Diego, California, 
United States, 2002), pages 222-227. 
Hill, L.L. (2000, September 18-20). Core elements of 
digital gazetteers: placenames, categories, and 
footprints. In Proceedings of the 4th European 
Conference on Digital Libraries, (Lisbon, Portugal, 
2000), pages 280-290. 
Holley, C.D. and Dansereau, D.F. (1984). Spatial 
learning strategies: Techniques, applications, and 
related issues. Academic Press, Orlando, Florida. 
Lin, C.Y. (2004). ROUGE: A package for automatic 
evaluation of summaries. In Proceedings of the 
Workshop on Text Summarization Branches Out, 
(Barcelona, Spain, 2004). 
Lin, C.Y. and Hovy, E. (2003, May-June). Automatic 
evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the Human 
Language Technology Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics, HLT-NAACL, (Edmonton, 
Canada, 2003), pages 71-78. 
Lin, D. and Pantel, P. (2002, August 24-September 1). 
Concept discovery from text. In Proceedings of the 
19th International Conference on Computational 
Linguistics, (Taipei, Taiwan, 2002), pages 1-7. 
Mani, I. (2001). Automatic Summarization. Mitkov, 
R. (Ed.) John Benjamins B.V., Amsterdam, The 
Netherlands. 
National Research Council. (1996). National Science 
Education Standards. National Academy Press, 
Washington, DC. 
Navigli, R. and Velardi, P. (2004). Learning domain 
ontologies from document warehouses and dedi-
cated websites. Computational Linguistics, 30 (2), 
pages 151-179. 
Novak, J.D. and Gowin, D.B. (1984). Learning how 
to learn. Cambridge University Press, New York, 
New York. 
O'Donnell, A.M., Dansereau, D.F. and Hall, R.H. 
(2002). Knowledge maps as scaffolds for cognitive 
processing. Educational Psychology Review, 14 
(1), pages 71-86. 
Project 2061. (1993). Benchmarks for science liter-
acy. Oxford University Press, New York, New 
York, United States. 
Radev, D.R., Jing, H. and Budzikowska, M. (2000). 
Centroid-based summarization of multiple docu-
ments: sentence extraction, utility-based evalua-
tion, and user studies. In Proceedings of the 
ANLP/NAACL 2000 Workshop on Summariza-
tion, (2000), pages 21-30. 
Salton, G. and Buckley, C. (1988). Term-weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24 (5), pages 513-
523. 
Wan, X. and Yang, J. (2006, June 5th-7th). Improved 
affinity graph based multi-document summariza-
tion. In Proceedings of the Human Language 
Technology Conference of the North American 
Chapter of the Association for Computational Lin-
guistics, (New York City, New York, 2006), pages 
181-184. 
Zouaq, A., Nkambou, R. and Frasson, C. (2007, July 
9-13). Learning a domain ontology in the Knowl-
edge Puzzle project. In Proceedings of the Fifth In-
ternational Workshop on Ontologies and Semantic 
Web for E-Learning, (Marina del Rey, California, 
2007). 
184
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 190?198, Prague, June 2007. c?2007 Association for Computational Linguistics
Towards Robust Unsupervised Personal Name Disambiguation 
Ying Chen 
Center for Spoken Language Research
University of Colorado at Boulder 
yc@colorado.edu 
James Martin 
Department of Computer Science 
University of Colorado at Boulder 
James.Martin@colorado.edu 
 
 
Abstract 
The increasing use of large open-domain 
document sources is exacerbating the 
problem of ambiguity in named entities.  
This paper explores the use of a range of 
syntactic and semantic features in unsu-
pervised clustering of documents that re-
sult from ad hoc queries containing names. 
From these experiments, we find that the 
use of robust syntactic and semantic fea-
tures can significantly improve the state of 
the art for disambiguation performance for 
personal names for both Chinese and Eng-
lish. 
1 Introduction 
An ever-increasing number of question answering, 
summarization and information extraction systems 
are coming to rely on heterogeneous sets of 
documents returned by open-domain search en-
gines from collections over which application 
developers have no control. A frequent special 
case of these applications involves queries 
containing named entities of various sorts and 
receives as a result a large set of possibly relevant 
documents upon which further deeper processing 
is focused. Not surprisingly, many, if not most, of 
the returned documents will be irrelevant to the 
goals of the application because of the massive 
ambiguity associated with the query names of 
people, places and organizations in large open 
collections. Without some means of separating 
documents that contain mentions of distinct 
entities, most of these applications will produce 
incorrect results. The work presented here, there-
fore, addresses the problem of automatically 
problem of automatically separating sets of news 
documents generated by queries containing per-
sonal names into coherent partitions. 
The approach we present here combines unsu-
pervised clustering methods with robust syntactic 
and semantic processing to automatically cluster 
returned news documents (and thereby entities) 
into homogeneous sets. This work follows on the 
work of Bagga & Baldwin (1998), Mann & 
Yarowsky (2003), Niu et al (2004), Li et al 
(2004), Pedersen et al (2005), and Malin (2005). 
The results described here advance this work 
through the use of syntactic and semantic features 
that can be robustly extracted from the kind of 
arbitrary news texts typically returned from open-
domain sources.  
The specific contributions reported here fall 
into two general areas related to robustness. In the 
first, we explore the use of features extracted from 
syntactic and semantic processing at a level that is 
robust to changes in genre and language. In par-
ticular, we seek to go beyond the kind of bag of 
local words features employed in earlier systems 
(Bagga & Baldwin, 1998; Gooi & Allan, 2004; 
Pedersen et al, 2005) that did not attempt to ex-
ploit deep semantic features that are difficult to 
extract, and to go beyond the kind of biographical 
information (Mann & Yarowsky, 2003) that is 
unlikely to occur with great frequency (such as 
place of birth, or family relationships) in many of 
the documents returned by typical search engines. 
The second contribution involves the application 
of these techniques to both English and Chinese 
news collections. As we?ll see, the methods are 
effective with both, but error analyses reveal in-
teresting differences between the two languages. 
190
The paper is organized as follows. Section 2 
addresses related work and compares our work 
with that of others. Section 3 introduces our new 
phrase-based features along two dimensions: from 
syntax to semantics; and from local sentential con-
texts to document-level contexts. Section 4 first 
describes our datasets and then analyzes the per-
formances of our system for both English and 
Chinese. Finally, we draw some conclusions. 
2 Previous work 
Personal name disambiguation is a difficult prob-
lem that has received less attention than those top-
ics that can be addressed via supervised learning 
systems. Most previous work (Bagga & Baldwin, 
1998; Mann & Yarowsky, 2003; Li et al, 2004; 
Gooi & Allan, 2004;  Malin, 2005; Pedersen et al, 
2005; Byung-Won On and Dongwon Lee, 2007) 
employed unsupervised methods because no large 
annotated corpus is available and because of the 
variety of the data distributions for different am-
biguous personal names. 
Since it is common for a single document to 
contain one or more mentions of the ambiguous 
personal name of interest, there is a need to define 
the object to be disambiguated (the ambiguous 
object). In Bagga & Baldwin (1998), Mann & 
Yarowsky (2003) and Gooi & Allan (2004), an 
ambiguous object refers to a single entity with the 
ambiguous personal name in a given document. 
The underlying assumption for this definition is 
?one person per document? (all mentions of the 
ambiguous personal name in one document refer 
to the same personal entity in reality). In Niu et al 
(2004) and Pedersen et al (2005), an ambiguous 
object is defined as a mention of the ambiguous 
personal name in a corpus.  
The first definition of the ambiguous object 
(document-level object) can include much infor-
mation derived from that document, so that it can 
be represented by rich features. The later defini-
tion of the ambiguous object (mention-level object) 
can simplify the detection of the ambiguous object, 
but because of the limited coverage, it usually can 
use only local context (the text around the men-
tion of the ambiguous personal name) and might 
miss some document-level information. The kind 
of name disambiguation based on mention-level 
objects really solves ?within-document name am-
biguity? and ?cross-document name ambiguity? 
simultaneously, and often has a higher perform-
ance than the kind based on document-level ob-
jects because two mentions of the ambiguous per-
sonal name in a document are very likely to refer 
to the same personal entity. From our news corpus, 
we also found that mentions of the ambiguous 
personal name of interest in a news article rarely 
refer to multiple entities, so our system will focus 
on the name disambiguation for document-level 
objects. 
In general, there are two types of information 
usually used in name disambiguation (Malin, 
2005): personal information and relational infor-
mation (explicit and implicit). Personal informa-
tion gives biographical information about the am-
biguous object, and relational information speci-
fies explicit or implicit relations between the am-
biguous object and other entities, such as a mem-
bership relation between ?John Smith? and ?Labor 
Party.? Usually, explicit relational information can 
be extracted from local context, and implicit rela-
tional information is far away from the mentions 
of the ambiguous object. A hard case of name dis-
ambiguation often needs implicit relational infor-
mation that provides a background for the am-
biguous object. For example, if two news articles 
in consideration report an event happening in 
?Labor Party,? this implicit relational information 
between ?John Smith? and ?Labor Party? can give 
a hint for name disambiguation if no personal or 
explicit relational information is available. 
Bagga & Baldwin (1998), Mann & Yarowsky 
(2003), Gooi & Allan (2004), Niu et al (2004), 
and Pedersen et al (2005) explore features in local 
context. Bagga & Baldwin (1998), Gooi & Allan 
(2004), and Pedersen et al (2005) use local token 
features; Mann & Yarowsky (2003) extract local 
biographical information; Niu et al (2004) use co-
occurring Named Entity (NE) phrases and NE 
relationships in local context. Most of these local 
contextual features are personal information or 
explicit relational information. 
Li et al (2004) and Malin (2005) consider 
named-entity disambiguation as a graph problem, 
and try to capture information related to the am-
biguous object beyond local context, even implicit 
relational information. Li et al (2004) use the EM 
algorithm to learn the global probability distribu-
tion among documents, entities, and representative 
mentions, and Malin (2005) constructs a social 
network graph to learn a similarity matrix.  
191
In this paper, we also explore both personal and 
relational information beyond local context. But 
we achieve it with a different approach: extracting 
these types of information by means of syntactic 
and semantic processing. We not only extract lo-
cal NE phrases as in Niu et al (2004), but also use 
our entity co-reference system to extract accurate 
and representative NE phrases occurring in a 
document which may have a relation to the am-
biguous object. At the same time, syntactic phrase 
information sometimes can overcome the imper-
fection of our NE system and therefore makes our 
disambiguation system more robust. 
3 Overall Methodology 
Our approach follows a common architecture for 
named-entity disambiguation: the detection of 
ambiguous objects, feature extraction and repre-
sentation, similarity matrix learning, and finally 
clustering. 
In our approach, all documents are preproc-
essed with a syntactic phrase chunker (Hacioglu, 
2004) and the EXERT1 system (Hacioglu et al 
2005; Chen & Hacioglu, 2006), a named-entity 
detection and co-reference resolution system that 
was developed for the ACE2 project. A syntactic 
phrase chunker segments a sentence into a se-
quence of base phrases. A base phrase is a syntac-
tic-level phrase that does not overlap another base 
phrase. Given a document, the EXERT system 
first detects all mentions of entities occurring in 
that document (named-entity detection) and then 
resolves the different mentions of an entity into 
one group that uniquely represents the entity 
(within-document co-reference resolution). The 
ACE 2005 task can detect seven types of named 
entities: person, organization, geo-political entity, 
location, facility, vehicle, and weapon; each type 
of named entity can occur in a document with any 
of three distinct formats: name, nominal construc-
tion, and pronoun. The F score of the syntactic 
phrase chunker, which is trained and tested on the 
Penn TreeBank, is 94.5, and the performances of 
the EXERT system are 82.9 (ACE value for 
named-entity detection) and 68.5 (ACE value for 
within-document co-reference resolution). 
                                                 
1 http://sds.colorado.edu/EXERT 
2 http://projects.ldc.upenn.edu/ace/ 
3.1 The detection of ambiguous objects  
In our approach, we assume that the ambiguous 
personal name has already been determined by the 
application. Moreover, we adopt the policy of 
?one person per document? as in Bagga & 
Baldwin (1998), and define an ambiguous object 
as a set of target entities given by the EXERT 
system. A target entity is an entity that has a 
mention of the ambiguous personal name. Given 
the definition of an ambiguous object, we define a 
local sentence (or local context) as a sentence that 
contains a mention of any target entity. 
3.2 Feature extraction and representation 
Since considerable personal and relational infor-
mation related to the ambiguous object resides in 
the noun phrases in the document, such as the per-
son?s job and the person?s location, we attempt to 
capture this noun phrase information along two 
dimensions: from syntax to semantics, and from 
local contexts to document-level contexts. 
Base noun phrase feature: To keep this feature 
focused, we extract only noun phrases occurring 
in the local sentences and the summarized sen-
tences (the headline + the first sentence of the 
document) of the document. The local sentences 
usually include personal or explicit relational in-
formation about the ambiguous object, and the 
summarized sentences of a news document usu-
ally give a short summary of the whole news story. 
With the syntactic phrase chunker, we develop 
two base noun phrase models: (i) Contextual base 
noun phrases (Contextual bnp), the base noun 
phrases in the local sentences; (ii) Summarized 
base noun phrases (Summarized bnp), the base 
noun phrases in the local sentences and the sum-
marized sentences. A base noun phrase of interest 
serves as an element in the feature vector. 
Named-Entity feature: Given the EXERT sys-
tem, a direct and simple way to extract some se-
mantic information is to use the named entities 
detected in the document. Based on their relation-
ship to the ambiguous personal name, the named 
entities identified in a text can be divided into 
three categories:  
(i) Target entity: an entity that has a mention 
of the ambiguous personal name. Target entities 
often include some personal information about the 
ambiguous object, such as the title, position, and 
so on.  
192
(ii) Local entity: an entity other than a target 
entity that has a mention occurring in any local 
sentence. Local entities often include entities that 
are closely related to the ambiguous object, such 
as employer, location and co-workers.  
(iii) Non-local entity: an entity that is not ei-
ther the local entity or the target entity. Non-local 
entities are often implicitly related to the ambigu-
ous object and provide background information 
for the ambiguous object. 
To assess how important these entities are to 
named-entity disambiguation, we create two kinds 
of entity models: (i) Contextual entities: the enti-
ties in the feature vector are target entities and 
local entities; (ii) Document entities: the entities 
in the feature vector include all entities in the 
document including target entities, local entities 
and non-local entities. 
Since a given entity can be represented by 
many mentions in a document, we choose a single 
representative mention to represent each entity. 
The representative mention is selected according 
to the following ordered preference list: longest 
NAME mention, longest NOMINAL mention.  A 
representative mention phrase serves as an ele-
ment in a feature vector. 
Although the mentions of contextual entities of-
ten overlap with contextual base noun phrases, the 
representative mention of a contextual entity often 
goes beyond local sentences, and is usually the 
first or longest mention of that contextual entity. 
Compared to contextual base noun phrases, the 
representative mention of a contextual entity often 
includes more detail and accurate information 
about the entity. On the other hand, the contextual 
base noun phrase feature detects all noun phrases 
occurring in local sentences that are not limited to 
the seven types of named entities discovered by 
the EXERT system. Compared to the contextual 
entity feature, the contextual base noun phrase 
Entity space 
Text space 
Feature Space 
Contextual base noun phrases?  feature vector: < Hope Mills police Capt. John Smith16, 
what16, he16, the statements16, no criminal violation16, what17, the individuals17, no direct 
threat17, Smith17, He and Thomas18, they18, Collins18, his bill18> 
Summarized base noun phrases?  feature vector: < Hope Mills police Capt. John Smith16, 
what16, he16, the statements16, no criminal violation16, what17, the individuals17, no direct 
threat17, Smith17, He and Thomas18, they18, Collins18, his bill18, Collins1, restaurant1, HOPE 
MILLS2, Commissioner Tonzie Collins2, a town restaurant2, an alleged run-in2, two work-
ers2, Feb. 212> 
Contextual entities?  feature vector: < Hope Mills police Capt. John Smith16, Jenny Tho-
mas4, Commissioner Tonzie Collins2, He and Thomas4, the individuals17> 
Document entities?  feature vector: < Hope Mills police Capt. John Smith 16, Jenny Tho-
mas4, Commissioner Tonzie Collins2, He and Thomas4, the individuals17, Andy?s 
Cheesesteaks4, HOPE MILLS 2, two workers2, the Village Shopping Center 4, Hope Mills 
Road 4 > 
Target entity:     < Hope Mills police Capt. John Smith16, he16, Smith17, He18> 
Local entity:       < Thomas18, Jenny Thomas4, manager4>,  
< Collins18, his18, Collins1, Commissioner Tonzie Collins 2>, ?? 
Non-local entity: < restaurant1, a town restaurant2, there2, Andy?s Cheesesteaks4>, ?? 
(Headline & S1) Collins banned from restaurant 
(S2) HOPE MILLS ? Commissioner Tonzie Collins has been banned from a town restau-
rant after an alleged run-in with two workers there Feb. 21. ?? 
(S4) ?In all fairness, that is not a representation of the town,? said Jenny Thomas, manager 
at Andy?s Cheesesteaks in the Village Shopping Center on Hope Mills Road. ?? 
(S16) Hope Mills police Capt. John Smith said based on what he read in the statements, 
no criminal violation was committed.  
(S17) ?Based on what the individuals involved said, there was no direct threat,? Smith said. 
(S18) He and Thomas said they don?t think Collins intentionally left without paying his 
bill. ?? 
 
Figure 1: A Sample of Feature Extraction 
193
feature is more general and can sometimes over-
come errors propagated from the named-entity 
system.  
To make this more concrete, the feature vectors 
for a document containing ?John Smith? are high-
lighted in Figure 1. The superscript number for 
each phrase refers to the sentence where the 
phrase is located, and we assume that the syntactic 
phrase chunker and the EXERT system work per-
fectly. 
3.3 Similarity matrix learning 
Given a pair of feature vectors consisting of 
phrase-based features, we need to choose a simi-
larity scheme to calculate the similarity. Because 
of the word-space delimiter in English, the feature 
vector for an English document comprises phrases, 
whereas that for a Chinese document comprises 
tokens. There are a number of similarity schemes 
for learning a similarity matrix from token-based 
feature vectors, but there are few schemes for 
phrase-based feature vectors.  
Cohen et al (2003) compared various similarity 
schemes for the task of matching English entity 
names and concluded that the hybrid scheme they 
call SoftTFIDF performs best. SoftTFIDF is a to-
ken-based similarity scheme that combines a stan-
dard TF-IDF weighting scheme with the Jaro-
Winkler distance function. Since Chinese feature 
vectors are token-based, we can directly use 
SoftTFIDF to learn the similarity matrix. However, 
English feature vectors are phrase-based, so we 
need to run SoftTFIDF iteratively and call it ?two-
level SoftTFIDF.? First, the standard SoftTFIDF 
is used to calculate the similarity between phrases 
in the pair of feature vectors; in the second phase, 
we reformulate the standard SoftTFIDF to calcu-
late the similarity for the pair of feature vectors.  
First, we introduce the standard SoftTFIDF. In 
a pair of feature vectors S and T, S = (s1, ? , sn ) 
and T = (t1, ? , tm). Here, si (i = 1?n) and tj (j = 
1?m) are substrings (tokens). Let CLOSE(?; S;T) 
be the set of substrings w?S such that there is 
some v?T satisfying dist(w; v) > ?. The Jaro-
Winkler distance function (Winkler, 1999) is 
dist(;). For w? CLOSE(?; S;T), let D(w; T) = 
);(max vwdistTv? . Then the standard SoftTFIDF 
is computed as 
)D( )V( )V(
  )( SoftTFIDF
);;(
w, Tw, Tw, S
S,T
TSCLOSEw
??
=
? ? ?   
)(IDF log  1)  (TF log  )(V' ww,Sw, S ?+=          
                               
? ?= S w, S
w, Sw, S
w
2)( V
)(  V  )( V                     
where TFw,S is the frequency of substrings w in S, 
and IDFw is the inverse of the fraction of docu-
ments in the corpus that contain w. In computing 
the similarity for the English phrase-based feature 
vectors, in the second step of ?two-level 
SoftTFIDF,? the substring w is a phrase and dist is 
the standard SoftTFIDF.  
So far, we have developed several feature mod-
els and learned the corresponding similarity ma-
trices, but clustering usually needs only one 
unique similarity matrix. Since a feature may have 
different effects for the disambiguation depending 
on the ambiguous personal name in consideration, 
to achieve the best disambiguation ability, each 
personal name may need its own weighting 
scheme to combine the given similarity matrices. 
However, learning that kind of weighting scheme 
is very difficult, so in this paper, we simply com-
bine the similarity matrices, assigning equal 
weight to each one. 
3.4 Clustering 
Although clustering is a well-studied area, a re-
maining research problem is to determine the op-
timal parameter setting during clustering, such as 
the number of clusters or the stop-threshold, a 
problem that is important for real tasks and that is 
not at all trivial. 
Since the focus of this paper is only on feature 
development, we simply employ a clustering 
method that can reflect the quality of the similar-
ity matrix for clustering. Here, we choose ag-
glomerative clustering with a single linkage. Since 
each personal name may need a different parame-
ter setting, to test the importance of the parameter 
setting for clustering, we use two kinds of stop-
thresholds for agglomerative clustering in our ex-
periments: first, to find the optimal stop-threshold 
for any ambiguous personal name and for each 
feature model, we run agglomerative clustering 
with all possible stop-thresholds, and choose the 
one that has the best performance as the optimal 
194
stop-threshold; second, we use a fixed stop-
threshold acquired from development data.  
4 Performance  
4.1 Data 
To capture the real data distribution, we use two 
sets of naturally occurring data: Bagga?s corpus 
and the Boulder Name corpus, which is a news 
corpus locally acquired from a web search. 
Bagga?s corpus is a document collection for the 
English personal name ?John Smith? that was 
used by Bagga & Baldwin (1998). There are 256 
articles that match the ?/John.*?Smith/? regular 
expression in 1996 and 1997 editions of the New 
York Times, and 94 distinct ?John Smith? personal 
entities are mentioned. Of these, 83 ?John Smiths? 
are mentioned in only one article (singleton clus-
ters containing only one object), and 11 other 
?John Smiths? are mentioned several times in the 
remaining 173 articles (non-singleton clusters 
containing more than one object). For the task of 
cross-document co-reference, Bagga & Baldwin 
(1998) chose 24 articles from 83 singleton clusters, 
and 173 other articles in 11 non-singleton clusters 
to create the final test data set ? Bagga?s corpus. 
We collected the Boulder Name corpus by first 
selecting four highly ambiguous personal names 
each in English and Chinese. For each personal 
name, we retrieved the first non-duplicated 100 
news articles from Google (Chinese) or Google 
news (English). There are four data sets for Eng-
lish personal names and four data sets for Chinese 
personal names: James Jones, John Smith, Mi-
chael Johnson, Robert Smith, and Li Gang, Li Hai, 
Liu Bo, Zhang Yong. 
Compared to Bagga?s corpus, which is limited 
to the New York Times, the documents in the 
Boulder Name corpus were collected from differ-
ent sources, and hence are more heterogeneous 
and noisy. This variety in the Boulder Name cor-
pus reflects the distribution of the real data and 
makes named-entity disambiguation harder.  
For each ambiguous personal name in both cor-
pora, the gold standard clusters have a long-tailed 
distribution - a high percentage of singleton clus-
ters plus a few non-singleton clusters. For exam-
ple, in the 111 documents containing ?John 
Smith? in the Boulder Name corpus, 53 ?John 
Smith? personal entities are mentioned. Of them, 
37 ?John Smiths? are mentioned only once. The 
long-tailed distribution brings some trouble to 
clustering, since in many clustering algorithms a 
singleton cluster is considered as a noisy point and 
therefore is ignored. 
4.2 Corpus performance 
Because of the long tail of the data set, we design 
a baseline using one cluster per document. To 
evaluate our disambiguation system, we choose 
the B-cubed scoring method that was used by 
Bagga & Baldwin (1998).  
In order to compare our work to that of others, 
we re-implement the model used by Bagga & 
Baldwin (1998). First, extracting all local sen-
tences produces a summary about the given am-
biguous object. Then, the object is represented by 
the tokens in its summary in the format of a vector, 
and the tokens in the feature vector are in their 
morphological root form and are filtered by a 
stop-word dictionary. Finally, the similarity matrix 
is learned by the TF-IDF method.   
Because both ?two-level SoftTFIDF? and ag-
glomerative clustering require a parameter setting, 
for each language, we reserve two ambiguous per-
sonal names from the Boulder Name corpus as 
development data (John Smith, Michael Johnson, 
Li Gang, Zhang Yong), and the other data are re-
served as test data: Bagga?s corpus and the other 
personal names in the Boulder Name corpus 
(Robert Smith, James Jones, Li Hai, Liu Bo).  
For any ambiguous personal name and for each 
feature model, we find the optimal stop-threshold 
for agglomerative clustering, and show the corre-
sponding performances in Table 1, Table 2 and 
Table 3. However, for the most robust feature 
model, Bagga + summarized bnp + document en-
tities, we learn the fixed stop-threshold for ag-
glomerative clustering from the development data 
(0.089 for English data and 0.078 for Chinese 
data), and show the corresponding performances 
in Table 4. 
4.2.1  Performance on Bagga?s corpus 
Table 1 shows the performance of each feature 
model for Bagga?s corpus with the optimal stop-
threshold. The metric here is the B-cubed F score 
(precision/recall).  
Because of the difference between Bagga?s re-
sources and ours (different versions of the named-
entity system and different dictionaries of the 
morphological root and the stop-words), our best 
195
B-cubed F score for Bagga?s model is 80.3? 4.3 
percent lower than the best performance reported 
by Bagga & Baldwin (1998): 84.6.  
From Table 1, we found that the syntactic fea-
tures (contextual bnp and summarized bnp) and 
semantic features (contextual entities and docu-
ment entities) consistently improve the perform-
ances, and all performances outperform the best 
result reported by Bagga & Baldwin (1998): 84.6   
 
Model B-cubed performance  
Gold standard cluster # 35 
Baseline 30.17 (100.00/17.78) 
Bagga 80.32 (94.77/69.70) 
Bagga + contextual bnp   89.16 (89.18/89.13) 
Bagga + summarized bnp 89.59 (92.60/86.78)    
Bagga + summarized bnp + contextual entities 89.60 (87.16/92.18)    
Bagga + summarized bnp + document entities 92.02 (93.10/90.97)    
Table 1:  Performances for Bagga?s corpus with the optimal stop-threshold   
 
                Name 
Model 
John Smith 
(dev) 
Michael Johnson
(dev) 
Robert Smith 
(test) 
James Jones 
(test) 
Average 
performance
Gold standard cluster # 53 52 65 24  
Baseline 64.63 (111) 67.97 (101) 78.79 (100) 37.50 (104) 62.22 
Bagga 82.63 (75) 89.07 (66) 91.56 (73) 86.42 (24) 87.42 
Bagga + contextual bnp   85.18 (62) 89.13 (65) 92.35 (74) 86.45 (22) 88.28 
Bagga + summarized bnp 85.97 (66) 91.08 (51) 93.17 (70) 90.11 (33) 90.08 
Bagga + summarized bnp 
+ contextual entities 
85.44 (70) 94.24 (55) 91.94 (73) 96.66 (24) 92.07 
Bagga + summarized bnp 
+ document entities 
91.94 (61) 92.55 (51) 93.48 (67) 97.10 (28) 93.77 
Table 2: Performances for the English Boulder Name corpus with the optimal stop-threshold  
 
                 Name 
Model 
Li Gang  
(dev) 
Zhang Yong 
(dev) 
Li Hai 
(test) 
Liu Bo 
(test) 
Average 
performance
Gold standard cluster # 57 63 57 45  
Baseline 72.61 (100) 76.83 (101) 74.03 (97) 62.07 (100) 71.39 
Bagga  96.21 (57) 96.43 (64) 94.51 (64) 91.66 (49) 94.70 
Bagga + contextual bnp   97.57 (57) 96.38 (66) 94.53 (64) 93.21 (51) 95.42 
Bagga + summarized bnp 98.50 (56) 96.17 (61) 95.38 (62) 93.21 (51) 95.81 
Bagga + summarized bnp 
+ contextual entities 
99.50 (58) 95.49 (63) 96.75 (58) 91.05 (52) 95.70 
Bagga + summarized bnp 
+ document entities 
99.50 (56) 94.57 (70) 98.57 (59) 97.02 (48) 97.41 
Table 3: Performances for the Chinese Boulder Name corpus with the optimal stop-threshold 
 
English Name John Smith 
(dev) 
Michael Johnson
(dev) 
Robert Smith 
(test) 
James Jones 
(test) 
Average 
performance
Bagga + summarized bnp 
+ document entities 
91.31 
(91.94)  
 90.57 
(92.55) 
 86.71 
(93.48) 
96.64 
(97.10) 
 91.31 
(93.77) 
Chinese Name Li Gang  
(dev) 
Zhang Yong 
(dev) 
Li Hai 
(test) 
Liu Bo 
(test) 
Average 
performance
Bagga + summarized bnp 
+ document entities 
 99.06 
(99.50) 
94.56 
(94.56) 
98.25  
(98.57) 
 89.18 
(97.02) 
 95.26 
(97.41) 
Table 4: Performances for the Boulder Name corpus with the fixed stop-threshold 
196
4.2.2 Performance on the Boulder Name cor-
pus 
Table 2 and Table 3 show the performance of each 
feature model with the optimal stop-threshold for 
the English and Chinese Boulder Name corpora, 
respectively. The metric is the B-cubed F score 
and the number in brackets is the corresponding 
cluster number. Since the same feature model has 
different contributions for different ambiguous 
personal names, we list the average performances 
for all ambiguous names in the last column in both 
tables. 
   Comparing Table 2 and Table 3, we find that 
Bagga?s model has different performances for the 
English and Chinese corpora. That means that 
contextual tokens have different contributions in 
the two languages. There are three apparent 
causes for this phenomenon. The first concerns 
the frequency of pronouns in English vs. pro-drop 
in Chinese. The typical usage of pronouns in Eng-
lish requires an accurate pronoun co-reference 
resolution that is very important for the local sen-
tence extraction in Bagga?s model. In the Boulder 
Name corpus, we found that ambiguous personal 
names occur in Chinese much more frequently 
than in English. For example, the string ?Liu Bo? 
occurs 876 times in the ?Liu Bo? data, but the 
string ?John Smith? occurs only 161 times in the 
?John Smith? data. The repetition of ambiguous 
personal names in Chinese reduces the burden on 
pronoun co-reference resolution and hence cap-
tures local information more accurately.  
The second factor is the fact that tokens in 
Bagga?s model for Chinese are words, but a Chi-
nese word is a unit bigger than an English word, 
and may contain more knowledge. For example, 
?the White House? has three words in English, 
and a word in Chinese. Since Chinese named-
entity detection can be considered a sub-problem 
of Chinese word segmentation, a word in Chinese 
can catch partial information about named entities.  
Finally, compared to Chinese news stories, 
English news stories are more likely to mention 
persons marginal to the story, and less likely to 
give the complete identifying information about 
them in local context. Those phenomena require 
more background information or implicit rela-
tional information to improve English named-
entity disambiguation. 
From Table 2 and Table 3, we see that the aver-
age performance of all ambiguous personal names 
is increased (from 87.42 to 93.77 for English and 
from 94.70 to 97.41 for Chinese) by incorporating 
more information: contextual bnp (contextual base 
noun phrases), summarized bnp (summarized base 
noun phrases), contextual entities, and document 
entities. This indicates that the phrase-based fea-
tures, the syntactic and semantic noun phrases, are 
very useful for disambiguation.  
From Table 2 and Table 3, we also see that the 
phrase-based features can improve the average 
performance, but not always for all ambiguous 
personal names. For example, the feature model 
?Bagga + summarized bnp + contextual entities? 
hurts the performance for ?Robert Smith.? As we 
mentioned above, the Boulder Name corpus is 
heterogeneous, so each feature does not make the 
same contribution to the disambiguation for any 
ambiguous personal name. What we need to do is 
to find a feature model that is robust for all am-
biguous personal names.  
In Table 4, we choose the last feature model?
Bagga + summarized bnp + document entities?as 
the final feature model, learn the fixed stop-
threshold for clustering from the development 
data, and show the corresponding performances as 
B-cubed F scores. The performances in italics are 
the performances with the optimal stop-threshold.  
From Table 4, we find that, with the exception of 
?Robert Smith? and ?Liu Bo,? the performances 
for other ambiguous personal names with the 
fixed threshold are close to the corresponding best 
performances. 
5 Conclusion 
This work has explored the problem of personal 
named-entity disambiguation for news corpora. 
Our experiments extend token-based information 
to include noun phrase-based information along 
two dimensions: from syntax to semantics, and 
from local sentential contexts to document-level 
contexts. From these experiments, we find that 
rich and broad information improves the disam-
biguation performance considerably for both Eng-
lish and Chinese. In the future, we will continue to 
explore additional semantic features that can be 
robustly extracted, including features derived 
from semantic relations and semantic role labels, 
and try to extend our work from news articles to 
197
web pages that include more noisy information. 
Finally, we have focused here primarily on feature 
development and not on clustering. We believe 
that the skewed long-tailed distribution that char-
acterizes this data requires the use of clustering 
algorithms tailored to this distribution. In particu-
lar, the large number of singleton clusters is an 
issue that confounds the standard clustering meth-
ods we have been employing. 
References 
A. Bagga and B. Baldwin. 1998. Entity?based Cross?
document Co?referencing Using the Vector Space 
Model. In 17th COLING. 
Y. Chen and K. Hacioglu. 2006. Exploration of 
Coreference Resolution: The ACE Entity Detection 
and Recognition Task. In 9th International Confer-
ence on TEXT, SPEECH and DIALOGUE. 
W. Cohen, P. Ravikumar, S. Fienberg. 2003. A Com-
parison of String Metrics for Name-Matching Tasks. 
In IJCAI-03 II-Web Workshop.  
C. H. Gooi and J. Allan. 2004. Cross-Document 
Coreference on a Large Scale Corpus. In NAACL  
K. Hacioglu, B. Douglas and Y. Chen.  2005. Detection 
of Entity Mentions Occurring in English and Chi-
nese Text. Computational Linguistics. 
K. Hacioglu. 2004. A Lightweight Semantic Chunking 
Model Based On Tagging. In HLT/NAACL. 
X. Li, P. Morie, and D. Roth. 2004. Robust Reading: 
Identification and Tracing of Ambiguous Names. In 
Proc. of  NAACL, pp. 17?24.  
B. Malin. 2005. Unsupervised Name Disambiguation 
via Social Network Similarity. SIAM. 
G. Mann and D. Yarowsky. 2003. Unsupervised Per-
sonal Name Disambiguation. In Proc. of CoNLL-
2003, Edmonton, Canada. 
C. Niu, W. Li, and R. K. Srihari. 2004. Weakly Super-
vised Learning for Cross-document Person Name 
Disambiguation Supported by Information Extrac-
tion. In ACL 
B. On and D. Lee. 2007. Scalable Name Disambigua-
tion using Multi-Level Graph Partition. SIAM. 
T. Pedersen, A. Purandare and A. Kulkarni. 2005. 
Name Discrimination by Clustering Similar Con-
texts. In Proc. of the Sixth International Conference 
on Intelligent Text Processing and Computational 
Linguistics, pages 226-237. Mexico City, Mexico. 
T. Pedersen and A. Kulkarni. 2007. Unsupervised Dis-
crimination of Person Names in Web Contexts. In 
Proc. of the Eighth International Conference on In-
telligent Text Processing and Computational Lin-
guistics. 
W. E. Winkler. 1999. The state of record linkage and 
current research  problems. Statistics of Income Di-
vision, Internal Revenue Service Publication R99/04.  
A. Yates and O. Etzioni. 2007. Unsupervised Resolu-
tion of Objects and Relations on the Web. In 
NAACL.  
198
Towards Robust Semantic Role Labeling
Sameer S. Pradhan?
BBN Technologies
Wayne Ward??
University of Colorado
James H. Martin?
University of Colorado
Most semantic role labeling (SRL) research has been focused on training and evaluating on
the same corpus. This strategy, although appropriate for initiating research, can lead to over-
training to the particular corpus. This article describes the operation of ASSERT, a state-of-the
art SRL system, and analyzes the robustness of the system when trained on one genre of data
and used to label a different genre. As a starting point, results are first presented for training
and testing the system on the PropBank corpus, which is annotated Wall Street Journal (WSJ)
data. Experiments are then presented to evaluate the portability of the system to another source of
data. These experiments are based on comparisons of performance using PropBanked WSJ data
and PropBanked Brown Corpus data. The results indicate that whereas syntactic parses and
argument identification transfer relatively well to a new corpus, argument classification does
not. An analysis of the reasons for this is presented and these generally point to the nature of the
more lexical/semantic features dominating the classification task where more general structural
features are dominant in the argument identification task.
1. Introduction
Automatic, accurate, and wide-coverage techniques that can annotate naturally oc-
curring text with semantic structure can play a key role in NLP applications such as
information extraction (Harabagiu, Bejan, and Morarescu 2005), question answering
(Narayanan and Harabagiu 2004), and summarization. Semantic role labeling (SRL) is
one method for producing such semantic structure. When presented with a sentence,
a semantic role labeler should, for each predicate in the sentence, first identify and
then label its semantic arguments. This process entails identifying groups of words
in a sentence that represent these semantic arguments and assigning specific labels to
them. In the bulk of recent work, this problem has been cast as a problem in supervised
machine learning. Using these techniques with hand-corrected syntactic parses, it has
? Department of Speech and Language Processing, 10 Moulton Street, Room 2/245, Cambridge, MA 02138.
E-mail: sameer@cemantix.org.
?? The Center for Spoken Language Research, Campus Box 594, Boulder, CO 80309.
E-mail: whw@colorado.edu.
? The Center for Spoken Language Research, Campus Box 594, Boulder, CO 80309.
E-mail: martin@colorado.edu.
Submission received: 15 July 2006; revised submission received: 3 May 2007; accepted for publication:
19 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
been possible to achieve accuracies within the range of human inter-annotator agree-
ment. More recent approaches have involved using improved features such as n-best
parses (Koomen et al 2005; Toutanova, Haghighi, and Manning 2005); exploiting argu-
ment interdependence (Jiang, Li, and Ng 2005); using information from fundamentally
different, and complementary syntactic, views (Pradhan, Ward et al 2005); combining
hypotheses from different labeling systems using inference (Ma`rquez et al 2005); as well
as applying novel learning paradigms (Punyakanok et al 2005; Toutanova, Haghighi,
and Manning 2005; Moschitti 2006) that try to capture more sequence and contextual
information. Some have also tried to jointly decode the syntactic and semantic structures
(Yi and Palmer 2005; Musillo and Merlo 2006). This problem has also been the subject
of two CoNLL shared tasks (Carreras and Ma`rquez 2004; Carreras and Ma`rquez 2005).
Although all of these systems perform quite well on the standard test data, they show
significant performance degradation when applied to test data drawn from a genre
different from the data on which the system was trained. The focus of this article is
to present results from an examination into the primary causes of the lack of portability
across genres of data.
To set the stage for these experiments we first describe the operation of ASSERT, our
state-of-the art SRL system. Results are presented for training and testing the system on
the PropBank corpus, which is annotatedWall Street Journal (WSJ) data.
Experiments are then presented to assess the portability of the system to another
genre of data. These experiments are based on comparisons of performance using
PropBanked WSJ data and PropBanked Brown corpus data. The results indicate that
whereas syntactic parses and identification of the argument bearing nodes transfer
relatively well to a new corpus, role classification does not. Analysis of the reasons for
this generally point to the nature of the more lexical/semantic features dominating the
classification task, as opposed to the more structural features that are relied upon for
identifying which constituents are associated with arguments.
2. Semantic Annotation and Corpora
In this article, we report on the task of reproducing the semantic labeling scheme used
by the PropBank corpus (Palmer, Gildea, and Kingsbury 2005). PropBank is a 300k-word
corpus in which predicate argument relations are marked for almost all occurrences
of non-copula verbs in the WSJ part of the Penn Treebank (Marcus, Santorini, and
Marcinkiewicz 1993). PropBank uses predicate independent labels that are sequential
from ARG0 to ARG5, where ARG0 is the PROTO-AGENT (usually the subject of a tran-
sitive verb) and ARG1 is the PROTO-PATIENT (usually its direct object). In addition to
these core arguments, additional adjunctive arguments, referred to as ARGMs, are also
marked. Some examples are ARGM-LOC, for locatives, and ARGM-TMP, for temporals.
Table 1 shows the argument labels associated with the predicate operate in PropBank.
Following is an example structure extracted from the PropBank corpus. The syntax
tree representation along with the argument labels is shown in Figure 1.
[ARG0 It] [predicate operates] [ARG1 stores] [ARGM?LOC mostly in Iowa and Nebraska].
The PropBank annotation scheme assumes that a semantic argument of a predicate
aligns with one or more nodes in the hand-corrected Treebank parses. Although most
frequently the arguments are identified by one node in the tree, there can be cases where
the arguments are discontinuous and more than one node is required to identify parts
of the arguments.
290
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Table 1
Argument labels associated with the predicate operate (sense: work) in the PropBank corpus.
Tag Description
ARG0 Agent, operator
ARG1 Thing operated
ARG2 Explicit patient (thing operated on)
ARG3 Explicit argument
ARG4 Explicit instrument
Treebank trees can also have trace nodes which refer to another node in the tree, but
do not have any words associated with them. These can also be marked as arguments.
As traces are typically not reproduced by current automatic parsers, we decided not
to consider them in our experiments?whether or not they represent arguments of a
predicate. None of the previous work has attempted to recover such trace arguments.
PropBank also contains arguments that are coreferential.
We treat discontinuous and coreferential arguments in accordance to the CoNLL
shared task on semantic role labeling. The first part of a discontinuous argument is
labeled as it is, and the second part of the argument is labeled with a prefix ?C-?
appended to it. All coreferential arguments are labeled with a prefix ?R-? appended.
We follow the standard convention of using Section 02 to Section 21 as the training
set, Section 00 as the development set, and Section 23 as the test set. The training set
comprises about 90,000 predicates instantiating about 250,000 arguments and the test
set comprises about 5,000 predicates instantiating about 12,000 arguments.
3. Task Description
In ASSERT, the task of semantic role labeling is implemented by assigning role labels to
constituents of a syntactic parse. Parts of the overall process can be analyzed as three
different tasks as introduced by Gildea and Jurafsky (2002):
1. Argument Identification?This is the process of identifying parsed
constituents in the sentence that represent semantic arguments of
Figure 1
Syntax tree for a sentence illustrating the PropBank tags.
291
Computational Linguistics Volume 34, Number 2
Figure 2
Syntax tree for a sentence illustrating the PropBank arguments.
a given predicate. Each node in a parse tree can be classified (with
respect to a given predicate) as either one that represents a semantic
argument (i.e., a NON-NULL node) or one that does not represent
any semantic argument (i.e., a NULL node).
2. Argument Classification?Given constituents known to represent
arguments of a predicate, this process assigns the appropriate
argument labels to them.
3. Argument Identification and Classification?A combination of the two tasks.
For example, in the tree shown in Figure 2, the node IN that dominates for is a
NULL node because it does not correspond to a semantic argument. The node NP
that dominates about 20 minutes is a NON-NULL node, because it does correspond to
a semantic argument?ARGM-TMP.
4. ASSERT (Automatic Statistical SEmantic Role Tagger)
4.1 System Architecture
ASSERT
1 produces a separate set of semantic role labels for each candidate predicate in
a sentence. Because PropBank only annotates arguments for non-copula/non-auxiliary
verbs, those are also the predicates considered by ASSERT. ASSERT performs constituent-
based role assignment. The basic inputs are a sentence and a syntactic parse of the
sentence. For each constituent in the parse tree, the system extracts a set of features
and uses a classifier to assign a label to the constituent. The set of labels used are the
PropBank argument labels plus NULL, which means no argument is assigned to that
constituent for the predicate under consideration.
Support vector machines (SVMs) (Burges 1998; Vapnik 1998) have been shown to
perform well on text classification tasks, where data is represented in a high dimen-
sional space using sparse feature vectors (Joachims 1998; Kudo and Matsumoto 2000;
Lodhi et al 2002). We formulate the problem as a multi-class classification problem
using an SVM classifier. We employ a ONE vs ALL (OVA) approach to train n classifiers
for a multi-class problem. The classifiers are trained to discriminate between examples
1 www.cemantix.org/assert.
292
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
of each class, and those belonging to all other classes combined. During testing, the
classifier scores on an example are combined to predict its class label.
ASSERT was developed using TinySVM2 along with YamCha3 (Kudo and
Matsumoto 2000, 2001) as the SVM training and classification software. The system
uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C = 1;
and, tolerance of the termination criterion, e = 0.001. SVMs output distances from the
classification hyperplane, not probabilities. These distances may not be comparable
across classifiers, especially if different features are used to train each binary classifier.
These raw SVM scores are converted to probabilities by fitting to a sigmoid function as
done by Platt (2000).
The architecture just described has the drawback that each argument classification
is made independently, without considering other arguments assigned to the same
predicate. This ignores a potentially important source of information: that a predicate is
likely to instantiate a certain set of arguments. To represent this information, a backed-
off trigram model is trained for the argument sequences. In this model, the predicate is
considered as an argument and is part of the sequence. This model represents not only
what arguments a predicate is likely to take, but also the probability of a given sequence
of arguments. During the classification process the system generates an argument
lattice using the n-best hypotheses for each node in the syntax tree. A Viterbi search
through the lattice uses the probabilities assigned by the sigmoid as the observation
probabilities, along with the argument sequence language model probabilities, to find
the maximum likelihood path such that each node is either assigned a value belonging
to the PropBank arguments, or NULL. The search is also constrained so that no two
nodes that overlap are both assigned NON-NULL labels.
4.2 Features
The feature set used in ASSERT is a combination of features described in Gildea and
Jurafsky (2002) as well as those introduced in Pradhan et al (2004), Surdeanu et al
(2003), and the syntactic-frame feature proposed in (Xue and Palmer 2004). Following is
the list of features used.
4.2.1 Predicate. This is the predicate whose arguments are being identified. The surface
form as well as the lemma are added as features.
4.2.2 Path. The syntactic path through the parse tree from the parse constituent to the
predicate being classified.
For example, in Figure 3, the path from ARG0 (The lawyers) to the predicate went is
represented with the string NP?S?VP?VBD. ? and ? represent upward and downward
movement in the tree, respectively.
4.2.3 Phrase Type. Syntactic category (NP, PP, etc.) of the constituent.
4.2.4 Position.Whether the constituent is before or after the predicate.
2 www.chasen.org/~taku/software/TinySVM/.
3 www.chasen.org/~taku/software/YamCha/.
293
Computational Linguistics Volume 34, Number 2
Figure 3
Illustration of path NP?S?VP?VBD.
4.2.5 Voice. Whether the predicate is realized as an active or passive construction. A
set of hand-written tgrep expressions operating on the syntax tree is used to identify
passives.
4.2.6 SubCategorization. This is the phrase structure rule expanding the predicate?s parent
node in the parse tree. For example, in Figure 3, the subcategorization for the predicate
?went? is VP?VBD-PP-NP.
4.2.7 Predicate Cluster. The distance function used for clustering is based on the intuition
that verbs with similar semantics will tend to have similar direct objects. For example,
verbs such as eat, devour, and savor will tend to all occur with direct objects describing
food. The clustering algorithm uses a database of verb?direct-object relations extracted
by Lin (1998). The verbs were clustered into 64 classes using the probabilistic co-
occurrence model of Hofmann and Puzicha (1998). We then use the verb class of the
current predicate as a feature.
4.2.8 Head Word. Syntactic head of the constituent.
4.2.9 Head Word POS. Part of speech of the head word.
4.2.10 Named Entities in Constituents. Binary features for seven named entities
(PERSON, ORGANIZATION, LOCATION, PERCENT, MONEY, TIME, DATE) tagged by
IdentiFinder (Bikel, Schwartz, and Weischedel 1999).
4.2.11 Path Generalizations.
1. Partial Path?Path from the constituent to the lowest common ancestor
of the predicate and the constituent.
2. Clause-based path variations?Position of the clause node (S, SBAR)
seems to be an important feature in argument identification (Hacioglu
et al 2004). Therefore we experimented with four clause-based path
feature variations.
(a) Replacing all the nodes in a path other than clause nodes with an
asterisk. For example, the path NP?S?VP?SBAR?NP?VP?VBD
becomes NP?S?*S?*?*?VBD.
294
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
(b) Retaining only the clause nodes in the path, which for the given
example would produce NP?S?S?VBD.
(c) Adding a binary feature that indicates whether the constituent is in
the same clause as the predicate.
(d) Collapsing the nodes between S nodes, which gives
NP?S?NP?VP?VBD.
3. Path n-grams?This feature decomposes a path into a series of trigrams.
For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes: NP?S?VP,
S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, and so on. Shorter paths were
padded with nulls.
4. Single character phrase tags?Each phrase category is clustered to a
category defined by the first character of the phrase label.
4.2.12 Predicate Context. We added the predicate context to capture predicate sense
variations. Two words before and two words after were added as features. The POS
of the words were also added as features.
4.2.13 Punctuation. Punctuation plays an particularly important role for some adjunctive
arguments, so punctuation on the left and right of the constituent are included as
features. The absence of punctuation in either location was indicated with a NULL
feature value.
4.2.14 Head Word of PP. Many adjunctive arguments, such as temporals and locatives,
occur as prepositional phrases in a sentence, and it is often the case that the head words
of those phrases, which are prepositions, are not very discriminative; for example, in
the city and in a few minutes both share the same head word in and neither contain a
named entity, but the former is ARGM-LOC, whereas the latter is ARGM-TMP. The head
word of the first noun phrase inside the prepositional phrase is used for this feature.
Preposition information is represented by appending it to the phrase type, for example,
?PP-in? instead of ?PP.?
4.2.15 First and Last Word/POS in Constituent. The first and last words in a constituent
along with their parts of speech.
4.2.16 Ordinal Constituent Position. In order to avoid false positives where constituents
far away from the predicate are spuriously identified as arguments, we added this
feature which is a concatenation of the constituent type and its ordinal position from
the predicate.
4.2.17 Constituent Tree Distance. This is a more fine-grained way of specifying the already
present position feature. This is the number of constituents that are encountered in the
path from the predicate to the constituent under consideration.
4.2.18 Constituent Relative Features. These are nine features representing the phrase type,
head word, and head word part of speech of the parent, and left and right siblings of
the constituent.
4.2.19 Temporal Cue Words. There are several temporal cue words that are not captured
by the named entity tagger and were added as binary features indicating their presence.
295
Computational Linguistics Volume 34, Number 2
The BOW toolkit was used to identify words and bigrams that had highest average
mutual information with the ARGM-TMP argument class.
4.2.20 Syntactic Frame. Sometimes there are multiple children under a constituent having
the same phrase type, and one or both of them represent arguments of the predicate. In
such situations, the path feature is not very good at discriminating between them, and
the position feature is also not very useful. To overcome this limitation, Xue and Palmer
(2004) proposed a feature which they call the syntactic frame. For example, if the sub-
categorization for the predicate is VP?VBD-NP-NP, then the syntactic frame feature
for the first NP in the sequence would be, ?vbd NP np,? and for the second it would be
?vbd np NP.?
4.3 Performance
Table 2 illustrates the performance of the system using Treebank parses and using parses
produced by a Charniak parser (Automatic). Precision (P), Recall (R), and F-scores are
given for the identification and combined tasks, and Classification Accuracy (A) for the
classification task. Classification performance using Charniak parses is only 1% absolute
worse than when using Treebank parses. On the other hand, argument identification
performance using Charniak parses is 10.9% absolute worse. About half of the ID errors
are due to missing constituents in the Charniak parse. Techniques to address the issue
of constituents missing from the syntactic parse tree are reported in Pradhan, Ward
et al (2005).
4.4 Feature Salience
In Pradhan, Hacioglu et al (2005) we reported on a series of experiments to show the
relative importance of features to the Identification task and the Classification task.
The data show that different features are more salient for each of the two tasks. For
the Identification task, the most salient features are the Path and Partial Path. The
Predicate was not particularly salient. For Classification, the most salient features are
Head Word, First Word, and Last Word of a constituent as well as the Predicate itself.
For Classification, the Path and Phrase Type features were not very salient.
A reasonable conclusion is that structural features dominate the Identification task,
whereas more specific lexical or semantic features are important for Classification. As
Table 2
Performance of ASSERT on WSJ test set (Section 23) using correct Treebank parses as well as
Charniak parses.
Parse Task P (%) R (%) F A (%)
Treebank Id. 97.5 96.1 96.8
Class. ? ? ? 93.0
Id. + Class. 91.8 90.5 91.2
Automatic Id. 87.8 84.1 85.9
Class. ? ? ? 92.0
Id. + Class. 81.7 78.4 80.0
296
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
we?ll see later, this pattern has critical implications for the portability of these features
across genres.
5. Robustness to Genre of Data
Most work on SRL systems has been focused on improving the labeling performance
on a test set belonging to the same genre of text as the training set. Both the Treebank on
which the syntactic parser is trained, and the PropBank on which the SRL systems are
trained represent articles from the year 1989 of theWall Street Journal. Improvements to
the system may reflect tuning to the specific data set rather than real progress. For this
technology to be widely accepted it is critical that it perform reasonably well on text
with styles different from the training data. The availability of PropBank annotation
for another corpus of a very different style than WSJ makes it possible to evaluate
the portability of SRL techniques, and to understand some of the factors affecting
performance.
5.1 The Brown Corpus
The Brown Corpus is a standard corpus of American English that consists of about one
million words of English text printed in the calendar year 1961 (Kuc?era and Francis
1967). The corpus contains about 500 samples of 2,000+ words each. The motivation
for creating this corpus was to create a heterogeneous sample of English text useful for
comparative language studies. Table 3 lists the sections in the Brown corpus.
5.2 Semantic Annotation
Release 3 of the Penn Treebank contains hand-corrected syntactic trees from a subset
of the Brown Corpus (sections F, G, K, L, M, N, P, and R). Sections belonging to the
newswire genre were not included because a considerable amount of similar material
was already available from the WSJ portion of the Treebank. Palmer, Gildea, and
Kingsbury (2005) annotated a significant portion of the Treebanked Brown corpus
Table 3
List of sections in the Brown corpus.
A. Press reportage
B. Press editorial
C. Press reviews (theater, books, music, and dance)
D. Religion
E. Skills and hobbies
F. Popular lore
G. Belles lettres, biography, memoirs, etc.
H. Miscellaneous
J. Learned
K. General fiction
L. Mystery and detective fiction
M. Science fiction
N. Adventure and Western fiction
P. Romance and love story
R. Humor
297
Computational Linguistics Volume 34, Number 2
with PropBank roles. The PropBanking philosophy is the same as described earlier.
In all, about 17,500 predicates are tagged with their semantic arguments. For these
experiments we use the release of the Brown PropBank dated September 2005.
Table 4 shows the number of predicates that have been tagged for each section:
6. Robustness Experiments
In this section, we present a series of experiments comparing the performance of ASSERT
on the WSJ corpus to performance on the Brown corpus. The intent is to understand
how well the algorithms and features transfer to other sources and to understand the
nature of any problems.
6.1 Cross-Genre Testing
The first experiment evaluates the performance of the system when it is trained on
annotated data from one genre of text (WSJ) and is used to label a test set from a different
genre (the Brown corpus). The ASSERT system described earlier, trained on WSJ Sec-
tions 02?21, was used to label arguments for the PropBanked portion of the Brown
corpus. As before, the Charniak parser was used to generate the syntax parse trees.
Table 5 shows the F-score for Identification and combined Identification and Classi-
fication for each of the eight different text genres as well as the overall performance
on Brown. As can be seen, there is a significant degradation across all the various
sections of Brown. In addition, although there is a noticeable drop in performance for
the Identification task, the bulk of the degradation comes in the combined task.
The following are among the likely factors contributing to this performance
degradation:
1. Syntactic parsing errors?The semantic role labeler is completely
dependent on the quality of the syntactic parses; missing, mislabeled,
and misplaced constituents will all lead to errors. Because the syntactic
parser used to generate the parse trees is heavily lexicalized, the genre
difference will have an impact on the accuracy of the parses, and the
features extracted from them.
2. The Brown corpus may in fact be fundamentally more difficult than the
WSJ. There are many potential sources for this kind of difficulty. Among
Table 4
Number of predicates that have been tagged in the PropBanked portion of the Brown corpus.
Section Total Propositions Total Lemmas
F 926 321
G 777 302
K 8,231 1,476
L 5,546 1,118
M 167 107
N 863 269
P 788 252
R 224 140
298
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Table 5
Performance on the entire PropBanked Brown corpus when ASSERT is trained on WSJ.
Train Test Id. F Id. + Class F
WSJ WSJ (Section 23) 85.9 80.0
WSJ Brown (Popular lore) 77.2 64.9
WSJ Brown (Biography, memoirs) 77.1 61.1
WSJ Brown (General fiction) 78.9 64.9
WSJ Brown (Detective fiction) 82.9 67.1
WSJ Brown (Science fiction) 83.8 64.5
WSJ Brown (Adventure) 82.5 65.5
WSJ Brown (Romance and love story) 81.2 63.9
WSJ Brown (Humor) 78.8 62.5
WSJ Brown (All) 81.2 63.9
Table 6
Deleted/missing argument-bearing constituents in Charniak parses of the WSJ test set
(Section 23) and the entire PropBanked Brown corpus.
Total Misses %
WSJ (Section 23) 13,612 851 6.2
Brown (Popular lore) 2,280 219 9.6
Brown (Biography, memoirs) 2,180 209 9.6
Brown (General fiction) 21,611 1,770 8.2
Brown (Detective fiction) 14,740 1,105 7.5
Brown (Science fiction) 405 23 5.7
Brown (Adventure) 2,144 169 7.9
Brown (Romance and love story) 1,928 136 7.1
Brown (Humor) 592 61 10.3
Brown (All) 45,880 3,692 8.1
the most obvious sources are a greater diversity in the range of use of
predicates and headwords in the Brown domain. That is, the lexical
features may be more varied in terms of predicate senses and raw
number of predicates. More consistent usage of predicates and
headwords in the WSJ may allow very specific features to be trained
in WSJ that will not be as well trained or as salient in Brown.
The following discussion explores each of these possibilities in turn.
Table 6 shows the percentage of argument-bearing nodes deleted from the syntactic
parse leading to an Identification error. The syntactic parser deletes 6.2% of the argu-
ment bearing nodes in the tree when it is trained and tested on WSJ. When tested on
Brown, this number increases to 8.1%, a relative increase of 30%. This effect goes some
way toward explaining the decrease in Identification performance, but does not explain
the large degradation in combined task performance.
The effect of errors from the syntactic parse can be removed by using the correct
syntactic trees from the Treebanks for both corpora. This permits an analysis of other
299
Computational Linguistics Volume 34, Number 2
factors affecting the performance difference. For this experiment, we evaluated per-
formance for all combinations of training and testing on WSJ and Brown. A test set
for the Brown corpus was generated by selecting every tenth sentence in the corpus.
The development set used by Bacchiani et al (2006) was withheld for future parameter
tuning. No parameter tuning was done for these experiments. The parameters used
for the data reported in Table 2 were used for all subsequent tests reported in this
article. This procedure results in a training set for Brown that contains approximately
14k predicates. In order to have training sets comparable in size for the two corpora,
stratified sampling was used to create a WSJ training set of the same size as the Brown
training set. Section 23 of WSJ is still used as the test set for that corpus.
Table 7 shows the results of this experiment. Rows 2 and 4 show the conditions
when the system is trained on the 14k predicate WSJ training. Testing on Brown vs. WSJ
results in a modest reduction in F-score from 95.3 to 93.0 for argument identification.
Although there is some reduction in Identification performance in the absence of errors
in the syntactic parse tree, the effect is not large. However, argument classification
shows a large drop in accuracy from 86.1% to 72.9%. These data reiterate the point that
syntactic parse errors are not the major factor accounting for the reduction in performance
for Brown.
The next point to note is the effect of varying the amount of training data for WSJ
for testing results on WSJ and Brown. The first row of Table 7 shows the performance
when ASSERT is trained on the full WSJ training set of Sections 2?21 (90k predicates).
The second row shows performance when it is trained on the reduced set of 14k pred-
icates. Whereas the F1 score for Identification dropped by 1.5 percentage points (from
96.8% to 95.3%) the Classification rate dropped by 6.9% percent absolute. Classification
seemingly requires considerable more data before its performance begins to asymptote.
Table 7
Performance when ASSERT is trained using correct Treebank parses, and is used to classify test
set from either the same genre or another. For each data set, the number of examples used for
training are shown in parentheses.
SRL Train SRL Test Task P (%) R (%) F A (%)
WSJ WSJ Id. 97.5 96.1 96.8
(90k) (5k) Class. 93.0
Id. + Class. 91.8 90.5 91.2
WSJ WSJ Id. 96.3 94.4 95.3
(14k) (5k) Class. 86.1
Id. + Class. 84.4 79.8 82.0
BROWN BROWN Id. 95.7 94.9 95.2
(14k) (1.6k) Class. 80.1
Id. + Class. 79.9 77.0 78.4
WSJ BROWN Id. 94.6 91.5 93.0
(14k) (1.6k) Class. 72.9
Id. + Class. 72.1 67.2 69.6
BROWN WSJ Id. 94.9 93.8 94.3
(14k) (5k) Class. 78.3
Id. + Class. 76.6 73.3 74.9
300
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Finally, row 3 shows the performance for training and testing on Brown. The
performance of argument Identification is essentially the same as when training and
testing on WSJ. However, argument Classification is 6 percentage points worse (80.1%
vs. 86.1%) when training and testing on Brown than when training and testing on WSJ.
This pattern is consistent with our third hypothesis given previously: Brown may be an
intrinsically harder corpus for this task.
Some possible causes for this difficulty are:
1. More unique predicates or head words than are seen in the WSJ set, so
there is less training data for each;
2. More predicate sense ambiguity in Brown;
3. Less consistent relations between predicates and head words;
4. A greater preponderance of difficult semantic roles in Brown;
5. Relatively fewer examples of predictive features such as named entities.
The remainder of this section explores each of these possibilities in turn.
In order to test the importance of predicate sense in this process, we added oracle
predicate sense information as a feature in ASSERT. Because only about 60% of the
PropBanked Brown corpus was tagged with predicate sense information, these results
are not directly comparable to the one reported in the earlier tables. In this case, both the
Brown training and test sets are subsets of the earlier ones, with about 10k predicates
in training and 1k in testing. For comparison, we used the same size WSJ training
data. Table 8 shows the performance when trained on WSJ and Brown, and tested on
Brown, with andwithout predicate sense information, and for both Treebank parses and
Charniak parses. We find that there is a small increase in the combined identification
and classification performance when trained on Brown and tested on Brown.
One reason for this could simply be the raw number of instances that are seen in
the training data. Because we know that Predicate and Head Word are two particularly
salient features for classification, the percentages of a combination of these features in
the Brown test set that are seen in both the training sets should be informative. This
information is shown in Table 9. In order to get a cross-corpus statistic, we also present
the same numbers on the WSJ test set.
Table 8
Performance on Brown test, using Brown and WSJ training sets, with and without oracle
predicate sense information when using Treebank parses.
Id. Id. + Class.
Train Predicate Sense P % R % F P % R % F
Brown
(10k) ? 95.6 95.4 95.5 78.6 76.2 77.4?
95.7 95.7 95.7 81.1 77.1 79.0
WSJ
(10k) ? 93.4 91.7 92.5 71.1 65.8 68.4?
93.3 91.8 92.5 71.3 66.1 68.6
301
Computational Linguistics Volume 34, Number 2
Table 9
Features seen in training for various test sets.
Test? WSJ Brown
Features T seen t seen T seen t seen
Corpora ? (%) (%) (%) (%)
WSJ Predicate Lemma (P) 76 94 65 80
Predicate Sense (S) 79 93 64 78
Head Word (HW) 61 87 49 76
P+HW 19 31 13 17
Brown Predicate Lemma (P) 64 85 86 94
Predicate Sense (S) 29 35 91 96
Head Word (HW) 37 63 68 87
P+HW 10 17 27 33
T = types; t = tokens.
It can be seen that for both theWSJ and Brown corpus test sets, the number of predi-
cate lemmas as well as the particular senses seen in the respective test sets is quite high.
However, a cross comparison shows that there is about a 15% drop in coverage from
WSJ/WSJ to WSJ/Brown. It is also interesting to note that for WSJ, the drop in coverage
for predicate lemmas is almost the same as that for individual predicate senses. This fur-
ther confirms the hypothesis thatWSJ has a more homogeneous collection of predicates.
When we compare the drop in coverage for Brown/Brown vs. WSJ/Brown, we find
about the same drop in coverage for predicate lemmas, but a much more significant
drop for the senses. This variation in senses in Brown is probably the reason that adding
sense information helps more for the Brown test set. In the WSJ case, the addition of
word sense as a feature does not add much information, and so the numbers are not
much different than for the baseline. Similarly, we can see that percentage of headwords
seen across the two genres also drop significantly, and they are much lower to begin
with. Finding the coverage for the predicate lemma and head word combination is still
worse, and this is not even considering the sense. Therefore, data sparseness is another
potential reason that the importance of the predicate sense feature does not reflect in the
performance numbers.
As noted earlier, another possible source of difficulty for Brown may be the distri-
bution of PropBank arguments in this corpus. Table 10 shows the classification perfor-
mance for each argument, for each of the four configurations (train on Brown orWSJ and
test on WSJ or Brown). Among the two most frequent arguments?ARG0 and ARG1?
ARG1 seems to be affected the most. When the training and test sets are from the same
genre, the performance on ARG0 is slightly worse on the Brown test set. ARG1 on the
other hand is about 5% worse on both precision and recall, when trained and tested on
Brown. For core-arguments ARG2?5 which are highly predicate sense dependent, there
is a much larger performance drop.
Finally, another possible reason for the drop in performance is the distribution of
named entities in the corpus. Table 11 shows the frequency of occurrence of name
entities in 10k WSJ and Brown training sets. It can be seen that number of organizations
talked about in Brown is much smaller than in WSJ, and there are more person names.
Also, monetary amounts which frequently fill the ARG3 and ARG4 slots are also much
more infrequent in Brown, and so is the incidence of percentages. This would definitely
have some impact on the usability of these features in the learned models.
302
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
7. Effect of Improved Syntactic Parses
Practical natural language processing systems will always use errorful automatic
parses, and so it would be interesting to find out how much syntactic parser errors hin-
der performance on the task of semantic role labeling. Fortunately, recent improvements
to the Charniak parser provided an opportunity to test this hypothesis. We use the latest
version of the Charniak parser that does n-best re-ranking (Charniak and Johnson 2005)
and the model that is self-trained using the North American News corpus (NANC).
This version adaptsmuch better to the Brown corpus (McClosky, Charniak, and Johnson
Table 10
Classification accuracy for each argument type in the WSJ (W) and Brown (B) test sets.
W?W B?B B?W W?B
Number in Number in P R P R P R P R
Argument WSJ Test Brown Test (%) (%) (%) (%) (%) (%) (%) (%)
ARG0 3,149 1,122 91.1 96.8 90.4 92.8 83.4 92.2 87.4 93.3
ARG1 4,264 1,375 90.2 92.0 85.0 88.5 78.7 79.7 83.4 89.0
ARG2 796 312 73.3 66.6 65.9 60.6 49.7 56.4 59.5 48.1
ARG3 128 25 74.3 40.6 71.4 20.0 30.8 16.0 28.6 4.7
ARG4 72 20 89.1 68.1 57.1 60.0 16.7 5.0 61.1 15.3
C-ARG0 2 4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
C-ARG1 165 34 91.5 64.8 80.0 35.3 64.7 32.4 82.1 19.4
R-ARG0 189 45 83.1 93.7 82.7 95.6 62.5 88.9 76.8 77.2
R-ARG1 122 44 77.8 63.1 91.7 75.0 64.5 45.5 54.5 59.8
ARGM-ADV 435 290 78.0 66.0 67.6 64.8 74.7 44.8 49.9 71.0
ARGM-CAU 65 15 82.5 72.3 80.0 53.3 62.5 66.7 86.0 56.9
ARGM-DIR 72 114 57.1 50.0 71.0 62.3 46.6 36.0 39.7 43.1
ARGM-DIS 270 65 87.6 86.7 81.0 72.3 54.1 70.8 89.6 64.1
ARGM-EXT 31 10 83.3 48.4 0.0 0.0 0.0 0.0 33.3 3.2
ARGM-LOC 317 147 73.8 80.8 60.8 70.7 52.6 48.3 60.6 65.6
ARGM-MNR 305 144 56.1 59.0 64.5 63.2 42.6 55.6 51.4 48.9
ARGM-MOD 454 129 99.6 100.0 100.0 100.0 100.0 99.2 99.6 100.0
ARGM-NEG 201 85 100.0 99.5 97.7 98.8 100.0 85.9 94.8 99.5
ARGM-PNC 99 43 60.4 58.6 66.7 55.8 54.8 39.5 52.8 57.6
ARGM-PRD 5 8 0.0 0.0 33.3 12.5 0.0 0.0 0.0 0.0
ARGM-TMP 978 280 85.4 90.4 84.8 85.4 71.3 83.6 82.2 76.0
W?B = ASSERT trained on B and used to classify W test set.
Table 11
Distribution of the named entities in a 10k data fromWSJ and Brown corpora.
Name Entity WSJ Brown
PERSON 1,274 2,037
ORGANIZATION 2,373 455
LOCATION 1,206 555
MONEY 831 32
DATE 710 136
PERCENT 457 5
TIME 9 21
303
Computational Linguistics Volume 34, Number 2
Table 12
Performance for different versions of the Charniak parser used in the experiments.
Train Test F
WSJ WSJ 91.0
WSJ Brown 85.2
Brown Brown 88.4
WSJ+NANC Brown 87.9
2006a, 2006b). We also use another model that is trained on the Brown corpus itself. The
performance of these parsers is shown in Table 12.
We describe the results of the following five experiments:
1. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked WSJ sentences. The syntactic parser (Charniak
parser) is itself trained on the WSJ training sections of the Treebank. This
is used to classify Section 23 of WSJ.
2. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked WSJ sentences. The syntactic parser (Charniak
parser) is itself trained on the WSJ training sections of the Treebank. This
is used to classify the Brown test set.
3. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked Brown corpus sentences. The syntactic parser
is trained using the WSJ portion of the Treebank. This is used to classify
the Brown test set.
4. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked Brown corpus sentences. The syntactic parser
is trained using the Brown training portion of the Treebank. This is used
to classify the Brown test set.
5. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked Brown corpus sentences. The syntactic parser
is the version that is self-trained using 2,500,000 sentences from NANC,
and where the starting version is trained only on WSJ data (McClosky,
Charniak, and Johnson 2006b). This is used to classify the Brown test set.
The same training and test sets used for the systems in Table 7 are used in this
experiment. Table 13 shows the results. For simplicity of discussion we have labeled the
five conditions as A, B, C, D, and E. Comparing conditions B and C shows that when the
features used to train ASSERT are extracted using a syntactic parser that is trained onWSJ
it performs at almost the same level on the task of identification, regardless of whether
it is trained on the PropBanked Brown corpus or the PropBanked WSJ corpus. This,
however, is about 5?6 F-score points lower than when all the three (the syntactic parser
training set, ASSERT training set, and ASSERT test set) are from the same genre?WSJ or
Brown, as seen in A and D. For the combined task, the gap between the performance
for conditions B and C is about 10 F-score points apart (59.1 vs. 69.8). Looking at the
argument classification accuracies, we see that using ASSERT trained on WSJ to test
Brown sentences results in a 12-point drop in F-score. Using ASSERT trained on Brown
304
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Table 13
Performance on WSJ and Brown test sets when ASSERT is trained on features extracted from
automatically generated syntactic parses.
Setup Parser Train SRL Train SRL Test Task P (%) R (%) F A (%)
A. WSJ WSJ WSJ Id. 87.3 84.8 86.0
(40k ? sec:00?21) (14k) (5k) Class. 84.1
Id. + Class. 77.5 69.7 73.4
B. WSJ WSJ Brown Id. 81.7 78.3 79.9
(40k ? sec:00?21) (14k) (1.6k) Class. 72.1
Id. + Class. 63.7 55.1 59.1
C. WSJ Brown Brown Id. 81.7 78.3 80.0
(40k ? sec:00?21) (14k) (1.6k) Class. 79.2
Id. + Class. 78.2 63.2 69.8
D. Brown Brown Brown Id. 87.6 82.3 84.8
(20k) (14k) (1.6k) Class. 78.9
Id. + Class. 77.4 62.1 68.9
E. WSJ+NANC Brown Brown Id. 87.7 82.5 85.0
(2,500k) (14k) (1.6k) Class. 79.9
Id. + Class. 77.2 64.4 70.0
H. WSJ+NANC Brown WSJ Id. 88.2 78.2 82.8
(2,500k) (14k) (5k) Class. 76.9
Id. + Class. 75.4 51.6 61.2
using the WSJ-trained syntactic parser reduces accuracy by about 5 F-score points.
When ASSERT is trained on Brown using a syntactic parser also trained on Brown, we
get a quite similar classification performance, which is again about 5 points lower than
what we get using all WSJ data. Finally, looking at conditions C and D we find that
the difference in performance on the combined task of identification and classification
using the Brown corpus for training ASSERT is very close (69.8 vs. 68.9) even though
the syntactic parser used in C has a performance that is about 3.2 points worse than
that used in D. This indicates that better parse structure is less important than lexical
semantic coverage for obtaining better performance on the Brown corpus.
8. Adapting to a New Genre
One possible way to ameliorate the effects of domain specificity is to incrementally
add small amounts of data from a new domain to the already available out-of-domain
training data. In the following experiments we explore this possibility by slowly adding
data from the Brown corpus to a fixed amount of WSJ data.
One section of the Brown corpus?section K?has about 8,200 predicates anno-
tated. Therefore, we will take six different scenarios?two in which we will use correct
Treebank parses, and the four others in which we will use automatically generated
parses using the variations used before. All training sets start with the same number
of examples as that of the Brown training set. The part of this section used as a test set
for the CoNLL 2005 shared task was used as the test set for these experiments. This test
set contains 804 predicates in 426 sentences of Brown section K.
305
Computational Linguistics Volume 34, Number 2
Table 14 shows the results. In all six settings, the performance on the task of
identification and classification improves gradually until about 5,625 examples of sec-
tion K, which is about 75% of the total added, above which it adds very little. Even
when the syntactic parser is trained on WSJ and the SRL is trained on WSJ, adding
7,500 instances of this new genre achieves almost the same performance as when all
three are from the same genre (67.2 vs. 69.9). For the task of argument identification, the
incremental addition of data from the new genre shows only minimal improvement.
The system that uses a self-trained syntactic parser performs slightly better than other
Table 14
Effect of incrementally adding data from a new genre.
Id. Id. + Class
Parser Train SRL Train P (%) R (%) F P (%) R (%) F
WSJ WSJ (14k) (Treebank parses)
(Treebank +0 examples from K 96.2 91.9 94.0 74.1 66.5 70.1
parses) +1,875 examples from K 96.1 92.9 94.5 77.6 71.3 74.3
+3,750 examples from K 96.3 94.2 95.1 79.1 74.1 76.5
+5,625 examples from K 96.4 94.8 95.6 80.4 76.1 78.1
+7,500 examples from K 96.4 95.2 95.8 80.2 76.1 78.1
Brown Brown (14k) (Treebank parses)
(Treebank +0 examples from K 96.1 94.2 95.1 77.1 73.0 75.0
parses) +1,875 examples from K 96.1 95.4 95.7 78.8 75.1 76.9
+3,750 examples from K 96.3 94.6 95.3 80.4 76.9 78.6
+5,625 examples from K 96.2 94.8 95.5 80.4 77.2 78.7
+7,500 examples from K 96.3 95.1 95.7 81.2 78.1 79.6
WSJ WSJ (14k)
(40k) +0 examples from K 83.1 78.8 80.9 65.2 55.7 60.1
+1,875 examples from K 83.4 79.3 81.3 68.9 57.5 62.7
+3,750 examples from K 83.9 79.1 81.4 71.8 59.3 64.9
+5,625 examples from K 84.5 79.5 81.9 74.3 61.3 67.2
+7,500 examples from K 84.8 79.4 82.0 74.8 61.0 67.2
WSJ Brown (14k)
(40k) +0 examples from K 85.7 77.2 81.2 74.4 57.0 64.5
+1,875 examples from K 85.7 77.6 81.4 75.1 58.7 65.9
+3,750 examples from K 85.6 78.1 81.7 76.1 59.6 66.9
+5,625 examples from K 85.7 78.5 81.9 76.9 60.5 67.7
+7,500 examples from K 85.9 78.1 81.7 76.8 59.8 67.2
Brown Brown (14k)
(20k) +0 examples from K 87.6 80.6 83.9 76.0 59.2 66.5
+1,875 examples from K 87.4 81.2 84.1 76.1 60.0 67.1
+3,750 examples from K 87.5 81.6 84.4 77.7 62.4 69.2
+5,625 examples from K 87.5 82.0 84.6 78.2 63.5 70.1
+7,500 examples from K 87.3 82.1 84.6 78.2 63.2 69.9
WSJ+NANC Brown (14k)
(2,500k) +0 examples from K 89.1 81.7 85.2 74.4 60.1 66.5
+1,875 examples from K 88.6 82.2 85.2 76.2 62.3 68.5
+3,750 examples from K 88.3 82.6 85.3 76.8 63.6 69.6
+5,625 examples from K 88.3 82.4 85.2 77.7 63.8 70.0
+7,500 examples from K 88.9 82.9 85.8 78.2 64.9 70.9
306
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
versions that use automatically generated syntactic parses. The improvement on the
identification performance is almost exclusively due to recall. The precision numbers
are almost unaffected, except when the labeler is trained on WSJ PropBank data.
9. Conclusions
In this article, we have presented results from a state-of-the-art Semantic Role Labeling
system trained on PropBankWSJ data and then used to label test sets from both theWSJ
corpus and the Brown corpus. The system?s performance on the Brown test set exhibited
a large drop compared to the WSJ test set. An analysis of these results revealed that the
subtask of Identification, determining which constituents of a syntax tree are arguments
of a predicate, is responsible for only a relatively small part of the drop in performance.
The Classification task, assigning labels to constituents known to be arguments, is where
the major performance loss occurs.
Several possible factors were examined to determine their effect on this perfor-
mance difference:
 The syntactic parser was trained on WSJ. It was shown that errors in the
syntactic parse are not a large factor in the overall performance difference.
The syntactic parser does not show a large degradation in performance
when run on Brown. Even more telling, there is still a large drop in
performance when training and testing using Treebank parses.
When the system was trained and tested on Brown, the performance was still
significantly worse than training and testing on WSJ, even when the amount of training
data is controlled for. Training and testing on Brown showed performance intermediate
between training and testing on WSJ and training on WSJ and testing on Brown. This
leads to our final hypothesis.
 The Brown corpus is in some sense fundamentally more difficult for this
problem. The most obvious reason for this is that it represents a more
heterogeneous source than the WSJ. Among the likely manifestations of
this is that predicates tend to have a single dominating sense in WSJ and
are more polysemous in Brown. Data was presented using gold-standard
word sense information for the predicates for training and testing Brown.
Adding predicate sense information has a large effect for some predicates,
but over the whole Brown test set has only a small effect. Fewer predicates
and headwords could allow very specific modeling of high frequency
predicates, and predicate?headword relations do have a large effect on
overall performance.
The initial experiment is a case of training on homogeneous data and testing on
different data. The more homogeneous training data allows the system to rely heavily
on specific features and relations in the data. It is usually the case that training on a
more heterogeneous data set does not give quite as high performance on test data from
the same corpus as more homogeneous data, but the heterogeneous data ports better to
other corpora. This is seen when training on Brown compared to WSJ. The observation
that the Identification task ports well while the classification task does not is consistent
with this explanation. For the Identification task, structural features such as path and
307
Computational Linguistics Volume 34, Number 2
partial path tend to be the most salient while the Classification task relies more heavily
on lexical/semantic features such as specific predicate-head word combinations.
The question now is what to do about this. Two possibilities are:
 Less homogeneous corpora?Rather than using many examples drawn
from one source, fewer examples could be drawn from many sources. This
would reduce the likelihood of learning idiosyncratic senses and argument
structures for predicates.
 Less specific features?Features, and the values they take on, should be
designed to reduce the likelihood of learning idiosyncratic aspects of the
training domain. Examples of this might include the use of more general
named entity classes, and the use of abstractions over specific headwords
and predicates rather than the words themselves.
Both of these manipulations would, in all likelihood, reduce performance on both
the training data and on test sets of the same genre as the training data. But they
would be more likely to lead to better generalization across genres. Training on very
homogeneous training sets and testing on similar test sets gives amisleading impression
of the performance of a system.
Acknowledgments
We are extremely grateful to Martha Palmer
for providing us with the PropBanked
Brown corpus, and to David McClosky for
providing us with hypotheses on the Brown
test set as well as a cross-validated version
of the Brown training data for the various
models reported in his work reported at
HLT 2006.
This research was partially supported by
the ARDA AQUAINT program via contract
OCG4423B and by the NSF via grants
IS-9978025 and ITR/HCI 0086132. Computer
time was provided by NSF ARI Grant
CDA-9601817, NSF MRI Grant CNS-0420873,
NASA AIST grant NAG2-1646, DOE SciDAC
grant DE-FG02-04ER63870, NSF sponsorship
of the National Center for Atmospheric
Research, and a grant from the IBM Shared
University Research (SUR) program.
References
Bacchiani, Michiel, Michael Riley, Brian
Roark, and Richard Sproat. 2006. MAP
adaptation of stochastic grammars.
Computer Speech and Language, 20(1):41?68.
Bikel, Daniel M., Richard Schwartz, and
Ralph M. Weischedel. 1999. An algorithm
that learns what?s in a name.Machine
Learning, 34:211?231.
Burges, Christopher J. C. 1998. A tutorial
on support vector machines for pattern
recognition. Data Mining and Knowledge
Discovery, 2(2):121?167.
Carreras, Xavier and Llu??s Ma`rquez. 2004.
Introduction to the CoNLL-2004 shared
task: Semantic role labeling. In Proceedings
of the Eighth Conference on Computational
Natural Language Learning (CoNLL),
pages 89?97, Boston, MA.
Carreras, Xavier and Llu??s Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning (CoNLL),
pages 152?164, Ann Arbor, MI.
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and maxent
discriminative reranking. In Proceedings of
the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL),
Ann Arbor, MI.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Hacioglu, Kadri, Sameer Pradhan, Wayne
Ward, James Martin, and Daniel Jurafsky.
2004. Semantic role labeling by tagging
syntactic chunks. In Proceedings of the
Eighth Conference on Computational
Natural Language Learning (CoNLL),
Boston, MA.
Harabagiu, Sanda, Cosmin Adrian Bejan,
and Paul Morarescu. 2005. Shallow
semantics for relation extraction. In
Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1061?1067, Edinburgh,
Scotland.
308
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Hofmann, Thomas and Jan Puzicha. 1998.
Statistical models for co-occurrence
data. Memo, Massachusetts Institute
of Technology Artificial Intelligence
Laboratory, Cambridge, MA.
Jiang, Zheng Ping, Jia Li, and Hwee Tou Ng.
2005. Semantic argument classification
exploiting argument interdependence. In
Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1067?1072, Edinburgh,
Scotland.
Joachims, Thorsten. 1998. Text categorization
with support vector machines: Learning
with many relevant features. In Proceedings
of the European Conference on Machine
Learning (ECML), pages 137?142,
Chemnitz, Germany.
Koomen, Peter, Vasin Punyakanok, Dan
Roth, and Wen-tau Yih. 2005. Generalized
inference with multiple semantic role
labeling systems. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL), pages 181?184,
Ann Arbor, MI.
Kuc?era, Henry and W. Nelson Francis. 1967.
Computational Analysis of Present-day
American English. Brown University Press,
Providence, RI.
Kudo, Taku and Yuji Matsumoto. 2000.
Use of support vector learning for chunk
identification. In Proceedings of the Fourth
Conference on Computational Natural
Language Learning (CoNLL), pages 142?144,
Lisbon, Portugal.
Kudo, Taku and Yuji Matsumoto. 2001.
Chunking with support vector machines.
In Proceedings of the Second Meeting of the
North American Chapter of the Association
for Computational Linguistics (NAACL),
Pittsburgh, PA.
Lin, Dekang. 1998. Automatic retrieval
and clustering of similar words. In
Proceedings of the Seventeenth International
Conference on Computational Linguistics
and Thirty Sixth Annual Meeting of the
Association of Computational Linguistics
(COLING/ACL), pages 768?774, Montreal,
Canada.
Lodhi, Huma, Craig Saunders, John
Shawe-Taylor, Nello Cristianini, and
Chris Watkins. 2002. Text classification
using string kernels. Journal of Machine
Learning Research, 2(Feb):419?444.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn treebank. Computational Linguistics,
19(2):313?330.
Ma`rquez, Llu??s, Mihai Surdeanu, Pere
Comas, and Jordi Turmo. 2005. A robust
combination strategy for semantic role
labeling. In Proceedings of the Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP),
pages 644?651, Vancouver, British
Columbia.
McClosky, David, Eugene Charniak,
and Mark Johnson. 2006a. Effective
self-training for parsing. In Proceedings
of the Human Language Technology
Conference/North American Chapter
of the Association of Computational
Linguistics (HLT/NAACL), pages 152?159,
New York, NY.
McClosky, David, Eugene Charniak, and
Mark Johnson. 2006b. Reranking and
self-training for parser adaptation. In
Proceedings of the Twenty First International
Conference on Computational Linguistics
and Forty Fourth Annual Meeting of the
Association for Computational Linguistics
(COLING/ACL), pages 337?344, Sydney,
Australia.
Moschitti, Alessandro. 2006. Syntactic
kernels for natural language learning:
The semantic role labeling case. In
Proceedings of the Human Language
Technology Conference/North American
Chapter of the Association of Computational
Linguistics (HLT/NAACL), pages 97?100,
New York, NY.
Musillo, Gabriele and Paola Merlo. 2006.
Accurate parsing of the proposition bank.
In Proceedings of the Human Language
Technology Conference/North American
Chapter of the Association of Computational
Linguistics (HLT/NAACL), pages 101?104,
New York, NY.
Narayanan, Srini and Sanda Harabagiu.
2004. Question answering based on
semantic structures. In Proceedings of the
International Conference on Computational
Linguistics (COLING), pages 693?701,
Geneva, Switzerland.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The Proposition Bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Platt, John. 2000. Probabilities for support
vector machines. In A. Smola, P. Bartlett,
B. Scholkopf, and D. Schuurmans, editors,
Advances in Large Margin Classifiers. MIT
Press, Cambridge, MA, pages 61?74.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James Martin,
and Dan Jurafsky. 2005. Support vector
309
Computational Linguistics Volume 34, Number 2
learning for semantic argument classification.
Machine Learning Journal, 60(1):11?39.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James Martin, and Dan Jurafsky.
2004. Shallow semantic parsing using
support vector machines. In Proceedings
of the Human Language Technology Conference/
North American Chapter of the Association of
Computational Linguistics (HLT/NAACL),
pages 233?240, Boston, MA.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James Martin, and Dan Jurafsky.
2005. Semantic role labeling using different
syntactic views. In Proceedings of the
Forty-Third Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 581?588, Ann Arbor, MI.
Punyakanok, Vasin, Dan Roth, Wen tau Yih,
and Dav Zimak. 2005. Learning and
inference over constrained output. In
Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1117?1123, Edinburgh,
Scotland.
Surdeanu, Mihai, Sanda Harabagiu,
John Williams, and Paul Aarseth. 2003.
Using predicate-argument structures for
information extraction. In Proceedings of the
Forty-First Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 8?15, Sapporo, Japan.
Toutanova, Kristina, Aria Haghighi, and
Christopher Manning. 2005. Joint learning
improves semantic role labeling. In
Proceedings of the Forty-Third Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 589?596,
Ann Arbor, MI.
Vapnik, Vladimir. 1998. Statistical Learning
Theory. Wiley, New York.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
labeling. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 88?94,
Barcelona, Spain.
Yi, Szu-ting and Martha Palmer. 2005. The
integration of syntactic parsing and
semantic role labeling. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL), pages 237?240,
Ann Arbor, MI.
310
Shallow Semantic Parsing using Support Vector Machines?
Sameer Pradhan, Wayne Ward,
Kadri Hacioglu, James H. Martin
Center for Spoken Language Research,
University of Colorado, Boulder, CO 80303
{spradhan,whw,hacioglu,martin}@cslr.colorado.edu
Dan Jurafsky
Department of Linguistics
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
In this paper, we propose a machine learning al-
gorithm for shallow semantic parsing, extend-
ing the work of Gildea and Jurafsky (2002),
Surdeanu et al (2003) and others. Our al-
gorithm is based on Support Vector Machines
which we show give an improvement in perfor-
mance over earlier classifiers. We show perfor-
mance improvements through a number of new
features and measure their ability to general-
ize to a new test set drawn from the AQUAINT
corpus.
1 Introduction
Automatic, accurate and wide-coverage techniques that
can annotate naturally occurring text with semantic argu-
ment structure can play a key role in NLP applications
such as Information Extraction, Question Answering and
Summarization. Shallow semantic parsing ? the process
of assigning a simple WHO did WHAT to WHOM, WHEN,
WHERE, WHY, HOW, etc. structure to sentences in text,
is the process of producing such a markup. When pre-
sented with a sentence, a parser should, for each predicate
in the sentence, identify and label the predicate?s seman-
tic arguments. This process entails identifying groups of
words in a sentence that represent these semantic argu-
ments and assigning specific labels to them.
In recent work, a number of researchers have cast this
problem as a tagging problem and have applied vari-
ous supervised machine learning techniques to it (Gildea
and Jurafsky (2000, 2002); Blaheta and Charniak (2000);
Gildea and Palmer (2002); Surdeanu et al (2003); Gildea
and Hockenmaier (2003); Chen and Rambow (2003);
Fleischman and Hovy (2003); Hacioglu and Ward (2003);
Thompson et al (2003); Pradhan et al (2003)). In this
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IS-9978025
paper, we report on a series of experiments exploring this
approach.
For the initial experiments, we adopted the approach
described by Gildea and Jurafsky (2002) (G&J) and eval-
uated a series of modifications to improve its perfor-
mance. In the experiments reported here, we first re-
placed their statistical classification algorithm with one
that uses Support Vector Machines and then added to the
existing feature set. We evaluate results using both hand-
corrected TreeBank syntactic parses, and actual parses
from the Charniak parser.
2 Semantic Annotation and Corpora
We will be reporting on results using PropBank1 (Kings-
bury et al, 2002), a 300k-word corpus in which predi-
cate argument relations are marked for part of the verbs
in the Wall Street Journal (WSJ) part of the Penn Tree-
Bank (Marcus et al, 1994). The arguments of a verb are
labeled ARG0 to ARG5, where ARG0 is the PROTO-
AGENT (usually the subject of a transitive verb) ARG1
is the PROTO-PATIENT (usually its direct object), etc.
PropBank attempts to treat semantically related verbs
consistently. In addition to these CORE ARGUMENTS,
additional ADJUNCTIVE ARGUMENTS, referred to as
ARGMs are also marked. Some examples are ARGM-
LOC, for locatives, and ARGM-TMP, for temporals. Fig-
ure 1 shows the syntax tree representation along with the
argument labels for an example structure extracted from
the PropBank corpus.
Most of the experiments in this paper, unless speci-
fied otherwise, are performed on the July 2002 release
of PropBank. A larger, cleaner, completely adjudicated
version of PropBank was made available in Feb 2004.
We will also report some final best performance numbers
on this corpus. PropBank was constructed by assigning
semantic arguments to constituents of the hand-corrected
TreeBank parses. The data comprise several sections of
the WSJ, and we follow the standard convention of using
1http://www.cis.upenn.edu/?ace/
Section-23 data as the test set. Section-02 to Section-
21 were used for training. In the July 2002 release, the
training set comprises about 51,000 sentences, instantiat-
ing about 132,000 arguments, and the test set comprises
2,700 sentences instantiating about 7,000 arguments. The
Feb 2004 release training set comprises about 85,000 sen-
tences instantiating about 250,000 arguments and the test
set comprises 5,000 sentences instantiating about 12,000
arguments.
[ARG0 He] [predicate talked] for [ARGM?TMP about
20 minutes].
S
hhhh
((((
NP
PRP
He
ARG0
VP
hhhh
((((
VBD
talked
predicate
PP
hhh
(((
IN
for
NULL
NP
hhhhh
(((((
about 20 minutes
ARGM ? TMP
Figure 1: Syntax tree for a sentence illustrating the Prop-
Bank tags.
3 Problem Description
The problem of shallow semantic parsing can be viewed
as three different tasks.
Argument Identification ? This is the process of identi-
fying parsed constituents in the sentence that represent
semantic arguments of a given predicate.
Argument Classification ? Given constituents known to
represent arguments of a predicate, assign the appropri-
ate argument labels to them.
Argument Identification and Classification ? A combina-
tion of the above two tasks.
Each node in the parse tree can be classified as either
one that represents a semantic argument (i.e., a NON-
NULL node) or one that does not represent any seman-
tic argument (i.e., a NULL node). The NON-NULL nodes
can then be further classified into the set of argument la-
bels. For example, in the tree of Figure 1, the node IN
that encompasses ?for? is a NULL node because it does
not correspond to a semantic argument. The node NP
that encompasses ?about 20 minutes? is a NON-NULL
node, since it does correspond to a semantic argument
? ARGM-TMP.
4 Baseline Features
Our baseline system uses the same set of features in-
troduced by G&J. Some of the features, viz., predicate,
voice and verb sub-categorization are shared by all the
nodes in the tree. All the others change with the con-
stituent under consideration.
? Predicate ? The predicate itself is used as a feature.
? Path ? The syntactic path through the parse tree
from the parse constituent to the predicate being
classified. For example, in Figure 1, the path from
ARG0 ? ?He? to the predicate talked, is represented
with the string NP?S?VP?VBD. ? and ? represent
upward and downward movement in the tree respec-
tively.
? Phrase Type ? This is the syntactic category (NP,
PP, S, etc.) of the phrase/constituent corresponding
to the semantic argument.
? Position ? This is a binary feature identifying
whether the phrase is before or after the predicate.
? Voice ? Whether the predicate is realized as an ac-
tive or passive construction.
? Head Word ? The syntactic head of the phrase. This
is calculated using a head word table described by
(Magerman, 1994) and modified by (Collins, 1999,
Appendix. A).
? Sub-categorization ? This is the phrase struc-
ture rule expanding the predicate?s parent node
in the parse tree. For example, in Figure 1, the
sub-categorization for the predicate talked is
VP?VBD-PP.
5 Classifier and Implementation
We formulate the parsing problem as a multi-class clas-
sification problem and use a Support Vector Machine
(SVM) classifier (Hacioglu et al, 2003; Pradhan et al
2003). Since SVMs are binary classifiers, we have to con-
vert the multi-class problem into a number of binary-class
problems. We use the ONE vs ALL (OVA) formalism,
which involves training n binary classifiers for a n-class
problem.
Since the training time taken by SVMs scales exponen-
tially with the number of examples, and about 80% of the
nodes in a syntactic tree have NULL argument labels, we
found it efficient to divide the training process into two
stages, while maintaining the same accuracy:
1. Filter out the nodes that have a very high probabil-
ity of being NULL. A binary NULL vs NON-NULL
classifier is trained on the entire dataset. A sigmoid
function is fitted to the raw scores to convert the
scores to probabilities as described by (Platt, 2000).
2. The remaining training data is used to train OVA
classifiers, one of which is the NULL-NON-NULL
classifier.
With this strategy only one classifier (NULL vs NON-
NULL) has to be trained on all of the data. The remaining
OVA classifiers are trained on the nodes passed by the
filter (approximately 20% of the total), resulting in a con-
siderable savings in training time.
In the testing stage, we do not perform any filtering
of NULL nodes. All the nodes are classified directly
as NULL or one of the arguments using the classifier
trained in step 2 above. We observe no significant per-
formance improvement even if we filter the most likely
NULL nodes in a first pass.
For our experiments, we used TinySVM2 along with
YamCha3 (Kudo and Matsumoto, 2000)
(Kudo and Matsumoto, 2001) as the SVM training and
test software. The system uses a polynomial kernel with
degree 2; the cost per unit violation of the margin, C=1;
and, tolerance of the termination criterion, e=0.001.
6 Baseline System Performance
Table 1 shows the baseline performance numbers on the
three tasks mentioned earlier; these results are based on
syntactic features computed from hand-corrected Tree-
Bank (hence LDC hand-corrected) parses.
For the argument identification and the combined iden-
tification and classification tasks, we report the precision
(P), recall (R) and the F14 scores, and for the argument
classification task we report the classification accuracy
(A). This test set and all test sets, unless noted otherwise
are Section-23 of PropBank.
Classes Task P R F1 A
(%) (%) (%)
ALL Id. 90.9 89.8 90.4
ARGs Classification - - - 87.9
Id. + Classification 83.3 78.5 80.8
CORE Id. 94.7 90.1 92.3
ARGs Classification - - - 91.4
Id. + Classification 88.4 84.1 86.2
Table 1: Baseline performance on all three tasks using
hand-corrected parses.
7 System Improvements
7.1 Disallowing Overlaps
The system as described above might label two con-
stituents NON-NULL even if they overlap in words. This
is a problem since overlapping arguments are not allowed
in PropBank. Among the overlapping constituents we re-
tain the one for which the SVM has the highest confi-
dence, and label the others NULL. The probabilities ob-
tained by applying the sigmoid function to the raw SVM
scores are used as the measure of confidence. Table 2
shows the performance of the parser on the task of iden-
tifying and labeling semantic arguments using the hand-
corrected parses. On all the system improvements, we
perform a ?2 test of significance at p = 0.05, and all the
2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/
3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/
4F1 = 2PRP+R
significant improvements are marked with an ?. In this
system, the overlap-removal decisions are taken indepen-
dently of each other.
P R F1
(%) (%)
Baseline 83.3 78.5 80.8
No Overlaps 85.4 78.1 ?81.6
Table 2: Improvements on the task of argument identi-
fication and classification after disallowing overlapping
constituents.
7.2 New Features
We tested several new features. Two were obtained from
the literature ? named entities in constituents and head
word part of speech. Other are novel features.
1. Named Entities in Constituents ? Following
Surdeanu et al (2003), we tagged 7 named en-
tities (PERSON, ORGANIZATION, LOCATION,
PERCENT, MONEY, TIME, DATE) using Identi-
Finder (Bikel et al, 1999) and added them as 7
binary features.
2. Head Word POS ? Surdeanu et al (2003) showed
that using the part of speech (POS) of the head word
gave a significant performance boost to their system.
Following that, we experimented with the addition
of this feature to our system.
3. Verb Clustering ? Since our training data is rel-
atively limited, any real world test set will con-
tain predicates that have not been seen in training.
In these cases, we can benefit from some informa-
tion about the predicate by using predicate clus-
ter as a feature. The verbs were clustered into 64
classes using the probabilistic co-occurrence model
of Hofmann and Puzicha (1998). The clustering al-
gorithm uses a database of verb-direct-object rela-
tions extracted by Lin (1998). We then use the verb
class of the current predicate as a feature.
4. Partial Path ? For the argument identification task,
path is the most salient feature. However, it is also
the most data sparse feature. To overcome this prob-
lem, we tried generalizing the path by adding a new
feature that contains only the part of the path from
the constituent to the lowest common ancestor of the
predicate and the constituent, which we call ?Partial-
Path?.
5. Verb Sense Information ? The arguments that a
predicate can take depend on the word sense of the
predicate. Each predicate tagged in the PropBank
corpus is assigned a separate set of arguments de-
pending on the sense in which it is used. Table 3
illustrates the argument sets for the predicate talk.
Depending on the sense of the predicate talk, either
ARG1 or ARG2 can identify the hearer. Absence of
this information can be potentially confusing to the
learning mechanism.
Talk sense 1: speak sense 2: persuade/dissuade
Tag Description Tag Description
ARG0 Talker ARG0 Talker
ARG1 Subject ARG1 Talked to
ARG2 Hearer ARG2 Secondary action
Table 3: Argument labels associated with the two senses
of predicate talk in PropBank corpus.
We added the oracle sense information extracted
from PropBank, to our features by treating each
sense of a predicate as a distinct predicate.
6. Head Word of Prepositional Phrases ? Many ad-
junctive arguments, such as temporals and locatives,
occur as prepositional phrases in a sentence, and
it is often the case that the head words of those
phrases, which are always prepositions, are not very
discriminative, eg., ?in the city?, ?in a few minutes?,
both share the same head word ?in? and neither
contain a named entity, but the former is ARGM-
LOC, whereas the latter is ARGM-TMP. Therefore,
we tried replacing the head word of a prepositional
phrase, with that of the first noun phrase inside the
prepositional phrase. We retained the preposition in-
formation by appending it to the phrase type, eg.,
?PP-in? instead of ?PP?.
7. First and Last Word/POS in Constituent ? Some
arguments tend to contain discriminative first and
last words so we tried using them along with their
part of speech as four new features.
8. Ordinal constituent position ? In order to avoid
false positives of the type where constituents far
away from the predicate are spuriously identified as
arguments, we added this feature which is a concate-
nation of the constituent type and its ordinal position
from the predicate.
9. Constituent tree distance ? This is a finer way of
specifying the already present position feature.
10. Constituent relative features ? These are nine fea-
tures representing the phrase type, head word and
head word part of speech of the parent, and left and
right siblings of the constituent in focus. These were
added on the intuition that encoding the tree context
this way might add robustness and improve general-
ization.
11. Temporal cue words ? There are several temporal
cue words that are not captured by the named entity
tagger and were considered for addition as a binary
feature indicating their presence.
12. Dynamic class context ? In the task of argument
classification, these are dynamic features that repre-
sent the hypotheses of at most previous two nodes
belonging to the same tree as the node being classi-
fied.
8 Feature Performance
Table 4 shows the effect each feature has on the ar-
gument classification and argument identification tasks,
when added individually to the baseline. Addition of
named entities improves the F1 score for adjunctive ar-
guments ARGM-LOC from 59% to ?68% and ARGM-
TMP from 78.8% to ?83.4%. But, since these arguments
are small in number compared to the core arguments, the
overall accuracy does not show a significant improve-
ment. We found that adding this feature to the NULL vs
NON-NULL classifier degraded its performance. It also
shows the contribution of replacing the head word and the
head word POS separately in the feature where the head
of a prepositional phrase is replaced by the head word
of the noun phrase inside it. Apparently, a combination
of relative features seem to have a significant improve-
ment on either or both the classification and identification
tasks, and so do the first and last words in the constituent.
Features Class ARGUMENT ID
Acc.
P R F1
Baseline 87.9 93.7 88.9 91.3
+ Named entities 88.1 - - -
+ Head POS ?88.6 94.4 90.1 ?92.2
+ Verb cluster 88.1 94.1 89.0 91.5
+ Partial path 88.2 93.3 88.9 91.1
+ Verb sense 88.1 93.7 89.5 91.5
+ Noun head PP (only POS) ?88.6 94.4 90.0 ?92.2
+ Noun head PP (only head) ?89.8 94.0 89.4 91.7
+ Noun head PP (both) ?89.9 94.7 90.5 ?92.6
+ First word in constituent ?89.0 94.4 91.1 ?92.7
+ Last word in constituent ?89.4 93.8 89.4 91.6
+ First POS in constituent 88.4 94.4 90.6 ?92.5
+ Last POS in constituent 88.3 93.6 89.1 91.3
+ Ordinal const. pos. concat. 87.7 93.7 89.2 91.4
+ Const. tree distance 88.0 93.7 89.5 91.5
+ Parent constituent 87.9 94.2 90.2 ?92.2
+ Parent head 85.8 94.2 90.5 ?92.3
+ Parent head POS ?88.5 94.3 90.3 ?92.3
+ Right sibling constituent 87.9 94.0 89.9 91.9
+ Right sibling head 87.9 94.4 89.9 ?92.1
+ Right sibling head POS 88.1 94.1 89.9 92.0
+ Left sibling constituent ?88.6 93.6 89.6 91.6
+ Left sibling head 86.9 93.9 86.1 89.9
+ Left sibling head POS ?88.8 93.5 89.3 91.4
+ Temporal cue words ?88.6 - - -
+ Dynamic class context 88.4 - - -
Table 4: Effect of each feature on the argument identifi-
cation and classification tasks when added to the baseline
system.
We tried two other ways of generalizing the head word:
i) adding the head word cluster as a feature, and ii) replac-
ing the head word with a named entity if it belonged to
any of the seven named entities mentioned earlier. Nei-
ther method showed any improvement. We also tried gen-
eralizing the path feature by i) compressing sequences of
identical labels, and ii) removing the direction in the path,
but none showed any improvement on the baseline.
8.1 Argument Sequence Information
In order to improve the performance of their statistical ar-
gument tagger, G&J used the fact that a predicate is likely
to instantiate a certain set of arguments. We use a similar
strategy, with some additional constraints: i) argument
ordering information is retained, and ii) the predicate is
considered as an argument and is part of the sequence.
We achieve this by training a trigram language model on
the argument sequences, so unlike G&J, we can also es-
timate the probability of argument sets not seen in the
training data. We first convert the raw SVM scores to
probabilities using a sigmoid function. Then, for each
sentence being parsed, we generate an argument lattice
using the n-best hypotheses for each node in the syn-
tax tree. We then perform a Viterbi search through the
lattice using the probabilities assigned by the sigmoid
as the observation probabilities, along with the language
model probabilities, to find the maximum likelihood path
through the lattice, such that each node is either assigned
a value belonging to the PROPBANK ARGUMENTs, or
NULL.
CORE ARGs/ P R F1
Hand-corrected parses (%) (%)
Baseline w/o overlaps 90.0 86.1 88.0
Common predicate 90.8 86.3 88.5
Specific predicate lemma 90.5 87.4 ?88.9
Table 5: Improvements on the task of argument identifi-
cation and tagging after performing a search through the
argument lattice.
The search is constrained in such a way that no two
NON-NULL nodes overlap with each other. To simplify
the search, we allowed only NULL assignments to nodes
having a NULL likelihood above a threshold. While train-
ing the language model, we can either use the actual pred-
icate to estimate the transition probabilities in and out
of the predicate, or we can perform a joint estimation
over all the predicates. We implemented both cases con-
sidering two best hypotheses, which always includes a
NULL (we add NULL to the list if it is not among the
top two). On performing the search, we found that the
overall performance improvement was not much differ-
ent than that obtained by resolving overlaps as mentioned
earlier. However, we found that there was an improve-
ment in the CORE ARGUMENT accuracy on the combined
task of identifying and assigning semantic arguments,
given hand-corrected parses, whereas the accuracy of the
ADJUNCTIVE ARGUMENTS slightly deteriorated. This
seems to be logical considering the fact that the ADJUNC-
TIVE ARGUMENTS are not linguistically constrained in
any way as to their position in the sequence of argu-
ments, or even the quantity. We therefore decided to
use this strategy only for the CORE ARGUMENTS. Al-
though, there was an increase in F1 score when the lan-
guage model probabilities were jointly estimated over all
the predicates, this improvement is not statistically signif-
icant. However, estimating the same using specific predi-
cate lemmas, showed a significant improvement in accu-
racy. The performance improvement is shown in Table 5.
9 Best System Performance
The best system is trained by first filtering the most
likely nulls using the best NULL vs NON-NULL classi-
fier trained using all the features whose argument identi-
fication F1 score is marked in bold in Table 4, and then
training a ONE vs ALL classifier using the data remain-
ing after performing the filtering and using the features
that contribute positively to the classification task ? ones
whose accuracies are marked in bold in Table 4. Table 6
shows the performance of this system.
Classes Task Hand-corrected parses
P R F1 A
(%) (%) (%)
ALL Id. 95.2 92.5 93.8
ARGs Classification - - - 91.0
Id. + Classification 88.9 84.6 86.7
CORE Id. 96.2 93.0 94.6
ARGs Classification - - - 93.9
Id. + Classification 90.5 87.4 88.9
Table 6: Best system performance on all tasks using
hand-corrected parses.
10 Using Automatic Parses
Thus far, we have reported results using hand-corrected
parses. In real-word applications, the system will have
to extract features from an automatically generated
parse. To evaluate this scenario, we used the Charniak
parser (Chaniak, 2001) to generate parses for PropBank
training and test data. We lemmatized the predicate using
the XTAG morphology database5 (Daniel et al, 1992).
Table 7 shows the performance degradation when
automatically generated parses are used.
11 Using Latest PropBank Data
Owing to the Feb 2004 release of much more and com-
pletely adjudicated PropBank data, we have a chance to
5ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.5/morph-
1.5.tar.gz
Classes Task Automatic parses
P R F1 A
(%) (%) (%)
ALL Id. 89.3 82.9 86.0
ARGs Classification - - - 90.0
Id. + Classification 84.0 75.3 79.4
CORE Id. 92.0 83.3 87.4
ARGs Classification - - - 90.5
Id. + Classification 86.4 78.4 82.2
Table 7: Performance degradation when using automatic
parses instead of hand-corrected ones.
report our performance numbers on this data set. Table 8
shows the same information as in previous Tables 6 and
7, but generated using the new data. Owing to time limi-
tations, we could not get the results on the argument iden-
tification task and the combined argument identification
and classification task using automatic parses.
ALL ARGs Task P R F1 A
(%) (%) (%)
HAND Id. 96.2 95.8 96.0
Classification - - - 93.0
Id. + Classification 89.9 89.0 89.4
AUTOMATIC Classification - - - 90.1
Table 8: Best system performance on all tasks using
hand-corrected parses using the latest PropBank data.
12 Feature Analysis
In analyzing the performance of the system, it is useful
to estimate the relative contribution of the various feature
sets used. Table 9 shows the argument classification ac-
curacies for combinations of features on the training and
test data, using hand-corrected parses, for all PropBank
arguments.
Features Accuracy
(%)
All 91.0
All except Path 90.8
All except Phrase Type 90.8
All except HW and HW -POS 90.7
All except All Phrases ?83.6
All except Predicate ?82.4
All except HW and FW and LW -POS ?75.1
Path, Predicate 74.4
Path, Phrase Type 47.2
Head Word 37.7
Path 28.0
Table 9: Performance of various feature combinations on
the task of argument classification.
In the upper part of Table 9 we see the degradation in
performance by leaving out one feature or a feature fam-
ily at a time. After the addition of all the new features,
it is the case that removal of no individual feature except
predicate degrades the classification performance signifi-
cantly, as there are some other features that provide com-
plimentary information. However, removal of predicate
information hurts performance significantly, so does the
removal of a family of features, eg., all phrase types, or
the head word (HW), first word (FW) and last word (LW)
information. The lower part of the table shows the per-
formance of some feature combinations by themselves.
Table 10 shows the feature salience on the task of ar-
gument identification. One important observation we can
make here is that the path feature is the most salient fea-
ture in the task of argument identification, whereas it is
the least salient in the task of argument classification. We
could not provide the numbers for argument identifica-
tion performance upon removal of the path feature since
that made the SVM training prohibitively slow, indicating
that the SVM had a very hard time separating the NULL
class from the NON-NULL class.
Features P R F1
(%) (%)
All 95.2 92.5 93.8
All except HW 95.1 92.3 93.7
All except Predicate 94.5 91.9 93.2
Table 10: Performance of various feature combinations
on the task of argument identification
13 Comparing Performance with Other
Systems
We compare our system against 4 other shallow semantic
parsers in the literature. In comparing systems, results are
reported for all the three types of tasks mentioned earlier.
13.1 Description of the Systems
The Gildea and Palmer (G&P) System.
The Gildea and Palmer (2002) system uses the same
features and the same classification mechanism used by
G&J. These results are reported on the December 2001
release of PropBank.
The Surdeanu et al System.
Surdeanu et al (2003) report results on two systems
using a decision tree classifier. One that uses exactly the
same features as the G&J system. We call this ?Surdeanu
System I.? They then show improved performance of an-
other system ? ?Surdeanu System II,? which uses some
additional features. These results are are reported on the
July 2002 release of PropBank.
The Gildea and Hockenmaier (G&H) System
The Gildea and Hockenmaier (2003) system uses fea-
tures extracted from Combinatory Categorial Grammar
(CCG) corresponding to the features that were used by
G&J and G&P systems. CCG is a form of dependency
grammar and is hoped to capture long distance relation-
ships better than a phrase structure grammar. The fea-
tures are combined using the same algorithm as in G&J
and G&P. They use a slightly newer ? November 2002 re-
lease of PropBank. We will refer to this as ?G&H System
I?.
The Chen and Rambow (C&R) System
Chen and Rambow report on two different systems,
also using a decision tree classifier. The first ?C&R Sys-
tem I? uses surface syntactic features much like the G&P
system. The second ?C&R System II? uses additional
syntactic and semantic representations that are extracted
from a Tree Adjoining Grammar (TAG) ? another gram-
mar formalism that better captures the syntactic proper-
ties of natural languages.
Classifier Accuracy
(%)
SVM 88
Decision Tree (Surdeanu et al, 2003) 79
Gildea and Palmer (2002) 77
Table 11: Argument classification using same features
but different classifiers.
13.2 Comparing Classifiers
Since two systems, in addition to ours, report results us-
ing the same set of features on the same data, we can
directly assess the influence of the classifiers. G&P sys-
tem estimates the posterior probabilities using several dif-
ferent feature sets and interpolate the estimates, while
Surdeanu et al (2003) use a decision tree classifier. Ta-
ble 11 shows a comparison between the three systems for
the task of argument classification.
13.3 Argument Identification (NULL vs NON-NULL)
Table 12 compares the results of the task of identify-
ing the parse constituents that represent semantic argu-
ments. As expected, the performance degrades consider-
ably when we extract features from an automatic parse as
opposed to a hand-corrected parse. This indicates that the
syntactic parser performance directly influences the argu-
ment boundary identification performance. This could be
attributed to the fact that the two features, viz., Path and
Head Word that have been seen to be good discriminators
of the semantically salient nodes in the syntax tree, are
derived from the syntax tree.
Classes System Hand Automatic
P R F1 P R F1
ALL SVM 95 92 94 89 83 86
ARGs Surdeanu System II - - 89 - - -
Surdeanu System I 85 84 85 - - -
Table 12: Argument identification
13.4 Argument Classification
Table 13 compares the argument classification accuracies
of various systems, and at various levels of classification
granularity, and parse accuracy. It can be seen that the
SVM System performs significantly better than all the
other systems on all PropBank arguments.
Classes System Hand Automatic
Accuracy Accuracy
ALL SVM 91 90
ARGs G&P 77 74
Surdeanu System II 84 -
Surdeanu System I 79 -
CORE SVM 93.9 90.5
ARGs C&R System II 93.5 -
C&R System I 92.4 -
Table 13: Argument classification
13.5 Argument Identification and Classification
Table 14 shows the results for the task where the system
first identifies candidate argument boundaries and then
labels them with the most likely argument. This is the
hardest of the three tasks outlined earlier. SVM does a
very good job of generalizing in both stages of process-
ing.
Classes System Hand Automatic
P R F1 P R F1
ALL SVM 89 85 87 84 75 79
ARGs G&H System I 76 68 72 71 63 67
G&P 71 64 67 58 50 54
CORE SVM System 90 87 89 86 78 82
ARGs G&H System I 82 79 80 76 73 75
C&R System II - - - 65 75 70
Table 14: Identification and classification
14 Generalization to a New Text Source
Thus far, in all experiments our unseen test data was
selected from the same source as the training data.
In order to see how well the features generalize to
texts drawn from a similar source, we used the classifier
trained on PropBank training data to test data drawn from
the AQUAINT corpus (LDC, 2002). We annotated 400
sentences from the AQUAINT corpus with PropBank
arguments. This is a collection of text from the New
York Times Inc., Associated Press Inc., and Xinhua
News Service (PropBank by comparison is drawn from
Wall Street Journal). The results are shown in Table 15.
Task P R F1 A
(%) (%) (%)
ALL Id. 75.8 71.4 73.5 -
ARGs Classification - - - 83.8
Id. + Classification 65.2 61.5 63.3 -
CORE Id. 88.4 74.4 80.8 -
ARGs Classification - - - 84.0
Id. + Classification 75.2 63.3 68.7 -
Table 15: Performance on the AQUAINT test set.
There is a significant drop in the precision and recall
numbers for the AQUAINT test set (compared to the pre-
cision and recall numbers for the PropBank test set which
were 84% and 75% respectively). One possible reason
for the drop in performance is relative coverage of the
features on the two test sets. The head word, path and
predicate features all have a large number of possible val-
ues and could contribute to lower coverage when moving
from one domain to another. Also, being more specific
they might not transfer well across domains.
Features Arguments non-Arguments
(%) (%)
Predicate, Path 87.60 2.91
Predicate, Head Word 48.90 26.55
Cluster, Path 96.31 4.99
Cluster, Head Word 83.85 60.14
Path 99.13 15.15
Head Word 93.02 90.59
Table 16: Feature Coverage on PropBank test set using
parser trained on PropBank training set.
Features Arguments non-Arguments
(%) (%)
Predicate, Path 62.11 4.66
Predicate, Head Word 30.26 17.41
Cluster, Path 87.19 10.68
Cluster, Head Word 65.82 45.43
Path 96.50 29.26
Head Word 84.65 83.54
Table 17: Coverage of features on AQUAINT test set us-
ing parser trained on PropBank training set.
Table 16 shows the coverage for features on the hand-
corrected PropBank test set. The tables show feature
coverage for constituents that were Arguments and con-
stituents that were NULL. About 99% of the predicates in
the AQUAINT test set were seen in the PropBank train-
ing set. Table 17 shows coverage for the same features on
the AQUAINT test set. We believe that the drop in cover-
age of the more predictive feature combinations explains
part of the drop in performance.
15 Conclusions
We have described an algorithm which significantly im-
proves the state-of-the-art in shallow semantic parsing.
Like previous work, our parser is based on a supervised
machine learning approach. Key aspects of our results
include significant improvement via an SVM classifier,
improvement from new features and a series of analytic
experiments on the contributions of the features. Adding
features that are generalizations of the more specific fea-
tures seemed to help. These features were named enti-
ties, head word part of speech and verb clusters. We also
analyzed the transferability of the features to a new text
source.
We would like to thank Ralph Weischedel and Scott Miller of
BBN Inc. for letting us use their named entity tagger ? Iden-
tiFinder; Martha Palmer for providing us with the PropBank
data, Valerie Krugler for tagging the AQUAINT test set with
PropBank arguments, and all the anonymous reviewers for their
helpful comments.
References
[Bikel et al1999] Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel.
1999. An algorithm that learns what?s in a name. Machine Learning, 34:211?
231.
[Blaheta and Charniak2000] Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In NAACL, pages 234?240.
[Chaniak2001] Eugene Chaniak. 2001. Immediate-head parsing for language
models. In ACL-01.
[Chen and Rambow2003] John Chen and Owen Rambow. 2003. Use of deep
linguistics features for the recognition and labeling of semantic arguments.
EMNLP-03.
[Collins1999] Michael John Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania,
Philadelphia.
[Daniel et al1992] K. Daniel, Y. Schabes, M. Zaidel, and D. Egedi. 1992. A freely
available wide coverage morphological analyzer for English. In COLING-92.
[Fleischman and Hovy2003] Michael Fleischman and Eduard Hovy. 2003. A
maximum entropy approach to framenet tagging. In HLT-03.
[Gildea and Hockenmaier2003] Dan Gildea and Julia Hockenmaier. 2003. Identi-
fying semantic roles using combinatory categorial grammar. In EMNLP-03.
[Gildea and Jurafsky2000] Daniel Gildea and Daniel Jurafsky. 2000. Automatic
labeling of semantic roles. In ACL-00, pages 512?520.
[Gildea and Jurafsky2002] Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics, 28(3):245?288.
[Gildea and Palmer2002] Daniel Gildea and Martha Palmer. 2002. The necessity
of syntactic parsing for predicate argument recognition. In ACL-02.
[Hacioglu and Ward2003] Kadri Hacioglu and Wayne Ward. 2003. Target word
detection and semantic role chunking using support vector machines. In HLT-
03.
[Hacioglu et al2003] Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Mar-
tin, and Dan Jurafsky. 2003. Shallow semantic parsing using support vector
machines. Technical Report TR-CSLR-2003-1, Center for Spoken Language
Research, Boulder, Colorado.
[Hofmann and Puzicha1998] Thomas Hofmann and Jan Puzicha. 1998. Statistical
models for co-occurrence data. Memo, MIT AI Laboratory.
[Kingsbury et al2002] Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002.
Adding semantic annotation to the Penn Treebank. In HLT-02.
[Kudo and Matsumoto2000] Taku Kudo and Yuji Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In CoNLL-00.
[Kudo and Matsumoto2001] Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In NAACL-01.
[LDC2002] LDC. 2002. The AQUAINT Corpus of English News Text, Catalog
no. LDC2002t31.
[Lin1998] Dekang Lin. 1998. Automatic retrieval and clustering of similar words.
In COLING-98.
[Magerman1994] David Magerman. 1994. Natural Language Parsing as Statisti-
cal Pattern Recognition. Ph.D. thesis, Stanford University, CA.
[Marcus et al1994] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn TreeBank: Annotating predicate argument structure.
[Platt2000] John Platt. 2000. Probabilities for support vector machines. In
A. Smola, P. Bartlett, B. Scolkopf, and D. Schuurmans, editors, Advances in
Large Margin Classifiers. MIT press.
[Pradhan et al2003] Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Mar-
tin, and Dan Jurafsky. 2003. Semantic role parsing: Adding semantic struc-
ture to unstructured text. In ICDM-03.
[Surdeanu et al2003] Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul
Aarseth. 2003. Using predicate-argument structures for information extrac-
tion. In ACL-03.
[Thompson et al2003] Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for semantic role labeling. In ECML-03.
Parsing Arguments of Nominalizations in English and Chinese?
Sameer Pradhan, Honglin Sun,
Wayne Ward, James H. Martin
Center for Spoken Language Research,
University of Colorado, Boulder, CO 80303
{spradhan,sunh,whw,martin}@cslr.colorado.edu
Dan Jurafsky
Department of Linguistics
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
In this paper, we use a machine learning frame-
work for semantic argument parsing, and apply
it to the task of parsing arguments of eventive
nominalizations in the FrameNet database. We
create a baseline system using a subset of fea-
tures introduced by Gildea and Jurafsky (2002),
which are directly applicable to nominal pred-
icates. We then investigate new features which
are designed to capture the novelties in nom-
inal argument structure and show a significant
performance improvement using these new fea-
tures. We also investigate the parsing perfor-
mance of nominalizations in Chinese and com-
pare the salience of the features for the two lan-
guages.
1 Introduction
The field of NLP had seen a resurgence of research in
shallow semantic analysis. The bulk of this recent work
views semantic analysis as a tagging, or labeling prob-
lem, and has applied various supervised machine learn-
ing techniques to it (Gildea and Jurafsky (2000, 2002);
Gildea and Palmer (2002); Surdeanu et al (2003); Ha-
cioglu and Ward (2003); Thompson et al (2003); Prad-
han et al (2003)). Note that, while all of these systems
are limited to the analysis of verbal predicates, many un-
derlying semantic relations are expressed via nouns, ad-
jectives, and prepositions. This paper presents a prelimi-
nary investigation into the semantic parsing of eventive
nominalizations (Grimshaw, 1990) in English and Chi-
nese.
2 Semantic Annotation and Corpora
For our experiments, we use the FrameNet database
(Baker et al, 1998) which contains frame-specific se-
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IS-9978025
mantic annotation of a number of predicates in English.
Predicates are grouped by the semantic frame that they
instantiate, depending on the sense of their usage, and
their arguments assume one of the frame elements or
roles specific to that frame. The predicate can be a verb,
noun, adjective, prepositional phrase, etc. FrameNet
contains about 500 different frame types and about 700
distinct frame elements. The following example illus-
trates the general idea. Here, the predicate ?complain?
instantiates a ?Statement? frame once as a nominal
predicate and once as a verbal predicate.
Did [Speaker she] make an official [Predicate:nominal com-
plaint] [Addressee to you] [Topic about the attack.]
[Message?Justice has not been done?] [Speaker he]
[Predicate:verbal complained.]
Nominal predicates in FrameNet include ultra-nominals
(Barker and Dowty, 1992), nominals and nominal-
izations. For the purposes of this study, a human analyst
went through the nominal predicates in FrameNet and
selected those that were identified as nominalizations
in NOMLEX (Macleod et al, 1998). Out of those,
the analyst then selected ones that were eventive
nominalizations.
These data comprise 7,333 annotated sentences, with
11,284 roles. There are 105 frames with about 190 dis-
tinct frame role1 types. A stratified sampling over predi-
cates was performed to select 80% of this data for train-
ing, 10% for development and another 10% for testing.
For the Chinese semantic parsing experiments, we se-
lected 22 nominalizations from the Penn Chinese Tree-
bank and tagged all the sentences containing these predi-
cates with PropBank (Kingsbury and Palmer, 2002) style
arguments ? ARG0, ARG1, etc. These consisted of 630
sentences. These are then split into two parts: 503 (80%)
for training and 127 (20%) for testing.
1We will use the terms role and arguments interchangeably
3 Baseline System
The primary assumption in our system is that a seman-
tic argument aligns with some syntactic constituent. The
goal is to identify and label constituents in a syntactic
tree that represent valid semantic arguments of a given
predicate. Unlike PropBank, there are no hand-corrected
parses available for the sentences in FrameNet, so we
cannot quantify the possible mis-alignment of the nomi-
nal arguments with syntactic constituents. The arguments
that do not align with any constituent are simply missed
by the current system.
3.1 Features We created a baseline system using
all and only those features introduced by Gildea and
Jurafsky that are directly applicable to nominal pred-
icates. Most of the features are extracted from the
syntactic parse of a sentence. We used the Charniak
parser (Chaniak, 2001) to parse the sentences in order to
perform feature extraction. The features are listed below:
Predicate ? The predicate lemma is used as a feature.
Path ? The syntactic path through the parse tree from the
parse constituent being classified to the predicate.
Constituent type ? This is the syntactic category (NP, PP,
S, etc.) of the constituent corresponding to the semantic
argument.
Position ? This is a binary feature identifying whether
the constituent is before or after the predicate.
Head word ? The syntactic head of the constituent.
3.2 Classifier and Implementation We formulate the
parsing problem as a multi-class classification problem
and use a Support Vector Machine (SVM) classifier in the
ONE vs ALL (OVA) formalism, which involves training
n classifiers for a n-class problem ? including the NULL
class. We use TinySVM2 along with YamCha3 (Kudo
and Matsumoto (2000, 2001)) as the SVM training and
test software.
3.3 Performance We evaluate our system on three
tasks: i) Argument Identification: Identifying parse con-
stituents that represent arguments of a given predicate, ii)
Argument Classification: Labeling the constituents that
are known to represent arguments with the most likely
roles, and iii) Argument Identification and Classification:
Finding constituents that represent arguments of a pred-
icate, and labeling them with the most likely roles. The
baseline performance on the three tasks is shown in Ta-
ble 1.
4 New Features
To improve the baseline performance we investigated ad-
ditional features that would provide useful information in
identifying arguments of nominalizations. Following is a
2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/
3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/
Task P R F?=1 A
(%) (%) (%)
Id. 81.7 65.7 72.8
Classification - - - 70.9
Id. + Classification 65.7 42.1 51.4
Table 1: Baseline performance on all three tasks.
description of each feature along with an intuitive justifi-
cation. Some of these features are not instantiated for a
particular constituent. In those cases, the respective fea-
ture values are set to ?UNK?.
1. Frame ? The frame instantiated by the particular sense
of the predicate in a sentence. This is an oracle feature.
2. Selected words/POS in constituent ? Nominal predi-
cates tend to assign arguments, most commonly through
postnominal of-complements, possessive prenominal
modifiers, etc. We added the values of the first and last
word in the constituent as two separate features. Another
two features represent the part of speech of these words.
3. Ordinal constituent position ? Arguments of nouns
tend to be located closer to the predicate than those
for verbs. This feature captures the ordinal position
of a particular constituent to the left or right of the
predicate on a left or right tree traversal, eg., first PP
from the predicate, second NP from the predicate, etc.
This feature along with the position will encode the
before/after information for the constituent.
4. Constituent tree distance ? Another way of quan-
tifying the position of the constituent is to identify its
index in the list of constituents that are encountered
during linear traversal of the tree from the predicate to
the constituent.
5. Intervening verb features ? Support verbs play an
important role in realizing the arguments of nominal
predicates. We use three classes of intervening verbs:
i) auxiliary verbs ? ones with part of speech AUX, ii)
light verbs ? a small set of known light verbs: took, take,
make, made, give, gave, went and go, and iii) other verbs
? with part of speech VBx. We added three features for
each: i) a binary feature indicating the presence of the
verb in between the predicate and the constituent ii) the
actual word as a feature, and iii) the path through the
tree from the constituent to the verb, as the subject of
intervening verbs sometimes tend to be arguments of
nominalizations. The following example could explain
the intuition behind this feature:
[Speaker Leapor] makes general [Predicate assertions] [Topic
about marriage]
6. Predicate NP expansion rule ? This is the noun
equivalent of the verb sub-categorization feature used by
Gildea and Jurafsky (2002). This is the expansion rule
instantiated by the parser, for the lowermost NP in the
tree, encompassing the predicate. This would tend to
cluster NPs with a similar internal structure and would
thus help finding argumentive modifiers.
7. Noun head of prepositional phrase constituents
? Instead of using the standard head word rule for
prepositional phrases, we use the head word of the first
NP inside the PP as the head of the PP and replace the
constituent type PP with PP-<preposition>.
8. Constituent sibling features ? These are six features
representing the constituent type, head word and part of
speech of the head word of the left and right siblings
of the constituent in consideration. These are used
to capture arguments represented by the modifiers of
nominalizations.
9. Partial-path from constituent to predicate ? This
is the path from the constituent to the lowest common
parent of the constituent and the predicate. This is used
to generalize the path statistics.
10. Is predicate plural ? A binary feature indicating
whether the predicate is singular or plural as they tend to
have different argument selection properties.
11. Genitives in constituent ? This is a binary feature
which is true if there is a genitive word (one with the part
of speech POS, PRP, PRP$ or WP$) in the constituent,
as these tend to be markers for nominal arguments as in
[Speaker Burma ?s] [Phenomenon oil] [Predicate search] hits
virgin forests
12. Constituent parent features ? Same as the sibling
features, except that that these are extracted from the
constituent?s parent.
13. Verb dominating predicate ? The head word of the
first VP ancestor of the predicate.
14. Named Entities in Constituent ? As in Surdeanu
et al (2003), this is represented as seven binary fea-
tures extracted after tagging the sentence with BBN?s
IdentiFinder (Bikel et al, 1999) named entity tagger.
5 Feature Analysis and Best System
Performance
5.1 English For the task of argument identification,
features 2, 3, 4, 5 (the verb itself, path to light-verb and
presence of a light verb), 6, 7, 9, 10 an 13 contributed pos-
itively to the performance. The Frame feature degrades
performance significantly. This could be just an artifact
of the data sparsity. We trained a new classifier using all
the features that contributed positively to the performance
and the F?=1 score increased from the baseline of 72.8%
to 76.3% (?2; p < 0.05).
For the task of argument classification, adding the
Frame feature to the baseline features, provided the most
significant improvement, increasing the classification
accuracy from 70.9% to 79.0% (?2; p < 0.05). All
other features added one-by-one to the baseline did
not bring any significant improvement to the baseline,
which might again be owing to the comparatively small
training and test data sizes. All the features together
produced a classification accuracy of 80.9%. Since the
Frame feature is an oracle, we were interested in finding
out what all the other features combined contributed.
We ran an experiment with all features, except Frame,
added to the baseline, and this produced an accuracy of
73.1%, which however, is not a statistically significant
improvement over the baseline of 70.9%.
For the task of argument identification and classifi-
cation, features 8 and 11 (right sibling head word part
of speech) hurt performance. We trained a classifier
using all the features that contributed positively to the
performance and the resulting system had an improved
F?=1 score of 56.5% compared to the baseline of 51.4%
(?2; p < 0.05).
We found that a significant subset of features that con-
tribute marginally to the classification performance, hurt
the identification task. Therefore, we decided to perform
a two-step process in which we use the set of features that
gave optimum performance for the argument identifica-
tion task and identify all likely argument nodes. Then, for
those nodes, we use all the available features and classify
them into one of the possible classes. This ?two-pass?
system performs slightly better than the ?one-pass? men-
tioned earlier. Again, we performed the second pass of
classification with and without the Frame feature.
Table 2 shows the improved performance numbers.
Task P R F?=1 A
(%) (%) (%)
Id. 83.8 70.0 76.3
Classification (w/o Frame) - - - 73.1
Classification (with Frame) - - - 80.9
Id. + Classification 69.4 47.6 56.5
(one-pass, w/o Frame)
Id. + Classification 62.2 53.1 57.3
(two-pass, w/o Frame)
Id. + Classification 69.4 59.2 63.9
(two-pass, with Frame)
Table 2: Best performance on all three tasks.
5.2 Chinese For the Chinese task, we use the one-pass
algorithm as used for English. A baseline system was
created using the same features as used for English (Sec-
tion 3). We evaluate this system on just the combined task
of argument identification and classification. The base-
line performance is shown in Table 3.
To improve the system?s performance over the base-
line, we added all the features discussed in Section 4, ex-
cept features Frame ? as the data was labeled in a Prop-
Bank fashion, there are no frames involved as in Frame-
Net; Plurals and Genitives ? as they are not realized the
same way morphologically in Chinese, and Named En-
tities ? owing to the unavailability of a Chinese Named
Entity tagger. We found that of these features, 2, 3, 4, 6, 7
and 13 hurt the performance when added to the baseline,
but the other features helped to some degree, although
not significantly. The improved performance is shown in
Table 3
Features P R F?=1
(%) (%)
Baseline 86.2 32.2 46.9
Baseline 83.9 44.1 57.8
+ more features
Table 3: Parsing performance for Chinese on the com-
bined task of identifying and classifying semantic argu-
ments.
An interesting linguistic phenomenon was observed
which explains part of the reason why recall for Chinese
argument parsing is so low. In Chinese, arguments
which are internal to the NP which encompasses the
nominalized predicate, tend to be multi-word, and are
not associated with any node in the parse tree. These
violates our basic assumption of the arguments aligning
with parse tree constituents, and are guaranteed to be
missed. In the case of English however, these tend to be
single word arguments which are represented by a leaf
in the parse tree and stand a chance of getting classified
correctly.
6 Conclusion
In this paper we investigated the task of identifying and
classifying arguments of eventive nominalizations in
FrameNet. The best system generates an F1 score of
57.3% on the combined task of argument identification
and classification using automatically extracted features
on a test set of about 700 sentences using a classifier
trained on about 6,000 sentences.
As noted earlier, the bulk of past research in this area
has focused on verbal predicates. Two notable exceptions
to this include the work of (Hull and Gomez, 1996) ? a
rule based system for identifying the semantic arguments
of nominal predicates, and the work of (Lapata, 2002)
on interpreting the relation between the head of a nom-
inalized compound and its modifier noun. Unfortunately,
meaningful comparisons to these efforts are difficult due
to differing evaluation metrics.
We would like to thank Ralph Weischedel and Scott Miller of
BBN Inc. for letting us use BBN?s named entity tagger ? Iden-
tiFinder; Ashley Thornton for identifying the sentences from
FrameNet with predicates that are eventive nominalizations.
References
[Baker et al1998] Collin F. Baker, Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley FrameNet project. In
COLING/ACL-98, pages 86?90, Montreal.
[Barker and Dowty1992] Chris Barker and David Dowty. 1992.
Non-verbal thematic proto-roles. In NELS-23, Amy Schafer,
ed., GSLA, Amherst, pages 49?62.
[Bikel et al1999] Daniel M. Bikel, Richard Schwartz, and
Ralph M. Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34:211?231.
[Chaniak2001] Eugene Chaniak. 2001. Immediate-head pars-
ing for language models. In ACL, Toulouse, France.
[Gildea and Jurafsky2000] Daniel Gildea and Daniel Jurafsky.
2000. Automatic labeling of semantic roles. In ACL, pages
512?520, Hong Kong, October.
[Gildea and Jurafsky2002] Daniel Gildea and Daniel Jurafsky.
2002. Automatic labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
[Gildea and Palmer2002] Daniel Gildea and Martha Palmer.
2002. The necessity of syntactic parsing for predicate ar-
gument recognition. In ACL, PA.
[Grimshaw1990] Jane Grimshaw. 1990. Argument Structure.
The MIT Press, US.
[Hacioglu and Ward2003] Kadri Hacioglu and Wayne Ward.
2003. Target word detection and semantic role chunking us-
ing support vector machines. In HLT, Edmonton, Canada.
[Hull and Gomez1996] Richard D. Hull and Fernando Gomez.
1996. Semantic interpretation of nominalizations. In AAAI
Conference, Oregon, pages 1062?1068.
[Kingsbury and Palmer2002] Paul Kingsbury and Martha Pal-
mer. 2002. From Treebank to PropBank. In LREC-2002,
Las Palmas, Canary Islands, Spain.
[Kudo and Matsumoto2000] Taku Kudo and Yuji Matsumoto.
2000. Use of support vector learning for chunk identifica-
tion. In CoNLL-2000, pages 142?144.
[Kudo and Matsumoto2001] Taku Kudo and Yuji Matsumoto.
2001. Chunking with support vector machines. In NAACL.
[Lapata2002] Maria Lapata. 2002. The disambiguation of nom-
inalizations. Computational Linguistics, 28(3):357?388.
[Macleod et al1998] C. Macleod, R. Grishman, A. Meyers,
L. Barrett, and R. Reeves. 1998. Nomlex: A lexicon of
nominalizations.
[Pradhan et al2003] Sameer Pradhan, Kadri Hacioglu, Wayne
Ward, James Martin, and Dan Jurafsky. 2003. Semantic role
parsing: Adding semantic structure to unstructured text. In
ICDM, Melbourne, Florida.
[Surdeanu et al2003] Mihai Surdeanu, Sanda Harabagiu, John
Williams, and Paul Aarseth. 2003. Using predicate-
argument structures for information extraction. In ACL, Sap-
poro, Japan.
[Thompson et al2003] Cynthia A. Thompson, Roger Levy, and
Christopher D. Manning. 2003. A generative model for se-
mantic role labeling. In ECML.
Proceedings of NAACL HLT 2007, pages 556?563,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Towards Robust Semantic Role Labeling
Sameer Pradhan
BBN Technologies
Cambridge, MA 02138
pradhan@bbn.com
Wayne Ward, James H. Martin
University of Colorado
Boulder, CO 80303
{whw,martin}@colorado.edu
Abstract
Most research on semantic role labeling
(SRL) has been focused on training and
evaluating on the same corpus in order
to develop the technology. This strategy,
while appropriate for initiating research,
can lead to over-training to the particular
corpus. The work presented in this pa-
per focuses on analyzing the robustness
of an SRL system when trained on one
genre of data and used to label a different
genre. Our state-of-the-art semantic role
labeling system, while performing well on
WSJ test data, shows significant perfor-
mance degradation when applied to data
from the Brown corpus. We present a se-
ries of experiments designed to investigate
the source of this lack of portability. These
experiments are based on comparisons of
performance using PropBanked WSJ data
and PropBanked Brown corpus data. Our
results indicate that while syntactic parses
and argument identification port relatively
well to a new genre, argument classifica-
tion does not. Our analysis of the reasons
for this is presented and generally point
to the nature of the more lexical/semantic
features dominating the classification task
and general structural features dominating
the argument identification task.
1 Introduction
Automatic, accurate and wide-coverage techniques
that can annotate naturally occurring text with se-
mantic argument structure play a key role in NLP
applications such as Information Extraction (Sur-
deanu et al, 2003; Harabagiu et al, 2005), Question
Answering (Narayanan and Harabagiu, 2004) and
Machine Translation (Boas, 2002; Chen and Fung,
2004). Semantic Role Labeling (SRL) is the pro-
cess of producing such a markup. When presented
with a sentence, a parser should, for each predicate
in the sentence, identify and label the predicate?s se-
mantic arguments. In recent work, a number of re-
searchers have cast this problem as a tagging prob-
lem and have applied various supervised machine
learning techniques to it. On the Wall Street Jour-
nal (WSJ) data, using correct syntactic parses, it is
possible to achieve accuracies rivaling human inter-
annotator agreement. However, the performance gap
widens when information derived from automatic
syntactic parses is used.
So far, most of the work on SRL systems has been
focused on improving the labeling performance on a
test set belonging to the same genre of text as the
training set. Both the Treebank on which the syntac-
tic parser is trained and the PropBank on which the
SRL systems are trained represent articles from the
year 1989 of the WSJ. While all these systems per-
form quite well on the WSJ test data, they show sig-
nificant performance degradation (approximately 10
point drop in F-score) when applied to label test data
that is different than the genre that WSJ represents
(Pradhan et al, 2004; Carreras and Ma`rquez, 2005).
556
Surprisingly, it does not matter much whether the
data is from another newswire, or a completely dif-
ferent type of text ? as in the Brown corpus. These
results indicate that the systems are being over-fit to
the specific genre of text. Many performance im-
provements on the WSJ PropBank corpus may re-
flect tuning to the corpus. For the technology to
be widely accepted and useful, it must be robust
to change in genre of the data. Until recently, data
tagged with similar semantic argument structure was
not available for multiple genres of text. Recently,
Palmer et al, (2005), have PropBanked a significant
portion of the Treebanked Brown corpus which en-
ables us to perform experiments to analyze the rea-
sons behind the performance degradation, and sug-
gest potential solutions.
2 Semantic Annotation and Corpora
In the PropBank1 corpus (Palmer et al, 2005), pred-
icate argument relations are marked for the verbs
in the text. PropBank was constructed by assign-
ing semantic arguments to constituents of the hand-
corrected Treebank parses. The arguments of a verb
are labeled ARG0 to ARG5, where ARG0 is the
PROTO-AGENT (usually the subject of a transitive
verb) ARG1 is the PROTO-PATIENT (usually its di-
rect object), etc. In addition to these CORE ARGU-
MENTS, 16 additional ADJUNCTIVE ARGUMENTS,
referred to as ARGMs are also marked.
More recently the PropBanking effort has been
extended to encompass multiple corpora. In this
study we use PropBanked versions of the Wall Street
Journal (WSJ) part of the Penn Treebank (Marcus et
al., 1994) and part of the Brown portion of the Penn
Treebank.
The WSJ PropBank data comprise 24 sections
of the WSJ, each section representing about 100
documents. PropBank release 1.0 contains about
114,000 predicates instantiating about 250,000 argu-
ments and covering about 3,200 verb lemmas. Sec-
tion 23, which is a standard test set and a test set
in some of our experiments, comprises 5,400 predi-
cates instantiating about 12,000 arguments.
The Brown corpus is a Standard Corpus of Ameri-
can English that consists of about one million words
of English text printed in the calendar year 1961
1http://www.cis.upenn.edu/?ace/
(Kuc?era and Francis, 1967). The corpus contains
about 500 samples of 2000+ words each. The idea
behind creating this corpus was to create a hetero-
geneous sample of English text so that it would be
useful for comparative language studies.
The Release 3 of the Penn Treebank contains the
hand parsed syntactic trees of a subset of the Brown
Corpus ? sections F, G, K, L, M, N, P and R. Palmer
et al, (2005) have recently PropBanked a signifi-
cant portion of this Treebanked Brown corpus. In
all, about 17,500 predicates are tagged with their se-
mantic arguments. For these experiments we used a
limited release of PropBank dated September 2005.
A small portion of the predicates ? about 8,000 have
also been tagged with frame sense information.
3 SRL System Description
We formulate the labeling task as a classification
problem as initiated by Gildea and Jurafsky (2002)
and use Support Vector Machine (SVM) classi-
fiers (2005). We use TinySVM2 along with Yam-
Cha3 (Kudo and Matsumoto, 2000) (Kudo and Mat-
sumoto, 2001) as the SVM training and classifica-
tion software. The system uses a polynomial kernel
with degree 2; the cost per unit violation of the mar-
gin, C=1; and, tolerance of the termination criterion,
e=0.001. More details of this system can be found
in Pradhan et al, (2005). The performance of this
system on section 23 of the WSJ when trained on
sections 02-21 is shown in Table 1
ALL ARGs Task P R F A
(%) (%) (%)
TREEBANK Id. 97.5 96.1 96.8
Class. - - - 93.0
Id. + Class. 91.8 90.5 91.2
AUTOMATIC Id. 86.9 84.2 85.5
Class. - - - 92.0
Id. + Class. 82.1 77.9 79.9
Table 1: Performance of the SRL system on WSJ
The performance of the SRL system is reported
on three different tasks, all of which are with respect
to a particular predicate: i) argument identification
(ID), is the task of identifying the set of words (here,
parse constituents) that represent a semantic role; ii)
argument classification (Class.), is the task of clas-
sifying parse constituents known to represent some
2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/
3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/
557
semantic role into one of the many semantic role
types; and iii) argument identification and classifi-
cation (ID + Class.), which involves both the iden-
tification of the parse constituents that represent se-
mantic roles of the predicate and their classification
into the respective semantic roles. As usual, argu-
ment classification is measured as percent accuracy
(A), whereas ID and ID + Class. are measured in
terms of precision (P), recall (R) and F-score (F)
? the harmonic mean of P and R. The first three
rows of Table 1 report performance for the system
that uses hand-corrected Treebank parses, and the
next three report performance for the SRL system
that uses automatically generated ? Charniak parser
? parses, both during training and testing.
4 Robustness Experiments
This section describes experiments that we per-
formed using the PropBanked Brown corpus in an
attempt to analyze the factors affecting the portabil-
ity of SRL systems.
4.1 How does the SRL system trained on WSJ
perform on Brown?
In order to test the robustness of the SRL system,
we used a system trained on the PropBanked WSJ
corpus to label data from the Brown corpus. We use
the entire PropBanked Brown corpus (about 17,500
predicates) as a test set for this experiment and use
the SRL system trained on WSJ sections 02-21 to
tag its arguments.
Table 2 shows the performance for training and
testing on WSJ, and for training on WSJ and testing
on Brown. There is a significant reduction in per-
formance when the system trained on WSJ is used
to label data from the Brown corpus. The degrada-
tion in the Identification task is small compared to
that of the combined Identification and Classifica-
tion task. A number of factors could be responsible
for the loss of performance. It is possible that the
SRL models are tuned to the particular vocabulary
and sense structure associated with the training data.
Also, since the syntactic parser that is used for gen-
erating the syntax parse trees (Charniak) is heavily
lexicalized and is trained on WSJ, it could have de-
creased accuracy on the Brown data resulting in re-
duced accuracy for Semantic Role Labeling. Since
the SRL algorithm walks the syntax tree classifying
each node, if no constituent node is present that cor-
responds to the correct argument, the system cannot
produce a correct labeling for the argument.
Train Test Id. Id. + Class
F F
WSJ WSJ 85.5 79.9
WSJ Brown 82.4 65.1
Table 2: Performance of the SRL system on Brown.
In order to check the extent to which constituent
nodes representing semantic arguments were deleted
from the syntax tree due to parser error, we gener-
ated the performance numbers which are shown in
Table 3. These numbers are for top one parse for the
Charniak parser, and represent not all parser errors,
but deletion of argument bearing constituent nodes.
Total Misses %
PropBank 12000 800 6.7
Brown 45880 3692 8.1
Table 3: Constituent deletions in WSJ and Brown.
The parser misses 6.7% of the argument-bearing
nodes in the PropBank test set and about 8.1% in
the Brown corpus. This indicates that the errors in
syntactic parsing account for a fairly small amount
of the argument deletions and probably do not con-
tributing significantly to the increased SRL error
rate. Obviously, just the presence of a argument-
bearing constituent does not necessarily guarantee
the correctness of the structural connections be-
tween itself and the predicate.
4.2 Identification vs Classification Performance
Different features tend to dominate in the identifi-
cation task vs the classification task. For example,
the path feature (representing the path in the syntax
tree from the argument to the predicate) is the sin-
gle most salient feature for the ID task and is not
very important in the classification task. In the next
experiment we look at cross genre performance of
the ID and Classification tasks. We used gold stan-
dard syntactic trees from the Treebank so there are
no errors in generating the syntactic structure. In
addition to training on the WSJ and testing on WSJ
and Brown, we trained the SRL system on a Brown
training set and tested it on a test set alo from the
Brown corpus. In generating the Brown training and
558
SRL SRL Task P R F A
Train Test (%) (%) (%)
WSJ WSJ Id. 97.5 96.1 96.8
(104k) (5k) Class. 93.0
Id. + Class. 91.8 90.5 91.2
WSJ WSJ Id. 96.3 94.4 95.3
(14k) (5k) Class. 86.1
Id. + Class. 84.4 79.8 82.0
BROWN BROWN Id. 95.7 94.9 95.2
(14k) (1.6k) Class. 80.1
Id. + Class. 79.9 77.0 78.4
WSJ BROWN Id. 94.2 91.4 92.7
(14k) (1.6k) Class. 72.0
Id. + Class. 71.8 65.8 68.6
Table 4: Performance of the SRL system using correct Treebank parses.
test sets, we used stratified sampling, which is often
used by the syntactic parsing community (Gildea,
2001). The test set was generated by selecting ev-
ery 10th sentence in the Brown Corpus. We also
held out the development set used by Bacchiani et
al., (2006) to tune system parameters in the future.
This procedure resulted in a training set of approxi-
mately 14,000 predicates and a test set of about 1600
predicates. We did not perform any parameter tun-
ing for any of the following experiments, and used
the parameter settings from the best performing ver-
sion of the SRL system as reported in Table1. We
compare the performance on this test set with that
obtained when the SRL system is trained using WSJ
sections 02-21 and use section 23 for testing. For
a more balanced comparison, we retrained the SRL
system on the same amount of data as used for train-
ing on Brown, and tested it on section 23. As usual,
trace information, and function tag information from
the Treebank is stripped out.
Table 4 shows the results. There is a fairly small
difference in argument Identification performance
when the SRL system is trained on 14,000 predi-
cates vs 104,000 predicates from the WSJ (F-score
95.3 vs 96.8). However, there is a considerable drop
in Classification accuracy (86.1% vs 93.0%). When
the SRL system is trained and tested on Brown data,
the argument Identification performance is not sig-
nificantly different than that for the system trained
and tested on WSJ data (F-score 95.2 vs 95.3). The
drop in argument Classification accuracy is much
more severe (86.1% vs 80.1%).
This same trend between ID and Classification is
even more pronounced when training on WSJ and
testing on Brown. For a system trained on WSJ,
there is a fairly small drop in performance of the
ID task when tested on Brown vs tested on WSJ (F-
score 92.7 vs 95.3). However, in this same condi-
tion, the Classification task has a very large drop in
performance (72.0% vs 86.1%).
So argument ID is not very sensitive to amount
of training data in a corpus, or to the genre of the
corpus, and ports well from WSJ to Brown. This ex-
periment supports the belief that there is no signifi-
cant drop in the task of identifying the right syntactic
constituents that are arguments ? and this is intuitive
since previous experiments have shown that the task
of argument identification is more dependent on the
structural features ? one such feature being the path
in the syntax tree.
Argument Classification seems to be the problem.
It requires more training data within the WSJ corpus,
does not perform as well when trained and tested on
Brown as it does for WSJ and does not port well
from WSJ to Brown. This suggests that the features
it uses are being over-fit to the training data and are
more idiosyncratic to a given dataset. In particular,
the predicate whose arguments are being identified,
and the head word of the syntactic constituent being
classified are both important features in the task of
argument classification.
As a generalization, the features used by the Iden-
tification task reflect structure and port well. The
features used by the Classification task reflect spe-
cific lexical usage and semantics, and tend to require
more training data and are more subject to over-
fitting. Even when training and testing on Brown,
Classification accuracy is considerably worse than
559
training and testing on WSJ (with comparable train-
ing set size). It is probably the case that the predi-
cates and head words in a homogeneous corpus such
as the WSJ are used more consistently, and tend to
have single dominant word senses. The Brown cor-
pus probably has much more variety in its lexical
usage and word senses.
4.3 How sensitive is semantic argument
prediction to the syntactic correctness
across genre?
This experiment examines the same cross-genre ef-
fects as the last experiment, but uses automatically
generated syntactic parses rather than gold standard
ones.
For this experiment, we used the same amount of
training data from WSJ as available in the Brown
training set ? that is about 14,000 predicates. The
examples from WSJ were selected randomly. The
Brown test set is the same as used in the previous
experiment, and the WSJ test set is the entire section
23.
Recently there have been some improvements to
the Charniak parser, use n-best re-ranking as re-
ported in (Charniak and Johnson, 2005) and self-
training and re-ranking using data from the North
American News corpus (NANC) and adapts much
better to the Brown corpus (McClosky et al, 2006a;
McClosky et al, 2006b). The performance of these
parsers as reported in the respective literature are
shown in Table 6 shows the performance (as re-
ported in the literature) of the Charniak parser: when
trained and tested on WSJ, when trained on WSJ and
tested on Brown, When trained and tested on Brown,
and when trained on WSJ and adapted with NANC.
Train Test F
WSJ WSJ 91.0
WSJ Brown 85.2
Brown Brown 88.4
WSJ+NANC Brown 87.9
Table 6: Charniak parser performance.
We describe the results of Semantic Role Label-
ing under the following five conditions:
1. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked WSJ sentences. The syntactic
parser ? Charniak parser ? is itself trained on
the WSJ training sections of the Treebank. This
is used for Semantic Role Labeling of section-
23 of WSJ.
2. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked WSJ sentences. The syntac-
tic parser ? Charniak parser ? is itself trained
on the WSJ training sections of the Treebank.
This is used to classify the Brown test set.
3. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked Brown corpus sentences. The
syntactic parser is trained using the WSJ por-
tion of the Treebank. This is used to classify
the Brown test set.
4. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked Brown corpus sentences. The
syntactic parser is trained using the Brown
training portion of the Treebank. This is used
to classify the Brown test set.
5. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked Brown corpus sentences. The
syntactic parser is the version that is self-
trained using 2,500,000 sentences from NANC,
and where the starting version is trained only
on WSJ data (McClosky et al, 2006b). This is
used to classify the Brown test set.
Table 5 shows the results. For simplicity of dis-
cussion we have tagged the five conditions as 1.,
2., 3., 4., and 5. Comparing conditions 2. and 3.
shows that when the features used to train the SRL
system are extracted using a syntactic parser that is
trained on WSJ it performs at almost the same level
on the task of Identification, regardless of whether
it is trained on the PropBanked Brown corpus or
the PropBanked WSJ corpus. This, however, is sig-
nificantly lower than when all the three ? the syn-
tactic parser training set, the SRL system training
set, and the SRL system test set, are from the same
genre (6 F-score points lower than condition 1, and
5 points lower than conditions 4 and 5). In case of
the combined task, the gap between the performance
for conditions 2 and 3 is about 10 points in F-score
(59.1 vs 69.8). Looking at the argument classifica-
tion accuracies, we see that using the SRL system
560
Setup Parser SRL SRL Task P R F A
Train Train Test (%) (%) (%)
1. WSJ WSJ WSJ Id. 87.3 84.8 86.0
(40k ? sec:00-21) (14k) (5k) Class. 84.1
Id. + Class. 77.5 69.7 73.4
2. WSJ WSJ Brown Id. 81.7 78.3 79.9
(40k ? sec:00-21) (14k) (1.6k) Class. 72.1
Id. + Class. 63.7 55.1 59.1
3. WSJ Brown Brown Id. 81.7 78.3 80.0
(40k ? sec:00-21) (14k) (1.6k) Class. 79.2
Id. + Class. 78.2 63.2 69.8
4. Brown Brown Brown Id. 87.6 82.3 84.8
(20k) (14k) (1.6k) Class. 78.9
Id. + Class. 77.4 62.1 68.9
5. WSJ+NANC Brown Brown Id. 87.7 82.5 85.0
(2,500k) (14k) (1.6k) Class. 79.9
Id. + Class. 77.2 64.4 70.0
Table 5: Performance on WSJ and Brown using automatic syntactic parses
trained on WSJ to test Brown sentences give a 12
point drop in F-score (84.1 vs 72.1). Using the SRL
system trained on Brown using WSJ trained syntac-
tic parser shows a drop in accuracy by about 5 F-
score points (84.1 to 79.2). When the SRL system is
trained on Brown using syntactic parser also trained
on Brown, we get a quite similar classification per-
formance, which is again about 5 points lower than
what we get using all WSJ data. This shows lexical
semantic features might be very important to get a
better argument classification on Brown corpus.
4.4 How much data is required to adapt to a
new genre?
We would like to know how much data from a new
genre we need to annotate and add to the training
data of an existing corpus to adapt the system such
that it gives the same level of performance as when
it is trained on the new genre.
One section of the Brown corpus ? section CK
has about 8,200 predicates annotated. We use six
different conditions ? two in which we use correct
Treebank parses, and the four others in which we
use automatically generated parses using the varia-
tions described before. All training sets start with
the same number of examples as in the Brown train-
ing set. The part of this section used as a test set for
the CoNLL 2005 shared task is used as the test set
here. It contains a total of about 800 predicates.
Table 7 shows a comparison of these conditions.
In all the six conditions, the performance on the task
of Identification and Classification improves gradu-
ally until about 5625 examples of section CK which
is about 75% of the total added, above which they
improve very little. In fact, even 50% of the new
data accounts for 90% of the performance differ-
ence. Even when the syntactic parser is trained on
WSJ and the SRL is trained on WSJ, adding 7,500
instances of the new genres allows it to achieve al-
most the same performance as when all three are
from the same genre (67.2 vs 69.9). Numbers for ar-
gument identification aren?t shown because adding
more data does not have any statistically signifi-
cant impact on its performance. The system that
uses self-trained syntactic parser seems to perform
slightly better than the rest of the versions that use
automatically generated syntactic parses. The preci-
sion numbers are almost unaffected ? except when
the labeler is trained on WSJ PropBank data.
4.5 How much does verb sense information
contribute?
In order to find out how important the verb sense
information is in the process of genre transfer, we
used the subset of PropBanked Brown corpus that
was tagged with verb sense information, ran an ex-
periment similar to that of Experiment 1. We used
the oracle sense information and correct syntactic in-
formation for this experiment.
Table 8 shows the results of this experiment.
There is about 1 point F-score increase on using
oracle sense information on the overall data. We
looked at predicates that had high perplexity in both
the training and test sets, and whose sense distribu-
561
Parser SRL Id. + Class Parser SRL Id. + Class
P R F P R F
Train Train (%) (%) (%) (%)
WSJ WSJ (14k) WSJ Brown (14k)
(Treebank parses) (Treebank parses)
+0 ex. from CK 74.1 66.5 70.1 (40k) +0 ex. from CK 74.4 57.0 64.5
+1875 ex. from CK 77.6 71.3 74.3 +1875 ex. from CK 75.1 58.7 65.9
+3750 ex. from CK 79.1 74.1 76.5 +3750 ex. from CK 76.1 59.6 66.9
+5625 ex. from CK 80.4 76.1 78.1 +5625 ex. from CK 76.9 60.5 67.7
+7500 ex. from CK 80.2 76.1 78.1 +7500 ex. from CK 76.8 59.8 67.2
Brown Brown (14k) Brown Brown (14k)
(Treebank parses) (Treebank parses)
+0 ex. from CK 77.1 73.0 75.0 (20k) +0 ex. from CK 76.0 59.2 66.5
+1875 ex. from CK 78.8 75.1 76.9 +1875 ex. from CK 76.1 60.0 67.1
+3750 ex. from CK 80.4 76.9 78.6 +3750 ex. from CK 77.7 62.4 69.2
+5625 ex. from CK 80.4 77.2 78.7 +5625 ex. from CK 78.2 63.5 70.1
+7500 ex. from CK 81.2 78.1 79.6 +7500 ex. from CK 78.2 63.2 69.9
WSJ WSJ (14k) WSJ+NANC Brown (14k)
(40k) +0 ex. from CK 65.2 55.7 60.1 (2,500k) +0 ex. from CK 74.4 60.1 66.5
+1875 ex. from CK 68.9 57.5 62.7 +1875 ex. from CK 76.2 62.3 68.5
+3750 ex. from CK 71.8 59.3 64.9 +3750 ex. from CK 76.8 63.6 69.6
+5625 ex. from CK 74.3 61.3 67.2 +5625 ex. from CK 77.7 63.8 70.0
+7500 ex. from CK 74.8 61.0 67.2 +7500 ex. from CK 78.2 64.9 70.9
Table 7: Effect of incrementally adding data from a new genre
Train Test Without Sense With Sense
Id. Id.
F F
WSJ Brown (All) 69.1 69.9
WSJ Brown (predicate: go) 46.9 48.9
Table 8: Influence of verb sense feature.
tion was different. One such predicate is ?go?. The
improvement on classifying the arguments of this
predicate was about 2 points (46.9 to 48.9), which
suggests that verb sense is more important when the
sense structure of the test corpus is more ambiguous
and is different from the training. Here we used ora-
cle verb sense information, but one can train a clas-
sifier as done by Girju et al, (2005) which achieves
a disambiguation accuracy in the 80s for within the
WSJ corpus.
5 Conclusions
Our experimental results on robustness to change in
genre can be summarized as follows:
? There is a significant drop in performance when
training and testing on different corpora ? for
both Treebank and Charniak parses
? In this process the classification task is more
disrupted than the identification task.
? There is a performance drop in classification
even when training and testing on Brown (com-
pared to training and testing on WSJ)
? The syntactic parser error is not a large part of
the degradation for the case of automatically
generated parses.
An error analysis leads us to believe that some
reasons for this behavior could be: i) lexical us-
ages that are specific to WSJ, ii) variation in sub-
categorization across corpora, iii) variation in word
sense distribution and iv) changes in topics and enti-
ties. Training and testing on the same corpora tends
to give a high weight to very specific semantic fea-
tures. Two possibilities remedies could be: i) using
less homogeneous corpora and ii) less specific fea-
tures, for eg., proper names are replaced with the
name entities that they represent. This way the sys-
tem could be forced to use the more general features.
Both of these manipulations would most likely re-
duce performance on the training set, and on test
sets of the same genre as the training data. But they
would be likely to generalize better.
6 Acknowledgments
We are extremely grateful to Martha Palmer for pro-
viding us with the PropBanked Brown corpus, and
to David McClosky for providing us with hypothe-
ses on the Brown test set as well as a cross-validated
562
version of the Brown training data for the various
models reported in his work reported at HLT 2006.
This research was partially supported by
the ARDA AQUAINT program via contract
OCG4423B and by the NSF via grants IS-9978025
and ITR/HCI 0086132.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Hans Boas. 2002. Bilingual framenet dictionaries for
machine translation. In Proceedings of LREC-2002.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2005, pages 152?164,
Ann Arbor, MI.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL-2005, pages 173?180,
Ann Arbor, MI.
Benfeng Chen and Pascale Fung. 2004. Automatic con-
struction of an english-chinese bilingual framenet. In
Proceedings of the HLT/NAACL-2004, Boston, MA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In In Proceedings of EMNLP-2001.
R. Girju, D. Roth, and M. Sammons. 2005. Token-
level disambiguation of verbnet classes. In Proceed-
ings of the Interdisciplinary Workshop on the Identifi-
cation and Representation of Verb Features and Verb
Classes, K. Erk, A. Melinger, and S. Schulte im Walde
(eds.).
Sanda Harabagiu, Cosmin Adrian Bejan, and
Paul Morarescu. 2005. Shallow semantics for relation
extraction. In IJCAI-2005, pages 1061?1067, Edin-
burgh, Scotland.
Henry Kuc?era and W. Nelson Francis. 1967. Com-
putational analysis of present-day American English.
Brown University Press, Providence, RI.
Taku Kudo and Yuji Matsumoto. 2000. Use of support
vector learning for chunk identification. In Proceed-
ings of CoNLL-2000 and LLL-2000, pages 142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of the
NAACL-2001.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure.
David McClosky, Eugene Charniak, and Mark Johnson.
2006a. Effective self-training for parsing. In Proceed-
ings of HLT/NAACL-2006, pages 152?159, New York
City, USA. Association for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2006b. Rerankinng and self-training for parser adapta-
tion. In Proceedings of COLING/ACL-2006, Sydney,
Australia.
Srini Narayanan and Sanda Harabagiu. 2004. Question
answering based on semantic structures. In Proceed-
ings of COLING-2004), Geneva, Switzerland.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-
ings of HLT/NAACL-2004, Boston, MA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2005. Semantic role label-
ing using different syntactic views. In Proceedings of
ACL-2005, Ann Arbor, MI.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL-2003, Sapporo, Japan.
563
Proceedings of the 43rd Annual Meeting of the ACL, pages 581?588,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labeling Using Different Syntactic Views?
Sameer Pradhan, Wayne Ward,
Kadri Hacioglu, James H. Martin
Center for Spoken Language Research,
University of Colorado,
Boulder, CO 80303
{spradhan,whw,hacioglu,martin}@cslr.colorado.edu
Daniel Jurafsky
Department of Linguistics,
Stanford University,
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
Semantic role labeling is the process of
annotating the predicate-argument struc-
ture in text with semantic labels. In this
paper we present a state-of-the-art base-
line semantic role labeling system based
on Support Vector Machine classifiers.
We show improvements on this system
by: i) adding new features including fea-
tures extracted from dependency parses,
ii) performing feature selection and cali-
bration and iii) combining parses obtained
from semantic parsers trained using dif-
ferent syntactic views. Error analysis of
the baseline system showed that approx-
imately half of the argument identifica-
tion errors resulted from parse errors in
which there was no syntactic constituent
that aligned with the correct argument. In
order to address this problem, we com-
bined semantic parses from a Minipar syn-
tactic parse and from a chunked syntac-
tic representation with our original base-
line system which was based on Charniak
parses. All of the reported techniques re-
sulted in performance improvements.
1 Introduction
Semantic Role Labeling is the process of annotat-
ing the predicate-argument structure in text with se-
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grants IS-9978025 and ITR/HCI 0086132
mantic labels (Gildea and Jurafsky, 2000; Gildea
and Jurafsky, 2002; Gildea and Palmer, 2002; Sur-
deanu et al, 2003; Hacioglu and Ward, 2003; Chen
and Rambow, 2003; Gildea and Hockenmaier, 2003;
Pradhan et al, 2004; Hacioglu, 2004). The architec-
ture underlying all of these systems introduces two
distinct sub-problems: the identification of syntactic
constituents that are semantic roles for a given pred-
icate, and the labeling of the those constituents with
the correct semantic role.
A detailed error analysis of our baseline system
indicates that the identification problem poses a sig-
nificant bottleneck to improving overall system per-
formance. The baseline system?s accuracy on the
task of labeling nodes known to represent semantic
arguments is 90%. On the other hand, the system?s
performance on the identification task is quite a bit
lower, achieving only 80% recall with 86% preci-
sion. There are two sources of these identification
errors: i) failures by the system to identify all and
only those constituents that correspond to semantic
roles, when those constituents are present in the syn-
tactic analysis, and ii) failures by the syntactic ana-
lyzer to provide the constituents that align with cor-
rect arguments. The work we present here is tailored
to address these two sources of error in the identifi-
cation problem.
The remainder of this paper is organized as fol-
lows. We first describe a baseline system based on
the best published techniques. We then report on
two sets of experiments using techniques that im-
prove performance on the problem of finding argu-
ments when they are present in the syntactic analy-
sis. In the first set of experiments we explore new
581
features, including features extracted from a parser
that provides a different syntactic view ? a Combi-
natory Categorial Grammar (CCG) parser (Hocken-
maier and Steedman, 2002). In the second set of
experiments, we explore approaches to identify opti-
mal subsets of features for each argument class, and
to calibrate the classifier probabilities.
We then report on experiments that address the
problem of arguments missing from a given syn-
tactic analysis. We investigate ways to combine
hypotheses generated from semantic role taggers
trained using different syntactic views ? one trained
using the Charniak parser (Charniak, 2000), another
on a rule-based dependency parser ? Minipar (Lin,
1998), and a third based on a flat, shallow syntactic
chunk representation (Hacioglu, 2004a). We show
that these three views complement each other to im-
prove performance.
2 Baseline System
For our experiments, we use Feb 2004 release of
PropBank1 (Kingsbury and Palmer, 2002; Palmer
et al, 2005), a corpus in which predicate argument
relations are marked for verbs in the Wall Street
Journal (WSJ) part of the Penn TreeBank (Marcus
et al, 1994). PropBank was constructed by as-
signing semantic arguments to constituents of hand-
corrected TreeBank parses. Arguments of a verb
are labeled ARG0 to ARG5, where ARG0 is the
PROTO-AGENT, ARG1 is the PROTO-PATIENT, etc.
In addition to these CORE ARGUMENTS, additional
ADJUNCTIVE ARGUMENTS, referred to as ARGMs
are also marked. Some examples are ARGM-LOC,
for locatives; ARGM-TMP, for temporals; ARGM-
MNR, for manner, etc. Figure 1 shows a syntax tree
along with the argument labels for an example ex-
tracted from PropBank. We use Sections 02-21 for
training, Section 00 for development and Section 23
for testing.
We formulate the semantic labeling problem as
a multi-class classification problem using Support
Vector Machine (SVM) classifier (Hacioglu et al,
2003; Pradhan et al, 2003; Pradhan et al, 2004)
TinySVM2 along with YamCha3 (Kudo and Mat-
1http://www.cis.upenn.edu/?ace/
2http://chasen.org/?taku/software/TinySVM/
3http://chasen.org/?taku/software/yamcha/
S
hhhh
((((
NP
hhhh
((((
The acquisition
ARG1
VP
```
   
VBD
was
NULL
VP
XXX
VBN
completed
predicate
PP
```
   
in September
ARGM?TMP
[ARG1 The acquisition] was [predicate completed] [ARGM?TMP in September].
Figure 1: Syntax tree for a sentence illustrating the
PropBank tags.
sumoto, 2000; Kudo and Matsumoto, 2001) are used
to implement the system. Using what is known as
the ONE VS ALL classification strategy, n binary
classifiers are trained, where n is number of seman-
tic classes including a NULL class.
The baseline feature set is a combination of fea-
tures introduced by Gildea and Jurafsky (2002) and
ones proposed in Pradhan et al, (2004), Surdeanu et
al., (2003) and the syntactic-frame feature proposed
in (Xue and Palmer, 2004). Table 1 lists the features
used.
PREDICATE LEMMA
PATH: Path from the constituent to the predicate in the parse tree.
POSITION: Whether the constituent is before or after the predicate.
VOICE
PREDICATE SUB-CATEGORIZATION
PREDICATE CLUSTER
HEAD WORD: Head word of the constituent.
HEAD WORD POS: POS of the head word
NAMED ENTITIES IN CONSTITUENTS: 7 named entities as 7 binary features.
PARTIAL PATH: Path from the constituent to the lowest common ancestor
of the predicate and the constituent.
VERB SENSE INFORMATION: Oracle verb sense information from PropBank
HEAD WORD OF PP: Head of PP replaced by head word of NP inside it,
and PP replaced by PP-preposition
FIRST AND LAST WORD/POS IN CONSTITUENT
ORDINAL CONSTITUENT POSITION
CONSTITUENT TREE DISTANCE
CONSTITUENT RELATIVE FEATURES: Nine features representing
the phrase type, head word and head word part of speech of the
parent, and left and right siblings of the constituent.
TEMPORAL CUE WORDS
DYNAMIC CLASS CONTEXT
SYNTACTIC FRAME
CONTENT WORD FEATURES: Content word, its POS and named entities
in the content word
Table 1: Features used in the Baseline system
As described in (Pradhan et al, 2004), we post-
process the n-best hypotheses using a trigram lan-
guage model of the argument sequence.
We analyze the performance on three tasks:
? Argument Identification ? This is the pro-
cess of identifying the parsed constituents in
the sentence that represent semantic arguments
of a given predicate.
582
? Argument Classification ? Given constituents
known to represent arguments of a predicate,
assign the appropriate argument labels to them.
? Argument Identification and Classification ?
A combination of the above two tasks.
ALL ARGs Task P R F1 A
(%) (%) (%)
HAND Id. 96.2 95.8 96.0
Classification - - - 93.0
Id. + Classification 89.9 89.0 89.4
AUTOMATIC Id. 86.8 80.0 83.3
Classification - - - 90.1
Id. + Classification 80.9 76.8 78.8
Table 2: Baseline system performance on all tasks
using hand-corrected parses and automatic parses on
PropBank data.
Table 2 shows the performance of the system us-
ing the hand corrected, TreeBank parses (HAND)
and using parses produced by a Charniak parser
(AUTOMATIC). Precision (P), Recall (R) and F1
scores are given for the identification and combined
tasks, and Classification Accuracy (A) for the clas-
sification task.
Classification performance using Charniak parses
is about 3% absolute worse than when using Tree-
Bank parses. On the other hand, argument identifi-
cation performance using Charniak parses is about
12.7% absolute worse. Half of these errors ? about
7% are due to missing constituents, and the other
half ? about 6% are due to mis-classifications.
Motivated by this severe degradation in argument
identification performance for automatic parses, we
examined a number of techniques for improving
argument identification. We made a number of
changes to the system which resulted in improved
performance. The changes fell into three categories:
i) new features, ii) feature selection and calibration,
and iii) combining parses from different syntactic
representations.
3 Additional Features
3.1 CCG Parse Features
While the Path feature has been identified to be very
important for the argument identification task, it is
one of the most sparse features and may be diffi-
cult to train or generalize (Pradhan et al, 2004; Xue
and Palmer, 2004). A dependency grammar should
generate shorter paths from the predicate to depen-
dent words in the sentence, and could be a more
robust complement to the phrase structure grammar
paths extracted from the Charniak parse tree. Gildea
and Hockenmaier (2003) report that using features
extracted from a Combinatory Categorial Grammar
(CCG) representation improves semantic labeling
performance on core arguments. We evaluated fea-
tures from a CCG parser combined with our baseline
feature set. We used three features that were intro-
duced by Gildea and Hockenmaier (2003):
? Phrase type ? This is the category of the max-
imal projection between the two words ? the
predicate and the dependent word.
? Categorial Path ? This is a feature formed by
concatenating the following three values: i) cat-
egory to which the dependent word belongs, ii)
the direction of dependence and iii) the slot in
the category filled by the dependent word.
? Tree Path ? This is the categorial analogue of
the path feature in the Charniak parse based
system, which traces the path from the depen-
dent word to the predicate through the binary
CCG tree.
Parallel to the hand-corrected TreeBank parses,
we also had access to correct CCG parses derived
from the TreeBank (Hockenmaier and Steedman,
2002a). We performed two sets of experiments.
One using the correct CCG parses, and the other us-
ing parses obtained using StatCCG4 parser (Hocken-
maier and Steedman, 2002). We incorporated these
features in the systems based on hand-corrected
TreeBank parses and Charniak parses respectively.
For each constituent in the Charniak parse tree, if
there was a dependency between the head word of
the constituent and the predicate, then the corre-
sponding CCG features for those words were added
to the features for that constituent. Table 3 shows the
performance of the system when these features were
added. The corresponding baseline performances
are mentioned in parentheses.
3.2 Other Features
We added several other features to the system. Po-
sition of the clause node (S, SBAR) seems to be
4Many thanks to Julia Hockenmaier for providing us with
the CCG bank as well as the StatCCG parser.
583
ALL ARGs Task P R F1
(%) (%)
HAND Id. 97.5 (96.2) 96.1 (95.8) 96.8 (96.0)
Id. + Class. 91.8 (89.9) 90.5 (89.0) 91.2 (89.4)
AUTOMATIC Id. 87.1 (86.8) 80.7 (80.0) 83.8 (83.3)
Id. + Class. 81.5 (80.9) 77.2 (76.8) 79.3 (78.8)
Table 3: Performance improvement upon adding
CCG features to the Baseline system.
an important feature in argument identification (Ha-
cioglu et al, 2004) therefore we experimented with
four clause-based path feature variations. We added
the predicate context to capture predicate sense vari-
ations. For some adjunctive arguments, punctuation
plays an important role, so we added some punctu-
ation features. All the new features are shown in
Table 4
CLAUSE-BASED PATH VARIATIONS:
I. Replacing all the nodes in a path other than clause nodes with an ?*?.
For example, the path NP?S?VP?SBAR?NP?VP?VBD
becomes NP?S?*S?*?*?VBD
II. Retaining only the clause nodes in the path, which for the above
example would produce NP?S?S?VBD,
III. Adding a binary feature that indicates whether the constituent
is in the same clause as the predicate,
IV. collapsing the nodes between S nodes which gives NP?S?NP?VP?VBD.
PATH N-GRAMS: This feature decomposes a path into a series of trigrams.
For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes:
NP?S?VP, S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, etc. We
used the first ten trigrams as ten features. Shorter paths were padded
with nulls.
SINGLE CHARACTER PHRASE TAGS: Each phrase category is clustered
to a category defined by the first character of the phrase label.
PREDICATE CONTEXT: Two words and two word POS around the
predicate and including the predicate were added as ten new features.
PUNCTUATION: Punctuation before and after the constituent were
added as two new features.
FEATURE CONTEXT: Features for argument bearing constituents
were added as features to the constituent being classified.
Table 4: Other Features
4 Feature Selection and Calibration
In the baseline system, we used the same set of fea-
tures for all the n binary ONE VS ALL classifiers.
Error analysis showed that some features specifi-
cally suited for one argument class, for example,
core arguments, tend to hurt performance on some
adjunctive arguments. Therefore, we thought that
selecting subsets of features for each argument class
might improve performance. To achieve this, we
performed a simple feature selection procedure. For
each argument, we started with the set of features in-
troduced by (Gildea and Jurafsky, 2002). We pruned
this set by training classifiers after leaving out one
feature at a time and checking its performance on
a development set. We used the ?2 significance
while making pruning decisions. Following that, we
added each of the other features one at a time to the
pruned baseline set of features and selected ones that
showed significantly improved performance. Since
the feature selection experiments were computation-
ally intensive, we performed them using 10k training
examples.
SVMs output distances not probabilities. These
distances may not be comparable across classifiers,
especially if different features are used to train each
binary classifier. In the baseline system, we used the
algorithm described by Platt (Platt, 2000) to convert
the SVM scores into probabilities by fitting to a sig-
moid. When all classifiers used the same set of fea-
tures, fitting all scores to a single sigmoid was found
to give the best performance. Since different fea-
ture sets are now used by the classifiers, we trained
a separate sigmoid for each classifier.
Raw Scores Probabilities
After lattice-rescoring
Uncalibrated Calibrated
(%) (%) (%)
Same Feat. same sigmoid 74.7 74.7 75.4
Selected Feat. diff. sigmoids 75.4 75.1 76.2
Table 5: Performance improvement on selecting fea-
tures per argument and calibrating the probabilities
on 10k training data.
Foster and Stine (2004) show that the pool-
adjacent-violators (PAV) algorithm (Barlow et al,
1972) provides a better method for converting raw
classifier scores to probabilities when Platt?s algo-
rithm fails. The probabilities resulting from either
conversions may not be properly calibrated. So, we
binned the probabilities and trained a warping func-
tion to calibrate them. For each argument classifier,
we used both the methods for converting raw SVM
scores into probabilities and calibrated them using
a development set. Then, we visually inspected
the calibrated plots for each classifier and chose the
method that showed better calibration as the calibra-
tion procedure for that classifier. Plots of the pre-
dicted probabilities versus true probabilities for the
ARGM-TMP VS ALL classifier, before and after cal-
ibration are shown in Figure 2. The performance im-
provement over a classifier that is trained using all
the features for all the classes is shown in Table 5.
Table 6 shows the performance of the system af-
ter adding the CCG features, additional features ex-
584
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Predicted Probability
Tr
ue
 P
ro
ba
bi
lity
Before Calibration
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Predicted Probability
Tr
ue
 P
ro
ba
bi
lity
After Calibration
Figure 2: Plots showing true probabilities versus predicted probabilities before and after calibration on the
test set for ARGM-TMP.
tracted from the Charniak parse tree, and performing
feature selection and calibration. Numbers in paren-
theses are the corresponding baseline performances.
TASK P R F1 A
(%) (%) (%)
Id. 86.9 (86.8) 84.2 (80.0) 85.5 (83.3)
Class. - - - 92.0 (90.1)
Id. + Class. 82.1 (80.9) 77.9 (76.8) 79.9 (78.8)
Table 6: Best system performance on all tasks using
automatically generated syntactic parses.
5 Alternative Syntactic Views
Adding new features can improve performance
when the syntactic representation being used for
classification contains the correct constituents. Ad-
ditional features can?t recover from the situation
where the parse tree being used for classification
doesn?t contain the correct constituent representing
an argument. Such parse errors account for about
7% absolute of the errors (or, about half of 12.7%)
for the Charniak parse based system. To address
these errors, we added two additional parse repre-
sentations: i) Minipar dependency parser, and ii)
chunking parser (Hacioglu et al, 2004). The hope is
that these parsers will produce different errors than
the Charniak parser since they represent different
syntactic views. The Charniak parser is trained on
the Penn TreeBank corpus. Minipar is a rule based
dependency parser. The chunking parser is trained
on PropBank and produces a flat syntactic represen-
tation that is very different from the full parse tree
produced by Charniak. A combination of the three
different parses could produce better results than any
single one.
5.1 Minipar-based Semantic Labeler
Minipar (Lin, 1998; Lin and Pantel, 2001) is a rule-
based dependency parser. It outputs dependencies
between a word called head and another called mod-
ifier. Each word can modify at most one word. The
dependency relationships form a dependency tree.
The set of words under each node in Minipar?s
dependency tree form a contiguous segment in the
original sentence and correspond to the constituent
in a constituent tree. We formulate the semantic la-
beling problem in the same way as in a constituent
structure parse, except we classify the nodes that
represent head words of constituents. A similar for-
mulation using dependency trees derived from Tree-
Bank was reported in Hacioglu (Hacioglu, 2004).
In that experiment, the dependency trees were de-
rived from hand-corrected TreeBank trees using
head word rules. Here, an SVM is trained to as-
sign PropBank argument labels to nodes in Minipar
dependency trees using the following features:
Table 8 shows the performance of the Minipar-
based semantic parser.
Minipar performance on the PropBank corpus is
substantially worse than the Charniak based system.
This is understandable from the fact that Minipar
is not designed to produce constituents that would
exactly match the constituent segmentation used in
TreeBank. In the test set, about 37% of the argu-
585
PREDICATE LEMMA
HEAD WORD: The word representing the node in the dependency tree.
HEAD WORD POS: Part of speech of the head word.
POS PATH: This is the path from the predicate to the head word through
the dependency tree connecting the part of speech of each node in the tree.
DEPENDENCY PATH: Each word that is connected to the head
word has a particular dependency relationship to the word. These
are represented as labels on the arc between the words. This
feature is the dependencies along the path that connects two words.
VOICE
POSITION
Table 7: Features used in the Baseline system using
Minipar parses.
Task P R F1
(%) (%)
Id. 73.5 43.8 54.6
Id. + Classification 66.2 36.7 47.2
Table 8: Baseline system performance on all tasks
using Minipar parses.
ments do not have corresponding constituents that
match its boundaries. In experiments reported by
Hacioglu (Hacioglu, 2004), a mismatch of about
8% was introduced in the transformation from hand-
corrected constituent trees to dependency trees. Us-
ing an errorful automatically generated tree, a still
higher mismatch would be expected. In case of
the CCG parses, as reported by Gildea and Hock-
enmaier (2003), the mismatch was about 23%. A
more realistic way to score the performance is to
score tags assigned to head words of constituents,
rather than considering the exact boundaries of the
constituents as reported by Gildea and Hocken-
maier (2003). The results for this system are shown
in Table 9.
Task P R F1
(%) (%)
CHARNIAK Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
MINIPAR Id. 83.3 61.1 70.5
Id. + Classification 72.9 53.5 61.7
Table 9: Head-word based performance using Char-
niak and Minipar parses.
5.2 Chunk-based Semantic Labeler
Hacioglu has previously described a chunk based se-
mantic labeling method (Hacioglu et al, 2004). This
system uses SVM classifiers to first chunk input text
into flat chunks or base phrases, each labeled with
a syntactic tag. A second SVM is trained to assign
semantic labels to the chunks. The system is trained
on the PropBank training data.
WORDS
PREDICATE LEMMAS
PART OF SPEECH TAGS
BP POSITIONS: The position of a token in a BP using the IOB2
representation (e.g. B-NP, I-NP, O, etc.)
CLAUSE TAGS: The tags that mark token positions in a sentence
with respect to clauses.
NAMED ENTITIES: The IOB tags of named entities.
TOKEN POSITION: The position of the phrase with respect to
the predicate. It has three values as ?before?, ?after? and ?-? (for
the predicate)
PATH: It defines a flat path between the token and the predicate
CLAUSE BRACKET PATTERNS
CLAUSE POSITION: A binary feature that identifies whether the
token is inside or outside the clause containing the predicate
HEADWORD SUFFIXES: suffixes of headwords of length 2, 3 and 4.
DISTANCE: Distance of the token from the predicate as a number
of base phrases, and the distance as the number of VP chunks.
LENGTH: the number of words in a token.
PREDICATE POS TAG: the part of speech category of the predicate
PREDICATE FREQUENCY: Frequent or rare using a threshold of 3.
PREDICATE BP CONTEXT: The chain of BPs centered at the predicate
within a window of size -2/+2.
PREDICATE POS CONTEXT: POS tags of words immediately preceding
and following the predicate.
PREDICATE ARGUMENT FRAMES: Left and right core argument patterns
around the predicate.
NUMBER OF PREDICATES: This is the number of predicates in
the sentence.
Table 10: Features used by chunk based classifier.
Table 10 lists the features used by this classifier.
For each token (base phrase) to be tagged, a set of
features is created from a fixed size context that sur-
rounds each token. In addition to the above features,
it also uses previous semantic tags that have already
been assigned to the tokens contained in the linguis-
tic context. A 5-token sliding window is used for the
context.
P R F1
(%) (%)
Id. and Classification 72.6 66.9 69.6
Table 11: Semantic chunker performance on the
combined task of Id. and classification.
SVMs were trained for begin (B) and inside (I)
classes of all arguments and outside (O) class for a
total of 78 one-vs-all classifiers. Again, TinySVM5
along with YamCha6 (Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2001) are used as the SVM
training and test software.
Table 11 presents the system performances on the
PropBank test set for the chunk-based system.
5http://chasen.org/?taku/software/TinySVM/
6http://chasen.org/?taku/software/yamcha/
586
6 Combining Semantic Labelers
We combined the semantic parses as follows: i)
scores for arguments were converted to calibrated
probabilities, and arguments with scores below a
threshold value were deleted. Separate thresholds
were used for each parser. ii) For the remaining ar-
guments, the more probable ones among overlap-
ping ones were selected. In the chunked system,
an argument could consist of a sequence of chunks.
The probability assigned to the begin tag of an ar-
gument was used as the probability of the sequence
of chunks forming an argument. Table 12 shows
the performance improvement after the combina-
tion. Again, numbers in parentheses are respective
baseline performances.
TASK P R F1
(%) (%)
Id. 85.9 (86.8) 88.3 (80.0) 87.1 (83.3)
Id. + Class. 81.3 (80.9) 80.7 (76.8) 81.0 (78.8)
Table 12: Constituent-based best system perfor-
mance on argument identification and argument
identification and classification tasks after combin-
ing all three semantic parses.
The main contribution of combining both the
Minipar based and the Charniak-based parsers was
significantly improved performance on ARG1 in ad-
dition to slight improvements to some other argu-
ments. Table 13 shows the effect on selected argu-
ments on sentences that were altered during the the
combination of Charniak-based and Chunk-based
parses.
Number of Propositions 107
Percentage of perfect props before combination 0.00
Percentage of perfect props after combination 45.95
Before After
P R F1 P R F1
(%) (%) (%) (%)
Overall 94.8 53.4 68.3 80.9 73.8 77.2
ARG0 96.0 85.7 90.5 92.5 89.2 90.9
ARG1 71.4 13.5 22.7 59.4 59.4 59.4
ARG2 100.0 20.0 33.3 50.0 20.0 28.5
ARGM-DIS 100.0 40.0 57.1 100.0 100.0 100.0
Table 13: Performance improvement on parses
changed during pair-wise Charniak and Chunk com-
bination.
A marked increase in number of propositions for
which all the arguments were identified correctly
from 0% to about 46% can be seen. Relatively few
predicates, 107 out of 4500, were affected by this
combination.
To give an idea of what the potential improve-
ments of the combinations could be, we performed
an oracle experiment for a combined system that
tags head words instead of exact constituents as we
did in case of Minipar-based and Charniak-based se-
mantic parser earlier. In case of chunks, first word in
prepositional base phrases was selected as the head
word, and for all other chunks, the last word was se-
lected to be the head word. If the correct argument
was found present in either the Charniak, Minipar or
Chunk hypotheses then that was selected. The re-
sults for this are shown in Table 14. It can be seen
that the head word based performance almost ap-
proaches the constituent based performance reported
on the hand-corrected parses in Table 3 and there
seems to be considerable scope for improvement.
Task P R F1
(%) (%)
C Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
C+M Id. 98.4 90.6 94.3
Id. + Classification 93.1 86.0 89.4
C+CH Id. 98.9 88.8 93.6
Id. + Classification 92.5 83.3 87.7
C+M+CH Id. 99.2 92.5 95.7
Id. + Classification 94.6 88.4 91.5
Table 14: Performance improvement on head word
based scoring after oracle combination. Charniak
(C), Minipar (M) and Chunker (CH).
Table 15 shows the performance improvement in
the actual system for pairwise combination of the
parsers and one using all three.
Task P R F1
(%) (%)
C Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
C+M Id. 91.7 89.9 90.8
Id. + Classification 85.0 83.9 84.5
C+CH Id. 91.5 91.1 91.3
Id. + Classification 84.9 84.3 84.7
C+M+CH Id. 91.5 91.9 91.7
Id. + Classification 85.1 85.5 85.2
Table 15: Performance improvement on head word
based scoring after combination. Charniak (C),
Minipar (M) and Chunker (CH).
587
7 Conclusions
We described a state-of-the-art baseline semantic
role labeling system based on Support Vector Ma-
chine classifiers. Experiments were conducted to
evaluate three types of improvements to the sys-
tem: i) adding new features including features ex-
tracted from a Combinatory Categorial Grammar
parse, ii) performing feature selection and calibra-
tion and iii) combining parses obtained from seman-
tic parsers trained using different syntactic views.
We combined semantic parses from a Minipar syn-
tactic parse and from a chunked syntactic repre-
sentation with our original baseline system which
was based on Charniak parses. The belief was that
semantic parses based on different syntactic views
would make different errors and that the combina-
tion would be complimentary. A simple combina-
tion of these representations did lead to improved
performance.
8 Acknowledgements
This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and
by the NSF via grants IS-9978025 and ITR/HCI
0086132. Computer time was provided by NSF
ARI Grant #CDA-9601817, NSF MRI Grant #CNS-
0420873, NASA AIST grant #NAG2-1646, DOE
SciDAC grant #DE-FG02-04ER63870, NSF spon-
sorship of the National Center for Atmospheric Re-
search, and a grant from the IBM Shared University
Research (SUR) program.
We would like to thank Ralph Weischedel and
Scott Miller of BBN Inc. for letting us use their
named entity tagger ? IdentiFinder; Martha Palmer
for providing us with the PropBank data; Dan Gildea
and Julia Hockenmaier for providing the gold stan-
dard CCG parser information, and all the anony-
mous reviewers for their helpful comments.
References
R. E. Barlow, D. J. Bartholomew, J. M. Bremmer, and H. D. Brunk. 1972. Statis-
tical Inference under Order Restrictions. Wiley, New York.
Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of
NAACL, pages 132?139, Seattle, Washington.
John Chen and Owen Rambow. 2003. Use of deep linguistics features for
the recognition and labeling of semantic arguments. In Proceedings of the
EMNLP, Sapporo, Japan.
Dean P. Foster and Robert A. Stine. 2004. Variable selection in data mining:
building a predictive model for bankruptcy. Journal of American Statistical
Association, 99, pages 303?313.
Dan Gildea and Julia Hockenmaier. 2003. Identifying semantic roles using com-
binatory categorial grammar. In Proceedings of the EMNLP, Sapporo, Japan.
Daniel Gildea and Daniel Jurafsky. 2000. Automatic labeling of semantic roles.
In Proceedings of ACL, pages 512?520, Hong Kong, October.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity of syntactic parsing for
predicate argument recognition. In Proceedings of ACL, Philadelphia, PA.
Kadri Hacioglu. 2004. Semantic role labeling using dependency trees. In Pro-
ceedings of COLING, Geneva, Switzerland.
Kadri Hacioglu and Wayne Ward. 2003. Target word detection and semantic role
chunking using support vector machines. In Proceedings of HLT/NAACL,
Edmonton, Canada.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Shallow semantic parsing using support vector machines. Technical
Report TR-CSLR-2003-1, Center for Spoken Language Research, Boulder,
Colorado.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Daniel Juraf-
sky. 2004. Semantic role labeling by tagging syntactic chunks. In Proceed-
ings of CoNLL-2004, Shared Task ? Semantic Role Labeling.
Kadri Hacioglu. 2004a. A lightweight semantic chunking model based on tag-
ging. In Proceedings of HLT/NAACL, Boston, MA.
Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical
parsing with combinatory grammars. In Proceedings of the ACL, pages 335?
342.
Julia Hockenmaier and Mark Steedman. 2002a. Acquiring compact lexicalized
grammars from a cleaner treebank. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation (LREC-2002), Las Pal-
mas, Canary Islands, Spain.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In
Proceedings of LREC, Las Palmas, Canary Islands, Spain.
Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk
identification. In Proceedings of CoNLL and LLL, pages 142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines.
In Proceedings of the NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question
answering. Natural Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In In Workshop
on the Evaluation of Parsing Systems, Granada, Spain.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann
Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument structure.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank:
An annotated corpus of semantic roles. To appear Computational Linguistics.
John Platt. 2000. Probabilities for support vector machines. In A. Smola,
P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large
Margin Classifiers. MIT press, Cambridge, MA.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Semantic role parsing: Adding semantic structure to unstructured text.
In Proceedings of ICDM, Melbourne, Florida.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2004. Shallow semantic parsing using support vector machines. In Proceed-
ings of HLT/NAACL, Boston, MA.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Us-
ing predicate-argument structures for information extraction. In Proceedings
of ACL, Sapporo, Japan.
Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role
labeling. In Proceedings of EMNLP, Barcelona, Spain.
588
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 17?20,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extractive Summaries for Educational Science Content 
Sebastian de la Chica, Faisal Ahmad, James H. Martin, Tamara Sumner 
Institute of Cognitive Science 
Department of Computer Science 
University of Colorado at Boulder 
sebastian.delachica, faisal.ahmad, james.martin, 
tamara.sumner@colorado.edu 
 
 
Abstract 
This paper describes an extractive summarizer 
for educational science content called 
COGENT. COGENT extends MEAD based 
on strategies elicited from an empirical study 
with domain and instructional experts. 
COGENT implements a hybrid approach inte-
grating both domain independent sentence 
scoring features and domain-aware features. 
Initial evaluation results indicate that 
COGENT outperforms existing summarizers 
and generates summaries that closely resem-
ble those generated by human experts. 
1 Introduction 
Knowledge maps consist of nodes containing rich 
concept descriptions interconnected using a limited 
set of relationship types (Holley and Dansereau, 
1984). Learning research indicates that knowledge 
maps may be useful for learners to understand the 
macro-level structure of an information space 
(O'Donnell et al, 2002). Knowledge maps have 
also emerged as an effective computational infra-
structure to support the automated generation of 
conceptual browsers. Such conceptual browsers 
appear to allow students to focus on the science 
content of large educational digital libraries (Sum-
ner et al, 2003), such as the Digital Library for 
Earth System Education (DLESE.org). Knowledge 
maps have also shown promise as domain and stu-
dent knowledge representations to support person-
alized learning interactions (de la Chica et al, 
2008). 
In this paper we describe our progress towards 
the generation of science concept inventories as 
summaries of digital library collections. Such in-
ventories provide the basis for the construction of 
knowledge maps useful both as computational 
knowledge representations and as learning re-
sources for presentation to the student. 
2 Related Work 
Our work is informed by efforts to automate the 
acquisition of ontology concepts from text. On-
toLearn extracts candidate domain terms from texts 
using a syntactic parse and updates an existing on-
tology with the identified concepts and relation-
ships (Navigli and Velardi, 2004). Knowledge 
Puzzle focuses on n-gram identification to produce 
a list of candidate terms pruned using information 
extraction techniques to derive the ontology 
(Zouaq et al, 2007). Lin and Pantel (2002) dis-
cover concepts using clustering by committee to 
group terms into conceptually related clusters. 
These approaches produce ontologies of very fine 
granularity and therefore graphs that may not be 
suitable for presentation to a student. 
Multi-document summarization (MDS) re-
search also informs our work. XDoX analyzes 
large document sets to extract important themes 
using n-gram scoring and clustering (Hardy et al, 
2002). Topic representation and topic themes have 
also served as the basis for the exploration of 
promising MDS techniques (Harabagiu and Laca-
tusu, 2005). Finally, MEAD is a widely used MDS 
and evaluation platform (Radev et al, 2000). 
While all these systems have produced promising 
results in automated evaluations, none have di-
rectly targeted educational content collections. 
17
3 Empirical Study 
We have conducted a study to capture how human 
experts processed digital library resources to create 
a domain knowledge map. Four geology and in-
structional design experts selected 20 resources 
from DLESE to construct a knowledge map on 
earthquakes and plates tectonics for high school 
age learners. The resulting knowledge map con-
sists of 564 concepts and 578 relationships. 
 
Figure 1. Expert knowledge map excerpt 
The concepts include 7,846  words, or 5% of 
the resources. Our experts relied on copying-and-
pasting (58%) and paraphrasing (37%) to create 
most concepts. Only 5% of the concepts could not 
be traced directly to the original resources. Rela-
tionship types were used in a Zipf-like distribution 
with the top 2 relationship types each accounting 
for more than 10% of all relationships: elabora-
tions (19%) and examples (14%). 
Analysis by an independent instructional expert 
indicates that this knowledge map provides ade-
quate coverage of nationally-recognized educa-
tional goals on earthquakes and plate tectonics for 
high school learners using the American Associa-
tion for the Advancement of Science (AAAS) 
Benchmarks (Project 2061, 1993). 
Verbal protocol analysis shows that all experts 
used external sources to create the knowledge map, 
including their own expertise, other digital library 
resources, and the National Science Education 
Standards (NSES), a comprehensive collection of 
nationally-recognized science learning goals for K-
12 students (National Research Council, 1996). 
We have examined sentence extraction agree-
ment between experts using the prevalence-
adjusted bias-adjusted (PABA) kappa to account 
for prevalence of judgments and conflicting biases 
amongst experts (Byrt et al, 1993). The average 
PABA-kappa value of 0.62 indicates that experts 
substantially agree on sentence extraction from 
digital library resources. This level of agreement 
suggests that these concepts may serve as the ref-
erence summary to evaluate our system. 
4 Summarizer for Science Education 
We have implemented an extractive summarizer 
for educational science content, COGENT, based 
on MEAD version 3.11 (Radev et al, 2000). 
COGENT complements the default MEAD sen-
tence scoring features with features based on find-
ings from the empirical study. COGENT 
represents a hybrid approach integrating bottom-up 
(hypertext and content word density) and top-down 
(educational standards and gazetteer) features.  
We model how human experts used external in-
formation sources with the educational standards 
feature. This feature leverages the text of the rele-
vant AAAS Benchmarks and associated NSES. 
Each sentence receives a score based on its TFIDF 
similarity to the textual contents of these learning 
goals and educational standards. 
We have developed a feature that reflects the 
large number of examples extracted by the experts. 
Earth science examples often refer to geographical 
locations and geological formations. The gazetteer 
feature checks named entities from each sentence 
against the Alexandria Digital Library (ADL) Gaz-
etteer (Hill, 2000). A gazetteer is a geo-referencing 
resource containing location and type information 
about place-names. Each sentence receives a 
TFIDF score based on place-name term frequency 
and overall uniqueness in the gazetteer. Our as-
sumption is that geographical locations with more 
unique names may be more pedagogically relevant. 
Based on the intuition that the HTML structure 
of a resource reflects relevancy, we have devel-
oped the hypertext feature. This feature computes a 
sentence score directly proportional to the HTML 
heading level and inversely proportional to the 
relative paragraph number within a heading and to 
the relative sentence position within a paragraph. 
18
To promote the extraction of sentences contain-
ing science concepts, we have developed the con-
tent word density feature. This feature computes 
the ratio of content to function words in a sentence. 
Function words are identified using a stopword list, 
and the feature only keeps sentences featuring 
more content words than function words. 
We compute the final sentence score by adding 
the MEAD default feature scores (centroid and 
position) to the COGENT feature scores (educa-
tional standards, gazetteer, and hypertext). 
COGENT keeps sentences that pass the cut-off 
constraints, including the MEAD sentence length 
of 9 and COGENT content word density of 50%. 
The default MEAD cosine re-ranker eliminates 
redundant sentences. Since the experts used 5% of 
the total word count in the resources, we produce 
summaries of that same length. 
5 Evaluation 
We have evaluated COGENT by processing the 20 
digital library resources used in the empirical study 
and comparing the output against the concepts 
identified by the experts. Three configurations are 
considered: Random, Default, and COGENT. The 
Random summary uses MEAD to extract random 
sentences. The Default summary uses the MEAD 
centroid, position and length default features. Fi-
nally, the COGENT summary extends MEAD with 
the COGENT features. 
We use ROUGE (Lin, 2004) to assess summary 
quality using common n-gram counts and longest 
common subsequence (LCS) measures. We report 
on ROUGE-1 (unigrams), ROUGE-2 (bigrams), 
ROUGE W-1.2 (weighted LCS), and ROUGE-S* 
(skip bigrams) as they have been shown to corre-
late well with human judgments for longer multi-
document summaries (Lin, 2004). Table 1 shows 
the results for recall (R), precision (P), and bal-
anced f-measure (F). 
  Random Default COGENT 
R 0.4855 0.4976 0.6073 
P 0.5026 0.5688 0.6034 R-1 
F 0.4939 0.5308 0.6054 
R 0.0972 0.1321 0.1907 
P 0.1006 0.1510 0.1895 R-2 
F 0.0989 0.1409 0.1901 
R 0.0929 0.0951 0.1185 
P 0.1533 0.1733 0.1877 R-W-1.2 
F 0.1157 0.1228 0.1453 
  Random Default COGENT 
R 0.2481 0.2620 0.3820 
P 0.2657 0.3424 0.3772 R-S* 
F 0.2566 0.2969 0.3796 
Table 1. Quality evaluation results 
Table 1 indicates that COGENT consistently 
outperforms the Random and Default summaries. 
These results indicate the promise of our approach 
to generate extractive summaries of educational 
science content. Given our interest in generating a 
pedagogically effective domain knowledge map, 
we have also conducted a content-centric evalua-
tion. 
To characterize the COGENT summary con-
tents, one of the authors manually constructed a 
summary corresponding to the best case output for 
an extractive summarizer. This Best Case summary 
comprises all the sentences from the resources that 
align to all the concepts selected by the experts. 
This summary comprises 621 sentences consisting 
of 13,116 words, or about a 9% word compression.  
We use ROUGE-L to examine the union LCS 
between the reference and candidate summaries, 
thus capturing their linguistic surface structure 
similarity. We also use MEAD to report on cosine 
similarity. Table 2 shows the results for recall (R), 
precision (P), and balanced f-measure (F). 
  Random 
(5%) 
Default 
(5%) 
COGENT 
(5%) 
Best Case  
(9%) 
R 0.4814 0.4919 0.6021 0.9669 
P 0.4982 0.5623 0.5982 0.6256 R-L 
F 0.4897 0.5248 0.6001 0.7597 
Cosine 0.5382 0.6748 0.8325 0.9323 
Table 2. Content evaluation results (word compression) 
The ROUGE-L scores consistently indicate that 
the COGENT summary may be closer to the refer-
ence in linguistic surface structure than either the 
Random or Default summaries. Since the 
COGENT ROUGE-L recall score (R=0. 6021) is 
lower than the Best Case (R=0.9669), it is likely 
that COGENT may be extracting different sen-
tences than those selected by the experts. Based on 
the high cosine similarity with the reference 
(0.8325), we hypothesize that COGENT may be 
selecting sentences that cover very similar con-
cepts to those selected by the experts, but ex-
pressed differently. 
Given the difference in word compression for 
the Best Case summary, we have performed an 
19
incremental analysis using the ROUGE-L measure 
shown in Figure 2. 
ROUGE-L COGENT Evaluation
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
0 5 10 15 20 25 30
MEAD Word Percent Compression
Recall Precision F-Measure
 
Figure 2. Incremental COGENT ROUGE-L analysis 
Figure 2 indicates that COGENT can match the 
Best Case recall (R=0.9669) by generating a longer 
summary. For educational applications, lengthier 
summaries may be better suited for computational 
purposes, such as diagnosing student understand-
ing, while shorter summaries may be more appro-
priate for display to the student. 
6 Conclusions 
COGENT extends MEAD based on strategies elic-
ited from an empirical study with domain and in-
structional experts. Initial evaluation results 
indicate that COGENT holds promise for identify-
ing important domain pedagogical concepts. We 
are exploring portability to other science education 
domains and machine learning techniques to con-
nect concepts into a knowledge map. Automating 
the creation of inventories of pedagogically impor-
tant concepts may represent an important step to-
wards scalable intelligent tutoring systems. 
Acknowledgements 
This research is funded in part by the National Sci-
ence Foundation under NSF IIS/ALT Award 
0537194. Any opinions, findings, and conclusions 
or recommendations expressed in this material are 
those of the author(s) and do not necessarily reflect 
the views of the NSF. 
References 
T. Byrt, J. Bishop and J. B. Carlin. Bias, prevalence, and 
kappa. Journal of Clinical Epidemiology, 46, 5 
(1993), 423-429. 
S. de la Chica, F. Ahmad, T. Sumner, J. H. Martin and 
K. Butcher. Computational foundations for personal-
izing instruction with digital libraries. International 
Journal of Digital Libraries, to appear in the Special 
Issue on Digital Libraries and Education. 
S. Harabagiu and F. Lacatusu. Topic themes for multi-
document summarization. In Proc. of the 28th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval, 
(Salvador, Brazil, 2005), 202-209. 
H. Hardy, N. Shimizu, T. Strzalkowski, L. Ting, G. B. 
Wise and X. Zhang. Summarizing large document 
sets using concept-based clustering. In Proc. of the 
Human Language Technology Conference 2002, 
(San Diego, California, United States, 2002), 222-
227. 
L. L. Hill. Core elements of digital gazetteers: place-
names, categories, and footprints. In Proc. of the 4th 
European Conference on Digital Libraries, (Lisbon, 
Portugal, 2000), 280-290. 
C. D. Holley and D. F. Dansereau. Spatial learning 
strategies: Techniques, applications, and related is-
sues. Academic Press, Orlando, Florida, 1984. 
C. Y. Lin. ROUGE: A package for automatic evaluation 
of summaries. In Proc. of the Workshop on Text 
Summarization Branches Out, (Barcelona, Spain, 
2004). 
D. Lin and P. Pantel. Concept discovery from text. In 
Proc. of the 19th International Conference on Com-
putational Linguistics, (Taipei, Taiwan, 2002), 1-7. 
National Research Council. National Science Education 
Standards. National Academy Press, Washington, 
DC, 1996. 
R. Navigli and P. Velardi. Learning domain ontologies 
from document warehouses and dedicated websites. 
Computational Linguistics, 30, 2 (2004), 151-179. 
A. M. O'Donnell, D. F. Dansereau and R. H. Hall. 
Knowledge maps as scaffolds for cognitive process-
ing. Educational Psychology Review, 14, 1 (2002), 
71-86. 
Project 2061. Benchmarks for science literacy. Oxford 
University Press, New York, New York, United 
States, 1993. 
D. R. Radev, H. Jing and M. Budzikowska. Centroid-
based summarization of multiple documents: sen-
tence extraction, utility-based evaluation, and user 
studies. In Proc. of the ANLP/NAACL 2000 Work-
shop on Summarization, (2000), 21-30. 
T. Sumner, S. Bhushan, F. Ahmad and Q. Gu. Design-
ing a language for creating conceptual browsing in-
terfaces for digital libraries. In Proc. of the 3rd 
ACM/IEEE-CS Joint Conference on Digital Librar-
ies, (Houston, Texas, 2003), 258-260. 
A. Zouaq, R. Nkambou and C. Frasson. Learning a do-
main ontology in the Knowledge Puzzle project. In 
Proc. of the Fifth International Workshop on Ontolo-
gies and Semantic Web for E-Learning, (Marina del 
Rey, California, 2007). 
20
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 177?180,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning Semantic Links from a Corpus of
Parallel Temporal and Causal Relations
Steven Bethard
Institute for Cognitive Science
Department of Computer Science
University of Colorado
Boulder, CO 80309, USA
steven.bethard@colorado.edu
James H. Martin
Institute for Cognitive Science
Department of Computer Science
University of Colorado
Boulder, CO 80309, USA
james.martin@colorado.edu
Abstract
Finding temporal and causal relations is cru-
cial to understanding the semantic structure
of a text. Since existing corpora provide no
parallel temporal and causal annotations, we
annotated 1000 conjoined event pairs, achiev-
ing inter-annotator agreement of 81.2% on
temporal relations and 77.8% on causal re-
lations. We trained machine learning mod-
els using features derived from WordNet and
the Google N-gram corpus, and they out-
performed a variety of baselines, achieving
an F-measure of 49.0 for temporals and 52.4
for causals. Analysis of these models sug-
gests that additional data will improve perfor-
mance, and that temporal information is cru-
cial to causal relation identification.
1 Introduction
Working out how events are tied together temporally
and causally is a crucial component for successful
natural language understanding. Consider the text:
(1) I ate a bad tuna sandwich, got food poisoning
and had to have a shot in my shoulder. wsj 0409
To understand the semantic structure here, a system
must order events along a timeline, recognizing that
getting food poisoning occurred BEFORE having a
shot. The system must also identify when an event
is not independent of the surrounding events, e.g.
got food poisoning was CAUSED by eating a bad
sandwich. Recognizing these temporal and causal
relations is crucial for applications like question an-
swering which must face queries like How did he get
food poisoning? or What was the treatment?
Currently, no existing resource has all the neces-
sary pieces for investigating parallel temporal and
causal phenomena. The TimeBank (Pustejovsky et
al., 2003) links events with BEFORE and AFTER
relations, but includes no causal links. PropBank
(Kingsbury and Palmer, 2002) identifies ARGM-TMP
and ARGM-CAU relations, but arguments may only
be temporal or causal, never both. Thus existing
corpora are missing some crucial pieces for study-
ing temporal-causal interactions. Our research aims
to fill these gaps by building a corpus of parallel
temporal and causal relations and exploring machine
learning approaches to extracting these relations.
2 Related Work
Much recent work on temporal relations revolved
around the TimeBank and TempEval (Verhagen et
al., 2007). These works annotated temporal relations
between events and times, but low inter-annotator
agreement made many TimeBank and TempEval
tasks difficult (Boguraev and Ando, 2005; Verha-
gen et al, 2007). Still, TempEval showed that on a
constrained tense identification task, systems could
achieve accuracies in the 80s, and Bethard and col-
leagues (Bethard et al, 2007) showed that temporal
relations between a verb and a complement clause
could be identified with accuracies of nearly 90%.
Recent work on causal relations has also found
that arbitrary relations in text are difficult to annotate
and give poor system performance (Reitter, 2003).
Girju and colleagues have made progress by select-
ing constrained pairs of events using web search pat-
terns. Both manually generated Cause-Effect pat-
terns (Girju et al, 2007) and patterns based on nouns
177
Full Train Test
Documents 556 344 212
Event pairs 1000 697 303
BEFORE relations 313 232 81
AFTER relations 16 11 5
CAUSAL relations 271 207 64
Table 1: Contents of the corpus and its train/test sections
Task Agreement Kappa F
Temporals 81.2 0.715 71.9
Causals 77.8 0.556 66.5
Table 2: Inter-annotator agreement by task.
linked causally in WordNet (Girju, 2003) were used
to collect examples for annotation, with the result-
ing corpora allowing machine learning models to
achieve performance in the 70s and 80s.
3 Conjoined Events Corpus
Prior work showed that finding temporal and causal
relations is more tractable in carefully selected cor-
pora. Thus we chose a simple construction that
frequently expressed both temporal and causal rela-
tions, and accounted for 10% of all adjacent verbal
events: events conjoined by the word and.
Our temporal annotation guidelines were based
on the guidelines for TimeBank and TempEval, aug-
mented with the guidelines of (Bethard et al, 2008).
Annotators used the labels:
BEFORE The first event fully precedes the second
AFTER The second event fully precedes the first
NO-REL Neither event clearly precedes the other
Our causal annotation guidelines were based on
paraphrasing rather than the intuitive notions of
cause used in prior work (Girju, 2003; Girju et al,
2007). Annotators selected the best paraphrase of
?and? from the following options:
CAUSAL and as a result, and as a consequence,
and enabled by that
NO-REL and independently, and for similar reasons
To build the corpus, we first identified verbs
that represented events by running the system of
(Bethard and Martin, 2006) on the TreeBank. We
then used a set of tree-walking rules to identify con-
joined event pairs. 1000 pairs were annotated by
two annotators and adjudicated by a third. Table 1
S
ADVP
RB
Then
NP
PRP
they
VP
VP CC VP
VBD
took
NP
DT
the
NN
art
PP
TO
to
NP
NNP
Acapulco
and
began
SVBD
VP
TO
to
VP
VB
trade
NP
some of it
PP
for cocaine
Figure 1: Syntactic tree from wsj 0450 with events took
and began highlighted.
and Table 2 give statistics for the resulting corpus1.
The annotators had substantial agreement on tem-
porals (81.2%) and moderate agreement on causals
(77.8%). We also report F-measure agreement, since
BEFORE, AFTER and CAUSAL relations are more in-
teresting than NO-REL. Annotators had F-measure
agreement of 71.9 on temporals and 66.5 causals.
4 Machine Learning Methods
We used our corpus for machine learning experi-
ments where relation identification was viewed as
pair-wise classification. Consider the sentence:
(2) The man who had brought it in for an esti-
mate had [EVENT returned] to collect it and was
[EVENT waiting] in the hall. wsj 0450
A temporal classifier should label returned-waiting
with BEFORE since returned occurred first, and a
causal classifier should label it CAUSAL since this
and can be paraphrased as and as a result.
We identified both syntactic and semantic features
for our task. These will be described using the ex-
ample event pair in Figure 1. Our syntactic features
characterized surrounding surface structures:
? The event words, lemmas and part-of-speech tags,
e.g. took, take, VBD and began, begin, VBD.
? All words, lemmas and part-of-speech tags in the
verb phrases of each event, e.g. took, take, VBD
and began, to, trade, begin, trade, VBD,TO,VB.
? The syntactic paths from the first event to
the common ancestor to the second event, e.g.
VBD>VP, VP and VP<VBD.
1Train: wsj 0416-wsj 0759. Test: wsj 0760-wsj 0971.
verbs.colorado.edu/?bethard/treebank-verb-conj-anns.xml
178
? All words before, between and after the event pair,
e.g. Then, they plus the, art, to, Acapulco, and
plus to, trade, some, of, it, for, cocaine.
Our semantic features encoded surrounding word
meanings. We used WordNet (Fellbaum, 1998) root
synsets (roots) and lexicographer file names (lex-
names) to derive the following features:
? All event roots and lexnames, e.g. take#33,
move#1 . . . body, change . . . for took and be#0,
begin#1 . . . change, communication . . . for began.
? All lexnames before, between and after the event
pair, e.g. all plus artifact, location, etc. plus pos-
session, artifact, etc.
? All roots and lexnames shared by both events, e.g.
took and began were both act#0, be#0 and change,
communication, etc.
? The least common ancestor (LCA) senses shared
by both events, e.g. took and began meet only at
their roots, so the LCA senses are act#0 and be#0.
We also extracted temporal and causal word associ-
ations from the Google N-gram corpus (Brants and
Franz, 2006), using <keyword> <pronoun>
<word> patterns, where before and after were the
keywords for temporals, and because was the key-
word for causals. Word scores were assigned as:
score(w) = log
(
Nkeyword(w)
N(w)
)
where Nkeyword(w) is the number of times the word
appeared in the keyword?s pattern, and N(w) is the
number of times the word was in the corpus. The
following features were derived from these scores:
? Whether the event score was in at least the N th
percentile, e.g. took?s ?6.1 because score placed
it above 84% of the scores, so the feature was true
for N = 70 and N = 80, but false for N = 90.
? Whether the first event score was greater than the
second by at least N , e.g. took and began have
after scores of ?6.3 and ?6.2 so the feature was
true for N = ?1, but false for N = 0 and N = 1.
5 Results
We trained SVMperf classifiers (Joachims, 2005) for
the temporal and causal relation tasks2 using the
2We built multi-class SVMs using the one-vs-rest approach
and used 5-fold cross-validation on the training data to set pa-
rameters. For temporals, C=0.1 (for syntactic-only models),
Temporals Causals
Model P R F1 P R F1
BEFORE 26.7 94.2 41.6 - - -
CAUSAL - - - 21.1 100.0 34.8
1st Event 35.0 24.4 28.8 31.0 20.3 24.5
2nd Event 36.1 30.2 32.9 22.4 17.2 19.5
POS Pair 46.7 8.1 13.9 30.0 4.7 8.1
Syntactic 36.5 53.5 43.4 24.4 79.7 37.4
Semantic 35.8 55.8 43.6 27.2 64.1 38.1
All 43.6 55.8 49.0 27.0 59.4 37.1
All+Tmp - - - 46.9 59.4 52.4
Table 3: Performance of the temporal relation identifica-
tion models: (A)ccuracy, (P)recision, (R)ecall and (F1)-
measure. The null label is NO-REL.
train/test split from Table 1 and the feature sets:
Syntactic The syntactic features from Section 4.
Semantic The semantic features from Section 4.
All Both syntactic and semantic features.
All+Tmp (Causals Only) Syntactic and semantic
features, plus the gold-standard temporal label.
We compared our models against several baselines,
using precision, recall and F-measure since the NO-
REL labels were uninteresting. Two simple baselines
had 0% recall: a lookup table of event word pairs3,
and the majority class (NO-REL) label for causals.
We therefore considered the following baselines:
BEFORE Classify all instances as BEFORE, the ma-
jority class label for temporals.
CAUSAL Classify all instances as CAUSAL.
1st Event Use a lookup table of 1st words and the
labels they were assigned in the training data.
2nd Event As 1st Event, but using 2nd words.
POS Pair As 1st Event, but using part of speech tag
pairs. POS tags encode tense, so this suggests the
performance of a tense-based classifier.
The results on our test data are shown in Table 3. For
temporal relations, the F-measures of all SVM mod-
els exceeded all baselines, with the combination of
syntactic and semantic features performing 5 points
better (43.6% precision and 55.8% recall) than either
feature set individually. This suggests that our syn-
tactic and semantic features encoded complemen-
tary information for the temporal relation task. For
C=1.0 (for all other models), and loss-function=F1 (for all
models). For causals, C=0.1 and loss-function=precision/recall
break even point (for all models).
3Only 3 word pairs from training were seen during testing.
179
Figure 2: Model precisions (dotted lines) and percent of
events in the test data seen during training (solid lines),
given increasing fractions of the training data.
causal relations, all SVM models again exceeded all
baselines, but combining syntactic features with se-
mantic ones gained little. However, knowing about
underlying temporal relations boosted performance
to 46.9% precision and 59.4% recall. This shows
that progress in causal relation identification will re-
quire knowledge of temporal relations.
We examined the effect of corpus size on our
models by training them on increasing fractions of
the training data and evaluating them on the test
data. The precisions of the resulting models are
shown as dotted lines in Figure 2. The models im-
prove steadily, and the causals precision can be seen
to follow the solid curves which show how event
coverage increases with increased training data. A
logarithmic trendline fit to these seen-event curves
suggests that annotating all 5,013 event pairs in the
Penn TreeBank could move event coverage up from
the mid 50s to the mid 80s. Thus annotating addi-
tional data should provide a substantial benefit to our
temporal and causal relation identification systems.
6 Conclusions
Our research fills a gap in existing corpora and NLP
systems, examining parallel temporal and causal re-
lations. We annotated 1000 event pairs conjoined
by the word and, assigning each pair both a tempo-
ral and causal relation. Annotators achieved 81.2%
agreement on temporal relations and 77.8% agree-
ment on causal relations. Using features based on
WordNet and the Google N-gram corpus, we trained
support vector machine models that achieved 49.0
F on temporal relations, and 37.1 F on causal rela-
tions. Providing temporal information to the causal
relations classifier boosted its results to 52.4 F. Fu-
ture work will investigate increasing the size of the
corpus and developing more statistical approaches
like the Google N-gram scores to take advantage of
large-scale resources to characterize word meaning.
Acknowledgments
This research was performed in part under an ap-
pointment to the U.S. Department of Homeland Se-
curity (DHS) Scholarship and Fellowship Program.
References
S. Bethard and J. H. Martin. 2006. Identification of event
mentions and their semantic class. In EMNLP-2006.
S. Bethard, J. H. Martin, and S. Klingenstein. 2007.
Timelines from text: Identification of syntactic tem-
poral relations. In ICSC-2007.
S. Bethard, W. Corvey, S. Klingenstein, and J. H. Martin.
2008. Building a corpus of temporal-causal structure.
In LREC-2008.
B. Boguraev and R. K. Ando. 2005. Timebank-
driven timeml analysis. In Annotating, Extracting
and Reasoning about Time and Events. IBFI, Schloss
Dagstuhl, Germany.
T. Brants and A. Franz. 2006. Web 1t 5-gram version 1.
Linguistic Data Consortium, Philadelphia.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Database. MIT Press.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Turney,
and D. Yuret. 2007. Semeval-2007 task 04: Classi-
fication of semantic relations between nominals. In
SemEval-2007.
R. Girju. 2003. Automatic detection of causal relations
for question answering. In ACL Workshop on Multi-
lingual Summarization and Question Answering.
T. Joachims. 2005. A support vector method for multi-
variate performance measures. In ICML-2005.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In LREC-2002.
J. Pustejovsky, P. Hanks, R. Saur??, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,
and M. Lazo. 2003. The timebank corpus. In Corpus
Linguistics, pages 647?656.
D. Reitter. 2003. Simple signals for complex
rhetorics: On rhetorical analysis with rich-feature sup-
port vector models. LDV-Forum, GLDV-Journal for
Computational Linguistics and Language Technology,
18(1/2):38?52.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
G. Katz, and J. Pustejovsky. 2007. Semeval-2007
task 15: Tempeval temporal relation identification. In
SemEval-2007.
180
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 241?244,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extracting a Representation from Text for Semantic Analysis  Rodney D. Nielsen1,2, Wayne Ward1,2, James H. Martin1, and Martha Palmer1 1 Center for Computational Language and Education Research, University of Colorado, Boulder 2 Boulder Language Technologies, 2960 Center Green Ct., Boulder, CO 80301 Rodney.Nielsen, Wayne.Ward, James.Martin, Martha.Palmer@Colorado.edu       Abstract We present a novel fine-grained semantic rep-resentation of text and an approach to con-structing it. This representation is largely extractable by today?s technologies and facili-tates more detailed semantic analysis. We dis-cuss the requirements driving the representation, suggest how it might be of value in the automated tutoring domain, and provide evidence of its validity. 1 Introduction This paper presents a new semantic representation intended to allow more detailed assessment of stu-dent responses to questions from an intelligent tu-toring system (ITS). Assessment within current ITSs generally provides little more than an indica-tion that the student?s response expressed the target knowledge or it did not. Furthermore, virtually all ITSs are developed in a very domain-specific way, with each new question requiring the handcrafting of new semantic extraction frames, parsers, logic representations, or knowledge-based ontologies (c.f., Jordan et al, 2004). This is also true of re-search in the area of scoring constructed response questions (e.g., Leacock, 2004). The goal of the representation described here is to facilitate domain-independent assessment of student responses to questions in the context of a known reference answer and to perform this as-sessment at a level of detail that will enable more effective ITS dialog. We have two key criteria for this representation: 1) it must be at a level that fa-cilitates detailed assessment of the learner?s under-standing, indicating exactly where and in what manner the answer did not meet expectations and 
2) the representation and assessment should be learnable by an automated system ? they should not require the handcrafting of domain-specific representations of any kind.  Rather than have a single expressed versus un-expressed assessment of the reference answer as a whole, we instead break the reference answer down into what we consider to be approximately its lowest level compositional facets. This roughly translates to the set of triples composed of labeled (typed) dependencies in a dependency parse of the reference answer. Breaking the reference answer down into fine-grained facets permits a more fo-cused assessment of the student?s response, but a simple yes or no entailment at the facet level still lacks semantic expressiveness with regard to the relation between the student?s answer and the facet in question, (e.g., did the student contradict the facet or completely fail to address it?) Therefore, it is also necessary to break the annotation labels into finer levels in order to specify more clearly the relationship between the student?s answer and the reference answer facet. The emphasis of this paper is on this fine-grained facet-based representation ? considerations in defining it, the process of extract-ing it, and the benefit of using it. 2 Representing the Target Knowledge We acquired grade 3-6 responses to 287 questions from the Assessing Science Knowledge (ASK) project (Lawrence Hall of Science, 2006). The re-sponses, which range in length from moderately short verb phrases to several sentences, cover all 16 diverse Full Option Science System teaching and learning modules spanning life science, physi-cal science, earth and space science, scientific rea-soning, and technology. We generated a corpus by transcribing a random sample (approx. 15400) of the students? handwritten responses. 
241
2.1 Knowledge Representation The ASK assessments included a reference answer for each constructed response question. These ref-erence answers were manually decomposed into fine-grained facets, roughly extracted from the re-lations in a syntactic dependency parse and a shal-low semantic parse. The decomposition is based closely on these well-established frameworks, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al, 2006). Figure 1 illustrates the process of deriving the constituent facets that comprise the representation of the final reference answer. We begin by deter-mining the dependency parse following the style of MaltParser (Nivre et al, 2006). This dependency parse was then modified in several ways. The ra-tionale for the modifications, which we elaborate below, is to increase the semantic content of facets. These more expressive facets are used later to gen-erate features for the assessment classification task. These types of modifications to the parser output address known limitations of current statistical parser outputs, and are reminiscent of the modifi-cations advocated by Briscoe and Carroll for more effective parser evaluation, (Briscoe, et. al, 2002). Example 1 illustrates the reference answer facets derived from the final dependencies in Figure 1, along with their glosses.  Figure 1. Reference answer representation revisions (1) The brass ring would not stick to the nail because the ring is not iron. (1a)  NMod(ring, brass)  (1a?) The ring is brass. (1b)  Theme_not(stick, ring) (1b?) The ring does not stick. (1c)  Destination_to_not(stick, nail) (1c?) Something does not stick to the nail. (1d)  Be_not(ring, iron) (1d?) The ring is not iron. (1e)  Cause_because(1b-c, 1d) (1e?) 1b and 1c are caused by 1d. Various linguistic theories take a different stance on what term should be the governor in a 
number of phrase types, particularly noun phrases. In this regard, the manual parses here varied from the style of MaltParser by raising lexical items to governor status when they contextually carried more significant semantics. In our example, the verb stick is made the governor of would, whose modifiers are reattached to stick. Similarly, the noun phrases the pattern of pigments and the bunch of leaves typically result in identical dependency parses. However, the word pattern is considered the governor of pigments; whereas, conversely the word leaves is treated as the governor of bunch because it carries more semantics. Then, terms that were not crucial to the student answer, frequently auxiliary verbs, were removed (e.g., the modal would and determiners in our example). Next, we incorporate prepositions into the de-pendency type labels following (Lin and Pantel, 2001). This results in the two dependencies vmod(stick, to) and pmod(to, nail), each of which carries little semantic value over its key lexical item, stick and nail, being combined into the sin-gle, more expressive dependency vmod_to(stick, nail), ultimately vmod is replaced with destination, as described below. Likewise, the dependencies connected by because are consolidated and be-cause is integrated into the new dependency type.  Next, copulas and a few similar verbs are also incorporated into the dependency types. The verb?s predicate is reattached to its subject, which be-comes the governor, and the dependency is labeled with the verb?s root. In our example, the two se-mantically impoverished dependencies sub(is, ring) and prd(is, iron) are combined to form the more meaningful dependency be(ring, iron). Then terms of negation are similarly incorporated into the dependency types. Finally, wherever a shallow semantic parse would identify a predicate argument structure, we used the thematic role labels in VerbNet (Kipper et al, 2000) between the predicate and the argu-ment?s headword, rather than the MaltParser de-pendency tags. This also involved adding new structural dependencies that a typical dependency parser would not generate. For example, in the sen-tence As it freezes the water will expand and crack the glass, typically the dependency between crack and its subject water is not generated since it would lead to a non-projective tree, but it does play the role of Agent in a semantic parse. In a small number of instances, these labels were also at-
242
tached to noun modifiers, most notably the Loca-tion label. For example, given the reference answer fragment The water on the floor had a much larger surface area, one of the facets extracted was Loca-tion_on(water, floor). We refer to facets that express relations between higher-level propositions as inter-propositional facets. An example of such a facet is (1e) above, connecting the proposition the brass ring did not stick to the nail to the proposition the ring is not iron. In addition to specifying the headwords of inter-propositional facets (stick and is, in 1e), we also note up to two key facets from each of the propositions that the relation is connecting (b, c, and d in example 1). Reference answer facets that are assumed to be understood by the learner a pri-ori, (e.g., because they are part of the question), are also annotated to indicate this.  There were a total of 2878 reference answer fac-ets, resulting in a mean of 10 facets per answer (median 8). Facets that were assumed to be under-stood a priori by students accounted for 33% of all facets and inter-propositional facets accounted for 11%. The results of automated annotation of stu-dent answers (section 3) focus on the facets that are not assumed to be understood a priori (67% of all facets); of these, 12% are inter-propositional.  A total of 36 different facet relation types were utilized. The majority, 21, are VerbNet thematic roles. Direction, Manner, and Purpose are Prop-Bank adjunctive argument labels (Palmer et al, 2005). Quantifier, Means, Cause-to-Know and copulas were added to the preceding roles. Finally, anything that did not fit into the above categories retained its dependency parse type: VMod (Verb Modifier), NMod (Noun Modifier), AMod (Adjec-tive or Adverb Modifier), and Root (Root was used when a single word in the answer, typically yes, no, agree, disagree, A-D, etc., stood alone without a significant relation to the remainder of the refer-ence answer; this occurred only 21 times, account-ing for fewer than 1% of the reference answer facets). The seven highest frequency relations are NMod, Theme, Cause, Be, Patient, AMod, and Location, which together account for 70% of the reference answer facet relations 2.2 Student Answer Annotation For each student answer, we annotated each reference answer facet to indicate whether and how 
the student addressed that facet. We settled on the five annotation categories in Table 1. These labels and the annotation process are detailed in (Nielsen et al, 2008b).  Understood: Reference answer facets directly ex-pressed or whose understanding is inferred Contradiction: Reference answer facets contradicted by negation, antonymous expressions, pragmatics, etc. Self-Contra: Reference answer facets that are both con-tradicted and implied (self contradictions) Diff-Arg: Reference answer facets whose core relation is expressed, but it has a different modifier or argument Unaddressed: Reference answer facets that are not ad-dressed at all by the student?s answer Table 1. Facet Annotation Labels 3 Automated Classification As partial validation of this knowledge representa-tion, we present results of an automatic assessment of our student answers. We start with the hand generated reference answer facets. We generate automatic parses for the reference answers and the student answers and automatically modify these parses to match our desired representation. Then for each reference answer facet, we extract features indicative of the student?s understanding of that facet. Finally, we train a machine learning classi-fier on training data and use it to classify unseen test examples, assigning a Table 1 label for each reference answer facet. We used a variety of linguistic features that as-sess the facets? similarity via lexical entailment probabilities following (Glickman et al, 2005), part of speech tags and lexical stem matches. They include information extracted from modified de-pendency parses such as relevant relation types and path edit distances. Revised dependency parses are used to align the terms and facet-level information for feature extraction. Remaining details can be found in (Nielsen et al, 2008a) and are not central to the semantic representation focus of this paper. Current classification accuracy, assigning a Table 1 label to each reference answer facet to indicate the student?s expressed understanding, is 79% within domain (assessing unseen answers to ques-tions associated with the training data) and 69% out of domain (assessing answers to questions re-garding entirely different science subjects). These results are 26% and 15% over the majority class baselines, respectively, and 21% and 6% over lexi-
243
cal entailment baselines based on Glickman et al (2005). 4 Discussion and Future Work Analyzing the results of reference facet extraction, there are many interesting open linguistic issues in this area. This includes the need for a more sophisticated treatment of adjectives, conjunctions, plurals and quantifiers, all of which are known to be beyond the abilities of state of the art parsers. Analyzing the dependency parses of 51 of the student answers, about 24% had errors that could easily lead to problems in assessment. Over half of these errors resulted from inopportune sentence segmentation due to run-on student sentences con-joined by and (e.g., the parse of a shorter string makes a higher pitch and a longer string makes a lower pitch, errantly conjoined a higher pitch and a longer string as the subject of makes a lower pitch, leaving a shorter string makes without an object). We are working on approaches to mitigate this problem.  In the long term, when the ITS generates its own questions and reference answers, the system will have to construct its own reference answer facets. The automatic construction of reference answer facets must deal with all of the issues described in this paper and is a significant area of future research. Other key areas of future research involve integrating the representation described here into an ITS and evaluating its impact. 5 Conclusion We presented a novel fine-grained semantic repre-sentation and evaluated it in the context of auto-mated tutoring. A significant contribution of this representation is that it will facilitate more precise tutor feedback, targeted to the specific facet of the reference answer and pertaining to the specific level of understanding expressed by the student. This representation could also be useful in areas such as question answering or document summari-zation, where a series of entailed facets could be composed to form a full answer or summary. The representation?s validity is partially demon-strated in the ability of annotators to reliably anno-tate inferences at this facet level, achieving substantial agreement (86%, Kappa=0.72) and by promising results in automatic assessment of stu-
dent answers at this facet level (up to 26% over baseline), particularly given that, in addition to the manual reference answer facet representation, an automatically extracted approximation of the rep-resentation was a key factor in the features utilized by the classifier.  The domain independent approach described here enables systems that can easily scale up to new content and learning environments, avoiding the need for lesson planners or technologists to create extensive new rules or classifiers for each new question the system must handle. This is an obligatory first step to the long-term goal of creat-ing ITSs that can truly engage children in natural unrestricted dialog, such as is required to perform high quality student directed Socratic tutoring. Acknowledgments This work was partially funded by Award Number 0551723 from the National Science Foundation. References  Briscoe, E., Carroll, J., Graham, J., and Copestake, A. 2002. Relational evaluation schemes. In Proc. of the Beyond PARSEVAL Workshop at LREC. Gildea, D. and Jurafsky, D. 2002. Automatic labeling of semantic roles. Computational Linguistics. Glickman, O, Dagan, I, and Koppel, M. 2005. Web Based Probabilistic Textual Entailment. In Proc RTE. Jordan, P, Makatchev, M, VanLehn, K. 2004. Combin-ing competing language understanding approaches in an intelligent tutoring system. In Proc ITS. Kipper, K, Dang, H, and Palmer, M. 2000. Class-Based Construction of a Verb Lexicon. In Proc. AAAI. Lawrence Hall of Science 2006. Assessing Science Knowledge (ASK), UC Berkeley, NSF-0242510 Leacock, C. 2004. Scoring free-response automatically: A case study of a large-scale Assessment. Examens. Lin, D & Pantel, P. 2001. Discovery of inference rules for Question Answering. In Natl. Lang. Engineering. Nielsen, R, Ward, W, and Martin, JH. 2008a. Learning to Assess Low-level Conceptual Understanding. In Proc. FLAIRS. Nielsen, R, Ward, W, Martin, JH and Palmer, P. 2008b. Annotating Students? Understanding of Science Con-cepts. In Proc. LREC. Nivre, J, Hall, J, Nilsson, J, Eryigit, G and Marinov, S. 2006. Labeled Pseudo-Projective Dependency Pars-ing with Support Vector Machines. In Proc. CoNLL. Palmer, M, Gildea, D, & Kingsbury, P. 2005. The proposition bank: An annotated corpus of semantic roles. In Computational Linguistics. 
244
Semantic Role Labeling by Tagging Syntactic Chunks?
Kadri Hacioglu1, Sameer Pradhan1, Wayne Ward1, James H. Martin1, Daniel Jurafsky2
1University of Colorado at Boulder, 2Stanford University
{hacioglu,spradhan,whw}@cslr.colorado.edu, martin@cs.colorado.edu, jurafsky@stanford.edu
Abstract
In this paper, we present a semantic role la-
beler (or chunker) that groups syntactic chunks
(i.e. base phrases) into the arguments of a pred-
icate. This is accomplished by casting the se-
mantic labeling as the classification of syntactic
chunks (e.g. NP-chunk, PP-chunk) into one of
several classes such as the beginning of an ar-
gument (B-ARG), inside an argument (I-ARG)
and outside an argument (O). This amounts to
tagging syntactic chunks with semantic labels
using the IOB representation. The chunker is
realized using support vector machines as one-
versus-all classifiers. We describe the represen-
tation of data and information used to accom-
plish the task. We participate in the ?closed
challenge? of the CoNLL-2004 shared task and
report results on both development and test
sets.
1 Introduction
In semantic role labeling the goal is to group sequences
of words together and classify them by using semantic la-
bels. For meaning representation the predicate-argument
structure that exists in most languages is used. In this
structure a word (most frequently a verb) is specified as
a predicate, and a number of word groups are considered
as arguments accompanying the word (or predicate).
In this paper, we select support vector machines
(SVMs) (Vapnik, 1995; Burges, 1998) to implement
the semantic role classifiers, due to their ability to han-
dle an extremely large number of (overlapping) features
with quite strong generalization properties. Support vec-
tor machines for semantic role chunking were first used
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IIS-9978025
in (Hacioglu and Ward, 2003) as word-by-word (W-by-
W) classifiers. The system was then applied to the
constituent-by-constituent (C-by-C) classification in (Ha-
cioglu et al, 2003). In (Pradhan et al, 2003; Prad-
han et al, 2004), several extensions to the basic system
have been proposed, extensively studied and systemati-
cally compared to other systems. In this paper, we imple-
ment a system that classifies syntactic chunks (i.e. base
phrases) instead of words or the constituents derived from
syntactic trees. This system is referred to as the phrase-
by-phrase (P-by-P) semantic role classifier. We partici-
pate in the ?closed challenge? of the CoNLL-2004 shared
task and report results on both development and test sets.
A detailed description of the task, data and related work
can be found in (Carreras and Ma`rquez, 2004).
2 System Description
2.1 Data Representation
In this paper, we change the representation of the original
data as follows:
? Bracketed representation of roles is converted into
IOB2 representation (Ramhsaw and Marcus, 1995;
Sang and Veenstra, 1995)
? Word tokens are collapsed into base phrase (BP) to-
kens.
Since the semantic annotation in the PropBank corpus
does not have any embedded structure there is no loss of
information in the first change. However, this results in
a simpler representation with a reduced set of tagging la-
bels. In the second change, it is possible to miss some
information in cases where the semantic chunks do not
align with the sequence of BPs. However, in Section 3.2
we show that the loss in performance due to the misalign-
ment is much less than the gain in performance that can
be achieved by the change in representation.
from
million
251.2
$
to
declined VBD
CD
NN
IN
CD
$
TO
Sales
%
CD
10
$
278.7
$
CD
NNS
*A2)
*
*
*A4)
*
*
million CD I?NP
O
*
*
B?NP
B?VP
B?NP
I?NP
B?PP
B?NP
I?NP
I?NP
B?PP
B?NP
I?NP
*
(S*
*
*
*
*
*
*
*
*
*
decline
?
?
?
?
?
?
?
?
?
?
?
?
*A3)
(A3*
(A4*
(A2*
(V*V)
(A1*A1)
*S)
O
NP
VP
NP
PP
NP
PP
NP
Sales
declined
NNS
VBD
% NN
TOto
million
from
million
B?NP
CD
CD
O
I?NP
B?PP
I?NP
B?PP
I?NP
?
?
?
?
?
?
?
*
*
*
*
*
* B?V
B?A2
B?A1
B?A3
B?A4
O
O
O
B?VP
IN
*S)
(S*
(b)(a)
. .
. .
decline
Figure 1: Illustration of change in data representation; (a) original word-by-word data representation (b) phrase-by-
phrase data representation used in this paper. Words are collapsed into base phrase types retaining only headwords
with their respective features. Bracketed representation of semantic role labels is converted into IOB2 representation.
See text for details.
The new representation is illustrated in Figure 1 along
with the original representation. Comparing both we note
the following differences and advantages in the new rep-
resentation:
? BPs are being classified instead of words.
? Only the BP headwords (rightmost words) are re-
tained as word information.
? The number of tagging steps is smaller.
? A fixed context spans a larger segment of a sentence.
Therefore, the P-by-P semantic role chunker classifies
larger units, ignores some of the words, uses a relatively
larger context for a given window size and performs the
labeling faster.
2.2 Features
The following features, which we refer to as the base fea-
tures, are provided in the shared task data for each sen-
tence;
? Words
? Predicate lemmas
? Part of Speech tags
? BP Positions: The position of a token in a BP using
the IOB2 representation (e.g. B-NP, I-NP, O etc.)
? Clause tags: The tags that mark token positions in a
sentence with respect to clauses. (e.g *S)*S) marks
a position that two clauses end)
? Named entities: The IOB tags of named entities.
There are four categories; LOC, ORG, PERSON
and MISC.
Using available information we have created the fol-
lowing token level features:
? Token Position: The position of the phrase with re-
spect to the predicate. It has three values as ?be-
fore?, ?after? and ?-? for the predicate.
? Path: It defines a flat path between the token and
the predicate as a chain of base phrases. At both
ends, the chain is terminated with the POS tags of
the predicate and the headword of the token.
? Clause bracket patterns: We use two patterns of
clauses for each token. One is the clause bracket
chain between the token and the predicate, and the
other is from the token to sentence begin or end de-
pending on token?s position with respect to the pred-
icate.
? Clause Position: a binary feature that indicates the
token is inside or outside of the clause which con-
tains the predicate
? Headword suffixes: suffixes of headwords of length
2, 3 and 4.
? Distance: we have two notions of distance; the first
is the distance of the token from the predicate as a
number of base phrases, and the second is the same
distance as the number of VP chunks.
? Length: the number of words in a token.
We also use some sentence level features:
? Predicate POS tag: the part of speech category of
the predicate
? Predicate Frequency; this is a feature which indi-
cates whether the predicate is frequent or rare with
respect to the training set. The threshold on the
counts is currently set to 3.
? Predicate BP Context : The chain of BPs centered
at the predicate within a window of size -2/+2.
? Predicate POS Context : The POS tags of the
words that immediately precede and follow the pred-
icate. The POS tag of a preposition is replaced with
the preposition itself.
? Predicate Argument Frames: We used the left and
right patterns of the core arguments (A0 through A5)
for each predicate . We used the three most frequent
argument frames for both sides depending on the po-
sition of the token in focus with respect to the pred-
icate. (e.g. raise has A0 and A1 AO (A0 being the
most frequent) as its left argument frames, and A1,
A1 A2 and A2 as the three most frequent right argu-
ment frames)
? Number of predicates: This is the number of pred-
icates in the sentence.
For each token (base phrase) to be tagged, a set of or-
dered features is created from a fixed size context that
surrounds each token. In addition to the above features,
we also use previous semantic IOB tags that have already
been assigned to the tokens contained in the context. A
5-token sliding window is used for the context. A greedy
left-to-right tagging is performed.
All of the above features are designed to implicitly cap-
ture the patterns of sentence constructs with respect to
different word/predicate usages and senses. We acknowl-
edge that they significantly overlap and extensive exper-
iments are required to determine the impact of each fea-
ture on the performance.
2.3 Classifier
All SVM classifiers were realized using TinySVM1 with
a polynomial kernel of degree 2 and the general purpose
SVM based chunker YamCha 2. SVMs were trained for
begin (B) and inside (I) classes of all arguments and one
outside (O) class for a total of 78 one-vs-all classifiers
(some arguments do not have an I-tag).
1http://cl.aist-nara.ac.jp/taku-ku/software/TinySVM
2http://cl.aist-nara.ac.jp/taku-ku/software/yamcha
Table 1: Comparison of W-by-W and P-by-P methods.
Both systems use the base features provided (i.e. no fea-
ture engineering is done). Results are on dev set.
Method Precision Recall F?=1
P-by-P 69.04% 54.68% 61.02
W-by-W 68.34% 45.16% 54.39
Table 2: Number of sentences and unique training exam-
ples in each method.
Method Sentences Training Examples
P-by-P 19K 347K
W-by-W 19K 534K
3 Experimental Results
3.1 Data and Evaluation Metrics
The data provided for the shared task is a part of the
February 2004 release of the PropBank corpus. It con-
sists of sections from the Wall Street Journal part of the
Penn Treebank. All experiments were carried out using
Sections 15-18 for training Section-20 for development
and Section-21 for testing. The results were evaluated for
precision, recall and F?=1 numbers using the srl-eval.pl
script provided by the shared task organizers.
3.2 W-by-W and P-by-P Experiments
In these experiments we used only the base features to
compare the two approaches. Table 1 illustrates the over-
all performance on the dev set. Although both systems
were trained using the same number of sentences, the ac-
tual number of training examples in each case were quite
different. Those numbers are presented in Table 2. It is
clear that P-by-P method uses much less data for the same
number of sentences. Despite this we particularly note a
considerable improvement in recall. Actually, the data
reduction was not without a cost. Some arguments have
been missed as they do not align with the base phrase
chunks due to inconsistencies in semantic annotation and
due to errors in automatic base phrase chunking. The per-
centage of this misalignment was around 2.5% (over the
dev set). We observed that nearly 45% of the mismatches
were for the ?outside? chunks. Therefore, sequences of
words with outside tags were not collapsed.
3.3 Best System Results
In these experiments all of the features described earlier
were used with the P-by-P system. Table 3 presents our
best system performance on the development set. Ad-
ditional features have improved the performance from
61.02 to 71.72. The performance of the same system on
the test set is similarly illustrated in Table 4.
Table 3: System results on development set.
Precision Recall F?=1
Overall 74.17% 69.42% 71.72
A0 82.86% 78.50% 80.62
A1 72.82% 73.97% 73.39
A2 60.16% 56.18% 58.10
A3 59.66% 47.65% 52.99
A4 83.21% 74.15% 78.42
A5 100.00% 75.00% 85.71
AM-ADV 52.52% 41.48% 46.35
AM-CAU 61.11% 41.51% 49.44
AM-DIR 47.37% 15.00% 22.78
AM-DIS 76.47% 76.47% 76.47
AM-EXT 74.07% 40.82% 52.63
AM-LOC 51.21% 46.09% 48.51
AM-MNR 51.04% 36.83% 42.78
AM-MOD 99.47% 95.63% 97.51
AM-NEG 99.20% 94.66% 96.88
AM-PNC 70.00% 28.00% 40.00
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 69.33% 58.37% 63.38
R-A0 91.55% 80.25% 85.53
R-A1 72.46% 67.57% 69.93
R-A2 100.00% 52.94% 69.23
R-AM-LOC 100.00% 25.00% 40.00
R-AM-TMP 0.00% 0.00% 0.00
V 99.05% 99.05% 99.05
4 Conclusions
We have described a semantic role chunker using SVMs.
The chunking method has been based on a chunked sen-
tence structure at both syntactic and semantic levels. We
have jointly performed semantic chunk segmentation and
labeling using a set of one-vs-all SVM classifiers on a
phrase-by-phrase basis. It has been argued that the new
representation has several advantages as compared to the
original representation. It yields a semantic role labeler
that classifies larger units, exploits relatively larger con-
text, uses less data (possibly, redundant and noisy data
are filtered out), runs faster and performs better.
References
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the CoNLL-2004 Shared Task: Semantic Role La-
beling in the same volume of Proc. of CoNLL?2004
Shared Task.
Christopher J. C. Burges. 1997. A Tutorial on Support
Vector Machines for Pattern Recognition. Data Min-
ing and Knowledge Discovery, 2(2), pages 1-47.
Kadri Hacioglu and Wayne Ward. 2003. Target word
Table 4: System results on test set.
Precision Recall F?=1
Overall 72.43% 66.77% 69.49
A0 82.93% 79.88% 81.37
A1 71.92% 71.33% 71.63
A2 49.37% 49.30% 49.33
A3 57.50% 46.00% 51.11
A4 87.10% 54.00% 66.67
A5 0.00% 0.00% 0.00
AM-ADV 53.36% 38.76% 44.91
AM-CAU 57.89% 22.45% 32.35
AM-DIR 37.84% 28.00% 32.18
AM-DIS 66.83% 62.44% 64.56
AM-EXT 70.00% 50.00% 58.33
AM-LOC 46.63% 36.40% 40.89
AM-MNR 50.31% 31.76% 38.94
AM-MOD 98.12% 92.88% 95.43
AM-NEG 91.11% 96.85% 93.89
AM-PNC 52.00% 15.29% 23.64
AM-PRD 0.00% 0.00% 0.00
AM-TMP 64.57% 50.74% 56.82
R-A0 90.21% 81.13% 85.43
R-A1 83.02% 62.86% 71.54
R-A2 100.00% 33.33% 50.00
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 60.00% 21.43% 31.58
V 98.46% 98.46% 98.46
Detection and Semantic Role Chunking Using Support
Vector Machines. Proc. of the HLT-NAACL-03.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James
Martin, and Dan Jurafsky. 2003. Shallow Semantic
Parsing Using Support Vector Machines. CSLR Tech.
Report, CSLR-TR-2003-1.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James
Martin, and Dan Jurafsky. 2003. Semantic Role Pars-
ing: Adding Semantic Structure to Unstructured Text.
Proc. of Int. Conf. on Data Mining (ICDM03).
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James
Martin, and Dan Jurafsky. 2004. Support Vector
Learning for Semantic Argument Classification. to ap-
pear in Journal of Machine Learning.
Lance E. Ramhsaw and Mitchell P. Marcus. 1995.
Text Chunking Using Transformation Based Learning.
Proc. of the 3rd ACL Workshop on Very Large Cor-
pora, pages 82-94.
Erik F. T. J. Sang, John Veenstra. 1999. Representing
Text Chunks. Proc. of EACL?99, pages 173-179.
Vladamir Vapnik 1995. The Nature of Statistical Learn-
ing Theory. Springer Verlag, New York, USA.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 217?220, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Chunking Combining Complementary Syntactic Views
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Martin and Daniel Jurafsky?
Center for Spoken Language Research, University of Colorado, Boulder, CO 80303
?Department of Linguistics, Stanford University, Stanford, CA 94305
{spradhan,hacioglu,whw,martin}@cslr.colorado.edu, jurafsky@stanford.edu
Abstract
This paper describes a semantic role la-
beling system that uses features derived
from different syntactic views, and com-
bines them within a phrase-based chunk-
ing paradigm. For an input sentence, syn-
tactic constituent structure parses are gen-
erated by a Charniak parser and a Collins
parser. Semantic role labels are assigned
to the constituents of each parse using
Support Vector Machine classifiers. The
resulting semantic role labels are con-
verted to an IOB representation. These
IOB representations are used as additional
features, along with flat syntactic chunks,
by a chunking SVM classifier that pro-
duces the final SRL output. This strategy
for combining features from three differ-
ent syntactic views gives a significant im-
provement in performance over roles pro-
duced by using any one of the syntactic
views individually.
1 Introduction
The task of Semantic Role Labeling (SRL) involves
tagging groups of words in a sentence with the se-
mantic roles that they play with respect to a particu-
lar predicate in that sentence. Our approach is to use
supervised machine learning classifiers to produce
the role labels based on features extracted from the
input. This approach is neutral to the particular set
of labels used, and will learn to tag input according
to the annotated data that it is trained on. The task
reported on here is to produce PropBank (Kingsbury
and Palmer, 2002) labels, given the features pro-
vided for the CoNLL-2005 closed task (Carreras and
Ma`rquez, 2005).
We have previously reported on using SVM clas-
sifiers for semantic role labeling. In this work, we
formulate the semantic labeling problem as a multi-
class classification problem using Support Vector
Machine (SVM) classifiers. Some of these systems
use features based on syntactic constituents pro-
duced by a Charniak parser (Pradhan et al, 2003;
Pradhan et al, 2004) and others use only a flat syn-
tactic representation produced by a syntactic chun-
ker (Hacioglu et al, 2003; Hacioglu and Ward,
2003; Hacioglu, 2004; Hacioglu et al, 2004). The
latter approach lacks the information provided by
the hierarchical syntactic structure, and the former
imposes a limitation that the possible candidate roles
should be one of the nodes already present in the
syntax tree. We found that, while the chunk based
systems are very efficient and robust, the systems
that use features based on full syntactic parses are
generally more accurate. Analysis of the source
of errors for the parse constituent based systems
showed that incorrect parses were a major source
of error. The syntactic parser did not produce any
constituent that corresponded to the correct segmen-
tation for the semantic argument. In Pradhan et al
(2005), we reported on a first attempt to overcome
this problem by combining semantic role labels pro-
duced from different syntactic parses. The hope is
that the syntactic parsers will make different errors,
and that combining their outputs will improve on
217
either system alone. This initial attempt used fea-
tures from a Charniak parser, a Minipar parser and a
chunk based parser. It did show some improvement
from the combination, but the method for combin-
ing the information was heuristic and sub-optimal.
In this paper, we report on what we believe is an im-
proved framework for combining information from
different syntactic views. Our goal is to preserve the
robustness and flexibility of the segmentation of the
phrase-based chunker, but to take advantage of fea-
tures from full syntactic parses. We also want to
combine features from different syntactic parses to
gain additional robustness. To this end, we use fea-
tures generated from a Charniak parser and a Collins
parser, as supplied for the CoNLL-2005 closed task.
2 System Description
We again formulate the semantic labeling problem
as a multi-class classification problem using Sup-
port Vector Machine (SVM) classifiers. TinySVM1
along with YamCha2 (Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2001) are used to implement
the system. Using what is known as the ONE VS
ALL classification strategy, n binary classifiers are
trained, where n is number of semantic classes in-
cluding a NULL class.
The general framework is to train separate seman-
tic role labeling systems for each of the parse tree
views, and then to use the role arguments output by
these systems as additional features in a semantic
role classifier using a flat syntactic view. The con-
stituent based classifiers walk a syntactic parse tree
and classify each node as NULL (no role) or as one
of the set of semantic roles. Chunk based systems
classify each base phrase as being the B(eginning)
of a semantic role, I(nside) a semantic role, or
O(utside) any semantic role (ie. NULL). This
is referred to as an IOB representation (Ramshaw
and Marcus, 1995). The constituent level roles are
mapped to the IOB representation used by the chun-
ker. The IOB tags are then used as features for a
separate base-phase semantic role labeler (chunker),
in addition to the standard set of features used by
the chunker. An n-fold cross-validation paradigm
is used to train the constituent based role classifiers
1http://chasen.org/?taku/software/TinySVM/
2http://chasen.org/?taku/software/yamcha/
and the chunk based classifier.
For the system reported here, two full syntactic
parsers were used, a Charniak parser and a Collins
parser. Features were extracted by first generating
the Collins and Charniak syntax trees from the word-
by-word decomposed trees in the CoNLL data. The
chunking system for combining all features was
trained using a 4-fold paradigm. In each fold, sepa-
rate SVM classifiers were trained for the Collins and
Charniak parses using 75% of the training data. That
is, one system assigned role labels to the nodes in
Charniak based trees and a separate system assigned
roles to nodes in Collins based trees. The other 25%
of the training data was then labeled by each of the
systems. Iterating this process 4 times created the
training set for the chunker. After the chunker was
trained, the Charniak and Collins based semantic la-
belers were then retrained using all of the training
data.
Two pieces of the system have problems scaling
to large training sets ? the final chunk based clas-
sifier and the NULL VS NON-NULL classifier for
the parse tree syntactic views. Two techniques were
used to reduce the amount of training data ? active
sampling and NULL filtering. The active sampling
process was performed as follows. We first train
a system using 10k seed examples from the train-
ing set. We then labeled an additional block of data
using this system. Any sentences containing an er-
ror were added to the seed training set. The sys-
tem was retrained and the procedure repeated until
there were no misclassified sentences remaining in
the training data. The set of examples produced by
this procedure was used to train the final NULL VS
NON-NULL classifier. The same procedure was car-
ried out for the chunking system. After both these
were trained, we tagged the training data using them
and removed all most likely NULLs from the data.
Table 1 lists the features used in the constituent
based systems. They are a combination of features
introduced by Gildea and Jurafsky (2002), ones pro-
posed in Pradhan et al (2004), Surdeanu et al
(2003) and the syntactic-frame feature proposed in
(Xue and Palmer, 2004). These features are ex-
tracted from the parse tree being labeled. In addition
to the features extracted from the parse tree being
labeled, five features were extracted from the other
parse tree (phrase, head word, head word POS, path
218
PREDICATE LEMMA
PATH: Path from the constituent to the predicate in the parse tree.
POSITION: Whether the constituent is before or after the predicate.
PREDICATE SUB-CATEGORIZATION
HEAD WORD: Head word of the constituent.
HEAD WORD POS: POS of the head word
NAMED ENTITIES IN CONSTITUENTS: Person, Organization, Location
and Miscellaneous.
PARTIAL PATH: Path from the constituent to the lowest common ancestor
of the predicate and the constituent.
HEAD WORD OF PP: Head of PP replaced by head word of NP inside it,
and PP replaced by PP-preposition
FIRST AND LAST WORD/POS IN CONSTITUENT
ORDINAL CONSTITUENT POSITION
CONSTITUENT TREE DISTANCE
CONSTITUENT RELATIVE FEATURES: Nine features representing
the phrase type, head word and head word part of speech of the
parent, and left and right siblings of the constituent.
SYNTACTIC FRAME
CONTENT WORD FEATURES: Content word, its POS and named entities
in the content word
CLAUSE-BASED PATH VARIATIONS:
I. Replacing all the nodes in a path other than clause nodes with an ?*?.
For example, the path NP?S?VP?SBAR?NP?VP?VBD
becomes NP?S?*S?*?*?VBD
II. Retaining only the clause nodes in the path, which for the above
example would produce NP?S?S?VBD,
III. Adding a binary feature that indicates whether the constituent
is in the same clause as the predicate,
IV. collapsing the nodes between S nodes which gives NP?S?NP?VP?VBD.
PATH N-GRAMS: This feature decomposes a path into a series of trigrams.
For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes:
NP?S?VP, S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, etc. We
used the first ten trigrams as ten features. Shorter paths were padded
with nulls.
SINGLE CHARACTER PHRASE TAGS: Each phrase category is clustered
to a category defined by the first character of the phrase label.
PREDICATE CONTEXT: Two words and two word POS around the
predicate and including the predicate were added as ten new features.
PUNCTUATION: Punctuation before and after the constituent were
added as two new features.
FEATURE CONTEXT: Features for argument bearing constituents
were added as features to the constituent being classified.
Table 1: Features used by the constituent-based sys-
tem
and predicate sub-categorization). So for example,
when assigning labels to constituents in a Charniak
parse, all of the features in Table 1 were extracted
from the Charniak tree, and in addition phrase, head
word, head word POS, path and sub-categorization
were extracted from the Collins tree. We have pre-
viously determined that using different sets of fea-
tures for each argument (role) achieves better results
than using the same set of features for all argument
classes. A simple feature selection was implemented
by adding features one by one to an initial set of
features and selecting those that contribute signifi-
cantly to the performance. As described in Pradhan
et al (2004), we post-process lattices of n-best de-
cision using a trigram language model of argument
sequences.
Table 2 lists the features used by the chunker.
These are the same set of features that were used
in the CoNLL-2004 semantic role labeling task by
Hacioglu, et al (2004) with the addition of the two
semantic argument (IOB) features. For each token
(base phrase) to be tagged, a set of features is created
from a fixed size context that surrounds each token.
In addition to the features in Table 2, it also uses pre-
vious semantic tags that have already been assigned
to the tokens contained in the linguistic context. A
5-token sliding window is used for the context.
SVMs were trained for begin (B) and inside (I)
classes of all arguments and an outside (O) class.
WORDS
PREDICATE LEMMAS
PART OF SPEECH TAGS
BP POSITIONS: The position of a token in a BP using the IOB2
representation (e.g. B-NP, I-NP, O, etc.)
CLAUSE TAGS: The tags that mark token positions in a sentence
with respect to clauses.
NAMED ENTITIES: The IOB tags of named entities.
TOKEN POSITION: The position of the phrase with respect to
the predicate. It has three values as ?before?, ?after? and ?-? (for
the predicate)
PATH: It defines a flat path between the token and the predicate
HIERARCHICAL PATH: Since we have the syntax tree for the sentences,
we also use the hierarchical path from the phrase being classified to the
base phrase containing the predicate.
CLAUSE BRACKET PATTERNS
CLAUSE POSITION: A binary feature that identifies whether the
token is inside or outside the clause containing the predicate
HEADWORD SUFFIXES: suffixes of headwords of length 2, 3 and 4.
DISTANCE: Distance of the token from the predicate as a number
of base phrases, and the distance as the number of VP chunks.
LENGTH: the number of words in a token.
PREDICATE POS TAG: the part of speech category of the predicate
PREDICATE FREQUENCY: Frequent or rare using a threshold of 3.
PREDICATE BP CONTEXT: The chain of BPs centered at the predicate
within a window of size -2/+2.
PREDICATE POS CONTEXT: POS tags of words immediately preceding
and following the predicate.
PREDICATE ARGUMENT FRAMES: Left and right core argument patterns
around the predicate.
DYNAMIC CLASS CONTEXT: Hypotheses generated for two preceeding
phrases.
NUMBER OF PREDICATES: This is the number of predicates in
the sentence.
CHARNIAK-BASED SEMANTIC IOB TAG: This is the IOB tag generated
using the tagger trained on Charniak trees
COLLINS-BASED SEMANTIC IOB TAG: This is the IOB tag generated
using the tagger trained on Collins? trees
Table 2: Features used by phrase-based chunker.
3 Experimental Results
Table 3 shows the results obtained on the WSJ de-
velopment set (Section 24), the WSJ test set (Section
23) and the Brown test set (Section ck/01-03)
4 Acknowledgments
This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and
by the NSF via grants IS-9978025 and ITR/HCI
219
Precision Recall F?=1
Development 80.90% 75.38% 78.04
Test WSJ 81.97% 73.27% 77.37
Test Brown 73.73% 61.51% 67.07
Test WSJ+Brown 80.93% 71.69% 76.03
Test WSJ Precision Recall F?=1
Overall 81.97% 73.27% 77.37
A0 91.39% 82.23% 86.57
A1 79.80% 76.23% 77.97
A2 68.61% 62.61% 65.47
A3 73.95% 50.87% 60.27
A4 78.65% 68.63% 73.30
A5 75.00% 60.00% 66.67
AM-ADV 61.64% 46.05% 52.71
AM-CAU 76.19% 43.84% 55.65
AM-DIR 53.33% 37.65% 44.14
AM-DIS 80.56% 63.44% 70.98
AM-EXT 100.00% 46.88% 63.83
AM-LOC 64.48% 51.52% 57.27
AM-MNR 62.90% 45.35% 52.70
AM-MOD 98.64% 92.38% 95.41
AM-NEG 98.21% 95.65% 96.92
AM-PNC 56.67% 44.35% 49.76
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 83.37% 71.94% 77.23
R-A0 94.29% 88.39% 91.24
R-A1 85.93% 74.36% 79.73
R-A2 100.00% 37.50% 54.55
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 90.00% 42.86% 58.06
R-AM-MNR 66.67% 33.33% 44.44
R-AM-TMP 75.00% 40.38% 52.50
V 98.86% 98.86% 98.86
Table 3: Overall results (top) and detailed results on
the WSJ test (bottom).
0086132. Computer time was provided by NSF
ARI Grant #CDA-9601817, NSF MRI Grant #CNS-
0420873, NASA AIST grant #NAG2-1646, DOE
SciDAC grant #DE-FG02-04ER63870, NSF spon-
sorship of the National Center for Atmospheric Re-
search, and a grant from the IBM Shared University
Research (SUR) program.
Special thanks to Matthew Woitaszek, Theron Vo-
ran and the other administrative team of the Hemi-
sphere and Occam Beowulf clusters. Without these
the training would never be possible.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. n Introduction to the CoNLL-2005
Shared Task: Semantic Role Labeling. In Proceedings of CoNLL-2005.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Kadri Hacioglu and Wayne Ward. 2003. Target word detection and semantic
role chunking using support vector machines. In Proceedings of the Human
Language Technology Conference, Edmonton, Canada.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Shallow semantic parsing using support vector machines. Technical
Report TR-CSLR-2003-1, Center for Spoken Language Research, Boulder,
Colorado.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Daniel Ju-
rafsky. 2004. Semantic role labeling by tagging syntactic chunks. In Pro-
ceedings of the 8th Conference on CoNLL-2004, Shared Task ? Semantic Role
Labeling.
Kadri Hacioglu. 2004. A lightweight semantic chunking model based on tagging.
In Proceedings of the Human Language Technology Conference /North Amer-
ican chapter of the Association of Computational Linguistics (HLT/NAACL),
Boston, MA.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In
Proceedings of the 3rd International Conference on Language Resources and
Evaluation (LREC-2002), Las Palmas, Canary Islands, Spain.
Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk
identification. In Proceedings of the 4th Conference on CoNLL-2000 and
LLL-2000, pages 142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines.
In Proceedings of the 2nd Meeting of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-2001).
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Semantic role parsing: Adding semantic structure to unstructured text.
In Proceedings of the International Conference on Data Mining (ICDM 2003),
Melbourne, Florida.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2004. Shallow semantic parsing using support vector machines. In Proceed-
ings of the Human Language Technology Conference/North American chapter
of the Association of Computational Linguistics (HLT/NAACL), Boston, MA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2005. Semantic role labeling using different syntactic views. In Proceedings
of the Association for Computational Linguistics 43rd annual meeting (ACL-
2005), Ann Arbor, MI.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using transformation-
based learning. In Proceedings of the Third Annual Workshop on Very Large
Corpora, pages 82?94. ACL.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Us-
ing predicate-argument structures for information extraction. In Proceedings
of the 41st Annual Meeting of the Association for Computational Linguistics,
Sapporo, Japan.
Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role
labeling. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing, Barcelona, Spain.
220
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 146?154,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Identification of Event Mentions and their Semantic Class 
 
 
Steven Bethard 
Department of Computer Science 
University of Colorado at Boulder 
430 UCB, Boulder, CO 80309, USA 
steven.bethard@colorado.edu 
James H. Martin 
Department of Computer Science 
University of Colorado at Boulder 
430 UCB, Boulder, CO 80309, USA 
james.martin@colorado.edu 
 
  
 
Abstract 
Complex tasks like question answering 
need to be able to identify events in text 
and the relations among those events. We 
show that this event identification task 
and a related task, identifying the seman-
tic class of these events, can both be for-
mulated as classification problems in a 
word-chunking paradigm. We introduce a 
variety of linguistically motivated fea-
tures for this task and then train a system 
that is able to identify events with a pre-
cision of 82% and a recall of 71%. We 
then show a variety of analyses of this 
model, and their implications for the 
event identification task. 
1 Introduction 
Research in question answering, machine transla-
tion and other fields has shown that being able to 
recognize the important entities in a text is often 
a critical component of these systems. Such en-
tity information gives the machine access to a 
deeper level of semantics than words alone can 
provide, and thus offers advantages for these 
complex tasks. Of course, texts are composed of 
much more than just sets of entities, and archi-
tectures that rely solely on word and entity-based 
techniques are likely to have difficulty with tasks 
that depend more heavily on event and temporal 
relations. Consider a question answering system 
that receives the following questions: 
? Is Anwar al-Sadat still the president of 
Egypt? 
? How did the linking of the Argentinean 
peso to the US dollar in 1991 contribute to 
economic crisis of Argentina in 2003? 
Processing such questions requires not only 
knowing what the important people, places and 
other entities are, but also what kind of events 
they are involved in, the roles they play in those 
events, and the relations among those events. 
Thus, we suggest that identifying such events in 
a text should play an important role in systems 
that attempt to address questions like these. 
Of course, to identify events in texts, we must 
define what exactly it is we mean by ?event?. In 
this work, we adopt a traditional linguistic defini-
tion of an event that divides words into two as-
pectual types: states and events. States describe 
situations that are static or unchanging for their 
duration, while events describe situations that 
involve some internal structure. For example, 
predicates like know and love would be states 
because if we know (or love) someone for a pe-
riod of time, we know (or love) that person at 
each point during the period. Predicates like run 
or deliver a sermon would be events because 
they are built of smaller dissimilar components: 
run includes raising and lowering of legs and 
deliver a sermon includes the various tongue 
movements required to produce words. 
To better explain how we approach the task of 
identifying such events, we first discuss some 
past work on related tasks. Then we briefly dis-
cuss the characteristics of the TimeBank, a cor-
pus containing event-annotated data. Next we 
present our formulation of event identification as 
a classification task and introduce the linguistic 
features that serve as input to the algorithm. Fi-
nally, we show the results of STEP (our ?System 
for Textual Event Parsing?) which applies these 
techniques to the TimeBank data. 
2 Related Efforts 
Such aspectual distinctions have been alive and 
well in the linguistic literature since at least the 
late 60s (Vendler, 1967). However, the use of the 
146
term event in natural language processing work 
has often diverged quite considerably from this 
linguistic notion. In the Topic Detection and 
Tracking (TDT) task, events were sets of docu-
ments that described ?some unique thing that 
happens at some point in time? (Allan et. al., 
1998). In the Message Understanding Confer-
ence (MUC), events were groups of phrases that 
formed a template relating participants, times 
and places to each other (Marsh and Per-
zanowski, 1997). In the work of Filatova and 
Hatzivassiloglou (2003), events consisted of a 
verb and two named-entities occurring together 
frequently across several documents on a topic. 
Several recent efforts have stayed close to the 
linguistic definition of events. One such example 
is the work of Siegel and McKeown (2000) 
which showed that machine learning models 
could be trained to identify some of the tradi-
tional linguistic aspectual distinctions. They 
manually annotated the verbs in a small set of 
texts as either state or event, and then used a va-
riety of linguistically motivated features to train 
machine learning models that were able to make 
the event/state distinction with 93.9% accuracy.  
Another closely related effort was the Evita 
system, developed by Saur? et. al. (2005). This 
work considered a corpus of events called 
TimeBank, whose annotation scheme was moti-
vated largely by the linguistic definitions of 
events. Saur? et. al. showed that a linguistically 
motivated and mainly rule-based algorithm could 
perform well on this task. 
Our work draws from both the Siegel and 
McKeown and Saur? et. al. works. We consider 
the same TimeBank corpus as Saur? et. al., but 
apply a statistical machine learning approach 
akin to that of Siegel and McKeown. We demon-
strate that combining machine learning tech-
niques with linguistically motivated features can 
produce models from the TimeBank data that are 
capable of making a variety of subtle aspectual 
distinctions. 
3 Events in the TimeBank 
TimeBank (Pustejovsky, et. al. 2003b) consists 
of just under 200 documents containing 70,000 
words; it is drawn from news texts from a variety 
of different domains, including newswire and 
transcribed broadcast news. These documents are 
annotated using the TimeML annotation scheme 
(Pustejovsky, et. al. 2003a), which aims to iden-
tify not just times and dates, but events and the 
temporal relations between these events. 
Of interest here are the EVENT annotations, 
of which TimeBank 1.1 has annotated 8312. 
TimeBank annotates a word or phrase as an 
EVENT if it describes a situation that can ?hap-
pen? or ?occur?, or if it describes a ?state? or 
?circumstance? that ?participate[s] in an opposi-
tion structure in a given text? (Pustejovsky, et. al. 
2003b). Note that the TimeBank events are not 
restricted to verbs; nouns and adjectives denote 
events as well. 
The TimeBank definition of event differs in a 
few ways from the traditional linguistic defini-
tion of event. TimeBank EVENTs include not 
only the normal linguistic events, but also some 
linguistic states, depending on the contexts in 
which they occur. For example1, in the sentence 
None of the people on board the airbus survived 
the crash the phrase on board would be consid-
ered to describe an EVENT because that state 
changes in the time span covered by the text. Not 
all linguistic states become TimeBank EVENTs 
in this manner, however. For example, the state 
described by New York is on the east coast holds 
true for a time span much longer than the typical 
newswire document and would therefore not be 
labeled as an EVENT. 
In addition to identifying which words in the 
TimeBank are EVENTs, the TimeBank also pro-
vides a semantic class label for each EVENT. 
The possible labels include OCCURRENCE, 
PERCEPTION, REPORTING, ASPECTUAL, 
STATE, I_STATE, I_ACTION, and MODAL, 
and are described in more detail in (Pustejovsky, 
et. al. 2003a). 
We consider two tasks on this data: 
(1) Identifying which words and phrases are 
EVENTs, and 
(2) Identifying their semantic classes. 
The next section describes how we turn these 
tasks into machine learning problems. 
4 Event Identification as Classification 
We view event identification as a classification 
task using a word-chunking paradigm similar to 
that used by Carreras et. al. (2002). For each 
word in a document, we assign a label indicating 
whether the word is inside or outside of an event. 
We use the standard B-I-O formulation of the 
word-chunking task that augments each class 
label with an indicator of whether the given word 
                                                 
1
 These examples are derived from (Pustejovsky, et. al. 
2003b) 
147
is (B)eginning, (I)nside or (O)utside of a chunk 
(Ramshaw & Marcus, 1995). So, for example, 
under this scheme, sentence (1) would have its 
words labeled as in Table 1. 
(1) The company?s sales force 
[EVENT(I_ACTION) applauded] the 
[EVENT(OCCURRENCE) shake up] 
The two columns of labels in Table 1 show how 
the class labels differ depending on our task. If 
we?re interested only in the simple event identi-
fication task, it?s sufficient to know that ap-
plauded and shake both begin events (and so 
have the label B), up is inside an event (and so 
has the label I), and all other words are outside 
events (and so have the label O). These labels are 
shown in the column labeled Event Label. If in 
addition to identifying events, we also want to 
identify their semantic classes, then we need to 
know that applauded begins an intentional action 
event (B_I_ACTION), shake begins an occur-
rence event (B_OCCURRENCE), up is inside an 
occurrence event (I_OCCURRENCE), and all 
other words are outside of events (O). These la-
bels are shown in the column labeled Event Se-
mantic Class Label. Note that while the eight 
semantic class labels in the TimeBank could po-
tentially introduce as many as 8 ? 2 + 1 = 17 
chunk labels, not all types of events appear as 
multi-word phrases, so we see only 13 of these 
labels in our data. 
5 Classifier Features 
Having cast the problem as a chunking task, our 
next step is to select and represent a useful set of 
features. In our case, since each classification 
instance is a word, our features need to provide 
the information that we deem important for rec-
ognizing whether a word is part of an event or 
not. We consider a number of such features, 
grouped into feature classes for the purposes of 
discussion. 
5.1 Text feature 
This feature is just the textual string for the word. 
5.2 Affix features 
These features attempt to isolate the potentially 
important subsequences of characters in the 
word.  These are intended to identify affixes that 
have a preference for different types of events. 
Affixes: These features identify the first three 
and four characters of the word, and the last three 
and four characters of the word. 
Nominalization suffix: This feature indicates 
which of the suffixes typically associated with 
nominalizations ? ing(s), ion(s), ment(s), and 
nce(s) ? the word ends with. This overlaps with 
the Suffixes feature, but allows the classifier to 
more easily treat nominalizations specially. 
5.3 Morphological features 
These features identify the various morphologi-
cal variants of a word, so that, for example, the 
words resist, resisted and resistance can all be 
identified as the same basic event type. 
Morphological stem: This feature gives the base 
form of the word, so for example, the stem of 
assisted is assist and the stem of investigations is 
investigation.  Stems are identified with a lookup 
table from the University of Pennsylvania of 
around 300,000 words. 
Root verb: This feature gives the verb from 
which the word is derived. For example, assis-
tance is derived from assist and investigation is 
derived from investigate.  Root verbs are identi-
fied with an in-house lookup table of around 
5000 nominalizations. 
5.4 Word class features 
These features attempt to group the words into 
different types of classes.  The intention here is 
to identify correlations between classes of words 
and classes of events, e.g. that events are more 
likely to be expressed as verbs or in verb phrases 
than they are as nouns. 
Part-of-speech: This feature contains the word?s 
part-of-speech based on the Penn Treebank tag 
set. Part-of-speech tags are assigned by the MX-
POST maximum-entropy based part-of-speech 
tagger (Ratnaparkhi, 1996). 
Word Event Label Event Semantic 
Class Label 
The O O 
company O O 
?s O O 
sales O O 
force O O 
applauded B B_I_ACTION 
the O O 
shake B B_OCCURRENCE 
up I I_OCCURRENCE 
. O O 
Table 1: Event chunks for sentence (1) 
148
Syntactic-chunk label: The value of this feature 
is a B-I-O style label indicating what kind of 
syntactic chunk the word is contained in, e.g. 
noun phrase, verb phrase, or prepositional 
phrase. These are assigned using a word-
chunking SVM-based system trained on the 
CoNLL-2000 data2 (which uses the lowest nodes 
of the Penn TreeBank syntactic trees to break 
sentences into base phrases). 
Word cluster: This feature indicates which verb 
or noun cluster the word is a member of. The 
clusters were derived from the co-occurrence 
statistics of verbs and their direct objects, in the 
same manner as Pradhan et. al. (2004). This pro-
duced 128 clusters (half verbs, half nouns) cover-
ing around 100,000 words. 
5.5 Governing features 
These features attempt to include some simple 
dependency information from the surrounding 
words, using the dependency parses produced by 
Minipar3.  These features aim to identify events 
that are expressed as phrases or that require 
knowledge of the surrounding phrase to be iden-
tified. 
Governing light verb: This feature indicates 
which, if any, of the light verbs be, have, get, 
give, make, put, and take governs the word. This 
is intended to capture adjectival predicates such 
as may be ready, and nominal predicates such as 
make an offer, where ready and offer should be 
identified as events. 
Determiner type: This feature indicates the type 
of determiner a noun phrase has. If the noun 
phrase has an explicit determiner, e.g. a, the or 
some, the value of this feature is the determiner 
itself. We use the determiners themselves as fea-
ture values here because they form a small, 
closed class of words. For open-class determiner-
like modifiers, we instead group them into 
classes.  For noun phrases that are explicitly 
quantified, like a million dollars, the value is 
CARDINAL, while for noun phrases modified 
by other possessive noun phrases, like Bush's 
real objectives, the value is GENITIVE. For 
noun phrases without a determiner-like modifier, 
the value is PROPER_NOUN, BARE_PLURAL 
or BARE_SINGULAR, depending on the noun 
type. 
                                                 
2
 http://cnts.uia.ac.be/conll2000/ 
3
 http://www.cs.ualberta.ca/~lindek/minipar.htm 
Subject determiner type: This feature indicates 
for a verb the determiner type (as above) of its 
subject. This is intended to distinguish generic 
sentences like Cats have fur from non-generics 
like The cat has fur.  
5.6 Temporal features 
These features try to identify temporal relations 
between words.  Since the duration of a situation 
is at the core of the TimeBank definition of 
events, features that can get at such information 
are particularly relevant. 
Time chunk label: The value of this feature is a 
B-I-O label indicating whether or not this word is 
contained in a temporal annotation. The temporal 
annotations are produced by a word-chunking 
SVM-based system trained on the temporal ex-
pressions (TIMEX2 annotations) in the TERN 
2004 data4.  In addition to identifying expres-
sions like Monday and this year, the TERN data 
identifies event-containing expressions like the 
time she arrived at her doctor's office. 
Governing temporal: This feature indicates 
which kind of temporal preposition governs the 
word. Since the TimeBank is particularly inter-
ested in which events start or end within the time 
span of the document, we consider prepositions 
likely to indicate such a change of state, includ-
ing after, before, during, following, since, till, 
until and while. 
Modifying temporal: This feature indicates 
which kind of temporal expression modifies the 
word. Temporal expressions are recognized as 
above, and the type of modification is either the 
preposition that joins the temporal annotation to 
the word, or ADVERBIAL for any non-
preposition modification. This is intended to cap-
ture that modifying temporal expressions often 
indicate event times, e.g. He ran the race in an 
hour. 
5.7 Negation feature 
This feature indicates which negative particle, 
e.g. not, never, etc., modifies the word. The idea 
is based Siegel and McKeown?s (2000) findings 
which suggested that in some corpora states oc-
cur more freely with negation than events do. 
5.8 WordNet hypernym features 
These features indicate to which of the WordNet 
noun and verb sub-hierarchies the word belongs. 
                                                 
4
 http://timex2.mitre.org/tern.html 
149
Rather than include all of the thousands of dif-
ferent sub-hierarchies in WordNet, we first se-
lected the most useful candidates by looking at 
the overlap with WordNet and our training data. 
For each hierarchy in WordNet, we considered a 
classifier that labeled all words in that hierarchy 
as events, and all words outside of that hierarchy 
as non-events5. We then evaluated these classifi-
ers on our training data, and selected the ten with 
the highest F-measures. This resulted in selecting 
the following synsets: 
? noun: state 
? noun: psychological feature 
? noun: event 
? verb: think, cogitate, cerebrate 
? verb: move, displace 
? noun: group, grouping 
? verb: act, move 
? noun: act, human action, human activity 
? noun: abstraction 
? noun: entity 
The values of the features were then whether or 
not the word fell into the hierarchy defined by 
each one of these roots. Note that since there are 
no WordNet senses labeled in our data, we ac-
cept a word as falling into one of the above hier-
archies if any of its senses fall into that hierar-
chy. 
6 Classifier Parameters 
The features described in the previous section 
give us a way to provide the learning algorithm 
with the necessary information to make a classi-
fication decision. The next step is to convert our 
training data into sets of features, and feed these 
classification instances to the learning algorithm. 
For the learning task, we use the TinySVM6 sup-
port vector machine (SVM) implementation in 
conjunction with YamCha7 (Kudo & Matsumoto, 
2001), a suite for general-purpose chunking. 
YamCha has a number of parameters that de-
fine how it learns. The first of these is the win-
dow width of the ?sliding window? that it uses. 
                                                 
5
 We also considered the reverse classifiers, which classi-
fied all words in the hierarchy as non-events and all words 
outside the hierarchy as events. 
6
 http://chasen.org/~taku/software/TinySVM/ 
7
 http://chasen.org/~taku/software/yamcha/ 
A sliding window is a way of including some of 
the context when the classification decision is 
made for a word. This is done by including the 
features of preceding and following words in 
addition to the features of the word to be classi-
fied. To illustrate this, we consider our earlier 
example, now augmented with some additional 
features in Table 2. 
To classify up in this scenario, we now look 
not only at its features, but at the features of 
some of the neighboring words. For example, if 
our window width was 1, the feature values we 
would use for classification would be those in the 
outlined box, that is, the features of shake, up 
and the sentence final period. Note that we do 
not include the classification labels for either up 
or the period since neither of these classifications 
is available at the time we try to classify up. Us-
ing such a sliding window allows YamCha to 
include important information, like that up is 
preceded by shake and that shake was identified 
as beginning an event. 
In addition to the window width parameter, 
YamCha also requires values for the following 
three parameters: the penalty for misclassifica-
tion (C), the kernel?s polynomial degree, and the 
method for applying binary classifiers to our 
multi-class problem, either pair-wise or one-vs-
rest. In our experiments, we chose a one-vs-rest 
multi-class scheme to keep training time down, 
and then tried different variations of all the other 
parameters to explore a variety of models. 
7 Baseline Models 
To be able to meaningfully evaluate the models 
we train, we needed to establish a reasonable 
baseline. Because the majority class baseline 
would simply label every word as a non-event, 
we introduce two baseline models that should be 
more reasonable: Memorize and Sim-Evita. 
Word POS Stem Label 
The DT the O 
company NN company O 
?s POS ?s O 
sales NNS sale O 
force NN force O 
applauded VBD applaud B 
The DT the O 
shake NN shake B 
up RP up  
. . .  
Table 2: A window of word features 
150
The Memorize baseline is essentially a lookup 
table ? it memorizes the training data. This sys-
tem assigns to each word the label with which it 
occurred most frequently in the training data, or 
the label O (not an event) if the word never oc-
curred in the training data. 
The Sim-Evita model is our attempt to simu-
late the Evita system (Saur? et. al. 2005). As part 
of its algorithm, Evita includes a check that de-
termines whether or not a word occurs as an 
event in TimeBank. It performs this check even 
when evaluated on TimeBank, and thus though 
Evita reports 74% precision and 87% recall, 
these numbers are artificially inflated because the 
system was trained and tested on the same cor-
pus. Thus we cannot directly compare our results 
to theirs. Instead, we simulate Evita by taking the 
information that it encodes as rules, and encod-
ing this instead as features which we provide to a 
YamCha-based system. 
Saur? et. al. (2005) provides a description of 
Evita?s rules, which, according to the text, are 
based on information from lexical stems, part of 
speech tags, syntactic chunks, weak stative 
predicates, copular verbs, complements of copu-
lar predicates, verbs with bare plural subjects and 
WordNet ancestors. We decided that the follow-
ing features most fully covered the same infor-
mation: 
? Text 
? Morphological stem 
? Part-of-speech 
? Syntactic-chunk label 
? Governing light verb 
? Subject determiner type 
? WordNet hypernyms 
We also decided that since Evita does not con-
sider a word-window around the word to be clas-
sified, we should set our window size parameter 
to zero. 
Because our approximation of Evita uses a 
feature-based statistical machine learning algo-
rithm instead of the rule-based Evita algorithm, it 
cannot predict how well Evita would perform if 
it had not used the same data for training and 
testing. However, it can give us an approxima-
tion of how well a model can perform using in-
formation similar to that of Evita. 
8 Results 
Having decided on our feature space, our learn-
ing model, and the baselines to which we will 
compare, we now describe the results of our 
models on the TimeBank. We selected a strati-
fied sample of 90% of the TimeBank data for a 
training set, and reserved the remaining 10% for 
testing8. 
We consider three evaluation measures: preci-
sion, recall and F-measure. Precision is defined 
as the number of B and I labels our system iden-
tifies correctly, divided by the total number of B 
and I labels our system predicted. Recall is de-
fined as the number of B and I labels our system 
identifies correctly, divided by the total number 
of B and I labels in the TimeBank data. F-
measure is defined as the geometric mean of pre-
cision and recall9. 
To determine the best parameter settings for 
the models, we performed cross-validations on 
our training data, leaving the testing data un-
touched. We divided the training data randomly 
into five equally-sized sections. Then, for each 
set of parameters to be evaluated, we determined 
a cross-validation F-measure by averaging the F-
measures of five runs, each tested on one of the 
training data sections and trained on the remain-
ing training data sections. We selected the pa-
rameters of the model that had the best cross-
validation F-measure on the training data as the 
parameters for the rest of our experiments. For 
the simple event identification model this se-
lected a window width of 2, polynomial degree 
of 3 and C value of 0.1, and for the event and 
class identification model this selected a window 
width of 1, polynomial degree of 1 and C value 
0.1. For the Sim-Evita simple event identification 
model this selected a degree of 2 and C value of 
0.01, and for the Sim-Evita event and class iden-
tification model, this selected a degree of 1 and C 
value of 1.0. 
Having selected the appropriate parameters for 
our learning algorithm, we then trained our SVM 
models on the training data. Table 3 presents the 
results of these models on the test data. Our 
model (named STEP above for ?System for Tex-
                                                 
8
 The testing documents were: 
APW19980219.0476, APW19980418.0210, 
NYT19980206.0466, PRI19980303.2000.2550, 
ea980120.1830.0071, and the wsj_XXXX_orig documents 
numbered 0122, 0157, 0172, 0313, 0348, 0541, 0584, 0667, 
0736, 0791, 0907, 0991 and 1033. 
9
 
RP
RPF
+
??
=
2
 
151
tual Event Parsing?) outperforms both baselines 
on both tasks. For simple event identification, the 
main win over both baselines is an increased re-
call. Our model achieves a recall of 70.6%, about 
5% better than our simulation of Evita, and 
nearly 15% better than the Memorize baseline. 
For event and class identification, the win is 
again in recall, though to a lesser degree. Our 
system achieves a recall of 51.2%, about 5% bet-
ter than Sim-Evita, and 10% better than Memo-
rize. On this task, we also achieve a precision of 
66.7%, about 10% better than the precision of 
Sim-Evita. This indicates that the model trained 
with no context window and using the Evita-like 
feature set was at a distinct disadvantage over the 
model which had access to all of the features. 
Table 4 and Table 5 show the results of our 
systems on various sub-tasks, with the ?%? col-
umn indicating what percent of the events in the 
test data each subtask contained. Table 4 shows 
that in both tasks, we do dramatically better on 
verbs than on nouns, especially as far as recall is 
concerned. This is relatively unsurprising ? not 
only is there more data for verbs (59% of event 
words are verbs, while only 28% are nouns), but 
our models generally do better on words they 
have seen before, and there are many more nouns 
we have not seen than there are verbs. 
Table 5 shows how well we did individually 
on each type of label. For simple event identifi-
cation (the top two rows) we can see that we do 
substantially better on B labels than on I labels, 
as we would expect since 92% of event words 
are labeled B. The label-wise performance for 
the event and class identification (the bottom 
seven rows) is more interesting. Our best per-
formance is actually on Reporting event words, 
even though the data is mainly Occurrence event 
words. One reason for this is that instances of the 
word said make up about 60% of Reporting 
event words in the TimeBank. The word said is 
relatively easy to get right because it comes with 
by far the most training data10, and because it is 
almost always an event: 98% of the time in the 
TimeBank, and 100% of the time in our test data. 
To determine how much each of the feature 
sets contributed to our models we also performed 
a pair of ablation studies. In each ablation study, 
we trained a series of models on successively 
fewer feature sets, removing the least important 
feature set each time. The least important feature 
set was determined by finding out which feature 
set?s removal caused the smallest drop in F-
measure. The result of this process was a list of 
our feature sets, ordered by importance. These 
lists are given for both tasks in Table 6, along 
with the precision, recall and F-measures of the 
various corresponding models.  Each row in 
Table 6 corresponds to a model trained on the 
feature sets named in that row and all the rows 
below it.  Thus, on the top row, no feature sets 
have been removed, and on the bottom row only 
one feature set remains. 
                                                 
10
 The word ?said? has over 600 instances in TimeBank. 
The word with the next most instances has just over 200 
 Event Identification Event and Class Identification 
Model Precision Recall F Precision Recall F  
Memorize 0.806 0.557 0.658 0.640 0.413 0.502  
Sim-Evita 0.812 0.659 0.727 0.571 0.459 0.509  
STEP 0.820 0.706 0.759 0.667 0.512 0.579  
Table 3: Overall results for both tasks 
 Event Identification Event and Class Identification 
 % Precision Recall F % Precision Recall F 
Verbs 59 0.864 0.903 0.883 59 0.714 0.701 0.707 
Nouns 28 0.729 0.432 0.543 28 0.473 0.261 0.337 
Table 4: Results by word class for both tasks 
 % Precision Recall F 
B 92 0.827 0.737 0.779 
I 8 0.679 0.339 0.452 
B Occurrence 44 0.633 0.727 0.677 
B State 14 0.519 0.136 0.215 
B Reporting 11 0.909 0.779 0.839 
B Istate 10 0.737 0.378 0.500 
B Iaction 10 0.480 0.174 0.255 
I State 7 0.818 0.173 0.286 
B Aspectual 3 0.684 0.684 0.684 
Table 5: Results by label 
152
So, for example, in the simple event identifica-
tion task, we see that the Governing, Negation, 
Affix and WordNet features are hurting the clas-
sifier somewhat ? a model trained without these 
features performs at an F-measure of 0.772, more 
than 1% better than a model including these fea-
tures. In contrast, we can see that for the event 
and semantic class identification task, the Word-
Net and Affix features are actually among the 
most important, with only the Word class fea-
tures accompanying them in the top three. These 
ablation results suggest that word class, textual, 
morphological and temporal information is most 
useful for simple event identification, and affix, 
WordNet and negation information is only really 
needed when the semantic class of an event must 
also be identified. 
The last thing we investigated was the effect 
of additional training data. To do so, we trained 
the model on increasing fractions of the training 
data, and measured the classification accuracy on 
the testing data of each of the models thus 
trained. The resulting graph is shown in Figure 1. 
The Majority line indicates the classifier accu-
racy when the classifier always guesses majority 
class, that is, (O)utside of an event. We can see 
from the two learning curves that even with only 
the small amount of data available in the 
TimeBank, our models are already reaching the 
level part of the learning curve at somewhere 
around 20% of the data. This suggests that, 
though additional data may help somewhat in the 
data sparseness problem, substantial further pro-
gress on this task will require new, more descrip-
tive features. 
9 Conclusions 
In this paper, we showed that statistical machine 
learning techniques can be successfully applied 
to the problem of identifying fine-grained events 
in a text. We formulated this task as a statistical 
classification task using a word-chunking para-
digm, where words are labeled as beginning, in-
side or outside of an event. We introduced a va-
riety of relevant linguistically-motivated fea-
tures, and showed that models trained in this way 
could perform quite well on the task, with a pre-
cision of 82% and a recall of 71%. This method 
extended to the task of identifying the semantic 
class of an event with a precision of 67% and a 
recall of 51%. Our analysis of these models indi-
cates that while the simple event identification 
task can be approached with mostly simple text 
and word-class based features, identifying the 
semantic class of an event requires features that 
encode more of the semantic context of the 
words. Finally, our training curves suggest that 
future research in this area should focus primar-
ily on identifying more discriminative features. 
Event Identification  Event and Class Identification 
Feature set Precision Recall F  Feature set Precision Recall F 
Governing 0.820 0.706 0.759  Governing 0.667 0.512 0.579 
Negation 0.824 0.713 0.765  Temporal 0.675 0.513 0.583 
Affix 0.826 0.715 0.766  Negation 0.672 0.510 0.580 
WordNet 0.818 0.723 0.768  Morphological 0.670 0.509 0.579 
Temporal 0.820 0.729 0.772  Text 0.671 0.505 0.576 
Morphological 0.816 0.727 0.769  WordNet 0.679 0.497 0.574 
Text 0.816 0.697 0.752  Word class 0.682 0.474 0.559 
Word class 0.719 0.677 0.697  Affix 0.720 0.421 0.531 
Table 6: Ablations for both tasks. For each task, the least important feature sets appear at the top of the 
table, and most important feature sets appear at the bottom. For each row, the precision, recall and F-
measure indicate the scores of a model trained with only the feature sets named in that row and the 
rows below it. 
0.
85
0.
89
0.
93
0.
97
0 0.2 0.4 0.6 0.8 1
Fraction of Training Data
A
cc
u
ra
cy
 
o
n
 
Te
st
 
D
at
a
Event Event+Semantic Majority
 
Figure 1: Learning Curves 
153
10 Acknowledgments 
This work was partially supported by a DHS fel-
lowship to the first author and by ARDA under 
AQUAINT project MDA908-02-C-0008. Com-
puter time was provided by NSF ARI Grant 
#CDA-9601817, NSF MRI Grant #CNS-
0420873, NASA AIST grant #NAG2-1646, DOE 
SciDAC grant #DE-FG02-04ER63870, NSF 
sponsorship of the National Center for Atmos-
pheric Research, and a grant from the IBM 
Shared University Research (SUR) program. 
Any opinions, findings, or recommendations are 
those of the authors and do not necessarily reflect 
the views of the sponsors. Particular thanks go to 
Wayne Ward and Martha Palmer for many help-
ful discussions and comments. 
References 
James Allan, Jaime Carbonell, George Dodding-ton, 
Jonathan Yamron and Yiming Yang. 1998. Topic 
Detection and Tracking Pilot Study: Final Report. 
In: Proceedings of DARPA Broadcast News Tran-
scription and Understanding Workshop. 
Xavier Carreras, Llu?s M?rquez and Llu?s Padr?. 
Named Entity Extraction using AdaBoost. 2002. In 
Proceedings of CoNNL-2002. 
Elena Filatova and Vasileios Hatzivassiloglou. Do-
main-Independent Detection, Extraction, and La-
beling of Atomic Events. 2003. In the Proceedings 
of Recent Advances in Natural Language Process-
ing Conference, September 2003. 
Taku Kudo and Yuji Matsumoto. 2001. Chunking 
with support vector machines. In Proceedings of 
NAACL 2001. 
Elaine Marsh and Dennis Perzanowski. 1997. MUC-7 
evaluation of IE technology: Over-view of results. 
In Proceedings of the Seventh MUC. 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, 
James H. Martin and Daniel Jurafsky. 2004. Shal-
low Semantic Parsing using Support Vector Ma-
chines. In Proceedings of HLT/NAACL 2004. 
James Pustejovsky, Jos? Casta?o, Robert Ingria, 
Roser Saur?, Robert Gaizauskas, Andrea Setzer and 
Graham Katz. TimeML: 2003a. Robust Specifica-
tion of Event and Temporal Expressions in Text. In 
Proceedings of the Fifth International Workshop on 
Computational Semantics (IWCS-5) 
James Pustejovsky, Patrick Hanks, Roser Saur?, An-
drew See, Robert Gaizauskas, Andrea Setzer, 
Dragomir Radev, Beth Sundheim, David Day, Lisa 
Ferro and Marcia Lazo. 2003b. The TIMEBANK 
Corpus. In Proceedings of Corpus Linguistics 
2003, 647-656. 
Lance A. Ramshaw and Mitchell P. Marcus. 1995. 
Text Chunking using Transformation-Based Learn-
ing. In Proceedings of the ACL Third Workshop 
on Very Large Corpora. 82-94. 
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of EMNLP 1996. 
Roser Saur?, Robert Knippen, Marc Verhagen and 
James Pustejovsky 2005. Evita: A Robust 
Event Recognizer For QA Systems. In Proceedings 
of HLT-EMNLP 2005. 
Eric V. Siegel and Kathleen R. McKeown. Learning 
Methods to Combine Linguistic Indicators: Im-
proving Aspectual Classification and Revealing 
Linguistic Insights. Computational Linguistics, 
26(4):595 627. 
Zeno Vendler. 1967. Verbs and times. In Linguistics 
and Philosophy. Cornell University Press, Ithaca, 
New 
154
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 125?128,
Prague, June 2007. c?2007 Association for Computational Linguistics
CU-COMSEM: Exploring Rich Features for Unsupervised Web Per-
sonal Name Disambiguation  
Ying Chen 
Center for Spoken Language Research
University of Colorado at Boulder 
yc@colorado.edu  
James Martin 
Department of Computer Science 
University of Colorado at Boulder 
James.Martin@colorado.edu 
 
 
Abstract 
The increasing number of web sources is 
exacerbating the named-entity ambiguity 
problem. This paper explores the use of 
various token-based and phrase-based fea-
tures in unsupervised clustering of web 
pages containing personal names. From 
these experiments, we find that the use of 
rich features can significantly improve the 
disambiguation performance for web per-
sonal names. 
1 Introduction 
As the sheer amount of web information expands 
at an ever more rapid pace, the named-entity am-
biguity problem becomes more and more serious 
in many fields, such as information integration, 
cross-document co-reference, and question an-
swering. Individuals are so glutted with informa-
tion that searching for data presents real problems. 
It is therefore crucial to develop methodologies 
that can efficiently disambiguate the ambiguous 
names from any given set of data. 
In the paper, we present an approach that com-
bines unsupervised clustering methods with rich 
feature extractions to automatically cluster re-
turned web pages according to which named en-
tity in reality the ambiguous personal name in a 
web page refers to. We make two contributions to 
approaches to web personal name disambiguation. 
First, we seek to go beyond the kind of bag-of-
words features employed in earlier systems 
(Bagga & Baldwin, 1998; Gooi & Allan, 2004; 
Pedersen et al, 2005), and attempt to exploit deep 
semantic features beyond the work of Mann & 
Yarowsky (2003). Second, we exploit some fea-
tures that are available only in a web corpus, such 
as URL information and related web pages.   
The paper is organized as follows. Section 2 in-
troduces our rich feature extractions along with 
their corresponding similarity matrix learning. In 
Section 3, we analyze the performance of our sys-
tem. Finally, we draw some conclusions. 
2 Methodology 
Our approach follows a common architecture for 
named-entity disambiguation: the detection of 
ambiguous objects, feature extractions and their 
corresponding similarity matrix learning, and fi-
nally clustering. 
Given a webpage, we first run a modified Beau-
tiful Soup1 (a HTML parser) to extract a clean text 
document for that webpage. In a clean text docu-
ment, noisy tokens, such as HTML tags and java 
codes, are removed as much as possible, and sen-
tence segmentation is partially done by following 
the indications of some special HTML tags. For 
example, a sentence should finish when it meets a 
?<table>? tag. Then each clean document contin-
ues to be preprocessed with MXTERMINATOR 
(a sentence segmenter), 2  the Penn Treebank to-
kenization,3 a syntactic phrase chunker (Hacioglu, 
2004), and a named-entity detection and co-
reference system for the ACE project4 called EX-
                                                 
1 http://www.crummy.com/software/BeautifulSoup 
2http://www.id.cbs.dk/~dh/corpus/tools/MXTERMINATOR.
html 
3 http://www.cis.upenn.edu/~treebank/tokenization.html 
4 http://www.nist.gov/speech/tests/ace 
125
ERT5 (Hacioglu et al 2005; Chen & Hacioglu, 
2006).  
2.1 The detection of ambiguous objects  
For a given ambiguous personal name, for each 
web page, we try to extract all mentions of the 
ambiguous personal name, using three possible 
varieties of the personal name. For example, the 
three regular expression patterns for ?Alexander 
Markham? are ?Alexander Markham,? ?Markham, 
Alexander,? and ?Alexander .\. Markham? (?.\.? 
can match a middle name). Web pages without 
any mention of the ambiguous personal name of 
interest are discarded and receive no further 
processing.   
Since it is common for a single document to 
contain one or more mentions of the ambiguous 
personal name of interest, there is a need to define 
the object to be disambiguated.  Here, we adopt 
the policy of ?one person per document? (all men-
tions of the ambiguous personal name in one web 
page are assumed to refer to the same personal 
entity in reality) as in Bagga & Baldwin (1998), 
Mann & Yarowsky (2003) and Gooi & Allan 
(2004). We therefore define an object as a single 
entity with the ambiguous personal name in a 
given web page. This definition of the object 
(document-level object) might be mistaken, be-
cause the mentions of the ambiguous personal 
name in a web page may refer to multiple entities, 
but we found that this is a rare case (most of those 
cases occur in genealogy web pages). On the other 
hand, a document-level object can include much 
information derived from that web page, so that it 
can be represented by rich features.   
Given this definition of an object, we define a 
target entity as an entity (outputted from the 
EXERT system) that includes a mention of the 
ambiguous personal name. Then, we define a local 
sentence as a sentence that contains a mention of 
any target entity. 
2.2 Feature extraction and similarity matrix 
learning 
Most of the previous work (Bagga & Baldwin, 
1998; Gooi & Allan; 2004; Pedersen et al, 2005) 
uses token information in the given documents. In 
this paper, we follow and extend their work espe-
cially for a web corpus. On the other hand, com-
                                                 
5 http://sds.colorado.edu/EXERT 
pared to a token, a phrase contains more informa-
tion for named-entity disambiguation. Therefore, 
we explore some phrase-based information in this 
paper. Finally, there are two kinds of feature vec-
tors developed in our system: token-based and 
phrase-based. A token-based feature vector is 
composed of tokens, and a phrase-based feature 
vector is composed of phrases.  
 
2.2.1 Token-based features 
There is a lot of token information available in a 
web page: the tokens occurring in that web page, 
the URL for that web page, and so on. Here, for 
each web page, we tried to extract tokens accord-
ing to the following schemes. 
Local tokens (Local): the tokens occurring in the 
local sentences in a given webpage; 
Full tokens (Full): the tokens occurring in a given 
webpage; 
URL tokens (URL): the tokens occurring in the 
URL of a given webpage. URL tokenization 
works as follows: split a URL at ?:? and ?.?, and 
then filter out stop words that are very common in 
URLs, such as ?com,? ?http,? and so on;  
Title tokens in root page (TTRP): the title tokens 
occurring in the root page of a given webpage. 
Here, we define the root page of a given webpage 
as the page whose URL is the first slash-
demarcated element (non-http) of the URL of the 
given webpage. For example, the root page of 
?http://www.leeds.ac.uk/calendar/court.htm? is 
?www.leeds.ac.uk?. We do not use all tokens in 
the root page because there may be a lot of noisy 
information. 
Although Local tokens and Full tokens often 
provide enough information for name disambigua-
tion, there are some ambiguity cases that can be 
solved only with the help of information beyond 
the given web page, such as URL tokens and 
TTRP tokens. For example, in the web page 
?Alexander Markham 009,? there is not sufficient 
information to identify the ?Alexander Markham.? 
But from its URL tokens (?leeds ac uk calendar 
court?) and the title tokens in its root page (?Uni-
versity of Leeds?), it is easy to infer that this 
?Alexander Markham? is from the University of 
Leeds, which can totally solve the name ambigu-
ity.  
Because of the noisy information in URL to-
kens and TTRP tokens, here we combine them 
with Local tokens, using the following policy: for 
126
each URL token and TTRP token, if the token is 
also one of the Local tokens of other web pages, 
add this token into the Local token list of the cur-
rent webpage. We do the same thing with Full 
tokens. 
  Except URL tokens, the other three kinds of 
tokens?Local tokens, Full tokens and TTRP to-
kens?are outputted from the Penn Treebank to-
kenization, filtered by a stop-word dictionary, and 
represented in their morphological root form. But 
tokens in web pages have special characteristics 
and need more post-processing. In particular, a 
token may be an email address or a URL that may 
contain some useful information. For example, 
?charlotte@la-par.org? indicates the ?Charlotte 
Bergeron? who works for PAR (the Public Affairs 
Research Council) in LA (Los Angeles). To cap-
ture the fine-grained information in an email ad-
dress or a URL, we do deep tokenization on these 
two kinds of tokens. For a URL, we do deep to-
kenization as URL tokenization; for an email ad-
dress, we split the email address at ?@? and ?.?, 
then filter out the stop words as in URL tokeniza-
tion.   
So far, we have developed two token-based fea-
ture vectors: a Local token feature vector and a 
Full token feature vector. Both of them may con-
tain URL and TTRP tokens. Given feature vectors, 
we need to find a way to learn the similarity ma-
trix. Here, we choose the standard TF-IDF method 
to calculate the similarity matrix. 
 
2.2.2 Phrase-based features 
Since considerable information related to the am-
biguous object resides in the noun phrases in a 
web page, such as the person?s job and the per-
son?s location, we attempt to capture this noun 
phrase information. The following section briefly 
describes how to extract and use the noun phrase 
information. For more detail, see Chen & Martin 
(2007). 
Contextual base noun phrase feature: With 
the syntactic phrase chunker, we extract all base 
noun phrases (non-overlapping syntactic phrases) 
occurring in the local sentences, which usually 
include some useful information about the am-
biguous object. A base noun phrase of interest 
serves as an element in the feature vector. 
Document named-entity feature: Given the 
EXERT system, a direct and simple way to use 
the semantic information is to extract all named 
entities in a web page. Since a given entity can be 
represented by many mentions in a document, we 
choose a single representative mention to repre-
sent each entity. The representative mention is 
selected according to the following ordered pref-
erence list: longest NAME mention, longest 
NOMINAL mention.  A representative mention 
phrase serves as an element in a feature vector. 
Given a pair of feature vectors consisting of 
phrase-based features, we need to choose a simi-
larity scheme to calculate the similarity matrix. 
Because of the word-space delimiter in English, 
the feature vector comprises phrases, so that a 
similarity scheme for phrase-based feature vectors 
is required. Chen & Martin (2007) introduced one 
of those similarity schemes, ?two-level 
SoftTFIDF?. First, a token-based similarity 
scheme, the standard SoftTFIDF (Cohen et al, 
2003), is used to calculate the similarity between 
phrases in the pair of feature vectors; in the sec-
ond phase, the standard SoftTFIDF is reformu-
lated to calculate the similarity for the pair of 
phrased-based feature vectors.  
First, we introduce the standard SoftTFIDF. In 
a pair of feature vectors S and T, S = (s1, ? , sn ) 
and T = (t1, ? , tm). Here, si (i = 1?n) and tj (j = 
1?m) are substrings (tokens). Let CLOSE(?; S;T) 
be the set of substrings w ?  S such that there is 
some v?  T satisfying dist(w; v) > ?. The Jaro-
Winkler distance function (Winkler, 1999) is 
dist(;). For w? CLOSE(?; S;T), let D(w; T) = 
);(max vwdistTv? . Then the standard SoftTFIDF 
is computed as 
 
)D( )V( )V(
  )( SoftTFIDF
);;(
w, Tw, Tw, S
S,T
TSCLOSEw
??
=
? ? ?   
)(IDF log  1)  (TF log  )(V' ww,Sw, S ?+=         
? ?= S w, S
w, Sw, S
w
2)( V
)(  V  )( V                  ,   
where TFw,S is the frequency of substrings w in S, 
and IDFw is the inverse of the fraction of docu-
ments in the corpus that contain w. To compute 
the similarity for the phrase-based feature vectors, 
in the second step of ?two-level SoftTFIDF,? the 
substring w is a phrase and dist is the standard 
SoftTFIDF.  
So far, we have developed several feature mod-
els and learned the corresponding similarity ma-
127
trices, but clustering usually needs only one 
unique similarity matrix. In the results reported 
here, we simply combine the similarity matrices, 
assigning equal weight to each one. 
2.3 Clustering 
Although clustering is a well-studied area, a re-
maining research problem is to determine the op-
timal parameter settings during clustering, such as 
the number of clusters or the stop-threshold, a 
problem that is important for real tasks and that is 
not at all trivial. Because currently we focus only 
on feature development, we choose agglomerative 
clustering with a single linkage, and simply use a 
fixed stop-threshold acquired from the training 
data.  
3 Performance  
Our system performs very well for the Semeval 
Web People corpus, and Table 1 shows the 
performances. There are two results in Table 1: 
One is gotten from the evaluation of Semeval 
Web People Track (SemEval), and the other is 
evaluated with B-cubed evaluation (Bagga and 
Baldwin, 1998). Both scores indicate that web 
personal name disambiguation needs more effort. 
 
 Purity Inverse 
Purity 
F  
(?=0.5)
F  
(?=0.2)
SemEval 0.72 0.88 0.78 0.83 
 Precision Recall F  
(?=0.5)
F  
(?=0.2)
B-cubed 0.61 0.83  0.70 0.77 
Table 1  The performances of the test data 
4 Conclusion 
Our experiments in web personal name disam-
biguation extend token-based information to a 
web corpus, and also include some noun phrase-
based information. From our experiment, we first 
find that it is not easy to extract a clean text 
document from a webpage because of much noisy 
information in it. Second, some common tools 
need to be adapted to a web corpus, such as sen-
tence segmentation and tokenization. Many NLP 
tools are developed for a news corpus, whereas a 
web corpus is noisier and often needs some spe-
cific processing. Third, in this paper, we use some 
URL information and noun phrase information in 
a rather simple way; more exploration is needed in 
the future. Besides the rich feature extraction, we 
also need more work on similarity combination 
and clustering. 
 
Acknowledgements 
Special thanks are extended to Praful Mangalath 
and Kirill Kireyev. 
References 
J. Artiles, J. Gonzalo. and S. Sekine. 2007. The SemE-
val-2007 WePS Evaluation: Establishing a bench-
mark for the Web People Search Task. In Proceed-
ings of Semeval 2007, Association for Computa-
tional Linguistics. 
A. Bagga and B. Baldwin. 1998. Entity?based Cross?
document Co?referencing Using the Vector Space 
Model. In 17th COLING. 
Y. Chen and K. Hacioglu. 2006. Exploration of 
Coreference Resolution: The ACE Entity Detection 
and Recognition Task. In 9th International Confer-
ence on TEXT, SPEECH and DIALOGUE. 
Y. Chen and J. Martin. 2007. Towards Robust Unsu-
pervised Personal Name Disambiguation. EMNLP. 
W. Cohen, P. Ravikumar, S. Fienberg. 2003. A Com-
parison of String Metrics for Name-Matching Tasks. 
In IJCAI-03 II-Web Workshop.  
C. H. Gooi and J. Allan. 2004. Cross-Document 
Coreference on a Large Scale Corpus. NAACL  
K. Hacioglu, B. Douglas and Y. Chen.  2005. Detection 
of Entity Mentions Occurring in English and Chi-
nese Text. Computational Linguistics. 
K. Hacioglu. 2004. A Lightweight Semantic Chunking 
Model Based On Tagging. In HLT/NAACL. 
B. Malin. 2005. Unsupervised Name Disambiguation 
via Social Network Similarity. SIAM. 
G. Mann and D. Yarowsky. 2003. Unsupervised Per-
sonal Name Disambiguation. In Proc. of CoNLL-
2003, Edmonton, Canada. 
T. Pedersen, A. Purandare and A. Kulkarni. 2005. 
Name Discrimination by Clustering Similar Con-
texts. In Proc. of the Sixth International Conference 
on Intelligent Text Processing and Computational 
Linguistics, pages 226-237. Mexico City, Mexico. 
W. E. Winkler. 1999. The state of record linkage and 
current research  problems. Statistics of Income Di-
vision, Internal Revenue Service Publication R99/04. 
128
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 129?132,
Prague, June 2007. c?2007 Association for Computational Linguistics
CU-TMP:
Temporal Relation Classification Using Syntactic and Semantic Features
Steven Bethard and James H. Martin
Department of Computer Science
University of Colorado at Boulder
430 UCB, Boulder, CO 80309, USA
{bethard,martin}@colorado.edu
Abstract
We approached the temporal relation identi-
fication tasks of TempEval 2007 as pair-wise
classification tasks. We introduced a va-
riety of syntactically and semantically mo-
tivated features, including temporal-logic-
based features derived from running our
Task B system on the Task A and C data.
We trained support vector machine models
and achieved the second highest accuracies
on the tasks: 61% on Task A, 75% on Task B
and 54% on Task C.
1 Introduction
In recent years, the temporal structure of text has be-
come a popular area of natural language processing
research. Consider a sentence like:
(1) The top commander of a Cambodian resistance
force said Thursday he has sent a team to
recover the remains of a British mine removal
expert kidnapped and presumed killed by
Khmer Rouge guerrillas almost two years ago.
English speakers immediately recognize that kid-
napping came first, then sending, and finally saying,
even though before and after never appeared in the
text. How can machines learn to do the same?
The 2007 TempEval competition tries to address
this question by establishing a common corpus on
which research systems can compete to find tempo-
ral relations (Verhagen et al, 2007). TempEval con-
siders the following types of event-time temporal re-
lations:
Task A Events1and times within the same sentence
Task B Events1 and document times
Task C Matrix verb events in adjacent sentences
In each of these tasks, systems attempt to annotate
pairs with one of the following relations: BEFORE,
BEFORE-OR-OVERLAP, OVERLAP, OVERLAP-OF-
AFTER, AFTER or VAGUE. Competing systems are
instructed to find all temporal relations of these
types in a corpus of newswire documents.
We approach these tasks as pair-wise classifi-
cation problems, where each event/time pair is
assigned one of the TempEval relation classes
(BEFORE, AFTER, etc.). Event/time pairs are en-
coded using syntactically and semantically moti-
vated features, and then used to train support vector
machine (SVM) classifiers.
The remainder of this paper is structured as fol-
lows. Section 2 describes the features used to char-
acterize event/time relations. Section 3 explains how
we used these features to train SVM models for each
task. Section 4 discusses the performance of our
models on the TempEval data, and Section 5 sum-
marizes the lessons learned and future directions.
2 Features
We used a variety of lexical, syntactic and semantic
features to characterize the different types of tempo-
ral relations. In each task, the events and times were
characterized using the features:
word The text of the event or time words
1TempEval only considers events that occurred at least 20
times in the TimeBank (Pustejovsky et al, 2003) corpus for
these tasks
129
Shhhhhhhhhhhhh
QQQ
(((((((((((((
PPhhhhh(((((
IN
For
NPhhhh((((
NPhhh(((
[TIMEthe quarter]
VPXXX
VBN
ended
NPXXX
[TIMESept 30]
NP
Delta
VPhhhhh(((((
VBD
[EVENTposted]
NPhhhhh(((((
net income of $133 million
Figure 1: A syntactic tree. The path between posted and the quarter is VBD-VP-S-PP-NP-NP
pos The parts of speech2of the words, e.g. this cru-
cial moment has the parts of speech DT-JJ-NN.
gov-prep Any prepositions governing the event or
time, e.g. in during the Iran-Iraq war, the
preposition during governs the event war, and
in after ten years, the preposition after governs
the time ten years.
gov-verb The verb that governs the event or time,
e.g. in rejected in peace talks, the verb rejected
governs the event talks, and in withdrawing on
Friday, the verb withdrawing governs the time
Friday. For events that are verbs, this feature is
just the event itself.
gov-verb-pos The part of speech2 of the governing
verb, e.g. withdrawing has the part of speech
VBG.
aux Any auxiliary verbs and adverbs modifying the
governing verb, e.g. in could not come, the
words could and not are considered auxiliaries
for the event come, and in will begin withdraw-
ing on Friday, the words will and begin are con-
sidered auxiliaries for the time Friday.
Events were further characterized using the features
(the last six use gold-standard TempEval markup):
modal Whether or not the event has one of the aux-
iliaries, can, will, shall, may, or any of their
variants (could, would, etc.).
gold-stem The stem, e.g. the stem of fallen is fall.
gold-pos The part-of-speech, e.g. NOUN or VERB.
gold-class The semantic class, e.g. REPORTING.
gold-tense The tense, e.g. PAST or PRESENT.
gold-aspect The aspect, e.g. PERFECTIVE.
gold-polarity The polarity, e.g. POS or NEG.
Times were further characterized using the follow-
ing gold-standard TempEval features:
2From MXPOST (ftp.cis.upenn.edu/pub/adwait/jmx/)
gold-type The type, e.g. DATE or TIME.
gold-value The value, e.g. PAST REF or 1990-09.
gold-func The temporal function, e.g. TRUE.
These gold-standard event and time features are sim-
ilar to those used by Mani and colleagues (2006).
The features above don?t capture much of the dif-
ferences between the tasks, so we introduced some
task-specific features. Task A included the features:
inter-time The count of time expressions between
the event and time, e.g. in Figure 1, there is
one time expression, Sept 30, between the event
posted and the time the quarter.
inter-path The syntactic path between the event
and the time, e.g. in Figure 1 the
path between posted and the quarter is
VBD>VP>S<PP<NP<NP.
inter-path-parts The path, broken into three parts:
the tags from the event to the lowest common
ancestor (LCA), the LCA, and the tags from the
LCA to the time, e.g. in Figure 1 the parts are
VBD>VP, S and PP<NP<NP.
inter-clause The number of clause nodes along the
syntactic path, e.g. in Figure 1 there is one
clause node along the path, the top S node.
Our syntactic features were derived from a syntactic
tree, though Boguraev and Ando (2005) suggest that
some could be derived from finite state grammars.
For Task C we included the following feature:
tense-rules The relation predicted by a set of tense
rules, where past tense events come BEFORE
present tense events, present tense events come
BEFORE future tense events, etc. In the text:
(2) Finally today, we [EVENT learned] that
the space agency has taken a giant leap
forward. Collins will be [EVENT named]
commander of Space Shuttle Columbia.
130
Since learned is in past tense and named is in
future, the relation is (learned BEFORE named).
In preliminary experiments, the Task B system had
the best performance, so we ran this system on the
data for Tasks A and C, and used the output to add
the following feature for both tasks:
task-b-rel The relation predicted by combining the
output of the Task B system with temporal
logic. For example, consider the text:
(3) [TIME 08-15-90 (=1990-08-15)]
Iraq?s Saddam Hussein
[TIME today (=1990-08-15)] sought
peace on another front by promising to
release soldiers captured during the
Iran-Iraq [EVENT war].
If Task B said (war BEFORE 08?15?90)
then since 08?15?90=1990?08?15=today,
the relation (war BEFORE today) must hold.
3 Models
Using the features described in the previous section,
each temporal relation ? an event paired with a time
or another event ? was translated into a set of fea-
ture values. Pairing those feature values with the
TempEval labels (BEFORE, AFTER, etc.) we trained
a statistical classifier for each task. We chose sup-
port vector machines3(SVMs) for our classifiers as
they have shown good performance on a variety of
natural language processing tasks (Kudo and Mat-
sumoto, 2001; Pradhan et al, 2005).
Using cross-validations on the training data, we
performed a simple feature selection where any fea-
ture whose removal improved the cross-validation
F-score was discarded. The resulting features for
each task are listed in Table 1. After feature selec-
tion, we set the SVM free parameters, e.g. the ker-
nel degree and cost of misclassification, by perform-
ing additional cross-validations on the training data,
and selecting the model parameters which yielded
the highest F-score for each task4.
3We used the TinySVM implementation from
http://chasen.org/%7Etaku/software/TinySVM/ and trained
one-vs-rest classifiers.
4We only experimented with polynomial kernels.
Feature Task A Task B Task C
event-word
event-pos X X
event-gov-prep X X
event-gov-verb X X
event-gov-verb-pos X X 2
event-aux X X X
modal X X
gold-stem X X 1
gold-pos X X
gold-class X X X
gold-tense X X X
gold-aspect X X
gold-polarity X X
time-word X
time-pos X
time-gov-prep X
time-gov-verb X
time-gov-verb-pos X
time-aux X
gold-type
gold-value X X
gold-func X
inter-time X
inter-path X
inter-path-parts X
inter-clause X
tense-rules X
task-b-rel X X
Table 1: Features used in each task. An X indicates
that the feature was used for that task. For Task C, 1
indicates that the feature was used only for the first
event and not the second, and 2 indicates the reverse.
Strict Relaxed
Task P R F P R F
A 0.61 0.61 0.61 0.63 0.63 0.63
B 0.75 0.75 0.75 0.76 0.76 0.76
C 0.54 0.54 0.54 0.60 0.60 0.60
Table 2: (P)recision, (R)ecall and (F)-measure of
the models on each task. Precision, recall and F-
measure are all equivalent to classification accuracy.
4 Results
We evaluated our classifers on the TempEval test
data. Because the Task A and C models derived fea-
tures from the Task B temporal relations, we first ran
the Task B classifer over all the data, and then ran the
Task A and Task C classifiers over their individual
data. The resulting temporal relation classifications
were evalutated using the standard TempEval scor-
ing script. Table 2 summarizes these results.
Our models achieved an accuracy of 61% on
Task A, 75% on Task B and 54% on Task C, the
second highest scores on all these tasks. The Temp-
131
Task Feature Removed Model Accuracy
A
- 0.663
time-gov-prep 0.650
gold-value 0.652
polarity 0.655
task-b-rel 0.656
B
- 0.809
event-aux 0.780
gold-stem 0.784
gold-class 0.794
C
- 0.534
event-gov-verb-2 0.522
event-aux-2 0.525
gold-class-1 0.526
gold-class-2 0.527
event-pos-2, task-b-rel 0.529
Table 3: Feature analysis. The ?-? lines show the
accuracy of the model with all features.
Eval scoring script also reported a relaxed measure
where for example, systems could get partial credit
for matching a gold standard label like OVERLAP-
OR-AFTER with OVERLAP or AFTER. Under this
measure, our models achieved an accuracy of 63%
on Task A, 76% on Task B and 60% on Task C, again
the second highest scores in the competition.
We performed a basic feature analysis where, for
each feature in a task, a model was trained with that
feature removed and all other features retained. We
evaluated the performance of the resulting models
using cross-validations on the training data5. Fea-
tures whose removal resulted in the largest drops in
model performance are listed in Table 3.
For Task A, the most important features were the
preposition governing the time and the time?s nor-
malized value. For Task B, the most important fea-
tures were the auxiliaries governing the event, and
the event?s stem. For Task C, the most important
features were the verb and auxiliaries governing the
second event. For both Tasks A and C, the features
based on the Task B relations were one of the top
six features. In general however, no single feature
dominated any one task ? the greatest drop in per-
formance from removing a feature was only 2.9%.
5 Conclusions
TempEval 2007 introduced a common dataset for
work on identifying temporal relations. We framed
5We used cross-validations on the training data to preserve
the validity of the TempEval test data for future research
the TempEval tasks as pair-wise classification prob-
lems where pairs of events and times were assigned
a temporal relation class. We introduced a variety of
syntactic and semantic features, including paths be-
tween constituents in a syntactic tree, and temporal
relations deduced by running our Task B system on
the Task A and C data. Our models achieved an ac-
curacy of 61% on Task A, 75% on Task B and 54%
on Task C. Analysis of these models indicated that
no single feature dominated any given task, and sug-
gested that future work should focus on new features
to better characterize temporal relations.
6 Acknowledgments
This research was performed under an appointment
of the first author to the DHS Scholarship and
Fellowship Program, administered by the ORISE
through an interagency agreement between DOE
and DHS. ORISE is managed by ORAU under DOE
contract number DE-AC05-06OR23100. All opin-
ions expressed in this paper are the author?s and
do not necessarily reflect the policies and views of
DHS, DOE, or ORAU/ORISE.
References
B. Boguraev and R. K. Ando. 2005. Timebank-driven
timeml analysis. In Graham Katz, James Pustejovsky,
and Frank Schilder, editors, Annotating, Extracting
and Reasoning about Time and Events, Dagstuhl Sem-
inars. German Research Foundation.
T. Kudo and Y. Matsumoto. 2001. Chunking with sup-
port vector machines. In NAACL.
I. Mani, M. Verhagen, B. Wellner, C. M. Lee, and
J. Pustejovsky. 2006. Machine learning of temporal
relations. In COLING/ACL.
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Mar-
tin, and D. Jurafsky. 2005. Support vector learning for
semantic argument classification. Machine Learning,
60(1):11?39.
J. Pustejovsky, P. Hanks, R. Saur, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,
and M. Lazo. 2003. The timebank corpus. In Corpus
Linguistics, pages 647?656.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple, and
J. Pustejovsky. 2007. Semeval-2007 task 15: Temp-
eval temporal relation identification. In SemEval-
2007: 4th International Workshop on Semantic Evalu-
ations.
132
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 10?18,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Classification Errors in a Domain-Independent Assessment System   Rodney D. Nielsen1,2, Wayne Ward1,2 and James H. Martin1 1 Center for Computational Language and Education Research, University of Colorado, Boulder 2 Boulder Language Technologies, 2960 Center Green Ct., Boulder, CO 80301 Rodney.Nielsen, Wayne.Ward, James.Martin@Colorado.edu       Abstract We present a domain-independent technique for assessing learners? constructed responses.  The system exceeds the accuracy of the ma-jority class baseline by 15.4% and a lexical baseline by 5.9%.  The emphasis of this paper is to provide an error analysis of performance, describing the types of errors committed, their frequency, and some issues in their resolution.   1 Introduction Assessment within state of the art Intelligent Tu-toring Systems (ITSs) generally provides little more than an indication that the student?s response expressed the target knowledge or it did not. There is no indication of exactly what facets of the con-cept a student contradicted or failed to express. Furthermore, virtually all ITSs are developed in a very domain-specific way, with each new question requiring the handcrafting of new semantic extrac-tion frames, parsers, logic representations, or knowledge-based ontologies (c.f., Graesser et al, 2001; Jordan et al, 2004; Peters et al, 2004; Roll et al, 2005; VanLehn et al, 2005). This is also true of research in the area of scoring constructed re-sponse questions (e.g., Callear et al, 2001; Lea-cock, 2004; Mitchell et al, 2002; Pulman and Sukkarieh, 2005). The present paper analyzes the errors of a system that was designed to address these limitations.   Rather than have a single expressed versus not-expressed assessment of the reference answer as a whole, we instead break the reference answer down into what we consider to be approximately 
its lowest level compositional facets. This roughly translates to the set of triples composed of labeled (typed) dependencies in a dependency parse of the reference answer. Breaking the reference answer down into fine-grained facets permits a more fo-cused assessment of the student?s response, but a simple yes or no entailment at the facet level still lacks semantic expressiveness with regard to the relation between the student?s answer and the facet in question, (e.g., did the student contradict the facet or just fail to address it?).  Therefore, it is also necessary to break the annotation labels into finer levels in order to specify more clearly the relationship between the student?s answer and the reference answer facet.   In this paper, we present an error analysis of our system, detailing the most frequent types of errors encountered in our implementation of a domain-independent ITS assessment component and dis-cuss plans for correcting or mitigating some of the errors.  The system expects constructed responses of a phrase to a few sentences, but does not rely on technology developed specifically for the domain or subject matter being tutored ? without changes, it should handle history as easily as science.  We first briefly describe the corpus used, the knowl-edge representation, and the annotation.  In section 3, we describe our assessment system.  Then we present the error analysis and discussion. 2 Assessing Student Answers 2.1 Corpus We acquired grade 3-6 responses to 287 questions from the Assessing Science Knowledge (ASK) project (Lawrence Hall of Science, 2006). The re-sponses, which range in length from moderately 
10
short verb phrases to several sentences, cover all 16 diverse teaching and learning modules, span-ning life science, physical science, earth and space science, scientific reasoning, and technology. We generated a corpus by transcribing a random sample (approx. 15400) of the students? handwritten responses. 2.2 Knowledge Representation The ASK assessments included a reference an-swer for each of their constructed response ques-tions.  We decomposed these reference answers into low-level facets, roughly extracted from the relations in a syntactic dependency parse and a shallow semantic parse. However, we use the word facet to refer to any fine-grained component of the reference answer semantics. The decomposition is based closely on these well-established frame-works, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al, 2006). These fac-ets are the basis for assessing learner answers. See (Nielsen et al, 2008b) for details on extracting the facets; here we simply sketch the makeup of the final assessed reference answer facets.   Example 1 presents a reference answer from the Magnetism and Electricity module and illustrates the facets derived from its dependency parse (shown in Figure 1), along with their glosses.  These facets represent the fine-grained knowledge the student is expected to address in their response.  (1) The brass ring would not stick to the nail be-cause the ring is not iron. (1a)  NMod(ring, brass)  (1a?) The ring is brass. (1b)  Theme_not(stick, ring) (1b?) The ring does not stick. (1c)  Destination_to_not(stick, nail) (1c?) Something does not stick to the nail. (1d)  Be_not(ring, iron) (1d?) The ring is not iron. (1e)  Cause_because(1b-c, 1d) (1e?) 1b and 1c are caused by 1d.  Figure 1. Reference answer representation revisions 
Typical facets, as in (1a), are derived directly from a dependency parse, in this case retaining its dependency type label, NMod (noun modifier).  Other facets, such as (1b-e), are the result of com-bining multiple dependencies, VMod(stick, to) and PMod(to, nail) in the case of (1c). When the head of the dependency is a verb, as in (1b,c), we use Thematic Roles from VerbNet (Kipper et al, 2000) and adjuncts from PropBank (Palmer et al, 2005) to label the facet relation.  Some copulas and simi-lar verbs were themselves used as facet relations, as in (1d).  Dependencies involving determiners and many modals, such as would, in ex. 1, are dis-carded and negations, such as not, are incorporated into the associated facets. We refer to facets that express relations between higher-level propositions as inter-propositional facets.  An example of such a facet is (1e) above, connecting the proposition the brass ring did not stick to the nail to the proposition the ring is not iron.  In addition to specifying the headwords of inter-propositional facets (stick and is, in 1e), we also note up to two key facets from each of the propositions that the relation is connecting (b, c, and d in ex. 1).  Reference answer facets that are assumed to be understood by the learner a priori, (generally because they are part of the information given in the question), are also annotated to indi-cate this. There were a total of 2878 reference answer fac-ets, with a mean of 10 facets per reference answer (median of 8).  Facets that were assumed to be un-derstood a priori by students accounted for 33% of all facets and inter-propositional facets accounted for 11%.  The experiments in automated annotation of student answers (section 3) focus on the facets that are not assumed to be understood a priori (67% of all facets); of these, 12% are inter-propositional. 2.3 Annotating Student Understanding After defining the reference answer facets, we annotated each student answer to indicate whether and how they addressed each reference answer facet. We settled on the annotation labels in Table 1. For a given student answer, one label is assigned for each facet in the associated reference answer.  These labels and the annotation process are de-tailed in (Nielsen et al, 2008a).  
11
Assumed: Reference answer facets that are assumed to be understood a priori based on the question Expressed: Any reference answer facet directly ex-pressed or inferred by simple reasoning Inferred: Reference answer facets whose understanding is inferred by pragmatics or nontrivial logical reasoning Contra-Expr: Reference answer facets directly contra-dicted by negation, antonymous expressions, and their paraphrases Contra-Infr: Reference answer facets contradicted by pragmatics or complex reasoning Self-Contra: Reference answer facets that are both con-tradicted and implied (self contradictions) Diff-Arg: Reference answer facets whose core relation is expressed, but it has a different modifier or argument Unaddressed: Reference answer facets that are not ad-dressed at all by the student?s answer Table 1. Facet Annotation Labels Example 2 shows a fragment of a question and associated reference answer broken down into its constituent facets with an indication of whether the facet is assumed to be understood a priori.  A cor-responding student answer is shown in (3) along with its final annotation in 2a?-c?.  It is assumed that the student understands that the pitch is higher (facet 2b), since this is given in the question and similarly it is assumed that the student will be ex-plaining what has the causal effect of producing this higher pitch (facet 2c).  Therefore, unless the student explicitly addresses these facets they are labeled Assumed.  The student phrase the string is long is aligned with reference answer facet 2a, since they are both expressing a property of the string, but since the phrase neither contradicts nor indicates an understanding of the facet, the facet is labeled Diff-Arg, 2a?.  The causal facet 2c? is la-beled Expressed, since the student expresses a causal relation and the cause and effect are each properly aligned.  In this way, the automated tutor will know the student is on track in attempting to address the cause and it can focus on remediating the student?s understanding of that cause. (2) Question: ... Write a note to David to tell him why the pitch gets higher rather than lower. Reference Answer: The string is tighter, so the pitch is higher... (2a) Be(string, tighter), --- (2b) Be(pitch, higher), Assumed (2c) Cause(2b, 2a), Assumed 
(3) David this is why because you don't listen to your teacher. If the string is long, the pitch will be high. (2a?) Be(string, tighter), Diff-Arg (2b?) Be(pitch, higher), Expressed (2c?) Cause(2b?, 2a?), Expressed A tutor will treat the labels Expressed, Inferred and Assumed all as Understood by the student and similarly Contra-Expr and Contra-Infr are com-bined as Contradicted.  These labels are kept sepa-rate in the annotation to facilitate training different systems to detect these different inference relation-ships, as well as to allow evaluation at that level.  The consolidated set of labels, comprised of Un-derstood, Contradicted, Self-Contra, Diff-Arg and Unaddressed, are referred to as the Tutor Labels. 3 Automated Classification A high level description of the assessment proce-dure is as follows. We start with the hand gener-ated reference answer facets. We generate automatic parses for the reference answers and the student answers and automatically modify these parses to match our desired representation. Then for each reference answer facet, we extract features indicative of the student?s understanding of that facet. Finally, we train a machine learning classi-fier on our training data and use it to classify un-seen test examples, assigning a Tutor Label (described in the preceding paragraph), for each reference answer facet.  3.1 Preprocessing and Representation Many of the features utilized by the machine learn-ing algorithm here are based on document co-occurrence counts.  We use three publicly available corpora (English Gigaword, The Reuters corpus, and Tipster) totaling 7.4M articles and 2.6B terms.  These corpora are all drawn from the news do-main, making them less than ideal sources for as-sessing student?s answers to science questions. We utilized these corpora to generate term relatedness statistics primarily because they comprised a read-ily available large body of text.  They were in-dexed and searched using Lucene, a publicly available Information Retrieval tool.   Before extracting features, we automatically generate dependency parses of the reference an-swers and student answers using MaltParser (Nivre 
12
et al, 2006).  These parses are then automatically modified in a way similar to the manual revisions made when extracting the reference answer facets, as sketched in section 2.2.  We reattach auxiliary verbs and their modifiers to the associated regular verbs.  We incorporate prepositions and copulas into the dependency relation labels, and similarly append negation terms onto the associated depend-ency relations.  These modifications, all made automatically, increase the likelihood that terms carrying significant semantic content are joined by dependencies that are utilized in feature extraction.  In the present work, we did not make use of a the-matic role labeler.   3.2 Machine Learning Features & Approach We investigated a variety of linguistic features and chose to utilize the features summarized in Table 2, informed by training set cross validation results.  The features assess the facets? lexical similarity via lexical entailment probabilities following (Glick-man et al, 2005), part of speech (POS) tags, and lexical stem matches.  They include syntactic in-formation extracted from the modified dependency parses such as relevant relation types and path edit distances.  Remaining features include information about polarity among other things.  The revised dependency parses described earlier are used in aligning the terms and facet-level information for feature extraction, as indicated in the feature de-scriptions.  The data was split into a training set and three test sets.  The first test set, Unseen Modules, con-sists of all the data from three of the 16 science modules, providing a domain-independent test set.  The second, Unseen Questions, consists of all the student answers associated with 22 randomly se-lected questions from the 233 questions in the re-maining 13 modules, providing a question-independent test set.  The third test set, Unseen Answers, was created by randomly assigning all of the facets from approximately 6% of the remaining learner answers to a test set with the remainder comprising the training set. In the present work, we utilize only the facets that were not assumed to be understood a priori. This selection resulted in a total of 54,967 training examples, 30,514 examples in the Unseen Modules test set, 6,699 in the Un-seen Questions test set and 3,159 examples in the Unseen Answers test set. 
Lexical Features Gov/Mod_MLE: The lexical entailment probabilities (LEPs) for the reference answer facet governor (Gov; e.g., string in 2a) and modifier (Mod; e.g., tighter in 2a) following (Glickman et al, 2005; c.f., Turney, 2001). The LEP of a reference answer word w is defined as: (1) ,  where v is a word in the student answer, nv is the # of docs (see section 3.1) containing v, and nw,v is the # of docs where w & v cooccur. {Ex. 2a: the LEPs for string?string and tension? tighter, respectively}? Gov/Mod_Match: True if the Gov (Mod) stem has an exact match in learner answer. {Ex. 2a: True for Gov: string, and (False for Mod: no stem match for tighter)}? Subordinate_MLEs: The lexical entailment probabili-ties for the primary constituent facets? Govs and Mods when the facet represents a relation between higher-level propositions (see inter-propositional facet defini-tion in section 2.2). {Ex. 2c: the LEPs for pitch?pitch, up?higher, string?string, and tension?tighter}? Syntactic Features Gov/Mod_POS: POS tags for the facet?s Gov and (Mod). {Ex. 2a: NN for string and (JJR for tighter)}? Facet/AlignedDep_Reltn: The labels of the facet and aligned learner answer dependency ? alignments were based on co-occurrence MLEs as with words, (i.e., they estimate the likelihood of seeing the reference answer dependency in a document given it contains the learner answer dependency ? replace words with dependencies in equation 1 above). {Ex. 2a: Be is the facet label and Have is the aligned student answer dependency}? Dep_Path_Edit_Dist: The edit distance between the dependency path connecting the facet?s Gov and Mod (not necessarily a single step due to parser errors) and the path connecting the aligned terms in the learner an-swer. Paths include the dependency relations generated in our modified parse with their attached prepositions, negations, etc, the direction of each dependency, and the POS tags of the terms on the path. The calculation ap-plies heuristics to judge the similarity of each part of the path (e.g., dropping a subject had a much higher cost than dropping an adjective).  Alignment for this feature was made based on which set of terms in an N-best list (N=5 in the present experiments) for the Gov and Mod resulted in the smallest edit distance.  The N-best list was generated based on the lexical entailment values (see Gov/Mod_MLE). {Ex. 2b: Distance(up:VMod> went:V<pitch:Subject, pitch:Be>higher)}? Other Features Consistent_Negation: True if the facet and aligned student dependency path had the same number of nega-tions. {Ex. 2a: True: neither one have a negation}? RA_CW_cnt: The number of content words (non-function words) in the reference answer. {Ex. 2: 5 = count(string, tighter, so, pitch & higher)}? ? Examples within {} braces are based on reference answer Ex. 2 and the learner answer:  The pitch went up because the string has more tension Table 2. Machine Learning Features 
13
We evaluated several machine learning algo-rithms (rules, trees, boosting, ensembles and an svm) and C4.5 (Quinlan, 1993) achieved the best results in cross validation on the training data.  Therefore, we used it to obtain all of the results presented here.  A number of classifiers performed comparably and Random Forests outperformed C4.5 with a previous feature set and subset of data.  A thorough analysis of the impact of the classifier chosen has not been completed at this time. 3.3 System Results Given a student answer, we generate a separate Tutor Label (described at the end of section 2.3) for each associated reference answer facet to indi-cate the level of understanding expressed in the student?s answer (similar to giving multiple marks on a test).  Table 3 shows the classifier?s Tutor La-bel accuracy over all reference answer facets in cross validation on the training set as well as on each of our test sets.  The columns first show two simpler baselines, the accuracy of a classifier that always chooses the most frequent class in the train-ing set ? Unaddressed, and the accuracy based on a lexical decision that chooses Understood if both the governing term and the modifier are present in the learner?s answer and outputs Unaddressed oth-erwise, (we also tried placing a threshold on the product of the governor and modifier lexical en-tailment probabilities following Glickman et al (2005), who achieved the best results in the first RTE challenge, but this gave virtually the same results as the word matching baseline).  The col-umn labeled Table 2 Features presents the results of our classifier. (Reduced Training is described in the Discussion section, which follows.)  Majority Label Lexical Baseline Table 2 Features Reduced Training Training Set CV 54.6 59.7 77.1  Unseen Answers 51.1 56.1 75.5  Unseen Questions 58.4 63.4 61.7 66.5 Unseen Modules 53.4 62.9 61.4 68.8 Table 3. Classifier Accuracy 4 Discussion and Error Analysis 4.1 Results Discussion The accuracy achieved, assessing learner answers within this new representation framework, repre-
sent an improvement of 24.4%, 3.3%, and 8.0% over the majority class baseline for Unseen An-swers, Questions, and Modules respectively.  Ac-curacy on Unseen Answers is also 19.4% better than the lexical baseline. However, this simple baseline outperformed the classifier on the other two test sets.  It seemed probable that the decision tree over fit the data due to bias in the data itself; specifically, since many of the students? answers are very similar, there are likely to be large clusters of identical feature-class pairings, which could re-sult in classifier decisions that do not generalize as well to other questions or domains.  This bias is not problematic when the test data is very similar to the training data, as is the case for our Unseen Answers test set, but would negatively affect per-formance on less similar data, such as our Unseen Questions and Modules.   To test this hypothesis, we reduced the size of our training set to about 8,000 randomly selected examples, which would result in fewer of these dense clusters, and retrained the classifier.  The result for Unseen Questions, shown in the Reduced Training column, was an improvement of 4.8%.  Given this promising improvement, we attempted to find the optimal training set size through cross-validation on the training data.  Specifically, we iterated over the science modules holding one module out, training on the other 12 and testing on the held out module. We analyzed the learning curve varying the number of randomly selected examples per facet.  We found the optimal accu-racy for training set cross-validation by averaging the results over all the modules and then trained a classifier on that number of random examples per facet in the training set and tested on the Unseen Modules test set.  The result was an increase in accuracy of 7.4% over training on the full training set.  In future work, we will investigate other more principled techniques to avoid this type of over-fitting, which we believe is somewhat atypical. 4.2 Error Analysis In order to focus future work on the areas most likely to benefit the system, an error analysis was performed based on the results of 13-fold cross-validation on the training data (one fold per science module). In other words, 13 C4.5 decision tree classifiers were built, one for each science module in the training set; each classifier was trained, 
14
utilizing the feature set shown in Table 2, on all of the data from 12 science modules and then tested on the data in the remaining, held-out module. This effectively simulates the Unseen Modules test condition. To our knowledge, no prior work has analyzed the assessment errors of such a domain-independent ITS. Several randomly selected examples were analyzed to look for patterns in the types of errors the system makes.  However, only specific categories of data were considered.  Specifically, only the subsets of errors that were most likely to lead to short-term system improvements were considered.  This included only examples where all of the annotators agreed on the annotation, since if the annotation was difficult for humans, it would probably be harder to construct features that would allow the machine learning algorithm to correct its error.  Second, only Expressed and Unaddressed facets were considered, since Inferred facets represent the more challenging judgments, typically based on pragmatic inferences.  Contradictions were excluded since there was almost no attempt to handle these in the present system.  Third, only facets that were not inter-propositional were considered, since the inter-propositional facets are more complicated to process and only represent 12% of the non-Assumed data. We discuss Expressed facets in the next section of the paper and Unaddressed in the following section. 4.3 Errors in Expressed Facets Without examining each example relative to the decision tree that classified it, it is not possible to know exactly what caused the errors.  The analysis here simply indicates what factors are involved in inferring whether the reference answer facets were understood and what relationships exist between the student answer and the reference answer facet.  We analyzed 100 random examples of errors where annotators considered the facet Expressed and the system labeled it Unaddressed, but the analysis only considered one example for any given reference answer facet.  Out of these 100 examples, only one looked as if it was probably incorrectly annotated.  We group the potential error factors seen in the data, listed in order of frequency, according to issues associated with paraphrases, logical inference, pragmatics, and 
preprocessing errors.  In the following paragraphs, these groups are broken down for a more fine-grained analysis.  In over half of the errors considered, there were two or more of these fine-grained factors involved. Paraphrase issues, taken broadly, are subdivided into three main categories: coreference resolution, lexical substitution, syntactic alternation and phrase-based paraphrases. Our results in this area are in line with (Bar-Haim et al, 2005), who considered which inference factors are involved in proving textual entailment. Three coreference resolution factors combined are involved in nearly 30% of the errors.  Students use on average 1.1 pronouns per answer and, more importantly, the pronouns tend to refer to key entities or concepts in the question and reference answer.  A pronoun was used in 15 of the errors (3 personal pronouns ? she, 11 uses of it, and 1 use of one).  It might be possible to correct many of these errors by simply aligning the pronouns to essentially all possible nouns in the reference answer and then choosing the single alignment that gives the learner the most credit. In 6 errors, the student referred to a concept by another term (e.g., substituting stuff for pieces). In another 6 errors, the student used one of the terms in a noun phrase from either the question or reference answer to refer to a concept where the reference answer facet included the other term as its modifier or vice versa. For example, one reference answer was looking for NMod(particles, clay) and Be(particles, light) and the student said Because clay is the lightest, which should have resulted in an Understood classification for the second facet (one could argue that there is an important distinction between the answers, but requiring elementary school students to answer at this level of specificity could result in an overwhelming number of interactions to clarify understanding). As a group, the simple lexical substitution categories (synonymy, hypernymy, hyponymy, meronymy, derivational changes, and other lexical paraphrases) appear more often in errors than any of the other factors with around 35 occurrences.  Roughly half of these relationships should be detectable using broad coverage lexical resources.  For example, substituting tiny for small, CO2 for gas, put for place, pen for ink and push for carry (WordNet entailment).  However, many of these lexical paraphrases are not necessarily associated 
15
in lexical resources such as WordNet.  For example, in the substitution of put the pennies for distribute the pennies, these terms are only connected at the top of the WordNet hierarchy at the Synset (move, displace).  Similarly, WordNet appears not to have any connection at all between have and contain. VerbNet alo does not show a relation between either pair of words. Concept definitions account for an additional 14 issues that could potentially be addressed by lexical resources such as WordNet. Vanderwende et al (2005) found that 34% of the Recognizing Textual Entailment Challenge test data could be handled by recognizing simple syntactic variations.  However, while syntactic variation is certainly common in the kids? data, it did not appear to be the primary factor in any of the system errors.  Most of the remaining paraphrase errors were classified as involving phrase-based paraphrases.  Examples here include ...it will heat up faster versus it got hotter faster and in the middle versus halfway between.  Six related errors essentially involved negation of an antonym, (e.g., substituting not a lot for little and no one has the same fingerprint for everyone has a different print).  Paraphrase recognition is an area that we intend to invest significant time in future research (c.f., Lin and Pantel, 2001; Dolan et al, 2004).  This research should also reduce the error rate on lexical paraphrases. The next most common issues after paraphrases were deep or logical reasoning and then pragmatics.  These two factors were involved in nearly 40% of the errors.  Examples of logical inference include recognizing that two cups have the same amount of water given the following student response, no, cup 1 would be a plastic cup 25 ml water and cup 2 paper cup 25 ml and 10 g sugar, and that two sounds must be very different in the case that ?it is easy to discriminate? Examples of pragmatic issues include recognizing that saying Because the vibrations implies that a rubber band is vibrating given the question context, and that the earth in the response ?the fulcrum is too close to the earth should be considered to be the load referred to in its reference answer. It is interesting that these are all examples that three annotators unanimously considered to be Expressed versus Inferred facets.  Finally, the remaining errors were largely the result of preprocessing issues.  At least two errors 
would be eliminated by simple data normalization (3?three and g?grams). Semantic role labeling has the potential to provide the classifier with information that would clearly indicate the relationships between the student and the reference answer, but there was only one error in which this came to mind as an important factor and it was not due to the role labels themselves, but because MaltParser labels only a single head. Specifically, in the sentence She could sit by the clothes and check every hour if one is dry or not, the pronoun She is attached as the subject of could sit, but check is left without a subject.   In previous work, analyzing the dependency parses of fifty one of the student answers, many had what were believed to be minor errors, 31% had significant errors, and 24% had errors that looked like they could easily lead to problems for the answer assessment classifier. Over half of the more serious dependency parse errors resulted from inopportune sentence segmentation due to run-on student sentences conjoined by and. To overcome these issues, the text could be parsed once using the original sentence segmentation and then again with alternative segmentations under conditions to be determined by further dependency parser error analysis.  One partial approach could be to split sentences when two noun phrases are conjoined and they occur between two verbs, as is the case in the preceding example, where the alternative segmentation results in correct parses. Then the system could choose the parse that is most consistent with the reference answer. While we believe improving the parser output will result in higher accuracy by the assessment classifier, there was little evidence to support this in the small number of parses examined in the assessment error analysis.  We only checked the parses when the dependency path features looked wrong and it was somewhat surprising that the classifier made an error (for example, when there were simple lexical substitutions involving very similar words) ? this was the case for only about 10-15 examples. Only two of these classification errors were associated with parser errors. However, better parses should lead to more reliable (less noisy) features, which in turn will allow the machine learning algorithm to more easily recognize which features are the most predictive. It should be emphasized that over half of the errors in Expressed facets involved more than one 
16
of the fine-grained factors discussed here. For example, to recognize the child understands a tree is blocking the sunlight based on the answer There is a shadow there because the sun is behind it and light cannot go through solid objects. Note, I think that question was kind of dumb, requires resolving it to the tree and the solid object mentioned to the tree, and then recognizing that light cannot go through [the tree] entails the tree blocks the light. 4.4 Errors in Unaddressed Facets Unlike the errors in Expressed facets, a number of the examples here appeared to be questionable annotations. For example, given the student answer fragment You could take a couple of cardboard houses and? 1 with thick glazed insulation?, all three annotators suggested they could not infer the student meant the insulation should be installed in one of the houses. Given the student answer Because the darker the color the faster it will heat up, the annotators did not infer that the student believed the sheeting chosen was the darkest color.  One of the biggest sources of errors in Unaddressed facets is the result of ignoring the context of words. For example, consider the question When you make an electromagnet, why does the core have to be iron or steel? and its reference answer Iron is the only common metal that can become a temporary magnet. Steel is made from iron. Then, given the student answer It has to be iron or steel because it has to pick up the washers, the system classified the facet Material_from(made, iron) as Understood based on the text has to be iron, but ignores the context, specifically, that this should be associated with the production of steel, Product(made, steel). Similarly, the student answer You could wrap the insulated wire to the iron nail and attach the battery and switch leads to the classification of Understood for a facet indicating to touch the nail to a permanent magnet to turn it into a temporary magnet, but wrapping the wire to the nail should have been aligned to a different method of making a temporary magnet. Many of the errors in Unaddressed facets appear to be the result of antonyms having very similar statistical co-occurrence patterns. Examples of errors here include confusing closer with greater distance and absorbs energy with reflects energy. 
However, both of these also may be annotation errors that should have been labeled Contra-Expr. The biggest source of error is simply classifying a number of facets as Understood if there is partial lexical similarity and perhaps syntactic similarity as in the case of accepting the balls are different in place of different girls. However, there are also a few cases where it is unclear why the decision was made, as in an example where the system apparently trusted that the student understood a complicated electrical circuit based on the student answer we learned it in class. The processes and the more informative features described in the preceding section describing errors in Expressed facets should allow the learning algorithm to focus on less noisy features and avoid many of the errors described in this section. However, additional features will need to be added to ensure appropriate lexical and phrasal alignment, which should also provide a significant benefit here. Future plans include training an alignment classifier separate from the assessment classifier.  5 Conclusion To our knowledge, this is the first work to success-fully assess constructed-response answers from elementary school students.  We achieved promis-ing results, 24.4% and 15.4% over the majority class baselines for Unseen Answers and Unseen Modules, respectively.  The annotated corpus asso-ciated with this work will be made available as a public resource for other researches working on educational assessment applications or other tex-tual entailment applications. The focus of this paper was to provide an error analysis of the domain-independent (Unseen Mod-ules) assessment condition.  We discussed the common types of issues involved in errors and their frequency when assessing young students? understanding of the fine-grained facets of refer-ence answers.  This domain-independent assess-ment will facilitate quicker adaptation of tutoring systems (or general test assessment systems) to new topics, avoiding the need for a significant ef-fort in hand-crafting new system components.   It is also a necessary prerequisite to enabling unre-stricted dialogue in tutoring systems.  
17
Acknowledgements We would like to thank the anonymous reviewers, whose comments improved the final paper.  This work was partially funded by Award Number 0551723 from the National Science Foundation. References  Bar-Haim, R., Szpektor, I. and Glickman, O. 2005. Definition and Analysis of Intermediate Entailment Levels. In Proc. Workshop on Empirical Modeling of Semantic Equivalence and Entailment. Callear, D., Jerrams-Smith, J., and Soh, V. 2001. CAA of short non-MCQ answers. In Proc. of the 5th Inter-national CAA conference, Loughborough. Dolan, W.B., Quirk, C, and Brockett, C. 2004. Unsu-pervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources. Pro-ceedings of COLING 2004, Geneva, Switzerland. Gildea, D. and Jurafsky, D. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28:3, 245?288. Glickman, O. and Dagan, WE., and Koppel, M. 2005. Web Based Probabilistic Textual Entailment. In Pro-ceedings of the PASCAL Recognizing Textual En-tailment Challenge Workshop. Graesser, A.C., Hu, X., Susarla, S., Harter, D., Person, N.K., Louwerse, M., Olde, B., and the Tutoring Re-search Group. 2001. AutoTutor: An Intelligent Tutor and Conversational Tutoring Scaffold. In Proceed-ings for the 10th International Conference of Artifi-cial Intelligence in Education San Antonio, TX, 47-49. Jordan, P.W., Makatchev, M., and VanLehn, K. 2004. Combining competing language understanding ap-proaches in an intelligent tutoring system. In J. C. Lester, R. M. Vicari, and F. Paraguacu, Eds.), 7th Conference on Intelligent Tutoring Systems, 346-357. Springer-Verlag Berlin Heidelberg. Kipper, K., Dang, H.T., and Palmer, M. 2000. Class-Based Construction of a Verb Lexicon. AAAI Seven-teenth National Conference on Artificial Intelligence, Austin, TX. Lawrence Hall of Science 2006. Assessing Science Knowledge (ASK), University of California at Ber-keley, NSF-0242510 Leacock, C. 2004. Scoring free-response automatically: A case study of a large-scale Assessment. Examens, 1(3). Lin, D. and Pantel, P. 2001. Discovery of inference rules for Question Answering. In Natural Language Engineering, 7(4):343-360. Mitchell, T., Russell, T., Broomhead, P. and Aldridge, N. 2002. Towards Robust Computerized Marking of Free-Text Responses. In Proc. of 6th International 
Computer Aided Assessment Conference, Loughbor-ough.  Nielsen, R., Ward, W., Martin, J. and Palmer, M. 2008a. Annotating Students? Understanding of Science Con-cepts. In Proc. LREC. Nielsen, R., Ward, W., Martin, J. and Palmer, M. 2008b. Extracting a Representation from Text for Semantic Analysis. In Proc. ACL-HLT. Nivre, J. and Scholz, M. 2004. Deterministic Depend-ency Parsing of English Text. In Proceedings of COLING, Geneva, Switzerland, August 23-27. Nivre, J., Hall, J., Nilsson, J., Eryigit, G. and Marinov, S. 2006. Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines. In Proceed-ings of the Tenth Conference on Computational Natural Language Learning (CoNLL). Palmer, M., Gildea, D., and Kingsbury, P. 2005. The proposition bank: An annotated corpus of semantic roles. In Computational Linguistics. Peters, S., Bratt, E.O., Clark, B., Pon-Barry, H., and Schultz, K. 2004. Intelligent Systems for Training Damage Control Assistants. In Proc. of Inter-service/Industry Training, Simulation, and Education Conference. Pulman, S.G. and Sukkarieh, J.Z. 2005. Automatic Short Answer Marking. In Proc. of the 2nd Workshop on Building Educational Applications Using NLP, ACL. Quinlan, J.R. 1993. C4.5: Programs for Machine Learn-ing. Morgan Kaufmann. Roll, WE., Baker, R.S., Aleven, V., McLaren, B.M., and Koedinger, K.R. 2005. Modeling Students? Metacog-nitive Errors in Two Intelligent Tutoring Systems. In L. Ardissono, P. Brna, and A. Mitrovic (Eds.), User Modeling, 379?388. Turney, P.D. 2001. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), 491?502. Vanderwende, L., Coughlin, D. and Dolan, WB. 2005. What Syntax can Contribute in the Entailment Task. In Proc. of the PASCAL Workshop for Recognizing Textual Entailment. VanLehn, K., Lynch, C., Schulze, K. Shapiro, J. A., Shelby, R., Taylor, L., Treacy, D., Weinstein, A., and Wintersgill, M. 2005. The Andes physics tutoring system: Five years of evaluations. In G. McCalla and C. K. Looi (Eds.), Proceedings of the 12th Interna-tional Conference on Artificial Intelligence in Educa-tion. Amsterdam: IOS Press.  
18
Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 9?16,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Topic Model Analysis of Metaphor Frequency for Psycholinguistic Stimuli
Steven Bethard
Computer Science Department
Stanford University
Stanford, CA 94305
bethard@stanford.edu
Vicky Tzuyin Lai
Department of Linguistics
University of Colorado
295 UCB, Boulder CO 80309
vicky.lai@colorado.edu
James H. Martin
Department of Computer Science
University of Colorado
430 UCB, Boulder CO 80309
james.martin@colorado.edu
Abstract
Psycholinguistic studies of metaphor process-
ing must control their stimuli not just for
word frequency but also for the frequency
with which a term is used metaphorically.
Thus, we consider the task of metaphor fre-
quency estimation, which predicts how often
target words will be used metaphorically. We
develop metaphor classifiers which represent
metaphorical domains through Latent Dirich-
let Allocation, and apply these classifiers to
the target words, aggregating their decisions to
estimate the metaphorical frequencies. Train-
ing on only 400 sentences, our models are able
to achieve 61.3% accuracy on metaphor clas-
sification and 77.8% accuracy on HIGH vs.
LOW metaphorical frequency estimation.
1 Introduction
Psycholinguistic studies of metaphor try to under-
stand metaphorical language comprehension by pre-
senting subjects with linguistic stimuli and observ-
ing their responses. Recent work has observed such
responses at the electrophysiological level, measur-
ing brain electrical activity as the stimuli are read
(Coulson and Petten, 2002; Tartter et al, 2002; Iaki-
mova et al, 2005; Arzouan et al, 2007; Lai et al,
2007). All these studies have attempted to make
comparisons across different types of stimuli (e.g.
literal vs. metaphorical) by holding the frequen-
cies of the target words constant across experimental
conditions. For example, Tartter et al (2002) com-
pared the metaphorical and literal sentences his face
was contorted by an angry cloud and his face was
contorted by an angry frown, where the two sen-
tences end in different words, but where the final
words cloud and frown had similar word frequen-
cies. As another example, Lai et al (2007) com-
pared the metaphorical and literal sentences Their
theories have collapsed and The old building has
collapsed, where the two sentences end in exactly
the same words, so the target word frequencies
across conditions were perfectly matched. In both
designs, controlling for word frequency allowed the
researchers to attribute the differences in experimen-
tal conditions to interesting factors, like figurativity,
rather than simple word frequency.
However, word frequency is not the only type of
frequency relevant to such experiments. In particu-
lar, metaphorical frequency, that is, how inherently
metaphorical one word is as compared to another,
may also play an important role in explaining the
psycholinguistic results. For example, if collapsed
is usually used literally, a greater processing effort
may be observed when a metaphorical instance of
collapsed is presented. Likewise, if collapsed is
usually used metaphorically, greater effort may be
observed when a literal instance is presented. Psy-
cholinguistic studies of metaphor have not, to date,
controlled for such metaphorical frequency because
there were no corpora or algorithms which could
provide the needed metaphorical frequencies.
The present study aims to address this deficiency
by producing models which can automatically esti-
mate how often a word is used metaphorically. We
build these models using only 50 examples each of
a small number of target words (< 10), rather than
requiring 50 or more examples of every target word
9
(100+) in the stimuli, as would be required by stan-
dard corpus linguistics methods. Our approach is
also novel in that it combines metaphor classifica-
tion with statistical topic models. Topic models are
intuitively promising for our task because they pro-
duce topics that seem to translate well to the theory
of conceptual domains, which suggests that, for ex-
ample, conceptual domains such as THEORIES and
BUILDINGS are used to understand Their theories
have collapsed. These topic models also show some
promise for distinguishing conventional metaphors
from novel metaphors.
2 Prior Work
Two types of prior research inform our current
study: corpus analyses investigating metaphor fre-
quency by hand, and machine learning models that
classify text as either literal or metaphorical. The
latter could be used to estimate metaphor frequen-
cies by applying the classifier to a corpus and aggre-
gating the classifications.
2.1 Metaphor Frequency
Researchers have manually estimated several differ-
ent kinds of metaphor frequency. Pollio et al (1990)
looked at overall metaphorical frequency, perform-
ing an exhaustive analysis of a variety of texts, and
concluding that there were about five metaphors for
every 100 words of text. Martin (1994) looked at
the frequency of different types of metaphor, us-
ing a sample of 600 sentences from the Wall Street
Journal (WSJ), and concluded among other things
that the most frequent type of WSJ metaphor was
VALUE is LOCATION, e.g. Spain Fund tumbled
23%. Martin (2006) looked at conditional probabil-
ities of metaphor, for example noting that in 2400
WSJ sentences, the probability of seeing an instance
of a metaphor was greatly increased after a first in-
stance had already been observed. However, none of
these studies provided the metaphorical frequencies
of individual words needed for our research.
Sardinha (2008) performed what is probably clos-
est to the type of analysis we are interested in.
Using a corpus of Portuguese conference calls,
Berber Sardinha identified 432 terms that were used
metaphorically. He then took 100 instances of each
of these terms in a general Brazilian corpus and
manually annotated them as being either literal or
metaphorical. Berber Sardinha found that on aver-
age these terms were used metaphorically 70% of
the time, and provided analysis of the metaphor-
ical frequencies of a number of individual terms.
While it is exactly these kinds of individual term
frequencies that we are after, we cannot use Berber
Sardinha?s data because his corpus was in Por-
tuguese while we are interested in English. This
brings out one of the main drawbacks of the corpus
annotation approach: moving to a new language (or
even a new genre) requires an extensive manual an-
notation project. Our goal is to avoid such costs by
taking advantage of machine learning techniques for
automatically identifying metaphorical text.
2.2 Metaphor Classification
Recent years have seen a rising interest in metaphor
classification systems. Birke and Sarkar (2006) took
a semi-supervised approach, collecting noisy exam-
ples of literal and non-literal sentences from both
WordNet and metaphor dictionaries, and using a
word-based measure of sentence similarity to group
sentences into literal and non-literal clusters. They
evaluated on hand-annotated sentences for 25 target
words and reported an F-score of 0.538, a substantial
improvement over the 0.294 majority class baseline.
Gedigian et al (2006) approached metaphor
identification as supervised classification, annotat-
ing around 4000 WSJ motion words as literal or
metaphorical, and training a maximum entropy clas-
sifier using as features based on named entities,
WordNet and semantic roles. They achieved an ac-
curacy of 95.1%, a decent improvement over the
very high majority class baseline of 93.8%.
Krishnakumaran and Zhu (2007) focused on three
syntactically constrained sub-types of metaphors:
nouns joined by be, nouns following verbs, and
nouns following adjectives. They combined Word-
Net hypernym information with bigram statistics
and a threshold, and evaluated their algorithm on
the Berkeley Master Metaphor List (Lakoff, 1994),
achieving an accuracy of around 46%.
All of these approaches produced models which
could be applied to new text to identify metaphors,
but each has some drawbacks for our task. The
WSJ study of Gedigian et al (2006) found 94% of
their target words to be metaphorical, a vastly differ-
10
Target L M M%
attacked 32 18 36%
born 45 5 10%
budding 16 34 68%
collapsed 10 40 80%
digest 7 43 86%
drifted 16 34 68%
floating 25 25 50%
sank 31 19 38%
spoke 47 3 6%
Total 229 221 49%
Table 1: Metaphorical (M) and literal (L) counts, and
metaphorical percentage (M%), for the annotated verbs.
ent number from the 49% for our target words (see
Section 3). Krishnakumaran and Zhu (2007) con-
sidered only a few different syntactic constructions,
but we need to consider all the ways a metaphor
may be expressed to evaulate overall metaphor fre-
quency. Birke and Sarkar (2006) did consider a va-
riety of target words in unrestricted text, but relied
on large scale language resources like WordNet and
metaphor dictionaries, while we are interested in ap-
proaches that are less resource intensive.
Thus, rather than basing our models on these prior
systems, we develop a novel approach to metaphor
frequency estimation based on using topic models to
operationalize metaphorical domains.
3 Data
The first step in building models of metaphorical
frequency is obtaining data for training and evalu-
ation. In one of the post-hoc analyses of the Lai et
al. (2007) experiment, 50 sentences from the British
National Corpus (BNC, 2007) were gathered for
each of nine of their target words. They annotated
each instance as either literal or metaphorical, and
then used these annotations to calculate metaphori-
cal frequencies for analysis.
This data served as our starting point for exploring
computational approaches to estimating metaphor-
ical frequency. Table 1 shows the nine verbs and
their metaphorical frequencies. Table 2 shows some
examples. Some verbs, such as digest, are almost al-
ways used metaphorically (86% of the time), while
other verbs, such as spoke, are almost always used
L Aye, that?s where I was born and reared.
M VATman threatens our budding entrepreneurs.
M Suddenly all her bravado collapsed.
L This makes it easier for us to digest the wheat.
L Gulls drifted lethargically on the swell.
M My heart sank as I looked around.
Table 2: Examples of sentences with metaphorical (M)
and literal (L) target words.
T# Most frequent words
00 book (4%) write (2%) read (2%) english (2%)
17 record (3%) music (2%) band (2%) play (2%)
42 social (3%) history (2%) culture (1%) society (1%)
58 film (3%) play (2%) theatre (1%) women (1%)
82 dog (9%) rabbit (2%) ferret (1%) pet (1%)
Table 3: Example topics (T#) from the BNC and their
most frequent words. Numbers in parentheses indicate
the percent of the topic each word represents.
literally (94% of the time). Annotation of just 50
instances of each of these nine verbs was time con-
suming, and yet to fully re-analyze the ERP results,
metaphorical frequencies would be needed for all of
the over 100 target words. Thus our goal was to au-
tomate this process.
4 Topic Models
Our approach to estimating metaphorical frequen-
cies was first to classify words in unrestricted text
as literal or metaphorical, and then to aggregate
those decisions to estimate a frequency. Thus, we
first needed to build a model which could iden-
tify metaphorical expressions. Our approach to this
problem was based on the theory of conceptual do-
mains, in which metaphors are seen as taking terms
from one domain (e.g. attacked) and applying them
to another domain (e.g. argument).
To operationalize these domains, we employed
statistical topic models, in particular, Latent Dirich-
let Allocation (LDA) (Blei et al, 2003). Intuitively,
LDA looks at how words co-occur in the documents
of a large corpus, and identifies topics or groups of
words that are semantically similar. For example,
Table 3 shows a few topics from the BNC. These
topics can be thought of as grouping words by their
semantic domains. For example, we might think of
topic 00 as the Book domain and topic 42 as the Soci-
ety domain. Because LDA generates topics that look
11
much like the source and target domains associated
with metaphors, we expect that LDA can provide a
boost to metaphor identification models.
The LDA algorithm is usually presented as a gen-
erative model, that is, as an imagined process that
someone might go through when writing a text. This
generative process looks something like:
1. Decide what topics you want to write about.
2. Pick one of those topics.
3. Think of words used to discuss that topic.
4. Pick one of those words.
5. To generate the next word, go back to 2.
This is a somewhat unrealistic description of the
writing process, but it gets at the idea that the words
in a document are topically coherent. Formally, the
process above can be described as:
1. For each document d select a topic distribution
?d ? Dir(?)
2. Select a topic z ? ?d
3. For each topic select a word distribution
?z ? Dir(?)
4. Select a word w ? ?z
The goal of the LDA learning algorithm then is to
maximize the likelihood of our documents, where
for one document p(d|?, ?) = ?Ni=1 p(wi|?, ?). Es-
timating these probabilities can be done in a few dif-
ferent ways, but in this paper we use Gibbs sampling
as it has been widely implemented and was available
in the LingPipe toolkit (Alias-i, 2008).
Gibbs sampling starts by randomly assigning top-
ics to all words in the corpus. Then the word-topic
distributions and document-topic distributions are
estimated using the following equations:
P (zi|zi?, wi, di, wi?, di?, ?, ?) = ?ij?jd?T
t=1 ?it?td
?ij = Cwordij+??W
k=1 Cwordkj+W?
?jd = Cdocdj+??T
k=1 Cdocdk+T?
Cwordij is the number of times word i was assigned
topic j, Cdocdj is the number of times topic j ap-
pears in document d, W is the total number of
unique words in the corpus, and T is the number
of topics requested. In essence, we count the num-
ber of times that a word is assigned a topic and
the number of times a topic appears in a document,
and we use these numbers to estimate word-topic
and document-topic probabilities. Once topics have
been assigned and distributions have been calcu-
lated, Gibbs sampling repeats the process, this time
selecting a new topic for each word by looking at
the calculated probabilities. The process is repeated
until the distributions become stable or a set number
of iterations is reached.
We ran LDA over the documents in the BNC, ex-
tracting 100 topics after 2000 iterations of Gibbs
sampling. We left the ? and ? parameters at their
LingPipe defaults of 0.1 and 0.01, respectively. Ta-
ble 3 shows some of the resulting topics.
5 Metaphor Frequency
Our primary goal was to use the topics produced by
LDA to help characterize words in terms of their
metaphorical frequency. We approached this prob-
lem by first training metaphor classifiers based on
LDA topics to identify target words in text as lit-
eral or metaphorical. Then we ran these classifiers
over unseen data, and aggregated the individual de-
cisions. The result is an approximate metaphorical
frequency for each word. The following sections de-
tail this process and discuss our preliminary results.
5.1 Metaphor Classification
Our data is composed of 50 sentences for each of
nine target words, with each sentence annotated as
either metaphorical or literal. We treated this as a
classification task, where the classifier took as input
a sentence containing a target word, and produced as
output either LITERAL or METAPHORICAL.
We trained support vector machine (SVM) clas-
sifiers on this data, using LDA topics as features.
For each of the sentences in our data, we used the
LDA topic models to assign topic probability distri-
butions to each of the words in the sentence. We then
summed the topic distributions over all the words in
the sentence to produce a sentence-wide topic dis-
tribution. The result was that for each sentence we
could say something like ?this sentence was com-
posed of 5% topic 00, 2% topic 01, 8% topic 02,
etc.? We used these sentence-level topic probabil-
ity distributions as features for an SVM classifier, in
particular, SVMperf (Joachims, 2005).
We compared this SVM-LDA model against two
baselines. The first was the standard majority class
12
classifier, which simply assigns all instances in the
test data whichever label (metaphorical or literal)
was most comon in the training data.
The second baseline was an SVM based on TF-
IDF features, a well known document classification
model (Joachims, 1998; Sebastiani, 2002; Lewis et
al., 2004). Under this approach, there is a numeric
feature for each of the 3000+ words in the training
data, and each word feature is assigned the weight:
|{w ? doc : w = word}|
|{w ? doc}| ?log
|{d ? docs}|
|{d ? docs : w ? d}|
Essentially, this formula means that the weight in-
creases with the number of times the word occurs
in the document, and decreases with the number of
documents in the corpus that contain that word. The
vectors of TF-IDF features are then normalized to
have Euclidean length 1.0, using the formula:
weight(word) = tf-idf(word)? ?
word?
tf-idf(word?)2
To evaluate our model against both the majority
class and the TF-IDF baselines, we ran nine-fold
cross-validations, where each fold corresponded to
a single target word. Note that this means that we
trained our models on the sentences of eight target
words, and tested on the sentences of the ninth tar-
get word. This is a harder evaluation than a strat-
ified cross-validation where all target words would
have been observed during training. But it is a much
more realistic evaluation for our task, where we want
to learn enough about metaphors from nine target
words that we can automatically classify instances
of the remaining 95.
Table 4 compares the performance of our SVM-
LDA model and the baseline models1. The major-
ity class classifier performs poorly, achieving only
26.4% accuracy2. The TF-IDF based model per-
forms much better, at 50.7% accuracy. However, our
SVM based on LDA features outperforms both base-
line models, achieving 54.9% accuracy.
1For all models, hyper parameters (the cost parameter, the
loss function, etc.) were set using only the training data of each
fold by running an inner eight-fold cross validation.
2This might be initially surprising since our corpus was 49%
metaphorical. Consider, however, that during cross validation,
holding out a more metaphorical target word for testing means
that our training data is more literal, and vice versa.
Model Accuracy
Majority Class 26.4%
SVM + TF-IDF 50.7%
SVM + LDA topics 54.9%
SVM + LDA topics + LDA groups 61.3%
Table 4: Model performance on the literal vs. metaphor-
ical classification task.
Type Most frequent words
CONCRETE book write read english novel
ABSTRACT god church christian jesus spirit
MIXED sleep dream earth theory moon
OTHER many time only number large
Table 5: Examples of annotated topics.
5.2 Annotating Topics
The metaphor classification results showed the ben-
efit of operationalizing metaphor domains as LDA
topics. But metaphors are typically viewed as map-
ping a concrete source domain onto an abstract tar-
get domain, and our LDA topics had no direct notion
of this concrete/abstract distinction. To try to repre-
sent this distinction, we manually annotated3 the 100
LDA topics with one of four labels: CONCRETE,
ABSTRACT, MIXED or OTHER. Table 5 shows ex-
amples of the annotated topics.
We then used the annotated topics to generate new
features for our classifiers. In addition to the original
100 topic probability features, we provided four new
probability features, one for each of our labels, cal-
culated by taking the sum of the probabilities of the
corresponding topics. For example, since topics 07,
13, 37 and 77 were identified as ABSTRACT topics,
the probability of the new ABSTRACT feature was
just the sum of the probabilities of the topic features
07, 13, 37 and 77. The last row of Table 4 shows
the performance of the SVM model trained with the
augmented feature set. This model outperforms all
our other models, achieving an accuracy of 61.3%
on the literal vs. metaphorical distinction.
These results are interesting because they show
that human analysis of LDA topics can add substan-
tial value for machine learning models at a low cost.
Annotating the entire set of 100 topics took under
3All annotation was performed by a single annotator. Future
work will measure inter-annotator agreement.
13
Model Accuracy
Majority Class 0.0%
SVM + TF-IDF 22.2%
SVM + LDA topics 55.6%
SVM + LDA topics + LDA groups 77.8%
Table 6: Model performance on the HIGH vs. LOW
metaphor frequency prediction task.
an hour, and yet provided a 6% gain in model ac-
curacy. The speed of annotation suggests that LDA
topics are conceptually accessible to humans, and
the performance boost suggests that manual group-
ing of LDA topics may be a fruitful area for feature
engineering.
5.3 Predicting Metaphorical Frequencies
Having constructed successful metaphor classifica-
tion models, we return to our question of metaphor-
ical frequency. Given a target word, can we pre-
dict the frequency with which that word will be
used metaphorically? Our models are not accurate
enough that we can expect the frequencies derived
from them to be exact predictions of metaphorical
frequency. But we may be able to distinguish, for
example, words with high metaphorical frequency
from words with low metaphorical frequency.
Thus, we evaluate our models on the binary task
of assigning target words an overall metaporical fre-
quency, either HIGH (? 50%) or LOW (< 50%). We
can perform this evaluation using the same data and
cross validation technique as before, this time exam-
ining each testing fold (which corresponds to a sin-
gle target word) and aggregating the metaphor clas-
sifications to get a metaphorical frequency estimate
of that target. Table 6 shows how the models fared
on this task. The majority class model misclassified
all the words, and the TF-IDF model managed to get
only two of the nine correct. The LDA models per-
formed better, with the model including the grouped
topic features achieving 77.8% accuracy. This sug-
gests that our model may already be good enough
to use for analysis of the original Lai experimental
data. Of course, this evaluation was carried out only
over the nine available target words, so additional
evaluation will be necessary to confirm these trends.
To further analyze our model performance, we
looked at the metaphorical frequency estimates for
Word True Predicted Difference
attacked 36% 24% -12%
born 10% 2% -8%
budding 68% 98% +30%
collapsed 80% 98% +18%
digest 86% 40% -46%
drifted 68% 92% +24%
floating 50% 100% +50%
sank 38% 26% -12%
spoke 6% 62% +56%
Table 7: Model performance on the HIGH vs. LOW
metaphor frequency prediction task.
each target word. Table 7 shows the estimates of
our best model along with the true metaphorical fre-
quencies. The three target words with the largest dif-
ferences between true and predicted accuracies are
spoke, floating and digest, with spoke and floating
predicted to be much more metaphorical than they
actually are, and digest predicted to be much less.
We also performed some analysis of the model er-
rors. In many cases it was difficult to judge why the
model succeeded or failed in identifying a metaphor,
but a couple of things stood out. First, 70% of the
digest instances our model misclassified were Di-
gest (capitalized), e.g. Middle East Economic Di-
gest. Our topic models were trained on all lower-
cased words, so Digest and digest were not distin-
guished. Re-training the models without collaps-
ing the case distinctions might address this prob-
lem. Second, spoke seems to be an inherently harder
term to classify because it co-occurs with so many
other topics. About 40% of the spoke instances oc-
curred as spoke of or spoke about, where speaking
about a metaphorical topic caused spoke to be inter-
preted metaphorically, and speaking about a literal
topic caused spoke to be interpreted literally. Ad-
dressing this problem would probably require some
understanding of argument structure, perhaps akin
to what was done by Gedigian et al (2006).
6 Metaphor Novelty
As a final exploration of topic models for metaphor-
ical domains, we considered metaphorical novelty,
as used in the original Lai experiment. In particular,
we were interested in how LDA topics might reflect
14
Type Stimulus Sentence
LIT Every soldier in the frontline was attacked
CON Every point in my argument was attacked
NOV Every second of our time was attacked
ANOM Every drop of rain was attacked
LIT The old building has collapsed
CON Their theories have collapsed
NOV Their compromises have collapsed
ANOM The apples have collapsed
Table 8: Example stimuli: literal (LIT), conventional
metaphor (CON), novel metaphor (NOV) and anomalous
(ANOM).
more conventional or more novel metaphors. In the
Lai experiment, conventional and novel metaphors
for a particular target word shared the same source
domain (e.g. WAR) but differed in the target domain
(e.g. ARGUMENT vs. TIME). If LDA topics are
a good operationalization of such domains, then it
should be possible use LDA topics to distinguish be-
tween conventional and novel metaphors.
To explore this area, we employed the stimuli
from the Lai experiment, and looked in particular
at the conventional and novel conditions. The Lai
experiment used 104 different target words, so these
data included 104 conventional metaphors and 104
novel metaphors. Novel metaphors were generated
for the Lai experiment by considering a conventional
source-target mapping and selecting a new target
domain. For example, the conventional metaphor
Every point in my argument was attacked maps
the source domain WAR to the target domain AR-
GUMENT, while the novel metaphor Every second
of our time was attacked maps the source domain
WAR to the target domain TIME. Table 8 shows ex-
ample stimulus sentences from the Lai experiment.
Though these experimental stimuli have the draw-
back of being manually constructed, not collected
from a corpus, they have the advantage of being
already annotated with a definition of novelty that
clearly distinguishes the two types of metaphors.
We performed a simple correlational analysis us-
ing the conventional and novel metaphors from the
Lai experiment. We produced topic distributions for
each stimulus, using our topic models trained on the
BNC. We then labeled conventional metaphors as -1
and novel metaphors as +1, and identified the top-
-0.19 like house old shop door look street room
-0.18 darlington programme club said durham hall
-0.15 film play theatre women actor work perform
-0.14 area local plan develop land house rural urban
-0.14 any sale good publish custom product price
Table 9: Top 5 topics correlated with conventionality.
0.20 freud sexual sophie male joanna people female
0.17 doctor leed rory dalek fergus date subject aug
0.13 book write read english novel publish reader
0.11 lorton kirov dougal jed manville vologski celia
0.09 war british france britain french nation europe
Table 10: Top 5 topics correlated with novelty.
ics that correlated best with this distinction. Table 9
shows the most negatively correlated (conventional)
topics and Table 10 shows the most positively corre-
lated (novel) topics.
Though even the best correlations are somewhat
low, there seem to be some trends in this analysis.
Conventional metaphors seem to correspond more
to concrete terms, like house, club, play and sale.
Novel metaphors have less of a coherent theme, in-
cluding terms like freud and sexual as well as names
like Rory, Kirov and Britain. This may reflect a
real distinction in the use of conventional and novel
metaphors, or it may be an artifact of how the exper-
imental stimuli were created. A deeper investigation
into the relations between LDA topics and metaphor
novelty will probably require annotating sentences
from some naturally occuring data.
7 Conclusions
We presented a novel two-phase approach to the task
of metaphorical frequency estimation. First, exam-
ples of a target word were automatically classified
as literal or metaphorical, and then these classifi-
cations were aggregated to estimate how often the
target word was used metaphorically. Our classi-
fiers operationalized metaphorical source and target
domains using topics derived from Latent Dirichlet
Allocation. Support vector machine classifiers took
these topic probability distributions and learned to
classify sentences as literal or metaphorical. These
models achieved 61.3% accuracy on the classifiation
task, and their aggregated classifications produced
an accuracy of 77.8% on the task of distinguishing
15
between target words with high and low metaphori-
cal frequencies.
Future work will perform a larger scale eval-
uation, and will use our model?s metaphorical
frequency estimates to analyze psycholinguistic
data. In particular, we will split the conventional
metaphorical sentences of Lai et al (2007) into
low and high-frequency items. If the low and
high frequency items display significantly differ-
ent brainwave patterns, then this could suggest that
metaphorical frequency of a given word plays a crit-
ical role in metaphor comprehension.
Future work will also explore frequency effects
that consider the sentential context in the stimulus
items. For example, a context like ?Their theories
have ? probably gives a higher expectation of a
metaphorical word filling in the blank than a context
like ?The old building has ?. Having a measure
of how much the words in the preceding context pre-
dict an upcoming metaphor would provide another
useful stimulus control.
References
Alias-i. 2008. LingPipe 3.7.0. http://alias-
i.com/lingpipe/, October.
Yossi Arzouan, Abraham Goldstein, and Miriam Faust.
2007. Brainwaves are stethoscopes: ERP correlates
of novel metaphor comprehension. Brain Research,
1160:69?81, July.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of non-
literal language. In European Chapter of the ACL
(EACL).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
BNC. 2007. The british national corpus, version 3
(BNC XML edition). Distributed by Oxford Univer-
sity Computing Services on behalf of the BNC Con-
sortium. http://www.natcorp.ox.ac.uk/.
Seana Coulson and Cyma Van Petten. 2002. Conceptual
integration and metaphor: an event-related potential
study. Memory & Cognition, 30(6):958?68, Septem-
ber. PMID: 12450098.
Matt Gedigian, John Bryant, Srini Narayanan, and Bran-
imir Ciric. 2006. Catching metaphors. In Workshop
On Scalable Natural Language Understanding.
Galina Iakimova, Christine Passerieux, Jean-Paul Lau-
rent, and Marie-Christine Hardy-Bayle. 2005.
ERPs of metaphoric, literal, and incongruous seman-
tic processing in schizophrenia. Psychophysiology,
42(4):380?390.
Thorsten Joachims, 1998. Text categorization with Sup-
port Vector Machines: Learning with many relevant
features, pages 137?142. Springer Berlin / Heidel-
berg.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings of
the 22nd international conference on Machine learn-
ing, pages 377?384, Bonn, Germany. ACM.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources. In
Workshop on Computational Approaches to Figurative
Language.
Vicky Tzuyin Lai, Tim Curran, and Lise Menn.
2007. The comprehension of conventional and novel
metaphors: An ERP study. In 13th Annual Confer-
ence on Architectures and Mechanisms for Language
Processing, August.
George Lakoff. 1994. Conceptual metaphor WWW
server. http://cogsci.berkeley.edu/lakoff/.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: a new benchmark collection for text
categorization research. J. Mach. Learn. Res., 5:361?
397.
James H. Martin. 1994. MetaBank: a Knowledge-Base
of metaphoric language conventions. Computational
Intelligence, 10(2):134?149.
James H. Martin. 2006. A rational analysis of the con-
text effect on metaphor processing. In Stefan Th. Gries
and Anatol Stefanowitsch, editors, Corpus-Based Ap-
proaches to Metaphor and Metonymy. Mouton de
Gruyter.
Howard R. Pollio, Michael K. Smith, and Marilyn R. Pol-
lio. 1990. Figurative language and cognitive psychol-
ogy. Language and Cognitive Processes, 5:141?167.
Tony Berber Sardinha. 2008. Metaphor probabilities
in corpora. In Mara Sofia Zanotto, Lynne Cameron,
and Marilda do Couto Cavalcanti, editors, Confronting
Metaphor in Use, pages 127?147. John Benjamins.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Surveys
(CSUR), 34(1):1?47.
Vivien C. Tartter, Hilary Gomes, Boris Dubrovsky, So-
phie Molholm, and Rosemarie Vala Stewart. 2002.
Novel metaphors appear anomalous at least momen-
tarily: Evidence from N400. Brain and Language,
80(3):488?509, March.
16
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 12?21,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Identifying science concepts and student misconceptions
in an interactive essay writing tutor
Steven Bethard
University of Colorado
Boulder, Colorado, USA
steven.bethard@colorado.edu
Ifeyinwa Okoye
University of Colorado
Boulder, Colorado, USA
ifeyinwa.okoye@colorado.edu
Md. Arafat Sultan
University of Colorado
Boulder, Colorado, USA
arafat.sultan@colorado.edu
Haojie Hang
University of Colorado
Boulder, Colorado, USA
haojie.hang@colorado.edu
James H. Martin
University of Colorado
Boulder, Colorado, USA
james.martin@colorado.edu
Tamara Sumner
University of Colorado
Boulder, Colorado, USA
tamara.sumner@colorado.edu
Abstract
We present initial steps towards an interac-
tive essay writing tutor that improves science
knowledge by analyzing student essays for mis-
conceptions and recommending science web-
pages that help correct those misconceptions.
We describe the five components in this sys-
tem: identifying core science concepts, deter-
mining appropriate pedagogical sequences for
the science concepts, identifying student mis-
conceptions in essays, aligning student miscon-
ceptions to science concepts, and recommend-
ing webpages to address misconceptions. We
provide initial models and evaluations of the
models for each component.
1 Introduction
Students come to class with a variety of misconcep-
tions present in their science knowledge. For ex-
ample, science assessments developed by the Amer-
ican Association for the Advancement of Science
(AAAS)1 showed that 49% of American 6th-8th
graders believe that the Earth?s tectonic plates are
only feet thick (while in fact they are miles thick)
and that 48% of American 6th-8th graders believe
that atoms of a solid are not moving (while in fact
all atoms are in constant motion). A key challenge
for interactive tutoring systems is thus to identify and
correct such student misconceptions.
In this article, we develop an interactive essay writ-
ing tutor that tries to address these challenges. The
tutor first examines a set of science webpages to iden-
tify key concepts (Section 4) and attempts to order
1http://assessment.aaas.org/
the science concepts in a pedagogically appropriate
learning path (Section 5). Then the tutor examines a
student essay and identifies misconception sentences
(Section 6) and aligns these misconceptions to the
true science concepts (Section 7). Finally, the tutor
suggests science webpages that can help the student
address each of the misconceptions (Section 8).
The key contributions of this work are:
? Demonstrating that a summarization approach
can identify core science concepts
? Showing how a learning path model can be boot-
strapped from webpages with grade metadata
? Developing models for misconception identifi-
cation based on textual entailment techniques
? Presenting an information retrieval approach to
aligning misconceptions to science concepts
? Designing a system that recommends webpages
to address student misconceptions
2 Related work
Interactive tutoring systems have been designed for
a variety of domains and applications. Dialog-based
tutoring systems, such as Why2-Atlas (VanLehn et
al., 2002), AutoTutor (Graesser et al, 2004) and
MetaTutor (Azevedo et al, 2008), interact with stu-
dents via questions and answers. Student knowledge
is judged by comparing student responses to knowl-
edge bases of domain concepts and misconceptions.
These knowledge bases are typically manually cu-
rated, and a new knowledge base must be constructed
for each new domain where the tutor is to be used.
12
Essay-based tutoring systems, such as Summary
Street (Wade-Stein and Kintsch, 2004) or CLICK
(de la Chica et al, 2008b), interact with students who
are writing a summary or essay. They compare what
the student has written to domain knowledge in the
form of textbooks or webpages. They typically do not
require a knowledge base to be manually constructed,
instead using natural language processing techniques
to compare the student?s essay to the information in
the textbooks or webpages.
The current work is inspired by these essay-based
tutoring systems, where interaction revolves around
essay writing. However, where Summary Street re-
lies primarily upon measuring how much of a text-
book a student essay has ?covered?, we aim to give
more detailed assessments that pinpoint specific stu-
dent misconceptions. CLICK targets a similar goal
to ours, but assumes that accurate knowledge maps
can be generated for both the domain knowledge and
for each student essay. Our approach does not re-
quire the automatic generation of knowledge maps,
instead working directly with the sentences in the
student essays and the webpages of science domain
knowledge.
3 System overview
Our system is composed of five key components.
First, a core concept identifier examines domain
knowledge (webpages) and identifies key concepts
(sentences) that describe the most important pieces
of knowledge in the domain. Second, a concept se-
quencer assigns a pedagogically appropriate order in
which a student should learn the identified core con-
cepts. Third, a misconception identifier examines the
student essay and identifies sentences that describe
misconceptions the student has about the domain.
Fourth, a misconception-concept aligner finds a core
concept that can be used to correct each misconcep-
tion. Finally, a recommender takes all the informa-
tion about core concepts and student misconceptions,
decides what order to address the misconceptions in,
and identifies a set of resources (webpages) for the
student to read.
To assemble this system, we draw on a variety of
existing datasets (and some data collection of our
own). For example, we use data from an annotation
study of concept coreness to evaluate our model for
identifying domain concepts, and we use data from
science assessments of the American Association for
the Advancement of Science to train and evaluate our
model for identifying misconceptions. We use this
disparate data to establish baseline models for each of
the tutor?s components. In the near future, this base-
line tutoring system will be used to collect student
essays and other data that will allow us to develop
more sophisticated model for each component.
4 Identifying core concepts
This first module aims at automatically identifying a
set of core concepts in a given set of digital library
resources or webpages. Core concepts in a subject
domain are critical ideas necessary to support deep
science learning and transfer in that domain. From
a digital learning perspective, availability of such
concepts helps in providing pedagogical feedback
to learners to support robust learning and also in
prioritizing instructional intervention (e.g., deciding
the order in which to treat student misconceptions).
A concept can be materialized using different levels
of linguistic expressions (e.g. phrases, sentences or
paragraphs), but for this work, we focus only on
individual sentences as expressions of concepts.
We used COGENT (de la Chica et al, 2008a), a
multi-document summarization system to extract con-
cepts (i.e. sentences) from a given set of resources.
In the following two subsections, we describe the
COGENT system, discuss how we used it for core
concept extraction and report the results of its evalu-
ation of effectiveness.
4.1 Model
COGENT is a text summarizer that builds on MEAD
(Radev et al, 2004), a multidocument summarization
and evaluation platform . MEAD was originally de-
veloped to summarize news articles. COGENT aims
to generate pedagogically useful summaries from
educational resources.
COGENT extends MEAD by incorporating new
features in the summarization process. MEAD uses
a set of generic (i.e. domain-independent) features to
evaluate each sentence in the given set of documents.
These features include the length of the sentence, the
distance from the sentence to the beginning of the
document, etc. Individual scores of a sentence along
13
these dimensions are combined to assign a total score
to the sentence. After removing redundant sentences,
MEAD then generates a summary using the sentences
that had the highest scores. A user-specified parame-
ter determines the number of sentences included in
the summary.
COGENT extends this framework by incorporat-
ing new domain-general and domain-specific features
in the sentence scoring process. The domain-general
features include a document structure feature, which
takes into account a sentence?s level in terms of
HTML headings, and a content word density fea-
ture, which computes the ratio of content words to
function words. The domain-specific features include
an educational standards feature, which uses a TF-
IDF based textual similarity score between a sentence
and nationally recognized educational goals from the
American Association for the Advancement of Sci-
ence (AAAS) Benchmarks (Project2061., 1993) and
the associated National Science Education Standards
(NRC, 1996), and a gazetteer feature, which scores
sentences highly that mention many unique names
from a gazetteer of named entities.
While in the past, COGENT was used primarily
as a summarization system, in the current work, we
evaluate its utility as a means of identifying core
concepts. That is, are the top sentences selected
by COGENT also the sentences describing the key
science concepts in the domain?
4.2 Evaluation
We evaluate the core concept extraction module by
assessing the extracted concepts against human ex-
pert annotations. We ran an annotation study where
two human experts assigned ?coreness? ratings to
a selected set of sentences collected from digital
resources in three science domains: Plate Tecton-
ics, Weather and Climate, and Biological Evolution.
These experts had been recruited based on their train-
ing and expertise in the selected subject domains.
First, a set of digital resources was selected from
the Digital Library for Earth System Education
(DLESE) 2 across the three subject domains. Then
COGENT was used to extract the top 5% sentences
for each domain. The experts then annotated each
extracted sentence with its coreness rating on a scale
2http://www.dlese.org
Extraction %
0.5% 1.0% 2.5% 5.0%
Plate Tectonics 3.33 3.27 3.00 2.81
Weather and Climate 3.13 2.97 3.07 2.99
Biological Evolution 2.00 2.13 2.46 2.25
Table 1: Average coreness of sentences extracted at differ-
ent percentages in each domain
of 1 to 4, 4 being the highest. Human annotation is
a time-consuming process and this is why we had
to limit the number of extracted sentences to a mod-
erate 5% (which is still more than 400 sentences).
17% of the sentences were double annotated and the
inter-rater reliability, measured by Spearman?s rho,
was 0.38. These expert ratings of sentences form the
basis of our evaluation.
Table 1 shows the average coreness assigned by the
experts to sentences extracted by COGENT in each
domain, for different extraction percentages. For ex-
ample, if COGENT is used to extract the top 1% of
sentences from all the Plate Tectonics resources, then
the average of their coreness ratings (as assigned by
the experts) is 3.27, representing a high level of core-
ness. This is essentially a measure of the precision
of COGENT at 1% extraction. Note that we cannot
calculate a measure of recall without asking experts
to annotate all of the domain sentences, a time con-
suming task which was outside of the scope of this
study.
The performance of COGENT was the best in the
Plate Tectonics domain since the domain-aware fea-
tures (e.g. the gazetteer features) used to train CO-
GENT were selected from this domain. In the ?near
domain? of Weather and Climate, the performance is
still good, but performance falls in the ?far domain?
of Biological Evolution, because of the significant
differences between the training domain and the test
domain. In the two latter domains, the performance
of COGENT was also inconsistent in that with an
increase in the extraction percentage, the average
coreness increased in some cases and decreased in
others. This inconsistency and overall degradation
in performance in the two latter domains are indica-
tive of the importance of introducing domain-aware
features into COGENT.
It is evident from the values in Table 1 that the
core concepts extraction module does a decent job,
14
especially when trained with appropriate domain-
aware features.
5 Sequencing core concepts
The goal of this next component is to take a set of
core science concepts (sentences), as produced by
the preceding module, and predict an appropriate se-
quence in which those concepts should be learned by
the student. Some concepts serve as building blocks
for other concepts, and thus it is essential to learn the
basic concepts first (and address any misconceptions
associated with them) before moving on to other con-
cepts that depend on the basic concepts. For example,
a student must first understand the concept of tectonic
plates before they can understand the concept of a
convergent plate boundary. The sequence of core
concepts that results from this module will serve as
input for the later module that prioritizes a student?s
misconceptions.
There may exist several different but reasonable
concept sequences (also known as learning paths) ?
the goal of this component is to recommend at least
one of these. As a first step, we focus on generating
a single concept sequence that represents a general
path through the learning goals, much like textbooks
and curriculums do.
5.1 Models
Our model for concept sequencing is a pair-wise
ordering model, that takes two concepts c1 and c2,
and predicts whether c1 should come before or after
c2 in the recommended learning path. Formally,
SEQUENCE(c1, c2) =
{
0 if c1 < c2
1 if c1 ? c2
To generate a complete ordering of concepts, we
construct a precedence table from these pair-wise
judgments and generate a path that is consistent with
these judgments.
We learn the SEQUENCE model as a supervised
classifier, where a feature vector is extracted for each
of the two concepts and the two feature vectors, con-
catenated, serve as the input to the classifier. For each
word in each concept, we include the following two
features:
? local word count - the number of times the
word appeared in this concept
? global word count - the log of the ratio between
the number of times the word occurred in the
concept and the number of times it occurred in
a background corpus, Gigaword (Graff, 2002)
These features are motivated by the work of Tanaka-
ishii et al(2010) that showed that local and global
word count features were sufficient to build a pair-
wise readability classifier that achieved 90% accu-
racy.
For the supervised classifier, we consider naive
Bayes, decision trees, and support vector machines.
5.2 Evaluation
To evaluate our concept sequencing model, we gath-
ered learning paths from experts in high school earth
science. Using the model from Section 4, we selected
30 core concepts for the domain of plate tectonics.
We asked two earth science experts to each come up
with two learning paths for these core concepts, with
the first path following an evidence or research based
and second path following a traditional learning path.
An evidence or research based learning path, is
a pedagogy where students are encouraged to use
the scientific method to learn about a phenomena, i.e
they gather information by observing the phenomena,
form a hypothesis, perform experiment, collect and
analyze data and then interpret the data and draw
conclusions that hopefully align with the current un-
derstanding about the phenomena. A teacher that
uses this learning path acts as a guide on the side. A
traditional learning path on the other hand, is the ped-
agogy where teachers are simply trying to pass on the
correct information to students rather than letting the
students discover the information themselves. In a
classroom environment, a teacher using this learning
path would be seen as the classical sage on stage.
We used the learning paths collected from the ex-
perts to form two test sets, one for the evidence-based
pedagogy, and one for the traditional pedagogy. For
each pedagogy, we asked which of all the possible
pair-wise orderings our experts agreed upon. For ex-
ample, if the first expert said that A < B < C and
the second expert said that A < C < B, then both
experts agreed that A < B and A < C, while they
disagreed on whether B < C or C < B. Note that
we evaluate pair-wise orderings here, not a complete
ranking of the concepts, because the experts did not
15
Pedagogy Pairs (%) c1 < c2 c1 ? c2
Evidence 637 (68%) 48.5% 51.5%
Traditional 613 (70%) 48.5% 51.5%
Table 2: Test sets for sequencing concepts. The Pairs
column shows how many pairs the experts agreed upon
(out of a total of 30 ? 29 = 870 pairs).
produce a total ordering of the concepts, only a par-
tial tree-like ordering. The experts put the concepts
in levels, with concepts in the same level having no
precedence relationship, while a concept in a lower
level preceded a concept in a higher level.
For our test sets, we selected only the pairs on
which both experts agreed. Table 2 shows that experts
agreed on 68-70% of the pair-wise orderings. Table
2 also shows the percentage of each type of pair-wise
ordering (c1 < c2 vs. c1 ? c2) present in the data.
Note that even though all concepts are paired with all
other concepts, because the experts do not produce
complete orderings, the number of agreements for
each type of ordering may not be the same. Consider
the case where expert E1 says that concepts A and
B are on the same level (i.e., A = B) and expert E2
says that concept A is in a lower level than concept
B (i.e., A < B). Then for the pair (A,B), they
disagree on the relation (E1 says A ? B while E2
says A < B) but for the pair (B,A) they agree on
the relation (they both say B ? A). As a result, the
c1 ? c2 class is slightly larger than the c1 < c2 class.
Since these data sets were small, we reserved them
for testing, and trained our pair-wise classification
model using a proxy task: ordering sentences by
grade. In this task, the model is given two sentences
s1 and s2, one written for middle school and written
for high school, and asked to decide whether s1 < s2
(i.e. s1 is the middle school sentence) or s2 < s1
(i.e. s2 is the middle school sentence). We expect
that a model for ordering sentences by grade should
also be a reasonable model for ordering concepts
for a pedagogical learning path. And importantly,
getting grade ordering data automatically is easy: the
Digital Library for Earth System Education (DLESE)
contains a variety of earth science resources with
metadata about the grade level they were written for.
To construct the training data, we searched the
DLESE website for text resources that contained
the words earthquake or plate tectonics. We col-
Baseline NaiveBayes SVM
Evidence 51.5% 60.8% 53.3%
Traditional 51.5% 56.6% 49.7%
Table 3: Accuracy result from Naive Bayes and SVM for
classifying the core concepts
lected 10 such resources for each of the two grade
cohorts, middle school (we allowed anything K-8)
and high school (we allowed anything 9+). We down-
loaded the webpage for each resource, and used CO-
GENT to extract the 20 most important sentences
from each. This resulted in 200 sentences for each
of the two grade cohorts. To create pairs of grade-
ordered sentences, we paired up middle and high
school concepts both ways: middle school first (i.e.
SEQUENCE(cm, ch) = 0) and high school first (i.e.
SEQUENCE(ch, cm) = 1). This resulted in 40,000
grade-ordered sentence pairs for training.
We then used this proxy-task training data to
train our models. We extracted 1702 unique non-
stopwords from the training data, resulting in 3404
features per concept, and 6808 features per con-
cept pair (i.e. per classification instance). On the
grade-ordering task, we evaluated three models using
WEKA3, a naive Bayes model, a decision tree (J48)
model, and a support vector machine (SVM) model.
Using a stratified 50/50 split of the training data, we
found that the naive Bayes and SVM models both
achieved an accuracy of 80.2%, while the decision
tree achieved only 62%. So, we selected the naive
Bayes and SVM models for our real task, concept
sequencing.
Table 3 shows the performance of the two models
on the expert judgments of concept sequencing. We
find that the naive Bayes model produces more expert-
like concept sequences than would be generated by
chance and also outperforms the SVM model on the
concept sequencing task. For the final output of the
module, we combine the pair-wise judgments into a
complete concept sequence, breaking any ties in the
pair-wise judgments by preferring the order of the
concepts in the output of the core concept identifier.
3http://www.cs.waikato.ac.nz/ml/weka/
16
6 Identifying student misconceptions
The previous components have focused on analyzing
the background knowledge ? finding core concepts
in the domain and selecting an appropriate learning
sequence for these concepts. The current component
focuses on the student essay, using the collected back-
ground knowledge to help analyze the essay and give
feedback.
Given a student essay, the goal of this component
is to identify which sentences in the essay are most
likely to be misconceptions. The task of misconcep-
tion identification is closely related to the task of
textual entailment (Dagan et al, 2006), in which the
goal is to predict if a hypothesis sentence, H, can be
reasonably concluded given another sentence, T. In
misconception identification, the goal is to predict if
a student sentence can be concluded from any com-
bination of the sentences in the domain knowledge,
similar to a textual entailment task with a single H
but many Ts. A student sentence that can not be
concluded from the domain knowledge is likely a
misconception.
6.1 Models
We developed two models for identifying student
misconceptions, inspired by work in textual entail-
ment that showed that a model that simply counts the
words in H that appeared in T, after expanding the
words in T using WordNet, achieves state-of-the-art
performance (Shnarch et al, 2011)4.
The Coverage model scores a student sentence
by counting the number of its words that are also in
some domain sentence. Low-scoring sentences are
likely misconceptions. Formally:
SCORE(s) =
|s ? d|
|s|
d =
?
s??D
EXPAND(s?)
where s is a student sentence (a list of words), D is
the set of domain sentences, and EXPAND performs
lexical expansion on the words of a sentence.
The Retrieval model indexes the domain sen-
tences with an information retrieval system (we use
4The paper also proposes a more elaborate probabilistic
model, but shows that the ?lexical coverage? model we adopt
here is quite competitive both with their probabilistic model and
with the top-performing systems of RTE5 and RTE6.
Lucene5), and scores a student sentence by querying
the index and summing the scores. Formally:
SCORE(s) =
?
s??D
SCORElucene(s, EXPAND(s
?))
where s, D and EXPAND are defined as before, and
SCORElucene is a cosine over TF-IDF vectors6.
For both the Coverage and Retrieval models, we
consider the following lexical expansion techniques
for defining the EXPAND function:
? tokens ? words in the sentence (no expansion)
? tokens, synsets ? words in the sentence, plus
all lemmas of all WordNet synsets of each word
? tokens, synsetsexpanded ? words in the sentence,
plus all lemmas of all WordNet synsets of each
word, plus all lemmas of derived forms, hy-
ponyms or meroynms of the WordNet synsets
? tokens, synsetsexpanded?4 ? words in the sen-
tence, plus all lemmas of all WordNet synsets of
each word, plus all lemmas of WordNet synsets
reachable by a path of no more than 4 links
through derived forms, hyponyms or meroynms
6.2 Evaluation
We evaluate the quality of our misconception identi-
fication models using data collected from the Amer-
ican Association for the Advancement of Science?s
Project 2061 Science Assessment Website7. This
website identifies the main ideas in various topics
under Life Science, Physical Science and Earth Sci-
ence, and for each idea provides several sentences
of description along with its individual concepts and
common student misconceptions.
We used 3 topics (17 ideas, averaging 6.2 descrip-
tion sentences, 7.1 concept sentences and 9.9 miscon-
ception sentences each) as a development set:
CE Cells
AM Atoms, Molecules, and States of Matter
PT Plate Tectonics
We used 11 topics (64 ideas, averaging 5.9 descrip-
tion sentences, 9.4 concept sentences and 8.6 miscon-
ception sentences each) as the test set:
5http://lucene.apache.org
6See org.apache.lucene.search.Similarity javadoc for details.
7http://assessment.aaas.org/
17
Model MAP P@1
Randomly ordered 0.607 0.607
Coverage - tokens 0.647 0.471
Coverage - tokens, synsets 0.633 0.529
Coverage - tokens, synsetsexpanded 0.650 0.471
Coverage - tokens, synsetsexpanded?4 0.690 0.706
Retrieval - tokens 0.665 0.529
Retrieval - tokens, synsets 0.641 0.471
Retrieval - tokens, synsetsexpanded 0.650 0.529
Retrieval - tokens, synsetsexpanded?4 0.684 0.647
Table 4: Development set results for identifying miscon-
ceptions.
EN Evolution and Natural Selection
BF Human Body Systems
IE Interdependence in Ecosystems
ME Matter and Energy in Living Systems
RH Reproduction, Genes, and Heredity
EG Energy: Forms, Transformation, Transfer. . .
FM Force and Motion
SC Substances, Chemical Reactions. . .
WC Weather and Climate: Basic Elements
CL Weather and Climate: Seasonal Differences
WE Weathering, Erosion, and Deposition
For the evaluation, we provide all of the idea?s de-
scription sentences as the domain knowledge, and
combine all of an idea?s concepts and misconcep-
tions into a ?student essay?8. We then ask the system
to rank the sentences in the essay, placing miscon-
ceptions above true concepts. Accuracy at placing
misconceptions at the top of the ranked list is then
measured using mean average precision (MAP) and
precision at the first item (P@1).
The models were compared to a chance baseline:
the expected MAP and P@1 if the concept and mis-
conception sentences were ordered randomly. Table 4
shows that on the development set, while all models
outperformed the random ordering baseline?s MAP
(0.607), only models with lexical expansion from
4-link WordNet chains outperformed the baseline?s
P@1 (0.607). The Coverage and Retrieval models us-
ing this expansion technique had comparable MAPs
8These ?student essays? are a naive approximation of real
essays, but the sentences are at least drawn from real student er-
rors. In the future, we hope to create an evaluation corpus where
real student essays have been annotated for misconceptions.
Model MAP P@1
Randomly ordered 0.487 0.487
Coverage - tokens, synsetsexpanded?4 0.603 0.578
Retrieval - tokens, synsetsexpanded?4 0.644 0.625
Table 5: Test set results for identifying misconceptions.
(0.690 vs. 0.684), but the Coverage model had a
higher P@1 (0.706 vs. 0.647). These top two mis-
conception identification models were evaluated on
the test set. Table 5 shows that both models again
outperformed the random ordering baseline, and the
Retrieval model outperformed the Coverage model
(0.644 vs. 0.603 MAP, 0.625 vs. 0.578 P@1).
7 Aligning misconceptions to concepts
The goal of this component is to take the miscon-
ception sentences identified in a student essay and
align them to the core science concepts identified for
the domain. For example, a student misconception
like Earth?s plates cannot bend would be aligned to
a science concept like Mountains form when plate
material slowly bends over time.
7.1 Models
The model for misconception-concept alignment
takes a similar approach to that of the Retrieval
model for misconception identification. The align-
ment model applies lexical expansion to each word
in a core science concept, indexes the expanded con-
cepts with an information retrieval system, and scores
each concept for its relevance to a student misconcep-
tion by querying the index with the misconception
and returning the index?s score for that concept. For-
mally:
SCORE(c) = SCORElucene(m, EXPAND(c))
where m is the query misconception, c is the science
concept, and EXPAND and SCORElucene are defined
as in the Retrieval model for misconception identi-
fication. The concept with the highest score is the
concept that best aligns to the student misconception
according to the model.
For lexical expansion, we consider the same defini-
tions of EXPAND as for misconception identification:
tokens; tokens, synsets; tokens, synsetsexpanded;
and tokens, synsetsexpanded?4.
18
Model MAP P@1
Randomly ordered 0.276 0.276
Alignment - Tokens 0.731 0.639
Alignment - Tokens, synsets 0.813 0.734
Alignment - tokens, synsetsexpanded 0.790 0.698
Alignment - Tokens, synsetsexpanded?4 0.762 0.639
Table 6: Development set results for aligning concepts to
misconceptions.
7.2 Evaluation
We again leverage the AAAS Science Assessments to
evaluate the misconception-concept alignment mod-
els. In addition to identifying key science ideas, and
the concepts and common misconceptions within
each idea, the AAAS Science Assessments provide
links between the misconceptions and the concepts.
Usually there is a single concept to which each mis-
conception is aligned, but the AAAS data aligns as
many as 16 concepts to a misconception in some
cases.
For the evaluation, we give the system one miscon-
ception from an idea, and the list of all concepts from
that idea, and ask the system to rank the concepts9.
If the system performs well, the concepts that are
aligned to the misconception should be ranked above
the other concepts. Accuracy at placing the aligned
concepts at the top of the ranked list is then measured
using mean average precision (MAP) and precision
at the first item (P@1).
The models were compared to a chance baseline:
the expected MAP and P@1 if the concept and mis-
conception sentences were ordered randomly. Ta-
ble 6 shows that on the development set, all models
outperformed the random ordering baseline. Lexi-
cal expansion with tokens and synsets achieved the
highest performance, 0.813 MAP and 0.734 P@1.
This model was evaluated on the test set, and Table 7
shows that the model again outperformed the random
ordering baseline, achieving 0.704 MAP and 0.611
P@1. Overall, these are promising results ? given a
student misconception, the model?s first choice for a
concept to address the misconception is helpful more
than 60% of the time.
9As discussed in Section 6.2, there are on average 9.4 con-
cepts per item. This is not too far off from the 10-20 core con-
cepts we typically expect the tutor to extract for each domain.
Model MAP P@1
Randomly ordered 0.259 0.259
Alignment - Tokens, synsets 0.704 0.611
Table 7: Test set results for aligning concepts to miscon-
ceptions.
8 Recommending resources
The goal of this component is to take a set of student
misconceptions, the core science concepts to which
each misconception is aligned, and the pedagogical
ordering of the core science concepts, and recom-
mend digital resources (webpages) to address the
most important of the misconceptions. For example,
a student that believes that water evaporates into the
air only when the air is very warm might be directed
to websites about evaporation and condensation. The
recommended resources are intended to help the stu-
dent quickly locate the concept knowledge necessary
to correct each of their misconceptions.
8.1 Models
The intuition behind our model is simple: sentences
from recommended resources should contain the
same or lexically related terminology as both the
misconception sentences and their aligned concepts.
As a first approach to this problem, we focus on the
overlap between recommended sentences and the
misconception sentences, and use an information re-
trieval approach to build a resource recommender.
First, the user gives the model a set of domain
knowledge webpages, and we use an information re-
trieval system (Lucene) to index each sentence from
each of the webpages. (Note that we index all sen-
tences, not just core concept sentences.) Given a
student misconception, we query the index and iden-
tify the source URL for each sentence that is returned.
We then return the list of the recommended URLs,
keeping only the first instance of each URL if dupli-
cates exist. Formally:
SCORE(url) = max
s?url
SCORElucene(m, s)
where url is a domain resource, s is a sentence from a
domain resource and m is the student misconception.
URLs are ranked by score and the top k URLs are
returned as recommendations.
19
8.2 Evaluation
As a preliminary evaluation of the resource recom-
mendation model, we obtained student misconcep-
tion sentences that had been aligned to concepts in
a knowledge map of plate tectonics (Ahmad, 2009).
The concepts in the knowledge map were originally
drawn from 37 domain webpages, thus each concept
could serve as a link between a student misconcep-
tion and a recommended webpage. For evaluation,
we took all 11 misconceptions for a single student,
where each misconception had been aligned through
the concepts to on average 3.4 URLs. For each mis-
conception, we asked the recommender model to
rank the 37 domain URLs in order of their relevance
to the student misconception.
We expect the final interactive essay writing sys-
tem to return up to k = 5 resources for each mis-
conception, so we evaluated the performance of the
recommender model in terms of precision at five
(P@5). That is, of the top five URLs recommended
by the system, how many were also recommended
by the experts? Averaging over the 11 student mis-
conception queries, the current model achieves P@5
of 32%, an acceptable initial baseline as randomly
recommending resources would achieve only P@5
of 9%.
9 Discussion
In this article, we have presented our initial steps
towards an interactive essay writing system that can
help students identify and remedy misconceptions in
their science knowledge. The system relies on tech-
niques drawn from a variety of areas of natural lan-
guage processing research, including multi-document
summarization, textual entailment and information
retrieval. Each component has been evaluated inde-
pendently and demonstrated promising initial perfor-
mance.
A variety of challenges remain for this effort. The
core concept identification system performs well on
the plate tectonics domain that it was originally de-
veloped for, but poorer on more distant domains,
suggesting the need for more domain-independent
features. The model for sequencing science concepts
pedagogically uses only the most basic of word-based
features, and could potentially benefit from features
drawn from other research areas such as text readabil-
ity. The misconception identification and alignment
models perform well on the AAAS science assess-
ments but have not yet been evaluated on real student
essays, which may require moving from lexical cover-
age models to more sophisticated entailment models.
Finally, the recommender model considers only in-
formation about the misconception sentence (not the
aligned core concept nor the pedagogical ordering of
concepts) and recommends entire resources instead
of directing students to specifically relevant sentences
or paragraphs.
Perhaps the most important challenge for this work
will be moving from evaluating the components in-
dependently to a whole-system evaluation in the con-
text of a real essay writing task. We are currently
designing a study to gather data on students using the
system, from which we hope to derive information
about which components are most reliable or useful
to the students. This information will help guide our
research to focus on improving the components that
yield the greatest benefits to the students.
References
[Ahmad2009] Faisal Ahmad. 2009. Generating conceptu-
ally personalized interactions for educational digital
libraries using concept maps. Ph.D. thesis, University
of Colorado at Boulder.
[Azevedo et al2008] Roger Azevedo, Amy Witherspoon,
Arthur Graesser, Danielle McNamara, Vasile Rus,
Zhiqiang Cai, Mihai Lintean, and Emily Siler. 2008.
MetaTutor: An adaptive hypermedia system for train-
ing and fostering self-regulated learning about complex
science topics. In Meeting of Society for Computers in
Psychology, November.
[Dagan et al2006] Ido Dagan, Oren Glickman, and Ber-
nardo Magnini. 2006. The PASCAL recognising
textual entailment challenge. In Joaquin Quin?onero
Candela, Ido Dagan, Bernardo Magnini, and Florence
d?Alche? Buc, editors, Machine Learning Challenges.
Evaluating Predictive Uncertainty, Visual Object Clas-
sification, and Recognising Tectual Entailment, volume
3944 of Lecture Notes in Computer Science, pages 177?
190. Springer Berlin / Heidelberg.
[de la Chica et al2008a] Sebastian de la Chica, Faisal Ah-
mad, James H. Martin, and Tamara Sumner. 2008a.
Pedagogically useful extractive summaries for science
education. In Proceedings of the 22nd International
Conference on Computational Linguistics - Volume 1,
COLING ?08, pages 177?184, Stroudsburg, PA, USA.
Association for Computational Linguistics.
20
[de la Chica et al2008b] Sebastian de la Chica, Faisal Ah-
mad, Tamara Sumner, James H. Martin, and Kirsten
Butcher. 2008b. Computational foundations for person-
alizing instruction with digital libraries. International
Journal on Digital Libraries, 9(1):3?18, July.
[Graesser et al2004] Arthur Graesser, Shulan Lu, George
Jackson, Heather Mitchell, Mathew Ventura, Andrew
Olney, and Max Louwerse. 2004. AutoTutor: A tutor
with dialogue in natural language. Behavior Research
Methods, 36:180?192.
[Graff2002] David Graff. 2002. English Gigaword. Lin-
guistic Data Consortium.
[NRC1996] National Research Council NRC. 1996.
National Science Education Standards. National
Academy Press, Washington DC.
[Project2061.1993] Project2061. 1993. Benchmarks for
Science Literacy. Oxford University Press, New York,
United States.
[Radev et al2004] Dragomir R. Radev, Hongyan Jing,
Ma?gorzata Stys?, and Daniel Tam. 2004. Centroid-
based summarization of multiple documents. Inf. Pro-
cess. Manage., 40(6):919?938, November.
[Shnarch et al2011] Eyal Shnarch, Jacob Goldberger, and
Ido Dagan. 2011. A probabilistic modeling frame-
work for lexical entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 558?563, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
[Tanaka-Ishii et al2010] K. Tanaka-Ishii, S. Tezuka, and
H. Terada. 2010. Sorting texts by readability. Compu-
tational Linguistics, 36(2):203?227.
[VanLehn et al2002] Kurt VanLehn, Pamela Jordan, Car-
olyn Rose?, Dumisizwe Bhembe, Michael Bo?ttner, Andy
Gaydos, Maxim Makatchev, Umarani Pappuswamy,
Michael Ringenberg, Antonio Roque, Stephanie Siler,
and Ramesh Srivastava. 2002. The architecture of
Why2-Atlas: A coach for qualitative physics essay writ-
ing. In Stefano Cerri, Guy Gouarde`res, and Fa?bio
Paraguac?u, editors, Intelligent Tutoring Systems, vol-
ume 2363 of Lecture Notes in Computer Science, pages
158?167. Springer Berlin / Heidelberg.
[Wade-Stein and Kintsch2004] David Wade-Stein and
Eileen Kintsch. 2004. Summary Street: Interactive
computer support for writing. Cognition and Instruc-
tion, 22(3):333?362.
21
