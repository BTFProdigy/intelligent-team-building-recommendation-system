Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 466?474,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Multimodal Subjectivity Analysis of Multiparty Conversation
Stephan Raaijmakers
TNO Information and
Communication Technology
Delft, The Netherlands
stephan.raaijmakers@tno.nl
Khiet Truong
TNO Defense, Security and Safety
Soesterberg, The Netherlands
khiet.truong@tno.nl
Theresa Wilson
School of Informatics
University of Edinburgh
Edingburgh, UK
twilson@inf.ed.ac.uk
Abstract
We investigate the combination of several
sources of information for the purpose of sub-
jectivity recognition and polarity classification
in meetings. We focus on features from two
modalities, transcribed words and acoustics,
and we compare the performance of three dif-
ferent textual representations: words, charac-
ters, and phonemes. Our experiments show
that character-level features outperform word-
level features for these tasks, and that a care-
ful fusion of all features yields the best perfor-
mance. 1
1 Introduction
Opinions, sentiments and other types of subjective
content are an important part of any meeting. Meet-
ing participants express pros and cons about ideas,
they support or oppose decisions, and they make
suggestions that may or may not be adopted. When
recorded and archived, meetings become a part of
the organizational knowledge, but their value is lim-
ited by the ability of tools to search and summa-
rize meeting content, including subjective content.
While progress has been made on recognizing pri-
marily objective meeting content, for example, in-
formation about the topics that are discussed (Hsueh
and Moore, 2006) and who is assigned to work on
given tasks (Purver et al, 2006), there has been
1This work was supported by the Dutch BSIK-project Mul-
timediaN, and the European IST Programme Project FP6-
0033812. This paper only reflects the authors? views and fund-
ing agencies are not liable for any use that may be made of the
information contained herein.
fairly little work specifically directed toward recog-
nizing subjective content.
In contrast, there has been a wealth of research
over the past several years on automatic subjectiv-
ity and sentiment analysis in text, including on-line
media. Partly inspired by the rapid growth of so-
cial media, such as blogs, as well as on-line news
and reviews, researchers are now actively address-
ing a wide variety of new tasks, ranging from blog
mining (e.g., finding opinion leaders in an on-line
community), to reputation management (e.g. find-
ing negative opinions about a company on the web),
to opinion-oriented summarization and question an-
swering. Yet many challenges remain, including
how best to represent and combine linguistic infor-
mation for subjectivity analysis. With the additional
modalities that are present when working with face-
to-face spoken communication, these challenges are
even more pronounced.
The work in this paper focuses on two tasks: (1)
recognizing subjective utterances and (2) discrimi-
nating between positive and negative subjective ut-
terances. An utterance may be subjective because
the speaker is expressing an opinion, because the
speaker is discussing someone else?s opinion, or be-
cause the speaker is eliciting the opinion of someone
else with a question.
We approach the above tasks as supervised ma-
chine learning problems, with the specific goal of
finding answers to the following research questions:
? Given a variety of information sources, such
as text arising from (transcribed) speech,
phoneme representations of the words in an ut-
terance, and acoustic features extracted from
466
the audio layer, which of these sources are par-
ticularly valuable for subjectivity analysis in
multiparty conversation?
? Does the combination of these sources lead to
further improvement?
? What are the optimal representations of these
information sources in terms of feature design
for a machine learning component?
A central tenet of our approach is that subword
representations, such as character and phoneme n-
grams, are beneficial for the tasks at hand.
2 Subword Features
Previous work has demonstrated that textual units
below the word level, such as character n-grams,
are valuable sources of information for various
text classification tasks. An example of character
n-grams is the set of 3-grams {#se, sen, ent,
nti, tim, ime, men, ent, nt#, t#a,
#an, ana, nal, aly, lys, ysi, sis,
is#} for the two-word phrase sentiment analysis.
The special symbol # represents a word boundary.
While it is not directly obvious that there is much
information in these truncated substrings, character
n-grams have successfully been used for fine-
grained classification tasks, such as named-entity
recognition (Klein et al, 2003) and subjective
sentence recognition (Raaijmakers and Kraaij,
2008), as well as a variety of document-level tasks
(Stamatatos, 2006; Zhang and Lee, 2006; Kanaris
and Stamatatos, 2007).
The informativeness of these low-level features
comes in part from a form of attenuation (Eisner,
1996): a slight abstraction of the underlying data
that leads to the formation of string equivalence
classes. For instance, words in a sentence will in-
variably share many character n-grams. Since ev-
ery unique character n-gram in an utterance consti-
tutes a separate feature, this leads to the formation
of string classes, which is a form of abstraction. For
example, Zhang and Lee (2006) investigate similar
subword representations, called key substring group
features. By compressing substrings in a corpus in a
trie (a prefix tree), and labeling entire sets of distri-
butionally equivalent substrings with one group la-
bel, an attenuation effect is obtained that proves very
beneficial for a number of text classification tasks.
Aside from attenuation effects, character n-
grams, especially those that represent word bound-
aries, have additional benefits. Treating word
boundaries as characters captures micro-phrasal in-
formation: short strings that express the transition
of one word to another. Stemming occurs naturally
within the set of initial character n-grams of a word,
where the suffix is left out. Also, some part-of-
speech information is captured. For example, the
modals could, would, should can be represented by
the 4-gram, ould, and the set of adverbs ending in
-ly can be represented by the 3-gram ly#.
A challenging thought is to extend the use of n-
grams to the level of phonemes, which comprise
the first symbolic level in the process of sound to
grapheme conversion. If n-grams of phonemes com-
pare favorably to word n-grams for the purpose of
sentiment classification, then significant speedups
can be obtained for online sentiment classification,
since tokenization of the raw speech signal can make
a halt at the phoneme level.
3 Data
For this work we use 13 meetings from the AMI
Meeting Corpus (Carletta et al, 2005). Each meet-
ing has four participants and is approximately 30
minutes long. The participants play specific roles
(e.g., Project Manager, Marketing Expert) and to-
gether function as a design team. Within the set of
13 meetings, there are a total of 20 participants, with
each participant taking part in two or three meet-
ings as part of the same design team. Meetings with
the same set of participants represent different stages
in the design process (e.g., Conceptual Design, De-
tailed Design).
The meetings used in the experiments have been
annotated for subjective content using the AMIDA
annotation scheme (Wilson, 2008). Table 1 lists
the types of annotations that are marked in the data.
There are three main categories of annotations, sub-
jective utterances, subjective questions, and objec-
tive polar utterances. A subjective utterance is a
span of words (or possibly sounds) where a pri-
vate state is being expressed either through choice of
words or prosody. A private state (Quirk et al, 1985)
467
is an internal mental or emotional state, including
opinions, beliefs, sentiments, emotions, evaluations,
uncertainties, and speculations, among others. Al-
though typically when a private state is expressed
it is the private state of the speaker, as in example
(1) below, an utterance may also be subjective be-
cause the speaker is talking about the private state
of someone else. For example, in (2) the negative
opinion attributed to the company is what makes the
utterance subjective.
(1) Finding them is really a pain, you know
(2) The company?s decided that teletext is out-
dated
Subjective questions are questions in which the
speaker is eliciting the private state of someone else.
In other words, the speaker is asking about what
someone else thinks, feels, wants, likes, etc., and the
speaker is expecting a response in which the other
person expresses what he or she thinks, feels, wants,
or likes. For example, both (3) and (4) below are
subjective questions.
(3) Do you like the large buttons?
(4) What do you think about the large buttons?
Objective polar utterances are statements or phrases
that describe positive or negative factual information
about something without conveying a private state.
The sentence The camera broke the first time I used
it gives an example of negative factual information;
generally, something breaking the first time it is used
is not good.
For the work in this paper, we focus on recog-
nizing subjectivity in general and distinguishing be-
tween positive and negative subjective utterances.
Positive subjective utterances are those in which any
of the following types of private states are expressed:
agreements, positive sentiments, positive sugges-
tions, arguing for something, beliefs from which
positive sentiments can be inferred, and positive re-
sponses to subjective questions. Negative subjective
utterances express private states that are the oppo-
site of those represented by the positive subjective
category: disagreements, negative sentiments, nega-
tive suggestions, arguing against something, beliefs
from which negative sentiments can be inferred, and
negative responses to subjective questions. Example
(5) below contains two positive subjective utterances
Table 1: AMIDA Subjectivity Annotation Types
Subjective Utterances
positive subjective
negative subjective
positive and negative subjective
uncertainty
other subjective
subjective fragment
Subjective Questions
positive subjective question
negative subjective question
general subjective question
Objective Polar Utterances
positive objective
negative objective
and one negative subjective utterance. Each annota-
tion is indicated by a pair of angle brackets.
(5) Um ?POS-SUBJ it?s very easy to use?.
Um ?NEG-SUBJ but unfortunately it does
lack the advanced functions? ?POS-SUBJ
which I I quite like having on the controls?.
The positive and negative subjective category is for
marking cases of positive and negative subjectivity
that are so closely interconnected that it is difficult
or impossible to separate the two. For example, (6)
below is marked as both positive and negative sub-
jective.
(6) Um ?POS-AND-NEG-SUBJ they?ve also
suggested that we um we only use the remote
control to control the television, not the VCR,
DVD or anything else?.
In (Wilson, 2008), agreement is measured for each
class separately at the level of dialogue act segments.
If a dialogue act overlaps with an annotation of a
particular type, then the segment is considered to
be labelled with that type. Table 2 gives the Kappa
(Cohen, 1960) and % agreement for subjective seg-
ments, positive and negative subjective segments,2
and subjective questions.
2A positive subjective segment is any dialogue act segment
that overlaps with a positive subjective utterance or a positive-
and-negative subjective utterance. The negative subjective seg-
ments are defined similarly.
468
Table 2: Interannotator agreement for the AMIDA sub-
jectivity annotations
Kappa % Agree
Subjective 0.56 79
Pos Subjective 0.58 84
Neg Subjective 0.62 92
Subjective Question 0.56 95
4 Experiments
We conduct two sets of classification experiments.
For the first set of experiments (Task 1), we auto-
matically distinguish between subjective and non-
subjective utterances. For the second set of ex-
periments (Task 2), we focus on distinguishing be-
tween positive and negative subjective utterances.
For both tasks, we use the manual dialogue act seg-
ments available as part of the AMI Corpus as the unit
of classification. For Task 1, a segment is considered
subjective if it overlaps with either a subjective utter-
ance or subjective question annotation. For Task 2,
the segments being classified are those that overlap
with positive or negative subjective utterances. For
this task, we exclude segments that are both positive
and negative. Although limiting the set of segments
to be classified to just those that are positive or nega-
tive makes the task somewhat artificial, it also allows
us to focus in on the performance of features specifi-
cally for this task.3 We use 6226 subjective and 8707
non-subjective dialog acts for Task 1 (with an aver-
age duration of 1.9s, standard deviation of 2.0s), and
3157 positive subjective and 1052 negative subjec-
tive dialog acts for Task 2 (average duration of 2.6s,
standard deviation of 2.3s).
The experiments are performed using 13-fold
cross validation. Each meeting constitutes a separate
fold for testing, e.g., all the segments from meeting 1
make up the test set for fold 1. Then, for a given fold,
the segments from the remaining 12 meetings are
used for training and parameter tuning, with roughly
a 85%, 7%, and 8% split between training, tuning,
and testing sets for each fold. The assignment to
training versus tuning set was random, with the only
constraint being that a segment could only be in the
tuning set for one fold of the data.
3In practice, this excludes about 7% of the positive/negative
segments.
The experiments we perform involve two steps.
First, we train and optimize a classifier for each type
of feature using BoosTexter (Schapire and Singer,
2000) AdaBoost.MH. Then, we investigate the per-
formance of all possible combinations of features
using linear combinations of the individual feature
classifiers.
4.1 Features
The two modalities that are investigated, prosodic,
and textual, are represented by four different
sets of features: prosody (PROS), word n-
grams (WORDS), character n-grams (CHARS), and
phoneme n-grams (PHONES).
Based on previous research on prosody modelling
in a meeting context (Wrede and Shriberg, 2003)
and on the literature in emotion research (Banse and
Scherer, 1996) we extract PROS features that are
mainly based on pitch, energy and the distribution of
energy in the long-term averaged spectrum (LTAS)
(see Table 3). These features are extracted at the
word level and aggregated to the dialogue-act level
by taking the average over the words per dialogue
act. We then normalize the features per speaker per
meeting by converting the raw feature values to z-
scores (z = (x ? ?)/?).
Table 3: Prosodic features used in experiments.
pitch mean, standard deviation, min-
imum, maximum, range, mean
absolute slope
intensity (en-
ergy)
mean, standard deviation, min-
imum, maximum, range, RMS
energy
distribution en-
ergy in LTAS
slope, Hammerberg index, cen-
tre of gravity, skewness
The textual features, WORDS and CHARS, and
the PHONES features are based on a manual tran-
scription of the speech. The PHONES were pro-
duced through dictionary lookup on the words in the
reference transcription. Both CHARS and PHONES
representations include word boundaries as informa-
tive tokens. The textual features for a given seg-
ment are simply all the WORDS/CHARS/PHONES
in that segment. Selection of n-grams is performed
by the learning algorithm.
469
4.2 Single Source Classifiers
We train four single source classifiers using BoosT-
exter, one for each type of feature. For the WORDS,
CHARS, and PHONES, we optimize the classi-
fier by performing a grid search over the parame-
ter space, varying the number of rounds of boosting
(100, 500, 1000, 2000, 5000), the length of the n-
gram (1, 2, 3, 4, 5), and the type of n-gram. Boos-
Texter can be run with three different n-gram con-
figurations: n-gram, s-gram, and f -gram. For the
default configuration (n-gram), BoosTexter searches
for n-grams up to length n. For example, if n = 3,
BoosTexter will consider 1-grams, 2-grams, and 3-
grams. For the s-gram configuration, BoosTexter
will in addition consider sparse n-grams (i.e., n-
grams containing wildcards), such as the * idea. For
the f -gram configuration, BoosTexter will only con-
sider n-grams of a maximum fixed length, e.g., if
n = 3 BoosTexter will only consider 3-grams. For
the PROS classifier, only the number of rounds of
boosting was varied. The parameters are selected
for each fold separately; the parameter set that pro-
duces the highest subjective F1 score on the tuning
set for Task 1, and the highest positive subjective F1
score for Task 2, is used to train the final classifier
for that fold.
4.3 Classifier combination
After the single source classifiers have been trained,
they have to be combined into an aggregate classi-
fier. To this end, we decided to apply a simple linear
interpolation strategy. Linear interpolation of mod-
els is the weighted combination of simple models to
form complex models, and has its roots in generative
language models (Jelinek and Mercer, 1980). (Raai-
jmakers, 2007) has demonstrated its use for discrim-
inative machine learning.
In the present binary class setting, BoosTexter
produces two decision values, one for every class.
For every individual single-source classifier (i.e.,
PROS, WORDS, CHARS and PHONES), separate
weights are estimated that are applied to the decision
values for the two classes produced by these classi-
fiers. These weights express the relative importance
of the single-source classifiers.
The prediction of an aggregate classifier for a
class c is then simply the sum of all weights for
all participating single-source classifiers applied to
the decision values these classifiers produce for this
class. The class with the maximum score wins, just
as in the simple non-aggregate case.
Formally, then, this linear interpolation strategy
finds for n single-source classifiers n interpolation
weights ?1, . . . ?n that minimize the empirical loss
(measured by a loss function L), with ?j the weight
of classifier j (? ? [0, 1]), and C jc (xi) the decision
value of class c produced by classifier j for datum xi
(a feature vector). The two classes are denoted with
0, 1. The true class for datum xi is denoted with x?i.
The loss function is in our case based on subjective
F-measure (Task 1) or positive subjective F-measure
(Task 2) measured on heldout development training
and test data.
The aggregate prediction x?i for datum xi on the
basis of n single-source classifiers then becomes
x?i = arg maxc (
n
?
j=1
?j ? Cjc=0(xi),
n
?
j=1
?j ? Cjc=1(xi))
(1)
and the lambdas are defined as
?nj = arg min?nj ?[0,1]
k
?
i
L(x?i, x?i;?j , . . . , ?n) (2)
The search process for these weights can easily be
implemented with a simple grid search over admis-
sible ranges.
In the experiments described below, we investi-
gate all possible combinations of the four differ-
ent sets of features (PROS, WORDS, CHARS, and
PHONES) to determine which combination yields
the best performance for subjectivity and subjective
polarity recognition.
5 Results and Discussion
Results for the two tasks are given in Tables 4 and 5
and in Figures 1 and 2. We use two baselines, listed
at the top of each table. The bullets in a given row
indicate the features that are being evaluated for a
given experiment. In Table 4, subjective F1, recall,
and precision are reported as well as overall accu-
racy. In Table 4, the F1, recall, and precision scores
are for the positive subjective class. All values in the
tables are averages over the 13 folds.
470
Table 4: Results Task 1: Subjective vs. Non-Subjective.
PROS WORDS CHARS PHONES F1 PREC REC ACC
BASE-SUBJ always chooses subjective class 60.3 43.4 100 43.4
BASE-RAND randomly chooses a class based on priors 41.8 42.9 41.3 50.6
single
? 54.6 55.3 54.5 63.1
? 60.5 68.5 54.5 71.0
? 61.7 67.5 57.2 71.1
? 60.3 66.4 55.5 70.2
double
? ? 63.9 72.1 57.6 73.4
? ? 65.6 71.9 60.3 74.0
? ? 64.6 72.3 58.4 73.7
? ? 66.2 73.8 60.1 74.9
? ? 65.2 73.2 58.8 74.3
? ? 66.1 72.8 60.7 74.5
triple
? ? ? 66.5 74.3 60.3 75.1
? ? ? 65.5 73.5 59.0 74.5
? ? ? 66.5 73.3 60.8 74.8
? ? ? 66.9 74.3 60.9 75.3
quartet ? ? ? ? 67.1 74.5 61.2 75.4
Table 5: Results Task 2: Positive Subjective vs. Negative Subjective.
PROS WORDS CHARS PHONES F1 PREC REC ACC
BASE-POS-SUBJ always chooses positive subjective class 85.6 75.0 100 75.0
BASE-RAND randomly chooses a class based on priors 75.1 74.4 76.1 62.4
single
? 84.8 74.8 98.1 73.9
? 85.6 79.6 93.1 76.8
? 85.9 81.9 90.5 78.0
? 85.5 80.5 91.3 77.0
double
? ? 88.7 83.0 95.4 81.9
? ? 88.7 83.1 95.1 81.8
? ? 88.5 83.3 94.4 81.6
? ? 89.5 84.2 95.7 83.3
? ? 89.2 83.7 95.5 82.8
? ? 89.0 84.2 94.6 82.6
triple
? ? ? 89.6 84.0 96.1 83.4
? ? ? 89.3 83.6 95.8 82.8
? ? ? 89.2 83.7 95.5 82.7
? ? ? 89.8 84.4 96.0 83.8
quartet ? ? ? ? 89.9 84.4 96.2 83.8
It is quite obvious that the combination of differ-
ent sources of information is beneficial, and in gen-
eral, the more information the better the results. The
best performing classifier for Task 1 uses all the fea-
tures, achieving a subjective F1 of 67.1. For Task 2,
the best performing classifier also uses all the fea-
tures, although it does not perform significantly bet-
ter than the classifier using only WORDS, CHARS,
and PHONES.4 This classifier achieves a positive-
subjective F1 of 89.9.
We measured the effects of adding more infor-
mation to the single source classifiers. These re-
sults are listed in Table 6. Of the various feature
types, prosody seems to be the least informative for
both subjectivity and polarity classification. In ad-
dition to producing the single-source classifier with
the lowest performance for both tasks, Table 6 shows
that when prosody is added, of all the features it is
least likely to yield significant improvements.
4We measured significance with the non-parametric
Wilcoxon signed rank test, p < 0.05.
Throughout the experiments, adding an additional
type of textual feature always yields higher results.
In all cases but two, these improvements are sig-
nificant. The best performing of the features are
the character n-grams. Of the single-source exper-
iments, the character n-grams achieve the best per-
formance, with significant improvements in F1 over
the other single-source classifiers for both Task 1
and Task 2. Also, adding character n-grams to other
feature combinations always gives significant im-
provements in performance.
An obvious question that remains is what the ef-
fect is of classifier interpolation on the results. To
answer this question, we conducted two additional
experiments for both tasks. First, we investigated
the performance of an uninterpolated combination
of the four single-source classifiers. In essence, this
combines the separate feature spaces without explic-
itly weighting them. Second, we investigated the re-
sults of training a single BoosTexter model using all
the features, essentially merging all feature spaces
471
Table 6: Addition of features separately (for Task 1 and 2): ?+? for a row-column pair (r, c) means that the addition
of column feature c to the row features r significantly improved r?s F1; ?-? indicates no significant improvement; ?X?
means ?not applicable?
+PROS + WORDS +CHARS +PHONES
Task 1 2 1 2 1 2 1 2
PROS X X + + + + + +
WORDS - + X X + + + +
CHARS - + - + X X - +
PHONES - + + + + + X X
PROS+WORDS X X X X + + + +
PROS+CHARS X X + + X X + +
PROS+PHONES X X + + + + X X
WORDS+CHARS + - X X X X + +
WORDS+PHONES + - X X + + X X
CHARS+PHONES + + + + X X X X
PROS+WORDS+CHARS X X X X X X + +
PROS+WORDS+PHONES X X X X + + X X
PROS+CHARS+PHONES X X + + X X X X
WORDS+CHARS+PHONES + - X X X X X X
F1
 m
ea
su
re
m
ajo
rity
ra
ndpro
s
wo
rds
ch
ars
ph
on
es
pro
s+
wo
rds
pro
s+
ch
ars
pro
s+
ph
on
es
wo
rds
+c
ha
rs
wo
rds
+p
ho
ne
s
ch
ars
+p
ho
ne
s
pro
s+
wo
rds
+c
ha
rs
pro
s+
wo
rds
+p
ho
ne
s
pro
s+
ph
on
es
+c
ha
rs
wo
rds
+c
ha
rs+
ph
on
es
pro
s+
wo
rds
+c
ha
rs+
ph
on
es
40
45
50
55
60
65
70
Figure 1: Results (F1) experiment 1: subjective vs. non-
subjective.
into one agglomerate feature space. The results for
these experiments are given in Table 7, along with
the results from the all-feature interpolated classifi-
cation for comparison.
The results in Table 7 show that interpolation
outperforms both the unweighted and single-model
combinations for both tasks. For Task 1, the ef-
fect of interpolation compared to a single model is
marginal (a .03 point difference in F1). However,
compared to the uninterpolated combination, inter-
polation gives a clear 3.1 points improvement of F1.
For Task 2, interpolation outperforms both the unin-
terpolated and single-model classifiers, with 2 and 3
points improvements in F1, respectively.
F1
 m
ea
su
re
m
ajo
rity
ra
ndpro
s
wo
rds
ch
ars
ph
on
es
pro
s+
wo
rds
pro
s+
ch
ars
pro
s+
ph
on
es
wo
rds
+c
ha
rs
wo
rds
+p
ho
ne
s
ch
ars
+p
ho
ne
s
pro
s+
wo
rds
+c
ha
rs
pro
s+
wo
rds
+p
ho
ne
s
pro
s+
ph
on
es
+c
ha
rs
wo
rds
+c
ha
rs+
ph
on
es
pro
s+
wo
rds
+c
ha
rs+
ph
on
es
70
75
80
85
90
95
10
0
Figure 2: Results (F1) experiment 2: positive subjective
vs. negative subjective.
6 Related Work
Previous work has demonstrated that textual units
below the word level, such as character n-grams,
are valuable sources of information. Character-
level models have successfully been used for named-
entity recognition (Klein et al, 2003), predicting
authorship (Keselj et al, 2003; Stamatatos, 2006),
text categorization (Zhang and Lee, 2006), web page
genre identification (Kanaris and Stamatatos, 2007),
and sentence-level subjectivity recognition (Raaij-
makers and Kraaij, 2008) In spoken-language data,
Hsueh (2008) achieves good results using chains
of phonemes to automatically segment meetings ac-
cording to topic. However, to the best of our knowl-
edge there has been no investigation to date on the
472
Table 7: Results of interpolated classifiers compared to
uninterpolated and single-model classifiers for all fea-
tures.
Task Combination ACC REC PREC F1
1
interpolated 75.4 61.2 74.5 67.1
uninterpolated 73.0 58.7 70.6 64.0
single model 74.7 62.1 72.7 66.8
2
interpolated 83.8 96.2 84.4 89.9
uninterpolated 79.8 98.0 79.7 87.9
single model 79.5 91.0 83.3 86.9
combination of character-level, phoneme-level, and
word-level models for any natural language classifi-
cation tasks.
In text, there has been a significant amount of
research on subjectivity and sentiment recognition,
ranging from work at the phrase level to work on
classifying sentences and documents. Sentence-
level subjectivity classification (e.g., (Riloff and
Wiebe, 2003; Yu and Hatzivassiloglou, 2003)) and
sentiment classification (e.g., (Yu and Hatzivas-
siloglou, 2003; Kim and Hovy, 2004; Hu and Liu,
2004; Popescu and Etzioni, 2005)) is the research
in text most closely related to our work. Of the
sentence-level research, the most similar is work
by Raaijmakers and Kraaij (2008) comparing word-
spanning character n-grams to word-internal char-
acter n-grams for subjectivity classification in news
data. They found that character n-grams spanning
words perform the best.
Research on recognizing subjective content in
multiparty conversation includes work by Somasun-
daran et al (2007) on recognizing sentiments and
arguing in meetings, work by Neiberg el al. (2006)
on recognizing positive, negative, and neutral emo-
tions in meetings, work on recognizing agreements
and disagreements in meetings (Hillard et al, 2003;
Galley et al, 2004; Hahn et al, 2006), and work
by Wrede and Shriberg (2003) on recognizing meet-
ing hotspots. Somasundaran et al use lexical and
discourse features to recognize sentences and turns
where meeting participants express sentiments or ar-
guing. They also use the AMI corpus in their work;
however, the use of different annotations and task
definitions makes it impossible to directly compare
their results and ours. Neiberg et al use acoustic?
prosodic features (Mel-frequency Cepstral Coeffi-
cients (MFCCs) and pitch features) and lexical n-
grams for recognizing emotions in the ISL Meeting
Corpus (Laskowski and Burger, 2006).
Agreements and disagreements are a subset of the
private states represented by the positive and neg-
ative subjective categories used in this work. To
recognise agreements and disagreements automati-
cally, Hillard et al train 3-way decision tree clas-
sifiers (agreement, disagreement, other) using both
word-based and prosodic features. Galley et al
model this task as a sequence tagging problem, and
investigate whether features capturing speaker inter-
actions are useful for recognizing agreements and
disagreements. Hahn et al investigate the use of
contrast classifiers (Peng et al, 2003) for the task,
using only lexical features.
Hotspots are places in a meeting in which the par-
ticipants are highly involved in the discussion. Al-
though high involvement does not necessarily equate
subjective content, in practice, we expect more sen-
timents, opinions, and arguments to be expressed
when participants are highly involved in the discus-
sion. In their work on recognizing meeting hotspots,
Wrede and Shriberg focus on evaluating the contri-
bution of various prosodic features, ignoring lexi-
cal features completely. The results of their study
helped to inform our choice of prosodic features for
the experiments in this paper.
7 Conclusions
In this paper, we investigated the use of prosodic
features, word n-grams, character n-grams, and
phoneme n-grams for subjectivity recognition and
polarity classification of dialog acts in multiparty
conversation. We show that character n-grams
outperform prosodic features, word n-grams and
phoneme n-grams in subjectiviy recognition and po-
larity classification. Combining these features sig-
nificantly improves performance. Comparing the
additive value of the four information sources avail-
able, prosodic information seem to be least in-
formative while character-level information indeed
proves to be a very valuable source. For subjectiv-
ity recognition, a combination of prosodic, word-
level, character-level, and phoneme-level informa-
tion yields the best performance. For polarity clas-
sification, the best performance is achieved with a
473
combination of words, characters and phonemes.
References
R. Banse and K. R. Scherer. 1996. Acoustic profiles in
vocal emotion expression. Journal of Personality and
Social Psychology, pages 614?636.
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guille-
mot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij,
M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska,
I. McCowan, W. Post, D. Reidsma, and P. Wellner.
2005. The AMI meeting corpus. In Proceedings of
the Measuring Behavior Symposium on ?Annotating
and Measuring Meeting Behavior?.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37?46.
J. Eisner. 1996. An empirical comparison of probability
models for dependency grammar. In Technical Report
IRCS-96-11, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania.
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: Use of bayesian networks to model
pragmatic dependencies. In Proceedings of ACL.
S. Hahn, R. Ladner, and M. Ostendorf. 2006. Agree-
ment/disagreement classification: Exploiting unla-
beled data using contrast classifiers. In Proceedings
of HLT/NAACL.
D. Hillard, M. Ostendorf, and E. Shriberg. 2003. De-
tection of agreement vs. disagreement in meetings:
Training with unlabeled data. In Proceedings of
HLT/NAACL.
P. Hsueh and J. Moore. 2006. Automatic topic seg-
mentation and lablelling in multiparty dialogue. In
Proceedings of IEEE/ACM Workshop on Spoken Lan-
guage Technology.
P. Hsueh. 2008. Audio-based unsupervised segmentation
of meeting dialogue. In Proceedings of ICASSP.
M. Hu and B. Liu. 2004. Mining and summarizing cus-
tomer reviews. In Proceedings of KDD.
F. Jelinek and R. L. Mercer. 1980. Interpolated estima-
tion of Markov source parameters from sparse data.
In Proceedings, Workshop on Pattern Recognition in
Practice, pages 381?397.
I. Kanaris and E. Stamatatos. 2007. Webpage genre iden-
tification using variable-length character n-grams. In
Proceedings of ICTAI.
V. Keselj, F. Peng, N. Cercone, and C. Thomas. 2003. N-
gram-based author profiles for authorship attribution.
In Proceedings of PACLING.
S. Kim and Eduard Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of Coling.
D. Klein, J. Smarr, H. Nguyen, and C.D. Manning. 2003.
Named entity recognition with character-level models.
In Proceedings of CoNLL.
K. Laskowski and S. Burger. 2006. Annotation and anal-
ysis of emotionally relevant behavior in the ISL meet-
ing corpus. In Proceedings of LREC 2006.
D. Neiberg, K. Elenius, and K. Laskowski. 2006. Emo-
tion recognition in spontaneous speech using GMMs.
In Proceedings of INTERSPEECH.
K. Peng, S. Vucetic, B. Han, H. Xie, and Z Obradovic.
2003. Exploiting unlabeled data for improving ac-
curacy of predictive data mining. In Proceedings of
ICDM.
A. Popescu and O. Etzioni. 2005. Extracting product
features and opinions from reviews. In Proceedings of
HLT/EMNLP.
M. Purver, P. Ehlen, and J. Niekrasz. 2006. Detecting
action items in multi-party meetings: Annotation and
initial experiments. In Proceedings of MLMI.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman, New York.
S. Raaijmakers and W. Kraaij. 2008. A shallow ap-
proach to subjectivity classification. In Proceedings
of ICWSM.
S. Raaijmakers. 2007. Sentiment classification with in-
terpolated information diffusion kernels. In Proceed-
ings of the First International Workshop on Data Min-
ing and Audience Intelligence for Advertising (AD-
KDD?07).
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
EMNLP.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A
boosting-based system for text categorization. Ma-
chine Learning, 39(2/3):135?168.
S. Somasundaran, J. Ruppenhofer, and J. Wiebe. 2007.
Detecting arguing and sentiment in meetings. In Pro-
ceedings of SIGdial.
E. Stamatatos. 2006. Ensemble-based author identifica-
tion using character n-grams. In Proceedings of TIR.
T. Wilson. 2008. Annotating subjective content in meet-
ings. In Proceedings of LREC.
B. Wrede and E. Shriberg. 2003. Spotting ?hot spots?
in meetings: Human judgments and prosodic cues. In
Proceedings of EUROSPEECH.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
Proceedings of EMNLP.
D. Zhang and W. S. Lee. 2006. Extracting key-substring-
group features for text classification. In Proceedings
of KDD.
474
In: Proceedings of CoNLL-2000 and LLL-2000, pages 55-60, Lisbon, Portugal, 2000. 
Learning Distributed Linguistic Classes 
Stephan Raa i jmakers  
Netherlands Organisation for Applied Scientific Research (TNO) 
Inst i tute for Applied Physics 
Delft 
The Netherlands 
raaijmakers@tpd, tno. nl 
Abst rac t  
Error-correcting output codes (ECOC) have 
emerged in machine learning as a success- 
ful implementation of the idea of distributed 
classes. Monadic class symbols are replaced 
by bit strings, which are learned by an ensem- 
ble of binary-valued classifiers (dichotomizers). 
In this study, the idea of ECOC is applied to 
memory-based language learning with local (k- 
nearest neighbor) classifiers. Regression analy- 
sis of the experimental results reveals that, in 
order for ECOC to be successful for language 
learning, the use of the Modified Value Differ- 
ence Metric (MVDM) is an important factor, 
which is explained in terms of population den- 
sity of the class hyperspace. 
1 In t roduct ion  
Supervised learning methods applied to natu- 
ral language classification tasks commonly op- 
erate on high-level symbolic representations, 
with linguistic lasses that are usually monadic, 
without internal structure (Daelemans et al, 
1996; Cardie et al, 1999; Roth, 1998). This 
contrasts with the distributed class encoding 
commonly found in neural networks (Schmid, 
1994). Error-correcting output codes (ECOC) 
have been introduced to machine learning as 
a principled and successful approach to dis- 
tributed class encoding (Dietterich and Bakiri, 
1995; Ricci and Aha, 1997; Berger, 1999). With 
ECOC, monadic classes are replaced by code- 
words, i.e. binary-valued vectors. An ensem- 
ble of separate classifiers (dichotomizers) must 
be trained to learn the binary subclassifications 
for every instance in the training set. During 
classification, the bit predictions of the vari- 
ous dichotomizers are combined to produce a 
codeword prediction. The class codeword which 
has minimal Hamming distance to the predicted 
codeword etermines the classification of the in- 
stance. Codewords are constructed such that 
their Hamming distance is maximal. Extra bits 
are added to allow for error recovery, allowing 
the correct class to be determinable even if some 
bits are wrong. An error-correcting output code 
for a k-class problem constitutes a matrix with 
k rows and 2 k-1-1 columns. Rows are the code- 
words corresponding to classes, and columns are 
binary subclassifications or bit functions fi such 
that, for an instance , and its codeword vector 
C 
fi(e) = ~-i(c) (1) 
(~-i(v) the i-th coordinate of vector v). If 
the minimum Hamming distance between ev- 
ery codeword is d, then the code has an error- 
correcting capability of \ [ -~ J .  Figure 1 shows 
the 5 x 15 ECOC matrix, for a 5-class problem. 
In this code, every codeword has a Hamming 
distance of at least 8 to the other codewords, 
so this code has an error-correcting capability 
of 3 bits. ECOC have two natural interpreta- 
011000100000001\ ]  01101001011101 ~ 
10100011101011 
11001110110001 
11110100011011 
Figure h ECOC for a five-class problem. 
tions. From an information-theoretic perspec- 
tive, classification with ECOC is like channel 
coding (Shannon, 1948): the class of a pattern 
to be classified is a datum sent over a noisy com- 
munication channel. The communication chan- 
nel consists of the trained classifier. The noise 
consists of the bias (systematic error) and vari- 
ance (training set-dependent error) of the classi- 
fier, which together make up for the overall error 
55 
of the classifier. The received message must be 
decoded before it can be interpreted as a classi- 
fication. Adding redundancy to a signal before 
transmission is a well-known technique in digi- 
tal communication to allow for the recovery of 
errors due to noise in the channel, and this is 
the key to the success of ECOC. From a ma- 
chine learning perspective, an error-correcting 
output code uniquely partitions the instances 
in the training set into two disjoint subclasses, 
0 or 1. This can be interpreted as learning a set 
of class boundaries. To illustrate this, consider 
the following binary code for a three-class prob- 
lem. (This actually is a one-of-c code with no 
error-correcting capability (the minimal Ham- 
ming distance between the codewords i 1). As 
such it is an error-correcting code with lowest 
error correction, but it serves to illustrate the 
point.) 
fl f2 f3 
C1 0 0 1 
C2 01  0 (2) 
C3 1 0 0 
For every combination of classes (C1-C2, C1- 
C3, C2-C3), the Hamming distance between the 
codewords i 2. These horizontal relations have 
vertical repercussions as well: for every such 
pair, two bit functions disagree in the classes 
they select. For C1-C2, f2 selects C2 and f3 se- 
lects C1. For C1-C3, f l  selects C3 and f3 selects 
C1. Finally, for C2-C3, f l  selects C3 and f2 se- 
lects C2. So, every class is selected two times, 
and this implies that every class boundary asso- 
ciated with that class in the feature hyperspace 
is learned twice. In general (Kong and Diet- 
terich, 1995), if the minimal Hamming distance 
between the codewords of an (error-correcting) 
code is d, then every class boundary is learned 
times. For the error-correcting code from above 
this implies an error correction of zero: only two 
votes support a class boundary, and no vote can 
be favored in case of a conflict. The decoding 
of the predicted bit string to a class symbol ap- 
pears to be a form of voting over class bound- 
aries (Kong and Dietterich, 1995), and is able to 
reduce both bias and variance of the classifier. 
2 D ichotomizer  Ensembles  
Dichotomizer ensembles must be diverse apart 
from accurate. Diversity is necessary in order 
to decorrelate he predictions of the various di- 
chotomizers. This is a consequence of the voting 
mechanism underlying ECOC, where bit func- 
tions can only outvote other bit functions if they 
do not make similar predictions. Selecting dif- 
ferent features per dichotomizer was proposed 
for this purpose (Ricci and Aha, 1997). An- 
other possibility is to add limited non-locality to 
a local classifier, since classifiers that use global 
information such as class probabilities during 
classification, are much less vulnerable to cor- 
related predictions. The following ideas were 
tested empirically on a suite of natural language 
learning tasks. 
? A careful feature selection approach, where 
every dichotomizer is trained to select (pos- 
sibly) different features. 
? A careless feature selection approach, 
where every bit is predicted by a voting 
committee of dichotomizers, each of which 
randomly selects features (akin in spirit to 
the Multiple Feature Subsets approach for 
non-distributed classifiers (Bay, 1999). 
? A careless feature selection approach, 
where blocks of two adjacent bits are pre- 
dicted by a voting committee of quadro- 
tomizers, each of which randomly selects 
features. Learning blocks of two bits al- 
lows for bit codes that are twice as long 
(larger error-correction), but with half as 
many classifiers. Assuming a normal dis- 
tribution of errors and bit values in every 2 
bits-block, there is a 25% chance that both 
bits in a 2-bit block are wrong. The other 
75% chance of one bit wrong would pro- 
duce performance equal to voting per bit. 
Formally, this implies a switch from N two- 
class problems to N/2 four-class problems, 
where separate regions of the class land- 
scape are learned jointly. 
? Adding non-locality to 1-3 in the form of 
larger values for k. 
? The use of the Modified Value Difference 
Metric, which alters the distribution of in- 
stances over the hyperspace of features, 
yielding different class boundaries. 
3 Memory-based  learn ing  
The memory-based learning paradigm views 
cognitive processing as reasoning by analogy. 
Cognitive classification tasks are carried out by 
56 
matching data to be classified with classified 
data stored in a knowledge base. This latter 
data set is called the training data, and its ele- 
ments are called instances. Every instance con- 
sists of a feature-value vector and a class label. 
Learning under the memory-based paradigm is 
lazy, and consists only of storing the training 
instances in a suitable data structure. The in- 
stance from the training set which resembles 
the most the item to be classified determines 
the classification of the latter. This instance is 
called the nearest neighbor, and models based 
on this approach to analogy are called nearest 
neighbor models (Duda and Hart, 1973). So- 
called k-nearest neighbor models elect a winner 
from the k nearest neighbors, where k is a pa- 
rameter and winner selection is usually based on 
class frequency. Resemblance between instances 
is measured using distance metrics, which come 
in many sorts. The simplest distance metric is 
the overlap metric: 
k (3) 5(vi, vj) = 0 if vi = vj 
5(vi, vj) = 1 if vi ? vj 
(~ri(I) is the i-th projection of the feature vec- 
tor I.) Another distance metric is the Mod- 
ified Value Difference Metric (MVDM) (Cost 
and Salzberg, 1993). The MVDM defines sim- 
ilarity between two feature values in terms of 
posterior probabilities: 
5(vi, vj) = ~ I P(c I vi) - P(c Ivj) l (4) 
cEClasses 
When two values share more classes, they are 
more similar, as 5 decreases. Memory-based 
learning has fruitfully been applied to natu- 
ral language processing, yielding state-of-the- 
art performance on all levels of linguistic analy- 
sis, including grapheme-to-phoneme conversion 
(van den Bosch and Daelemans, 1993), PoS- 
tagging (Daelemans et al, 1996), and shallow 
parsing (Cardie et al, 1999). In this study, 
the following memory-based models are used, 
all available from the TIMBL package (Daele- 
mans et al, 1999). IB i - IG is a k-nearest dis- 
tance classifier which employs a weighted over- 
lap metric: 
~(I~, b )  = ~ wkS(~k(/~), ~( I j ) )  (5) 
k 
In stead of drawing winners from the k-nearest 
neighbors pool, IBi-IG selects from a pool of 
instances for k nearest distances. Features are 
separately weighted based on Quinlan's infor- 
mation gain ratio (Quinlan, 1993), which mea- 
sures the informativity of features for predicting 
class labels. This can be computed by subtract- 
ing the entropy of the knowledge of the feature 
values from the general entropy of the class la- 
bels. The first quantity is normalized with the a 
priori probabilities of the various feature values 
of feature F: 
H(C)  - Eveva  es(F) P(v) ? H(QF=v\]) (6) 
Here, H(C) is the class entropy, defined as 
H(C) =-  ~ P(c) log 2P(c). (7) 
cEClass 
H(C\[F=v\] ) is the class entropy computed over 
the subset of instances that have v as value for 
Fi. Normalization for features with many values 
is obtained by dividing the information gain for 
a feature by the entropy of its value set (called 
the split info of feature Fi. 
H(C)--~veValues(Fi) P(v)xH(C\[F=v\]) 
Wi ---- split_in f o( Fi ) 
split - info(Fi) = - ~ P(v) log 2 P(v) 
vE Values( Fi ) 
(s) 
IGTREE is a heuristic approximation of IB1- 
IG which has comparable accuracy, but is op- 
timized for speed. It is insensitive to k-values 
larger than 1, and uses value-class cooccurrence 
information when exact matches fail. 
4 Experiments 
The effects of a distributed class representa- 
tion on generalization accuracy were measured 
using an experimental matrix based on 5 lin- 
guistic datasets, and 8 experimental condi- 
tions, addressing feature selection-based ECOC 
vs. voting-based ECOC, MVDM, values of 
k larger than 1, and dichotomizer weight- 
ing. The following linguistic tasks were used. 
DIMIN is a Dutch diminutive formation task de- 
rived from the Celex lexical database for Dutch 
(Baayen et al, 1993). It predicts Dutch nomi- 
nal diminutive suffixes from phonetic properties 
(phonemes and stress markers) of maximally the 
57 
last three syllables of the noun. The STRESS 
task, also derived from the Dutch Celex lexP 
cal database, assigns primary stress on the ba- 
sis of phonemic values. MORPH assigns mor- 
phological boundaries (a.o. root morpheme, 
stress-changing affix, inflectional morpheme), 
based on English CELEX data. The WSJ- 
NPVP task deals with NP-VP chunking of PoS- 
tagged Wall Street Journal material. GRAPHON, 
finally, is a grapheme-to-phoneme conversion 
task for English based on the English Celex lex- 
ical database. Numeric characteristics of the 
different asks are listed in table 1. All tasks 
with the exception of GRAPHON happened to 
be five-class problems; for GRAPHON, a five- 
class subset was taken from the original training 
set, in order to keep computational demands 
manageable. The tasks were subjected to the 
Data set Features Classes Instances 
DIMIN 12 5 3,000 
STRESS 12 5 3,000 
MORPH 9 5 300,000 
NPVP 8 5 200,000 
GRAPHON 7 5 73,525 
Table 1: Data sets. 
8 different experimental situations of table 2. 
For feature selection-based ECOC, backward se- 
quential feature elimination was used (Raaij- 
makers, 1999), repeatedly eliminating features 
in turn and evaluating each elimination step 
with 10-fold cross-validation. For dichotomizer 
weighting, error information of the dichotomiz- 
ers, determined from separate unweighted 10- 
fold cross-validation experiments on a separate 
training set, produced a weighted Hamming dis- 
tance metric. Error-based weights were based 
on raising a small constant ~ in the interval 
\[0, 1) to the power of the number of errors made 
by the dichotomizer (Cesa-Bianchi et al, 1996). 
Random feature selection drawing features with 
replacement created feature sets of both differ- 
ent size and composition for every dichotomizer. 
5 Resu l ts  
Table 3 lists the generalization accuracies for 
the control groups, and table 4 for the ECOC 
algorithms. All accuracy results are based on 
10-fold cross-validation, with p < 0.05 using 
paired t-tests. The results show that dis- 
ALGORITHM DESCRIPTION 
E1 
?2 
E3 
E4 
?5 
?6 
?7 
$8 
ECOC, feature selection per bit (15), 
k----l, unweighted 
ECOC, feature selection per bit (15), 
k----l, weighted 
ECOC, feature selection per bit (15), 
MVDM, k=l, unweighted 
ECOC, feature selection per bit (15), 
MVDM, k=l, weighted 
ECOC, feature selection per bit (15), 
MVDM, k----3, unweighted 
ECOC, feature selection per bit (15), 
MVDM, k=3, weighted 
ECOC, voting (100) per bit (30), 
MVDM, k=3 
ECOC, voting (100) per bit block 
(15), MVDM, k=3 
Table 2: Algorithms 
GRouP I II III IV 
IBi-IG IBi-IG IBi-IG IBi-IG' 
k=l k=3 k=l k=3 
MVDM MVDM 
98.1?0.5 DIMIN 
STRESS 
MORPH 
NPVP 
GRAPHON 
98.1?0.5 
83.5?2.6 
92.5?1.4 
96.4?0.2 
97.1?2.4 
95.8?0.5 
81.3?2.9 
92.0?1.4 
97.1?0.2 
97.2?2.3 
97.7?0.7 
86.2?2.0 
92.5?1.4 
97.0?0.1 
97.7?0.7 
86.7?1.8 
92.5?1.4 
97.0?0.1 
97.7?0.8 
Table 3: Generalization accuracies control groups. 
tributed class representations can lead to sta- 
tistically significant accuracy gains for a variety 
of linguistic tasks. The ECOC algorithm based 
on feature selection and weighted Hamming dis- 
tance performs best. Voting-based ECOC per- 
forms poorly on DIMIN and STRESS with vot- 
ing per bit, but significant accuracy gains are 
achieved by voting per block, putting it on a par 
with the best performing algorithm. Regression 
analysis was applied to investigate the effect of 
the Modified Value Difference Metric on ECOC 
accuracy. First, the accuracy gain of MVDM 
as a function of the information gain ratio of 
the features was computed. The results show a 
high correlation (0.82, significant at p < 0.05) 
between these variables, indicating a linear re- 
lation. This is in line with the idea underlying 
MVDM: whenever two feature values are very 
predictive of a shared class, they contribute to 
the similarity between the instances they belong 
to, which will lead to more accurate classifiers. 
Next, regression analysis was applied to deter- 
mine the effect of MVDM on ECOC, by relating 
the accuracy gain of MVDM (k=3) compared to 
58 
TASK ?1 (I) g2(I) $3 (III) $4 (III) $5 (IV) $6 (IV) $7 (IV) g8 ($6) 
DIMIN 98.6=k0.4x/ 98.5=k0.4x/ 98.6:k0.6~/ 98.7::k0.6x/ 98.8::k0.5x/ 98.9::k0.4~/ 96.6:k0.9x 98.4=E0.4 
STRESS 85.3::kl.Sx/ 86.3::k2.0X/ 88.2=hl.7x/ 88.8=t:1.7X/ 88.2:kl.7x/ 89.3::kl.9~/ 86.5=k2.3x 88.8::kl.7 
MORPH 93.2:kl.6x/ 93.2=kl.5x/ 93.2=kl.3x/ 93.2=kl.3~/ 93.2=1=1.6~/ 93.2=kl.5~/ 93.0::kl.6x/ 93.4:t=1.5x/ 
NPVP t 96.8::k0.1~/ 96.9::k0.2x/ 96.8=E0.1 96.9:k0.1 96.8::k0.1 96.9=k0.1 96.8=h0.2x 96.8=t:0.2 
GRAPHON 98.2=t=0.7 98.3=t=0.7 98.4:k0.6X/ 98.3=E0.5X/ 98.3::h0.6X/ 98.5::k0-5X/ 97.6=k0.7x 97.6:h0.8x 
Table 4: Generalization accuracies for feature selection-based ECOC (x/ indicates significant improvement over 
control group (in round brackets) , and x deterioration at p < 0.05 using paired t-tests). A 1" indicates 25 voters for 
performance reasons. 
control group II to the accuracy gain of ECOC 
(algorithm $6, compared to control group IV). 
The correlation between these two variables is 
very high (0.93, significant at p < 0.05), again 
indicative of a linear relation. From the per- 
spective of learning class boundaries, the strong 
effect of MVDM on ECOC accuracy can be un- 
derstood as follows. When the overlap metric is 
used, members of a training set belonging to the 
same class may be situated arbitrarily remote 
from each other in the feature hyperspace. For 
instance, consider the following two instances 
taken from DIMIN: 
. . . . . . . . .  d ,A ,k , je  
. . . . . . . . .  d,A,x,je 
(Hyphens indicate absence of feature values.) 
These two instances encode the diminutive for- 
mation of Dutch dakje (little roo\]~ from dak 
(roo\]~, and dagje (lit. little day, proverbially 
used) from dag (day). Here, the values k and x, 
corresponding to the velar stop 'k' and the ve- 
lar fricative 'g', are minimally different from a 
phonetic perspective. Yet, these two instances 
have coordinates on the twelfth dimension of 
the feature hyperspace that have nothing to do 
with each other. The overlap treats the k-x 
value clash just like any other value clash. This 
phenomenon may lead to a situation where in- 
habitants of the same class are scattered over 
the feature hyperspace. In contrast, a value dif- 
ference metric like MVDM which attempts to 
group feature values on the basis of class cooc- 
currence information, might group k and x to- 
gether if they share enough classes. The effect 
of MVDM on the density of the feature hyper- 
space can be compared with the density ob- 
tained with the overlap metric as follows. First, 
plot a random numerical transform of a feature 
space. For expository reasons, it is adequate 
to restrict attention to a low-dimensional (e.g. 
two-dimensional) subset of the feature space, for 
a specific class C. Then, plot an MVDM trans- 
form of this feature space, where every coordi- 
nate (a, b) is transformed into (P(Cla) ,  P(C I 
b)). This idea is applied to a subset of DIMIN, 
consisting of all instances classified as j e (one 
of the five diminutive suffixes for Dutch). The 
features for this subset were limited to the last 
two, consisting of the rhyme and coda of the 
last syllable of the word, clearly the most infor- 
mative features for this task. Figure 2 displays 
the two scatter plots. As can be seen, instances 
are widely scattered over the feature space for 
the numerical transform, whereas the MVDM- 
based transform forms many clusters and pro- 
duces much higher density. In a condensed fea- 
70 
o60 
~30 
?D 
20 
++ ? + 
% +++ 
t4  t ++t + 
++ +% +#$+ + + + 
+ + ++ ? + 
$? St  ~$~ , ?t  + 
+ + + ~ ? 
$:$ +t+ - t t - i - t~ + +? ? 
i i | I I I I I 
5 10 '15 20 25 30 35 40 45 
Feature 11 DIMIN (random) 
'i 
0.9 
0.8 
0.7 
_z 0.6 
~. 0.4 
0.3 
. 0.2 
0.1 
0 
-~.@ + ? 4, ?41. 
0,1 0.2 0.3 0.4 0.5 0.6 0.7 
Feature I | DIMIN (MVDM) 
Figure 2: Random numerical transform of feature val- 
ues based on the overlap metric (left) vs. numerical 
transform of feature values based on MVDM (right), for 
a two-features-one-class subset of DIMIN. 
ture hyperspace the number of class boundaries 
to be learned per bit function reduces. For in- 
stance, figures 3 displays the class boundaries 
for a relatively condensed feature hyperspace, 
where classes form localized populations, and a 
scattered feature hyperspace, with classes dis- 
tributed over non-adjacent regions. The num- 
ber of class boundaries in the scattered feature 
space is much higher, and this will put an addi- 
59 
tional burden on the learning problems consti- 
tuted by the various bit functions. 
C1 b13~ 3 b35 
hi2 i C5 
b15 
C2 
b24 bl2ii I 
C4 b14 CI 
Fl 
F2 bl2i I C2 b24i I C4 
b35i C3 ~ _ ~  
b3 ii C4 b24ii C2 
FI 
Figure 3: Condensed feature space (left) vs. scattered 
feature space (right). 
6 Conc lus ions  
The use of error-correcting output codes 
(ECOC) for representing natural language 
classes has been empirically validated for a suite 
of linguistic tasks. Results indicate that ECOC 
can be useful for datasets with features with 
high class predictivity. These sets typically tend 
to benefit from the Modified Value Difference 
Metric, which creates a condensed hyperspace 
of features. This in turn leads to a lower num- 
ber of class boundaries to be learned per bit 
function, which simplifies the binary subclas- 
sification tasks. A voting algorithm for learn- 
ing blocks of bits proves as accurate as an ex- 
pensive feature-selecting algorithm. Future re- 
search will address further mechanisms of learn- 
ing complex regions of the class boundary land- 
scape, as well as alternative rror-correcting ap- 
proaches to classification. 
Acknowledgements  
Thanks go to Francesco Ricci for assistance in 
generating the error-correcting codes used in 
this paper. David Aha and the members of the 
Induction of Linguistic Knowledge (ILK) Group 
of Tilburg University and Antwerp University 
are thanked for helpful comments and criticism. 
References  
H. Baayen, R. Piepenbrock, and H. van Rijn. 1993. 
The CELEX database on CD-ROM. Linguistic 
Data Consortium. Philadelpha, PA. 
S. Bay. 1999. Nearest neighbor classification from 
multiple feature subsets. Intelligent Data Analy- 
sis, 3(3):191-209. 
A. Berger. 1999. Error-correcting output coding 
for text classification. Proceedings of IJCAI'99: 
Workshop on machine learning for information 
filtering. 
C. Cardie, S. Mardis, and D. Pierce. 1999. Com- 
bining error-driven pruning and classification for 
partial parsing. Proceedings of the Sixteenth In- 
ternational Conference on Machine Learning, pp. 
87-96. 
N. Cesa-Bianchi, Y. Freund, D. Helmbold, and 
M. Warmuth. 1996. On-line prediction and con- 
version strategies. Machine Learning 27:71-110. 
S. Cost and S. Salzberg. 1993. A weighted near- 
est neighbor algorithm for learning with symbolic 
features. Machine Learning,10:57-78. 
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 
1996. Mbt: A memory-based part of speech tag- 
ger generator. Proceedings of the Fourth Work- 
shop on Very Large Corpora, ACL SIGDAT. 
W. Daelemans, J. Zavrel, K. Van der Sloot, and 
A. Van den Bosch. 1999. Timbh Tilburg memory 
based learner, version 2.0, reference guide. ILK 
Technical Report - ILK 99-01. Tilburg. 
T. Dietterich and G. Bakiri. 1995. Solving multi- 
class learning problems via error-correcting out- 
put codes. Journal of Artificial Intelligence Re- 
search, 2:263-286. 
R. Duda and P. Hart. 1973. Pattern classification 
and scene analysis. Wiley Press. 
E. Kong and T. Dietterich. 1995. Error-correcting 
output coding corrects bias and variance. Pro- 
ceedings of the 12th International Conference on 
Machine Learning. 
J.R. Quinlan. 1993. C4.5: Programs for Machine 
Learning. Morgan Kaufmann, San Mateo, Ca. 
S. Raaijmakers. 1999. Finding representations for
memory-based language learning. Proceedings of 
CoNLL-1999. 
F. Ricci and D. Aha. 1997. Extending local learners 
with error-correcting output codes. Proceedings of 
the l~th Conference on Machine Learning. 
D. Roth. 1998. A learning approach to shallow pars- 
ing. Proceedings EMNLP- WVLC'99. 
H. Schmid. 1994. Part-of-speech tagging with neu- 
ral networks. Proceedings COLING-9~. 
C. Shannon. 1948. A mathematical theory of com- 
munication. Bell System Technical Journal,27:7, 
pp. 379-423, 27:10, pp. 623-656. 
A. van den Bosch and W. Daelemans. 1993. Data- 
oriented methods for grapheme-to-phoneme con-
version. Proceedings of the 6th Conference of the 
EACL. 
60 
