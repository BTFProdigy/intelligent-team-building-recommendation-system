Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 969?976
Manchester, August 2008
CollabRank: Towards a Collaborative Approach to Single-Document 
Keyphrase Extraction 
Xiaojun Wan and Jianguo Xiao 
Institute of Computer Science and Technology 
Peking University, Beijing 100871, China 
{wanxiaojun,xiaojianguo}@icst.pku.edu.cn 
 
Abstract 
Previous methods usually conduct the 
keyphrase extraction task for single docu-
ments separately without interactions for 
each document, under the assumption 
that the documents are considered inde-
pendent of each other. This paper pro-
poses a novel approach named Col-
labRank to collaborative single-document 
keyphrase extraction by making use of 
mutual influences of multiple documents 
within a cluster context. CollabRank is 
implemented by first employing the clus-
tering algorithm to obtain appropriate 
document clusters, and then using the 
graph-based ranking algorithm for col-
laborative single-document keyphrase ex-
traction within each cluster. Experimental 
results demonstrate the encouraging per-
formance of the proposed approach. Dif-
ferent clustering algorithms have been 
investigated and we find that the system 
performance relies positively on the qual-
ity of document clusters. 
1 Introduction 
A keyphrase is defined as a meaningful and sig-
nificant expression consisting of one or more 
words in a document. Appropriate keyphrases 
can be considered as a highly condensed sum-
mary for a document, and they can be used as a 
label for the document to supplement or replace 
the title or summary, thus facilitating users? fast 
browsing and reading. Moreover, document key-
phrases have been successfully used in the fol-
lowing IR and NLP tasks: document indexing 
(Gutwin et al, 1999), document classification 
(Krulwich and Burkey, 1996), document cluster-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
ing (Zhang et al, 2004; Hammouda et al, 2005) 
and document summarization (Berger and Mittal, 
2000; Buyukkokten et al, 2001). 
Keyphrases are usually manually assigned by 
authors, especially for journal or conference arti-
cles. However, the vast majority of documents 
(e.g. news articles, magazine articles) do not 
have keyphrases, therefore it is beneficial to 
automatically extract a few keyphrases from a 
given document to deliver the main content of 
the document. Here, keyphrases are selected 
from within the body of the input document, 
without a predefined list (i.e. controlled vocabu-
lary). Most previous work focuses on keyphrase 
extraction for journal or conference articles, 
while this paper focus on keyphrase extraction 
for news articles because news article is one of 
the most popular document genres on the web 
and most news articles have no author-assigned 
keyphrases. 
Very often, keyphrases of all single documents 
in a document set are required to be extracted. 
However, all previous methods extract key-
phrases for a specified document based only on 
the information contained in that document, such 
as the phrase?s TFIDF, position and other syntac-
tic information in the document. One common 
assumption of existing methods is that the docu-
ments are independent of each other. Hence the 
keyphrase extraction task is conducted separately 
without interactions for each document. However, 
the multiple documents within an appropriate 
cluster context usually have mutual influences 
and contain useful clues which can help to ex-
tract keyphrases from each other. For example, 
two documents about the same topic ?earth-
quake? would share a few common phrases, e.g. 
?earthquake?, ?victim?, and they can provide 
additional knowledge for each other to better 
evaluate and extract salient keyphrases from each 
other. The idea is borrowed from human?s per-
ception that a user would better understand a 
topic expressed in a document if the user reads 
more documents about the same topic.  
969
Based on the above assumption, we propose a 
novel framework for collaborative single-
document keyphrase extraction by making use of 
the additional information from multiple docu-
ments within an appropriate cluster context. The 
collaborative framework for keyphrase extraction 
consists of the step of obtaining the cluster con-
text and the step of collaborative keyphrase ex-
traction in each cluster. In this study, the cluster 
context is obtained by applying the clustering 
algorithm on the document set, and we have in-
vestigated how the cluster context influences the 
keyphrase extraction performance by employing 
different clustering algorithms. The graph-based 
ranking algorithm is employed for collaborative 
keyphrase extraction for each document in a 
specified cluster. Instead of making only use of 
the word relationships in a single document, the 
algorithm can incorporate the ?voting? or ?rec-
ommendations? between words in all the docu-
ments of the cluster, thus making use of the 
global information existing in the cluster context. 
The above implementation of the collaborative 
framework is denoted as CollabRank in this pa-
per. 
Experiments have been performed on a dataset 
consisting of 308 news articles with human-
annotated keyphrases, and the results demon-
strate the good effectiveness of the CollabRank 
approach. We also find that the extraction per-
formance is positively correlated with the quality 
of cluster context, and existing clustering algo-
rithms can yield appropriate cluster context for 
collaborative keyphrase extraction. 
The rest of this paper is organized as follows: 
Section 2 introduces the related work. The pro-
posed CollabRank is described in detail in Sec-
tion 3. Empirical evaluation is demonstrated in 
Section 4 and lastly we conclude this paper in 
Section 5. 
2 Related Work 
The methods for keyphrase (or keyword) extrac-
tion can be roughly categorized into either unsu-
pervised or supervised.  
Unsupervised methods usually involve assign-
ing a saliency score to each candidate phrases by 
considering various features. Krulwich and Bur-
key (1996) use heuristics based on syntactic 
clues to extract keyphrases from a document. 
Barker and Cornacchia (2000) propose a simple 
system for choosing noun phrases from a docu-
ment as keyphrases. Mu?oz (1996) uses an unsu-
pervised learning algorithm to discover two-word 
keyphrases. The algorithm is based on Adaptive 
Resonance Theory (ART) neural networks. 
Steier and Belew (1993) use the mutual informa-
tion statistics to discover two-word keyphrases. 
Tomokiyo and Hurst (2003) use pointwise KL-
divergence between multiple language models 
for scoring both phraseness and informativeness 
of phrases. More recently, Mihalcea and Tarau 
(2004) propose the TextRank model to rank key-
words based on the co-occurrence links between 
words. Such algorithms make use of ?voting? or 
?recommendations? between words to extract 
keyphrases. 
Supervised machine learning algorithms have 
been proposed to classify a candidate phrase into 
either keyphrase or not. GenEx (Turney, 2000) 
and Kea (Frank et al, 1999; Witten et al, 1999) 
are two typical systems, and the most important 
features for classifying a candidate phrase are the 
frequency and location of the phrase in the 
document. More linguistic knowledge has been 
explored by Hulth (2003). Statistical associations 
between keyphrases have been used to enhance 
the coherence of the extracted keyphrases (Tur-
ney, 2003). Song et al (2003) present an infor-
mation gain-based keyphrase extraction system 
called KPSpotter. Medelyan and Witten (2006) 
propose KEA++ that enhances automatic key-
phrase extraction by using semantic information 
on terms and phrases gleaned from a domain-
specific thesaurus. Nguyen and Kan (2007) focus 
on keyphrase extraction in scientific publications 
by using new features that capture salient mor-
phological phenomena found in scientific key-
phrases.  
The tasks of keyphrase extraction and docu-
ment summarization are similar and thus they 
have been conducted in a uniform framework. 
Zha (2002) proposes a method for simultaneous 
keyphrase extraction and text summarization by 
using the heterogeneous sentence-to-word rela-
tionships. Wan et al (2007a) propose an iterative 
reinforcement approach to simultaneous key-
phrase extraction and text summarization. Other 
related works include web page keyword extrac-
tion (Kelleher and Luz, 2005; Zhang et al, 2005; 
Chen et al, 2005), advertising keywords finding 
(Yih et al, 2006). 
To the best of our knowledge, all previous 
work conducts the task of keyphrase extraction 
for each single document independently, without 
making use of the collaborative knowledge in 
multiple documents. We focus on unsupervised 
methods in this study. 
970
3 The Proposed CollabRank Approach 
3.1 Framework Description 
Given a document set for keyphrase extraction of 
each single document, CollabRank first employs 
the clustering algorithm to group the documents 
into a few clusters. The documents within each 
cluster are expected to be topic-related and each 
cluster can be considered as a context for any 
document in the cluster. Given a document clus-
ter, CollabRank makes use of the global word 
relationships in the cluster to evaluate and rank 
candidate phrases for each single document in 
the cluster based on the graph-based ranking al-
gorithm. Figure 1 gives the framework of the 
proposed approach.  
1. Document Clustering: Group the documents in the 
document set D into a few clusters using the cluster-
ing algorithm;  
2. Collaborative Keyphrase Extraction: For each 
cluster C, perform the following steps respectively 
to extract keyphrases for single documents in the 
cluster in a batch mode: 
1) Cluster-level Word Evaluation: Build a 
global affinity graph G based on all candidate 
words restricted by syntactic filters in the documents 
of the given cluster C, and employ the graph-ranking 
based algorithm to compute the cluster-level sali-
ency score for each word. 
2) Document-level Keyphase Extraction: For 
any single document d in the cluster, evaluate the 
candidate phrases in the document based on the 
scores of the words contained in the phrases, and fi-
nally choose a few phrases with highest scores as 
the keyphrases of the document. 
Figure 1. The Framework of CollabRank 
In the first step of the above framework, dif-
ferent clustering algorithms will yield different 
clusters. The documents in a high-quality cluster 
are usually deemed to be highly topic-related (i.e. 
appropriate cluster context), while the documents 
in a low-quality cluster are usually not topic-
related (i.e. inappropriate cluster context). The 
quality of a cluster will influence the reliability 
of the contextual information for evaluating the 
words in the cluster. A number of clustering al-
gorithms will be investigated in the experiments, 
including the agglomerative algorithm (both av-
erage-link and complete-link), the divisive algo-
rithm, and the kmeans algorithm (Jain et al, 
1999), whose details will be described in the 
evalution section. 
In the second step of the above framework, 
substep 1) aims to evaluate all candidate words 
in the cluster based on the graph-based ranking 
algorithm. The global affinity graph aims to re-
flect the cluster-level co-occurrence relationships 
between all candidate words in the documents of 
the given cluster. The saliency scores of the 
words are computed based on the global affinity 
graph to indicate how much information about 
the main topic the words reflect. Substep 2) aims 
to evaluate candidate phrases of each single 
document based on the cluster-level word scores, 
and then choose a few salient phrases as key-
phrases of the document. Substep 1) is performed 
on all documents in the cluster in order to evalu-
ate the words from a global perspective, while 
substep 2) is performed on each single document 
in order to extract keyphrases from a local per-
spective. A keyphrase of a document is expected 
to include highly salient words. We can see that 
the keyphrase extraction tasks are conducted in a 
batch mode for each cluster. The substeps of 1) 
and 2) will be described in next sections respec-
tively. If substep 1) is performed on each single 
document without considering the cluster context, 
the approach is degenerated into the simple Tex-
tRank model (Mihalcea and Tarau, 2004), which 
is denoted as SingleRank in this paper.  
It is noteworthy that in addition to the graph-
based ranking algorithm, other keyphrase extrac-
tion methods can also be integrated in the pro-
posed collaborative framework to exploit the col-
laborative knowledge in the cluster context.  
3.2 Cluster-Level Word Evaluation 
Like the PageRank algorithm (Page et al, 1998), 
the graph-based ranking algorithm employed in 
this study is essentially a way of deciding the 
importance of a vertex within a graph based on 
global information recursively drawn from the 
entire graph. The basic idea is that of ?voting? or 
?recommendation? between the vertices. A link 
between two vertices is considered as a vote cast 
from one vertex to the other vertex. The score 
associated with a vertex is determined by the 
votes that are cast for it, and the score of the ver-
tices casting these votes.  
Formally, given a specified cluster C, let G=(V, 
E) be an undirected graph to reflect the relation-
ships between words in the cluster. V is the set of 
vertices and each vertex is a candidate word2 in 
the cluster. Because not all words in the docu-
ments are good indicators of keyphrases, the 
words added to the graph are restricted with syn-
tactic filters, i.e., only the words with a certain 
part of speech are added. As in Mihalcea and 
Tarau (2004), the documents are tagged by a 
                                                 
2 The original words are used without stemming. 
971
POS tagger, and only the nouns and adjectives 
are added into the vertex set3. E is the set of 
edges, which is a subset of V?V. Each edge eij in 
E is associated with an affinity weight aff(vi,vj) 
between words vi and vj. The weight is computed 
based on the co-occurrence relation between the 
two words, controlled by the distance between 
word occurrences. The co-occurrence relation 
can express cohesion relationships between 
words. Two vertices are connected if the corre-
sponding words co-occur at least once within a 
window of maximum k words, where k can be set 
anywhere from 2 to 20 words. The affinity 
weight aff(vi,vj) is simply set to be the count of 
the controlled co-occurrences between the words 
vi and vj in the whole cluster as follows: 
)()( ji
Cd
dji ,vvcount,vvaff ?
?
=  (1) 
where countd(vi,vj) is the count of the controlled 
co-occurrences between words vi and vj  in docu-
ment d.  
The graph is built based on the whole cluster 
and it is called Global Affinity Graph.  The big-
gest difference between CollabRank and 
SingleRank is that SingleRank builds a local 
graph based on each single document.  
We use an affinity matrix M to describe G 
with each entry corresponding to the weight of 
an edge in the graph. M = (Mi,j)|V|?|V| is defined as 
follows: 
 
otherwise0
;  and   with  links if)(
??
? ?
=
,   
jiv v,   ,vvaff
M jijii,j
 
(2) 
Then M is normalized to M~ as follows to make 
the sum of each row equal to 1: 
??
??
?
?
=
??
==
otherwise0
0if~
|V|
1
|V|
1
   ,             
M ,   MM
M j
i,j
j
i,ji,j
i,j
 
 
(3) 
Based on the global affinity graph G, the clus-
ter-level saliency score WordScoreclus(vi) for 
word vi can be deduced from those of all other 
words linked with it and it can be formulated in a 
recursive form as in the PageRank algorithm: 
?
?
?
+??=
iall j
j,ijclusiclus V
MvWordScorevWordScore
||
)1(~)()( ??  
(4) 
And the matrix form is: 
e
V
?M? T r
rr
||
)1(~ ?? ?+=   (5) 
                                                 
3 The corresponding POS tags of the candidate words 
include ?JJ?, ?NN?, ?NNS?, ?NNP?, ?NNPS?. We 
used the Stanford log-linear POS tagger (Toutanova 
and Manning, 2000) in this study.  
where 1||)]([ ?= Viclus vWordScore?
r
is the vector of 
word saliency scores. er  is a vector with all ele-
ments equaling to 1. ? is the damping factor usu-
ally set to 0.85, as in the PageRank algorithm. 
The above process can be considered as a 
Markov chain by taking the words as the states 
and the corresponding transition matrix is given 
by TT ee
|V|
M rr)1(~ ?? ?+ . The stationary probabil-
ity distribution of each state is obtained by the 
principal eigenvector of the transition matrix.  
For implementation, the initial scores of the 
words are set to 1 and the iteration algorithm in 
Equation (4) is adopted to compute the new 
scores of the words. Usually the convergence of 
the iteration algorithm is achieved when the dif-
ference between the scores computed at two suc-
cessive iterations for any words falls below a 
given threshold (0.0001 in this study).  
For SingleRank, the saliency score Word-
Scoredoc(vi) for word vi is computed in the same 
iterative way based on the local graph for the 
single document.  
3.3 Document-Level Keyphrase Extraction 
After the scores of all candidate words in the 
cluster have been computed, candidate phrases 
are selected and evaluated for each single docu-
ment in the cluster. The candidate words (i.e. 
nouns and adjectives) of a specified document d 
in the cluster, which is a subset of V, are marked 
in the document text, and sequences of adjacent 
candidate words are collapsed into a multi-word 
phrase. The phrases ending with an adjective are 
not allowed, and only the phrases ending with a 
noun are collected as the candidate phrases for 
the document. For instance, in the following sen-
tence: ?Mad/JJ cow/NN disease/NN has/VBZ 
killed/VBN 10,000/CD cattle/NNS?, the candi-
date phrases are ?Mad cow disease? and ?cattle?. 
The score of a candidate phrase pi is computed 
by summing the cluster-level saliency scores of 
the words contained in the phrase. 
 ?
?
=
ij pv
jclusi vWordScorepePhraseScor )()(  (6) 
All the candidate phrases in the document are 
ranked in decreasing order of the phrase scores 
and the top n phrases are selected as the key-
phrases of the document. n ranges from 1 to 20 in 
this study. Similarly for SingleRank, the phrase 
score is computed based on the document-level 
saliency scores of the words.  
972
4 Empirical Evaluation 
4.1 Data Set 
To our knowledge, there is no gold standard 
news dataset with assigned keyphrases for 
evaluation. So we manually annotated the 
DUC2001 dataset   (Over, 2001) and used the 
annotated dataset for evaluation in this study. 
The dataset was originally used for document 
summarization. It consisted of 309 news articles 
collected from TREC-9, in which two articles 
were duplicate (i.e. d05a\FBIS-41815 and 
d05a\FBIS-41815~). The average length of the 
documents was 740 words. Two graduate stu-
dents were employed to manually label the key-
phrases for each document. At most 10 key-
phrases could be assigned to each document. The 
annotation process lasted two weeks. The Kappa 
statistic for measuring inter-agreement among 
annotators was 0.70. And the annotation conflicts 
between the two subjects were solved by discus-
sion. Finally, 2488 keyphrases were labeled for 
the dataset. The average keyphrase number per 
document was 8.08 and the average word num-
ber per keyphrase was 2.09.  
The articles have been grouped into 30 clusters 
manually by NIST annotators for multi-
document summarization, and the documents 
within each cluster were topic-related or relevant. 
The manually labeled clusters were considered as 
the ground truth clusters or gold clusters. In order 
to investigate existing clustering algorithms, the 
documents in the clusters were mixed together to 
form the whole document set for automatic clus-
tering. 
4.2 Document Clustering Algorithm 
In the experiments, several popular clustering 
algorithms and random clustering algorithms are 
explored to produce cluster contexts. Note that 
we have already known the number (i.e. 30) of 
the clusters for the dataset beforehand and thus 
we simply use it as input for the following clus-
tering algorithms4.  
Gold Standard Clustering: It is a pseudo 
clustering algorithm by manually grouping the 
documents. We use the ground truth clusters as 
the upperbound of the following automatic clus-
tering algorithms.  
Kmeans Clustering: It is a partition based 
clustering algorithm. The algorithm randomly 
                                                 
4 How to obtain the number of desired clusters is not 
the focus of this study. 
selects 30 documents as the initial centroids of 
the 30 clusters and then iteratively assigns all 
documents to the closest cluster, and recomputes 
the centroid of each cluster, until the centroids do 
not change. The similarity between a document 
and a cluster centroid is computed using the 
standard Cosine measure.   
Agglomerative (AverageLink) Clustering: It 
is a bottom-up hierarchical clustering algorithm 
and starts with the points as individual clusters 
and, at each step, merges the most similar or 
closest pair of clusters, until the number of the 
clusters reduces to the desired number 30. The 
similarity between two clusters is computed us-
ing the AverageLink method, which computes 
the average of the Cosine similarity values be-
tween any pair of documents belonging to the 
two clusters respectively as follows:   
21
1 1
21
)(
)(
cc
,ddsim
,ccsim
m
i
n
j
ji
?
=
??
= =  
 
(7) 
where di, dj are two documents in cluster c1 and 
cluster c2 respectively, and |c1| and |c2| are respec-
tively the numbers of documents in clusters c1 
and c2. 
Agglomerative (CompleteLink) Clustering: 
It differs from the above agglomerative (Aver-
ageLink) clustering algorithm only in that the 
similarity between two clusters is computed us-
ing the CompleteLink method, which computes 
the minimum of the Cosine similarity values be-
tween any pair of documents belonging to the 
two clusters respectively as follows:   
)}({min)(
2121 jic,dcd
,ddsim,ccsim
ji ??
=  (8) 
Divisive Clustering: It is a top-down hierar-
chical clustering algorithm and starts with one, 
all-inclusive cluster and, at each step, splits the 
largest cluster (i.e. the cluster with most docu-
ments) into two small clusters using the Kmeans 
algorithm until the number of clusters increases 
to the desired number 30.  
Random Clustering: It produces 30 clusters 
by randomly assigning each document into one 
of the k clusters.  Three different randomization 
processes are performed and we denote them as 
Random1, Random2 and Random3, respectively. 
CollabRank relies on the clustering algorithm 
for document clustering, and the combination of 
CollabRank and any clustering algorithm will be 
investigated.  
4.3 Evaluation Metric 
For evaluation of document clustering results, we 
adopt the widely used F-Measure to measure the 
973
performance of the clustering algorithm (i.e. the 
quality of the clusters) by comparing the pro-
duced clusters with the gold clusters (classes) 
(Jain et al, 1999).  
For evaluation of keyphrase extraction results, 
the automatic extracted keyphrases are compared 
with the manually labeled keyphrases. The words 
are converted to their corresponding basic forms 
using word stemming before comparison. The 
precision p=countcorrect/countsystem, recall 
r=countcorrect/counthuman, F-measure (F=2pr/(p+r)) 
are used as evaluation metrics, where countcorrect 
is the total number of correct keyphrases ex-
tracted by the system, and countsystem is the total 
number of automatic extracted keyphrases, and 
counthuman is the total number of human-labeled 
keyphrases. 
4.4 Evaluation Results 
First of all, we show the document clustering 
results in Table 1. The gold standard clustering 
result is the upperbound of all automatic cluster-
ing results. Seen from the table, all the four 
popular clustering algorithms (i.e. CompleteLink, 
AverageLink, KMeans and Divisive) perform 
much better than the three random clustering al-
gorithms (i.e. Random1, Random2 and Ran-
dom3). Different clustering results lead to differ-
ent document relationships and a high-quality 
cluster produced by popular algorithms is 
deemed to build an appropriate cluster context 
for collaborative keyphrase extraction. 
Clustering Algorithm F-Measure 
Gold 1.000 
CompleteLink 0.907 
AverageLink 0.877 
Divisive 0.924 
Kmeans 0.866 
Random1 0.187 
Random2 0.189 
Random3 0.183 
Table 1. Clustering Results  
Now we show the results for keyphrase extrac-
tion. In the experiments, the keyphrase number is 
typically set to 10 and the co-occurrence window 
size is also simply set to 10. Table 2 gives the 
comparison results of baseline methods and the 
proposed CollabRank methods with different 
clustering algorithms. The TFIDF baseline com-
putes the word scores for each single document 
based on the word?s TFIDF value. The SingleR-
ank baseline computes the word scores for each 
single document based on the graph-based rank-
ing algorithm. The two baselines do not make 
use of the cluster context.  
Seen from Table 2, the CollabRank methods 
with the gold standard clustering algorithm or 
popular clustering algorithms (i.e. Kmeans, 
CompleteLink, AverageLink and Divisive) per-
form much better than the baseline methods over 
all three metrics. The results demonstrate the 
good effectiveness of the proposed collaborative 
framework. We can also see that the performance 
is positively correlated with the clustering results. 
The CollabRank method with the best perform-
ing gold standard clustering results achieves the 
best performance. While the methods with low-
quality clustering results (i.e. the three random 
clustering results) do not perform well, even 
much worse than the baseline SingleRank 
method. This is because that the documents in a 
low-quality cluster are not truly topic-related, 
and the mutual influences between the docu-
ments are not reliable for evaluating words from 
a global perspective. 
System Precision Recall F-measure
TFIDF 0.232 0.281 0.254 
SingleRank 0.247 0.303 0.272 
CollabRank 
(Gold) 0.283 0.348 0.312 
CollabRank 
(Kmeans) 0.276 0.339 0.304 
CollabRank 
(CompleteLink) 0.281 0.345 0.310 
CollabRank 
(AverageLink) 0.277 0.340 0.306 
CollabRank 
(Divisive) 0.274 0.337 0.302 
CollabRank 
(Random1) 0.210 0.258 0.232 
CollabRank 
(Random2) 0.216 0.265 0.238 
CollabRank 
(Random3) 0.209 0.257 0.231 
Table 2. Keyphrase Extraction Results 
In order to investigate how the co-occurrence 
window size k and the keyphrase number n influ-
ence the performance, we first vary k from 2 to 
20 when n is fixed as 10 and the results are 
shown in Figures 2-4 over three metrics respec-
tively. The results demonstrate that all the meth-
ods are not significantly affected by the window 
size. We then vary n from 1 to 20 when k is fixed 
as 10 and the results are shown in Figures 5-7.  
The results demonstrate that the precision values 
decrease with the increase of n, and the recall 
values increases with the increase of n, while the 
F-measure values first increase and then tend to 
decrease with the increase of n.  
We can also see from Figures 2-7 that the Col-
labRank methods with high-quality clustering 
results always perform better than the baseline 
974
SingleRank method under different window sizes 
and different keyphrase numbers, and they al-
ways  lead to poor performance with low-quality 
clustering results. This further proves that an ap-
propriate cluster context is very important for the 
CollabRank method. Fortunately, existing clus-
tering algorithms can obtain the desired cluster 
context.  
 
0 . 1 7
1 8 1 6
K e y p h r a s e  n u m b e r  n
P
r
e
c
i
s
i
o
n
SingleRank CollabRank(Gold) CollabRank(Kmeans)
CollabRank(CompleteLink) CollabRank(AverageLink) CollabRank(Divisive)
CollabRank(Random1) CollabRank(Random2) CollabRank(Random3)  
0.19
0.21
0.23
0.25
0.27
0.29
2 4 6 8 10 12 14 16 18 20
Window size k
Pr
ec
is
io
n
 
Figure 2. Precision vs. Window Size k 
0.23
0.25
0.27
0.29
0.31
0.33
0.35
2 4 6 8 10 12 14 16 18 20
Window size k
R
ec
al
l
Figure 3. Recall vs. Window Size k 
0.2
0.22
0.24
0.26
0.28
0.3
0.32
2 4 6 8 10 12 14 16 18 20
Window size k
F-
m
ea
su
re
Figure 4. F-measure vs. Window Size k 
0.17
0.22
0.27
0.32
0.37
0.42
0.47
1 2 4 6 8 10 12 14 16 18 20
Keyphrase number n
Pr
ec
is
io
n
 
Figure 5. Precision vs. Keyphrase Number 
n 
0.02
0.07
0.12
0.17
0.22
0.27
0.32
0.37
0.42
0.47
1 2 4 6 8 10 12 14 16 18 20
Keyphrase number n
R
ec
al
l
Figure 6. Recall vs. Keyphrase Number n
0.05
0.1
0.15
0.2
0.25
0.3
1 2 4 6 8 10 12 14 16 18 20
Keyphrase number n
F-
m
ea
su
re
Figure 7. F-measure vs. Keyphrase Num-
ber n 
 
The proposed CollabRank method makes only 
use of the global information based on the global 
graph for the cluster. In order to investigate the 
relative contributions from the whole cluster and 
the single document to the final performance, we 
experiment with the method named RankFusion 
which makes both of the cluster-level global in-
formation and the document-level local informa-
tion. The overall word score WordScorefusion(vi) 
for word vi in a document in RankFusion is a lin-
ear combination of the global word score and the 
local word score as follows: 
where ??[0,1] is the fusion weight. Then the 
phrase score is computed based on the fusion 
scores of the words. The RankFusion method is 
the same with CollabRank if ?=1 and it is the 
same with SingleRank if ?=0.  
Figure 8 shows the F-measure curves for the 
RankFusion methods with different high-quality 
clustering algorithms under different fusion 
weights. We can see that when ??(0.5,1), the 
RankFusion methods with high-quality clusters 
can outperform both the corresponding SingleR-
ank and the corresponding CollabRank. However, 
the performance improvements of RankFusion 
over CollabRank are not significant. We can 
conclude that the cluster-level global information 
plays the key role for evaluating the true saliency 
of the words.  
0.27
0.28
0.29
0.3
0.31
0.32
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fusion weight ?
F-
m
ea
su
re
Gold Kmeans CompleteLink
AverageLink Divisive
 
Figure 8. RankFusion Results (F-measure) vs. Fusion 
Weight ? 
5 Conclusion and Future Work 
In this paper, we propose a novel approach 
named CollabRank for collaborative single-
document keyphrase extraction, which makes use 
of the mutual influences between documents in 
appropriate cluster context to better evaluate the 
saliency of words and phrases. Experimental re-
)()1()()( idociclusifusion vWordScorevWordScorevWordScore ??+?= ?? (9) 
975
sults demonstrate the good effectiveness of Col-
labRank. We also find that the clustering algo-
rithm is important for obtaining the appropriate 
cluster context and the low-quality clustering 
results will deteriorate the extraction perform-
ance. It is encouraging that most existing popular 
clustering algorithms can meet the demands of 
the proposed approach.    
The proposed collaborative framework has 
more implementations than the implementation 
based on the graph-based ranking algorithm in 
this study. In future work, we will explore other 
keyphrase extraction methods in the proposed 
collaborative framework to validate the robust-
ness of the framework.  
Acknowledgements 
This work was supported by the National Science 
Foundation of China (No.60703064), the Re-
search Fund for the Doctoral Program of Higher 
Education of China (No.20070001059) and the 
National High Technology Research and Devel-
opment Program of China (No.2008AA01Z421).  
References 
A. Berger and V. Mittal. 2000. OCELOT: A system for 
summarizing Web Pages. In Proceedings of SIGIR2000. 
K. Barker and N. Cornacchia. 2000. Using nounphrase 
heads to extract document keyphrases. In Canadian Confer-
ence on AI. 
O. Buyukkokten, H. Garcia-Molina, and A. Paepcke. 2001. 
Seeing the whole in parts: text summarization for web 
browsing on handheld devices. In Proceedings of 
WWW2001. 
M. Chen, J.-T. Sun, H.-J. Zeng and K.-Y. Lam. 2005. A 
practical system for keyphrase extraction for web pages. In 
Proceedings of CIKM2005. 
E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin, and C. G. 
Nevill-Manning. 1999. Domain-specific keyphrase extrac-
tion. Proceedings of IJCAI-99, pp. 668-673.  
C. Gutwin, G. W. Paynter, I. H. Witten, C. G. Nevill-
Manning and E. Frank. 1999. Improving browsing in digital 
libraries with keyphrase indexes. Journal of Decision Sup-
port Systems, 27, 81-104. 
K. M. Hammouda, D. N. Matute and M. S. Kamel. 2005. 
CorePhrase: keyphrase extraction for document clustering. 
In Proceedings of MLDM2005. 
A. Hulth. 2003. Improved automatic keyword extraction 
given more linguistic knowledge. In Proceedings of 
EMNLP2003, Japan, August. 
A. K. Jain, M. N. Murty and P. J. Flynn. 1999. Data cluster-
ing: a review. ACM Computing Surveys, 31(3):264-323. 
D. Kelleher and S. Luz. 2005. Automatic hypertext key-
phrase detection. In Proceedings of IJCAI2005. 
B. Krulwich and C. Burkey. 1996. Learning user informa-
tion interests through the extraction of semantically signifi-
cant phrases. In AAAI 1996 Spring Symposium on Machine 
Learning in Information Access.  
O. Medelyan and I. H. Witten. 2006. Thesaurus based auto-
matic keyphrase indexing. In Proceedings of JCDL2006. 
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing order 
into texts. In Proceedings of EMNLP2004. 
A. Mu?oz. 1996. Compound key word generation from 
document databases using a hierarchical clustering ART 
model. Intelligent Data Analysis, 1(1). 
T. D. Nguyen and M.-Y. Kan. 2007. Keyphrase extraction 
in scientific publications. In Proceedings of ICADL2007. 
P. Over. 2001. Introduction to DUC-2001: an intrinsic 
evaluation of generic news text summarization systems. In 
Proceedings of DUC2001. 
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998. The 
pagerank citation ranking: Bringing order to the web. Tech-
nical report, Stanford Digital Libraries. 
M. Song, I.-Y. Song and X. Hu. 2003. KPSpotter: a flexible 
information gain-based keyphrase extraction system. In 
Proceedings of WIDM2003. 
A. M. Steier and R. K. Belew. 1993. Exporting 
phrases: A statistical analysis of topical language.  In 
Proceedings of Second Symposium on Document Analysis 
and Information Retrieval, pp. 179-190. 
T. Tomokiyo and M. Hurst. 2003. A language model ap-
proach to keyphrase extraction. In: Proceedings of ACL 
Workshop on Multiword Expressions. 
K. Toutanova and C. D. Manning. 2000. Enriching the 
knowledge sources used in a maximum entropy Part-of-
Speech tagger. In Proceedings of EMNLP/VLC-2000. 
P. D. Turney. 2000. Learning algorithms for keyphrase ex-
traction. Information Retrieval, 2:303-336. 
P. D. Turney. 2003. Coherent keyphrase extraction via web 
mining. In Proc. of IJCAI-03, pages 434?439. 
X. Wan, J. Yang and J. Xiao. 2007a. Towards an iterative 
reinforcement approach for simultaneous document summa-
rization and keyword extraction. In Proceedings of 
ACL2007. 
I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G. 
Nevill-Manning. 1999. KEA: Practical automatic keyphrase 
extraction. Proceedings of Digital Libraries 99 (DL'99), pp. 
254-256. 
W.-T. Yih, J. Goodman and V. R. Carvalho. 2006. Finding 
advertising keywords on web pages. In Proceedings of 
WWW2006.  
H. Y. Zha. 2002. Generic summarization and keyphrase 
extraction using mutual reinforcement principle and sen-
tence clustering. In Proceedings of SIGIR2002, pp. 113-120. 
Y. Zhang, N. Zincir-Heywood, and E. Milios. 2004. Term-
Based Clustering and Summarization of Web Page Collec-
tions. In Proceedings of the Seventeenth Conference of the 
Canadian Society for Computational Studies of Intelligence. 
Y. Zhang, N. Zincir-Heywood and E. Milios. 2005. Narra-
tive text classification for automatic key phrase extraction in 
web document corpora. In Proceedings of WIDM2005. 
976
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 552?559,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Towards an Iterative Reinforcement Approach for Simultaneous 
Document Summarization and Keyword Extraction 
Xiaojun Wan                     Jianwu Yang                     Jianguo Xiao 
Institute of Computer Science and Technology  
Peking University, Beijing 100871, China 
{wanxiaojun,yangjianwu,xiaojianguo}@icst.pku.edu.cn
Abstract 
Though both document summarization and 
keyword extraction aim to extract concise 
representations from documents, these two 
tasks have usually been investigated inde-
pendently. This paper proposes a novel it-
erative reinforcement approach to simulta-
neously extracting summary and keywords 
from single document under the assump-
tion that the summary and keywords of a 
document can be mutually boosted. The 
approach can naturally make full use of the 
reinforcement between sentences and key-
words by fusing three kinds of relation-
ships between sentences and words, either 
homogeneous or heterogeneous. Experi-
mental results show the effectiveness of the 
proposed approach for both tasks. The cor-
pus-based approach is validated to work 
almost as well as the knowledge-based ap-
proach for computing word semantics.  
1 Introduction 
Text summarization is the process of creating a 
compressed version of a given document that de-
livers the main topic of the document. Keyword 
extraction is the process of extracting a few salient 
words (or phrases) from a given text and using the 
words to represent the text. The two tasks are simi-
lar in essence because they both aim to extract 
concise representations for documents. Automatic 
text summarization and keyword extraction have 
drawn much attention for a long time because they 
both are very important for many text applications, 
including document retrieval, document clustering, 
etc.  For example, keywords of a document can be 
used for document indexing and thus benefit to 
improve the performance of document retrieval, 
and document summary can help to facilitate users 
to browse the search results and improve users? 
search experience.  
Text summaries and keywords can be either 
query-relevant or generic. Generic summary and 
keyword should reflect the main topics of the 
document without any additional clues and prior 
knowledge. In this paper, we focus on generic 
document summarization and keyword extraction 
for single documents. 
Document summarization and keyword extrac-
tion have been widely explored in the natural lan-
guage processing and information retrieval com-
munities. A series of workshops and conferences 
on automatic text summarization (e.g. SUMMAC, 
DUC and NTCIR) have advanced the technology 
and produced a couple of experimental online sys-
tems. In recent years, graph-based ranking algo-
rithms have been successfully used for document 
summarization (Mihalcea and Tarau, 2004, 2005; 
ErKan and Radev, 2004) and keyword extraction 
(Mihalcea and Tarau, 2004). Such algorithms make 
use of ?voting? or ?recommendations? between 
sentences (or words) to extract sentences (or key-
words). Though the two tasks essentially share 
much in common, most algorithms have been de-
veloped particularly for either document summari-
zation or keyword extraction.  
Zha (2002) proposes a method for simultaneous 
keyphrase extraction and text summarization by 
using only the heterogeneous sentence-to-word 
relationships. Inspired by this, we aim to take into 
account all the three kinds of relationships among 
sentences and words (i.e. the homogeneous rela-
tionships between words, the homogeneous rela-
tionships between sentences, and the heterogene-
ous relationships between words and sentences) in 
552
a unified framework for both document summari-
zation and keyword extraction. The importance of 
a sentence (word) is determined by both the impor-
tance of related sentences (words) and the impor-
tance of related words (sentences). The proposed 
approach can be considered as a generalized form 
of previous graph-based ranking algorithms and 
Zha?s work (Zha, 2002).  
In this study, we propose an iterative reinforce-
ment approach to realize the above idea. The pro-
posed approach is evaluated on the DUC2002 
dataset and the results demonstrate its effectiveness 
for both document summarization and keyword 
extraction. Both knowledge-based approach and 
corpus-based approach have been investigated to 
compute word semantics and they both perform 
very well.  
The rest of this paper is organized as follows: 
Section 2 introduces related works. The details of 
the proposed approach are described in Section 3. 
Section 4 presents and discusses the evaluation 
results. Lastly we conclude our paper in Section 5. 
2 Related Works 
2.1 Document Summarization 
Generally speaking, single document summariza-
tion methods can be either extraction-based or ab-
straction-based and we focus on extraction-based 
methods in this study. 
Extraction-based methods usually assign a sali-
ency score to each sentence and then rank the sen-
tences in the document. The scores are usually 
computed based on a combination of statistical and 
linguistic features, including term frequency, sen-
tence position, cue words, stigma words, topic sig-
nature (Hovy and Lin, 1997; Lin and Hovy, 2000), 
etc. Machine learning methods have also been em-
ployed to extract sentences, including unsupervised 
methods (Nomoto and Matsumoto, 2001) and su-
pervised methods (Kupiec et al, 1995; Conroy and 
O?Leary, 2001; Amini and Gallinari, 2002; Shen et 
al., 2007). Other methods include maximal mar-
ginal relevance (MMR) (Carbonell and Goldstein, 
1998), latent semantic analysis (LSA) (Gong and 
Liu, 2001). In Zha (2002), the mutual reinforce-
ment principle is employed to iteratively extract 
key phrases and sentences from a document.   
Most recently, graph-based ranking methods, in-
cluding TextRank ((Mihalcea and Tarau, 2004, 
2005) and LexPageRank (ErKan and Radev, 2004) 
have been proposed for document summarization. 
Similar to Kleinberg?s HITS algorithm (Kleinberg, 
1999) or Google?s PageRank (Brin and Page, 
1998), these methods first build a graph based on 
the similarity between sentences in a document and 
then the importance of a sentence is determined by 
taking into account global information on the 
graph recursively, rather than relying only on local 
sentence-specific information. 
2.2 Keyword Extraction 
Keyword (or keyphrase) extraction usually in-
volves assigning a saliency score to each candidate 
keyword by considering various features. Krulwich 
and Burkey (1996) use heuristics to extract key-
phrases from a document. The heuristics are based 
on syntactic clues, such as the use of italics, the 
presence of phrases in section headers, and the use 
of acronyms. Mu?oz (1996) uses an unsupervised 
learning algorithm to discover two-word key-
phrases. The algorithm is based on Adaptive Reso-
nance Theory (ART) neural networks. Steier and 
Belew (1993) use the mutual information statistics 
to discover two-word keyphrases. 
Supervised machine learning algorithms have 
been proposed to classify a candidate phrase into 
either keyphrase or not. GenEx (Turney, 2000) and 
Kea (Frank et al, 1999; Witten et al, 1999) are 
two typical systems, and the most important fea-
tures for classifying a candidate phrase are the fre-
quency and location of the phrase in the document. 
More linguistic knowledge (such as syntactic fea-
tures) has been explored by Hulth (2003). More 
recently, Mihalcea and Tarau (2004) propose the 
TextRank model to rank keywords based on the 
co-occurrence links between words. 
3 Iterative Reinforcement Approach 
3.1 Overview 
The proposed approach is intuitively based on the 
following assumptions: 
Assumption 1: A sentence should be salient if it 
is heavily linked with other salient sentences, and a 
word should be salient if it is heavily linked with 
other salient words. 
Assumption 2: A sentence should be salient if it 
contains many salient words, and a word should be 
salient if it appears in many salient sentences. 
The first assumption is similar to PageRank 
which makes use of mutual ?recommendations? 
553
between homogeneous objects to rank objects. The 
second assumption is similar to HITS if words and 
sentences are considered as authorities and hubs 
respectively. In other words, the proposed ap-
proach aims to fuse the ideas of PageRank and 
HITS in a unified framework.  
In more detail, given the heterogeneous data 
points of sentences and words, the following three 
kinds of relationships are fused in the proposed 
approach: 
SS-Relationship: It reflects the homogeneous 
relationships between sentences, usually computed 
by their content similarity. 
WW-Relationship: It reflects the homogeneous 
relationships between words, usually computed by 
knowledge-based approach or corpus-based ap-
proach. 
SW-Relationship: It reflects the heterogeneous 
relationships between sentences and words, usually 
computed as the relative importance of a word in a 
sentence. 
Figure 1 gives an illustration of the relationships.  
 
Figure 1. Illustration of the Relationships 
 
The proposed approach first builds three graphs 
to reflect the above relationships respectively, and 
then iteratively computes the saliency scores of the 
sentences and words based on the graphs. Finally, 
the algorithm converges and each sentence or word 
gets its saliency score. The sentences with high 
saliency scores are chosen into the summary, and 
the words with high saliency scores are combined 
to produce the keywords. 
3.2 Graph Building 
3.2.1  Sentence-to-Sentence Graph ( SS-Graph)  
Given the sentence collection S={si | 1IiIm} of a 
document,  if each sentence is considered as a node, 
the sentence collection can be modeled as an undi-
rected graph by generating an edge between two 
sentences if their content similarity exceeds 0, i.e. 
an undirected link between si and sj (iKj) is con-
structed and the associated weight is their content 
similarity. Thus, we construct an undirected graph 
GSS to reflect the homogeneous relationship be-
tween sentences. The content similarity between 
two sentences is computed with the cosine measure. 
We use an adjacency matrix U to describe GSS with 
each entry corresponding to the weight of a link in 
the graph. U= [Uij]m?m is defined as follows: 



?

?
otherwise,
j, if iss
ss
U ji
ji
ij
0
rr
rr
(1) 
where is and js
r are the corresponding term vec-
tors of sentences si and sj respectively. The weight 
associated with term t is calculated with tft.isft,
where tft is the frequency of term t in the sentence 
and isft is the inverse sentence frequency of term t,
i.e. 1+log(N/nt), where N is the total number of 
sentences and nt is the number of sentences con-
taining term t in a background corpus. Note that 
other measures (e.g. Jaccard, Dice, Overlap, etc.) 
can also be explored to compute the content simi-
larity between sentences, and we simply choose the 
cosine measure in this study. 
Then U is normalized to U~ as follows to make 
the sum of each row equal to 1: 


 ?
erwise , oth 
U, if UUU
m
j
ij
m
j
ijij
ij
0
0~
11 (2) 
3.2.2  Word-to-Word Graph ( WW-Graph)  
Given the word collection T={tj|1IjIn } of a docu-
ment1 , the semantic similarity between any two 
words ti and tj can be computed using approaches 
that are either knowledge-based or corpus-based 
(Mihalcea et al, 2006).   
Knowledge-based measures of word semantic 
similarity try to quantify the degree to which two 
words are semantically related using information 
drawn from semantic networks. WordNet (Fell-
baum, 1998) is a lexical database where each 
 
1 The stopwords defined in the Smart system have been re-
moved from the collection. 
sentence
word
SS
WW
SW
554
unique meaning of a word is represented by a 
synonym set or synset. Each synset has a gloss that 
defines the concept that it represents. Synsets are 
connected to each other through explicit semantic 
relations that are defined in WordNet. Many ap-
proaches have been proposed to measure semantic 
relatedness based on WordNet. The measures vary 
from simple edge-counting to attempt to factor in 
peculiarities of the network structure by consider-
ing link direction, relative path, and density, such 
as  vector, lesk, hso, lch, wup, path, res, lin and jcn 
(Pedersen et al, 2004). For example, ?cat? and 
?dog? has higher semantic similarity than ?cat? 
and ?computer?. In this study, we implement the 
vector measure to efficiently evaluate the similari-
ties of a large number of word pairs. The vector 
measure (Patwardhan, 2003) creates a co?
occurrence matrix from a corpus made up of the 
WordNet glosses. Each content word used in a 
WordNet gloss has an associated context vector. 
Each gloss is represented by a gloss vector that is 
the average of all the context vectors of the words 
found in the gloss. Relatedness between concepts 
is measured by finding the cosine between a pair of 
gloss vectors. 
 Corpus-based measures of word semantic simi-
larity try to identify the degree of similarity be-
tween words using information exclusively derived 
from large corpora. Such measures as mutual in-
formation (Turney 2001), latent semantic analysis 
(Landauer et al, 1998), log-likelihood ratio (Dun-
ning, 1993) have been proposed to evaluate word 
semantic similarity based on the co-occurrence 
information on a large corpus. In this study, we 
simply choose the mutual information to compute 
the semantic similarity between word ti and tj as 
follows: 
)()(
)(log)(
ji
ji
ji tptp
,ttpN,ttsim
 (3) 
which indicates the degree of statistical depend-
ence between ti and tj. Here, N is the total number 
of words in the corpus and p(ti) and p(tj) are re-
spectively the probabilities of the occurrences of ti
and tj, i.e. count(ti)/N and count(tj)/N, where 
count(ti) and count(tj) are the frequencies of ti and tj.
p(ti, tj) is the probability of the co-occurrence of ti
and tj within a window with a predefined size k, i.e. 
count(ti, tj)/N, where count(ti, tj) is the number of 
the times ti and tj co-occur within the window.  
Similar to the SS-Graph, we can build an undi-
rected graph GWW to reflect the homogeneous rela-
tionship between words, in which each node corre-
sponds to a word and the weight associated with 
the edge between any different word ti and tj is 
computed by either the WordNet-based vector 
measure or the corpus-based mutual information 
measure. We use an adjacency matrix V to de-
scribe GWW with each entry corresponding to the 
weight of a link in the graph. V= [Vij]n?n, where Vij 
=sim(ti, tj) if iKj and Vij=0 if i=j.
Then V is similarly normalized to V~ to make 
the sum of each row equal to 1. 
3.2.3  Sentence-to-Word Graph ( SW-Graph)  
Given the sentence collection S={si | 1IiIm} and 
the word collection T={tj|1IjIn } of a document, 
we can build a weighted bipartite graph GSW from S
and T in the following way: if word tj appears in 
sentence si, we then create an edge between si and 
tj. A nonnegative weight aff(si,tj) is specified on the 
edge, which is proportional to the importance of 
word tj in sentence si, computed as follows: 

i
jj
st
tt
tt
ji isftf
isftf
,tsaff )(  (4)
where t represents a unique term in si and tft, isft
are respectively the term frequency in the sentence 
and the inverse sentence frequency.  
We use an adjacency (affinity) matrix 
W=[Wij]m?n to describe GSW  with each entry Wij 
corresponding to aff(si,tj). Similarly, W is normal-
ized to W~ to make the sum of each row equal to 1. 
In addition, we normalize the transpose of W, i.e. 
WT, to W? to make the sum of each row in WT
equal to 1. 
3.3 Reinforcement Algorithm 
We use two column vectors u=[u(si)]m?1 and v
=[v(tj)]n?1 to denote the saliency scores of the sen-
tences and words in the specified document. The 
assumptions introduced in Section 3.1 can be ren-
dered as follows: 
 j jjii suUsu )(~)( (5) 
 i iijj tvVtv )(~)( (6) 
 j jjii tvWsu )(?)( (7) 
555
 i iijj suWtv )(~)( (8) 
After fusing the above equations, we can obtain 
the following iterative forms: 
 n
j
jji
m
j
jjii tvW)suU*su
11
)(?)(~)( (9)
 m
i
iij
n
i
iijj suW)tvV*tv
11
)(~)(~)( (10)
And the matrix form is: 
vWuUu TT )* ?~ (11)
uWvVv TT )* ~~ (12) 
where * and ) specify the relative contributions to 
the final saliency scores from the homogeneous 
nodes and the heterogeneous nodes and we have 
*+)=1. In order to guarantee the convergence of 
the iterative form, u and v are normalized after 
each iteration. 
For numerical computation of the saliency 
scores, the initial scores of all sentences and words 
are set to 1 and the following two steps are alter-
nated until convergence, 
1. Compute and normalize the scores of sen-
tences: 
)(n-T)(n-T(n) )* 11 ?~ vWuUu ,
1
(n)(n)(n) / uuu
2. Compute and normalize the scores of words: 
)(n-T)(n-T(n) )* 11 ~~ uWvVv ,
1
(n)(n)(n) / vvv
where u(n) and v(n) denote the vectors computed at 
the n-th iteration.   
Usually the convergence of the iteration algo-
rithm is achieved when the difference between the 
scores computed at two successive iterations for 
any sentences and words falls below a given 
threshold (0.0001 in this study).  
4 Empirical Evaluation 
4.1 Summarization Evaluation 
4.1.1 Evaluation Setup 
We used task 1 of DUC2002 (DUC, 2002) for 
evaluation. The task aimed to evaluate generic 
summaries with a length of approximately 100 
words or less. DUC2002 provided 567 English 
news articles collected from TREC-9 for single-
document summarization task. The sentences in 
each article have been separated and the sentence 
information was stored into files.  
In the experiments, the background corpus for 
using the mutual information measure to compute 
word semantics simply consisted of all the docu-
ments from DUC2001 to DUC2005, which could 
be easily expanded by adding more documents. 
The stopwords were removed and the remaining 
words were converted to the basic forms based on 
WordNet. Then the semantic similarity values be-
tween the words were computed.   
We used the ROUGE (Lin and Hovy, 2003) 
toolkit (i.e.ROUGEeval-1.4.2 in this study) for 
evaluation, which has been widely adopted by 
DUC for automatic summarization evaluation. It 
measured summary quality by counting overlap-
ping units such as the n-gram, word sequences and 
word pairs between the candidate summary and the 
reference summary. ROUGE toolkit reported sepa-
rate scores for 1, 2, 3 and 4-gram, and also for 
longest common subsequence co-occurrences. 
Among these different scores, unigram-based 
ROUGE score (ROUGE-1) has been shown to 
agree with human judgment most (Lin and Hovy, 
2003). We showed three of the ROUGE metrics in 
the experimental results: ROUGE-1 (unigram-
based), ROUGE-2 (bigram-based), and ROUGE-
W (based on weighted longest common subse-
quence, weight=1.2).  
In order to truncate summaries longer than the 
length limit, we used the ?-l? option 2 in the 
ROUGE toolkit. 
4.1.2 Evaluation Results 
For simplicity, the parameters in the proposed ap-
proach are simply set to *=)=0.5, which means 
that the contributions from sentences and words 
are equally important. We adopt the WordNet-
based vector measure (WN) and the corpus-based 
mutual information measure (MI) for computing 
the semantic similarity between words.  When us-
ing the mutual information measure, we heuristi-
cally set the window size k to 2, 5 and 10, respec-
tively.  
The proposed approaches with different word 
similarity measures (WN and MI) are compared 
 
2 The ?-l? option is very important for fair comparison. Some 
previous works not adopting this option are likely to overes-
timate the ROUGE scores.  
556
with two solid baselines: SentenceRank and Mutu-
alRank. SentenceRank is proposed in Mihalcea and 
Tarau (2004) to make use of only the sentence-to-
sentence relationships to rank sentences, which 
outperforms most popular summarization methods. 
MutualRank is proposed in Zha (2002) to make use 
of only the sentence-to-word relationships to rank 
sentences and words. For all the summarization 
methods, after the sentences are ranked by their 
saliency scores, we can apply a variant form of the 
MMR algorithm to remove redundancy and choose 
both the salient and novel sentences to the sum-
mary. Table 1 gives the comparison results of the 
methods before removing redundancy and Table 2 
gives the comparison results of the methods after 
removing redundancy. 
 
System ROUGE-1 ROUGE-2 ROUGE-W
Our Approach
(WN) 0.47100
*# 0.20424*# 0.16336#
Our Approach
(MI:k=2) 0.46711
# 0.20195# 0.16257#
Our Approach
(MI:k=5) 0.46803
# 0.20259# 0.16310#
Our Approach
(MI:k=10) 0.46823
# 0.20301# 0.16294#
SentenceRank 0.45591 0.19201 0.15789 
MutualRank 0.43743 0.17986 0.15333 
Table 1. Summarization Performance before Re-
moving Redundancy (w/o MMR) 
 
System ROUGE-1 ROUGE-2 ROUGE-W
Our Approach
(WN) 0.47329
*# 0.20249# 0.16352#
Our Approach
(MI:k=2) 0.47281
# 0.20281# 0.16373#
Our Approach
(MI:k=5) 0.47282
# 0.20249# 0.16343#
Our Approach
(MI:k=10) 0.47223
# 0.20225# 0.16308#
SentenceRank 0.46261 0.19457 0.16018 
MutualRank 0.43805 0.17253 0.15221 
Table 2. Summarization Performance after Remov-
ing Redundancy (w/ MMR) 
 (* indicates that the improvement over SentenceRank is sig-
nificant and # indicates that the improvement over Mutual-
Rank is significant, both by comparing the 95% confidence 
intervals provided by the ROUGE package.)
Seen from Tables 1 and 2, the proposed ap-
proaches always outperform the two baselines over 
all three metrics with different word semantic 
measures. Moreover, no matter whether the MMR 
algorithm is applied or not, almost all performance 
improvements over MutualRank are significant 
and the ROUGE-1 performance improvements 
over SentenceRank are significant when using 
WordNet-based measure (WN). Word semantics 
can be naturally incorporated into the computation 
process, which addresses the problem that Sen-
tenceRank cannot take into account word seman-
tics, and thus improves the summarization per-
formance. We also observe that the corpus-based 
measure (MI) works almost as well as the knowl-
edge-based measure (WN) for computing word 
semantic similarity. 
In order to better understand the relative contri-
butions from the sentence nodes and the word 
nodes, the parameter * is varied from 0 to 1. The 
larger * is, the more contribution is given from the 
sentences through the SS-Graph, while the less 
contribution is given from the words through the 
SW-Graph. Figures 2-4 show the curves over three 
ROUGE scores with respect to *. Without loss of 
generality, we use the case of k=5 for the MI 
measure as an illustration. The curves are similar 
to Figures 2-4 when k=2 and k=10.   
 
0.435
0.44
0.445
0.45
0.455
0.46
0.465
0.47
0.475
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1*
RO
UG
E-
1
MI(w/o MMR) MI(w/ MMR)
WN(w/o MMR) WN(w/ MMR)
Figure 2. ROUGE-1 vs. *
0.17
0.175
0.18
0.185
0.19
0.195
0.2
0.205
0.21
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
*
RO
UG
E-
2
MI(w/o MMR) MI(w/ MMR)
WN(w/o MMR) WN(w/ MMR)
Figure 3. ROUGE-2 vs. *
557
0.151
0.153
0.155
0.157
0.159
0.161
0.163
0.165
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
*
RO
UG
E-
W
MI(w/o MMR) MI(w/ MMR)
WN(w/o MMR) WN(w/ MMR)
Figure 4. ROUGE-W vs. *
Seen from Figures 2-4, no matter whether the 
MMR algorithm is applied or not (i.e. w/o MMR 
or w/ MMR), the ROUGE scores based on either 
word semantic measure (MI or WN) achieves the 
peak when * is set between 0.4 and 0.6. The per-
formance values decrease sharply when * is very 
large (near to 1) or very small (near to 0). The 
curves demonstrate that both the contribution from 
the sentences and the contribution from the words 
are important for ranking sentences; moreover, the 
contributions are almost equally important. Loss of 
either contribution will much deteriorate the final 
performance.  
Similar results and observations have been ob-
tained on task 1 of DUC2001 in our study and the 
details are omitted due to page limit. 
4.2 Keyword Evaluation 
4.1.1   Evaluation Setup 
In this study we performed a preliminary evalua-
tion of keyword extraction. The evaluation was 
conducted on the single word level instead of the 
multi-word phrase (n-gram) level, in other words, 
we compared the automatically extracted unigrams 
(words) and the manually labeled unigrams 
(words). The reasons were that: 1) there existed 
partial matching between phrases and it was not 
trivial to define an accurate measure to evaluate 
phrase quality; 2) each phrase was in fact com-
posed of a few words, so the keyphrases could be 
obtained by combining the consecutive keywords.  
We used 34 documents in the first five docu-
ment clusters in DUC2002 dataset (i.e. d061-d065).  
At most 10 salient words were manually labeled 
for each document to represent the document and 
the average number of manually assigned key-
words was 6.8. Each approach returned 10 words 
with highest saliency scores as the keywords. The 
extracted 10 words were compared with the manu-
ally labeled keywords. The words were converted 
to their corresponding basic forms based on 
WordNet before comparison. The precision p, re-
call r, F-measure (F=2pr/(p+r)) were obtained for 
each document and then the values were averaged 
over all documents for evaluation purpose. 
4.1.2 Evaluation Results 
Table 3 gives the comparison results. The proposed 
approaches are compared with two baselines: 
WordRank and MutualRank. WordRank is pro-
posed in Mihalcea and Tarau (2004) to make use 
of only the co-occurrence relationships between 
words to rank words, which outperforms tradi-
tional keyword extraction methods. The window 
size k for WordRank is also set to 2, 5 and 10, re-
spectively. 
 
System Precision Recall F-measure
Our Approach
(WN) 0.413 0.504 0.454 
Our Approach
(MI:k=2) 0.428 0.485 0.455 
Our Approach
(MI:k=5) 0.425 0.491 0.456 
Our Approach
(MI:k=10) 0.393 0.455 0.422 
WordRank 
(k=2) 0.373 0.412 0.392 
WordRank 
(k=5) 0.368 0.422 0.393 
WordRank 
(k=10) 0.379 0.407 0.393 
MutualRank 0.355 0.397 0.375 
Table 3. The Performance of Keyword Extraction  
Seen from the table, the proposed approaches 
significantly outperform the baseline approaches. 
Both the corpus-based measure (MI) and the 
knowledge-based measure (WN) perform well on 
the task of keyword extraction. 
A running example is given below to demon-
strate the results: 
Document ID: D062/AP891018-0301 
Labeled keywords:
insurance earthquake insurer damage california Francisco 
pay 
Extracted keywords:
WN: insurance earthquake insurer quake california 
spokesman cost million wednesday damage 
MI(k=5): insurance insurer earthquake percent benefit 
california property damage estimate rate 
558
5 Conclusion and Future Work 
In this paper we propose a novel approach to si-
multaneously document summarization and key-
word extraction for single documents by fusing the 
sentence-to-sentence, word-to-word, sentence-to-
word relationships in a unified framework. The 
semantics between words computed by either cor-
pus-based approach or knowledge-based approach 
can be incorporated into the framework in a natural 
way. Evaluation results demonstrate the perform-
ance improvement of the proposed approach over 
the baselines for both tasks. 
In this study, only the mutual information meas-
ure and the vector measure are employed to com-
pute word semantics, and in future work many 
other measures mentioned earlier will be investi-
gated in the framework in order to show the ro-
bustness of the framework. The evaluation of key-
word extraction is preliminary in this study, and 
we will conduct more thorough experiments to 
make the results more convincing. Furthermore, 
the proposed approach will be applied to multi-
document summarization and keyword extraction, 
which are considered more difficult than single 
document summarization and keyword extraction. 
Acknowledgements 
This work was supported by the National Science 
Foundation of China (60642001). 
References 
M. R. Amini and P. Gallinari. 2002. The use of unlabeled data to 
improve supervised learning for text summarization. In Pro-
ceedings of SIGIR2002, 105-112. 
S. Brin and L. Page. 1998. The anatomy of a large-scale hypertex-
tual Web search engine. Computer Networks and ISDN Sys-
tems, 30(1?7). 
J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-
based reranking for reordering documents and producing 
summaries. In Proceedings of SIGIR-1998, 335-336. 
J. M. Conroy and D. P. O?Leary. 2001. Text summarization via 
Hidden Markov Models. In Proceedings of SIGIR2001, 406-
407. 
DUC. 2002. The Document Understanding Workshop 2002. 
http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html 
T. Dunning. 1993. Accurate methods for the statistics of surprise 
and coincidence. Computational Linguistics 19, 61?74. 
G. ErKan and D. R. Radev. 2004. LexPageRank: Prestige in 
multi-document text summarization. In Proceedings of 
EMNLP2004.
C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. 
The MIT Press.  
E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin, and C. G. 
Nevill-Manning. 1999. Domain-specific keyphrase extraction. 
Proceedings of IJCAI-99, pp. 668-673.  
Y. H. Gong and X. Liu. 2001. Generic text summarization using 
Relevance Measure and Latent Semantic Analysis. In Proceed-
ings of SIGIR2001, 19-25. 
E. Hovy and C. Y. Lin. 1997. Automated text summarization in 
SUMMARIST. In Proceeding of ACL?1997/EACL?1997 Wor-
shop on Intelligent Scalable Text Summarization.
A. Hulth. 2003. Improved automatic keyword extraction given 
more linguistic knowledge. In Proceedings of EMNLP2003,
Japan, August. 
J. M. Kleinberg. 1999. Authoritative sources in a hyperlinked 
environment. Journal of the ACM, 46(5):604?632. 
B. Krulwich and C. Burkey. 1996. Learning user information 
interests through the extraction of semantically significant 
phrases. In AAAI 1996 Spring Symposium on Machine Learn-
ing in Information Access.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A.trainable document 
summarizer. In Proceedings of SIGIR1995, 68-73. 
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduction to 
latent semantic analysis. Discourse Processes 25. 
C. Y. Lin and  E. Hovy. 2000. The automated acquisition of topic 
signatures for text Summarization. In Proceedings of ACL-
2000, 495-501. 
C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of summa-
ries using n-gram co-occurrence statistics. In Proceedings of 
HLT-NAACL2003, Edmonton, Canada, May. 
R. Mihalcea, C. Corley, and C. Strapparava. 2006. Corpus-based 
and knowledge-based measures of text semantic similarity. In 
Proceedings of AAAI-06.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing order into 
texts. In Proceedings of EMNLP2004.
R. Mihalcea and P.Tarau. 2005. A language independent algo-
rithm for single and multiple document summarization. In 
Proceedings of IJCNLP2005.
A. Mu?oz. 1996. Compound key word generation from document 
databases using a hierarchical clustering ART model. Intelli-
gent Data Analysis, 1(1). 
T. Nomoto and Y. Matsumoto. 2001. A new approach to unsuper-
vised text summarization. In Proceedings of SIGIR2001, 26-34. 
S. Patwardhan. 2003. Incorporating dictionary and corpus infor-
mation into a context vector measure of semantic relatedness. 
Master?s thesis, Univ. of Minnesota, Duluth. 
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. Word-
Net::Similarity ? Measuring the relatedness of concepts. In 
Proceedings of AAAI-04.
D. Shen, J.-T. Sun, H. Li, Q. Yang, and Z. Chen. 2007. Document 
Summarization using Conditional Random Fields. In Proceed-
ings of IJCAI 07.
A. M. Steier and R. K. Belew. 1993. Exporting phrases: A statisti-
cal analysis of topical language.  In Proceedings of Second 
Symposium on Document Analysis and Information Retrieval,
pp. 179-190. 
P. D. Turney. 2000. Learning algorithms for keyphrase extraction. 
Information Retrieval, 2:303-336. 
P. Turney. 2001. Mining the web for synonyms: PMI-IR versus 
LSA on TOEFL. In Proceedings of ECML-2001.
I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G. 
Nevill-Manning. 1999. KEA: Practical automatic keyphrase 
extraction. Proceedings of Digital Libraries 99 (DL'99), pp. 
254-256. 
H. Y. Zha. 2002. Generic summarization and keyphrase extraction 
using mutual reinforcement principle and sentence clustering. 
In Proceedings of SIGIR2002, pp. 113-120. 
559
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1840?1850,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Collective Opinion Target Extraction in Chinese Microblogs 
 
 
 
Xinjie Zhou, Xiaojun Wan* and Jianguo Xiao 
Institute of Computer Science and Technology   
The MOE Key Laboratory of Computational Linguistics   
Peking University  
No. 5, Yiheyuan Road, Beijing, China 
{zhouxinjie,wanxiaojun,xiaojianguo}@pku.edu.cn 
 
 
Abstract 
Microblog messages pose severe challenges 
for current sentiment analysis techniques due 
to some inherent characteristics such as the 
length limit and informal writing style. In this 
paper, we study the problem of extracting 
opinion targets of Chinese microblog messag-
es. Such fine-grained word-level task has not 
been well investigated in microblogs yet. We 
propose an unsupervised label propagation al-
gorithm to address the problem. The opinion 
targets of all messages in a topic are collec-
tively extracted based on the assumption that 
similar messages may focus on similar opinion 
targets. Topics in microblogs are identified by 
hashtags or using clustering algorithms. Ex-
perimental results on Chinese microblogs 
show the effectiveness of our framework and 
algorithms. 
1 Introduction 
Microblogging services such as Twitter 1 , Sina 
Weibo2 and Tencent Weibo3 have swept across the 
globe in recent years. Users of microblogs range 
from celebrities to ordinary people, who usually 
express their emotions or attitudes towards a broad 
range of topics. It is reported that there are more 
than 340 million tweets per day on Twitter and 
more than 200 million on Sina Weibo. A tweet 
means a post on Twitter. Since we mainly focus 
on Chinese microblogs instead of Twitter in this 
paper, we will refer to a post as a message. Each 
message is limited to 140 Chinese characters and 
usually contains several sentences. 
                                                          
* Xiaojun Wan is the corresponding author. 
1 https://twitter.com 
2 http://weibo.com/ 
3 http://t.qq.com/ 
Currently, researches on microblog sentiment 
analysis have been conducted on polarity classifi-
cation (Barbosa and Feng, 2010; Jiang el al., 2011; 
Speriosu et al, 2011) and have been proved to be 
useful in many applications, such as opinion poll-
ing (Tang et al, 2012), election prediction 
(Tumasjan et al, 2010) and even stock market 
prediction (Bollen et al, 2011). However, classify-
ing microblog texts at the sentence level is often 
insufficient for applications because it does not 
identify the opinion targets. In this paper, we will 
study the task of opinion target extraction for Chi-
nese microblog messages.  
Opinion target extraction aims to find the object 
to which the opinion is expressed. For example, in 
the sentence ?The sound quality is good!?, ?sound 
quality? is the opinion target. This task is mostly 
studied in customer review texts in which opinion 
targets are often referred as features or aspects 
(Liu, 2012). Most of the opinion target extraction 
approaches rely on dependency parsing (Zhuang et 
al., 2006; Jakob and Gurevych, 2010; Qiu et al, 
2011) and are regarded as a domain-dependent 
task (Li et al, 2012a). However, such approaches 
are not suitable for microblogs because the natural 
language processing tools perform poorly on mi-
croblog texts due to their inherent characteristics. 
Studies show that one of the state-of-the-art part-
of-speech taggers - OpenNLP only achieves the 
accuracy of 74% on tweets (Liu et al 2011). The 
syntactic analysis tool that generates dependency 
relation may perform even worse. Besides, mi-
croblog messages may express opinion in different 
ways and do not always contain opinion words, 
which lowers the performance of methods utiliz-
ing opinion words to find opinion targets.  
In this study, we propose an unsupervised 
method to collectively extract the opinion targets 
from opinionated sentences in the same topic. 
1840
Topics are directly identified by hashtags. We first 
present a dynamic programming based segmenta-
tion algorithm for Chinese hashtag segmentation. 
By leveraging the contents in a topic, our segmen-
tation algorithm can successfully identify out-of-
vocabulary words and achieve promising results. 
Afterwards, all the noun phrases in each sentence 
and the hashtag segments are extracted as opinion 
target candidates. We propose an unsupervised 
label propagation algorithm to collectively rank 
the candidates of all sentences based on the as-
sumption that similar sentences in a topic may 
share the same opinion targets. Finally, for each 
sentence, the candidate which gets the highest 
score after unsupervised label propagation is se-
lected as the opinion target. 
Our contributions in this study are summarized 
as follows: 1) our method considers not only the 
explicit opinion targets within the sentence but 
also the implicit opinion targets in the hashtag or 
mentioned in the previous sentence. 2) We devel-
op an efficient algorithm to segment Chinese 
hashtags. It can successfully identify out-of-
vocabulary words by leveraging contextual infor-
mation and help to improve the segmentation per-
formance of the messages in the topic. 3) We 
develop an unsupervised label propagation algo-
rithm for collective opinion target extraction. La-
bel propagation (Zhu and Ghahramani, 2002) aims 
to spread label distributions from a small training 
set throughout the graph.   However, our unsuper-
vised algorithm leverages the connection between 
two adjacent unlabeled nodes to find the correct 
labels for both of them. The proposed unsuper-
vised method does not need any training corpus 
which will cost much human labor especially for 
fine-grained annotation. 4) To the best of our 
knowledge, the task of opinion target extraction in 
microblogs has not been well studied yet. It is 
more challenging than microblog sentiment classi-
fication and opinion target extraction in review 
texts.  
2 Characteristics of Chinese Microblogs 
Most of previous microblog sentiment analysis 
researches focus on Twitter and especially in Eng-
lish. However, the analysis of Chinese microblogs 
has some differences with that of Twitter: 1) Chi-
nese word segmentation is a necessary step for 
Chinese sentiment analysis, but the existing seg-
mentation tool performs poorly on microblogs 
because the microblog texts are much different 
from regular texts. 2) Wang et al (2011) find that 
hashtags in English tweets are used to highlight 
the sentiment information such as ? #love?, 
?#sucks? or serve as user-annotated coarse topics 
such as ?#news?, ?#sports?. But in Chinese mi-
croblogs, most of the hashtags are used to indicate 
fine-grained topics such as #NBA??????# 
(#NBAFinalG7#). Besides, hashtags in Twitter 
always appear within a sentence such as ?I love 
#BarackObama!? while hashtags in Chinese mi-
croblogs are always isolated and are surrounded 
by two # symbols such as ?#??????# ??
?!? (?#BarackObama# I love him??). 
It is noteworthy that topics aggregated by the 
same hashtag play an important role in Chinese 
microblog websites. These websites often provide 
an individual webpage4 to list hot topics and invite 
people to participate in the discussion, where each 
topic consists of tens of thousands of messages 
with the same hashtag. The hot topics have a wide 
coverage of timely events and entities. Analyzing 
the opinion targets of these topics can help to get a 
deeper overview of the public attitudes towards 
the entities involved in the hot topics. 
3 Motivation 
As described above, #hashtags# in Chinese mi-
croblogs often indicate fine-grained topics. In this 
study, we aim to collectively extract the opinion 
targets of messages with the same hashtag, i.e. in 
the same topic. Opinion target of a sentence can be 
divided into two types, one of which called explic-
it target appears in the sentence such as ?I love 
Obama?, and the other one called implicit target 
                                                          
4 http://huati.weibo.com/ 
Topic Sentence 
#??????# 
#Property publicity 
of government offic
-ials# 
1. ????? 
(Just for show?) 
2. ???????????? 
(Property publicity is just a show 
in China.) 
#???????# 
#Philippine navy 
vessel hits Chinese 
fishing boat# 
1. ????????? 
(The government is not tough 
enough.) 
2. ??????????? 
(Why cannot the government take 
a tougher line?) 
Table 1. Motivation Examples 
1841
may appear out of the sentence, for example, the 
sentence ?Just for show!?  in Table 1 directly 
comments on the target in the hashtag ?#Property 
publicity of government officials#? . Such implicit 
opinion targets are not considered in previous 
works and are more difficult to extract than explic-
it targets. However, we believe that the contextual 
information will help to locate both of the two 
kinds of opinion targets because similar sentences 
in a topic may share the same opinion target, 
which provides the possibility for collective ex-
traction. 
Table 1 shows the motivation examples of two 
topics and four sentences. The two sentences in 
each topic are considered to be similar because 
they share several Chinese words. In the topic #?
?????# (#Property publicity of government 
officials#), the first sentence omits the opinion 
target. However, the second one contains an ex-
plicit target ?????? (?property publicity?) in 
the sentence. If we find the correct opinion target 
for sentence 2, we can infer that sentence 1 may 
have an implicit opinion target similar to the opin-
ion target in sentence 2. In the second topic, both 
sentences contain a noun word ???? (?govern-
ment?). The similarity between these two sentenc-
es may indicate that both of the two sentences are 
expressing opinion on ????. 
Based on the above observation, we can assume 
that similar sentences in a topic may have the 
same opinion targets. Such assumption can help to 
locate both explicit and implicit opinion targets. 
Following this idea, we firstly extract all the noun 
phrases in each sentence as opinion target candi-
dates after applying Chinese word segmentation 
and part-of-speech tagging. Afterwards, an unsu-
pervised label propagation algorithm is proposed 
to rank these candidates for all sentences in the 
topic. 
In our methods, hashtags are used to find gold-
standard topics. For messages without hashtags, an 
alternative way is to generate pseudo topics by 
clustering microblogs messages and then apply the 
proposed algorithm to each pseudo topic. The de-
tailed discussion of such general circumstance is 
shown in Section 5.7. 
4 Methodology 
4.1 Context-Aware Hashtag Segmentation 
In our approach, the Chinese word segmentations 
of hashtags and topic contents are treated separate-
ly. Existing Chinese word segmentation tools 
work poorly on microblog texts. The segmentation 
errors especially on opinion target words will di-
rectly influence the results of part-of-speech tag-
ging and candidate extraction. However, some of 
the opinion target words in a topic are often in-
cluded in the hashtag. By finding the correct seg-
ments of a hashtag and adding them to the user 
dictionary of the Chinese word segmentation tool, 
we can remarkably improve the overall segmenta-
tion performance.  
The following example can help to understand 
the idea better. In the topic #90????# (means 
?A young man hits an old man?), ?90?? (literally 
?90 later? and means a young man born in the 90s) 
is an important word because it is the opinion tar-
get of many sentences. However, existing Chinese 
word segmentation tools will regard it as two sep-
arate words ?90? and ??? (?later?). Then in the 
part-of-speech tagging stage, ?90? will be tagged 
as number and ??? will be tagged as localizer. As 
we only extract noun phrases as opinion target 
candidates, the wrong segmentation on ?90 ?? 
makes it impossible to find the right opinion target. 
Such error may occur many times in sentences that 
mention the word ?90?? and express opinion on 
it. In our method, the message texts of the topic 
are utilized to identify such out-of-vocabulary 
words based on its frequency in the topic. For ex-
ample, the high frequency of ?90?? is a strong 
indication that it should be regard as a single word. 
After segmenting the hashtag correctly into ?90?
/?/???, we can add the hashtag segments to the 
user dictionary of the segmentation tool to further 
segment the message texts of the topic. 
The basic idea for our hashtag segmentation al-
gorithm is to regard strings that appear frequently 
in a topic as words. Formally, given a hashtag h 
that contains n Chinese characters c1c2...cn. We 
want to segment into several words w1w2...wm, 
where each word is formed by one of more charac-
ters. 
Firstly, we define the stickiness score for a Chi-
nese string c1c2...cn based on the Symmetrical 
Conditional Probability (SCP) (Silva and Lopes, 
1999): 
1842
2
1 2
1 2
1 1
1
Pr( ... )( ... ) 1 Pr( ... )Pr( ... )1
n
n n
i i n
i
c c cSCP c c c
c c c cn ??
?
? ?
 (1) 
and SCP(c1) = Pr(c1)
2 for string with only one 
character. Pr(c1c2...cn) is the occurrence frequency 
of the string in the topic.  
Following (Li et al, 2012b), we smooth the 
SCP value by taking logarithm calculation. Be-
sides, the length of the string is taken into consid-
eration, 
 
1 2 1 2( ... ) log ( ... )n nSCP c c c n SCP c c c? ? ? (2) 
where n is the number of characters in the string. 
Then the stickiness score is defined by the sig-
moid function as follows: 
 
1 21 2 ( ... )
2( ... ) 1 nn SCP c c cStickiness c c c e ??? ?
 (3) 
For the hashtag h = c1c2...cn, we want to seg-
ment it into m words w1w2...wm which maximize 
the following equation, 
 
1
max ( )m i
i
Stickness w
?
?
  (4) 
The optimization of Equation (4) can be solved 
efficiently by dynamic programming which itera-
tively segments a string into two substrings. Dif-
ferent from (Li et al, 2012b) which calculates the 
SCP value of each string based on Microsoft Web 
N-Gram, our hashtag segmentation algorithm only 
uses the topic content and do not need any addi-
tional corpus. 
4.2 Candidate Extraction 
After segmenting the hashtag, all the hashtag seg-
ments with length greater than one are added to 
the user dictionary of the Chinese word segmenta-
tion tool ICTCLAS5 to further segment the mes-
sage texts of the topic. It also assigns the part-of-
speech tag for each word after segmentation. The 
noun phrases in each sentence is extracted by the 
following regular expression:
( | )( ) .noun adj noun adj noun??That means a 
noun phrase can only include nouns, adjectives 
and the Chinese word ??? (?of?). It should begin 
with a noun or adjective and end with a noun. For 
                                                          
5 http://www.ictclas.org/ 
example, in the following sentence, ???/n ?/u 
??/n ??/n ?/v ??/n ?/w? (?Chinese edu-
cation system has problems.?), ????????? 
(?Chinese education system?) and ???? (?prob-
lem?) are extracted as noun phrases.  
The character number of a noun phrase is lim-
ited between two and seven Chinese characters. 
For each sentence, all phrases that match the regu-
lar expression and meet the length restriction are 
extracted as explicit opinion target candidates. The 
hashtag segments are regarded as implicit candi-
dates for all sentences. Besides, some opinionated 
sentences in microblogs do not contain any noun 
phase, such as ?????? ? (?So boring!?). 
These sentences may express opinion on object 
that has been mentioned before. Therefore, the 
explicit candidates of the previous sentence in the 
same message are also taken as the implicit candi-
dates for such sentences.  
We do not use any syntactic parsing tool to ex-
tract noun phrases because the parsing results on 
microblogs are not reliable. A performance com-
parison of our rule based method and the state-of-
the-art syntactic parser will be shown in Section 5. 
4.3 Unsupervised Label Propagation for 
Candidate Ranking 
We simply assume that each opinionated sentence 
has one opinion target, which is consistent with 
Algorithm 1 Unsupervised Label Propagation 
Input: 
Graph:                              , ,G V E W?? ?  
Candidate Similarity:     M MS R ???  
Prior labeling:                 1 MvY R ???  for v?V  
Filtering Matrix:             M MvF R ??? for v?V 
Probability:                       pinj and pcont 
Output: 
 Label vector:                   1? MvY R ???  
1: for all v?V do 
2:      
v? vY Y?  
3: end for 
4: repeat 
5:       for all v?V do 
6:         
? ?
,
?v uv u vu V u vD W Y S F? ?? ? ??
 
7:        ? inj contv v vY p Y p D? ?  
8:       end for 
9: until convergence 
 
1843
the statistical result of our dataset that over 93% 
sentences have only one opinion target and each 
sentence has an average of 1.09 targets. Therefore, 
the most confident candidate of each sentence will 
be selected as the opinion target. In this section, 
we introduce an unsupervised graph-based label 
propagation algorithm to collectively rank the 
candidates of all sentences in a topic.  
Label propagation (Zhu and Ghahramani, 2002; 
Talukdar and Crammer, 2009) is a semi-
supervised algorithm which spreads label distribu-
tions from a small set of nodes seeded with some 
initial label information throughout the graph. The 
basic idea is to use information from the labeled 
nodes to label the adjacent nodes in the graph. 
However, our idea is to use the connection be-
tween different nodes to find the correct labels for 
all of them. Our unsupervised label propagation 
algorithm is summarized in Algorithm 1. Sentenc-
es are regarded as nodes and candidates of each 
sentence are regarded as labels. The label vector 
for each node is initialized based on the results of 
the candidate extraction step, which means no 
manually-labeled instances are needed in our 
model. In each iteration, the label vector of one 
node is propagated to the adjacent nodes. Both the 
sentence (node) similarity and the candidate (label) 
similarity are considered during propagation. Fi-
nally, we select the candidate with the highest 
score in the label vector as the opinion target for 
each sentence. The details of Algorithm 1 are pre-
sented as follows. 
Formally, an undirected graph , ,G V E W?? ?  
is built for each topic. A node v?V represents a 
sentence in the topic and an edge e = (a, b) ?E 
indicates that the labels of the two vertices should 
be similar.  W  is the normalized weight matrix to 
reflect the strength of this similarity. The similari-
ty between two nodes Wab is simply calculated by 
using the cosine measure (Salton et al, 1975) of 
the two sentences. 
 
( , ) a bab a b
a b
T TW cos T T T T
?? ? ?
 (5) 
where Ta and Tb are the term vectors of sentences a 
and b represented by the standard vector space 
model and weighted by term frequency. After cal-
culating the similarity matrix W, we get the weight 
matrix W  by normalizing each row of W such that 
1abb W ??
. 
For each sentence (node) v, a candidate set Cv is 
extracted in the previous step. The candidate set 
CT for the whole topic is the union of all Cv, 
 
vCT C?
 (6) 
The total number of candidates in the topic is 
denoted by M = |CT|. We calculate the candidate 
similarity matrix M MS R ???  based on Jaccard In-
dex: 
 ( ) ( ) 1( ) ( )
i j
ij
i j
A CT A CTS i j MA CT A CT? ? ? ?
 (7) 
where A(CTi) and A(CTj) are the Chinese character 
sets of the i-th and j-th candidates in CT respec-
tively. 
Candidates are regarded as labels in our model 
and without loss of generality we assume that the 
possible labels for the whole topic are L = {1?M} 
and each label in L corresponds to a unique can-
didate in CT. For each node v?V, a label vector 
1 MvY R ???  is initialized as 
  
? ? 10 k vv k k v
w L CY k ML C
??? ? ?? ??
 (8) 
where w is the initial weight of the candidate. We 
set w = we if Lk is an explicit candidate (extracted 
noun phrase) of v and w = wi if Lk is an implicit 
candidate (hashtag segment or inherited from pre-
vious sentence) of v. If Lk is not a candidate of the 
current sentence, then the corresponding value in 
the label vector is 0. These values which are ini-
tialized as zero should always remain zero during 
the propagation algorithm because the correspond-
ing label does not belong to the candidate set Cv of 
node v. To reset the values on these positions, a 
diagonal matrix M MvF R ???  is created for all nodes 
v, 
 
? ? ? ?? ?
1 0 10 0
v k
v kk
v k
YF k MY
? ??? ? ?? ???
 (9) 
where the subscript kk denotes the k-th position in 
the diagonal of matrix Fv. We can right-multiply 
Yv by Fv to clear the values of the invalid candi-
1844
dates. Figure 1 shows an example of creating the 
filtering matrix for a label vector. 
The propagation process is formalized via two 
possible actions: inject and continue, with pre-
defined probabilities pinj and pcont. Their sum is 
unit: pinj + pcont = 1. In each iteration, every node is 
influenced by its adjacent nodes. The propagation 
influence for each node v is 
 ? ?
,
?v uv u vu V u vD W Y S F? ?? ? ??
 (10) 
where 
u?Y  is the label vector of node u at the previ-
ous iteration. By multiplying the candidate simi-
larity matrix S, we aim to propagate the score of 
the i-th candidate of node u not only to the i-th 
candidate of node v, but also to all the other can-
didates. Wuv measures the strength of such propa-
gation. The filtering matrix Fv is used to clear the 
values of the invalid candidates as described 
above. 
Then the label vector of node v is updated as 
follow, 
  ? inj contv v vY p Y p D? ?  (11) 
When the positions of the largest values in all 
label vectors keep unchanged in ten iterations, it is 
regarded that the algorithm has already converged.  
5 Experiments 
5.1 Dataset 
We use the dataset from the 2012 Chinese Mi-
croblog Sentiment Analysis Evaluation (CMSAE)6 
held by China Computer Federation (CCF). There 
are three tasks in the evaluation: subjectivity clas-
sification, polarity classification and opinion target 
extraction. The dataset contains 20 topics collect-
ed from Tencent Weibo, a popular Chinese mi-
croblogging website. All the messages in a topic 
contain the same hashtag. The dataset has a total 
                                                          
6 http://tcci.ccf.org.cn/conference/2012/pages/page04_eva.
html. The dataset can also be publicly accessed on the website. 
of 17518 messages and 31675 sentences. In each 
topic, 100 messages are manually annotated with 
subjectivity, polarity and opinion targets. A total 
of 2361opinion targets are annotated for 2152 
opinionated sentences.  
5.2 Evaluation Metric  
Precision, recall and F-measure are used in the 
evaluation. Since expression boundaries are hard 
to define exactly in annotation guidelines (Wiebe 
et al, 2005), both the strict evaluation metric and 
the soft evaluation metric are used in CMSAE. 
Strict Evaluation: For a proposed opinion tar-
get, it is regarded as correct only if it covers the 
same span with the annotation result. Note that, in 
CMSAE, an opinion target should be proposed 
along with its polarity. The correctness of the po-
larity is also necessary. 
Soft Evaluation: The soft evaluation metric 
presented in (Johansson and Moschitti, 2010) is 
adopted by CMSAE. The span coverage c be-
tween each pair of the proposed target span s and 
the gold standard span s? is calculated as follows, 
 
? ?, s sc s s s
??? ? ?
 (12) 
In Equation 12, the operator |?| counts Chinese 
characters, and the intersection ? gives the set of 
characters that two spans have in common. 
Using the span coverage, the span set coverage 
C of a set of spans S with respect to another set S? 
is 
? ?, ( , )
s S s S
C S S c s s
? ?? ?
?? ???
 (13) 
The soft precision P and recall R of a proposed 
set of spans S?  with respect to a gold standard set 
S is defined as follows: 
 ? ?( , ) ( , )Precision Recall? | || |
C S S C S S
SS? ?
 (14) 
Note that the operator |?| counts spans in Equation 
14. The soft F-measure is the harmonic mean of 
soft precision and recall. 
5.3 Comparison Methods 
Our proposed approach is first compared with the 
CMSAE teams. 
CMSAE Teams: Sixteen teams participated in 
the opinion target extraction task of CMSAE. The 
methods of the top 3 teams are used as baselines 
? ?
1 0 0 0
0 1 0 0
1 1 0.5 0
0 0 1 0
0 0 0 0
v vY F
? ?
? ?
? ?? ? ?
? ?
? ?
? ?
 
Figure 1. Example of filtering matrix 
1845
here. They are denoted as Team-1, Team-2 and 
Team-3 respectively. The average result of all the 
sixteen teams is also included and is denoted as 
Team-Avg. We will briefly introduce the best 
team?s method. The most important component of 
their model is a topic-dependent opinion target 
lexicon which is called object sheet. If a word or 
phrase in the object sheet appears in a sentence or 
a hashtag, it is extracted as opinion target. The 
object sheet is manually built for each topic, 
which means their method cannot be applied to 
new topics. 
The following models are also used for compar-
ison. 
AssocMi: We implement the unsupervised 
method for opinion target extraction based on (Hu 
and Liu, 2004), which relies on association mining 
and a sentiment lexicon to extract frequent and 
infrequent product features. 
CRF: The CRF-based method used in (Jakob 
and Gurevych, 2010) is also used for comparison. 
We implement both the single-domain and cross-
domain models. Both models are evaluated using 
5-fold cross-validation. More specifically, the sin-
gle-domain model, denoted as CRF-S, trains dif-
ferent models for different topics. In each cross-
validation round, 80 percent of each topic is used 
for training and the other 20 percent is used for 
test. The cross-domain model, denoted as CRF-C, 
uses 16 topics for training and the rest 4 topics for 
test in each round.  
5.4 Comparison Results  
CMSAE requires all the teams to perform the sub-
jectivity and polarity classification task in advance. 
The opinion targets are extracted only for opinion-
ated sentences and should be proposed along with 
their polarity. To make a fair comparison, we di-
rectly use the subjectivity and polarity classifica-
tion results of Team-1. Then our unsupervised 
label propagation (ULP) method is used to extract 
the opinion targets for the proposed opinionated 
sentences. The parameters of our method are 
simply set as pinj = pcont = 0.5, we = 1 and wi = 0.5. 
Table 2 lists the comparison results with 
CMSAE teams. The average F-measure of all 
teams is 0.12 and 0.20 in strict and soft evaluation, 
respectively. It shows that opinion target extrac-
tion is a quite hard problem in Chinese microblogs. 
Our method performs better than all the teams. It 
increases by 10% and 13% in the two kinds of F-
measure compared to the best team. Besides, we 
do not need any prior information of the topics 
while Team-1 has to manually build an opinion 
target lexicon for each topic. 
To compare with the other opinion target ex-
traction methods, we only use gold-standard opin-
ionated sentences for evaluation and do not 
classify the polarity of the opinion targets. Table 3 
shows the experimental results of the four models. 
Our approach achieves the best result among them. 
AssocMi performs worst in strict evaluation but 
gets better results than the two CRF-based models 
in soft evaluation. The two CRF-based models 
achieve high precision but low recall. We can also 
observe that CRF-S is much more effective than 
CRF-C. It achieves high results because it has al-
ready seen the opinion targets in the training set. 
However, it is impossible to build such single-
domain model in practical applications because 
Method 
Strict Soft 
Precision Recall F-Measure Precision Recall F-Measure 
AssocMi 0.22 0.20 0.21 0.47 0.43 0.45 
CRF-C 0.59 0.15 0.24 0.70 0.18 0.28 
CRF-S 0.61 0.27 0.35 0.73 0.31 0.41 
ULP 0.43 0.39 0.41 0.61 0.55 0.58 
Table 3. Comparison results with baseline methods (only gold-standard opinionated sentences are used) 
Method. 
 
Strict Soft 
Precision Recall F-measure Precision Recall F-measure 
Team-Avg 0.17 0.09 0.12 0.29 0.15 0.20 
Team-3 0.26 0.16 0.20 0.40 0.25 0.31 
Team-2 0.31 0.18 0.23 0.40 0.22 0.29 
Team-1 0.30 0.27 0.29 0.39  0.36 0.37 
ULP 0.37 0.27 0.32 0.48 0.37 0.42 
Table 2. Comparison results with CMSAE teams (with subjectivity and polarity classification in advance) 
1846
labeled instances are not available for new topics. 
Our proposed method does not require any train-
ing data and gets an increase of 17% over CRF-S 
and 70% over CRF-C in strict evaluation. In terms 
of soft evaluation, we achieve an increase of 41% 
and 107% over the two CRF models.  
5.5 Parameter Sensitivity Study 
In this section, we study the parameter sensitivity. 
There are two major parameters in our algorithm: 
the initial weight w for both explicit and implicit 
candidates in Equation 8 and the injection proba-
bility pinj in Equation 11.  
The initial weights of explicit and implicit can-
didates are set differently because the explicit can-
didates are more likely to be the opinion targets. 
These two kinds of initial weights are denoted as 
we and wi for explicit and implicit candidate, re-
spectively. To study the impact of the initial 
weights, we fix we at 1 and tune wi because we 
only care about the relative contribution of them. 
The injection probability is fixed at 0.5. Figure 2(a) 
displays the opinion target extraction performance 
when wi varies from 0 to 1.5. Due to limited space, 
we only list the strict F-measure of opinion target 
extraction evaluated on opinioned sentences (same 
experimental setup as Table 3).  
In particular, when wi is equal to 0, only explicit 
candidates are considered. When wi becomes larg-
er than 1, the implicit candidates become more 
important than explicit candidates. From the curve 
in Figure 2(a), we can observe that the implicit 
candidates help to improve the performance sig-
nificantly when wi varies from 0 to 0.1. The per-
formance reaches the peak when wi = 0.7 and 
declines rapidly when wi gets larger than 1.  
To study the impact of injection probability pinj, 
we fix the initial weights for explicit and implicit 
candidates as 1 and 0.5, respectively. Figure 2(b) 
shows the results of opinion target extraction with 
respect to different values of the injection proba-
bility. We can observe that the performance keeps 
steady except for the two extreme values 0 and 1. 
From the above two figures, we can conclude that 
our proposed method performs well and robustly 
with a wide range of parameter values. 
5.6 Analysis of Candidate Extraction  
Candidate extraction is an important step in our 
proposed method. If the correct opinion target is 
not extracted as a candidate, the ranking step will 
be in vain. As described in Section 3, we develop 
a hashtag segmentation algorithm and use a rule 
based method to extract noun phrases from each 
sentence. We do not use any parsing tool because 
we believe the performance of these tools is not 
good enough when applied on microblogs. A 
quantitative comparison is shown in this section.  
We use one of the state-of-the-art syntactic 
analysis tools - Berkeley Parser (Petrov et al, 
2006) for comparison here. Noun phrases are di-
rectly extracted from the parsing results. Our 
method HS+Rule leverages the hashtag segments 
to enhance the segmentation result and extracts 
explicit candidate using a regular expression. To 
demonstrate the effectiveness of our hashtag seg-
mentation algorithm, the second comparison base-
line Rule directly uses ICTCLAS to segment the 
whole topic content and labels each word with its 
part-of-speech tag. The explicit candidates are ex-
tracted by using the same regular expression. 
The performance on candidate extraction is 
compared in Table 4. The second column shows 
the number of all extracted candidates for all the 
opinionated sentences by different methods. The 
third column shows the number of correct opinion 
targets among them. We can find that the two rule-
based models both outperform Berkeley Parser 
and our HS+Rule method finds 14% more correct 
opinion targets than Rule. It proves the effective-
ness of our hashtag segmentation algorithm. The 
Method Total Correct 
F-Measure of Opinion 
Target Extraction 
Strict Soft 
Berkley 
Parser 
4554 877 0.36 0.56 
Rule 4105 918 0.37 0.56 
HS + Rule 4094 1042 0.41 0.58 
Table 4. Performance of candidate extraction and 
opinion target extraction 
0.3
0.32
0.34
0.36
0.38
0.4
0.42
0.44
0 0.2 0.4 0.6 0.8 1
Precision
Recall
F-Measure
pinj 
0.3
0.32
0.34
0.36
0.38
0.4
0.42
0.44
0 0.2 0.4 0.6 0.8 1 1.2 1.4
Precision
Recall
F-Measure
wi 
(a) Initial Candidate Weight        (b) Injection Probability 
Figure 2. Influence of the parameters 
 
1847
total number of candidates extracted by HS+Rule 
is also less than the other two methods. Therefore, 
the performance of label propagation will be im-
proved when there are fewer candidates to rank. It 
can be demonstrated by the F-measure of opinion 
target extraction in the fourth and fifth columns. 
The experiments are conducted on opinionated 
sentence only as above. By using HS+Rule to ex-
tract candidates, our label propagation algorithm 
gets the highest F-measure in both evaluation met-
rics.  
5.7 Performance on Pseudo Topics by Mes-
sage Clustering 
In our collective extraction algorithm, topics are 
directly identified by hashtags. For messages 
without hashtags, we can first employ clustering 
algorithms to obtain pseudo topics (clusters) and 
then exploiting the topic-oriented algorithm for 
collective opinion target extraction. To test the 
performance of the proposed method in such cir-
cumstance, we use the popular clustering algo-
rithm - Affinity Propagation (Frey and Dueck, 
2007) to generate topics. The experimental results 
are shown in Table 5. APCluster means that the 
messages are clustered after removing all the 
hashtags. APCluster+HS means that all the 
hashtags are retained as normal texts for calculat-
ing message similarity. Therefore, the clustering 
performance can be largely improved. The stand-
ard cosine similarity is used to measure the dis-
tance between microblog messages for Affinity 
Propagation in the above two methods. The last 
method denoted as GoldCluster directly uses 
hashtags to identify the gold-standard topics which 
shows the upper bound of the performance. After 
clustering microblogs, the opinion targets of mes-
sages in each cluster are collectively extracted by 
the proposed unsupervised label propagation algo-
rithm. The experiments are conducted on opinion-
ated sentences only. 
From the results, we can see that clustering mi-
croblogs without hashtags is a quite difficult job 
which only gets an F-Measure of 0.27. However, 
the corresponding opinion target extraction per-
formance is still promising, which outperforms the 
AssocMi and CRF-C methods in Table 3. With the 
help of hashtags, the clustering performance of 
APCluster+HS is largely improved and the opin-
ion target extraction performance is also increased. 
It outperforms all the baseline methods in Table 3. 
The above results reveal that our proposed unsu-
pervised label propagation algorithm works well 
in pseudo topics and the performance can be in-
creased with better clustering results. Therefore, 
we can try to incorporate other social network in-
formation to improve the message clustering per-
formance, which will be studied in our future 
work. 
6 Related Work 
Sentiment analysis, a.k.a. opinion mining, is the 
field of studying and analyzing people?s opinions, 
sentiments, evaluations, appraisals, attitudes, and 
emotions (Liu, 2012). Most of the previous senti-
ment analysis researches focus on customer re-
views (Pang et al, 2002; Hu and Liu, 2004) and 
some of them focus on news (Kim and Hovy, 
2006) and blogs (Draya et al, 2009). However, 
sentiment analysis on microblogs has recently at-
tracted much attention and has been proved to be 
very useful in many applications. 
Classification of opinion polarity is the most 
common task studied in microblogs. Go et.al 
(2009) follow the supervised machine learning 
approach of Pang et al (2002) to classify the po-
larity of each tweet by distant supervision. The 
training dataset of their method is not manually 
labeled but automatically collected using the 
emoticons. Barbosa and Feng (2010) use the simi-
lar pseudo training data collected from three 
online websites which provide Twitter sentiment 
analysis services. Speriosu et al (2009) explore 
the possibility of exploiting the Twitter follower 
graph to improve polarity classification.  
Opinion target extraction is a fine-grained 
word-level task of sentiment analysis. Currently, 
this task has not been well studied in microblogs 
yet. It is mostly performed on product reviews 
where opinion targets are always described as 
product features or aspects. The pioneering re-
search on this task is conducted by Hu and Liu 
Clustering Method 
F-Measure 
of Clustering 
F-Measure of Opinion 
Target Extraction 
Strict Soft 
APCluster 0.27 0.35 0.50 
APCluster+HS 0.71 0.37 0.55 
GoldCluster 1.00 0.41 0.58 
Table 5.  Performance of clustering and opinion 
target extraction 
1848
(2004) who propose a method which extracts fre-
quent nouns and noun phrases as the opinion tar-
gets. Jakob and Gurevych (2010) model the 
problem as a sequence labeling task based on 
Conditional Random Fields (CRF). Qiu et al 
(2011) propose a double propagation method to 
extract opinion word and opinion target simulta-
neously. Liu et al (2012) use the word translation 
model in a monolingual scenario to mine the asso-
ciations between opinion targets and opinion 
words.  
7 Conclusion and Future Work  
In this paper, we study the problem of opinion 
target extraction in Chinese microblogs which has 
not been well investigated yet. We propose an un-
supervised label propagation algorithm to collec-
tively rank the opinion target candidates of all 
sentences in a topic. We also propose a dynamic 
programming based algorithm for segmenting 
Chinese hashtags. Experimental results show the 
effectiveness of our method. 
In future work, we will try to collect and anno-
tate data for microblogs in other languages to test 
the robustness of our method. The repost and reply 
messages can also be integrated into our graph 
model to help improve the results.  
Acknowledgments 
The work was supported by NSFC (61170166), 
Beijing Nova Program (2008B03) and National 
High-Tech R&D Program (2012AA011101).  
References  
Barbosa Luciano and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data. 
In Proceedings of the 23rd International Conference 
on Computational Linguistics: Posters. Association 
for Computational Linguistics, 2010. 
Johan Bollen, Huina Mao and Xiaojun Zeng. 2011. 
Twitter mood predicts the stock market. Journal of 
Computational Science 2.1 (2011): 1-8. 
G?rard Dray, Michel Planti?, Ali Harb, Pascal Poncelet, 
Mathieu Roche and Fran?ois Trousset. 2009. Opin-
ion Mining from Blogs. In International Journal of 
Computer Informa-tion Systems and Industrial Man-
agement Applications. 
Brendan J. Frey and Delbert Dueck. 2007. "Clustering 
by passing messages between data points." Science 
315.5814 (2007): 972-976. 
Alec Go, Richa Bhayani and Lei Huang. 2009. Twitter 
sentiment classification using distant supervision. 
CS224N Project Report, Stanford (2009): 1-12. 
Minqing Hu and Bing Liu. Mining and summarizing 
customer reviews. 2004. In Proceedings of the tenth 
ACM SIGKDD international conference on 
Knowledge discovery and data mining, pp. 168-177. 
ACM. 
Long Jiang , Mo Yu, Ming Zhou, Xiaohua Liu and 
Tiejun Zhao. 2011. Target-dependent twitter senti-
ment classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational 
Linguistics: Human Language Technologies, vol. 1, 
pp. 151-160. 
Niklas Jakob and Iryna Gurevych. Extracting opinion 
targets in a single-and cross-domain setting with 
conditional random fields. 2010. In Proceedings of 
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics. 
Richard Johansson and Alessandro Moschitti. 2010. 
Syntactic and semantic structure for opinion expres-
sion detection. Proceedings of the Fourteenth Con-
ference on Computational Natural Language 
Learning. Association for Computational Linguistics. 
Soo-Min Kim and Eduard Hovy. 2006. Extracting 
Opinions, Opinion Holders and Topics Expressed in 
Online News Media Text. In Proceedings of the 
ACL Workshop on Sentiment and Subjectivity in 
Text, 2006, pp. 1?8.  
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang and 
Xiaoyan Zhu. 2012a. Cross-Domain Co-Extraction 
of Sentiment and Topic Lexicons. In Proceedings of 
the 50th Annual Meeting of the Association for 
Computational Linguistics, pages 410?419, Jeju, 
Republic of Korea, 8-14 July 2012. 
Chenliang Li, Jianshu Weng, Qi He, Yuxia Yao, 
Anwitaman Datta, Aixin Sun and Bu-Sung Lee. 
2012b. Twiner: Named entity recognition in targeted 
twitter stream. In Proceedings of the 35th interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pp. 721-730. 
ACM. 
Xiaohua Liu, Kuan Li, Ming Zhou and Zhongyang 
Xiong. 2011. Collective semantic role labeling for 
tweets with clustering. In Proceedings of the Twen-
ty-Second international joint conference on Artificial 
1849
Intelligence-Volume Volume Three, pp. 1832-1837. 
AAAI Press. 
Bing Liu. 2012. Sentiment analysis and opinion mining. 
Synthesis Lectures on Human Language Technolo-
gies 5.1 (2012): 1-167. 
Kang Liu, Liheng Xu and Jun Zhao. 2012. Opinion 
Target Extraction Using Word-Based Translation 
Model. In Proceedings of the 2012 Joint Conference 
on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing. 
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 
2002. Thumbs up?: sentiment classification using 
machine learning techniques. In Proceedings of the 
ACL-02 conference on Empirical methods in natural 
language processing-Volume 10, pp. 79-86. Associa-
tion for Computational Linguistics. 
Slav Petrov, Leon Barrett, Romain Thibaux and Dan 
Klein. Learning accurate, compact, and interpretable 
tree annotation. 2006. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics 
and the 44th annual meeting of the Association for 
Computational Linguistics, pp. 433-440. 
Guang Qiu, Bing Liu, Jiajun Bu and Chun Chen. 2011. 
Opinion word expansion and target extraction 
through double propagation. Computational linguis-
tics 37, no. 1 (2011): 9-27. 
G. Salton, A. Wong and C. S. Yang. 1975. A Vector 
Space Model for Automatic Indexing, Communica-
tions of the ACM, vol. 18, nr. 11, pages 613?620. 
J. F. da Silva and G. P. Lopes. 1999. A local maxima 
method and a fair dispersion normalization for ex-
tracting multi-word units from corpora. In Proc. of 
the 6th Meeting on Mathematics of Language . 
Michael Speriosu, Nikita Sudan, Sid Upadhyay and 
Jason Baldridge. 2011. Twitter polarity classification 
with label propagation over lexical links and the fol-
lower graph. In Proceedings of the First Workshop 
on Unsupervised Learning in NLP, pp. 53-63. Asso-
ciation for Computational Linguistics, 2011. 
Partha Talukdar and Koby Crammer. New regularized 
algorithms for transductive learning. 2009. Machine 
Learning and Knowledge Discovery in Databases 
(2009): 442-457. 
Jie Tang, Yuan Zhang, Jimeng Sun, Jinhai Rao, Wen-
jing Yu, Yiran Chen and A. C. M. Fong. 2012. 
Quantitative study of individual emotional states in 
social networks. Affective Computing, IEEE Trans-
actions on 3, no. 2 (2012): 132-144. 
Andranik Tumasjan, Timm O. Sprenger, Philipp G. 
Sandner and Isabell M. Welpe. 2010. Predicting 
elections with twitter: What 140 characters reveal 
about political sentiment. In Proceedings of the 
fourth international aaai conference on weblogs and 
social media, pp. 178-185. 
X. Zhu and Z. Ghahramani. 2002. Learning from la-
beled and unlabeled data with label propagation. 
Technical report, CMU CALD tech report. 
Li Zhuang, Feng Jing and Xiaoyan Zhu. 2006. Movie 
review mining and summarization. In Proceedings of 
the ACM 15th Conference on Information and 
Knowledge Management, pages 43?50, Arlington, 
Virginia, USA, November. 
 
1850
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1828?1833,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Joint Decoding of Tree Transduction Models for Sentence Compression
Jin-ge Yao Xiaojun Wan Jianguo Xiao
Institute of Computer Science and Technology, Peking University, Beijing 100871, China
Key Laboratory of Computational Linguistic (Peking University), MOE, China
{yaojinge, wanxiaojun, xiaojianguo}@pku.edu.cn
Abstract
In this paper, we provide a new method for
decoding tree transduction based sentence
compression models augmented with lan-
guage model scores, by jointly decoding
two components. In our proposed so-
lution, rich local discriminative features
can be easily integrated without increasing
computational complexity. Utilizing an
unobvious fact that the resulted two com-
ponents can be independently decoded, we
conduct efficient joint decoding based on
dual decomposition. Experimental results
show that our method outperforms tradi-
tional beam search decoding and achieves
the state-of-the-art performance.
1 Introduction
Sentence compression is the task of generating a
grammatical and shorter summary for a long sen-
tence while preserving its most important informa-
tion. One specific instantiation is deletion-based
compression, namely generating a compression by
dropping words. Various approaches have been
proposed to challenge the task of deletion-based
compression. Earlier pioneering works (Knight
and Marcu, 2000) considered several insightful
approaches, including noisy-channel based gen-
erative models and discriminative decision tree
models. Structured discriminative compression
models (McDonald, 2006) are capable of inte-
grating rich features and have been proved effec-
tive for this task. Another powerful paradigm for
sentence compression should be mentioned here
is constraints-based compression,including inte-
ger linear programming solutions (Clarke and La-
pata, 2008) and first-order Markov logic networks
(Huang et al., 2012; Yoshikawa et al., 2012).
A notable class of methods that explicitly deal
with syntactic structures are tree transduction
models (Cohn and Lapata, 2007; Cohn and Lap-
ata, 2009). In such models a synchronous gram-
mar is extracted from a corpus of parallel syn-
tax trees with leaves aligned. Compressions are
generated from the grammar with learned weights.
Previous works have noticed that local coherence
is usually needed by introducing ngram language
model scores, which will make accurate decoding
intractable. Traditional approaches conduct beam
search to find approximate solutions (Cohn and
Lapata, 2009).
In this paper we propose a joint decoding strat-
egy to challenge this decoding task. We ad-
dress the problem as jointly decoding a simple
tree transduction model that only considers rule
weights and an ngram compression model. Al-
though either part can be independently solved by
dynamic programming, the naive way to integrate
two groups of partial scores into a huge dynamic
programming chart table is computationally im-
practical. We provide an effective dual decompo-
sition solution that utilizes the efficient decoding
of both parts. By integrating rich structured fea-
tures that cannot be efficiently involved in normal
formulation, results get significantly improved.
2 Motivation
Under the tree transduction models, the sentence
compression task is formulated as learning a map-
ping from an input source syntax tree to a target
tree with reduced number of leaves. This map-
ping is known as a synchronous grammar. The
synchronous grammar discussed through out this
paper will be synchronous tree substitution gram-
mar (STSG), as in previous studies.
In such formulations, sentence compression is
finding the best derivation from a syntax tree that
produces a simpler target tree, under the current
definition of grammar and learned parameters.
Each derivation is attached with a score. For the
sake of efficient decoding, the score often decom-
1828
poses with rules involved in the derivation. A typ-
ical score definition for a derivation y of source
tree x is in such form (Cohn and Lapata, 2008;
Cohn and Lapata, 2009):
S(x,y)=
?
r?y
w
T
?
r
(x)+logP (ngram(y)) (1)
The first term is a weighted sum of features ?
r
(x)
defined on each rule r. It is plausible to introduce
local scores from ngram models. The second term
in the above score definition is added with such
purpose.
Cohn and Lapata (2009) explained that ex-
act decoding of Equation 1 is intractable. They
proposed a beam search decoding strategy cou-
pled with cube-pruning heuristic (Chiang, 2007),
which can further improve decoding efficiency at
the cost of largely losing exactness in log probabil-
ity calculations. For efficiency reasons, rich local
ngram features have not been introduced as well.
3 Components of Joint Decoding
The score in Equation 1 consists of two parts: sum
of weighted rule features and local ngram scores
retrieved from a language model. There is an im-
plicit fact that either part can be used alone with
slight modifications to generate a coarse candidate
compression. Therefore, we can build a joint de-
coding system that consists of these two indepen-
dently decodable components.
In this section we will refer to these two in-
dependent models as the pure tree transduction
model and the pure ngram compression model,
described in Section 3.1 and Section 3.2 respec-
tively. There is a direct generalization of the
ngram model by introducing rich local features,
which results in the structured discriminative mod-
els (Section 3.3).
3.1 Pure Tree Transduction model
By merely considering scores from tree transduc-
tion rules, i.e. the first part of Equation 1, we can
have our scores factorized with rules. Then finding
the best derivation from a STSG grammar can be
easily solved by a dynamic programming process
described by Cohn and Lapata (2007).
This simplified pure tree transduction model
can still produce decent compressions if the rule
weights are properly learned during training.
3.2 Pure Ngram based Compression
The pure ngram based model will try to find
the most locally smooth compression, reflected
by having the maximum log probability score of
ngrams.
To avoid the trivial solution of deleting all
words, we find the target compression with speci-
fied length by dynamic programming.
Furthermore, we can integrate features other
than log probabilities. This is equivalent to using a
structured discriminative model with rich features
on ngrams of candidate compressions.
3.3 Structured Discriminative Model
The structured discriminative model proposed by
McDonald (2006) defines rich features on bigrams
of possible compressions. The score is defined as
weighted linear combination of those features:
f(x, z) =
|z|
?
j=2
w ? f(x, L(z
j?1
), L(z
j
)) (2)
where the functionL(z
k
) maps a token z
k
in com-
pression z back to the index of the original sen-
tence x. Decoding can still be efficiently done by
dynamic programming.
With rich local structural information, the struc-
tured discriminative model can play a complemen-
tary role to the tree transduction model that focus
more on global syntactic structures.
4 Joint Decoding
From now on the remaining issue is jointly de-
coding the components. Either part factorizes
over local structures: rules for the tree transduc-
tion model and ngrams for the language model or
structured discriminative model. We may build a
large dynamic programming table to utilize this
kind of locality. Unfortunately this is computa-
tionally impractical. It is mathematically equiva-
lent to perform exact dynamic programming de-
coding of Equation 1, which would consume
asymptotically O(SRL
2(n?1)V
)
1
time for build-
ing the chart (Cohn and Lapata, 2009). Cohn and
Lapata (2009) proposed a beam search approxima-
tion along with cube-pruning heuristics to reduce
the time complexity down to O(SRBV )
2
.
1
S, R, L and V denote respectively for the number of
source tree nodes, the number of rules, size of target lexicon
and number of variables involved in each rule.
2
B denotes the beam width.
1829
In this work we utilize the efficiency of indepen-
dent decoding from the two components respec-
tively and then combine their solutions according
to certain standards. This naturally results in a
dual decomposition (Rush et al., 2010) solution.
Dual decomposition has been applied in sev-
eral natural language processing tasks, including
dependency parsing (Koo et al., 2010), machine
translation (Chang and Collins, 2011; Rush and
Collins, 2011) and information extraction (Re-
ichart and Barzilay, 2012). However, the strength
of this inference strategy has seldom been noticed
in researches on language generation tasks.
We briefly describe the formulation here.
4.1 Description
We denote the pure tree transduction part and the
pure ngram part as g(y) and f(z) respectively.
Then joint decoding is equivalent to solving:
max
y?Y,z?Z
g(y) + f(z) (3)
s.t. z
kt
= y
kt
, ?k ? {1, ..., n}, ?t ? {0, 1},
where y denotes a derivation which yields a final
compression {y
1
, ...,y
m
}. This derivation comes
from a pure tree transduction model. z denotes the
compression composed of {z
1
, ..., z
m
} from an
ngram compression model. Without loss of gener-
ality, we consider y
k
and z
k
as indicators that take
value 1 if the k?s token of original sentence has
been preserved in the compression and 0 if it has
been deleted. In the constraints of problem 3, y
kt
or z
kt
denote indicator variables that take value 1
if y
k
or z
k
= t and 0 otherwise.
Let L(u,y, z) be the Lagrangian of (3). Then
the dual objective naturally factorizes into two
parts that can be evaluated independently:
L(u) = max
y?Y,z?Z
L(u,y, z)
= max
y?Y,z?Z
g(y) + f(z) +
?
k,t
u
kt
(z
kt
? y
kt
)
= max
y?Y
(g(y)?
?
k,t
u
kt
y
kt
) +
max
z?Z
(f(z) +
?
k,t
u
kt
z
kt
)
With this factorization, Algorithm 1 tries to
solve the dual problem min
u
L(u) by alternatively
decoding each component.
This framework is feasible and plausible in that
the two subproblems (line 3 and line 4 in Algo-
rithm 1) can be easily solved with slight modifica-
Algorithm 1 Dual Decomposition Joint Decoding
1: Initialization: u
(0)
k
= 0, ?k ? {1, ..., n}
2: for i = 1 to MAX ITER do
3: y
(i)
? argmax
y?Y
(g(y)?
?
k,t
u
(i?1)
kt
y
kt
)
4: z
(i)
? argmax
z?Z
(f(z) +
?
k,t
u
(i?1)
kt
z
kt
)
5: if y
(i)
kt
= z
(i)
kt
?k ?t then
6: return (y
(i)
, z
(i)
)
7: else
8: u
(i)
kt
? u
(i?1)
kt
? ?
i
(z
(i)
kt
? y
(i)
kt
)
9: end if
10: end for
tions on the values of the original dynamic pro-
gramming chart. Joint decoding of a pure tree
transduction model and a structured discriminative
model is almost the same.
The asymptotic time complexity of Algorithm 1
is O(k(SRV + L
2(n?1)
)), where k denotes the
number of iterations. This is a significant re-
duction of O(SRL
2(n?1)V
) by directly solving
the original problem and is also comparable to
O(SRBV ) of conducting beam search decoding.
We apply a similar heuristic with Rush and
Collins (2012) to set the step size ?
i
=
1
t+1
, where
t < i is the number of past iterations that increase
the dual value. This setting decreases the step
size only when the dual value moves towards the
wrong direction. We limit the maximum iteration
number to 50 and return the best primal solution
y
(i)
among all previous iterations for cases that do
not converge in reasonable time.
5 Experiments
5.1 Baselines
The pure tree transduction model and the discrim-
inative model naturally become part of our base-
lines for comparison
3
. Besides comparing our
methods against the tree-transduction model with
ngram scores by beam search decoding, we also
compare them against the available previous work
from Galanis and Androutsopoulos (2010). This
state-of-the-art work adopts a two-stage method to
rerank results generated by a discriminative maxi-
mum entropy model.
5.2 Data Preparation
We evaluated our methods on two standard cor-
pora
4
, refer to as Written and Spoken respectively.
3
The pure ngram language model should not be consid-
ered here as it requires additional length constraints and in
general does not produce competitive results at all merely by
itself.
4
Available at http://jamesclarke.net/research/resources
1830
We split the datasets according to Table 1.
Table 1: Dataset partition (number of sentences)
Corpus Training Development Testing
Written 1,014 324 294
Spoken 931 83 254
All tree transduction models require parallel
parse trees with aligned leaves. We parsed all sen-
tences with the Stanford Parser
5
and aligned sen-
tence pairs with minimum edit distance heuristic
6
. Syntactic features of the discriminative model
were also taken from these parse trees.
For systems involving ngram scores, we trained
a trigram language model on the Reuters Corpus
(Volume 1)
7
with modified Kneser-Ney smooth-
ing, using the widely used tool SRILM
8
.
5.3 Model Training
The training process of a tree transduction model
followed similarly to Cohn and Lapata (2007) us-
ing structured SVMs (Tsochantaridis et al., 2005).
The structured discriminative models were trained
according to McDonald (2006).
5.4 Evaluation Metrics
We assessed the compression results by the F1-
score of grammatical relations (provided by a
dependency parser) of generated compressions
against the gold-standard compression (Clarke and
Lapata, 2006). All systems were controlled to pro-
duce similar compression ratios (CR) for fair com-
parison. We also reported manual evaluation on a
sampled subset of 30 sentences from each dataset.
Three unpaid volunteers with self-reported fluency
in English were asked to rate every candidate. Rat-
ings are in the form of 1-5 scores for each com-
pression.
6 Results
We report test set performance of the struc-
tured discriminative model, the pure tree transduc-
tion (T3), Galanis and Androutsopoulos (2010)?s
method (G&A2010), tree transduction with lan-
guage model scores by beam search and the pro-
posed joint decoding solutions.
5
http://nlp.stanford.edu/software/lex-parser.shtml
6
Ties were broken by always aligning a token in compres-
sion to its last appearance in the original sentence. This may
better preserve the alignments of full constituents.
7
http://trec.nist.gov/data/reuters/reuters.html
8
http://www-speech.sri.com/projects/srilm/
Table 2 shows the compression ratios and F-
measure of grammatical relations in average for
each dataset. Table 3 presents averaged human rat-
ing results for each dataset. We carried out pair-
wise t-test to examine the statistical significance
of the differences
9
. In both datasets joint decod-
ing with dual decomposition solution outperforms
other systems, especially when structured models
involved. We can also find certain improvements
of joint modeling with dual decomposition on the
original beam search decoding of Equation 1, un-
der very close compression ratios.
Joint decoding of pure tree transduction and dis-
criminative model gives better performance than
the joint model of tree transduction and language
model. From Table 3 we can see that integrat-
ing discriminative model will mostly improve the
preservation of important information rather than
grammaticality. This is reasonable under the fact
that the language model is trained on large scale
data and will often preserve local grammatical co-
herence, while the discriminative model is trained
on small but more compression specific corpora.
Table 2: Results of automatic evaluation. (?:
sig. diff. from T3+LM(DD); *: sig. diff. from
T3+Discr.(DD) for p < 0.01)
Written CR(%) GR-F1(%)
Discriminative 70.3 52.4
??
G&A2010 71.6 60.2
?
Pure Tree-Transduction 72.6 52.3
??
T3+LM (Beam Search) 70.4 58.8
?
T3+LM (Dual Decomp.) 70.7 60.5
T3+Discr. (Dual Decomp.) 71.0 62.3
Gold-Standard 71.4 100.0
Spoken CR(%) GR-F1(%)
Discriminative 69.5 50.6
??
G&A2010 71.7 59.2
?
Pure Tree-Transduction 73.6 53.8
??
T3+LM (Beam Search) 75.5 59.5
?
T3+LM (Dual Decomp.) 75.3 61.5
T3+Discr. (Dual Decomp.) 74.9 63.3
Gold-Standard 72.4 100.0
Table 4 shows some examples of compressed
sentences produced by all the systems in compar-
ison. The two groups of outputs are compressions
of one sentence from the Written corpora and
the Spoken corpora respectively. Ungrammatical
compressions can be found very often by several
baselines for different reasons, such as the outputs
from pure tree transduction and the discriminative
model in the first group. The reason behind the
9
For all multiple comparisons in this paper, significance
level was adjusted by the Holm-Bonferroni method.
1831
Table 3: Results of human rating. (?: sig.
diff. from T3+LM(DD); *: sig. diff. from
T3+Discr.(DD), for p < 0.01)
Written GR. Imp. CR(%)
Discriminative 3.92
??
3.46
??
70.6
G&A2010 4.11
??
3.50
??
72.4
Pure Tree-Transduction 3.85
??
3.42
??
70.1
T3+LM (Beam Search) 4.22
??
3.69
?
73.0
T3+LM (Dual Decomp.) 4.63 3.98 73.2
T3+Discr. (Dual Decomp.) 4.62 4.25 73.5
Gold-Standard 4.89 4.76 72.9
Spoken GR. Imp. CR(%)
Discriminative 3.95
??
3.62
??
71.2
G&A2010 4.09
??
3.96
?
72.5
Pure Tree-Transduction 3.92
??
3.55
??
71.4
T3+LM (Beam Search) 4.20
?
3.78
?
75.0
T3+LM (Dual Decomp.) 4.35 4.18 74.5
T3+Discr. (Dual Decomp.) 4.47 4.26 74.7
Gold-Standard 4.83 4.80 73.1
under generation of pure tree transduction is that it
mainly deals with global syntactic integrity merely
in terms of the application of synchronous rules.
Introducing language model scores will smooth
the candidate compressions and avoid many ag-
gressive decisions of tree transduction. Discrim-
inative models are good at local decisions with
poor consideration of grammaticality. We can see
that the joint models have collected their predic-
tive power together. Unfortunately we can still
observe some redundancy from our outputs in the
examples. The size of training corpus is not large
enough to provide enough lexicalized information.
On the other hand, the time consumption of
the joint model with dual decomposition decoding
in our experiments matched the aforementioned
asymptotic analysis. The training process based
on new decoding method consumes similar time
as beam search with cube-pruning heuristic.
7 Conclusion and Future Work
In this paper we propose a joint decoding scheme
for tree transduction based sentence compression.
Experimental results suggest that the proposed
framework works well. The overall performance
gets further improved under our framework by in-
troducing the structured discriminative model.
As several recent efforts have focused on ex-
tracting large-scale parallel corpus for sentence
compression (Filippova and Altun, 2013), we
would like to study how larger corpora can af-
fect tree transduction and our joint decoding so-
Table 4: Example outputs
Original: It was very high for people who took their
full-time education beyond the age of 18 , and higher
among women than men for all art forms except jazz
and art galleries .
Discr.: It was high for people took education higher
among women .
(Galanis and Androutsopoulos, 2010): It was high for
people who took their education beyond the age of 18 ,
and higher among women .
Pure T3: It was very high for people who took .
T3+LM-BeamSearch: It was very high for people who
took their education beyond the age of 18 , and higher
among women than men .
T3+LM-DualDecomp: It was very high for people who
took their education beyond the age of 18 , and higher
among women than men .
T3+Discr.: It was high for people who took education
beyond the age of 18 , and higher among women than
men .
Gold-Standard: It was very high for people who took
full-time education beyond 18 , and higher among
women for all except jazz and galleries .
Original: But they are still continuing to search the
area to try and see if there were , in fact , any further
shooting incidents .
Discr.: they are continuing to search the area to try and
see if there were , further shooting incidents .
(Galanis and Androutsopoulos, 2010): But they are still
continuing to search the area to try and see if there
were , in fact , any further shooting incidents .
Pure T3: they are continuing to search the area to try
and see if there were any further shooting incidents .
T3+LM-BeamSearch: But they are continuing to
search the area to try and see if there were , in fact ,
any further shooting incidents .
T3+LM-DualDecomp: But they are continuing to
search the area to try and see if there were any further
shooting incidents .
T3+Discr.: they are continuing to search the area to try
and see if there were further shooting incidents .
Gold-Standard: they are continuing to search the area
to see if there were any further incidents .
lution. Meanwhile, We would like to explore on
how other text-rewriting problems can be formu-
lated as a joint model and be applicable to similar
strategies described in this work.
Acknowledgements
This work was supported by National Hi-Tech Re-
search and Development Program (863 Program)
of China (2014AA015102, 2012AA011101) and
National Natural Science Foundation of China
(61170166, 61331011). We also thank the anony-
mous reviewers for very helpful comments.
The contact author of this paper, according to
the meaning given to this role by Peking Univer-
sity, is Xiaojun Wan.
1832
References
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through
lagrangian relaxation. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 26?37, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201?228.
James Clarke and Mirella Lapata. 2006. Models
for sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computa-
tional Linguistics, pages 377?384. Association for
Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31:273?381.
Trevor Cohn and Mirella Lapata. 2007. Large mar-
gin synchronous generation and its application to
sentence compression. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), pages
73?82, Prague, Czech Republic, June. Association
for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics-Volume 1, pages 137?144. Asso-
ciation for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research, 34:637?674.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compres-
sion. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1481?1491, Seattle, Washington, USA,
October. Association for Computational Linguistics.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 885?893, Los Angeles, California,
June. Association for Computational Linguistics.
Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.
2012. Using first-order logic to compress sentences.
In AAAI.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. In AAAI/IAAI, pages 703?710.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1288?1298, Cambridge, MA, October.
Association for Computational Linguistics.
Ryan T McDonald. 2006. Discriminative sentence
compression with soft syntactic evidence. In EACL.
Roi Reichart and Regina Barzilay. 2012. Multi-event
extraction guided by global constraints. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 70?
79, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
Alexander M. Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
72?82, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Alexander M Rush and Michael Collins. 2012. A tuto-
rial on dual decomposition and lagrangian relaxation
for inference in natural language processing. Jour-
nal of Artificial Intelligence Research, 45:305?362.
Alexander M Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1?11, Cambridge, MA, October.
Association for Computational Linguistics.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent out-
put variables. In Journal of Machine Learning Re-
search, pages 1453?1484.
Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and
Manabu Okumura. 2012. Sentence compression
with semantic role constraints. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Short Papers-Volume 2, pages
349?353. Association for Computational Linguis-
tics.
1833
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 917?926,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Cross-Language Document Summarization Based on Machine 
Translation Quality Prediction 
 
Xiaojun Wan, Huiying Li and Jianguo Xiao 
Institute of Compute Science and Technology, Peking University, Beijing 100871, China 
Key Laboratory of Computational Linguistics (Peking University), MOE, China 
{wanxiaojun,lihuiying,xiaojianguo}@icst.pku.edu.cn 
 
 
Abstract 
 
Cross-language document summarization is a 
task of producing a summary in one language 
for a document set in a different language. Ex-
isting methods simply use machine translation 
for document translation or summary transla-
tion. However, current machine translation 
services are far from satisfactory, which re-
sults in that the quality of the cross-language 
summary is usually very poor, both in read-
ability and content.  In this paper, we propose 
to consider the translation quality of each sen-
tence in the English-to-Chinese cross-language 
summarization process. First, the translation 
quality of each English sentence in the docu-
ment set is predicted with the SVM regression 
method, and then the quality score of each sen-
tence is incorporated into the summarization 
process. Finally, the English sentences with 
high translation quality and high informative-
ness are selected and translated to form the 
Chinese summary. Experimental results dem-
onstrate the effectiveness and usefulness of the 
proposed approach.  
 
1 Introduction 
Given a document or document set in one source 
language, cross-language document summariza-
tion aims to produce a summary in a different 
target language. In this study, we focus on Eng-
lish-to-Chinese document summarization for the 
purpose of helping Chinese readers to quickly 
understand the major content of an English docu-
ment or document set. This task is very impor-
tant in the field of multilingual information ac-
cess.  
Till now, most previous work focuses on 
monolingual document summarization, but 
cross-language document summarization has re-
ceived little attention in the past years. A 
straightforward way for cross-language docu-
ment summarization is to translate the summary 
from the source language to the target language 
by using machine translation services. However, 
though machine translation techniques have been 
advanced a lot, the machine translation quality is 
far from satisfactory, and in many cases, the 
translated texts are hard to understand. Therefore, 
the translated summary is likely to be hard to 
understand by readers, i.e., the summary quality 
is likely to be very poor. For example, the trans-
lated Chinese sentence for an ordinary English 
sentence (?It is also Mr Baker who is making the 
most of presidential powers to dispense lar-
gesse.?) by using Google Translate is ?????
????????????????????. 
The translated sentence is hard to understand 
because it contains incorrect translations and it is 
very disfluent. If such sentences are selected into 
the summary, the quality of the summary would 
be very poor.  
In order to address the above problem, we 
propose to consider the translation quality of the 
English sentences in the summarization process. 
In particular, the translation quality of each Eng-
lish sentence is predicted by using the SVM re-
gression method, and then the predicted MT 
quality score of each sentence is incorporated 
into the sentence evaluation process, and finally 
both informative and easy-to-translate sentences 
are selected and translated to form the Chinese 
summary.  
An empirical evaluation is conducted to evalu-
ate the performance of machine translation qual-
ity prediction, and a user study is performed to 
evaluate the cross-language summary quality. 
The results demonstrate the effectiveness of the 
proposed approach.  
The rest of this paper is organized as follows: 
Section 2 introduces related work. The system is 
overviewed in Section 3. In Sections 4 and 5, we 
present the detailed algorithms and evaluation 
917
results of machine translation quality prediction 
and cross-language summarization, respectively. 
We discuss in Section 6 and conclude this paper 
in Section 7.  
2 Related Work 
2.1 Machine Translation Quality Prediction 
Machine translation evaluation aims to assess the 
correctness and quality of the translation. Usu-
ally, the human reference translation is provided, 
and various methods and metrics have been de-
veloped for comparing the system-translated text 
and the human reference text. For example, the 
BLEU metric, the NIST metric and their relatives 
are all based on the idea that the more shared 
substrings the system-translated text has with the 
human reference translation, the better the trans-
lation is. Blatz et al (2003) investigate training 
sentence-level confidence measures using a vari-
ety of fuzzy match scores. Albrecht and Hwa 
(2007) rely on regression algorithms and refer-
ence-based features to measure the quality of 
sentences.  
Transition evaluation without using reference 
translations has also been investigated. Quirk 
(2004) presents a supervised method for training 
a sentence level confidence measure on transla-
tion output using a human-annotated corpus. 
Features derived from the source sentence and 
the target sentence (e.g. sentence length, perplex-
ity, etc.) and features about the translation proc-
ess are leveraged. Gamon et al (2005) investi-
gate the possibility of evaluating MT quality and 
fluency at the sentence level in the absence of 
reference translations, and they can improve on 
the correlation between language model perplex-
ity scores and human judgment by combing these 
perplexity scores with class probabilities from a 
machine-learned classifier. Specia et al (2009) 
use the ICM theory to identify the threshold to 
map a continuous predicted score into ?good? or 
?bad? categories. Chae and Nenkova (2009) use 
surface syntactic features to assess the fluency of 
machine translation results.   
In this study, we further predict the translation 
quality of an English sentence before the ma-
chine translation process, i.e., we do not leverage 
reference translation and the target sentence.  
2.2 Document Summarization  
Document summarization methods can be gener-
ally categorized into extraction-based methods 
and abstraction-based methods. In this paper, we 
focus on extraction-based methods. Extraction-
based summarization methods usually assign 
each sentence a saliency score and then rank the 
sentences in a document or document set.  
  For single document summarization, the sen-
tence score is usually computed by empirical 
combination of a number of statistical and lin-
guistic feature values, such as term frequency, 
sentence position, cue words, stigma words, 
topic signature (Luhn 1969; Lin and Hovy, 2000). 
The summary sentences can also be selected by 
using machine learning methods (Kupiec et al, 
1995; Amini and Gallinari, 2002) or graph-based 
methods (ErKan and Radev, 2004; Mihalcea and 
Tarau, 2004). Other methods include mutual re-
inforcement principle (Zha 2002; Wan et al, 
2007). 
  For multi-document summarization, the cen-
troid-based method (Radev et al, 2004) is a typi-
cal method, and it scores sentences based on 
cluster centroids, position and TFIDF features. 
NeATS (Lin and Hovy, 2002) makes use of new 
features such as topic signature to select impor-
tant sentences. Machine Learning based ap-
proaches have also been proposed for combining 
various sentence features (Wong et al, 2008).  
The influences of input difficulty on summariza-
tion performance have been investigated in 
(Nenkova and Louis, 2008). Graph-based meth-
ods have also been used to rank sentences in a 
document set. For example, Mihalcea and Tarau 
(2005) extend the TextRank algorithm to com-
pute sentence importance in a document set. 
Cluster-level information has been incorporated 
in the graph model to better evaluate sentences 
(Wan and Yang, 2008). Topic-focused or query 
biased multi-document summarization has also 
been investigated (Wan et al, 2006). Wan et al 
(2010) propose the EUSUM system for extract-
ing easy-to-understand English summaries for 
non-native readers.  
Several pilot studies have been performed for 
the cross-language summarization task by simply 
using document translation or summary transla-
tion. Leuski et al (2003) use machine translation 
for English headline generation for Hindi docu-
ments. Lim et al (2004) propose to generate a 
Japanese summary without using a Japanese 
summarization system, by first translating Japa-
nese documents into Korean documents, and 
then extracting summary sentences by using Ko-
rean summarizer, and finally mapping Korean 
summary sentences to Japanese summary sen-
tences. Chalendar et al (2005) focuses on se-
mantic analysis and sentence generation tech-
niques for cross-language summarization. Orasan 
918
and Chiorean (2008) propose to produce summa-
ries with the MMR method from Romanian news 
articles and then automatically translate the 
summaries into English. Cross language query 
based summarization has been investigated in 
(Pingali et al, 2007), where the query and the 
documents are in different languages. Other re-
lated work includes multilingual summarization 
(Lin et al, 2005), which aims to create summa-
ries from multiple sources in multiple languages. 
Siddharthan and McKeown (2005) use the in-
formation redundancy in multilingual input to 
correct errors in machine translation and thus 
improve the quality of multilingual summaries.  
3 The Proposed Approach 
Previous methods for cross-language summariza-
tion usually consist of two steps: one step for 
summarization and one step for translation. Dif-
ferent order of the two steps can lead to the fol-
lowing two basic English-to-Chinese summariza-
tion methods:   
Late Translation (LateTrans): Firstly, an 
English summary is produced for the English 
document set by using existing summarization 
methods. Then, the English summary is auto-
matically translated into the corresponding Chi-
nese summary by using machine translation ser-
vices.  
Early Translation (EarlyTrans): Firstly, the 
English documents are translated into Chinese 
documents by using machine translation services. 
Then, a Chinese summary is produced for the 
translated Chinese documents.  
Generally speaking, the LateTrans method has 
a few advantages over the EarlyTrans method: 
1) The LateTrans method is much more effi-
cient than the EarlyTrans method, because only a 
very few summary sentences are required to be 
translated in the LateTrans method, whereas all 
the sentences in the documents are required to be 
translated in the EarlyTrans method.  
2)  The LateTrans method is deemed to be 
more effective than the EarlyTrans method, be-
cause the translation errors of the sentences have 
great influences on the summary sentence extrac-
tion in the EarlyTrans method. 
Thus in this study, we adopt the LateTrans 
method as our baseline method. We also adopt 
the late translation strategy for our proposed ap-
proach. 
In the baseline method, a translated Chinese 
sentence is selected into the summary because 
the original English sentence is informative. 
However, an informative and fluent English sen-
tence is likely to be translated into an uninforma-
tive and disfluent Chinese sentence, and there-
fore, this sentence cannot be selected into the 
summary.  
In order to address the above problem of exist-
ing methods, our proposed approach takes into 
account a novel factor of each sentence for cross-
language summary extraction. Each English sen-
tence is associated with a score indicating its 
translation quality. An English sentence with 
high translation quality score is more likely to be 
selected into the original English summary, and 
such English summary can be translated into a 
better Chinese summary.   Figure 1 gives the ar-
chitecture of our proposed approach.  
 
 
Figure 1: Architecture of our proposed ap-
proach 
Seen from the figure, our proposed approach 
consists of four main steps: 1) The machine 
translation quality score of each English sentence 
is predicted by using regression methods; 2) The 
informativeness score of each English sentence is 
computed by using existing methods; 3) The 
English summary is produced by making use of 
both the machine translation quality score and 
the informativeness score; 4) The extracted Eng-
lish summary is translated into Chinese summary 
by using machine translation services.  
In this study, we adopt Google Translate1 for 
English-to-Chinese translation. Google Translate 
is one of the state-of-the-art commercial machine 
translation systems used today. It applies statisti-
cal learning techniques to build a translation 
                                                 
1 http://translate.google.com/translate_t 
English 
Sentences 
Sentence    
MT Quality 
Prediction
Sentence      
Informativeness 
Evaluation 
English 
Summary 
Extraction 
EN-to-CN 
Machine 
Translation 
Chinese Summary 
Informativeness score 
English summary 
MT quality score
919
model based on both monolingual text in the tar-
get language and aligned text consisting of ex-
amples of human translations between the lan-
guages. 
The first step and the evaluation results will be 
described in Section 4, and the other steps and 
the evaluation results will be described together 
in Section 5.  
4 Machine Translation Quality Predic-
tion  
4.1 Methodology 
In this study, machine translation (MT) quality 
reflects both the translation accuracy and the flu-
ency of the translated sentence. An English sen-
tence with high MT quality score is likely to be 
translated into an accurate and fluent Chinese 
sentence, which can be easily read and under-
stand by Chinese readers.  The MT quality pre-
diction is a task of mapping an English sentence 
to a numerical value corresponding to a quality 
level. The larger the value is, the more accurately 
and fluently the sentence can be translated into 
Chinese sentence.  
As introduced in Section 2.1, several related 
work has used regression and classification 
methods for MT quality prediction without refer-
ence translations. In our approach, the MT qual-
ity of each sentence in the documents is also pre-
dicted without reference translations. The differ-
ence between our task and previous work is that 
previous work can make use of both features in 
source sentence and features in target sentence, 
while our task only leverages features in source 
sentence, because in the late translation strategy, 
the English sentences in the documents have not 
been translated yet at this step.  
In this study, we adopt the ?-support vector re-
gression (?-SVR) method (Vapnik 1995) for the 
sentence-level MT quality prediction task.  The 
SVR algorithm is firmly grounded in the frame-
work of statistical learning theory (VC theory). 
The goal of a regression algorithm is to fit a flat 
function to the given training data points. 
Formally, given a set of training data points 
D={(xi,yi)| i=1,2,?,n} ? Rd?R,  where xi is input 
feature vector and yi is associated score, the goal 
is to fit a function f which approximates the rela-
tion inherited between the data set points. The 
standard form is:  
??
==
++
n
i
i
n
i
i
T
bw
CCww
1
*
1,,, 2
1  min
*
??
??
 
Subject to 
iii
T ybxfw ?? +??+)(  
*)( ii
T
i bxfwy ?? +???  
.,...,1  ,0,, * niii =????  
  The constant C>0 is a parameter for determin-
ing the trade-off between the flatness of f and the 
amount up to which deviations larger than ? are 
tolerated.   
  In the experiments, we use the LIBSVM tool 
(Chang and Lin, 2001) with the RBF kernel for 
the task, and we use the parameter selection tool 
of 10-fold cross validation via grid search to find 
the best parameters on the training set with re-
spect to mean squared error (MSE), and then use 
the best parameters to train on the whole training 
set.   
  We use the following two groups of features 
for each sentence: the first group includes several 
basic features, and the second group includes 
several parse based features2. They are all de-
rived based on the source English sentence.  
  The basic features are as follows: 
1) Sentence length:  It refers to the number of 
words in the sentence.   
2) Sub-sentence number: It refers to the num-
ber of sub-sentences in the sentence. We 
simply use the punctuation marks as indica-
tors of sub-sentences. 
3) Average sub-sentence length: It refers to 
the average number of words in the sub-
sentences within the sentence.   
4) Percentage of nouns and adjectives: It re-
fers to the percentage of noun words or ad-
jective words in the in the sentence. 
5) Number of question words: It refers to the 
number of question words (who, whom, 
whose, when, where, which, how, why, what) 
in the sentence. 
  We use the Stanford Lexicalized Parser (Klein 
and Manning, 2002) with the provided English 
PCFG model to parse a sentence into a parse tree. 
The output tree is a context-free phrase structure 
grammar representation of the sentence. The 
parse features are then selected as follows: 
1) Depth of the parse tree:  It refers to the 
depth of the generated parse tree.  
2) Number of SBARs in the parse tree:  
SBAR is defined as a clause introduced by a 
(possibly empty) subordinating conjunction. 
It is an indictor of sentence complexity.  
                                                 
2  Other features, including n-gram frequency, perplexity 
features, etc., are not useful in our study. MT features are 
not used because Google Translate is used as a black box.  
920
3) Number of NPs in the parse tree:  It refers 
to the number of noun phrases in the parse 
tree.   
4) Number of VPs in the parse tree:  It refers 
to the number of verb phrases in the parse 
tree.    
  All the above feature values are scaled by us-
ing the provided svm-scale program.   
At this step, each English sentence si can be 
associated with a MT quality score TransScore(si) 
predicted by the ?-SVR method. The score is fi-
nally normalized by dividing by the maximum 
score. 
4.2 Evaluation  
4.2.1 Evaluation Setup 
In the experiments, we first constructed the gold-
standard dataset in the following way:  
DUC2001 provided 309 English news articles 
for document summarization tasks, and the arti-
cles were grouped into 30 document sets. The 
news articles were selected from TREC-9. We 
chose five document sets (d04, d05, d06, d08, 
d11) with 54 news articles out of the DUC2001 
document sets. The documents were then split 
into sentences and we used 1736 sentences for 
evaluation. All the sentences were automatically 
translated into Chinese sentences by using the 
Google Translate service. 
Two Chinese college students were employed 
for data annotation. They read the original Eng-
lish sentence and the translated Chinese sentence, 
and then manually labeled the overall translation 
quality score for each sentence, separately. The 
translation quality is an overall measure for both 
the translation accuracy and the readability of the 
translated sentence.  The score ranges between 1 
and 5, and 1 means ?very bad?, and 5 means 
?very good?, and 3 means ?normal?. The correla-
tion between the two sets of labeled scores is 
0.646. The final translation quality score was the 
average of the scores provided by the two anno-
tators.  
After annotation, we randomly separated the 
labeled sentence set into a training set of 1428 
sentences and a test set of 308 sentences. We 
then used the LIBSVM tool for training and test-
ing. 
Two metrics were used for evaluating the pre-
diction results. The two metrics are as follows: 
Mean Square Error (MSE): This metric is a 
measure of how correct each of the prediction 
values is on average, penalizing more severe er-
rors more heavily. Given the set of prediction 
scores for the test sentences: },...1|?{? niyY i == , and 
the manually assigned scores for the sentences: 
},...1|{ niyY i == , the MSE of the prediction result 
is defined as  
?
=
?=
n
i
ii yyn
YMSE
1
2)?(1)?(  
Pearson?s Correlation Coefficient (?):  This 
metric is a measure of whether the trends of pre-
diction values matched the trends for human-
labeled data. The coefficient between Y and Y?  is 
defined as  
yy
n
i
ii
sns
yyyy
?
1
)??)((?
=
??
=?  
where y and y?  are the sample means of Y and 
Y? , ys and ys ? are the sample standard deviations 
of Y and Y? . 
4.2.2 Evaluation Results 
Table 1 shows the prediction results. We can see 
that the overall results are promising. And the 
correlation is moderately high. The results are 
acceptable because we only make use of the fea-
tures derived from the source sentence. The re-
sults guarantee that the use of MT quality scores 
in the summarization process is feasible.  
We can also see that both the basic features 
and the parse features are beneficial to the over-
all prediction results.   
  
Feature Set MSE ? 
Basic features 0.709 0.399 
Parse features 0.702 0.395 
All features 0.683 0.433 
Table 1: Prediction results 
5 Cross-Language Document Summari-
zation  
5.1 Methodology 
In this section, we first compute the informative-
ness score for each sentence. The score reflect 
how the sentence expresses the major topic in the 
documents. Various existing methods can be 
used for computing the score. In this study, we 
adopt the centroid-based method. 
The centroid-based method is the algorithm 
used in the MEAD system. The method uses a 
heuristic and simple way to sum the sentence 
scores computed based on different features. The 
score for each sentence is a linear combination of 
921
the weights computed based on the following 
three features: 
  Centroid-based Weight. The sentences close 
to the centroid of the document set are usually 
more important than the sentences farther away. 
And the centroid weight C(si) of a sentence si is 
calculated as the cosine similarity  between the 
sentence text and the concatenated text for the 
whole document set D. The weight is then nor-
malized by dividing the maximal weight. 
  Sentence Position. The leading several sen-
tences of a document are usually important. So 
we calculate for each sentence a weight to reflect 
its position priority as P(si)=1-(i-1)/n, where i is 
the sequence of the sentence si and n is the total 
number of sentences in the document. Obviously, 
i ranges from 1 to n.  
  First Sentence Similarity. Because the first 
sentence of a document is very important, a sen-
tence similar to the first sentence is also impor-
tant. Thus we use the cosine similarity value be-
tween a sentence and the corresponding first sen-
tence in the same document as the weight F(si) 
for sentence si. 
  After all the above weights are calculated for 
each sentence, we sum all the weights and get the 
overall score for the sentence as follows: 
)()()()( iiii sFsPsCsInfoScore ?+?+?= ???  
where ?, ? and ? are parameters reflecting the 
importance of different features. We empirically 
set ?=?=?=1.  
  After the informativeness scores for all sen-
tences are computed, the score of each sentence 
is normalized by dividing by the maximum score.  
After we obtain the MT quality score and the 
informativeness score of each sentence in the 
document set, we linearly combine the two 
scores to get the overall score of each sentence.  
Formally, let TransScore(si)?[0,1] and Info-
Score(si)?[0,1] denote the MT quality score and 
the informativeness score of sentence si, the 
overall score of the sentence is: 
where ??[0,1] is a parameter controlling the 
influences of the two factors. If ? is set to 0, the 
summary is extracted without considering the 
MT quality factor. In the experiments, we em-
pirically set the parameter to 0.3 in order to bal-
ance the two factors of content informativeness 
and translation quality.   
For multi-document summarization, some sen-
tences are highly overlapping with each other, 
and thus we apply the same greedy algorithm in 
(Wan et al, 2006) to penalize the sentences 
highly overlapping with other highly scored sen-
tences, and finally the informative, novel, and 
easy-to-translate sentences are chosen into the 
English summary. 
  Finally, the sentences in the English summary 
are translated into the corresponding Chinese 
sentences by using Google Translate, and the 
Chinese summary is formed.   
5.2 Evaluation 
5.2.1 Evaluation Setup 
In this experiment, we used the document sets 
provided by DUC2001 for evaluation. As men-
tioned in Section 4.2.1, DUC2001 provided 30 
English document sets for generic multi-
document summarization. The average document 
number per document set was 10. The sentences 
in each article have been separated and the sen-
tence information has been stored into files. Ge-
neric reference English summaries were pro-
vided by NIST annotators for evaluation. In our 
study, we aimed to produce Chinese summaries 
for the English document sets. The summary 
length was limited to five sentences, i.e. each 
summary consisted of five sentences. 
The DUC2001 dataset was divided into the 
following two datasets:  
Ideal Dataset: We have manually labeled the 
MT quality scores for the sentences in five 
document sets (d04-d11), and we directly used 
the manually labeled scores in the summarization 
process. The ideal dataset contained these five 
document sets. 
Real Dataset: The MT quality scores for the 
sentences in the remaining 25 document sets 
were automatically predicted by using the 
learned SVM regression model. And we used the 
automatically predicted scores in the summariza-
tion process. The real dataset contained these 25 
document sets. 
  We performed two evaluation procedures: one 
based on the ideal dataset to validate the 
feasibility of the proposed approach, and 
the other based on the real dataset to 
demonstrate the effectiveness of the proposed 
approach in real applications.  
To date, various methods and metrics have 
been developed for English summary evaluation 
by comparing system summary with reference 
summary, such as the pyramid method (Nenkova 
et al, 2007) and the ROUGE metrics (Lin and 
Hovy, 2003). However, such methods or metrics 
cannot be directly used for evaluating Chinese 
summary without reference Chinese summary.  
)()()1()( iii sTransScoresInfoScoresreOverallSco ?+??= ??
922
Instead, we developed an evaluation protocol as 
follows: 
The evaluation was based on human scoring. 
Four Chinese college students participated in the 
evaluation as subjects. We have developed a 
friendly tool for helping the subjects to evaluate 
each Chinese summary from the following three 
aspects: 
Content: This aspect indicates how much a 
summary reflects the major content of the docu-
ment set. After reading a summary, each user can 
select a score between 1 and 5 for the summary. 
1 means ?very uninformative? and 5 means 
?very informative?. 
Readability:  This aspect indicates the read-
ability level of the whole summary. After reading 
a summary, each user can select a score between 
1 and 5 for the summary. 1 means ?hard to read?, 
and 5 means ?easy to read?. 
Overall:  This aspect indicates the overall 
quality of a summary. After reading a summary, 
each user can select a score between 1 and 5 for 
the summary. 1 means ?very bad?, and 5 means 
?very good?. 
We performed the evaluation procedures on 
the ideal dataset and the read dataset, separately. 
During each evaluation procedure, we compared 
our proposed approach (?=0.3) with the baseline 
approach without considering the MT quality 
factor (?=0). And the two summaries produced 
by the two systems for the same document set 
were presented in the same interface, and then 
the four subjects assigned scores to each sum-
mary after they read and compared the two 
summaries.  And the assigned scores were finally 
averaged across the documents sets and across 
the subjects.  
5.2.2 Evaluation Results 
Table 2 shows the evaluation results on the ideal 
dataset with 5 document sets. We can see that 
based on the manually labeled MT quality scores, 
the Chinese summaries produced by our pro-
posed approach are significantly better than that 
produced by the baseline approach over all three 
aspects. All subjects agree that our proposed ap-
proach can produce more informative and easy-
to-read Chinese summaries than the baseline ap-
proach.   
Table 3 shows the evaluation results on the 
real dataset with 25 document sets. We can see 
that based on the automatically predicted MT 
quality scores, the Chinese summaries produced 
by our proposed approach are significantly better 
than that produced by the baseline approach over 
the readability aspect and the overall aspect. Al-
most all subjects agree that our proposed ap-
proach can produce more easy-to-read and high-
quality Chinese summaries than the baseline ap-
proach.   
Comparing the evaluation results in the two 
tables, we can find that the performance differ-
ence between the two approaches on the ideal 
dataset is bigger than that on the real dataset, es-
pecially on the content aspect. The results dem-
onstrate that the more accurate the MT quality 
scores are, the more significant the performance 
improvement is.  
   Overall, the proposed approach is effective to 
produce good-quality Chinese summaries for 
English document sets. 
 
 Baseline Approach Proposed Approach 
 content readability overall content readability overall 
Subject1 3.2 2.6 2.8 3.4 3.0 3.4 
Subject2 3.0 3.2 3.2 3.4 3.6 3.4 
Subject3 3.4 2.8 3.2 3.6 3.8 3.8 
Subject4 3.2 3.0 3.2 3.8 3.8 3.8 
Average 3.2 2.9 3.1 3.55* 3.55* 3.6* 
Table 2: Evaluation results on the ideal dataset (5 document sets) 
 Baseline Approach Proposed Approach 
 content readability overall content readability overall 
Subject1 2.64 2.56 2.60 2.80 3.24 2.96 
Subject2 3.60 2.76 3.36 3.52 3.28 3.64 
Subject3 3.52 3.72 3.44 3.56 3.80 3.48 
Subject4 3.16 2.96 3.12 3.16 3.44 3.52 
Average 3.23 3.00 3.13 3.26 3.44* 3.40* 
Table 3: Evaluation results on the real dataset (25 document sets) 
(* indicates the difference between the average score of the proposed approach and that of the baseline approach 
is statistically significant by using t-test.) 
923
 5.2.3 Example Analysis 
In this section, we give two running examples to 
better show the effectiveness of our proposed 
approach. The Chinese sentences and the original 
English sentences in the summary are presented 
together. The normalized MT quality score for 
each sentence is also given at the end of the Chi-
nese sentence.  
 
Document set 1: D04 from the ideal dataset 
Summary by baseline approach: 
s1: ?????????????????????73???
?37???????????????-?????????
????????(0.56) 
(US INSURERS expect to pay out an estimated Dollars 7.3bn 
(Pounds 3.7bn) in Florida as a result of Hurricane Andrew - by far 
the costliest disaster the industry has ever faced. ) 
s2: ?????????????????????????
???????????????????????????
?????????(0.67) 
(THERE are growing signs that Hurricane Andrew, unwelcome as 
it was for the devastated inhabitants of Florida and Louisiana, may 
in the end do no harm to the re-election campaign of President 
George Bush.) 
s3: ?????????????????????????
???????????????4000???&#39;? (0.44) 
(GENERAL ACCIDENT said yesterday that insurance claims 
arising from Hurricane Andrew could 'cost it as much as Dollars 
40m'.) 
s4: ???????????????4??????????
??? (0.56) 
(In the Bahamas, government spokesman Mr Jimmy Curry said 
four deaths had been reported on outlying eastern islands.) 
s5: ??????1.6??????????????????
???????????????????????????
???(0.44) 
(New Orleans, with a population of 1.6m, is particularly vulnerable 
because the city lies below sea level, has the Mississippi River 
running through its centre and a large lake immediately to the north.) 
 
Summary by proposed approach: 
s1: ?????????????????????73???
?37???????????????-?????????
????????(0.56) 
(US INSURERS expect to pay out an estimated Dollars 7.3bn 
(Pounds 3.7bn) in Florida as a result of Hurricane Andrew - by far 
the costliest disaster the industry has ever faced.) 
s2: ?????????????????????????
???????????????????????????
?????????(0.67) 
(THERE are growing signs that Hurricane Andrew, unwelcome as 
it was for the devastated inhabitants of Florida and Louisiana, may 
in the end do no harm to the re-election campaign of President 
George Bush.) 
s3: ???????????????4??????????
???(0.56) 
(In the Bahamas, government spokesman Mr Jimmy Curry said 
four deaths had been reported on outlying eastern islands.) 
s4: ?????????????????????????
??????? (0.89) 
(The brunt of the losses are likely to be concentrated among US 
insurers, industry analysts said yesterday.) 
s5: ?????????????(1.0) 
(In north Miami, damage is minimal.) 
 
Document set 2: D54 from the real dataset 
Summary by baseline approach: 
s1: ????11?6???????????????????
???????(0.57) 
(Two propositions on California's Nov. 6 ballot would, among other 
things, limit the terms of statewide officeholders and state legisla-
tors.) 
s2: ?????????????????????????
?????????(0.36) 
(One reason is that term limits would open up politics to many 
people now excluded from office by career incumbents.) 
s3: ?????????????????????????
??????????(0.20) 
(Proposals to limit the terms of members of Congress and of state 
legislators are popular and getting more so, according to the pundits 
and the polls.) 
s4: ?????????????????????????
?????????????????(0.24) 
(State statutes that bar first-time candidates from running for Con-
gress have been held to add to the qualifications set forth in the 
Constitution and have been invalidated.) 
s5: ?????????????????????????
?????????????????????????(0.20) 
(Another argument is that a citizen Congress with its continuing 
flow of fresh faces into Washington would result in better govern-
ment than that provided by representatives with lengthy tenure.) 
Summary by proposed approach: 
s1: ???? 11? 6??????????????????
????????(0.57) 
(Two propositions on California's Nov. 6 ballot would, among other 
things, limit the terms of statewide officeholders and state legisla-
tors.) 
s2: ?????????????????????????
?????????(0.36) 
(One reason is that term limits would open up politics to many 
people now excluded from office by career incumbents.) 
s3: ?????????????????????????
?????????????????????????(0.20) 
(Another argument is that a citizen Congress with its continuing 
flow of fresh faces into Washington would result in better govern-
ment than that provided by representatives with lengthy tenure.) 
s4: ????????????????????????
?????????????(0.39) 
(There are two solid reasons for congressional term limitation that 
economists, at least those of the public-choice persuasion, should 
fully appreciate.) 
s5: ?????????????????????????
?????(0.47) 
(The root of the problems with Congress is that, barring major 
scandal, it is almost impossible to defeat an incumbent.) 
6 Discussion  
In this study, we adopt the late translation strat-
egy for cross-document summarization. As men-
tioned earlier, the late translation strategy has 
some advantages over the early translation strat-
egy. However, in the early translation strategy, 
we can use the features derived from both the 
source English sentence and the target Chinese 
sentence to improve the MT quality prediction 
results.  
Overall, the framework of our proposed ap-
proach can be easily adapted for cross-document 
summarization with the early translation strategy. 
924
And an empirical comparison between the two 
strategies is left as our future work. 
Though this study focuses on English-to-
Chinese document summarization, cross-
language summarization tasks for other lan-
guages can also be solved by using our proposed 
approach.  
7 Conclusion and Future Work  
In this study we propose a novel approach to ad-
dress the cross-language document summariza-
tion task. Our proposed approach predicts the 
MT quality score of each English sentence and 
then incorporates the score into the summariza-
tion process. The user study results verify the 
effectiveness of the approach. 
In future work, we will manually translate 
English reference summaries into Chinese refer-
ence summaries, and then adopt the ROUGE 
metrics to perform automatic evaluation of the 
extracted Chinese summaries by comparing them 
with the Chinese reference summaries. Moreover, 
we will further improve the sentence?s MT qual-
ity by using sentence compression or sentence 
reduction techniques.  
Acknowledgments 
This work was supported by NSFC (60873155), 
Beijing Nova Program (2008B03), NCET 
(NCET-08-0006), RFDP (20070001059) and 
National High-tech R&D Program 
(2008AA01Z421). We thank the students for 
participating in the user study. We also thank the 
anonymous reviewers for their useful comments. 
References  
J. Albrecht and R. Hwa. 2007. A re-examination of 
machine learning approaches for sentence-level mt 
evaluation. In Proceedings of ACL2007. 
M. R. Amini, P. Gallinari. 2002. The Use of Unla-
beled Data to Improve Supervised Learning for 
Text Summarization. In Proceedings of SIGIR2002. 
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. 
Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 
2003. Confidence estimation for statistical machine 
translation. Johns Hopkins Summer Workshop Fi-
nal Report. 
J. Chae and A. Nenkova. 2009. Predicting the fluency 
of text with shallow structural features: case studies 
of machine translation and human-written text. In 
Proceedings of EACL2009. 
G. de Chalendar, R. Besan?on, O. Ferret, G. Grefen-
stette, and O. Mesnard. 2005. Crosslingual summa-
rization with thematic extraction, syntactic sen-
tence simplification, and bilingual generation. In 
Workshop on Crossing Barriers in Text Summari-
zation Research, 5th International Conference on 
Recent Advances in Natural Language Processing  
(RANLP2005). 
C.-C. Chang and C.-J. Lin. 2001. LIBSVM : a library 
for support vector machines. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
G. ErKan, D. R. Radev. LexPageRank. 2004. Prestige 
in Multi-Document Text Summarization. In Pro-
ceedings of EMNLP2004. 
M. Gamon, A. Aue, and M. Smets. 2005. Sentence-
level MT evaluation without reference translations: 
beyond language modeling. In Proceedings of 
EAMT2005. 
D. Klein and C. D. Manning. 2002. Fast Exact Infer-
ence with a Factored Model for Natural Language 
Parsing. In Proceedings of NIPS2002. 
J. Kupiec, J. Pedersen, F. Chen. 1995. A.Trainable 
Document Summarizer. In Proceedings of 
SIGIR1995. 
A. Leuski, C.-Y. Lin, L. Zhou, U. Germann, F. J. Och, 
E. Hovy. 2003. Cross-lingual C*ST*RD: English 
access to Hindi information. ACM Transactions on 
Asian Language Information Processing, 2(3): 
245-269. 
J.-M. Lim, I.-S. Kang, J.-H. Lee. 2004. Multi-
document summarization using cross-language 
texts. In Proceedings of NTCIR-4.  
C. Y. Lin, E. Hovy. 2000. The Automated Acquisition 
of Topic Signatures for Text Summarization. In 
Proceedings of the 17th Conference on Computa-
tional Linguistics. 
C..-Y. Lin and E.. H. Hovy. 2002. From Single to 
Multi-document Summarization: A Prototype Sys-
tem and its Evaluation. In Proceedings of ACL-02. 
C.-Y. Lin and E.H. Hovy. 2003. Automatic Evalua-
tion of Summaries Using N-gram Co-occurrence 
Statistics. In Proceedings of HLT-NAACL -03. 
C.-Y. Lin, L. Zhou, and E. Hovy. 2005. Multilingual 
summarization evaluation 2005: automatic evalua-
tion report. In Proceedings of MSE (ACL-2005 
Workshop). 
H. P. Luhn. 1969. The Automatic Creation of litera-
ture Abstracts. IBM Journal of Research and De-
velopment, 2(2). 
R. Mihalcea, P. Tarau. 2004. TextRank: Bringing 
Order into Texts. In Proceedings of EMNLP2004. 
R. Mihalcea and P. Tarau. 2005. A language inde-
pendent algorithm for single and multiple docu-
ment summarization. In Proceedings of IJCNLP-05. 
A. Nenkova and A. Louis. 2008. Can you summarize 
this? Identifying correlates of input difficulty for 
generic multi-document summarization. In Pro-
ceedings of ACL-08:HLT. 
A. Nenkova, R. Passonneau, and K. McKeown. 2007. 
The Pyramid method: incorporating human content 
selection variation in summarization evaluation. 
925
ACM Transactions on Speech and Language Proc-
essing (TSLP), 4(2). 
C. Orasan, and O. A. Chiorean. 2008. Evaluation of a 
Crosslingual Romanian-English Multi-document 
Summariser. In Proceedings of 6th Language Re-
sources and Evaluation Conference (LREC2008). 
P. Pingali, J. Jagarlamudi and V. Varma. 2007. Ex-
periments in cross language query focused multi-
document summarization. In Workshop on Cross 
Lingual Information Access Addressing the Infor-
mation Need of Multilingual Societies in 
IJCAI2007. 
C. Quirk. 2004. Training a sentence-level machine 
translation confidence measure. In Proceedings of 
LREC2004. 
D. R. Radev, H. Y. Jing, M. Stys and D. Tam. 2004. 
Centroid-based summarization of multiple docu-
ments. Information Processing and Management, 
40: 919-938. 
A. Siddharthan and K. McKeown. 2005. Improving 
multilingual summarization: using redundancy in 
the input to correct MT errors. In Proceedings of 
HLT/EMNLP-2005. 
L. Specia, Z. Wang, M. Turchi, J. Shawe-Taylor, C. 
Saunders. 2009. Improving the Confidence of Ma-
chine Translation Quality Estimates. In MT Summit 
2009 (Machine Translation Summit XII). 
V. Vapnik. 1995. The Nature of Statistical Learning 
Theory. Springer. 
X. Wan, H. Li and J. Xiao. 2010. EUSUM: extracting 
easy-to-understand English summaries for non-
native readers. In Proceedings of  SIGIR2010. 
X. Wan, J. Yang and J. Xiao. 2006. Using cross-
document random walks for topic-focused multi-
documetn summarization. In Proceedings of 
WI2006. 
X. Wan and J. Yang. 2008. Multi-document summari-
zation using cluster-based link analysis. In Pro-
ceedings of SIGIR-08. 
X. Wan, J. Yang and J. Xiao. 2007. Towards an Itera-
tive Reinforcement Approach for Simultaneous 
Document Summarization and Keyword Extraction. 
In Proceedings of ACL2007.  
K.-F. Wong, M. Wu and W. Li. 2008. Extractive sum-
marization using supervised and semi-supervised 
learning. In Proceedings of COLING-08. 
H. Y. Zha. 2002. Generic Summarization and Key-
phrase Extraction Using Mutual Reinforcement 
Principle and Sentence Clustering. In Proceedings 
of SIGIR2002. 
 
926
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 648?653,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Comparative News Summarization Using Linear Programming
Xiaojiang Huang Xiaojun Wan? Jianguo Xiao
Institute of Computer Science and Technology, Peking University, Beijing 100871, China
Key Laboratory of Computational Linguistic (Peking University), MOE, China
{huangxiaojiang, wanxiaojun, xiaojianguo}@icst.pku.edu.cn
Abstract
Comparative News Summarization aims to
highlight the commonalities and differences
between two comparable news topics. In
this study, we propose a novel approach to
generating comparative news summaries. We
formulate the task as an optimization problem
of selecting proper sentences to maximize the
comparativeness within the summary and the
representativeness to both news topics. We
consider semantic-related cross-topic concept
pairs as comparative evidences, and con-
sider topic-related concepts as representative
evidences. The optimization problem is
addressed by using a linear programming
model. The experimental results demonstrate
the effectiveness of our proposed model.
1 Introduction
Comparative News Summarization aims to highlight
the commonalities and differences between two
comparable news topics. It can help users to analyze
trends, draw lessons from the past, and gain insights
about similar situations. For example, by comparing
the information about mining accidents in Chile and
China, we can discover what leads to the different
endings and how to avoid those tragedies.
Comparative text mining has drawn much atten-
tion in recent years. The proposed works differ
in the domain of corpus, the source of comparison
and the representing form of results. So far, most
researches focus on comparing review opinions of
products (Liu et al, 2005; Jindal and Liu, 2006a;
?Corresponding author
Jindal and Liu, 2006b; Lerman and McDonald,
2009; Kim and Zhai, 2009). A reason is that the
aspects in reviews are easy to be extracted and the
comparisons have simple patterns, e.g. positive
vs. negative. A few other works have also
tried to compare facts and views in news article
(Zhai et al, 2004) and Blogs (Wang et al, 2009).
The comparative information can be extracted from
explicit comparative sentences (Jindal and Liu,
2006a; Jindal and Liu, 2006b; Huang et al, 2008),
or mined implicitly by matching up features of
objects in the same aspects (Zhai et al, 2004; Liu
et al, 2005; Kim and Zhai, 2009; Sun et al,
2006). The comparisons can be represented by
charts (Liu et al, 2005), word clusters (Zhai et al,
2004), key phrases(Sun et al, 2006), and summaries
which consist of pairs of sentences or text sections
(Kim and Zhai, 2009; Lerman and McDonald,
2009; Wang et al, 2009). Among these forms,
the comparative summary conveys rich information
with good readability, so it keeps attracting interest
in the research community. In general, document
summarization can be performed by extraction or
abstraction (Mani, 2001). Due to the difficulty
of natural sentence generation, most automatic
summarization systems are extraction-based. They
select salient sentences to maximize the objective
functions of generated summaries (Carbonell and
Goldstein, 1998; McDonald, 2007; Lerman and
McDonald, 2009; Kim and Zhai, 2009; Gillick et al,
2009). The major difference between the traditional
summarization task and the comparative summa-
rization task is that traditional summarization task
places equal emphasis on all kinds of information in
648
the source, while comparative summarization task
only focuses on the comparisons between objects.
News is one of the most important channels for
acquiring information. However, it is more difficult
to extract comparisons in news articles than in
reviews. The aspects are much diverse in news.
They can be the time of the events, the person
involved, the attitudes of participants, etc. These
aspects can be expressed explicitly or implicitly in
many ways. For example, ?storm? and ?rain? both
talk about ?weather?, and thus they can form a
potential comparison. All these issues raise great
challenges to comparative summarization in the
news domain.
In this study, we propose a novel approach for
comparative news summarization. We consider
comparativeness and representativeness as well as
redundancy in an objective function, and solve the
optimization problem by using linear programming
to extract proper comparable sentences. More
specifically, we consider a pair of sentences
comparative if they share comparative concepts;
we also consider a sentence representative if it
contains important concepts about the topic. Thus
a good comparative summary contains important
comparative pairs, as well as important concepts
about individual topics. Experimental results
demonstrate the effectiveness of our model, which
outperforms the baseline systems in quality of
comparison identification and summarization.
2 Problem Definition
2.1 Comparison
A comparison identifies the commonalities or
differences among objects. It basically consists
of four components: the comparee (i.e. what is
compared), the standard (i.e. to what the compare
is compared), the aspect (i.e. the scale on which
the comparee and standard are measured), and the
result (i.e. the predicate that describes the positions
of the comparee and standard). For example, ?Chile
is richer than Haiti.? is a typical comparison, where
the comparee is ?Chile?; the standard is ?Haiti?; the
comparative aspect is wealth, which is implied by
?richer?; and the result is that Chile is superior to
Haiti.
A comparison can be expressed explicitly in a
comparative sentence, or be described implicitly
in a section of text which describes the individual
characteristics of each object point-by-point. For
example, the following text
Haiti is an extremely poor country.
Chile is a rich country.
also suggests that Chile is richer than Haiti.
2.2 Comparative News Summarization
The task of comparative news summarization is to
briefly sum up the commonalities and differences
between two comparable news topics by using
human readable sentences. The summarization
system is given two collections of news articles,
each of which is related to a topic. The system
should find latent comparative aspects, and generate
descriptions of those aspects in a pairwise way, i.e.
including descriptions of two topics simultaneously
in each aspect. For example, when comparing
the earthquake in Haiti with the one in Chile,
the summary should contain the intensity of each
temblor, the damages in each disaster area, the
reactions of each government, etc.
Formally, let t1 and t2 be two comparable news
topics, and D1 and D2 be two collections of
articles about each topic respectively. The task of
comparative summarization is to generate a short
abstract which conveys the important comparisons
{< t1, t2, r1i, r2i >}, where r1i and r2i are
descriptions about topic t1 and t2 in the same
latent aspect ai respectively. The summary can be
considered as a combination of two components,
each of which is related to a news topic. It can also
be subdivided into several sections, each of which
focuses on a major aspect. The comparisons should
have good quality, i.e., be clear and representative to
both topics. The coverage of comparisons should be
as wide as possible, which means the aspects should
not be redundant because of the length limit.
3 Proposed Approach
It is natural to select the explicit comparative
sentences as comparative summary, because they
express comparison explicitly in good qualities.
However, they do not appear frequently in regular
news articles so that the coverage is limited. Instead,
649
it is more feasible to extract individual descriptions
of each topic over the same aspects and then
generate comparisons.
To discover latent comparative aspects, we
consider a sentence as a bag of concepts, each of
which has an atom meaning. If two sentences have
same concepts in common, they are likely to discuss
the same aspect and thus they may be comparable
with each other. For example,
Lionel Messi named FIFA Word Player of
the Year 2010.
Cristiano Ronalo Crowned FIFA Word
Player of the Year 2009.
The two sentences compare on the ?FIFA Word
Player of the Year?, which is contained in both
sentences. Furthermore, semantic related concepts
can also represent comparisons. For example,
?snow? and ?sunny? can indicate a comparison
on ?weather?; ?alive? and ?death? can imply a
comparison on ?rescue result?. Thus the pairs
of semantic related concepts can be considered as
evidences of comparisons.
A comparative summary should contain as many
comparative evidences as possible. Besides, it
should convey important information in the original
documents. Since we model the text with a
collection of concept units, the summary should
contain as many important concepts as possible.
An important concept is likely to be mentioned
frequently in the documents, and thus we use the
frequency as a measure of a concept?s importance.
Obviously, the more accurate the extracted
concepts are, the better we can represent the
meaning of a text. However, it is not easy to extract
semantic concepts accurately. In this study, we
use words, named entities and bigrams to simply
represent concepts, and leave the more complex
concept extraction for future work.
Based on the above ideas, we can formulate
the summarization task as an optimization problem.
Formally, letCi = {cij} be the set of concepts in the
document set Di, (i = 1, 2). Each concept cij has a
weight wij ? R. ocij ? {0, 1} is a binary variable
indicating whether the concept cij is presented in the
summary. A cross-topic concept pair < c1j , c2k >
has a weight ujk ? R that indicates whether it
implies a important comparison. opjk is a binary
variable indicating whether the pair is presented in
the summary. Then the objective function score of a
comparative summary can be estimated as follows:
?
|C1|
?
j=1
|C2|
?
k=1
ujk ?opjk +(1??)
2
?
i=1
|Ci|
?
j=1
wij ?ocij (1)
The first component of the function estimates the
comparativeness within the summary and the second
component estimates the representativeness to both
topics. ? ? [0, 1] is a factor that balances these two
factors. In this study, we set ? = 0.55.
The weights of concepts are calculated as follows:
wij = tfij ? idfij (2)
where tfij is the term frequency of the concept cij
in the document set Di, and idfij is the inverse
document frequency calculated over a background
corpus.
The weights of concept pairs are calculated as
follows:
ujk =
{
(w1j + w2k)/2, if rel(c1j , c2k) > ?
0, otherwise
(3)
where rel(c1j , c2k) is the semantic relevance be-
tween two concepts, and it is calculated using the
algorithms basing on WordNet (Pedersen et al,
2004). If the relevance is higher than the threshold
? (0.2 in this study), then the concept pair is
considered as an evidence of comparison.
Note that a concept pair will not be presented in
the summary unless both the concepts are presented,
i.e.
opjk ? oc1j (4)
opjk ? oc2k (5)
In order to avoid bias towards the concepts which
have more related concepts, we only count the most
important relation of each concept, i.e.
?
k
opjk ? 1, ?j (6)
?
j
opjk ? 1, ?k (7)
The algorithm selects proper sentences to max-
imize the objective function. Formally, let Si =
650
{sik} be the set of sentences in Di, ocsijk be
a binary variable indicating whether concept cij
occurs in sentence sik, and osik be a binary variable
indicating whether sik is presented in the summary.
If sik is selected in the summary, then all the
concepts in it are presented in the summary, i.e.
ocij ? ocsijk ? osik, ?1 ? j ? |Ci| (8)
Meanwhile, a concept will not be present in the
summary unless it is contained in some selected
sentences, i.e.
ocij ?
|Si|
?
k=1
ocsijk ? osik (9)
Finally, the summary should satisfy a length
constraint:
2
?
i=1
|Si|
?
k=1
lik ? osik ? L (10)
where lik is the length of sentence sik, and L is the
maximal summary length.
The optimization of the defined objective function
under above constraints is an integer linear program-
ming (ILP) problem. Though the ILP problems
are generally NP-hard, considerable works have
been done and several software solutions have been
released to solve them efficiently.1
4 Experiment
4.1 Dataset
Because of the novelty of the comparative news
summarization task, there is no existing data set
for evaluating. We thus create our own. We first
choose five pairs of comparable topics, then retrieve
ten related news articles for each topic using the
Google News2 search engine. Finally we write the
comparative summary for each topic pair manually.
The topics are showed in table 1.
4.2 Evaluation Metrics
We evaluate the models with following measures:
Comparison Precision / Recall / F-measure:
let aa and am be the numbers of all aspects
1We use IBM ILOG CPLEX optimizer to solve the problem.
2http://news.google.com
ID Topic 1 Topic 2
1 Haiti Earth quake Chile Earthquake
2 Chile Mining Acci-
dent
New Zealand Mining
Accident
3 Iraq Withdrawal Afghanistan
Withdrawal
4 Apple iPad 2 BlackBerry Playbook
5 2006 FIFAWorld Cup 2010 FIFAWorld Cup
Table 1: Comparable topic pairs in the dataset.
involved in the automatically generated summary
and manually written summary respectively; ca
be the number of human agreed comparative
aspects in the automatically generated summary.
The comparison precision (CP ), comparison recall
(CR) and comparison F-measure (CF ) are defined
as follows:
CP = ca
aa
; CR = ca
am
; CF = 2 ? CP ? CR
CP + CR
ROUGE: the ROUGE is a widely used metric
in summarization evaluation. It measures summary
quality by counting overlapping units between the
candidate summary and the reference summary (Lin
and Hovy, 2003). In the experiment, we report
the f-measure values of ROUGE-1, ROUGE-2 and
ROUGE-SU4, which count overlapping unigrams,
bigrams and skip-4-grams respectively. To evaluate
whether the summary is related to both topics,
we also split each comparative summary into two
topic-related parts, evaluate them respectively, and
report the mean of the two ROUGE values (denoted
as MROUGE).
4.3 Baseline Systems
Non-Comparative Model (NCM): The
non-comparative model treats the task as a
traditional summarization problem and selects the
important sentences from each document collection.
The model is adapted from our approach by setting
? = 0 in the objection function 1.
Co-Ranking Model (CRM): The co-ranking
model makes use of the relations within each
topic and relations across the topics to reinforce
scores of the comparison related sentences. The
model is adapted from (Wan et al, 2007). The
651
SS, WW and SW relationships are replaced by
relationships between two sentences within each
topic and relationships between two sentences from
different topics.
4.4 Experiment Results
We apply all the systems to generate comparative
summaries with a length limit of 200 words. The
evaluation results are shown in table 2. Compared
with baseline models, our linear programming based
comparative model (denoted as LPCM) achieves
best scores over all metrics. It is expected to find
that the NCM model does not perform well in this
task because it does not focus on the comparisons.
The CRM model utilizes the similarity between
two topics to enhance the score of comparison
related sentences. However, it does not guarantee
to choose pairwise sentences to form comparisons.
The LPCM model focus on both comparativeness
and representativeness at the same time, and thus
it achieves good performance on both comparison
extraction and summarization. Figure 1 shows
an example of comparative summary generated by
using the CLPM model. The summary describes
several comparisons between two FIFA World Cups
in 2006 and 2010. Most of the comparisons are clear
and representative.
5 Conclusion
In this study, we propose a novel approach to
summing up the commonalities and differences
between two news topics. We formulate the
task as an optimization problem of selecting
sentences to maximize the score of comparative and
representative evidences. The experiment results
show that our model is effective in comparison
extraction and summarization.
In future work, we will utilize more semantic
information such as localized latent topics to help
capture comparative aspects, and use machine
learning technologies to tune weights of concepts.
Acknowledgments
This work was supported by NSFC (60873155),
Beijing Nova Program (2008B03) and NCET
(NCET-08-0006).
Model CP CR CF ROUGE-1 ROUGE-2 ROUGE-su4 MROUGE-1 MROUGE-2 MROUGE-su4
NCM 0.238 0.262 0.247 0.398 0.146 0.174 0.350 0.122 0.148
CRM 0.313 0.285 0.289 0.426 0.194 0.226 0.355 0.146 0.175
LPCM 0.359 0.419 0.386 0.427 0.205 0.234 0.380 0.171 0.192
Table 2: Evaluation results of systems
World Cup 2006 World Cup 2010
The 2006 Fifa World Cup drew to a close on Sunday
with Italy claiming their fourth crown after beating
France in a penalty shoot-out.
Spain have won the 2010 FIFA World Cup South Africa
final, defeating Netherlands 1-0 with a wonderful goal
from Andres Iniesta deep into extra-time.
Zidane won the Golden Ball over Italians Fabio
Cannavaro and Andrea Pirlo.
Uruguay star striker Diego Forlan won the Golden
Ball Award as he was named the best player of the
tournament at the FIFA World Cup 2010 in South
Africa.
Lukas Podolski was named the inaugural Gillette Best
Young Player.
German youngster Thomas Mueller got double delight
after his side finished third in the tournament as he was
named Young Player of the World Cup
Germany striker Miroslav Klose was the Golden Shoe
winner for the tournament?s leading scorer.
Among the winners were goalkeeper and captain Iker
Casillas who won the Golden Glove Award.
England?s fans brought more colour than their team. Only four of the 212 matches played drew more that
40,000 fans.
Figure 1: A sample comparative summary generated by using the LPCM model
652
References
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 335?336. ACM.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-Tur. 2009. A global optimization
framework for meeting summarization. In Proceed-
ings of the 2009 IEEE International Conference on
Acoustics, Speech and Signal Processing, ICASSP
?09, pages 4769?4772, Washington, DC, USA. IEEE
Computer Society.
Xiaojiang. Huang, Xiaojun. Wan, Jianwu. Yang, and
Jianguo. Xiao. 2008. Learning to Identify
Comparative Sentences in Chinese Text. PRICAI
2008: Trends in Artificial Intelligence, pages 187?198.
Nitin Jindal and Bing Liu. 2006a. Identifying compar-
ative sentences in text documents. In Proceedings of
the 29th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 244?251. ACM.
Nitin Jindal and Bing Liu. 2006b. Mining comparative
sentences and relations. In proceedings of the 21st
national conference on Artificial intelligence - Volume
2, pages 1331?1336. AAAI Press.
Hyun Duk Kim and ChengXiang Zhai. 2009. Generating
comparative summaries of contradictory opinions in
text. In Proceeding of the 18th ACM conference
on Information and knowledge management, pages
385?394. ACM.
Kevin Lerman and Ryan McDonald. 2009. Contrastive
summarization: an experiment with consumer reviews.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Companion Volume: Short Papers, pages
113?116. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 71?78,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the Web. In Proceedings of the 14th international
conference on World Wide Web, pages 342?351. ACM.
Inderjeet Mani. 2001. Automatic summarization. Natu-
ral Language Processing. John Benjamins Publishing
Company.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proceedings of the 29th European conference on IR re-
search, ECIR?07, pages 557?564, Berlin, Heidelberg.
Springer-Verlag.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet:: Similarity: measuring the
relatedness of concepts. In Demonstration Papers at
HLT-NAACL 2004 on XX, pages 38?41. Association
for Computational Linguistics.
Jian-Tao Sun, Xuanhui Wang, Dou Shen, Hua-Jun Zeng,
and Zheng Chen. 2006. CWS: a comparative
web search system. In Proceedings of the 15th
international conference on World Wide Web, pages
467?476. ACM.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Towards an iterative reinforcement approach for
simultaneous document summarization and keyword
extraction. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
552?559, Prague, Czech Republic, June. Association
for Computational Linguistics.
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Comparative document summarization
via discriminative sentence selection. In Proceeding
of the 18th ACM conference on Information and
knowledge management, pages 1963?1966. ACM.
ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004.
A cross-collection mixture model for comparative text
mining. In Proceedings of the tenth ACM SIGKDD
international conference on Knowledge discovery and
data mining, pages 743?748. ACM.
653
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 87?91,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Learning to Order Natural Language Texts 
Jiwei Tana, b, Xiaojun Wana* and Jianguo Xiaoa 
aInstitute of Computer Science and Technology, The MOE Key Laboratory of Computa-
tional Linguistics, Peking University, China 
bSchool of Information Science and Technology, Beijing Normal University, China 
tanjiwei8@gmail.com, {wanxiaojun,jgxiao}@pku.edu.cn 
 
Abstract 
Ordering texts is an important task for many 
NLP applications. Most previous works on 
summary sentence ordering rely on the contex-
tual information (e.g. adjacent sentences) of 
each sentence in the source document. In this 
paper, we investigate a more challenging task 
of ordering a set of unordered sentences with-
out any contextual information. We introduce 
a set of features to characterize the order and 
coherence of natural language texts, and use 
the learning to rank technique to determine the 
order of any two sentences. We also propose 
to use the genetic algorithm to determine the 
total order of all sentences. Evaluation results 
on a news corpus show the effectiveness of 
our proposed method. 
1 Introduction 
Ordering texts is an important task in many natu-
ral language processing (NLP) applications. It is 
typically applicable in the text generation field, 
both for concept-to-text generation and text-to-
text generation (Lapata, 2003), such as multiple 
document summarization (MDS), question an-
swering and so on. However, ordering a set of 
sentences into a coherent text is still a hard and 
challenging problem for computers. 
Previous works on sentence ordering mainly 
focus on the MDS task (Barzilay et al, 2002; 
Okazaki et al, 2004; Nie et al, 2006; Ji and 
Pulman, 2006; Madnani et al, 2007; Zhang et al, 
2010; He et al, 2006; Bollegala et al, 2005; Bol-
legala et al, 2010). In this task, each summary 
sentence is extracted from a source document. 
The timestamp of the source documents and the 
adjacent sentences in the source documents can 
be used as important clues for ordering summary 
sentences. 
In this study, we investigate a more challeng-
ing and more general task of ordering a set of 
unordered sentences (e.g. randomly shuffle the 
                                                 
* Xiaojun Wan is the corresponding author. 
sentences in a text paragraph) without any con-
textual information. This task can be applied to 
almost all text generation applications without 
restriction. 
In order to address this challenging task, we 
first introduce a few useful features to character-
ize the order and coherence of natural language 
texts, and then propose to use the learning to 
rank algorithm to determine the order of two sen-
tences. Moreover, we propose to use the genetic 
algorithm to decide the overall text order. Evalu-
ations are conducted on a news corpus, and the 
results show the prominence of our method. Each 
component technique or feature in our method 
has also been validated.  
2 Related Work 
For works taking no use of source document, 
Lapata (2003) proposed a probabilistic model 
which learns constraints on sentence ordering 
from a corpus of texts. Experimental evaluation 
indicated the importance of several learned lexi-
cal and syntactic features. However, the model 
only works well when using single feature, but 
unfortunately, it becomes worse when multiple 
features are combined. Barzilay and Lee (2004) 
investigated the utility of domain-specific con-
tent model for representing topic and topic shifts 
and the model performed well on the five se-
lected domains. Nahnsen (2009) employed fea-
tures which were based on discourse entities, 
shallow syntactic analysis, and temporal prece-
dence relations retrieved from VerbOcean. How-
ever, the model does not perform well on data-
sets describing the consequences of events. 
3 Our Proposed Method  
3.1 Overview 
The task of text ordering can be modeled like 
(Cohen et al, 1998), as measuring the coherence 
of a text by summing the association strength of 
any sentence pairs. Then the objective of a text 
ordering model is to find a permutation which 
can maximize the summation. 
87
Formally, we define an association strength 
function PREF( , ) Ru v ?  to measure how strong 
it is that sentence u  should be arranged before 
sentence v  (denoted as u v; ). We then define 
function AGREE( ,PREF)?  as: 
, : ( ) ( )
AGREE( ,PREF) = PREF( , )
u v u v
u v
? ?
?
>
? (1)
where ?  denotes a sentence permutation and 
( ) ( )u v? ?>  means u v;  in the permutation ? . 
Then the objective of finding an overall order of 
the sentences becomes finding a permutation ?  
to maximize AGREE( ,PREF)? . 
The main framework is made up of two parts: 
defining a pairwise order relation and determin-
ing an overall order. Our study focuses on both 
the two parts by learning a better pairwise rela-
tion and proposing a better search strategy, as 
described respectively in next sections. 
3.2 Pairwise Relation Learning 
The goal for pairwise relation learning is defin-
ing the strength function PREF for any sentence 
pair. In our method we define the function PREF 
by combining multiple features. 
Method: Traditionally, there are two main 
methods for defining a strength function: inte-
grating features by a linear combination (He et 
al., 2006; Bollegala et al, 2005) or by a binary 
classifier (Bollegala et al, 2010). However, the 
binary classification method is very coarse-
grained since it considers any pair of sentences 
either ?positive? or ?negative?. Instead we pro-
pose to use a better model of learning to rank to 
integrate multiple features.  
In this study, we use Ranking SVM imple-
mented in the svmrank toolkit (Joachims, 2002; 
Joachims, 2006) as the ranking model. The ex-
amples to be ranked in our ranking model are 
sequential sentence pairs like u v; . The feature 
values for a training example are generated by a 
few feature functions ( , )if u v , and we will intro-
duce the features later. We build the training ex-
amples for svmrank  as follows:  
For a training query, which is a paragraph with 
n  sequential sentences as 1 2 ... ns s s; ; ; , we 
can get 2 ( 1)nA n n= ?  training examples. For 
pairs like ( 0)a a ks s k+ >;  the target rank values 
are set to n k? , which means that the longer the 
distance between the two sentences is, the small-
er the target value is. Other pairs like a k as s+ ;  
are all set to 0. In order to better capture the or-
der information of each feature, for every sen-
tence pair u v; , we derive four feature values 
from each function ( , )if u v , which are listed as 
follows: 
,1 ( , )iiV f u v=  (2)
,2
1 / 2, if ( , ) ( , ) 0
( , )
, otherwise
( , ) ( , )
i i
i i
i i
f u v f v u
V f u v
f u v f v u
+ =??
= ?? +?
(3)
,3
1 / if ( , ) 0
( , ) / ( , ), otherwise
i
y S y u
i
i i
y S y u
S f u y
V
f u v f u y
? ? ?
? ? ?
? =?
= ???
?
?
?
(4)
,4
1 / if ( , ) 0
( , ) / ( , ), otherwise
i
x S x v
i
i i
x S x v
S f x v
V
f u v f x v
? ? ?
? ? ?
? =?
= ???
?
?
?
(5)
where S  is the set of all sentences in a paragraph 
and S  is the number of sentences in S . The 
three additional feature values of (3) (4) (5) are 
defined to measure the priority of u v;  to v u; ,   
u v;  to { , }u y S u v? ? ?;  and u v;  to 
{ , }x S u v v? ? ? ;  respectively, by calculating 
the proportion of ( , )if u v  in respective summa-
tions. 
The learned model can be used to predict tar-
get values for new examples. A paragraph of un-
ordered sentences is viewed as a test query, and 
the predicted target value for u v;  is set as 
PREF( , )u v . 
Features: We select four types of features to 
characterize text coherence. Every type of fea-
tures is quantified with several functions distin-
guished by i  in the formulation of ( , )if u v  and 
normalized to [0,1] . The features and definitions 
of ( , )if u v  are introduced in Table 1. 
Type Description 
sim( , )u v  
Similarity 
sim(latter( ),former( ))u v  
overlap ( , ) / min(| |,| |)j u v u v  
Overlap overlap (latter( ),former( ))
overlap ( , )
j
j
u v
u v
Number of  
coreference chains Coreference
Number of 
coreference words 
Noun 
Verb 
Verb & noun dependency 
Probability
Model 
Adjective & adverb 
Table 1: Features used in our model. 
88
As in Table 1, function sim( , )u v  denotes the 
cosine similarity of sentence u  and v ; latter( )u  
and former( )v  denotes the latter half part of u  
and  the former part of v  respectively, which are 
separated by the most centered comma (if exists) 
or word (if no comma exits); overlap ( , )j u v  de-
notes the number of mutual words of u  and v , 
for 1,2,3j =  representing lemmatized noun, 
verb and adjective or adverb respectively; | |u  is 
the number of words of sentence u . The value 
will be set to 0 if the denominator is 0.  
For the coreference features we use the ARK-
ref 1  tool. It can output the coreference chains 
containing words which represent the same entity 
for two sequential sentences u v; .  
The probability model originates from (Lapata, 
2003), and we implement the model with four 
features of lemmatized noun, verb, adjective or 
adverb, and verb and noun related dependency.  
3.3 Overall Order Determination 
Cohen et al (1998) proved finding a permutation 
?  to maximize AGREE( ,PREF)?  is NP-
complete. To solve this, they proposed a greedy 
algorithm for finding an approximately optimal 
order. Most later works adopted the greedy 
search strategy to determine the overall order.  
However, a greedy algorithm does not always 
lead to satisfactory results, as our experiment 
shows in Section 4.2. Therefore, we propose to 
use the genetic algorithm (Holland, 1992) as the 
search strategy, which can lead to better results. 
Genetic Algorithm: The genetic algorithm 
(GA) is an artificial intelligence algorithm for 
optimization and search problems. The key point 
of using GA is modeling the individual, fitness 
function and three operators of crossover, muta-
tion and selection. Once a problem is modeled, 
the algorithm can be constructed conventionally. 
In our method we set a permutation ?  as an 
individual encoded by a numerical path, for ex-
ample a permutation 2 1 3s s s; ;  is encoded as (2 
1 3). Then the function AGREE( ,PREF)?  is just 
the fitness function. We adopt the order-based 
crossover operator which is described in (Davis, 
1985). The mutation operator is a random inver-
sion of two sentences. For selection operator we 
take a tournament selection operator which ran-
domly selects two individuals to choose the one 
with the greater fitness value AGREE( ,PREF)? . 
                                                 
1 http://www.ark.cs.cmu.edu/ARKref/ 
After several generations of evolution, the indi-
vidual with the greatest fitness value will be a 
close solution to the optimal result. 
4 Experiments 
4.1 Experiment Setup 
Data Set and Evaluation Metric: We con-
ducted the experiments on the North American 
News Text Corpus2. We trained the model on 80 
thousand paragraphs and tested with 200 shuffled 
paragraphs. We use Kendall?s ?  as the evalua-
tion metric, which is based on the number of in-
versions in the rankings.  
Comparisons: It is incomparable with other 
methods for summary sentence ordering based 
on special summarization corpus, so we imple-
mented Lapata?s probability model for compari-
son, which is considered the state of the art for 
this task. In addition, we implemented a random 
ordering as a baseline. We also tried to use a 
classification model in place of the ranking mod-
el. In the classification model, sentence pairs like 
1a as s +;  were viewed as positive examples and 
all other pairs were viewed as negative examples. 
When deciding the overall order for either rank-
ing or classification model we used three search 
strategies: greedy, genetic and exhaustive (or 
brutal) algorithms. In addition, we conducted a 
series of experiments to evaluate the effect of 
each feature. For each feature, we tested in two 
experiments, one of which only contained the 
single feature and the other one contained all the 
other features. For comparative analysis of fea-
tures, we tested with an exhaustive search algo-
rithm to determine the overall order.  
4.2 Experiment Results 
The comparison results in Table 2 show that our 
Ranking SVM based method improves the per-
formance over the baselines and the classifica-
tion based method with any of the search algo-
rithms. We can also see the greedy search strat-
egy does not perform well and the genetic algo-
rithm can provide a good approximate solution to 
obtain optimal results. 
Method Greedy Exhaustive Genetic
Baseline -0.0127 
Probability 0.1859 
Classification 0.5006 0.5360 0.5264
Ranking 0.5191 0.5768 0.5747
Table 2: Average ?  of different methods. 
                                                 
2 The corpus is available from 
http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalog
Id=LDC98T30 
89
Ranking vs. Classification: It is not surpris-
ing that the ranking model is better, because 
when using a classification model, an example 
should be labeled either positive or negative. It is 
not very reasonable to label a sentence pair like 
( 1)a a ks s k+ >;  as a negative example, nor a pos-
itive one, because in some cases, it is easy to 
conclude one sentence should be arranged after 
another but hard to decide whether they should 
be adjacent. As we see in the function AGREE , 
the value of PREF( , )a a ks s +  also contributes to 
the summation. In a ranking model, this informa-
tion can be quantified by the different priorities 
of sentence pairs with different distances. 
Single Feature Effect: The effects of differ-
ent types of features are shown in Table 3. Prob 
denotes Lapata?s probability model with differ-
ent features.  
Feature Only Removed
Similarity 0.0721 0.4614 
Overlap 0.1284 0.4631 
Coreference 0.0734 0.4704 
Probnoun 0.3679 0.3932 
Probverb 0.0615 0.4544 
Probadjective&adverb 0.2650 0.4258 
Probdependency 0.2687 0.4892 
All 0.5768 
Table 3: Effects of different features. 
It can be seen in Table 3 that all these features 
contribute to the final result. The two features of 
noun probability and dependency probability 
play an important role as demonstrated in (La-
pata, 2003). Other features also improve the final 
performance. A paragraph which is ordered en-
tirely right by our method is shown in Figure 1. 
 
Sentences which should be arranged together 
tend to have a higher similarity and overlap. Like 
sentence (3) and (4) in Figure 1, they have a 
highest cosine similarity of 0.2240 and most 
overlap words of ?Israel? and ?nuclear?. How-
ever, the similarity or overlap of the two sen-
tences does not help to decide which sentence 
should be arranged before another. In this case 
the overlap and similarity of half part of the sen-
tences may help. For example latter((3)) and 
former((4)) share an overlap of ?Israel? while 
there is no overlap for latter((4)) and former((3)). 
Coreference is also an important clue for or-
dering natural language texts. When we use a 
pronoun to represent an entity, it always has oc-
curred before. For example when conducting 
coreference resolution for (1) (2); , it will be 
found that ?He? refers to ?Vanunu?. Otherwise 
for (2) (1); , no coreference chain will be found.  
4.3 Genetic Algorithm 
There are three main parameters for GA includ-
ing the crossover probability (PC), the mutation 
probability (PM) and the population size (PS). 
There is no definite selection for these parame-
ters. In our study we experimented with a wide 
range of parameter values to see the effect of 
each parameter. It is hard to traverse all possible 
combinations so when testing a parameter we 
fixed the other two parameters. The results are 
shown in Table 4. 
  Value
Para Avg Max Min Stddev
PS 0.5731 0.5859 0.5606 0.0046
PC 0.5733 0.5806 0.5605 0.0038
PM 0.5741 0.5803 0.5337 0.0045
Table 4: Results of GA with different parameters. 
As we can see in Table 4, when adjusting the 
three parameters the average ?  values are all 
close to the exhaustive result of 0.5768 and their 
standard deviations are low. Table 4 shows that 
in our case the genetic algorithm is not very sen-
sible to the parameters. In the experiments, we 
set PS to 30, PC to 0.5 and PM to 0.05, and 
reached a value of 0.5747, which is very close to 
the theoretical upper bound of 0.5768. 
5 Conclusion and Discussion  
In this paper we propose a method for ordering 
sentences which have no contextual information 
by making use of Ranking SVM and the genetic 
algorithm. Evaluation results demonstrate the 
good effectiveness of our method. 
In future work, we will explore more features 
such as semantic features to further improve the 
performance. 
Acknowledgments 
The work was supported by NSFC (61170166), 
Beijing Nova Program (2008B03) and National 
High-Tech R&D Program (2012AA011101). 
(1) Vanunu, 43, is serving an 18-year sentence for 
treason.  
(2) He was kidnapped by Israel's Mossad spy 
agency in Rome in 1986 after giving The Sun-
day Times of London photographs of the in-
side of the Dimona reactor.  
(3) From the photographs, experts determined 
that Israel had the world's sixth largest stock-
pile of nuclear weapons.  
(4) Israel has never confirmed or denied that it 
has a nuclear capability. 
Figure 1: A right ordered paragraph. 
90
References  
Danushka Bollegala, Naoaki Okazaki, Mitsuru Ishi-
zuka. 2005. A machine learning approach to sen-
tence ordering for multi-document summarization 
and its evaluation. In Proceedings of the Second in-
ternational joint conference on Natural Language 
Processing (IJCNLP '05), 624-635. 
Danushka Bollegala, Naoaki Okazaki, and Mitsuru 
Ishizuka. 2010. A bottom-up approach to sentence 
ordering for multi-document summarization. Inf. 
Process. Manage. 46, 1 (January 2010), 89-109. 
John H. Holland. 1992. Adaptation in Natural and 
Artificial Systems: An Introductory Analysis with 
Applications to Biology, Control and Artificial In-
telligence. MIT Press, Cambridge, MA, USA. 
Lawrence Davis. 1985. Applying adaptive algorithms 
to epistatic domains. In Proceedings of the 9th in-
ternational joint conference on Artificial intelli-
gence - Volume 1 (IJCAI'85), Aravind Joshi (Ed.), 
Vol. 1. Morgan Kaufmann Publishers Inc., San 
Francisco, CA, USA, 162-164. 
Mirella Lapata. 2003. Probabilistic text structuring: 
experiments with sentence ordering. InProceedings 
of the 41st Annual Meeting on Association for 
Computational Linguistics - Volume 1(ACL '03), 
Vol. 1. Association for Computational Linguistics, 
Stroudsburg, PA, USA, 545-552.  
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishi-
zuka. 2004. Improving chronological sentence or-
dering by precedence relation. In Proceedings of 
the 20th international conference on Computa-
tional Linguistics (COLING '04). Association for 
Computational Linguistics, Stroudsburg, PA, 
USA, , Article 750 . 
Nitin Madnani, Rebecca Passonneau, Necip Fazil 
Ayan, John M. Conroy, Bonnie J. Dorr, Judith L. 
Klavans, Dianne P. O'Leary, and Judith D. Schle-
singer. 2007. Measuring variability in sentence or-
dering for news summarization. In Proceedings of 
the Eleventh European Workshop on Natural Lan-
guage Generation (ENLG '07), Stephan Busemann 
(Ed.). Association for Computational Linguistics, 
Stroudsburg, PA, USA, 81-88. 
Paul D. Ji and Stephen Pulman. 2006. Sentence order-
ing with manifold-based classification in multi-
document summarization. In Proceedings of the 
2006 Conference on Empirical Methods in Natural 
Language Processing (EMNLP '06). Association 
for Computational Linguistics, Stroudsburg, PA, 
USA, 526-533. 
Regina Barzilay, Noemie Elhadad, and Kathleen 
McKeown. 2002. Inferring strategies for sentence 
ordering in multidocument news summarization. 
Journal of Artificial Intelligence Research, 17:35?
55. 
Regina Barzilay and Lillian Lee. 2004. Catching the 
drift: Probabilistic content models, with applica-
tions to generation and summarization. In HLT-
NAACL2004: Proceedings of the Main Conference, 
pages 113?120. 
Renxian Zhang, Wenjie Li, and Qin Lu. 2010. Sen-
tence ordering with event-enriched semantics and 
two-layered clustering for multi-document news 
summarization. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics: 
Posters (COLING '10). Association for Computa-
tional Linguistics, Stroudsburg, PA, USA, 1489-
1497. 
Thade Nahnsen. 2009. Domain-independent shallow 
sentence ordering. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference 
of the North American Chapter of the Association 
for Computational Linguistics, Companion Volume: 
Student Research Workshop and Doctoral Consor-
tium (SRWS '09). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 78-83. 
Thorsten Joachims. 2002. Optimizing search engines 
using click through data. In Proceedings of the 
eighth ACM SIGKDD international conference on 
Knowledge discovery and data mining (KDD '02). 
ACM, New York, NY, USA, 133-142. 
Thorsten Joachims. 2006. Training linear SVMs in 
linear time. In Proceedings of the 12th ACM 
SIGKDD international conference on Knowledge 
discovery and data mining (KDD '06). ACM, New 
York, NY, USA, 217-226. 
William W. Cohen, Robert E. Schapire, and Yoram 
Singer. 1998. Learning to order things. InProceed-
ings of the 1997 conference on Advances in neural 
information processing systems 10(NIPS '97), Mi-
chael I. Jordan, Michael J. Kearns, and Sara A. 
Solla (Eds.). MIT Press, Cambridge, MA, USA, 
451-457. 
Yanxiang He, Dexi Liu, Hua Yang, Donghong Ji, 
Chong Teng, and Wenqing Qi. 2006. A hybrid sen-
tence ordering strategy in multi-document summa-
rization. In Proceedings of the 7th international 
conference on Web Information Systems (WISE'06), 
Karl Aberer, Zhiyong Peng, Elke A. Rundensteiner, 
Yanchun Zhang, and Xuhui Li (Eds.). Springer-
Verlag, Berlin, Heidelberg, 339-349. 
Yu Nie, Donghong Ji, and Lingpeng Yang. 2006. An 
adjacency model for sentence ordering in multi-
document summarization. In Proceedings of the 
Third Asia conference on Information Retrieval 
Technology (AIRS'06), 313-322. 
91
