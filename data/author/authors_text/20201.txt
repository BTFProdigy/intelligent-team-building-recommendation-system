Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 223?229,
Dublin, Ireland, August 23-24, 2014.
DCU: Aspect-based Polarity Classification for SemEval Task 4
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab Barman
Dasha Bogdanova, Jennifer Foster and Lamia Tounsi
CNGL Centre for Global Intelligent Content
National Centre for Language Technology
School of Computing
Dublin City University
Dublin, Ireland
{jwagner,parora,scortes,ubarman}@computing.dcu.ie
{dbogdanova,jfoster,ltounsi}@computing.dcu.ie
Abstract
We describe the work carried out by DCU
on the Aspect Based Sentiment Analysis
task at SemEval 2014. Our team submit-
ted one constrained run for the restaurant
domain and one for the laptop domain for
sub-task B (aspect term polarity predic-
tion), ranking highest out of 36 systems on
the restaurant test set and joint highest out
of 32 systems on the laptop test set.
1 Introduction
This paper describes DCU?s participation in the
Aspect Term Polarity sub-task of the Aspect Based
Sentiment Analysis task at SemEval 2014, which
focuses on predicting the sentiment polarity of as-
pect terms for a restaurant and a laptop dataset.
Given, for example, the sentence I have had so
many problems with the computer and the aspect
term the computer, the task is to predict whether
the sentiment expressed towards the aspect term is
positive, negative, neutral or conflict.
Our polarity classification system uses super-
vised machine learning with support vector ma-
chines (SVM) (Boser et al., 1992) to classify an
aspect term into one of the four classes. The fea-
tures we employ are word n-grams (with n rang-
ing from 1 to 5) in a window around the aspect
term, as well as features derived from scores as-
signed by a sentiment lexicon. Furthermore, to
reduce data sparsity, we experiment with replacing
sentiment-bearing words in our n-gram feature set
with their polarity scores according to the lexicon
and/or their part-of-speech tag.
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
The paper is organised as follows: in Section 2,
we describe the sentiment lexicons used in this
work and detail the process by which they are
combined, filtered and extended; in Section 3, we
describe our baseline method, a heuristic approach
which makes use of the sentiment lexicon, fol-
lowed by our machine learning method which in-
corporates the rule-based method as features in ad-
dition to word n-gram features; in Section 4, we
present the results of both methods on the training
and test data, and perform an error analysis on the
test set; in Section 5, we compare our approach to
previous research in sentiment classification; Sec-
tion 6 discusses efficiency of our system and on-
going work to improve its speed; finally, in Sec-
tion 7, we conclude and provide suggestions as to
how this research could be fruitfully extended.
2 Sentiment Lexicons
The following four lexicons are employed:
1. MPQA
1
(Wilson et al., 2005) classifies a
word or a stem and its part of speech tag
into positive, negative, both or neutral with
a strong or weak subjectivity.
2. SentiWordNet
2
(Baccianella et al., 2010)
specifies the positive, negative and objective
scores of a synset and its part of speech tag.
3. General Inquirer
3
indicates whether a word
expresses positive or negative sentiment.
4. Bing Liu?s Opinion Lexicon
4
(Hu and Liu,
1
http://mpqa.cs.pitt.edu/lexicons/
subj_lexicon/
2
http://sentiwordnet.isti.cnr.it/
3
http://www.wjh.harvard.edu/
?
inquirer/
inqtabs.txt
4
http://www.cs.uic.edu/
?
liub/FBS/
sentiment-analysis.html#lexicon
223
2004) indicates whether a word expresses
positive or negative sentiment.
2.1 Lexicon Combination
Since the four lexicons differ in their level of detail
and in how they present information, it is neces-
sary, when combining them, to consolidate the in-
formation and present it in a uniform manner. Our
combination strategy assigns a sentiment score to
a word as follows:
? MPQA: 1 for strong positive subjectivity, -1
for strong negative subjectivity, 0.5 for weak
positive subjectivity, -0.5 for weak negative
subjectivity, and 0 otherwise
? SentiWordNet: The positive score if the pos-
itive score is greater than the negative and ob-
jective scores, the negative score if the nega-
tive score is greater than the positive and the
objective scores, and 0 otherwise
? General Inquirer and Bing Liu?s Opinion
Lexicon: 1 for positive and -1 for negative
The above four scores are summed to arrive at a
final score between -4 and 4 for a word.
5
2.2 Lexicon Filtering
Initial experiments with our sentiment lexicon and
the training data led us to believe that there were
many irrelevant entries that, although capable of
conveying sentiment in some other context, were
not contributing to the sentiment of aspect terms
in the two domains of the task. Therefore, these
words are manually filtered from the lexicon. Ex-
amples of deleted words are just, clearly, indi-
rectly, really and back.
2.3 Adding Domain-Specific Words
A manual inspection of the training data revealed
words missing from the merged sentiment lexicon
but which do express sentiment in these domains.
Examples are mouthwatering, watery and better-
configured. We add these to the lexicon with a
score of either 1 or -1 (depending on their polarity
in the training data). We also add words (e.g. zesty,
acrid) from an online list of culinary terms.
6
5
We also tried to vote over the four lexicon scores but this
did not improve over summing.
6
http://world-food-and-wine.com/
describing-food
2.4 Handling Variation
In order to ensure that all inflected forms of a
word are covered, we lemmatise the words in the
training data using the IMS TreeTagger (Schmid,
1994) and we construct new possibilities using a
suffix list. To correct misspelled words, we con-
sider the corrected form of a misspelled word to be
the form with the highest frequency in a reference
corpus
7
among all the forms within an edit dis-
tance of 1 and 2 from the misspelled word (Norvig,
2012). Multi-word expressions of the form x-y
are added with the polarity of xy or x, as in laid-
back/laidback and well-shaped/well. Expressions
x y, are added with the polarity of x-y, as in so
so/so-so.
3 Methodology
We first build a rule-based system which classi-
fies the polarity of an aspect term based solely on
the scores assigned by the sentiment lexicon. We
then explore different ways of converting the rule-
based system into features which can then be com-
bined with bag-of-n-gram features in a supervised
machine learning set-up.
3.1 Rule-Based Approach
In order to predict the polarity of an aspect term,
we sum the polarity scores of all the words in the
surrounding sentence according to our sentiment
lexicon. Since not all the sentiment words occur-
ring in a sentence influence the polarity of the as-
pect term to the same extent, it is important to
weight the score of each sentiment word by its dis-
tance to the aspect term. Therefore, for each word
in the sentence which is found in our lexicon we
take the score from the lexicon and divide it by its
distance to the aspect term. The distance is calcu-
lated using the sum of the following three distance
functions:
? Token Distance: This function calculates the
difference in the position of the sentiment
word and the aspect term by counting the to-
kens between them.
7
The reference corpus consists of about a million
words retrieved from several public domain books from
Project Gutenberg (http://www.gutenberg.org/),
lists of most frequent words from Wiktionary (http:
//en.wiktionary.org/wiki/Wiktionary:
Frequency_lists) and the British National Corpus
(http://www.kilgarriff.co.uk/bnc-readme.
html) and two thousand laptop reviews crawled from CNET
(http://www.cnet.com/).
224
? Discourse Chunk Distance: This function
counts the discourse chunks that must be
crossed in order to get from the sentiment
word to the aspect term. If the sentiment
word and the aspect term are in the same
discourse chunk, then the distance is zero.
We use the discourse segmenter described in
(Tofiloski et al., 2009).
? Dependency Path Distance: This function
calculates the shortest path between the sen-
timent word and the aspect term in a syntac-
tic dependency graph for the sentence, pro-
duced by parsing the sentence with a PCFG-
LA parser (Attia et al., 2010) trained on con-
sumer review data (Le Roux et al., 2012)
8
,
and converting the resulting phrase-structure
tree into a dependency graph using the Stan-
ford converter (de Marneffe and Manning,
2008) (version 3.3.1).
Since our lexicon also contains multi-word ex-
pressions such as finger licking, we also look up
bigrams and trigrams from the input sentence in
our lexicon. Negation is handled by reversing the
polarity of sentiment words that appear within a
window of three words of the following negators:
not, n?t, no and never.
For each aspect term, we use the distance-
weighted sum of the polarity scores to predict one
of the three classes positive, negative and neutral.
9
After experimenting with various thresholds we
settled on the following simple strategy: if the po-
larity score for an aspect term is greater than zero
then it is classified as positive, if the score is less
than zero, then it is classified as negative, other-
wise it is classified as neutral.
3.2 Machine Learning Approach
We train a four-way SVM classifier for each do-
main (laptop and restaurant), using Weka?s SMO
implementation (Platt, 1998; Hall et al., 2009).
10
8
To facilitate parsing, the data was normalised using the
process described in (Le Roux et al., 2012) with minor mod-
ifications, e. g. treatment of non-breakable space characters,
abbreviations and emoticons. The normalised version of the
data was used for all experiments.
9
We also experimented with classifying aspect terms as
conflict when the individual scores for positive and negative
sentiment were both relatively high. However, this proved
unsuccessful.
10
We also experimented with logistic regression, random
forests, k-nearest neighbour, naive Bayes and multi-layer per-
ceptron in Weka, but did not match performance of an SVM
trained with default parameters.
Transf. n c n-gram Freq.
-L? 2 2 cord with 1
AL? 2 2 <aspect> with 56
ALS? 1 4 <negu080> 595
ALSR- 1 4 <negu080> 502
AL? 2 4 and skip 1
ALSR- 2 4 and <negu080> 25
ALSRP 1 4 <negu080>/vb 308
Table 1: 7 of the 2,640 bag-of-n-gram features
extracted for the aspect term cord from the lap-
top training sentence I charge it at night and skip
taking the cord with me because of the good bat-
tery life. The last column shows the frequency of
the feature in the training data. Transformations:
A=aspect, L=lowercase, S=score, R=restricted to
certain POS, P=POS annotation
Our system submission uses bag-of-n-gram fea-
tures and features derived from the rule-based ap-
proach. Decisions about parameters are made in 5-
fold cross-validation on the training data provided
for the task.
3.2.1 Bag-of-N-gram Features
We extract features encoding the presence of spe-
cific lower-cased n-grams (L) (n = 1, ..., 5) in
the context of the aspect term to be classified (c
words to the left and c words to the right with
c = 1, ..., 5, inf) for 10 combinations of trans-
formations: replacement of the aspect term with
<ASPECT> (A), replacement of sentiment words
with a discretised score (S), restriction (R) of the
sentiment word replacement to certain parts-of-
speech, and annotation of the discretised score
with the POS (P) of the sentiment word. An ex-
ample is shown in Table 1.
3.2.2 Adding Rule-Based Score Features
We explore two approaches for incorporating in-
formation from the rule-based approach (Sec-
tion 3.1) into our SVM classifier. The first ap-
proach is to encode polarity scores directly as the
following four features:
1. distance-weighted sum of scores of positive
words in the sentence
2. distance-weighted sum of scores of negative
words in the sentence
3. number of positive words in the sentence
225
4. number of negative words in the sentence
The second approach is less direct: for each do-
main, we train J48 decision trees with minimum
leaf size 60 using the four rule-based features de-
scribed above. We then use the decision rules
and the conjunctions leading from the root node
to each leaf node to binarise the above four basic
score features, producing 122 features. Further-
more, we add normalised absolute values, rank of
values and interval indicators, producing 48 fea-
tures.
3.2.3 Submitted Runs
We eliminate features that have redundant value
columns for the training data, and we apply fre-
quency thresholds (13, 18, 25 and 35) to further
reduce the number of features. We perform a grid-
search to optimise the parameters C and ? of the
SVM RBF kernel. We choose the system to sub-
mit based on average cross-validation accuracy.
We experiment with combinations of the three fea-
ture sets described above. We choose the bina-
rised features over the raw rule-based scores be-
cause cross-validation results are inferior for the
rule-based scores in initial experiments with fea-
ture frequency threshold 35: 70.26 vs. 71.36 for
laptop and 72.06 vs. 72.15 for restaurant. There-
fore, we decide to focus on systems with binarised
score features for lower feature frequency thresh-
olds, which are more CPU-intensive to train. For
both domains, the system we end up submitting
is a combination of the n-gram features and the
binarised features with parameters C = 3.981,
? = 0.003311 for the laptop data, C = 1.445,
? = 0.003311 for the restaurant data, and a fre-
quency threshold of 13.
4 Results and Analysis
Table 2 shows the training and test accuracy of
the task baseline system (Pontiki et al., 2014), a
majority baseline classifying everything as posi-
tive, our rule-based system and our submitted sys-
tem. The restaurant domain has a higher accuracy
than the laptop domain for all systems, the SVM
system outperforms the rule-based system on both
domains, and the test accuracy is higher than the
training accuracy for all systems in the restaurant
domain.
We observe that the majority of our systems? er-
rors fall into the following categories:
Dataset System Training Test
Laptop Baseline ? 51.1%
Laptop All positive 41.9% 52.1%
Laptop Rule-based 65.4% 67.7%
Laptop SVM 72.3% 70.5%
Restaurant Baseline ? 64.3%
Restaurant All positive 58.6% 64.2%
Restaurant Rule-based 69.5% 77.8%
Restaurant SVM 72.7% 81.0%
Table 2: Accuracy of the task baseline system, a
system classifying everything as positive, our rule-
based system and our submitted SVM-based sys-
tem on train (5-fold cross-validation) and test sets
? Sentiment not expressed explicitly: The
sentiment cannot be inferred from local lexi-
cal and syntactic information, e. g. The sushi
is cut in blocks bigger than my cell phone.
? Non-obvious expression of negation: For
example, The Management was less than ac-
comodating [sic]. The rule-based approach
does not capture such cases and there are
not enough similar training examples for the
SVM to learn to correctly classify them.
? Conflict cases: The training data contains
too few examples of conflict sentences for the
system to learn to detect them.
11
For the restaurant domain, there are more than
fifty cases where the rule-based approach fails to
detect sentiment, but the machine learning ap-
proach classifies it correctly. Most of these cases
contain no sentiment lexicon words, thus the rule-
based system marks them as being neutral. How-
ever, the machine learning system was able to fig-
ure out the correct polarity. Examples of such
cases include Try the rose roll (not on menu) and
The gnocchi literally melts in your mouth!. Fur-
thermore, in the laptop domain, a number of the
errors made by the rule-based system arise from
the ambiguous nature of some lexicon words. For
example, the sentence Only 2 usb ports ... seems
kind of ... limited is misclassified because the
word kind is considered to be positive.
There are a few cases where the rule-based sys-
tem outperforms the machine learning one. It hap-
pens when a sentence contains a rare word with
strong polarity, e. g. the word heavenly in The
11
We only classify one test instance as conflict.
226
chocolate raspberry cake is heavenly - not too
sweet, but full of flavor.
5 Related Work
The use of supervised machine learning with bag-
of-word or bag-of-n-gram feature sets has been
a standard approach to the problem of sentiment
polarity classification since the seminal work by
Pang et al. (2002) on movie review polarity pre-
diction. Heuristic methods which rely on a lexi-
con of sentiment words have also been widespread
and much of the research in this area has been
devoted to the unsupervised induction of good
quality sentiment indicators (see, for example,
Hatzivassiloglou and McKeown (1997) and Tur-
ney (2002), and Liu (2010) for an overview). The
integration of sentiment lexicon scores as fea-
tures in supervised machine learning to supple-
ment standard bag-of-n-gram features has also
been employed before (see, for example, Bak-
liwal et al. (2013)). The replacement of train-
ing/test words with scores/labels from sentiment
lexicons has also been used by Baccianella et
al. (2009), who supplement n-grams such as hor-
rible location with generalised expressions such
as NEGATIVE location. Linguistic features which
capture generalisations at the level of syntax (Mat-
sumoto et al., 2005), semantics (Johansson and
Moschitti, 2010) and discourse (Lazaridou et al.,
2013) have also been widely applied. In using bi-
narised features derived from the nodes of a deci-
sion tree, we are following our recent work which
uses the same technique in a different task: quality
estimation for machine translation (Rubino et al.,
2012; Rubino et al., 2013).
The main novelty in our system lies not in the
individual techniques but rather in they way they
are combined and integrated. For example, our
combination of token/chunk/dependency path dis-
tance used to weight the relationship between a
sentiment word and the aspect term has ? to the
best of our knowledge ? not been applied before.
6 Efficiency
Building a system for a shared task, we focus
solely on the accuracy of the system in all our deci-
sions. For example, we parse all training and test
data multiple times using different grammars to
increase sentence coverage from 99.87% to 100%.
To offer a more practical system, we work on
implementing a simplified, fully automated sys-
tem that is more efficient. So far, we replaced
time-consuming parsing with POS tagging. The
system accepts as input and generates as output
valid SemEval ABSA XML documents.
12
After
extracting the text and the aspect terms from the
input, the text is normalised using the process de-
scribed in Footnote 8. The feature extraction is
performed as described in Section 3 with the fol-
lowing modifications:
? The POS information used by the n-gram
feature extractor is obtained using the IMS
TreeTagger (Schmid, 1994) instead of using
the PCFG-LA parser (Attia et al., 2010).
? The distance used by the rule-based approach
is the token distance only, instead of a com-
bination of three distance functions.
The sentiment lexicon and the classification mod-
els used are described in Sections 2 and 3 respec-
tively.
The test sets containing 800 sentences are POS
tagged in less than half a second each. Surpris-
ingly, accuracy of aspect term polarity prediction
increases to 71.4% (from 70.5% for the submitted
system) on the laptop test set, using the same SVM
parameters as for the submitted system. However,
we see a degradation to 78.8% (from 81.0% for the
submitted system) for the restaurant test set. This
is an encouraging result as the SVM parameters
are not yet fully optimised for the slightly different
information and as the remaining modifications to
be implemented should not change accuracy any
further.
The next bottleneck that needs to be addressed
before the system can be used in applications re-
quiring quick responses is the current implementa-
tion of the n-gram feature extractor: It enumerates
all n-grams (for all context window sizes and n-
gram transformations) only to then intersect these
features with the list of selected features. For the
shared task, this made sense as we initially need
all features to make our selection of features, and
as we only need to run the feature extractor a few
times. For a practical system that has to process
new test sets frequently, however, it will be more
efficient to check for each selected feature whether
the respective event occurs in the input.
12
We validate documents using the XML schema defini-
tion provided on the shared task website.
227
7 Conclusion
We have described our aspect term polarity predic-
tion system, which employs supervised machine
learning using a combination of n-grams and sen-
timent lexicon features. Although our submitted
system performs very well, it is interesting to note
that our rule-based system is not that far behind.
This suggests that a state-of-the-art system can be
build without machine learning and that careful
design of the other system components is impor-
tant. However, the very good performance of our
machine-learning-based system also suggests that
word n-gram features do provide useful informa-
tion that is missed by a sentiment lexicon alone,
and that it is always worthwhile to perform careful
parameter tuning to eke out as much as possible
from such an approach.
Future work should investigate how much each
system component contributes to the overall per-
formance, e. g. lexicon combination, lemmatisa-
tion, spelling correction, other normalisations,
negation handling, distance function and n-gram
feature transformations. There is also room for
improvements in most of these components, e. g.
our handling of complex negations. Detection of
conflicts also needs more attention. Features in-
dicating the presence of trigger words for negation
and conflicts that are currently used only internally
in the rule-based component could be added to the
SVM feature set. It would also be interesting to
see how the compositional approach described by
Socher et al. (2013) handles these difficult cases.
The score features could be easily augmented by
breaking down scores by the four employed lexi-
cons. This way, the SVM can choose to combine
the information from these scores differently than
just summing them, allowing it to learn more com-
plex relations. Lexicon filtering and addition of
domain-specific entries could be automated to re-
duce the time needed to adjust to a new domain.
Finally, machine learning methods that can effi-
ciently handle large feature sets such as logistic
regression should be tried with the full feature set
(not applying frequency thresholds).
Acknowledgements
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
CNGL (www.cngl.ie) at Dublin City University.
The authors wish to acknowledge the DJEI/DES/
SFI/HEA Irish Centre for High-End Computing
(ICHEC) for the provision of computational facil-
ities and support. We are grateful to Qun Liu and
Josef van Genabith for their helpful comments.
References
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statis-
tical latent-variable parsing models for arabic, en-
glish and french. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 67?75.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Multi-facet rating of product reviews.
In Proceedings of ECIR, pages 461?472.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh Conference
on International Language Resources and Evalua-
tion (LREC?10).
Akshat Bakliwal, Jennifer Foster, Jennifer van der Puil,
Ron O?Brien, Lamia Tounsi, and Mark Hughes.
2013. Sentiment analysis of political tweets: To-
wards an accurate classifier. In Proceedings of the
NAACL Workshop on Language Analysis in Social
Media, pages 49?58.
Bernhard E. Boser, Isabelle M. Guyon, and
Vladimir N. Vapnik. 1992. A training algo-
rithm for optimal margin classifiers. In Proceedings
of the Fifth Annual Workshop on Computational
Learning Theory, pages 144?152.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In COLING 2008 Workshop on Cross-
framework and Cross-domain Parser Evaluation.,
pages 1?8.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th Annual Meeting
of the ACL and the 8th Conference of the European
Chapter of the ACL, pages 174?181.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177.
228
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion ex-
pression detection. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 67?76.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of
the 51th Annual Meeting of the Association for
Computational Linguistics, pages 1630?1639.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-
sul Samad Zadeh Kaljahi, and Anton Bryl. 2012.
DCU-Paris13 systems for the SANCL 2012 shared
task. Notes of the First Workshop on Syntactic
Analysis of Non-Canonical Language (SANCL).
Bing Liu. 2010. Sentiment analysis and subjectivity.
In Handbook of Natural Language Processing.
Shotaro Matsumoto, Hiroya Takamura, and Manubu
Okumura, 2005. Advances in Knowledge Discovery
and Data Mining, volume 3518 of Lecture Notes in
Computer Science, chapter Sentiment Classification
Using Word Sub-sequences and Dependency Sub-
trees, pages 301?311.
Peter Norvig. 2012. How to write a spelling corrector.
http://norvig.com/spell-correct.
html. [Online; accessed 2014-03-19].
Po Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86.
John C. Platt. 1998. Fast training of support vec-
tor machines using sequential minimal optimization.
In B. Schoelkopf, C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods - Support Vector
Learning, pages 185?208.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval).
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. Dcu-symantec submission for
the wmt 2012 quality estimation task. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 138?144.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP, pages 1631?
1642.
Milan Tofiloski, Julian Brooke, and Maite Taboada.
2009. A syntactic and lexical-based discourse seg-
menter. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, ACLShort ?09, pages 77?
80.
Peter Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
cation of reviews. In Proceedings of the ACL, pages
417?424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354.
229
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13?23,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Code Mixing: A Challenge for Language Identification in the Language of
Social Media
Utsab Barman, Amitava Das
?
, Joachim Wagner and Jennifer Foster
CNGL Centre for Global Intelligent Content, National Centre for Language Technology
School of Computing, Dublin City University, Dublin, Ireland
?
Department of Computer Science and Engineering
University of North Texas, Denton, Texas, USA
{ubarman,jwagner,jfoster}@computing.dcu.ie
amitava.das@unt.edu
Abstract
In social media communication, multilin-
gual speakers often switch between lan-
guages, and, in such an environment, au-
tomatic language identification becomes
both a necessary and challenging task.
In this paper, we describe our work in
progress on the problem of automatic
language identification for the language
of social media. We describe a new
dataset that we are in the process of cre-
ating, which contains Facebook posts and
comments that exhibit code mixing be-
tween Bengali, English and Hindi. We
also present some preliminary word-level
language identification experiments using
this dataset. Different techniques are
employed, including a simple unsuper-
vised dictionary-based approach, super-
vised word-level classification with and
without contextual clues, and sequence la-
belling using Conditional Random Fields.
We find that the dictionary-based approach
is surpassed by supervised classification
and sequence labelling, and that it is im-
portant to take contextual clues into con-
sideration.
1 Introduction
Automatic processing and understanding of Social
Media Content (SMC) is currently attracting much
attention from the Natural Language Processing
research community. Although English is still by
far the most popular language in SMC, its domi-
nance is receding. Hong et al. (2011), for exam-
ple, applied an automatic language detection algo-
rithm to over 62 million tweets to identify the top
10 most popular languages on Twitter. They found
that only half of the tweets were in English. More-
over, mixing multiple languages together (code
mixing) is a popular trend in social media users
from language-dense areas (C?ardenas-Claros and
Isharyanti, 2009; Shafie and Nayan, 2013). In
a scenario where speakers switch between lan-
guages within a conversation, sentence or even
word, the task of automatic language identifica-
tion becomes increasingly important to facilitate
further processing.
Speakers whose first language uses a non-
Roman alphabet write using the Roman alphabet
for convenience (phonetic typing) which increases
the likelihood of code mixing with a Roman-
alphabet language. This can be especially ob-
served in South-East Asia and in the Indian sub-
continent. The following is a code mixing com-
ment taken from a Facebook group of Indian uni-
versity students:
Original: Yaar tu to, GOD hain. tui JU
te ki korchis? Hail u man!
Translation: Buddy you are GOD. What
are you doing in JU? Hail u man!
This comment is written in three languages: En-
glish, Hindi (italics), and Bengali (boldface). For
Bengali and Hindi, phonetic typing has been used.
We follow in the footsteps of recent work on
language identification for SMC (Hughes et al.,
2006; Baldwin and Lui, 2010; Bergsma et al.,
2012), focusing specifically on the problem of
word-level language identification for code mixing
SMC. Our corpus for this task is collected from
Facebook and contains instances of Bengali(BN)-
English(EN)-Hindi(HI) code mixing.
The paper is organized as follows: in Section 2,
we review related research in the area of code
mixing and language identification; in Section 3,
we describe our code mixing corpus, the data it-
13
self and the annotation process; in Section 4, we
list the tools and resources which we use in our
language identification experiments, described in
Section 5. Finally, in Section 6, we conclude
and provide suggestions for future research on this
topic.
2 Background and Related Work
The problem of language identification has been
investigated for half a century (Gold, 1967) and
that of computational analysis of code switching
for several decades (Joshi, 1982), but there has
been less work on automatic language identifi-
cation for multilingual code-mixed texts. Before
turning to that topic, we first briefly survey studies
on the general characteristics of code mixing.
Code mixing is a normal, natural product of
bilingual and multilingual language use. Signif-
icant studies of the phenomenon can be found
in the linguistics literature (Milroy and Muysken,
1995; Alex, 2008; Auer, 2013). These works
mainly discuss the sociological and conversational
necessities behind code mixing as well as its lin-
guistic nature. Scholars distinguish between inter-
sentence, intra-sentence and intra-word code mix-
ing.
Several researchers have investigated the rea-
sons for and the types of code mixing. Initial stud-
ies on Chinese-English code mixing in Hong Kong
(Li, 2000) and Macao (San, 2009) indicated that
mainly linguistic motivations were triggering the
code mixing in those highly bilingual societies.
Hidayat (2012) showed that Facebook users tend
to mainly use inter-sentential switching over intra-
sentential, and report that 45% of the switching
was instigated by real lexical needs, 40% was used
for talking about a particular topic, and 5% for
content clarification. The predominance of inter-
sentential code mixing in social media text was
also noted in the study by San (2009), which com-
pared the mixing in blog posts to that in the spoken
language in Macao. Dewaele (2010) claims that
?strong emotional arousal? increases the frequency
of code mixing. Dey and Fung (2014) present
a speech corpus of English-Hindi code mixing in
student interviews and analyse the motivations for
code mixing and in what grammatical contexts
code mixing occurs.
Turning to the work on automatic analysis of
code mixing, there have been some studies on de-
tecting code mixing in speech (Solorio and Liu,
2008a; Weiner et al., 2012). Solorio and Liu
(2008b) try to predict the points inside a set of spo-
ken Spanish-English sentences where the speak-
ers switch between the two languages. Other
studies have looked at code mixing in differ-
ent types of short texts, such as information re-
trieval queries (Gottron and Lipka, 2010) and SMS
messages (Farrugia, 2004; Rosner and Farrugia,
2007). Yamaguchi and Tanaka-Ishii (2012) per-
form language identification using artificial mul-
tilingual data, created by randomly sampling text
segments from monolingual documents. King
and Abney (2013) used weakly semi-supervised
methods to perform word-level language identifi-
cation. A dataset of 30 languages has been used
in their work. They explore several language
identification approaches, including a Naive Bayes
classifier for individual word-level classification
and sequence labelling with Conditional Random
Fields trained with Generalized Expectation crite-
ria (Mann and McCallum, 2008; Mann and Mc-
Callum, 2010), which achieved the highest scores.
Another very recent work on this topic is (Nguyen
and Do?gru?oz, 2013). They report on language
identification experiments performed on Turkish
and Dutch forum data. Experiments have been
carried out using language models, dictionaries,
logistic regression classification and Conditional
Random Fields. They find that language models
are more robust than dictionaries and that contex-
tual information is helpful for the task.
3 Corpus Acquisition
Taking into account the claim that code mixing is
frequent among speakers who are multilingual and
younger in age (C?ardenas-Claros and Isharyanti,
2009), we choose an Indian student community
between the 20-30 year age group as our data
source. India is a country with 30 spoken lan-
guages, among which 22 are official. code mix-
ing is very frequent in the Indian sub-continent
because languages change within very short geo-
distances and people generally have a basic knowl-
edge of their neighboring languages.
A Facebook group
1
and 11 Facebook users
(known to the authors) were selected to obtain
publicly available posts and comments. The Face-
book graph API explorer was used for data collec-
tion. Since these Facebook users are from West
Bengal, the most dominant language is Bengali
1
https://www.facebook.com/jumatrimonial
14
(Native Language), followed by English and then
Hindi (National Language of India). The posts
and comments in Bengali and Hindi script were
discarded during data collection, resulting in 2335
posts and 9813 comments.
3.1 Annotation
Four annotators took part in the annotation task.
Three were computer science students and the
other was one of the authors. The annotators are
proficient in all three languages of our corpus. A
simple annotation tool was developed which en-
abled these annotators to identify and distinguish
the different languages present in the content by
tagging them. Annotators were supplied with 4
basic tags (viz. sentence, fragment, inclusion and
wlcm (word-level code mixing)) to annotate differ-
ent levels of code mixing. Under each tag, six at-
tributes were provided, viz. English (en), Bengali
(bn), Hindi (hi), Mixed (mixd), Universal (univ)
and Undefined (undef). The attribute univ is as-
sociated with symbols, numbers, emoticons and
universal expressions (e.g. hahaha, lol). The at-
tribute undef is specified for a sentence or a word
for which no language tags can be attributed or
cannot be categorized as univ. In addition, anno-
tators were instructed to annotate named entities
separately. What follows are descriptions of each
of the annotation tags.
Sentence (sent): This tag refers to a sentence
and can be used to mark inter-sentential code mix-
ing. Annotators were instructed to identify a sen-
tence with its base language (e.g. en, bn, hi and
mixd) or with other types (e.g. univ, undef ) as the
first task of annotation. Only the attribute mixd is
used to refer to a sentence which contains multi-
ple languages in the same proportion. A sentence
may contain any number of inclusions, fragments
and word-level code mixing. A sentence can be at-
tributed as univ if and only if it contains symbols,
numbers, emoticons, chat acronyms and no other
words (Hindi, English or Bengali). A sentence can
be attributed as undef if it is not a sentence marked
as univ and has words/tokens that can not be cate-
gorized as Hindi, English or Bengali. Some exam-
ples of sentence-level annotations are the follow-
ing:
1. English-Sentence:
[sent-lang=?en?] what a.....6 hrs long...but re-
ally nice tennis.... [/sent]
2. Bengali-Sentence:
[sent-lang=?bn?] shubho nabo borsho.. :)
[/sent]
3. Hindi Sentence:
[sent-lang=?hi?] karwa sachh ..... :( [/sent]
4. Mixed-Sentence:
[sent-lang=?mixd?] [frag-lang=?hi?] oye
hoye ..... angreji me kahte hai ke [/frag]
[frag-lang=?en?] I love u.. !!! [/frag] [/sent]
5. Univ-Sentence:
[sent-lang=?univ?] hahahahahahah....!!!!!
[/sent]
6. Undef-Sentence:
[sent-lang=?undef?] Hablando de una triple
amenaza. [/sent]
Fragment (frag): This refers to a group of for-
eign words, grammatically related, in a sentence.
The presence of this tag in a sentence conveys that
intra-sentential code mixing has occurred within
the sentence boundary. Identification of fragments
(if present) in a sentence was the second task of
annotation. A sentence (sent) with attribute mixd
must contain multiple fragments (frag) with a spe-
cific language attribute. In the fourth example
above, the sentence contains a Hindi fragment oye
hoye ..... angreji me kahte hai ke and an English
fragment I love u.. !!!, hence it is considered as a
mixd sentence. A fragment can have any number
of inclusions and word-level code mixing. In the
first example below, Jio is a popular Bengali word
appearing in the English fragment Jio.. good joke,
hence tagged as a Bengali inclusion. One can ar-
gue that the word Jio could be a separate Bengali
inclusion (i.e. can be tagged as a Bengali inclu-
sion outside the English fragment). But looking
at the syntactic pattern and the sense expressed by
the comment, the annotator kept it as a single unit.
In the second example below, an instance of word-
level code mixing, typer, has been found in an En-
glish fragment (where the root English word type
has the Bengali suffix r).
1. Fragment with Inclusion:
[sent-lang=?mixd?] [frag-lang=?en?] [incl-
lang=?bn?] Jio.. [/incl] good joke [/frag] [frag
lang=?bn?] ?amar Babin? [/frag] [/sent]
2. Fragment with Word-Level code mixing:
[sent-lang=?mixd?] [frag-lang=?en?] ? I will
find u and marry you ? [/frag] [frag-
lang=?bn?] [wlcm-type=?en-and-bn-suffix?]
typer [/wlcm] hoe glo to! :D [/frag] [/sent]
15
Inclusion (incl): An inclusion is a foreign word
or phrase in a sentence or in a fragment which
is assimilated or used very frequently in native
language. Identification of inclusions can be per-
formed after annotating a sentence and fragment
(if present in that sentence). An inclusion within a
sentence or fragment also denotes intra-sentential
code mixing. In the example below, seriously is an
English inclusion which is assimilated in today?s
colloquial Bengali and Hindi. The only tag that an
inclusion may contain is word-level code mixing.
1. Sentence with Inclusion:
[sent-lang=?bn?] Na re [incl-lang=?en?] seri-
ously [/incl] ami khub kharap achi. [/sent]
Word-Level code mixing (wlcm): This is the
smallest unit of code mixing. This tag was in-
troduced to capture intra-word code mixing and
denotes cases where code mixing has occurred
within a single word. Identifying word-level code
mixing is the last task of annotation. Annotators
were told to mention the type of word-level code
mixing in the form of an attribute (Base Language
+ Second Language) format. Some examples are
provided below. In the first example below, the
root word class is English and e is an Bengali suf-
fix that has been added. In the third example be-
low, the opposite can be observed ? the root word
Kando is Bengali, and an English suffix z has been
added. In the second example below, a named en-
tity suman is present with a Bengali suffix er.
1. Word-Level code mixing (EN-BN):
[wlcm-type=?en-and-bn-suffix?] classe
[/wlcm]
2. Word-Level code mixing (NE-BN):
[wlcm-type=?NE-and-bn-suffix?] sumaner
[/wlcm]
3. Word-Level code mixing (BN-EN):
[wlcm-type=?bn-and-en-suffix?] kandoz
[/wlcm]
3.1.1 Inter Annotator Agreement
We calculate word-level inter annotator agreement
(Cohen?s Kappa) on a subset of 100 comments
(randomly selected) between two annotators. Two
annotators are in agreement about a word if they
both annotate the word with the same attribute
(en, bn, hi, univ, undef ), regardless of whether
the word is inside an inclusion, fragment or sen-
tence. Our observations that the word-level anno-
tation process is not a very ambiguous task and
that annotation instruction is also straightforward
are confirmed in a high inter-annotator agreement
(IAA) with a Kappa value of 0.884.
3.2 Data Characteristics
Tag-level and word-level statistics of annotated
data that reveal the characteristics of our data set
are described in Table 1 and in Table 2 respec-
tively. More than 56% of total sentences and al-
most 40% of total tokens are in Bengali, which is
the dominant language of this corpus. English is
the second most dominant language covering al-
most 33% of total tokens and 35% of total sen-
tences. The amount of Hindi data is substantially
lower ? nearly 1.75% of total tokens and 2% of to-
tal sentences. However, English inclusions (84%
of total inclusions) are more prominent than Hindi
or Bengali inclusions and there are a substantial
number of English fragments (almost 52% of total
fragments) present in our corpus. This means that
English is the main language involved in the code
mixing.
Statistics of Different Tags
Tags En Bn Hi Mixd Univ Undef
sent 5,370 8,523 354 204 746 15
frag 288 213 40 0 6 0
incl 7,377 262 94 0 1,032 1
wlcm 477
Name Entity 3,602
Acronym 691
Table 1: Tag-level statistics
Word-Level Tag Count
EN 66,298
BN 79,899
HI 3,440
WLCM 633
NE 5,233
ACRO 715
UNIV 39,291
UNDEF 61
Table 2: Word-level statistics
3.2.1 Code Mixing Types
In our corpus, inter- and intra-sentential code mix-
ing are more prominent than word-level code mix-
ing, which is similar to the findings of (Hidayat,
2012) . Our corpus contains every type of code
mixing in English, Hindi and Bengali viz. in-
ter/intra sentential and word-level as described in
the previous section. Some examples of different
types of code mixing in our corpus are presented
below.
16
1. Inter-Sentential:
[sent-lang=?hi?] Itna izzat diye aapne mujhe
!!! [/sent]
[sent-lang=?en?] Tears of joy. :?( :?( [/sent]
2. Intra-Sentential:
[sent-lang=?bn?] [incl-lang=?en?] by d way
[/incl] ei [frag-lang=?en?] my craving arms
shall forever remain empty .. never hold u
close .. [/frag] line ta baddo [incl-lang=?en?]
cheezy [/incl] :P ;) [/sent]
3. Word-Level:
[sent-lang=?bn?] [incl-lang=?en?] 1st yr
[/incl] eo to ei [wlcm-type=?en+bnSuffix?]
tymer [/wlcm] modhye sobar jute jay ..
[/sent]
3.2.2 Ambiguous Words
Annotators were instructed to tag an English word
as English irrespective of any influence of word
borrowing or foreign inclusion but an inspection of
the annotations revealed that English words were
sometimes annotated as Bengali or Hindi. To un-
derstand this phenomenon we processed the list
of language (EN,BN and HI) word types (total
26,475) and observed the percentage of types that
were not always annotated with the one language
throughout the corpus. The results are presented in
Table 3. Almost 7% of total types are ambiguous
(i.e. tagged in different languages during annota-
tion). Among them, a substantial amount (5.58%)
are English/Bengali.
Label(s) Count Percentage
EN 9,109 34.40
BN 14,345 54.18
HI 1,039 3.92
EN or BN 1,479 5.58
EN or HI 61 0.23
BN or HI 277 1.04
EN or BN or HI 165 0.62
Table 3: Statistics of ambiguous and monolingual
word types
There are two reasons why this is happening:
Same Words Across Languages Some words
are the same (e.g. baba, maa, na, khali) in Hindi
and Bengali because both of the languages orig-
inated from a single language Sanskrit and share
a good amount of common vocabulary. It also
occurred in English-Hindi and English-Bengali as
a result of word borrowing. Most of these are
commonly used inclusions like clg, dept, ques-
tion, cigarette, and topic. Sometimes the anno-
tators were careful enough to tag such words as
English and sometimes these words were tagged
in the annotators? native languages. During cross
checking of the annotated data the same error pat-
terns were observed for multiple annotators, i.e.
tagging commonly used foreign words into native
language. It only demonstrates that these English
words are highly assimilated in the conversational
vocabulary of Bengali and Hindi.
Phonetic Similarity of Spellings Due to pho-
netic typing some words share the same surface
form across two and sometimes across three lan-
guages. As an example, to is a word in the three
languages: it has occurred 1209 times as English,
715 times as Bengali and 55 times as Hindi in our
data. The meaning of these words (e.g. to, bolo,
die) are different in different languages. This phe-
nomenon is perhaps exacerbated by the trend to-
wards short and noisy spelling in SMC.
4 Tools and Resources
We have used the following resources and tools in
our experiment.
Dictionaries
1. British National Corpus (BNC): We com-
pile a word frequency list from the BNC (As-
ton and Burnard, 1998).
2. SEMEVAL 2013 Twitter Corpus (Se-
mevalTwitter): To cope with the language
of social media we use the SEMEVAL 2013
(Nakov et al., 2013) training data for the
Twitter sentiment analysis task. This data
comes from a popular social media site and
hence is likely to reflect the linguistic proper-
ties of SMC.
3. Lexical Normalization List (LexNorm-
List): Spelling variation is a well-known
phenomenon in SMC. We use a lexical nor-
malization dictionary created by Han et al.
(2012) to handle the different spelling vari-
ations in our data.
Machine Learning Toolkits
1. WEKA: We use the Weka toolkit (Hall et
al., 2009) for our experiments in decision tree
training.
2. MALLET: CRF learning is applied using the
MALLET toolkit (McCallum, 2002).
17
3. Liblinear: We apply Support Vector Ma-
chine (SVM) learning with a linear kernel us-
ing the Liblinear package (Fan et al., 2008).
NLP Tools For data tokenization we used the
CMU Tweet-Tokenizer (Owoputi et al., 2013).
5 Experiments
Since our training data is entirely labelled at the
word-level by human annotators, we address the
word-level language identification task in a fully
supervised way.
Out of the total data, 15% is set aside as a
blind test set, while the rest is employed in our ex-
periments through a 5-fold cross-validation setup.
There is a substantial amount of token overlap be-
tween the cross-validation data and the test set ?
88% of total EN tokens, 86% of total Bengali to-
kens and 57% of total Hindi tokens of the test set
are present in the cross-validation data.
2
We address the problem of word-level in three
different ways:
1. A simple heuristic-based approach which
uses a combination of our dictionaries to clas-
sify the language of a word
2. Word-level classification using supervised
machine learning with SVMs but no contex-
tual information
3. Word-level classification using supervised
machine learning with SVMs and sequence
labelling using CRFs, both employing con-
textual information
Named entities and instances of word-level
code mixing are excluded from evaluation. For
systems which do not take the context of a word
into account, i.e. the dictionary-based approach
(Section 5.1) and the SVM approach without con-
textual clues (Section 5.2), named entities and in-
stances of word-level code mixing can be safely
excluded from training. For systems which do
take context into account, the CRF system (Sec-
tion 5.3.1) and the SVM system with contextual
clues (Section 5.3.2), these are included in train-
ing, because to exclude them would result in un-
realistic contexts. This means that these systems
2
We found 25 comments and 17 posts common between
the cross-validation data and the test set. The reason for this
is that users of social media often express themselves in a
concise way. Almost all of these common data consisted of 1
to 3 token(s). In most of the cases these tokens were emoti-
cons, symbols or universal expressions such as wow and lol.
As the percentage of these comments is low, we keep these
comments as they are.
can classify a word to be a named entity or an in-
stance of word-level code mixing. To avoid this,
we implement a post-processor which backs off in
these cases to a system which hasn?t seen named
entities or word-level code mixing in training (see
Section 5.3).
5.1 Dictionary-Based Detection
We start with dictionary-based language detec-
tion. Generally a dictionary-based language de-
tector predicts the language of a word based on
its frequency in multiple language dictionaries. In
our data the Bengali and Hindi tokens are phoneti-
cally typed. As no such transliterated dictionary is,
to our knowledge, available for Bengali and Hindi,
we use the training set words as dictionaries. For
words that have multiple annotations in training
data (ambiguous words), we select the majority
tag based on frequency, e.g. the word to will al-
ways be tagged as English.
Our English dictionaries are those described
in Section 4 (BNC, LexNormList, SemEvalTwit-
ter) and the training set words. For LexNorm-
List, we have no frequency information, and so
we consider it as a simple word list. To pre-
dict the language of a word, dictionaries with nor-
malized frequency were considered first (BNC,
SemEvalTwitter, Training Data), if not found,
word list look-up was performed. The predicted
language is chosen based on the dominant lan-
guage(s) of the corpus if the word appears in mul-
tiple dictionaries with same frequency or if the
word does not appear in any dictionary or list.
A simple rule-based method is applied to pre-
dict universal expressions. A token is considered
as univ if any of the following conditions satisfies:
? All characters of the token are symbols or
numbers.
? The token contains certain repetitions identi-
fied by regular expressions.(e.g. hahaha).
? The token is a hash-tag or an URL or
mention-tags (e.g. @Sumit).
? Tokens (e.g. lol) identified by a word list
compiled from the relevant 4/5th of the train-
ing data.
Table 4 shows the results of dictionary-based
detection obtained from 5-fold cross-validation
averaging. We try different combinations and fre-
quency thresholds of the above dictionaries. We
find that using a normalized frequency is helpful
18
and that a combination of LexNormList and Train-
ing Data dictionaries is suited best for our data.
Hence, we consider this as our baseline language
identification system.
Dictionary Accuracy(%)
BNC 80.09
SemevalTwitter 77.61
LexNormList 79.86
Training Data 90.21
LexNormList+TrainingData (Baseline) 93.12
Table 4: Average cross-validation accuracy of
dictionary-based detection
5.2 Word-Level Classification without
Contextual Clues
The following feature types are employed:
1. Char-n-grams (G): We start with a character
n-gram-based approach (Cavnar and Tren-
kle, 1994), which is most common and fol-
lowed by many language identification re-
searchers. Following the work of King and
Abney (2013), we select character n-grams
(n=1 to 5) and the word as the features in our
experiments.
2. Presence in Dictionaries (D): We use pres-
ence in a dictionary as a features for all avail-
able dictionaries in previous experiments.
3. Length of words (L): Instead of using the
raw length value as a feature, we follow our
previous work (Rubino et al., 2013; Wagner
et al., 2014) and create multiple features for
length using a decision tree (J48). We use
length as the only feature to train a decision
tree for each fold and use the nodes obtained
from the tree to create boolean features.
4. Capitalization (C): We use 3 boolean fea-
tures to encode capitalization information:
whether any letter in the word is capitalized,
whether all letters in the word are capitalized
and whether the first letter is capitalized.
We perform experiments with an SVM classifier
(linear kernel) for different combination of these
features.
3
Parameter optimizations (C range 2
-15
to 2
10
) for SVM are performed for each feature
3
According to (Hsu et al., 2010) the SVM linear kernel
with parameter C optimization is good enough when dealing
with a large number of features. Though an RBF kernel can
be more effective than a linear one, it is possible only after
proper optimization of C and ? parameters, which is compu-
tational expensive for such a large feature set.
Features Accuracy Features Accuracy
G 94.62 GD 94.67
GL 94.62 GDL 94.73
GC 94.64 GDC 94.72
GLC 94.64 GDLC 94.75
Table 5: Average cross-validation accuracy for
SVM word-level classification (without context),
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features
GDLC: 94.75%
GLC: 94.64% GDL: 94.73% GDC: 94.72%
GL: 94.62% GC: 94.64% GD: 94.67%
G: 94.62%
Figure 1: Average cross-validation accuracy for
SVM word-level classification (without context),
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features: cube visualization
set and best cross-validation accuracy is found for
the GDLC-based run (94.75%) at C=1 (see Table 5
and Fig. 1).
We also investigate the use of a dictionary-to-
char-n-gram back-off model ? the idea is to ap-
ply the char-n-gram model SVM-GDLC for those
words for which a majority-based decision is taken
during dictionary-based detection. However, it
does not outperform the SVM. Hence, we select
SVM-GDLC for the next steps of our experiments
as the best exemplar of our individual word-level
classifier (without contextual clues).
5.3 Language Identification with Contextual
Clues
Contextual clues can play a very important role in
word-level language identification. As an exam-
ple, a part of a comment is presented from cross-
validation fold 1 that contains the word die which
is wrongly classified by the SVM classifier. The
frequency of die in the training set of fold 1 is 6
for English, 31 for Bengali and 0 for Hindi.
Gold Data: ..../univ the/en movie/en
for/en which/en i/en can/en die/en for/en
19
Features Order-0 Order-1 Order-2
G 92.80 95.16 95.36
GD 93.42 95.59 95.98
GL 92.82 95.14 95.41
GDL 93.47 95.60 95.94
GC 92.07 94.60 95.05
GDC 93.47 95.62 95.98
GLC 92.36 94.53 95.02
GDLC 93.47 95.58 95.98
Table 6: Average cross-validation accuracy of
CRF-based predictions where G = char-n-gram, L
= length feature, D = single dictionary-based la-
bels (baseline system) and C = capitalization fea-
tures
...../univ
SVM Output: ..../univ the/en
movie/en for/en which/en i/en can/en
die/bn for/en ...../univ
We now investigate whether contextual informa-
tion can correct the mis-classified tags.
Although named entities and word-level code
mixing are excluded from evaluation, when deal-
ing with context it is important to consider named
entity and word-level code mixing during training
because these may contain some important infor-
mation. We include these tokens in the training
data for our context-based experiments, labelling
them as other. The presence of this new label may
affect the prediction for a language token during
classification and sequence labelling. To avoid this
situation, a 4-way (bn, hi, en, univ) backoff classi-
fier is trained separately on English, Hindi, Ben-
gali and universal tokens. During evaluation of
any context-based system we discard named en-
tity and word-level code mixing from the predic-
tion of that system. If any of the remaining tokens
is predicted as other we back off to the decision
of the 4-way classifier for that token. For the CRF
experiments (Section 5.3.1), the backoff classifier
is a CRF system, and, for the SVM experiments
(Section 5.3.2), the backoff classifier is an SVM
system.
5.3.1 Conditional Random Fields (CRF)
As our goal is to apply contextual clues, we first
employ Conditional Random Fields (CRF), an ap-
proach which takes history into account in pre-
dicting the optimal sequence of labels. We em-
ploy a linear chain CRF with an increasing or-
der (Order-0, Order-1 and Order-2) with 200 it-
erations for different feature combinations (used
GDLC: 95.98%
GLC: 95.02% GDL: 95.94% GDC: 95.98%
GL: 95.41% GC: 95.05% GD: 95.98%
G: 95.36%
Figure 2: CRF Order-2 results: cube visualisation
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features
Context Accuracy (%)
GDLC + P
1
94.66
GDLC + P
2
94.55
GDLC + N
1
94.53
GDLC + N
2
94.37
GDLC + P
1
N
1
95.14
GDLC + P
2
N
2
94.55
Table 7: Average cross-validation accuracy of
SVM (GDLC) context-based runs, where P-i =
previous i word(s) , N-i = next i word(s)
in SVM-based runs). However, we observe that
accuracy of CRF based runs decreases when bi-
narized length features (see Section 5.2 and dic-
tionary features (a feature for each dictionary) are
involved. Hence, we use the dictionary-based pre-
dictions of the baseline system to generate a single
dictionary feature for each token and only the raw
length value of a token instead of binarized length
features. The results are presented in Table 6 and
the second order results are visualized in Fig. 2.
As expected, the performance increases as the
order increases from zero to one and two. The use
of a single dictionary feature is also helpful. The
results for GDC, GDLC, and GD based runs are
almost similar (95.98%). However, we choose the
GDC system because it performed slightly better
(95.989%) than the GDLC (95.983%) and the GD
(95.983%) systems.
5.3.2 SVM with Context
We also add contextual clues to our SVM classi-
fier. To obtain contextual information we include
the previous and next two words as features in
the SVM-GDLC-based run.
4
All possible com-
4
We also experimented with extracting all GDLC features
for the context words but this did not help.
20
binations are considered during experiments (Ta-
ble 7). After C parameter optimization, the best
cross-validation accuracy is found for the P
1
N
1
(one word previous and one word next) run with
C=0.125 (95.14%).
5.4 Test Set Results
We apply our best dictionary-based system, our
best SVM system (with and without context) and
our best CRF system to the held-out test set. The
results are shown in Table 8. Our best result is
achieved using the CRF model (95.76%).
5.5 Error Analysis
Manual error analysis shows the limitations of
these systems. The word-level classifier without
contextual clues does not perform well with Hindi
data. The number of Hindi tokens is quite low.
Only 2.4% (4,658) of total tokens of the training
data are Hindi, out of which 55.36% are bilin-
gually ambiguous and 29.51% are tri-lingually
ambiguous tokens. Individual word-level systems
often fail to assign proper labels to ambiguous
words, but adding context information helps to
overcome this problem. Considering the previ-
ous example of die, both context-based SVM and
CRF systems classify it properly. Though the final
system CRF-GDC performs well, it also has some
limitations, failing to identify the language for the
tokens which appear very frequently in three lan-
guages (e.g. are, na, pic).
6 Conclusion
We have presented an initial study on automatic
language identification with Indian language code
mixing from social media communication. We
described our dataset of Bengali-Hindi-English
Facebook comments and we presented the results
of our word-level classification experiments on
this dataset. Our experimental results lead us to
conclude that character n-gram features are useful
for this task, contextual information is also impor-
tant and that information from dictionaries can be
effectively incorporated as features.
In the future we plan to apply the techniques
and feature sets that we used in these experiments
to other datasets. We have already started this by
applying variants of the systems presented here to
the Nepali-English and Spanish-English datasets
which were introduced as part of the 2014 code
mixing shared task (Solorio et al., 2014; Barman
et al., 2014).
We did not include word-level code mixing in
our experiments ? in our future experiments we
will explore ways to identify and segment this type
of code mixing. It will be also important to find the
best way to handle inclusions since there is a fine
line between word borrowing and code mixing.
Acknowledgements
This research is supported by the Science Founda-
tion Ireland (Grant 12/CE/I2267) as part of CNGL
(www.cngl.ie) at Dublin City University. The
authors wish to acknowledge the DJEI/DES/SFI/
HEA for the provision of computational facili-
ties and support. Our special thanks to Soumik
Mandal from Jadavpur University, India for co-
ordinating the annotation task. We also thank the
administrator of JUMatrimonial and the 11 Face-
book users who agreed that we can use their posts
for their support and permission.
References
Beatrice Alex. 2008. Automatic detection of English
inclusions in mixed-lingual data with an application
to parsing. Ph.D. thesis, School of Informatics, The
University of Edinburgh, Edinburgh, UK.
Guy Aston and Lou Burnard. 1998. The BNC hand-
book: exploring the British National Corpus with
SARA. Capstone.
Peter Auer. 2013. Code-Switching in Conversation:
Language, Interaction and Identity. Routledge.
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229?237. Association for Computational Lin-
guistics.
Utsab Barman, Joachim Wagner, Grzegorz Chrupa?a,
and Jennifer Foster. 2014. DCU-UVT: Word-
level language classification with code-mixed data.
In Proceedings of the First Workshop on Com-
putational Approaches to Code-Switching. EMNLP
2014, Conference on Empirical Methods in Natural
Language Processing, Doha, Qatar. Association for
Computational Linguistics.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65?74. Associ-
ation for Computational Linguistics.
21
System
Precision (%) Recall (%) Accuracy
(%)EN BN HI UNIV EN BN HI UNIV
Baseline (Dictionary) 92.67 90.73 80.64 99.67 92.28 94.63 43.47 94.99 93.64
SVM-GDLC 92.49 94.89 80.31 99.34 96.23 94.28 44.92 97.07 95.21
SVM-P
1
N
1
93.51 95.56 83.18 99.42 96.63 95.23 55.94 96.95 95.52
CRF-GDC 94.77 94.88 91.86 99.34 95.65 96.22 55.65 97.73 95.76
Table 8: Test set results for Baseline (Dictionary), SVM-GDLC, SVM-P1N1 and CRF-GDC
MS C?ardenas-Claros and N Isharyanti. 2009. Code-
switching and code-mixing in internet chatting:
Between?yes,?ya,?and?si?-a case study. The Jalt Call
Journal, 5(3):67?78.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Theo Pavlidis,
editor, Proceedings of SDAIR-94, Third Annual
Symposium on Document Analysis and Information
Retrieval, pages 161?175.
Jean-Marc Dewaele. 2010. Emotions in Multiple Lan-
guages. Palgrave Macmillan.
Anik Dey and Pascale Fung. 2014. A Hindi-
English code-switching corpus. In Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), pages 2410?
2413, Reykjavik, Iceland. European Language Re-
sources Association (ELRA).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Paulseph-John Farrugia. 2004. TTS pre-processing is-
sues for mixed language support. In Proceedings
of CSAW?04, the second Computer Science Annual
Workshop, pages 36?41. Department of Computer
Science & A.I., University of Malta.
E Mark Gold. 1967. Language identification in the
limit. Information and control, 10(5):447?474.
Thomas Gottron and Nedim Lipka. 2010. A compar-
ison of language identification approaches on short,
query-style texts. In Advances in Information Re-
trieval, pages 611?614. Springer.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explor. Newsl., 11(1):10?18.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation dictio-
nary for microblogs. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 421?432. Association
for Computational Linguistics.
Taofik Hidayat. 2012. An analysis of code switch-
ing used by facebookers: a case study in a
social network site. Student essay for the
study programme ?Pendidikan Bahasa Ing-
gris? (English Education) at STKIP Siliwangi
Bandung, Indonesia, http://publikasi.
stkipsiliwangi.ac.id/files/2012/
10/08220227-taofik-hidayat.pdf.
Lichan Hong, Gregorio Convertino, and Ed H. Chi.
2011. Language matters in twitter: A large scale
study. In Proceedings of the Fifth International
AAAI Conference on Weblogs and Social Media
(ICWSM-11), pages 518?521, Barcelona, Spain. As-
sociation for the Advancement of Artificial Intelli-
gence.
Chih-Wei Hsu, Chih-Chung Chang, and Chih-
Jen Lin. 2010. A practical guide to sup-
port vector classification. Technical re-
port. Department of Computer Science, Na-
tional Taiwan University, Taiwan, https:
//www.cs.sfu.ca/people/Faculty/
teaching/726/spring11/svmguide.pdf.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proc. of the 5th edition of the Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2006), pages 485?488, Genoa, Italy.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In J. Horeck?y, ed-
itor, Proceedings of the 9th conference on Compu-
tational linguistics - Volume 1 (COLING?82), pages
145?150. Academia Praha, North-Holland Publish-
ing Company.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110?
1119, Atlanta, Georgia. Association for Computa-
tional Linguistics.
David C. S. Li. 2000. Cantonese-English code-
switching research in Hong Kong: a Y2K review.
World Englishes, 19(3):305?322.
Gideon S. Mann and Andrew McCallum. 2008.
Generalized expectation criteria for semi-supervised
learning of conditional random fields. In Proceed-
ings of ACL-08: HLT, pages 870?878, Columbus,
Ohio. Association for Computational Linguistics.
22
Gideon S. Mann and Andrew McCallum. 2010.
Generalized expectation criteria for semi-supervised
learning with weakly labeled data. The Journal of
Machine Learning Research, 11:955?984.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Lesley Milroy and Pieter Muysken, editors. 1995. One
speaker, two languages: Cross-disciplinary per-
spectives on code-switching. Cambridge University
Press.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA. Association for Com-
putational Linguistics.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2013), pages 857?862, Seattle,
Washington, USA. Association for Computational
Linguistics.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2013), pages 380?390, Atlanta, Geor-
gia. Association for Computational Linguistics.
Mike Rosner and Paulseph-John Farrugia. 2007. A
tagging algorithm for mixed language identifica-
tion in a noisy domain. In INTERSPEECH-2007,
8th Annual Conference of the International Speech
Communication Association, pages 190?193. ISCA
Archive.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397, Sofia, Bulgaria.
Association for Computational Linguistics.
Hong Ka San. 2009. Chinese-English code-switching
in blogs by Macao young people. Master?s the-
sis, The University of Edinburgh, Edinburgh, UK.
http://hdl.handle.net/1842/3626.
Latisha Asmaak Shafie and Surina Nayan. 2013.
Languages, code-switching practice and primary
functions of facebook among university students.
Study in English Language Teaching, 1(1):187?
199. http://www.scholink.org/ojs/
index.php/selt.
Thamar Solorio and Yang Liu. 2008a. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981. Association for
Computational Linguistics.
Thamar Solorio and Yang Liu. 2008b. Part-of-speech
tagging for English-Spanish code-switched text. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1051?
1060. Association for Computational Linguistics.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar. Associ-
ation for Computational Linguistics.
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. DCU: Aspect-based polarity
classification for SemEval task 4. In Proceedings
of the International Workshop on Semantic Evalu-
ation (SemEval-2014), pages 392?397, Dublin, Ire-
land. Association for Computational Linguistics.
Jochen Weiner, Ngoc Thang Vu, Dominic Telaar, Flo-
rian Metze, Tanja Schultz, Dau-Cheng Lyu, Eng-
Siong Chng, and Haizhou Li. 2012. Integration
of language identification into a recognition system
for spoken conversations containing code-switches.
In Proceedings of the 3rd Workshop on Spoken Lan-
guage Technologies for Under-resourced Languages
(SLTU?12), Cape Town, South Africa. International
Research Center MICA.
Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012.
Text segmentation by language using minimum de-
scription length. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 969?978.
Association for Computational Linguistics.
23
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 127?132,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
DCU-UVT: Word-Level Language Classification with Code-Mixed Data
Utsab Barman, Joachim Wagner, Grzegorz Chrupa?a
?
and Jennifer Foster
CNGL Centre for Global Intelligent Content, National Centre for Language Technology
School of Computing, Dublin City University, Dublin, Ireland
?
Tilburg School of Humanities, Department of Communication and Information Sciences
Tilburg University, Tilburg, The Netherlands
{ubarman,jwagner,jfoster}@computing.dcu.ie
G.A.Chrupala@uvt.nl
Abstract
This paper describes the DCU-UVT
team?s participation in the Language Iden-
tification in Code-Switched Data shared
task in the Workshop on Computational
Approaches to Code Switching. Word-
level classification experiments were car-
ried out using a simple dictionary-based
method, linear kernel support vector ma-
chines (SVMs) with and without con-
textual clues, and a k-nearest neighbour
approach. Based on these experiments,
we select our SVM-based system with
contextual clues as our final system and
present results for the Nepali-English and
Spanish-English datasets.
1 Introduction
This paper describes DCU-UVT?s participation
in the shared task Language Identification in
Code-Switched Data (Solorio et al., 2014) at
the Workshop on Computational Approaches to
Code Switching, EMNLP, 2014. The task is to
make word-level predictions (six labels: lang1,
lang2, ne, mixed, ambiguous and other) for mixed-
language user generated content. We submit pre-
dictions for Nepali-English and Spanish-English
data and perform experiments using dictionaries, a
k-nearest neighbour (k-NN) classifier and a linear-
kernel SVM classifier.
In our dictionary-based approach, we investi-
gate the use of different English dictionaries as
well as the training data. In the k-NN based
approach, we use string edit distance, character-
n-gram overlap and context similarity to make
predictions. For the SVM approach, we experi-
ment with context-independent (word, character-
n-grams, length of a word and capitalisation in-
formation) and context-sensitive (adding the pre-
vious and next word as bigrams) features in differ-
ent combinations. We also experiment with adding
features from the k-NN approach and another set
of features from a neural network. Based on per-
formance in cross-validation, we select the SVM
classifier with basic features (word, character-n-
grams, length of a word, capitalisation information
and context) as our final system.
2 Background
While the problem of automatically identify-
ing and analysing code-mixing has been iden-
tified over 30 years ago (Joshi, 1982), it has
only recently drawn wider attention. Specific
problems addressed include language identifica-
tion in multilingual documents, identification of
code-switching points and POS tagging (Solorio
and Liu, 2008b) of code-mixing data. Ap-
proaches taken to the problem of identifying code-
mixing include the use of dictionaries (Nguyen
and Do?gru?oz, 2013; Barman et al., 2014; El-
fardy et al., 2013; Solorio and Liu, 2008b), lan-
guage models (Alex, 2008; Nguyen and Do?gru?oz,
2013; Elfardy et al., 2013), morphological and
phonological analysis (Elfardy et al., 2013; El-
fardy and Diab, 2012) and various machine learn-
ing algorithms such as sequence labelling with
Hidden Markov Models (Farrugia, 2004; Ros-
ner and Farrugia, 2007) and Conditional Random
Fields (Nguyen and Do?gru?oz, 2013; King and
Abney, 2013), as well as word-level classifica-
tion using Naive Bayes (Solorio and Liu, 2008a),
logistic regression (Nguyen and Do?gru?oz, 2013)
and SVMs (Barman et al., 2014), using features
such as word, POS, lemma and character-n-grams.
Language pairs that have been explored include
English-Maltese (Farrugia, 2004; Rosner and Far-
rugia, 2007), English-Spanish (Solorio and Liu,
2008b), Turkish-Dutch (Nguyen and Do?gru?oz,
127
2013), modern standard Arabic-Egyptian di-
alect (Elfardy et al., 2013), Mandarin-English (Li
et al., 2012; Lyu et al., 2010), and English-Hindi-
Bengali (Barman et al., 2014).
3 Data Statistics
The training data provided for this task consists of
tweets. Unfortunately, because of deleted tweets,
the full training set could not be downloaded. Out
of 9,993 Nepali-English training tweets, we were
able to download 9,668 and out of 11,400 Spanish-
English training tweets, we were able to download
11,353. Table 1 shows the token-level statistics of
the two datasets.
Label Nepali-English Spanish-English
lang1 (en) 43,185 76,204
lang2 (ne/es) 59,579 32,477
ne 3,821 2,814
ambiguous 125 341
mixed 112 51
other 34,566 21,813
Table 1: Number of tokens in the Nepali-English
and Spanish-English training data for each label
Nepali (lang2) is the dominant language in
the Nepali-English training data but for Spanish-
English, English (lang1) is dominant. The third
largest group contains tokens with the label other.
These are mentions (@username), punctuation
symbols, emoticons, numbers (except numbers
that represent words such as 2 for to), words in a
language other than lang1 and lang2 and unintel-
ligible words. Named entities (ne) are much less
frequent and mixed language words (e.g. ramri-
ness) and words for which there is not enough con-
text to disambiguate them are rare. Hash tags are
annotated as if the hash symbol was not there, e.g.
#truestory is labelled lang1.
4 Experiments
All experiments are carried out for Nepali-English
data. Later we apply the best approach to Spanish-
English. We train our systems in a five-fold cross-
validation and obtain best parameters based on
average cross-validation results. Cross-validation
splits are made based on users, i.e. we avoid the
occurrence of a user?s tweets both in training and
test splits for each cross-validation run. We ad-
dress the task with the following approaches:
1. a simple dictionary-based classifier,
Resource Accuracy
BNC 43.61
LexNorm 54.60
TrainingData 89.53
TrainingData+BNC+LexNorm 90.71
Table 2: Average cross-validation accuracy of
dictionary-based prediction for Nepali-English
2. classification using supervised machine
learning with k-nearest neighbour, and
3. classification using supervised machine
learning with SVMs.
4.1 Dictionary-Based Detection
We start with a simple dictionary-based approach
using as dictionaries (a) the British National Cor-
pus (BNC) (Aston and Burnard, 1998), (b) Han
et al.?s lexical normalisation dictionary (LexNorm)
(Han et al., 2012) and (c) the training data.
The BNC and LexNorm dictionaries are built by
recording all words occurring in the respective
corpus or word list as English. For the BNC, we
also collect word frequency information. For the
training data, we obtain dictionaries for each of the
six labels and each of the five cross-validation runs
(using the relevant 4/5 of training data).
To make a prediction, we consult all dictionar-
ies. If there are more than one candidate label,
we choose the label for which the frequency for
the query token is highest. To account for the fact
that the BNC is much larger than the training data,
we normalise all frequencies before comparison.
LexNorm has no frequency information, hence it
is added to our system as a simple word list (we
consider the language of a word to be English if it
appears in LexNorm). If a word appears in multi-
ple dictionaries with the same frequency or if the
word does not appear in any dictionary or list, the
predicted language is chosen based on the domi-
nant language(s)/label(s) of the corpus.
We experiment with the individual dictionar-
ies and the combination of all three dictionaries,
among which the combination achieves the high-
est cross-validation accuracy (90.71%). Table 2
shows the results of dictionary-based detection ob-
tained in five-fold cross-validation.
4.2 Classification with k-NN
For Nepali-English, we also experiment with a
simple k-nearest neighbour (k-NN) approach. For
each test item, we select a subset of the training
data using string edit distance and n-gram overlap
128
and choose the majority label of the subset as our
prediction. For efficiency, we first select k
1
items
that share an n-gram with the token to be classi-
fied.
1
The set of k
1
items is then re-ranked ac-
cording to string edit distance to the test item and
the best k
2
matches are used to make a prediction.
Apart from varying k
1
and k
2
, we experiment
with (a) lowercasing strings, (b) including context
by concatenating the previous, current and next
token, and (c) weighting context by first calcu-
lating edit distances for the previous, current and
next token separately and using a weighted aver-
age. The best configuration we found in cross-
validation uses lowercasing with k
1
= 800 and
k
2
= 16 but no context information. It achieves
an accuracy of 94.97%.
4.3 SVM Classification
We experiment with linear kernel SVM classifiers
using Liblinear (Fan et al., 2008). Parameter opti-
misation
2
is performed for each feature set combi-
nation to obtain best cross-validation accuracy.
4.3.1 Basic Features
Following Barman et al. (2014), our basic features
are:
Char-N-Grams (G): We start with a charac-
ter n-gram-based approach (Cavnar and Trenkle,
1994). Following King and Abney (2013), we se-
lect lowercased character n-grams (n=1 to 5) and
the word as the features in our experiments.
Dictionary-Based Labels (D): We use presence
in the dictionary of the 5,000 most frequent words
in the BNC and presence in the LexNorm dictio-
nary as binary features.
3
Length of words (L): We create multiple fea-
tures for token length using a decision tree (J48).
We use length as the only feature to train a deci-
sion tree for each fold and use the nodes obtained
from the tree to create boolean features (Rubino et
al., 2013; Wagner et al., 2014).
1
Starting with n = 5, we decrease n until there are at
least k
1
items and then we randomly remove items added in
the last augmentation step to arrive at exactly k
1
items. (For
n = 0, we randomly sample from the full training data.)
2
C = 2
i
with i = ?15,?14, ..., 10
3
We chose these parameters based on experiments with
each dictionary, combinations of dictionaries and various fre-
quency thresholds. We apply a frequency threshold to the
BNC to increase precision. We rank the words according to
frequency and used the rank as a threshold (e.g. top-5K, top-
10K etc.). With the top 5,000 ranked words and C = 0.25,
we obtained best accuracy (96.40%).
Features Accuracy Features Accuracy
G 96.02 GD 96.27
GL 96.11 GDL 96.32
GC 96.15 GDC 96.20
GLC 96.21 GDLC 96.40
Table 3: Average cross-validation accuracy of 6-
way SVMs on the Nepali-English data set; G =
char-n-gram, L = binary length features, D = dict.-
based labels and C = capitalisation features
Context Accuracy(%)
GDLC + P
1
96.41
GDLC + P
2
96.38
GDLC + N
1
96.41
GDLC + N
2
96.41
GDLC + P
1
+ N
1
96.42
GDLC + P
2
+ N
2
96.41
Table 4: Average cross-validation accuracy of 6-
way SVMs using contextual features for Nepali-
English
Capitalisation (C): We choose 3 boolean
features to encode capitalisation information:
whether any letter in the word is capitalised,
whether all letters in the word are capitalised and
whether the first letter is capitalised.
Context (P
i
and N
j
): We consider the previous
i and next j token to be combined with the current
token, forming an (i+1)-gram and a (j+1)-gram,
which we add as features. Six settings are tested.
Table 4 shows that using the bigrams formed with
the previous and next word are the best combina-
tion for the task (among those tested).
Among the eight combinations of the first four
feature sets that contain the first set (G), Table 3
shows that the 6-way SVM classifier
4
performs
best with all features sets (GDLC), achieving
96.40% accuracy. Adding contextual information
P
i
N
j
to GDLC, Table 4 shows best results for
i=j=1, achieving 96.42% accuracy, only slightly
ahead of the context-independent system.
4.3.2 Neural Network (Elman) and k-NN
Features
We experiment with two additional features sets
not covered by Barman et al. (2014):
Neural Network (Elman): We extract features
from the hidden layer of a recurrent neural net-
4
We also test 3-way SVM classification (lang1, lang2 and
other) and heuristic post-processing, but it does not outper-
form our 6-way classification runs.
129
Systems Accuracy
GDLC 96.40
k-NN 95.10
Elman 89.96
GDLC+k-NN 96.31
GDLC+Elman 96.46
GDLC+k-NN+Elman 96.40
GDLC+P
1
N
1
96.42
k-NN+P
1
N
1
95.11
Elman+P
1
N
1
91.53
GDLC+P
1
N
1
+k-NN 96.33
GDLC+P
1
N
1
+Elman 96.45
GDLC+P
1
N
1
+k-NN+Elman 96.40
Table 5: Average cross-validation accuracy of 6-
way SVMs of combinations of GDLC, k-NN, El-
man and P
1
N
1
features for Nepali-English
work that has been trained to predict the next char-
acter in a string (Chrupa?a, 2014). The 10 most ac-
tive units of the hidden layer for each of the initial
4 bytes and final 4 bytes of each token are bina-
rised by using a threshold of 0.5.
k-Nearest Neighbour (kNN): We obtain fea-
tures from our basic k-NN approach (Section 4.2),
encoding the prediction of the k-NN model with
six binary features (one for each label) and a nu-
meric feature for each label stating the relative
number of votes for the label, e.g. if k
2
= 16
and 12 votes are for lang1 the value of the fea-
ture votes4lang1 will be 0.75. Furthermore, we
add two features stating the minimum and maxi-
mum edit distance between the test token and the
k
2
selected training tokens.
Table 5 shows cross-validation results for these
new feature sets with and without the P
1
N
1
con-
text features. Excluding the GDLC features, we
can see that best accuracy is with k-NN and P
1
N
1
features (95.11%). For Elman features, the accu-
racy is lower (91.53% with context). In combina-
tion with the GDLC features, however, the Elman
features can achieve a small improvement over
the GDLC+P
1
N
1
combination (+0.04 percentage
points): 96.46% accuracy for the GDLC+Elman
setting (without P
1
N
1
features). Furthermore, the
k-NN features do not combine well.
5
4.3.3 Final System and Test Results
At the time of submission of predictions, we had
an error in our GDLC+Elman feature combiner re-
5
A possible explanation may be that the k-NN features
are based on only 3 of 5 folds for the training data (3 folds
are used to make predictions for the 4th set) but 4 of 5 folds
are used for test data predictions in each cross-validation run.
Tweets
Token-Level Tweet-Level
Nepali-English 96.3 95.8
Spanish-English 84.4 80.4
Surprise Genre
Token-Level Post-Level
Nepali-English 85.6 77.5
Spanish-English 94.4 80.0
Table 6: Test set results (overall accuracy) for
Nepali-English and Spanish-English tweet data
and surprise genre
sulting in slightly lower performance. Therefore,
we selected SVM-GDLC-P
1
N
1
as our final ap-
proach and trained the final two systems using the
full training data for Nepali-English and Spanish-
English respectively. While we knew that C =
0.125 is best for Nepali-English from our experi-
ments, we had to re-tune parameter C for Spanish-
English using cross-validation on the training data.
We found best accuracy of 94.16% for Spanish-
English with C = 128. Final predictions for the
test sets are made using these systems.
Table 6 shows the test set results. The test
set for this task is divided into tweets and a sur-
prise genre. For the tweets, we achieve 96.3%
and 84.4% accuracy (overall token-level accuracy)
in Nepali-English and in Spanish-English respec-
tively. For this surprise genre (a collection of posts
from Facebook and blogs), we achieve 85.6% for
Nepali-English and 94.4% for Spanish-English.
5 Conclusion
To summarise, we achieved reasonable accuracy
with a 6-way SVM classifier by employing basic
features only. We found that using dictionaries
is helpful, as are contextual features. The perfor-
mance of the k-NN classifier is also notable: it is
only 1.45 percentage points behind the final SVM-
based system (in terms of cross-validation accu-
racy). Adding neural network features can further
increase the accuracy of systems.
Briefly opening the test files to check for for-
matting issues, we notice that the surprise genre
data contains language-specific scripts that could
easily be addressed in an English vs. non-English
scenario.
Acknowledgments
This research is supported by the Science Founda-
tion Ireland (Grant 12/CE/I2267) as part of CNGL
(www.cngl.ie) at Dublin City University.
130
References
Beatrice Alex. 2008. Automatic detection of English
inclusions in mixed-lingual data with an application
to parsing. Ph.D. thesis, School of Informatics, The
University of Edinburgh, Edinburgh, UK.
Guy Aston and Lou Burnard. 1998. The BNC hand-
book: exploring the British National Corpus with
SARA. Capstone.
Utsab Barman, Amitava Das, Joachim Wagner, and
Jennifer Foster. 2014. Code-mixing: A challenge
for language identification in the language of so-
cial media. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Theo Pavlidis,
editor, Proceedings of SDAIR-94, Third Annual
Symposium on Document Analysis and Information
Retrieval, pages 161?175.
Grzegorz Chrupa?a. 2014. Normalizing tweets with
edit scripts and recurrent neural embeddings. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 680?686, Baltimore, Mary-
land, June. Association for Computational Linguis-
tics.
Heba Elfardy and Mona Diab. 2012. Token level
identification of linguistic code switching. In Pro-
ceedings of Proceedings of COLING 2012: Posters
(the 24th International Conference on Computa-
tional Linguistics), pages 287?296, Mumbai, India.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code switch point detection in Ara-
bic. In Natural Language Processing and Informa-
tion Systems, pages 412?416. Springer.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Paulseph-John Farrugia. 2004. TTS pre-processing is-
sues for mixed language support. In Proceedings
of CSAW?04, the second Computer Science Annual
Workshop, pages 36?41. Department of Computer
Science & A.I., University of Malta.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation dictio-
nary for microblogs. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 421?432. Association
for Computational Linguistics.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In J. Horeck?y, ed-
itor, Proceedings of the 9th conference on Compu-
tational linguistics - Volume 1 (COLING?82), pages
145?150. Academia Praha, North-Holland Publish-
ing Company.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110?
1119, Atlanta, Georgia, June. Association for Com-
putational Linguistics.
Ying Li, Yue Yu, and Pascale Fung. 2012. A
mandarin-english code-switching corpus. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Mehmet Uur Doan, Bente Mae-
gaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC?12), Istanbul,
Turkey, may. European Language Resources Asso-
ciation (ELRA).
Dau-Cheng Lyu, Tien Ping Tan, Engsiong Chng, and
Haizhou Li. 2010. SEAME: A Mandarin-English
code-switching speech corpus in South-East Asia.
In INTERSPEECH 2010, 11th Annual Conference
of the International Speech Communication Asso-
ciation, volume 10, pages 1986?1989, Makuhari,
Chiba, Japan. ISCA Archive.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2013), pages 857?862, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.
Mike Rosner and Paulseph-John Farrugia. 2007. A
tagging algorithm for mixed language identifica-
tion in a noisy domain. In INTERSPEECH-2007,
8th Annual Conference of the International Speech
Communication Association, pages 190?193. ISCA
Archive.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397, Sofia, Bulgaria.
Association for Computational Linguistics.
Thamar Solorio and Yang Liu. 2008a. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981. Association for
Computational Linguistics.
Thamar Solorio and Yang Liu. 2008b. Part-of-speech
tagging for English-Spanish code-switched text. In
131
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1051?
1060. Association for Computational Linguistics.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. DCU: Aspect-based polarity
classification for SemEval task 4. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval-2014), pages 392?397, Dublin, Ireland,
August. Association for Computational Linguistics.
132
