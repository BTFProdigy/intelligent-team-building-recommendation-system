Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Machine Learning Techniques to Build a Comma Checker for 
Basque
I?aki Alegria Bertol Arrieta Arantza Diaz de Ilarraza Eli Izagirre Montse Maritxalar
Computer Engineering Faculty. University of the Basque Country.
Manuel de Lardizabal Pasealekua, 1
20018 Donostia, Basque Country, Spain.
{acpalloi,bertol,jipdisaa,jibizole,jipmaanm}@ehu.es
Abstract
In this paper, we describe the research 
using  machine  learning  techniques  to 
build a comma checker to be integrated 
in a grammar checker for Basque. After 
several experiments, and trained with a 
little corpus of 100,000 words, the sys?
tem guesses correctly not placing com?
mas with a precision of 96% and a re?
call of 98%. It also gets a precision of 
70% and a recall of 49% in the task of 
placing  commas.  Finally,  we  have 
shown  that  these  results  can  be  im?
proved using a bigger and a more ho?
mogeneous  corpus  to  train,  that  is,  a 
bigger corpus written by one unique au?
thor. 
1 Introduction
In the last years, there have been many studies 
aimed  at  building  a  grammar  checker  for  the 
Basque language (Ansa et al, 2004; Diaz De Il?
arraza et al, 2005). These works have been fo?
cused, mainly, on building rule sets ??taking into 
account syntactic information extracted from the 
corpus  automatically??  that  detect  some  erro?
neous grammar forms. The research here presen?
ted wants to complement the earlier work by fo?
cusing on  the  style  and the  punctuation of  the 
texts. To be precise, we have experimented using 
machine learning techniques for the special case 
of the comma, to evaluate their performance and 
to analyse the possibility of applying it in other 
tasks of the grammar checker.  
However,  developing  a  punctuation  checker 
encounters  one  problem  in  particular:  the  fact 
that the punctuation rules are not totally estab?
lished. In general, there is no problem when us?
ing the  full  stop,  the  question mark or  the ex?
clamation mark.  Santos (1998) highlights these 
marks are reliable punctuation marks, while all 
the rest are unreliable. Errors related to the reli?
able ones (putting or not the initial  question or 
exclamation mark depending on the language, for 
instance) are not so hard to treat. A rule set to 
correct some of these has already been defined 
for the Basque language (Ansa et al, 2004). In 
contrast, the comma is the most polyvalent and, 
thus, the least defined punctuation mark (Bayrak?
tar et al, 1998; Hill and Murray, 1998). The am?
biguity of the comma, in fact,  has been shown 
often (Bayraktar et  al.,  1998; Beeferman et al, 
1998;  Van  Delden  S.  and  Gomez  F.,  2002). 
These works have shown the lack of fixed rules 
about the comma. There are only some intuitive 
and  generally  accepted  rules,  but  they  are  not 
used in a standard way. In Basque, this problem 
gets even more evident, since the standardisation 
and  normalisation  of  the  language  began  only 
about twenty?five years ago and it  has not fin?
ished yet. Morphology is mostly defined, but, on 
the contrary, as far as syntax is concerned, there 
is  quite  work  to  do.  In  punctuation  and  style, 
some basic rules have been defined and accepted 
by the Basque Language Academy (Zubimendi, 
2004).  However,  there  are  not  final  decisions 
about the case of the comma. 
Nevertheless,  since  Nunberg?s  monograph 
(Nunberg, 1990), the importance of the comma 
has  been  undeniable,  mainly  in  these  two  as?
pects: i) as a due to the syntax of the sentence 
(Nunberg, 1990; Bayraktar et al, 1998; Garzia, 
1997), and ii) as a basis to improve some natural 
language  processing  tools  (syntactic  analysers, 
error  detection  tools?),  as  well  as  to  develop 
some  new  ones  (Briscoe  and  Carroll,  1995; 
Jones, 1996). The relevance of the comma for the 
syntax of the sentence may be easily proved with 
some clarifying examples where the sentence is 
understood in  one or  other  way,  depending on 
whether  a  comma  is  placed  or  not  (Nunberg, 
1990): 
a. Those students who can, contribute to the 
United Fund. 
b. Those students who can contribute to the 
United Fund. 
1
In the same sense,  it  is  obvious  that  a  well 
punctuated  text,  or  more  concretely,  a  correct 
placement of the commas, would help consider?
ably  in  the  automatic  syntactic  analysis  of  the 
sentence,  and, therefore,  in the development of 
more and better tools in the NLP field. Say and 
Akman (1997) summarise the research efforts in 
this direction.
As an important background for our work, we 
note  where  the  linguistic  information  on  the 
comma for the Basque language was formalised. 
This  information  was  extracted  after  analysing 
the  theories  of  some experts  in  Basque  syntax 
and punctuation (Aldezabal et al, 2003). In fact, 
although no final decisions have been taken by 
the Basque Language Academy yet,  the theory 
formalised in the above mentioned work has suc?
ceeded in unifying the main points of view about 
the  punctuation in  Basque.  Obviously,  this  has 
been the basis for our work. 
2 Learning commas
We have designed two different but combinable 
ways to get the comma checker:
? based on clause boundaries
? based directly on corpus
Bearing  in  mind  the formalised  theory  of 
Aldezabal et  al.  (2003)1,  we realised that if  we 
got to split the sentence into clauses, it would be 
quite easy to develop rules for detecting the exact 
places where commas would have to go. Thus, 
the best way to build a comma checker would be 
to get, first, a clause identification tool. 
Recent papers in this area report quite good 
results using machine learning techniques. Car?
reras and M?rquez (2003) get one of the best per?
formances in this  task (84.36% in test).  There?
fore, we decided to adopt this as a basis in order 
to  get  an  automatic  clause  splitting  tool  for 
Basque.  But  as  it  is  known,  machine  learning 
techniques cannot be applied if no training cor?
pus is available, and one year ago, when we star?
ted this  process,  Basque texts  with this  tagged 
clause splits were not available.
Therefore, we decided to use the second al?
ternative.  We  had  available  some  corpora  of 
Basque, and we decided to try learning commas 
from raw text, since a previous tagging was not 
needed. The problem with the raw text is that its 
commas are not the result of applying consistent 
rules.
1 From now on, we will speak about this as ?the accepted theory of Basque 
punctuation?. 
Related work
Machine learning techniques have been applied 
in many fields and for  many purposes,  but  we 
have found only one reference in the literature 
related to the use of machine learning techniques 
to assign commas automatically. 
Hardt (2001) describes research in using the 
Brill tagger (Brill 1994; Brill, 1995) to learn to 
identify incorrect commas in Danish. The system 
was developed by randomly inserting commas in 
a text, which were tagged as incorrect, while the 
original  commas  were  tagged  as  correct.  This 
system identifies incorrect commas with a preci?
sion  of  91%  and  a  recall  of  77%,  but  Hardt 
(2001) does not mention anything about identify?
ing correct commas. 
In  our  proposal,  we have tried  to  carry out 
both aspects, taking as a basis other works that 
also use machine learning techniques in similar 
problems  such  as  clause  splitting  (Tjong  Kim 
Sang E.F. and D?jean H., 2001) or detection of 
chunks (Tjong Kim Sang E.F. and Buchholz S., 
2000).
3 Experimental setup
Corpora
As we have mentioned before, some corpora 
in Basque are available. Therefore, our first task 
was to select the training corpora, taking into ac?
count that well punctuated corpora were needed 
to train the machine correctly. For that purpose, 
we looked for corpora that satisfied as much as 
possible our ?accepted theory of Basque punctu?
ation?.  The  corpora  of  the  unique  newspaper 
written in Basque, called  Egunkaria (nowadays 
Berria), were chosen, since they are supposed to 
use the ?accepted theory of Basque punctuation?. 
Nevertheless,  after  some brief  verifications, we 
realised that the texts of the corpora do not fully 
match with our theory. This can be understood 
considering that a lot of people work in a news?
paper. That is, every journalist can use his own 
interpretation of  the  ?accepted theory?,  even if 
all of them were instructed to use it in the same 
way. Therefore, doing this  research, we had in 
mind that the results we would get were not go?
ing to be perfect.
To counteract this problem, we also collected 
more  homogeneous  corpora  from  prestigious 
writers: a translation of a book of philosophy and 
a novel. Details about these corpora are shown in 
Table 1.
2
Size of the corpora
Corpora from the newspaper Egunkaria 420,000 words
Philosophy texts written by one unique author 25,000 words
Literature texts written by one unique author 25,000 words
Table 1. Dimensions of the used corpora
A short version of the first corpus was used in 
different experiments in order to tune the system 
(see section 4). The differences between the re?
sults  depending on the type of  the corpora are 
shown in section 5.
Evaluation
Results are shown using the standard measures in 
this area: precision, recall and f?measure2, which 
are calculated based on the test corpus. The res?
ults are shown in two colums ("0" and "1") that 
correspond to the result categories used. The res?
ults for the column ?0? are the ones for the in?
stances that are not followed by a comma. On the 
contrary, the results for the column ?1? are the 
results for the instances that should be followed 
by a comma. 
Since  our  final  goal  is  to  build  a  comma 
checker,  the precision in the column ?1? is the 
most  important  data  for  us,  although the recall 
for the same column is also relevant. In this kind 
of tools, the most important thing is to first ob?
tain all the comma proposals right (precision in 
columns ?1?), and then to obtain all the possible 
commas (recall in columns ?1?).
Baselines
In  the  beginning,  we  calculated  two  possible 
baselines based on a big part of the newspaper 
corpora in order to choose the best one. 
The  first  one  was  based  on  the  number  of 
commas  that  appeared  in  these  texts.  In  other 
words,  we  calculated  how  many  commas  ap?
peared in the corpora (8% out of all words), and 
then we put commas randomly in this proportion 
in the test corpus. The results obtained were not 
very good (see Table 2, baseline1), especially for 
the  instances  ?followed by  a  comma? (column 
?1?).
The second baseline was developed using the 
list  of  words appearing before a comma in the 
training corpora. In the test corpus, a word was 
tagged as ?followed by a comma? if it was one of 
the words of the mentioned list. The results (see 
baseline 2, in Table 2) were better, in this case, 
for the instances followed by a comma (column 
named  ?1?).  But,  on  the  contrary,  baseline  1 
provided us with better results for the instances 
not followed by a comma (column named ?0?). 
That is why we decided to take, as our baseline, 
2 f?measure = 2*precision*recall / (precision+recall)
the best data offered by each baseline (the ones 
in bold in table 2). 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
baseline 1 0.927 0.924 0.926 0.076 0.079 0.078
baseline 2 0.946 0.556 0.700 0,096 0.596 0.165
Table 2: The baselines
Methods and attributes
We  use  the  WEKA3 implementation  of  these 
classifiers: the Naive Bayes based classifier (Na?
iveBayes),  the  support  vector  machine  based 
classifier  (SMO)  and  the  decision?tree  (C4.5) 
based one (j48).
It  has  to  be  pointed  out  that  commas  were 
taken  away  from  the  original  corpora.  At  the 
same time, for each token, we stored whether it 
was followed by a  comma or not.  That  is,  for 
each  word  (token),  it  was  stored  whether  a 
comma was placed next to it or not. Therefore, 
each token in the corpus is equivalent to an ex?
ample (an instance). The attributes of each token 
are based on the token itself and some surround?
ing ones. The application window describes the 
number of tokens considered as information for 
each token.
Our initial application window was [?5, +5]; 
that means we took into account the previous and 
following 5 words (with their corresponding at?
tributes)  as  valid  information  for  each  word. 
However, we tuned the system with different ap?
plication windows (see section 4). 
Nevertheless, the attributes managed for each 
word can be as complex as we want. We could 
only use words, but we thought some morpho?
syntactic information would be beneficial for the 
machine to learn. Hence, we decided to include 
as much information as we could extract using 
the shallow syntactic parser of Basque (Aduriz et 
al.,  2004).  This  parser  uses  the  tokeniser,  the 
lemmatiser, the chunker and the morphosyntactic 
disambiguator  developed by  the  IXA4 research 
group. 
The attributes we chose to use for each token 
were the following:
? word?form
? lemma
? category 
? subcategory
? declension case
? subordinate?clause type
3 WEKA is a collection of machine learning algorithms for data mining tasks 
(http://www.cs.waikato.ac.nz/ml/weka/).
4 http://ixa.si.ehu.es
3
? beginning of chunk (verb, nominal, enti?
ty, postposition)
? end of chunk (verb, nominal, entity, post?
position)
? part of an apposition
? other  binary  features:  multiple  word  to?
ken,  full  stop,  suspension  points,  colon, 
semicolon,  exclamation  mark  and  ques?
tion mark 
We also included some additional  attributes 
which were automatically calculated: 
? number of verb chunks to the beginning 
and to the end of the sentence 
? number of nominal chunks to the begin?
ning and to the end of the sentence
? number  of  subordinate?clause  marks  to 
the beginning and to the end of the sen?
tence
? distance (in tokens) to the beginning and 
to the end of the sentence 
We also did other experiments using binary 
attributes that correspond to most used colloca?
tions (see section 4).
Besides, we used the result attribute ?comma? 
to store whether a comma was placed after each 
token. 
4 Experiments
Dimension of the corpus
In  this  test,  we  employed the  attributes  de?
scribed in section 3 and an initial window of [?5, 
+5], which means we took into account the pre?
vious 5 tokens and the following 5. We also used 
the C4.5 algorithm initially, since this algorithm 
gets very good results in other similar machine 
learning  tasks  related  to  the  surface  syntax 
(Alegria et al, 2004).
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
100,000 train / 30,000 test 0,955 0,981 0,968 0,635 0,417 0,503
160,000 train / 45,000 test 0,947 0,981 0,964 0,687 0,43 0,529
330,000 train / 90,000 test 0,96 0,982 0,971 0,701 0,504 0,587
Table 3. Results depending on the size of corpora 
(C4.5 algorithm; [?5,+5] window).
As it  can be seen in table 3, the bigger the 
corpus,  the  better  the results,  but  logically,  the 
time expended to obtain the results also increases 
considerably. That is why we chose the smallest 
corpus  for  doing  the  remaining  tests  (100,000 
words  to  train  and  30,000  words  to  test).  We 
thought that the size of this corpus was enough to 
get good comparative results. This test, anyway, 
suggested that the best  results  we could obtain 
would  be  always  improvable  using  more  and 
more corpora. 
Selecting the window
Using the corpus and the attributes described be?
fore, we did some tests to decide the best applic?
ation window. As we have already mentioned, in 
some problems of this type, the information of 
the  surrounding  words  may  contain  important 
data to decide the result of the current word. 
In this test, we wanted to decide the best ap?
plication window for our problem. 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
-5+5 0,955 0,981 0,968 0,635 0,417 0,503
-2+5 0,956 0,982 0,969 0,648 0,431 0,518
-3+5 0,957 0,979 0,968 0,627 0,441 0,518
-4+5 0,957 0,98 0,968 0,634 0,446 0,52
-5+2 0,956 0,982 0,969 0,65 0,424 0,514
-5+3 0,956 0,981 0,969 0,643 0,432 0,517
-5+4 0,955 0,982 0,968 0,64 0,417 0,505
-6+2 0,956 0,982 0,969 0,645 0,421 0,509
-6+3 0,956 0,982 0,969 0,646 0,426 0,514
-8+2 0,956 0,982 0,969 0,645 0,425 0,513
-8+3 0,956 0,979 0,967 0,615 0,431 0,507
-8+8 0,956 0,978 0,967 0,604 0,422 0,497
Table  4.  Results  depending  on  the  application 
window (C4.5 algorithm; 100,000 train / 30,000 
test)
As it can be seen, the best f?measure for the 
instances followed by a comma was obtained us?
ing the application window [?4,+5]. However, as 
we have said before, we are more interested in 
the precision. Thus, the application window [?5
,+2] gets the best precision, and, besides, its f?
measure is almost the same as the best one. This 
is the reason why we decided to choose the [?5
,+2] application window. 
Selecting the classifier
With  the  selected  attributes,  the  corpus  of 
130,000 words and the application window of [?5
, +2], the next step was to select the best classifi?
er for our problem. We tried the WEKA imple?
mentation of these classifiers:  the Naive Bayes 
based classifier (NaiveBayes), the support vector 
machine based classifier (SMO) and the decision 
tree based one (j48).  Table 5 shows the results 
obtained:
4
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
NB 0,948 0,956 0,952 0,376 0,335 0,355
SMO 0,936 0,994 0,965 0,672 0,143 0,236
J48 0,956 0,982 0,969 0,652 0,424 0,514
Table 5. Results depending on the classifier 
(100,000 train / 30,000 test; [?5, +2] window).
As we can see, the f?measure for the instances 
not followed by a comma (column ?0?) is almost 
the same for the three classifiers, but, on the con?
trary, there is a considerable difference when we 
refer  to  the  instances  followed  by  a  comma 
(column ?1?). The best f?measure gives the C4.5 
based classifier (J48) due to the better recall, al?
though the best precision is for the support vector 
machine  based  classifier  (SMO).  Definitively, 
the Na?ve Bayes (NB) based classifier was dis?
carded, but we had to think about the final goal 
of our research to choose between the other two 
classifiers.  Since our  final  goal  was to  build  a 
comma checker, we would have to have chosen 
the classifier that gave us the best precision, that 
is, the support vector machine based one. But the 
recall of the support vector machine based classi?
fier was not as good as expected to be selected. 
Consequently,  we  decided  to  choose  the  C4.5 
based classifier. 
Selecting examples
At this  moment,  the results  we get  seem to be 
quite good for the instances not  followed by a 
comma, but  not  so good for  the  instances  that 
should follow a comma. This could be explained 
by the fact that we have no balanced training cor?
pus. In other words, in a normal text, there are a 
lot  of  instances not  followed by a  comma, but 
there are not so many followed by it. Thus, our 
training  corpus,  logically,  has  very  different 
amounts of instances followed by a comma and 
not followed by a comma. That is the reason why 
the system will learn more easily to avoid the un?
necessary  commas  than  placing  the  necessary 
ones. 
Therefore,  we  resolved  to  train  the  system 
with a corpus where the number of instances fol?
lowed by a comma and not followed by a comma 
was the same. For that purpose, we prepared a 
perl program that changed the initial corpus, and 
saved only x words for each word followed by a 
comma. 
In  table  6,  we can see  the  obtained results. 
One to one means that in that case, the training 
corpus  had  one  instance  not  followed  by  a 
comma, for each instance followed by a comma. 
On the  other  hand,  one to  two means that  the 
training corpus had two instances not  followed 
by  a  comma  for  each  word  followed  by  a 
comma, and so on. 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
normal 0,955 0,981 0,968 0,635 0,417 0,503
one to one 0,989 0,633 0,772 0,164 0,912 0,277
one to two 0,977 0,902 0,938 0,367 0,725 0,487
one to three 0,969 0,934 0,951 0,427 0,621 0,506
one to four 0,966 0,952 0,959 0,484 0,575 0,526
one to five 0,966 0,961 0,963 0,534 0,568 0,55
one to six 0,963 0,966 0,964 0,55 0,524 0,537
Table  6.  Results  depending  on  the  number  of 
words  kept  for  each  comma  (C4.5  algorithm; 
100,000 train / 30,000 test; [?5, +2] window). 
As  observed  in  the  previous  table,  the  best 
precision in the case of the instances followed by 
a comma is the original one: the training corpus 
where  no  instances  were  removed.  Note  that 
these results are referred as normal in table 6.
The corpus where a unique instance not fol?
lowed by a comma is kept for each instance fol?
lowed by a comma gets the best  recall  results, 
but the precision decreases notably. 
The  best  f?measure  for  the  instances  that 
should be followed by a comma is obtained by 
the one to five scheme, but as mentioned before, 
a comma checker must take care of offering cor?
rect comma proposals. In other words, as the pre?
cision of the original corpus is quite better (ten 
points better), we decided to continue our work 
with  the  first  choice:  the  corpus  where  no  in?
stances were removed. 
Adding new attributes
Keeping the best results obtained in the tests de?
scribed above (C4.5 with the [?5,  +2] window, 
and not removing any ?not comma? instances), 
we thought that giving importance to the words 
that appear normally before the comma would in?
crease our results. Therefore, we did the follow?
ing tests: 
1) To search a big corpus in order to extract 
the most  frequent  one hundred words  that  pre?
cede a  comma,  the  most  frequent  one hundred 
pairs of words (bigrams) that precede a comma, 
and the most frequent one hundred sets of three 
words (trigrams) that precede a comma, and use 
them as attributes in the learning process. 
2) To use only three attributes instead of the 
mentioned three hundred to encode the informa?
tion  about  preceding  words.  The  first  attribute 
would indicate whether a word is or not one of 
5
the  most  frequent  one  hundred  words.  The 
second attribute would mean whether a word is 
or not the last part of one of the most frequent 
one hundred pairs of words. And the third attrib?
ute would mean whether a word is or not the last 
part of one of the most frequent one hundred sets 
of three words. 
3) The case (1), but with a little difference: 
removing the attributes ?word? and ?lemma? of 
each instance. 
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
(0): normal 0,956 0,982 0,969 0,652 0,424 0,514
(1): 300 attributes + 0,96 0,983 0,972 0,696 0,486 0,572
(2): 3 attributes + 0,96 0,981 0,97 0,665 0,481 0,558
(3): 300 attributes +,  
no lemma, no word 0,955 0,987 0,971 0,71 0,406 0,517
Table 7. Results depending on the new attributes 
used (C4.5 algorithm; 100,000 train / 30,000 test; 
[?5, +2] window; not removed instances).
Table 7 shows that case number 1 (putting the 
300 data as attributes) improves the precision of 
putting  commas  (column  ?1?)  in  more  than  4 
points. Besides, it also improves the recall, and, 
thus, we improve almost 6 points its f?measure. 
The third test gives the best precision, but the 
recall decreases considerably. Hence, we decided 
to choose the case number 1, in table 7.
5 Effect of the corpus type
As we have said before (see section 3), depend?
ing on the quality of the texts, the results could 
be different.
In table 8, we can see the results using the dif?
ferent types of corpus described in table 1. Obvi?
ously,  to  give  a  correct  comparison,  we  have 
used the same size for all the corpora (20,000 in?
stances to train and 5,000 instances to test, which 
is the maximum size we have been able to ac?
quire for the three mentioned corpora).
0 1
Prec. Rec. Meas. Prec. Rec. Meas.
Newspaper 0.923 0.977 0.949 0.445 0.188 0.264
Philosophy 0.932 0.961 0.946 0.583 0.44 0.501
Literature 0.925 0.976 0.95 0.53 0.259 0.348
Table 8. Results depending on the type of corpo?
ra (20,000 train / 5,000 test).
The first line shows the results obtained using 
the short version of the newspaper. The second 
line  describes  the  results  obtained  using  the 
translation of a book of philosophy, written com?
pletely by one author. And the third one presents 
the  results  obtained  using  a  novel  written  in 
Basque. 
In any case, the results prove that our hypo?
thesis  was  correct.  Using  texts  written  by  a 
unique author improves the results. The book of 
philosophy has the best precision and the best re?
call.  It  could be  because it  has  very long sen?
tences  and  because  philosophical  texts  use  a 
stricter syntax comparing with the free style of a 
literature writer.  
As it was impossible for us to collect the ne?
cessary  amount  of  unique  author  corpora,  we 
could not go further in our tests.
6 Conclusions and future work
We have used machine learning techniques for 
the  task  of  placing  commas  automatically  in 
texts. As far as we know, it is quite a novel ap?
plication field. Hardt (2001) described a system 
which identified incorrect commas with a preci?
sion of 91% and a recall of 77% (using 600,000 
words  to  train).  These  results  are  comparable 
with the ones we obtain for the task of guessing 
correctly when not to place commas (see column 
?0? in the tables). Using 100,000 words to train, 
we obtain 96% of precision and 98.3% of recall. 
The main reason could be that we use more in?
formation to learn.
However, we have not obtained as good res?
ults as we hoped in the task of placing commas 
(we  get  a  precision  of  69.6%  and  a  recall  of 
48.6%). Nevertheless, in this particular task, we 
have  improved  considerably  with  the  designed 
tests, and more improvements could be obtained 
using more corpora and more specific corpora as 
texts written by a unique author or by using sci?
entific texts. 
Moreover,  we have detected some possible 
problems that could have brought these regular 
results in the mentioned task:
? No fixed rules for commas in the Basque 
language
? Negative influence when training using 
corpora from different writers
In this sense, we have carried out a little ex?
periment with some English corpora. Our hypo?
thesis was that a completely settled language like 
English,  where  comma  rules  are  more  or  less 
fixed, would obtain better results. Taking a com?
parative English corpus5 and similar learning at?
tributes6 to  Basque?s  one,  we  got,  for  the  in?
stances  followed  by  a  comma  (column  ?1?  in 
tables), a better precision (%83.3) than the best 
5 A newspaper corpus, from Reuters
6 Linguistic information obtained using Freeling (http://garraf.ep?
sevg.upc.es/freeling/)
6
one obtained for the Basque language. However, 
the recall was worse than ours: %38.7. We have 
to take into account that we used less learning at?
tributes with the English corpus and that we did 
not  change  the  application  window chosen  for 
the Basque experiment. Another application win?
dow would have been probably more suitable for 
English.  Therefore, we believe that with a few 
tests  we  easily  would  achieve  a  better  recall. 
These  results,  anyway,  confirm our  hypothesis 
and our diagnosis of the detected problems. 
Nevertheless,  we think the presented results 
for the Basque language could be improved. One 
way would  be  to  use  ?information  gain? tech?
niques in order to carry out the feature selection. 
On the other hand, we think that more syntactic 
information, concretely clause splits tags, would 
be especially beneficial to detect those commas 
named delimiters by Nunberg (1990).
In fact, our main future research will consist 
on clause identification. Based on the ?accepted 
theory of the comma?, we can assure that a good 
identification of clauses (together with some sig?
nificant linguistic information we already have) 
would enable us to put commas correctly in any 
text,  just  implementing some simple rules.  Be?
sides, a combination of both methods ??learning 
commas  and  putting  commas  after  identifying 
clauses??  would  probably  improve  the  results 
even more. 
Finally,  we contemplate building an ICALL 
(Intelligent Computer Assisted Language Learn?
ing) system to help learners to put commas cor?
rectly.
Acknowledgements
We would like to thank all the people who have 
collaborated in this research: Juan Garzia,  Joxe 
Ramon  Etxeberria,  Igone  Zabala,  Juan  Carlos 
Odriozola, Agurtzane Elorduy, Ainara Ondarra, 
Larraitz Uria and Elisabete Pociello. 
This research is supported by the University 
of  the  Basque  Country  (9/UPV00141.226?
14601/2002) and the Ministry of Industry of the 
Basque  Government  (XUXENG  project, 
OD02UN52).
References
Aduriz  I., Aranzabe  M., Arriola  J., D?az  de  Ilarraza 
A., Gojenola  K., Oronoz  M., Uria  L.   2004.
A  Cascaded  Syntactic  Analyser  for  Basque  
Computational  Linguistics  and  Intelligent  Text  
Processing. 2945  LNCS  Series.pg.  124?135. 
Springer Verlag. Berlin (Germany).
Aldezabal I., Aranzabe M., Arrieta B., Maritxalar M., 
Oronoz M. 2003.  Toward a punctuation checker 
for Basque. Atala Workshop on Punctuation. Paris 
(France).
Alegria I., Arregi  O., Ezeiza N., Fernandez I., Urizar 
R. 2004. Design and Development of a Named En?
tity  Recognizer  for  an  Agglutinative  Language. 
First International Joint Conference on NLP (IJC?
NLP?04). Workshop on Named Entity Recognition. 
Ansa O., Arregi X., Arrieta B., Ezeiza N., Fernandez 
I.,  Garmendia  A.,  Gojenola  K.,  Laskurain  B., 
Mart?nez  E.,  Oronoz  M.,  Otegi  A.,  Sarasola  K., 
Uria L. 2004. Integrating NLP Tools for Basque in  
Text Editors. Workshop on International Proofing 
Tools  and Language Technologies.  University  of 
Patras (Greece).
Aranzabe M., Arriola J.M., D?az de Ilarraza A.  2004.
Towards  a  Dependency  Parser  of  Basque.
Proceedings of the Coling 2004 Workshop on Re?
cent Advances in Dependency Grammar. Geneva 
(Switzerland).
Bayraktar M., Say B., Akman V. 1998. An Analysis of  
English Punctuation:  the special  case of  comma. 
International  Journal  of  Corpus  Linguistics 
3(1):pp. 33?57.  John  Benjamins  Publishing  Com?
pany. Amsterdam (The Netherlands).
Beeferman D.,  Berger  A.,  Lafferty  J.  1998.  Cyber?
punc: a lightweight punctuation annotation system 
for speech. Proceedings of the IEEE International 
Conference on Acoustics, Speech and Signal Pro?
cessing, pages 689?692, Seattle (WA).
Brill, E. 1994.  Some Advances in rule?based part of  
speech tagging. In Proceedings of the Twelfth Na?
tional Conference on Artificial Intelligence. Seattle 
(WA). 
Brill,  E.  1995.  Transformation?based  error?driven 
learning and natural language processing: a case 
study  in  part  of  speech  tagging. Computational 
Linguistics 21(4). MIT Press. Cambridge (MA).
Briscoe T., Carroll J. 1995.  Developing and evaluat?
ing a probabilistic lr parser of part?of?speech and  
punctuation  labels.  ACL/SIGPARSE 4th  interna?
tional Workshop on Parsing Technologies, Prague / 
Karlovy Vary (Czech Republic). 
Carreras X., M?rquez L. 2003. Phrase Recognition by 
Filtering and Ranking with Perceptrons. Proceed?
ings of the 4th RANLP Conference. Borovets (Bul?
garia).
D?az de  Ilarraza A., Gojenola K., Oronoz M.   2005.
Design and Development of a System for the De?
tection of Agreement Errors in Basque. CICLing?
2005, Sixth International Conference on Intelligent 
Text  Processing  and  Computational  Linguistics. 
Mexico City (Mexico).
Garzia  J.  1997.  Joskera  Lantegi. Herri  Arduralar?
itzaren Euskal Erakundea. Gasteiz, Basque Country 
(Spain).
7
Hardt D. 2001.  Comma checking in Danish.  Corpus 
linguistics. Lancaster (England). 
Hill R.L., Murray W.S. 1998.  Commas and Spaces: 
the Point of Punctuation. 11th Annual CUNY Con?
ference  on  Human  Sentence  Processing.  New 
Brunswick, New Jersey (USA). 
Jones B. 1996. Towards a Syntactic Account of Punc?
tuation. Proceedings of the 16th International Con?
ference on Computational Linguistics. Copenhagen 
(Denmark). 
Nunberg,  G.  1990.  The  linguistics  of  punctuation. 
Center for the Study of Language and Information. 
Leland Stanford Junior University (USA).
Say B., Akman V. 1996.  Information?Based Aspects 
of  Punctuation.  Proceedings  ACL/SIGPARSE In?
ternational  Meeting  on  Punctuation  in  Computa?
tional  Linguistics,  pages  pp. 49?56,  Santa  Cruz, 
California (USA). 
Tjong Kim Sang E.F. and Buchholz S. 2000.  Intro?
duction to the CoNLL?2000 shared task: chunking. 
In  proceedings  of  CoNLL?2000  and  LLL?2000. 
Lisbon (Portugal).
Tjong Kim Sang E.F. and D?jean H. 2001. Introduc?
tion to the CoNLL?2001 shared task: clause identi?
fication. In proceedings of CoNLL?2001. Tolouse 
(France).
Van Delden  S.,  Gomez  F.  2002.  Combining  Finite 
State Automata and a Greedy Learning Algorithm 
to Determine the Syntactic Roles of Commas. 14th 
IEEE International Conference on Tools with Arti?
ficial Intelligence. Washington, D.C. (USA)
Zubimendi,  J.R. 2004.  Ortotipografia.  Estilo liburu?
aren lehen atala. Eusko Jaurlaritzaren Argitalpen 
Zerbitzu  Nagusia.  Gasteiz,  Basque  Country 
(Spain).
8
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 13?17,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
BAD: An assistant tool for making verses in Basque
Manex Agirrezabal, In?aki Alegria, Bertol Arrieta
University of the Basque Country (UPV/EHU)
maguirrezaba008@ikasle.ehu.es, i.alegria@ehu.es, bertol@ehu.es
Mans Hulden
Ikerbasque (Basque Science Foundation)
mhulden@email.arizona.edu
Abstract
We present work on a verse-composition
assistant for composing, checking correct-
ness of, and singing traditional Basque
bertsoak?impromptu verses on particular
themes. A performing bertsolari?a verse
singer in the Basque Country?must ad-
here to strict rules that dictate the format
and content of the verses sung. To help
the aspiring bertsolari, we provide a tool
that includes a web interface that is able
to analyze, correct, provide suggestions and
synonyms, and tentatively also sing (using
text-to-speech synthesis) verses composed
by the user.
1 Introduction
In the Basque Country there exists a long-
standing live performance tradition of improvis-
ing verses?a type of ex tempore composition and
singing called bertsolaritza. Verses in bertsolar-
itza can be seen as discourses with strict rules
governing the technical structure of them: verses
must contain a certain number of lines and each
line must have a defined number of syllables, cer-
tain lines have to rhyme in certain patterns, and so
forth.
In this paper we present a web-based assistant
tool for constructing verses (bertsoak) according
to the rules of bertsolaritza (Garzia et al 2001).
If the reader is interested in this topic, we rec-
ommend watching the 2011 film Bertsolari1 2, di-
rected by Asier Altuna.
1IMDB: http://www.imdb.com/title/tt2058583
2Trailer on: http://vimeo.com/9355066
2 Relationship to earlier work
There exist some prior works dealing with Basque
verse-making and computer technologies, such as
BertsolariXa (Arrieta et al, 2001), which is a
rhyme search tool implemented as finite-state au-
tomata using the two-level morphology formal-
ism. The tool also contains other features, includ-
ing semantic categorization of words, narrowing
word-searches to certain themes, etc. While Bert-
solariXa focuses mostly on the word-level, the
current work also includes constraints on over-
all verse structure in its implementation as well
as a synonym search tool, a melody suggestion
system, and possibilities for plugging in text-to-
speech synthesis of verses.
2.1 The Bertsolari tradition
Bertsolaritza is very ingrained in the Basque
Country and championships, competitions and
get-togethers on bertsolaritza are quite common.
Usually the competitors in such event, called bert-
solaris, are given a theme to produce a verse on
under some very limited time constraints.
But the Basque Country is not the only place
that hosts such troubadour traditions?similar
customs are present in many other countries such
as Cuba, Brazil, Argentina, etc. The goal of the
current tool is to be generalizable, and so appli-
cable to various strategies of verse improvisation,
and possibly be useful not only for Basque speak-
ers, but also for others.
Below we briefly present an example of a verse
made in the Basque Country. In 1986 Andoni
Egan?a (a well-known bertsolari) was asked to
sing a bertso and assigned a topic. In the verse,
he was asked to play the role of an old person
who lived alone, and who realized that he could
13
not even tie his shoes. Within a few seconds he
composed and sang three verses. Here, we ana-
lyze the first verse.
Verse:
Gazte aroan ibili arren
gustora tirriki-tarra,
denbora honen joan etorriak
ederki jo dit gitarra,
gorputza daukat ximeldurikan
ta eskuen punta zaharra,
denborarekin seko galdu det
gazte aroko indarra,
ez al da pena gizon mardul bat
hola ibili beharra.
Translation:
Even when I was young
I was always on a spree
over time
I have been punished
I have a crumpled body
and the tip of the hands very old,
Over time I lost
the strength I had when I was young,
It?s a shame that a strong man
has to end up like me.
The special charm of bertsolaritza improvi-
sation is that people proficient in the art can
quickly express a variety of ideas, although they
are working with very restrictive rules concern-
ing the number of syllables in words they use,
and how the words must rhyme. We must take
into account that Andoni Egan?a was able to sing
this verse within a few seconds of being given
the topic, and also, that it complies exactly with
a certain metric. In this case, the verse contains
eight lines, each odd line consisting of ten sylla-
bles, and each even line of eight syllables, with
the even lines rhyming.
Formal training in the bertsolari tradition also
exists in the Basque Country. In the last 20 to
30 years, an important movement has developed
that aims to provide instruction to upcoming gen-
erations on how to create verses (orally or in
writing). This kind of instruction usually takes
place in learning centers called bertso-eskolak,
which in English roughly means, ?verse-making
schools.? The proliferation of this movement has
produced a strong base of young bertsolaris, of
whom many achieve an outstanding level of im-
provisation skills.
3 The BAD tool
BAD is the acronym for ?Bertsotarako Arbel Dig-
itala?, roughly ?Digital verse board.? The aim
of the tool is to serve as a general assistant for
bertsolari-style verse composition and help verse-
making learners in their learning process.
This tool has been developed using the PHP
programming language, but it contains certain
parts developed using finite-state technology. The
main functions of this tool, which will be dis-
cussed in more detail in the next five sections, are
the following: visualization of the verse structure,
structure checking, rhyme and synonym searching
and verse singing.
3.1 Verse structure
The main rules of the bertsolari verse are that a
verse must consist of a certain predefined number
of lines and each line in turn, of a predefined num-
ber of syllables. Traditionally, about a hundred
different schemes are used, and the tool provides
support for all these patterns. For example, the
structure called ?Hamarreko handia? has ten lines
and ten syllables in the odd-numbered lines, and
eight syllables in the even-numbered lines. In this
structure, the even-numbered lines have to rhyme.
Selecting this scheme, the tool will mark the cor-
responding lines with their requirements.
The web interface can be seen in figure 1,
which shows the general layout of the tool, illus-
trated with the example verse referred to above?
we see that each line has been approved in terms
of line length and syllable structure by the tool.
We have designed a database in which the main
verse structures are saved so that when the user se-
lects one verse schema, the system knows exactly
the number of lines it must contain, where must
it rhyme and how many syllables each line should
have. Those schemata are also linked to melodies,
each melody corresponding to one possible struc-
ture.
3.2 Structure checking
After writing the verse, the system can evaluate if
it is technically correct, i.e. if the overall structure
is correct and if each line in the form abides by the
required syllable count and rhyming scheme. The
syllable counter is implemented using the foma
software (Hulden, 2009), and the implementation
(Hulden, 2006) can be found on the homepage of
14
Figure 1: A verse written in the BAD web application.
foma.3
Separately, we have also developed a rhyme
checker, which extracts special patterns in the
lines that must rhyme and checks their confor-
mity.
These patterns are extracted using foma (see
section 3.4) after which some phonological rules
are applied. For example, an example rule era
? {era, eda, ega, eba}, models the fact that any
word ending in era, for example, etxera, will
rhyme with all words that end in era, eda, eba or
ega. These rhyming patterns have been extracted
according to the phonological laws described in
(Amuriza, 1981).
3.3 Synonym search
Usually, people who write verses tend to quickly
exhaust their vocabulary and ideas with to ex-
press what they want to say, or encounter prob-
lems with the number of syllables in various ten-
tative words they have in mind. For example,
if the verse-maker wants to say something con-
taining the word ?family,? (familia in Euskera, a
four-syllable word) but is forced to use a three-
syllable word in a particular context, the inter-
face provides for possibilities to look for three-
syllable synonyms of the word familia, producing
the word sendia? a word whose meaning is oth-
erwise the same, and made up of three syllables.
For developing the synonym search, we used a
modified version of the Basque Wordnet (Pociello
3http://foma.googlecode.com
et al, 2010), originally developed by the IXA
group at the University of the Basque Country.
Within Wordnet we search the synsets for the in-
coming word, and the words that correspond to
those synsets are returned.
3.4 Rhyme search
The classical and most well-known problem in
bertsolaritza concern the rhyming patterns. As
mentioned, various lines within a verse are re-
quired to rhyme, according to certain predefined
schemata. To search for words that rhyme with
other words in a verse, the BAD tool contains a
rhyme search engine. In the interface, this is lo-
cated in the right part of the BAD tool main view,
as seen in figure 2.
The rhyme searcher is built upon finite-state
technology, commonly used for developing mor-
phological and phonological analyzers, and calls
upon the freely available foma-tool, to calculate
matching and nonmatching rhyme schemes.
Its grammar is made up of regular expressions
that are used to identify phonological patterns in
final syllables in the input word. The result of
the search is the intersection of these patterns and
all the words generated from a morphological de-
scription of Basque (Alegria et al, 1996)?that
is, a list of all words that match both the required
phonological constraints given (rhyming) and a
morphological description of Basque.
Based upon figure 2, if we search rhymes for
the word landa (cottage), the system proposes a
15
Figure 2: The response of the rhyme search engine.
set of words that can be filtered depending on the
number of syllables required. Among this list of
words, we can find some words that end in anda,
such as, Irlanda (Ireland) or eztanda (explosion),
but through the application of phonological equiv-
alency rules we also find terms like ganga (vault).
3.5 Singing synthesis
Another characteristic, as mentioned, is that, in
the end, the verses are intended to be sung in-
stead of only being textually represented. Based
on other ongoing work in singing synthesis, we
have designed a system for singing the verses en-
tered into the system in Basque.
This is based on the ?singing mode? of the Fes-
tival text-to-speech system (Taylor et al, 1998).
The advantage of using this is that Festival is
open-source and has given us ample opportunities
to modify its behavior. However, as Festival does
not currently support Basque directly, we have re-
lied on the Spanish support of the Festival sys-
tem.4
4While morphologically and syntactically, Spanish and
Basque have no relationship whatsoever, phonetically the
languages are quite close, with only a few phonemes, syl-
Based on current work by the Aholab research
team in Bilbao?a lab that works on Basque
speech synthesis and recognition?we have im-
plemented a singing module for BAD, based on
the text-to-speech HTS engine (Erro et al, 2010).
Our application is able to sing the composed
verses entered into the system in Basque, with a
choice of various standard melodies for bertsolar-
itza.5
4 Discussion and future work
Now that the BAD tool has been developed, our
intention is to evaluate it. To make a qualita-
tive evaluation we have gotten in touch with some
verse-making schools (bertso-eskola), so that they
can test the system and send us their feedback us-
ing a form. Once the evaluation is made, we will
improve it according to the feedback and the sys-
tem will be made public.
Our ultimate goal is to develop a system able to
create verses automatically. To achieve this long-
term goal, there is plenty of work to do and ba-
sic research to be done. We have in our hands a
good corpus of 3,500 Basque verse transcriptions,
so we intend to study these verses from a mor-
phological, syntactical, semantical and pragmatic
point of view.
In the short term, we also plan to expand the
synonym search to be able to provide searches
for semantically related words and subjects (and
not just synonyms), like hypernyms or hyponyms.
The Basque WordNet provides a good opportu-
nity for this, as one is easily able to traverse the
WordNet to encounter words with varying degrees
of semantic similarity.
Another feature that we want to develop is a
system that receives as input a verse together with
a MIDI file, and where the system automatically
sings the verse to the music provided.
Finally, in order for the system to be
able to provide better proposals for the verse
artist?including perhaps humorous and creative
proposals?we intend to work with approaches
to computational creativity. We are considering
different approaches to this topic, such as in the
work on Hahacronym (Stock et al, 2005) or the
Standup riddle builder (Ritchie et al, 2001).
labification rules, and stress rules being different enough to
disturb the system?s behavior.
5However, this functionality is not available on the web
interface as of yet.
16
Figure 3: The BAD application before entering a verse, showing two possible rhyme patterns.
Acknowledgments
This research has been partially funded by the
Basque Government (Research Groups, IT344-
10).
References
In?aki Alegria, Xabier Artola, Kepa Sarasola and
Miriam Urkia, ?Automatic morphological analysis
of Basque?, Literary and Linguistic Computing,
ALLC, 1996
Xabier Amuriza, ?Hiztegi errimatua?, Alfabetatze Eu-
skalduntze Koordinakundea, 1981
Bertol Arrieta, In?aki Alegria, Xabier Arregi, ?An as-
sistant tool for Verse-Making in Basque based on
Two-Level Morphology?, 2001
Daniel Erro, In?aki Sainz, Ibon Saratxaga, Eva Navas,
Inma Herna?ez, ?MFCC+F0 Extraction and Wave-
form Reconstruction using HNM: Preliminary Re-
sults in an HMM-based Synthesizer?, 2010
Joxerra Garzia, Jon Sarasua, and Andoni Egan?a,
?The art of bertsolaritza: improvised Basque verse
singing?, Bertsolari liburuak, 2001
John E. Hopcroft, Rajeev Motwani, Jeffrey D. Ullman,
?Introduccio?n a la teor??a de Auto?matas, Lenguajes
y Computacio?n?, Pearson educacio?n, 2002
Mans Hulden, ?Foma: a finite-state compiler and li-
brary?, Proceedings of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: Demonstrations Session, p. 29?
32, 2009
Mans Hulden, ?Finite-state syllabification?, Finite-
State Methods and Natural Language Processing, p.
86 ? 96, Springer, 2006
Kimmo Koskenniemi. ?Two-level morphology: A
general computational model for word-form recog-
nition and production?. Publication 11, University
of Helsinki, Department of General Linguistics,
Helsinki, 1983.
Eli Pociello, Eneko Agirre, Izaskun Aldezabal,
?Methodology and construction of the Basque
WordNet?, 2010
Graeme Ritchie, ?Current directions in computational
humour?, Artificial Intelligence Review, Volume
16, Number 2, p. 119 ? 135, Springer, 2001
Graeme Ritchie, Ruli Manurung, Helen Pain, Annalu
Waller, Dave O?Mara, ?The STANDUP interactive
riddle builder?, Volume 2, Number 2, p. 67 ? 69,
IEEE Intelligent Systems, 2006
Oliviero Stock and Carlo Strapparava, ?Hahacronym:
A computational humor system?, Proceedings of
the ACL 2005 on Interactive poster and demonstra-
tion sessions, p. 113 ? 116, Association for Compu-
tational Linguistics, 2005
Paul Taylor, Alan W. Black and Richard Caley, ?The
architecture of the Festival speech synthesis sys-
tem?, International Speech Communication Asso-
ciation, 1998
17
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 35?39,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Finite-state technology in a verse-making tool
Manex Agirrezabal, In?aki Alegria, Bertol Arrieta
University of the Basque Country (UPV/EHU)
maguirrezaba008@ikasle.ehu.es, i.alegria@ehu.es, bertol@ehu.es
Mans Hulden
Ikerbasque (Basque Science Foundation)
mhulden@email.arizona.edu
Abstract
This paper presents a set of tools designed to
assist traditional Basque verse writers during
the composition process. In this article we
are going to focus on the parts that have been
created using finite-state technology: this in-
cludes tools such as syllable counters, rhyme
checkers and a rhyme search utility.
1 The BAD tool and the Basque singing
tradition
The BAD tool is an assistant tool for verse-makers
in the Basque bertsolari tradition. This is a form
of improvised verse composition and singing where
participants are asked to produce impromptu com-
positions around themes which are given to them
following one of many alternative verse formats.
The variety of verse schemata that exist all impose
fairly strict structural requirements on the composer.
Verses in the bertsolari tradition must consist of a
specified number of lines, each with a fixed num-
ber of syllables. Also, strict rhyme patterns must
be followed. The structural requirements are con-
sidered the most difficult element in the bertsolar-
itza?however, well-trained bertsolaris can usually
produce verses that fulfill the structural prerequisites
in a very limited time.
The BAD tool presented here is mainly di-
rected at those with less experience in the tradi-
tion such as students. One particular target group
are the bertso-eskola-s (verse-making schools) that
have been growing in popularity?these are schools
found throughout the Basque Country that train
young people in the art of bertsolaritza.
The primary functionality of the tool is illustrated
in figure 1 which shows the main view of the util-
ity. The user is offered a form in which a verse
can be written, after which the system checks the
technical correctness of the poem. To perform this
task, several finite state transducer-based modules,
are used, some of them involving the metrics (syl-
lable counter) of the verse, and others the rhyme
(rhyme searcher and checker). The tool has support
for 150 well known verse meters.
In the following sections, we will outline the tech-
nology used in each of the parts in the system.
2 Related work
Much of the existing technology for Basque mor-
phology and phonology uses finite-state technology,
including earlier work on rhyme patterns (Arrieta
et al, 2001). In our work, we have used the Basque
morphological description (Alegria et al, 1996) in
the rhyme search module. Arrieta et al (2001) de-
velop a system where, among other things, users can
search for words that rhyme with an introduced pat-
tern. It is implemented in the formalism of two-level
morphology (Koskenniemi, 1983) and compiled into
finite-state transducers.
We have used the open-source foma finite-state
compiler to develop all the finite-state based parts
of our tool.1. After compiling the transducers, we
use them in our own application through the C/C++
API provided with foma.
3 Syllable counter
As mentioned, each line in a verse must contain a
specified number of syllables. The syllable counter
module that checks whether this is the case consists
of a submodule that performs the syllabification it-
self as well as a module that yields variants produced
by optional apocope and syncope effects. For the
syllabification itself, we use the approach described
in Hulden (2006), with some modifications to cap-
ture Basque phonology.
1In our examples, FST expressions are written using foma
syntax. For details, visit http://foma.googlecode.com
35
Figure 1: A verse written in the BAD web application.
3.1 Syllabification
Basque syllables can be modeled by assuming a
maximum onset principle together with a sonority
hierarchy where obstruents are the least sonorous el-
ement, followed in sonority by the liquids, the nasals
and the glides. The syllable nuclei are always a sin-
gle vowel (a,e,i,o,u) or a combination of a low vowel
(a,e) and a high vowel (i,o,u) or a high vowel and an-
other high vowel.
The syllabifier relies on a chain of composed re-
placement rules (Beesley and Karttunen, 2003) com-
piled into finite-state transducers. These defini-
tions are shown in figure 2. The overall strategy
is to first mark off the nuclei in a word by the rule
MarkNuclei which takes advantage of a left-to-
right longest replacement rule. This is to ensure that
diphthongs do not get split into separate syllables
by the subsequent syllabification process. Follow-
ing this, syllables are marked off by the markSyll-
rule, which inserts periods after legitimate syllables.
This rule takes advantage of the shortest-leftmost re-
placement strategy?in effect minimizing the coda
and maximizing the size of the onset of a syllable to
the extent permitted by the allowed onsets and co-
das, defined in Onset and Coda, respectively.
To illustrate this process, supposing that we
are syllabifying the Basque word intransitiboa.
The first step in the syllabification process is
to mark the nuclei in the word, resulting in
{i}ntr{a}ns{i}t{i}b{o}{a}. In the more com-
plex syllabification step, the markSyll rule as-
sures that the juncture ntr gets divided as n.tr be-
cause nt.r would produce a non-maximal onset,
and i.ntr would in turn produce an illegal onset in
define Obs [f|h|j|k|p|s|t|t s|t z|t x|x|
z|b|d|g|v|d d|t t];
define LiqNasGli [l|r|r r|y|n|m];
define LowV [a|e|o];
define HighV [i|u];
define V LowV | HighV;
define Nucleus [V | LowV HighV |
[HighV HighV - [i i] - [u u]]];
define Onset (Obs) (LiqNasGli);
define Coda C?<4;
define MarkNuclei Nucleus @-> %{ ... %};
define Syll Onset %{ Nucleus %} Coda;
define markSyll Syll @> ... "." || _ Syll ;
define cleanUp %{|%} -> 0;
regex MarkNuclei .o. markSyll .o. cleanUp;
Figure 2: Syllable definition
the second syllable. The final syllabification, af-
ter markup removal by the Cleanup rule, is then
in.tran.si.ti.bo.a. This process is illustrated in fig-
ure 3
In bertsolaritza, Basque verse-makers follow this
type of syllable counting in the majority if cases;
however, there is some flexibility as regards the syl-
labification process. For example, suppose that the
phrase ta lehenengo urtian needs to fit a line which
must contain six syllables. If we count the sylla-
bles using the algorithm shown above, we receive a
count of eight (ta le.hen.en.go ur.ti.an). However,
in the word lehenengo we can identify the syncope
pattern vowel-h-vowel, with the two vowels being
identical. In such cases, we may simply replace
the entire sequence by a single vowel (ehe ? e).
This is phonetically equivalent to shortening the ehe-
sequence (for those dialects where the orthographi-
cal h is silent). With this modification, we can fit
36
the line in a 7 syllable structure. We can, however,
further reduce the line to 6 syllables by a second
type of process that merges the last syllable of one
word with the first of the next one and then resyl-
labifying. Hence, ta lehenengo urtian, using the
modifications explained above, could be reduced to
ta.le.nen.gour.ti.an, which would fit the 6 syllable
structure. This production of syllabification variants
is shown in figure 4.
transformazioei
tr{a}nsf{o}rm{a}z{i}{o}{ei}
markNuclei
syllabify
tr{a}ns.f{o}r.m{a}.z{i}.{o}.{ei}
cleanUp
trans.for.ma.zi.o.ei
Figure 3: Normal syllabification.
trarnsfomn
trzarnzsfzomn
marrkNuculkeuis
kreybskeym
trzarnzsfzomn trnzsfzomn
tratnsftronnm
tzratznsftzroznnm
marrkNuculkeuis
kreybskeym
tzratznsftzroznnm tzratznstzroznnm
Figure 4: Flexible syllabification.
4 Finite-state technology for rhymes
4.1 Basque rhyme patterns and rules
Similar to the flexibility in syllabification, Basque
rhyme schemes also allows for a certain amount
of leeway that bertsolaris can take advantage of.
The widely consulted rhyming dictionary Hiztegi
Errimatua (Amuriza, 1981) contains documented a
number of phonological alternations that are accept-
able as off-rhymes: for example the stops p, t, and k
are often interchangeable, as are some other phono-
logical groups. Figure 5 illustrates the definitions
for interchangeable phonemes when rhyming. The
interchangeability is done as a prelude to rhyme
checking, whereby phonemes in certain groups,
such as p, are replaced by an abstract symbol de-
noting the group (e.g. PTK).
4.2 Rhyme checker
The rhyme checker itself in BAD was originally de-
veloped as a php-script, and then reimplemented as
define plosvl [p | t | k];
define rplosv [b | d | g | r];
define sib [s | z | x];
define nas [n | m];
define plosvlconv ptk -> PTK;
define rplosvconv bdgr -> BDGR;
define sibconv sib -> SZX;
define nasconv nas -> NM;
define phoRules plosvlconv .o. rplosvconv .o.
sibconv .o. nasconv ;
Figure 5: Conflation of consonant groups before rhyme
checking.
a purely finite-state system. In this section we will
focus on the finite-state based one.
As the php version takes advantage of syllabifica-
tion, the one developed with transducers does not.
Instead, it relies on a series of replacement rules and
the special eq() operator available in foma. An
implementation of this is given in figure 6. As input
to the system, the two words to be checked are as-
sumed to be provided one after the other, joined by
a hyphen. Then, the system (by rule rhympat1)
identifies the segments that do not participate in the
rhyme and marks them off with ?{? and ?}? symbols
(e.g. landa-ganga ? <{l}anda>-<{g}anga>).
The third rule (rhympat3) removes everything
that is between ?{? and ?}?, leaving us only with
the segments relevant for the rhyming pattern (e.g.
<anda>-<anga>). Subsequent to this rule, we
apply the phonological grouping reductions men-
tioned above in section 4.1, producing, for example
(<aNMBDGRa>-<aNMBDGRa>).
After this reduction, we use the eq(X,L,R)-
operator in foma, which from a transducer X, filters
out those words in the output where material be-
tween the specified delimiter symbols L and R are
unequal. In our case, we use the < and > symbols
as delimiters, yielding a final transducer that does
not accept non-rhyming words.
4.3 Rhyme search
The BAD tool also includes a component for search-
ing words that rhyme with a given word. It is devel-
oped in php and uses a finite-state component like-
wise developed with foma.
Similarly to the techniques previously described,
it relies on extracting the segments relevant to the
37
define rhympat1 [0:"{" ?* 0:"}"
[[[V+ C+] (V) V] | [(C) V V]] C* ];
# constraining V V C pattern
define rhympat2 ?[?* V "}" V C];
# cleaning non-rhyme part
define rhympat3 "{" ?* "}" -> 0;
define rhympat rhympat1 .o. rhympat2 .o.
rhympat3;
# rhyming pattern on each word
# and phonological changes
define MarkPattern rhympat .o.
phoRules .o. patroiak;
# verifying if elements between < and >
# are equal
define MarkTwoPatterns
0:%< MarkPattern 0:%> %-
0:%< MarkPattern 0:%> ;
define Verify _eq(MarkTwoPatterns, %<, %>)
regex Verify .o. Clean;
Figure 6: Rhyme checking using foma.
rhyme, after which phonological rules are applied
(as in 4.1) to yield phonetically related forms. For
example, introducing the pattern era, the system re-
turns four phonetically similar forms era, eda, ega,
and eba. Then, these responses are fed to a trans-
ducer that returns a list of words with the same end-
ings. To this end, we take advantage of a finite-state
morphological description of Basque (Alegria et al,
1996).
As this transducer returns a set of words which
may be very comprehensive?including words not
commonly used, or very long compounds?we then
apply a frequency-based filter to reduce the set of
possible rhymes. To construct the filter, we used
a newspaper corpus, (Egunkaria2) and extracted the
frequencies of each word form. Using the frequency
counts, we defined a transducer that returns a word?s
frequency, using which we can extract only the n-
most frequent candidates for rhymes. The system
also offers the possibility to limit the number of syl-
lables that desired rhyming words may contain. The
syllable filtering system and the frequency limiting
parts have been developed in php. Figure 7 shows
the principle of the rhyme search?s finite-state com-
ponent.
5 Evaluation
As we had available to us a rhyme checker written
in php before implementing the finite-state version,
2http://berria.info
regex phoRules .o. phoRules.i .o.
0:?* ?* .o. dictionary ;
Figure 7: Rhyme search using foma
it allowed for a comparison of the application speed
of each. We ran an experiment introducing 250,000
pairs of words to the two rhyme checkers and mea-
sured the time each system needed to reply. The
FST-based checker was roughly 25 times faster than
the one developed in php.
It is also important to mention that these tools
are going to be evaluated in an academic environ-
ment. As that evaluation has not been done yet, we
made another evaluation in our NLP group in or-
der to detect errors in terms of syllabification and
rhyme quality. The general feeling of the experiment
was that the BAD tool works well, but we had some
efficiency problems when many people worked to-
gether. To face this problem some tools are being
implemented as a server.
6 Discussion & Future work
Once the main tools of the BAD have been devel-
oped, we intend to focus on two different lines of
development. The first one is to extend to flexibil-
ity of rhyme checking. There are as of yet patterns
which are acceptable as rhymes to bertsolaris that
the system does not yet recognize. For example,
the words filma and errima will not be accepted by
the current system, as the two rhymes ilma and ima
are deemed to be incompatible. In reality, these two
words are acceptable as rhymes by bertsolaris, as
the l is not very phonetically prominent. However,
adding flexibility also involves controlling for over-
generation in rhymes. Other reduction patterns not
currently covered by the system include phenomena
such as synaloepha?omission of vowels at word
boundaries when one word ends and the next one
begins with a vowel.
Also, we intend to include a catalogue of melodies
in the system. These are traditional melodies that
usually go along with a specific meter. Some 3,000
melodies are catalogued (Dorronsoro, 1995). We are
also using the components described in this article in
another project whose aim is to construct a robot ca-
pable to find, generate and sing verses automatically.
38
Acknowledgments
This research has been partially funded by the Span-
ish Ministry of Education and Science (OpenMT-
2, TIN2009-14675-C03) and partially funded by the
Basque Government (Research Groups, IT344-10).
We would like to acknowledge Aitzol Astigarraga
for his help in the development of this project. He
has been instrumental in our work, and we intend to
continue working with him. Also we must mention
the Association of Friends of Bertsolaritza, whose
verse corpora has been used to test and develop these
tools and to develop new ones.
References
Alegria, I., Artola, X., Sarasola, K., and Urkia,
M. (1996). Automatic morphological analysis
of Basque. Literary and Linguistic Computing,
11(4):193?203.
Amuriza, X. (1981). Hiztegi errimatua [Rhyme Dic-
tionary]. Alfabetatze Euskalduntze Koordinakun-
dea.
Arrieta, B., Alegria, I., and Arregi, X. (2001). An
assistant tool for verse-making in Basque based
on two-level morphology. Literary and linguistic
computing, 16(1):29?43.
Beesley, K. R. and Karttunen, L. (2003). Finite state
morphology. CSLI.
Dorronsoro, J. (1995). Bertso doinutegia [Verse
melodies repository]. Euskal Herriko Bertsolari
Elkartea.
Hulden, M. (2006). Finite-state syllabification.
Finite-State Methods and Natural Language Pro-
cessing, pages 86?96.
Koskenniemi, K. (1983). Two-level morphology:
A general computational model for word-form
production and generation. Publications of the
Department of General Linguistics, University of
Helsinki. Helsinki: University of Helsinki.
39
Proceedings of the 14th European Workshop on Natural Language Generation, pages 162?166,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
POS-tag based poetry generation with WordNet
Manex Agirrezabal, Bertol Arrieta, Aitzol Astigarraga
University of the Basque Country (UPV/EHU)
IXA NLP Group
Dept. of Computer Science
20018 Donostia
sgpagzam@ehu.es
bertol@ehu.es
aitzol.astigarraga@ehu.es
Mans Hulden
University of Helsinki
Department of Modern Languages
Helsinki, Finland
mhulden@email.arizona.edu
Abstract
In this paper we present the preliminary work of
a Basque poetry generation system. Basically,
we have extracted the POS-tag sequences from
some verse corpora and calculated the probabil-
ity of each sequence. For the generation process
we have defined 3 different experiments: Based
on a strophe from the corpora, we (a) replace
each word with other according to its POS-tag
and suffixes, (b) replace each noun and adjective
with another equally inflected word and (c) re-
place only nouns with semantically related ones
(inflected). Finally we evaluate those strategies
using a Turing Test-like evaluation.
1 Introduction
Poetry generation is one of the dream tasks of Natural
Language Processing (NLP). In this text we point out
an approach to generate Basque strophes automatically
using some corpora, morphological information and
a lexical database. The presented method is not tied
to a specific language, but it is especially suitable for
inflected languages, as the POS information used in
some tasks with success in non inflected languages is
not enough for inflected ones. We have used the POS-
tags with their inflectional information to learn usual
structures in Basque poetry.
This work is part of a more general and complete
project, called BertsoBOT (Astigarraga et al, 2013).
BertsoBOT is a robot capable of creating and singing
Basque verses automatically. The robot joins together
in a single system techniques from robotics, NLP and
speech synthesis and recognition. The work presented
in this paper comes to improve the generation module
of the mentioned system.
Although our intention is to create whole verses, in
this paper we present the first steps towards it: the cre-
ation of strophes. Additionally, Basque verses have
to rhyme, but in these first experiments we have not
considered it.
Basque language
Basque language is spoken along the Basque Country1
by approximately 700.000 people. Although there is a
standardized form of the language, it is common the use
of non-standard dialects in certain regions, mainly in
spoken language.
Basque is a morphologically rich language, which is
an obvious feature if we analyze the multiple declension
cases2 that can be usedwith only oneword. For example,
the phrase ?with the friends? can be expressed with only
one word, ?lagunekin?.
lagunekin = lagun (friend) + ak (plural determiner) +
kin (with)
Art of bertsolaritza
The art of impromptu verse-making, bertsolaritza, is
very ingrained in the BasqueCountry. The performances
of verse-makers are quite usual and a big championship
is held every four years which congregates 15.000 peo-
ple, approximately. One tipical work to do for the verse-
makers is to sing verses extempore, given a topic. The
particularity of these verses is that they have to follow
strict constraints of meter and rhyme. In the case of
a metric structure of verses known as ?zortziko txikia?
1http://en.wikipedia.org/wiki/Basque Country (greater region)
2en.wikipedia.org/wiki/Basque grammar#Declension
162
(small of eight), the poem must have eight lines. The
union of each odd line with the next even line, form a
strophe. Each strophe, has a small structure3 and must
rhyme with the others. Below, you can see an example
of a verse, with lauko txikia4 stanza:
Neurriz eta errimaz With meter and rhyme
kantatzea hitza, to sing the word
horra hor ze kirol mota bertsolaritza is
den bertsolaritza. that kind of sport
2 State of the art
A good review of computer guided poetry can be found
in (Gerva?s, 2010). Most relevant ones include:
WASP
The WASP system (Gerva?s, 2000) can be considered
one of first serious attempts to build an automatic poetry
generator system. It is based on the generate-and-test
paradigm of problem solving. Simple solutions are
generated and then coupled with an evaluation function
for metric constraints, producing acceptable results.
ASPERA
ASPERA (Gerva?s, 2001) is a case-based reasoning
(CBR) system for poetry generation. It generates poetry
based on the information provided by the user: a prose
description of the intended message, a specific stanza
for the final poem, a set of verse examples on that stanza,
and a group of words that the final poem must contain.
The system was implemented using CLIPS rule-
based system, and follows the four typical CBR steps:
Retrieval, Reuse, Revise and Retain.
POEVOLVE
Levy (Levy, 2001) went on to develop an evolution-
ary model of poetry generation. POEVOLVE creates
limericks taking as a reference the human way of poetry
writing. The POEVOLVE system works as follows:
an initial population is created from a group of words
that include phonetic and stress information. Rhymes
that meet the requirements are selected and then more
words are selected to fill the rest of the verse-line
based on their stress information. A genetic algorithm
is employed to modify the words that compose the
313 syllables with a caesura after the 7th syllable
4Lauko txikia: The same as zortziko txikia but with four lines,
instead of eight.
limerick. Evaluation is performed by a neural network
trained on human judgements. It must be said that this
system does not take syntax and semantics into account.
McGonnagall
Manurung presented also an evolutionary approach
to generate poetry (Manurung, 2003). The poem gen-
eration process is formulated as a state space search
problem using stochastic hill-climbing. The overall pro-
cess is divided in two steps: evaluation and evolution.
During the evaluation phase, a group of individuals is
formed based on initial information, target semantics
and target phonetics. This group of initial individuals
is then evaluated taking into account different aspects
such as phonetics, semantics and surface form. Each
individual receives a score, and in the evolution step, the
subset with higher scores is selected for reproduction.
The resulting mutated individuals derive, hopefully, in
better versions of the poem.
3 Creating strophes
Our goal is to create Basque strophes automatically. But
strophes written by combining words randomly usually
do not have any sense. For words have any meaning
when combined together, they must be organized fol-
lowing particular patterns. Towards this end we have
applied and tested different methodologies. We use a
morphological analyzer to extract POS and inflection
patterns in strophes, and to create new ones following
those schemes. The idea is to find the most commonly
used patterns so that we can use them in new strophes.
We also improve the results taking semantics into ac-
count. In the next lines we are going to describe some
resources we have used.
3.1 Corpora
For the learning process of the usual POS-tag patterns
we have employed some Basque verse corpora yielded
by theAssociation of the Friends of Bertsolaritza5 (AFB).
Those are impromptu verses sung by Basque verse-
makers and the transcriptions of this collection have
been done by members of the information center6 of the
AFB.
For this work, we are going to exploit three corpora,
5http://www.bertsozale.com/en
6http://bdb.bertsozale.com/en/orriak/get/7-xenpelar-
dokumentazio-zentroa
163
each one following a classic stanza in Basque verses: (a)
small stanza, (b) big stanza and (c) habanera.
a) Small stanza
This corpus has approximately 10.000 lines. Each
line of this corpus is composed by a strophe containing
13 syllables with a caesura between the 7th and the 8th
syllable. This stanza is used to sing sprightly verses
composed by compact ideas.
b) Big stanza
In this case, this corpus has about 8.000 lines and
each line has 18 syllables with a caesura after the 10th
syllable. Depending on the chosen melody, this stanza
can also have a complementary pause in the 5th syllable.
The topics of this type of verses tend to be more epic or
dramatic.
c) Habanera
This corpus has just about 1000 lines and they are
composed by 16-syllable lines with a caesura after the
8th syllable. It is commonly used when the verse-maker
has to compose a verse alone about a topic.
3.2 POS sequence extraction
To extract the POS-tags, we use a Basque analyzer de-
veloped by members of IXA NLP group (Aduriz et al,
2004), which involve phrasal morphologic analysis and
disambiguation, among other matters.
Once calculated the POS-tags, we estimated the most
probable POS sequences using POS-tag ngrams. We did
this in order to know which POS-tag sequence would
better fit for each stanza. For example, an acceptable
POS-tag sequence in the small stanza corpus would be
?NN-NN-JJ-VB?. This pattern could be extracted from
this strophe, which is correct.
Mirenekin+NN zakurra+NN zoriontsua+JJ da+VB.
(With Miren)+NN (the dog)+NN is+VB happy+JJ.
But to have the POS-tag pattern is not enough for a
good generation.
Special issues in the categorization of words in
Basque
The gist is that Basque is an agglutinative language,
so there is plenty information included in the suffixes
of the words. Because of that, if we don?t retain any
information about suffixes, we would lose some impor-
tant data. In Basque, we can apply declension to nouns,
pronouns, adjectives and determiners. Therefore, we
need to save the declension case information to do a
correct generation. When a set of words compound a
noun phrase, only one of the words will be inflected.
Some verbs, when they are part of a subourdinate
clause, can also be inflected. In these cases, we have to
extract the suffixes of the verb of that clause, because it
expresses the type of clause.
All this information is essential if we do not want to
lose the meaning of the clause. Below, you can see an
example of generation of strophes in Basque using only
POS-tags:
Mirenekin+NN lagunekin+NN zoriontsua+JJ da+VB.
(With Miren)+NN (with the friends)+NN is+VB happy+JJ.
As you can see, the phrase ?with Miren with the
friends is happy? is not grammatically correct. Storing
the declension information, that creation would not be
allowed and one of the clauses created by the system
could be:
Mirenekin+NN COM mahaia+NN ABS zoriontsua+JJ ABS da+VB.
(With Miren)+NN COM (the desk)+NN ABS is+VB happy+JJ ABS.
The addition of the declension information will avoid
some grammatical errors in the generation process. But
when the changed element is a verb, the system can
insert one that does not follow the same subcategoriza-
tion7, which will lead us to a grammatical error too.
So, changing the verb without more information can be
uncertain.
3.3 Semantic information
On the other hand, if we take a look at the last example,
it is not correct to say that the desk is happy. To avoid
these cases, we posed the use of the Basque WordNet
(Fellbaum, 2010) (Pociello et al, 2011). We used it to
change words with related ones.
3.4 Morphological generation
Finally, it is important the fact that Basque is an inflected
language. So, we need to have a morphological gener-
ator (Alegria et al, 2010) to create the corresponding
inflected forms of the words. This generator is based
on the Basque morphology description (Alegria et al,
1996).
4 Experiments
In this work, we have performed a set of experiments
to analyze different strategies for the generation of stro-
7The subcategorization indicates the syntactic arguments re-
quired or allowed in some lexical items (usually verbs).
164
phes in Basque. In the following lines, we explain the
ameliorations we get in each experiment.
The first experiment creates strophes by inserting
words that are consistent with each POS-tag and its
inflection information. We first get some of the most
common POS-tag sequences and for each POS-tag se-
quence the application returns two strophes. The first
strophe uses words from the same verse corpus to make
substitutions. The second one uses words from the
EPEC corpus (Aduriz et al, 2006).
The second experiment creates clauses, but chang-
ing only the nouns and adjectives from original strophes
from the corpus. We mantain the inflection information.
In this experiment we also get two strophes for each pat-
tern sequence, as in the previous attempt (verse corpus
and EPEC corpus). With this constraint we avoid the
creation of incorrect strophes because of the problem of
subcategorization (explained in section 3.2).
The third experiment makes small changes in the
original strophes (from the corpus), as it only replaces
each noun for a semantically related noun. The related
noun can be: (a) Antonym of the original word or (b)
hyponym of the hypernyms of the original word. In
order of preference, first we try to change each name
with one of its antonyms. If there is no antonym, then
we try to get the hypernyms of the word to return their
hyponims. Once the new word has been found, we
add the needed suffixes (the same ones that had the
words from the corpus) in order to fit correctly in the
strophe, using the morphological generator. The change
of words with related ones gives us the chance to express
semantically similar sentences using different words.
5 Evaluation
Once the experiments were finished, we made an evalu-
ation in order to analyze the quality of the automatically
generated strophes. The evaluation of computer gener-
ated poetry is nowadays fuzzy, so we defined a Turing
Test-like evaluation. We contacted two linguists that had
not done any work on this project, so that the evaluation
be as objective as possible. We prepared 135 strophes
interleaving some created by the machine with others
from the corpus. We asked the evaluators to guess if the
strophe was done by the machine or by a human. We
only draw conclusions using machine-generated stro-
phes, as we want to know how many of them percolate
as human-generated ones. In the next table you can
see the rate of sentences created by the machine and
suposed to be done by humans:
EXPERIMENT
Evaluator 1 1 2 3
Percolated as human 0.033 0.259 0.75
Evaluator 2
Percolated as human 0.333 0.481 0.75
As you can see, according to Evaluator 1, the first
experiment was not very worthy, as the only 3.3% of
the machine generated strophes percolated as human
generated ones. The second experiment got better re-
sults, and the 26% of the strophes were thought to be
human generated ones. As expected, the strophes of
the third experiment are the most trustworthy ones. The
results given by the second evaluator are higher, but the
important fact is the increase of the progression over the
experiments.
6 Discussion & Future Work
In this paper we have presented a set of experiments
for the automatic generation of poetry using POS and
inflectional tag patterns and some semantics. In the
last section we show the Turing Test-like evaluation to
measure the reliability of each experiment. This will be
part of a whole poetry analysis and generation system.
In the future, we intend to change verbs from stro-
phes controlling the subcategorization of them in order
to enable the creation of well-formed strophes about a
constrained topic. Also, we plan to use a frame seman-
tics resource, such as FrameNet, and after creating a
strophe, make some modifications to get an acceptable
semantic meaning.
165
References
Aduriz, I., Aranzabe, M., Arriola, J., de Ilarraza, A.,
Gojenola, K., Oronoz, M., and Uria, L. (2004). A cas-
caded syntactic analyser for Basque. Computational
Linguistics and Intelligent Text Processing, pages 124?
134.
Aduriz, I., Aranzabe, M. J., Arriola, J. M., Atutxa, A.,
de Ilarraza, D. A., Ezeiza, N., Gojenola, K., Oronoz,
M., Soroa, A., and Urizar, R. (2006). Methodology
and steps towards the construction of EPEC, a corpus
of written Basque tagged at morphological and syn-
tactic levels for automatic processing. Language and
Computers, 56(1):1?15.
Alegria, I., Artola, X., Sarasola, K., and Urkia, M.
(1996). Automatic morphological analysis of Basque.
Literary and Linguistic Computing, 11(4):193?203.
Alegria, I., Etxeberria, I., Hulden, M., and Maritxalar,
M. (2010). Porting Basque morphological grammars
to foma, an open-source tool. Finite-State Methods
and Natural Language Processing, pages 105?113.
Astigarraga, A., Agirrezabal, M., Lazkano, E., Jauregi,
E., and Sierra, B. (2013). Bertsobot: the first min-
strel robot. 6th International Conference on Human
System Interaction, Gdansk.
Fellbaum, C. (2010). WordNet. Springer.
Gerva?s, P. (2000). Wasp: Evaluation of different strate-
gies for the automatic generation of Spanish verse. In
Proceedings of the AISB-00 Symposium on Creative
& Cultural Aspects of AI, pages 93?100.
Gerva?s, P. (2001). An expert system for the composition
of formal spanish poetry. Knowledge-Based Systems,
14(3):181?188.
Gerva?s, P. (2010). Engineering linguistic creativity: Bird
flight and jet planes. In Proceedings of the NAACL
HLT 2010 Second Workshop on Computational Ap-
proaches to Linguistic Creativity, pages 23?30. Asso-
ciation for Computational Linguistics.
Levy, R. P. (2001). A computational model of poetic
creativity with neural network as measure of adaptive
fitness. In Proccedings of the ICCBR-01 Workshop
on Creative Systems. Citeseer.
Manurung, R. (2003). An evolutionary algorithm ap-
proach to poetry generation. PhD thesis, School of
informatics, University of Edinburgh.
Pociello, E., Agirre, E., and Aldezabal, I. (2011).
Methodology and construction of the Basque Word-
net. Language resources and evaluation, 45(2):121?
142.
166
