Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1688?1699,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Boosting Cross-Language Retrieval by Learning
Bilingual Phrase Associations from Relevance Rankings
Artem Sokolov and Laura Jehl and Felix Hieber and Stefan Riezler
Department of Computational Linguistics
Heidelberg University, 69120 Heidelberg, Germany
{sokolov,jehl,hieber,riezler}@cl.uni-heidelberg.de
Abstract
We present an approach to learning bilin-
gual n-gram correspondences from relevance
rankings of English documents for Japanese
queries. We show that directly optimizing
cross-lingual rankings rivals and complements
machine translation-based cross-language in-
formation retrieval (CLIR). We propose an ef-
ficient boosting algorithm that deals with very
large cross-product spaces of word correspon-
dences. We show in an experimental evalu-
ation on patent prior art search that our ap-
proach, and in particular a consensus-based
combination of boosting and translation-based
approaches, yields substantial improvements
in CLIR performance. Our training and test
data are made publicly available.
1 Introduction
The central problem addressed in Cross-Language
Information Retrieval (CLIR) is that of translating
or projecting a query into the language of the docu-
ment repository across which retrieval is performed.
There are two main approaches to tackle this prob-
lem: The first approach leverages the standard Sta-
tistical Machine Translation (SMT) machinery to
produce a single best translation that is used as
search query in the target language. We will hence-
forth call this the direct translation approach. This
technique is particularly useful if large amounts of
data are available in domain-specific form.
Alternative approaches avoid to solve the hard
problem of word reordering, and instead rely on
token-to-token translations that are used to project
the query terms into the target language with a
probabilistic weighting of the standard term tf-idf
scheme. Darwish and Oard (2003) termed this
method the probabilistic structured query approach.
The advantage of this technique is an implicit query
expansion effect due to the use of probability distri-
butions over term translations (Xu et al, 2001). Re-
cent research has shown that leveraging query con-
text by extracting term translation probabilities from
n-best direct translations of queries instead of using
context-free translation tables outperforms both di-
rect translation and context-free projection (Ture et
al., 2012b; Ture et al, 2012a).
While direct translation as well as probabilistic
structured query approaches use machine learning to
optimize the SMT module, retrieval is done by stan-
dard search algorithms in both approaches. For ex-
ample, Google?s CLIR approach uses their standard
proprietary search engine (Chin et al, 2008). Ture et
al. (2012b; 2012a) use standard retrieval algorithms
such as BM25 (Robertson et al, 1998). That means,
machine learning in SMT-based approaches concen-
trates on the cross-language aspect of CLIR and is
agnostic of the ultimate ranking task.
In this paper, we present a method to project
search queries into the target language that is com-
plementary to SMT-based CLIR approaches. Our
method learns a table of n-gram correspondences by
direct optimization of a ranking objective on rele-
vance rankings of English documents for Japanese
queries. Our model is similar to the approach of
Bai et al (2010) who characterize their technique as
?Learning to rank with (a Lot of) Word Features?.
Given a set of search queries q ? IRQ and docu-
1688
ments d ? IRD, where the jth dimension of a vector
indicates the occurrence of the jth word for dictio-
naries of size Q and D, we want to learn a score
f(q,d) between a query and a given document us-
ing the model1
f(q,d) = q>Wd =
Q?
i=1
D?
j=1
qiWijdj .
We take a pairwise ranking approach to optimiza-
tion. That is, given labeled data in the form of a
set R of tuples (q,d+,d?), where d+ is a relevant
(or higher ranked) document and d? an irrelevant
(or lower ranked) document for query q, the goal
is to find a weight matrix W ? IRQ?D such that
f(q,d+) > f(q,d?) for all data tuples from R.
The scoring model learns weights for all possible
correspondences of query terms and document terms
by directly optimizing the ranking objective at hand.
Such a phrase table contains domain-specific word
associations that are useful to discern relevant from
irrelevant documents, something that is orthogonal
and complementary to standard SMT models.
The challenge of our approach can be explained
by constructing a joint feature map ? from the outer
product of the vectors q and d where
?((i?1)D+j)(q,d) = (q? d)ij = (qd
>)ij . (1)
Using this feature map, we see that the score func-
tion f can be written in the standard form of a lin-
ear model that computes the inner product between
a weight vector w and a feature vector ? where
w, ? ? IRQ?D and
f(q,d) = ?w, ?(q,d)?. (2)
While various standard algorithms exist to optimize
linear models, the difficulty lies in the memory foot-
print and capacity of the word-based model. A full-
sized model includes Q ? D parameters which is
easily in the billions even for moderately sized dic-
tionaries. Clearly, an efficient implementation and
remedies against overfitting are essential.
The main contribution of our paper is the pre-
sentation of algorithms that make learning a phrase
1With bold letters we denote vectors for query q and docu-
ment d. Vector components are denoted with normal font letters
and indices (e.g., qi).
table by direct rank optimization feasible, and an
experimental verification of the benefits of this ap-
proach, especially with regard to a combination
of the orthogonal information sources of ranking-
based and SMT-based CLIR approaches. Our ap-
proach builds upon a boosting framework for pair-
wise ranking (Freund et al, 2003) that allows the
model to grow incrementally, thus avoiding having
to deal with the full matrix W . Furthermore, we
present an implementation of boosting that utilizes
parallel estimation on bootstrap samples from the
training set for increased efficiency and reduced er-
ror (Breiman, 1996). Our ?bagged boosting? ap-
proach allows to combine incremental feature selec-
tion, parallel training, and efficient management of
large data structures.
We show in an experimental evaluation on large-
scale retrieval on patent abstracts that our boosting
approach is comparable in MAP and improves sig-
nificantly by 13-15 PRES points over very competi-
tive translation-based CLIR systems that are trained
on 1.8 million parallel sentence pairs from Japanese-
English patent documents. Moreover, a combination
of the orthogonal information learned in ranking-
based and translation-based approaches improves
over 7 MAP points and over 15 PRES points over the
respective translation-based system in a consensus-
based voting approach following the Borda Count
technique (Aslam and Montague, 2001).
2 Related Work
Recent research in CLIR follows the two main
paradigms of direct translation and probabilistic
structured query approaches. An example for the
first approach is the work of Magdy and Jones
(2011) who presented an efficient technique to adapt
off-the-shelf SMT systems for CLIR by training
them on data pre-processed for retrieval (case fold-
ing, stopword removal, stemming). Nikoulina et al
(2012) presented an approach to direct translation-
based CLIR where the n-best list of an SMT system
is re-ranked according to the MAP performance of
the translated queries. The probabilistic structured
query approach has seen a lot of work on context-
aware query expansion across languages, based on
various similarity statistics (Ballesteros and Croft,
1998; Gao et al, 2001; Lavrenko et al, 2002; Gao
1689
et al, 2007). At the time of writing this paper, the
most recent extension to this paradigm is Ture et
al. (2012a). In addition to projecting terms from
n-best translations, they propose a projection ex-
tracted from the hierarchical phrase- based grammar
models, and a scoring method based on multi-token
terms. Since the latter techniques achieved only
marginal improvements over the context-sensitive
query translation from n-best lists, we did not pur-
sue them in our work.
CLIR in the context of patent prior art search
was done as extrinsic evaluation at the NTCIR
PatentMT2 workshops until 2010, and has been on-
going in the CLEF-IP3 benchmarking workshops
since 2009. However, most workshop participants
did either not make use of automatic translation at
all, or they used an off-the-shelf translation tool.
This is due to the CLEF-IP data collection where
parts of patent documents are provided as man-
ual translations into three languages. In order to
evaluate CLIR in a truly cross-lingual scenario, we
created a large patent CLIR dataset where queries
and documents are Japanese and English patent ab-
stracts, respectively.
Ranking approaches to CLIR have been presented
by Guo and Gomes (2009) who use pairwise rank-
ing for patent retrieval. Their method is a classical
learning-to-rank setup where retrieval scores such as
tf-idf or BM25 are combined with domain knowl-
edge on patent class, inventor, date, location, etc.
into a dense feature vector of a few hundred fea-
tures. Methods to learn word-based translation cor-
respondences from supervised ranking signals have
been presented by Bai et al (2010) and Chen et
al. (2010). These approaches tackle the problem of
complexity and capacity of the cross product matrix
of word correspondences from different directions.
The first proposes to learn a low rank representa-
tion of the matrix; the second deploys sparse online
learning under `1 regularization to keep the matrix
small. Both approaches are mainly evaluated in a
monolingual setting. The cross-lingual evaluation
presented in Bai et al (2010) uses weak translation-
based baselines and non-public data such that a di-
rect comparison is not possible.
2http://research.nii.ac.jp/ntcir/ntcir/
3http://www.ifs.tuwien.ac.at/?clef-ip/
A combination of bagging and boosting in the
context of retrieval has been presented by Pavlov et
al. (2010) and Ganjisaffar et al (2011). This work
is done in a standard learning-to-rank setup using a
few hundred dense features trained on hundreds of
thousands of pairs. Our setup deals with billions of
sparse features (from the cross-product of the un-
restricted dictionaries) trained on millions of pairs
(sampled from a much larger space). Parallel boost-
ing where all feature weights are updated simultane-
ously has been presented by Collins et al (2002) and
Canini et al (2010). The first method distributes the
gradient calculation for different features among dif-
ferent compute nodes. This is not possible in our ap-
proach because we construct the cross-product ma-
trix on-the-fly. The second approach requires sub-
stantial efforts in changing the data representation
to use the MapReduce framework. Overall, one of
the goals of our work is sequential updating for im-
plicit feature selection, something that runs contrary
to parallel boosting.
3 CLIR Approaches
3.1 Direct translation approach
For direct translation, we use the SCFG decoder
cdec (Dyer et al, 2010)4 and build grammars us-
ing its implementation of the suffix array extraction
method described in Lopez (2007). Word align-
ments are built from all parallel data using mgiza5
and the Moses scripts6. SCFG models use the same
settings as described in Chiang (2007). Training
and querying of a modified Kneser-Ney smoothed 5-
gram language model are done on the English side
of the training data using KenLM (Heafield, 2011)7.
Model parameters were optimized using cdec?s im-
plementation of MERT (Och (2003)).
At retrieval time, all queries are translated
sentence-wise and subsequently re-joined to form
one query per patent. Our baseline retrieval system
uses the Okapi BM25 scores for document ranking.
4https://github.com/redpony/cdec
5http://www.kyloo.net/software/doku.php/
mgiza:overview
6http://www.statmt.org/moses/?n=Moses.
SupportTools
7http://kheafield.com/code/kenlm/
estimation/
1690
3.2 Probabilistic structured query approach
Early Probabilistic Structured Query approaches
(Xu et al, 2001; Darwish and Oard, 2003) represent
translation options by lexical, i.e., token-to-token
translation tables that are estimated using standard
word alignment techniques (Och and Ney, 2000).
Later approaches (Ture et al, 2012b; Ture et al,
2012a) extract translation options from the decoder?s
n-best list for translating a particular query. The
central idea is to let the language model choose flu-
ent, context-aware translations for each query term
during decoding. This retains the desired query-
expansion effect of probabilistic structured models,
but it reduces query drift by filtering translations
with respect to the context of the full query.
A projection of source language query terms f ?
F into the target language is achieved by repre-
senting each source token f by its probabilistically
weighted translations. The score of target document
E, given source language query F , is computed by
calculating the BM25 rank over projected term fre-
quency and document frequency weights as follows:
score(E|F ) =
?
f?F
BM25(tf(f,E), df(f)) (3)
tf(f,E) =
?
e?Ef
tf(e, E)p(e|f)
df(f) =
?
e?Ef
df(e)p(e|f)
where Ef = {e ? E|p(e|f) > pL} is the set of
translation options for query term f with probability
greater than pL. We also use a cumulative threshold
pC so that only the most probable options are added
until pC is reached.
Ture et al (2012b; 2012a) achieved best retrieval
performance by interpolating between (context-free)
lexical translation probabilities plex estimated on
symmetrized word alignments, and (context-aware)
translation probabilities pnbest estimated on the n-
best list of an SMT decoder:
p(e|f) = ?pnbest(e|f) + (1? ?)plex(e|f) (4)
pnbest(e|f) is estimated by calculating expectations
of term translations from k-best translations:
pnbest(e|f) =
?n
k=1 ak(e, f)D(k, F )?n
k=1
?
e? ak(e
?, f)D(k, F )
where ak(e, f) is a function indicating an alignment
of target term e to source term f in the kth derivation
of queryF , andD(k, F ) is the model score of the kth
derivation in the n-best list for query F .
We use the same hierarchical phrase-based sys-
tem that was used for direct translation to calcu-
late n-best translations for the probabilistic struc-
tured query approach. This allows us to extract
word alignments between source and target text for
F from the SCFG rules used in the derivation. The
concept of self-translation is covered by the de-
coder?s ability to use pass-through rules if words or
phrases cannot be translated.
Probabilistic structured queries that include
context-aware estimates of translation probabilities
require a preservation of sentence-wise context-
sensitivity also in retrieval. Thus, unlike the direct
translation approach, we compute weighted term
and document frequencies for each sentence s in
query F separately. The scoring (3) of a target doc-
ument for a multiple sentence query then becomes:
score(E|F ) =
?
s in F
?
f?s
BM25(tf(f,E), df(f))
3.3 Direct Phrase Table Learning from
Relevance Rankings
Pairwise Ranking using Boosting The general
form of the RankBoost algorithm (Freund et al,
2003; Collins and Koo, 2005) defines a scoring
function f(q,d) on query q and document d as a
weighted linear combination of T weak learners ht
such that f(q,d) =
?T
t=1wtht(q,d). Weak learn-
ers can belong to an arbitrary family of functions,
but in our case they are restricted to the simplest
case of unparameterized indicator functions select-
ing components of the feature vector ?(q,d) in (1)
such that f is of the standard linear form (2). In our
experiments, these features indicate the presence of
pairs of uni- and bi-grams from the source-side vo-
cabulary of query terms and the target-side vocabu-
lary of document-terms, respectively. Furthermore,
in order to simulate the pass-through behavior of
SMT, we introduce additional features to the model
that indicate the identity of terms in source and tar-
get. All identity features have the same fixed weight
?, which is found on the development set.
For training, we are given labeled data in the form
1691
of a set R of tuples (q,d+,d?), where d+ is a rel-
evant (or higher ranked) document and d? an ir-
relevant (or lower ranked) document for query q.
RankBoost?s objective is to correctly rank query-
document pairs such that f(q,d+) > f(q,d?) for
all data tuples from R. RankBoost achieves this by
optimizing the following convex exponential loss:
Lexp =
?
(q,d+,d?)?R
D(q,d+,d?)ef(q,d
?)?f(q,d+),
where D(q,d+,d?) is a non-negative importance
function on pairs of documents for a given q.
We optimize Lexp in a greedy iterative fash-
ion, which closely follows an efficient algorithm of
Collins and Koo (2005) for the case of binary-valued
h. In each step, the single feature h is selected that
provides the largest decrease of Lexp, i.e., that has
the largest projection on the direction of the gradi-
ent ?hLexp. Because of the sequential nature of
the algorithm, RankBoost implicitly performs auto-
matic feature selection and regularization (Rosset et
al., 2004), which is crucial to reduce complexity and
capacity for our application.
Parallelization and Bagging To achieve paral-
lelization we use a variant of bagging (Breiman,
1996) on top of boosting, which has been observed
to improve performance, reduce variance and is
trivial to parallelize. The procedure is described
as part of Algorithm 1: From the set of prefer-
ence pairs R, draw S equal-sized samples with
replacement and distribute to nodes. Then, us-
ing each of the samples as a training set, sep-
arate boosting models {wst , h
s
t}, s = 1 . . . S are
trained that contain the same number of features
t = 1 . . . T . Finally the models are averaged:
f(q,d) = 1S
?
t
?
sw
s
th
s
t (q,d).
Algorithm The entire training procedure is out-
lined in Algorithm 1. For each possible feature h
we maintain auxiliary variables W+h and W
?
h :
W?h =
?
(q,d+,d?):h(q,d+)?h(q,d?)=?1
D(q,d+,d?),
which are the cumulative weights of correctly and
incorrectly ranked instances by a candidate feature
h. The absolute value of ?Lexp/?h can be ex-
pressed as
?
?
?
W+h ?
?
W?h
?
? which is used as fea-
ture selection criterion (Collins and Koo, 2005).
The optimum of minimizing Lexp over w (with
fixed h) can be shown to be w = 12 ln
W+h +Z
W?h +Z
,
where  is a smoothing parameter to avoid prob-
lems with small W?h (Schapire and Singer, 1999),
and Z =
?
(q,d+,d?)?RD(q,d
+,d?). Further-
more, for each step t of the learning process, values
of D are updated to concentrate on pairs that have
not been correctly ranked so far:
Dt+1 = Dt ? e
wt
(
ht(q,d?)?ht(q,d+)
)
. (5)
Finally, to speed up learning, on iteration t we
recalculate W?h only for those h that cooccur
with previously selected ht and keep the rest un-
changed (Collins and Koo, 2005).
Algorithm 1: Bagged Boosting
Input: training tuplesR, max number of features
T , initial D0, smoothing param.  ' 10?5
Initialize:
fromR draw S samples with replacement and
distribute to nodes
Learn:
for all samples s = 1 . . . S in parallel do
calculate W+h ,W
?
h , Z on sample?s data
for all t = 1 . . . T do
choose ht = argmaxh
?
?
?
W+h ?
?
W?h
?
?
and wt = 12 ln
W+h +Z
W?h +Z
update Dt according to (5)
update W?h for all h that cooccur with ht
end
return to master {hst , w
s
t }, t = 1 . . . T
end
Bagging:
return scoring function
f(q,d) = 1S
?
t
?
sw
s
th
s
t (q,d)
Implementation Because of the total number of
features (billions) there are several obstacles for the
straight-forward implementation of Algorithm 1.
First, we cannot directly access all pairs (q,d)
containing a particular feature h needed for calcu-
lating W?h . Building an inverted index is compli-
cated as it needs to fit into memory for fast fre-
1692
quent access8. We resort to the on-the-fly creation of
the cross-product space of features, following prior
work by Grangier and Bengio (2008) and Goel et al
(2008). That is, while processing a pair (q,d), we
update W?h for all h found for the pair.
Second, even if the explicit representation of all
features is avoided by on-the-fly feature construc-
tion, we still need to keep all W?h in addressable
RAM. To achieve that we use hash kernels (Shi et
al., 2009) and map original features into b-bit integer
hashes. The values W?h? for new, ?hashed?, features
h? become W?h? =
?
h:HASH(h)=h?W
?
h . We used
the MurmurHash3 function on the UTF-8 represen-
tations of features and b = 30 (resulting in more
than 1 billion distinct hashes).
4 Model Combination by Borda Counts
SMT-based approaches to CLIR and our boosting
approach have different strengths. The SMT-based
approaches produce fluent translations that are use-
ful for matching general passages written in natu-
ral language. Both baseline SMT-based approaches
presented above are agnostic of the ultimate retrieval
task and are not specifically adapted for it. The
boosting method, on the other hand, learns domain-
specific word associations that are useful to discern
relevant from irrelevant documents. In order to com-
bine these orthogonal sources of information in a
way that democratically respects each approach we
use Borda Counts, i.e., a consensus-based voting
procedure that has been successfully employed to
aggregate ranked lists of documents for metasearch
(Aslam and Montague, 2001).
We implemented a weighted version of the Borda
Count method where each voter has a fixed amount
of voting points which she is free to distribute among
the candidates to indicate the amount of preference
she is giving to each of them. In the case of retrieval,
for each q, the candidates are the scored documents
in the retrieved subset of the whole document set.
The aggregate score fagg for two rankings f1(q,d)
8It is possible to construct separate query and document in-
verted indices and intersect them on the fly to determine the
set of documents that contains some pair of words. In practice,
however, we found the overhead of set intersection during each
feature access prohibitive.
and f2(q,d) for all (q,d) in the test set is then:
fagg(q,d) = ?
f1(q,d)
?
d f1(q,d)
+(1??)
f2(q,d)
?
d f2(q,d)
.
In practice, the normalizations sum over the top K
retrieved documents. If a document is present only
in the top-K list of one system, its score is con-
sidered zero for the other system. The aggregated
scores fagg(q,d) are sorted in descending order and
top K scores are kept for evaluation.
Using the terminology proposed by Belkin et
al. (1995), combining several systems? scores with
Borda Counts can be viewed as the ?data fusion?
approach to IR, that merges outputs of the systems,
while the PSQ baseline is an example of the ?query
combination? approach that extends the query at the
input. Both techniques were earlier found to have
similar performance in CLIR tasks based on direct
translation, with a preference for the data fusion ap-
proach (Jones and Lam-Adesina, 2002).
5 Translation and Ranking Data
5.1 Parallel Translation Data
For Japanese-to-English patent translation we used
data provided by the organizers of the NTCIR9
workshop for the JP-EN PatentMT subtask. In par-
ticular, we used the data provided for NTCIR-7 (Fu-
jii et al, 2008), consisting of 1.8 million parallel
sentence pairs from the years 1993-2002 for train-
ing. For parameter tuning we used the develop-
ment set of the NTCIR-8 test collection, consisting
of 2,000 sentence pairs. The data were extracted
from the description section of patents published
by the Japanese Patent Office (JPO) and the United
States Patent and Trademark Office (USPTO) by the
method described in Utiyama and Isahara (2007).
Japanese text was segmented using the MeCab10
toolkit. Following Feng et al (2011), we applied
a modified version of the compound splitter de-
scribed in Koehn and Knight (2003) to katakana
terms, which are often transliterations of English
compound words. As these are usually not split by
MeCab, they can cause a large number of out-of-
vocabulary terms.
9http://research.nii.ac.jp/ntcir/ntcir/
10https://code.google.com/p/mecab/
1693
#queries #relevant #unique docs
train 107,061 1,422,253 888,127
dev 2,000 26,478 25,669
test 2,000 25,173 24,668
Table 1: Statistics of ranking data.
For the English side of the training data, we ap-
plied a modified version of the tokenizer included in
the Moses scripts. This tokenizer relies on a list of
non-breaking prefixes which mark expressions that
are usually followed by a ?.? (period). We cus-
tomized the list of prefixes by adding some abbrevi-
ations like ?Chem?, ?FIG? or ?Pat?, which are spe-
cific to patent documents.
5.2 Ranking Data from Patent Citations
Graf and Azzopardi (2008) describe a method to ex-
tract relevance judgements for patent retrieval from
patent citations. The key idea is to regard patent doc-
uments that are cited in a query patent, either by the
patent applicant, or by the patent examiner or in a
patent office?s search report, as relevant for the query
patent. Furthermore, patent documents that are re-
lated to the query patent via a patent family relation-
ship, i.e., patents granted by different patent author-
ities but related to the same invention, are regarded
as relevant. We assign three integer relevance levels
to these three categories of relationships, with high-
est relevance (3) for family patents, lower relevance
for patents cited in search reports by patent examin-
ers (2), and lowest relevance level (1) for applicants?
citations. We also include all patents which are in
the same patent family as an applicant or examiner
citation to avoid false negatives. This methodol-
ogy has been used to create patent retrieval data at
CLEF-IP11 and proved very useful to automatically
create a patent retrieval dataset for our experiments.
For the creation of our dataset, we used the
MAREC12 citation graph to extract patents in cita-
tion or family relation. Since the Japanese portion
of the MAREC corpus only contains English ab-
stracts, but not the Japanese full texts, we merged
the patent documents in the NTCIR-10 test collec-
tion described above with the Japanese (JP) section
11http://www.ifs.tuwien.ac.at/?clef-ip/
12http://www.ifs.tuwien.ac.at/imp/marec.
shtml
of MAREC. Title, abstract, description and claims
were added to the MAREC-JP data if the docu-
ment was available in NTCIR. In order to keep par-
allel data for SMT training separate from ranking
data, we used only data from the years 2003-2005
to extract training data for ranking, and two small
datasets of 2,000 queries each from the years 2006-
2007 for development and testing. Table 1 gives an
overview over the data used for ranking. For de-
velopment and test data, we randomly added irrele-
vant documents from the NTCIR-10 collection until
we obtained two pools of 100,000 documents. The
necessary information to reproduce the exact train,
development and test data samples is downloadable
from authors? webpage13.
The experiments reported here use only the ab-
stract of the Japanese and English patents in our
training, development and test collection.
6 Experiments
6.1 System Development
System development and evaluation in our exper-
iments was done on the ranking data described
in the previous section (see Table 1). We report
Mean Average Precision (MAP) scores, using the
trec eval (ver. 8.1) script from the TREC evalu-
ation campaign14, with a limit of top K = 1, 000 re-
trieved documents for each query. Furthermore, we
use the Patent Retrieval Evaluation Score (PRES)15
introduced by Magdy and Jones (2010). This met-
ric accounts for both precision and recall. In the
study by Magdy and Jones (2010), PRES agreed
with MAP in almost 80% of cases, and both agreed
on the ranks of the best and the worst IR system.
Both MAP and PRES scores are reported in the same
range [0, 1], and 0.01 stands for 1 MAP (PRES)
point. Statistical significance of pairwise system
comparisons was assessed using the paired random-
ization test (Noreen, 1989; Smucker et al, 2007).
For each system, optimal meta-parameter settings
were found by choosing the configuration with high-
est MAP score on the development set. These results
13http://www.cl.uni-heidelberg.de/
statnlpgroup/boostclir
14http://trec.nist.gov/trec_eval
15http://www.computing.dcu.ie/?wmagdy/
Scripts/PRESeval.htm
1694
method MAP PRESdev test dev test
1 DT 0.2636 0.2555 0.5669 0.5681
2 PSQ lexical table 0.2520 0.2444 0.5445 0.5498
3 PSQ n-best table 0.2698 0.2659 0.5789 0.5851
Boost-1g 0.2064 1230.1982 0.5850 120.6122
Boost-2g 0.2526 30.2474 0.6900 1230.7196
Table 2: MAP and PRES scores for CLIR methods (best
configurations) on the development and test sets. Prefixed
numbers denote statistical significance of a pairwise com-
parison with the baseline indicated by the superscript. For
example, the bottom right result shows that Boost-2g is
significantly better than DT (method 1), PSQ lexical ta-
ble (method 2) and PSQ n-best table (method 3).
(together with PRES results) are shown in the sec-
ond and fourth column of Table 2.
The direct translation approach (DT) was devel-
oped in three configurations: no stopword filtering,
small stopword list (52 words) and a large stopword
list (543 words). The last configuration achieved the
highest score (MAP 0.2636).
The probabilistic structured query (PSQ) ap-
proach was developed using the lexical translation
table and the translation table estimated on the de-
coder?s n-best list, both optionally pruned with a
variable lower pL and cumulative pC threshold on
the word pair probability in the table (Section 3.2).
A further meta-parameter of PSQ was whether to use
standard or unique n-best lists. Finally, all variants
were coupled with the same stopword filters as in
the DT approach. The configurations that achieved
the highest scores were: MAP 0.2520 for PSQ with
a lexical table (pL = 0.01, pC = 0.95, no stop-
word filtering), and MAP 0.2698 for PSQ with a
translation table estimated on the n-best list (pL =
0.005, pC = 0.95, large stopword list). Interpolat-
ing between lexical and n-best tables did not im-
prove results in our experiments, thus we set ? = 1
in equation (4).
Each SMT-based system was run with 4 different
MERT optimizations, leading to variations of less
than 1 MAP point for each system. The best con-
figurations for DT and PSQ on the development set
were fixed and used for evaluation on the test set.
Training of the boosting approach (Boost) was
done in parallel on bootstrap samples from the train-
ing data. First, a query q (i.e., a Japanese abstract)
was sampled uniformly from all training queries.
method MAP PRESdev test dev test
DT + PSQ n-best 0.2778 ?0.2726 0.5884 ?0.5942
DT + Boost-1g 0.2778 ?0.2728 0.6157 ?0.6225
DT + Boost-2g 0.3309 ?0.3300 0.7132 ?0.7279
PSQ lexical + Boost-1g 0.2695 ?0.2653 0.6068 ?0.6131
PSQ lexical + Boost-2g 0.3215 ?0.3187 0.7071 ?0.7240
PSQ n-best + Boost-1g 0.2863 ?0.2850 0.6309 ?0.6402
PSQ n-best + Boost-2g 0.3439 ?0.3416 0.7212 ?0.7376
Table 3: MAP and PRES scores of the aggregated mod-
els on the development and test sets. Development scores
correspond to peaks in Figures 1 and 3, respectively, for
MAP and PRES; test scores are given for the ??s deliv-
ering these peaks on the development set. Prefixed ? in-
dicates statistical significance of the result difference be-
tween aggregated system and the respective translation-
based system used in the aggregation.
Then we sampled independently and uniformly a
relevant document d+ (i.e., an English abstract)
from the English patents marked relevant for the
Japanese patent, and a random document d? from
the whole pool of English patent abstracts. If d?
had a relevance score greater or equal to the rele-
vance score of d+, it was resampled. The initial im-
portance weight D0 for a triplet (q,d+,d?) was set
to the positive difference in relevance scores for d+
and d?. Each bootstrap sample consisted of 10 pairs
of documents for each of 10, 000 queries, resulting
in 100, 000 training instances per sample.
The Boost approach was developed for uni-gram
and combined uni- and bi-gram versions. We ob-
served that the performance of the Boost method
continuously improved with the number of iterations
T and with the number of samples S, but saturated
at about 15-20 samples without visible over-fitting
in the tested range of T . Therefore we arbitrarily
stopped training after obtaining 5, 000 features per
sample, and used 35 samples for uni-gram version
and 65 samples for the combined bi-gram version,
resulting in models with 104K and 172K unique fea-
tures, respectively. The optimal values for the pass-
through weight ? were found to be 0.3 and 0.2 for
the uni-gram and bi-gram models on the develop-
ment set. The best configuration of uni-gram and
bi-gram model achieved MAP scores of 0.2064 and
0.2526 the development set. Using stopword filters
during training did not improve the results here.
1695
0.24
0.25
0.26
0.27
0.28
0.29
0.30
0.31
0.32
0.33
0.34
0.35
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
M
A
P
?
DT + Boost-2gPSQ lexical + Boost-2gPSQ n-best + Boost-2gDT, 0.2636PSQ lexical, 0.2520PSQ n-best, 0.2698Boost-2g, 0.2526PSQ n-best + DT
Figure 1: MAP rank aggregation for combinations of the
bi-gram boosting and the baselines on the dev set.
0.24
0.25
0.26
0.27
0.28
0.29
0.30
0.31
0.32
0.33
0.34
0.35
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
M
A
P
?
dev, PSQ n-best + Boost-2gtest, PSQ n-best + Boost-2gdev, PSQ n-best, 0.2698test, PSQ n-best, 0.2659dev, Boost-2g, 0.2526test, Boost-2g, 0.2474
Figure 2: MAP rank aggregation for the bi-gram boosting
and the ?PSQ n-best table? approach on dev and test sets.
0.54
0.56
0.58
0.60
0.62
0.64
0.66
0.68
0.70
0.72
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
P
R
E
S
?
DT + Boost-2gPSQ lexical + Boost-2gPSQ n-best + Boost-2gDT, 0.5669PSQ lexical, 0.5445PSQ n-best, 0.5789Boost-2g, 0.6900PSQ n-best + DT
Figure 3: PRES rank aggregation for combinations of the
bi-gram boosting and the baselines on the dev set.
6.2 Testing and Model Combination
The third and the fifth columns of Table 2 give a
comparison of the MAP scores of the baseline ap-
proaches and the Boost model evaluated individu-
ally on the test set. Each score corresponds to the
best configuration found on the development set. We
see that the PSQ approach using n-best lists for pro-
jection outperforms all other methods in terms of
MAP, but loses to both Boost approaches when eval-
uated with PRES. Direct translation is about 1 MAP
point lower than PSQ n-best; Boost with combined
uni- and bi-grams is another 0.8 MAP points worse,
but is better in terms of PRES, especially for the bi-
gram version. Given the fact that the complex SMT
system behind the direct translation and PSQ ap-
proach is trained and tuned on very large in-domain
datasets, the performance of the bare phrase table
induced by the Boost method is respectable.
Our best results are obtained by a combination
of the orthogonal information sources of the SMT
and the Boost approaches. We evaluated the Borda
Count aggregation scheme on the development data
in order to find the optimal value for ? ? [0, 1]. The
interpolation was done for the best combined uni-
and bi-gram boosting model with the best variants of
the DT and PSQ approaches. As can be seen from
Figures 1 and 3, rank aggregation by Borda Count
outperforms both individual approaches by a large
margin. Figure 2 verifies that the results are trans-
ferable from the development set to the test set. The
best performing system combination on the develop-
ment data is also optimal on the test data.
Table 3 shows the retrieval performance of the
best baseline model (PSQ n-best) combined with
the best Boost model (bi-gram), with an impres-
sive gain of over 7 MAP points (15 PRES points)
over the best individual baseline result from Table 2.
Even when, according to the PRES measure (Fig-
ure 3), the Boost-2g system is better on its own, in-
jecting complementary information from the PSQ or
DT approach still contributes several points. Simi-
lar gains are obtained by model combination of the
DT approach with the best Boost model. However,
a combination of the SMT-based CLIR approaches
DT and PSQ barely improved results over the best
input model. In summary, aggregating rankings is
helpful for orthogonal systems, but not for systems
including similar information.
1696
6.3 Analysis
Table 4 lists some of the top-200 selected features
for the boosting approach (the most common trans-
lation of the Japanese term is put in subscript).
We see that the direct ranking approach is able
to penalize uni- and bi-gram cooccurrences that are
harmful for retrieval by assigning them a negative
weight, e.g., the pairing of??resolution with image.
Pairs of uni- and bi-grams that are useful for re-
trieval are boosted by positive weights, e.g., the pair
??compression,?machine and compressor captures an
important compound. Further examples, not shown
in the table, are matches of the same source (tar-
get) n-gram with several different target (source) n-
grams, e.g., the Japanese term ??image is paired
not only with its main translation, but also with
dozens of related notions: video, picture, scanning,
printing, photosensitive, pixel, background etc. This
has a query expansion effect that is not possible in
systems that use one translation or a small list of n-
best translations. In addition, associations of source
n-grams with overlapping target n-grams help boost
the final score: e.g., the same term??image is pos-
itively paired with target bi-grams as {an,original},
{original,image} and {image,for}. This has the ef-
fect of compensating for the lack of handling phrase
overlaps in an SMT decoder.
7 Conclusion
We presented a boosting approach to induce a table
of bilingual n-gram correspondences by direct pref-
erence learning on relevance rankings. This table
can be seen as a phrase table that encodes word-
based information that is orthogonal and comple-
mentary to the information in standard translation-
based CLIR approaches. We compared our boosting
approach to very competitive CLIR baselines that
use a complex SMT system trained and tuned on
large in-domain datasets. Furthermore, our patent
retrieval setup gives SMT-based approaches an ad-
vantage in that queries consist of several normal-
length sentences, as opposed to the short queries
common to web search. Despite this and despite the
tiny size (about 170K parameters) of the boosting
phrase table, compared to standard SMT phrase ta-
bles, this approach reached performance similar to
direct translation using a full SMT model in terms
t ht (uni- & bi-grams) wt
1 ?layer - layer 1.29
2 ???data - data 1.13
3 ??circuit - circuit 1.13
76 ?in - voltage -0.39
77 ?guide,?power - conductive 1.25
81 ??resolution - image -0.25
99 ??speed - transmission 1.68
100 ??LCD - liquid,crystal 1.73
123 ?power - force 0.91
124 ??compression,?machine - compressor 2.83
132 ????cable - cable 1.81
133 ?hyper,??sound wave - ultrasonic 3.34
169 ??particle - particles 1.57
170 ??calculation - for,each 1.14
184 ???rotor - rotor 2.01
185 ??detection,?vessel - detector 1.43
Table 4: Examples of the features found by boosting.
of MAP, and was significantly better in terms of
PRES. Overall, we obtained the best results by a
model combination using consensus- based voting
where the best SMT-based approach was combined
with the boosting phrase table (gaining more than 7
MAP or 15 PRES points). We attribute this to the
fact that the boosting approach augments SMT ap-
proaches with valuable information that is hard to
get in approaches that are agnostic about the rank-
ing data and the ranking task at hand.
The experimental setup presented in this paper
uses relevance links between patent abstracts as
ranking data. While this technique is useful to de-
velop patent retrieval systems, it would be interest-
ing to see if our results transfer to patent retrieval
scenarios where full patent documents are used in-
stead of only abstracts, or to standard CLIR scenar-
ios that use short search queries in retrieval.
Acknowledgements
The research presented in this paper was supported
in part by DFG grant ?Cross-language Learning-to-
Rank for Patent Retrieval?. We would like to thank
Eugen Ruppert for his contribution to the ranking
data construction.
1697
References
Javed A. Aslam and Mark Montague. 2001. Models for
metasearch. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval (SIGIR?01), New Orleans, LA.
Bing Bai, Jason Weston, David Grangier, Ronan
Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier
Chapelle, and Kilian Weinberger. 2010. Learning to
rank with (a lot of) word features. Information Re-
trieval Journal, 13(3):291?314.
Lisa Ballesteros and W. Bruce Croft. 1998. Resolving
ambiguity for cross-language retrieval. In Proceedings
of the ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?98), Mel-
bourne, Australia.
Nicholas J. Belkin, Paul Kantor, Edward A. Fox, and
Joseph A. Shaw. 1995. Combining the evidence
of multiple query representations for information re-
trieval. Inf. Process. Manage., 31(3):431?448.
Leo Breiman. 1996. Bagging predictors. Journal of Ma-
chine Learning Research, 24:123?140.
Kevin Canini, Tushar Chandra, Eugene Ie, Jim McFad-
den, Ken Goldman, Mike Gunter, Jeremiah Harm-
sen, Kristen LeFevre, Dmitry Lepikhin, Tomas Lloret
Llinares, Indraneel Mukherjee, Fernando Pereira, Josh
Redstone, Tal Shaked, and Yoram Singer. 2010.
Sibyl: A system for large scale machine learning.
In LADIS: The 4th ACM SIGOPS/SIGACT Workshop
on Large Scale Distributed Systems and Middleware,
Zurich, Switzerland.
Xi Chen, Bing Bai, Yanjun Qi, Qihang Ling, and Jaime
Carbonell. 2010. Learning preferences with millions
of parameters by enforcing sparsity. In Proceedings
of the IEEE International Conference on Data Mining
(ICDM?10), Sydney, Australia.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Jeffrey Chin, Maureen Heymans, Alexandre Kojoukhov,
Jocelyn Lin, and Hui Tan. 2008. Cross-language
information retrieval. Patent Application. US
2008/0288474 A1.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?69.
Michael Collins, Robert E. Schapire, and Yoram Singer.
2002. Logistic regression, AdaBoost and Bregman
distances. Journal of Machine Learning Research,
48(1-3):253?285.
Kareem Darwish and Douglas W. Oard. 2003. Proba-
bilistic structured query methods. In Proceedings. of
the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR?03), Toronto,
Canada.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, Upp-
sala, Sweden.
Minwei Feng, Christoph Schmidt, Joern Wuebker,
Stephan Peitz, Markus Freitag, and Hermann Ney.
2011. The RWTH Aachen system for NTCIR-9
PatentMT. In Proceedings of the NTCIR-9 Workshop,
Tokyo, Japan.
Yoav Freund, Ray Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. Journal of Machine Learning
Research, 4:933?969.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent trans-
lation task at the NTCIR-7 workshop. In Proceedings
of NTCIR-7 Workshop Meeting, Tokyo, Japan.
Yasser Ganjisaffar, Rich Caruana, and Cristina Videira
Lopes. 2011. Bagging gradient-boosted trees for
high precision, low variance ranking models. In Pro-
ceedings of the ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR?11),
Beijing, China.
Jianfeng Gao, Jian-Yun Nie, Endong Xun, Jian Zhang,
Ming Zhou, and Changning Huang. 2001. Improv-
ing query translation for cross-language information
retrieval using statistical models. In Proceedings of
the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR?01), New Or-
leans, LA.
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Jian Hu,
Kam-Fai Wong, and Hsiao-Wuen Hon. 2007. Cross-
lingual query suggestion using query logs of different
languages. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval (SIGIR?07), Amsterdam, The Netherlands.
Sharad Goel, John Langford, and Alexander L. Strehl.
2008. Predictive indexing for fast search. In Advances
in Neural Information Processing Systems, Vancouver,
Canada.
Erik Graf and Leif Azzopardi. 2008. A methodology
for building a patent test collection for prior art search.
In Proceedings of the 2nd International Workshop on
Evaluating Information Access (EVIA), Tokyo, Japan.
David Grangier and Samy Bengio. 2008. A discrimi-
native kernel-based approach to rank images from text
queries. IEEE Transactions on Pattern Analysis and
Machine Intelligence (PAMI), 30(8):1371?1384.
Yunsong Guo and Carla Gomes. 2009. Ranking struc-
tured documents: A large margin based approach for
1698
patent prior art search. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI?09), Pasadena, CA.
Kenneth Heafield. 2011. KenLM: faster and smaller lan-
guage model queries. In Proceedings of the EMNLP
2011 Sixth Workshop on Statistical Machine Transla-
tion (WMT?11), Edinburgh, UK.
Gareth J.F. Jones and Adenike M. Lam-Adesina. 2002.
Combination methods for improving the reliability of
machine translation based cross-language information
retrieval. In Proceedings of the 13th Irish Interna-
tional Conference on Artificial Intelligence and Cog-
nitive Science (AICS?02), Limerick, Ireland.
Philipp Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of the
Conference on European Chapter of the Association
for Computational Linguistics (EACL?03), Budapest,
Hungary.
Victor Lavrenko, Martin Choquette, and W. Bruce Croft.
2002. Cross-lingual relevance models. In Proceed-
ings of the ACM Conference on Research and Devel-
opment in Information Retrieval (SIGIR?02), Tampere,
Finland.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2007), Prague,
Czech Republic.
Walid Magdy and Gareth J.F. Jones. 2010. PRES: a
score metric for evaluating recall-oriented information
retrieval applications. In Proceedings of the ACM SI-
GIR conference on Research and development in in-
formation retrieval (SIGIR?10), New York, NY.
Walid Magdy and Gareth J. F. Jones. 2011. An efficient
method for using machine translation technologies in
cross-language patent search. In Proceedings of the
20th ACM Conference on Informationand Knowledge
Management (CIKM?11), Glasgow, Scotland, UK.
Vassilina Nikoulina, Bogomil Kovachev, Nikolaos Lagos,
and Christof Monz. 2012. Adaptation of statistical
machine translation model for cross-lingual informa-
tion retrieval in a service context. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics (EACL?12),
Avignon, France.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Meeting of the Association for Computational Linguis-
tics (ACL?00), Hongkong, China.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Meeting on Association for Computational Lin-
guistics (ACL?03), Sapporo, Japan.
Dmitry Pavlov, Alexey Gorodilov, and Cliff A. Brunk.
2010. Bagboo: a scalable hybrid bagging-the-
boosting model. In Proceedings of the 19th ACM
International Conference on Information and Knowl-
edge Management (CIKM?10), Toronto, Canada.
Stephen E. Robertson, Steve Walker, and Micheline
Hancock-Beaulieu. 1998. Okapi at TREC-7. In
Proceedings of the Seventh Text REtrieval Conference
(TREC-7), Gaithersburg, MD.
Saharon Rosset, Ji Zhu, and Trevor Hastie. 2004. Boost-
ing as a regularized path to a maximum margin clas-
sifier. Journal of Machine Learning Research, 5:941?
973.
Robert E. Schapire and Yoram Singer. 1999. Im-
proved boosting algorithms using confidence-rated
predictions. Journal of Machine Learning Research,
37(3):297?336.
Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alexander J. Smola, Alexander L. Strehl, and
Vishy Vishwanathan. 2009. Hash Kernels. In Pro-
ceedings of the 12th Int. Conference on Artificial In-
telligence and Statistics (AISTATS?09), Irvine, CA.
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests for
information retrieval evaluation. In Proceedings of the
16th ACM conference on Conference on Information
and Knowledge Management (CIKM ?07), New York,
NY.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012a.
Combining statistical translation techniques for cross-
language information retrieval. In Proceedings of the
International Conference on Computational Linguis-
tics (COLING 2012), Bombay, India.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012b.
Looking inside the box: Context-sensitive translation
for cross-language information retrieval. In Proceed-
ings of the ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR 2012),
Portland, OR.
Masao Utiyama and Hitoshi Isahara. 2007. A Japanese-
English patent parallel corpus. In Proceedings of MT
Summit XI, Copenhagen, Denmark.
Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual in-
formation retrieval. In Proceedings of the ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval (SIGIR?01), New York, NY.
1699
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239?248,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Source-side Preordering for Translation using Logistic Regression and
Depth-first Branch-and-Bound Search
?
Laura Jehl
?
Adri
`
a de Gispert
?
Mark Hopkins
?
William Byrne
?
?
Dept. of Computational Linguistics, Heidelberg University. 69120 Heidelberg, Germany
jehl@cl.uni-heidelberg.de
?
SDL Research. East Road, Cambridge CB1 1BH, U.K.
{agispert,mhopkins,bbyrne}@sdl.com
Abstract
We present a simple preordering approach
for machine translation based on a feature-
rich logistic regression model to predict
whether two children of the same node
in the source-side parse tree should be
swapped or not. Given the pair-wise chil-
dren regression scores we conduct an effi-
cient depth-first branch-and-bound search
through the space of possible children per-
mutations, avoiding using a cascade of
classifiers or limiting the list of possi-
ble ordering outcomes. We report exper-
iments in translating English to Japanese
and Korean, demonstrating superior per-
formance as (a) the number of crossing
links drops by more than 10% absolute
with respect to other state-of-the-art pre-
ordering approaches, (b) BLEU scores im-
prove on 2.2 points over the baseline with
lexicalised reordering model, and (c) de-
coding can be carried out 80 times faster.
1 Introduction
Source-side preordering for translation is the task
of rearranging the order of a given source sen-
tence so that it best resembles the order of the tar-
get sentence. It is a divide-and-conquer strategy
aiming to decouple long-range word movement
from the core translation task. The main advan-
tage is that translation becomes computationally
cheaper as less word movement needs to be con-
sidered, which results in faster and better transla-
tions, if preordering is done well and efficiently.
Preordering also can facilitate better estimation
of alignment and translation models as the paral-
lel data becomes more monotonically-aligned, and
?
This work was done during an internship of the first au-
thor at SDL Research, Cambridge.
translation gains can be obtained for various sys-
tem architectures, e.g. phrase-based, hierarchical
phrase-based, etc.
For these reasons, preordering has a clear re-
search and commercial interest, as reflected by the
extensive previous work on the subject (see Sec-
tion 2). From these approaches, we are particu-
larly interested in those that (i) involve little or no
human intervention, (ii) require limited computa-
tional resources at runtime, and (iii) make use of
available linguistic analysis tools.
In this paper we propose a novel preordering
approach based on a logistic regression model
trained to predict whether to swap nodes in
the source-side dependency tree. For each pair
of sibling nodes in the tree, the model uses a
feature-rich representation that includes lexical
cues to make relative reordering predictions be-
tween them. Given these predictions, we conduct
a depth-first branch-and-bound search through
the space of possible permutations of all sibling
nodes, using the regression scores to guide the
search. This approach has multiple advantages.
First, the search for permutations is efficient and
does not require specific heuristics or hard limits
for nodes with many children. Second, the inclu-
sion of the regression prediction directly into the
search allows for finer-grained global decisions as
the predictions that the model is more confident
about are preferred. Finally, the use of a single
regression model to handle any number of child
nodes avoids incurring sparsity issues, while al-
lowing the integration of a vast number of features
into the preordering model.
We empirically contrast our proposed method
against another preordering approach based on
automatically-extracted rules when translating En-
glish into Japanese and Korean. We demonstrate
a significant reduction in number of crossing links
of more than 10% absolute, as well as translation
gains of over 2.2 BLEU points over the baseline.
239
We also show it outperforms a multi-class classifi-
cation approach and analyse why this is the case.
2 Related work
One useful way to organize previous preordering
techniques is by how they incorporate linguistic
knowledge.
On one end of the spectrum we find those ap-
proaches that rely on syntactic parsers and hu-
man knowledge, typically encoded via a set of
hand-crafted rules for parse tree rewriting or trans-
formation. Examples of these can be found
for French-English (Xia and McCord, 2004),
German-English (Collins et al., 2005), Chinese-
English (Wang et al., 2007), English-Arabic (Badr
et al., 2009), English-Hindi (Ramanathan et al.,
2009), English-Korean (Hong et al., 2009), and
English-Japanese (Lee et al., 2010; Isozaki et
al., 2010). A generic set of rules for transform-
ing SVO to SOV languages has also been de-
scribed (Xu et al., 2009). The main advantage of
these approaches is that a relatively small set of
good rules can yield significant improvements in
translation. The common criticism they receive is
that they are language-specific.
On the other end of the spectrum, there are pre-
ordering models that rely neither on human knowl-
edge nor on syntactic analysis, but only on word
alignments. One such approach is to form a cas-
cade of two translation systems, where the first
one translates the source to its preordered ver-
sion (Costa-juss`a and Fonollosa, 2006). Alterna-
tively, one can define models that assign a cost to
the relative position of each pair of words in the
sentence, and search for the sequence that opti-
mizes the global score as a linear ordering prob-
lem (Tromble and Eisner, 2009) or as a travel-
ing salesman problem (Visweswariah et al., 2011).
Yet another line of work attempts to automatically
induce a parse tree and a preordering model from
word alignments (DeNero and Uszkoreit, 2011;
Neubig et al., 2012). These approaches are at-
tractive due to their minimal reliance on linguistic
knowledge. However, their findings reveal that the
best performance is obtained when using human-
aligned data which is expensive to create.
Somewhere in the middle of the spectrum are
works that rely on automatic source-language syn-
tactic parses, but no direct human intervention.
Preordering rules can be automatically extracted
from word alignments and constituent trees (Li
et al., 2007; Habash, 2007; Visweswariah et
al., 2010), dependency trees (Genzel, 2010) or
predicate-argument structures (Wu et al., 2011),
or simply part-of-speech sequences (Crego and
Mari?no, 2006; Rottmann and Vogel, 2007). Rules
are assigned a cost based on Maximum En-
tropy (Li et al., 2007) or Maximum Likelihood es-
timation (Visweswariah et al., 2010), or directly
on their ability to make the training corpus more
monotonic (Genzel, 2010). The latter performs
very well in practice but comes at the cost of a
brute-force extraction heuristic that cannot incor-
porate lexical information. Recently, other ap-
proaches treat ordering the children of a node as
a learning to rank (Yang et al., 2012) or discrimi-
native multi-classification task (Lerner and Petrov,
2013). These are appealing for their use of finer-
grained lexical information, but they struggle to
adequately handle nodes with multiple children.
Our approach is closely related to this latter
work, as we are interested in feature-rich discrim-
inative approaches that automatically learn pre-
ordering rules from source-side dependency trees.
Similarly to Yang et al. (2012) we train a large
discriminative linear model, but rather than model
each child?s position in an ordered list of children,
we model a more natural pair-wise swap / no-swap
preference (like Tromble and Eisner (2009) did at
the word level). We then incorporate this model
into a global, efficient branch-and-bound search
through the space of permutations. In this way, we
avoid an error-prone cascade of classifiers or any
limit on the possible ordering outcomes (Lerner
and Petrov, 2013).
3 Preordering using logistic regression
and branch-and-bound search
Like Genzel (2010), our method starts with depen-
dency parses of source sentences (which we con-
vert to shallow constituent trees; see Figure 1 for
an example), and reorders the source text by per-
muting sibling nodes in the parse tree. For each
non-terminal node, we first apply a logistic regres-
sion model which predicts, for each pair of child
nodes, the probability that they should be swapped
or kept in their original order. We then apply
a depth-first branch-and-bound search to find the
global optimal reordering of children.
240
VB
he
NN
1
could
MD
2
stand
VB
3
NN
4
the
DT
smell
NN
nsubj
aux
HEAD
dobj
det HEAD
Figure 1: Shallow constituent tree generated from
the dependency tree. Non-terminal nodes inherit
the tag from the head.
3.1 Logistic regression
We build a regression model that assigns a prob-
ability of swapping any two sibling nodes, a and
b, in the source-side dependency tree. The proba-
bility of swapping them is denoted p(a, b) and the
probability of keeping them in their original order
is 1 ? p(a, b). We use LIBLINEAR (Fan et al.,
2008) for training an L1-regularised logistic re-
gression model based on positively and negatively
labelled samples.
3.1.1 Training data
We generate training examples for the logistic re-
gression from word-aligned parallel data which is
annotated with source-side dependency trees. For
each non-terminal node, we extract all possible
pairs of child nodes. For each pair, we obtain a
binary label y ? {?1, 1} by calculating whether
swapping the two nodes would reduce the number
of crossing alignment links. The crossing score of
having two nodes a and b in the given order is
cs(a, b) := |{(i, j) ? A
a
?A
b
: i > j}|
where A
a
and A
b
are the target-side positions to
which the words spanned by a and b are aligned.
The label is then given as
y(a, b) =
{
1 , cs(a, b) > cs(b, a)
?1 , cs(b, a) > cs(a, b)
Instances for which cs(a, b) = cs(b, a) are not
included in the training data. This usually happens
if either A
a
or A
b
is empty, and in this case the
alignments provide no indication of which order
is better. We also discard any samples from nodes
that have more than 16 children, as these are rare
cases that often result from parsing errors.

1
2 3 4
2 3
2
2
1
. . .
. . .
Figure 2: Branch-and-bound search: Partial search
space of permutations for a dependency tree node
with four children. The gray node marks a goal
node. For the root node of the tree in Figure 1, the
permutation corresponding to this path (1,4,3,2)
would produce ?he the smell stand could?.
3.1.2 Features
Using a machine learning setup allows us to in-
corporate fine-grained information in the form of
features. We use the following features to charac-
terise pairs of nodes:
l The dependency labels of each node
t The part-of-speech tags of each node.
hw The head words and classes of each node.
lm, rm The left-most and right-most words and classes
of a node.
dst The distances between each node and the head.
gap If there is a gap between nodes, the left-most
and right-most words and classes in the gap.
In order to keep the size of our feature space
manageable, we only consider features which oc-
cur at least 5 times
1
. For the lexical features, we
use the top 100 vocabulary items from our training
data, and 51 clusters generated by mkcls (Och,
1999). Similarly to previous work (Genzel, 2010;
Yang et al., 2012), we also explore feature con-
junctions. For the tag and label classes, we gen-
erate all possible combinations up to a given size.
For the lexical and distance features, we explicitly
specify conjunctions with the tag and label fea-
tures. Results for various feature configurations
are discussed in Section 4.3.1.
3.2 Search
For each non-terminal node in the source-side de-
pendency tree, we search for the best possible
1
Additional feature selection is achieved through L1-
regularisation.
241
permutation of its children. We define the score
of a permutation pi as the product of the proba-
bilities of its node pair orientations (swapped or
unswapped):
score(pi) =
?
1?i<j?k|pi[i]>pi[j]
p(i, j)
?
?
1?i<j?k|pi[i]<pi[j]
1? p(i, j)
Here, we represent a permutation pi of k nodes
as a k-length sequence containing each integer in
{1, ..., k} exactly once. Define a partial permu-
tation of k nodes as a k
?
< k length sequence
containing each integer in {1, ..., k} at most once.
We can construct a search space over partial per-
mutations in the natural way (see Figure 2). The
root node represents the empty sequence  and has
score 1. Then, given a search node representing
a k
?
-length partial permutation pi
?
, its successor
nodes are obtained by extending it by one element:
score(pi
?
? ?i?) = score(pi
?
)
?
?
j?V |i>j
p(i, j)
?
?
j?V |i<j
1? p(i, j)
where V = {1, ..., k}\(pi
?
? ?i?) is the set of source
child positions that have not yet been visited. Ob-
serve that the nodes at search depth k correspond
exactly to the set of complete permutations. To
search this space, we employ depth-first branch-
and-bound (Balas and Toth, 1983) as our search
algorithm. The idea of branch-and-bound is to
remember the best scoring goal node found thus
far, abandoning any partial paths that cannot lead
to a better scoring goal node. Algorithm 1 gives
pseudocode for the algorithm
2
. If the initial bound
(bound
0
) is set to 0, the search is guaranteed to
find the optimal solution. By raising the bound,
which acts as an under-estimate of the best scor-
ing permutation, search can be faster but possibly
fail to find any solution. All our experiments were
done with bound
0
= 0, i.e. exact search, but we
discuss search time in detail and pruning alterna-
tives in Section 4.3.2.
Since we use a logistic regression model and in-
corporate its predictions directly as swap probabil-
ities, our search prefers those permutations with
swaps which the model is more confident about.
2
See (Poole and Mackworth, 2010) for more details and a
worked example.
Algorithm 1 Depth-first branch-and-bound
Require: k: maximum sequence length, : empty sequence,
bound
0
: initial bound
procedure BNBSEARCH(, bound
0
, k)
best path? ?
bound? bound
0
SEARCH(??)
return best path
end procedure
procedure SEARCH(pi
?
)
if score(pi
?
) > bound then
if |pi
?
| = k then
best path? ?pi
?
?
bound? score(pi
?
)
return
else
for each i ? {1, ..., k}\pi
?
do
SEARCH(pi
?
? ?i?)
end for
end if
end if
end procedure
4 Experiments
4.1 Setup
We report translation results in English-to-
Japanese/Korean. Our corpora are comprised of
generic parallel data extracted from the web, with
some documents extracted manually and some au-
tomatically crawled. Both have about 6M sentence
pairs and roughly 100M words per language.
The dev and test sets are also generic. Source
sentences were extracted from the web and one
target reference was produced by a bilingual
speaker. These sentences were chosen to evenly
represent 10 domains, including world news,
chat/SMS, health, sport, science, business, and
others. The dev/test sets contain 602/903 sen-
tences and 14K/20K words each. We do English
part-of-speech tagging using SVMTool (Gim?enez
and M`arquez, 2004) and dependency parsing us-
ing MaltParser (Nivre et al., 2007).
For translation experiments, we use a phrase-
based decoder that incorporates a set of standard
features and a hierarchical reordering model (Gal-
ley and Manning, 2008) with weights tuned us-
ing MERT to optimize the character-based BLEU
score on the dev set. The Japanese and Korean lan-
guage models are 5-grams estimated on > 350M
words of generic web text.
For training the logistic regression model, we
automatically align the parallel training data and
intersect the source-to-target and target-to-source
alignments. We reserve a random 5K-sentence
242
approach EJ cs (%) EK cs (%)
rule-based (Genzel, 2010) 61.9 64.2
multi-class 65.2 -
df-bnb 51.4 51.8
Table 1: Percentage of the original crossing score
on the heldout set, obtained after applying each
preordering approach in English-Japanese (EJ,
left) and Korean (EK, right). Lower is better.
subset for intrinsic evaluation of preordering, and
use the remainder for model parameter estimation.
We evaluate our preordering approach with lo-
gistic regression and depth-first branch-and-bound
search (in short, ?df-bnb?) both in terms of reorder-
ing via crossing score reduction on the heldout set,
and in terms of translation quality as measured by
character-based BLEU on the test set.
4.2 Preordering baselines
We contrast our work against two data-driven pre-
ordering approaches. First, we implemented the
rule-based approach of Genzel (2010) and opti-
mised its multiple parameters for our task. We
report only the best results achieved, which corre-
spond to using ?100K training sentences for rule
extraction, applying a sliding window width of 3
children, and creating rule sequences of?60 rules.
This approach cannot incorporate lexical features
as that would make the brute-force rule extraction
algorithm unmanageable.
We also implemented a multi-class classifica-
tion setup where we directly predict complete per-
mutations of children nodes using multi-class clas-
sification (Lerner and Petrov, 2013). While this
is straightforward for small numbers of children,
it leads to a very large number of possible per-
mutations for larger sets of children nodes, mak-
ing classification too difficult. While Lerner and
Petrov (2013) use a cascade of classifiers and im-
pose a hard limit on the possible reordering out-
comes to solve this, we follow Genzel?s heuristic:
rather than looking at the complete set of children,
we apply a sliding window of size 3 starting from
the left, and make classification/reordering deci-
sions for each window separately. Since the win-
dows overlap, decisions made for the first window
affect the order of nodes in the second window,
etc. We address this by soliciting decisions from
the classifier on the fly as we preorder. One lim-
Figure 3: Crossing scores and classification accu-
racy improve with training data size.
itation of this approach is that it is able to move
children only within the window. We try to rem-
edy this by applying the method iteratively, each
time re-training the classifier on the preordered
data from the previous run.
4.3 Crossing score
We now report contrastive results in the intrin-
sic preordering task, as measured by the num-
ber of crossing links (Genzel, 2010; Yang et al.,
2012) on the 5K held-out set. Without preorder-
ing, there is an average of 22.2 crossing links in
English-Japanese and 20.2 in English-Korean. Ta-
ble 1 shows what percentage of these links re-
main after applying each preordering approach to
the data. We find that the ?df-bnb? method out-
performs the other approaches in both language
pairs, achieving more than 10 additional percent-
age points reduction over the rule-based approach.
Interestingly, the multi-class approach is not able
to match the rule-based approach despite using ad-
ditional lexical cues. We hypothesise that this is
due to the sliding window heuristic, which causes
a mismatch in train-test conditions: while samples
are not independent of each other at test time due
to window overlaps, they are considered to be so
when training the classifier.
4.3.1 Impact of training size and feature
configuration
We now report the effects of feature configura-
tion and training data size for the English-Japanese
case. We assess our ?df-bnb? approach in terms of
the classification accuracy of the trained logistic
243
features used acc (%) cs (%)
l,t,hw,lm,rm,dst,gap 82.43 51.3
l,t,hw,lm,rm,dst 82.44 51.4
l,t,hw,lm,rm 82.32 53.1
l,t,hw 82.02 55
l,t 81.07 58.4
Table 2: Ablation tests showing crossing scores
and classification accuracy as features are re-
moved. All models were trained on 8M samples.
regression model (using it to predict ?1 labels in
the held-out set) and by the percentage of crossing
alignment links reduced by preordering.
Figure 3 shows the performance of the logistic
regression model over different training set sizes,
extracted from the training corpus as described in
Section 3. We observe a constant increase in pre-
diction accuracy, mirrored by a steady decrease in
crossing score. However, gains are less for more
than 8M training examples. Note that a small vari-
ation in accuracy can produce a large variation in
crossing score if two nodes are swapped which
have a large number of crossing alignments.
Table 2 shows an ablation test for various fea-
ture configurations. We start with all features, in-
cluding head word and class (hw), left-most and
right-most word in each node?s span (lm, rm), each
node?s distance to the head (dst), and left-most
and right-most word of the gap between nodes
(gap). We then proceed by removing features to
end with only label and tag features (l,t), as in
Genzel (2010). For each configuration, we gener-
ated all tag- and label- combinations of size 2. We
then specified combinations between tag and label
and all other features. For the lexical features we
always used conjunctions of the word itself, and its
class. Class information is included for all words,
not just those in the top 100 vocabulary. Table 2
shows that lexical and distance feature groups con-
tribute to prediction accuracy and crossing score,
except for the gap features, which we omit from
further experiments.
4.3.2 Run time
We now demonstrate the efficiency of branch-and-
bound search for the problem of finding the opti-
mum permutation of n children at runtime. Even
though in the worst case the search could ex-
plore all n! permutations, making it prohibitive for
Figure 4: Average number of nodes explored in
branch-and-bound search by number of children.
nodes with many children, in practice this does
not happen. Many low-scoring paths are discarded
early by branch-and-bound search so that the opti-
mal solution can be found quickly. The top curve
in Figure 4 shows the average number of nodes
explored in searches run on our validation set (5K
sentences) as a function of the number of children.
All instances are far from the worst case
3
.
In our experiments, the time needed to conduct
exact search (bound
0
= 0) was not a problem ex-
cept for a few bad cases (nodes with more than 16
children), which we simply chose not to preorder;
in our data, 90% of the nodes have less than 6 chil-
dren, while only 0.9% have 10 children or more, so
this omission does not affect performance notice-
ably. We verified this on our held-out set, by car-
rying out exhaustive searches. We found that not
preordering nodes with 16 children did not worsen
the crossing score. In fact, setting a harsher limit
of 10 nodes would still produce a crossing score
of 51.9%, compared to the best score of 51.4%.
There are various ways to speed up the search,
if needed. First, one could impose a hard limit
on the number of explored nodes
4
. As shown
in Figure 4, a limit of 4K would still allow ex-
act search on average for permutations of up to
11 children, while stopping search early for more
children. We tested this for limits of 1K/4K nodes
and obtained crossing scores of 51.9/51.5%. Al-
ternatively, one could define a higher initial bound;
since the score of a path is a product of proba-
bilities, one would select a threshold probability
3
Note that 12!?479M nodes, whereas our search finds the
optimal permutation path after exploring <10K nodes.
4
As long as the limit exceeds the permutation length, a
solution will always be found as search is depth-first.
244
d approach ?LRM ? +LRM ?
baseline 25.39 - 26.62 -
rule-based 25.93 +0.54 27.65 +1.03
10
multi-class 25.60 +0.21 26.10 ?0.52
df-bnb 26.73 +1.34 28.09 +1.47
baseline 25.07 - 25.92 -
rule-based 26.35 +1.28 27.54 +1.62
4
multi-class 25.37 +0.30 26.31 +0.39
df-bnb 26.98 +1.91 28.13 +2.21
Table 3: English-Japanese BLEU scores with var-
ious preordering approaches (and improvement
over baseline) under two distortion limits d. Re-
sults reported both excluding and including lexi-
calised reordering model features (LRM).
p and calculate a bound depending on the size n
of the permutation as bound
0
= p
n?(n?1)
2
. Exam-
ples of this would be the lower curves of Figure 4.
The curve labels show the crossing score produced
with each threshold, and in parenthesis the per-
centage of searches that fail to find a solution with
a better score than bound
0
, in which case children
are left in their original order. As shown, this strat-
egy proves less effective than simply limiting the
number of explored nodes, because the more fre-
quent cases with less children remain unaffected.
4.4 Translation performance
Table 3 reports English-Japanese translation re-
sults for two different values of the distortion limit
d, i.e. the maximum number of source words that
the decoder is allowed to jump during search. We
draw the following conclusions. Firstly, all the
preordering approaches outperform the baseline
and the BLEU score gain they provide increases as
the distortion limit decreases. This is further anal-
ysed in Figure 5, where we report BLEU as a func-
tion of the distortion limit in decoding for both
English-Japanese and English-Korean. This re-
veals the power of preordering as a targeted strat-
egy to obtain high performance at fast decoding
times, since d can be drastically reduced with-
out performance degradation which leads to huge
decoding speed-ups; this is consistent with the
observations in (Xu et al., 2009; Genzel, 2010;
Visweswariah et al., 2011). We also find that with
preordering it is possible to apply harsher pruning
conditions in decoding while still maintaining the
Figure 5: BLEU scores as a function of distor-
tion limit in decoder (+LRM case). Top: English-
Japanese. Bottom: English-Korean.
exact same performance, achieving further speed-
ups. With preordering, our system is able to de-
code 80 times faster while producing translation
output of the same quality.
Secondly, we observe that the preordering
gains, which are correlated with the crossing score
reductions of Table 1, are largely orthogonal to
the gains obtained when incorporating a lexi-
calised reordering model (LRM). In fact, preorder-
ing gains are slightly larger with LRM, suggest-
ing that this reordering model can be better esti-
mated with preordered text. This echoes the notion
that reordering models are particularly sensitive
to alignment noise (DeNero and Uszkoreit, 2011;
Neubig et al., 2012; Visweswariah et al., 2013),
and that a ?more monotonic? training corpus leads
to better translation models.
Finally, ?df-bnb? outperforms all other preorder-
ing approaches, and achieves an extra 0.5?0.8
BLEU over the rule-based one even at zero distor-
tion limit. This is consistent with the substantial
crossing score reductions reported in Section 4.3.
We argue that these improvements are due to
the usage of lexical features to facilitate finer-
grained ordering decisions, and to our better
search through the children permutation space
which is not restricted by sliding windows, does
245
E
x
a
m
p
l
e
1
reference [
1
?????]
Barlow
[
2
???]
the smell
[
3
??]
endure
[
4
??????]
could
[
5
???]
hoped
[
6
?]
source [
1
Barlow] [
5
hoped] he [
4
could] [
3
stand] [
2
the smell] [
6
.]
preordered [
1
Barlow] he [
2
the smell] [
3
stand] [
4
could] [
5
hoped] [
6
.]
E
x
a
m
p
l
e
2
reference [
1
????]
my own
[
2
??]
experience
[
3
????]
in
, [
4
???????]
Rosa Parks
[
5
???]
called
[
6
???]
black
[
7
???]
woman
, [
8
???]
one day
[
9
????????]
somehow
[
10
???]
bus of
[
11
?????]
back seat in
[
12
??]
sit
??? [
13
????]
told being
[
14
???]
of
[
15
?????]
was fed up with
?
source [
3
In] [
1
my own] [
2
experience] , a [
6
black] [
7
woman] [
5
named] [
4
Rosa Parks] [
14
was just tired] [
8
one day]
[
14
of] [
13
being told] [
12
to sit] [
11
in the back] [
10
of the bus] .
rule-based [
1
my own] [
2
experience] [
3
In] [
14
was just tired] [
13
being told] [
10
the bus of] [
11
the back in] [
12
sit to] [
14
of]
[
8
one day] , [
6
a black] [
7
woman] [
4
Rosa Parks] [
5
named] .
df-bnb [
1
my own] [
2
experience] [
3
In] , [
5
named] [
6
a black] [
7
woman] [
4
Rosa Parks] [
10
the bus of] [
11
the back in]
[
12
sit to] [
13
told being] [
14
of] [
8
one day] [
14
was just tired] .
E
x
a
m
p
l
e
3
reference [
1
????]
we
?[
2
????]
quite
[
3
???]
Xi?an
[
4
??]
like
[
5
?]
to
[
6
?????]
come have
?
source [
1
we] [
6
have come] [
5
to] [
2
quite] [
4
like] [
1
xi?an] .
rule-based [
1
we] [
2
quite] [
4
like] [
3
xi?an] [
5
to] [
6
come have] .
df-bnb [
1
we] have [
2
quite] [
3
xi?an] [
4
like] [
5
to] [
6
come] .
baseline ????????????????
rule-based ??????????????????
df-bnb ????????????????
Table 4: Examples from our test data illustrating the differences between the preordering approaches.
not depend heavily on getting the right decision
in a multi-class scenario, and which incorporates
regression to carry out a score-driven search.
4.5 Analysis
Table 4 gives three English-Japanese examples
to illustrate the different preordering approaches.
The first, very short, example is preordered cor-
rectly by the rule-based and the df-bnb approach,
as the order of the brackets matches the order of
the Japanese reference.
For longer sentences we see more differences
between approaches, as illustrated by Example 2.
In this case, both approaches succeed at moving
prepositions to the back of the phrase (?my expe-
rience in?, ?the bus of?). However, while the df-
bnb approach correctly moves the predicate of the
second clause (?was just tired?) to the back, the
rule-based approach incorrectly moves the subject
(?a black woman named Rosa Parks?) to this posi-
tion - possibly because of the verb ?named? which
occurs in the phrase. This could be an indication
that the df-bnb is better suited for more compli-
cated constructions. With the exception of phrases
4 and 8, all other phrases are in the correct order
in the df-bnb reordering. None of the approaches
manage to reorder ?a black woman named Rosa
Parks? to the correct order.
Example 3 shows that the translations into
Japanese also reflect preordering quality. The
original source results in ?like? being translated
as the main verb (which is incorrectly interpreted
as ?to be like, to be equal to?). The rule-based
version correctly moves ?have come? to the end,
but fails to swap ?xi?an? and ?like?, resulting in
?come? being interpreted as a full verb, rather than
an auxiliary. Only the df-bnb version achieves al-
most perfect reordering, resulting in the correct
word choice of ?? (to get to, to become) for
?have come to?.
5
5 Conclusion
We have presented a novel preordering approach
that estimates a preference for swapping or not
swapping pairs of children nodes in the source-
side dependency tree by training a feature-rich
logistic regression model. Given the pair-wise
scores, we efficiently search through the space
of possible children permutations using depth-first
branch-and-bound search. The approach is able
to incorporate large numbers of features includ-
ing lexical cues, is efficient at runtime even with
a large number of children, and proves superior to
other state-of-the-art preordering approaches both
in terms of crossing score and translation perfor-
mance.
5
This translation is still not perfect, since it uses the wrong
level of politeness, an important distinction in Japanese.
246
References
Ibrahim Badr, Rabih Zbib, and James Glass. 2009.
Syntactic Phrase Reordering for English-to-Arabic
Statistical Machine Translation. In Proceedings of
EACL, pages 86?93, Athens, Greece.
Egon Balas and Paolo Toth. 1983. Branch and
Bound Methods for the Traveling Salesman Prob-
lem. Carnegie-Mellon Univ. Pittsburgh PA Manage-
ment Sciences Research Group.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan.
Marta R. Costa-juss`a and Jos?e A. R. Fonollosa. 2006.
Statistical Machine Reordering. In Proceedings of
EMNLP, pages 70?76, Sydney, Australia.
Josep M. Crego and Jos?e B. Mari?no. 2006. Integra-
tion of POStag-based Source Reordering into SMT
Decoding by an Extended Search Graph. In Pro-
ceedings of AMTA, pages 29?36, Cambridge, Mas-
sachusetts.
John DeNero and Jakob Uszkoreit. 2011. Inducing
Sentence Structure from Parallel Corpora for Re-
ordering. In Proceedings of EMNLP, pages 193?
203, Edinburgh, Scotland, UK.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of EMNLP, pages 847?
855, Honolulu, Hawaii.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of COLING, pages 376?384,
Beijing, China.
Jes?us Gim?enez and Llu??s M`arquez. 2004. SVMTool:
A general POS tagger generator based on Support
Vector Machines. In Proceedings of LREC, Lisbon,
Portugal.
Nizar Habash. 2007. Syntactic Preprocessing for Sta-
tistical Machine Translation. In Proceedings of MT-
Summit, pages 215?222, Copenhagen, Denmark.
Gumwon Hong, Seung-Wook Lee, and Hae-Chang
Rim. 2009. Bridging Morpho-Syntactic Gap be-
tween Source and Target Sentences for English-
Korean Statistical Machine Translation. In Proceed-
ings of ACL-IJCNLP, pages 233?236, Suntec, Sin-
gapore.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A Simple Re-
ordering Rule for SOV Languages. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 244?251, Up-
psala, Sweden.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.
2010. Constituent Reordering and Syntax Models
for English-to-Japanese Statistical Machine Trans-
lation. In Proceedings of COLING, pages 626?634,
Beijing, China.
Uri Lerner and Slav Petrov. 2013. Source-Side Clas-
sifier Preordering for Machine Translation. In Pro-
ceedings of EMNLP, Seattle, USA.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A Probabilistic
Approach to Syntax-based Reordering for Statistical
Machine Translation. In Proceedings of ACL, pages
720?727, Prague, Czech Republic.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a Discriminative Parser to Optimize
Machine Translation Reordering. In Proceedings of
EMNLP-CoNLL, pages 843?853, Jeju Island, Korea.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
EACL, pages 71?76, Bergen, Norway.
David L. Poole and Alan K. Mackworth. 2010. Ar-
tificial Intelligence: Foundations of Computational
Agents. Cambridge University Press. Full text on-
line at http://artint.info.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and Morphology: Addressing the crux
of the fluency problem in English-Hindi SMT. In
Proceedings of ACL-IJCNLP, pages 800?808, Sun-
tec, Singapore.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
TMI, pages 171?180, Sk?ovde, Sweden.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of EMNLP, pages 1007?1016, Singapore.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with auto-
matically derived rules for improved statistical ma-
chine translation. In Proceedings of COLING, pages
1119?1127, Beijing, China.
247
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for
improved machine translation. In Proceedings of
EMNLP, pages 486?496, Edinburgh, United King-
dom.
Karthik Visweswariah, Mitesh M. Khapra, and Anan-
thakrishnan Ramanathan. 2013. Cut the noise: Mu-
tually reinforcing reordering and alignments for im-
proved machine translation. In Proceedings of ACL,
pages 1275?1284, Sofia, Bulgaria.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese Syntactic Reordering for Statistical
Machine Translation. In Proceedings of EMNLP-
CoNLL, pages 737?745, Prague, Czech Republic.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
Pre-ordering Rules from Predicate-Argument Struc-
tures. In Proceedings of IJCNLP, pages 29?37, Chi-
ang Mai, Thailand.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of COLING,
Geneva, Switzerland.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a Dependency Parser to Improve
SMT for Subject-Object-Verb Languages. In Pro-
ceedings of HTL-NAACL, pages 245?253, Boulder,
Colorado.
Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.
2012. A ranking-based approach to word reordering
for statistical machine translation. In Proceedings of
ACL, pages 912?920, Jeju Island, Korea.
248
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 323?327,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Task Alternation in Parallel Sentence Retrieval for Twitter Translation
Felix Hieber and Laura Jehl and Stefan Riezler
Department of Computational Linguistics
Heidelberg University
69120 Heidelberg, Germany
{jehl,hieber,riezler}@cl.uni-heidelberg.de
Abstract
We present an approach to mine com-
parable data for parallel sentences us-
ing translation-based cross-lingual infor-
mation retrieval (CLIR). By iteratively al-
ternating between the tasks of retrieval
and translation, an initial general-domain
model is allowed to adapt to in-domain
data. Adaptation is done by training the
translation system on a few thousand sen-
tences retrieved in the step before. Our
setup is time- and memory-efficient and of
similar quality as CLIR-based adaptation
on millions of parallel sentences.
1 Introduction
Statistical Machine Translation (SMT) crucially
relies on large amounts of bilingual data (Brown et
al., 1993). Unfortunately sentence-parallel bilin-
gual data are not always available. Various ap-
proaches have been presented to remedy this prob-
lem by mining parallel sentences from comparable
data, for example by using cross-lingual informa-
tion retrieval (CLIR) techniques to retrieve a target
language sentence for a source language sentence
treated as a query. Most such approaches try to
overcome the noise inherent in automatically ex-
tracted parallel data by sheer size. However, find-
ing good quality parallel data from noisy resources
like Twitter requires sophisticated retrieval meth-
ods. Running these methods on millions of queries
and documents can take weeks.
Our method aims to achieve improvements sim-
ilar to large-scale parallel sentence extraction ap-
proaches, while requiring only a fraction of the ex-
tracted data and considerably less computing re-
sources. Our key idea is to extend a straightfor-
ward application of translation-based CLIR to an
iterative method: Instead of attempting to retrieve
in one step as many parallel sentences as possible,
we allow the retrieval model to gradually adapt to
new data by using an SMT model trained on the
freshly retrieved sentence pairs in the translation-
based retrieval step. We alternate between the
tasks of translation-based retrieval of target sen-
tences, and the task of SMT, by re-training the
SMT model on the data that were retrieved in the
previous step. This task alternation is done itera-
tively until the number of newly added pairs stabi-
lizes at a relatively small value.
In our experiments on Arabic-English Twitter
translation, we achieved improvements of over 1
BLEU point over a strong baseline that uses in-
domain data for language modeling and parameter
tuning. Compared to a CLIR-approach which ex-
tracts more than 3 million parallel sentences from
a noisy comparable corpus, our system produces
similar results in terms of BLEU using only about
40 thousand sentences for training in each of a
few iterations, thus being much more time- and
resource-efficient.
2 Related Work
In the terminology of semi-supervised learning
(Abney, 2008), our method resembles self-training
and co-training by training a learning method on
its own predictions. It is different in the aspect of
task alternation: The SMT model trained on re-
trieved sentence pairs is not used for generating
training data, but for scoring noisy parallel data
in a translation-based retrieval setup. Our method
also incorporates aspects of transductive learning
in that candidate sentences used as queries are fil-
tered for out-of-vocabulary (OOV) words and sim-
ilarity to sentences in the development set in or-
der to maximize the impact of translation-based
retrieval.
Our work most closely resembles approaches
that make use of variants of SMT to mine com-
parable corpora for parallel sentences. Recent
work uses word-based translation (Munteanu and
323
Marcu, 2005; Munteanu and Marcu, 2006), full-
sentence translation (Abdul-Rauf and Schwenk,
2009; Uszkoreit et al, 2010), or a sophisticated
interpolation of word-based and contextual trans-
lation of full sentences (Snover et al, 2008; Jehl
et al, 2012; Ture and Lin, 2012) to project source
language sentences into the target language for re-
trieval. The novel aspect of task alternation in-
troduced in this paper can be applied to all ap-
proaches incorporating SMT for sentence retrieval
from comparable data.
For our baseline system we use in-domain lan-
guage models (Bertoldi and Federico, 2009) and
meta-parameter tuning on in-domain development
sets (Koehn and Schroeder, 2007).
3 CLIR for Parallel Sentence Retrieval
3.1 Context-Sensitive Translation for CLIR
Our CLIR model extends the translation-based re-
trieval model of Xu et al (2001). While transla-
tion options in this approach are given by a lexical
translation table, we also select translation options
estimated from the decoder?s n-best list for trans-
lating a particular query. The central idea is to let
the language model choose fluent, context-aware
translations for each query term during decoding.
For mapping source language query terms to
target language query terms, we follow Ture et
al. (2012a; 2012). Given a source language query
Q with query terms qj , we project it into the tar-
get language by representing each source token qj
by its probabilistically weighted translations. The
score of target documentD, given source language
query Q, is computed by calculating the Okapi
BM25 rank (Robertson et al, 1998) over projected
term frequency and document frequency weights
as follows:
score(D|Q) =
|Q|?
j=1
bm25(tf(qj , D), df(qj))
tf(q,D) =
|Tq|?
i=1
tf(ti, D)P (ti|q)
df(q) =
|Tq|?
i=1
df(ti)P (ti|q)
where Tq = {t|P (t|q) > L} is the set of trans-
lation options for query term q with probability
greater than L. Following Ture et al (2012a;
2012) we impose a cumulative thresholdC, so that
only the most probable options are added until C
is reached.
Like Ture et al (2012a; 2012) we achieved best
retrieval performance when translation probabil-
ities are calculated as an interpolation between
(context-free) lexical translation probabilities Plex
estimated on symmetrized word alignments, and
(context-aware) translation probabilities Pnbest es-
timated on the n-best list of an SMT decoder:
P (t|q) = ?Pnbest(t|q) + (1? ?)Plex(t|q) (1)
Pnbest(t|q) is the decoder?s confidence to trans-
late q into t within the context of query Q. Let
ak(t, q) be a function indicating an alignment of
target term t to source term q in the k-th deriva-
tion of query Q. Then we can estimate Pnbest(t|q)
as follows:
Pnbest(t|q) =
?n
k=1 ak(t, q)D(k,Q)?n
k=1 ak(?, q)D(k,Q)
(2)
D(k,Q) is the model score of the k-th derivation
in the n-best list for query Q.
In our work, we use hierarchical phrase-based
translation (Chiang, 2007), as implemented in the
cdec framework (Dyer et al, 2010). This allows
us to extract word alignments between source and
target text for Q from the SCFG rules used in the
derivation. The concept of self-translation is cov-
ered by the decoder?s ability to use pass-through
rules if words or phrases cannot be translated.
3.2 Task Alternation in CLIR
The key idea of our approach is to iteratively al-
ternate between the tasks of retrieval and trans-
lation for efficient mining of parallel sentences.
We allow the initial general-domain CLIR model
to adapt to in-domain data over multiple itera-
tions. Since our set of in-domain queries was
small (see 4.2), we trained an adapted SMT model
on the concatenation of general-domain sentences
and in-domain sentences retrieved in the step be-
fore, rather than working with separate models.
Algorithm 1 shows the iterative task alternation
procedure. In terms of semi-supervised learning,
we can view algorithm 1 as non-persistent as we
do not keep labels/pairs from previous iterations.
We have tried different variations of label persis-
tency but did not find any improvements. A sim-
ilar effect of preventing the SMT model to ?for-
get? general-domain knowledge across iterations
is achieved by mixing models from current and
previous iterations. This is accomplished in two
ways: First, by linearly interpolating the transla-
tion option weights P (t|q) from the current and
324
Algorithm 1 Task Alternation
Require: source language TweetsQsrc, target language TweetsDtrg , general-domain parallel sentences Sgen, general-domain
SMT model Mgen, interpolation parameter ?
procedure TASK-ALTERNATION(Qsrc, Dtrg, Sgen,Mgen, ?)
t? 1
while true do
Sin ? ? . Start with empty parallel in-domain sentences
if t == 1 then
M (t)clir ?Mgen . Start with general-domain SMT model for CLIRelse
M (t)clir ? ?M
(t?1)
smt + (1? ?)M (t)smt . Use mixture of previous and current SMT model for CLIRend if
Sin ? CLIR(Qsrc, Dtrg,M (t)clir) . Retrieve top 1 target language Tweets for each source language query
M (t+1)smt ? TRAIN(Sgen + Sin) . Train SMT model on general-domain and retrieved in-domain data
t? t+ 1
end while
end procedure
BLEU (test) # of in-domain sents
Standard DA 14.05 -
Full-scale CLIR 14.97 3,198,913
Task alternation 15.31 ?40k
Table 1: Standard Domain Adaptation with in-domain LM
and tuning; Full-scale CLIR yielding over 3M in-domain par-
allel sentences; Task alternation (? = 0.1, iteration 7) using
?40k parallel sentences per iteration.
previous model with interpolation parameter ?.
Second, by always using Plex(t|q) weights esti-
mated from word alignments on Sgen.
We experimented with different ways of using
the ranked retrieval results for each query and
found that taking just the highest ranked docu-
ment yielded the best results. This returns one pair
of parallel Twitter messages per query, which are
then used as additional training data for the SMT
model in each iteration.
4 Experiments
4.1 Data
We trained the general domain model Mgen on
data from the NIST evaluation campaign, includ-
ing UN reports, newswire, broadcast news and
blogs. Since we were interested in relative im-
provements rather than absolute performance, we
sampled 1 million parallel sentences Sgen from the
originally over 5.8 million parallel sentences.
We used a large corpus of Twitter messages,
originally created by Jehl et al (2012), as com-
parable in-domain data. Language identification
was carried out with an off-the-shelf tool (Lui and
Baldwin, 2012). We kept only Tweets classified
as Arabic or English with over 95% confidence.
After removing duplicates, we obtained 5.5 mil-
lion Arabic Tweets and 3.7 million English Tweets
(Dtrg). Jehl et al (2012) also supply a set of 1,022
Arabic Tweets with 3 English translations each for
evaluation purposes, which was created by crowd-
sourcing translation on Amazon Mechanical Turk.
We randomly split the parallel sentences into 511
sentences for development and 511 sentences for
testing. All URLs and user names in Tweets were
replaced by common placeholders. Hashtags were
kept, since they might be helpful in the retrieval
step. Since the evaluation data do not contain any
hashtags, URLs or user names, we apply a post-
processing step after decoding in which we re-
move those tokens.
4.2 Transductive Setup
Our method can be considered transductive in two
ways. First, all Twitter data were collected by
keyword-based crawling. Therefore, we can ex-
pect a topical similarity between development, test
and training data. Second, since our setup aims
for speed, we created a small set of queries Qsrc,
consisting of the source side of the evaluation data
and similar Tweets. Similarity was defined by
two criteria: First, we ranked all Arabic Tweets
with respect to their term overlap with the devel-
opment and test Tweets. Smoothed per-sentence
BLEU (Lin and Och, 2004) was used as a similar-
ity metric. OOV-coverage served as a second cri-
terion to remedy the problem of unknown words
in Twitter translation. We first created a general
list of all OOVs in the evaluation data under Mgen
(3,069 out of 7,641 types). For each of the top 100
BLEU-ranked Tweets, we counted OOV-coverage
with respect to the corresponding source Tweet
and the general OOV list. We only kept Tweets
325
0 1 2 3 4 5 6 7 8iteration
14.05
14.97
15.31
16.00
BLE
U (t
est)
(a)
?=0.0?=0.1?=0.5?=0.9
1 2 3 4 5 6 7 8iteration 0
10000
20000
30000
40000
50000
60000
70000
# n
ew p
airs
(b)
?=0.0?=0.1?=0.5?=0.9
Figure 1: Learning curves for varying ? parameters. (a) BLEU scores and (b) number of new pairs added per iteration.
containing at least one OOV term from the corre-
sponding source Tweet and two OOV terms from
the general list, resulting in 65,643 Arabic queries
covering 86% of all OOVs. Our query set Qsrc
performed better (14.76 BLEU) after one iteration
than a similar-sized set of random queries (13.39).
4.3 Experimental Results
We simulated the full-scale retrieval approach by
Jehl et al (2012) with the CLIR model described
in section 3. It took 14 days to run 5.5M Arabic
queries on 3.7M English documents. In contrast,
our iterative approach completed a single iteration
in less than 24 hours.1
In the absence of a Twitter data set for re-
trieval, we selected the parameters ? = 0.6 (eq.1),
L = 0.005 and C = 0.95 in a mate-finding
task on Wikipedia data. The n-best list size for
Pnbest(t|q) was 1000. All SMT models included
a 5-gram language model built from the English
side of the NIST data plus the English side of the
Twitter corpus Dtrg. Word alignments were cre-
ated using GIZA++ (Och and Ney, 2003). Rule
extraction and parameter tuning (MERT) was car-
ried out with cdec, using standard features. We
ran MERT 5 times per iteration, carrying over the
weights which achieved median performance on
the development set to the next iteration.
Table 1 reports median BLEU scores on test of
our standard adaptation baseline, the full-scale re-
trieval approach and the best result from our task
alternation systems. Approximate randomization
tests (Noreen, 1989; Riezler and Maxwell, 2005)
showed that improvements of full-scale retrieval
and task alternation over the baseline were statis-
1Retrieval was done in 4 batches on a Hadoop cluster us-
ing 190 mappers at once.
tically significant. Differences between full-scale
retrieval and task alternation were not significant.2
Figure 1 illustrates the impact of ?, which con-
trols the importance of the previous model com-
pared to the current one, on median BLEU (a) and
change of Sin (b) over iterations. For all ?, few
iterations suffice to reach or surpass full-scale re-
trieval performance. Yet, no run achieved good
performance after one iteration, showing that the
transductive setup must be combined with task al-
ternation to be effective. While we see fluctuations
in BLEU for all ?-values, ? = 0.1 achieves high
scores faster and more consistently, pointing to-
wards selecting a bolder updating strategy. This
is also supported by plot (b), which indicates that
choosing ? = 0.1 leads to faster stabilization in
the pairs added per iteration (Sin). We used this
stabilization as a stopping criterion.
5 Conclusion
We presented a method that makes translation-
based CLIR feasible for mining parallel sentences
from large amounts of comparable data. The key
of our approach is a translation-based high-quality
retrieval model which gradually adapts to the tar-
get domain by iteratively re-training the underly-
ing SMT model on a few thousand parallel sen-
tences retrieved in the step before. The number
of new pairs added per iteration stabilizes to a
few thousand after 7 iterations, yielding an SMT
model that improves 0.35 BLEU points over a
model trained on millions of retrieved pairs.
2Note that our full-scale results are not directly compara-
ble to those of Jehl et al (2012) since our setup uses less than
one fifth of the NIST data, a different decoder, a new CLIR
approach, and a different development and test split.
326
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL?09), Athens, Greece.
Steven Abney. 2008. Semisupervised Learning for
Computational Linguistics. Chapman and Hall.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of the
4th EACL Workshop on Statistical Machine Transla-
tion (WMT?09), Athens, Greece.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2).
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions (ACL?10), Uppsala, Sweden.
Laura Jehl, Felix Hieber, and Stefan Riezler. 2012.
Twitter translation using translation-based cross-
lingual retrieval. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation
(WMT?12), Montreal, Quebec, Canada.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine trans-
lation. In Proceedings of the Second Workshop on
Statistical Machine Translation, Prague, Czech Re-
public.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics
for machine translation. In Proceedings the 20th In-
ternational Conference on Computational Linguis-
tics (COLING?04).
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics, Demo Session
(ACL?12), Jeju, Republic of Korea.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4).
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics (COLING-ACL?06), Sydney,
Australia.
Eric W. Noreen. 1989. Computer Intensive Meth-
ods for Testing Hypotheses. An Introduction. Wiley,
New York.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1).
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL-05 Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, MI.
Stephen E. Robertson, Steve Walker, and Micheline
Hancock-Beaulieu. 1998. Okapi at TREC-7. In
Proceedings of the Seventh Text REtrieval Confer-
ence (TREC-7), Gaithersburg, MD.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?08), Honolulu, Hawaii.
Ferhan Ture and Jimmy Lin. 2012. Why not grab a
free lunch? mining large corpora for parallel sen-
tences to improve translation modeling. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT?12),
Montreal, Canada.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard.
2012. Combining statistical translation techniques
for cross-language information retrieval. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics (COLING?12), Mumbai, India.
Ferhan Ture, Jimmy Lin, and Douglas W. Oard. 2012a.
Looking inside the box: Context-sensitive transla-
tion for cross-language information retrieval. In
Proceedings of the ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR?12), Portland, OR.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel doc-
ument mining for machine translation. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (COLING?10), Beijing,
China.
Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual
information retrieval. In Proceedings of the 24th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR?01), New York, NY.
327
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 410?421,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Twitter Translation using Translation-Based Cross-Lingual Retrieval
Laura Jehl and Felix Hieber and Stefan Riezler
Department of Computational Linguistics
Heidelberg University
69120 Heidelberg, Germany
{jehl,hieber,riezler}@cl.uni-heidelberg.de
Abstract
Microblogging services such as Twitter have
become popular media for real-time user-
created news reporting. Such communica-
tion often happens in parallel in different lan-
guages, e.g., microblog posts related to the
same events of the Arab spring were written
in Arabic and in English. The goal of this
paper is to exploit this parallelism in order
to eliminate the main bottleneck in automatic
Twitter translation, namely the lack of bilin-
gual sentence pairs for training SMT systems.
We show that translation-based cross-lingual
information retrieval can retrieve microblog
messages across languages that are similar
enough to be used to train a standard phrase-
based SMT pipeline. Our method outper-
forms other approaches to domain adaptation
for SMT such as language model adaptation,
meta-parameter tuning, or self-translation.
1 Introduction
Among the various social media platforms, mi-
croblogging services such as Twitter1 have become
popular communication tools. This is due to the easy
accessibility of microblogging platforms via inter-
net or mobile phones, and due to the need for a fast
mode of communication that microblogging satis-
fies: Twitter messages are short (limited to 140 char-
acters) and simultaneous (due to frequent updates by
prolific microbloggers). Twitter users form a social
network by ?following? the updates of other users,
either reciprocal or one-way. The topics discussed
in Twitter messages range from private chatter to im-
portant real-time witness reports.
1http://twitter.com/
Events such as the Arab spring have shown the
power and also the shortcomings of this new mode
of communication. Microblogging services played a
crucial role in quickly spreading the news about im-
portant events, furthermore they were useful in help-
ing organizers plan their protest. The fact that news
on microblogging platforms is sometimes ahead of
newswire is one of the most interesting facets of
this new medium. However, while Twitter messag-
ing is happening in multiple languages, most net-
works of ?friends? and ?followers? are monolingual
and only about 40% of all messages are in English2.
One solution to sharing news quickly and interna-
tionally was crowdsourcing manual translations, for
example at Meedan3, a nonprofit organization built
to share news and opinion between the Arabic and
English speaking world, by translating articles and
blogs, using machine translation and human expert
corrections.
The goal of our research is to automate this trans-
lation process, with a further aim of providing rapid
crosslingual data access for downstream applica-
tions. The automated translation of microblogging
messages is facing two main problems. First, there
are no bilingual sentence pair data from microblog-
ging domains available. Second, the colloquial, non-
standard language of many microblogging messages
makes it very difficult to adapt a machine translation
system trained on any of the available bilingual re-
sources such as transcriptions from political organi-
zations or news text.
The approach presented in this paper aims to ex-
ploit the fact that microblogging often happens in
2http://semiocast.com/publications/2011_
11_24_Arabic_highest_growth_on_Twitter
3http://news.meedan.net
410
parallel in different languages, e.g., microblog posts
related to the same events of the Arab spring were
published in parallel in Arabic and in English. The
central idea is to crawl a large set of topically related
Arabic and English microblogging messages, and
use Arabic microblog messages as search queries in
a cross-lingual information retrieval (CLIR) setup.
We use the probabilistic translation-based retrieval
technique of Xu et al (2001) that naturally inte-
grates translation tables for cross-lingual retrieval.
The retrieval results are then used as input to a stan-
dard SMT pipeline to train translation models, start-
ing from unsupervised induction of word alignments
(Och and Ney, 2000) to phrase-extraction (Och and
Ney, 2004) and phrase-based decoding (Koehn et al,
2007). We investigate several filtering techniques
for retrieval and phrase extraction (Munteanu and
Marcu, 2006; Snover et al, 2008) and find a straight-
forward application of phrase extraction from sym-
metrized alignments to be optimal. Furthermore, we
compare our approach to related domain adaptation
techniques for SMT and find our approach to yield
large improvements over all related techniques.
Finally, a side-product of our research is a cor-
pus of around 1,000 Arabic Twitter messages with
3 manual English translations each, which were cre-
ated using crowdsourcing techniques. This corpus
is used for development and testing in our experi-
ments.
2 Related Work
SMT for user-generated noisy data has been pio-
neered at the 2011 Workshop on Statistical Ma-
chine Translation that featured a translation task of
Haitian Creole emergency SMS messages4. This
task is very similar to the problem of Twitter transla-
tion since SMS contain noisy, abbreviated language.
The research papers related to the featured transla-
tion task deploy several approaches to domain adap-
tation, including crowdsourcing (Hu et al, 2011)
or extraction of parallel sentences from comparable
data (Hewavitharana et al, 2011).
The use of crowdsourcing to evaluate machine
translation and to build development sets was pi-
oneered by Callison-Burch (2009) and Zaidan and
4http://www.statmt.org/wmt11/
featured-translation-task.html
Callison-Burch (2009). Crowdsourcing has its lim-
its when it comes to generating parallel training data
on the scale of millions of parallel sentences. In
our work, we use crowdsourcing via Amazon Me-
chanical Turk5 to create a development and test cor-
pus that includes 3 English translations for each of
around 1,000 Arabic microblog messages.
There is a substantial amount of previous work on
extracting parallel sentences from comparable data
such as newswire text (Fung and Cheung, 2004;
Munteanu and Marcu, 2005; Tillmann and ming Xu,
2009) and on finding parallel phrases in non-parallel
sentences (Munteanu and Marcu, 2006; Quirk et al,
2007; Cettolo et al, 2010; Vogel and Hewavitha-
rana, 2011). The approach that is closest to our
work is that of Munteanu and Marcu (2006): They
use standard information retrieval together with sim-
ple word-based translation for CLIR, and extract
phrases from the retrieval results using a clean bilin-
gual lexicon and an averaging filter. In this ap-
proach, filtering and cleaning techniques in align-
ment and phrase extraction have to compensate for
low-quality retrieval results. In our approach, the fo-
cus is on high-quality retrieval.
As our experimental results show, the main im-
provement of our technique is a decrease in out-of-
vocabulary (OOV) rate at an increase of the per-
centage of correctly translated unigrams and bi-
grams. Similar work on solving domain adaptation
for SMT by mining unseen words has been pre-
sented by Snover et al (2008) and Daum? and Ja-
garlamudi (2011). Both approaches show improve-
ments by adding new phrase tables; however, both
approaches rely on techniques that require larger
comparable texts for mining unseen words. Since
in our case documents are very short (they consist
of 140 character sequences), these techniques are
not applicable. However, the advantage of the fact
that microblog messages resemble sentences is that
we can apply standard word- and phrase-alignment
techniques directly to the retrieval results.
Further approaches to domain adaptation for SMT
include adaptation using in-domain language mod-
els (Bertoldi and Federico, 2009), meta-parameter
tuning on in-domain development sets (Koehn and
Schroeder, 2007), or translation model adaptation
5http://www.turk.com
411
using self-translations of in-domain source language
texts (Ueffing et al, 2007). In our experiments we
compare our approach to these domain adaptation
techniques.
3 Cross-Lingual Retrieval via Statistical
Translation
3.1 Retrieval Model
In our approach, comparable candidates for domain
adaptation are selected via cross-lingual retrieval.
In a probabilistic retrieval framework, we estimate
the probability of a relevant document microblog
message D given a query microblog message Q,
P (D|Q). Following Bayes rule, this can be sim-
plified to ranking documents according to the like-
lihood P (Q|D) if we assume a uniform prior over
documents.
score(Q,D) = P (D|Q) = P (D)P (Q|D)P (Q) (1)
Our model is defined as follows:
score(Q,D) = P (Q|D) =
?
q?Q
P (q|D) (2)
P (q|D) = ?Pmix(q|D)
? ?? ?
mixture model
+(1? ?) PML(q|C)
? ?? ?
query collection backoff
(3)
Pmix(q|D) = ?
?
d?D
T (q|d)PML(d|D)
? ?? ?
translation model
(4)
+(1? ?)PML(q|D)
? ?? ?
self-translation
Our retrieval model is related to monolingual re-
trieval models such as the language-modeling ap-
proach of Ponte and Croft (1998) and the monolin-
gual statistical translation approach of Berger and
Lafferty (1999). Xu et al (2001) extend the former
approaches to the cross-lingual setting by adding a
term translation table. They describe their model in
terms of a Hidden Markov Model with two states
that generate query terms: First, a document state
generates terms d in the document language and then
translates them into a query term q. Second, a back-
off state generates query terms q directly in the query
language. In the document state the probability of
emitting q depends on all d that translate to q, ac-
cording to a translation distribution T . This is esti-
mated by marginalizing out d as
?
d T (q|d)P (d|D).
In the backoff state the probability PML(q|C) of
emitting a query term is estimated as the relative
frequency of this term within a corpus in the query
language. The probability of transitioning into the
document state or the backoff state is given by ? and
1? ?.
We view this model from a smoothing perspective
where the backoff state is linearly interpolated with
the translation probability using a mixture weight
? to control the weighting between both terms.
Furthermore, we expand Xu et al (2001)?s gen-
erative model to incorporate the concept of ?self-
translation?, introduced by Xue et al (2008) in a
monolingual question-answering context: Twitter
messages across languages usually share relevant
terms such as hashtags, named entities or user men-
tions. Therefore, we model the event of a query
term literally occurring in the document in a sepa-
rate model that is itself linearly interpolated with a
parameter ? with the translation model.
We implemented the model based on a Lucene6
index, which allows efficient storage of term-
document and document-term vectors. To mini-
mize retrieval time, we consider only those doc-
uments as retrieval candidates where at least one
term translates to a query term, according to the
translation table T . Stopwords were removed for
both queries and documents. Compared to com-
mon inverted index retrieval implementations, our
model is quite slow since the document-term vectors
have to be loaded. However, multi-threading sup-
port and batch retrieval on a Hadoop cluster made
the model tractable. On the upside, the translation-
based model allows greater precision in finding
the candidates for comparable microblog messages
than simpler approaches that use a combination of
tfidf matching and n-best query term expansion:
The translation-based retrieval exploits all possi-
ble alignments between query and document terms
which is particularly important for short documents
such as microblog messages.
3.2 In-Domain Phrase Extraction
To prepare the extraction of phrases from retrieval
results, we conducted cross-lingual retrieval in both
directions: retrieving Arabic documents using En-
glish microblog messages as queries and vice versa.
6http://lucene.apache.org/core/
412
For each run we kept the top N retrieved documents.
Each document was then paired with its query to
generate pseudo-parallel data.
We tried two approaches for using this data to
improve our translations. The first, more restric-
tive method makes use of the word alignments we
obtained from 5.8 million clean parallel training
data from the NIST evaluation campaign. The re-
trieval step generates word-alignments in the direc-
tion D ? Q. After retrieval, the reverse alignment
for each query-document pair is also generated by
using a translation table in the direction Q ? D. An
alignment point between a query term q and a docu-
ment term d is created, iff T (q|d) or T (d|q) exist in
the translation tables D ? Q or Q ? D. Based on
these word-alignments, we extract phrases by apply-
ing the grow-diag-final-and heuristic and using Och
and Ney (2004)?s phrase extraction algorithm as im-
plemented in Moses7 (Koehn et al, 2007). We con-
ducted experiments using different constraints on
the number of alignment points required for a pair
to be considered as well as the value of N . Our first
technique resembles the technique of Munteanu and
Marcu (2006) who also perform phrase extraction
by combining clean alignment lexica for initial sig-
nals with heuristics to smooth alignments for final
fragment extraction.
While we obtained some gains using our heuris-
tics, we are aware that our method is severely re-
stricted in that it only learns new words which are
in the vicinity of known words. We therefore also
tried the bolder approach of treating our data as
parallel and running unsupervised word alignment8
(Och and Ney, 2000) directly on the query-document
pairs to obtain new world alignments and build a
phrase table. In contrast to previous work (Snover
et al, 2008; Daum? and Jagarlamudi, 2011), we can
take advantage of the sentence-like character of mi-
croblog messages and treat queries and retrieval re-
sults similar to sentence aligned data.
For both extraction methods, the standard five
translation features from the new phrase table
(phrase translation probability and lexical weight-
ing in both directions, phrase penalty) were added to
the translation features in Moses. We tried different
7http://statmt.org/moses/
8http://code.google.com/p/giza-pp/
al-Gaddafi, al-Qaddhafi, assad, babrain, bahrain,
egypt, gadaffi, gaddaffi, gaddafi, Gheddafi, homs,
human rights, human-rights, humanrights, libia, li-
bian, libya, libyan, lybia, lybian, lybya, lybyan,
manama, Misrata, nabeelrajab, nato, oman, Pos-
itiveLibyaTweets, Qaddhafi, sirte, syria, tripoli,
tripolis, yemen;
Table 1: Keywords used for Twitter crawl.
modes of combining new and original phrase table,
namely using either one or using the new phrase ta-
ble as backoff in case no phrase translation is found
in the original phrase table.
4 Data
4.1 Twitter Crawl
We crawled Twitter messages from September 20,
2011 until January 23, 2012 via the Streaming API9
in keyword-tracking mode, obtaining 25.5M Twit-
ter messages (tweets) in various languages. Table 1
shows the list of keywords that were chosen to re-
trieve microblog messages related to the events of
the Arab spring.10
In order to separate the microblog message cor-
pus by languages, we applied a Naive Bayes lan-
guage identifier11. This yielded a distribution with
the six most common languages (of 52) being Ara-
bic (57%), English (33%), Somali (2%), Spanish
(2%), Indonesian (1.5%), German (0.7%). We kept
only microblog messages classified as English or
Arabic with confidence greater 0.9. Keyword-based
crawling creates a strong bias towards the domain
of the keywords and it does not guarantee that all
microblog messages regarding a certain topic or re-
gion are retrieved or that all retrieved messages are
related to the Arab Spring and human righs in the
middle east. Additionally, retweets artificially in-
9https://dev.twitter.com/docs/
streaming-api/
10The Twitter Streaming API allows up to 400 tracking key-
words that are matched to uppercase, lowercase and quoted
variations of the keywords. Partial matching such as ?tripolis?
matching ?tripoli? as well as Arabic Unicode characters are not
supported. We extended our keywords over time by analyzing
the crawl, e.g., by introducing spelling variants and hashtags.
11Language Detection Library for Java, by
Shuyo Nakatani (http://code.google.com/p/
language-detection/).
413
Arabic English
tweets + retweets 14,565,513 8,501,788
tweets 6,614,126 5,129,829
avg. retweet/tweet 11.62 7.27
unique users 180,271 865,202
avg. tweets/user 36.6 5.9
Table 2: Twitter corpus statistics
flate the size of the data, although there are no new
terms added. Therefore, we removed all duplicate
retweets that did not introduce additional terms to
the original tweet. Table 2 explains the shrinkage
of the dataset after removing retweets - compared
to English users, a smaller number of Arabic users
produced a much larger number of retweets. Inter-
estingly, 56,087 users tweet a substantial amount in
both languages. This suggests that users spread mes-
sages simultaneously in Arabic and English.
4.2 Creating a Small Parallel Twitter Corpus
using Crowdsourcing
For the evaluation of our method, a small amount
of parallel in-domain data was required. Since there
are no corpora of translated microblog messages, we
decided to use Amazon Mechanical Turk12 to cre-
ate our own evaluation set, following the exploratory
work of Zaidan and Callison-Burch (2011b). We
randomly selected 2,000 Arabic microblog mes-
sages. Hashtags, user mentions and URLs were re-
moved from each microblog message beforehand,
because they do not need to be translated and would
just artificially inflate scores at test time. The mi-
croblog messages were then manually cleaned and
pruned. We discarded messages which contained
almost no text or large portions of other languages
and removed remaining Twitter markup. In the end,
1,022 microblog messages were used in the Me-
chanical Turk task. We split the data into batches
of ten sentences which comprised one HIT (human
intelligence task). Each HIT had to be completed by
three workers. In order to have some control over
translation quality, we inserted one control sentence
per HIT, taken from the LDC-GALE Phase 1 Arabic
Blog Parallel Text. Turkers were rewarded 10 cents
per translation. Following Zaidan and Callison-
Burch (2011b), all Arabic sentences were converted
12http://www.turk.com
into images in order to prevent turkers from past-
ing them into online machine translation engines.
Our final corpus consists of 1,022 translated mi-
croblog messages with three translations each. An
example containing translations for one of the sen-
tences which we inserted for quality checking pur-
poses, along with the reference translation, is given
in table 3. It can be seen that translators sometimes
made grammar mistakes or odd word choices. They
also tended to omit punctuation marks. However,
translations also contained reasonable translation al-
ternatives (such as ?gathered? or ?collected?). We
also asked translators to insert an ?unknown? token
whenever they were unable to translate a word. Our
HIT setup did not allow workers to skip a sentence,
forcing them to complete an entire batch. In order to
account for translation variants we decided to use all
three translations obtained via Mechanical Turk as
multiple references instead of just keeping the top
translation. We randomly split our small parallel
corpus, using half of the microblog messages for de-
velopment and half for testing.
4.3 Preprocessing
Besides removal of Twitter markup, several addi-
tional preprocessing steps such as digit normaliza-
tion were applied to the data. We also decided to ap-
ply the Buckwalter Arabic transliteration scheme13
to avoid encoding difficulties. Habash and Sadat
(2006) have shown that tokenization is helpful for
translating Arabic. We therefore decided to ap-
ply a more involved tokenization scheme than sim-
ple whitespace splitting to our data. As the re-
trieval relies on translation tables, all data need
to be tokenized the same way. We are aware
of the MADA+TOKAN Arabic morphological an-
alyzer and tokenizer (Habash and Rambow, 2005),
however, this toolkit produces very in-depth analy-
ses of the data and thus led to difficulties when we
tried to scale it to millions of sentences/microblog
messages. That is why we only used MADA for
transliteration and chose to implement the simpler
approach by Lee et al (2003) for tokenization. This
approach only requires a small set of annotated data
to obtain a list of prefixes and suffixes and uses n-
13http://www.qamus.org/transliteration.
htm
414
REFERENCE breaking the silence, a campaign group made up of israeli soldiers, gathered anonymous accounts from 26 soldiers.
TRANSLATION1 and breaking silence is a group of israeli soldiers that had unknown statistics from 26 soldiers israeli
TRANSLATION2 breaking the silence by a group of israeli soldiers who gathered unidentified statistics from 26 israeli soldier.
TRANSLATION3 breaking the silence is a group of israeli soldiers that collected unknown statistics of 26 israeli soldiers
Table 3: Example turker translations.
gram-models to determine the most likely prefix?-
stem-suffix? split of a word.14
5 Twitter Translation Experiments
We conducted a series of experiments to evaluate
our strategy of using CLIR and phrase-extraction to
extract comparable data in the Twitter domain. We
also explored more standard ways of domain adap-
tation such as using English microblog messages to
build an in-domain language model, or generating
synthetic bilingual corpora from monolingual data.
All experiments were conducted using the Moses
machine translation system15 (Koehn et al, 2007)
with standard settings. Language models were
built using the SRILM toolkit16 (Stolcke, 2002).
For all experiments, we report lowercased BLEU-
4 scores (Papineni et al, 2001) as calculated by
Moses? multi-bleu script. For assessing signifi-
cance, we apply the approximate randomization test
(Noreen, 1989; Riezler and Maxwell, 2005). We
consider pairwise differing results scoring a p-value
< 0.05 as significant.
Our baseline model was trained using 5,823,363
million parallel sentences in Modern Standard
Arabic (MSA) (198,500,436 tokens) and English
(193,671,201 tokens) from the NIST evaluation
campaign. This data contains parallel text from dif-
ferent domains, including UN reports, newsgroups,
newswire, broadcast news and weblogs.
5.1 Domain Adaption using Monolingual
Resources
As a first step, we used the available in-domain
data for a combination of domain adaptation tech-
14The n-gram-model required for tokenization was trained on
5.8 million Modern Standard Arabic sentences from the NIST
evaluation campaign. This data had previously been tokenized
with the same method, trained to match the Penn Arabic Tree-
bank, v3.
15http://statmt.org/moses/
16http://www.speech.sri.com/projects/
srilm/
niques similar to Bertoldi and Federico (2009).
There were three different adaptation measures:
First, the turker-generated development set was used
for optimizing the weights of the decoding meta-
parameters, as introduced by Koehn and Schroeder
(2007). Second, the English microblog messages in
our crawl were used to build an in-domain language
model. This adaptation technique was first proposed
by Zhao et al (2004). Third, the Arabic portion of
our crawl was used to synthetically generate addi-
tional parallel training data. This was accomplished
by machine-translating the Arabic microblog mes-
sages with the best system after performing the first
two adaptation steps. Since decoding is very time-
intensive, only 1 million randomly selected Ara-
bic microblog messages were used to generate syn-
thetic parallel data. This new data was then used
to train another phrase table. Such self-translation
techniques have been introduced by Ueffing et al
(2007). All results were evaluated against a base-
line of using only NIST data for translation model,
language model and weight optimization.
Our results are shown in table 4. Using an in-
domain development set while leaving everything
else untouched led to an improvement of approxi-
mately 1 BLEU point. Three experiments involv-
ing the Twitter language model confirm Bertoldi
and Federico (2009)?s findings that the language
model was most helpful. The BLEU-score could
be improved by 1.5 to 2 points in all experiments.
When using an in-domain language model, there
was no significant difference between deploying an
in-domain or out-of-domain development set. We
also compared the effect of using only the in-domain
language model to that of adding the in-domain
language model as an extra feature while keeping
the NIST language model.17 There was no signif-
17The weights for both language models were optimized
along with all other translation feature weights, rather than run-
ning an extra optimization step to interpolate between both lan-
guage models, since Koehn and Schroeder (2007) showed that
415
Run Translation Model Language Model Dev Set BLEU %
1 NIST NIST NIST 13.90
2 NIST NIST Twitter 14.83?
3 NIST Twitter NIST 15.98?
4 NIST Twitter Twitter 15.68?
5 NIST Twitter & NIST Twitter 16.04?
6 self-train Twitter & NIST Twitter 15.79?
7 self-train & NIST Twitter & NIST Twitter 15.94?
Table 4: Domain adaptation experiments. Asterisks indicate significant improvements over baseline (1).
Run Twitter Phrases extraction method # sentence pairs # extracted phrases BLEU %
8 top 3 retrieval results heuristics 14,855,985 6,508,141 17.04?
9 top 1 retrieval results GIZA++ 5,141,065 54,260,537 18.73??
10 retrieval intersection GIZA++ 3,452,566 29,091,009 18.85??
11 retrieval intersection as backoff GIZA++ 3,452,566 29,091,009 18.93??
Table 5: CLIR domain adaptation experiments. All weights were optimized on the Twitter dev set and used
the Twitter and NIST language models. One Asterisk indicates a significant improvement over baseline run
(5) from table 4. Two Asterisks indicate a significant improvement over run (8).
icant difference between both runs. However, for
further adaptation experiments we used the system
with the highest absolute BLEU score. In our case,
using synthetically generated data was not help-
ful, yielding similar results as the language model
experiments above. As has been observed before
by Bertoldi and Federico (2009), it did not matter
whether the synthetic data were used on their own
or in addition to the original training data.
5.2 Domain Adaptation using
Translation-based CLIR
Meta-parameters ?, ? ? [0, 1] of the retrieval model
were tuned in a mate-finding experiment: Mate-
finding refers to the task of retrieving the single rel-
evant document for a query. In our case, each source
tweet in the crowdsourced development set had ex-
actly one ?mate?, namely the crowdsourced transla-
tion that was ranked best in a further crowdsourced
ranking task. Using the retrieval model described
in section 3 we achieved precision@1 scores above
95% in finding the translations of a tweet when ?
and ? were set to 0.9. We fixed these parameter set-
tings for all following experiments. The translation
table was taken from the baseline experiments in ta-
ble 4. During retrieval, we kept up to 10 highest
scoring documents per query.
both strategies yielded the same results.
We first employed heuristic phrase extraction
based on the word alignments generated from the
NIST data as described above. To avoid learning
too much noise, maximum phrase length was re-
stricted to 3 (the default is 7). To evaluate the effects
of choosing more restrictive or more lax settings,
we ran experiments varying the following configu-
rations:
1. Constraints on alignment points:
? no constraints,
? 3+ alignment points in each direction,
? 3+ alignment points in both directions,
? 5+ alignment points in both directions.
2. Constraints on retrieval ranking:
? top 10 results,
? top 3 results,
? top 1 results,
? retrieval intersection (results found in both
retrieval directions)
We obtained improvements for all combinations
of these configurations. However, we observed that
requiring 5 common alignment points was too strict,
since few pairs met this constraint. We also noticed
that using only the top 3 retrieval results was benefi-
cial to performance, suggesting that more compara-
ble microblog messages were indeed ranked higher.
416
Using extraction heuristics we gained maximally 1.0
BLEU using the top 3 retrieval results and requiring
at least 3 alignment points in both alignment direc-
tions (see first line in table 5). However, other con-
figurations produced very similar results.
While heuristics led to small incremental im-
provements, we achieved a much larger improve-
ment by training a new phrase table from scratch us-
ing GIZA++. Again, we restricted maximum phrase
length to 3 words. In order to keep phrase table
size manageable, we had to restrict retrieval to top-
1 results or only use retrieval results in the inter-
section of retrieval directions. Best results are ob-
tained when combining phrase tables extracted from
GIZA++ alignments in the intersection of retrieval
results with NIST phrase tables in backoff mode (see
last line in table 5).
6 Error Analysis
Our cross-lingual retrieval approach succeeded in
finding nearly parallel tweets, confirming our hy-
pothesis that such data actually exists. Examples are
given in table 6.
Table 7 shows a more detailed breakdown of our
translation scores. First, standard adaptation meth-
ods increased n-gram precision, suggesting that us-
ing in-domain adaptation data caused the system to
choose more suitable words. As expected, there was
no reduction in OOVs, since using an in-domain
language model and development set does not in-
troduce new vocabulary. Heuristic phrase extrac-
tion again produced small improvements in n-gram
precision while reducing the number of unknown
words. Learning a new phrase table with GIZA++
produced substantial improvements both in OOV-
rate and in n-gram precision.
Nevertheless, even the scores of the adapted sys-
tem are still fairly low and translation quality as
judged by inspection of the output can be very poor.
This suggests that the language used on Twitter still
poses a great challenge, due to its variety of styles
as well as the users? tendency to use non-standard
spelling and colloquial or dialectal expressions. Our
development set contained many different genres,
from Qu?ran verses over news headlines to personal
chatter. Another difficulty was posed by dialectal
Arabic content. To gain an impression of the amount
of dialectal content in our data, we used the Arabic
Online Commentary Dataset created by Zaidan and
Callison-Burch (2011a) to classify our test set. Ta-
ble 8 shows the distribution of dialects in our test
data according to language model probability. This
distribution should be viewed with a grain of salt,
since the shortness of tweets might cause unreliable
results when using a model based on word frequen-
cies for classification. Still, the results suggest that
there is a high proportion of dialectal content and
spelling variation in our data, causing a large num-
ber of OOVs. For example, the preposition ??,
meaning ?in? is often written as Y?. Our phrase
table trained only on standard Arabic data as well as
our extraction heuristic failed to translate this fre-
quently occurring word. Only when retraining a
phrase table with GIZA++ did we translate it cor-
rectly.
Dialect # Sentences
Egyptian 141
Levantine 147
Gulf 78
Modern Standard Arabic 145
Table 8: Dialectal content in our test set as classified
by the AOC dataset.
Table 9 gives examples of translations generated
using different adaptation methods in comparison to
the references and the Google translation service to
illustrate strengths and weaknesses of our approach.
Example 1 shows a case where unknown words were
learned through translation model adaptation. Note
that even the Google translator did not recognize
the word ?ys? which was transliterated as
?Msellat?. Zaidan and Callison-Burch (2011a) point
out that dialectal variants are often transliterated
by Google. Note also, that the unadapted transla-
tion erroneously translated the place name ?sitra? as
?jacket?, a mistake which was also made in two of
the references and by Google. The same happened
to the place name ?wadyan?, which could also be
taken as meaning ?and religions?. This error was
enforced by our preprocessing step incorrectly split-
ting off the prefix ?w? which often carries the mean-
ing ?and?. In addition to that, the two runs which
used translation model adaptation each dropped a
part of the input sentence (?in sitra?, ?firing?). We
417
ARABIC TWEET fO?  Y?  ?yybyl?   w?d?? ??AyF ?? @q?  ?  d??? ?s?rf?  Hy?r?  
 ?  
GOOGLE TRANSLATION AFP confirms that the French President Gaddafi Libyans tried to call and forgiveness
ENGLISH TWEET french president assures that will be taken to court and tells the libyans to forgive each other
ARABIC TWEET Hym?  ??   ? rO? Y? ?wmm?  A?rJ ?ym ??C ? A?E Crq? ?AO?  ?y\n EAh
GOOGLE TRANSLATION NTRA decide to increase the number of all mobile operators in Egypt a commencement from Thursday
ENGLISH TWEET ntra decide to increase the number of all mobile operators in starting from thursday
ARABIC TWEET ?CA? ?lV ??rV ?? r?An? ?w? dm  Yl? ?y?  dyhK? 
GOOGLE TRANSLATION Shahid Amin AA Day January through gunshot
ENGLISH TWEET martyr amin ali ahmed on jan by gunshot
Table 6: Examples of nearly parallel tweets found by our retrieval method.
Adaptation method OOV-rate %/absolute unigram precision %/absolute bigram precision %/absolute output length (words)
None 22.56/2216 51.1/5020 20.2/1882 9832
LM and Dev 20.05/2220 51.4/5442 22.1/2227 10595
Retrieval (heuristic) 17.47/1790 53.5/5484 23.6/2299 10246
Retrieval (GIZA++) 4.22/439 56.1/5834 26.1/2575 10395
Table 7: OOV-rate and precision for different adaptation methods.
attribute this to that fact that the phrase table extrac-
tion often produced one-to-many alignments when
only one alignment point was known. In Example 2
GIZA++ extraction clearly outperformed heuristic
phrase extraction. This example also shows that our
method is good at learning proper names. While
the first two examples resemble news text, Exam-
ple 3 is a more informal message. It is particularly
interesting to note that with GIZA++ extraction the
term ?shabiha? is learned, which is commonly used
in Syria to mean ?thugs? and specifically refers to
armed civilians who assault protesters against Bashir
Al-Assad?s regime. Example 4 also shows substan-
tial OOV reduction. However, the term ? rtns
 r??  (?in Opera Central?, the location of Telecom
Egypt) is incorrectly translated as ?really opera?.
7 Conclusion
We presented an approach to translation of mi-
croblog messages from the Twitter domain. The
main obstacle to state-of-the-art SMT of such data
is the complete lack of sentence-parallel training
data. We presented a technique that uses translation-
based CLIR to find relevant Arabic Twitter messages
given English Twitter queries, and applies a standard
pipeline for unsupervised training of phrase-based
SMT to retrieval results. We found this straight-
forward technique to outperform more conservative
techniques to extract phrases from comparable data
and also to outperform techniques using monolin-
gual resources for language model adaptation, meta-
parameter tuning, or self-translation.
The greatest benefit of our approach is a signifi-
cant reduction of OOV terms at a simultaneous im-
provement of correct unigram and bigram transla-
tions. Despite this positive net effect, we still find
a considerable amount of noise in the automati-
cally extracted phrase tables. Noise reduction by
improved pre-processing and by more sophisticated
training will be subject to future work. Furthermore,
we would like to investigate a tighter integration of
CLIR and SMT training by using forced decoding
techniques for CLIR and by a integrating a feedback
loop into retrieval and training.
Acknowledgments
We would like to thank Julia Ostertag for several it-
erations of manual error analysis of Arabic transla-
tion output.
418
EXAMPLE 1
SRC ?w?d?  ?ys? ?lW? Tlrt? ?A?  ? ?tq 	?K?   w? ?rtF
GOOGLE Riot troops stormed the jacket and religions foot and launches Msellat tears
NO ADAPTATION jacket riot forces storm and religions foot ?ys? ?lW? tears
LM AND DEV sitra and religions of the foot of the riot forces storm ?ys? ?lW? tears
RETRIEVAL (HEURISTIC) in sitra riot police storming and religions of tear gas on foot
RETRIEVAL (GIZA++) the riot police stormed and religions of the foot firing tear gas
REF0 vest riot forces break into wadyan by foot and trough gas tear
REF1 sotra the riot forces enter on foot and shoot tear bombs
REF2 the cover for riot police enters wadian walking and shoot tear bombs
EXAMPLE 2
SRC Yq?w`?  ?tq? ?? ?wy?  dtyF A?A?
GOOGLE Obama will speak today the death of al-Awlaki
NO ADAPTATION dtyF A?A? today killed Yq?w`? 
LM AND DEV dtyF A?A? friday for the killing of Yq?w`? 
RETRIEVAL (HEURISTIC) A?A? today on the killing of
RETRIEVAL (GIZA++) obama today on the al awlaki killing
REF0 obama will talk today about the killing of al - awlaki
REF1 obama is talking today about el awlaqi death
REF2 obama will speak today about the killing of al - awlaqi
EXAMPLE 3
SRC (: ?wy?ts? ?Am ?? TybK? 
GOOGLE Cbihh in Hama are crying :)
NO ADAPTATION TybK?  mired in calling for help : )
LM AND DEV TybK?  in hama calling for help : )
RETRIEVAL (HEURISTIC) inside the protectors of the calling for help : )
RETRIEVAL (GIZA++) shabiha in hama calling for help : )
REF0 the gangsters in hama are asking for help
REF1 the gangs in hamah are peading :)
REF2 the thugs in hama are calling for help :)
EXAMPLE 4
SRC  r??  ? rtns T?r? Y? T?rK?  Hy?C ??zt? ?AO?? T?rOm?A ?wl?A? :: ???r?
GOOGLE Freedom :: Telecom Egypt workers holding company?s president in a room Psontral Opera
NO ADAPTATION : : free workers ?AO?? T?rOm?A holding company chairman  r??  ? rtns Y? chamber
LM AND DEV : : workers free ?AO?? T?rOm?A holding company chairman Y? r??  ? rtns room
RETRIEVAL (HEURISTIC) free : : afcd T?rOm?A hold ceo hostage ppl is the president of the chamber of  r??  ? rtns
RETRIEVAL (GIZA++) egypt : : workers telecom workers are holding the head of the company in the chamber of really opera
REF0 freedom :: workers in the egyptian for communication are holding the company president in a room in the opera central
REF1 freedom , workers in egypt for calls detain the head of the company in a room in opera central
REF2 hurriya :: workers in telecom egypt detaining the president of the company in a room in the opera central
Table 9: Example output using different adaptation methods.
References
Adam Berger and John Lafferty. 1999. Information re-
trieval as statistical translation. In Proceedings of the
22nd ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval (SIGIR?99), Berkeley,
CA.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the 4th
EACL Workshop on Statistical Machine Translation,
Athens, Greece.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: Evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?09), Singapore.
M. Cettolo, M. Federico, and N. Bertoldi. 2010. Min-
ing parallel fragments from comparable texts. In Pro-
ceedings of the 7th International Workshop on Spoken
419
Language Translation, Paris, France.
Hal Daum? and Jagadeesh Jagarlamudi. 2011. Domain
adaptation for machine translation by mining unseen
words. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT?11), Portland,
OR.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon ex-
traction via bootstrapping and EM. In Proceedings of
the 2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP?04), Barcelona, Spain.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics (ACL?05), Ann Arbor, MI.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the Human Language Technology Con-
ference - North American Chapter of the Association
for Computational Linguistics annual meeting (HLT-
NAACL?06), New York, NY.
Sanjika Hewavitharana, Nguyen Bach, Qin Gao, Vamshi
Ambati, and Stephan Vogel. 2011. CMU haitian
creole-english translation system for WMT 2011. In
Proceedings of the 6th Workshop on Statistical Ma-
chine Translation, Edinburgh, Scotland, UK.
Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Ei-
delman, Olivia Buzek, and Benjamin B. Bederson.
2011. The value of monolingual crowdsourcing in
a real-world translation scenario: Simulation using
haitian creole emergency SMS messages. In Proceed-
ings of the 6th Workshop on Statistical Machine Trans-
lation, Edinburgh, Scotland, UK.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, Prague, Czech Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Birch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the ACL 2007 Demo and Poster Sessions,
Prague, Czech Republic.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based arabic word segmentation. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics (ACL?03), Sapporo, Japan.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics (COLING-ACL?06), Sydney, Aus-
tralia.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL?00), Hongkong, China.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Report
IBM Research Division Technical Report, RC22176
(W0190-022), Yorktown Heights, N.Y.
Jay M. Ponte and Bruce W. Croft. 1998. A language
modeling approach to information retrieval. Proceed-
ings of the 21st annual international ACM SIGIR con-
ference on Research and development in information
retrieval (SIGIR?98).
Chris Quirk, Raghavendra Udupa U, and Arul Menezes.
2007. Generative models of noisy translations with
applications to parallel fragment extraction. In Pro-
ceedings of MT Summit XI, Copenhagen , Denmark.
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL-05 Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, MI.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation us-
ing comparable corpora. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP?08), Honolulu, Hawaii.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, Denver,
CO.
Christoph Tillmann and Jian ming Xu. 2009. A sim-
ple sentence-level extraction algorithm for comparable
data. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
420
can Chapter of the Association for Computational Lin-
guistic (NAACL-HLT?09), Boulder, CO.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics
(ACL?07), Prague, Czech Republic.
Stephan Vogel and Sanjika Hewavitharana. 2011. Ex-
tracting parallel phrases from comparable data. In
Proceedings of the 4th Workshop on Building and Us-
ing Comparable Corpora: Comparable Corpora and
the Web, Portland, OR.
Jinxi Xu, Ralph Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual in-
formation retrieval. In Proceedings of the 24th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR?01),
New York, NY.
Xiaobing Xue, Jiwoon Jeon, and Bruce Croft. 2008. Re-
trieval models for question and answer archives. In
Proceedings of the 31st Annual International ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval (SIGIR?08), Singapore.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasi-
bility of human-in-the-loop minimum error rate train-
ing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP?09), Singapore.
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The arabic online commentary dataset: an annotated
dataset of informal arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?11), Port-
land, OR.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?11), Portland, OR.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics (COLING?04), Geneva, Switzer-
land.
421
