Linguistic Interpretation of Emotions for Affect Sensing from Text 
Mostafa Al Masum Shaikh 
Dept. of Information and 
Communication Engineering 
University of Tokyo 
7-3-1 Hongo, Bunkyo-Ku 
113-8656 Tokyo, Japan 
almasum@gmail.com 
Helmut Prendinger 
Digital Contents and Media 
Sciences Research Division 
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda Ku
101-8430 Tokyo, Japan 
helmut@nii.ac.jp 
Mitsuru Ishizuka 
Dept. of Information and 
Communication Engineering
University of Tokyo 
7-3-1 Hongo, Bunkyo-Ku 
113-8656 Tokyo, Japan 
ishizuka@ieee.org 
 
Abstract 
Several approaches have already been em-
ployed to ?sense? affective information from 
text, but none of those ever considered the 
cognitive and appraisal structure of individ-
ual emotions. Hence this paper aims at inter-
preting the cognitive theory of emotions 
known as the OCC emotion model, from a 
linguistic standpoint. The paper provides 
rules for the OCC emotion types for the task 
of sensing affective information from text. 
Since the OCC emotions are associated with 
several cognitive variables, we explain how 
the values could be assigned to those by ana-
lyzing and processing natural language com-
ponents. Empirical results indicate that our 
system outperforms another state-of-the-art 
system.   
1 Introduction and Motivation 
While various conceptual models, computational 
methods, techniques, and tools are reported in 
(Shanahan et. al., 2006), we argue that the current 
work for sensing the affect communicated by text 
is incomplete and often gives inaccurate results. It 
is true that the assessment of affective content is 
inevitably subjective and subject to considerable 
disagreement. Yet the interest in sentiment or af-
fect based text categorization is increasing with the 
large amount of text becoming available on the 
Internet. A brief discussion on available ap-
proaches is given in (Shaikh et. al., 2007a; Liu et. 
al., 2003). For example, keyword spotting, lexical 
affinity, statistical and hand crafted approaches 
target affective lexicons which are not sufficient to 
recognize affective information from text, because 
according to a linguistic survey (Pennebaker et. al., 
2003), only 4% of words used in written texts carry 
affective content. 
In this paper we consider the contextual-
valenced based approached (i.e., SenseNet) as dis-
cussed by Shaikh et. al., (2007a, 2007b) and con-
sider their SenseNet as the basis of our know-
ledgebase. For simplicity, we use the words ?sen-
timent? and ?opinion? synonymously and consider 
sentiment sensing as the prior task of ?affect? or 
?emotion? sensing. The SenseNet can sense either 
positive or negative ?sentiment?, but it cannot clas-
sify different emotions. Therefore, this paper ex-
plains how the SenseNet can be employed to sense 
emotions from text. So the primary focus of this 
paper is to provide a set of rules for emotions char-
acterized by the OCC (Ortony et. al., 1988) emo-
tion model and discuss how the rules are imple-
mented.  
2 Affect Sensing from Text 
2.1 Extending Valence Assignment Approach 
for Emotions Classification 
For the task of affect sensing from text we should 
incorporate both commonsense knowledge and 
cognitive structure of emotions along with the se-
mantic interpretation of the words used in a sen-
tence. We have chosen the OCC model of emo-
tions for this task. The rule-based definition of the 
OCC emotion types characterized by a rich set of 
linguistic tokens makes it appropriate to cope with 
the valence assignment approach for affect sensing 
from text.   
2.2 Characterization of OCC Emotions  
The OCC emotion types can be characterized by 
appropriate rules interplaying with several vari-
ables. There are two kinds of variables, namely, 
895
emotion inducing variables (event, agent and ob-
ject based variables) and emotion intensity vari-
ables. The event-based variables are calculated 
with respect to the event which is usually a verb-
object pair found in the sentence. For example, the 
sentence, John bought Mary an ice-cream, gives 
an event as ?buy, ice-cream?. The variables are 
enlisted in Table 1. In general we call them ?emo-
tion variables?. 
Type Variable Name 
agent_fondness (af) agent based 
cognitive_strength (cs) 
object_fondness (of) object based 
object_appealing (oa) 
self_reaction (sr) 
self_presumption (sp) 
other_presumption (op) 
prospect (pros) 
status (stat) 
unexpectedness (unexp) 
self_appraisal (sa) 
event based 
valenced_reaction (vr) 
event_deservingness (ed) 
effort_of_action (eoa) 
expected_deviation (edev) 
intensity  
event_familiarity (ef) 
        Table 1. OCC emotion variables 
 
The OCC emotion model specifies 22 emotion 
types and 2 cognitive states. For example, OCC 
model literally defines ?Happy-for? as ?Pleased 
about a Desirable event for a Liked agent?, and 
?Fear? as ?Displeased about Negative Prospect of 
an Undesirable Unconfirmed event?. Our goal is to 
represent these literal definitions by rules inter-
playing with the emotion variables so that the sys-
tem can evaluate and get either a ?true? or ?false? 
value. For example, we have an input text txt, that 
has an agent a, associated with an event e, and we 
have a program entity x that detects emotion from 
txt. We can now represent the rule for ?Happy-for? 
emotion as, x senses ?Happy-for? if the following 
condition holds. 
[Linguisitc_Token_found_for_HappyFor(txt) and 
No_Negation_Found (txt)] or [vr =True and sr (e, 
txt) = ?Pleased? and op(e, txt) = ?Desirable? and  
af (x, txt) = ?Liked? and cs (a,x) = ?Other?]    
 
3 Implementation of the Rules 
In this section, we first briefly discuss about the 
SenseNet and its different linguistic resources. 
Then we explain the ?emotion variables?, their 
enumerated values and how the values are assigned 
to the respective variables. 
3.1 SenseNet  
Semantic Parser. The SenseNet has imple-
mented a semantic parser using Machinese Syntax 
(Connexor Oy, 2005) that produces XML-
formatted syntactic output for an input text. For 
example, the sentence, ?My mother presented me a 
nice wrist watch on my birthday and made deli-
cious pancakes.?, the output of the semantic parser 
is shown in Table 2.  
 
Triplet Output of Semantic Parser 
Triplet 1 [['Subject Name:', 'mother', 'Subject 
Type:', 'Person', 'Subject Attrib:', 
['PRON PERS GEN SG1:i']], ['Action 
Name:', 'present', 'Action Status:', 
'Past ', 'Action Attrib:', ['time: my 
birthday', 'Dependency: and']], ['Ob-
ject Name:', 'watch', 'Object Type:', 'N 
NOM SG', 'Object Attrib:', ['Deter-
miner: a', 'A ABS: nice', 'N NOM SG: 
wrist', 'Goal: i']]] 
Triplet 2 [['Subject Name:', 'mother', 'Subject 
Type:', 'Person', 'Subject Attrib:', []], 
['Action Name:', 'make', 'Action 
Status:', 'Past ', 'Action Attrib:', []], 
['Object Name:', 'pancake', 'Object 
Type:', 'N NOM PL', 'Object Attrib:', 
['A ABS: delicious']]] 
Table2.  Semantic Verb-Frames outputted by Se-
mantic Parser 
  
Semantic parser outputs each semantic verb-frame 
of a sentence as a triplet of ?subject, verb, and ob-
ject?. Hence, one obtains multiple triplets if the 
parser encounters multiple verbs in a sentence. In 
our case, we consider each triplet to indicate an 
event encoding the information about ?who is do-
ing what and how?. Therefore, the output given in 
Table 2 has two events, which are dependent to 
each other as indicated by ?dependency? keyword 
in the action attribute of Triplet 1. 
Valenced Output. SenseNet is the implementa-
tion of contextual valence based approach that 
deals with semantic relationships of the words in a 
sentence and assign contextual-valence using a set 
of rules and prior-valence of the words. It outputs a 
numerical value ranging from -15 to +15 flagged 
as the ?sentence-valence? for each input sentence. 
896
For examples, SenseNet outputs -11.158 and 
+10.973 for the inputs, ?The attack killed three 
innocent civilians.? and ?It is difficult to take bad 
photo with this camera.?, respectively. These val-
ues indicate a numerical measure of negative or 
positive sentiments carried by the sentences.  
Scored-list of Action, Adjective, and Adverb. 
SenseNet has initially assigned prior-valence to 
928 verbs, 948 adjectives and 144 adverbs by 
manual investigations of eight judges where the 
inter-agreement among the judges are reported as 
reliable (i.e., the Kappa value is 0.914). The judges 
have manually counted the number of positive and 
negative senses of each word of a selected list ac-
cording to the contextual explanations of each 
sense found in WordNet 2.1. A database of words 
with prior-valence assigned using Equations (1) to 
(3) is developed and scores are stored in the scale 
of -5 to 5. 
Prior-Valence = Average (((Positive-Sense 
Count ? Negative-Sense Count)/Total Sense 
Count) * 5.0) 
(1)
Prospect Polarity = if (Positive-Sense Count > 
Negative-Sense Count) then 1 else -1  
Prospective Valence = Average(max(Positive-
Sense Count, Negative-Sense Count)/Total 
Sense Count) * 5.0*Prospect Polarity) 
(2)
Praiseworthy Valence = Average (Prior-
Valence + Prospective Valence) 
(3)
 
Scored-list of Nouns. SenseNet does an auto-
matic approach to assign prior-valence to nouns by 
employing ConceptNet (Liu and Singh, 2004). A 
value between -5 to 5 is assigned as the valence for 
an unrated noun or concept as follows. To assign a 
prior-valence to a concept, the system collects all 
semantically connected entities that ConceptNet 
returns for the input concept. For example, to get 
the prior-valence for the noun ?rocket?, the system 
failed to find it in the existing knowledgebase, but 
from the action list of the concept the system re-
turned the value 4.112 by averaging the scores of 
the verbs ?carry (4.438)?, ?contain (4.167)?, ?fly 
(3.036)?, ?launch (5.00)? and ?go (3.917)?. 
3.2 Assigning Values to the Emotion Variables 
According to the OCC model, the values for the 
variables self_presumption (sp) and self_reaction 
(sr) are ?Desirable? or ?Undesirable?, and 
?Pleased? or ?Displeased? respectively. For exam-
ple, for the events ?buy ice-cream?, ?present wrist 
watch?, ?kill innocent civilians? referred in the 
example sentences  SenseNet returns contextual 
valence as +7.832, +8.817 and -8.458, respec-
tively. According to SenseNet scoring system the 
valence range for an event (i.e., verb, object pair) 
is ?10. Thereby we decide that for an event if the 
valence is positive (i.e., ?buy ice-cream?), sp and 
sr are set as ?Desirable? and ?Pleased?, and in the 
case of negative valence (i.e., ?Kill innocent civil-
ian?) sp and sr are set to ?Undesirable? and ?Dis-
pleased?, respectively.  
The values for other_presumption (op) could be 
set ?Desirable? or ?Undesirable?. For the sentence 
?A terrorist escaped from the Jail?, the value for 
op (for the event ?escape from jail?) is presumably 
?Desirable? for the agent ?terrorist? but it gets 
?Undesirable? and ?Displeased? for sp and sr be-
cause of negative valence (i.e., -6.715) of the 
event. From SenseNet we get the valence for ter-
rorist as -3.620. Thus in this case we set op as ?De-
sirable? because of having a negative valenced 
event associated with a negative valenced agent. 
Similarly we have the following simple rules to 
assign the values to op. 
? If a positive valenced event is associated with 
a positive valenced agent, op is set ?Desir-
able?. e.g., the Teacher was awarded the best-
teacher award. [(teacher, +4.167) , (award 
best-teacher award, +8.741)]  
? If a negative valenced event is associated with 
a positive valenced agent, op is set ?Undesir-
able?. e.g., the employee was sacked from the 
job.  [(employee, +3.445), (sack from job, -
6.981)]   
? If a positive valenced event is associated with 
a negative valenced agent, op is set ?Undesir-
able?. e.g., the criminal was punished for the 
crime. [(criminal,-3.095), (punish for crime, 
+5.591)] 
In this context and in accordance to the OCC 
model, the value for cognitive_strength (cs) indi-
cates how closely the computer program considers 
selfness. This value is set as ?Self? if the agent de-
scribed in the text is a first person (i.e., I or We); 
otherwise it is set as ?Other?. For the sentence, ?I 
wish I could win the lottery.?, cs is set ?Self?, but 
for the sentence, ?Susan won the million dollar 
lottery.?, cs is set ?Other?. 
According to the OCC model, prospect of an 
event involves a conscious expectation that it will 
897
occur in the future, and the value for the variable 
prospect (pros) can be either ?Positive? or ?Nega-
tive?. In the aforementioned equation (2), Sense-
Net considers either the positive or negative sense-
count (whichever is the maximum for a verb) to 
calculate ?prospective valence? with the notion of 
semantic orientation towards optimistic-pessimistic 
scale. In order to assign pros value to an event we 
also consider the ?prospective valence? of the verb 
instead of ?prior-valence? of that verb. Thus ?posi-
tive? or ?negative? is assigned according to a cer-
tain threshold (i.e., ?3.5) for ?positive? or ?nega-
tive? valence obtained for that event. For example, 
the events ?admit into university?, ?kill innocent 
people?, ?do it?, SenseNet returns  +9.375, -8.728, 
+2.921, respectively and according to this valence, 
pros of the events is set to ?positive?, ?negative? 
and ?null?, respectively.  
The variable status (stat) has the values like: 
?Unconfirmed?, ?Confirmed? and ?Disconfirmed?. 
We decide if the tense of the verb is present or fu-
ture, the value is set to ?Unconfirmed? (e.g., I am 
trying to solve it.); and if it is past or modal with-
out a negation, stat is set ?Confirmed? (e.g., I suc-
ceeded.), but with a negation, stat is set ?Discon-
firmed? (e.g., I did not succeed.). 
If the valence of the agent/object is positive, 
?Liked? is set to the variables agent_fondness (af) 
and object_fondness (of) variables, otherwise 
?Not-liked? is set. For example, for the sentences, 
?The hero appeared to save the girl.?, and ?A ter-
rorist escaped from the Jail?, af for ?hero? and 
?terrorist? is set to ?Liked? and ?Not-Liked? be-
cause of positive and negative valence. Similarly, 
of is set ?Liked? and ?Not-Liked? for ?girl? and 
?Jail? respectively.  
The value for self_appraisal (sa) can be either 
?Praiseworthy? or ?Blameworthy?. In the afore-
mentioned equation (3) SenseNet takes the average 
of ?Prior Valence? and ?Prospective Valence? of a 
verb with the notion of average semantic orienta-
tion of the verb from both good-bad and optimis-
tic-pessimistic perspective. Like assigning pros 
value to an event we consider the ?praiseworthy 
valence? of the verb to assign value to sa. Thereby 
for the same events discussed above to explain 
pros assignment, the value for sa is set ?Praisewor-
thy?, ?Blameworthy? and ?null?, respectively. 
The value of object_appealing (oa) indicates 
whether an object is ?Attractive? or ?Unattractive?. 
In order to assign a value to oa, we deal with two 
scores (i.e., object valence, and familiarity valence) 
having the following heuristic. ?Attractive? is set if 
the object has a positive valence with a familiarity 
valence less than a certain threshold. Reversely 
?Unattractive? is set if the object has a negative 
valence with a familiarity valence above a certain 
threshold. The familiarity valence is obtained from 
the ConceptNet by calculating the percentage of 
nodes (out of 300,000 concept-nodes) linking to 
and from the given object/concept. For example, 
the familiarity valence for ?restaurant?, ?thief? and 
?diamond ring? is 0.242%, 0.120% and 0.013%, 
respectively. Heuristically we kept the threshold 
0.10% to signal familiarity and unfamiliarity of an 
object. Thus ?diamond ring? and ?thief? gets ?At-
tractive? and ?Unattractive? set for oa, but ?restau-
rant? gets ?null? accordingly. 
The value for valenced_reaction (vr) is set either 
?True? or ?False? in order to initiate further analy-
sis to sense emotions or decide the sentence(s) as 
expressing a neutral emotion. We consider vr to be 
?True? if the ?sentence-valence? returned by Sen-
seNet is either above than 3.5 or less than -3.5. For 
example, ?I go.?, doesn?t lead to further process-
ing (i.e., sentence-valence is +3.250) but ?I go to 
gym everyday.?, leads to classify emotion because 
of the sentence-valence +7.351 obtained from Sen-
seNet. The value to the variable unexpectedness 
(unexp) is set ?true? if there is a linguistic token to 
represent suddenness (e.g., abruptly, suddenly, 
swiftly etc.) in the input sentence, otherwise 
?false? is set. We have a list of such tokens to indi-
cate suddenness.  
OCC model has several variables to signify emo-
tional intensity. For example, the value for the in-
tensity variable event_deservingness (ed) is set 
?High? for an event having a higher positive va-
lence (i.e., above +7.0) or ?Low? for higher nega-
tive one (i.e., less than -7.0). If an action is quali-
fied with an adverb (e.g., He worked very hard) or 
target object qualified with an adjective (e.g., I am 
looking for a quiet place) without a negation, the 
value for effort_of_action (eoa) is set ?Obvious?, 
otherwise ?Not-Obvious?. Another variable called 
expected_deviation (edev) indicates the difference 
between the event and its actor. For example, in 
the sentence ?The police caught the criminal fi-
nally.?, the actor ?police? and the event ?catch 
criminal? don?t deviate because the action is pre-
sumably expected by the actor. We set the value 
for edev to ?Low? if ConceptNet can find any se-
898
mantic relationship between the actor and event; 
otherwise ?High? is set. For example, for sentence 
?the student invented the theory.?, edev is set 
?High? because ConceptNet doesn?t return any 
relationship between ?student? and ?invent?. The 
values ?Common? or ?Uncommon? are set for 
event_familiarity (ef) according to the familiarity 
valence obtained from ConceptNet for the input 
event as discussed before. 
4.3 The rules for the OCC Emotion Types 
In section 2.2 we briefly illustrated how a rule 
for the OCC defined emotion (e.g., happy-for) is 
characterized. Now using the same notion we enlist 
the rules for the OCC model defined emotion 
types. Although in txt there might be multiple e 
described and we also deal with such cases to get 
the resultant emotion types from txt, but we don?t 
discuss that in the scope of this paper and describe 
the simple cases. Thus, the rules for emotion types 
are given considering an event e, for example, the 
program x senses ?Joy? for e if following condition 
is true: 
 [Linguisitc_Token_found_for_Joy(txt) and 
No_Negation_Found (txt)] or [vr= true and sr= 
?Pleased? and sp= ?Desirable? and cs= ?Self?] 
(i.e., literally joy means being ?pleased about a de-
sirable event?.) Since we have the token words for 
each emotion types, we omit the first condition in 
the subsequent rules for space limitations. The 
rules for the emotion are listed as following and 
due to space limitations we are not providing the 
rules for all the emotions. 
? if (vr= true and sr= ?Pleased? and pros= ?Posi-
tive? and sp= ?Desirable? and status= ?Uncon-
firmed?), ?hope? is true. 
? if (vr= true and sr = ?Displeased? and pros= 
?Negative? and sp= ?Undesirable? and 
status=?Unconfirmed?), ?fear? is true. 
? if (vr= true and sr = ?Pleased? and pros= 
?Negative? and sp= ?Undesirable? and status= 
?Disconfirmed?), ?relief? is true. 
? if (vr= true and sr = ?Displeased? and pros= 
?Positive? and sp= ?Desirable? and status= 
?Disconfirmed?), ?disappointment? is true. 
? if (vr= true and sr= ?Displeased? and sa= 
?Blameworthy? and sp= ?Undesirable? and 
cs=?Self?), ?shame? is true. 
? if (vr= true and sp= ?Desirable? and sr= 
?Pleased? and of= ?Liked? and oa= ?Attrac-
tive?), ?love? is true. 
? if (vr= true and sp= ?Undesirable? and sr= 
?Displeased? and of= ?Not-Liked? and oa= 
?Unattractive?), ?hate? is true. 
The OCC model has four complex emotions 
namely, ?gratification?, ?remorse?, ?gratitude? and 
?anger?. For example: 
? If both ?joy? and ?pride? are true, ?gratifica-
tion? is also true. 
? If both ?distress? and ?reproach? are true, ?an-
ger? is also true. 
The cognitive states ?Shock? (i.e.; unpleasant sur-
prise) and ?Surprise? (i.e., pleasant surprise) are 
ruled as; If both ?distress? and unexp are true, 
?shock? is true. (e.g., The bad news came unex-
pectedly.). Similarly, if both ?joy? and unexp are 
true, ?surprise? is true. (e.g., I suddenly met my 
school friend in Tokyo.) 
Like Liu et al (2003), we also believe that a 
statement may contain more than one type of emo-
tions. In our case, the 22 emotion types and two 
cognitive states are grouped into seven groups, 
namely, well-being emotion, fortune of other emo-
tion, prospect based emotion, cognitive state, attri-
bution emotion, attraction emotion, and compound 
emotion. Hence an input sentence may contain one 
of the emotion types from each group. For exam-
ple, the sentence ?I suddenly got to know that my 
paper won the best paper award.?, outputs the fol-
lowing emotions: {Joy, Satisfaction, Surprise, 
Pride, Gratification}.The sentence ?She failed to 
pass the entrance examination.?, outputs {Dis-
tress, Sorry-for, Disappointment, Reproach, An-
ger} emotion types. In order to reduce the number 
of emotions, we consider the intensity variables. 
For the first set of emotions, we can reduce it to 
{Satisfaction, Surprise, Pride} because ?Joy? 
doesn?t have any intensity variables and the inten-
sity variables ed and edev are set to ?High? in this 
case. 
4 Test and Evaluation 
The similar system like ours is Liu?s system (Liu 
et. al., 2003). It is a rule based system, and it seems 
to be the best performing system for sentence-level 
affect sensing that senses happy, fearful, sad, an-
gry, disgust, and surprise emotions. On the practi-
cal side, it is freely available on the Internet. Ex-
899
ample input and output are enlisted to given an 
idea about the outputs of the two systems.   
Input: I avoided the accident luckily. 
Liu?s output: fearful(26%),happy (18%), angry 
(12%),sad(8%),surprised(7%),disgusted (0%) 
Ours output: valence: +11.453; [joy, pride, relief, 
surprise, gratification] 
Input: Susan bought a lottery ticket and she was 
lucky to win the million dollar lottery. 
Liu?s output: sad(21%), happy(18%), fearful 
(13%),angry(11%),disgusted(0%),surprised (0%) 
Ours: valence: +12.533; [happy-for, satisfaction, ad-
miration, love] 
We evaluated our system to assess the accuracy 
of sentence-level affect sensing when compared to 
human-ranked scores (as ?gold standard?) for 200 
sentences assessed by two systems. The sentences 
were collected from Internet based sources for re-
views of products, movies, and news. In order to 
conduct system?s performance and acceptance test 
we have two systems X (i.e., Liu?s System) and Y 
(i.e., our system). The judges were not told about 
the characteristics of any of the systems. Each 
judge receives the output from both X and Y for 
each input sentence and can accept either both out-
puts or anyone of the two or reject both. Thus %X 
means the percentage of the number of acceptances 
received by X in terms of accuracy of output. Simi-
larly %Y, %XY, and %!XY indicate the percent-
age of acceptances received by the system Y, both 
the systems and neither of the two systems respec-
tively. For example, for the input sentence ?She is 
extremely generous, but not very tolerant with peo-
ple who don't agree with her.?, among the 5 judges 
3 accepted the output of Y, 2 accepted the output 
of X. Since the majority of the judges accepted Y, 
vote for this sentence was counter for Y. Thus the 
vote for each sentence is counted. Outcome of our 
experiment is reported below while the valence 
range to classify a neutral sentence is considered 
?3.5 for the SenseNet upon which system Y is 
built.  
System Y received 16.069% more acceptances 
than that of X, which indicates that the output of Y 
is more acceptable and accurate than that of X. 
Though the test was conducted with a small group 
of judges with relatively small input size, but the 
experiment result (i.e., 82% accuracy with an aver-
age precision 76.49%, recall 81.04% and F-score 
78% for classifying positive, negative and neutral 
classes using the same dataset) for sentiment sens-
ing reported by SenseNet, provides an optimistic 
believe that the result would not vary even the sur-
vey is conducted with larger group of judges. Ta-
ble 3 summarizes the experimental result for 200 
sentences. 
Data-Set of 200 Sentences 
%X %Y %XY %!XY 
20.344 36.413 24.283 18.96 
 Table 3. Experimental Result 
5  Conclusion 
In order to perform more testing and usability 
study, we plan to implement a web-based user in-
terface where any user can input a chunk of text 
and get outputs from the both systems mentioned 
above. Thereby we can get user?s acceptance test 
in terms of accuracy of output. Next we plan to 
perform the task of affect sensing using online re-
sources (e.g., blogs, reviews, etc.).  
Reference 
Connexor Oy. 2005. Machinese Syntax, web-site: 
http://www.connexor.com/connexor/ 
Hugo Liu and Push Singh. 2004. ConceptNet: A Practi-
cal Commonsense Reasoning Toolkit, BT Technol-
ogy Journal, 22(4):211-226, Kluwer Academic Pub-
lishers. 
Hugo Liu, Henry Lieberman, and Ted Selker. 2003. A 
Model of Textual Affect Sensing using Real-World 
Knowledge, In Proc. IUI 03, pp. 125-132, Miami, 
USA, ACM. 
Opinmind, Discovering Bloggers, (2006), 
http://www.opinmind.com/ 
Andrew Ortony, Gerald L. Clore and Allan Collins. 
1988. The Cognitive Structure of Emotions, Cam-
bridge University Press. 
James W. Pennebaker, Martha E. Francis, and Roger J. 
Booth. 2001. Linguistic inquiry and word count: 
LIWC (2nd ed.) [Computer software]. Mahwah, NJ: 
Erlbaum. 
Mostafa A. M. Shaikh, Helmut Prendinger and Mitsuru 
Ishizuka. 2007a. SenseNet: A Linguistic Tool to 
Visualize Numerical-Valance Based Sentiment of 
Textual Data, In Proc. ICON-2007, pages 147-152. 
Mostafa A. M. Shaikh, Helmut Prendinger and Mitsuru 
Ishizuka. 2007b. Assessing Sentiment of Text by 
Semantic Dependency and Contextual Valence 
Analysis. In Proc. ACII 07, pp. 191-202. 
James G. Shanahan, Yan Qu and Janyce Wiebe (Eds.). 
2006. Computing Attitude and Affect in Text: The-
ory and Applications, Springer. 
900
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 665?673,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Novel Discourse Parser Based on
Support Vector Machine Classification
David A. duVerle
National Institute of Informatics
Tokyo, Japan
Pierre & Marie Curie University
Paris, France
dave@nii.ac.jp
Helmut Prendinger
National Institute of Informatics
Tokyo, Japan
helmut@nii.ac.jp
Abstract
This paper introduces a new algorithm to
parse discourse within the framework of
Rhetorical Structure Theory (RST). Our
method is based on recent advances in the
field of statistical machine learning (mul-
tivariate capabilities of Support Vector
Machines) and a rich feature space. RST
offers a formal framework for hierarchical
text organization with strong applications
in discourse analysis and text generation.
We demonstrate automated annotation of
a text with RST hierarchically organised
relations, with results comparable to those
achieved by specially trained human anno-
tators. Using a rich set of shallow lexical,
syntactic and structural features from the
input text, our parser achieves, in linear
time, 73.9% of professional annotators?
human agreement F-score. The parser is
5% to 12% more accurate than current
state-of-the-art parsers.
1 Introduction
According to Mann and Thompson (1988), all
well-written text is supported by a hierarchically
structured set of coherence relations which reflect
the authors intent. The goal of discourse parsing
is to extract this high-level, rhetorical structure.
Dependency parsing and other forms of syn-
tactic analysis provide information on the gram-
matical structure of text at the sentential level.
Discourse parsing, on the other hand, focuses
on a higher-level view of text, allowing some
flexibility in the choice of formal representation
while providing a wide range of applications
in both analytical and computational linguistics.
Rhetorical Structure Theory (Mann and Thomp-
son, 1988) provides a framework to analyze and
study text coherence by defining and applying a set
of structural relations to composing units (?spans?)
of text. Annotation of a text within the RST
formalism will produce a tree-like structure that
not only reflects text-coherence but also provides
input for powerful algorithmic tools for tasks such
as text regeneration (Piwek et al, 2007).
RST parsing can be seen as a two-step process:
1. Segmentation of the input text into elemen-
tary discourse units (?edus?).
2. Generation of the rhetorical structure tree
based on ?rhetorical relations? (or ?coherence
relations?) as labels of the tree, with the edus
constituting its terminal nodes.
Mann and Thompson (1988) empirically estab-
lished 110 distinct rhetorical relations, but pointed
out that this set was flexible and open-ended.
In addition to rhetorical relations, RST defines
the notion of ?nucleus?, the relatively more
important part of the text, and ?satellite?, which
is subordinate to the nucleus. In Fig. 1, the left-
most edu constitutes the satellite (indicated by
out-going arrow), and the right-hand statement
constitutes the nucleus. Observe that the nucleus
itself is a compound of nucleus and satellite.
Several attempts to automate discourse parsing
have been made. Marcu and Soricut focussed
on sentence-level parsing and developed two
probabilistic models that use syntactic and lexical
information (Soricut and Marcu, 2003). Although
their algorithm, called ?SPADE?, does not produce
full-text parse, it demonstrates a correlation
between syntactic and discourse information, and
their use to identify rhetorical relations even if no
signaling cue words are present.
665
RTEMPORAL
After plummet-
ing 1.8% at one
point during the
day, 	
CONTRAST
the composite re-
bounded a little,
but finished down
5.52, at 461.70.
Figure 1: Example of a simple RST tree (Source:
RST Discourse Treebank (Carlson et al, 2001),
wsj0667).
To the best of our knowledge, Reitter?s (2003b)
was the only previous research based exclusively
on feature-rich supervised learning to produce
text-level RST discourse parse trees. However,
his full outline for a working parser, using chart-
parsing-style techniques, was never implemented.
LeThanh et al (2004) proposed a multi-step
algorithm to segment and organize text spans into
trees for each successive level of text organization:
first at sentence level, then paragraph and finally
text. The multi-level approach taken by their
algorithm mitigates the combinatorial explosion
effect without treating it entirely. At the text-level,
and despite the use of beam search to explore the
solution space, the algorithm needs to produce and
score a large number of trees in order to extract
the best candidate, leading, in our experience, to
impractical calculation times for large input.
More recently, Baldridge and Lascarides (2005)
successfully implemented a probabilistic parser
that uses headed trees to label discourse relations.
Restricting the scope of their research to texts in
dialog form exclusively, they elected to use the
more specific framework of Segmented Discourse
Representation Theory (Asher and Lascarides,
2003) instead of RST.
In this paper, we advanced the state-of-the-art
in general discourse parsing, with an implemented
solution that is computationally efficient and suf-
ficiently accurate for use in real-time interactive
applications. The rest of this paper is organized
as follows: Section 2 describes the general
architecture of our system along with the choices
we made with regard to supervised learning.
Section 3 explains the different characteristics of
the input text used to train our system. Section 4
presents our results, and Section 5 concludes the
paper.
2 Building a Discourse Parser
2.1 Assumptions and Restrictions
In our work, we focused exclusively on the second
step of the discourse parsing problem, i.e., con-
structing the RST tree from a sequence of edus that
have been segmented beforehand. The motivation
for leaving aside segmenting were both practical
? previous discourse parsing efforts (Soricut and
Marcu, 2003; LeThanh et al, 2004) already
provide alternatives for standalone segmenting
tools ? and scientific, namely, the greater need for
improvements in labeling. Current state-of-the-art
results in automatic segmenting are much closer
to human levels than full structure labeling (F-
score ratios of automatic performance over gold
standard reported in LeThanh et al (2004): 90.2%
for segmentation, 70.1% for parsing).
Another restriction is to use the reduced set
of 18 rhetorical relations defined in Carlson
et al (2001) and previously used by Soricut
and Marcu (2003). In this set, the 75 re-
lations originally used in the RST Discourse
Treebank (RST-DT) corpus (Carlson et al,
2001) are partitioned into 18 classes accord-
ing to rhetorical similarity (e.g.: PROBLEM-
SOLUTION, QUESTION-ANSWER, STATEMENT-
RESPONSE, TOPIC-COMMENT and COMMENT-
TOPIC are all grouped under one TOPIC-
COMMENT relation). In accord with previous
research (Soricut and Marcu, 2003; Reitter,
2003b; LeThanh et al, 2004), we turned all n-
ary rhetorical relations into nested binary relations
(a trivial graph transformation), resulting in more
algorithmically manageable binary trees. Finally,
we assumed full conformity to the ?Principle of
sequentiality? (Marcu, 2000), which guarantees
that only adjacent spans of text can be put
in relation within an RST tree, and drastically
reduces the size of the solution space.
2.2 Support Vector Machines
At the core of our system is a set of classifiers,
trained through supervised-learning, which, given
two consecutive spans (atomic edus or RST
sub-trees) in an input document, will score the
likelihood of a direct structural relation as well
as probabilities for such a relation?s label and
nuclearity. Using these classifiers and a straight-
forward bottom-up tree-building algorithm, we
can produce a valid tree close to human cross-
666
validation levels (our gold standard) in linear time-
complexity (see Fig. 2).
SVM Classification
Training Corpus 
(RST-TB)
Test Corpus
Segmentation 
(SPADE)
Penn 
Treebank
Tokenized EDUs
EDUs
Lexicalized 
Syntax Trees
Syntax Parsing (Charniak's nlparse)
Syntax Trees
Lexicalization
Lexicalization
Lexicalized 
Syntax Trees
Syntax Trees
Alignment
Feature Extraction Alignment
Feature ExtractionSVM Training
SVM Models (Binary and Multiclass)
Bottom-up Tree Construction
Scored RS sub-trees
Rhetorical Structure Tree
Tokenization
Tokenized EDUs
Figure 2: Full system workflow.
In order to improve classification accuracy, it is
convenient to train two separate classifiers:
? S: A binary classifier, for structure (existence
of a connecting node between the two input
sub-trees).
? L: A multi-class classifier, for rhetorical
relation and nuclearity labeling.
Using our reduced set of 18 super-
relations and considering only valid
nuclearity options (e.g., (ATTRIBUTION, N, S)
and (ATTRIBUTION, S, N), but not
(ATTRIBUTION, N, N), as ATTRIBUTION is
a purely hypotactic relation group), we come up
with a set of 41 classes for our algorithm.
Support Vector Machines (SVM) (Vapnik,
1995) are used to model classifiers S and L. SVM
refers to a set of supervised learning algorithms
that are based on margin maximization. Given
our specific type of classification problem, SVMs
offer many properties of particular interest. First,
as maximum margin classifiers, they sidestep
the common issue of overfitting (Scholkopf et
al., 1995), and ensure a better control over
the generalization error (limiting the impact of
using homogeneous newspaper articles that could
carry important biases in prose style and lexical
content). Second, SVMs offer more resilience to
noisy input. Third, depending on the parameters
used (see the use of kernel functions below),
training time complexity?s dependence on feature
vector size is low, in some cases linear. This makes
SVM well-fitted to treat classification problems
involving relatively large feature spaces such as
ours (? 105 features). Finally, while most
probabilistic classifiers, such as Naive Bayes,
strongly assume feature independence, SVMs
achieve very good results regardless of input
correlations, which is a desirable property for
language-related tasks.
SVM algorithms make use of the ?kernel
trick? (Aizerman et al, 1964), a method for using
linear classifiers to solve non-linear problems.
Kernel methods essentially map input data to
a higher-dimensional space before attempting to
classify them. The choice of a fitting kernel
function requires careful analysis of the data and
must weigh the effects on both performance and
training time. A compromise needs to be found
during evaluation between the general efficiency
of non-linear kernels (such as polynomial or
Radial Basis Function) and low time-complexity
of using a linear function (see Sect. 4).
Because the original SVM algorithms build bi-
nary classifiers, multi-label classification requires
some adaptation. A possible approach is to
reduce the multi-classification problem through a
set of binary classifiers, each trained either on
a single class (?one vs. all?) or by pair (?one
vs. one?). Recent research suggests keeping the
classification whole, with a reformulation of the
original optimization problem to accommodate
multiple labels (?C & S?) (Crammer and Singer,
2002).
2.3 Input Data and Feature Extraction
Both S and L classifiers are trained using
manually annotated documents taken from the
RST-DT corpus. Optimal parameters (when
applicable) for each kernel function are obtained
through automated grid search with n-fold cross-
validation (Staelin, 2003) on the training corpus,
while a separate test set is used for performance
evaluation. In training mode, classification
instances are built by parsing manually annotated
trees from the RST-DT corpus paired with
lexicalized syntax trees (LS Trees) for each
sentence (see Sect. 3). Syntax trees are taken
667
directly from the Penn Treebank corpus (which
covers a superset of the RST-DT corpus), then
?lexicalized? (i.e. tagged with lexical ?heads? on
each internal node of the syntactic tree) using a
set of canonical head-projection rules (Magerman,
1995; Collins, 2003). Due to small differences
in the way they were tokenized and pre-treated,
rhetorical tree and LST are rarely a perfect match:
optimal alignment is found by minimizing edit
distances between word sequences.
2.4 Tree-building Algorithm
By repeatedly applying the two classifiers and
following a naive bottom-up tree-construction
method, we are able to obtain a globally satisfying
RST tree for the entire text with excellent time-
complexity.
The algorithm starts with a list of all atomic
discourse sub-trees (made of single edus in their
text order) and recursively selects the best match
between adjacent sub-trees (using binary classifier
S), labels the newly created sub-tree (using multi-
label classifier L) and updates scoring for S, until
only one sub-tree is left: the complete rhetorical
parse tree for the input text.
It can be noted that, thanks to the principle
of sequentiality (see Sect. 2.1), each time two
sub-trees are merged into a new sub-tree, only
connections with adjacent spans on each side are
affected, and therefore, only two new scores need
to be computed. Since our SVM classifiers work
in linear time, the overall time-complexity of our
algorithm is O(n).
3 Features
Instrumental to our system?s performance is
the choice of a set of salient characteristics
(?features?) to be used as input to the SVM
algorithm for training and classification. Once the
features are determined, classification instances
can be formally represented as a vector of values
inR.
We use n-fold validation on S and L classifiers
to assess the impact of some sets of features
on general performance and eliminate redundant
features. However, we worked under the (verified)
assumption that SVMs? capacity to handle high-
dimensional data and resilience to input noise limit
the negative impact of non-useful features.
In the following list of features, obtained
empirically by trial-and-error, features suffixed by
?S[pan]? are sub-tree-specific features, symmetri-
cally extracted from both left and right candidate
spans. Features suffixed by ?F[ull]? are a function
of the two sub-trees considered as a pair. Multi-
label features are turned into sets of binary values
and trees use a trivial fixed-length binary encoding
that assumes fixed depth.
3.1 Textual Organization
As evidenced by a number of discourse-parsing ef-
forts focusing on intra-sentential parsing (Marcu,
2000; Soricut and Marcu, 2003), there is a strong
correlation between different organizational levels
of textual units and sub-trees of the RST tree
both at the sentence-level and the paragraph level.
Although such correspondences are not a rule
(sentences and particularly paragraphs, can often
be found split across separate sub-trees), they
provide valuable high-level clues, particularly in
the task of scoring span relation priority (classifier
S):
Ex.: ?Belong to same sentence?F, ?Belong
to same paragraph?F, ?Number of paragraph
boundaries?S, ?Number of sentence bound-
aries?S. . .
As pointed out by Reitter (Reitter, 2003a), we
can hypothesize a correlation between span length
and some relations (for example, the satellite in a
CONTRAST relation will tend to be shorter than
the nucleus). Therefore, it seems useful to encode
different measures of span size and positioning,
using either tokens or edus as a distance unit:
Ex.: ?Length in tokens?S, ?Length in edus?S,
?Distance to beginning of sentence in tokens?S,
?Size of span over sentence in edus?S, ?Distance
to end of sentence in tokens?S. . .
In order to better adjust to length variations
between different types of text, some features in
the above set are duplicated using relative, rather
than absolute, values for positioning and distance.
3.2 Lexical Clues and Punctuation
While not always present, discourse markers
(connectives, cue-words or cue-phrases, etc) have
been shown to give good indications on discourse
structure and labeling, particularly at the sentence-
level (Marcu, 2000). We use an empirical n-
gram dictionary (for n ? {1, 2, 3}) built from the
training corpus and culled by frequency. As an
advantage over explicit cue-words list, this method
668
also takes into account non-lexical signals such
as punctuation and sentence/paragraph boundaries
(inserted as artificial tokens in the original text
during input formatting) which would otherwise
necessitate a separate treatment.
We counted and encoded n-gram occurrences
while considering only the first and last n tokens
of each span. While raising the encoding size
compared to a ?bag of words? approach, this
gave us significantly better performance (classifier
accuracy improved by more than 5%), particularly
when combined with main constituent features
(see Sect. 3.5 below). This is consistent with the
suggestion that most meaningful rhetorical signals
are located on the edge of the span (Schilder,
2002).
We validated this approach by comparing
it to results obtained with an explicit list
of approximately 300 discourse-signaling cue-
phrases (Oberlander et al, 1999): performance
when using the list of cue-phrases alone was
substantially lower than n-grams.
3.3 Simple Syntactic Clues
In order to complement signal detection and to
achieve better generalization (smaller dependency
on lexical content), we opted to add shallow
syntactic clues by encoding part-of-speech (POS)
tags for both prefix and suffix in each span. Using
prefixes or suffixes of length higher than n = 3 did
not seem to improve performance significantly.
3.4 Dominance Sets
A promising concept introduced by Soricut and
Marcu (2003) in their sentence-level parser is the
identification of ?dominance sets? in the syntax
parse trees associated to each input sentence. For
example, it could be difficult to correctly identify
the scope of the ATTRIBUTION relation in the
example shown in Fig. 3. By using the associated
syntax tree and studying the sub-trees spanned by
each edu (see Fig. 4), it is possible to quickly infer
a logical nesting order (?dominance?) between
them: 1A > 1B > 1C. This order allows us
to favor the relation between 1B and 1C over a
relation between 1A and 1B, and thus helps us
to make the right structural decision and pick the
right-hand tree on Fig. 3.
In addition to POS tags around the frontier
between each dominance set (see colored nodes
in Fig. 4), Soricut and Marcu (2003) note that in
order to achieve good results on relation labeling,
[Shoney?s Inc. said]1A [it will report a write-off
of $2.5 million, or seven cents a share, for its
fourth quarter]1B [ended yesterday.]1C (wsj0667)
	
ELABORATION
R
ATTRIBUTION
1A 1B
1C
R
ATTRIBUTION
1A
	
ELABORATION
1B 1C
Figure 3: Two possible RST parses for a sentence.
it is necessary to also consider lexical informa-
tion (obtained through head word projection of
terminal nodes to higher internal nodes). Based
on this definition of dominance sets, we include a
set of syntactic, lexical and tree-structural features
that aim at a good approximation of Marcu &
Soricut?s rule-based analysis of dominance sets
while keeping parsing complexity low.
Ex.: ?Distance to root of the syntax tree?S,
?Distance to common ancestor in the syn-
tax tree?S, ?Dominating node?s lexical head
in span?S, ?Common ancestor?s POS tag?F,
?Common ancestor?s lexical head?F, ?Domi-
nating node?s POS tag?F (diamonds in Figure
4, ?Dominated node?s POS tag?F (circles in
Figure 4), ?Dominated node?s sibling?s POS
tag?F (rectangles in Figure 4), ?Relative position
of lexical head in sentence?S. . .
3.5 Strong Compositionality Criterion
We make use of Marcu?s ?Strong Compositionality
Criterion? (Marcu, 1996) through a very simple
and limited set of features, replicating shallow lex-
ical and syntactic features (previously described in
Sections 3.2 and 3.3) on a single representative
edu (dubbed main constituent) for each span.
Main constituents are selected recursively using
nuclearity information. We purposely keep
the number of features extracted from main
constituents comparatively low (therefore limiting
the extra dimensionality cost), as we believe our
use of rhetorical sub-structures ultimately encodes
a variation of Marcu?s compositionality criterion
(see Sect. 3.6).
3.6 Rhetorical Sub-structure
A large majority of the features considered so far
focus exclusively on sentence-level information.
669
1A.
1B.
1C.
S
NP-SBJ
NP
NNP
Shoney
POS
's
NNP
Inc.
VP
VBD
said
SBAR
S
NP-SBJ
PRP
it
VP
MD
will
VP
VB
report
NP
NP
DT
a
NN
write-off
PP
IN
of
NP
NP
QP
$
$
CD
2.5
CD
million
,
,
CC
or
NP
NP
CD
seven
NNS
cents
NP-ADV
DT
a
NN
share
,
,
PP
IN
for
NP
NP
PRP$
its
JJ
fourth
NN
quarter
VP
VBN
ended
NP-TMP
NN
yesterday
.
.
(said)
(will)
(quarter)
(ended)
(quarter)
(said)
(said)
(will)(it)
Figure 4: Using dominance sets to prioritize structural relations.
Circled nodes define dominance sets and studying the frontiers between circles and diamonds gives us a dominance order
between each of the three sub-trees considered: 1A > 1B > 1C. Head words obtained through partial lexicalization have been
added between parenthesis.
In order to efficiently label higher-level relations,
we need more structural features that can guide
good classification decision on large spans. Hence
the idea of encoding each span?s rhetorical subtree
into the feature vector seems natural.
Beside the role of nuclearity in the sub-structure
implied by Marcu?s compositionality criterion (see
Sect. 3.5), we expect to see certain correlations
between the relation being classified and relation
patterns in either sub-tree, based on theoretical
considerations and practical observations. The
original RST theory suggests the use of ?schemas?
as higher-order patterns of relations motivated by
linguistic theories and verified through empirical
analysis of annotated trees (Mann and Thompson,
1988). In addition, some level of correlation
between relations at different levels of the tree
can be informally observed throughout the corpus.
This is trivially the case for n-ary relations
such as LIST which have been binarized in our
representation, i.e., the presence of several LIST
relations in rightmost nodes of a subtree greatly
increases the probability that the parent relation
might be a LIST itself.
4 Evaluation
4.1 General Considerations
In looking to evaluate the performance of our
system, we had to work with a number of
constraints and difficulties tied to variations in the
methodologies used across past works, as well
as a lack of consensus with regard to a common
evaluation corpus. In order to accommodate these
divergences while providing figures to evaluate
both relative and absolute performance of our
algorithm, we used three different test sets.
Absolute performance is measured on the official
test subset of the RST-DT corpus. A similarly
available subset of doubly-annotated documents
from the RST-DT is used to compare results
with human agreement on the same task. Lastly,
performance against past algorithms is evaluated
with another subset of the RST-DT, such as used
by LeThanh et al (2004) in their own evaluation.
4.2 Raw SVM Classification
Although our final goal is to achieve good
performance on the entire tree-building task, a
useful intermediate evaluation of our system can
be conducted by measuring raw performance of
SVM classifiers. Binary classifier S is trained
on 52,683 instances (split approximately 1/3,
2/3 between positive and negative examples),
extracted from 350 documents, and tested on
8,558 instances extracted from 50 documents. The
feature space dimension is 136,987. Classifier L
is trained on 17,742 instances (labeled across 41
classes) and tested on 2,887 instances, of same
dimension as for S.
Classifier: Binary (S) Multi-label (L) Reitter
Kernel Linear Polyn. RBF Linear RBF RBF
Software liblinear svmlight svmlight svmmulticlass libsvm svmlight
Multi-label - C&S 1 vs. 1 1 vs. all
Training time 21.4s 5m53s 12m 15m 23m 216m
Accuracy 82.2 85.0 82.9 65.8 66.8 61.0
Table 1: SVM Classifier performance. Regarding
?Multi-label?, see Sect. 2.2.
The noticeably good performance of linear
670
kernel methods in the results presented in Table 1
compared to more complex polynomial and RBF
kernels, would indicate that our data separates
fairly well linearly: a commonly observed effect
of high-dimensional input (Chen et al, 2007) such
as ours (> 100,000 features).
A baseline for absolute comparison on the
multi-label classification task is given by Reit-
ter (2003a) on a similar classifier, which assumes
perfect segmentation of the input, as ours does.
Reitter?s accuracy results of 61% match a smaller
set of training instances (7976 instances from
240 documents compared to 17,742 instances in
our case) but with considerably less classes (16
rhetorical relation labels with no nuclearity, as
opposed to our 41 nuclearized relation classes).
Based on these differences, this sub-component of
our system, with an accuracy of 66.8%, seems to
perform well.
Taking into account matters of performance and
runtime complexity, we selected a linear kernel for
S and an optimally parameterized RBF kernel for
L, using modified versions of the liblinear and
libsvm software packages. All further evaluations
noted here were conducted with these.
4.3 Full System Performance
A measure of our full system?s performance is
realized by comparing structure and labeling of
the RST tree produced by our algorithm to that
obtained through manual annotation (our gold
standard). Standard performance indicators for
such a task are precision, recall and F-score as
measured by the PARSEVAL metrics (Black et al,
1991), with the specific adaptations to the case of
RST trees made by Marcu (2000, page 143-144).
Our first evaluation (see Table 2) was conducted
using the standard test subset of 41 files provided
by the RST-DT corpus. In order to more
accurately compare our results to the gold standard
(defined as manual agreement between human
annotators), we also evaluated performance using
the 52 doubly-annotated files present in the RST-
DT as test set (see Table 3). In each case, the
remaining 340?350 files are used for training.
For each corpus evaluation, the system is
run twice: once using perfectly-segmented in-
put (taken from the RST-DT), and once using
the output of the SPADE segmenter (Soricut and
Marcu, 2003). The first measure gives us a good
idea of our system?s optimal performance (given
optimal input), while the other gives us a more
real-world evaluation, apt for comparison with
other systems.
In each case, parse trees are evaluated using the
four following, increasingly complex, matching
criteria: blank tree structure (?S?), tree structure
with nuclearity (?N?), tree structure with rhetorical
relations (?R?) and our final goal: fully labeled
structure with both nuclearity and rhetorical
relation labels (?F?).
Segment. Manual SPADE
S N R F S N R F
Precision 83.0 68.4 55.3 54.8 69.5 56.1 44.9 44.4
Recall 83.0 68.4 55.3 54.8 69.2 55.8 44.7 44.2
F-Score 83.0 68.4 55.3 54.8 69.3 56.0 44.8 44.3
Table 2: Discourse-parser evaluation depending
on segmentation using standard test subset
System performance Human agreement
Segment. Manual SPADE -
S N R F S N R F S N R F
Precision 84.1 70.6 55.6 55.1 70.6 58.1 46.0 45.6 88.0 77.5 66.0 65.2
Recall 84.1 70.6 55.6 55.1 71.2 58.6 46.4 46.0 88.1 77.6 66.1 65.3
F-Score 84.1 70.6 55.6 55.1 70.9 58.3 46.2 45.8 88.1 77.5 66.0 65.3
Table 3: Comparing to human-agreement de-
pending on segmentation using doubly-annotated
subset
Note: When using perfect segmentation, preci-
sion and recall are identical since both trees have
same number of constituents.
4.4 Comparison with other Algorithms
To the best of our knowledge, only two fully
functional text-level discourse parsing algorithms
for general text have published their results:
Marcu?s decision-tree-based parser (Marcu, 2000)
and the multi-level rule-based system built by
LeThanh et al (2004). For each one, evaluation
was conducted on a different corpus, using
unavailable documents for Marcu?s and a selection
of 21 documents from the RST-DT (distinct
from RST-DT?s test set) for LeThanh?s. We
therefore retrained and evaluated our classifier,
using LeThanh?s set of 21 documents as testing
subset (and the rest for training) and compared
performance (see Table 4). In order to achieve
the most uniform conditions possible, we use
LeThanh?s results on 14 classes (Marcu?s use 15,
ours 18) and select SPADE segmentation figures
for both our system and Marcu?s (LeThanh?s
671
system uses its own segmenter and does not
provide figures for perfectly segmented input).
Structure Nuclearity Relations
Algorithm M lT dV M lT dV M lT dV
Precision 65.8 54.5 72.4 54.0 47.8 57.8 34.3 40.5 47.8
Recall 34.0 52.9 73.3 21.6 46.4 58.5 13.0 39.3 48.4
F-score 44.8 53.7 72.8 30.9 47.1 58.1 18.8 39.9 48.1
Table 4: Side-by-side text-level algorithms com-
parison: Marcu (M), LeThanh et al (lT) and ours
(dV)
Some discrepancies between reported human
agreement F-scores suggest that, despite our
best efforts, evaluation metrics used by each
author might differ. Another explanation may lie
in discrepancies between training/testing subsets
used. In order to take into account possibly
varying levels of difficulties between corpora, we
therefore divided each F-score by the value for
human agreement, such as measured by each
author (see Table 5). This ratio should give us a
fairer measure of success for the algorithm taking
into account how well it succeeds in reaching near-
human level.
Structure Nuclearity Relations
Algorithm M lT dV M lT dV M lT dV
F?scorealgo
F?scorehuman 56.0 73.9 83.0 42.9 71.8 75.6 25.7 70.1 73.9
Table 5: Performance scaled by human agreement
scores: Marcu (M), LeThahn et al (lT) and ours
(dV)
Table 5 shows 83%, 75.6% and 73.9% of human
agreement F-scores in structure, nuclearity and
relation parsing, respectively. Qualified by the
(practical) problems of establishing comparison
conditions with scientific rigor, the scores indicate
that our system outperforms the previous state-
of-the-art (LeThanh?s 73.9%, 71.8% and 70.1%).
As suggested by previous research (Soricut and
Marcu, 2003), these scores could likely be
further improved with the use of better-performing
segmenting algorithms. It can however be noted
that our system seems considerably less sensitive
to imperfect segmenting than previous efforts. For
instance, when switching from manual segmen-
tation to automatic, our performance decreases
by 12.3% and 12.9% (respectively for structure
and relation F-scores) compared to 46% and 67%
for Marcu?s system (LeThanh?s performance on
perfect input is unknown).
5 Conclusions and Future Work
In this paper, we have shown that it is possible
to build an accurate automatic text-level discourse
parser based on supervised machine-learning
algorithms, using a feature-driven approach and
a manually annotated corpus. Importantly, our
system achieves its accuracy in linear complexity
of the input size with excellent runtime per-
formance. The entire test subset in the RST-
DT corpus could be fully annotated in a matter
of minutes. This opens the way to many
novel applications in real-time natural language
processing and generation, such as the RST-based
transformation of monological text into dialogues
acted by virtual agents in real-time (Hernault et al,
2008).
Future directions for this work notably include
a better tree-building algorithm, with improved
exploration of the solution space. Borrowing
techniques from generic global optimization meta-
algorithms such as simulated annealing (Kirk-
patrick et al, 1983) should allow us to better
deal with issues of local optimality while retaining
acceptable time-complexity.
A complete online discourse parser, incorpo-
rating the parsing tool presented above com-
bined with a new segmenting method has since
been made freely available at http://nlp.
prendingerlab.net/hilda/.
Acknowledgements
This project was jointly funded by Prendinger
Lab (NII, Tokyo) and the National Institute
for Informatics (Tokyo), as part of a MOU
(Memorandum of Understanding) program with
Pierre & Marie Curie University (Paris).
672
References
M.A. Aizerman, E.M. Braverman, and L.I. Rozonoer.
1964. Theoretical foundations of the potential
function method in pattern recognition learning.
Automation and Remote Control, 25(6):821?837.
N. Asher and A. Lascarides. 2003. Logics of
conversation. Cambridge University Press.
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse structure. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning, volume 96, page 103.
E. Black, S. Abney, S. Flickenger, C. Gdaniec,
C. Grishman, P. Harrison, D. Hindle, R. Ingria,
F. Jelinek, J. Klavans, M. Liberman, et al 1991.
Procedure for quantitatively comparing the syntactic
coverage of English grammars. Proceedings of the
workshop on Speech and Natural Language, pages
306?311.
L. Carlson, D. Marcu, and M.E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of Rhetorical Structure Theory. Proceedings
of the Second SIGdial Workshop on Discourse and
Dialogue-Volume 16, pages 1?10.
D. Chen, Q. He, and X. Wang. 2007. On
linear separability of data sets in feature space.
Neurocomputing, 70(13-15):2441?2448.
M. Collins. 2003. Head-Driven Statistical Models
for Natural Language Parsing. Computational
Linguistics, 29(4):589?637.
K. Crammer and Y. Singer. 2002. On the algorithmic
implementation of multiclass kernel-based vector
machines. The Journal of Machine Learning
Research, 2:265?292.
H. Hernault, P. Piwek, H. Prendinger, and M. Ishizuka.
2008. Generating dialogues for virtual agents using
nested textual coherence relations. Proceedings
of the 8th International Conference on Intelligent
Virtual Agents (IVA?08), LNAI, 5208:139?145, Sept.
S. Kirkpatrick, CD Gelatt, and MP Vecchi. 1983.
Optimization by Simulated Annealing. Science,
220(4598):671?680.
H. LeThanh, G. Abeysinghe, and C. Huyck. 2004.
Generating discourse structures for written texts.
Proceedings of the 20th international conference on
Computational Linguistics.
D.M. Magerman. 1995. Statistical decision-tree
models for parsing. Proceedings of the 33rd
annual meeting on Association for Computational
Linguistics, pages 276?283.
W.C. Mann and S.A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
D. Marcu. 1996. Building Up Rhetorical Structure
Trees. Proceedings of the National Conference on
Artificial Intelligence, pages 1069?1074.
D. Marcu. 2000. The theory and practice of discourse
parsing and summarization. MIT Press.
J. Oberlander, J.D. Moore, J. Oberlander, A. Knott,
and J. Moore. 1999. Cue phrases in discourse:
further evidence for the core: contributor distinction.
Proceedings of the 1999 Levels of Representation in
Discourse Workshop (LORID?99), pages 87?93.
P. Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.
2007. Generating dialogues between virtual agents
automatically from text. Proceedings of the
7th International Conference on Intelligent Virtual
Agents (IVA ?07), LNCS, 4722:161.
D. Reitter. 2003a. Rhetorical Analysis with Rich-
Feature Support Vector Models. Unpublished
Master?s thesis, University of Potsdam, Potsdam,
Germany.
D. Reitter. 2003b. Simple Signals for Complex
Rhetorics: On Rhetorical Analysis with Rich-
Feature Support Vector Models. Language, 18(52).
F. Schilder. 2002. Robust discourse parsing via
discourse markers, topicality and position. Natural
Language Engineering, 8(2-3):235?255.
B. Scholkopf, C. Burges, and V. Vapnik. 1995. Ex-
tracting Support Data for a Given Task. Knowledge
Discovery and Data Mining, pages 252?257.
R. Soricut and D. Marcu. 2003. Sentence
level discourse parsing using syntactic and lexical
information. Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, 1:149?156.
C. Staelin. 2003. Parameter selection for support
vector machines. Hewlett-Packard Company, Tech.
Rep. HPL-2002-354R1.
V.N. Vapnik. 1995. The nature of statistical learning
theory. Springer-Verlag New York, Inc., New York,
NY, USA.
673
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 806?814,
Beijing, August 2010
Recognition of Affect, Judgment, and Appreciation in Text 
Alena Neviarouskaya 
University of Tokyo 
lena@mi.ci.i.u-
tokyo.ac.jp 
Helmut Prendinger 
Nat. Institute of Informatics 
Tokyo 
helmut@nii.ac.jp 
Mitsuru Ishizuka 
University of Tokyo 
ishizuka@i.u-
tokyo.ac.jp 
 
Abstract 
The main task we address in our research 
is classification of text using fine-grained 
attitude labels. The developed @AM sys-
tem relies on the compositionality prin-
ciple and a novel approach based on the 
rules elaborated for semantically distinct 
verb classes. The evaluation of our me-
thod on 1000 sentences, that describe 
personal experiences, showed promising 
results: average accuracy on the fine-
grained level (14 labels) was 62%, on the 
middle level (7 labels) ? 71%, and on the 
top level (3 labels) ? 88%. 
1 Introduction and Related Work 
With rapidly growing online sources aimed at 
encouraging and stimulating people?s discussions 
concerning personal, public or social issues 
(news, blogs, discussion forums, etc.), there is a 
great need in development of a computational 
tool for the analysis of people?s attitudes. Ac-
cording to the Appraisal Theory (Martin and 
White, 2005), attitude types define the specifics 
of appraisal being expressed: affect (personal 
emotional state), judgment (social or ethical ap-
praisal of other?s behaviour), and appreciation 
(evaluation of phenomena). 
To analyse contextual sentiment of a phrase or 
a sentence, rule-based approaches (Nasukawa 
and Yi, 2003; Moilanen and Pulman, 2007; Sub-
rahmanian and Reforgiato, 2008), a machine-
learning method using not only lexical but also 
syntactic features (Wilson et al, 2005), and a 
model of integration of machine learning ap-
proach with compositional semantics (Choi and 
Cardie, 2008) were proposed. With the aim to 
recognize fine-grained emotions from text on the 
level of distinct sentences, researchers have em-
ployed a keyword spotting technique (Chuang 
and Wu, 2004; Strapparava et al, 2007), a tech-
nique calculating emotion scores using Pointwise 
Mutual Information (PMI) (Kozareva et al, 
2007), an approach inspired by common-sense 
knowledge (Liu et al, 2003), rule-based linguis-
tic approaches (Boucouvalas, 2003; Chaumartin, 
2007), machine-learning methods (Alm, 2008; 
Aman and Szpakowicz, 2008; Strapparava and 
Mihalcea, 2008), and an ensemble based multi-
label classification technique (Bhowmick et al, 
2009). 
Early attempts to focus on distinct attitude 
types in the task of attitude analysis were made 
by Taboada and Grieve (2004), who determined 
a potential value of adjectives for affect, judge-
ment and appreciation by calculating the PMI 
with the pronoun-copular pairs ?I was (affect)?, 
?He was (judgement)?, and ?It was (apprecia-
tion)?, and Whitelaw et al (2005), who used a 
machine learning technique (SVM) with fine-
grained semantic distinctions in features (attitude 
type, orientation) in combination with ?bag of 
words? to classify movie reviews. However, the 
concentration only on adjectives expressing ap-
praisal and their modifiers greatly narrows the 
potential of the Whitelaw et al (2005) approach. 
In this paper we introduce our system @AM 
(ATtitude Analysis Model), which (1) classifies 
sentences according to the fine-grained attitude 
labels (nine affect categories (Izard, 1971): ?an-
ger?, ?disgust?, ?fear?, ?guilt?, ?interest?, ?joy?, 
?sadness?, ?shame?, ?surprise?; four polarity la-
bels for judgment and appreciation: ?POS jud?, 
?NEG jud?, ?POS app?, ?NEG app?; and ?neu-
tral?); (2) assigns the strength of the attitude; and 
(3) determines the level of confidence, with 
which the attitude is expressed. @AM relies on a 
compositionality principle and a novel approach 
806
based on the rules elaborated for semantically 
distinct verb classes. 
2 Lexicon for Attitide Analysis 
We built a lexicon for attitude analysis that in-
cludes: (1) attitude-conveying terms; (2) modifi-
ers; (3) ?functional? words; and (4) modal opera-
tors. 
2.1 The Core of Lexicon 
As a core of lexicon for attitude analysis, we em-
ploy an Affect database and extended version of 
the SentiFul database developed by Neviar-
ouskaya et al (2009). The affective features of 
each emotion-related word are encoded using 
nine emotion labels (?anger?, ?disgust?, ?fear?, 
?guilt?, ?interest?, ?joy?, ?sadness?, ?shame?, and 
?surprise?) and corresponding emotion intensities 
that range from 0.0 to 1.0. The original version 
of SentiFul database, which contains sentiment-
conveying adjectives, adverbs, nouns, and verbs 
annotated by sentiment polarity, polarity scores 
and weights, was manually extended using atti-
tude labels. Some examples of annotated atti-
tude-conveying words are listed in Table 1. It is 
important to note here that some words may ex-
press different attitude types (affect, judgment, 
appreciation) depending on context; such lexical 
entries were annotated by all possible categories. 
POS Word Category Intensity 
adjective honorable 
unfriendly 
POS jud 
NEG aff (sadness) 
NEG jud 
NEG app 
0.3 
0.5 
0.5 
0.5 
adverb gleefully POS aff (joy) 0.9 
noun abnormality NEG app 0.25 
verb frighten 
desire 
NEG aff (fear) 
POS aff (interest) 
POS aff (joy) 
0.8 
1.0 
0.5 
Table 1. Examples of attitude-conveying words 
and their annotations. 
2.2 Modifiers and Functional Words 
We collected 138 modifiers that have an impact 
on contextual attitude features of related words, 
phrases, or clauses. They include: 
1. Adverbs of degree (e.g., ?significantly?, 
?slightly? etc.) and affirmation (e.g., ?absolutely?, 
?seemingly?) that have an influence on the 
strength of the attitude of related words. Two 
annotators gave coefficients for intensity degree 
strengthening or weakening (from 0.0 to 2.0) to 
each adverb, and the result was averaged (e.g., 
coeff(?slightly?) = 0.2). 
2. Negation words (e.g., ?never?, ?nothing? 
etc.) reversing the polarity of related statement. 
3. Adverbs of doubt (e.g., ?scarcely?, 
?hardly? etc.) and falseness (e.g., ?wrongly? etc.) 
reversing the polarity of related statement. 
4. Prepositions (e.g., ?without?, ?despite? etc.) 
neutralizing the attitude of related words. 
5. Condition operators (e.g., ?if?, ?even 
though? etc.) that neutralize the attitude of related 
words. 
We distinguish two types of ?functional? words 
that influence contextual attitude and its strength:  
1. Intensifying adjectives (e.g., ?rising?, ?rap-
idly-growing?), nouns (e.g., ?increase?), and 
verbs (e.g., ?to grow?, ?to rocket?) that increase 
the strength of attitude of related words. 
2. Reversing adjectives (e.g., ?reduced?), 
nouns (e.g., ?termination), and verbs (e.g., ?to 
decrease?, ?to limit?, ?to diminish?), which re-
verse the prior polarity of related words. 
2.3 Modal Operators 
Consideration of the modal operators in the tasks 
of opinion mining and attitude analysis is very 
important, as they indicate a degree of person?s 
belief in the truth of the proposition, which is 
subjective in nature (Hoye, 1997). Modals are 
distinguished by their confidence level. We col-
lected modal operators of two categories: modal 
verbs (13 verbs) and modal adverbs (61 adverbs). 
Three human annotators assigned the confidence 
level ranging from 0.0 to 1.0 to each modal verb 
and adverb; these ratings were averaged (e.g., 
conf(?vaguely?) = 0.17, conf(?arguably?) = 0.63, 
conf(?would?) = 0.8, conf(?veritably?) = 1.0). 
3 Compositionality Principle 
Our algorithm for attitude classification is de-
signed based on the compositionality principle, 
according to which we determine the attitudinal 
meaning of a sentence by composing the pieces 
that correspond to lexical units or other linguistic 
constituent types governed by the rules of polari-
ty reversal, aggregation (fusion), propagation, 
domination, neutralization, and intensification, at 
various grammatical levels. 
Polarity reversal means that a phrase or 
statement containing an attitude-conveying 
807
term/phrase with prior positive polarity becomes 
negative, and vice versa. The rule of polarity re-
versal is applied in three cases: (1) negation 
word-modifier in relation with an attitude-
conveying statement (e.g., ?never? & 
POS(?succeed?) => NEG(?never succeed?)); (2) 
adverb of doubt in relation with attitude-
conveying statement (e.g., ?scarcely? & 
POS(?relax?) => NEG(?scarcely relax?)); (3) 
functional word of reversing type in relation with 
attitude-conveying statement (e.g., adjective ?re-
duced? & POS(?enthusiasm?) => NEG(?reduced 
enthusiasm?)). In the case of judgment and ap-
preciation, the use of the polarity reversal rule is 
straightforward (?POS jud? <=> ?NEG jud?, 
?POS app? <=> ?NEG app?). However, it is not 
trivial to find pairs of opposite emotions in the 
case of a fine-grained classification, except for 
?joy? and ?sadness?. Therefore, we assume that 
(1) the opposite emotion for three positive emo-
tions, i.e. ?interest?, ?joy?, and ?surprise?, is ?sad-
ness? (?POS aff? => ?sadness?); and (2) the oppo-
site emotion for six negative emotions, i.e. ?an-
ger?, ?disgust?, ?fear?, ?guilt?, ?sadness?, and 
?shame?, is ?joy? (?NEG aff? => ?joy?). 
The rules of aggregation (fusion) are as fol-
lows: (1) if polarities of attitude-conveying terms 
in adjective-noun, noun-noun, adverb-adjective, 
adverb-verb phrases have opposite directions, 
mixed polarity with dominant polarity of a pre-
modifier is assigned to the phrase (e.g., 
POS(?beautiful?) & NEG(?fight?) => POS-
neg(?beautiful fight?); NEG(?shamelessly?) & 
POS(?celebrate?) => NEG-pos(?shamelessly 
celebrate?)); otherwise (2) the resulting polarity 
is based on the equal polarities of terms, and the 
strength of attitude is measured as a maximum 
between polarity scores (intensities) of terms 
(max(score1,score2)).  
The rule of propagation is useful, as proposed 
in (Nasukawa and Yi, 2003), for the task of the 
detection of local sentiments for given subjects. 
?Propagation? verbs propagate the sentiment to-
wards the arguments; ?transfer? verbs transmit 
sentiments among the arguments. The rule of 
propagation is applied when a verb of ?propaga-
tion? or ?transfer? type is used in a phrase/clause 
and sentiment of an argument that has prior neu-
tral polarity needs to be investigated (e.g., 
PROP-POS(?to admire?) & ?his behaviour? => 
POS(?his behaviour?); ?Mr. X? & 
TRANS(?supports?) & NEG(?crime business?) 
=> NEG(?Mr. X?)).  
The rules of domination are as follows: (1) if 
polarities of a verb (this rule is applied only for 
certain classes of verbs) and an object in a clause 
have opposite directions, the polarity of verb is 
prevailing (e.g., NEG(?to deceive?) & 
POS(?hopes?) => NEG(?to deceive hopes?)); (2) 
if compound sentence joints clauses using coor-
dinate connector ?but?, the attitude features of a 
clause following after the connector are domi-
nant (e.g., ?NEG(It was hard to climb a mountain 
all night long), but POS(a magnificent view re-
warded the traveler at the morning).? => 
POS(whole sentence)). 
The rule of neutralization is applied when 
preposition-modifier or condition operator relate 
to the attitude-conveying statement (e.g., ?de-
spite? & NEG(?worries?) => NEUT(?despite 
worries?)). 
The rule of intensification means strengthen-
ing or weakening of the polarity score (intensity), 
and is applied when: 
1. adverb of degree or affirmation relates to 
attitude-conveying term (e.g., 
Pos_score(?happy?) < Pos_score(?extremely hap-
py?)); 
2. adjective or adverb is used in a compara-
tive or superlative form (e.g., Neg_score(?sad?) < 
Neg_score(?sadder?) < Neg_score (?saddest?)). 
Our method is capable of processing sentences of 
different complexity, including simple, com-
pound, complex (with complement and relative 
clauses), and complex-compound sentences. We 
employ Connexor Machinese Syntax parser 
(http://www.connexor.eu/) that returns 
lemmas, parts of speech, dependency functions, 
syntactic function tags, and morphological tags. 
When handling the parser output, we represent 
the sentence as a set of primitive clauses. Each 
clause might include Subject formation, Verb 
formation and Object formation, each of which 
may consist of a main element (subject, verb, or 
object) and its attributives and complements. For 
the processing of complex or compound sen-
tences, we build a so-called ?relation matrix?, 
which contains information about dependences 
(e.g., coordination, subordination, condition, 
contingency, etc.) between different clauses in a 
sentence. While applying the compositionality 
principle, we consecutively assign attitude fea-
808
tures to words, phrases, formations, clauses, and 
finally, to the whole sentence. 
4 Consideration of the Semantics of 
Verbs 
All sentences must include a verb, because the 
verb tells us what action the subject is perform-
ing and object is receiving. In order to elaborate 
rules for attitude analysis based on the semantics 
of verbs, we investigated VerbNet (Kipper et al, 
2007), the largest on-line verb lexicon that is or-
ganized into verb classes characterized by syn-
tactic and semantic coherence among members 
of a class. Based on the thorough analysis of 270 
first-level classes of VerbNet and their members, 
73 verb classes (1) were found useful for the task 
of attitude analysis, and (2) were further classi-
fied into 22 classes differentiated by the role that 
members play in attitude analysis and by rules 
applied to them. Our classification is shown in 
Table 2. 
For each of our verb classes, we developed set 
of rules that are applied to attitude analysis on 
the phrase/clause-level. Some verb classes (e.g., 
?Psychological state or emotional reaction?, 
?Judgment?, ?Bodily state and damage to the 
body?, ?Preservation? etc.) include verbs anno-
tated by attitude type, prior polarity orientation, 
and the strength of attitude. The attitude features 
of phrases that involve positively or negatively 
charged verbs from such classes are context-
sensitive and are defined by means of rules de-
signed for each of the class. 
As an example, we provide short description 
and rules elaborated for the subclass ?Object-
centered (oriented) emotional state?. 
Features: subject experiences emotions towards 
some stimulus; verb prior polarity: positive or 
negative; context-sensitive. 
Verb-Object rules (subject is ignored): 
1. ?Interior perspective? (subject?s inner emotion 
state or attitude): 
S & V+(?admires?) & O+(?his brave heart?) 
=> (fusion, max(V_score,O_score)) => ?POS 
aff?. 
S & V+(?admires?) & O-(?mafia leader?) => 
(verb valence dominance, V_score) => ?POS 
aff?. 
S & V-(?disdains?) & O+(?his honesty?) => 
(verb valence dominance, V_score) => ?NEG 
aff?. 
Verb class (verb samples) 
1 Psychological state or emotional reaction 
1.1 Object-centered (oriented) emotional state (adore)
1.2 Subject-driven change in emotional state (trans.)
(charm, inspire, bother) 
1.3 Subject-driven change in emotional state (intrans.)
(appeal to, grate on) 
2 Judgment 
2.1 Positive judgment (bless, honor) 
2.2 Negative judgment (blame, punish) 
3 Favorable attitude (accept, allow, tolerate) 
4 Adverse (unfavorable) attitude (discourage, forbid) 
5 Favorable or adverse calibratable changes of state 
(grow, decline) 
6 Verbs of removing 
6.1 Verbs of removing with neutral charge (delete) 
6.2 Verbs of removing with negative charge (expel) 
6.3 Verbs of removing with positive charge (evacuate)
7 Negatively charged change of state (break, crush) 
8 Bodily state and damage to the body (sicken, injure) 
9 Aspectual verbs 
9.1 Initiation, continuation of activity, and sustaining 
(begin, continue, maintain) 
9.2 Termination of activity (quit, finish) 
10 Preservation (defend, insure) 
11 Verbs of destruction and killing (damage, poison) 
12 Disappearance (disappear, die) 
13 Limitation and subjugation (confine, restrict) 
14 Assistance (succor, help) 
15 Obtaining (win, earn) 
16 Communication indicator/reinforcement of attitude 
(guess, complain, deny) 
17 Verbs of leaving (abandon, desert) 
18 Changes in social status or condition (canonize) 
19 Success and failure 
19.1 Success (succeed, manage) 
19.2 Failure (fail, flub) 
20 Emotional nonverbal expression (smile, weep) 
21 Social interaction (marry, divorce) 
22 Transmitting verbs (supply, provide) 
Table 2. Verb classes for attitude analysis. 
S & V-(?disdains?) & O-(?criminal activities?) 
=> (fusion, max(V_score,O_score)) => ?NEG 
aff?. 
2. ?Exterior perspective? (social/ethical judg-
ment): 
S & V+(?admires?) & O+(?his brave heart?) 
=> (fusion, max(V_score,O_score)) => ?POS 
jud?. 
S & V+(?admires?) & O-(?mafia leader?) => 
(verb valence reversal, max(V_score,O_score)) 
=> ?NEG jud?. 
S & V-(?disdains?) & O+(?his honesty?) => 
(verb valence dominance, 
max(V_score,O_score)) => ?NEG jud?. 
S & V-(?disdains?) & O-(?criminal activities?) 
=> (verb valence reversal, 
max(V_score,O_score)) => ?POS jud?. 
809
3. In case of neutral object => attitude type and 
prior polarity of verb, verb score (V_score). 
Verb-PP (prepositional phrase) rules: 
1. In case of negatively charged verb and PP 
starting with ?from? => verb dominance:  
S & V-(?suffers?) & PP-(?from illness?) => in-
terior: ?NEG aff?; exterior: ?NEG jud?. 
S & V-(?suffers?) & PP+ (?from love?) => inte-
rior: ?NEG aff?; exterior: ?NEG jud?. 
2. In case of positively charged verb and PP 
starting with ?in?/?for? => treat PP the same way 
as object (see above): 
S & V+(?believes?) & PP-(?in evil?) => inte-
rior: ?POS aff?; exterior: ?NEG jud?. 
S & V+(?believes?) & PP+(?in kindness?) => 
interior: ?POS aff?; exterior: ?POS jud?. 
In the majority of rules the strength of attitude is 
measured as a maximum between attitude scores 
(for example, the attitude conveyed by ?to suffer 
from grave illness? is stronger than that of ?to 
suffer from slight illness?). 
In contrast to the rules of ?Object-centered 
(oriented) emotional state? subclass, which ig-
nore attitude features of a subject in a sentence, 
the rules elaborated for the ?Subject-driven 
change in emotional state (trans.)? disregard the 
attitude features of object, as in sentences involv-
ing members of this subclass object experiences 
emotion, and subject causes the emotional state. 
For example (due to limitation of space, here and 
below we provide only some cases): 
S(?Classical music?) & V+(?calmed?) & O-
(?disobedient child?) => interior: ?POS aff?; exte-
rior: ?POS app?. 
S-(?Fatal consequences of GM food intake?) & 
V-(?frighten?) & O(?me?) => interior: ?NEG aff?; 
exterior: ?NEG app?. 
The Verb-Object rules for the ?Judgment? sub-
classes, namely ?Positive judgment? and ?Nega-
tive judgment?, are very close to those defined 
for the subclass ?Object-centered (oriented) 
emotional state?. However, Verb-PP rules have 
some specifics: for both positive and negative 
judgment verbs, we treat PP starting with 
?for?/?of?/?as? the same way as object in Verb-
Object rules. For example: 
S(?He?) & V-(?blamed?) & O+(?innocent per-
son?) => interior: ?NEG jud?; exterior: ?NEG 
jud?. 
S(?They?) & V-(?punished?) & O(?him?) & PP-
(?for his misdeed?) => interior: ?NEG jud?; exte-
rior: ?POS jud?. 
Verbs from classes ?Favorable attitude? and 
?Adverse (unfavorable) attitude? have prior neu-
tral polarity and positive or negative reinforce-
ment, correspondingly, that means that they only 
impact on the polarity and strength of non-
neutral phrase (object in a sentence written in 
active voice, or subject in a sentence written in 
passive voice, or PP in case of some verbs). The 
rules are: 
1. If verb belongs to the ?Favorable attitude? 
class and the polarity of phrase is not neutral, 
then the attitude score of the phrase is intensified 
(symbol ?^? means intensification): 
S(?They?) & [V pos. reinforcement](?elected?) 
& O+(?fair judge?) => ?POS app?; O_score^. 
S(?They?) & [V pos. reinforcement](?elected?) 
& O-(?corrupt candidate?) => ?NEG app?; 
O_score^. 
2. If verb belongs to the ?Adverse (unfavorable) 
attitude? class and the polarity of phrase is not 
neutral, then the polarity of phrase is reversed 
and score is intensified: 
S(?They?) & [V neg. reinforce-
ment](?prevented?) & O-(?the spread of disease?) 
=> ?POS app?; O_score^. 
S+(?His achievements?) & [V neg. reinforce-
ment](?were overstated?) => ?NEG app?; 
S_score^. 
Below are examples of processing the sentences 
with verbs from ?Verbs of removing? class. 
?Verbs of removing with neutral charge?: 
S(?The tape-recorder?) & [V neutral 
rem.](?automatically ejects?) & O-neutral(?the 
tape?) => neutral. 
S(?The safety invention?) & [V neutral 
rem.](?ejected?) & O(?the pilot?) & PP-(?from 
burning plane?) => ?POS app?; PP_score^. 
?Verbs of removing with negative charge?: 
S(?Manager?) & [V neg. rem.](?fired?) & O-
(?careless employee?) & PP(?from the company?) 
=> ?POS app?; max(V_score,O_score).  
?Verbs of removing with positive charge?: 
S(?They?) & [V pos. rem.](?evacuated?) & 
O(?children?) & PP-(?from dangerous place?) => 
?POS app?; max(V_score,PP_score). 
Along with modal verbs and modal adverbs, 
members of the ?Communication indica-
tor/reinforcement of attitude? verb class also in-
810
dicate the confidence level or degree of certainty 
concerning given opinion. Features are: subject 
(communicator) expresses statement 
with/without attitude; statement is PP starting 
with ?of?, ?on?, ?against?, ?about?, ?concerning?, 
?regarding?, ?that?, ?how? etc.; ground: positive 
or negative; reinforcement: positive or negative. 
The rules are: 
1. If the polarity of expressed statement is neu-
tral, then the attitude is neutral: 
S(?Professor?) & [V pos. ground, pos. rein-
forcement, confidence:0.83](?dwelled?) & PP-
neutral(?on a question?) => neutral. 
2. If the polarity of expressed statement is not 
neutral and the reinforcement is positive, then the 
score of the statement (PP) is intensified: 
S(?Jane?) & [V neg. ground, pos. reinforce-
ment, confidence:0.8](?is complaining?) & PP-
(?of a headache again?) => ?NEG app?; 
PP_score^; confidence:0.8. 
3. If the polarity of expressed statement is not 
neutral and reinforcement is negative, then the 
polarity of the statement (PP) is reversed and 
score is intensified: 
S(?Max?) & [V neg. ground, neg. reinforce-
ment, confidence:0.2](?doubt?) & PP-{?that? 
S+(?his good fortune?) & [V termination](?will 
ever end?)} => ?POS app?; PP_score^; confi-
dence:0.2.  
In the last example, to measure the sentiment of 
PP, we apply rule for the verb ?end? from the 
?Termination of activity? class, which reverses 
the non-neutral polarity of subject (in intransitive 
use of verb) or object (in transitive use of verb). 
For example, the polarity of both sentences ?My 
whole enthusiasm and excitement disappear like 
a bubble touching a hot needle? and ?They dis-
continued helping children? is negative. 
5 Decision on Attitude Label 
The decision on the most appropriate final label 
for the clause, in case @AM annotates it using 
different attitude types according to the words 
with multiple annotations (e.g., see word ?un-
friendly? in Table 1) or based on the availability 
of the words conveying different attitude types, 
is made based on the analysis of: 
1) morphological tags of nominal heads and 
their premodifiers in the clause (e.g., first person 
pronoun, third person pronoun, demonstrative 
pronoun, nominative or genitive noun, etc.); 
2) the sequence of hypernymic semantic re-
lations of a particular noun in WordNet (Miller, 
1990), which allows to determine its conceptual 
domain (e.g., ?person, human being?, ?artifact?, 
?event?, etc.);  
3) the annotations from the Stanford 
Named Entity Recognizer (Finkel et al 2005) 
that labels PERSON, ORGANIZATION, and 
LOCATION entities.  
For ex., ?I feel highly unfriendly attitude towards 
me? conveys emotion (?NEG aff?: ?sadness?), 
while ?The shop assistant?s behavior was really 
unfriendly? and ?Plastic bags are environment 
unfriendly? express judgment (?NEG jud?) and 
appreciation (?NEG app?), correspondingly. 
6 Evaluation 
For the experiments, we used our own data set, 
as, to the best of our knowledge, there is no pub-
licly available data set of sentences annotated by 
the fine-grained labels proposed in our work. In 
order to evaluate the performance of our algo-
rithm, we created the data set of sentences ex-
tracted from personal stories about life expe-
riences that were anonymously published on the 
Experience Project website 
(www.experienceproject.com), where 
people share personal experiences, thoughts, 
opinions, feelings, passions, and confessions 
through the network of personal stories. With 
over 4 million experiences accumulated (as of 
February 2010), Experience Project is a perfect 
source for researchers interested in studying dif-
ferent types of attitude expressed through text. 
6.1 Data Set Description 
For our experiment we extracted 1000 sentences1 
from various stories grouped by topics within 13 
different categories, such as ?Arts and entertain-
ment?, ?Current events?, ?Education?, ?Family 
and friends?, ?Health and wellness?, ?Relation-
ships and romance? and others, on the Expe-
rience Project website. Sentences were collected 
from 358 distinct topic groups, such as ?I still 
remember September 11?, ?I am intelligent but 
airheaded?, ?I think bullfighting is cruel?, ?I quit 
smoking?, ?I am a fashion victim?, ?I was 
adopted? and others. 
                                                 
1 This annotated data set is freely available upon request. 
811
We considered three hierarchical levels of atti-
tude labels in our experiment (see Figure 1). 
Three independent annotators labeled the sen-
tences with one of 14 categories from the ALL 
level and a corresponding score (the strength or 
intensity value). These annotations were further 
interpreted using labels from the MID and the 
TOP levels. Fleiss? Kappa coefficient was used 
as a measure of reliability of human raters? anno-
tations. The agreement coefficient on 1000 sen-
tences was 0.53 on ALL level, 0.57 on MID level, 
and 0.73 on TOP level. 
Only those sentences, on which at least two 
out of three human raters completely agreed, 
were included in the gold standards for our expe-
riment. Three gold standards were created ac-
cording to the hierarchy of attitude labels. Fleiss? 
Kappa coefficients are 0.62, 0.63, and 0.74 on 
ALL, MID, and TOP levels, correspondingly. 
Table 3 shows the distributions of labels in the 
gold standards. 
ALL level MID level 
Label Number Label Number 
anger 45 POS aff 233 
disgust 21 NEG aff 332 
fear 54 POS jud 66 
guilt 22 NEG jud 78 
interest 84 POS app 100 
joy 95 NEG app 29 
sadness 133 neutral 87 
shame 18 total 925 
surprise 36  
POS jud 66 TOP level 
NEG jud 78 Label Number 
POS app 100 POS 437 
NEG app 29 NEG 473 
neutral 87 neutral 87 
total 868 total 997 
Table 3. Label distributions in gold standards. 
6.2 Results 
The results of a simple method selecting the atti-
tude label with the maximum intensity from the 
annotations of sentence tokens found in the data-
base were considered as the baseline. After 
processing each sentence from the data set by the 
baseline method and our @AM system, we 
measured averaged accuracy, precision, recall, 
and F-score for each label in ALL, MID, and 
TOP levels. The results are shown in Table 4. 
As seen from the obtained results, our algo-
rithm performed with high accuracy significantly 
surpassing the baselines in all levels of attitude 
hierarchy, thus demonstrating the contribution of 
the sentence parsing and our hand-crafted rules 
to the reliable recognition of attitude from text. 
Two-tailed t-tests with significance level of 0.05 
showed that the differences in accuracy between 
the baseline method and our @AM system are 
statistically significant (p<0.001) in fine-grained 
as well as coarse-grained classifications. 
In the case of fine-grained attitude recognition 
(ALL level), the highest precision was obtained 
for ?shame? (0.923) and ?NEG jud? (0.889), 
while the highest recall was received for ?sad-
ness? (0.917) and ?joy? (0.905) emotions at the 
cost of low precision (0.528 and 0.439, corre-
spondingly). The algorithm performed with the 
worst results in recognition of ?NEG app? and 
?neutral?. 
The analysis of a confusion matrix for the 
ALL level revealed the following top confusions 
of our system: (1) ?anger?, ?fear?, ?guilt?, ?shame?, 
?NEG jud?, ?NEG app? and ?neutral? were pre-
dominantly incorrectly predicted as ?sadness? 
(for ex., @AM resulted in ?sadness? for the sen-
tence ?I know we have several months left before 
the election, but I am already sick and tired of 
seeing the ads on TV?, while human annotations 
were ?anger?/?anger?/?disgust?); (2) ?interest?, 
?POS jud? and ?POS app? were mostly confused 
with ?joy? by our algorithm (e.g., @AM classi-
fied the sentence ?It?s one of those life changing 
artifacts that we must have in order to have hap-
pier, healthier lives? as ?joy?(-ful), while human 
annotations were ?POS app?/?POS 
app?/?interest?). 
Our system achieved high precision for all 
categories on the MID level (Table 4), with the 
exception of ?NEG app? and ?neutral?, although 
    
TOP POS NEG neutral
    
MID POS aff POS jud 
POS 
app NEG aff 
NEG 
jud 
NEG 
app neutral
        
ALL interest joy surprise POS jud 
POS 
app anger disgust fear guilt sadness shame
NEG 
jud 
NEG 
app neutral
Figure 1. Hierarchy of attitude labels. 
812
high recall was obtained only in the case of cate-
gories related to affect (?POS aff?, ?NEG aff?). 
These results indicate that affect sensing is easier 
than recognition of judgment or appreciation 
from text. TOP level results (Table 4) show that 
our algorithm classifies sentences that convey 
positive or negative sentiment with high accura-
cy (92% and 91%, correspondingly). On the oth-
er hand, ?neutral? sentences still pose a challenge. 
The analysis of errors revealed that system re-
quires common sense or additional context to 
deal with sentences like ?All through my life I?ve 
felt like I?m second fiddle? (gold standard: ?sad-
ness?; @AM: ?neutral?) or ?For me every minute 
on my horse is alike an hour in heaven!? (gold 
standard: ?joy?; @AM: ?neutral?).  
We also evaluated the system performance 
with regard to attitude intensity estimation. The 
percentage of attitude-conveying sentences (not 
considering neutral ones), on which the result of 
our system conformed to the fine-grained gold 
standard (ALL level), according to the measured 
distance between intensities given by human ra-
ters (averaged values) and those obtained by our 
system is shown in Table 5. As seen from the 
table, our system achieved satisfactory results in 
estimation of the strength of attitude expressed 
through text. 
 
Range of intensity 
difference 
Percent of sen-
tences, % 
[0.0 ? 0.2] 55.5 
(0.2 ? 0.4] 29.5 
(0.4 ? 0.6] 12.2 
(0.6 ? 0.8] 2.6 
(0.8 ? 1.0] 0.2 
Table 5. Results on intensity. 
7 Conclusions 
In this paper we introduced @AM, which is so 
far, to the best of our knowledge, the only system 
classifying sentences using fine-grained attitude 
types, and extensively dealing with the semantics 
of verbs in attitude analysis. Our composition 
approach broadens the coverage of sentences 
with complex contextual attitude. The evaluation 
results indicate that @AM achieved reliable re-
sults in the task of textual attitude analysis. The 
limitations include dependency on lexicon and 
on accuracy of the parser. The primary objective 
for the future research is to develop a method for 
the extraction of reasons behind the expressed 
attitude. 
Level Label Baseline method @AM Accuracy Precision Recall F-score Accuracy Precision Recall F-score 
ALL 
anger 
0.437 
0.742 0.511 0.605 
0.621 
0.818 0.600 0.692 
disgust 0.600 0.857 0.706 0.818 0.857 0.837 
fear 0.727 0.741 0.734 0.768 0.796 0.782 
guilt 0.667 0.364 0.471 0.833 0.455 0.588 
interest 0.380 0.357 0.368 0.772 0.524 0.624 
joy 0.266 0.579 0.364 0.439 0.905 0.591 
sadness 0.454 0.632 0.528 0.528 0.917 0.670 
shame 0.818 0.500 0.621 0.923 0.667 0.774 
surprise 0.625 0.694 0.658 0.750 0.833 0.789 
POS jud 0.429 0.227 0.297 0.824 0.424 0.560 
NEG jud 0.524 0.141 0.222 0.889 0.410 0.561 
POS app 0.349 0.150 0.210 0.755 0.400 0.523 
NEG app 0.250 0.138 0.178 0.529 0.310 0.391 
neutral 0.408 0.483 0.442 0.559 0.437 0.490 
MID 
POS aff 
0.524 
0.464 0.695 0.557 
0.709 
0.668 0.888 0.762 
NEG aff 0.692 0.711 0.701 0.765 0.910 0.831 
POS jud 0.405 0.227 0.291 0.800 0.424 0.554 
NEG jud 0.458 0.141 0.216 0.842 0.410 0.552 
POS app 0.333 0.150 0.207 0.741 0.400 0.519 
NEG app 0.222 0.138 0.170 0.474 0.310 0.375 
neutral 0.378 0.483 0.424 0.514 0.437 0.472 
TOP 
POS 
0.732 
0.745 0.796 0.770 
0.879 
0.918 0.920 0.919 
NEG 0.831 0.719 0.771 0.912 0.922 0.917 
neutral 0.347 0.483 0.404 0.469 0.437 0.452 
Table 4. Results of the evaluation of performance of the baseline method and @AM system. 
813
References 
Alm, Cecilia O. 2008. Affect in Text and Speech. PhD 
Dissertation. University of Illinois at Urbana-
Champaign. 
Aman, Saima, and Stan Szpakowicz. 2008. Using 
Roget's Thesaurus for Fine-Grained Emotion Rec-
ognition. Proceedings of the Third International 
Joint Conference on Natural Language Processing, 
Hyderabad, India, pp. 296-302. 
Bhowmick, Plaban K., Anupam Basu, and Pabitra 
Mitra. 2009. Reader Perspective Emotion Analysis 
in Text through Ensemble based Multi-Label Clas-
sification Framework. Computer and Information 
Science, 2 (4): 64-74. 
Boucouvalas, Anthony C. 2003. Real Time Text-to-
Emotion Engine for Expressive Internet Communi-
cations. Being There: Concepts, Effects and Mea-
surement of User Presence in Synthetic Environ-
ments, Ios Press, pp. 306-318. 
Chaumartin, Francois-Regis. 2007. UPAR7: A Know-
ledge-based System for Headline Sentiment Tag-
ging. Proceedings of the SemEval-2007 Interna-
tional Workshop, pp. 422-425. 
Choi, Yejin, and Claire Cardie. 2008. Learning with 
Compositional Semantics as Structural Inference 
for Subsentential Sentiment Analysis. Proceedings 
of the Conference on Empirical Methods in Natural 
Language Processing, pp. 793-801. 
Chuang, Ze-Jing, and Chung-Hsien Wu. 2004. Multi-
modal Emotion Recognition from Speech and Text. 
Computational Linguistic and Chinese Language 
Processing, 9(2): 45-62. 
Finkel, Jenny R., Trond Grenager, and Christopher 
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs 
Sampling. Proceedings of the 43nd Annual Meet-
ing of the ACL, pp. 363-370. 
Hoye, Leo. 1997. Adverbs and Modality in English. 
New York: Addison Wesley Longman Inc. 
Izard, Carroll E. 1971. The Face of Emotion. New 
York: Appleton-Century-Crofts. 
Kipper, Karin, Anna Korhonen, Neville Ryant, and 
Martha Palmer. 2007. A Large-scale Classification 
of English Verbs. Language Resources and Evalu-
ation, 42 (1): 21-40. 
Kozareva, Zornitsa, Borja Navarro, Sonia Vazquez, 
and Andres Montoyo, A. 2007. UA-ZBSA: A 
Headline Emotion Classification through Web In-
formation. Proceedings of the SemEval-2007 In-
ternational Workshop, pp. 334-337. 
Liu, Hugo, Henry Lieberman, and Ted Selker. 2003. 
A Model of Textual Affect Sensing Using Real-
World Knowledge. Proceedings of IUI-2003, pp. 
125-132. 
Martin, James R., and Peter R.R. White. 2005. The 
Language of Evaluation: Appraisal in English. 
Palgrave, London, UK. 
Miller, George A. 1990. WordNet: An On-line Lexi-
cal Database. International Journal of Lexicogra-
phy, Special Issue, 3 (4): 235-312. 
Moilanen, Karo, and Stephen Pulman. 2007. Senti-
ment Composition. Proceedings of the Recent Ad-
vances in Natural Language Processing Interna-
tional Conference, pp. 378-382. 
Nasukawa, Tetsuya, and Jeonghee Yi. 2003. Senti-
ment Analysis: Capturing Favorability using Natu-
ral Language Processing. Proceedings of the 2nd 
International Conference on Knowledge Capture, 
pp. 70-77. 
Neviarouskaya, Alena, Helmut Prendinger, and Mit-
suru Ishizuka. 2009. SentiFul: Generating a Relia-
ble Lexicon for Sentiment Analysis. Proceedings 
of the International Conference on Affective Com-
puting and Intelligent Interaction, IEEE, Amster-
dam, Netherlands, pp. 363-368. 
Strapparava, Carlo, and Rada Mihalcea. 2008. Learn-
ing to Identify Emotions in Text. Proceedings of 
the 2008 ACM Symposium on Applied Computing, 
Fortaleza, Brazil, pp. 1556-1560. 
Strapparava, Carlo, Alessandro Valitutti, and Oliviero 
Stock. 2007. Dances with Words. Proceedings of 
the International Joint Conference on Artificial In-
telligence, pp. 1719-1724. 
Subrahmanian, V.S., and Diego Reforgiato. 2008. 
AVA: Adjective-Verb-Adverb Combinations for 
Sentiment Analysis. Intelligent Systems, IEEE, 23 
(4): 43-50. 
Taboada, Maite, and Jack Grieve. 2004. Analyzing 
Appraisal Automatically. Proceedings of AAAI 
Spring Symposium on Exploring Attitude and Af-
fect in Text, pp.158-161. 
Whitelaw, Casey, Navendu Garg, and Shlomo Arga-
mon. 2005. Using Appraisal Groups for Sentiment 
Analysis. Proceedings of the 14th ACM Interna-
tional Conference on Information and Knowledge 
Management, CIKM, Bremen, Germany, pp. 625-
631. 
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann. 
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. Proceedings of HLT-
EMNLP-2005, ACL, pp. 347-354. 
814
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 80?88,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
@AM: Textual Attitude Analysis Model 
 
 
Alena Neviarouskaya Helmut Prendinger Mitsuru Ishizuka
University of Tokyo Nat. Institute of Informatics University of Tokyo 
7-3-1 Hongo, Bunkyo-ku 2-1-2 Hitotsubashi Chiyoda 7-3-1 Hongo, Bunkyo-ku 
Tokyo 113-8656, Japan Tokyo 101-8430, Japan Tokyo 113-8656, Japan 
lena@mi.ci.i.u-tokyo.ac.jp helmut@nii.ac.jp ishizuka@i.u-tokyo.ac.jp
 
 
 
 
 
 
Abstract 
The automatic analysis and classification of 
text using fine-grained attitude labels is the 
main task we address in our research. The de-
veloped @AM system relies on compositio-
nality principle and a novel approach based on 
the rules elaborated for semantically distinct 
verb classes. The evaluation of our method on 
1000 sentences, that describe personal expe-
riences, showed promising results: average 
accuracy on fine-grained level was 62%, on 
middle level ? 71%, and on top level ? 88%. 
1 Introduction and Related Work 
With rapidly growing online sources aimed at en-
couraging and stimulating people?s discussions 
concerning personal, public or social issues (news, 
blogs, discussion forums, etc.), there is a great 
need in development of a computational tool for 
the analysis of people?s attitudes. According to the 
Appraisal Theory (Martin and White, 2005), atti-
tude types define the specifics of appraisal being 
expressed: affect (personal emotional state), judg-
ment (social or ethical appraisal of other?s behav-
iour), and appreciation (evaluation of phenomena). 
To analyse contextual sentiment (polarity) of a 
phrase or a sentence, rule-based approaches (Na-
sukawa and Yi, 2003; Mulder et al, 2004; Moila-
nen and Pulman, 2007; Subrahmanian and 
Reforgiato, 2008), a machine-learning method us-
ing not only lexical but also syntactic features 
(Wilson et al, 2005), and a model of integration of 
machine learning approach with compositional 
semantics (Choi and Cardie, 2008) were proposed. 
With the aim to recognize fine-grained emotions 
from text on the level of distinct sentences, re-
searchers have employed a keyword spotting tech-
nique (Olveres et al, 1998; Chuang and Wu, 2004; 
Strapparava et al, 2007), a technique calculating 
emotion scores using Pointwise Mutual Informa-
tion (PMI) (Kozareva et al, 2007), an approach 
inspired by common-sense knowledge (Liu et al, 
2003), rule-based linguistic approaches (Boucou-
valas, 2003; Chaumartin, 2007), machine-learning 
methods (Alm, 2008; Aman and Szpakowicz, 
2008; Strapparava and Mihalcea, 2008), and an 
ensemble based multi-label classification technique 
(Bhowmick et al, 2009). 
Early attempts to focus on distinct attitude types 
in the task of attitude analysis were made by Ta-
boada and Grieve (2004), who determined a poten-
tial value of adjectives for affect, judgement and 
appreciation by calculating the PMI with the pro-
noun-copular pairs ?I was (affect)?, ?He was 
(judgement)?, and ?It was (appreciation)?, and 
Whitelaw et al (2005), who used machine learning 
technique (SVM) with fine-grained semantic dis-
tinctions in features (attitude type, orientation) in 
combination with ?bag of words? to classify movie 
reviews. However, the concentration only on ad-
jectives, that express appraisal, and their modifiers, 
greatly narrows the potential of the Whitelaw et 
al.?s (2005) approach. 
In this paper we introduce our system @AM 
(ATtitude Analysis Model), which (1) classifies 
80
sentences according to the fine-grained attitude 
labels (nine affect categories (Izard, 1971): ?anger?, 
?disgust?, ?fear?, ?guilt?, ?interest?, ?joy?, ?sadness?, 
?shame?, ?surprise?; four polarity labels for judg-
ment and appreciation: ?POS jud?, ?NEG jud?, 
?POS app?, ?NEG app?; and ?neutral?); (2) assigns 
the strength of the attitude; and (3) determines the 
level of confidence, with which the attitude is ex-
pressed. @AM relies on compositionality principle 
and a novel approach based on the rules elaborated 
for semantically distinct verb classes. 
2 Lexicon for Attitide Analysis 
We built the lexicon for attitude analysis that in-
cludes: (1) attitude-conveying terms; (2) modifiers; 
(3) ?functional? words; and (4) modal operators. 
2.1 The Core of Lexicon 
As a core of lexicon for attitude analysis, we em-
ploy Affect database and extended version of Sen-
tiFul database developed by Neviarouskaya et al 
(2009). The affective features of each emotion-
related word are encoded using nine emotion labels 
(?anger?, ?disgust?, ?fear?, ?guilt?, ?interest?, ?joy?, 
?sadness?, ?shame?, and ?surprise?) and correspond-
ing emotion intensities that range from 0.0 to 1.0. 
The original version of SentiFul database, which 
contains sentiment-conveying adjectives, adverbs, 
nouns, and verbs annotated by sentiment polarity, 
polarity scores and weights, was manually ex-
tended using attitude labels. Some examples of 
annotated attitude-conveying words are listed in 
Table 1. It is important to note here that some 
words could express different attitude types (affect, 
judgment, appreciation) depending on context; 
such lexical entries were annotated by all possible 
categories. 
 
POS Word Category Intensity 
adjective honorable 
unfriendly 
POS jud 
NEG aff (sadness) 
NEG jud 
NEG app 
0.3 
0.5 
0.5 
0.5 
adverb gleefully POS aff (joy) 0.9 
noun abnormality NEG app 0.25 
verb frighten 
desire 
NEG aff (fear) 
POS aff (interest) 
POS aff (joy) 
0.8 
1.0 
0.5 
Table 1. Examples of attitude-conveying words and 
their annotations. 
2.2 Modifiers and Functional Words 
The robust attitude analysis method should rely not 
only on attitude-conveying terms, but also on mod-
ifiers and contextual valence shifters (term intro-
duced by Polanyi and Zaenen (2004)), which are 
integral parts of our lexicon. 
We collected 138 modifiers that have an impact 
on contextual attitude features of related words, 
phrases, or clauses. They include: 
1. Adverbs of degree (e.g., ?significantly?, 
?slightly? etc.) and adverbs of affirmation (e.g., 
?absolutely?, ?seemingly?) that have an influence on 
the strength of attitude of the related words. 
2. Negation words (e.g., ?never?, ?nothing? 
etc.) that reverse the polarity of related statement. 
3. Adverbs of doubt (e.g., ?scarcely?, ?hardly? 
etc.) and adverbs of falseness (e.g., ?wrongly? etc.) 
that reverse the polarity of related statement. 
4. Prepositions (e.g., ?without?, ?despite? etc.) 
that neutralize the attitude of related words. 
5. Condition operators (e.g., ?if?, ?even though? 
etc.) that neutralize the attitude of related words. 
Adverbs of degree and adverbs of affirmation af-
fect on related verbs, adjectives, or another adverb. 
Two annotators gave coefficients for intensity de-
gree strengthening or weakening (from 0.0 to 2.0) 
to each of 112 collected adverbs, and the result was 
averaged (e.g., coeff(?perfectly?) = 1.9, 
coeff(?slightly?) = 0.2). 
We distinguish two types of ?functional? words 
that influence contextual attitude and its strength:  
1. Intensifying adjectives (e.g., ?rising?, ?rap-
idly-growing?), nouns (e.g., ?increase?, ?up-tick?), 
and verbs (e.g., ?to grow?, ?to rocket?), which in-
crease the strength of attitude of related words. 
2. Reversing adjectives (e.g., ?reduced?), nouns 
(e.g., ?termination?, ?reduction?), and verbs (e.g., 
?to decrease?, ?to limit?, ?to diminish?), which re-
verse the prior polarity of related words. 
2.3 Modal Operators 
Consideration of the modal operators in the tasks 
of opinion mining and attitude analysis is very im-
portant, as they indicate a degree of person?s belief 
in the truth of the proposition, which is subjective 
in nature. Modal expressions point to likelihood 
and clearly involve the speaker?s judgment (Hoye, 
1997). Modals are distinguished by the confidence 
level. 
81
We collected modal operators of two categories:  
1. Modal verbs (13 verbs). 
2. Modal adverbs (61 adverbs). 
Three human annotators assigned the confidence 
level, which ranges from 0.0 to 1.0, to each modal 
verb and adverb; these ratings were averaged (e.g., 
conf(?vaguely?) = 0.17, conf(?may?) = 0.27, 
conf(?arguably?) = 0.63, conf(?would?) = 0.8, 
conf(?veritably?) = 1.0).  
3 Compositionality Principle 
Words in a sentence are interrelated and, hence, 
each of them can influence the overall meaning 
and attitudinal bias of a statement. The algorithm 
for the attitude classification is designed based on 
the compositionality principle, according to which 
we determine the attitudinal meaning of a sentence 
by composing the pieces that correspond to lexical 
units or other linguistic constituent types governed 
by the rules of polarity reversal, aggregation (fu-
sion), propagation, domination, neutralization, and 
intensification, at various grammatical levels. 
Polarity reversal means that phrase or statement 
containing attitude-conveying term/phrase with 
prior positive polarity becomes negative, and vice 
versa. The rule of polarity reversal is applied in 
three cases: (1) negation word-modifier in relation 
with attitude-conveying statement (e.g., ?never? & 
POS(?succeed?) => NEG(?never succeed?)); (2) 
adverb of doubt in relation with attitude-conveying 
statement (e.g., ?scarcely? & POS(?relax?) => 
NEG(?scarcely relax?)); (3) functional word of 
reversing type in relation with attitude-conveying 
statement (e.g., adjective ?reduced? & 
POS(?enthusiasm?) => NEG(?reduced enthusi-
asm?)). In the case of judgment and appreciation, 
the use of polarity reversal rule is straightforward 
(?POS jud? <=> ?NEG jud?, ?POS app? <=> ?NEG 
app?). However, it is not trivial to find pairs of op-
posite emotions in the case of a fine-grained classi-
fication, except for ?joy? and ?sadness?. Therefore, 
we assume that (1) opposite emotion for three posi-
tive emotions, such as ?interest?, ?joy?, and ?sur-
prise?, is ?sadness? (?POS aff? => ?sadness?); and 
(2) opposite emotion for six negative emotions, 
such as ?anger?, ?disgust?, ?fear?, ?guilt?, ?sadness?, 
and ?shame?, is ?joy? (?NEG aff? => ?joy?). 
The rules of aggregation (fusion) are as follows: 
(1) if polarities of attitude-conveying terms in ad-
jective-noun, noun-noun, adverb-adjective, adverb-
verb phrases have opposite directions, mixed po-
larity with dominant polarity of a descriptive term 
is assigned to the phrase (e.g., POS(?beautiful?) & 
NEG(?fight?) => POS-neg(?beautiful fight?); 
NEG(?shamelessly?) & POS(?celebrate?) => NEG-
pos(?shamelessly celebrate?)); otherwise (2) the 
resulting polarity is based on the equal polarities of 
terms, and the strength of attitude is measured as a 
maximum between polarity scores (intensities) of 
terms (max(score1,score2)).  
The rule of propagation is useful, as proposed in 
(Nasukawa and Yi, 2003), for the task of detection 
of local sentiments for given subjects. ?Propaga-
tion? verbs propagate the sentiment towards the 
arguments; ?transfer? verbs transmit sentiments 
among the arguments. The rule of propagation is 
applied when verb of ?propagation? or ?transfer? 
type is used in a phrase/clause and sentiment of an 
argument that has prior neutral polarity needs to be 
investigated (e.g., PROP-POS(?to admire?) & ?his 
behaviour? => POS(?his behaviour?); ?Mr. X? & 
TRANS(?supports?) & NEG(?crime business?) => 
NEG(?Mr. X?)).  
The rules of domination are as follows: (1) if po-
larities of verb (this rule is applied only for certain 
classes of verbs) and object in a clause have oppo-
site directions, the polarity of verb is prevailing 
(e.g., NEG(?to deceive?) & POS(?hopes?) => 
NEG(?to deceive hopes?)); (2) if compound sen-
tence joints clauses using coordinate connector 
?but?, the attitude features of a clause following 
after the connector are dominant (e.g., ?NEG(It 
was hard to climb a mountain all night long), but 
POS(a magnificent view rewarded the traveler at 
the morning).? => POS(whole sentence)). 
The rule of neutralization is applied when 
preposition-modifier or condition operator relate to 
the attitude-conveying statement (e.g., ?despite? & 
NEG(?worries?) => NEUT(?despite worries?)). 
The rule of intensification means strengthening 
or weakening of the polarity score (intensity), and 
is applied when: 
1. adverb of degree or affirmation relates to at-
titude-conveying term (e.g., Pos_score(?extremely 
happy?) > Pos_score(?happy?)); 
2. adjective or adverb is used in a comparative 
or superlative form (e.g., Neg_score(?sad?) < 
Neg_score(?sadder?) < Neg_score (?saddest?)). 
Our method is capable of processing sentences of 
different complexity, including simple, compound, 
complex (with complement and relative clauses), 
82
and complex-compound sentences. To understand 
how words and concepts relate to each other in a 
sentence, we employ Connexor Machinese Syntax 
parser (http://www.connexor.eu/) that 
returns lemmas, parts of speech, dependency func-
tions, syntactic function tags, and morphological 
tags. When handling the parser output, we repre-
sent the sentence as a set of primitive clauses. Each 
clause might include Subject formation, Verb for-
mation and Object formation, each of which may 
consist of a main element (subject, verb, or object) 
and its attributives and complements. For the 
processing of complex or compound sentences, we 
build a so-called ?relation matrix?, which contains 
information about dependences (e.g., coordination, 
subordination, condition, contingency, etc.) be-
tween different clauses in a sentence. 
The annotations of words are taken from our at-
titude-conveying lexicon. The decision on most 
appropriate label, in case of words with multiple 
annotations (e.g., word ?unfriendly? in Table 1), is 
made based on (1) the analysis of morphological 
tags of nominal heads and their premodifiers in the 
sentence (e.g., first person pronoun, third person 
pronoun, demonstrative pronoun, nominative or 
genitive noun, etc.); (2) the analysis of the se-
quence of hypernymic semantic relations of a par-
ticular noun in WordNet (Miller, 1990), which 
allows to determine its conceptual domain (e.g., 
?person, human being?, ?artifact?, ?event?, etc.). 
For ex., ?I feel highly unfriendly attitude towards 
me? conveys ?NEG aff? (?sadness?), while ?Shop 
assistant?s behavior was really unfriendly? and 
?Plastic bags are environment unfriendly? express 
?NEG jud? and ?NEG app?, correspondingly. 
While applying the compositionality principle, 
we consecutively assign attitude features to words, 
phrases, formations, clauses, and finally, to the 
whole sentence. 
4 Consideration of the Semantics of Verbs 
All sentences must include a verb, because the 
verb tells us what action the subject is performing 
and object is receiving. In order to elaborate rules 
for attitude analysis based on the semantics of 
verbs, we investigated VerbNet (Kipper et al, 
2007), the largest on-line verb lexicon that is orga-
nized into verb classes characterized by syntactic 
and semantic coherence among members of a 
class. Based on the thorough analysis of 270 first-
level classes of VerbNet and their members, 73 
verb classes (1) were found useful for the task of 
attitude analysis, and (2) were further classified 
into 22 classes differentiated by the role that mem-
bers play in attitude analysis and by rules applied 
to them. Our classification is shown in Table 2. 
 
Verb class (verb samples) 
1 Psychological state or emotional reaction 
1.1 Object-centered (oriented) emotional state (adore, re-
gret) 
1.2 Subject-driven change in emotional state (trans.)
(charm, inspire, bother) 
1.3 Subject-driven change in emotional state (intrans.) (ap-
peal to, grate on) 
2 Judgment 
2.1 Positive judgment (bless, honor) 
2.2 Negative judgment (blame, punish) 
3 Favorable attitude (accept, allow, tolerate) 
4 Adverse (unfavorable) attitude (discourage, elude, forbid) 
5 Favorable or adverse calibratable changes of state (grow, 
decline) 
6 Verbs of removing 
6.1 Verbs of removing with neutral charge (delete, remove)
6.2 Verbs of removing with negative charge (deport, expel)
6.3 Verbs of removing with positive charge (evacuate, 
cure) 
7 Negatively charged change of state (break, crush, smash) 
8 Bodily state and damage to the body (sicken, injure) 
9 Aspectual verbs 
9.1 Initiation, continuation of activity, and sustaining (be-
gin, continue, maintain) 
9.2 Termination of activity (quit, finish) 
10 Preservation (defend, insure) 
11 Verbs of destruction and killing (damage, poison) 
12 Disappearance (disappear, die) 
13 Limitation and subjugation (confine, restrict) 
14 Assistance (succor, help) 
15 Obtaining (win, earn) 
16 Communication indicator/reinforcement of attitude (guess, 
complain, deny) 
17 Verbs of leaving (abandon, desert) 
18 Changes in social status or condition (canonize, widow) 
19 Success and failure 
19.1 Success (succeed, manage) 
19.2 Failure (fail, flub) 
20 Emotional nonverbal expression (smile, weep) 
21 Social interaction (marry, divorce) 
22 Transmitting verbs (supply, provide) 
Table 2. Verb classes defined for attitude analysis. 
 
For each of our verb classes, we developed set 
of rules that are applied to attitude analysis on the 
phrase/clause-level. Some verb classes include 
verbs annotated by attitude type, prior polarity 
orientation, and the strength of attitude: ?Psycho-
logical state or emotional reaction?, ?Judgment?, 
?Verbs of removing with negative charge?, ?Verbs 
83
of removing with positive charge?, ?Negatively 
charged change of state?, ?Bodily state and dam-
age to the body?, ?Preservation?, and others. The 
attitude features of phrases, which involve posi-
tively or negatively charged verbs from such 
classes, are context-sensitive, and are defined by 
means of rules designed for each of the class. 
As an example, below we provide short descrip-
tion and rules elaborated for the subclass ?Object-
centered (oriented) emotional state?. 
Features: subject experiences emotions towards 
some stimulus; verb prior polarity: positive or neg-
ative; context-sensitive. 
Verb-Object rules (subject is ignored): 
1. ?Interior perspective? (subject?s inner emotion 
state or attitude): 
S & V+(?admires?) & O+(?his brave heart?) => 
(fusion, max(V_score,O_score)) => ?POS aff?. 
S & V+(?admires?) & O-(?mafia leader?) => 
(verb valence dominance, V_score) => ?POS aff?. 
S & V-(?disdains?) & O+(?his honesty?) => 
(verb valence dominance, V_score) => ?NEG aff?. 
S & V-(?disdains?) & O-(?criminal activities?) 
=> (fusion, max(V_score,O_score)) => ?NEG aff?. 
2. ?Exterior perspective? (social/ethical judgment): 
S & V+(?admires?) & O+(?his brave heart?) => 
(fusion, max(V_score,O_score)) => ?POS jud?. 
S & V+(?admires?) & O-(?mafia leader?) => 
(verb valence reversal, max(V_score,O_score)) => 
?NEG jud?. 
S & V-(?disdains?) & O+(?his honesty?) => 
(verb valence dominance, max(V_score,O_score)) 
=> ?NEG jud?. 
S & V-(?disdains?) & O-(?criminal activities?) 
=> (verb valence reversal, max(V_score,O_score)) 
=> ?POS jud?. 
3. In case of neutral object => attitude type and 
prior polarity of verb, verb score (V_score). 
Verb-PP (prepositional phrase) rules: 
1. In case of negatively charged verb and PP start-
ing with ?from? => verb valence dominance:  
S & V-(?suffers?) & PP-(?from illness?) => inte-
rior: ?NEG aff?; exterior: ?NEG jud?. 
S & V-(?suffers?) & PP+ (?from love?) => inte-
rior: ?NEG aff?; exterior: ?NEG jud?. 
2. In case of positively charged verb and PP start-
ing with ?in?/?for?, treat PP same as object (see 
above): 
S & V+(?believes?) & PP-(?in evil?) => interior: 
?POS aff?; exterior: ?NEG jud?. 
S & V+(?believes?) & PP+(?in kindness?) => in-
terior: ?POS aff?; exterior: ?POS jud?. 
In the majority of rules the strength of attitude is 
measured as a maximum between attitude scores of 
a verb and an object (max(V_score,O_score)), be-
cause strength of overall attitude depends on both 
scores. For example, attitude conveyed by ?to suf-
fer from grave illness? is stronger than that of ?to 
suffer from slight illness?. 
In contrast to the rules of ?Object-centered 
(oriented) emotional state? subclass, which ignore 
attitude features of a subject in a sentence, the rules 
elaborated for the ?Subject-driven change in emo-
tional state (trans.)? disregard the attitude features 
of object, as in sentences involving members of 
this subclass object experiences emotion, and sub-
ject causes the emotional state. For example (due 
to limitation of space, here and below we provide 
only some cases): 
S(?Classical music?) & V+(?calmed?) & O-
(?disobedient child?) => interior: ?POS aff?; exte-
rior: ?POS app?. 
S-(?Fatal consequences of GM food intake?) & 
V-(?frighten?) & O(?me?) => interior: ?NEG aff?; 
exterior: ?NEG app?. 
The Verb-Object rules for the subclasses ?Positive 
judgment? and ?Negative judgment? (verbs from 
?Judgment? class relate to a judgment or opinion 
that someone may have in reaction to something) 
are very close to those defined for the subclass 
?Object-centered (oriented) emotional state?. 
However, Verb-PP rules have some specifics: for 
both positive and negative judgment verbs, we 
treat PP starting with ?for?/?of?/?as? same as object 
in Verb-Object rules. For example: 
S(?He?) & V-(?blamed?) & O+(?innocent per-
son?) => interior: ?NEG jud?; exterior: ?NEG jud?. 
S(?They?) & V-(?punished?) & O(?him?) & PP-
(?for his misdeed?) => interior: ?NEG jud?; exte-
rior: ?POS jud?. 
Verbs from classes ?Favorable attitude? and ?Ad-
verse (unfavorable) attitude? have prior neutral 
polarity and positive or negative reinforcement, 
correspondingly, that means that they only impact 
on the polarity and strength of non-neutral phrase 
(object in a sentence written in active voice, or 
subject in a sentence written in passive voice, or 
PP in case of some verbs).  
Rules: 
1. If verb belongs to the ?Favorable attitude? class 
and the polarity of phrase is not neutral, then the 
84
attitude score of the phrase is intensified (we use 
symbol ?^? to indicate intensification): 
S(?They?) & [V pos. reinforcement](?elected?) & 
O+(?fair judge?) => ?POS app?; O_score^. 
S(?They?) & [V pos. reinforcement](?elected?) & 
O-(?corrupt candidate?) => ?NEG app?; O_score^. 
2. If verb belongs to the ?Adverse (unfavorable) 
attitude? class and the polarity of phrase is not neu-
tral, then the polarity of phrase is reversed and 
score is intensified: 
S(?They?) & [V neg. reinforcement](?prevented?) 
& O-(?the spread of disease?) => ?POS app?; 
O_score^. 
S+(?His achievements?) & [V neg. reinforce-
ment](?were overstated?) => ?NEG app?; S_score^. 
Below are examples of processing the sentences 
with verbs from ?Verbs of removing? class. 
?Verbs of removing with neutral charge?: 
S(?The tape-recorder?) & [V neutral 
rem.](?automatically ejects?) & O-neutral(?the 
tape?) => neutral. 
S(?The safety invention?) & [V neutral 
rem.](?ejected?) & O(?the pilot?) & PP-(?from 
burning plane?) => ?POS app?; PP_score^. 
?Verbs of removing with negative charge?: 
S(?Manager?) & [V neg. rem.](?fired?) & O-
(?careless employee?) & PP(?from the company?) 
=> ?POS app?; max(V_score,O_score).  
?Verbs of removing with positive charge?: 
S(?They?) & [V pos. rem.](?evacuated?) & 
O(?children?) & PP-(?from dangerous place?) => 
?POS app?; max(V_score,PP_score). 
Along with modal verbs and modal adverbs, mem-
bers of the ?Communication indica-
tor/reinforcement of attitude? verb class also 
indicate the confidence level or degree of certainty 
concerning given opinion.  
Features: subject (communicator) expresses state-
ment with/without attitude; statement is PP starting 
with ?of?, ?on?, ?against?, ?about?, ?concerning?, 
?regarding?, ?that?, ?how? etc.; ground: positive or 
negative; reinforcement: positive or negative. 
Rules: 
1. If the polarity of expressed statement is neutral, 
then the attitude is neutral: 
S(?Professor?) & [V pos. ground, pos. rein-
forcement, confidence:0.83](?dwelled?) & PP-
neutral(?on a question?) => neutral. 
2. If the polarity of expressed statement is not neu-
tral and the reinforcement is positive, then the po-
larity score of the statement (PP) is intensified: 
S(?Jane?) & [V neg. ground, pos. reinforcement, 
confidence:0.8](?is complaining?) & PP-(?of a 
headache again?) => ?NEG app?; PP_score^; con-
fidence:0.8. 
3. If the polarity of expressed statement is not neu-
tral and reinforcement is negative, then the polarity 
of the statement (PP) is reversed and score is inten-
sified: 
S(?Max?) & [V neg. ground, neg. reinforcement, 
confidence:0.2](?doubt?) & PP-{?that? S+(?his 
good fortune?) & [V termination](?will ever end?)} 
=> ?POS app?; PP_score^; confidence:0.2.  
In the last example, to measure the sentiment of 
PP, we apply rule for the verb ?end? from the 
?Termination of activity? class, which reverses the 
non-neutral polarity of subject (in intransitive use 
of verb) or object (in transitive use of verb). For 
example, the polarity of the following sentence 
with positive PP is negative: ?They discontinued 
helping children?. 
5 Evaluation 
In order to evaluate the performance of our algo-
rithm, we conducted experiment on the set of sen-
tences extracted from personal stories about life 
experiences that were anonymously published on 
the social networking website Experience Project 
(www.experienceproject.com). This web-
site represents an interactive platform that allows 
people to share personal experiences, thoughts, 
opinions, feelings, passions, and confessions 
through the network of personal stories. With over 
4 million experiences accumulated (as of February 
2010), Experience Project is a perfect source for 
researchers interested in studying different types of 
attitude expressed through text. 
5.1 Data Set Description 
For our experiment we extracted 1000 sentences 
from various stories grouped by topics within 13 
different categories, such as ?Arts and entertain-
ment?, ?Current events?, ?Education?, ?Family and 
friends?, ?Health and wellness?, ?Relationships 
and romance? and others, on the Experience 
Project. Sentences were collected from 358 dis-
tinct topic groups, such as ?I still remember Sep-
tember 11?, ?I am intelligent but airheaded?, ?I 
think bullfighting is cruel?, ?I quit smoking?, ?I am 
a fashion victim?, ?I was adopted? and others. 
85
We considered three hierarchical levels of atti-
tude labels in our experiment (see Figure 1). Three 
independent annotators labeled the sentences with 
one of 14 categories from ALL level and a corres-
ponding score (the strength or intensity value). 
These annotations were further interpreted using 
labels from MID and TOP levels. Fleiss? Kappa 
coefficient was used as a measure of reliability of 
human raters? annotations. The agreement coeffi-
cient on 1000 sentences was 0.53 on ALL level, 
0.57 on MID level, and 0.73 on TOP level. 
Only those sentences, on which at least two out 
of three human raters completely agreed, were in-
cluded in the ?gold standard? for our experiment. 
Three ?gold standards? were created according to 
the hierarchy of attitude labels. Fleiss? Kappa coef-
ficients are 0.62, 0.63, and 0.74 on ALL, MID, and 
TOP levels, correspondingly. Table 3 shows the 
distributions of labels in the ?gold standards?. 
 
ALL level MID level 
Label Number Label Number 
anger 45 POS aff 233 
disgust 21 NEG aff 332 
fear 54 POS jud 66 
guilt 22 NEG jud 78 
interest 84 POS app 100 
joy 95 NEG app 29 
sadness 133 neutral 87 
shame 18 total 925 
surprise 36  
POS jud 66 TOP level 
NEG jud 78 Label Number 
POS app 100 POS 437 
NEG app 29 NEG 473 
neutral 87 neutral 87 
total 868 total 997 
Table 3. Label distributions in the ?gold standards?. 
5.2 Results 
After processing each sentence from the data set by 
our system, we measured averaged accuracy, pre-
cision, recall, and F-score for each label within 
ALL, MID, and TOP levels. The results are shown 
in Table 4. The ratio of the most frequent attitude 
label in the ?gold standard? was considered as the 
baseline. As seen from the obtained results, our 
algorithm performed with high accuracy signifi-
cantly surpassing the baselines on all levels of atti-
tude hierarchy (except ?neutral? category on the 
TOP level, which is probably due to the unbal-
anced distribution of labels in the ?gold standard?, 
where ?neutral? sentences constitute less than 9%). 
 
ALL level 
Baseline 0.153 
Label Accuracy Precision Recall F-score 
anger 
0.621 
0.818 0.600 0.692 
disgust 0.818 0.857 0.837 
fear 0.768 0.796 0.782 
guilt 0.833 0.455 0.588 
interest 0.772 0.524 0.624 
joy 0.439 0.905 0.591 
sadness 0.528 0.917 0.670 
shame 0.923 0.667 0.774 
surprise 0.750 0.833 0.789 
POS jud 0.824 0.424 0.560 
NEG jud 0.889 0.410 0.561 
POS app 0.755 0.400 0.523 
NEG app 0.529 0.310 0.391 
neutral 0.559 0.437 0.490 
MID level 
Baseline 0.359 
Label Accuracy Precision Recall F-score 
POS aff 
0.709 
0.668 0.888 0.762 
NEG aff 0.765 0.910 0.831 
POS jud 0.800 0.424 0.554 
NEG jud 0.842 0.410 0.552 
POS app 0.741 0.400 0.519 
NEG app 0.474 0.310 0.375 
neutral 0.514 0.437 0.472 
TOP level 
Baseline 0.474 
Label Accuracy Precision Recall F-score 
POS 
0.879 
0.918 0.920 0.919 
NEG 0.912 0.922 0.917 
neutral 0.469 0.437 0.452 
Table 4. Results of the system performance evaluation. 
 
In the case of fine-grained attitude recognition 
(ALL level), the highest precision was obtained for 
?shame? (0.923) and ?NEG jud? (0.889), while the 
highest recall was received for ?sadness? (0.917) 
    
TOP POS NEG neutral
    
MID POS aff POS jud 
POS 
app NEG aff 
NEG 
jud 
NEG 
app neutral
        
ALL interest joy surprise POS jud 
POS 
app anger disgust fear guilt sadness shame 
NEG 
jud 
NEG 
app neutral
 
Figure 1. Hierarchy of attitude labels. 
 
86
and ?joy? (0.905) emotions at the cost of low preci-
sion (0.528 and 0.439, correspondingly). The algo-
rithm performed with the worst results in 
recognition of ?NEG app? and ?neutral?. 
The analysis of a confusion matrix for the ALL 
level revealed the following top confusions of our 
system (see Table 5): (1) ?anger?, ?fear?, ?guilt?, 
?shame?, ?NEG jud?, ?NEG app? and ?neutral? were 
predominantly incorrectly predicted as ?sadness? 
(for ex., @AM resulted in ?sadness? for the sen-
tence ?I know we have several months left before 
the election, but I am already sick and tired of see-
ing the ads on TV?, while human annotations were 
?anger?/?anger?/?disgust?); (2) ?interest?, ?POS jud? 
and ?POS app? were mostly confused with ?joy? by 
our algorithm (e.g., @AM classified the sentence 
?It?s one of those life changing artifacts that we 
must have in order to have happier, healthier lives? 
as ?joy?(-ful), while human annotations were ?POS 
app?/?POS app?/?interest?). 
 
Actual 
label 
Incorrectly predicted labels (%), in descending 
order 
anger sadness (28.9%), joy (4.4%), neutral (4.4%), 
NEG app (2.2%) 
disgust anger (4.8%), sadness (4.8%), NEG jud (4.8%) 
fear sadness (13%), joy (5.6%), POS app (1.9%) 
guilt sadness (50%), anger (4.5%) 
interest joy (33.3%), neutral (7.1%), sadness (3.6%), POS 
app (2.4%), fear (1.2%) 
joy interest (3.2%), POS app (3.2%), sadness (1.1%), 
surprise (1.1%), neutral (1.1%) 
sadness neutral (3.8%), joy (1.5%), anger (0.8%), fear 
(0.8%), guilt (0.8%), NEG app (0.8%) 
shame sadness (16.7%), fear (5.6%), guilt (5.6%), NEG 
jud (5.6%) 
surprise fear (5.6%), neutral (5.6%), joy (2.8%), POS jud 
(2.8%) 
POS jud joy (37.9%), POS app (9.1%), interest (4.5%), 
sadness (1.5%), surprise (1.5%), NEG jud 
(1.5%), neutral (1.5%) 
NEG jud sadness (37.2%), anger (3.8%), disgust (3.8%), 
neutral (3.8%) 
POS app joy (37%), neutral (9%), surprise (7%), interest 
(3%), POS jud (3%), sadness (1%) 
NEG app sadness (44.8%), fear (13.8%), disgust (3.4%), 
surprise (3.4%), neutral (3.4%) 
neutral sadness (29.9%), joy (13.8%), interest (3.4%), 
fear (2.3%), POS jud (2.3%), NEG app (2.3%), 
NEG jud (1.1%), POS app (1.1%) 
Table 5. Data from a confusion matrix for ALL level. 
 
Our system achieved high precision for all cate-
gories on the MID level (Table 4), with the excep-
tion of ?NEG app? and ?neutral?, although high 
recall was obtained only in the case of categories 
related to affect (?POS aff?, ?NEG aff?). These re-
sults indicate that affect sensing is easier than rec-
ognition of judgment or appreciation from text. 
TOP level results (Table 4) show that our algo-
rithm classifies sentences that convey positive or 
negative sentiment with high accuracy (92% and 
91%, correspondingly). On the other hand, ?neu-
tral? sentences still pose a challenge. 
The analysis of errors revealed that system re-
quires common sense or additional context to deal 
with sentences like ?All through my life I?ve felt 
like I?m second fiddle? (?gold standard?: ?sadness?; 
@AM: ?neutral?) or ?For me every minute on my 
horse is alike an hour in heaven!? (?gold stan-
dard?: ?joy?; @AM: ?neutral?).  
We also evaluated the system performance with 
regard to attitude intensity estimation. The percen-
tage of attitude-conveying sentences (not consider-
ing neutral ones), on which the result of our system 
conformed to the fine-grained ?gold standard? 
(ALL level), according to the measured distance 
between intensities given by human raters (aver-
aged values) and those obtained by our system is 
shown in Table 6. As seen from the table, our sys-
tem achieved satisfactory results in estimation of 
the strength of attitude expressed through text. 
 
Range of intensity 
difference 
Percent of 
sentences, % 
[0.0 ? 0.2] 55.5 
(0.2 ? 0.4] 29.5 
(0.4 ? 0.6] 12.2 
(0.6 ? 0.8] 2.6 
(0.8 ? 1.0] 0.2 
Table 6. Results on intensity. 
6 Conclusions 
In this paper we introduced @AM, which is so far, 
to the best of our knowledge, the only system clas-
sifying sentences using fine-grained attitude types, 
and extensively dealing with the semantics of 
verbs in attitude analysis. Our composition ap-
proach broadens the coverage of sentences with 
complex contextual attitude. The evaluation results 
indicate that @AM achieved reliable results in the 
task of textual attitude analysis. The limitations 
include dependency on lexicon and on accuracy of 
the parser. The primary objective for the future 
research is to use the results of named-entity rec-
ognition software in our algorithm. 
 
87
References  
Cecilia O. Alm. 2008. Affect in Text and Speech. PhD 
Dissertation. University of Illinois at Urbana-
Champaign. 
Saima Aman and Stan Szpakowicz. 2008. Using Roget's 
Thesaurus for Fine-Grained Emotion Recognition. 
Proceedings of the Third International Joint Confe-
rence on Natural Language Processing IJCNLP 
2008, Hyderabad, India, pp. 296-302. 
Plaban Kumar Bhowmick, Anupam Basu, and Pabitra 
Mitra. 2009. Reader Perspective Emotion Analysis in 
Text through Ensemble based Multi-Label Classifi-
cation Framework. Computer and Information 
Science, 2 (4): 64-74. 
Anthony C. Boucouvalas. 2003. Real Time Text-to-
Emotion Engine for Expressive Internet Communica-
tions. Being There: Concepts, Effects and Measure-
ment of User Presence in Synthetic Environments, 
Ios Press, pp. 306-318. 
Francois-Regis Chaumartin. 2007. UPAR7: A Know-
ledge-based System for Headline Sentiment Tagging. 
Proceedings of the Fourth International Workshop 
on Semantic Evaluations (SemEval-2007), Prague, 
Czech Republic, pp. 422-425. 
Yejin Choi and Claire Cardie. 2008. Learning with 
Compositional Semantics as Structural Inference for 
Subsentential Sentiment Analysis. Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 793-801. 
Ze-Jing Chuang and Chung-Hsien Wu. 2004. Multi-
modal Emotion Recognition from Speech and Text. 
Computational Linguistic and Chinese Language 
Processing, 9(2): 45-62. 
Leo Hoye. 1997. Adverbs and Modality in English. New 
York: Addison Wesley Longman Inc. 
Carroll E. Izard. 1971. The Face of Emotion. New York: 
Appleton-Century-Crofts. 
Karin Kipper, Anna Korhonen, Neville Ryant, and Mar-
tha Palmer. 2007. A Large-scale Classification of 
English Verbs. Language Resources and Evaluation, 
42 (1): 21-40. 
Zornitsa Kozareva, Borja Navarro, Sonia Vazquez, and 
Andres Montoyo, A. 2007. UA-ZBSA: A Headline 
Emotion Classification through Web Information. 
Proceedings of the Fourth International Workshop 
on Semantic Evaluations, pp. 334-337. 
Hugo Liu, Henry Lieberman, and Ted Selker. 2003. A 
Model of Textual Affect Sensing Using Real-World 
Knowledge. Proceedings of the International Confe-
rence on Intelligent User Interfaces, pp. 125-132. 
James R. Martin and Peter R.R. White. 2005. The Lan-
guage of Evaluation: Appraisal in English. Palgrave, 
London, UK. 
George A. Miller. 1990. WordNet: An On-line Lexical 
Database. International Journal of Lexicography, 
Special Issue, 3 (4): 235-312. 
Karo Moilanen and Stephen Pulman. 2007. Sentiment 
Composition. Proceedings of the Recent Advances in 
Natural Language Processing International Confe-
rence, pp. 378-382. 
Matthijs Mulder, Anton Nijholt, Marten den Uyl, and 
Peter Terpstra. 2004. A Lexical Grammatical Imple-
mentation of Affect. Proceedings of the Seventh In-
ternational Conference on Text, Speech and 
Dialogue, pp. 171-178. 
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment 
Analysis: Capturing Favorability using Natural Lan-
guage Processing. Proceedings of the 2nd Interna-
tional Conference on Knowledge Capture, pp. 70-77. 
Alena Neviarouskaya, Helmut Prendinger, and Mitsuru 
Ishizuka. 2009. SentiFul: Generating a Reliable Lex-
icon for Sentiment Analysis. Proceedings of the In-
ternational Conference on Affective Computing and 
Intelligent Interaction, IEEE, Amsterdam, Nether-
lands, pp. 363-368. 
J. Olveres, M. Billinghurst, J. Savage, and A. Holden. 
1998. Intelligent, Expressive Avatars. Proceedings of 
the First Workshop on Embodied Conversational 
Characters, pp. 47-55. 
Livia Polanyi and Annie Zaenen. 2004. Contextual Va-
lence Shifters. Working Notes of the AAAI Spring 
Symposium on Exploring Attitude and Affect in Text: 
Theories and Applications. 
Carlo Strapparava and Rada Mihalcea. 2008. Learning 
to Identify Emotions in Text. Proceedings of the 
2008 ACM Symposium on Applied Computing, Forta-
leza, Brazil, pp. 1556-1560. 
Carlo Strapparava, Alessandro Valitutti, and Oliviero 
Stock. 2007. Dances with Words. Proceedings of the 
International Joint Conference on Artificial Intelli-
gence, Hyderabad, India, pp. 1719-1724. 
V.S. Subrahmanian and Diego Reforgiato. 2008. AVA: 
Adjective-Verb-Adverb Combinations for Sentiment 
Analysis. Intelligent Systems, IEEE, 23 (4): 43-50. 
Maite Taboada and Jack Grieve. 2004. Analyzing Ap-
praisal Automatically. Proceedings of American As-
sociation for Artificial Intelligence Spring 
Symposium on Exploring Attitude and Affect in Text, 
pp.158-161. 
Casey Whitelaw, Navendu Garg, and Shlomo Argamon. 
2005. Using Appraisal Groups for Sentiment Analy-
sis. Proceedings of the 14th ACM International Con-
ference on Information and Knowledge Management, 
CIKM, Bremen, Germany, pp. 625-631. 
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. Proceedings of Human 
Language Technology Conference and Conference 
on Empirical Methods in Natural Language 
Processing, Vancouver: ACL, pp. 347-354. 
88
