Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 1?9,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Cognitive-based Annotation System for Emotion Computing 
 
 
Ying Chen, Sophia Y. M. Lee and Chu-Ren Huang 
Department of Chinese & Bilingual Studies 
The Hong Kong Polytechnic University 
{chenying3176,sophiaym,churen.huang}@gmail.com 
 
  
 
Abstract 
Emotion computing is very important for 
expressive information extraction. In this 
paper, we provide a robust and versatile 
emotion annotation scheme based on cog-
nitive emotion theories, which not only 
can annotate both explicit and implicit 
emotion expressions, but also can encode 
different levels of emotion information for 
the given emotion content. In addition, 
motivated by a cognitive framework, an 
automatic emotion annotation system is 
developed, and large and comparatively 
high-quality emotion corpora are created 
for emotion computing, one in Chinese 
and the other in English. Such an annota-
tion system can be easily adapted for dif-
ferent kinds of emotion applications and 
be extended to other languages. 
1 Introduction 
Affective information is important for human 
language technology, and sentiment analysis, a 
coarse-grained affective computing (Shanahan et 
al., 2006), which is attitude assessment, has be-
come the most salient trend. The polarity-driven 
approach in sentiment analysis is, however, often 
criticized as too general to satisfy some applica-
tions, such as advertisement design and robot 
design, and one way to capture more fine-grained 
affective information is to detect emotion expres-
sions. Unlike sentiment, emotions are cognitive-
based, which consistently occur across domains 
because of its human psychological activities. 
We believe that emotion computing, which is a 
fine-grained and cognitive-based framework of 
affective computing, will provide a more robust 
and versatile model for human language technol-
ogy. 
Since the concept of emotion is very compli-
cated and subjective, comparing to some annota-
tions such as POS annotation and Chinese word 
segmentation annotation, emotion annotation is 
highly labor intensive as it requires careful hu-
man judgment. Both explicit and implicit emo-
tions must be recognized and tagged during emo-
tion annotation, therefore, emotion annotation is 
not a simple assignment exercise as in POS an-
notation. Technically, emotion annotation can be 
divided into two subtasks: emotion detection (i.e. 
differentiate emotional content from neutral con-
tent), which is a very important task for affective 
information extraction, and emotion classifica-
tion (i.e. assign emotion tags to emotional con-
tent.)  
Emotion computing often requires a large and 
high-quality annotated data, however, there is a 
lack of this kind of corpus. This is not only be-
cause of the enormous human involvement, but 
also because of the unavailability of emotion an-
notation scheme, which is robust and versatile 
for both emotion annotation and emotion com-
puting. Tokuhisa et al (2008) is the only work 
that explores the issue of emotion detection 
while most of the previous studies concentrate on 
the emotion classification given a known emo-
tion context (Mihalcea and Liu, 2006; Kozareva 
et al, 2007.) Even for emotion classification, 
some issues remain unresolved, such as the com-
plicated relationships among different emotion 
types, emotion type selection, and so on. Thus, it 
is still far from solving the emotion problem if 
emotion annotation is just considered as emo-
tion-tag assignment.  
In this paper, we first explore the relationships 
among different emotion types with the support 
of a proposed emotion taxonomy, which com-
bines some psychological theories and linguistic 
semantics. Based on the emotion taxonomy, a 
robust and versatile emotion annotation scheme 
is designed and used in both Chinese and English 
1
emotion corpora. Our emotion annotation 
scheme is very flexible, which is only a layer 
added to a sentence, although it can easily be 
extended to a higher level of a text. Our annota-
tion scheme not only can provide the emotion 
type information, but also can encode the infor-
mation regarding the relationship between emo-
tions. With this versatile annotated emotion in-
formation, different NLP users can extract dif-
ferent emotion information from a given anno-
tated corpus according to their applications.  
With such an emotion annotation scheme, a 
large and comparatively high-quality annotated 
emotion corpus is built for emotion computing 
through an unsupervised approach. Tokuhisa et 
al. (2008) pointed out that besides emotion cor-
pus, neutral corpus (i.e. sentences containing no 
emotion) is also very important for emotion 
computing. Therefore, a high-quality neutral 
corpus is also automatically collected using con-
textual information. These two corpora are com-
bined to form a complete emotion-driven corpus 
for emotion computing. Although the unsuper-
vised method cannot provide a perfectly-
annotated corpus, it can easily adapt for different 
emotion computing.  
The remainder of this paper is organized as 
follows. In Section 2, we give an overview of the 
previous work on emotion annotation and some 
related psychological and linguistic theories. In 
Section 3, we describe our emotion taxonomy 
and emotion annotation scheme. Section 4 dis-
cusses how the unsupervised corpus is created.  
Section 5 presents the pilot experiments for emo-
tion computing with our corpus, which suggests 
that the unsupervised approach of our corpus 
creation is effective. Finally, a conclusion is 
made in Section 5. 
2 Related work 
There is no clear consensus among many psy-
chological and linguistic theories on the concept 
of emotions. Here, we limit our work by the clas-
sic definition of ?emotions? (Cannon, 1927): 
Emotion is the felt awareness of bodily reactions 
to something perceived or thought. 
Emotion is a complicated concept, and there 
are complicated relationships among different 
emotions. For example, the relationship between 
?discouraged? and ?sad? is different with the one 
between ?remorse? and ?sad.? Hobbs & Gordon 
(2008) and Mathieu (2005) explore emotions 
mainly from a lexical semantics perspective, and 
Schr?der et al (2006) designed an annotation 
scheme, EARL, mainly for speech processing. 
Because of the disagreements in emotion theories, 
EARL did not explore the relationships among 
emotion types. In this paper, we focus on emo-
tions in written data, which is very different from 
that of in spoken data in terms of expressions. 
Here, we first adopt psychological theories 
(Plutchik, 1980; Turner, 2000) to create an emo-
tion taxonomy, and then design an emotion anno-
tation scheme based on the taxonomy. 
Since most of the previous emotion corpora 
are either too small (Xu et al, 2008) or compara-
tively ineffective in terms of accuracy (Tokuhisa 
et al, 2008), they cannot satisfy the requirements 
of emotion computing. In this paper, based on 
Natural Semantic Metalanguage (NSM), a cogni-
tive approach to human emotions (which will be 
discussed in the later section), we create an au-
tomatic emotion annotation system. While this 
annotation system needs only a little training da-
ta and does not require human supervision, the 
corpus still maintains a comparatively high qual-
ity. Another significant advantage of our auto-
matic annotation system is that it can easily adapt 
to different emotion applications by simply sup-
plying different training data. 
Most of the existing emotion theories study 
emotions from the biological and psychological 
perspectives, hence they cannot easily apply to 
NLP. Fortunately, NSM, one of the prominent 
cognitive models exploring human emotions, 
offers a comprehensive and practical approach to 
emotions (Wierbicka 1996.) NSM describes 
complex and abstract concepts, such as emotions, 
in terms of simpler and concrete ones. In such a 
way, emotions are decomposed as complex 
events involving a cause and a mental state, 
which can be further described with a set of uni-
versal, irreducible cores called semantic primi-
tives. This approach identifies the exact differ-
ences and connections between emotion concepts 
in terms of the causes, which provide an imme-
diate cue for emotion detection and classification. 
We believe that the NSM model offers a plausi-
ble framework to be implemented for automatic 
emotion computing.  
3 Emotion annotation scheme 
3.1 The emotion taxonomy 
Although there are many emotion theories devel-
oped in different fields, such as biology, psy-
chology, and linguistics, most of them agree that 
emotion can be divided into primary emotions 
and complex emotions (i.e. the combinations of 
2
primary emotions.) There is still controversy 
over the selection of primary emotions, nonethe-
less, ?happiness?, ?sadness?, ?anger?, and ?fear? 
are considered as primary emotions by most of 
emotion theories.  
Plutchik?s emotion taxonomy (Plutchik 1980), 
one of the classic emotion taxonomies, also fol-
lows the division of primary emotions and com-
plex emotions, and Turner's taxonomy (Turner 
2000), which is based on Plutchik?s work, allows  
more flexible combinations of primary emotions. 
In this paper, we adopt Turner?s taxonomy, with 
the two main points emphasized: 
1) For each primary emotion, it is divided into 
three levels according to its intensity: high, mod-
erate, and low. Besides ?happiness,? ?sadness,? 
?anger? and ?fear,? Turner also suggests that 
?disgust? and ?surprise? can be primary emo-
tions (Turner 1996; Turner 2007). In Chinese, 
the character ??? (?surprise?) has a strong abil-
ity to form many emotion words, such as ?? 
(surprise and happiness), and ?? (surprise and 
fear), which is consistent with the explanation of 
?surprise? emotion by Plutchik (1991): ?when 
the stimulus has been evaluated, the surprise may 
quickly change to any other emotion.? Therefore, 
in our annotation scheme, we consider ?happi-
ness,? ?sadness,? ?anger,? ?fear,? and ?surprise? 
as primary emotions. 
2) Complex emotion can be divided into first-
order complex emotions (consisting of two pri-
mary emotions), second-order complex emotions 
(consisting of three primary emotions), and so on, 
according to the number of primary emotions 
that involves in the complex emotion. For exam-
ple, ?pride? (happiness + fear) is a first-order 
complex emotion, which contains a greater 
amount of ?happiness? with a lesser amount of 
?fear.? 
Tables 1 and 2 show some keywords in Turn-
er?s taxonomy, and the symbol ?//? is to separate 
different emotion types. Table 1 lists the five 
most common English keywords and their cor-
responding primary emotions, and Table 2 lists 
the English keywords and their corresponding 
complex emotions. In Table 2, several emotion 
keywords, which express similar emotion 
meaning, are grouped into an emotion type. For 
example, the emotion keywords ?awe, reverence, 
veneration? are grouped into emotion type 
?awe.? For a complex emotion, the order of pri-
mary emotions indicates the importance of those 
primary emotions for that complex emotion. For 
examples, ?envy? is ?fear + anger,? which con-
tains a greater amount of ?fear? with a lesser 
amount of ?anger? whereas ?awe? is ?fear + 
happiness,? which contains a greater amount of 
?fear? with a lesser amount of ?happiness.?  
For English emotion keywords, as Turner?s 
taxonomy missed some common emotion key-
words, we add the emotion keywords from 
Plutchik's taxonomy. Besides, unlike Chinese, 
English words have morphological variations, for 
example, the emotion keyword ?pride? can occur 
in text with the various formats: ?pride,? 
?prides,? ?prided,? ?proud,? ?proudly.? As 
shown in Tables 1 and 2, there are 188 English 
lemmas in our taxonomy. In total, there are 720 
emotion keywords if morphology is taken into 
account.  
Since Turner?s emotion taxonomy is cogni-
tive-based, it is versatile for different languages 
although there is no one-to-one mapping. We 
also explore Chinese emotion taxonomy in our 
previous work (Chen at el., 2009). We first select 
emotion keywords from the cognitive-based feel-
ing words listed in Xu and Tao (2003), and then 
map those emotion keywords to Turner?s taxon-
omy with adaptation for some cases. Lastly, 
some polysemous emotion keywords are re-
moved to reduce ambiguity, and 226 Chinese 
emotion keywords remain. 
Moreover, Turner?s taxonomy is a compara-
tively flexible structure, and more extensions can 
be done for different applications. For example, 
for a complex emotion, not only its primary emo-
tions are listed, but also the intensity of the pri-
mary emotions can be given. For instance, three 
emotion types, which belong to ?anger + fear,? 
are extended as follows: 
Jealousy:      Anger (Moderate) + Fear (Moderate) 
Suspicion:    Anger (Low) + Fear (Low) 
Abhorrence: Anger (High) + Fear (Low) 
Finally, we should admit that the emotion tax-
onomy is still an on-going research topic and 
needs further exploration, such as the position of 
a given emotion keyword in the emotion taxon-
omy, whether and how to group similar emotion 
keywords, and how to decompose a complex 
emotion into primary emotions. 
3.2 The emotion annotation scheme 
Given Turner?s taxonomy, we design our annota-
tion scheme to encode this kind of emotion in-
formation. Our emotion annotation scheme is 
XML scheme, and conforms with the Text En-
coding Initiative (TEI) scheme with some modi-
fications. The emotion scheme is a layer just
3
Primary Emotions Keywords 
Happiness High: ecstatic, eager, joy, enthusiastic, happy//Moderate: cheerful, satisfy, pleased, enjoy, interest//Low: 
sanguine, serene, content, grateful 
Fear High: horror, terror//Moderate: misgivings, self-conscious, scare, panic, anxious//Low: bewilder, reluct, 
shy, puzzles, confuse 
Anger High: dislike, disgust, outrage, furious, hate//Moderate: contentious, offend, frustrate, hostile, an-
gry//Low: contemptuous, agitate, irritate, annoy, impatient 
Sadness High: deject, despondent, sorrow, anguish, despair//Moderate: gloomy, dismay, sad, unhappy, disap-
point//Low: dispirit, downcast, discourage 
Surprise High: astonish//Moderate: startled, amaze, surprise 
Table1: Primary emotions and some corresponding keywords 
Combinations Keywords 
Happiness + Fear Wonder: wonder, wondering, hopeful//Pride: pride, boastful 
Happiness + Anger Vengeance: vengeance, vengeful//Calm: appeased, calmed, calm, soothed//Bemused: bemused 
Happiness + Sadness Yearning: nostalgia, yearning 
Fear + Happiness Awe: awe, reverence, veneration 
Fear + Anger Antagonism: antagonism, revulsed//Envy: envy 
Fear + Sadness Worried: dread, wariness, pensive, helpless, apprehension, worried 
Anger +Happiness Unfriendly: snubbing, mollified, rudeness, placated, apathetic, unsympathetic, unfriendly, unaffection-
ate//Sarcastic: sarcastic 
Anger + Fear Jealousy: jealous//Suspicion: suspicion, distrustful//Abhorrence: abhorrence 
Anger + Sadness Depressed: bitter, depression//Intolerant: intolerant  
Sadness +Happiness Acceptance: acceptance, tolerant//Solace: moroseness, solace, melancholy 
Sadness+ Fear Hopeless: forlorn, lonely, hopeless, miserable//Remorseful: remorseful, ashamed, humiliated 
Sadness+ Anger Discontent: aggrieved, discontent, dissatisfied, unfulfilled//Boredom: boredom//Grief: grief, sullenness 
Surprise + Happiness Delight: delight 
Surprise + Sadness Embarrassed: embarrassed 
Table 2:  First-order complex emotions and some corresponding keywords 
 
beyond a sentence, and encodes emotion infor-
mation for a sentence. This annotation scheme 
can be compatible for any TEI-based annotated 
corpora as long as sentences are clearly marked. 
The emotion-related elements (tags) in our 
annotation scheme are described as follows. For 
easy demonstration, our elements are defined 
with the format of British National Corpus 
(BNC) annotation scheme1 , and our examples 
are also based on BNC annotated text. Figure 1 
gives the definition of each element, and Figure 
2 shows several examples using our annotation 
scheme. Note that <s> element is a tag for a sen-
tence-like division of a text, and its attribute ?n? 
gives the sentence index. In Figure 2, Sentence 1, 
which expresses emotions by emotion keywords, 
contains two types of emotions: ?surprise? (pri-
mary emotion) and ?jealousy? (complex emo-
tion); Sentence 2 is a neutral sentence. 
<emotion> element 
It is used only when the sentence expresses 
emotions. It contains a list of <emotionType> 
elements and a <s> element. As a sentence may 
                                                 
1 http://www.natcorp.ox.ac.uk/XMLedition/U
RG/ 
express several emotions, an <emotion> element 
can contain several <emotionType> elements, 
and each <emotionType> element describes an 
emotion occurring in that sentence separately. 
<neutral> element 
It is used only when the sentence does not 
contain any emotion expression. It contains only 
a <s> element. 
<emotionType> element 
It describes a type of emotion in that sentence.  
It contains an ordered sequence of <pri-
maryEmotion> elements. Attribute ?name? pro-
vides the name of the emotion type, such as 
?surprise?, ?jealousy,? and so on, and it is op-
tional. If the emotion type is a primary emotion, 
the <emotionType> element will have only one 
<primaryEmotion> element, which encodes the 
information of this primary emotion. If the emo-
tion is a complex emotion, the <emotionType> 
element will have several <primaryEmotion> 
elements (each of them describes the primary 
emotion involved in that complex emotion.) At-
tribute ?keyword? is an optional attribution if 
annotators want to provide the indicator of a text 
for that emotion. 
4
   
<primaryEmtion> element 
It describes the property of a primary emotion 
involved in the emotion type. There are three 
attributes: ?order,? ?name,? and ?intensity.?  
?Order? gives the weight of this primary emo-
tion in the emotion type, and the weight value 
decreases with the ascending ?order? value. 
?Name? and ?intensity? provide the name and 
intensity of a primary emotion. To encode the 
information in our emotion taxonomy, the value 
of ?order? is {1,2,3,4,5}, the value of ?name? is 
{?happiness,? ?sadness,? ?anger,?  ?fear?, ?sur-
prise? }, and  the value of ?intensity? is {?high?, 
?moderate?, ?low?.} 
The <primaryEmotion> element seems to be 
redundant because its encoded information can 
be obtained from the given emotion taxonomy if 
the name of the emotion type is available, but 
the presence of this element can make our anno-
tation scheme more robust. Sometimes emotion 
is so complicated (especially for those emotion 
expressions without any explicit emotion key-
word) that an annotator may not be able to find 
an exact emotion type to match this emotion, or 
to list all involved primary emotions. For those 
subtle cases, emotion annotation can be simpli-
fied to list the involved primary emotions as 
many as possible through <primaryEmotion> 
elements. For example, in Sentence 3 in Figure 2, 
although there is no emotion keyword occurring, 
the word ?hurt? indicates the presence of an 
emotion, which at least involves ?sadness.? 
However, because it is hard to explicitly list oth-
er primary emotions, therefore, we give only the 
annotation of ?sadness.?  
Our annotation scheme has the versatility to 
provide emotion data for different applications. 
For example, if textual information input anno-
tated with our scheme is provided for the Japa-
nese robot Saya (Hashimoto et al 2006) to con-
trol her facial emotion expression, a simple 
mapping from our 24 emotion types can be done 
automatically to Saya?s six emotion types, i.e. 
surprise, fear, disgust, anger, happiness, and 
sadness. As four of her emotion types are also 
unique primary emotions, using information en-
coded in <emotionType> element and <pri-
maryEmotion> element will ensure unique 
many-to-one mapping and the correct robotic 
expressions. A trickier case involves her ?anger? 
and ?disgust? emotions. The emotion type ?an-
ger? in our taxonomy includes emotion words 
?anger? and ?disgust?. However, with the ?key-
word? information provided in <emotionType> 
element, a small subset of ?anger? emotion in 
our taxonomy can be mapped to ?disgust? in 
Saya?s system. For example, we could map 
keywords ?dislike, disgust, hate? to ?disgust?, 
element emotion 
{ 
(emotionType)+, 
<s> 
} 
element emotionType 
{ 
attribute name (optional), 
attribute keyword (optional), 
(primaryEmotion)+ 
} 
element primaryEmotion 
{ 
attribute order (optional), 
attribute name (necessary), 
attribute intensity (optional) 
} 
element neutral 
{  
<s> 
} 
Figure 1: The definition of emotion-related elements 
<emotion> 
<emotionType name =  "surprise"  keyword ="surprised"> 
<primaryEmotion  order =  "1" name =  "surprise"  intensity = "moderate"></primaryEmotion> 
</emotionType>   
<emotionType name = "jealousy"  keyword = ?jealousy?> 
<primaryEmotion  order =  "1"  name = "anger" intensity =  "moderate"></primaryEmotion> 
<primaryEmotion  order =  "2"  name =  "fear"   intensity =  "moderate"></primaryEmotion> 
</emotionType> 
<s n = "1"> Hari was surprised at the rush of pure jealousy that swept over her at the mention of Emily Grenfell .</s> 
</emotion> 
<neutral> 
<s n = "2"> By law no attempts may be made to hasten death or prolong the life of the sufferer . </s> 
</neutral> 
<emotion> 
<emotionType> 
<primaryEmotion name =  "sadness"></primaryEmotion> 
</emotionType>    
<s n = "3">He looked hurt when she did n't join him , his emotions transparent as a child 's . </s> 
</emotion> 
Figure 2: The example of sentence annotation 
 
5
and all the remaining ones, such as ?outrage, 
furious,? to ?anger.? 
4 Emotion-driven corpus creation 
Similar to most corpora, our corpus creation is 
designed to satisfy the requirements of real emo-
tion computing. Emotions can be expressed with 
or without emotion vocabulary in the text. It 
seems to be intuitive that emotion computing for 
a context with emotion keywords can be satis-
factory when the collection of emotion vocabu-
lary is comprehensive, such as ?joyful? indicates 
the presence of ?happiness? emotion. However, 
this intuitive approach cannot work well because 
of the ambiguity of some emotion keywords and 
the emotion context shift as the sentiment shift 
(Polanyi and Zaenen, 2004). Moreover, the de-
tection of emotions in a context without emotion 
keywords is very challenging. To deal with these 
problems, we build the emotion corpus, which is 
motivated by the NSM theory. 
According to the NSM theory, an emotion is 
provoked by a stimulus. This indicates one pos-
sible way to detect emotions in text, i.e. the de-
tection of emotional stimulus, which is often 
provided in the text. In other words, emotion 
corpus is a collection of emotion stimuli. Since 
emotion is subjective, the stimulus-based ap-
proach works only when its context is provided. 
For example, the stimulus ? ?build a gym for 
this community? ? may cause different emotions, 
such as ?surprise?, ?happy? and so on, depend-
ing on its context. We also notice that the text 
containing an emotion keyword may contain 
emotional stimulus and its context. Thus, a natu-
ral corpus creation approach comes out. 
 In our system, a pattern-based approach is 
used to collect the emotion corpus, which is sim-
ilar to the one used in Tokuhisa et al (2008), but 
we do not limit to event-driven emotions 
(Kozareva et al, 2008), and adjust our rules to 
improve the quality of emotion annotation. 
There are five steps in our emotion sentence an-
notation as given below, and Steps (2) and (3) 
are to improve the annotation quality. 
1) Extract emotion sentences: sentences con-
taining emotion keywords are extracted by 
keyword matching.  
2) Delete ambiguous structures: some ambigu-
ous sentences, which contain structures such 
as negation and modal, are filtered out.  
3) Delete ambiguous emotion keywords: if an 
emotion keyword is very ambiguous, all sen-
tences containing this ambiguous emotion 
keyword are filtered out. 
4) Give emotion tags: each remaining sentence 
is marked with its emotion tag according to the 
emotion type which the focus emotion word 
belongs to (refer to Tables 1 and 2.) 
5) Ignore the focus emotion keywords: for 
emotion computing, the emotion word is re-
moved from each sentence.  
 Polanyi and Zaenen (2004) addressed the is-
sue of polarity-based sentiment context shift, 
and the similar phenomenon also exists in emo-
tion expressions. In our corpus creation, two 
kinds of contextual structures are handled with: 
the negation structure and the modal structure. 
In both English and Chinese, a negated emotion 
expression can be interpreted as one of the three 
possible meanings (as shown in Figure 3): oppo-
site to the target emotion (S1), deny the exis-
tence of the target emotion (S2), or confirm the 
existence of the target emotion (S3). The modal 
structure often indicates that the emotion expres-
sion is based on the counter-factual assumption, 
hence the emotion does not exist at all (S4 and 
S5 in Figure 3). Although Chinese and English 
have different interpretations about the modal 
structure, for emotion analysis, those sentences 
often do not express an emotion. Therefore, to 
ensure the quality of the emotion corpus, all sen-
tences containing a negation structure or a modal 
structure, which are detected by some rules plus 
a list of keywords (negation polarity words for 
the negation structure, and modal words for the 
modal structure), are removed. 
 
To overcome the high ambiguity of some 
emotion keywords, after Step (2), for each emo-
tion keyword, five sentences are randomly se-
lected and annotated by two annotators. If the 
accuracy of five sentences is lower than 40%, 
this emotion keyword is removed from our emo-
tion taxonomy. Finally, 191 Chinese keywords 
and 645 English keywords are remained.  
Tokuhisa et al found that a big challenge for 
emotion computing, especially for emotion de-
tection, is to collect neutral sentences. Since 
neutral sentences are unmarked and hard to de-
tect, we develop a na?ve yet effective algorithm 
S1  (Neg_Happiness): I am not happy about that. 
S2 (Netural): Though the palazzo is our family home, my 
father had never been very happy there. 
S3  (Pos_Happiness): I 've never been so happy. 
S4  (Netural): I can die happy if you will look after them when 
I have gone.  
S5  (Netural): Then you could move over there and we'd all be 
happy. 
Figure 3: Structures for emotion shift 
6
to create a neutral corpus. A sentence is consid-
ered as neutral only when the sentence itself and 
its context (i.e. the previous sentence and the 
following sentence) do not contain any of the 
given emotion keywords. 
We run our emotion sentence extraction and 
neutral sentence extraction on three corpora: the 
Sinica Corpus (Chinese), the Chinese Gigaword 
Corpus, and the British National Corpus (BNC, 
English), and create three emotion corpora and 
three neutral corpora separately. The Sinica 
Corpus is a balanced Chinese corpus, which in-
cludes documents in 15 kinds of genres; The 
Chinese Gigaword Corpus is a huge collection 
of news reports; The BNC is also a balanced 
corpus, which collects documents from different 
domains.  
To estimate the accuracy of our emotion sen-
tence extraction, we randomly select about 1000 
sentences from the three emotion corpora, and 
have two annotators to check it. Table 3 lists the 
accuracy of those emotions sentences (emotion 
corpus.) To test how good this straightforward 
neutral sentence extraction strategy is, about 
1000 sentences are randomly selected from each 
of the three neutral corpora and are checked by 
two annotators. Table 3 lists the accuracy of 
those neutral sentences (neutral corpus.)  
 Emotion corpus Neutral corpus 
Gigaword 82.17 98.61 
Sinica 77.56 98.39 
BNC 69.36 99.50 
Table 3: The accuracy of the emotion-driven corpora 
From Table 3, the high accuracy of neutral 
corpus proves that our approach is effective in 
extracting neutral sentences from the document-
based corpus which contains contextual informa-
tion. Although the accuracy of emotion corpus is 
lower, it is still much higher than the one re-
ported by Kozareva et al (2008), i.e. 49.4. The 
accuracy is significantly increased by deleting 
ambiguous emotion keywords in Step (3). For 
the 2,474 randomly selected Chinese sentences, 
the overall accuracy of the remaining 1,751 sen-
tence is increased by about 14% after Step (3). 
For the 803 randomly selected English sentences, 
the accuracy of the remaining 473 sentence is 
increased about 21% after Step (3). Whether or 
how the ambiguous emotion keywords in Step 3 
are removed is a tradeoff between the coverage 
and the accuracy of the emotion corpus.  
From Table 3, we also find that the accuracy 
of English emotion corpus is much lower than 
Chinese emotion corpus, which indicates Eng-
lish emotion sentences expressed by emotion 
keywords are more ambiguous than that of Chi-
nese. Moreover, during our emotion corpus 
building, 20.2% of Sinica sentences and 22.4% 
of Gigaword sentences are removed in Step (2) 
and (3), on the contrary, 41.2% of BNC sen-
tences are deleted. Although it is more difficult 
to develop the rules in Step (2) and (3) for Chi-
nese than for English, it also confirms the higher 
ambiguity of emotion expressions in English due 
to the ambiguity of emotion keyword. Finally, 
because of the comparatively-high percentage of 
the sentences removed in Step (2) and (3), more 
exploration about those sentences is needed, 
such as the emotion distribution, the expression 
patterns and so on, and how to re-incorporate 
them into the emotion corpus without hurting the 
whole quality is also our future work.  
We also explore emotions through the sen-
tences (no-emotion-keyword sentences) that do 
not contain any given emotion keyword, because 
our approach extracts only partial neutral sen-
tences and partial emotion sentences in reality. 
For each corpus, about 1000 no-emotion-
keyword sentences are randomly selected and 
checked by two annotators. It is surprising that 
only about 1% of those sentences express emo-
tions. This indicates that it is important for real 
emotion computing, which mainly works on 
formal written text, to deal with the emotion ex-
pressions which contain emotion keywords and 
however are ambiguous, such as the sentences 
deleted in Steps (2) and (3). More exploration is 
needed for the emotion and neutral sentence dis-
tribution on other kinds of written text, such as 
blogs, and on spoken text. 
The unsupervised corpus creation approach 
can easily be adapted for different languages and 
different emotion applications, provided that the 
keyword collection and patterns in Step (2) and 
(3) need some changes.  Moreover, another big 
advantage of our approach is that it can avoid 
the controversy during emotion annotation. 
Emotion is subjective, and therefore disagree-
ment for emotion types often arises if the emo-
tion is not expressed through an explicit emotion 
keyword.  
Overall, the annotated corpus created by the 
unsupervised approach has a comparatively high 
quality, and is suitable for the emotion comput-
ing. As the size of the neutral corpus is much 
bigger than its corresponding emotion corpus, to 
avoid model bias, we randomly select some neu-
tral sentences from the neutral corpus, combin-
7
ing with its corresponding emotion sentences to 
form a complete emotion-driven corpus. 
5 Emotion computing system 
In this paper, we present some pilot work to 
prove that our emotion-driven corpus is useful 
for emotion computing. With the inclusion of 
neutral sentences, emotion detection and classi-
fication is simplified into a general classification 
problem, and a supervised machine learning 
method can be directly applied if enough anno-
tated data are obtained. Here, we choose the 
MaxEnt learning in Mallet as a classifier. 
 Both the Sinica Corpus and the Chinese Gi-
gaword Corpus are segmented, and POS-tagged. 
This allows us to implement the bag-of-words 
approach in the focus sentences in both Chinese 
and English. However, emotions are mostly hu-
man attitudes or expectations arising from situa-
tions, where situations are often expressed in 
more than a single word. Such kind of situations 
tends to be more easily extracted by word bi-
grams (2-gram word) than by word unigram (1-
gram word.) To take this into account, besides 1-
gram words, we also extract word bi-grams from 
the focus sentences.  
There are too many emotion types in our cor-
pus, which can cause data sparse; therefore, we 
choose the most frequent emotions to do explo-
ration. Besides the five primary emotions, for 
Chinese, we select another nine complex emo-
tions, and for English, we select another four 
complex emotions. Other emotion types are re-
named as ?Other Emotions.? 
Since Chinese emotion-driven corpus is much 
larger than the English one, to fairly compare the 
performance, we reduce the size of Chinese cor-
pus in our experiments. Then, for each corpus, 
we reserve 80% as the training data, 10% as the 
development data, and 10% as the test data 
(there are two sets of test data as follows.) In the 
evaluation, for each emotion sentence, if our 
system detects one of its emotion tags, we con-
sider this sentence is correctly tagged. 
Test data set 1 (TDS 1): contains about 10% 
of the sentences from the complete emotion-
driven corpus, and emotion tags are automati-
cally given during the corpus creation.  
Test data set 2 (TDS 2): contains the sen-
tences used in Table 3, which is checked by two 
annotators. If more than one emotion tags co-
exist in a sentence, all of them are chosen to la-
bel the sentence. If there exists an emotion that 
does not belong to any of the emotion types, it is 
labeled as ?Other Emotions.? 
Table 4 shows the performance (accuracy) of 
our system for Test data set 1 and 2 for both 
Chinese and English. We notice that our corpus 
creation approach is effective for emotion com-
puting. As we expect, the 2-gram words can par-
tially catch the emotion stimulus, and improves 
the performances. However, the overall per-
formance is still very low, which indicates that 
emotion computing is a difficult task. From the 
error analysis, it is surprised that for Chinese, 
the mislabeling of emotion sentences as neutral 
sentences (?emotion? vs. ?neutral?) is a common 
error, and whereas, for English, two kinds of 
errors: ?emotion? vs. ?neutral? and ?focus emo-
tions? vs. ?Other emotions? (the mislabeling of a 
sentence with a focus emotion as ?Other emo-
tions,?) occupy at least 50%. The error distribu-
tion confirms the importance of emotion detec-
tion during emotion computing. The high fre-
quency of the error of ?focus emotions? vs. 
?Other Emotions? in English may be because 
there are fewer focus emotion types for English.  
 1-gram words  {1,2}-gram words 
Chinese TDS 1 53.92 58.75 
English TDS 1 44.02 48.20 
Chinese TDS 2 37.18 39.95 
English TDS 2 33.24 36.31 
Table 4: The performances of our system for the test data  
6 Conclusion 
Emotion, no matter its annotation or computing, 
is still a new and difficult topic. In this paper, we 
apply emotion theories to design a cognitive-
based emotion annotation scheme, which are 
robust and versatile so that it can encode differ-
ent levels of emotion information for different 
emotion computing. Moreover, motivated from 
NSM, we develop an unsupervised approach to 
create a large and comparatively high-quality 
corpus for emotion computing, which is proven 
in our pilot experiments to be useful. Moreover, 
this approach makes emotion computing for dif-
ferent applications possible through a little mod-
ification. 
Certainly, there are some issues remaining un-
solved. For corpus construction, we will explore 
emotion distribution in other kinds of corpora, 
such as blog and dialog, and make analysis of 
ambiguous emotion sentences, such as negation 
structure and modal structure. For emotion com-
puting, we did only pilot experiments and more 
work needs to be done, such as feature extrac-
tion. 
8
References  
W. B. Cannon. 1927. The James-Lange theory of 
emotions: A Critical Examination and an Alterna-
tive Theory. American Journal of Psychology, 39, 
106-124. 
Y. Chen, S. Y. M. Lee and C. R. Huang, 2009. Con-
struction of Chinese Emotion Corpus with an Un-
supervised Approach. In CNCCL-2009, 2009. (in 
Chinese) 
T. Hashimoto, S. Hiramatsu, T. Tsuji and H. Kobaya-
shi. 2006. Development of the Face Robot SAYA 
for Rich Facial Expressions. SICE-ICASE Inter-
national Joint Conference, Busan,Korea. 
J. Hobbs and A. Gordon. 2008. The Deep Lexical 
Semantics of Emotions. Workshop on Sentiment 
Analysis: Emotion, Metaphor, Ontology and 
Terminology (EMOT-08), 6th International con-
ference on Language Resources and Evaluation 
(LREC-08), Marrakech, Morocco, May 27, 2008. 
P. Livia, A. Zaenen. 2004. Contextual Valence Shift-
ers. In Shanahan, J. G., Y. Qu, and J. Wiebe 
(Eds.), Computing Attitude and Affect in Text: 
Theory and Applications, pp. 1-10. 
Z. Kozareva, Borja Navarro, Sonia Vazquez, and 
Andres Nibtoyo. 2007. UA-ZBSA: A Headline 
Emotion Classification through Web Information. 
In Proceedings of the 4th International Workshop 
on Semantic Evaluations.  
Y. Y. Mathieu. 2005.  Annotations of Emotions and 
Feelings in Texts. In Conference on Affective 
Computing and intelligent Interaction (ACII2005), 
Beijing, Springer Lecture Notes in Computer Sci-
ence, pp. 350-357. 
R. Mihalcefa, and Hugo Liu. 2006. A Corpus-based 
Approach to Finding Happiness. In Proceedings 
of AAAI.  
R. Plutchik. 1991. The Emotions. University Press of 
America, Inc. 
R. Plutchik. 1980. Emotions: A psychoevolutionary 
synthesis. New York:Harper & Row. 
M. Schr?der, H. Pirker and M. Lamolle. 2006. First 
suggestions for an emotion annotation and repre-
sentation language. In L. Deviller et al (Ed.), 
Proceedings of LREC'06 Workshop on Corpora 
for Research on Emotion and Affect (pp. 88-92). 
Genoa, Italy. 
J. G. Shanahan, Y. Qu and J. Wiebe. 2006. Comput-
ing attitude and affect in text: theory and applica-
tions, Springer. 
R. Tokuhisa, K. Inui, and Y. Matsumoto (Eds.) 2008. 
Emotion Classification Using Massive Examples 
Extracted from the Web. COLING.   
J. H. Turner. 2007. Human Emotions: A sociological 
theory. New York : Routledge, 2007. 
J. H. Turner. 2000. On the origins of human emotions: 
A sociological inquiry into the evolution of hu-
man affect. Stanford, CA: Stanford University 
Press.  
J. H. Turner. 1996. The Evolution of Emotions in 
Humans: A Darwinian?Durkheimian Analysis. 
Journal for the theory of social behaviour26:1-34 
L. Xu, H. Lin, J. ZHAO.2008. Construction and 
Analysis of Emotional Corpus. JOURNAL OF 
CHINESE INFORMA TION PROCESSIN. 
X. Y. Xu, and J. H. Tao. 2003. The study of affective 
categorization in Chinese. The 1st Chinese Con-
ference on Affective Computing and Intelligent 
Interaction. Beijing, China. 
A. Wierzbicka, 1996. Semantics: Primes and Univer-
sals. Oxford: Oxford University Press. 
 
9
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 190?198, Prague, June 2007. c?2007 Association for Computational Linguistics
Towards Robust Unsupervised Personal Name Disambiguation 
Ying Chen 
Center for Spoken Language Research
University of Colorado at Boulder 
yc@colorado.edu 
James Martin 
Department of Computer Science 
University of Colorado at Boulder 
James.Martin@colorado.edu 
 
 
Abstract 
The increasing use of large open-domain 
document sources is exacerbating the 
problem of ambiguity in named entities.  
This paper explores the use of a range of 
syntactic and semantic features in unsu-
pervised clustering of documents that re-
sult from ad hoc queries containing names. 
From these experiments, we find that the 
use of robust syntactic and semantic fea-
tures can significantly improve the state of 
the art for disambiguation performance for 
personal names for both Chinese and Eng-
lish. 
1 Introduction 
An ever-increasing number of question answering, 
summarization and information extraction systems 
are coming to rely on heterogeneous sets of 
documents returned by open-domain search en-
gines from collections over which application 
developers have no control. A frequent special 
case of these applications involves queries 
containing named entities of various sorts and 
receives as a result a large set of possibly relevant 
documents upon which further deeper processing 
is focused. Not surprisingly, many, if not most, of 
the returned documents will be irrelevant to the 
goals of the application because of the massive 
ambiguity associated with the query names of 
people, places and organizations in large open 
collections. Without some means of separating 
documents that contain mentions of distinct 
entities, most of these applications will produce 
incorrect results. The work presented here, there-
fore, addresses the problem of automatically 
problem of automatically separating sets of news 
documents generated by queries containing per-
sonal names into coherent partitions. 
The approach we present here combines unsu-
pervised clustering methods with robust syntactic 
and semantic processing to automatically cluster 
returned news documents (and thereby entities) 
into homogeneous sets. This work follows on the 
work of Bagga & Baldwin (1998), Mann & 
Yarowsky (2003), Niu et al (2004), Li et al 
(2004), Pedersen et al (2005), and Malin (2005). 
The results described here advance this work 
through the use of syntactic and semantic features 
that can be robustly extracted from the kind of 
arbitrary news texts typically returned from open-
domain sources.  
The specific contributions reported here fall 
into two general areas related to robustness. In the 
first, we explore the use of features extracted from 
syntactic and semantic processing at a level that is 
robust to changes in genre and language. In par-
ticular, we seek to go beyond the kind of bag of 
local words features employed in earlier systems 
(Bagga & Baldwin, 1998; Gooi & Allan, 2004; 
Pedersen et al, 2005) that did not attempt to ex-
ploit deep semantic features that are difficult to 
extract, and to go beyond the kind of biographical 
information (Mann & Yarowsky, 2003) that is 
unlikely to occur with great frequency (such as 
place of birth, or family relationships) in many of 
the documents returned by typical search engines. 
The second contribution involves the application 
of these techniques to both English and Chinese 
news collections. As we?ll see, the methods are 
effective with both, but error analyses reveal in-
teresting differences between the two languages. 
190
The paper is organized as follows. Section 2 
addresses related work and compares our work 
with that of others. Section 3 introduces our new 
phrase-based features along two dimensions: from 
syntax to semantics; and from local sentential con-
texts to document-level contexts. Section 4 first 
describes our datasets and then analyzes the per-
formances of our system for both English and 
Chinese. Finally, we draw some conclusions. 
2 Previous work 
Personal name disambiguation is a difficult prob-
lem that has received less attention than those top-
ics that can be addressed via supervised learning 
systems. Most previous work (Bagga & Baldwin, 
1998; Mann & Yarowsky, 2003; Li et al, 2004; 
Gooi & Allan, 2004;  Malin, 2005; Pedersen et al, 
2005; Byung-Won On and Dongwon Lee, 2007) 
employed unsupervised methods because no large 
annotated corpus is available and because of the 
variety of the data distributions for different am-
biguous personal names. 
Since it is common for a single document to 
contain one or more mentions of the ambiguous 
personal name of interest, there is a need to define 
the object to be disambiguated (the ambiguous 
object). In Bagga & Baldwin (1998), Mann & 
Yarowsky (2003) and Gooi & Allan (2004), an 
ambiguous object refers to a single entity with the 
ambiguous personal name in a given document. 
The underlying assumption for this definition is 
?one person per document? (all mentions of the 
ambiguous personal name in one document refer 
to the same personal entity in reality). In Niu et al 
(2004) and Pedersen et al (2005), an ambiguous 
object is defined as a mention of the ambiguous 
personal name in a corpus.  
The first definition of the ambiguous object 
(document-level object) can include much infor-
mation derived from that document, so that it can 
be represented by rich features. The later defini-
tion of the ambiguous object (mention-level object) 
can simplify the detection of the ambiguous object, 
but because of the limited coverage, it usually can 
use only local context (the text around the men-
tion of the ambiguous personal name) and might 
miss some document-level information. The kind 
of name disambiguation based on mention-level 
objects really solves ?within-document name am-
biguity? and ?cross-document name ambiguity? 
simultaneously, and often has a higher perform-
ance than the kind based on document-level ob-
jects because two mentions of the ambiguous per-
sonal name in a document are very likely to refer 
to the same personal entity. From our news corpus, 
we also found that mentions of the ambiguous 
personal name of interest in a news article rarely 
refer to multiple entities, so our system will focus 
on the name disambiguation for document-level 
objects. 
In general, there are two types of information 
usually used in name disambiguation (Malin, 
2005): personal information and relational infor-
mation (explicit and implicit). Personal informa-
tion gives biographical information about the am-
biguous object, and relational information speci-
fies explicit or implicit relations between the am-
biguous object and other entities, such as a mem-
bership relation between ?John Smith? and ?Labor 
Party.? Usually, explicit relational information can 
be extracted from local context, and implicit rela-
tional information is far away from the mentions 
of the ambiguous object. A hard case of name dis-
ambiguation often needs implicit relational infor-
mation that provides a background for the am-
biguous object. For example, if two news articles 
in consideration report an event happening in 
?Labor Party,? this implicit relational information 
between ?John Smith? and ?Labor Party? can give 
a hint for name disambiguation if no personal or 
explicit relational information is available. 
Bagga & Baldwin (1998), Mann & Yarowsky 
(2003), Gooi & Allan (2004), Niu et al (2004), 
and Pedersen et al (2005) explore features in local 
context. Bagga & Baldwin (1998), Gooi & Allan 
(2004), and Pedersen et al (2005) use local token 
features; Mann & Yarowsky (2003) extract local 
biographical information; Niu et al (2004) use co-
occurring Named Entity (NE) phrases and NE 
relationships in local context. Most of these local 
contextual features are personal information or 
explicit relational information. 
Li et al (2004) and Malin (2005) consider 
named-entity disambiguation as a graph problem, 
and try to capture information related to the am-
biguous object beyond local context, even implicit 
relational information. Li et al (2004) use the EM 
algorithm to learn the global probability distribu-
tion among documents, entities, and representative 
mentions, and Malin (2005) constructs a social 
network graph to learn a similarity matrix.  
191
In this paper, we also explore both personal and 
relational information beyond local context. But 
we achieve it with a different approach: extracting 
these types of information by means of syntactic 
and semantic processing. We not only extract lo-
cal NE phrases as in Niu et al (2004), but also use 
our entity co-reference system to extract accurate 
and representative NE phrases occurring in a 
document which may have a relation to the am-
biguous object. At the same time, syntactic phrase 
information sometimes can overcome the imper-
fection of our NE system and therefore makes our 
disambiguation system more robust. 
3 Overall Methodology 
Our approach follows a common architecture for 
named-entity disambiguation: the detection of 
ambiguous objects, feature extraction and repre-
sentation, similarity matrix learning, and finally 
clustering. 
In our approach, all documents are preproc-
essed with a syntactic phrase chunker (Hacioglu, 
2004) and the EXERT1 system (Hacioglu et al 
2005; Chen & Hacioglu, 2006), a named-entity 
detection and co-reference resolution system that 
was developed for the ACE2 project. A syntactic 
phrase chunker segments a sentence into a se-
quence of base phrases. A base phrase is a syntac-
tic-level phrase that does not overlap another base 
phrase. Given a document, the EXERT system 
first detects all mentions of entities occurring in 
that document (named-entity detection) and then 
resolves the different mentions of an entity into 
one group that uniquely represents the entity 
(within-document co-reference resolution). The 
ACE 2005 task can detect seven types of named 
entities: person, organization, geo-political entity, 
location, facility, vehicle, and weapon; each type 
of named entity can occur in a document with any 
of three distinct formats: name, nominal construc-
tion, and pronoun. The F score of the syntactic 
phrase chunker, which is trained and tested on the 
Penn TreeBank, is 94.5, and the performances of 
the EXERT system are 82.9 (ACE value for 
named-entity detection) and 68.5 (ACE value for 
within-document co-reference resolution). 
                                                 
1 http://sds.colorado.edu/EXERT 
2 http://projects.ldc.upenn.edu/ace/ 
3.1 The detection of ambiguous objects  
In our approach, we assume that the ambiguous 
personal name has already been determined by the 
application. Moreover, we adopt the policy of 
?one person per document? as in Bagga & 
Baldwin (1998), and define an ambiguous object 
as a set of target entities given by the EXERT 
system. A target entity is an entity that has a 
mention of the ambiguous personal name. Given 
the definition of an ambiguous object, we define a 
local sentence (or local context) as a sentence that 
contains a mention of any target entity. 
3.2 Feature extraction and representation 
Since considerable personal and relational infor-
mation related to the ambiguous object resides in 
the noun phrases in the document, such as the per-
son?s job and the person?s location, we attempt to 
capture this noun phrase information along two 
dimensions: from syntax to semantics, and from 
local contexts to document-level contexts. 
Base noun phrase feature: To keep this feature 
focused, we extract only noun phrases occurring 
in the local sentences and the summarized sen-
tences (the headline + the first sentence of the 
document) of the document. The local sentences 
usually include personal or explicit relational in-
formation about the ambiguous object, and the 
summarized sentences of a news document usu-
ally give a short summary of the whole news story. 
With the syntactic phrase chunker, we develop 
two base noun phrase models: (i) Contextual base 
noun phrases (Contextual bnp), the base noun 
phrases in the local sentences; (ii) Summarized 
base noun phrases (Summarized bnp), the base 
noun phrases in the local sentences and the sum-
marized sentences. A base noun phrase of interest 
serves as an element in the feature vector. 
Named-Entity feature: Given the EXERT sys-
tem, a direct and simple way to extract some se-
mantic information is to use the named entities 
detected in the document. Based on their relation-
ship to the ambiguous personal name, the named 
entities identified in a text can be divided into 
three categories:  
(i) Target entity: an entity that has a mention 
of the ambiguous personal name. Target entities 
often include some personal information about the 
ambiguous object, such as the title, position, and 
so on.  
192
(ii) Local entity: an entity other than a target 
entity that has a mention occurring in any local 
sentence. Local entities often include entities that 
are closely related to the ambiguous object, such 
as employer, location and co-workers.  
(iii) Non-local entity: an entity that is not ei-
ther the local entity or the target entity. Non-local 
entities are often implicitly related to the ambigu-
ous object and provide background information 
for the ambiguous object. 
To assess how important these entities are to 
named-entity disambiguation, we create two kinds 
of entity models: (i) Contextual entities: the enti-
ties in the feature vector are target entities and 
local entities; (ii) Document entities: the entities 
in the feature vector include all entities in the 
document including target entities, local entities 
and non-local entities. 
Since a given entity can be represented by 
many mentions in a document, we choose a single 
representative mention to represent each entity. 
The representative mention is selected according 
to the following ordered preference list: longest 
NAME mention, longest NOMINAL mention.  A 
representative mention phrase serves as an ele-
ment in a feature vector. 
Although the mentions of contextual entities of-
ten overlap with contextual base noun phrases, the 
representative mention of a contextual entity often 
goes beyond local sentences, and is usually the 
first or longest mention of that contextual entity. 
Compared to contextual base noun phrases, the 
representative mention of a contextual entity often 
includes more detail and accurate information 
about the entity. On the other hand, the contextual 
base noun phrase feature detects all noun phrases 
occurring in local sentences that are not limited to 
the seven types of named entities discovered by 
the EXERT system. Compared to the contextual 
entity feature, the contextual base noun phrase 
Entity space 
Text space 
Feature Space 
Contextual base noun phrases?  feature vector: < Hope Mills police Capt. John Smith16, 
what16, he16, the statements16, no criminal violation16, what17, the individuals17, no direct 
threat17, Smith17, He and Thomas18, they18, Collins18, his bill18> 
Summarized base noun phrases?  feature vector: < Hope Mills police Capt. John Smith16, 
what16, he16, the statements16, no criminal violation16, what17, the individuals17, no direct 
threat17, Smith17, He and Thomas18, they18, Collins18, his bill18, Collins1, restaurant1, HOPE 
MILLS2, Commissioner Tonzie Collins2, a town restaurant2, an alleged run-in2, two work-
ers2, Feb. 212> 
Contextual entities?  feature vector: < Hope Mills police Capt. John Smith16, Jenny Tho-
mas4, Commissioner Tonzie Collins2, He and Thomas4, the individuals17> 
Document entities?  feature vector: < Hope Mills police Capt. John Smith 16, Jenny Tho-
mas4, Commissioner Tonzie Collins2, He and Thomas4, the individuals17, Andy?s 
Cheesesteaks4, HOPE MILLS 2, two workers2, the Village Shopping Center 4, Hope Mills 
Road 4 > 
Target entity:     < Hope Mills police Capt. John Smith16, he16, Smith17, He18> 
Local entity:       < Thomas18, Jenny Thomas4, manager4>,  
< Collins18, his18, Collins1, Commissioner Tonzie Collins 2>, ?? 
Non-local entity: < restaurant1, a town restaurant2, there2, Andy?s Cheesesteaks4>, ?? 
(Headline & S1) Collins banned from restaurant 
(S2) HOPE MILLS ? Commissioner Tonzie Collins has been banned from a town restau-
rant after an alleged run-in with two workers there Feb. 21. ?? 
(S4) ?In all fairness, that is not a representation of the town,? said Jenny Thomas, manager 
at Andy?s Cheesesteaks in the Village Shopping Center on Hope Mills Road. ?? 
(S16) Hope Mills police Capt. John Smith said based on what he read in the statements, 
no criminal violation was committed.  
(S17) ?Based on what the individuals involved said, there was no direct threat,? Smith said. 
(S18) He and Thomas said they don?t think Collins intentionally left without paying his 
bill. ?? 
 
Figure 1: A Sample of Feature Extraction 
193
feature is more general and can sometimes over-
come errors propagated from the named-entity 
system.  
To make this more concrete, the feature vectors 
for a document containing ?John Smith? are high-
lighted in Figure 1. The superscript number for 
each phrase refers to the sentence where the 
phrase is located, and we assume that the syntactic 
phrase chunker and the EXERT system work per-
fectly. 
3.3 Similarity matrix learning 
Given a pair of feature vectors consisting of 
phrase-based features, we need to choose a simi-
larity scheme to calculate the similarity. Because 
of the word-space delimiter in English, the feature 
vector for an English document comprises phrases, 
whereas that for a Chinese document comprises 
tokens. There are a number of similarity schemes 
for learning a similarity matrix from token-based 
feature vectors, but there are few schemes for 
phrase-based feature vectors.  
Cohen et al (2003) compared various similarity 
schemes for the task of matching English entity 
names and concluded that the hybrid scheme they 
call SoftTFIDF performs best. SoftTFIDF is a to-
ken-based similarity scheme that combines a stan-
dard TF-IDF weighting scheme with the Jaro-
Winkler distance function. Since Chinese feature 
vectors are token-based, we can directly use 
SoftTFIDF to learn the similarity matrix. However, 
English feature vectors are phrase-based, so we 
need to run SoftTFIDF iteratively and call it ?two-
level SoftTFIDF.? First, the standard SoftTFIDF 
is used to calculate the similarity between phrases 
in the pair of feature vectors; in the second phase, 
we reformulate the standard SoftTFIDF to calcu-
late the similarity for the pair of feature vectors.  
First, we introduce the standard SoftTFIDF. In 
a pair of feature vectors S and T, S = (s1, ? , sn ) 
and T = (t1, ? , tm). Here, si (i = 1?n) and tj (j = 
1?m) are substrings (tokens). Let CLOSE(?; S;T) 
be the set of substrings w?S such that there is 
some v?T satisfying dist(w; v) > ?. The Jaro-
Winkler distance function (Winkler, 1999) is 
dist(;). For w? CLOSE(?; S;T), let D(w; T) = 
);(max vwdistTv? . Then the standard SoftTFIDF 
is computed as 
)D( )V( )V(
  )( SoftTFIDF
);;(
w, Tw, Tw, S
S,T
TSCLOSEw
??
=
? ? ?   
)(IDF log  1)  (TF log  )(V' ww,Sw, S ?+=          
                               
? ?= S w, S
w, Sw, S
w
2)( V
)(  V  )( V                     
where TFw,S is the frequency of substrings w in S, 
and IDFw is the inverse of the fraction of docu-
ments in the corpus that contain w. In computing 
the similarity for the English phrase-based feature 
vectors, in the second step of ?two-level 
SoftTFIDF,? the substring w is a phrase and dist is 
the standard SoftTFIDF.  
So far, we have developed several feature mod-
els and learned the corresponding similarity ma-
trices, but clustering usually needs only one 
unique similarity matrix. Since a feature may have 
different effects for the disambiguation depending 
on the ambiguous personal name in consideration, 
to achieve the best disambiguation ability, each 
personal name may need its own weighting 
scheme to combine the given similarity matrices. 
However, learning that kind of weighting scheme 
is very difficult, so in this paper, we simply com-
bine the similarity matrices, assigning equal 
weight to each one. 
3.4 Clustering 
Although clustering is a well-studied area, a re-
maining research problem is to determine the op-
timal parameter setting during clustering, such as 
the number of clusters or the stop-threshold, a 
problem that is important for real tasks and that is 
not at all trivial. 
Since the focus of this paper is only on feature 
development, we simply employ a clustering 
method that can reflect the quality of the similar-
ity matrix for clustering. Here, we choose ag-
glomerative clustering with a single linkage. Since 
each personal name may need a different parame-
ter setting, to test the importance of the parameter 
setting for clustering, we use two kinds of stop-
thresholds for agglomerative clustering in our ex-
periments: first, to find the optimal stop-threshold 
for any ambiguous personal name and for each 
feature model, we run agglomerative clustering 
with all possible stop-thresholds, and choose the 
one that has the best performance as the optimal 
194
stop-threshold; second, we use a fixed stop-
threshold acquired from development data.  
4 Performance  
4.1 Data 
To capture the real data distribution, we use two 
sets of naturally occurring data: Bagga?s corpus 
and the Boulder Name corpus, which is a news 
corpus locally acquired from a web search. 
Bagga?s corpus is a document collection for the 
English personal name ?John Smith? that was 
used by Bagga & Baldwin (1998). There are 256 
articles that match the ?/John.*?Smith/? regular 
expression in 1996 and 1997 editions of the New 
York Times, and 94 distinct ?John Smith? personal 
entities are mentioned. Of these, 83 ?John Smiths? 
are mentioned in only one article (singleton clus-
ters containing only one object), and 11 other 
?John Smiths? are mentioned several times in the 
remaining 173 articles (non-singleton clusters 
containing more than one object). For the task of 
cross-document co-reference, Bagga & Baldwin 
(1998) chose 24 articles from 83 singleton clusters, 
and 173 other articles in 11 non-singleton clusters 
to create the final test data set ? Bagga?s corpus. 
We collected the Boulder Name corpus by first 
selecting four highly ambiguous personal names 
each in English and Chinese. For each personal 
name, we retrieved the first non-duplicated 100 
news articles from Google (Chinese) or Google 
news (English). There are four data sets for Eng-
lish personal names and four data sets for Chinese 
personal names: James Jones, John Smith, Mi-
chael Johnson, Robert Smith, and Li Gang, Li Hai, 
Liu Bo, Zhang Yong. 
Compared to Bagga?s corpus, which is limited 
to the New York Times, the documents in the 
Boulder Name corpus were collected from differ-
ent sources, and hence are more heterogeneous 
and noisy. This variety in the Boulder Name cor-
pus reflects the distribution of the real data and 
makes named-entity disambiguation harder.  
For each ambiguous personal name in both cor-
pora, the gold standard clusters have a long-tailed 
distribution - a high percentage of singleton clus-
ters plus a few non-singleton clusters. For exam-
ple, in the 111 documents containing ?John 
Smith? in the Boulder Name corpus, 53 ?John 
Smith? personal entities are mentioned. Of them, 
37 ?John Smiths? are mentioned only once. The 
long-tailed distribution brings some trouble to 
clustering, since in many clustering algorithms a 
singleton cluster is considered as a noisy point and 
therefore is ignored. 
4.2 Corpus performance 
Because of the long tail of the data set, we design 
a baseline using one cluster per document. To 
evaluate our disambiguation system, we choose 
the B-cubed scoring method that was used by 
Bagga & Baldwin (1998).  
In order to compare our work to that of others, 
we re-implement the model used by Bagga & 
Baldwin (1998). First, extracting all local sen-
tences produces a summary about the given am-
biguous object. Then, the object is represented by 
the tokens in its summary in the format of a vector, 
and the tokens in the feature vector are in their 
morphological root form and are filtered by a 
stop-word dictionary. Finally, the similarity matrix 
is learned by the TF-IDF method.   
Because both ?two-level SoftTFIDF? and ag-
glomerative clustering require a parameter setting, 
for each language, we reserve two ambiguous per-
sonal names from the Boulder Name corpus as 
development data (John Smith, Michael Johnson, 
Li Gang, Zhang Yong), and the other data are re-
served as test data: Bagga?s corpus and the other 
personal names in the Boulder Name corpus 
(Robert Smith, James Jones, Li Hai, Liu Bo).  
For any ambiguous personal name and for each 
feature model, we find the optimal stop-threshold 
for agglomerative clustering, and show the corre-
sponding performances in Table 1, Table 2 and 
Table 3. However, for the most robust feature 
model, Bagga + summarized bnp + document en-
tities, we learn the fixed stop-threshold for ag-
glomerative clustering from the development data 
(0.089 for English data and 0.078 for Chinese 
data), and show the corresponding performances 
in Table 4. 
4.2.1  Performance on Bagga?s corpus 
Table 1 shows the performance of each feature 
model for Bagga?s corpus with the optimal stop-
threshold. The metric here is the B-cubed F score 
(precision/recall).  
Because of the difference between Bagga?s re-
sources and ours (different versions of the named-
entity system and different dictionaries of the 
morphological root and the stop-words), our best 
195
B-cubed F score for Bagga?s model is 80.3? 4.3 
percent lower than the best performance reported 
by Bagga & Baldwin (1998): 84.6.  
From Table 1, we found that the syntactic fea-
tures (contextual bnp and summarized bnp) and 
semantic features (contextual entities and docu-
ment entities) consistently improve the perform-
ances, and all performances outperform the best 
result reported by Bagga & Baldwin (1998): 84.6   
 
Model B-cubed performance  
Gold standard cluster # 35 
Baseline 30.17 (100.00/17.78) 
Bagga 80.32 (94.77/69.70) 
Bagga + contextual bnp   89.16 (89.18/89.13) 
Bagga + summarized bnp 89.59 (92.60/86.78)    
Bagga + summarized bnp + contextual entities 89.60 (87.16/92.18)    
Bagga + summarized bnp + document entities 92.02 (93.10/90.97)    
Table 1:  Performances for Bagga?s corpus with the optimal stop-threshold   
 
                Name 
Model 
John Smith 
(dev) 
Michael Johnson
(dev) 
Robert Smith 
(test) 
James Jones 
(test) 
Average 
performance
Gold standard cluster # 53 52 65 24  
Baseline 64.63 (111) 67.97 (101) 78.79 (100) 37.50 (104) 62.22 
Bagga 82.63 (75) 89.07 (66) 91.56 (73) 86.42 (24) 87.42 
Bagga + contextual bnp   85.18 (62) 89.13 (65) 92.35 (74) 86.45 (22) 88.28 
Bagga + summarized bnp 85.97 (66) 91.08 (51) 93.17 (70) 90.11 (33) 90.08 
Bagga + summarized bnp 
+ contextual entities 
85.44 (70) 94.24 (55) 91.94 (73) 96.66 (24) 92.07 
Bagga + summarized bnp 
+ document entities 
91.94 (61) 92.55 (51) 93.48 (67) 97.10 (28) 93.77 
Table 2: Performances for the English Boulder Name corpus with the optimal stop-threshold  
 
                 Name 
Model 
Li Gang  
(dev) 
Zhang Yong 
(dev) 
Li Hai 
(test) 
Liu Bo 
(test) 
Average 
performance
Gold standard cluster # 57 63 57 45  
Baseline 72.61 (100) 76.83 (101) 74.03 (97) 62.07 (100) 71.39 
Bagga  96.21 (57) 96.43 (64) 94.51 (64) 91.66 (49) 94.70 
Bagga + contextual bnp   97.57 (57) 96.38 (66) 94.53 (64) 93.21 (51) 95.42 
Bagga + summarized bnp 98.50 (56) 96.17 (61) 95.38 (62) 93.21 (51) 95.81 
Bagga + summarized bnp 
+ contextual entities 
99.50 (58) 95.49 (63) 96.75 (58) 91.05 (52) 95.70 
Bagga + summarized bnp 
+ document entities 
99.50 (56) 94.57 (70) 98.57 (59) 97.02 (48) 97.41 
Table 3: Performances for the Chinese Boulder Name corpus with the optimal stop-threshold 
 
English Name John Smith 
(dev) 
Michael Johnson
(dev) 
Robert Smith 
(test) 
James Jones 
(test) 
Average 
performance
Bagga + summarized bnp 
+ document entities 
91.31 
(91.94)  
 90.57 
(92.55) 
 86.71 
(93.48) 
96.64 
(97.10) 
 91.31 
(93.77) 
Chinese Name Li Gang  
(dev) 
Zhang Yong 
(dev) 
Li Hai 
(test) 
Liu Bo 
(test) 
Average 
performance
Bagga + summarized bnp 
+ document entities 
 99.06 
(99.50) 
94.56 
(94.56) 
98.25  
(98.57) 
 89.18 
(97.02) 
 95.26 
(97.41) 
Table 4: Performances for the Boulder Name corpus with the fixed stop-threshold 
196
4.2.2 Performance on the Boulder Name cor-
pus 
Table 2 and Table 3 show the performance of each 
feature model with the optimal stop-threshold for 
the English and Chinese Boulder Name corpora, 
respectively. The metric is the B-cubed F score 
and the number in brackets is the corresponding 
cluster number. Since the same feature model has 
different contributions for different ambiguous 
personal names, we list the average performances 
for all ambiguous names in the last column in both 
tables. 
   Comparing Table 2 and Table 3, we find that 
Bagga?s model has different performances for the 
English and Chinese corpora. That means that 
contextual tokens have different contributions in 
the two languages. There are three apparent 
causes for this phenomenon. The first concerns 
the frequency of pronouns in English vs. pro-drop 
in Chinese. The typical usage of pronouns in Eng-
lish requires an accurate pronoun co-reference 
resolution that is very important for the local sen-
tence extraction in Bagga?s model. In the Boulder 
Name corpus, we found that ambiguous personal 
names occur in Chinese much more frequently 
than in English. For example, the string ?Liu Bo? 
occurs 876 times in the ?Liu Bo? data, but the 
string ?John Smith? occurs only 161 times in the 
?John Smith? data. The repetition of ambiguous 
personal names in Chinese reduces the burden on 
pronoun co-reference resolution and hence cap-
tures local information more accurately.  
The second factor is the fact that tokens in 
Bagga?s model for Chinese are words, but a Chi-
nese word is a unit bigger than an English word, 
and may contain more knowledge. For example, 
?the White House? has three words in English, 
and a word in Chinese. Since Chinese named-
entity detection can be considered a sub-problem 
of Chinese word segmentation, a word in Chinese 
can catch partial information about named entities.  
Finally, compared to Chinese news stories, 
English news stories are more likely to mention 
persons marginal to the story, and less likely to 
give the complete identifying information about 
them in local context. Those phenomena require 
more background information or implicit rela-
tional information to improve English named-
entity disambiguation. 
From Table 2 and Table 3, we see that the aver-
age performance of all ambiguous personal names 
is increased (from 87.42 to 93.77 for English and 
from 94.70 to 97.41 for Chinese) by incorporating 
more information: contextual bnp (contextual base 
noun phrases), summarized bnp (summarized base 
noun phrases), contextual entities, and document 
entities. This indicates that the phrase-based fea-
tures, the syntactic and semantic noun phrases, are 
very useful for disambiguation.  
From Table 2 and Table 3, we also see that the 
phrase-based features can improve the average 
performance, but not always for all ambiguous 
personal names. For example, the feature model 
?Bagga + summarized bnp + contextual entities? 
hurts the performance for ?Robert Smith.? As we 
mentioned above, the Boulder Name corpus is 
heterogeneous, so each feature does not make the 
same contribution to the disambiguation for any 
ambiguous personal name. What we need to do is 
to find a feature model that is robust for all am-
biguous personal names.  
In Table 4, we choose the last feature model?
Bagga + summarized bnp + document entities?as 
the final feature model, learn the fixed stop-
threshold for clustering from the development 
data, and show the corresponding performances as 
B-cubed F scores. The performances in italics are 
the performances with the optimal stop-threshold.  
From Table 4, we find that, with the exception of 
?Robert Smith? and ?Liu Bo,? the performances 
for other ambiguous personal names with the 
fixed threshold are close to the corresponding best 
performances. 
5 Conclusion 
This work has explored the problem of personal 
named-entity disambiguation for news corpora. 
Our experiments extend token-based information 
to include noun phrase-based information along 
two dimensions: from syntax to semantics, and 
from local sentential contexts to document-level 
contexts. From these experiments, we find that 
rich and broad information improves the disam-
biguation performance considerably for both Eng-
lish and Chinese. In the future, we will continue to 
explore additional semantic features that can be 
robustly extracted, including features derived 
from semantic relations and semantic role labels, 
and try to extend our work from news articles to 
197
web pages that include more noisy information. 
Finally, we have focused here primarily on feature 
development and not on clustering. We believe 
that the skewed long-tailed distribution that char-
acterizes this data requires the use of clustering 
algorithms tailored to this distribution. In particu-
lar, the large number of singleton clusters is an 
issue that confounds the standard clustering meth-
ods we have been employing. 
References 
A. Bagga and B. Baldwin. 1998. Entity?based Cross?
document Co?referencing Using the Vector Space 
Model. In 17th COLING. 
Y. Chen and K. Hacioglu. 2006. Exploration of 
Coreference Resolution: The ACE Entity Detection 
and Recognition Task. In 9th International Confer-
ence on TEXT, SPEECH and DIALOGUE. 
W. Cohen, P. Ravikumar, S. Fienberg. 2003. A Com-
parison of String Metrics for Name-Matching Tasks. 
In IJCAI-03 II-Web Workshop.  
C. H. Gooi and J. Allan. 2004. Cross-Document 
Coreference on a Large Scale Corpus. In NAACL  
K. Hacioglu, B. Douglas and Y. Chen.  2005. Detection 
of Entity Mentions Occurring in English and Chi-
nese Text. Computational Linguistics. 
K. Hacioglu. 2004. A Lightweight Semantic Chunking 
Model Based On Tagging. In HLT/NAACL. 
X. Li, P. Morie, and D. Roth. 2004. Robust Reading: 
Identification and Tracing of Ambiguous Names. In 
Proc. of  NAACL, pp. 17?24.  
B. Malin. 2005. Unsupervised Name Disambiguation 
via Social Network Similarity. SIAM. 
G. Mann and D. Yarowsky. 2003. Unsupervised Per-
sonal Name Disambiguation. In Proc. of CoNLL-
2003, Edmonton, Canada. 
C. Niu, W. Li, and R. K. Srihari. 2004. Weakly Super-
vised Learning for Cross-document Person Name 
Disambiguation Supported by Information Extrac-
tion. In ACL 
B. On and D. Lee. 2007. Scalable Name Disambigua-
tion using Multi-Level Graph Partition. SIAM. 
T. Pedersen, A. Purandare and A. Kulkarni. 2005. 
Name Discrimination by Clustering Similar Con-
texts. In Proc. of the Sixth International Conference 
on Intelligent Text Processing and Computational 
Linguistics, pages 226-237. Mexico City, Mexico. 
T. Pedersen and A. Kulkarni. 2007. Unsupervised Dis-
crimination of Person Names in Web Contexts. In 
Proc. of the Eighth International Conference on In-
telligent Text Processing and Computational Lin-
guistics. 
W. E. Winkler. 1999. The state of record linkage and 
current research  problems. Statistics of Income Di-
vision, Internal Revenue Service Publication R99/04.  
A. Yates and O. Etzioni. 2007. Unsupervised Resolu-
tion of Objects and Relations on the Web. In 
NAACL.  
198
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 379?386, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Detection of Entity Mentions Occurring in English and Chinese Text
Kadri Hacioglu, Benjamin Douglas and Ying Chen
Center for Spoken Language Research
University of Colorado at Boulder
{hacioglu,benjamin.douglas,yc}@colorado.edu
Abstract
In this paper, we describe an integrated
approach to entity mention detection that
yields a monolithic, almost language in-
dependent system. It is optimal in the
sense that all categorical constraints are si-
multaneously considered. The system is
compact and easy to develop and main-
tain, since only a single set of features and
classifiers are needed to be designed and
optimized. It is implemented using one-
versus-all support vector machine (SVM)
classifiers and a number of feature extrac-
tors at several linguistic levels. SVMs
are well known for their ability to han-
dle a large set of overlapping features with
theoretically sound generalization proper-
ties. Data sparsity might be an impor-
tant issue as a result of a large number
of classes and relatively moderate train-
ing data size. However, we report re-
sults that the integrated system performs
as good as a pipelined system that decom-
poses the problem into a few smaller sub-
tasks. We conduct all our experiments us-
ing ACE 2004 data, evaluate the systems
using ACE metrics and report competitive
performance.
1 Introduction
The entity-relation (ER) model (Chen, 1976) views
the physical world as a collection of entities with
complex relationships. Automatic extraction of
this model from raw text is important for creat-
ing a knowledge base (such as relational databases,
marked-up text etc.) that can be used to achieve bet-
ter end-to-end performances in several natural lan-
guage processing (NLP) applications including in-
formation retrieval, question answering and machine
translation. For example, in a typical QA system this
knowledge base can be used to facilitate extraction
of answers and retrieval of relevant documents.
Entities and relations in a document can be men-
tioned in several different ways. For example, a per-
son entity, e.g. Bill Clinton, can be expressed in
many different ways such as The President, Presi-
dent Clinton, Mr. Clinton, he, him etc. Similarly,
one can express a geo-political entity, e.g. United
States, as his country or another person entity, e.g.
Hillary Clinton, as his wife, and their relation to the
entity Bill Clinton as ?president-of? and ?family?,
respectively. It is clear that the detection of these
mentions is the first crucial step for the extraction of
the ER model to populate a database or an ontology.
Extraction of entities and their relationships is
usually done in a pipelined system that first iden-
tifies entity mentions, next resolves mentions into
unique entities (co-reference) and finally finds rela-
tions among them (Florian et al, 2004; Kambhatla,
2004). In that architecture, the errors in the first
stage propagate and reduce the performance of sub-
sequent stages; namely, co-reference resolver, that
clusters all different mentions of an entity into a
unique entity, and relation finder, that links entities
according to their relationships. In fact, the subtask
of entity mention detection itself is a very challeng-
379
Table 1: Categorical structure of entities in ACE program
Entity Mention
Entity Mention
Type Sub-Type Class Type Role
ing subtask since respective expressions can have
relatively complex syntactic and categorical (?se-
mantic?) structures. That is, entity mentions in a
body of text can occur in relatively complex embed-
ded constructs with many attributes. Table 1 illus-
trates the categorical structure of an entity mention
as specified in the Automatic Content Extraction
(ACE) program run by NIST (ACE, 2004). Com-
pared to the previous years the number of entity
types and subtypes is greater.
The following segment of a sentence provides a
typical example of the annotation:
[The [[Jordanian] military] spokesman] added ...
For simplicity, the entity mention attributes are
excluded. The annotation clearly shows the em-
bedded structure of entity mentions. We identify
three entity mentions as The Jordanian military
spokesman, Jordanian military and Jordanian.
Due to its complex nature, it is not uncommon that
the mention detection task itself is also divided into
a number of smaller sub-tasks. However, in this pa-
per, we adopt an integrated classification approach
to this problem that yields a monolithic structure.
This allows all attributes, which define the categori-
cal (?semantic?) structure of a mention, to be jointly
considered. The system has the ability to achieve
better performance in principle provided that there
is ?enough? data to train, is easier to maintain and
develop, and has a single set of features and classi-
fiers to be engineered. All possible class labels are
obtained by filling in the values of each attribute in
the label etype subtype class mtype role, where,
to avoid confusion, etype and mtype are used to de-
note entity and mention types, respectively.
Our data representation requires segmenting doc-
uments into sentences and then tokenizing sentences
into words and punctuation. Each word is then as-
signed a label depending on its role in the mention.
This data representation reduces the problem to a
tagging task. For each token in focus, we create a
number of features at lexical, syntactic and semantic
levels. Additionally, we augment those features us-
ing features from external resources (e.g. named en-
tity taggers, gazetteers, wordnet). We train a number
of one-versus-all classifiers (Allwein et. al, 2000)
using SVMs (Vapnik, 1995; Burges, 1998). During
testing, classification of each token is performed in
a greedy left-to-right manner using a finite-size slid-
ing context window centered at the token in focus
(Kudo and Matsumato, 2000).
This approach yields a large number of classes
and a large number of overlapping features. We used
a machine learning framework based on SVM clas-
sification since a large number of classes (in a one-
versus-all set-up) and a large number of overlapping
features can be easily handled with good general-
ization properties. We argue that data sparsity and
computational complexity is not as severe as it might
be expected in the other machine learning methods
that are based on maximum likelihood parameter es-
timation. In other words, we claim that the large set
of classification labels and training data sparseness
are not major drawbacks. To provide evidence for
this we also consider an approach that divides the
task into relatively simpler tasks with considerably
smaller numbers of labels. The approach yields a
pipelined structure in which the decisions in earlier
stages are used in later stages. We report results that
the integrated approach performs similar to, and in
some cases, even slightly better than the pipelined
structure.
We also implement a novel post-processing
scheme based on an entity base (EB) created from
the tagged test data. This is motivated by the fact
that an entity is identically referenced several times
in a document. However, depending on the capital-
ization information of the entity mention and context
in which it occurs, the entity can be missed at several
positions in the document. A simple postprocess-
ing algorithm that checks untagged tokens with low
confidence against the EB is implemented. In doing
so, it is highly likely that some of those missed en-
tities could be identified. This is expected to reduce
misses at the expense of false alarms. We report re-
sults that support our expectation.
The paper is organized as follows. Section 2 de-
scribes the ACE 2004 data used for training and
evaluation. In Section 3, the problem is explained
380
Table 2: ACE 2004 corpus statistics for English and Chinese
text.
Language Train Test
English ? 150K words ? 50K words
Chinese ? 150K words ? 50K words
and its data representation is introduced. Section 4
describes the general system architecture, that con-
sists of a number of feature extractors, a (machine-
learned) classifier and a simple post processor. In
section 5, the features used for both English and
Chinese systems are described. In section 6, we de-
scribe an alternative pipelined system. A novel post
processing algorithm is introduced in section 7. Sec-
tion 8 reports experimental results. Concluding re-
marks are made in the final section.
2 ACE Data
The ACE 2004 corpus consists of various text an-
notated for entities and relations. This corpus was
created by the Linguistic Data Consortiom (LDC)
in three languages: English, Chinese and Arabic
(with support from the ACE program that began in
1999). Resources for data are newswire reports and
broadcast news programs. Table 2 gives train and
test statistics of this corpus for English and Chinese
languages. Both languages have almost the same
amount of data for both training and evaluation.
3 Problem Description and Data
Representation
As shown in Table 1, an entity mention is charac-
terized along 5 dimensions; namely etype, sub-type
class, mtype and role. The ACE program speci-
fies seven entity types; person, organization, geo-
political, location, facility, vehicle, weapon. All en-
tity types except person are further divided into sev-
eral sub-types. For example, organization has gov-
ernment, commercial, educational, non-prot and
other as its sub-types. The class attribute describes
the kind of reference the entity mention makes to
the entity in the world by taking one of the values
{generic, specic, negative, under-specied} . En-
tity mentions are further characterized according to
linguistic types of references as name (proper noun),
nominal (common noun), pronominal (pronoun) and
premodifier. The role of entity mention applies only
to geo-political entities indicating the role of the en-
tity in the context of the mention as one of person,
location, organization and geo-political. For further
details the reader is referred to (ACE, 2004)
All entity mentions in the original data are
XML tagged with their respective attributes. In
addition to the full extent of mentions, mention
heads are also tagged. Referring to the previous
example, the entity mention ?The Jordanian military
spokesman? which refers to a PERSON has the
word ?spokesman? as its head. Similarly, the entity
mention ?Jordanian military? which refers to an
ORGANIZATION has the word ?military? as its
head. If one reduces the problem of entity mention
detection to the detection of its head, the nature
of the problem changes and the annotation of data
becomes flat;
The [GPE Jordanian] [ORG military] [PERspokesman] .....
This allows us to consider the problem as a
tagging/chunking problem and describe each word
as beginning (B) an entity mention, inside (I) an
entity mention or outside (O) an entity mention
(Ramhsaw and Marcus, 1995; Sang and Veenstra,
1999). However, we believe that the information
regarding the embedded structure in which the
heads of entities occur is also useful for subse-
quent stages of an IE system including inference
of relations among heads occurring in the same
embedded construct. So, in addition to the IOB tags
we introduce bracketing tags that might partially
recover the embedded structure surrounding the
heads. We refer to the following simple example
[Javier Trevino] was [the campaign manager for
[the [ruling party] candidate [Fox] beat ]].
to illustrate our tokenwise vertical representa-
tion:
#SNT BEG#
Javier B-PER NAM
Trevino I-PER NAM
was O
381
Lexical
Analysis
Syntactic
Analysis
Semantic
Analysis
External
Taggers Lookup
Resource 
Feature Combiner
Documents Preprocessor
SVM
Models
Tagged
Documents
PostprocessorClassifier
WordNet
Gazetteers
Figure 1: System Architecture
the (*
campaign *
manager B-PER NOM
for *
the (*
ruling *
party B-ORG NOM
candidate B-PER NOM
Fox B-PER NAM
beat *))
. O
#SNT END#
If one does not use the bracketing representation, all
non-head tokens will be labeled as ?Outside?. We
believe that it is useful to discriminate the tokens that
take part in mentions from those that do not occur in
mentions.
4 General System Architecture
The general system block diagram is illustrated in
Figure 1. It consists of a pre-processor, several fea-
ture extractors, a classifier and a post-processing
module. Although the architecture is language in-
dependent, there are some minor language specific
differences in some modules depending on the na-
ture of the language and availability of resources for
that language. In the following, we briefly describe
both English and Chinese systems and indicate dif-
ferences between them.
In the English system, the pre-processor segments
the documents into sentences. It also includes a
caser that restores the capitalization information of
text without case (e.g. broadcast news) and a to-
kenizer that separates contractions and punctuation
from words. Tokenized sentences are then processed
at different linguistic levels to create features. At
this stage, we employ a lexical pattern analyzer,
part-of-speech tagger, a base phrase chunker, a syn-
tactic parser, a dependency analyzer, look-up inter-
faces to external knowledge sources, and external
small scale named entity taggers trained on different
genres of text with different machine learning algo-
rithms. All features are combined and then input to
a classifier based on one-versus-all SVM classifiers.
Finally, we perform simple post-processing to make
sure that the final bracketing information is consis-
tent.
The POS tagger and BP chunker are trained
in-house using the Penn TreeBank. The syntac-
tic parser is the Charniak parser which has mod-
els trained on the Penn TreeBank. The depen-
dency analyzer performs dependency analysis using
a set of head rules. The software was generously
made available to us by the University of Maryland.
The look-up interface to external knowledge sources
such as WordNet or gazetteers is implemented using
simple pattern matching.
In the Chinese system, the pre-processor is
slightly different from that of the English system.
It (obviously) does not need a caser and consid-
ers single Chinese characters as the minimal units
of processing. It jointly segments a document
into sentences and words. Then, it passes both
word and sentence segmentation information to the
subsequent stages along with Chinese characters.
The SVM-based joint sentence/word segmenter is
trained using the Chinese TreeBank (CTB). Linguis-
382
tic analysis at different levels is performed in a man-
ner similar to the analysis in the English system.
In the Chinese system, the CTB is used to train
a SVM-based POS tagger and BP chunker. The
syntactic parser is trained on the CTB using Dan
Bikel?s parser. Dependency analysis is performed
as in the English system using a set of Chinese head
rules. Several in-house external taggers are trained
using SVMs and different corpora. We have used
only gazetteers for chinese as external knowledge
sources.
5 Features
The following features are used in the English sys-
tem:
? tokens: words in their original and all lower-
cased forms
? n-grams: token prefixes and suffixes of length
less than and equal to four
? lexical patterns: indicate case information
(all lower-case, mixed case, first letter capital,
all upper-case), is hyphen, type (numeral, al-
phanumeral, alpha, other)
? Part of Speech tags
? BP Positions: The position of a token in a BP
using the IOB representation (e.g. B-NP, I-NP,
O etc.)
? Clause tags: The tags that mark token posi-
tions in a sentence with respect to clauses. (e.g
*S)*S) marks a position that two clauses end)
? Named entities-1: The IOB tags of named en-
tities. There are four categories; LOC, ORG,
PERSON and MISC. A SVM-based tagger
which is trained on CoNLL 2003 shared task
data is used.
? Named entities-2: IOB tags of named enti-
ties found by the Identifinder (Bikel et. al,
1999); a HMM-based named entity tagger with
29 classes
? Named entities-3: IOB tags from a named en-
tity tagger trained on MUC-6 and MUC-7 data
using only the entity classes PERSON, LOCA-
TION and ORGANIZATION.
? Gazetteer labels: indicate the name of the list
to which the token belongs. Simple pattern
matching is employed here.
? WordNet categories: concepts or class names
in the WordNet 2.0 hypernym hierarchy rooted
at ?entity? concept. We trace hypernym hier-
archies of the two most frequent senses of to-
kens that are tagged as nouns (NN, NNS, NNP
etc.) to the top concepts. We count the num-
ber of concepts (that match to ACE entity types
and subtypes) that occur in the hypernym hier-
archy indicating that token is a (kind of) con-
cept. The concepts (i.e entity/types/subtypes)
with the maximum counts in the top two senses
are selected as features (can also be considered
as ?maybe? labels)
? Syntactic tags: patterns of non-terminals and
brackets that indicate the position of tokens in
syntactic trees.
? Head words: words that the tokens depend
? POS of Head words:
? main verb: the verb at which the dependency
parse tree is rooted.
? Relations: the grammatical and semantic rela-
tions between tokens and their heads.
? Head word flag: indicates whether the token
plays a role of head in the sentence.
The features used in the Chinese system are
? tokens: Chinese characters
? token positions: IOB tags that indicate posi-
tion of characters in words
? Part of Speech tags: POS tags of words to
which tokens (characters) belong
? BP Positions: The position of a token in a BP
using the IOB representation (e.g. B-NP, I-NP,
O etc.)
? Named entities-1: IOB tags of two type of en-
tities; location and person. A SVM based tag-
ger trained on part of the Sinica corpus from
Taiwan is used to generate these features.
383
? Named entities-2: IOB tags of named enti-
ties: person, location, organization etc. An-
other SVM based tagger trained on the People
Daily data from mainland of China.
? Gazetteer labels: indicate the name of the list
to which the token belongs. Simple pattern
matching is employed here. Examples are la-
bels that indicate Chinese last name, foreign
person last name, first name etc.
? Syntactic labels: base phrase chunk labels and
paths in syntactic trees
? Head words: as determined by Chinese depen-
dency analysis
? POS of Head words:
? Relations: the grammatical and semantic rela-
tions between tokens and their heads.
6 A Pipelined System
As mentioned earlier the structure of entity men-
tion categories is very complex. Considering all at-
tributes together yields a large number of classes.
One can argue that the large number of classes and
data sparsity is an important issue here that it might
have significant effect on performance. However,
several attempts to divide the task into simpler sub-
tasks have failed to yield a system with a better per-
formance than that of the integrated system. In this
section, we describe one such system.
The system consists of three stages in cascade: (i)
entity mention extent detector, , (ii) mention type de-
tector and (iii) entity type, subtype and mention role
detector. Referring to the earlier example, the data
representation in terms of class labels at each level
is as follows:
#SNT BEG#
Javier (* B-NAM PER
Trevino *) I-NAM PER
was O O O
the (* O O
campaign * O O
manager * B-NOM PER
for * O O
the (* O O
ruling (* O O
party *) B-NOM ORG
candidate *) B-NOM PER
Fox * B-NAM PER
beat *)) O O
. O O O
#SNT END#
where the second column is for the extent labels of
mentions in bracketed representation, the third col-
umn is for the mention type labels in IOB represen-
tation and the last column is for the type labels (sub-
type and role labels are omitted for the sake of sim-
plicity) of entity mentions in plain representation.
The pipelined system operates as follows. First it
detects embedding structure of mention extents. Us-
ing that information the second stage identifies the
type of mentions. In the final stage, the system iden-
tifies entity types, subtypes and mention roles using
information (as features within context) from previ-
ous stages. Finally we combine all information into
entity mention attributes and resolve inconsistencies
by simple postprocessing.
Here, we have not done any feature selection spe-
cific to each stage. Instead we used the same fea-
tures in all stages. One can argue that this is not the
optimal set up for a cascaded system; separate fea-
ture design and selection should be made for each
stage. Also we acknowledge that there are several
other ways of dividing the task into smaller, simpler
subtasks. Although we have not explored all pos-
sible pipelined architectures with all possible fea-
ture selections , we conjecture that the data sparsity
is not as big an issue in SVMs as expected to be
in the other machine learning algorithms based on
maximum likelihood parameter estimation such as
those based on maximum entropy (ME) or condi-
tional random fields (CRF) frameworks.
7 A Novel Post-Processing Method
In our experiments, we have consistently observed
that the identical mentions of a unique entity are
missed depending on the missing capitalization in-
formation, unseen context and errors in feature ex-
traction. For example, although the name mention
of person ?Eminem? is captured at several positions
in the document, the entity mention ?eminem? is
missed, probably, due to its missing capitalization.
384
Table 3: Statistics on ACE 2004 data.
Language Train Samples Test Samples # Joint Classes # Pipelined Classes
Extent MType EType-SubTypey-Role
English ? 167K ? 61K 384 24 9 93
Chinese ? 307K ? 105K 374 15 7 95
As a solution we propose a post-processing
method that is based on an entity base (EB) cre-
ated from the tagged text. We populate the EB with
all entity mentions (particularly with those that have
name values) identified in the text. After we create
the EB, we tag the text again by case insensitive pat-
tern matching. We determine all tagged tokens that
were initially left untagged or tagged with a differ-
ent label by the SVM classifier. Using the SVM out-
put (distance from separating hyperplane) as a confi-
dence measure, we accept or reject the new tag based
on a preselected threshold.
8 Experiments and Results
In this section, we describe the experiments con-
ducted and results obtained using the ACE 2004
data. The number of training and test examples,
which are words/punctuations in English and char-
acters in Chinese, are summarized in Table 3. The
number of classes in the joint task and in each
pipelined subtask are also included.
In the first set of experiments we evaluated our
integrated system and investigated the performance
with respect to broad classes of features introduced
in section 5, by adding one group of features at a
time. Grouping of features into broad classes were
done as follows:
? baseline features: tokens
? lexical features: POS, lexical patterns
? syntactic features: base phrase chunks, syntac-
tic tree features
? ?semantic? features: heads and grammatical re-
lations
? external features: features from external re-
sources; e.g. wordnet, gazetteers, other entity
taggers etc.
Table 4: English system performance with respect to broad
classes of features; lex: lexical features, syn: syntactic fea-
tures, sem: ?semantic? features, ext: external features, Fuw:unweighted F-score, Fw: weighted F-score, ACE: ACE value.
Feature class Fuw Fw ACE
baseline (tokens) 56.5 54.8 36.1
baseline+lex 76.8 86.7 75.6
baseline+lex+syn 76.9 87.4 76.8
baseline+lex+syn+sem 77.1 87.8 77.6
baseline+lex+syn+sem+ext 82.0 90.7 82.9
The results are summarized in Table 4 and Table
5 for both English and Chinese systems. Both un-
weighted and weighted F-scores, and also ACE val-
ues are reported. It is interesting to note that sig-
nificant gains were achieved by simple lexical and
external features when they are added. The degree
of improvement by using computationally intensive
syntactic and dependency analysis is marginal. This
might partly be due to the type of features derived
from parse trees and partly due to the mismatch of
the genre of text to the text on which the syntactic
chunker and parser is trained. Since the dependency
analysis is based on the syntactic analysis using a
set of head rules, the extracted dependency based
features might also be inaccurate. Although we
observed moderate improvement for English, those
features slightly hurt the performance of the Chinese
system. This is because of the fact that the Chinese
syntactic parser performs relatively worse than the
English syntactic parser.
Table 6 presents the integrated and pipelined sys-
tem performances using all features extracted for
English and Chinese. Post-processing results are
also included. It shows notable performance im-
provement with the recovery of many misses by
post-processing. It should be noted that, in the
pipelined architecture the post-processing is per-
formed twice; at both mention and entity levels.
385
Table 5: Chinese system performance with respect to broad
classes of features; lex: lexical features, syn: syntactic fea-
tures, sem: ?semantic? features, ext: external features, Fuw:unweighted F-score, Fw: weighted F-score, ACE: ACE value.
Feature class Fuw Fw ACE
baseline (tokens) 77.6 83.5 70.8
baseline+lex 78.3 85.2 73.4
baseline+lex+syn 76.1 83.7 70.8
baseline+lex+syn+sem 74.8 83.6 70.8
baseline+lex+syn+sem+ext 78.4 86.8 76.1
9 Conclusions
We have discussed the significance of the entity
mention detection in ER model extraction from
raw text and presented the complex syntactic and
categorical structure of the entity mentions speci-
fied in the ACE program. We have explored dif-
ferent ways of representing the problem and im-
plemented two architecturally different (supervised)
machine-learning based systems to accomplish the
task; namely, a monolithic system and a cascaded
system. We have described those systems in detail
and empirically compared them. Both systems have
achieved comparable performances on English text.
However, the integrated system has achived moder-
ately better performance on Chinese text. We have
argued that it is easier to develop and maintain the
monolithic system since it has a single set of features
and classifiers to be tuned. We believe that the per-
formance levels achieved at mid 80s (in ACE values)
for English and at upper 70s for Chinese, using only
the ACE data, are competitive. We have introduced
a post-processing algorithm based on an entity base
created during the testing. It has worked very well
for both languages to recover several missed en-
tity mentions and considerably improved the perfor-
mance.
10 Acknowledgement
We extend our special thanks to Wayne Ward,
Steven Bethard, James H. Martin and Dan Juraf-
sky for their useful feedback during this work. This
work is supported by the ARDA Aquaint II Program
via contract NBCHC040040.
Table 6: English and Chinese system performances with all
features and post-processing: Fuw: unweighted F-score, Fw:weighted F-score, ACE: ACE value.
English System Fuw Fw ACE
Integrated 82.0 90.7 82.9
Pipelined 82.1 90.8 83.1
Integrated+Post 82.2 91.5 84.3
Pipelined+Post 82.3 91.3 84.0
Chinese System
Integrated 78.4 86.8 76.1
Pipelined 76.9 85.7 74.1
Integrated+Post 79.6 87.7 77.5
Pipelined+Post 79.1 86.6 75.6
References
E. L. Allwein, R. E Schapire and Y. Singer. 2000. Re-
ducing multiclass to binary: A unifying approach formargin classifiers. Journal of Machine Learning Re-search, 1:113-141,
Dan M. Bikel, Robert L. Schwartz, and Ralph M.Weischedel. 1999 An algorithm that learns what?s
in a name. Machine Learning, Vol. 34, pp. 211-231.
Chiristopher J. C. Burges 1998. Tutorial on Support Vec-tor Machines for Pattern Recognition. Data Miningand Knowledge Discovery, 2(2), pages 1-47.
Peter P. Chen 1976. The Entity-Relationship Model:Toward a Unified View of Data. ACM Trans. onDatabase Systems, Vol. 1, No. 1, pages 1-36.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. AStatistical Model for Multilingual Entity Detection and
Tracking. Proceedings of HLT-2004.
Nanda Kambhatla. 2004. Combining Lexical Syntactic
and Semantic Features with Maximum Entropy Mod-els for Extracting Relations. Proceedings of ACL-04.
Taku Kudo and Yuji Matsumato. 2000. Use of support
vector learning for chunk identification. Proc. of the4th Conference on Very Large corpora, pages 142-144.
Lance E. Ramhsaw and Mitchel P. Marcus. 1995.Text Chunking Using Transformation Based Learning.Proceedings of the 3rd ACL Workshop on Very LargeCorpora, pages 82-94.
Erik F. T. J. Sangand and Jorn Veenstra 1999. Repre-senting text chunks. Proceedings of EACL?99, pages
173-179.
The Automatic Content Extraction (ACE) Evaluation
Plan. 2004. www.nist.gov/speech/tests/ace/
Vladamir Vapnik. 1995. The Nature of Statistical Learn-ing Theory. Springer Verlag, New York, USA.
386
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 125?128,
Prague, June 2007. c?2007 Association for Computational Linguistics
CU-COMSEM: Exploring Rich Features for Unsupervised Web Per-
sonal Name Disambiguation  
Ying Chen 
Center for Spoken Language Research
University of Colorado at Boulder 
yc@colorado.edu  
James Martin 
Department of Computer Science 
University of Colorado at Boulder 
James.Martin@colorado.edu 
 
 
Abstract 
The increasing number of web sources is 
exacerbating the named-entity ambiguity 
problem. This paper explores the use of 
various token-based and phrase-based fea-
tures in unsupervised clustering of web 
pages containing personal names. From 
these experiments, we find that the use of 
rich features can significantly improve the 
disambiguation performance for web per-
sonal names. 
1 Introduction 
As the sheer amount of web information expands 
at an ever more rapid pace, the named-entity am-
biguity problem becomes more and more serious 
in many fields, such as information integration, 
cross-document co-reference, and question an-
swering. Individuals are so glutted with informa-
tion that searching for data presents real problems. 
It is therefore crucial to develop methodologies 
that can efficiently disambiguate the ambiguous 
names from any given set of data. 
In the paper, we present an approach that com-
bines unsupervised clustering methods with rich 
feature extractions to automatically cluster re-
turned web pages according to which named en-
tity in reality the ambiguous personal name in a 
web page refers to. We make two contributions to 
approaches to web personal name disambiguation. 
First, we seek to go beyond the kind of bag-of-
words features employed in earlier systems 
(Bagga & Baldwin, 1998; Gooi & Allan, 2004; 
Pedersen et al, 2005), and attempt to exploit deep 
semantic features beyond the work of Mann & 
Yarowsky (2003). Second, we exploit some fea-
tures that are available only in a web corpus, such 
as URL information and related web pages.   
The paper is organized as follows. Section 2 in-
troduces our rich feature extractions along with 
their corresponding similarity matrix learning. In 
Section 3, we analyze the performance of our sys-
tem. Finally, we draw some conclusions. 
2 Methodology 
Our approach follows a common architecture for 
named-entity disambiguation: the detection of 
ambiguous objects, feature extractions and their 
corresponding similarity matrix learning, and fi-
nally clustering. 
Given a webpage, we first run a modified Beau-
tiful Soup1 (a HTML parser) to extract a clean text 
document for that webpage. In a clean text docu-
ment, noisy tokens, such as HTML tags and java 
codes, are removed as much as possible, and sen-
tence segmentation is partially done by following 
the indications of some special HTML tags. For 
example, a sentence should finish when it meets a 
?<table>? tag. Then each clean document contin-
ues to be preprocessed with MXTERMINATOR 
(a sentence segmenter), 2  the Penn Treebank to-
kenization,3 a syntactic phrase chunker (Hacioglu, 
2004), and a named-entity detection and co-
reference system for the ACE project4 called EX-
                                                 
1 http://www.crummy.com/software/BeautifulSoup 
2http://www.id.cbs.dk/~dh/corpus/tools/MXTERMINATOR.
html 
3 http://www.cis.upenn.edu/~treebank/tokenization.html 
4 http://www.nist.gov/speech/tests/ace 
125
ERT5 (Hacioglu et al 2005; Chen & Hacioglu, 
2006).  
2.1 The detection of ambiguous objects  
For a given ambiguous personal name, for each 
web page, we try to extract all mentions of the 
ambiguous personal name, using three possible 
varieties of the personal name. For example, the 
three regular expression patterns for ?Alexander 
Markham? are ?Alexander Markham,? ?Markham, 
Alexander,? and ?Alexander .\. Markham? (?.\.? 
can match a middle name). Web pages without 
any mention of the ambiguous personal name of 
interest are discarded and receive no further 
processing.   
Since it is common for a single document to 
contain one or more mentions of the ambiguous 
personal name of interest, there is a need to define 
the object to be disambiguated.  Here, we adopt 
the policy of ?one person per document? (all men-
tions of the ambiguous personal name in one web 
page are assumed to refer to the same personal 
entity in reality) as in Bagga & Baldwin (1998), 
Mann & Yarowsky (2003) and Gooi & Allan 
(2004). We therefore define an object as a single 
entity with the ambiguous personal name in a 
given web page. This definition of the object 
(document-level object) might be mistaken, be-
cause the mentions of the ambiguous personal 
name in a web page may refer to multiple entities, 
but we found that this is a rare case (most of those 
cases occur in genealogy web pages). On the other 
hand, a document-level object can include much 
information derived from that web page, so that it 
can be represented by rich features.   
Given this definition of an object, we define a 
target entity as an entity (outputted from the 
EXERT system) that includes a mention of the 
ambiguous personal name. Then, we define a local 
sentence as a sentence that contains a mention of 
any target entity. 
2.2 Feature extraction and similarity matrix 
learning 
Most of the previous work (Bagga & Baldwin, 
1998; Gooi & Allan; 2004; Pedersen et al, 2005) 
uses token information in the given documents. In 
this paper, we follow and extend their work espe-
cially for a web corpus. On the other hand, com-
                                                 
5 http://sds.colorado.edu/EXERT 
pared to a token, a phrase contains more informa-
tion for named-entity disambiguation. Therefore, 
we explore some phrase-based information in this 
paper. Finally, there are two kinds of feature vec-
tors developed in our system: token-based and 
phrase-based. A token-based feature vector is 
composed of tokens, and a phrase-based feature 
vector is composed of phrases.  
 
2.2.1 Token-based features 
There is a lot of token information available in a 
web page: the tokens occurring in that web page, 
the URL for that web page, and so on. Here, for 
each web page, we tried to extract tokens accord-
ing to the following schemes. 
Local tokens (Local): the tokens occurring in the 
local sentences in a given webpage; 
Full tokens (Full): the tokens occurring in a given 
webpage; 
URL tokens (URL): the tokens occurring in the 
URL of a given webpage. URL tokenization 
works as follows: split a URL at ?:? and ?.?, and 
then filter out stop words that are very common in 
URLs, such as ?com,? ?http,? and so on;  
Title tokens in root page (TTRP): the title tokens 
occurring in the root page of a given webpage. 
Here, we define the root page of a given webpage 
as the page whose URL is the first slash-
demarcated element (non-http) of the URL of the 
given webpage. For example, the root page of 
?http://www.leeds.ac.uk/calendar/court.htm? is 
?www.leeds.ac.uk?. We do not use all tokens in 
the root page because there may be a lot of noisy 
information. 
Although Local tokens and Full tokens often 
provide enough information for name disambigua-
tion, there are some ambiguity cases that can be 
solved only with the help of information beyond 
the given web page, such as URL tokens and 
TTRP tokens. For example, in the web page 
?Alexander Markham 009,? there is not sufficient 
information to identify the ?Alexander Markham.? 
But from its URL tokens (?leeds ac uk calendar 
court?) and the title tokens in its root page (?Uni-
versity of Leeds?), it is easy to infer that this 
?Alexander Markham? is from the University of 
Leeds, which can totally solve the name ambigu-
ity.  
Because of the noisy information in URL to-
kens and TTRP tokens, here we combine them 
with Local tokens, using the following policy: for 
126
each URL token and TTRP token, if the token is 
also one of the Local tokens of other web pages, 
add this token into the Local token list of the cur-
rent webpage. We do the same thing with Full 
tokens. 
  Except URL tokens, the other three kinds of 
tokens?Local tokens, Full tokens and TTRP to-
kens?are outputted from the Penn Treebank to-
kenization, filtered by a stop-word dictionary, and 
represented in their morphological root form. But 
tokens in web pages have special characteristics 
and need more post-processing. In particular, a 
token may be an email address or a URL that may 
contain some useful information. For example, 
?charlotte@la-par.org? indicates the ?Charlotte 
Bergeron? who works for PAR (the Public Affairs 
Research Council) in LA (Los Angeles). To cap-
ture the fine-grained information in an email ad-
dress or a URL, we do deep tokenization on these 
two kinds of tokens. For a URL, we do deep to-
kenization as URL tokenization; for an email ad-
dress, we split the email address at ?@? and ?.?, 
then filter out the stop words as in URL tokeniza-
tion.   
So far, we have developed two token-based fea-
ture vectors: a Local token feature vector and a 
Full token feature vector. Both of them may con-
tain URL and TTRP tokens. Given feature vectors, 
we need to find a way to learn the similarity ma-
trix. Here, we choose the standard TF-IDF method 
to calculate the similarity matrix. 
 
2.2.2 Phrase-based features 
Since considerable information related to the am-
biguous object resides in the noun phrases in a 
web page, such as the person?s job and the per-
son?s location, we attempt to capture this noun 
phrase information. The following section briefly 
describes how to extract and use the noun phrase 
information. For more detail, see Chen & Martin 
(2007). 
Contextual base noun phrase feature: With 
the syntactic phrase chunker, we extract all base 
noun phrases (non-overlapping syntactic phrases) 
occurring in the local sentences, which usually 
include some useful information about the am-
biguous object. A base noun phrase of interest 
serves as an element in the feature vector. 
Document named-entity feature: Given the 
EXERT system, a direct and simple way to use 
the semantic information is to extract all named 
entities in a web page. Since a given entity can be 
represented by many mentions in a document, we 
choose a single representative mention to repre-
sent each entity. The representative mention is 
selected according to the following ordered pref-
erence list: longest NAME mention, longest 
NOMINAL mention.  A representative mention 
phrase serves as an element in a feature vector. 
Given a pair of feature vectors consisting of 
phrase-based features, we need to choose a simi-
larity scheme to calculate the similarity matrix. 
Because of the word-space delimiter in English, 
the feature vector comprises phrases, so that a 
similarity scheme for phrase-based feature vectors 
is required. Chen & Martin (2007) introduced one 
of those similarity schemes, ?two-level 
SoftTFIDF?. First, a token-based similarity 
scheme, the standard SoftTFIDF (Cohen et al, 
2003), is used to calculate the similarity between 
phrases in the pair of feature vectors; in the sec-
ond phase, the standard SoftTFIDF is reformu-
lated to calculate the similarity for the pair of 
phrased-based feature vectors.  
First, we introduce the standard SoftTFIDF. In 
a pair of feature vectors S and T, S = (s1, ? , sn ) 
and T = (t1, ? , tm). Here, si (i = 1?n) and tj (j = 
1?m) are substrings (tokens). Let CLOSE(?; S;T) 
be the set of substrings w ?  S such that there is 
some v?  T satisfying dist(w; v) > ?. The Jaro-
Winkler distance function (Winkler, 1999) is 
dist(;). For w? CLOSE(?; S;T), let D(w; T) = 
);(max vwdistTv? . Then the standard SoftTFIDF 
is computed as 
 
)D( )V( )V(
  )( SoftTFIDF
);;(
w, Tw, Tw, S
S,T
TSCLOSEw
??
=
? ? ?   
)(IDF log  1)  (TF log  )(V' ww,Sw, S ?+=         
? ?= S w, S
w, Sw, S
w
2)( V
)(  V  )( V                  ,   
where TFw,S is the frequency of substrings w in S, 
and IDFw is the inverse of the fraction of docu-
ments in the corpus that contain w. To compute 
the similarity for the phrase-based feature vectors, 
in the second step of ?two-level SoftTFIDF,? the 
substring w is a phrase and dist is the standard 
SoftTFIDF.  
So far, we have developed several feature mod-
els and learned the corresponding similarity ma-
127
trices, but clustering usually needs only one 
unique similarity matrix. In the results reported 
here, we simply combine the similarity matrices, 
assigning equal weight to each one. 
2.3 Clustering 
Although clustering is a well-studied area, a re-
maining research problem is to determine the op-
timal parameter settings during clustering, such as 
the number of clusters or the stop-threshold, a 
problem that is important for real tasks and that is 
not at all trivial. Because currently we focus only 
on feature development, we choose agglomerative 
clustering with a single linkage, and simply use a 
fixed stop-threshold acquired from the training 
data.  
3 Performance  
Our system performs very well for the Semeval 
Web People corpus, and Table 1 shows the 
performances. There are two results in Table 1: 
One is gotten from the evaluation of Semeval 
Web People Track (SemEval), and the other is 
evaluated with B-cubed evaluation (Bagga and 
Baldwin, 1998). Both scores indicate that web 
personal name disambiguation needs more effort. 
 
 Purity Inverse 
Purity 
F  
(?=0.5)
F  
(?=0.2)
SemEval 0.72 0.88 0.78 0.83 
 Precision Recall F  
(?=0.5)
F  
(?=0.2)
B-cubed 0.61 0.83  0.70 0.77 
Table 1  The performances of the test data 
4 Conclusion 
Our experiments in web personal name disam-
biguation extend token-based information to a 
web corpus, and also include some noun phrase-
based information. From our experiment, we first 
find that it is not easy to extract a clean text 
document from a webpage because of much noisy 
information in it. Second, some common tools 
need to be adapted to a web corpus, such as sen-
tence segmentation and tokenization. Many NLP 
tools are developed for a news corpus, whereas a 
web corpus is noisier and often needs some spe-
cific processing. Third, in this paper, we use some 
URL information and noun phrase information in 
a rather simple way; more exploration is needed in 
the future. Besides the rich feature extraction, we 
also need more work on similarity combination 
and clustering. 
 
Acknowledgements 
Special thanks are extended to Praful Mangalath 
and Kirill Kireyev. 
References 
J. Artiles, J. Gonzalo. and S. Sekine. 2007. The SemE-
val-2007 WePS Evaluation: Establishing a bench-
mark for the Web People Search Task. In Proceed-
ings of Semeval 2007, Association for Computa-
tional Linguistics. 
A. Bagga and B. Baldwin. 1998. Entity?based Cross?
document Co?referencing Using the Vector Space 
Model. In 17th COLING. 
Y. Chen and K. Hacioglu. 2006. Exploration of 
Coreference Resolution: The ACE Entity Detection 
and Recognition Task. In 9th International Confer-
ence on TEXT, SPEECH and DIALOGUE. 
Y. Chen and J. Martin. 2007. Towards Robust Unsu-
pervised Personal Name Disambiguation. EMNLP. 
W. Cohen, P. Ravikumar, S. Fienberg. 2003. A Com-
parison of String Metrics for Name-Matching Tasks. 
In IJCAI-03 II-Web Workshop.  
C. H. Gooi and J. Allan. 2004. Cross-Document 
Coreference on a Large Scale Corpus. NAACL  
K. Hacioglu, B. Douglas and Y. Chen.  2005. Detection 
of Entity Mentions Occurring in English and Chi-
nese Text. Computational Linguistics. 
K. Hacioglu. 2004. A Lightweight Semantic Chunking 
Model Based On Tagging. In HLT/NAACL. 
B. Malin. 2005. Unsupervised Name Disambiguation 
via Social Network Similarity. SIAM. 
G. Mann and D. Yarowsky. 2003. Unsupervised Per-
sonal Name Disambiguation. In Proc. of CoNLL-
2003, Edmonton, Canada. 
T. Pedersen, A. Purandare and A. Kulkarni. 2005. 
Name Discrimination by Clustering Similar Con-
texts. In Proc. of the Sixth International Conference 
on Intelligent Text Processing and Computational 
Linguistics, pages 226-237. Mexico City, Mexico. 
W. E. Winkler. 1999. The state of record linkage and 
current research  problems. Statistics of Income Di-
vision, Internal Revenue Service Publication R99/04. 
128
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 179?187,
Beijing, August 2010
Emotion Cause Detection with Linguistic Constructions 
 Ying Chen*?, Sophia Yat Mei Lee?, Shoushan Li?, Chu-Ren Huang?  
*
 Dep. of Computer Engineering 
China Agricultural University 
?Dep. of Chinese and Bilingual Studies 
The Hong Kong Polytechnic University 
{chenying3176,sophiaym,shoushan.li,churenhuang}@gmail.com 
 
 
Abstract 
This paper proposes a multi-label ap-
proach to detect emotion causes. The 
multi-label model not only detects mul-
ti-clause causes, but also captures the 
long-distance information to facilitate 
emotion cause detection. In addition, 
based on the linguistic analysis, we cre-
ate two sets of linguistic patterns during 
feature extraction. Both manually gener-
alized patterns and automatically gener-
alized patterns are designed to extract 
general cause expressions or specific 
constructions for emotion causes. Ex-
periments show that our system 
achieves a performance much higher 
than a baseline model.   
1 Introduction 
Text-based emotion processing has been a cen-
ter of attention in the NLP field in the past few 
years. Most previous researches have focused 
on detecting the surface information of emo-
tions, especially emotion classes, e.g., ?happi-
ness? and ?anger? (Mihalcea and Liu 2006, 
Strapparava and Mihalcea 2008, Abbasi et al 
2008, Tokuhisa et al 2008). Although most 
emotion theories recognize the important role of 
causes in emotion analysis (Descartes, 1649; 
James, 1884; Plutchik 1980, Wierzbicka 1999), 
very few studies explore the interactions be-
tween emotion and causes. Emotion-cause in-
teraction is the eventive relation which poten-
tially yields the most crucial information in 
terms of information extraction. For instance, 
knowing the existence of an emotion is often 
insufficient to predict future events or decide on 
the best reaction. However, if the emotion cause 
is known in addition to the type of emotion, 
prediction of future events or assessment of po-
tential implications can be done more reliably. 
In other words, when emotion is treated as an 
event, causal relation is the pivotal relation to 
discover. In this paper, we explore one of the 
crucial deep level types of information of emo-
tion, i.e. cause events.  
Our study focuses on explicit emotions in 
which emotions are often presented by emotion 
keywords such as ?shocked? in ?He was 
shocked after hearing the news?. Emotion caus-
es are the explicitly expressed propositions that 
evoke the presence of the corresponding emo-
tions. They can be expressed by verbs, nomi-
nalizations, and nominals. Lee et al (2010a) 
explore the causes of explicit emotions by con-
structing a Chinese emotion cause corpus. 
Based on this corpus, we formalize the emotion 
cause detection problem through extensive data 
analysis. We find that ~14% emotion causes are 
complicated events containing multi-clauses, to 
which previous cause detection systems can 
hardly be applied directly. Most previous cause 
detection systems focus on the causal relation 
between a pair of small-size text units, such as 
clauses or phrases. They are thus not able to 
detect emotion causes that are multi-clauses. In 
this paper, we formalize emotion cause detec-
tion as a multi-label classification task (i.e. each 
instance may contain more than one label), 
which allows us to capture long-distance infor-
mation for emotion cause detection. 
In term of feature extraction, as emotion 
cause detection is a case of cause detection, 
some typical patterns used in existing cause de-
tection systems, e.g., ?because? and ?thus?, can 
be adopted. In addition, various linguistic cues 
are examined which potentially indicate emo-
tion causes, such as causative verbs and epis-
temic markers (Lee at al. 2010a). Then some 
linguistic patterns of emotion causes are manu-
179
ally generalized by examining the linguistic 
context of the empirical data (Lee et al, 2010b). 
It is expected that these manually generalized 
patterns often yield a low-coverage problem. 
Thus, we extracted features which enable us to 
automatically capture more emotion-specific 
constructions. Experiments show that such an 
integrated system with various linguistic fea-
tures performs promisingly well. We believe 
that the present study should provide the foun-
dation for future research on emotion analysis, 
such as the detection of implicit emotion or 
cause.  
The paper is organized as follows. Section 2 
discusses the related work on cause-effect de-
tection. Section 3 briefly describes the emotion 
cause corpus, and then presents our data analy-
sis. Section 4 introduces the multi-label classifi-
cation system for emotion cause detection. Sec-
tion 5 describes the two kinds of features for our 
system, one is based on hand-coded patterns and 
the other is the generalized features. Section 6 
presents the evaluation and performance of our 
system. Section 7 highlights our main contribu-
tions and the possible future work. 
2 Related Work 
Most previous studies on textual emotion proc-
essing focus on emotion recognition or classifi-
cation given a known emotion context (Mihal-
cea and Liu 2006, Strapparava and Mihalcea 
2008, Abbasi et al 2008, Tokuhisa et al 2008). 
However, the performance is far from satisfac-
tory. One crucial problem in these works is that 
they limit the emotion analysis to a simple clas-
sification and do not explore the underlying in-
formation regarding emotions. Most theories 
conclude that emotions are often invoked by the 
perception of external events. An effective emo-
tion recognition model should thus take this into 
account.  
To the best of our knowledge, little research 
has been done with respect to emotion cause 
detection. Lee et al (2010a) first investigate the 
interactions between emotions and the corre-
sponding causes from a linguistic perspective. 
They annotate a small-scale emotion cause cor-
pus, and identify six groups of linguistic cues 
facilitating emotion cause detection. Based on 
these findings, they develop a rule-based system 
for automatic emotion cause detection (Lee et 
al., 2010b).  
Emotion cause detection can be considered as 
a kind of causal relation detection, which has 
been intensively studied for years. Most previ-
ous cause detection studies focus on a specific 
domain, such as aviation (Persing and Ng, 2009) 
and finance (Low, et al, 2001). Few works 
(Marcu and Echihabi, 2002; Girju, 2003; Chang 
and Choi, 2005) examine causal relation for 
open domains. 
In recognizing causal relations, most existing 
systems involve two steps: 1) cause candidate 
identification; 2) causal relation detection. To 
simplify the task, most systems omit the step of 
identifying cause candidates. Instead, they often 
predefine or filter out possible causes based on 
domain knowledge, e.g., 14 kinds of cause types 
are identified for aviation incidents (Persing and 
Ng, 2009). For events without specific domain 
information, open-domain systems choose to 
limit their cause candidate. For example, the 
cause-effect pairs are limited to two noun 
phrases (Chang and Choi, 2005; Girju, 2003), or 
two clauses connected with fixed conjunction 
words (Marcu and Echihabi, 2002). 
Given pairs of cause-effect candidates, causal 
relation detection is considered as a binary clas-
sification problem, i.e. ?causal? vs. ?non-
causal?. In general, there are two kinds of in-
formation extracted to identify the causal rela-
tion. One is patterns or constructions expressing 
a cause-effect relation (Chang and Choi, 2005; 
Girju, 2003), and the other is semantic informa-
tion underlying in a text (Marcu and Echihabi, 
2002; Persing and Ng, 2009), such as word pair 
probability. Undoubtedly, the two kinds of in-
formation usually interact with each other in a 
real cause detection system. 
In the literature, the three common classifica-
tion methods, i.e. unsupervised, semi-supervised, 
and supervised, have all been used for cause 
detection systems. Marcu and Echihabi (2002) 
first collected a cause corpus using an unsuper-
vised approach with the help of several conjunc-
tion words, such as ?because? and ?thus?, and 
determined the causal relation for a clause pair 
using the word pair probability. Chang and Choi 
(2005) used a semi-supervised method to recur-
sively learn lexical patterns for cause recogni-
tion based on syntactic trees. Bethard and Mar-
tin (2008) put various causal information in a 
180
supervised classifier, such as the temporal in-
formation and syntactic information.  
For our emotion cause detection, several 
practical issues need to be investigated and re-
solved. First, for the identification of cause can-
didates, we need to define a reasonable span of 
a cause. Based on our data analysis, we find that 
emotion causes often appear across phrases or 
even clauses. Second, although in emotion 
cause detection the effect is fixed, the cause is 
open-domain. We also notice that besides the 
common patterns, emotion causes have their 
own expression patterns. An effective emotion 
cause detection system should take them into 
account. 
3 Corpus Analysis  
In this section, we briefly introduce the Chinese 
emotion cause corpus (Lee et al, 2010a), and 
discuss emotion cause distribution. 
3.1 Emotion Cause corpus 
Lee at al. (2010a) made the first attempt to ex-
plore the correlation between emotions and 
causes, and annotate a Chinese emotion cause 
corpus. The emotion cause corpus focuses on 
five primary emotions, namely ?happiness?, 
?sadness?, ?fear?, ?anger?, and ?surprise?. The 
emotions are explicitly expressed by emotion 
keywords, e.g., gao1xing4 ?happy?, shang1xin1 
?sad?, etc. The corpus is created as follows. 
1. 6,058 entries of Chinese sentences are ex-
tracted from the Academia Sinica Balanced 
Corpus of Mandarin Chinese (Sinica Cor-
pus) with the pattern-match method as well 
as the list of 91 Chinese primary emotion 
keywords (Chen et al, 2009). Each entry 
contains the focus sentence with the emo-
tion keyword ?<FocusSentence>? plus the 
sentence before ?<PrefixSentence>? and 
after ?<SuffixSentence>? it. For each entry, 
the emotion keywords are indexed since 
more than one emotion may be presented in 
an entry;  
2. Some preprocessing, such as balancing the 
number of entry among emotions, is done 
to remove some entries. Finally, 5,629 en-
tries remain; 
3. Each emotion keyword is annotated with 
its corresponding causes if existing. An 
emotion keyword can sometimes be associ-
ated with more than one cause, in such a 
case, both causes are marked. Moreover, 
the cause type is also identified, which is 
either a nominal event or a verbal event (a 
verb or a nominalization).  
Lee at al. (2010a) notice that 72% of the ex-
tracted entries express emotions, and 80% of the 
emotional entries have a cause. 
3.2 The Analysis of Emotion Causes 
To have a deeper understanding of emotion 
cause detection, we take a closer look at the 
emotion cause distribution, including the distri-
bution of emotion cause occurrence and the dis-
tribution of emotion cause text. 
 
The occurrence of emotion causes: According 
to most emotion theories, an emotion is gener-
ally invoked by an external event. The corpus 
shows that, however, 20% of the emotional en-
tries have no cause. Entries without causes ex-
plicitly expressed are mainly due to the follow-
ing reasons: 
i) There is not enough contextual information, 
for instance the previous or the suffix sentence 
is interjections, e.g., en heng ?aha?;  
ii) When the focus sentence is the beginning 
or the ending of a paragraph, no prefix sentence 
or suffix sentence can be extracted as the con-
text. In this case, the cause may be beyond the 
context;  
iii) The cause is obscure, which can be very 
abstract or even unknown reasons.  
 
The emotion cause text: A cause is considered 
as a proposition. It is generally assumed that a 
proposition has a verb which optionally takes a 
noun occurring before it as the subject and a 
noun after it as the object. However, a cause can 
also be expressed as a nominal. In other words, 
both the predicate and the two arguments are 
optional provided that at least one of them is 
present. Thus, the fundamental issue in design-
ing a cause detection system is the definition of 
the span of a cause text. As mentioned, most 
previous studies on causal relations choose to 
ignore the identification of cause candidates. In 
this paper, we first analyze the distribution of 
cause text and then determine the cause candi-
dates for an emotion. 
Based on the emotion cause corpus, we find 
that emotion causes are more likely to be ex-
181
pressed by verbal events than nominal events 
(85% vs. 15%). Although a nominalization (a 
kind of verbal events) is usually a noun phrase, 
a proposition containing a verb plays a salient 
role in the expressions of emotion causes, and 
thus a cause candidate are more likely to be a 
clause-based unit. 
In addition, the actual cause can sometimes 
be too long and complicated, which involves 
several events. In order to explore the span of a 
cause text, we do the following analysis. 
 
Table 1: The clause distribution of cause texts 
Position Cause (%) Position Cause (%) 
Left_0 12.90 Right _0 15.54 
Left_1 31.37 Right _1  9.55 
Left_2 13.31 Right_n  
(n>1) 
9.18 
Left_n 
(n>2) 
10.15   
Total  67.73  32.27 
 
Table 2: The multi-clause distribution of cause 
text 
Same clause % Cross-clauses % 
Left_0 16.80 Left_2_1_0 0.25 
Left_1 31.82 Left_2_1 10.84 
Left_2 7.33 Left_1_0 0.62 
Right _0 18.97 Right_0_1 2.55 
Right _1  10.59   
Total 85.75  14.25 
 
Firstly, for each emotion keyword, an entry is 
segmented into clauses with four punctuations 
(i.e. commas, periods, question marks and ex-
clamation marks), and thus an entry becomes a 
list of cause candidates. For example, when an 
entry has four clauses, its corresponding list of 
cause candidates contains five text units, i.e. 
<left_2, left_1, left_0, right_0, right_1>. If we 
assume the clause where emotion keyword lo-
cates is a focus clause, ?left_2? and ?left_1? are 
previous two clauses, and ?right_1? is the fol-
lowing one. ?left_0? and ?right_0? are the partial 
texts of the focus clause, which locate in the left 
side of and the right side of the emotion key-
word, respectively. Moreover, a cause candidate 
must contain either a noun or a verb because a 
cause is either a verbal event or a nominal event; 
otherwise, it will be removed from the list. 
Secondly, we calculate whether a cause can-
didate overlaps with the real cause, as shown in 
Table 1. We find that emotion causes are more 
likely to occur in the left of emotion keyword. 
This observation is consistent with the fact that 
an emotion is often trigged by an external hap-
pened event. Thirdly, for all causes occurring 
between ?left_2? and ?right_1?, we calculate 
whether a cause occurs across clauses, as in Ta-
ble 2. We observe that most causes locate 
within the same clause of the representation of 
the emotion (85.57%). This suggests that a 
clause may be the most appropriate unit to de-
tect a cause. 
 
4 Emotion Cause Detection Based on 
Multi-label Classification 
A cause detection system is to identify the caus-
al relation between a pair of two text units. For 
emotion cause detection, one of the two text 
units is fixed (i.e. the emotion keyword), and 
therefore the remaining two unresolved issues 
are the identification of the other text unit and 
the causal relation. 
From the above data analysis, there are two 
observations. First, most emotion causes are 
verbal events, which are often expressed by a 
proposition (or a clause). Thus, we define an-
other text unit as a clause, namely a cause can-
didate. Second, as most emotion causes occur 
between ?left_2? and ?right_1? (~80%), we de-
fine the cause candidates for an emotion as 
<left_2, left_1, left_0, right_0, right_1>.  
Differing from the existing cause systems, we 
formalize emotion cause detection as a multi-
label problem. In other words, given an emotion 
keyword and its context, its label is the loca-
tions of its causes, such as ?left_1, left_0?. This 
multi-label-based formalization of the cause 
detection task has two advantages. First, it is an 
integrated system detecting causes for an emo-
tion from the contextual information. In most 
previous cause detection systems, a causal rela-
tion is identified based on the information be-
tween two small text units, i.e. a pair of clauses 
or noun phrases, and therefore it is often the 
case that long-distance information is missed. 
Second, the multi-label-based tagging is able to 
182
capture the relationship between two cause can-
didates. For example, ?left_2? and ?left_1? are 
often combined as a complicated event as a 
cause.   
As a multi-label classification task, every 
multi-label classifier is applicable. In this study, 
we use a simple strategy: we treat each possible 
combination of labels appearing in the training 
data as a unique label. Note that an emotion 
without causes is labeled as ?None?. This con-
verts multi-label classification to single-label 
classification, which is suitable for any multi-
class classification technologies. In particular, 
we choose a Max Entropy tool, Mallet1, to per-
form the classification.  
5 Linguistic Features  
As explained, there are basically two kinds of 
features for cause detection, namely pattern-
based features and semantic-based features. In 
this study, we develop two sets of patterns 
based on linguistic analysis: one is a set of ma-
nually generalized patterns, and the other con-
tains automatically generalized patterns. All of 
these patterns explore causal constructions ei-
ther for general causal relations or for specific 
emotion cause relations. 
5.1 Linguistic Cues  
Based on the linguistic analysis, Lee et al 
(2010a) identify six groups of linguistic cue 
words that are highly collocated with emotion 
causes, as shown in Table 3. Each group of the 
linguistic cues serves as an indicator marking 
the causes in different emotional constructions. 
In this paper, these groups of linguistic cues are 
reinterpreted from the computational perspec-
tive, and are used to develop pattern-based fea-
tures for the emotion cause detection system.  
 
Table 3:  Linguistic cue words for emotion 
cause detection (Lee et al 2010a) 
Group Cue Words 
I: 
Prepositions 
?for? as in ?I will do this for you?: wei4, 
wei4le 
?for? as in ?He is too old for the job?: 
dui4, dui4yu2 
?as?: yi3 
                                                 
1
 http://mallet.cs.umass.edu/ 
II: 
Conjunctions 
?because?: yin1, yin1wei4, you2yu2 
?so?: yu1shi4, suo3yi3, yin1er2 
?but?: ke3shi4 
III:  
Light Verbs ?to make?: rang4, ling4, shi3 
IV: 
Reported 
Verbs 
?to think about?: xiang3dao4, 
xiang3qi3, yi1xiang3, xiang3 lai2 
?to talk about?: shuo1dao4, shuo1qi3, 
yi1shuo1, jiang3dao4, jiang3qi3, 
yi1jiang3, tan2dao4, tan2qi3, yi1tan2, 
ti2dao4, ti2qi3, yi1ti2 
V: 
Epistemic 
Markers 
?to hear?: ting1, ting1dao4, ting1shuo1 
?to see?: kan4, kan4dao4, kan4jian4, 
jian4dao4, jian4, yan3kan4, qiao2jian4 
?to know?: zhi1dao4, de2zhi1, de2xi1, 
huo4zhi1, huo4xi1, fa1xian4, fa1jue2 
?to exist?: you3 
VI: 
Others 
?is?: deshi4 
?say?: deshuo1 
?at?: yu2 
?can?: neng2  
 
For emotion cause processing, Group I and II 
contain cues which are for general cause detec-
tion, and while Group III, IV and V include 
cues specifically for emotion cause detection. 
Group VI includes other linguistic cues that do 
not fall into any of the five groups.  
Group I covers some prepositions which all 
roughly mean ?for?, and Group II contains the 
conjunctions that explicitly mark the emotion 
cause. Group I is expected to capture the prepo-
sitions constructions in the focus clause where 
the emotion keyword locates. Group II tends to 
capture the rhetorical relation expressed by con-
junction words so as to infer causal relation 
among multi-clauses. These two groups are typ-
ical features for general cause detection. 
Group III includes three common light verbs 
which correspond to the English equivalents ?to 
make? or ?to cause?. Although these light verbs 
themselves do not convey any concrete meaning, 
they are often associated with several construc-
tions to express emotions and at the same time 
indicate the position of emotion causes. For ex-
ample, ?The birthday party made her happy?.  
One apparent difference between emotion 
causes and general causes is that emotions are 
often triggered by human activities or the per-
ception of such activities, e.g., ?glad to say? or 
?glad to hear?. Those human activities are often 
strong indicators for the location of emotion 
183
causes. Group IV and V are used to capture this 
kind of information. Group IV is a list of verbs 
of thinking and talking, and Group V includes 
four types of epistemic markers which are usu-
ally verbs marking the cognitive awareness of 
emotions in the complement position. The epis-
temic markers include verbs of seeing, hearing, 
knowing, and existing. 
  
5.2 Linguistic Patterns  
With the six groups of linguistic cues, we gen-
eralize 14 rules used in Lee et al (2010b) to 
locate the clause positions of an emotion cause, 
as shown in Table 4. The abbreviations used in 
the rules are given as follows:  
 
C = Cause 
K = Emotion keyword 
B = Clauses before the focus clause 
F = Focus clause/the clause containing the emotion 
verb 
A = Clauses after the focus clause 
 
Table 4: Linguistic rules for emotion cause de-
tection (Lee et al 2010b) 
No. Rules 
1 i) C(B/F) + III(F)  + K(F)  
ii) C = the nearest N/V before I in F/B 
2 i)  IV/V/I/II(B/F) + C(B/F) + K(F)  
ii) C = the nearest N/V before K in F 
3 i) I/II/IV/V (B) + C(B)  + K(F)  
ii) C = the nearest N/V after I/II/IV/V in B 
4 i) K(F) + V/VI(F) + C(F/A)  
ii) C = the nearest N/V after V/VI in F/A 
5 i) K(F)+II(A)+C(A)  
ii) C = the nearest N/V after II in A 
6 i) III(F) + K(F) + C(F/A)  
ii) C = the nearest N/V after K in F or A 
7 i) yue4 C yue4 K ?the more C the more K? (F)   
ii) C = the V in between the two yue4?s in F 
8 i) K(F) + C(F)  
ii) C = the nearest N/V after K in F 
9 i) V(F) + K(F)  
ii) C = V+(an aspectual marker) in F 
10 i) K(F)  + de ?possession?(F) + C(F)  
ii) C = the nearest N/V +?+N after de in F 
12 i) K(B) + IV (B) + C(F)   
ii) C = the nearest N/V after IV in F 
13 i) IV(B) + C(B) + K(F)  
ii) C = the nearest N/V after IV in B 
14 i) C(B) +  K(F)  
ii) C = the nearest N/V before K in B  
 
For illustration, an example of the rule descrip-
tion is given in Rule 1. 
Rule 1: 
i) C(B/F) + III(F) + K(F)  
ii) C = the nearest N/V before III in F/B  
 
Rule 1 indicates that the cause (C) comes before 
Group III cue words. Theoretically, in identify-
ing C, we look for the nearest verb/noun occur-
ring before Group III cue words in the focus 
clause (F) or the clauses before the focus clause 
(B), and consider the clause containing this 
verb/noun as a cause. Practically, for each cause 
candidate, i.e. ?left_1?, if it contains this 
verb/noun, we create a feature with 
?left_1_rule_1=1?. 
5.3 Generalized Patterns  
Rule-based patterns usually achieve a rather 
high accuracy, but suffer from low coverage. To 
avoid this shortcoming, we extract a generalized 
feature automatically according to the rules in 
Table 4. The features are able to detect two 
kinds of constructions, namely functional con-
structions, i.e. rhetorical constructions, and spe-
cific constructions for emotion causes.  
Local functional constructions: a cause occur-
ring in the focus clause is often expressed with 
certain functional words, such as ?because of?, 
?due to?. In order to capture the various expres-
sions of these functional constructions, we iden-
tify all functional words around the given emo-
tion keyword. For an emotion keyword, we 
search ?left_0? from the right until a noun or a 
verb is found. Next, all unigrams and bigrams 
between the noun or the verb and the emotion 
keyword are extracted. The same applies to 
?right_0?. 
Long-distance conjunction constructions: 
Group II enumerates only some typical conjunc-
tion words. To capture more general rhetorical 
relations, according to the given POS tags, the 
conjunction word is extracted for each cause 
candidate, if it occurs at the beginning of the 
candidate. 
Generalized action and epistemic verbs: 
Group IV and V cover only partial action and 
epistemic verbs. To capture possible related ex-
pressions, we take the advantage of Chinese 
characters. In Chinese, each character itself usu-
ally has a meaning and some characters have a 
strong capability to create words with extended 
meaning. For example, the character ?ting1-
listen? combines with other characters to create 
184
words expressing ?listening?, such as ting1jian4, 
ting1wen5. With the selected characters regard-
ing reported verbs and epistemic markers, each 
cause candidate is checked to see whether it 
contains the predefined characters.  
6 Experiments 
For the emotion cause corpus, we reserve 80% 
as the training data, 10% as the development 
data, and 10% as the test data. During evalua-
tion, we first convert the multi-label tag output-
ted from our system into a binary tag (?Y? 
means the presence of a causal relation; ?N? in-
dicates the absence of a causal relation) between 
the emotion keyword and each candidate in its 
corresponding cause candidates. Thus, the 
evaluation scores for binary classification based 
on three common measures, i.e. precision, recall 
and F-score, are chosen. 
6.1 Linguistic Feature Analysis 
According to the distribution in Table 1, we de-
sign a naive baseline to allow feature analysis. 
The baseline searches for the cause candidates 
in the order of <left_1, right_0, left_2, left_0, 
right_1>. If the candidate contains a noun or 
verb, consider this clause as a cause and stop. 
We run the multi-label system with different 
groups of features and the performances are 
shown in Table 5. The feature set begins with 
linguistic patterns (LP), and is then incorporated 
with local functional constructions (LFC), long-
distance conjunction constructions (LCC), and 
generalized action and epistemic verbs (GAE), 
one by one. Since the ?N? tag is overwhelming, 
we report only the Mac average scores for both 
?Y? and ?N? tags.  
In Table 5, we first notice that the perform-
ances achieve significant improvement from the 
baseline to the final system (~17%). This indi-
cates that our linguistic features are effective for 
emotion cause detection. In addition, we ob-
serve that LP and LFC are the best two effective 
features, whereas LCC and GAE have slight 
contributions. This shows that our feature ex-
traction has a strong capability to detect local 
causal constructions, and is yet unable to detect 
the long-distance or semantic causal informa-
tion. Here, ?local? refers to the information in 
the focus clause. We also find that incorporating 
LFC, which is a pure local feature, generally 
improves the performances of all cause candi-
dates, i.e. ~5% improvement for ?left_1?. This 
indicates that our multi-label integrated system 
is able to convey information among cause can-
didates.  
 
Table 5: The overall performance with different 
feature sets of the multi-label system 
 Precision Recall F-score 
Baseline 56.64 57.70 56.96 
LP 74.92 66.70 69.21 
+ LFC 72.80 71.94 72.35 
+ LCC 73.60 72.50 73.02 
+ GAE 73.90 72.70 73.26 
 
Table 6: The separate performances for ?Y? and 
?N? tags of the multi-label system 
 ?Y? ?N? 
Baseline 33.06 80.85 
LP 48.32 90.11 
+ LFC 55.45 89.24 
+ LCC 56.48 89.57 
+ GPE 56.84 89.68 
 
Table 6 shows the performances (F-scores) 
for ?Y? and ?N? tags separately. First, we notice 
that the performances of the ?N? tag are much 
better than the ones of ?Y? tag. Second, it is sur-
prising that incorporating the linguistic features 
significantly improves only the ?Y? tag (from 
33% to 56%), but does not affect ?N? tag. This 
suggests that our linguistic features are effective 
to detect the presence of causal relation, and yet 
do not hurt the detection of ?non_causal? rela-
tion. For the ?Y? tag, the features LP and LFC 
achieve ~15% and ~7% improvements respec-
tively. LCC and GPE, on the other hand, show 
slight improvements only. 
Finally, Table 7 shows the detailed perform-
ances of our multi-label system with all features. 
The last row shows the overall performances of 
?Y? and ?N? tags. For the ?Y? tag, the closer the 
cause candidates are to the emotion keyword, 
the better performances the system achieves. 
This proves that the features we propose effec-
tively detect local emotion causes, more effort, 
185
Table 7: The detailed performance for the multi-label system including all features 
?Y? tag Precision Recall F-score ?N? tag Precision Recall F-score 
Left_0 68.92 68.92 68.92 Left_0 93.72 93.72 93.72 
Left_1 57.63 63.35 60.36 Left_1 82.90 79.22 81.02 
Left_2 29.27 20.69 24.24 Left_2 89.23 92.93 91.04 
Right_0 67.78 64.89 66.30 Right_0 82.63 84.41 83.51 
Right_1 54.84 30.91 39.54 Right_1 92.00 96.90 94.38 
Total 58.84 54.98 56.84 Total 88.96 90.42 89.68 
 
Table 8: The detailed performance for the single-label system including all features 
?Y? tag Precision Recall F-score ?N? tag Precision Recall F-score 
Left_0 65.39  68.92 67.11 Left_0 93.65  92.62 93.13 
Left_1 61.19  50.93 55.59 Left_1 79.64   85.60 82.51 
Left_2 28.57   20.69 24.00 Left_2 89.20   92.68 90.91 
Right_0 70.13   57.45 63.16 Right_0 80.30  87.63 83.81 
Right_1 33.33   40.00 36.36 Right_1 92.50   90.24 91.36 
Total 55.67   50.00 52.68 Total 87.85  90.08 88.95 
 
however, should be put on the detection of 
long-distance causes. In addition, we find that 
the detection of long-distance causes usually 
relies on two kinds of information for inference: 
rhetorical relation and deep semantic informa-
tion. 
6.2 Modeling Analysis 
To compare our multi-label model with single-
label models, we create a single-label system as 
follows. The single-label model is a binary 
classification for a pair comprising the emotion 
keyword and a candidate in its corresponding 
cause candidates. For each pair, all linguistic 
features are extracted only from the focus 
clause and its corresponding cause candidate. 
Note that we only use the features in the focus 
clause for ?left_0? and ?right_0?. The perform-
ances are shown in Table 8. 
Comparing Tables 7 and 8, all F-scores of 
the ?Y? tag increase and the performances of 
the ?N? tag remain almost the same for both the 
single-label model and our multi-label model. 
We also find that the multi-label model takes 
more advantage of local information, and im-
proves the performances, particularly for 
?left_1?.  
To take an in-depth analysis of the cause de-
tection capability of the multi-label model, an 
evaluation is designed that the label is treated 
as a tag from the multi-label classifier. Due to 
the tag sparseness problem (as in Table 2), only 
the ?left_2, left_1? tag is detected in the test 
data, and its performance is 21% precision, 
26% recall and 23% F-score. Furthermore, we 
notice that ~18% of the ?left_1? tags are de-
tected through this combination tag. This 
shows that some causes need to take into ac-
count the mutual information between clauses. 
Although the scores are low, it still shows that 
our multi-label model provides an effective 
way of detecting some of the multi-clauses 
causes. 
7 Conclusion 
We treat emotion cause detection as a multi-
label task, and develop two sets of linguistic 
features for emotion cause detection based on 
linguistic cues. The experiments on the small-
scale corpus show that both the multi-label 
model and the linguistic features are able to 
effectively detect emotion causes. The auto-
matic detection of emotion cause will in turn 
allow us to extract directly relevant information 
for public opinion mining and event prediction. 
It can also be used to improve emotion detec-
tion and classification. In the future, we will 
attempt to improve our system from two as-
pects. On the one hand, we will explore more 
powerful multi-label classification models for 
our system. On the other hand, we will investi-
gate more linguistic patterns or semantic in-
formation to further help emotion cause detec-
tion. 
186
References 
Abbasi, A., H. Chen, S. Thoms, and T. Fu. 2008. 
Affect Analysis of Web Forums and Blogs using 
Correlation Ensembles?. In IEEE Tran. Knowl-
edge and Data Engineering, vol. 20(9), pp. 1168-
1180. 
Bethard, S. and J. Martin. 2008. Learning Semantic 
Links from a Corpus of Parallel Temporal and 
Causal Relations. In Proceedings of ACL. 
Descartes, R. 1649. The Passions of the Soul. In J. 
Cottingham et al (Eds), The Philosophical Writ-
ings of Descartes. Vol. 1: 325-404. 
Chang, D.-S. and K.-S. Choi. 2006. Incremental cue 
phrase learning and bootstrapping method for 
causality extraction using cue phrase and word 
pair probabilities. Information Processing and 
Management. 42(3): 662-678. 
Chen, Y., S. Y. M. Lee and C.-R. Huang. 2009. Are 
Emotions Enumerable or Decomposable? And 
Its Implications for Emotion Processing. In Pro-
ceedings of the 23rd Pacific Asia Conference on 
Language, Information and Computation. 
Girju, R. 2003. Automatic Detection of Causal Re-
lations for Question Answering. In the 41st An-
nual Meeting of the Association for Computa-
tional Linguistics, Workshop on Multilingual 
Summarization and Question Answering - Ma-
chine Learning and Beyond, Sapporo, Japan. 
James, W. 1884. What is an Emotion? Mind, 
9(34):188?205. 
Lee, S. Y. M., Y. Chen and C.-R. Huang. 2010a. A 
Text-driven Rule-based System for Emotion 
Cause Detection. In Proceedings of NAACL-HLT 
2010 Workshop on Computational Approaches to 
Analysis and Generation of Emotion in Text. 
Lee, S. Y. M., Y. Chen, S. Li and C.-R. Huang. 
2010b. Emotion Cause Events: Corpus Construc-
tion and Analysis. In Proceedings of LREC 2010. 
Low, B. T., K. Chan , L. L. Choi , M. Y. Chin , S. L. 
Lay. 2001. Semantic Expectation-Based Causa-
tion Knowledge Extraction: A Study on Hong 
Kong Stock Movement Analysis, In Proceedings 
of the 5th Pacific-Asia Conference on Knowledge 
Discovery and Data Mining, p.114-123, April 
16-18.  
Marcu, D., and A. Echihabi. 2002. An Unsupervised 
Approach to Recognizing Discourse Relations. In 
Proceedings of ACL. 
Mihalcea, R. and H. Liu. 2006. A Corpus-based 
Approach to Finding Happiness. In Proceedings 
of the AAAI Spring Symposium on Computational 
Approaches to Weblogs.  
Persing, I. and V. Ng. 2009. Semi-Supervised Cause 
Identification from Aviation Safety Reports. In 
Proceedings of ACL. 
Plutchik, R. 1980. Emotions: A Psychoevolutionary 
Synthesis. New York: Harper & Row. 
Strapparava, C. and R. Mihalcea. 2008. Learning to 
Identify Emotions in Text. In Proceedings of the 
ACM Conference on Applied Computing ACM-
SAC. 
Tokuhisa, R., K. Inui, and Y. Matsumoto. 2008. 
Emotion recognition Using Massive Examples 
Extracted from the Web. In Proceedings of COL-
ING. 
Wierzbicka, A. 1999. Emotions across Languages 
and Cultures: Diversity and Universals. Cam-
bridge: Cambridge University Press. 
 
 
 
187
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 635?643,
Beijing, August 2010
Sentiment Classification and Polarity Shifting 
 
Shoushan Li??  Sophia Yat Mei Lee?  Ying Chen?  Chu-Ren Huang?  Guodong Zhou?  
 
?Department of CBS 
The Hong Kong Polytechnic University 
{shoushan.li, sophiaym, 
chenying3176, churenhuang} 
@gmail.com 
 
?
 Natural Language Processing Lab 
School of Computer Science and 
Technology 
      Soochow University
gdzhou@suda.edu.cn 
 
  
Abstract 
Polarity shifting marked by various 
linguistic structures has been a challenge 
to automatic sentiment classification. In 
this paper, we propose a machine learning 
approach to incorporate polarity shifting 
information into a document-level 
sentiment classification system. First, a 
feature selection method is adopted to 
automatically generate the training data 
for a binary classifier on polarity shifting 
detection of sentences. Then, by using the 
obtained binary classifier, each document 
in the original polarity classification 
training data is split into two partitions, 
polarity-shifted and polarity-unshifted, 
which are used to train two base 
classifiers respectively for further 
classifier combination. The experimental 
results across four different domains 
demonstrate the effectiveness of our 
approach. 
1 Introduction 
Sentiment classification is a special task of text 
classification whose objective is to classify a text 
according to the sentimental polarities of 
opinions it contains (Pang et al, 2002), e.g., 
favorable or unfavorable, positive or negative. 
This task has received considerable interests in 
the computational linguistic community due to its 
potential applications.  
In the literature, machine learning approaches 
have dominated the research in sentiment 
classification and achieved the state-of-the-art 
performance (e.g., Kennedy and Inkpen, 2006; 
Pang et al, 2002). In a typical machine learning 
approach, a document (text) is modeled as a 
bag-of-words, i.e. a set of content words without 
any word order or syntactic relation information. 
In other words, the underlying assumption is that 
the sentimental orientation of the whole text 
depends on the sum of the sentimental polarities 
of content words. Although this assumption is 
reasonable and has led to initial success, it is 
linguistically unsound since many function 
words and constructions can shift the 
sentimental polarities of a text. For example, in 
the sentence ?The chair is not comfortable?, the 
polarity of the word ?comfortable? is positive 
while the polarity of the whole sentence is 
reversed because of the negation word ?not?. 
Therefore, the overall sentiment of a document is 
not necessarily the sum of the content parts 
(Turney, 2002). This phenomenon is one main 
reason why machine learning approaches fail 
under some circumstances. 
As a typical case of polarity shifting, negation 
has been paid close attention and widely studied 
in the literature (Na et al, 2004; Wilson et al, 
2009; Kennedy and Inkpen, 2006). Generally, 
there are two steps to incorporate negation 
information into a system: negation detection 
and negation classification. For negation 
detection, some negation trigger words, such as 
?no?, ?not?, and ?never?, are usually applied to 
recognize negation phrases or sentences. As for 
negation classification, one way to import 
negation information is to directly reverse the 
polarity of the words which contain negation 
trigger words as far as term-counting approaches 
are considered (Kennedy and Inkpen, 2006). An 
alternative way is to add some negation features 
(e.g., negation bigrams or negation phrases) into 
635
machine learning approaches (Na et al, 2004). 
Such approaches have achieved certain success.  
There are, however, some shortcomings with 
current approaches in incorporating negation 
information. In terms of negation detection, 
firstly, the negation trigger word dictionary is 
either manually constructed or relies on existing 
resources. This leads to certain limitations 
concerning the quality and coverage of the 
dictionary. Secondly, it is difficult to adapt 
negation detection to other languages due to its 
language dependence nature of negation 
constructions and words. Thirdly, apart from 
negation, many other phenomena, e.g., contrast 
transition with trigger words like ?but?, 
?however?, and ?nevertheless?, can shift the 
sentimental polarity of a phrase or sentence. 
Therefore, considering negation alone is 
inadequate to deal with the polarity shifting 
problem, especially for document-level 
sentiment classification. 
In terms of negation classification, although it 
is easy for term-counting approaches to integrate 
negation information, they rarely outperform a 
machine learning baseline (Kennedy and Inkpen, 
2006). Even for machine learning approaches, 
although negation information is sometimes 
effective for local cases (e.g., not good), it fails 
on long-distance cases (e.g., I don?t think it is 
good). 
In this paper, we first propose a feature 
selection method to automatically generate a 
large scale polarity shifting training data for 
polarity shifting detection of sentences. Then, a 
classifier combination method is presented for 
incorporating polarity shifting information. 
Compared with previous ones, our approach 
highlights the following advantages?First of all, 
we apply a binary classifier to detect polarity 
shifting rather than merely relying on trigger 
words or phrases. This enables our approach to 
handle different kinds of polarity shifting 
phenomena. More importantly, a feature 
selection method is presented to automatically 
generate the labeled training data for polarity 
shifting detection of sentences. 
The remainder of this paper is organized as 
follows. Section 2 introduces the related work of 
sentiment classification. Section 3 presents our 
approach in details. Experimental results are 
presented and analyzed in Section 4. Finally, 
Section 5 draws the conclusion and outlines the 
future work. 
2 Related Work 
Generally, sentiment classification can be 
performed at four different levels: word level 
(Wiebe, 2000), phrase level (Wilson et al, 2009), 
sentence level (Kim and Hovy, 2004; Liu et al, 
2005), and document level (Turney, 2002; Pang 
et al, 2002; Pang and Lee, 2004; Riloff et al, 
2006). This paper focuses on document-level 
sentiment classification. 
In the literature, there are mainly two kinds of 
approaches on document-level sentiment 
classification: term-counting approaches 
(lexicon-based) and machine learning 
approaches (corpus-based). Term-counting 
approaches usually involve deriving a sentiment 
measure by calculating the total number of 
negative and positive terms (Turney, 2002; Kim 
and Hovy, 2004; Kennedy and Inkpen, 2006). 
Machine learning approaches recast the 
sentiment classification problem as a statistical 
classification task (Pang and Lee, 2004). 
Compared to term-counting approaches, 
machine learning approaches usually achieve 
much better performance (Pang et al, 2002; 
Kennedy and Inkpen, 2006), and have been 
adopted to more complicated scenarios, such as 
domain adaptation (Blitzer et al, 2007), 
multi-domain learning (Li and Zong, 2008) and 
semi-supervised learning (Wan, 2009; Dasgupta 
and Ng, 2009) for sentiment classification. 
Polarity shifting plays a crucial role in 
phrase-level, sentence-level, and document-level 
sentiment classification. However, most of 
previous studies merely focus on negation 
shifting (polarity shifting caused by the negation 
structure). As one pioneer research on sentiment 
classification, Pang et al (2002) propose a 
machine learning approach to tackle negation 
shifting by adding the tag ?not? to every word 
between a negation trigger word/phrase (e.g., not, 
isn't, didn't, etc.) and the first punctuation mark 
following the negation trigger word/phrase. To 
their disappointment, considering negation 
shifting has a negligible effect and even slightly 
harms the overall performance. Kennedy and 
Inkpen (2006) explore negation shifting by 
incorporating negation bigrams as additional 
features into machine learning approaches. The 
636
experimental results show that considering 
sentiment shifting greatly improves the 
performance of term-counting approaches but 
only slightly improves the performance of 
machine learning approaches. Other studies such 
as Na et al (2004), Ding et al (2008), and Wilson 
et al (2009) also explore negation shifting and 
achieve some improvements1. Nonetheless, as far 
as machine learning approaches are concerned, 
the improvement is rather insignificant (normally 
less than 1%). More recently, Ikeda et al (2008) 
first propose a machine learning approach to 
detect polarity shifting for sentence-level 
sentiment classification, based on a 
manually-constructed dictionary containing 
thousands of positive and negative sentimental 
words, and then adopt a term-counting approach 
to incorporate polarity shifting information. 
3 Sentiment Classification with Polarity 
Shifting Detection 
 
 
Figure 1: General framework of our approach 
 
The motivation of our approach is to improve the 
performance of sentiment classification by robust 
treatment of sentiment polarity shifting between 
sentences. With the help of a binary classifier, the 
sentences in a document are divided into two 
parts: sentences which contain polarity shifting 
structures and sentences without any polarity 
shifting structure. Figure 1 illustrates the general 
framework of our approach. Note that this 
framework is a general one, that is, different 
polarity shifting detection methods can be applied 
to differentiate polarity-shifted sentences from 
those polarity-unshifted sentences and different 
                                                      
1
 Note that Ding et al (2006) also consider but-clause, another 
important structure for sentiment shifting. Wilson et al (2009) use 
conjunctive and dependency relations among polarity words. 
polarity classification methods can be adopted to 
incorporate sentiment shifting information. For 
clarification, the training data used for polarity 
shifting detection and polarity classification are 
referred to as the polarity shifting training data 
and the polarity classification training data, 
respectively. 
3.1 Polarity Shifting Detection 
In this paper, polarity shifting means that the 
polarity of a sentence is different from the 
polarity expressed by the sum of the content 
words in the sentence. For example, in the 
sentence ?I am not disappointed?, the negation 
structure makes the polarity of the word 
'disappointed' different from that of the whole 
sentence (negative vs. positive). Apart from the 
negation structure, many other linguistic 
structures allow polarity shifting, such as 
contrast transition, modals, and 
pre-suppositional items (Polanyi and Zaenen, 
2006). We refer these structures as polarity 
shifting structures. 
One of the great challenges in building a 
polarity shifting detector lies on the lack of 
relevant training data since manually creating a 
large scale corpus of polarity shifting sentences 
is time-consuming and labor-intensive. Ikeda et 
al. (2008) propose an automatic way for 
collecting the polarity shifting training data 
based on a manually-constructed large-scale 
dictionary. Instead, we adopt a feature selection 
method to build a large scale training corpus of 
polarity shifting sentences, given only the 
already available document-level polarity 
classification training data. With the help of the 
feature selection method, the top-ranked word 
features with strong sentimental polarity 
orientation, e.g., ?great?, ?love?, ?worst? are first 
chosen as the polarity trigger words. Then, those 
sentences with the top-ranked polarity trigger 
words in both categories of positive and negative 
documents are selected. Finally, those candidate 
sentences taking opposite-polarity compared to 
the containing trigger word are deemed as 
polarity-shifted. 
The basic idea of automatically generating the 
polarity shifting training data is based on the 
assumption that the real polarity of a word or 
phrase is decided by the major polarity category 
where the word or phrase appears more often. As 
a result, the sentences in the 
Polarity Shifting 
Detector 
Documents 
 
Polarity-shifted 
Sentences 
Polarity-unshifted 
Sentences 
Polarity Classifier Positive/Negative 
637
frequently-occurring category would be seen as 
polarity-unshifted while the sentences in the 
infrequently-occurring category would be seen 
as polarity-shifted. 
In the literature, various feature selection 
methods, such as Mutual Information (MI), 
Information Gain (IG) and Bi-Normal Separation 
(BNS) (Yang and Pedersen, 1997; Forman 2003), 
have been employed to cope with the problem of 
the high-dimensional feature space which is 
normal in sentiment classification.  
In this paper, we employ the theoretical 
framework, proposed by Li et al (2009), 
including two basic measurements, i.e. frequency 
measurement and ratio measurement, where the 
first measures, the document frequency of a term 
in one category, and the second measures, the 
ratio between the document frequency in one 
category and other categories. In particular, a 
novel method called Weighed Frequency and 
Odds (WFO) is proposed to incorporate both 
basic measurements: 
1( | )( , ) ( | ) {max(0, log )}( | )
i
i i
i
P t cWFO t c P t c
P t c
? ??
=  
where ( | )iP t c  denotes the probability that a 
document x contains the term t with the 
condition that x belongs to category ic ; 
( | )iP t c  denotes the probability that a document 
x contains the term t with the condition that x 
does not belong to category ic . The left part of 
the formula ( | )iP t c  implies the first basic 
measurement and the right part 
log( ( | ) / ( | ))i iP t c P t c  implies the second one. 
The parameter ?  0 1?? ?? ?is thus to tune the 
weight between the two basic measurements. 
Especially, when ?  equals 0, the WFO method 
fades to the MI method which fully prefers the 
second basic measurement. 
Figure 2 illustrates our algorithm for 
automatically generating the polarity shifting 
training data where 1c and 2c denote the two 
sentimental orientation categories, i.e. negative 
and positive. Step A segments a document into 
sentences with punctuations. Besides, two 
special words, ?but? and ?and?, are used to 
further segment some contrast transition 
structures and compound sentences. Step B 
employs the WFO method to rank all features 
including the words. Step D extracts those 
polarity-shifted and polarity-unshifted sentences 
containing top it ?  where maxN denotes the 
upper-limit number of sentences in each 
category of the polarity shifting training data and 
#(x) denotes the total number of the elements in 
x. Apart from that, the first word in the following 
sentence is also included to capture a common 
kind of long-distance polarity shifting structure: 
contrast transition. Thus, important trigger words 
like ?however? and ?but? may be considered. 
Finally, Step E guarantees the balance between 
the two categories of the polarity shifting 
training data. 
Given the polarity shifting training data, we 
apply SVM classification algorithm to train a 
polarity-shifting detector with word unigram 
features. 
Input: 
The polarity classification training data: the negative 
sentimental document set 
1c
D and the positive sentimental 
document set
 2c
D . 
Output: 
    The polarity shifting training data: the 
polarity-unshifted sentence set unshiftS  and the polarity- 
shifted sentence set
 
shiftS . 
Procedure: 
A. Segment documents 
1c
D  and  
2c
D  to single 
sentences  
1c
S  and  
2c
S . 
B. Apply feature selection on the polarity classification  
training data and get the ranked features, 
1( ,..., ,..., )top top i top Nt t t? ? ?  
C. shiftS  = {}, unshiftS  = {} 
D. For  top it ?  in  1( ,..., ,..., )top top i top Nt t t? ? ? : 
D1) if #( shiftS )> maxN : break 
D2) Collect all sentences  
1,top i c
S
?
 and  
2,top i c
S
?
 
which contain  top it ?  from  1cS  and  2cS  
respectively 
D3)  if #(
1,top i c
S
?
)>#(
2,top i c
S
?
): 
put  
2,top i c
S
?
 into  shiftS  
put  
1,top i c
S
?
 into  unshiftS  
else: 
put  
1,top i c
S
?
 into  shiftS  
put  
2,top i c
S
?
 into  unshiftS  
E. Randomly select 
maxN sentences from unshiftS as the 
output of 
unshiftS  
 
Figure 2: The algorithm for automatically 
generating the polarity shifting training data 
 
638
3.2 Polarity Classification with Classifier 
Combination  
After polarity shifting detection, each document 
in the polarity classification training data is 
divided into two parts, one containing 
polarity-shifted sentences and the other 
containing polarity-unshifted sentences, which 
are used to form the polarity-shifted training data 
and the polarity-unshifted training data. In this 
way, two different polarity classifiers, If  and 
2f , can be trained on the polarity-shifted 
training data and the polarity-unshifted training 
data respectively. Along with classifier 3f , 
trained on all original polarity classification 
training data, we now have three base classifiers 
in hand for possible classifier combination via a 
multiple classifier system. 
The key issue in constructing a multiple 
classifier system (MCS) is to find a suitable way 
to combine the outputs of the base classifiers. In 
MCS literature, various methods are available 
for combining the outputs, such as fixed rules 
including the voting rule, the product rule and 
the sum rule (Kittler et al, 1998) and trained 
rules including the weighted sum rule (Fumera 
and Roli, 2005) and the meta-learning 
approaches (Vilalta and Drissi, 2002). In this 
study, we employ the product rule, a popular 
fixed rule, and stacking (D?eroski and ?enko, 
2004), a well-known trained rule, to combine the 
outputs. 
Formally, each base classifier provides some 
kind of confidence measurements, e.g., posterior 
probabilities of the test sample belonging to each 
class. Formally, each base classifier 
 ( 1,2,3)lf l =  assigns a test sample (denoted as 
lx ) a posterior probability vector ( )lP x

:  
1 2( ) ( | ), ( | ))tl l lP x p c x p c x= (

 
where 1( | )lp c x  denotes the probability that the 
-thl base classifier considers the sample 
belonging 1c . 
The product rule combines the base classifiers 
by multiplying the posterior possibilities and 
using the multiplied possibility for decision, i.e. 
3
1
      arg max ( | )j i l
i l
assign y c when j p c x
=
? = ?  
Stacking belongs to well-known 
meta-learning (Vilalta and Drissi, 2002). The 
key idea behind meta-learning is to train a 
meta-classifier with input attributes that are the 
outputs of the base classifiers. Hence, 
meta-learning usually needs some development 
data for generating the meta-training data. Let 
'x  denote a feature vector of a sample from the 
development data. The output of the -thl base 
classifier lf on this sample is the probability 
distribution over the category set 1 2{ , }c c , i.e. 
1 2( ' ) ( ( | ' ), ( | ' ))l l l lP x p c x p c x=

 
A meta-classifier can be trained using the 
development data with the meta-level feature 
vector 2 3metax R ??  
1 2 3( ( ' ), ( ' ), ( ' ))meta l l lx P x P x P x= = ==
  
 
Stacking is a specific meta-learning rule, in 
which a leave-one-out or a cross-validation 
procedure on the training data is applied to 
generate the meta-training data instead of using 
extra development data. In our experiments, we 
perform stacking with 10-fold cross-validation to 
generate the meta-training data. 
4 Experimentation 
4.1 Experimental Setting 
The experiments are carried out on product 
reviews from four domains: books, DVDs, 
electronics, and kitchen appliances (Blitzer et al, 
2007)2. Each domain contains 1000 positive and 
1000 negative reviews. 
For sentiment classification, all classifiers 
including the polarity shifting detector, three 
base classifiers and the meta-classifier in 
stacking are trained by SVM using the 
SVM-light tool 3  with Logistic Regression 
method for probability measuring (Platt, 1999). 
In all the experiments, each dataset is 
randomly and evenly split into two subsets: 50% 
documents as the training data and the remaining 
50% as the test data. The features include word 
unigrams and bigrams with Boolean weights. 
4.2 Experimental Results on Polarity 
Shifting Data 
To better understand the polarity shifting 
phenomena in document-level sentiment 
classification, we randomly investigate 200 
                                                      
2
 This data set is collected by Blitzer et al (2007): 
http://www.seas.upenn.edu/~mdredze/datasets/sentiment/ 
3
 It is available at: http://svmlight.joachims.org/ 
639
polarity-shifted sentences, together with their 
contexts (i.e. the sentences before and after it), 
automatically generated by the WFO ( 0? = ) 
feature selection method. We find that nearly 
half of the automatically generated polarity- 
shifted sentences are actually polarity-unshifted 
sentences or difficult to decide. That is to say, 
the polarity shifting training data is noisy to 
some extent. One main reason is that some 
automatically selected trigger words do not 
really contain sentiment information, e.g., ?hear?, 
?information? etc. Another reason is that some 
reversed opinion is given in a review without 
any explicit polarity shifting structures.  
To gain more insights, we manually checked 
100 sentences which are explicitly 
polarity-shifted and can also be judged by 
human according to their contexts. Table 1 
presents some typical structures causing polarity 
shifting. It shows that the most common polarity 
shifting type is Explicit Negation (37%), usually 
expressed by trigger words such as ?not?, ?no?, or 
?without?, e.g., in the sentence ?I am not happy 
with this flashcard at all?. Another common type 
of polarity shifting is Contrast Transition (20%), 
expressed by trigger words such as ?however?, 
e.g., in the sentence ?It is large and stylish, 
however, I cannot recommend it because of the 
lid?. Other less common yet productive polarity 
shifting types include Exception and Until. 
Exception structure is usually expressed by the 
trigger phrase ?the only? to indicate the one and 
only advantage of the product, e.g., in the 
sentence ?The only thing that I like about it is 
that bamboo is a renewable resource?. Until 
structure is often expressed by the trigger word 
?until? to show the reversed polarity, e.g. in the 
sentence ?This unit was a great addition until the 
probe went bad after only a few months?. 
 
Polarity Shifting 
Structures 
Trigger 
Words/Phrases 
Distribution 
(%) 
Explicit Negation not, no, without 37 
Contrast Transition but, however, 
unfortunately 
20 
Implicit Negation avoid, hardly,  7 
False Impression look, seem 6 
Likelihood probably, perhaps 5 
Counter-factual should, would 5 
Exception the only 5 
Until until 3 
Table 1: Statistics on various polarity shifting 
structures 
4.3 Experimental Results on Polarity 
Classification 
For comparison, several classifiers with different 
classification methods are developed.  
1) Baseline classifier, which applies SVM with 
all unigrams and bigrams. Note that it also 
serves as a base classifier in the following 
combined classifiers. 
2) Base classifier 1, a base classifier for the 
classifier combination method. It works on the 
polarity-unshifted data.  
3) Base classifier 2, another base classifier for 
the classifier combination method. It works on 
the polarity-shifted data. 
4) Negation classifier, which applies SVM with 
all unigrams and bigrams plus negation bigrams. 
It is a natural extension of the baseline classifier 
with the consideration of negation bigrams. In 
this study, the negation bigrams are collected 
using some negation trigger words, such as ?not? 
and ?never?. If a negation trigger word is found 
in a sentence, each word in the sentence is 
attached with the word ?_not? to form a negation 
bigram. 
5) Product classifier, which combines the 
baseline classifier, the base classifier 1 and the 
base classifier 2 using the product rule. 
6) Stacking classifier, a combined classifier 
similar to the Product classifier. It uses the 
stacking classifier combination method instead 
of the product rule.  
Please note that we do not compare our approach 
with the one as proposed in Ikeda et al (2008) 
due to the absence of a manually-collected 
sentiment dictionary. Besides, it is well known 
that a combination strategy itself is capable of 
improving the classification performance. To 
justify whether the improvement is due to the 
combination strategy or our polarity shifting 
detection or both, we first randomly split the 
training data into two portions and train two base 
classifiers on each portion, then apply the 
stacking method to combine them along with the 
baseline classifier. The corresponding results are 
shown as ?Random+Stacking? in Table 2. Finally, 
in our experiments, t-test is performed to 
evaluate the significance of the performance 
improvement between two systems employing 
different methods (Yang and Liu, 1999). 
 
640
Domain Baseline Base  
Classifier 
1 
Base  
Classifier 
2 
Negation 
Classifier 
Random 
+ 
Stacking 
Shifting 
+ 
Product 
Shifting 
+ 
Stacking 
Book 0.755 0.756 0.670 0.759 0.764 0.772 0.785 
DVD 0.750 0.743 0.667 0.748 0.759 0.768 0.770 
Electronic 0.779 0.786 0.711 0.785 0.789 0.820 0.830 
Kitchen 0.818 0.814 0.683 0.826 0.835 0.840 0.849 
Table 2: Performance comparison of different classifiers with equally-splitting between training and test data 
 
Performance comparison of different 
classifiers 
Table 2 shows the accuracy results of different 
methods using 2000 polarity shifted sentences 
and 2000 polarity-unshifted sentences to train the 
polarity shifting detector (Nmax=2000). Compared 
to the baseline classifier, it shows that: 1) The 
base classifier 1, which only uses the 
polarity-unshifted sentences as the training data, 
achieves similar performance. 2)  The base 
classifier 2 achieves much lower performance 
due to much fewer sentences involved. 3) 
Including negation bigrams usually allows 
insignificant improvements (p-value>0.1), which 
is consistent with most of previous works (Pang 
et al, 2002; Kennedy and Inkpen, 2006). 4) Both 
the product and stacking classifiers with polarity 
shifting detection significantly improve the 
performance (p-value<0.05). Compared to the 
product rule, the stacking classifier is preferable, 
probably due to the performance unbalance 
among the individual classifiers, e.g., the 
performance of the base classifier 2 is much 
lower than the other two. Although stacking with 
two randomly generated base classifiers, i.e. 
?Random + Stacking?, also consistently 
outperforms the baseline classifier, the 
improvements are much lower than what has 
been achieved by our approach. This suggests 
that both the classifier combination strategy and 
polarity shifting detection contribute to the 
overall performance improvement. 
Effect of WFO feature selection method 
Figure 3 presents the accuracy curve of the 
stacking classifier when using different Lambda 
( ? ) values in the WFO feature selection method. 
It shows that those feature selection methods 
which prefer frequency information, e.g., MI and 
BNS, are better in automatically generating the 
polarity shifting training data. This is reasonable 
since high frequency terms, e.g., ?is?, ?it?, ?a?, 
etc., tend to obey our assumption that the real 
polarity of one top term should belong to the 
polarity category where the term appears 
frequently. 
Performance of the Stacking Classifier
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Lambda=0 0.25 0.5 0.75 1
Ac
cu
ra
cy
Book DVD Electronic Kitchen
Figure 3: Performance of the stacking classifier using 
WFO with different Lambda ( ? ) values 
 Performance of the Stacking Classifier
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
200 500 1000 1500 2000 3000 4000 6000 8000
Ac
cu
ra
cy
Book DVD Electronic Kitchen
 Figure 4: Performance of the stacking classifier over 
different sizes of the polarity shifting training data 
(with Nmax sentences in each category) 
Effect of a classifier over different sizes of the 
polarity shifting training data 
Another factor which might influence the 
overall performance is the size of the polarity 
shifting training data. Figure 4 presents the 
overall performance on different numbers of the 
polarity shifting sentences when using the 
stacking classifier. It shows that 1000 to 4000 
sentences are enough for the performance 
improvement. When the number is too large, the 
noisy training data may harm polarity shifting 
detection. When the number is too small, it is not 
enough for the automatically generated polarity 
shifting training data to capture various polarity 
shifting structures. 
641
30% 40% 50% 60% 70% 80% 90% 100%
0.6
0.65
0.7
0.75
0.8
Domain: Book
The traning data sizes
Ac
c
ur
ac
y
 
 
Baseline BaseClassifier 1 BaseClassifier 2 Stacking
30% 40% 50% 60% 70% 80% 90% 100%
0.6
0.65
0.7
0.75
0.8
Domain: DVD
The traning data sizes
Ac
cu
ra
cy
30% 40% 50% 60% 70% 80% 90% 100%
0.65
0.7
0.75
0.8
0.85
0.9
Domain: Electronic
The traning data sizes
Ac
cu
ra
cy
30% 40% 50% 60% 70% 80% 90% 100%
0.65
0.7
0.75
0.8
0.85
0.9
Domain: Kitchen
The traning data sizes
Ac
c
ur
ac
y
 
 
Figure 5: Performance of different classifiers over different sizes of the polarity classification training data 
 
Effect of different classifiers over different 
sizes of the polarity classification training data 
Figure 5 shows the classification results of 
different classifiers with varying sizes of the 
polarity classification training data. It shows that 
our approach is able to improve the overall 
performance robustly. We also notice the big 
difference between the performance of the 
baseline classifier and that of the base classifier 
1 when using 30% training data in Book domain 
and 90% training data in DVD domain. Detailed 
exploration of the polarity shifting sentences in 
the training data shows that this difference is 
mainly attributed to the poor performance of the 
polarity shifting detector. Even so, the stacking 
classifier guarantees no worse performance than 
the baseline classifier. 
5 Conclusion and Future Work 
In this paper, we propose a novel approach to 
incorporate polarity shifting information into 
document-level sentiment classification. In our 
approach, we first propose a 
machine-learning-based classifier to detect 
polarity shifting and then apply two classifier 
combination methods to perform polarity 
classification. Particularly, the polarity shifting 
training data is automatically generated through 
a feature selection method. As shown in our 
experimental results, our approach is able to 
consistently improve the overall performance 
across different domains and training data sizes, 
although the automatically generated polarity 
shifting training data is prone to noise. 
Furthermore, we conclude that those feature 
selection methods, which prefer frequency 
information, e.g., MI and BNS, are good choices 
for generating the polarity shifting training data. 
In our future work, we will explore better 
ways in generating less-noisy polarity shifting 
training data. In addition, since our approach is 
language-independent, it is readily applicable to 
sentiment classification tasks in other languages. 
For availability of the automatically generated 
polarity shifting training data, please contact the 
first author (for research purpose only). 
Acknowledgments 
This research work has been partially supported 
by Start-up Grant for Newly Appointed 
Professors, No. 1-BBZM in the Hong Kong 
Polytechnic University and two NSFC grants, 
No. 60873150 and No. 90920004. We also thank 
the three anonymous reviewers for their helpful 
comments. 
642
References 
Blitzer J., M. Dredze, and F. Pereira. 2007. 
Biographies, Bollywood, Boom-boxes and 
Blenders: Domain Adaptation for Sentiment 
Classification. In Proceedings of ACL-07. 
Dasgupta S. and V. Ng. 2009. Mine the Easy and 
Classify the Hard: Experiments with Automatic 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Ding X., B. Liu, and P. Yu. 2008. A Holistic 
Lexicon-based Approach to Opinion Mining. In 
Proceedings of the International Conference on 
Web Search and Web Data Mining, WSDM-08. 
D?eroski S. and B. ?enko. 2004. Is Combining 
Classifiers with Stacking Better than Selecting the 
Best One? Machine Learning, vol.54(3), 
pp.255-273, 2004. 
Forman G. 2003. An Extensive Empirical Study of 
Feature Selection Metrics for Text Classification. 
The Journal of Machine Learning Research, 3(1), 
pp.1289-1305. 
Fumera G. and F. Roli. 2005. A Theoretical and 
Experimental Analysis of Linear Combiners for 
Multiple Classifier Systems. IEEE Trans. PAMI, 
vol.27, pp.942?956, 2005 
Ikeda D., H. Takamura, L. Ratinov, and M. Okumura. 
2008. Learning to Shift the Polarity of Words for 
Sentiment Classification. In Proceedings of 
IJCNLP-08. 
Kennedy, A. and D. Inkpen. 2006. Sentiment 
Classification of Movie Reviews using Contextual 
Valence Shifters. Computational Intelligence, 
vol.22(2), pp.110-125, 2006. 
Kim S. and E. Hovy. 2004. Determining the 
Sentiment of Opinions. In Proceedings of 
COLING-04. 
Kittler J., M. Hatef, R. Duin, and J. Matas. 1998. On 
Combining Classifiers. IEEE Trans. PAMI, vol.20, 
pp.226-239, 1998 
Li S., R. Xia, C. Zong, and C. Huang. 2009. A 
Framework of Feature Selection Methods for Text 
Categorization. In Proceedings of 
ACL-IJCNLP-09. 
Li S. and C. Zong. 2008. Multi-domain Sentiment 
Classification. In Proceedings of ACL-08: HLT, 
short paper. 
Liu B., M. Hu, and J. Cheng. 2005. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
In Proceedings of WWW-05. 
Na J., H. Sui, C. Khoo, S. Chan, and Y. Zhou. 2004. 
Effectiveness of Simple Linguistic Processing in 
Automatic Sentiment Classification of Product 
Reviews. In Conference of the International 
Society for Knowledge Organization (ISKO-04). 
Pang B. and L. Lee. 2004. A Sentimental Education: 
Sentiment Analysis using Subjectivity 
Summarization based on Minimum Cuts. In 
Proceedings of ACL-04. 
Pang B., L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification using Machine 
Learning Techniques. In Proceedings of 
EMNLP-02. 
Platt J. 1999. Probabilistic Outputs for Support 
Vector Machines and Comparisons to Regularized 
Likelihood Methods. In: A. Smola, P. Bartlett, B. 
Schoelkopf and D. Schuurmans (Eds.): Advances 
in Large Margin Classiers. MIT Press, Cambridge, 
61?74. 
Polanyi L. and A. Zaenen. 2006. Contextual Valence 
Shifters. Computing attitude and affect in text: 
Theory and application. Springer Verlag. 
Riloff E., S. Patwardhan, and J. Wiebe. 2006. Feature 
Subsumption for Opinion Analysis. In 
Proceedings of EMNLP-06. 
Turney P. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. In Proceedings of 
ACL-02. 
Vilalta R. and Y. Drissi. 2002. A Perspective View 
and Survey of Meta-learning. Artificial 
Intelligence Review, 18(2), pp. 77?95. 
Wan X. 2009. Co-Training for Cross-Lingual 
Sentiment Classification. In Proceedings of 
ACL-IJCNLP-09. 
Wiebe J. 2000. Learning Subjective Adjectives from 
Corpora. In Proceedings of AAAI-2000. 
Wilson T., J. Wiebe, and P. Hoffmann. 2009. 
Recognizing Contextual Polarity: An Exploration 
of Features for Phrase-Level Sentiment Analysis. 
Computational Linguistics, vol.35(3), pp.399-433, 
2009. 
Yang Y. and X. Liu, X. 1999. A Re-Examination of 
Text Categorization methods. In Proceedings of 
SIGIR-99. 
Yang Y. and J. Pedersen. 1997. A Comparative Study 
on Feature Selection in Text Categorization. In 
Proceedings of ICML-97. 
643
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 45?53,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Text-driven Rule-based System for Emotion Cause Detection 
 
Sophia Yat Mei Lee? Ying Chen?* Chu-Ren Huang?? 
?Department of Chinese and Bilingual Studies  
The Hong Kong Polytechnic University 
*Department of Computer Engineering 
China Agriculture University 
?Institute of Linguistics  
Academia Sinica 
{sophiaym, chenying3176, churenhuang}@gmail.com 
 
 
 
  
Abstract 
Emotion cause detection is a new research area 
in emotion processing even though most theo-
ries of emotion treat recognition of a triggering 
cause event as an integral part of emotion. As a 
first step towards fully automatic inference of 
cause-emotion correlation, we propose a text-
driven, rule-based approach to emotion cause 
detection in this paper. First of all, a Chinese 
emotion cause annotated corpus is constructed 
based on our proposed annotation scheme. By 
analyzing the corpus data, we identify seven 
groups of linguistic cues and generalize two 
sets of linguistic rules for detection of emotion 
causes. With the linguistic rules, we then de-
velop a rule-based system for emotion cause 
detection. In addition, we propose an evaluation 
scheme with two phases for performance as-
sessment. Experiments show that our system 
achieves a promising performance for cause oc-
currence detection as well as cause event detec-
tion. The current study should lay the ground 
for future research on the inferences of implicit 
information and the discovery of new informa-
tion based on cause-event relation. 
1 Introduction 
Text-based emotion processing has attracted plenty 
of attention in NLP. Most research has focused on 
the emotion detection and classification by 
identifying the emotion types, for instances 
happiness and sadness, for a given sentence or 
document (Alm 2005, Mihalcea and Liu 2006, 
Tokuhisa et al 2008). However, on top of this 
surface level information, deeper level information 
regarding emotions, such as the experiencer, cause, 
and result of an emotion, needs to be extracted and 
analyzed for real world applications (Alm 2009). 
In this paper, we aim at mining one of the crucial 
deep level types of information, i.e. emotion cause, 
which provides useful information for applications 
ranging from economic forecasting, public opinion 
mining, to product design. Emotion cause detection 
is a new research area in emotion processing. In 
emotion processing, the cause event and emotion 
correlation is a fertile ground for extraction and 
entailment of new information. As a first step 
towards fully automatic inference of cause-
emotion correlation, we propose a text-driven, 
rule-based approach to emotion cause detection in 
this paper.  
 Most theories of emotion treat recognition of 
a triggering cause event as an integral part of 
emotional experience (Descartes 1649, James 1884, 
Plutchik 1962, Wierzbicka 1999). In this study, 
cause events refer to the explicitly expressed 
arguments or events that evoke the presence of the 
corresponding emotions. They are usually 
expressed by means of propositions, 
nominalizations, and nominals. For example, ?they 
like it? is the cause event of the emotion happiness 
in the sentence ?I was very happy that they like it?. 
Note that we only take into account emotions that 
are explicitly expressed, which are usually 
presented by emotion keywords, e.g. ?This 
surprises me?. Implicit emotions that require 
inference or connotation are not processed in this 
first study. In this study, we first build a Chinese 
emotion cause annotated corpus with five primary 
emotions, i.e. happiness, sadness, anger, fear, and 
surprise. We then examine various linguistic cues 
which help detect emotion cause events: the 
position of cause event and experiencer relative to 
the emotion keyword, causative verbs (e.g. rang4 
?to cause?), action verbs (e.g. xiang3dao4 ?to think 
about?), epistemic markers (e.g. kan4jian4 ?to 
see?), conjunctions (e.g. yin1wei4 ?because?), and 
prepositions (e.g. dui4yu2 ?for?). With the help of 
45
these cues, a list of linguistic rules is generalized. 
Based on the linguistic rules, we develop a rule-
based system for emotion cause detection. 
Experiments show that such a rule-based system 
performs promisingly well. We believe that the 
current study should lay the ground for future 
research on inferences and discovery of new 
information based on cause-event relation, such as 
detection of implicit emotion or cause, as well as 
prediction of public opinion based on cause events, 
etc.  
The paper is organized as follows. Section 2 
discusses the related work on various aspects of 
emotion analysis. Section 3 describes the construc-
tion of the emotion cause corpus. Section 4 
presents our rule-based system for emotion cause 
detection. Section 5 describes its evaluation and 
performance. Section 6 highlights our main contri-
butions. 
2 Previous Work 
We discuss previous studies on emotion analysis in 
this section, and underline fundamental yet unre-
solved issues. We survey the previous attempts on 
textual emotion processing and how the present 
study differs.  
2.1 Emotion Classes 
Various approaches to emotion classification were 
proposed in different fields, such as philosophy 
(Spinoza 1675, James 1884), biology (Darwin 
1859, linguistics (Wierzbicka 1999, K?vecses 
2000), neuropsychology (Plutchik 1962, Turner 
1996), and computer science (Ortony et al 1988, 
Picard 1995), as well as varying from language to 
language. Although there is lack of agreement 
among different theories on emotion classification, 
a small number of primary emotions are commonly 
assumed. Other emotions are secondary emotions 
which are the mixtures of the primary emotions.  
Researchers have attempted to propose the list 
of primary emotions, varying from two to ten basic 
emotions (Ekman 1984, Plutchik 1980, Turner 
2000). Fear and anger appear on every list, whe-
reas happiness and sadness appear on most of the 
lists. These four emotions, i.e. fear, anger, happi-
ness, and sadness, are the most common primary 
emotions. Other less common primary emotions 
are surprise, disgust, shame, distress, guilt, interest, 
pain, and acceptance.  
In this study, we adopt Turner?s emotion clas-
sification (2000), which identifies five primary 
emotions, namely happiness, sadness, fear, anger, 
and surprise. Turner?s list consists of primary emo-
tions agreed upon by most previous work. 
2.2 Emotion Processing in Text 
Textual emotion processing is still in its early stag-
es in NLP. Most of the previous works focus on 
emotion classification given a known emotion con-
text such as a sentence or a document using either 
rule-based (Masum et al 2007, Chaumartin 2007) 
or statistical approaches (Mihalcea and Liu 2005, 
Kozareva et al 2007). However, the performance 
is far from satisfactory. What is more, many basic 
issues remain unresolved, for instances, the rela-
tionships among emotions, emotion type selection, 
etc. Tokuhisa et al (2008) was the first to explore 
both the issues of emotion detection and classifica-
tion. It created a Japanese emotion-provoking 
event corpus for an emotion classification task us-
ing an unsupervised approach. However, only 
49.4% of cases were correctly labeled. Chen et al 
(2009) developed two cognitive-based Chinese 
emotion corpora using a semi-unsupervised ap-
proach, i.e. an emotion-sentence (sentences con-
taining emotions) corpus and a neutral-sentence 
(sentences containing no emotion) corpus. They 
showed that studies based on the emotion-sentence 
corpus (~70%) outperform previous corpora. 
Little research, if not none, has been done to 
examine the interactions between emotions and the 
corresponding cause events, which may make a 
great step towards an effective emotion classifica-
tion model. The lack of research on cause events 
restricted current emotion analysis to simple classi-
ficatory work without exploring the potentials of 
the rich applications of putting emotion ?in con-
text?. In fact, emotions can be invoked by percep-
tions of external events and in turn trigger 
reactions. The ability to detect implicit invoking 
causes as well as predict actual reactions will add 
rich dimensions to emotion analysis and lead to 
further research on event computing.  
3 Emotion Cause Corpus  
This section briefly describes how the emotion 
cause corpus is constructed. We first explain what 
46
an emotion cause is and discuss how emotion 
cause is linguistically expressed in Chinese. We 
then describe the corpus data and the annotation 
scheme. For more detailed discussion on the con-
struction of the emotion cause corpus, please refer 
to Lee et al (2010). 
3.1 Cause Events 
Following Talmy (2000), the cause of an emotion 
should be an event itself. In this work, it is called a 
cause event. By cause event, we do not necessarily 
mean the actual trigger of the emotion or what 
leads to the emotion. Rather, it refers to the imme-
diate cause of the emotion, which can be the actual 
trigger event or the perception of the trigger event. 
Adapting TimeML annotation scheme (Saur? et al 
2004), events refer to situations that happen or oc-
cur. In this study, cause events specifically refer to 
the explicitly expressed arguments or events that 
are highly linked with the presence of the corres-
ponding emotions. In Lee et al?s (2010) corpus, 
cause events are categorized into two types: verbal 
events and nominal events. Verbal events refer to 
events that involve verbs (i.e. propositions and 
nominalizations), whereas nominal events are 
simply nouns (i.e. nominals). Some examples of 
cause event types are given in bold face in (1)-(6). 
 
(1) Zhe4-DET tou2-CL niu2-cattle de-POSS zhu3ren2-owner, 
yan3kan4-see zi4ji3-oneself de-POSS niu2-cattle 
re3chu1-cause huo4-trouble lai2-come le-ASP, 
fei1chang2-very hai4pa4-frighten, jiu4-then ba3-PREP 
zhe4-DET tou2-CL niu2-cattle di1jia4-low price 
mai4chu1-sell.  
 ?The owner was frightened to see that his cattle 
caused troubles, so he sold it at a low price.? 
 
(2) Mei2-not  xiang3dao4-think  ta1-3.SG.F  shuo1-say  de-
POSS  dou1-all shi4-is  zhen1-true  hua4-word,  rang4-
lead  ta1-3.SG.M  zhen4jing1-shocked  bu4yi3-very. 
 ?He was shocked that what she said was the 
truth.? 
 
(3) Ta1-3.SG.M  dui4-for  zhe4-DET  ge4-CL  chong1man3-
full of  nong2hou4-dense  ai4yi4-love  de-DE xiang3fa3-
idea  gao1xing4-happy de-DE  shou3wu3zu2dao3-flourish. 
 ?He was very happy about this loving idea.? 
 
(4) Zhe4-DET ci4-CL yan3chu1-performance de-POSS 
jing1zhi4-exquisite dao4shi4-is ling4-cause wo3-1.SG 
shi2fen1-very jing1ya4-surprise.  
 ?I was very surprised by this exquisite perfor-
mance.?   
 
(5) Ni2ao4-Leo de-POSS hua4-word hen3-very ling4-make 
kai3luo4lin2-Caroline shang1xin1-sad. 
 ?Caroline was very saddened by Leo?s words.? 
 
(6) Dui4yu2-for wei4lai2-future, lao3shi2shuo1-frankly wo3-
1.SG hen3-very hai4pa4-scared.  
 ?Frankly, I am very scared about the future.? 
 
The causes in (1) and (2) are propositional causes, 
which indicate the actual events involved in caus-
ing the emotions. The ones in (3) and (4) are no-
minalized causes, whereas (5) and (6) involve 
nominal causes  
3.2 Corpus Data and Annotation Scheme 
Based on the list of 91 Chinese primary emotion 
keywords identified in Chen et al (2009), we ex-
tract 6,058 instances of sentences by keyword 
matching from the Sinica Corpus 1 , which is a 
tagged balanced corpus of Mandarin Chinese con-
taining a total of ten million words. Each instance 
contains the focus sentence with the emotion key-
word ?<FocusSentence>? plus the sentence before 
?<PrefixSentence>? and after ?<SuffixSentence>? 
it. The extracted instances include all primary emo-
tion keywords occurring in the Sinica Corpus ex-
cept for the emotion class happiness, as the 
keywords of happiness exceptionally outnumber 
other emotion classes. In order to balance the 
number of each emotion class, we set the upper 
limit at about 1,600 instances for each primary 
emotion.  
Note that the presence of emotion keywords 
does not necessarily convey emotional information 
due to different possible reasons such as negative 
polarity and sense ambiguity. Hence, by manual 
inspection, we remove instances that 1) are non-
emotional; 2) contain highly ambiguous emotion 
keywords, such as ru2yi4 ?wish-fulfilled?, hai4xiu1 
?to be shy?, wei2nan2 ?to feel awkward?, from the 
corpus. After the removal, the remaining instances 
in the emotion cause corpus is 5,629. Among the 
remaining instances, we also remove the emotion 
keywords in which the instances do not express 
that particular emotion and yet are emotional. The 
total emotion keywords in the corpus is 5,958. 
For each emotional instance, two annotators 
manually annotate cause events of each keyword. 
Since more than one emotion can be present in an 
                                                           
1
 http://dbo.sinica.edu.tw/SinicaCorpus/ 
47
instance, the emotion keywords are tagged as 
<emotionword id=0>, <emotionword id=1>, and 
so on.  
 
573 Y 0/shang1 xin1/Sadness  
<PrefixSentence> ma1ma ye3 wen4 le ling2 ju1, dan4 shi4 
mei2 you3 ren4 jian4 dao4 xiao3 bai2. </PrefixSentence> 
<FocusSentence>wei4 le [*01n] zhe4 jian4 shi4 [*02n] , wo3 
ceng2 <emotionword id=0>shang1 xin1</emotionword> le 
hou2 jiu3,dan4 ye3 wu2 ji3 yu4 shi4. </FocusSentence> <Suf-
fixSentence>mei3 dang1 zai4 kan4 dao4 bai2 se4 de qi4 gou3, 
bu4 jin4 hui4 xiang3 qi3 xiao3 bai2 </SuffixSentence> 
 
573 Y 0/to be sad/Sadness  
<PrefixSentence> Mom also asked the neighbors, but no one 
ever saw Little White. </PrefixSentence> <FocusSentence> 
Because of [*01n] this [*02n] , I have been feeling very <emo-
tionword id=0> sad </emotionword> for a long time, but this 
did not help.  </FocusSentence> <SuffixSentence> Whenever 
[I] see a white stray dog, [I] cannot help thinking of Little 
White. </SuffixSentence> 
Figure 1: An Example of Cause Event Annotation 
 
Figure 1 shows an example of annotated emotional 
sentences in corpus, presented as pinyin with tones, 
followed by an English translation. For an emotion 
keyword tagged as <id=0>, [*01n] marks the be-
ginning of its cause event while [*02n] marks the 
end. The ?0? shows which index of emotion key-
word it refers to, ?1? marks the beginning of the 
cause event, ?2? marks the end, and ?n? indicates 
that the cause is a nominal event. For an emotion 
keyword tagged as <id=1>, [*11e] marks the be-
ginning of the cause event while [*12e] marks the 
end, in which ?e? refers to a verbal event, i.e. ei-
ther a proposition or a nominalization. An emotion 
keyword can sometimes be associated with more 
than one cause, in which case both causes are 
marked. The emotional sentences containing no 
explicitly expressed cause event remain as they are. 
The actual number of extracted instances of 
each emotion class to be analyzed, the positive 
emotional instances, and the instances with cause 
events are presented in Table 1.  
 
Table 1: Summary of Corpus Data 
Emotions No. of Instances Extracted Emotional With Causes 
Happiness 1,644 1,327 1,132 (85%) 
Sadness 901 616 468 (76%) 
Fear 897 689 567 (82%) 
Anger 1,175 847 629 (74%) 
Surprise 1,341 781 664 (85%) 
Total 5,958 4,260 (72%) 3,460 (81%) 
We can see that 72% of the extracted instances ex-
press emotions, and 81% of the emotional in-
stances have a cause. The corpus contains 
happiness (1,327) instances the most and sadness 
(616) the least. For each emotion type, about 81% 
of the emotional sentences, on average, are consi-
dered as containing a cause event, with surprise 
the highest (85%) and anger the lowest (73%). 
This indicates that an emotion mostly occurs with 
the cause event explicitly expressed in the text, 
which confirms the prominent role of cause events 
in expressing an emotion. 
4 A Rule-based System for Cause Detec-
tion  
4.1 Linguistic Analysis of Emotion Causes 
By analyzing the corpus data, we examine the 
correlations between emotions and cause events in 
terms of various linguistic cues: the position of 
cause event, verbs, epistemic markers, 
conjunctions, and prepositions. We hypothesize 
that these cues will facilitate the detection of 
emotion cause events.  
 First, we calculate the distribution of cause 
event types of each emotion as well as the position 
of cause events relative to emotion keywords and 
experiencers. The total number of emotional 
instances regarding each emotion is given in Table 
2.  
 
Table 2: Cause Event Position of Each Emotion 
Emotions Cause Type (%) Cause Position (%) 
Event Nominal Left Right 
Happiness 76 6 74 29 
Sadness 67 8 80 20 
Fear 68 13 52 48 
Anger 55 18 71 26 
Surprise 73 12 59 41 
 
Table 2 suggests that emotion cause events tend to 
be expressed by verbal events than nominal events 
and that cause events tend to occur at the position 
to the left of the emotion keyword, with fear (52%) 
being no preference. This may be attributed to the 
fact that fear can be triggered by either factive or 
potential causes, which is rare for other primary 
emotions. For fear, factive causes tend to take the 
left position whereas potential causes tend to take 
the right position. 
48
 Second, we identify seven groups of 
linguistic cues that are highly collocated with 
cause events (Lee et al 2010), as shown in Table 3.  
 
Table 3: Seven Groups of Linguistic Cues 
Group Cue Words 
I ?to cause?: rang4, ling4, shi3  
II ?to think about?: e.g. xiang3 dao4, xiang3 qi3, yi1 
xiang3  
?to talk about?: e.g. shuo1dao4, jiang3dao4, tan2dao4  
III ?to say?: e.g. shuo1, dao4 
IV ?to see?: e.g. kan4dao4, kan4jian4, jian4dao4  
?to hear?: e.g. ting1dao4, ting1 shuo1 
?to know?: e.g. zhi1dao4, de2zhi1, fa1xian4 
?to exist?: you3 
V ?for? as in ?I will do this for you?: wei4, wei4le 
?for? as in ?He is too old for the job?: dui4, dui4yu2 
VI ?because?: yin1, yin1wei4, you2yu2 
VII ?is?: deshi4 
?at?: yu2 
?can?: neng2 
 
Group I includes three common causative verbs, 
and Group II a list of verbs of thinking and talking. 
Group III is a list of say verbs. Group IV includes 
four types of epistemic markers which are usually 
verbs marking the cognitive awareness of emotion 
in the complement position (Lee and Huang 2009). 
The epistemic markers include verbs of seeing, 
hearing, knowing, and existing. Group V covers 
some prepositions which all roughly mean ?for?. 
Group VI contains the conjunctions that explicitly 
mark the emotion cause. Group VII includes other 
linguistic cues that do not fall into any of the six 
groups. Each group of linguistic cues serves as an 
indicator marking the cause events in different 
structures of emotional constructions, in which 
Group I specifically marks the end of the cause 
events while the other six groups marks the 
beginning of the cause events. 
4.2 Linguistic Rules for Cause Detection 
We examine 100 emotional sentences of each emo-
tion keyword randomly extracted from the devel-
opment data, and generalize some rules for 
identifying the cause of the corresponding emotion 
verb (Lee 2010). The cause is considered as a 
proposition. It is generally assumed that a proposi-
tion has a verb which optionally takes a noun oc-
curring before it as the subject and a noun after it 
as the object. However, a cause can also be ex-
pressed as a nominal. In other words, both the pre-
dicate and the two arguments are optional provided 
that at least one of them is present.  
We also manually identify the position of the 
experiencer as well as the linguistic cues discussed 
in Section 4.1. All these components may occur in 
the clause containing the emotion verb (focus 
clause), the clause before the focus clause, or the 
clause after the focus clause. The abbreviations 
used in the rules are given as follows:  
 
C = Cause event 
E = Experiencer 
K = Keyword/emotion verb 
B = Clause before the focus clause 
F = Focus clause/the clause containing the emotion verb 
A = Clause after the focus clause 
 
For illustration, an example of the rule description 
is given in Rule 1. 
 
Rule 1: 
i) C(B/F) + I(F) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh after I in F 
iii) C = the nearest (N)+(V)+(N) before I in F/B  
 
Rule 1 indicates that the experiencer (E) appears to 
be the nearest Na (common noun)/ Nb (proper 
noun)/ Nc (place noun)/ Nh (pronoun) after Group 
I cue words in the focus clause (F), while, at the 
same time, it comes before the keyword (K). Be-
sides, the cause (C) comes before Group I cue 
words. We simplify the proposition as a structure 
of (N)+(V)+(N), which is very likely to contain the 
cause event. Theoretically, in identifying C, we 
should first look for the nearest verb occurring be-
fore Group I cue words in the focus sentence (F) or 
the clause before the focus clause (B), and consider 
this verb as an anchor. From this verb, we search to 
the left for the nearest noun, and consider it as the 
subject; we then search to the right for the nearest 
noun until the presence of the cue words, and con-
sider it as the object. The detected subject, verb, 
and object form the cause event. In most cases, the 
experiencer is covertly expressed. It is, however, 
difficult to detect such causes in practice as causes 
may contain no verbs, and the two arguments are 
optional. Therefore, we take the clause instead of 
the structure of (N)+(V)+(N) as the actual cause. 
Examples are given in (7) and (8). For both sen-
tences, the clause that comes before the cue word 
is taken as the cause event of the emotion in ques-
tion. 
 
49
(7) [C yi1 la1 ke4 xi4 jun1 wu3 qi4 de bao4 guang1], [I 
shi3] [E lian2 he2 guo2 da4 wei2][K zhen4 jing1] . 
?[C The revealing of Iraq?s secret bacteriological 
weapons] [K shocked] [E the United Nations].? 
 
(8) [C heng2 shan1 jin1 tian1 ti2 chu1 ci2 cheng2], [I 
ling4] [E da4 ban3] zhi4 wei2 [K fen4 nu4] ? 
?[C Yokoyama submitted his resignation today], [K 
angered] [E the people of Osaka].? 
 
Table 4 summarizes the generalized rules for de-
tecting the cause events of the five primary emo-
tions in Chinese. We identify two sets of rules: 1) 
the specific rules that apply to all emotional in-
stances (i.e. rules 1-13); 2) the general rules that 
apply to the emotional instances in which causes 
are not found after applying the specific set of 
rules (i.e. rules 14 and 15).  
 
Table 4: Linguistic Rules for Cause Detection 
No. Rules 
1 i) C(B/F) + I(F) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh after I in F 
iii) C = the nearest (N)+(V)+(N) before I in F/B 
2 i) E(B/F) + II/IV/V/VI(B/F) + C(B/F) + K(F) 
ii) E=the nearest Na/Nb/Nc/Nh before II/IV/V/VI in B/F 
iii) C = the nearest (N)+(V)+(N) before K in F 
3 i) II/IV/V/VI (B) + C(B) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after II/IV/V/VI in B 
4 i) E(B/F) + K(F) + IV/VII(F) + C(F/A) 
ii) E = a: the nearest Na/Nb/Nc/Nh before K in F; b: the 
first Na/Nb/Nc/Nh in B 
iii) C = the nearest (N)+(V)+(N) after IV/VII in F/A 
5 i) E(F)+K(F)+VI(A)+C(A) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after VI in A 
6 i) I(F) + E(F) + K(F) + C(F/A) 
ii) E = the nearest Na/Nb/Nc/Nh after I in F 
iii) C = the nearest (N)+(V)+(N) after K in F or A 
7 i) E(B/F) + yue4 C yue4 K ?the more C the more K? (F)  
ii) E = the nearest Na/Nb/Nc/Nh before the first yue4 in 
B/F 
iii) C = the V in between the two yue4?s in F 
8 i) E(F) + K(F) + C(F) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after K in F 
9 i) E(F) + IV(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh before IV in F 
iii) C = IV+(an aspectual marker) in F 
10 i) K(F) + E(F) + de ?possession?(F) + C(F) 
ii) E = the nearest Na/Nb/Nc/Nh after K in F 
iii) C = the nearest (N)+V+(N)+?+N after de in F 
11 i) C(F) + K(F) + E(F) 
ii) E = the nearest Na/Nb/Nc/Nh after K in F 
iii) C = the nearest (N)+(V)+(N) before K in F 
12 i) E(B) + K(B) + III (B) + C(F)  
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after III in F 
13 i) III(B) + C(B) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) after III in B 
14 i) C(B) + E(F) + K(F) 
ii) E = the nearest Na/Nb/Nc/Nh before K in F 
iii) C = the nearest (N)+(V)+(N) before K in B  
15 i) E(B) +C(B) + K(F)  
ii) E = the first Na/Nb/Nc/Nh in B 
iii) C = the nearest (N)+(V)+(N) before K in B 
 
 
Constraints are set to each rule to filter out incor-
rect causes. For instances, in Rule 1, the emotion 
keyword cannot be followed by the words de ?pos-
session?/ deshi4 ?is that?/ shi4 ?is? since it is very 
likely to have the cause event occurring after such 
words; in Rule 2, the cue word in III yuo3 ?to ex-
ist? is excluded as it causes noises; whereas for 
Rule 4, it only applies to instances containing 
keywords of happiness, fear, and surprise. 
5 Experiment  
5.1 Evaluation Metrics 
An evaluation scheme is designed to assess the 
ability to extract the cause of an emotion in context. 
We specifically look into two phases of the per-
formance of such a cause recognition system. 
Phase 1 assesses the detection of an emotion co-
occurrence with a cause; Phrase 2 evaluates the 
recognition of the cause texts for an emotion. 
 
Overall Evaluation:  
The definitions of related metrics are presented in 
Figure 2. For each emotion in a sentence, if neither 
the gold-standard file nor the system file has a 
cause, both precision and recall score 1; otherwise, 
precision and recall are calculated by the scoring 
method ScoreForTwoListOfCauses. As an emotion 
may have more than one cause, ScoreForTwoLis-
tOfCauses calculates the overlap scores between 
two lists of cause texts. Since emotion cause rec-
ognition is rather complicated, two relaxed string 
match scoring methods are selected to compare 
two cause texts, ScoreForTwoStrings: Relaxed 
Match 1 uses the minimal overlap between the 
gold-standard cause and the system cause. The sys-
tem cause is considered as correct provided that 
there is at least one overlapping Chinese character; 
Relaxed Match 2 is more rigid which takes into 
account the overlap text length during scoring. 
 
50
Phase 1: The Detection of Cause Occurrence 
The detection of cause occurrence is considered a 
preliminary task for emotion cause recognition and 
is compounded by the fact that neutral sentences 
are difficult to detect, as observed in Tokuhisa et al 
(2008). For Phase 1, each emotion keyword in a 
sentence has a binary tag: Y (i.e. with a cause) or 
N (without a cause). Similar to other NLP tasks, 
we adopt the common evaluation metrics, i.e. accu-
racy, precision, recall, and F score. 
 
Phase 2: The Detection of Causes 
The evaluation in Phase 2 is limited to the emotion 
keywords with a cause either in the gold-standard 
file or in the system file. The performance is calcu-
lated as in Overall Evaluation scheme. 
 
 
 
5.2 Results and Discussion 
We use 80% sentences as the development data, 
and 20% as the test data. The baseline is designed 
as follows: find a verb to the left of the keyword in 
question, and consider the clause containing the 
verb as a cause.  
Table 5 shows the performances of the overall 
evaluation. We find that the overall performances 
of our system have significantly improved using 
Relaxed Match 1 and Relaxed Match 2 by 19% 
and 19% respectively. Although the overall per-
formance of our system (47.95% F-score for Re-
laxed Match 1 and 41.67% for Relaxed Match 2) is 
not yet very high, it marks a good start for emotion 
 
Overall evaluation formula: 
 Precision (GF, SF) =  
ScoreForTwoListOfCauses ( , ) 
1 
j j
j
j
i i
i i
S GF em S
S SF em S
SCList GCList
?
?
?
?
? ?
? ?
 
 Recall (GF, SF) =  
ScoreForTwoListOfCauses ( , ) 
1 
j j
j
j
i i
i i
S GF em S
S GF em S
SCList GCList
?
?
?
?
? ?
? ?  
Where GF and SF are the gold-standard cause file and system cause file respectively, and both files include 
the same sentences. Si is a sentence, and emj is an emotion keyword in Si. GCListj and SCListj are the lists 
of the gold-standard causes and system causes respectively for the emotion keyword emj.  
 
ScoreForTwoListOfCauses (GCList, SCList):  
 If there is no cause in either GCList or SCList: Precision = 1; Recall = 1 
     Else: 
        Precision =  
( , )
| |
i j
GCi GCListSCj SCList
Max ScoreTwoStrings GC SC
SCList
?
?
?
 
Recall     =  
( , )
| |
i j
SCj SCListGCi GCList
Max ScoreTwoStrings GC SC
GCList
?
?
?
 
 
ScoreForTwoStrings(GC, SC): GC is a gold-standard cause text, and SC is a system cause text. 
Relaxed Match 1:  If overlap existing, both precision and recall are 1; Else, both are 0. 
Relaxed Match 2:    Precision (GC, SC) = ( )
( )
Len overlapText
Len SC
  
Recall (GC, SC)   = ( )
( )
Len overlapText
Len GC
  
Figure 2: The Definitions of Metrics for Cause Detection 
 
51
 Relaxed Match 1 Relaxed Match 2 
 Precision Recall F-score Precision Recall F-score 
Baseline 25.94 31.99  28.65 17.77 29.62  22.21 
Our System 45.06  51.24 47.95 39.89 43.63 41.67 
Table 5: The Overall Performances 
 
 Baseline Rule-based System 
Emotions Precision Recall F-score Precision Recall F-score 
With causes 99.42 79.74 88.50 96.871 80.851 88.139 
Without causes 4.39 66.67 8.23 13.158 52.632 21.053 
Table 7: The Detailed Performances in Phase 1 
 
 Relaxed Match 1 Relaxed Match 2 
 Precision Recall F-score Precision Recall F-score 
Baseline 25.37 39.28 30.83 17.09 36.29  23.24 
Our System 44.64 61.30  51.66 39.18 51.68 44.57 
Table 8: The Detailed Performances in Phase 2 
 
 Baseline Rule-based System 
Accuracy 79.56 79.38 
Table 6: The Overall Accuracy in Phase 1 
 
cause detection and extraction. 
Table 6 and 7 show the performances of the 
baseline and our rule-based system in Phase 1. Ta-
ble 6 shows the overall accuracy, and Table 7 
shows the detailed performances. In Table 6, we 
find that our system and the baseline have similar 
high accuracy scores. Yet Table 7 shows that both 
systems achieve a high performance for emotions 
with a cause, but much worse for emotions without 
a cause. It is important to note that even though the 
naive baseline system has comparably high per-
formance with our rule-based system in judging 
whether there is a cause in context, this result is 
biased by two facts. First, as the corpus contains 
more than 80% of sentences with emotion, a sys-
tem which is biased toward detecting a cause, such 
as the baseline system, naturally performs well. In 
addition, once the actual cause is examined, we can 
see that the baseline actually detects a lot of false 
positives in the sense that the cause it identifies is 
only correct in 4.39%. Our rule-based system 
shows great promise in being able to deal with 
neutral sentences effectively and being able to 
detect the correct cause at least three times more 
often than the baseline.  
Table 8 shows the performances in Phase 2. 
Comparing to the baseline, we find that our rules 
improve the performance of cause recognition us-
ing Relaxed Match 1 and 2 scoring by 21% and 
21% respectively. On the one hand, the 7% gap in 
F-score between Relaxed Match 1 and 2 also indi-
cates that our rules can effectively locate the clause 
of a cause. On the other hand, the rather low per-
formances of the baseline show that most causes 
recognized by the baseline are wrong although the 
baseline effectively detects the cause occurrence, 
as indicated in Table 7. In addition, we check the 
accuracy (precision) and contribution (recall) of 
each rule. In descending order, the top four accu-
rate rules are: Rules 7, 10, 11, and 1; and the top 
four contributive rules are: Rules 2, 15, 14, and 3.  
6 Conclusion  
Emotion processing has been a great challenge in 
NLP. Given the fact that an emotion is often trig-
gered by cause events and that cause events are 
integral parts of emotion, we propose a linguistic-
driven rule-based system for emotion cause detec-
tion, which is proven to be effective. In particular, 
we construct a Chinese emotion cause corpus an-
notated with emotions and the corresponding cause 
events. Since manual detection of cause events is 
labor-intensive and time-consuming, we intend to 
use the emotion cause corpus to produce automatic 
extraction system for emotion cause events with 
machine learning methods. We believe that our 
rule-based system is useful for many real world 
applications. For instance, the information regard-
ing causal relations of emotions is important for 
product design, political evaluation, etc. Such a 
system also shed light on emotion processing as 
the detected emotion cause events can serve as 
clues for the identification of implicit emotions.  
52
References  
Alm, C. O., D. Roth and R. Sproat. 2005. Emotions 
from Text: Machine Learning for Text-based Emo-
tion Prediction. In Proceedings of the Human Lan-
guage Technology Conference and the 2005 
Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, Canada, 6-8 October, 
pp. 579-586. 
Alm, C. O. 2009. Affect in Text and Speech. VDM 
Verlag: Saarbr?cken. 
Chen, Y., S. Y. M. Lee and C.-R. Huang. 2009. A Cog-
nitive-based Annotation System for Emotion Com-
puting. In Proceedings of the Third Linguistic 
Annotation Workshop (The LAW III), ACL 2009. 
Chaumartin, F.-R. 2007. A Knowledgebased System for 
Headline Sentiment Tagging. In Proceedings of the 
4th International Workshop on Semantic Evalua-
tions. 
Darwin, C. 1859. On the Origin of Species by Means of 
Natural Selection. London: John Murray. 
Descartes, R. 1649. The Passions of the Soul. In J. Cot-
tingham et al (Eds), The Philosophical Writings of 
Descartes. Vol. 1, 325-404. 
Ekman, P. 1984. Expression and the Nature of Emotion. 
In Scherer, K. and P. Ekman (Eds.), Approaches to 
Emotion. Hillsdale, N.J.: Lawrence Erlbaum. 319-
343. 
James, W. 1884. What is an Emotion? Mind, 9(34):188?
205. 
Kozareva, Z., B. Navarro, S. Vazquez, and A. Nibtoyo. 
2007. UA-ZBSA: A Headline Emotion Classifica-
tion through Web Information. In Proceedings of the 
4th International Workshop on Semantic Evalua-
tions.  
K?vecses, Z. 2000. Metaphor and Emotion: Language, 
Culture and Body in Human Feeling. Cambridge: 
Cambridge University Press. 
Lee, S. Y. M. 2010. A Linguistic Approach towards 
Emotion Detection and Classification. Ph.D. Disser-
tation. Hong Kong. 
Lee, S. Y. M., C. Ying, and C.-R. Huang. 2010. Emo-
tion Cause Events: Corpus Construction and Analy-
sis. In Proceedings of The Seventh International 
Conference on Language Resources and Evaluation 
(LREC 2010). May 19-21. Malta. 
Lee, S. Y. M. and C.-R. Huang. 2009. Explicit Epistem-
ic Markup of Causes in Emotion Constructions. The 
Fifth International Conference on Contemporary 
Chinese Grammar. Hong Kong. November 27 - De-
cember 1. 
Masum, S. M., H. Prendinger, and M. Ishizuka. 2007. 
Emotion Sensitive News Agent: An Approach To-
wards User Centric Emotion Sensing from the News. 
In Proceedings of the IEEE/WIC/ACM International 
Conference on Web Intelligence. 
Mihalcea, R. and H. Liu. 2006. A Corpus-based Ap-
proach to Finding Happiness. In Proceedings of the 
AAAI Spring Symposium on Computational Ap-
proaches to Weblogs.  
Ortony A., G. L. Clone, and A. Collins. 1988. The Cog-
nitive Structure of Emotions. New York: Cambridge 
University Press. 
Picard, R.W. 1995. Affective Computing. Cambridge. 
MA: The MIT Press. 
Plutchik, R. 1980. Emotions: A Psychoevolutionary 
Synthesis. New York: Harper & Row. 
Saur?, R., J. Littman, R. Knippen, R. Gaizauskas, A. 
Setzer, and J. Pustejovsky. 2004. TimeML Annota-
tion Guidelines. http://www.timeml.org. 
Spinoza, B. 1985. Ethics. In E. Curley, The Collected 
Works of Spinoza. Princeton, N.J.: Princeton Univer-
sity Press. Vol 1. 
Talmy, L. 2000. Toward a Cognitive Semantics. Vol. 
1and 2. Cambridge: MIT Press. 
Tokuhisa, R., K. Inui, and Y. Matsumoto. 2008. Emo-
tion Classification Using Massive Examples Ex-
tracted from the Web. In Proceedings of COLING.   
Turner, J. H. 1996. The Evolution of Emotions in Hu-
mans: A Darwinian-Durkheimian Analysis. Journal 
for the Theory of Social Behaviour, 26:1-34. 
Turner, J. H. 2000. On the Origins of Human Emotions: 
A Sociological Inquiry into the Evolution of Human 
Affect. California: Stanford University Press. 
Wierzbicka, A. 1999. Emotions Across Languages and 
Cultures: Diversity and Universals. Cambridge: 
Cambridge University Press. 
 
53
 Textual Emotion Processing From Event Analysis 
 
 
Chu-Ren Huang?, Ying Chen*?, Sophia Yat Mei Lee?  
?Department of Chinese and Bilingual Studies * Department of Computer Engineering 
The Hong Kong Polytechnic University China Agricultural University 
{churenhuang, chenying3176, sophiaym}@gmail.com 
  
 
 
 
 
 
 
Abstract 
Textual emotion recognition has gained a lot of 
attention recent years; it is however less devel-
oped due to the complexity nature of emotion. In 
this paper, we start with the discussion of a num-
ber of fundamental yet unresolved issues concern-
ing emotion, which includes its definition, 
representation and technology. We then propose 
an alternative solution for emotion recognition 
taking into account of emotion causes. Two pilot 
experiments are done to justify our proposal. The 
first experiment explores the impact of emotion 
recognition. It shows that the context contains rich 
and crucial information that effectively help emo-
tion recognition. The other experiment examines 
emotion cause events in the context. We find that 
most emotions are expressed with the presence of 
causes. The experiments prove that emotion cause 
serves as an important cue for emotion recognition. 
We suggest that the combination of both emotion 
study and event analysis would be a fruitful direc-
tion for deep emotion processing. 
1 Introduction 
The study of emotion attracts increasingly greater 
attention in the field of NLP due to its emerging 
wide applications, such as customer care (Gupta et 
al., 2010), and social information understanding 
(Lisa and Steyvers, 2010). In contrast to sentiment, 
which is the external subjective evaluation, emo-
tion mainly concentrates on the internal mental 
state of human (Ortony et al, 1987). Emotion is 
indeed a highly complicated concept that raises a 
lot of controversies in the theories of emotion re-
garding the fundamental issues such as emotion 
definition, emotion structure and so on. The com-
plexity nature of emotion concept makes auto-
matic emotion processing rather challenging. 
Most emotion studies put great effort on emo-
tion recognition, identifying emotion classes, such 
as happiness, sadness, and fear. On top of this 
surface level information, deeper level informa-
tion regarding emotions such as the experiencer, 
cause, and result of an emotion, needs to be ex-
tracted and analyzed for real world applications. 
In this paper, we discuss these two closely related 
emotion tasks, namely emotion recognition and 
emotion cause detection and how they contribute 
to emotion processing. 
For emotion recognition, we construct an emo-
tion corpus for explicit emotions with an unsuper-
vised method. Explicit emotions are emotions 
represented by emotion keywords such as e.g., 
?shocked? in ?He was shocked after hearing the 
news?.  In the course of emotion recognition, the 
keyword in an explicit emotion expression is de-
leted and only contextual information remains. In 
our pilot experiments, the context-based emotion 
identification works fairly well. This implies that 
plenty of information is provided in the context 
for emotion recognition. Moreover, with an in-
depth analysis of the data, we observe that it is 
often the case that emotions co-occur and interact 
in a sentence. In this paper, we deal with emotion 
recognition from a dependent view so as to cap-
ture complicated emotion expressions.   
Emotion is often invoked by an event, which in 
turn is very likely to elicit an event (Descartes 
1649, James 1884, Plutchik 1980, Wierzbicka 
1999). Despite the fact that most researches rec-
ognize the important role of events in emotion 
theories, little work, if not none, attempts to make 
explicit link between events and emotion. In this 
paper, we examine emotion constructions based 
on contextual information which often contains 
considerable relevant eventive information. In 
particular, the correlations between emotion and 
cause events will be explored based on empirical 
data. Emotion causes refer to explicitly expressed 
propositions that evoke the corresponding emo-
tions.  
To enhance emotion recognition, we examine 
emotion causes occurring in the context of an 
emotion. First, we manually annotate causes for 
emotions in our explicit emotion corpus. Since an 
emotion cause can be a complicated event, we 
model emotion cause detection as a multi-label 
problem to detect a cross-clause emotion cause. 
Furthermore, an in-depth linguistic analysis is 
done to capture the different constructions in ex-
pressing emotion causes.  
The paper is organized as follows. Section 2 
discusses some related work regarding emotion 
recognition and emotion cause detection. In Sec-
tion 3, we present our context-based emotion cor-
pus and provide some data analysis. Section 4 
describes our emotion recognition system, and 
discusses the experiments and results. In Section 5, 
we examine our emotion cause detection system, 
and discuss the performances. Finally, Section 6 
concludes our main findings for emotion process-
ing from the event perspective.   
2 Related Work  
Most current emotion studies focus on the task of 
emotion recognition, especially in affective lexi-
con construction. In comparison with emotion 
recognition, emotion cause detection is a rather 
new research area, which account for emotions 
based on the correlations between emotions and 
cause events. This section discusses the related 
research on emotion recognition and emotion 
cause detection. 
2.1 Emotion Recognition 
Although emotion recognition has been inten-
sively studied, some issues concerning emotion 
remain unresolved, such as emotion definition, 
emotion representation, and emotion classification 
technologies. 
For the emotion definition, emotion has been 
well-known for its abstract and uncertain defini-
tion which hinders emotion processing as a whole. 
Ortony et al, (1987) conducted an empirical study 
for a structure of affective lexicon based on the 
~500 words used in previous emotion studies. 
However, most of the emotion corpora in NLP try 
to avoid the emotion definition problem. Instead, 
they choose to rely on the intuition of annotators 
(Ren?s Blog Emotion Corpus, RBEC, Quan and 
Ren, 2009) or authors (Mishne?s blog emotion 
corpus, Mishne, 2005). Therefore, one of the cru-
cial drawbacks of emotion corpora is the problem 
of poor quality. In this paper, we explore emotion 
annotation from a different perspective. We con-
centrate on explicit emotions, and utilize their 
contextual information for emotion recognition.  
In terms of emotion representation, textual 
emotion corpora are basically annotated using ei-
ther the enumerative representation or the compo-
sitional representation (Chen et al, 2009). The 
enumerative representation assigns an emotion a 
unique label, such as pride and jealousy. The 
compositional representation represents an emo-
tion through a vector with a small set of fixed ba-
sic emotions with associated strength. For instance, 
pride is decomposed into ?happiness + fear? ac-
cording to Turner (2000).  
With regard to emotion recognition technolo-
gies, there are two kinds of classification models. 
One is based on an independent view (Mishne, 
2005; Mihalcea and Liu, 2006; Aman and Szpa-
kowicz, 2007; Tokuhisa et al, 2008; Strapparava 
and Mihalcea, 2008), and the other is a dependent 
view (Abbasi et al 2008; Keshtkar and Inkpen, 
2009). The independent view treats emotions sep-
arately, and often chooses a single-label classifica-
tion approach to identify emotions. In contrast, the 
dependent view takes into account complicated 
emotion expressions, such as emotion interaction 
and emotion co-occurrences, and thus requires 
more complicated models. Abbasi et al (2008) 
adopt an ensemble classifier to detect the co-
occurrences of different emotions; Keshtkar and 
Inkpen (2009) use iteratively single-label classifi-
ers in the top-down order of a given emotion hier-
archy. In this paper, we examine emotion 
recognition as a multi-label problem and investi-
gate several multi-label classification approaches.    
 2.2 Emotion Cause Detection 
Although most emotion theories recognize the 
important role of causes in emotion analysis (Des-
cartes, 1649; James, 1884; Plutchik, 1962; Wierz-
bicka 1996), yet very few studies in NLP explore 
the event composition and causal relation of emo-
tions. As a pilot study, the current study proposes 
an emotion cause detection system.  
Emotion cause detection can be considered as a 
kind of causal relation detection between two 
events. In other words, emotion is envisioned as 
an event type which triggers another event, i.e. 
cause event. We attempt to examine emotion 
cause relations for open domains. However, not 
much work (Marcu and Echihabi, 2002; Girju, 
2003; Chang and Choi, 2006) has been done on 
this kind of general causal relation for open do-
mains. 
Most existing causal relation detection systems 
contain two steps: 1) cause candidate identifica-
tion; 2) causal relation detection. However, Step 1) 
is often oversimplified in real systems. For exam-
ple, the cause-effect pairs are limited to two noun 
phrases (Chang and Choi, 2005; Girju, 2003), or 
two clauses connected with selected conjunction 
words (Marcu and Echihabi, 2002). Moreover, the 
task of Step 2) often is considered as a binary 
classification problem, i.e. ?causal? vs. ?non-
causal?.  
With regard to feature extraction, there are two 
kinds of information extracted to identify the 
causal relation in Step 2). One is constructions 
expressing a cause-effect relation (Chang and 
Choi, 2005; Girju, 2003), and the other is seman-
tic information in a text (Marcu and Echihabi, 
2002; Persing and Ng, 2009), such as word pair 
probability. Undoubtedly, the two kinds of infor-
mation often interact with each other in a real 
cause detection system. 
3 Emotion Annotated Sinica Corpus 
(EASC) 
EASC is an emotion annotated corpus comprising 
two kinds of sentences: emotional-sentence corpus 
and neutral-sentence corpus. It involves two com-
ponents: one for emotion recognition, which is 
created with an unsupervised method (Chen et al 
2009), and the other is for emotion cause detection, 
which is manually annotated (Chen et al 2010).  
3.1 The Corpus for Emotion Recognition 
With the help of a set of rules and a collection of 
high quality emotion keywords, a pattern-based 
approach is used to extract emotional sentences 
and neutral sentences from the Academia Sinica 
Balanced Corpus of Mandarin Chinese (Sinica 
Corpus). If an emotion keyword occurring in a 
sentence satisfies the given patterns, its corre-
sponding emotion type will be listed for that sen-
tence. As for emotion recognition, each detected 
keyword in a sentence is removed, in other words, 
the sentence provides only the context of that 
emotion. Due to the overwhelming of neutral sen-
tences, EASC only contains partial neutral sen-
tences besides emotional sentences. For 
experiments, 995 sentences are randomly selected 
for human annotation, which serve as the test data. 
The remaining 17,243 sentences are used as the 
training data.  
In addition, in the course of creating the emo-
tion corpus, Chen et al (2009) list the emotion 
labels in a sentence using the enumerative repre-
sentation. Besides, an emotion taxonomy is pro-
vided to re-annotate an emotion with the 
compositional representation. With the taxonomy, 
an emotion is decomposed into a combination of 
primary emotions (i.e. happiness, fear, anger, 
sadness, and surprise). 
From this corpus, we observe that ~54% emo-
tional sentences contain two emotions, yet only 
~2% sentences contain more than two emotions. 
This implies emotion recognition is a typical mul-
ti-label problem. Particularly, more effort should 
be put on the co-occurrences of two emotions. 
3.2 The Corpus for Emotion Cause De-
tection 
Most emotion theories agree that the five primary 
emotions (i.e. happiness, sadness, fear, anger, and 
surprise) are prototypical emotions. Therefore, for 
emotion cause detection, we only deal with the 
emotional sentences containing a keyword repre-
senting one of these primary emotions. Beyond a 
focus sentence, its context (the previous sentence 
and the following sentence) is also extracted, and 
those three sentences constitute an entry. After 
filtering non-emotional and ambiguous sentences, 
5,629 entries remain in the emotion cause corpus.  
Each emotion keyword is annotated with its 
corresponding causes if existing. An emotion 
keyword can sometimes be associated with more 
than one cause, in such a case, both causes are 
marked. Moreover, the cause type is also identi-
fied, which is either a nominal event or a verbal 
event (a verb or a nominalization).  
From the corpus, we notice that 72% of the ex-
tracted entries express emotions, and 80% of the 
emotional entries have a cause, which means that 
causal event is a strong indicator for emotion rec-
ognition.  
Furthermore, since the actual cause can some-
times be so complicated that it involves several 
events, we investigate the span of a cause text as 
follows. For each emotion keyword, an entry is 
segmented into clauses with some punctuations, 
and thus an entry becomes a list of cause candi-
dates. In terms of the cause distribution, we find 
~90% causes occurring between ?left_2? and 
?right_1?. Therefore, our cause search is limited to 
the list of cause candidates which contains five 
text units, i.e. <left_2, left_1, left_0, right_0, 
right_1>. If the clause where emotion keyword 
locates is assumed as a focus clause, ?left_2? and 
?left_1? are the two previous clauses, and ?right_1? 
is the following one. ?left_0? and ?right_0? are the 
partial texts of the focus clause, which locate in 
the left side of and the right side of the emotion 
keyword, respectively. Finally, we find that ~14% 
causes occur cross clauses. 
4 Emotion Processing with multi-label 
models   
4.1 Multi-label Classification for Emo-
tion recognition 
Based on our corpus, two critical issues for emo-
tion recognition need to be dealt with: emotion 
interaction and emotion co-occurrences. Co-
occurrence of multiple emotions in a sentence 
makes emotion recognition a multi-label problem. 
Furthermore, the interaction among different emo-
tions in a sentence requires a multi-label model to 
have a dependent view. In this paper, we explore 
two simple multi-label models for emotion recog-
nition. 
The Binary-based (BB) model: decompose the 
task into multiple independent binary classifiers 
(i.e., ?1? for the presence of one emotion; ?0? for 
the absence of one emotion), where each emotion 
is allocated a classifier. For each test instance, all 
labels (emotions) from the classifiers compose a 
vector. 
The label powset (LP) model: treat each possible 
combination of labels appearing in the training 
data as a unique label, and convert multi-label 
classification to single-label classification.  
Both the BB model and the LP model need a 
multi-class classifier. For our experiment, we 
choose a Max Entropy package, Mallet1. In this 
paper, we use only words in the focus sentence as 
features. 
4.2 Emotion Recognition Experiments 
To demonstrate the impact of our context-based 
emotion corpus to emotion recognition, we com-
pare EASC data to Ren?s Blog Emotion Corpus 
(RBEC). RBEC is a human-annotated emotion 
corpus for both explicit emotions and implicit 
emotions. It adopts the compositional representa-
tion with eight emotion dimensions (anger, anxi-
ety, expect, hate, joy, love, sorrow, and surprise). 
For each dimension, a numerical value ranging in 
{0.0, 0.1, 0.2... 1.0} indicates the intensity of the 
emotion in question. There are totally 35,096 sen-
tences in RBEC. To fairly compare with the 
EASC data, we convert a numerical value to a 
binary value. An emotion exists in a sentence only 
when its corresponding intensity value is greater 
than 0.  
For RBEC data, we use 80% of the corpus as 
the training data, 10% as the development data, 
and 10% as the test data. For EASC, apart from 
the test data, we divide its training data into two 
sets: 90% for our training data, and 10% for our 
development data. For evaluation of a multi-label 
task, three measures are used: accuracy (extract 
match ratio), Micro F1, and Macro F1. Accuracy 
is the extract match ratio of the whole assignments 
in data, and Micro F1 and Macro F1 are the aver- 
 
 
 
                                                           
1
 http://mallet.cs.umass.edu/ 
Table 1: The overall performances for the multi-label models   
 
 
 
 
 
 
 
 
age scores of F scores of all possible values for all 
variables. Micro F1 takes the emotion distribution 
into account, while Macro F1 is just the average 
of all F scores. Note that due to the overwhelming 
percentage of value 0 in the multi-label task, dur-
ing the calculating of Micro F1 and Macro F1, 
most previous multi-label systems take only value 
1 (indicating the existence of the emotion) into 
account. 
In Table 1, we notice that the emotion recogni-
tion system on our context-based corpus achieves 
similar performance as the one on human-
annotated corpus. This implies that there is rich 
contextual information with respect to emotion 
identification. 
5 Emotion Cause Detection 
Most emotion theories agree that there is a strong 
relationship between emotions and events (Des-
cartes 1649, James 1884, Plutchik 1980, Wierz-
bicka 1999). Among the rich information in the 
context of an emotion, cause event is the most 
crucial component of emotion. We therefore at-
tempt to explore emotion causes, and extract 
causes for emotion automatically.  
5.1 Emotion Cause Detection 
Based on the cause distribution analysis in Section 
3.2, in contrast to binary classification used in 
previous work, we formalize emotion cause detec-
tion as a multi-label problem as follows.  
Given an emotion keyword and its context, its 
label is the locations of its causes, such as ?left_1, 
left_0?. Then, we use the LP model to identify the 
cause for each sentence as well as an emotion 
keyword. With regard to emotion cause detection, 
the LP model is more suitable than the BB model 
because the LP model can easily capture the pos-
sible label combinations.  
   In terms of feature extraction, unlike emotion 
recognition, emotion cause detection relies more 
on linguistic constructions, such as ?The BP oil 
spill makes the country angry?, ?I am sad because 
of the oil spill problem? and so on. 
According to our linguistic analysis, we cre-
ate 14 patterns to extraction some common emo-
tion cause expressions. Some patterns are 
designed for general cause detection using linguis-
tic cues such as conjunctions and prepositions. 
Others are designed for some specific emotion 
cause expressions, such as epistemic markers and 
reported verbs. Furthermore, to avoid the low 
coverage problem of the rule-based patterns, we 
create another set of features, which is a group of 
generalized patterns. For details, please refer to 
Chen et al (2010).  
5.2 Experiments 
For EASC, we reserve 80% as the training data, 
10% as the development data, and 10% as the test 
data. For evaluation, we first convert a multi-label 
tag outputted from our system into a binary tag 
(?Y? means the presence of a causal relation; ?N? 
means the absence of a causal relation) between 
the emotion keyword and each candidate in its 
corresponding cause candidates. We then adopt 
three common measures, i.e. precision, recall and 
F-score, to evaluate the result. 
A naive baseline is designed as follows: The 
baseline searches for the cause candidates in the 
order of <left_1, right_0, left_2, left_0, right_1>. 
If the candidate contains a noun or a verb, this 
clause is considered as a cause and the search 
stops. 
Table 2 shows the overall performances of our 
emotion cause detection system. First, our system 
based on a multi-label approach as well as power-
ful linguistic features significantly outperforms 
the na?ve baseline. Moreover, the greatest im-
provement is attributed to the 14 linguistic pat-
terns (LP). This implies the importance of 
linguistic cues for cause detection. Moreover, the 
general patterns (GP) achieve much better per-
 EASC RBEC 
 BB LP BB LP 
Accuracy 21.30 28.07 22.99 28.33 
Micro F1 41.96 46.25 44.77 44.74  
Macro F1 34.78 35.52 36.48 38.88  
formance on the recall and yet slightly hurt on the 
precision. 
The performances (F-scores) for ?Y? and ?N? 
tags separately are shown in Table 3. First, we 
notice that the performances of the ?N? tag are 
much better than the ones of ?Y? tag. Second, it is 
surprising that incorporating the linguistic features 
significantly improves the ?Y? tag only (from 33% 
to 56%), but does not affect ?N? tag. This suggests 
that our linguistic features are effective to detect 
the presence of causal relation, and yet do not hurt 
the detections of ?non_causal? relation. Further-
more, the general feature achieves ~8% improve-
ments for the ?Y? tag. 
 
Table 2: The overall performance with different 
feature sets of the multi-label system 
 Precision Recall F-score 
Baseline 56.64 57.70 56.96 
LP 74.92 66.70 69.21 
+ GP 73.90 72.70 73.26 
 
Table 3: The separate performances for ?Y? and 
?N? tags of the multi-label system 
 ?Y? ?N? 
Baseline 33.06 80.85 
LP 48.32 90.11 
+ GP 56.84 89.68 
 
6 Discussions 
Many previous works on emotion recognition 
concentrated on emotion keyword detection. 
However, Ortony et al (1987) pointed out the dif-
ficulty of emotion keyword annotation, be it man-
ual or automatic annotation. Emotion keywords 
are rather ambiguous, and also contain other in-
formation besides affective information, such as 
behavior and cognition. Therefore, contextual in-
formation provides important cues for emotion 
recognition. Furthermore, we propose an alterna-
tive way to explore emotion recognition, which is 
based on emotion cause. Through two pilot ex-
periments, we justify the importance of emotion 
contextual information for emotion recognition, 
particularly emotion cause.  
We first examine emotion processing in terms 
of events. Context information is found to be very 
important for emotion recognition. Furthermore, 
most emotions are expressed with the presence of 
causes in context, which implies that emotion 
cause is the crucial information for emotion rec-
ognition. In addition, emotion cause detection also 
explores deep understanding of an emotion. Com-
pared to emotion recognition, emotion cause de-
tection requires more semantic and pragmatic 
information. In this paper, based on the in-depth 
linguistic analysis, we extract different kinds of 
constructs to identify cause events for an emotion.  
To conclude, emotion processing is a compli-
cated problem. In terms of emotion keywords, 
how to understand appropriately to enhance emo-
tion recognition needs more exploration. With 
respect to emotion causes, first, event processing 
itself is a challenging topic, such as event extrac-
tion and co-reference. Second, how to combine 
event and emotion in NLP is still unclear, but it is 
a direction for further emotion studies.  
References  
Abbasi, A., H. Chen, S. Thoms, and T. Fu. 2008. Af-
fect Analysis of Web Forums and Blogs using Cor-
relation Ensembles?. In IEEE Tran. Knowledge and 
Data Engineering, vol. 20(9), pp. 1168-1180. 
Aman, S. and S. Szpakowicz. 2007. Identifying Ex-
pressions of Emotion in Text. In Proceedings of 
10th International Conference on Text, Speech and 
Dialogue, Lecture Notes in Computer Science 4629, 
196--205.   
Chang, D.-S. and K.-S. Choi. 2006. Incremental cue 
phrase learning and bootstrapping method for cau-
sality extraction using cue phrase and word pair 
probabilities. Information Processing and Man-
agement. 42(3): 662-678. 
Chen, Y., S. Y. M. Lee and C.-R. Huang. 2009. Are 
Emotions Enumerable or Decomposable? And Its 
Implications for Emotion Processing. In Proceed-
ings of the 23rd Pacific Asia Conference on Lan-
guage, Information and Computation.  
Chen, Y., Y. M. Lee, S. Li and C.-R. Huang. 2010. 
Emotion Cause Detection with Linguistic Construc-
tions. In Proceedings of the 23rd International 
Conference on Computational Linguistics. 
Descartes, R. 1649. The Passions of the Soul. USA: 
Hackett Publishing Company. 
Ghazi, D., D. Inkpen and S. Szpakowicz. 2010. Hierar-
chical versus Flat Classification of Emotions in Text. 
In Proceedings of NAACL-HLT 2010 Workshop on 
Computational Approaches to Analysis and Genera-
tion of Emotion in Text. Los Angeles, CA: NAACL. 
Girju, R. 2003. Automatic Detection of Causal Rela-
tions for Question Answering. In the 41st Annual 
Meeting of the Association for Computational Lin-
guistics, Workshop on Multilingual Summarization 
and Question Answering - Machine Learning and 
Beyond, Sapporo, Japan. 
Gupta, N., M. Gilbert, and G. D. Fabbrizio. Emotion 
Detection in Email Customer Care. In Proceedings 
of NAACL-HLT 2010 Workshop on Computational 
Approaches to Analysis and Generation of Emotion 
in Text. 
James, W. 1884. What is an Emotion? Mind, 9(34): 
188?205. 
Keshtkar, F. and D. Inkpen. 2009. Using Sentiment 
Orientation Features for Mood Classification in 
Blog Corpus. In Proceedings of IEEE International 
Conference on Natural Language Processing and 
Knowledge Eng. (IEEE NLP-KE 2009), Sep. 24-27. 
Marcu, D. and A. Echihabi. 2002. An Unsupervised 
Approach to Recognizing Discourse Relations. In 
Proceedings of ACL. 
Mihalcea, R., and H. Liu. 2006. A Corpus-based Ap-
proach to Finding Happiness. In Proceedings of 
AAAI. 
Mishne, G. 2005. Experiments with Mood Classifica-
tion in Blog Posts. In Proceedings of Style2005 ? the 
1st Workshop on Stylistic Analysis of Text for Infor-
mation Access, at SIGIR 2005. 
Ortony, A., G. L. Clore, and M. A. Foss. 1987. The 
Referential Structure of the Affective Lexicon. Cog-
nitive Science, 11: 341-364. 
Pang B., L. Lee and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment Classification Using Machine Learn-
ing Techniques. In Proceedings of EMNLP02, 79-86. 
Pearl, L. and M. Steyvers. 2010. Identifying Emotions, 
Intentions, and Attitudes in Text Using a Game with 
a Purpose. In Proceedings of NAACL-HLT 2010 
Workshop on Computational Approaches to Analy-
sis and Generation of Emotion in Text. Los Angeles, 
CA: NAACL. 
Persing, Isaac and Vincent Ng. 2009. Semi-Supervised 
Cause Identification from Aviation Safety Reports. 
In Proceedings of ACL. 
Plutchik, R. 1980. Emotions: A Psychoevolutionary 
Synthesis. New York: Harper & Row. 
Quan, C. and F. Ren. 2009. Construction of a Blog 
Emotion Corpus for Chinese Expression Analysis. 
In Proceedings of EMNLP. 
Strapparava, C. and R. Mihalcea. 2008. Learning to 
Identify Emotions in Text. In Proceedings of the 
ACM Conference on Applied Computing ACM-SAC. 
Tokuhisa, R., K. Inui, and Y. Matsumoto. 2008. Emo-
tion Classification Using Massive Examples Ex-
tracted from the Web. In Proceedings of COLING.   
Turner, J. H. 2000. On the Origins of Human Emotions: 
A Sociological Inquiry into the Evolution of Human 
Affect. California: Stanford University Press. 
Wierzbicka, A. 1999. Emotions across Languages and 
Cultures: Diversity and Universals. Cambridge: 
Cambridge University Press. 
  
 The Chinese Persons Name Disambiguation Evaluation: Exploration of 
Personal Name Disambiguation in Chinese News 
 
 
Ying Chen*, Peng Jin?, Wenjie Li?,Chu-Ren Huang? 
* China Agricultural University ?Leshan Teachers? College ?The Hong Kong Polytechnic University 
chenying3176@gmail.com jandp@pku.edu.cn cswjli@comp.polyu.edu.hk 
  churenhuang@gmail.com 
   
 
 
 
 
 
 
Abstract 
Personal name disambiguation becomes hot as it 
provides a way to incorporate semantic under-
standing into information retrieval. In this cam-
paign, we explore Chinese personal name 
disambiguation in news. In order to examine how 
well disambiguation technologies work, we con-
centrate on news articles, which is well-formatted 
and whose genre is well-studied. We then design a 
diagnosis test to explore the impact of Chinese 
word segmentation to personal name disambigua-
tion. 
1 Introduction 
Incorporating semantic understanding technolo-
gies from the field of NLP becomes one of further 
directions for information retrieval. Among them, 
named entity disambiguation, which intends to 
use state-of-the-art named entity processing to 
enhance a search engine, is a hot research issue. 
Because of the popularity of personal names in 
queries, more efforts are put on personal name 
disambiguation. The personal name disambigua-
tion used both in Web Personal Search (WePS1) 
and our campaign is defined as follow. Given 
documents containing a personal name in interest, 
the task is to cluster them according to which en-
tity the name in a document refers to.  
WePS, which explores English personal name 
disambiguation, has been held twice (Artiles et al, 
                                                           
1
 http://nlp.uned.es/weps/ 
2007, 2009). Compared to the one in English, per-
sonal name disambiguation in Chinese has special 
issues, such as Chinese text processing and Chi-
nese personal naming system. Therefore, we hold 
Chinese personal name disambiguation (CPND) to 
explore those problems. In this campaign, we 
mainly examine the relationships between Chinese 
word segmentation and Chinese personal name 
disambiguation.  
Moreover, from our experiences in WePS 
(Chen et al, 2007, 2009), we notice that webpages 
are so noisy that text pre-processing that extracts 
useful text for disambiguation needs much effort. 
In fact, text pre-processing for webpages is rather 
complicated, such as deleting of HTML tags, the 
detection of JavaScript codes and so on. Therefore, 
the final system performance in the WePS cam-
paign sometimes does not reflect the disambigua-
tion power of the system, and instead it shows the 
comprehensive result of text pre-processing as 
well as disambiguation. In order to focus on per-
sonal name disambiguation, we choose news 
documents in CPND. 
The paper is organized as follows. Section 2 de-
scribes our formal test including datasets and 
evaluation. Section 3 introduces the diagnosis test, 
which explores the impact of Chinese word seg-
mentation to personal name disambiguation. Sec-
tion 4 describes our campaign, and Section 5 
presents the results of the participating systems. 
Finally, Section 6 concludes our main findings in 
this campaign. 
 
2 The Formal Test 
2.1 Datasets 
To avoid the difficulty to clean a webpage, we 
choose news articles in this campaign. Given a 
full name in Chinese, we search the character-
based personal name string in all documents of 
Chinese Gigaword Corpus, a large Chinese news 
collection. If a document contains the name, it is 
belonged to the dataset of this name. To ensure 
the popularity of a personal name, we keep only a 
personal name whose corresponding dataset com-
prises more than 100 documents. In addition, if 
there are more than 300 documents in that dataset, 
we randomly select 300 articles to annotate. Fi-
nally, there are totally 58 personal names and 
12,534 news articles used in our data, where 32 
names are in the development data and 26 names 
are in the test data, as shown Appendix Table 4 
and 5 separately.   
From Table 4 and 5, we can find that the ambi-
guity (the document number per cluster) distribu-
tion is much different between the development 
data and the test data.  In fact, the ambiguity var-
ies with a personal name in interest, such as the 
popularity of the name in the given corpus, the 
celebrity degree of the name, and so on.    
2.2 Evaluation  
In WePS, Artiles et al (2009) made an intensive 
study of clustering evaluation metrics, and found 
that B-Cubed metric is an appropriate evaluation 
approach. Moreover, in order to handle overlap-
ping clusters (i.e. a personal name in a document 
refers to more than one person entity in reality), 
we extend B-Cubed metric as Table 1, where S = 
{S1, S2, ?} is a system clustering  and R = {R1, 
R2, ?} is a gold-standard clustering. The final 
performance of a system clustering for a personal 
name is the F score (?= 0.5), and the final per-
formance of a system is the Mac F score, the aver-
age of the F scores of all personal names. 
Moreover, Artiles et al (2009) also discuss 
three cheat systems: one-in-one, all-in-one, and 
the hybrid cheat system. One-in-one assigns each 
document into a cluster, and in contrast, all-in-one 
put all documents into one cluster. The hybrid 
cheat system just incorporates all clusters both in 
one-in-one and all-in-one clustering. Although the 
hybrid cheat system can achieve fairly good per-
formance, it is not useful for real applications. In 
the formal test, these three systems serve as the 
baseline.  
 
 Formula 
 
Precision 
?
? ?
?
? ? ??
?
S
S d dR
 i
 i i j j
S
i
S S i
ji
R;R
|S|
|S|
|RS|
 
max
 
 
Recall 
?
? ?
?
? ? ??
?
R
R d dS
 i
 i i j j
R
i
R i
ji
R S;S
|R|
|R|
|SR|
 
max
 
Table 1: the formula of the modified B-cubed 
metrics 
3 The Diagnosis Test 
Because of no word delimiter, Chinese text proc-
essing often needs to do Chinese word segmenta-
tion first. In order to explore the relationship 
between personal name disambiguation and word 
segmentation, we provide a diagnosis data which 
attempts to examine the impact of word segmenta-
tion to disambiguation.  
Firstly, for each personal name, its correspond-
ing dataset will be manually divided into three 
groups as follows. The disambiguation system 
then runs for each group of documents. The three 
clustering outputs are merged into the final clus-
tering for that personal name.  
(1) Exactly matching: news articles contain-
ing personal names that exactly match 
the query personal name. 
(2) Partially matching: news articles contain-
ing personal names that are super-strings 
of the query personal name. For instance, 
an article that has a person named with 
????? (Gao Jun Tian)  is retrieved 
for the query personal name ???? (Gao 
Jun).  
(3) Discarded: news articles containing 
character sequences that match the query 
personal name string and however in fact 
are not a personal name. For instance, an 
article that has the string ??????
?? (Zui Gao Jun Shi Fa Yuan: supreme 
military court) is also retrieved for the  
personal name ???? (Gao Jun).  
 
This diagnosis test is designed to simulate the 
realistic scenario where Chinese word segmenta-
tion works before personal name disambiguation. 
If a Chinese word segmenter works perfectly, a 
word-based matching can be used to retrieve the 
documents containing a personal name, and arti-
cles in Groups (2) and (3) should not be returned. 
The personal name disambiguation task that is 
limited to the documents in Group (1) should be 
simpler. 
Moreover, in this diagnosis test, we propose a 
baseline based on the gold-standard word segmen-
tation as follows, namely the word-segment sys-
tem.  
1) All articles in the ?exactly matching? 
group are merged into a cluster, and all 
articles in the ?discarded? group are 
merged into a cluster. 
2) In the ?partially matching? group, enti-
ties exactly sharing the same personal 
name are merged into a cluster.  For ex-
ample, all articles containing ????? 
(Gao Jun Tian) are merged into a cluster, 
and all articles containing???? (Gao 
Jun Hua) are merged into another cluster. 
4 Campaign Design  
4.1 The Participants 
The task of Chinese personal name disambigua-
tion in news has attracted the participation of 10 
teams. As a team can submit at most 2 results, 
there are 17 submissions from the 10 teams in the 
formal test, and there are 11 submissions from 7 
teams in the diagnosis.  
4.2 System descriptions 
Regarding system architecture, all systems are 
based on clustering, and most of them comprise 
two components: feature extraction and clustering. 
However, NEU-1 and HITSZ_CITYU develop a 
different clustering, which in fact is a cascaded 
clustering. Taking the advantage of the properties 
of a news article, both systems first divide the 
dataset for a personal name into two groups ac-
cording to whether the person in question is a re-
porter of the news. They then choose a different 
strategy to make further clustering for each group.    
In terms of feature extraction, we find that all 
systems except SoochowHY use word segmenta-
tion as pre-processing. Moreover, most systems 
choose named entity detection to enhance their 
feature extraction. In addition, character-based 
bigrams are also used in some systems.  In Ap-
pendix Table 6, we give the summary of word 
segmentation and named entity detection used in 
the participating systems. 
Regarding clustering algorithms, agglomerative 
hierarchical clustering is popular in the submis-
sions. Moreover, we find that weight learning is 
very crucial for similarity matrix, which has a big 
impact to the final clustering performance. Be-
sides the popular Boolean and TFIDF weighting 
schemes, SoochowHY and NEU-2 use different 
weighting learning. NEU-2 manually assigns 
weights to different kinds of features. So-
ochowHY develops an algorithm that iteratively 
learns a weight for a character-based n-gram.  
5 Results  
We first provide the performances of the formal 
test, and make some analysis. We then present and 
discuss the performances of the diagnosis test.  
5.1 Results of the Formal test 
For the formal test, we show the performances of 
11 submissions from 10 teams in Table 2. For 
each team, we keep only the better result except 
the NEU team because they use different tech-
nologies in their two submissions (NEU_1 and 
NEU_2).  
From Table 2, we first observe that 7 submis-
sions perform better than the hybrid cheat system. 
In contrast, in Artiles et al (2009), only 3 teams 
can beat the hybrid system. From our analysis, 
this may attribute to the following facts.  
1) Personal name disambiguation on Chinese 
may be easier than the one on English. For 
example, one of key issues in personal name 
disambiguation is to capture the occurrences 
of a query name in text. However, various 
personal name expressions, such as the use of 
 Precision Recall Macro F  
NEU_1 95.76 88.37 91.47 
NEU_2 95.08 88.62 91.15 
HITSZ_CITYU 83.99 93.29 87.42 
ICL_1 83.68 92.23 86.94 
DLUT_1 82.69 91.33 86.36 
BUPT_1 80.33 94.52 85.79 
XMU 90.55 84.88 85.72 
Hybrid cheat system 73.48 100 82.37 
HIT_ITNLP_2 91.08 62.75 71.03 
BIT 80.2 68.75 68.4 
ALL_IN_ONE 52.54 100 61.74 
BUPT_pris02 72.39 58.35 57.68 
SoochowHY_2 84.51 44.17 51.42 
ONE_IN_ONE 94.42 14.41 21.07 
Table 2: The B-Cubed performances of the formal test  
  
 Precision Recall Macro F  
NEU_1 95.6 89.74 92.14 
NEU_2 94.53 89.99 91.66 
XMU 89.84 89.84 89.08 
ICL_1 84.53 93.42 87.96 
BUPT_1 80.43 95.41 86.18 
Word_segment system 71.11 100 80.92 
BUPT_pris01 77.91 75.09 74.25 
BIT 94.62 63.32 72.48 
SoochowHY 87.22 58.52 61.85 
Table 3: The B-Cubed performances of the diagnosis test   
 
middle names in English, cause many prob-
lems during recognizing of the occurrences 
of a personal name in interest. 
2) We works on news articles, which have less 
noisy information compared to webpages 
used in Artiles et al (2009). More efforts are 
put on the exploration directly on disam-
biguation, not on text pre-processing. Fur-
thermore, most of systems extract features 
based on some popular NLP techniques, such 
as Chinese word segmentation, named entity 
recognition and POS tagger. As those tools 
usually are developed based on news corpora, 
they should extract high-quality features for 
disambiguation in our task.  
 
We then notice that the NEU team achieves the 
best performance. From their system description, 
we find that they make some special processing 
just for this task. For example, they develop a per-
sonal name recognition system to detect the occur-
rences of a query name in a news article, and a 
cascaded clustering for different kinds of persons. 
5.2 Results of the Diagnosis test 
We present the performances of 8 submissions for 
the diagnosis test from 7 teams in Table 3 as the 
format of Table 2. Meanwhile, we use the word-
segment system as the baseline.  
  Comparing Table 2 and 3, we first find that the 
word-segment system has a lower performance 
than the hybrid cheat system although the word-
segment system is more useful for real applica-
tions. This implies the importance to develop an 
appropriate evaluation method for clustering. 
From Table 3, five submissions achieve better 
performances than the word-segment system.  
Given the gold-standard word segmentation on 
personal names in the diagnosis test, from Table 3, 
our total impression is that the top systems take 
less advantages, and the bottom systems take 
more. This indicates that bottom systems suffer 
from their low-quality word segmentation and 
named entity detection. For example, 
BUPT_pris01 increases ~22% F score (from 
52.81% to 74.25%). 
6 Conclusions 
This campaign follows the work of WePS, and 
explores Chinese personal name disambiguation 
on news. We examine two issues: one is for Chi-
nese word segmentation, and the other is noisy 
information. As Chinese word segmentation usu-
ally is a pre-processing for most NLP processing, 
we investigate the impact of word segmentation to 
disambiguation. To avoid noisy information for 
disambiguation, such as HTML tags in webpage 
used in WePS, we choose news article to work on 
so that we can capture how good the state-of-the-
art disambiguation technique is. 
References  
 Artiles, Javier, Julio Gonzalo and Satoshi Sekine.2007. 
The SemEval-2007 WePS Evaluation: Establishing 
a benchmark for the Web People Search Task. In 
Proceedings of Semeval 2007, Association for Com-
putational Linguistics. 
Artiles, Javier, Julio Gonzalo and Satoshi Sekine. 2009. 
WePS 2 Evaluation Campaign: overview of the Web 
People Search Clustering Task. In 2nd Web People 
Search Evaluation Workshop (WePS 2009), 18th 
WWW Conference. 
Bagga, Amit and Breck Baldwin.1998. Entity-based 
Cross-document Co-referencing Using the Vector 
Space Model. In Proceedings of the 17th Interna-
tional Conference on Computational Linguistics.  
Chen, Ying and James H. Martin. 2007. CU-COMSEM: 
Exploring Rich Features for Unsupervised Web Per-
sonal Name Disambiguation. In Proceedings of Se-
meval 2007, Association for Computational 
Linguistics.  
Chen, Ying, Sophia Yat Mei Lee and Chu-Ren Huang. 
2009. PolyUHK: A Robust Information Extraction 
System for Web Personal Names. In 2nd Web Peo-
ple Search Evaluation Workshop (WePS 2009), 18th 
WWW Conference.  
 
 
Appendix 
 
name document #  cluster # document #  per cluster 
?? 
155 37 4.19 
?? 
301 42 7.17 
?? 
300 5 60 
?? 
105 30 3.5 
?? 
156 42 3.71 
??? 
350 15 23.33 
?? 
269 70 3.84 
?? 
257 8 32.13 
?? 
211 109 1.94 
?? 
177 36 4.92 
?? 
358 165 2.17 
?? 
300 20 15 
?? 
140 57 2.46 
?? 
300 27 11.11 
?? 
296 73 4.05 
?? 
135 75 1.8 
?? 
297 14 21.21 
?? 
110 24 4.58 
?? 
207 68 3.04 
?? 
131 26 5.04 
?? 
145 22 6.59 
?? 
164 15 10.93 
??? 
247 20 12.35 
?? 
173 34 5.09 
??? 
171 21 8.14 
?? 
170 34 5 
?? 
195 32 6.09 
?? 
301 22 13.68 
?? 
318 76 4.18 
?? 
234 117 2 
?? 
134 9 14.89 
?? 
123 7 17.57 
 
6930 1352 5.13 
Table 4: The training data distribution 
 
 
name document #  cluster # document #  per cluster 
?? 
190 96 1.99 
?? 
191 5 38.2 
?? 
258 16 16.13 
??? 
224 32 7 
??? 
118 29 4.07 
?? 
239 21 11.38 
?? 
208 43 4.84 
??? 
201 17 11.82 
??? 
317 3 105.67 
??? 
151 6 25.17 
?? 
188 61 3.08 
??? 
200 2 100 
?? 
213 69 3.09 
??? 
182 5 36.4 
?? 
278 11 25.27 
?? 
180 4 45 
??? 
286 1 286 
?? 
206 38 5.42 
?? 
193 16 12.06 
??? 
172 9 19.11 
?? 
174 5 34.8 
?? 
299 39 7.67 
?? 
233 90 2.59 
?? 
300 13 23.08 
?? 
141 25 5.64 
??? 
262 13 20.15 
 
5604 669 8.38 
Table5: The test data distribution 
 
 
 Word segmentation Named Entity 
NEU Name: Neucsp 
Source: 1998 People's Daily   
Name: in-house 
HITSZ_CITYU   
ICL Name: LTP 
F score: 96.5% 
Source:  2nd SIGHAN 
Name: LTP   
 
DLUT   
BUPT Name: in-house 
F score: 96.5% 
Source: SIGHAN 2010 
 
XMU Name: in-house 
Source: 1998 People's Daily 
F score: 97.8% 
 
HIT_ITNLP Name: IRLAS 
Source: 1998 People's Daily 
F score: 97.4% 
Name: IRLAS 
 
BIT Name: ICTCLAS2010   
Precision: ~97% 
Source: 1998 People's Daily   
Name: ICTCLAS2010   
 
BUPT_pris Name: LTP 
 
Name: LTP 
SoochowHY None None 
Table 6: The summary of word segmentation and named entity detection used in the participants 
 
* LTP(Language Technology Platform) 
 
