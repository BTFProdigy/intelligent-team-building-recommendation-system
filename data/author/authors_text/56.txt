Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 417?424
Manchester, August 2008
A Local Alignment Kernel in the Context of NLP
Sophia Katrenko
Informatics Institute
University of Amsterdam
the Netherlands
katrenko@science.uva.nl
Pieter Adriaans
Informatics Institute
University of Amsterdam
the Netherlands
pietera@science.uva.nl
Abstract
This paper discusses local alignment ker-
nels in the context of the relation extrac-
tion task. We define a local alignment
kernel based on the Smith-Waterman mea-
sure as a sequence similarity metric and
proceed with a range of possibilities for
computing a similarity between elements
of sequences. We propose to use distri-
butional similarity measures on elements
and by doing so we are able to incorporate
extra information from the unlabeled data
into a learning task. Our experiments sug-
gest that a LA kernel provides promising
results on some biomedical corpora largely
outperforming a baseline.
1 Introduction
Relation extraction is one of the tasks in the
natural language processing which is constantly
revisited. To date, there are many methods
which have been proposed to tackle it. Such ap-
proaches often benefit from using syntactic infor-
mation (Bunescu and Mooney, 2006) and back-
ground knowledge (Sekimizu et al, 1998). How-
ever, it would be interesting to employ additional
information not necessarily contained in the train-
ing set. This paper presents a contribution to the
work on relation extraction by combining statisti-
cal information with string distance measures. In
particular, we propose to use a local alignment ker-
nel to detect relations.
The paper is organized as follows. We start with
the definition of a local alignment kernel and show
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
how it is defined on the Smith-Waterman measure.
We proceed by discussing how a substitution ma-
trix can be constructed in the context of natural
language processing tasks. Once a method is de-
scribed, we turn to the task of relation extraction
and present an experimental part. We conclude by
mentioning possible future directions.
2 A Local Aligment Kernel
Kernel methods are widely used for a variety of
natural language processing task, starting from
PoS tagging to information extraction. Many of
the approaches employ the idea of combining ker-
nels together which leads to a convolution kernel
(Haussler, 1999). The examples of convolution
methods being successfully used in NLP are ker-
nels based on dependency trees and shallow pars-
ing (Moschitti, 2006; Zelenko et al, 2003). Local
alignment (LA) kernels also belong to the family
of convolution kernels but have not yet been ap-
plied to NLP problems.
Although the approaches listed above proved to
be accurate, they only use kernels which are de-
signed by computing inner products between vec-
tors of sequences. Intuitively, methods using more
elaborate measures of similarity could provide bet-
ter results but kernels defined on such measures
are not necessarily positive semi-definite. Recent
work in the biomedical field shows that it is pos-
sible to design valid kernels based on a similarity
measure by solving the diagonal dominance prob-
lem to ensure the semi-definiteness (Saigo et al,
2006). To illustrate it, Saigo et al (2004) con-
sider the Smith-Waterman (SW) similarity mea-
sure (Smith and Waterman, 1981) which has of-
ten been used to compare two sequences of amino
acids. The original Smith-Waterman score is cal-
culated to achieve the best local alignment allow-
417
ing gaps.
The Smith-Waterman measure belongs to the
string distance metrics which can be divided into
term-based, edit-distance and HMM based metrics
(Cohen et al, 2003). Term-based distances such
as metrics based on TF-IDF score, consider a pair
of word sequences as two sets of words neglecting
their order. In contrast, edit string distances treat
the entire sequences and, by comparing them, cal-
culate the minimal number of the transformation
operations converting a sequence x into a sequence
y. Examples of string edit distances are Leven-
shtein, Needleman-Wunsch and Smith-Waterman
metrics. Levenshtein distance has been used in
natural language processing field as a component
in the variety of tasks, including semantic role la-
beling (Tjong Kim Sang et al, 2005), construc-
tion of the paraphrase corpora (Dolan et al, 2004),
evaluation of machine translation output (Leusch
et al, 2003), and others. Smith-Waterman distance
is mostly used in the biological domain, there are,
however, some applications of a modified Smith-
Waterman distance to the text data as well (Monge
and Elkan, 1996), (Cohen et al, 2003). HMM
based measures present probabilistic extensions of
edit distances.
According to the definition of a LA kernel,
two strings (sequences) are considered similar if
they have many local alignments with high scores
(Saigo et al, 2006). Given two sequences x =
x
1
x
2
. . . x
n
and y = y
1
y
2
. . . y
m
of length n and m
respectively, Smith-Waterman distance is defined
as the local alignment score of their best align-
ment:
SW (x, y) = max
pi?A(x,y)
s(x, y, pi) (1)
In the equation above, s(x, y, pi) is a score of a
local alignment pi of sequence x and y and A de-
notes the set of all possible alignments. This defi-
nition can be rewritten by means of dynamic pro-
gramming as follows:
SW (i, j) = max
8
>
<
>
:
0
SW (i? 1, j ? 1) + d(x
i
, y
j
)
SW (i? 1, j)?G
SW (i, j ? 1)?G
(2)
In Equation 2, d(x
i
, y
j
) denotes a substitution
score between two elements x
i
and y
j
and G
stands for a gap penalty.
Unfortunately, the direct application of the
Smith-Waterman score will not result in the valid
kernel. A valid kernel based on the Smith-
Waterman distance can be defined by summing up
the contribution of all possible alignments as fol-
lows (Saigo et al, 2004):
K
LA
=
?
pi?A(x,y)
?
??s(x,y,pi) (3)
It is shown that in the limit a LA kernel ap-
proaches the Smith-Waterman score:
lim
???
ln
(
1
?
K
LA
(x, y)
)
= SW (x, y) (4)
The results in the biological domain suggest that
kernels based on the Smith-Waterman distance are
more relevant for the comparison of amino acids
than string kernels. It is not clear whether this
holds when applied to natural language process-
ing tasks. In our view, it depends on the param-
eters which are used, such as a substitution ma-
trix and the penalty gaps. It has been shown by
Saigo (2006) that given a substitution matrix which
is equal to the identity matrix and no penalty gap,
the Smith-Waterman score is a string kernel.
2.1 How to define a substitution matrix
d(?, ?)?
In order to use Smith-Waterman distance for our
purposes, it is necessary to define a substitution
matrix. Unlike a matrix in the original Smith-
Waterman measure defined by the similarity of
amino acids or a substitution matrix in (Monge and
Elkan, 1996) based on the exact and approximate
match of two characters (for instance, m and n),
we introduce a matrix based on the distributional
similarity measures. In our view, they are the most
natural measures for the text data. In other words,
if we are to compare any two words given two se-
quences of words, the elements sharing the same
contexts should be more similar to each other than
those that do not. In the context of the LA kernel,
such metrics can be especially useful. Consider,
for instance, the labeled sequences of words which
are used as input for a machine learning method.
To compare the sequences, we have to be able to
compare their elements, i.e. words. Now, if there
are some words in the test data that do not occur
in the training set, it is still possible to carry out a
418
comparison if additional evidence is present. Such
evidence can be provided by the distributional sim-
ilarity metrics.
There are a number of measures proposed over
the years, including such metrics as cosine, dice
coefficient, and Jaccard distance. Distributional
similarity measures have been extensively studied
in (Lee, 1999; Weeds et al, 2004).
We have chosen the following metrics: dice, co-
sine and l2 (euclidean) whose definitions are given
in Table 1. Here, x
i
and y
j
denote two words and
c stands for a context. Similarly to (Lee, 1999),
we use unsmoothed relative frequencies to derive
probability estimates P . In the definition of the
dice coefficient, F (x
i
) = {c : P (c|x
i
) > 0}.
We are mainly interested in the symmetric mea-
sures (d(x
i
, y
j
) = d(y
j
, x
i
)) because of a symmet-
ric positive semi-definite matrix required by ker-
nel methods. Consequently, such measures as the
skew divergence were excluded from the consider-
ation (Lee, 1999).
The Euclidean measure as defined in Table 1
does not necessarily vary from 0 to 1. It was there-
fore normalized by dividing an l2 score in Table 1
by a maximum score and retracting it from 1.
Measure Formula
cosine d(x
i
, y
j
) =
P
c
P (c|x
i
)?P (c|y
j
)
?
P
c
P (c|x
i
)
2
P
c
P (c|y
j
)
2
dice d(x
i
, y
j
) =
2?F (x
i
)?F (y
j
)
F (x
i
)?F (y
j
)
l2 d(x
i
, y
j
) =
p
P
c
(P (c|x
i
)? P (c|y
j
))
2
Table 1: Distributional similarity measures.
3 A relation extraction task
Many approaches to relation extraction consider
syntactic information. In this paper we focus on
dependency parsing. The experiments in the past
have already shown syntactic analysis to be useful
for relation learning. Like other work we extract
a path between two nodes which correspond to the
arguments of a binary relation. We also assume
that each analysis results in a tree and since it is an
acyclic graph, there exists only one path between
each pair of nodes. We do not consider, however,
the other structures that might be derived from the
full syntactic analysis as in, for example, subtree
kernels (Moschitti, 2006).
Consider, for instance, an example of interac-
tion among proteins (5) whose syntactic analysis
is given in Fig. 1. Here, there is a relation between
Cbf3 and three proteins, Cbf3a, Cbf3b and Cbf3c
expressed by a verb contain. We believe that this
partial information extracted from the dependency
trees should be sufficient for relation learning and
can be used as a representation for the learning
method.
(5) Cbf3 contains three proteins, Cbf3a, Cbf3b
and Cbf3c.
contains
nsubj
dobj
Cbf3 proteins
conj and
num
conj and
conj and
Cbf3a three Cbf3b Cbf3c
Figure 1: Stanford parser output
Representation: dependency paths
Cfb3 nsubj? contains dobj? proteins conj and? Cbf3a
Cfb3 nsubj? contains dobj? proteins conj and? Cbf3b
Cfb3 nsubj? contains dobj? proteins conj and? Cbf3c
4 Experiments
4.1 Set-up
Data We use two corpora which both come from
the biomedical field and contain annotations of
either interacting proteins BC-PPI (1,000 sen-
tences)1 or the interactions among proteins and
genes LLL2 (77 sentences in the training set and
87 in the test set) (Ne?dellec, 2005). The BC-PPI
corpus was created by sampling sentences from the
BioCreAtive challenge, the LLL corpus was com-
posed by querying Medline with the term Bacillus
subtilis. The difference between the two corpora
lies in the directionality of interactions. The for-
mer corpus contains both symmetric and asymmet-
ric interactions while in the latter they are strictly
asymmetric. We analyzed the BC corpus with the
Stanford parser. 3 The LLL corpus has already
been preprocessed by the Link parser.
To estimate distributional similarity, we use
TREC 2006 Genomics collection (Hersch,
2006) which contains 162,259 documents from
1available from http://www2.informatik.
hu-berlin.de/?hakenber/
2available from http://genome.jouy.inra.fr/
texte/LLLchallenge/
3available from http://nlp.stanford.edu/
software/lex-parser.shtml\#Download
419
49 journals. All documents have been prepro-
cessed by removing HTML-tags, citations in the
text and reference sections and stemmed by the
Porter stemmer (van Rijsbergen et al, 1980).
Furthermore, the query-likelihood approach with
Dirichlet smoothing (Chen, 1996) is used to
retrieve document passages given a query. All
words occurring in the set of input sequences are
fed as queries. Immediate context surrounding
each pair of words is used as features to calculate
distributional similarity of these words. We set the
context window to ?2 (2 tokens to the right and 2
tokens to the left of a word in focus) and do not
perform any kind of further preprocessing such as
PoS tagging.
Recall that in Section 2.1 we defined a substi-
tution matrix solely based on the words. How-
ever, the representation we employ also contains
information on syntactic functions and directions
(Fig. 1). To take this into account, we revise
the definition of d(?, ?). We assume sequences
x = x
1
x
2
. . . x
n
and y = y
1
y
2
. . . y
m
to contain
words (x
i
?W ) and syntactic functions accompa-
nied by direction (x
i
/?W ). Then,
d
?
(x
i
, y
j
) =
8
>
>
<
>
>
:
d(x
i
, y
j
) x
i
, y
j
?W
1 x
i
, y
j
/?W & x
i
= y
j
0 x
i
, y
j
/?W & x
i
6= y
j
0 x
i
?W & y
j
/?W
0 x
i
/?W & y
j
?W
(6)
Baseline To test how well local alignment ker-
nels perform compared to the kernels proposed in
the past, we implemented a method described in
(Bunescu and Mooney, 2005) as a baseline. Here,
similarly to our approach, the shortest path be-
tween relation arguments is extracted and a ker-
nel between two sequences (paths) x and y is com-
puted as follows:
K(x, y) =
{
0 m 6= n
?
n
i=1
f(x
i
, y
i
) m = n
(7)
In Eq. 7, f(x
i
, y
i
) is the number of com-
mon features shared by x
i
and y
i
. Bunescu and
Mooney (2005) use several features such as word
(protesters), part of speech tag (NNS), gener-
alized part of speech tag (Noun), and entity type
(e.g., PERSON ) if applicable. In addition, a di-
rection feature (? or?) is employed. In our ex-
periments we also use lemma, part of speech tag
and direction but we do not consider an entity type
or negative polarity of items.
Kernels that we compute are used together with
LibSVM (Chang and Lin, 2001) to detect hyper-
planes separating positive examples from the neg-
ative ones. Before plugging all kernel matrices into
LibSVM, they were normalized as in Eq. 8.
K(x
?
, y
?
) =
K(x, y)
?
K(x, x)K(y, y)
+ 1 (8)
To compute LA matrices we use the distributed
ASCI supercomputer 3 (DAS-3) 4 which allows us
to speed up the process of sequence comparison.
In particular, because of symmetricity of the result-
ing matrices for n sequences we need to carry out
n(n?1)/2 comparisons to build a matrix. Compu-
tations are done in parallel by reserving a number
of nodes of DAS-3 and concatenating the outputs
later on.
4.2 Experiment I: Distributional measures
and their impact on the final performance
Distributional similarity measures have been used
for various tasks in the past. For instance, (Lee,
1999) employs them to detect similar nouns based
on the verb-object cooccurrence pairs. The results
suggest the Jaccard?s coefficient to be one of the
best performing measures followed by some others
including cosine. Euclidean distance fell into the
group with the largest error rates. It is of consider-
able interest to test whether these metrics have an
impact on the performance of a LA kernel. We do
not employ Jaccard?s measure but the dice coeffi-
cient is monotonic in it.
While computing a distributional similarity, it
may happen that a given word x does not occur
in the corpus. To handle such cases, we always set
d(x, x) = 1. To estimate distributional similarity,
a number of hits returned by querying the TREC
collection is set to 500. Gaps are defined through
the gaps opening and extension costs. In our ex-
periments, the gap opening cost is set to 1.2, the
extension cost to 0.2 and the scaling parameter ?
to 1.
The 10-fold cross-validation results on the
BC-PPI corpus are presented in Table 2 and on
the LLL training data set in Table 3. The LA kernel
4http://www.cs.vu.nl/das3
420
based on the distributional similarity measures per-
forms significantly better than the baseline. In con-
trast to the baseline, it is able to handle sequences
of different lengths including gaps. According to
the Eq. 7, a comparison of any two sequences of
different lengths results in the 0-score. Neverthe-
less it still yields high recall while precision is
much lower. Interestingly, the results of the short-
est path approach on the ACE corpus (Bunescu and
Mooney, 2005) were reversed by boosting preci-
sion while decreasing recall.
Method Pr,% R,% F
1
,%
LAK-dice 75.56 79.72 77.56
LAK-cosine 76.4 80.66 78.13
LAK-l2 77.56 79.31 78.42
Baseline 32.04 75.63 45.00
Table 2: Results on the BC-PPI data set
At first glance, the LA kernel based on the distri-
butional similarity measures that we selected pro-
vides similar performance. We can notice that the
l2 metric seems to be the best performing measure.
On the BC-PPI data, the method based on the l2
measure outperforms the methods based on dice
and on cosine but the differences are not signifi-
cant.
On the LLL data set, the LA method using distri-
butional similarity measures significantly outper-
forms both baselines and also yields better results
than an approach based on shallow linguistic in-
formation (Giuliano et al, 2006). Giuliano et al
(2006) use no syntactic information. Recent work
reported in (Fundel, 2007) also uses dependency
information but in contrast to our method, it serves
as representation on which extraction rules are de-
fined.
The choice of the distributional measure does
not seem to affect the overall performance very
much. But in contrast to the BC-PPI data set, the
kernels which use dice and cosine measures sig-
nificantly outperform the one based on l2 (paired
t-test, ? = 0.01).
Method Pr,% R,% F
1
,%
LAK-dice 74.25 87.94 80.51
LAK-cosine 73.99 88.23 80.48
LAK-l2 69.28 87.6 77.37
(Fundel, 2007) 68 83 75
(Giuliano et al, 2006) 62.10 61.30 61.70
Baseline 39.02 100.00 56.13
Table 3: Results on the LLL data set
coreferences Pr,% R,% F
1
,%
with (LAK-dice) 60.00 31.00 40.90
w/o (LAK-dice) 71.00 50.00 58.60
with (Giuliano et al, 2006) 29.00 31.00 30.00
w/o (Giuliano et al, 2006) 54.80 62.90 58.60
Table 4: Results on the LLL test data set
We also verified how well our method performs
on the LLL test data. Surpisingly, precision is
still high (for both subsets, with co-references and
without them) while recall suffers. We hypothesize
that it is due to the fact that for some sentences
only incomplete parses are provided and, conse-
quently, no dependency paths between the entities
are found. For 91 out of 567 possible interaction
pairs generated on the test data, there is no depen-
dency path extracted. In contrast, work reported in
(Giuliano et al, 2006) does not make use of syn-
tactic information which on the data without coref-
erences yields higher recall.
4.2.1 Experiment Ia: Impact of distributional
measures estimation
We believe that accuracy of LA kernel crucially
depends on the substitution matrix, i.e. an accu-
rate estimate of distributional similarity. In most
cases, to obtain accurate estimates it is needed to
use a large corpus. However, it is unclear whether
differences in the estimates derived from corpora
of different sizes would affect the overall perfor-
mance of the LA kernel. To investigate it, we con-
ducted several experiments by varying a number of
retrieved passages.
Table 5 contains the most similar words to ad-
here, expression and sigF detected by the dice
measure in descending order (by varying the num-
ber of passages retrieved per query). While the or-
der of the most similar words for sigF does not
change very much from one setting to another, es-
timates for adhere and expression depend more on
the number of passages retrieved. Moreover, not
only the actual ordering changes, but also the num-
ber of similar words does. For instance, while
there are only four words similar to adhere found
when 100 passages per each query are used, al-
ready 12 similar words to adhere are detected
when the count of extracted documents is set to
1,000 passages per query.
We also notice that the most similar words to
421
adhere expression sigF
dice@100 contribute, belong, bind, map processing, overlap, production, cotC, tagA,
localization, sequestration rocG, tagF, whiG
dice@500 contribute, belong, bind, end, localization, presence, sigE, comK,
occur, result processing, absence cotC, sigG, tagA
dice@1,000 contribute, bind, convert, presence, assembly, localization, sigE, comK,
occur, belong processing, activation cotC, sigG, tagA
dice@1,500 bind, contribute, convert, localization, assembly, presence, sigE, comK,
correspond, belong activation, processing cotC, sigG, tagA
Table 5: Top 5 similar words (LLL data set)
sigF are all named entities. Even though sigF does
not occur in the training data, we can still hypoth-
esize that it is likely to be a target of the relation
because of sigE, cotC and tagA. These three genes
can be found in the training set and they are usually
targets (second argument) of the interaction rela-
tion.
Table 6 shows results on the LLL data set by
varying the size of the data used for estimation of
distributional similarity (dice measure). We ob-
serve the decrease in precision and in recall when
increasing the number of hits to 1,500. Changing
the number of hits from 500 to 1,000 results in a
subtle increase in recall.
Size Pr,% R,% F
1
,%
dice@500 74.25 87.94 80.51
dice@1,000 74.38 88.02 80.62
dice@1,500 69.87 86.85 77.43
Table 6: Estimation settings for the LLL data set
Size Pr,% R,% F
1
,%
dice@100 75.56 79.72 77.58
dice@500 76.72 81.01 78.8
dice@1,000 76.56 80.78 78.61
Table 7: Estimation settings for the BC-PPI data
set
The results on the BC-PPI data set show a simi-
lar tendency. However the observed differences are
not statistically significant in the latter case. These
subtle changes in recall and precision can be at-
tributed to the relatively low absolute values of the
similarity scores. For instance, even though an or-
der of similar words in Table 5 changes while in-
creasing the data used for estimation, a difference
between the absolute values can be quite small.
4.2.2 Experiment Ib: Impact of the scaling
parameter ?
Saigo et al (2004) have already shown that the
parameter ? has the significant impact on the re-
sults accuracy. We have also carried out some pre-
liminary experiments by setting the opening gap to
12, the extension gap to 2 and by varying the pa-
rameter ?. The kernel matrices were normalized
as in Eq. 8. The results on the BC-PPI data set
(dice500) are given in Table 8.
? Pr,% R,% F
1
,%
0.5 17.72 94.87 29.85
1 38.84 89.42 54.14
10 67.72 76.67 71.90
Table 8: Impact of ? on the performance on the
BC-PPI data set
The results indicate that decreasing ? leads to
the decrease in the overall performance. However,
if the values of gap penalties are lower and ? is set
to 1, the results are better. This suggests that the
final performance of the LA kernel is influenced
by a combination of parameters and their choice is
crucial for obtaining the good performance.
5 Related Work
We have already mentioned some relevant work
on relation extraction while introducing the local
alignment kernel. Most work done for relation
extraction considers binary relations in sentential
context (McDonald, 2005). Current techniques for
relation extraction include hand-written patterns
(Sekimizu et al, 1998), kernel methods (Zelenko
et al, 2003), pattern induction methods (Snow et
al., 2005), and finite-state automata (Pustejovsky
et al, 2002).
Kernel methods have become very popular
in natural language processing in general and
422
for learning relations, in particular (Culotta and
Sorensen, 2004). There are many kernels defined
for the text data. For instance, string kernels are
special kernels which consider inner products of
all subsequences from the given sequences of el-
ements (Lodhi et al, 2002). They can be further
extended to syllable kernels which proved to per-
form well for text categorization (Saunders et al,
2002).
For relation learning, Zelenko et al(2003) use
shallow parsing in conjunction with contiguous
and non-contiguous kernels to learn relations.
Bunescu et al(2006) define several kernels to ac-
complish the same task. First, they introduce
the sequential kernels and show that such method
out-performs the longest match approach. Next,
Bunescu et al (2006) propose kernels for the paths
in dependency trees (which is referred to as a short-
est path between two arguments of a given rela-
tion). In this paper we used their method based on
dependency parsing as one of the baselines. Giu-
liano (2006) takes this approach further by defin-
ing several kernels using local context and senten-
tial context. An advantage of Giuliano?s method
(2006) lies in the simpler representation which
does not use syntactic structure. In this case, even
if parsing fails on certain sentences, it is still pos-
sible to handle them.
6 Conclusions
We presented a novel approach to relation extrac-
tion which is based on the local alignments of se-
quences. To compare two sequences, additional
information is used which is not necessarily con-
tained in the training data. By employing distribu-
tional measures we obtain a considerable improve-
ment over two baselines and work reported before.
The choice of a distributional similarity measure
does not seem to affect the overall performance
very much. Based on the experiments we have
conducted, we conclude that the LA kernel using
dice and cosine measures perform similarly on the
LLL data set and the BC-PPI corpus. On the LLL
corpus, the LA kernel employing l2 shows a sig-
nificant decrease in performance. But concerning
statistical significance, the method using dice sig-
nificantly outperforms the one based l2 measure
only on LLL corpus while there is no significant
improvement on the BC-PPI data set noticed.
We use contextual information to measure dis-
tributional similarity. In this setting any two words
can be compared no matter which parts of speech
they belong to. As dependency paths contain vari-
ous words along with nouns and verbs, other meth-
ods often mentioned in the literature would be
more difficult to use. However, in the future we
are going to extend this approach by using syntac-
tically analyzed corpora and by estimating distri-
butional similarity from it. It would allow us to use
more accurate estimates and to discriminate be-
tween lexically ambiguous words. Similarity mea-
sures on the words that belong to other parts of
speech can be still estimated using the local con-
text only.
Acknowledgments
The authors thank Vera Hollink, Victor de Boer
and anonymous reviewers for their valuable com-
ments. This work was carried out in the con-
text of the Virtual Laboratory for e-Science project
(www.vl-e.nl). This project is supported by a
BSIK grant from the Dutch Ministry of Education,
Culture and Science (OC&W) and is part of the
ICT innovation program of the Ministry of Eco-
nomic Affairs (EZ).
References
Yevgeny (Eugene) Agichtein. 2005. Extracting Re-
lations from Large Text Collections. Ph.D. Thesis,
Columbia University.
Razvan C. Bunescu and Raymond J.Mooney. 2006.
Extracting Relations from Text. From Word Se-
quences to Dependency Paths. In book ?Text Mining
and Natural Language Processing?, Anne Kao and
Steve Poteet (Eds).
Razvan C. Bunescu and Raymond J.Mooney. 2005. A
Shortest Path Dependency Kernel for Relation Ex-
traction. In Proceedings of the Joint Conference
on Human Language Technology / Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
Vancouver, BC.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm
Stanley F. Chen and Joshua Goodman. 1996. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. In ACL?96.
William W. Cohen, Pradeep Ravikumar and Stephen
Fienberg. 2003. A Comparison of String Distance
Metrics for Name-Matching Tasks. In IIWeb 2003,
pages 73-78.
423
Aron Culotta and Jeffrey Sorensen. 2003. Dependency
Tree Kernels for Relation Extraction. In ACL 2003.
William B. Dolan, Chris Quirk and Chris Brockett.
2004. Unsupervised Construction of Large Para-
phrase Corpora: Exploiting Massively Parallel News
Sources. In Proceedings of COLING 2004, Geneva,
Switzerland.
Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer. 2007.
RelEx - Relation Extraction using dependency parse
trees. In Bioinformatics, vol. 23, no. 3.
Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-
mano. 2006. Exploiting Shallow Linguistic Infor-
mation for Relation Extraction from Biomedical Lit-
erature. In EACL 2006.
David Haussler. 1999. Convolution Kernels on Dis-
crete Structures. UC Santa Cruz Technical Report
UCS-CRL-99-10.
William Hersch, Aaron M. Cohen, Phoebe Roberts and
Hari K. Rakapalli. 2006. TREC 2006 Genomics
Track Overview. In Proceedings of the 15th Text Re-
trieval Conference.
Lillian Lee. 1999. Measures of distributional similar-
ity. In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Com-
putational Linguistics, pages 25-32.
G. Leusch, N. Ueffing and H. Ney. 2003. A Novel
String-to-String Distance Measure with Applications
to Machine Translation Evaluation. In Machine
Translation Summit IX, New Orleans, LO, pages
240-247.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Christianini, and Chris Watkins. 2002. Text
Classification using String Kernels. In Journal of
Machine Learning Research, 2, pages 419-444.
Ryan McDonald. 2005. Extracting Relations from Un-
structured Text. Technical Report: MS-CIS-05-06.
Alvaro E. Monge and Charles Elkan. 1996. The Field
Matching Problem: Algorithms and Applications. In
KDD 1996, pages 267-270.
Alessandro Moschitti. Efficient Convolution Kernels
for Dependency and Constituent Syntactic Trees. In
ECML 2006, pages 318-329.
Cl. Ne?dellec. 2005. Learning Language in Logic -
Genic Interaction Extraction Challenge. In Proceed-
ings of the Learning Language in Logic workshop.
J. Pustejovsky, J. Castano, J. Zhang, B. Cochran, M.
Kotecki. 2002. Robust Relational Parsing over
Biomedical Literature: Extracting Inhibit Relations.
Pacific Symposium on Biocomputing.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand,
James Dowdall, Christos Andronis, Andreas Per-
sidis, and Ourania Konstanti. 2004. Mining re-
lations in the GENIA corpus. In ?Second Euro-
pean Workshop on Data Mining and Text Mining for
Bioinformatics?, in conjunction with ECML/PKDD
2004, September.
Hiroto Saigo, Jean-Philippe Vert, Nobuhisa Ueda and
Tatsuya Akutsu. 2004. Protein homology detection
using string alignment kernels. In ?Bioinformatics?,
vol. 20 no. 11, pages 1682-1689.
Hiroto Saigo, Jean-Philippe Vert, and Tatsuya Akutsu.
2006. Optimizing amino acid substitution matrices
with a local alignment kernel. In ?BMC Bioinfor-
matics?, 7:246.
C. Saunders, H. Tschach, and J. Shawe-Taylor. 2002.
Syllables and other String Kernel Extensions. In
Proceesings of the Nineteenth International Confer-
ence on Machine Learning (ICML?02).
T. Sekimizu, H. Park, and J. Tsujii. 1998. Identify-
ing the interaction between genes and gene products
based on frequently seen verbs in medline abstracts.
In Genome Informatics.
T. F. Smith and M. S. Waterman. 1987. Identifica-
tion of Common Molecular Subsequences. In J. Mol.
Biol. 147, 195?197.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. NIPS 17.
Erik Tjong Kim Sang, Sander Canisius, Antal van den
Bosch and Toine Bogers. 2005. Applying spelling
error correction techniques for improving semantic
role labeling. In Proceedings of the Ninth Confer-
ence on Natural Language Learning, CoNLL-2005,
June 29-30, 2005, Ann Arbor, MI.
Lonneke van der Plas and Jo?rg Tiedemann. 2006. Find-
ing Synonyms Using Automatic Word Alignment
and Measures of Distributional Similarity. In Pro-
ceedings of ACL/Coling.
C. J. van Rijsbergen, S. E. Robertson and M. F. Porter.
1980. New models in probabilistic information re-
trieval. London: British Library. (British Library
Research and Development Report, no. 5587).
Julie Weeds, David Weir and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of CoLing 2004.
Dmitry Zelenko, Ch. Aone, and A. Richardella. 2003.
Kernel Methods for Relation Extraction. Journal of
Machine Learning Research 3 (2003), 1083-1106.
424
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 185?188,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Semantic Types of Some Generic Relation Arguments:
Detection and Evaluation
Sophia Katrenko
Institute of Informatics
University of Amsterdam
the Netherlands
katrenko@science.uva.nl
Pieter Adriaans
Institute of Informatics
University of Amsterdam
the Netherlands
pietera@science.uva.nl
Abstract
This paper presents an approach to detec-
tion of the semantic types of relation argu-
ments employing the WordNet hierarchy. Us-
ing the SemEval-2007 data, we show that
the method allows to generalize relation ar-
guments with high precision for such generic
relations as Origin-Entity, Content-Container,
Instrument-Agency and some other.
1 Introduction and Motivation
A common approach to learning relations is com-
posed from two steps, identification of arguments
and relation validation. This methodology is widely
used in different domains, such as biomedical. For
instance, in order to extract instances of a relation of
protein interactions, one has to first identify all pro-
tein names in text and, second, verify if a relation
between them holds.
Clearly, if arguments are already given, accuracy
of relation validation is higher compared to the sit-
uation when the arguments have to be identified au-
tomatically. In either case, this methodology is ef-
fective for the domain-dependent relations but is not
considered for more generic relation types. If a rela-
tion is more generic, such as Part-Whole, it is more
difficult to identify its arguments because they can
be of many different semantic types. An exam-
ple below contains a causality relation (virus causes
flu). Note that syntactic information is not sufficient
to be able to detect such relation mention and the
background knowledge is needed.
A person infected with a particular flu virus
strain develops antibody against that virus.
In this paper we propose a method to detect se-
mantic types of the generic relation arguments. For
the Part-Whole relation, it is known that it embraces
such subtypes as Member-Collection or Place-Area
while there is not much information on the other re-
lation types. We do not claim semantic typing to
be sufficient to recognize relation mentions in text,
however, it would be interesting to examine the ac-
curacy of relation extraction when the background
knowledge only is used. Our aim is therefore to dis-
cover precise generalizations per relation type rather
than to cover all possible relation mentions.
2 A Method: Making Semantic Types of
Arguments Explicit
We propose a method for generalizing relation argu-
ment types based on the positive and negative exam-
ples of a given relation type. It is also necessary that
the arguments of a relation are annotated using some
semantic taxonomy, such as WordNet (Fellbaum,
1998). Our hypothesis is as follows: because of
the positive and negative examples, it should be pos-
sible to restrict semantic types of arguments using
negative examples. If negative examples are nearly
positive, the results of such generalization should be
precise. Or, in machine learning terms, such neg-
ative examples are close to the decision boundary
and if used during generalization, precision will be
boosted. If negative examples are far from the de-
cision boundary, their use will most likely not help
to identify semantic types and will result in over-
generalization.
To test this hypothesis, we use an idea borrowed
from induction of the deterministic finite automata.
185
Gx1 Gy1
Gy2
Gy3
Gx4 Gy4
Gx1 LCSGy1 ,Gy2 ,Gy3
Gx4
Gy2
Gy4
Figure 1: Generalization process.
More precisely, to infer deterministic finite automata
(DFA) from positive and negative examples, one first
builds the maximal canonical automaton (MCA)
(Pernot et al, 2005) with one starting state and a
separate sequence of states for each positive exam-
ple and then uses a merging strategy such that no
negative examples are accepted.
Similarly, for a positive example < xi, yi > we
collect all f hyperonyms Hxi = h1xi , h
2
xi , . . . , h
f
xi
for xi where h1xi is an immediate hyperonym and h
f
xi
is the most general hyperonym. The same is done for
yi. Next, we use all negative examples to find Gxi
and Gyi which are generalization types of the argu-
ments of a given positive example < xi, yi >. In
other words, we perform generalization per relation
argument in a form of one positive example vs. all
negative examples. Because of the multi-inheritance
present in WordNet, it is possible to find more hy-
peronymy paths than one. To take it into account,
the most general hyperonym hfxi equals to a splitting
point/node.
It is reasonable to assume that the presence of a
general semantic category of one argument will re-
quire a more specific semantic category for the other.
Generalization per argument is, on the one hand,
useful because none of the arguments share a seman-
tic category with the corresponding arguments of all
negative examples. On the other hand, it is too re-
strictive if one aims at identification of the relation
type. To avoid this, we propose to generalize seman-
tic category of one argument by taking into account
a semantic category of the other. In particular, one
can represent a binary relation as a bipartite graph
where the corresponding nodes (relation arguments)
are connected. A natural way of generalizing would
be to combine the nodes which differ on the basis of
their similarity. In case of WordNet, we can use a
least common subsumer (LCS) of the nodes. Given
the bipartite graph in Figure 1, it can be done as fol-
lows. For every vertex Gxi in one part which is con-
nected to several vertices Gy1 , . . . , Gyk in the other,
we compute LCS of Gy1 , . . . , Gyk . Note that we re-
quire the semantic contrains on both arguments to be
satisfied in order to validate a given relation. Gener-
alization via LCS is carried out in both directions.
This step is described in more detail in Algorithm 1.
Algorithm 1 Generalization via LCS
1: MemoryM = ?
2: Direction: ?
3: for all < Gxi , Gyi >? G do
4: Collect all < Gxj , Gyj >, j = 0, . . . , l s. t.
Gxi = Gxj
5: if exists < Gxk , Gyj > s. t. Gxi 6= Gxk then
6: G = G ? {< Gxj , Gyj >}
7: end if
8: Compute L = LCSGy0 ,...,Gyl
9: Replace < Gxj , Gyj >,j = 0, . . . , l with <
Gxj ,L > in G
10: M =M? {< Gxj ,L >}
11: end for
12: Direction: ?
13: for all < Gxi , Gyi >? G do
14: Collect all < Gxj , Gyj >, j = 0, . . . , l s. t. Gyi =
Gyj and
< Gxj , Gyj >/?M
15: Compute L = LCSGx0 ,...,Gxl
16: Replace < Gxj , Gyj >, j = 0, . . . , l with <
L, Gyj > in G
17: end for
18: return G
Example Consider, for instance, two sentences
from the SemEval data (Instrument-Agency rela-
tion).
013 ?The test is made by inserting the
end of a <e1>jimmy</e1> or other
<e2>burglar</e2>?s tool and endeavouring
to produce impressions similar to those which
have been found on doors or windows.?
WordNet(e1) = ?jimmy%1:06:00::?, Word-
Net(e2) = ?burglar%1:18:00::?, Instrument-
Agency(e1, e2) = ?true?
040 ?<e1>Thieves</e1> used a
<e2>blowtorch</e2> and bolt cutters
to force their way through a fenced area
186
topped with razor wire.? WordNet(e1) =
?thief%1:18:00::?, WordNet(e2) = ?blow-
torch%1:06:00::?, Instrument-Agency(e2, e1)
= ?true?
First, we find the sense keys corresponding
to the relation arguments, (?jimmy%1:06:00::?,
?burglar%1:18:00::?) = (jimmy#1, burglar#1)
and (?blowtorch%1:06:00::?, ?thief%1:18:00::?) =
(blowtorch#1, thief#1).By using negative exam-
ples, we obtain the following pairs: (apparatus#1,
bad person#1) and (bar#3, bad person#1). These
pairs share the second argument and it makes
it possible to apply generalization in the direc-
tion ?. LCS of apparatus#1 and bar#3 is
instrumentality#3 and hence the generalized pair
becomes (instrumentality#3, bad person#1).
Note that an order in which the directions are cho-
sen in Algorithm 1 does not affect the resulting gen-
eralizations. Keeping all generalized pairs in the
memory M ensures that whatever direction (? or
?) a user chooses first, the output of the algorithm
will be the same.
Until now, we have considered generalization in
one step only. It would be natural to extend this ap-
proach to the iterative generalization such that it is
performed until no further generalization steps can
be made (it corresponds either to the two specific ar-
gument types or to the situation when the top of the
hierarchy is reached). However, such method would
most likely result in overgeneralization by boost-
ing recall but drastically decreasing precision. As
an alternative we propose to use memory MI de-
fined over the iterations. After each iteration step
every generalized pair < Gxi , Gyi > is applied to
the training set and if it accepts at least one negative
example, it is either removed from the set G (first
iteration) or this generalization pair is decomposed
back into the pairs it was formed from (all other it-
erations). By employing backtracking we guarantee
that empirical error on the training set Eemp = 0.
3 Evaluation
Data For semantic type detection, we use 7 binary
relations from the training set of the SemEval-2007
competition, all definitions of which share the re-
quirement of the syntactic closeness of the argu-
ments. Further, their definitions have various restric-
tions on the nature of the arguments. Short descrip-
tion of the relation types we study is given below.
Cause-Effect(X,Y) This relation takes place if, given
a sentence S, it is possible to entail that X is the cause
of Y . Y is usually not an entity but a nominal denoting
occurrence (activity or event).
Instrument-Agency(X,Y) This relation is true if S en-
tails the fact that X is the instrument of Y (Y uses X).
Further, X is an entity and Y is an actor or an activity.
Product-Producer(X,Y) X is a product of Y , or Y
produces X , where X is any abstract or concrete object.
Origin-Entity(X,Y) X is the origin of Y where X can
be spatial or material and Y is the entity derived from the
origin.
Theme-Tool(X,Y) The tool Y is intended for X is ei-
ther its result or something that is acted upon.
Part-Whole(X,Y) X is part of Y and this rela-
tion can be one of the following five types: Place-
Area, Stuff-Object, Portion-Mass, Member-Collection
and Component-Integral object.
Content-Container(X,Y) A sentence S entails the
fact that X is stored inside Y . Moreover, X is not a com-
ponent of Y and can be removed from it.
We hypothesize that Cause-Effect and Part-Whole
are the relation types which may require sentential
information to be detected. These two relations al-
low a greater variety of arguments and the seman-
tic information alone might be not sufficient. Such
relation types as Product-Producer or Instrument-
Agency are likely to benefit more from the external
knowledge. Our method depends on the positive and
negative examples in the training set and on the se-
mantic hierarchy we use. If some parts of the hierar-
chy are more flat, the resulting patterns may be too
general.
As not all examples have been annotated with
the information from WordNet, we removed them
form the test data while conducting this experiment.
Content-Container turned out to be the only rela-
tion type whose examples are fully annotated. In
contrast, Product-Producer is a relation type with
the most information missing (9 examples removed).
There is no reason to treat relation mentions as mu-
tually exclusive, therefore, only negative example
provided for a particular relation type are used to
determine semantic types of its arguments.
Discussion The entire generalization process re-
sults in a zero-error on the training set. It does
not, however, guarantee to hold given a new data
set. The loss in precision on the unseen exam-
187
Relation type P, % R, % A, % B-A, %
Origin-Entity 100 26.5 67.5 55.6
Content-Container 81.8 47.4 67.6 51.4
Cause-Effect 100 2.8 52.7 51.2
Instrument-Agency 78.3 48.7 67.6 51.3
Product-Producer 77.8 38.2 52.4 66.7
Theme-Tool 66.7 8.3 65.2 59.2
Part-Whole 66.7 15.4 66.2 63.9
avg. 81.6 26.8 62.7 57.0
Table 1: Performance on the test data
ples can be caused by the generalization pairs where
both arguments are generalized to the higher level
in the hierarchy than it ought to be. To check
how the algorithm behaves, we first evaluate the
specialization step on the test data from the Se-
mEval challenge. Among all the relation types,
only Instrument-Agency, Part-Whole and Content-
Container fail to obtain 100% precision after the
specialization step. It means that, already at this
stage, there are some false positives and the contex-
tual classification is required to achieve better per-
formance.
The results of the method introduced here are pre-
sented in Table 1. Systems which participated in
SemEval were categorized depending on the input
information they have used. The category Word-
Net implies that WordNet was employed but it does
not exclude a possibility of using other resources.
Therefore, to estimate how well our method per-
forms, we calculated accuracy and compared it
against a baseline that always returns the most fre-
quent class label (B-A). Given the results of the
teams participating in the challenge, the organizers
mention Product-Producer as one of the easiest rela-
tions, while Origin-Entity and Theme-Tool are con-
sidered to be ones of the hardest to detect (Girju
et al, 2007). Interestingly, Origin-Entity obtains
the highest precision compared to the other relation
types while using our approach.
Table 2 contains some examples of the semantic
types we found for each relation. Some of them
are quite specific (e.g., Origin-Entity), while the
other arguments may be very general (e.g., Cause-
Effect). The examples of the patterns for Part-
Whole can be divided in several subtypes, such as
Member-Collection (person#1, social group#1),
Place-Area (top side#1, whole#2) or Stuff-Object
(germanium#1, mineral#1).
Relation (GX , GY )
Content-
Container
(physical entity#1, vessel#3)
Instrument- (instrumentality#3, bad person#1)
Agency (printing machine#1, employee#1)
Cause- (cognitive operation#1, joy#1)
Effect (entity#1, harm#2)
(cognitive content#1,
communication#2)
Product- (knowledge#1, social unit#1)
Producer (content#2, individual#1)
(instrumentality#3,
business organisation#1)
Origin- (article#1, section#1)
Entity (vegetation#1, plant part#1)
(physical entity#1, fat#1)
Theme- (abstract entity#1, implementation#2)
Tool (animal#1, water#6)
(nonaccomplishment#1,
human action#1)
Part- (top side#1, whole#2)
Whole (germanium#1, mineral#1)
(person#1, social group#1)
Table 2: Some examples per relation type.
4 Conclusions
As expected, the semantic types derived for such
relations as Origin-Entity, Content-Container and
Instrument-Agency provide high precision on the
test data. In contrast, precision for Theme-Tool is
the lowest which has been noted by the participants
of the SemEval-2007. In terms of accuracy, Cause-
Effect seems to obtain 100% precision but low recall
and accuracy. An explanation for that might be a
fact that causation can be characterized by a great
variety of argument types many of which have been
absent in the training data. Origin-Entity obtains the
maximal precision with accuracy much higher than
baseline.
References
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Nicholas Pernot, Antoine Cornue?jols, and Michele Se-
bag. 2005. Phase transition within grammatical infer-
ence. In Proceedings of IJCAI 2005.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of Semantic
Relations between Nominals. In ACL 2007.
188
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 88?93,
Prague, June 2007. c?2007 Association for Computational Linguistics
Named Entity Recognition for Ukrainian: A Resource-Light Approach
Sophia Katrenko
HCSL, University of Amsterdam,
Kruislaan 419, 1098VA Amsterdam,
the Netherlands
katrenko@science.uva.nl
Pieter Adriaans
HCSL, University of Amsterdam,
Kruislaan 419, 1098VA Amsterdam,
the Netherlands
pitera@science.uva.nl
Abstract
Named entity recognition (NER) is a subtask
of information extraction (IE) which can be
used further on for different purposes. In this
paper, we discuss named entity recognition
for Ukrainian language, which is a Slavonic
language with a rich morphology. The ap-
proach we follow uses a restricted number of
features. We show that it is feasible to boost
performance by considering several heuris-
tics and patterns acquired from the Web data.
1 Introduction
The information extraction task has proved to be dif-
ficult for a variety of domains (Riloff, 1995). The
extracted information can further be used for ques-
tion answering, information retrieval and other ap-
plications. Depending on the final purpose, the ex-
tracted information can be of different type, e.g.,
temporal events, locations, etc. The information cor-
responding to locations and names, is referred to as
the information about named entities. Hence, named
entity recognition constitutes a subtask of the infor-
mation extraction in general.
It is especially challenging to extract the named
entities from the text sources written in languages
other than English which, in practice, is supported
by the results of the shared tasks on the named entity
recognition (Tjong Kim Sang, 2002).
Named entity recognition for the languages with
a rich morphology and a free word order is difficult
because of several reasons. The entropy of texts in
such languages is usually higher than the entropy
of English texts. It is either needed to use such
resources as morphological analyzers to reduce the
data sparseness or to annotate a large amount of data
in order to obtain a good performance. Luckily, the
free word order is not crucial for the named entity
recognition task as the local context of a named en-
tity should be sufficient for its detection. Besides,
a free word order usually implies a free order of
constituents (such as noun phrases or verb phrases)
rather than words as such. For instance, although
(1)1 is grammatically correct and can occur in the
data, it would be less frequent than (2).
 
	


In: Proceedings of CoNLL-2000 and LLL-2000, pages 176-183, Lisbon, Portugal, 2000. 
Learning from a Substructural Perspective 
P ie ter  Adr iaans  and Er ik  de  Haas  
Syllogic, 
P.O. Box 2729, 3800GG Amersfoort,  The Netherlands, 
and  
University of Amsterdam, Fac. of Mathematics,  Computer  Science, Physics and Astronomy, 
Plantage Muidergracht 24, 1018TV Amsterdam, The Netherlands 
pieter, adriaans@ps.net, erik@propersolution.nl 
Abst rac t  
In this paper we study learning from a logical 
perspective. We show that there is a strong re- 
lationship between a learning strategy, its for- 
mal learning framework and its logical represen- 
tational theory. This relationship enables one 
to translate learnability results from one theory 
to another. Moreover if we go from a classi- 
cal logic theory to a substructural logic theory, 
we can transform learnability results of logical 
concepts to results for string languages. In this 
paper we will demonstrate such a translation by 
transforming the Valiant learnability result for 
boolean concepts to a learnability :result for a 
class of string pattern languages. 
1 In t roduct ion  
There is a strong relation between a learn- 
ing strategy, its formal learning framework and 
its representational theory. Such a representa- 
tional theory typically is (equivalent to) a logic. 
As an example for this strong relationship as- 
sume that the implication A ~ B is a given 
fact, and you observe A; then you can deduce 
B, which means that you can learn B from A 
based on the underlying representational the- 
ory. The learning strategy is very tightly con- 
nected to its underlying logic. Continuing the 
above example, suppose you observe -~B. In a 
representational theory based on classical ogic 
you may deduce ~A given the fact A ~ B. 
In intuitionistic logic however, this deduction 
is not valid. This example shows that the char- 
acter of the representational theory is essential 
for your learning strategy, in terms of what can 
be learned from the facts and examples. 
In the science of the representational theo- 
ries, i.e. logic, it is a common approach to 
connect different representational theories, and 
transform results of one representational theory 
to results in an other representational theory. 
Interesting is now whether we can transform 
learnability results of learning strategies within 
one representational theory to others. Observe 
that to get from a first order calculus to a string 
calculus one needs to eliminate structural rules 
from the calculus. Imagine now that we do the 
same transformation to the learning strategies, 
we would come up with a learning strategy for 
the substructural string calculus tarting from a 
learning strategy for the full first order calculus. 
The observation that learning categorial 
grammars translates to the task of learning 
derivations in a substructural logic theory moti- 
vates a research program that investigates learn- 
ing strategies from a logical point of view (Adri- 
aans and de Haas, 1999). Many domains for 
learning tasks can be embedded in a formal 
learning framework based on a logical repre- 
sentational theory. In Adriaans and de Haas 
(1999) we presented two examples of substruc- 
tural logics, that were suitable representational 
theories for different learning tasks; The first 
example was the Lambek calculus for learning 
categorial grammars, the second example dealt 
with a substructural logic that was designed to 
study modern Object Oriented modeling lan- 
guages like UML (OMG, 1997), (Fowler, 1997). 
In the first case the representation theory is first 
order logic without structural rules, the formal 
learning theory from a logical point of view is 
inductive substructural logic programming and 
an example of a learning strategy in this frame- 
work is EMILE, a learning algorithm that learns 
categorial grammars (Adriaans, 1992). 
In this paper we concentrate on the trans- 
formation of classical logic to substructural 
logic and show that Valiant's proof of PAC- 
176 
learnability of boolean concepts can be trans- 
formed to a PAC learnability proof for learning 
a class of finite languages. We discuss the ex- 
tension of this learnability approach to the full 
range of substructural logics. Our strategy in 
exploring the concept of learning is to look at 
the logical structure of a learning algorithm, and 
by this reveal the inner working of the learning 
strategy. 
In Valiant (1984) the principle of Probably 
Approximately Correct learning (PAC learning) 
was introduced. There it has been shown that 
k-CNF (k-length Conjunctive Normal Form) 
boolean concepts can be learned efficiently in 
the model of PAC learning. For the proof 
that shows that these boolean concepts can be 
learned efficiently Valiant presents a learning al- 
gorithm and shows by probabilistic arguments 
that boolean concept can be PAC learned in 
polynomial time. In this paper we investigate 
the logical mechanism behind the learning al- 
gorithm. By revealing the logical mechanism 
behind this learning algorithm we are able to 
study PAC learnability of various other logics in 
the substructural landscape of first order propo- 
sitional ogic. 
In this paper we will first briefly introduce 
substructural logic in section 2. Consequently 
we will reconstruct in section 3 Valiant's result 
on learnability of boolean concepts in terms of 
logic. Then in section 4 we will show that the 
learnability result of Valiant for k-CNF boolean 
concepts can be transformed to a learnability re- 
sult for a grammar of string patterns denoted by 
a substructural variant of the k-CNF formulas. 
We will conclude this paper with a discussion 
an indicate how this result could be extended 
to learnability results for categorial grammars. 
2 Subst ructura l  log ic  
In Gentzen style sequential formalisms a sub- 
structural logic shows itself by the absence of 
(some of) the so-called structural rules. Exam- 
ples of such logics are relevance logic (Dunn, 
1986), linear logic (Girard, 1987) and BCK logic 
(Grishin, 1974). Notable is the substructural 
behavior of categorial logic, which in its proto- 
type form is the Lambek calculus. Categorial 
logics are motivated by its use as grammar for 
natural languages. The absence of the struc- 
tural rules degrades the abstraction of sets in 
the semantic domain to strings, where elements 
in a string have position and arity, while they 
do not have that in a set. As we will see further 
on in this paper the elimination of the struc- 
tural rules in the learning context of the boolean 
concepts will transform the learning framework 
from sets of valuated variables to strings of val- 
uated variables. 
Example  2.1 In a domain of sets the following 
'expressions' are equivalent, while they are not 
in the domain of strings: 
a, a, b, a ~ a, b, b 
In a calculus with all the structural rules the fea- 
tures 'position' and 'arity' are irrelevant in the 
semantic domain, because aggregates that differ 
in these features can be proved equivalent with 
the structural rules. To see this observe that 
the left side of the above equation can be trans- 
formed to the right side by performing the fol- 
lowing operation: 
a, a, b, a 
a, b, a 
a, a, b 
a, b 
a, b, b 
contract a, a in .first two positions 
to a 
exchange b, a in last to positions to 
a,b 
contract again a, a in first two 
positions to a 
weaken expression b in last position 
to b, b 
In figure 2 we list the axiomatics of the first 
order propositional sequent calculus 1, with the 
axioms , the cut rule, rules for the connectives 
and the structural rules for exchange, weakening 
and contraction. 
3 PAC Boolean concept learning 
rev is i ted  
In this section we describe the principle of Prob- 
ably Approximately Correct Learning (PAC 
learning) of Boolean concepts. We will reveal 
1Note that  in the variant we use here we have a special 
case of the RA rule. 
177 
representat iona l  
theory 
First order 
propositional ) ~  
logic j I 
formal  learning 
f ramework  
learn ing st rategy 
Boolean \ ~ PAC learning , 
-4 concepts ~ k-CNF ) 
1 
Substructural 
proposition= ) 411 
1 1 
String . PAC learning , 
,~  languages 
Figure 1: Relation between learning strategy, learning framework and representational theory 
(Ax) A ~ A (Cut) 
(LA) F ,A ,B~A (RA) 
F, AAB~A 
(LV) F ,A~A F ,B~A 
F,A V B ~ A (RV) 
F =~ A,A F~,A,~ A 
F', F ~ A', A 
F~A,A  F t~B,A  
F,F t =~ AAB,  A 
F ~ A,A F ~ B ,A  
F~AVB,  A F~AVB,  A 
(Ex) F'AAB'F~=-~ A 
F,B A A,F ~ ~ A 
F~A 
(Weak) F, A ~ A 
(Contr) F, A, A ~ A 
F ,A~A 
Figure 2: First order propositional sequent calculus 
the logical deduction process behind the learn- 
ing algorithm. 
Consider the sample space for boolean con- 
cepts. An example is a vector denoting the 
truth (presence,l) or falsehood (absence,0) of 
propositional variables. Such an example vec- 
tor can be described by a formula consisting of 
the conjunction of all propositional variables or 
negations of propositional variables, depending 
on the fact whether there is a 1 or a 0 in the 
position of the propositional variable name in 
the vector. A collection of vectors, i.e. a con- 
cept, in its turn can be denoted by a formula 
too, being the disjunction of all the formula's of 
the vectors. 
Example  3.1 Let universe U = {a,b} and let 
concept f = {(0, 1)}, then the following formula 
exactly describes f :  
~Ab 
178 
A little more extensive: Let uni- 
verse \[.j, = {a,b,c} and let concept 
f '  = {(0, 0, 0), (0, 0, 1), (0, 1, 1), (1, 1, 1)} 
Then the following formula exactly describes f l  
(with a clear translation): 
(~AbAa) V (~ AbA c) V (~A bA c) V (aAbAc) 
Note that these formulas are in Disjunctive nor- 
mal form (DNF). 
An interesting observation ow is that the 
learning algorithm of Valiant that learns k-CNF 
formulas actually is trying to prove the equiv- 
alence between a DNF formula and a k-CNF 
formula. 
Example  3.2 Let universe U = {a,b} and let 
concept f = {(0, 1)}, then the following sequent 
should be 'learned' by a 2-CNF learning algo- 
rithm 2: 
~ A b ,?:,. (aVb) A (~Vb) A (~Vb) 
A little more extensive: Let U' = 
{a, b, c} and let concept f '  = 
{(0, 0, 0), (0, 0, 1), (0, 1, 1), (1, 1, 1)} Then 
the following sequent should be 'learned' by a 
2-CNF learning algorithm: 
(~ Ab A ~) V (HAhA c) V (~A bA c) V (aAbAc) 
(~V b) A (~V b) A (a V b) 
The above observation says in logical terms 
that the learning algorithm needs to implement 
an inductive procedure to find this desired proof 
and the concluding concept description (2-CNF 
formula) from examples. In the search space for 
this proof the learning algorithm can use the ax- 
ioms and rules from the representational theory. 
In the framework of boolean concept learning 
this means that the learning algorithm may use 
all the rules and axioms from the representa- 
tional theory of classical propositional logic. 
Example  3.3 Let IJ = {a, b} and let concept 
f = {(0, 1)} and assume f can be represented 
by a 2-CNF formula, to learn the 2-CNF de- 
scription of concept f the learning algorithm 
needs to find the proof for a sequent starting 
2i.e. an algorithm that can learn 2-CNF boolean con- 
cepts. 
from the DNF formula ~ A b to a 2-CNF for- 
mula and vice versa (?~.) and to do so it may 
use all the rules and axioms from the first or- 
der propositional calculus including the struc- 
tural rules. The proof for one side of such a 
sequent is spelled out in figure 3. 
In general an inductive logic programming al- 
gorithm for the underlying representational the- 
ory can do the job of learning the concept; i.e. 
from the examples (DNF formulas) one can in- 
duce possible sequents, targeting on a 2-CNF 
sequent on the righthand side. The learning al- 
gorithm we present here is more specific and 
simply shows that an efficient algorithm for the 
proof search exists. 
The steps: 
1. Form the collection G of all 2-CNF 
clauses (p V q) 
2. do l times 
(a) 
(b) 
pick an example al A.-. Aam 
form the collection of all 
2-CNF clauses deducible from 
al A ... A am and intersect this 
collection with G resulting in 
a new C 
Correctness proof  (outl ine): By (Ax), 
(RV), (Weak), (LA) and (Ex) we can proof 
that for any conjunction (i.e. example vector) 
a l  A . . .  A am we have for all 1 _< i < m and 
any b a clause of a 2-CNF in which ai occurs 
with b, hence having all clauses deducible from 
the vector proven individually enabling one to 
form the collection of all clauses deducible from 
a vector; i.e. 
al A ...  Aam ~ ai Vb 
al A ...  A am :::*" b V ai 
By (RA) and (Contr) we can proof the conjunc- 
tion of an arbitrary subset of all the clauses de- 
ducible from the vector, in particular all those 
clauses that happen to be common to all the 
vectors for each individual vector we have seen 
so far, hence proving the 2-CNF for every indi- 
vidual vector; i.e. 
al A .. ? A am ~ clause1 A .. ? A clausep 
179 
b ~ b (Ax) b =*- b (Ax) (av) (av) 
b ~ E V b (Weak) b =~ a V b (Weak) 
gg=t-E(Ax) (Rv) b ,g~ggVb (L^) b ,g~aVb (LA) 
EE=~,,.EVb:_ (Weak) bAE=*,ggVb bAg=~aVb (Sx) E ,b~EVb_  (L^) ggAb~EVb (Ex) EAb=~aVb (RA) 
EAb~EVb (E A b), (E A b) =-~ (E V b) A (a V b) (a^) 
(E A b), (E A b), (E A b) ~ (E V b) A (E V b) A (a V b) 
(Contr) 
(EAb) , (EAb)~(EVb)A(EVb)A(aVb)  
(~A b) =* (~Vb) A (EV b) A (a V b) 
(Contr) 
Figure 3: Proof to be found for boolean concept learning 
Now by (LV)  we can prove the complete DNF 
to 2-CNF sequent; i.e. 
vector1 V ? ? ? V vector/ ~ clause1 A ? ? ? A clausep 
It is easy to see that for the above algorithm 
the same complexity analysis holds as for the 
Valiant algorithm, because we have the same 
progression in l steps, an the individual steps 
have constant overhead. 
4 PAC learn ing  subst ructura l  log ic  
When we transform the representational theory 
of the boolean concept learning framework to a 
substructural logic, we do the following: 
? eliminate the structural rules from the cal- 
culus of first order propositional logic 
When we want to translate the learnability re- 
sult of k-CNF expressible boolean concepts we 
need to do the same with the formal learning 
framework and the strategy (algorithm). In 
other words: 
? the learning framework will contain con- 
cepts that are sensitive to the features 
which were before abstracted by the struc- 
tural rules ('position' and 'arity' ) 
? the learning algorithm from above is no 
longer allowed to use the structural rules 
in its inductive steps. 
Below we present a learning algorithm for 
the substructural logic representational theory. 
Suppose again the universe U = {al , . . .  ,an}, 
and the concept f is a CNF expressible concept 
for vectors of length m. 
1. start with m empty clauses (i.e. disjunction 
of zero literals) clause1,. . . ,  clausem 
2. do l times 
(a) pick an example al A . . .  A am 
(b) for all 1 < i < m add ai to clause/ if 
ai does not occur in clause/. 
Cor rectness  proo f  (out l ine) :  By (Ax) and 
(RV) we can proof for any ai that the sequent 
ai =-~ clause/for any clause/containing ai as one 
of its disjuncts, especially for a clause/contain- 
ing next to ai all the a~ from the former exam- 
ples. Then by (RA) and (LA) we can position 
all the vectors and clauses in the right-hand po- 
sition; i.e. 
al A . . .  A am ~ clause1 A -.. A clausem 
Hence justifying the adding of the literal ai of 
a vector in clausei. Now (LV)  completes the 
sequent for all the example vectors; i.e. 
(al A . . .  A am) V (a i A . . .  A aim ) V . . . 
clause1 A .-. A clausem 
For the algorithmic complexity in terms of 
PAC learning, suppose we want present exam- 
ples of concept f and that the algorithm learned 
concept ff in l steps. Concept ff then de- 
scribes a subset of concept f because on every 
position in the CNF formula contains a sub- 
set of the allowed variables; i.e. those vari- 
ables that have encountered in the examples 3.
anote that the CNF formula's can only describe par- 
ticular sets of n-strings; namely those sets that are com- 
plete for varying symbols locally on the different posi- 
tions in the string. 
180 
~ ~ (Ax) 
~vb 
b ~ b (Ax) b ~ b (Ax) (RV) (RV) 
b~Vb b~aVb (at) 
b, b =* (~V) A (a V b) (RV) (LA) 
bAb~ (gVD) A(aVb)  (at) 
~,b Ab ~ (~V b) A (~V b) A (a V b) 
gAbAb ~ (~Vb) A (~Vb) A (aVb) 
(LA) 
(EAEA a) V (gAEA b) V (gA bAa) V (EA bA b) V (bAEA a) 
V(bAEA b) V (bA bA a) V (bA bA b) ~ (gVb) A (gV b) A (a V b) 
(LV) 
Figure 4: Proof to be found for string pattern learning 
Now let e = P( fA f  ~) be the error then again 
5 = (1 - e) TM is the confidence parameter as we 
have m positions in the string. By the same 
argument as for the Valiant algorithm we may 
conclude that e and 5 decrease xponentially in 
the number of examples l, meaning that we have 
an efficient polynomial t ime learning algorithm 
for arbitrary e and 5. 
5 D iscuss ion  
We showed that the learnability result of 
Valiant for learning boolean concepts can be 
transformed to a learnability result for pat- 
tern languages by looking at the transforma- 
tion of the underlying representational theories; 
i.e. looking at the transformation from clas- 
sical first order propositional logic (underlying 
the boolean concepts) to substructural first or- 
der propositional logic (underlying the pattern 
languages). An interesting extension would be 
to look at the substructural concept language 
that includes implication (instead of the CNF 
formula's only). A language that allows impli- 
cation coincides with the full Lambek calculus, 
and a learning algorithm and learnability result 
for this framework amounts to results for all lan- 
guages that can be described by context free 
grammars. This is subject to future research. 
References  " 
P. Adriaans and E. de Haas. 1999. Grammar in- 
duction as substructural inductive logic program- 
ming. In Proceedings ofthe workshop on Learning 
Language in Logic (LLL99), pages 117-126, Bled, 
Slovenia, jun. 
P. Adriaans. 1992. Language Learning from a Cate- 
gorial Perspective. Ph.D. thesis, Universiteit van 
Amsterdam. Academisch proefschrift. 
J. Dunn. 1986. Relevance logic and entailment. In 
F. Guenthner D. Gabbay, editor, Handbook of 
Philosophical Logic III, pages 117-224. D. Reidel 
Publishing Company. 
M. Fowler. 1997. UML Distilled: Applying the Stan- 
dard Object Modeling Language. Addison Wesley 
Longman. 
J.-Y. Girard. 1987. Linear logic. Theoretical Com- 
purer Science, 50:1-102. 
V.N. Grishin. 1974. A non-standard logic, and its 
applications to set theory. In Studies in formal- 
ized languages and nonclassical logics, pages 135- 
171. Nanka. 
Object Management Group OMG. 1997. Uml 1.1 
specification. OMG documents ad970802-ad0809. 
L.G. Valiant. 1984. Theory of the learnable. Comm. 
o/the ACM, 27:1134-1142. 
181 
Addendum:  PAC l earn ing  
The model of PAC learning arises from the work 
of Valiant (Valiant, 1984). In this model of 
learning it is assumed that we have a sample 
space U* of vectors over an alphabet U, where 
each position in a vector denotes the presence 
(1) or absence (0) of a symbol a ~_-- U in the 
sample vector. A concept f is a subset of vec- 
tors from the sample space U*. 
Example 5.1 Let  U = {a ,b}  be an alphabet, 
then the following table describes the sample 
space U* over U: 
a b 
0 0 
0 1 
1 0 
1 1 
an example of a concept is f := {(0, 1)} and an 
other example is g := {(0, 0), (0, 1), (1, 1)}. 
A concept can be learned by an algorithm by 
giving this algorithm positive and/or  negative 
examples of the target concept to be learned. 
An algorithm efficiently learns a concept if this 
algorithm produces a description of this con- 
cept in polynomial time. Informally eL concept is 
PAC (Probably Approximately Correct) learned 
if the algorithm produces a description of a con- 
cept that is by approximation the same as the 
target concept from which examples are feeded 
into the algorithm. A collection of concepts con- 
stitutes to a concept class. A concept class can 
be (PAC) learned if all the concepts in the con- 
cept class can be (PAC) learned. 
Def in i t ion  5.2 (PAC Learnable) Let F be a 
concept class, 5 (0 < 5 < 1) a confidence param- 
eter, c (0 < e < 1) an error parameter. A con- 
cept class F is PAC learnable if for all targets 
f E F and all probability distributions P on the 
sample space U* the learning algorithm A out- 
puts a concept g E F such that with probability 
(1-5)  it holds that we have a chance on an error 
with P ( f  Ag) _< e (where fag  = (f -g )U(g - f ) )  
We are especially interested in concept classes 
that are defined by some formalism (language). 
In other words a language can describe come 
collection of concepts. An example of such 
a language is the language of boolean formu- 
las. A boolean formula describes a concept 
that consists of all the vectors over the alpha- 
bet of propositional variable names that satisfy 
the formula. These concepts are called boolean 
concepts. 
Example  5.3 Let U := {a, b} be an alphabet of 
propositional variable names. Then the formula 
A b describes the concept f := {(0, 1)} of the 
sample space U*; and the formula ~V b describes 
the concept g := {(0, 0), (0, 1), (1, 1)}. 
In Valiant (1984) Valiant proves that the lan- 
guage of k-CNF boolean formula's can be ef- 
ficiently PAC learned. This means that for an 
arbitrary k the concept class defined by the lan- 
guage of k-CNF formula's can be PAC learned 
by an algorithm in a polynomial number of 
steps. Below we briefly recapitulate this result. 
Def in i t ion  5.4 (Boolean concept languages) 
Let U be a set of propositional variable names, 
then the language L of boolean formulas is de- 
fined by: 
L := UIL V LIL A LIL 
A literal is a propositional variable or a negation 
of a propositional variable; i.e. 
LIT := UIU 
A conjunction of a collection of formulas C is 
a finite sequence of formulas from C connected 
by the binary connective A; i.e. 
CON(C) := CICON(C) A C 
A disjunction of a collection of formulas C is a 
finite sequence of formulas from C connected by 
the binary connective V; i.e. 
DIS(C) := CIDIS(C) V C 
A formula is a CNF.formula (Conjunctive Nor- 
mal Form) if the formula is a conjunction of 
disjunctions of literals. A formula is a k-CNF 
formula if all the disjuctions in the formula are 
of length k. A formula is a DNF formula (Dis- 
junctive Normal Form) if the formula is a dis- 
junction of conjunctions of literals. 
Theorem 5.5 (Valiant (198~)) The classes of 
k-CNF boolean concept languages are PAC 
learnable in polynomial time. 
182 
v. 
v2 ~ 
al an 
sam pie space 
(set of all vectors) 
fa r  
Figure 5: Valiant's proof 
P roo f  (out l ine) :  Let U := {a l , . . . ,an}(n  ?
Af) be a alphabet and let concept f be a set 
of vectors V := {vl , . . . ,Vm}(m _< n) over U*, 
which is equivalent to the k-CNF formula A. 
Let P be an arbitrary probability distribution 
over concept f such that Ev~e/P(vi) = 1; i.e. 
P( f )  -- 1. Examples picked using the distribu- 
tion based on P will be feeded into the following 
learning algorithm: 
? Form the col lect ion G := {ci,... ,Cnk } 
of all the clauses (disjunctions of 
l iterals) of length k. 
? do l t imes 
- v := pick-an-example 
- for each ci in G 
? delete ci from G if v 7-z ci 
Now suppose that the algorithm learned con- 
cept f '  from l examples (l taken from the algo- 
rithm). The concept f '  now is a concept hat 
is a subset of f ,  because it may not have seen 
enough examples to eliminate all the clauses 
that are in conflict with f ;  i.e. there are still 
clauses in ff' restricting this concept in the con- 
junction of clauses, while it is disqualified by a 
vector in f .  What is the size of the number of 
examples I we need to let f '  approximate f with 
for boolean concept learning 
a confidence 5 and error e. We have that 
P( f )  = 1 
= P ( fA f ' )  
(the error is the chance of rejecting an 
example in f because it is not in f ' )  
= (1 - , )m 
(confidence is the chance of not making an 
error after learning from I examples) 
thus 
ln5 < lln(1 - c) 
resulting in the following expression for h 
ln5 
l<  
- ln (1  - e) 
This means that the confidence parameter 5 and 
the error parameter e are exponentially small 
w.r.t, the number of examples l feeded into the 
learning algorithm. This means that for an arbi- 
trary 5 and e we can keep l polynomial because 
the 5 and e decrease xponentially with respect 
to I. 
183 
