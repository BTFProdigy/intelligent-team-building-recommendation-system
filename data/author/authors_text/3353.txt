Proceedings of the 3rd Workshop on Constraints and Language Processing (CSLP-06), pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Constraints in Language Processing: Do Grammars Count?  
 
 
Marieke van der Feen 
Department of Artificial  
Intelligence, 
University of Groningen, 
Grote Kruisstraat 2/1,  
9712 TS Groningen, 
The Netherlands 
mvdfeen@ai.rug.nl 
Petra Hendriks 
Center for Language and 
Cognition Groningen, 
University of Groningen, 
P.O. Box 716,  
9700 AS Groningen, 
The Netherlands 
p.hendriks@rug.nl 
John Hoeks 
Center for Language and 
Cognition Groningen, 
University of Groningen 
P.O. Box 716,  
9700 AS Groningen, 
The Netherlands 
j.c.j.hoeks@rug.nl 
 
  
 
Abstract 
One of the central assumptions of Opti-
mality Theory is the hypothesis of strict 
domination among constraints. A few 
studies have suggested that this hypothe-
sis is too strong and should be abandoned 
in favor of a weaker cumulativity hy-
pothesis. If this suggestion is correct, we 
should be able to find evidence for cumu-
lativity in the comprehension of Gapping 
sentences, which lack explicit syntactic 
clues in the form of the presence of a fi-
nite verb. On the basis of a comparison 
between several computational models of 
constraint evaluation, we conclude that 
the comprehension of Gapping sentences 
does not yield compelling evidence 
against the strict domination hypothesis. 
1 Introduction 
A linguistic framework which has gained a con-
siderable amount of attention in recent years is 
Optimality Theory (Prince and Smolensky, 
1993/2004). Optimality Theory (henceforth OT) 
is not only used for analyzing and explaining 
linguistic phenomena in the domain of phonol-
ogy, but also in the domains of morphology, syn-
tax, semantics and pragmatics. In contrast to 
more traditional linguistic frameworks, OT as-
sumes grammatical constraints to be violable. 
Because constraints are formulated in such a way 
that they are maximally general (and perhaps 
even universal across languages), these con-
straints may conflict. To resolve conflicts among 
constraints, constraints are assumed to differ in 
strength. It is better to violate a weaker constraint 
than it is to violate a stronger constraint. The 
grammatical structure is the one that violates the 
least highly ranked (i.e., strong) constraints. 
A fundamental property of OT is the principle 
of strict domination. This means that each con-
straint has complete priority over all constraints 
ranked lower in the constraint hierarchy. A num-
ber of recent studies, however, have called into 
question this fundamental property of OT. Keller 
(2001) argues that constraint violations must be 
cumulative to account for the pattern of relative 
acceptability with respect to the phenomenon of 
Gapping. J?ger and Rosenbach (to appear) draw 
a similar conclusion on the basis of the observed 
variation with respect to the English genitive (the 
king?s palace versus the palace of the king).  
In this study, we focus on the linguistic phe-
nomenon of Gapping. The central question is 
whether the comprehension of Gapping sen-
tences provides evidence in favor of cumulativity 
of constraint violations. In section 2, we intro-
duce the phenomenon and discuss the possibility 
of an OT model of Gapping. In section 3, we 
consider different kinds of cumulativity. Section 
4 discusses the way we modeled four different 
evaluation algorithms based on these kinds of 
cumulativity. A comparison between our compu-
tational models of constraint evaluation in sec-
tion 5 suggests that the comprehension of Gap-
ping does not provide compelling evidence for 
abandoning the strict domination hypothesis.  
2 Gapping 
Gapping is a grammatical operation that deletes 
certain subconstituents in the second conjunct of 
a coordinate structure, as in (1): 
1
 (1) Some ate beans, and others rice. 
 
The deleted material always includes the finite 
verb, but may also include further constituents 
such as the direct object. As a result, it may not 
always be possible to uniquely identify which 
elements were left out. As an example, consider 
the following sentence: 
 
(2) John greeted Paul yesterday and George 
today. 
 
This sentence is ambiguous between reading (3), 
where first John greeted Paul, and then John 
greeted George, and reading (4), where first John 
greeted Paul, and then George greeted Paul. 
  
(3) John greeted Paul yesterday and John 
greeted George today. 
(4) John greeted Paul yesterday and George 
greeted Paul today. 
 
The reading in (3) is traditionally analyzed as 
resulting from the operation of conjunction re-
duction, whereas the reading in (4) is analyzed as 
resulting from Gapping of the finite verb and the 
direct object. 
2.1 Functional constraints on Gapping  
Based on previous work on Gapping, Kuno 
(1976) notes that several non-syntactic factors 
affect the acceptability and interpretation of Gap-
ping. One of these factors is the distance between 
the remnants in the second conjunct and their 
counterparts in the first conjunct: 
  
(5) The Minimal Distance Principle: 
 The two constituents left behind by Gap-
ping can be most readily coupled with 
the constituents (of the same structures) 
in the first conjunct that were processed 
last of all.  
 
According to this principle, interpretation (3) 
should be preferred for sentence (2) because it is 
more preferable to couple George in the second 
conjunct to the direct object Paul in the first con-
junct, than to the more distant subject John. This 
preference is confirmed by experimental evi-
dence (Carlson, 2001). A further principle about 
Gapping is that the deleted material has to repre-
sent contextually given information, whereas the 
remnants in the second conjunct have to consti-
tute new information. This is captured in the fol-
lowing principle: 
 
(6) The Functional Sentence Perspective 
(FSP) Principle of Gapping: 
a. Constituents deleted by Gapping 
must be contextually known. On the 
other hand, the two constituents left be-
hind by Gapping necessarily represent 
new information and, therefore, must be 
paired with constituents in the first con-
junct that represent new information.  
b. It is generally the case that the 
closer a given constituent is to sentence-
final position, the newer the information 
it represents in the sentence.  
c. Constituents that are clearly 
marked for nonanaphoricity necessarily 
represent new information in violation of 
(b). Similarly, constituents that appear 
closest tot sentence-final position neces-
sarily represent old information (in vio-
lation of (b)) if coreferential constituents 
appear in the corresponding position in 
the preceding discourse. 
 
This principle explains the observation that in a 
suitable context, interpretation (4) can become 
the preferred interpretation for (2) (but see Hoeks 
et al (2006) for experimental evidence that in 
addition to context also prosody has to be in ac-
cordance with a Gapping reading to make this 
reading the preferred reading):  
 
(7) When did John and George greet Paul? 
John greeted Paul yesterday and George 
greeted Paul today. 
 
In this example, John, Paul and George are all 
contextually introduced. But only John and 
George are subjects in the context sentence and 
hence can be interpreted as contrastive topics in 
the target sentence. Contrast has a similar effect 
as newness. Because of this effect of context, the 
Gapping reading can become the preferred read-
ing for (2). Two further principles proposed by 
Kuno are (8) and (9). 
 
(8) The Tendency for Subject-Predicate In-
terpretation: 
When Gapping leaves an NP and a VP 
behind, the two constituents are readily 
interpreted as constituting a sentential 
pattern, with the NP representing the 
subject of the VP. 
2
(9) The Requirement for Simplex-Sentential 
Relationship: 
The two constituents left over by Gap-
ping are most readily interpretable as en-
tering into a simplex-sentential relation-
ship. The intelligibility of gapped sen-
tences declines drastically if there is no 
such relationship between the two con-
stituents. 
 
The principle in (8) is meant to account for a dif-
ference in preference with object control verbs 
versus subject control verbs. The principle in (9) 
reflects the observation that Gapping cannot 
leave behind remnants that are part of a subordi-
nate clause. Kuno notes that this latter constraint 
seems to be the strongest of the four principles, 
being nearly inviolable, but does not make the 
interaction between his principles explicit. 
2.2 An OT model of Gapping  
As Kuno already observes, the FSP Principle 
seems to be able to override the Minimal Dis-
tance Principle. This observation is regarded by 
Keller (2001) as evidence that Gapping is subject 
to constraint competition in an optimality theo-
retic sense. Based on Kuno?s principles, Keller 
develops an OT model of Gapping, which is able 
to account for the pattern of relative acceptability 
of Gapping sentences. According to this model, 
the degree of acceptability of a candidate struc-
ture depends on the number and type of re-
rankings required to make the structure optimal 
(Keller, 1998). 
Keller?s OT model differs from standard OT 
in a number of ways. Firstly, a distinction is 
made between soft and hard constraints. Hard 
constraints cause strong acceptability when vio-
lated, while violation of soft constraints causes 
only mild unacceptability. According to Keller, 
the Requirement for Simplex-Sentential Rela-
tionship is such a hard constraint. The distinction 
between soft and hard constraints is needed in 
Keller?s model to avoid the problem of overgen-
eration of acceptability differences.  
Secondly, Keller?s model assumes that con-
straint violations are cumulative. According to 
his model, the degree of unacceptability in-
creases with the number of constraints violated. 
In standard OT, on the other hand, no number of 
violations of weaker constraints can override one 
violation of a stronger constraint, in accordance 
with the principle of strict domination. 
The aim of Keller?s OT model is to account 
for the pattern of relative acceptability of Gap-
ping sentences. The aim of the present study, on 
the other hand, is to account for the comprehen-
sion of Gapping sentences. Nevertheless, we fol-
low Keller in adopting Kuno?s functional princi-
ples (reformulated as OT constraints) for our OT 
model because Kuno?s principles are principles 
of comprehension. 
Our model differs from Keller?s model in sev-
eral essential aspects, though. We assume that all 
constraints are violable, in accordance with the 
basic assumptions of OT. Because certain strong 
constraints are not violated by the data under dis-
cussion, they simply appear to be inviolable. 
Keller?s second assumption, the assumption that 
constraint violations are cumulative, is the topic 
of investigation of this study.  
3 Cumulativity of constraint violations 
In this section we discuss the different ways OT 
constraints can interact. In principle, OT con-
straints can interact in an unrestricted way, or in 
one of several more or less restricted ways.  
3.1 Unrestricted constraint interaction  
OT as a linguistic theory is derived from Har-
monic Grammar (Legendre et al, 1990). In Har-
monic Grammar (henceforth HG), each con-
straint is associated with a positive or negative 
numerical weight value. For each candidate, a 
so-called Harmony value is calculated by sum-
ming the numerically weighted constraints. From 
the set of candidates, the candidate with the 
highest Harmony value is selected as the optimal 
candidate. Consequently, the interaction among 
constraints in HG is cumulative. Each constraint 
violation lowers the Harmony value of the can-
didate. This type of constraint interaction is es-
sentially unrestricted.  
To account for natural language interpretation, 
however, unrestricted cumulativity is too liberal, 
as is shown by OT analyses of other phenomena. 
With respect to Gapping, if Kuno and Keller are 
right, no amount of violations on weaker con-
straints of an interpretation satisfying Simplex-
Sentential Relationship can make an interpreta-
tion violating Simplex-Sentential Relationship 
the preferred one:  
 
(10) Who did John promise to examine who?  
John promised Paul to examine George, 
and Ringo Bob. 
 
If Simplex-Sentential Relationship indeed is a 
strong constraint, (10) should only mean that 
3
Ringo promised to examine Bob (satisfying Sim-
plex-Sentential Relationship but violating the 
Minimal Distance Principle and the FSP), and 
never that John promised to examine Bob (vio-
lating Simplex-Sentential Relationship).  
For the analysis of natural language, therefore, 
but also for the establishment of cross-linguistic 
generalizations (see Legendre et al, 2006), we 
seem to require a type of constraint interaction 
which is more restricted than simple numerical 
constraint weighting. 
3.2 Restricted constraint interaction  
In this section we discuss four ways to restrict 
constraint interaction: (1) strict domination, (2) 
local restricted cumulativity, (3) global restricted 
cumulativity, and (4) Keller?s counting cumula-
tivity.  
 
 A B C D 
  Candidate 1  * * * 
      Candidate 2 *!    
 
Tableau 1: Strict domination 
 
Strict domination is illustrated in tableau 1. The 
constraints are ordered from left to right in the 
top row in order of descending strength. Under 
strict domination, no number of violations of the 
weaker constraints B, C and D is able to override 
a violation of the strongest constraint A. 
 
 A B C D 
      Candidate 1   *! *! 
  Candidate 2  *   
 
Tableau 2: Local restricted cumulativity 
 
Tableau 2 illustrates local restricted cumulativity. 
When the weaker constraints C and D are simul-
taneously violated, their joint effect can be 
stronger than their linear sum. As a result, to-
gether they are able to override the immediately 
dominating constraint B. This type of cumulativ-
ity is similar to the effects of local conjunction. 
The result is a conjoined constraint C&D, which 
is ranked immediately above constraint B in the 
hierarchy.  
 
 A B C D 
      Candidate 1   *! *! 
  Candidate 2 *    
 
Tableau 3: Global restricted cumulativity 
An illustration of global restricted cumulativity is 
given in tableau 3. In this case, the weaker con-
straints C and D together are able to override a 
stronger, but not necessarily immediately domi-
nating, constraint A. Again, this type of cumula-
tivity is similar to the effects of local conjunc-
tion. The result is a conjoined constraint C&D, 
which is ranked anywhere above C and D in the 
hierarchy. 
 
 A B C D 
      Candidate 1  * *! * 
  Candidate 2 *    
 
Tableau 4: Keller?s counting cumulativity 
 
Keller?s counting cumulativity is illustrated in 
tableau 4. For Keller?s cumulativity, the hierar-
chical relation between the constraints is irrele-
vant. The candidate with the fewest constraint 
violations is always optimal. In Keller?s model, 
constraint violations are assumed to result in a 
gradient pattern. The more constraints are vio-
lated by a given Gapping construction, the less 
acceptable the construction is predicted to be. Of 
course, this type of cumulativity will greatly 
overgenerate in production as well as in compre-
hension if every constraint violation counts as an 
equally serious violation. For this reason, a sys-
tem employing this type of cumulativity must 
make a distinction between soft and hard con-
straints. Hard constraints cause strong unaccept-
ability. This extra assumption serves to restrict 
the overgenerating power of this type of cumula-
tivity. 
The four types of cumulativity discussed here 
differ in the amount of freedom they allow. Strict 
domination is the most restricted type of con-
straint interaction, local restricted cumulativity 
the one but most restricted type, global restricted 
cumulativity the two but most restricted type, 
and Keller?s cumulativity the least restricted 
type. As a result, strict domination yields the 
strongest  hypothesis, and Keller?s cumulativity 
the weakest hypothesis. The question we set out 
to answer in the next section is how strongly 
constraint interaction must be restricted to ac-
count for the comprehension of Gapping sen-
tences. 
4 Testing the evaluation algorithms 
To test the predictions of the four types of cumu-
lativity discussed in the previous section, a com-
puter model was developed in Prolog. The input 
4
to the model is a Gapping sentence in Dutch. The 
first conjunct is manually parsed. Information 
about the givenness of its constituents, the selec-
tional restrictions of the main verb of the first 
conjunct, and featural information for all NPs is 
added. The output of the model is formed by the 
possible couplings of constituents in the second 
conjunct with constituents in the first conjunct. 
In addition, for each possible coupling the con-
straint profile is given. For each possible cou-
pling, the model also gives a reconstruction of 
the second conjunct by placing the constituents 
from the second conjunct in the position of the 
constituents they are coupled with in the first 
conjunct. 
4.1 Constraint ranking 
The constraints implemented in the model were 
Kuno?s principles, reformulated as OT con-
straints, augmented with constraints on parallel-
ism (cf. Carlson, 2001), thematic selection 
(Hoeks and Hendriks, 2005) and word order 
(Lamers and de Hoop, 2004). The constraint 
ranking used is: 
 
(11) Categorial Parallelism >> Simplex-
Sentential Relationship >> FSP >> The-
matic Selection >> Subject Precedes Ob-
ject >> Syntactic Parallelism >> Mini-
mal Distance >> Subject-Predicate In-
terpretation >> Featural Parallelism 
 
The constraint Categorial Parallelism is added to 
ensure that constituents are coupled with con-
stituents of the same syntactic category only. It 
prevents, for example, that in (2) today is cou-
pled with Paul. Thematic Selection expresses the 
selectional restrictions verbs may impose on their 
arguments. For example, the verb bake requires 
an inanimate object, the verb introduce requires 
an animate object, and the verb take can combine 
with either an animate or an inanimate object 
(see section 4.3). The constraint Thematic Selec-
tion is violated if the candidate interpretation 
does not satisfy these selectional restrictions, for 
example if the object of the verb bake is animate. 
According to the constraint Subject Precedes Ob-
ject, the subject must linearly precede the object. 
Syntactic Parallelism requires the two conjuncts 
to have the same syntactic structure. The con-
straint Featural Parallelism, finally, promotes the 
coupling of constituents which share features 
such as animacy, definiteness, number and gen-
der. The ranking of these constraints was deter-
mined on the basis of the literature (Carlson, 
2001; Kuno, 1976) and via comparison of rele-
vant sentences and their meanings. 
4.2 Computational considerations 
The different types of cumulativity were compu-
tationally modeled as different ways of evaluat-
ing the constraint profiles.  
Strict domination can be modeled as numeri-
cal weighting with exponential weights.  
Local restricted cumulativity can be modeled 
as numerical weighting as well, if the weights are 
chosen in such a way that the sum of two adja-
cent constraints is larger than the weight of the 
directly dominating constraint. This is the case if, 
for example, B is 0.50, C is 0.26, and D is 0.25 in 
tableau 2. In our model, local restricted cumula-
tivity only applies to the constraints Thematic 
Selection, Subject Precedes Object and Syntactic 
Parallelism, and allows the constraints Subject 
Precedes Object and Syntactic Parallelism to-
gether to override the directly dominating con-
straint Thematic Selection.  
Global restricted cumulativity, on the other 
hand, cannot be captured straightforwardly in a 
system with weight values. To implement this 
evaluation method, therefore, we made explicit 
use of constraint conjunction. The newly formed 
conjoined constraint C&D was located in the 
hierarchy somewhere above its constituting con-
straints C and D. Because violation of this con-
joined constraint is dependent on the violation of 
each of the constituting constraints, the new con-
straint can only be evaluated in a second round 
of evaluation after all other constraints have been 
evaluated. This is an unfortunate complication of 
our implementation. Legendre et al (2006: 352) 
show that this type of cumulativity can be im-
plemented with weight values if constraint con-
junction is assumed to involve a superlinear 
combination of weights (through summation as 
well as multiplication). In our model, only the 
constraints Minimal Distance and Subject-
Predicate Interpretation were allowed to conjoin. 
The resulting conjoined constraint was located 
above Categorial Parallelism in the hierarchy.  
For the fourth method of evaluation, Keller?s 
counting cumulativity, simply counting the num-
ber of constraint violations suffices. By applying 
one of these four evaluation algorithms, the 
computational model yields an optimal interpre-
tation for each combination of input and evalua-
tion algorithm. 
5
4.3 Input sentences 
To test the four evaluation algorithms, we fed the 
model three types of input: (i) 10 Gapping sen-
tences taken from a corpus, (ii) test sentences 
taken from all five conditions of Carlson?s 
(2001) study on Gapping, and (iii) 15 hand-
crafted sentences. 
The Eindhoven corpus (uit den Boogaart, 
1975) is an annotated corpus of Dutch written 
text of about 750 000 words. We scanned the 
corpus for suitable Gapping sentences, which 
had to occur unembedded, contain an overt con-
junction, and should not involve other deletion 
operations as well. Unfortunately, we only found 
10 such Gapping sentences in the corpus, pre-
sumably because Gapping is quite rare. For all 
ten sentences, all evaluation methods produced 
the same outputs. Nine out of the ten optimal 
interpretations did not violate any of the con-
straints. One sentence involved a constraint vio-
lation by all models, namely a violation of the 
constraint Featural Parallelism: 
 
(12) Groep 1 trok de arm na vijftien minuten 
uit de testkamer, en groep 4 na een uur. 
Group 1 pulled the arm after fifteen 
minutes from the test room and group 4 
after an hour. 
 
The most plausible interpretation of this sentence 
is the interpretation that group 4 pulled the arm 
from the test room after an hour. The interpreta-
tion selected by all evaluation methods, however, 
was that group 1 pulled group 4 from the test 
room after an hour, thus satisfying Minimal Dis-
tance but violating Featural Parallelism. It may 
be that the strong parallelism between group 1 
and group 4 sets up a contrast which evokes the 
constraint FSP even in the absence of an explicit 
linguistic context. If this is true, Minimal Dis-
tance must be violated in order to satisfy FSP.  
We also fed the models test sentences taken 
from Carlson?s (2001) written questionnaire. 
Carlson studied the interaction between The-
matic Selection, Featural Parallelism and Mini-
mal Distance by varying verb type (see the dis-
cussion of Thematic Selection in section 4.1) and 
properties of the noun phrases. She distinguished 
five conditions: the Bake A condition (Alice 
bakes cakes for tourists and Caroline for her 
family), the Bake B condition (Alice bakes cakes 
for tourists and brownies for her family), the 
Take A condition (Josh visited the office during 
the vacation and Sarah during the week), the 
Take B condition (Josh visited Marjorie during 
the vacation and Sarah during the week) and the 
Introduce condition (Dan amazed the judges with 
his talent and James with his musicality).  
The four evaluation algorithms behaved ex-
actly the same on all five conditions of Carlson 
because none of Carlson?s sentences involves a 
simultaneous violation of Subject Precedes Ob-
ject and Syntactic Parallelism (which would give 
rise to local restricted cumulativity in our model) 
or a simultaneous violation of Minimal Distance 
and Subject-Predicate Interpretation (which 
would give rise to global restricted cumulativity 
in our model). As a result, all models yielded a 
100% Gapping response for Carlson?s Bake A 
condition (compared to Carlson?s subjects 81%) 
because for all models a violation of Thematic 
Selection is more serious than a violation of 
Minimal Distance. Furthermore, all models 
yielded a 100% non-Gapping response for her 
Bake B condition (compared to Carlson?s sub-
jects 97%) because a Gapping response violates 
Thematic Selection, Minimal Distance and Fea-
tural Parallelism whereas a non-Gapping re-
sponse satisfies all three constraints. Finally, all 
models yielded a 100% non-Gapping response 
for Carlson?s Take A condition (compared to 
Carlson?s subjects 60%), her Take B condition 
(compared to Carlson?s subjects 96%) and her 
Introduce condition (compared to Carlson?s sub-
jects 79%) because for all models a violation of 
Minimal Distance is more serious than a viola-
tion of Featural Parallelism, given the constraint 
ranking in (11).  
So all models correctly predicted the interpre-
tational preferences found in Carlson?s experi-
ment. However, subjects? percentages of non-
Gapping responses on the Take A, Take B and 
Introduce condition varied considerably. This 
variation seems to be due to differences between 
the features of the NPs involved. In particular, in 
the Take A condition the feature animacy played 
a role, which seems to have a stronger effect than 
the other grammatical features that were manipu-
lated. However, our constraint Featural Parallel-
ism does not distinguish between animacy and 
other grammatical features. Moreover, our OT 
model is unable to capture the gradience that 
seems to result from the interaction between fea-
tures. 
4.4 Generating different predictions 
Because the four evaluation algorithms behaved 
identically on all sentences taken from the corpus 
as well as on all sentences types from Carlson?s 
6
study, we had to construct sentences on the basis 
of expected constraint violations in order to gen-
erate different predictions for the four evaluation 
algorithms. The following sentence is predicted 
to distinguish between strict domination and lo-
cal restricted cumulativity: 
 
(13) John picked a rose, and a tulip Paul. 
 
If hearers interpret this sentence as meaning that 
a tulip picked Paul, they will have violated the 
stronger constraint Thematic Selection in order 
to satisfy the two weaker constraints Subject 
Precedes Object and Syntactic Parallelism. This 
then would constitute evidence for local re-
stricted cumulativity. If, on the other hand, hear-
ers interpret this sentence as meaning that Paul 
picked a tulip, then this is evidence for strict 
domination. Sentence (14) distinguishes between 
strict domination and global restricted cumulativ-
ity: 
 
(14) John asked him to get Paul, and George 
to bring Ringo. 
 
Because him is a pronoun, it counts as given for 
evaluating the constraint FSP. If hearers interpret 
this sentence as meaning that John asked George 
to bring Ringo, they will have violated the 
stronger constraint FSP in order to satisfy the 
weaker constraints Minimal Distance and Sub-
ject-Predicate Interpretation. Because FSP does 
not immediately dominate the weaker con-
straints, this would be evidence for global re-
stricted cumulativity. To distinguish between 
strict domination and Keller?s counting cumula-
tivity, consider the following sentence: 
 
(15) The children promised John to stop, and 
the neighbors to continue. 
 
If hearers interpret this sentence as meaning that 
the neighbors promised John to continue, they 
violate the single stronger constraint Minimal 
Distance in favor of satisfaction of the two 
weaker constraints Subject-Predicate Interpreta-
tion and Featural Parallelism. Because these con-
straints would all be considered soft constraints 
according to Keller?s distinction between hard 
and soft constraints, Keller?s counting cumulativ-
ity predicts that this interpretation is preferred. 
The strict domination hypothesis, on the other 
hand, predicts that the interpretation is preferred 
according to which the children promised the 
neighbors to continue, since it is more important 
to satisfy the stronger constraint Minimal Dis-
tance than any number of weaker constraints.  
5 Results and discussion 
For all Gapping sentences occurring in the Eind-
hoven corpus and all Gapping sentences taken 
from the written part of Carlson?s psycholinguis-
tic study, the four evaluation algorithms yielded 
identical results. These sentences therefore do 
not shed any light on the central question of this 
study, namely whether the strict domination hy-
pothesis should be abandoned in favor of a 
weaker cumulativity hypothesis. 
To determine which evaluation algorithm 
models the way comprehenders process language 
best, we must look at the interpretations of sen-
tences such as (13), (14) and (15). We presented 
10 participants with a written questionnaire, 
which included 15 sentences distinguishing be-
tween the four evaluation algorithms. The reader 
is referred to van der Feen (2005) for the com-
plete list of sentences. The results show that 
there does not seem to be a clear preference in 
interpretation for sentences such as (13), leaving 
the distinction between strict domination and 
local restricted cumulativity undecided. For sen-
tences such as (14), on the other hand, there 
seems to be a clear preference for the reading 
supported by global restricted cumulativity. Sen-
tences such as (15), finally, show no effects at all 
of Keller?s counting cumulativity. For only one 
sentence only one subject preferred the interpre-
tation according to which the neighbors promised 
John to continue, which favors the strict domina-
tion hypothesis and goes against Keller?s cumu-
lativity algorithm. This suggests that constraints 
on comprehension may be different from the 
principles governing acceptability judgments. 
Boersma (2004) argues that the paralinguistic 
task of providing acceptability judgments in-
volves comprehension, but under a reverse map-
ping between meaning and form. An alternative 
view is that acceptability judgments involve a 
mapping from the given form to its optimal 
meaning (?what do I think the sentence means??), 
followed by a mapping from that meaning to the 
optimal form for that meaning (?how would I 
express that meaning??), thus involving princi-
ples of comprehension as well as production.  
To conclude, there seems to be a slight indica-
tion of global restricted cumulativity in the com-
prehension of Gapping, but further study with a 
larger pool of subjects is required to confirm 
these initial findings.  
7
However, a few remarks are in place here. 
First, note that for hearers to prefer the interpre-
tation that Paul picked a tulip for (13), the hearer 
has to find some motivation in the linguistic con-
text of the utterance for why the speaker chose to 
put the object first. In the absence of such a con-
text supporting a non-canonical word order, the 
reading that Paul picked a tulip might be dis-
preferred anyway. 
Also in sentence (14), context seems to play a 
crucial role. Although in general pronouns may 
be used to refer to given material, in certain con-
texts pronouns can be emphatically stressed. If 
the pronoun in (14) is stressed, it is much easier 
to couple George to him to obtain the reading 
that John asked George to bring Ringo. This ef-
fect of context and prosody may have been the 
main reason for the observed preferences. 
6 Conclusion 
A central principle of Optimality Theory is the 
hypothesis of strict domination among con-
straints. In this paper we investigated whether 
this hypothesis should be abandoned in favor of 
the weaker hypothesis that constraint violations 
are cumulative. Studying the effects of four dif-
ferent evaluation algorithms (three of which dis-
play some kind of cumulativity) on the compre-
hension of Gapping sentences, we found a slight 
indication of cumulativity effects. However, 
these effects are likely to disappear if the context 
and prosodic structure of the utterance are taken 
into account. 
Acknowledgments 
The authors thank three anonymous reviewers 
for their useful comments and suggestions. This 
research was funded by grant # 015.001.103 
from  NWO, awarded to Petra Hendriks.  
References 
Paul Boersma. 2004. A stochastic OT account of 
paralinguistic tasks such as grammaticality and 
prototypicality judgments. Unpublished manu-
script, University of Amsterdam. Rutgers Optimal-
ity Archive #648. 
Katy Carlson. 2001. The Effects of Parallelism and 
Prosody in the Processing of Gapping Structures. 
Language and Speech, 44(1):1-26. 
John Hoeks, Petra Hendriks, and Louisa Zijlstra. 
2006. The Predominance of Nonstructural Factors 
in the Processing of Gapping Sentences. In: R. Sun 
and N. Miyake (eds.), Proceedings of the 28th An-
nual Conference of the Cognitive Science Society. 
John Hoeks, and Petra Hendriks. 2005. Optimality 
Theory and human sentence processing: The case 
of coordination. In: B.G. Bara, L. Barsalou, and M. 
Bucciarelli (eds.), Proceedings of the 27th Annual 
Meeting of the Cognitive Science Society, Erlbaum, 
Mahwah, NJ, pp. 959-964. 
Gerhard J?ger, and Anette Rosenbach. To appear. The 
winner takes it all - almost. Cumulativity in gram-
matical variation. Linguistics. 
Frank Keller. 1998. Gradient Grammaticality as an 
Effect of Selective Constraint Re-ranking. In: M.C. 
Gruber, D. Higgins, K.S. Olson, and T. Wysocki 
(eds.) Papers from the 34th Meeting of the Chicago 
Linguistic Society. Vol. 2: The Panels, Chicago, pp. 
95-109. 
Frank Keller. 2001. Experimental Evidence for Con-
straint Competition in Gapping Constructions. In: 
G. M?ller and W. Sternefeld (eds.), Competition in 
Syntax, Mouton de Gruyter, Berlin, pp. 211-248. 
Susumo Kuno. 1976. Gapping: A Functional Analy-
sis. Linguistic Inquiry, 7:300-318. 
Monique Lamers, and Helen de Hoop. 2004. The role 
of animacy information in human sentence proc-
essing captured in four conflicting constraints. In: 
H. Christiansen, P. Rossen Skadhauge, and J. Vil-
ladsen (eds.), Constraint Solving and Language 
Processing. Workshop proceedings, Roskilde De-
partment of Computer Science, Roskilde Univer-
sity, pp. 102-113. 
G?raldine Legendre, Yoshiro Miyata, and Paul 
Smolensky. 1990. Harmonic Grammar - A formal 
multi-level theory of linguistic well-formedness: 
An application. In: Proceedings of the Twelfth An-
nual Conference of the Cognitive Science Society, 
Erlbaum, Cambridge, MA, pp. 388-395. 
G?raldine Legendre, Antonella Sorace, and Paul 
Smolensky. 2006. The Optimality Theory - Har-
monic Grammar Connection. In: P. Smolensky and 
G. Legendre (eds.), The Harmonic Mind, Vol. 2, 
MIT Press, Cambridge, MA, pp. 339-402. 
Alan Prince, and Paul Smolensky. 2004. Optimality 
Theory: Constraint interaction in generative 
grammar. Oxford, Blackwell. Previously distrib-
uted as Technical Report RuCCSTR-2, New 
Brunswick NJ, Rutgers Center for Cognitive Sci-
ence, Rutgers University, 1993. 
P.C. Uit den Boogaart. 1975. Woordfrequenties in 
geschreven en gesproken Nederlands. Werkgroep 
Frequentie-onderzoek van het Nederlands. Oost-
hoek, Scheltema & Holkema, Utrecht. 
Marieke Van der Feen. 2005. Do rules add up? A 
study of the application of Optimality Theory to the 
interpretation of gapping. MSc Thesis Artificial 
Intelligence, University of Groningen. 
 
8
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 72?80,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Modeling the Noun Phrase versus Sentence Coordination Ambiguity in
Dutch: Evidence from Surprisal Theory
Harm Brouwer
University of Groningen
Groningen, the Netherlands
harm.brouwer@rug.nl
Hartmut Fitz
University of Groningen
Groningen, the Netherlands
h.fitz@rug.nl
John C. J. Hoeks
University of Groningen
Groningen, the Netherlands
j.c.j.hoeks@rug.nl
Abstract
This paper investigates whether surprisal
theory can account for differential pro-
cessing difficulty in the NP-/S-coordina-
tion ambiguity in Dutch. Surprisal is es-
timated using a Probabilistic Context-Free
Grammar (PCFG), which is induced from
an automatically annotated corpus. We
find that our lexicalized surprisal model
can account for the reading time data from
a classic experiment on this ambiguity by
Frazier (1987). We argue that syntactic
and lexical probabilities, as specified in a
PCFG, are sufficient to account for what is
commonly referred to as an NP-coordina-
tion preference.
1 Introduction
Language comprehension is incremental in that
meaning is continuously assigned to utterances
as they are encountered word-by-word (Altmann
and Kamide, 1999). Not all words, however, are
equally easy to process. A word?s processing dif-
ficulty is affected by, for instance, its frequency or
its effect on the syntactic and semantic interpreta-
tion of a sentence. A recent theory of sentence pro-
cessing, surprisal theory (Hale, 2001; Levy, 2008),
combines several of these aspects into one single
concept, namely the surprisal of a word. A word?s
surprisal is proportional to its expectancy, i.e., the
extent to which that word is expected (or pre-
dicted). The processing difficulty a word causes
during comprehension is argued to be related lin-
early to its surprisal; the higher the surprisal value
of a word, the more difficult it is to process.
In this paper we investigate whether surprisal
theory can account for the processing difficulty
involved in sentences containing the noun phrase
(NP) versus sentence (S) coordination ambiguity.
The sentences in (1), from a self-paced reading ex-
periment by Frazier (1987), exemplify this ambi-
guity:
(1) a. Piet
Piet
kuste
kissed
Marie
Marie
en
and
/
/
haar zusje
her sister
/
/
ook
too
[1,222ms; NP-coordination]
b. Piet
Piet
kuste
kissed
Marie
Marie
en
and
/
/
haar zusje
her sister
/
/
lachte
laughed
[1,596ms; S-coordination]
Both sentences are temporarily ambiguous in the
boldface region. Sentence (1-a) is disambiguated
as an NP-coordination by the sentence-final ad-
verb ook. Sentence (1-b), on the other hand, is dis-
ambiguated as an S-coordination by the sentence-
final verb lachte. Frazier found that the verb lachte
in sentence (1-b) takes longer to process (1,596
ms) than the adverb ook (1,222 ms) in (1-a).
Frazier (1987) explained these findings by as-
suming that the human language processor ad-
heres to the so-called minimal attachment prin-
ciple. According to this principle, the sentence
processor projects the simplest syntactic struc-
ture which is compatible with the material read
at any point in time. NP-coordination is syntac-
tically simpler than S-coordination in that it re-
quires less phrasal nodes to be projected. Hence,
the processor is biased towards NP- over S-coor-
dination. Processing costs are incurred when this
initial preference has to be revised in the disam-
biguating region, as in sentence (1-b), resulting in
longer reading times. Hoeks et al (2006) have
shown that the NP-coordination preference can be
reduced, but not entirely eliminated, when poor
thematic fit between the verb and a potential object
make an NP-coordination less likely (e.g., Jasper
sanded the board and the carpenter laughed). We
argue here that this residual preference for NP-
coordination can be explained in terms of syntac-
tic and lexical expectation within the framework
of surprisal theory. In contrast to the minimal at-
tachment principle, surprisal theory does not pos-
72
tulate specific kinds of syntactic representations or
rely on a metric of syntactic complexity to predict
processing behavior.
This paper is organized as follows. In section
2 below, we briefly sketch basic surprisal theory.
Then we describe how we induced a grammar
from a large annotated Dutch corpus and how sur-
prisal was estimated from this grammar (section
3). In section 4, we describe Frazier?s experiment
on the NP-/S-coordination ambiguity in more de-
tail, and present our surprisal-based simulations of
this data. We conclude with a discussion of our re-
sults in section 5.
2 Surprisal Theory
As was mentioned in the introduction, language
processing is highly incremental, and proceeds on
a more or less word-by-word basis. This suggests
that a person?s difficulty with processing a sen-
tence can be modeled on a word level as proposed
by Attneave (1959). Furthermore, it has recently
been suggested that one of the characteristics of
the comprehension system that makes it so fast,
is its ability to anticipate what a speaker will say
next. In other words, the language comprehension
system works predictively (Otten et al, 2007; van
Berkum et al, 2005). Surprisal theory is a model
of differential processing difficulty which accom-
modates both these properties of the comprehen-
sion system, incremental processing and word pre-
diction (Hale, 2001; Levy, 2008). In this theory,
the processing difficulty of a sentence is a func-
tion of word processing difficulty. A word?s dif-
ficulty is inversely proportional to its expectancy,
i.e., the extent to which the word was expected or
predicted in the context in which it occurred. The
lower a word?s expectancy, the more difficult it is
to process. A word?s surprisal is linearly related to
its difficulty. Consequently, words with lower con-
ditional probabilities (expectancy) lead to higher
surprisal than words with higher conditional prob-
abilities.
Surprisal theory is, to some extent, indepen-
dent of the language model that generates condi-
tional word probabilities. Different models can
be used to estimate these probabilities. For all
such models, however, a clear distinction can be
made between lexicalized and unlexicalized sur-
prisal. In lexicalized surprisal, the input to the lan-
guage model is a sequence of words (i.e., a sen-
tence). In unlexicalized surprisal, the input is a
sequence of word categories (i.e., part-of-speech
tags). While previous studies have used unlexical-
ized surprisal to predict reading times, evidence
for lexicalized surprisal is rather sparse. Smith
and Levy (2008) investigated the relation between
lexicalized surprisal and reading time data for nat-
uralistic texts. Using a trigram language model,
they showed that there was a linear relationship
between the two measures. Demberg and Keller
(2008) examined whether this relation extended
beyond transitional probabilities and found no sig-
nificant effects. This state of affairs is somewhat
unfortunate for surprisal theory since input to the
human language processor consists of sequences
of words, not part-of-speech tags. In our study we
therefore used lexicalized surprisal to investigate
whether it can account for reading time data from
the NP-/S-coordination ambiguity in Dutch. Lex-
icalized surprisal furthermore allows us to study
how syntactic expectations might be modulated or
even reversed by lexical expectations in temporar-
ily ambiguous sentences.
2.1 Probabilistic Context Free Grammars
Both Hale (2001) and Levy (2008) used a Prob-
abilistic Context Free Grammar (PCFG) as a lan-
guage model in their implementations of surprisal
theory. A PCFG consists of a set of rewrite rules
which are assigned some probability (Charniak,
1993):
S ? NP, VP 1.0
NP ? Det, N 0.5
NP ? NP, VP 0.5
. . . ? . . . . . .
In this toy grammar, for instance, a noun phrase
placeholder can be rewritten to a determiner fol-
lowed by a noun symbol with probability 0.5.
From such a PCFG, the probability of a sentence
can be estimated as the product of the probabili-
ties of all the rules used to derive the sentence. If
a sentence has multiple derivations, its probabil-
ity is the sum of the probabilities for each deriva-
tion. For our purpose, we also needed to obtain the
probability of partial sentences, called prefix prob-
abilities. The prefix probability P (w1...wi) of a
partial sentence w1...wi is the sum of the probabil-
ities of all sentences generated by the PCFG which
share the initial segment w1...wi. Hale (2001)
pointed out that the ratio of the prefix probabilities
P (w1 . . . wi) and P (w1 . . . wi?1) equals precisely
the conditional probability of word wi. Given a
73
PCFG, the difficulty of word wi can therefore be
defined as:
difficulty(wi) ? ?log2
[
P (w1 . . . wi)
P (w1 . . . wi?1)
]
.
Surprisal theory requires a probabilistic lan-
guage model that generates some form of word
expectancy. The theory itself, however, is largely
neutral with respect to which model is employed.
Models other than PCFGs can be used to esti-
mate surprisal. Nederhof et al (1998), for in-
stance, show that prefix probabilities, and there-
fore surprisal, can be estimated from Tree Adjoin-
ing Grammars. This approach was taken in Dem-
berg and Keller (2009). Other approaches have
used trigram models (Smith and Levy, 2008), Sim-
ple Recurrent Networks of the Elman type (Frank,
2009), Markov models and Echo-state Networks
(Frank and Bod, 2010). This illustrates that sur-
prisal theory is not committed to specific claims
about the structural representations that language
takes in the human mind. It rather functions as a
?causal bottleneck? between the representations of
a language model, and expectation-based compre-
hension difficulty (Levy, 2008). In other words,
comprehension difficulty does not critically de-
pend on the structural representations postulated
by the language model which is harnessed to gen-
erate word expectancy.
The use of PCFGs raises some important ques-
tions on parallelism in language processing. A
prefix probability can be interpreted as a prob-
ability distribution over all analyses compatible
with a partial sentence. Since partial sentences
can sometimes be completed in an indefinite num-
ber of ways, it seems both practically and psycho-
logically implausible to implement this distribu-
tion as an enumeration over complete structures.
Instead, prefix probabilities should be estimated
as a by-product of incremental processing, as in
Stolcke?s (1995) parser (see section 3.2). This
approach, however, still leaves open how many
analyses are considered in parallel; does the hu-
man sentence processor employ full or limited par-
allelism? Jurafsky (1996) showed that full par-
allelism becomes more and more unmanageable
when the amount of information used for disam-
biguation increases. Levy, on the other hand, ar-
gued that studies of probabilistic parsing reveal
that typically a small number of analyses are as-
signed the majority of probability mass (Roark,
2001). Thus, even when assuming full parallelism,
only a small number of ?relevant? analyses would
be considered in parallel.
3 Grammar and Parser
3.1 Grammar Induction
In our simulations, we used a PCFG to model
the phrase structure of natural language. To in-
duce such a grammar, an annotated corpus was
required. We used Alpino (van Noord, 2006)?
a robust and wide-coverage dependency parser
for Dutch?to automatically generate such a cor-
pus, annotated with phrase structure, for 204.000
sentences, which were randomly extracted from
Dutch newspapers. These analyses were then
used to induce a PCFG consisting of 650 gram-
mar rules, 89 non-terminals, and 208.133 termi-
nals (lexical items).1 Moreover, 29 of the 89 non-
terminals could result in epsilon productions.
The Alpino parser constructed the phrase struc-
ture analyses automatically. Despite Alpino?s high
accuracy, some analyses might not be entirely cor-
rect. Nonetheless, the overall quality of Alpino?s
analyses is sufficient for corpus studies, and since
surprisal theory relies largely on corpus features,
we believe the small number of (partially) incor-
rect analyses should not affect the surprisal esti-
mates computed from our PCFG.
3.2 Earley-Stolcke Parser
To compute prefix probabilities in our model we
implemented Stolcke?s (1995) probabilistic modi-
fication of Earley?s (1970) parsing algorithm. An
Earley-Stolcke parser is a breadth-first parser. At
each point in processing, the parser maintains a
collection of states that reflect all possible analy-
ses of a partial sentence thus far. A state is a record
that keeps track of:
(a) the position up to which a sentence has been
processed,
(b) the grammar rule that is applied,
(c) a ?dot position? indicating which part of the
rule has been processed thus far, and
(d) the leftmost edge of the partial string gener-
ated by the rule.
1A PCFG can be induced by estimating the relative fre-
quency of each CFG rule A? ?:
P (A? ?) = count(A??)?
?
count(A??)
.
74
The collection of states is constantly expanded by
three operations. First upcoming structural and
lexical material is predicted. For all predictions,
new states are added with the ?dot? placed on
the leftmost side of the rule. Then it is deter-
mined whether there is a state that predicts the next
word in the input sentence. If this is the case, a
new state is added with the ?dot? placed right to
the predicted word. A third operation looks for
states with the ?dot? rightmost to a grammar rule,
and then tries to find states which have the com-
pleted state as their leftmost edge. If such states
are found, the ?dot? in these states is moved to
the right of this edge. This step is repeated until
no more new states are added. These three op-
erations are cyclically performed until the entire
sentence is processed. Our grammar contained
29 non-terminals that could result in epsilon pro-
ductions. Due to the way epsilon productions are
handled within the Earley-Stolcke parser (i.e., by
means of ?spontaneous dot shifting?), having a
large number of epsilon productions leads to a
large number of predicted and completed edges.
As a consequence, pursuing all possible analyses
may become computationally infeasible. To over-
come this problem, we modified the Earley-Stol-
cke parser with a beam ?. In prediction and com-
pletion, only the ?-number of states with the high-
est probabilities are added.2 This constrains the
number of states generated by the parser and en-
forces limited parallelism.
4 NP-/S-coordination ambiguities
4.1 Frazier?s experiment
Our aim was to determine to what extent lexi-
calized surprisal theory can account for reading
time data for the NP-/S-coordination ambiguity in
Dutch. This type of ambiguity was investigated
by Frazier (1987) using a self-paced reading ex-
periment. The sentences in (2) are part of Fra-
zier?s materials. Sentence (2-a) and (2-b) exem-
plify an NP-/S-coordination ambiguity. The sen-
tences are identical and temporarily ambiguous up
to the NP haar zusje (her sister). In (2-a) this
NP is followed by the adverb ook, and therefore
disambiguated to be part of an NP-coordination;
Marie and haar zusje are conjoined. In (2-b), on
other hand, the same NP is followed by the verb
lachte, and therefore disambiguated as the sub-
2A similar approach was used in Roark (2001) and
Frank (2009).
ject of a conjoined sentence; Piet kuste Marie and
haar zusje lachte are conjoined.
(2) a. Piet
Pete
kuste
kissed
Marie
Marie
en
and
haar
her
zusje
sister
ook
too
(Ambiguous; NP-coordination)
b. Piet
Pete
kuste
kissed
Marie
Marie
en
and
haar
her
zusje
sister
lachte
laughed
(Ambiguous; S-coordination)
c. Annie
Annie
zag
saw
haar
her
zusje
sister
ook
too
(Unambiguous; NP-control)
d. Annie
Annie
zag
saw
dat
that
haar
her
zusje
sister
lachte
laughed
(Unambiguous; S-control)
Sentence (2-c) and (2-d) functioned as unambigu-
ous controls. These sentences are identical up to
the verb zag. In (2-c), the verb is followed by
the single NP haar zusje, and subsequently the ad-
verb ook. The adverb eliminates the possibility of
an NP-coordination. In (2-d), on the other hand,
the same verb is followed by the complementizer
dat, indicating that the clause her sister laughed is
a subordinate clause (the complementizer is oblig-
atory in Dutch).
Frazier constructed twelve sets consisting of
four of such sentences each. The 48 sentences
were divided into three frames. The first frame
included all the material up to the critical NP
haar zusje in (2). The second frame contained only
the critical NP itself, and the third frame contained
all the material that followed this NP.
40 native Dutch speakers participated in the ex-
periment. Reading times for the final frames were
collected using a self-paced reading task. Figure 1
depicts the mean reading times for each of the four
conditions.
Frazier found a significant interaction between
Type of Coordination (NP- versus S-coordination)
and Ambiguity (ambiguous versus control) indi-
cating that the effect of disambiguation was larger
for S-coordinations (ambiguous: 1596 ms; con-
trol: 1141 ms) than for NP-coordinations (ambigu-
ous: 1222 ms; control: 1082 ms).
4.2 Simulations
We simulated Frazier?s experiment in our model.
Since one set of sentences contained a word that
was not covered by our lexicon (set 11; ?Lor-
raine?), we used only eleven of the twelve sets
of test items from her study. The remaining 44
sentences were successfully analyzed. In our first
75
NP?coord/control S?coord/control
type of coordination
m
ea
n 
re
ad
ing
 tim
es
 (m
s)
40
0
80
0
12
00
16
00
ambiguous
unambiguous
Figure 1: Reading time data for the NP-/S-coordi-
nation ambiguity (Frazier, 1987).
simulation we fixed a beam of ? = 16. Figure 2
depicts surprisal values in the sentence-final frame
as estimated by our model. When final frames
contained multiple words, we averaged the sur-
prisal values for these words. As Figure 2 shows,
NP?coord/control S?coord/control
type of coordination
m
ea
n 
su
rpr
isa
l
50
00
55
00
60
00
65
00
70
00
75
00
ambiguous
unambiguous
Figure 2: Mean surprisal values for the final frame
in the model (? = 16).
our model successfully replicated the effects re-
ported in Frazier (1987): In both types of coordi-
nations there was a difference in mean surprisal
between the ambiguous sentences and the con-
trols, but in the S-coordinations this effect was
larger than in the sentences with NP-coordination.
Statistical analyses confirmed our findings. An
ANOVA on surprisal values per item revealed an
interaction between Type of Coordination (NP- vs.
S-coordination) and Ambiguity (ambiguous vs.
control), which was marginally significant (p =
0.06), most probably due to the small number of
beam
dif
fer
en
ce
 in
 m
ea
ns
 (N
P* 
? S
*)
?600
?400
?200
0
200
32 16 8 4
NP?/S?control
NP?/S?coordination
Figure 3: Differences between NP versus S sur-
prisal for different beam sizes (?s).
items (i.e., 11) available for this statistical test (re-
call that the test in the original experiment was
based on 40 participants). Follow-up analyses re-
vealed that the difference between S-coordination
and S-control was significant (p < 0.05), whereas
the difference between NP-coordination and NP-
control was not (p = 0.527).
To test the robustness of these findings, we re-
peated the simulation with different beam sizes
(?s) by iteratively halving the beam, starting with
? = 32. Figure 3 shows the differences in
mean surprisal between NP-coordination and S-
coordination, and NP-control and S-control. With
the beam set to four (? = 4), we did not obtain full
analyses for all test items. Consequently, two sets
of items had to be disregarded (sets 8 and 9). For
the remaining items, however, we obtained an NP-
coordination preference for all beam sizes. The
largest difference occurred for ? = 16. When
the beam was set to ? ? 8, the difference stabi-
lized. Taking everything into account, the model
with ? = 16 led to the best overall match with
Frazier?s reading time data.
As for the interaction, Figure 4 depicts the dif-
ferences in mean surprisal between NP-coordina-
tion and NP-control, and S-coordination and S-
control. These results indicate that we robustly
replicated the interaction between coordination
type and ambiguity. For all beam sizes, S-co-
ordination benefited more from disambiguation
than NP-coordination, i.e., the difference in means
between S-coordination and S-control was larger
76
beam
dif
fer
en
ce
 in
 m
ea
ns
 (*?
coo
rd. 
? *
?co
ntro
l)
0
500
1000
1500
32 16 8 4
NP?coordination/NP?controlS?coordination/S?control
Figure 4: Differences in coordination versus con-
trol surprisal for different beam sizes (?s).
than the difference in means between NP-coordi-
nation and NP-control.
In our simulations, we found that surprisal the-
ory can account for reading time data from a clas-
sic experiment on the NP-/S-coordination ambigu-
ity in Dutch reported by Frazier (1987). This sug-
gests that the interplay between syntactic and lex-
ical expectancy might be sufficient to explain an
NP-coordination preference in human subjects. In
the remainder of this section, we analyze our re-
sults and explain how this preference arises in the
model.
4.3 Model Analysis
To determine what caused the NP-preference in
our model, we inspected surprisal differences
item-by-item. Whether the NP-coordination pref-
erence was syntactic or lexical in nature should
be reflected in the grammar. If it was syntactic,
NP-coordination would have a higher probability
than S-coordination according to our PCFG. If, on
the other hand, it was lexical, NP- and S-coor-
dination should be equally probable syntactically.
Another possibility, however, is that syntactic and
lexical probabilities interacted. If this was the
case, we should expect NP-coordinations to lead
to lower surprisal values on average only, but not
necessarily on every item. Figure 5 shows the es-
timated surprisal values per sentence-final frame
for the ambiguous condition and Figure 6 for the
unambiguous condition. Figure 5 indicates that
although NP-coordination led to lower surprisal
sentences
su
rpr
isa
ls
5000
6000
7000
8000
1 2 3 4 5 6 7 8 9 10 12
NP?coordinationS?coordination
Figure 5: Surprisal per sentence for final frames in
the ambiguous condition.
sentences
su
rpr
isa
ls
5000
6000
7000
1 2 3 4 5 6 7 8 9 10 12
NP?controlS?control
Figure 6: Surprisal per sentence for final frames in
the unambiguous condition.
overall (see Figure 2), this was not the case for all
tested items. A similar pattern was found for the
NP-control versus S-control items in Figure 6. S-
controls led to lower surprisal overall, but not for
all items. Manual inspection of the grammar re-
vealed a bias towards NP-coordination. A total of
115 PCFG rules concerned coordination (? 18%
of the entire grammar). As these rules expanded
the same grammatical category, their probabilities
summed to 1. A rule-by-rule inspection showed
that approximately 48% of the probability mass
was assigned to rules that dealt with NP-coordi-
nations, 22% to rules that dealt with S-coordina-
tions, and the remaining 30% to rules that dealt
with coordination in other structures. In other
77
words, there was a clear preference for NP-coordi-
nation in the grammar. Despite this bias, for some
tested items the S-coordination received lower sur-
prisal than the NP-coordination (Figure 5). In-
dividual NP-coordination rules might have lower
probability than individual S-coordination rules,
so the overall preference for NP-coordination in
the grammar therefore does not have to be re-
flected in every test item. Secondly, syntactic
probabilities could be modified by lexical proba-
bilities. Suppose for a pair of test items that NP-
coordination was syntactically preferred over S-
coordination. If the sentence was disambiguated
as an NP-coordination by a highly improbable lex-
ical item, and disambiguated as an S-coordination
by a highly probable lexical item, surprisal for the
NP-coordination might turn out higher than sur-
prisal for the S-coordination. In this way, lexical
factors could override the NP-coordination bias in
the grammar, leading to a preference for S-coordi-
nation in some items.
To summarize, the PCFG displayed an over-
all NP-coordination preference when surprisal was
averaged over the test sentences and this result is
consistent with the findings of Frazier (1987). The
NP-coordination preference, however, was not in-
variably reflected on an item-by-item basis. Some
S-coordinations showed lower surprisal than the
corresponding NP-coordinations. This reversal of
processing difficulty can be explained in terms of
differences in individual rules, and in terms of in-
teractions between syntactic and lexical probabil-
ities. This suggests that specific lexical expecta-
tions might have a much stronger effect on disam-
biguation preferences than supposed by the min-
imal attachment principle. Unfortunately, Frazier
(1987) only reported mean reading times for the
two coordination types.3 It would be interesting to
compare the predictions from our surprisal model
with human data item-by-item in order to validate
the magnitude of lexical effects we found in the
model.
5 Discussion
In this paper we have shown that a model of lex-
icalized surprisal, based on an automatically in-
duced PCFG, can account for the NP-/S-ambiguity
reading time data of Frazier (1987). We found
3Thus it was not possible to determine the strength of the
correlation between reading times in Frazier?s study and sur-
prisal in our model.
these results to be robust for a critical model pa-
rameter (beam size), which suggests that syntac-
tic processing in human comprehension might be
based on limited parallelism only. Surprisal the-
ory models processing difficulty on a word level.
A word?s difficulty is related to the expectations
the language processor forms, given the structural
and lexical material that precedes it. The model
showed a clear preference for NP-coordination
which suggests that structural and lexical expec-
tations as estimated from a corpus might be suffi-
cient to explain the NP-coordination bias in human
sentence processing.
Our account of this bias differs considerably
from the original account proposed by Frazier
(minimal attachment principle) in a number of
ways. Frazier?s explanation is based on a met-
ric of syntactic complexity which in turn depends
on quite specific syntactic representations of a
language?s phrase structure. Surprisal theory, on
the other hand, is largely neutral with respect to
the form syntactic representations take in the hu-
man mind.4 Moreover, differential processing in
surprisal-based models does not require the speci-
fication of a notion of syntactic complexity. Both
these aspects make surprisal theory a parsimo-
nious explanatory framework. The minimal at-
tachment principle postulates that the bias towards
NP-coordination is an initial processing primitive.
In contrast, the bias in our simulations is a func-
tion of the model?s input history and linguistic
experience from which the grammar is induced.
It is further modulated by the immediate context
from which upcoming words are predicted dur-
ing processing. Consequently, the model?s prefer-
ence for one structural type can vary across sen-
tence tokens and even be reversed on occasion.
We argued that our grammar showed an over-
all preference for NP-coordination but this pref-
erence was not necessarily reflected on each and
every rule that dealt with coordinations. Some S-
coordination rules could have higher probability
than NP-coordination rules. In addition, syntac-
tic expectations were modified by lexical expec-
tations. Thus, even when NP-coordination was
structurally favored over S-coordination, highly
unexpected lexical material could lead to more
processing difficulty for NP-coordination than for
4This is not to say, of course, that the choice of language
model to estimate surprisal is completely irrelevant; differ-
ent models will yield different degrees of fit, see Frank and
Bod (2010).
78
S-coordination. Surprisal theory allows us to build
a formally precise computational model of read-
ing time data which generates testable, quantita-
tive predictions about the differential processing
of individual test items. These predictions (Figure
5) indicate that mean reading times for a set of NP-
/S-coordination sentences may not be adequate to
tap the origin of differential processing difficulty.
Our results are consistent with the findings of
Hoeks et al (2002), who also found evidence
for an NP-coordination preference in a self-paced
reading experiment as well as in an eye-tracking
experiment. They suggested that NP-coordination
might be easier to process because it has a sim-
pler topic structure than S-coordination. The for-
mer only has one topic, whereas the latter has two.
Hoeks et al (2002) argue that having more than
one topic is unexpected. Sentences with more than
one topic will therefore cause more processing dif-
ficulty. This preference for simple topic-structure
that was evident in language comprehension may
also be present in language production, and hence
in language corpora. Thus, it may very well be
the case that the NP-coordination preference that
was present in our training corpus may have had
a pragmatic origin related to topic-structure. The
outcome of our surprisal model is also compati-
ble with the results of Hoeks et al (2006) who
found that thematic information can strongly re-
duce but not completely eliminate the NP-coordi-
nation preference. Surprisal theory is explicitly
built on the assumption that multiple sources of
information can interact in parallel at any point in
time during sentence processing. Accordingly, we
suggest here that the residual preference for NP-
coordination found in the study of Hoeks et al
(2006) might be explained in terms of syntactic
and lexical expectation. And finally, our approach
is consistent with a large body of evidence indi-
cating that language comprehension is incremen-
tal and makes use of expectation-driven word pre-
diction (Pickering and Garrod, 2007). It remains
to be tested whether our model can explain behav-
ioral data from the processing of ambiguities other
than the Dutch NP- versus S-coordination case.
References
G. Altmann and Y. Kamide. 1999. Incremental inter-
pretation at verbs: Restricting the domain of subse-
quent reference. Cognition, 73:247?264.
F. Attneave. 1959. Applications of Information Theory
to Psychology: A summary of basic concepts, meth-
ods, and results. Holt, Rinehart and Winston.
E. Charniak. 1993. Statistical Language Learning.
MIT Press.
V. Demberg and F. Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109:193?210.
V. Demberg and F. Keller. 2009. A computational
model of prediction in human parsing: Unifying lo-
cality and surprisal effects. In Proceedings of the
31st Annual Conference of the Cognitive Science So-
ciety, Amsterdam, the Netherlands.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6:451?455.
S. Frank and R. Bod. 2010. The irrelevance of hi-
erarchical structure to human sentence processing.
Unpublished manuscript.
S. Frank. 2009. Surprisal-based comparison between a
symbolic and a connectionist model of sentence pro-
cessing. In Proceedings of the 31th Annual Confer-
ence of the Cognitive Science Society, pages 1139?
1144, Amsterdam, the Netherlands.
L. Frazier. 1987. Syntactic processing: Evidence from
Dutch. Natural Langauge and Linguistic Theory,
5:519?559.
J. Hale. 2001. A probabilistic Earley parser as a psy-
cholinguistic model. In Proceedings of the 2nd Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics, volume 2,
pages 159?166.
J. Hoeks, W. Vonk, and H. Schriefers. 2002. Process-
ing coordinated structures in context: The effect of
topic-structure on ambiguity resolution. Journal of
Memory and Language, 46:99?119.
J. Hoeks, P. Hendriks, W. Vonk, C. Brown, and P. Ha-
goort. 2006. Processing the noun phrase versus sen-
tence coordination ambiguity: Thematic informa-
tion does not completely eliminate processing dif-
ficulty. The Quarterly Journal of Experimental Psy-
chology, 59:1581?1599.
D. Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science, 20:137?147.
R. Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106:1126?1177.
M. Nederhof, A. Sarkar, and G. Satta. 1998. Prefix
probabilities from stochastic tree adjoining gram-
mar. In Proceedings of COLING-ACL ?98, pages
953?959, Montreal.
M. Otten, M. Nieuwland, and J. van Berkum. 2007.
Great expectations: Specific lexical anticipation in-
fluences the processing of spoken language. BMC
Neuroscience.
79
M. Pickering and S. Garrod. 2007. Do people use lan-
guage production to make predictions during com-
prehension? Trends in Cognitive Sciences, 11:105?
110.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27:249?276.
N. Smith and R. Levy. 2008. Optimal processing times
in reading: A formal model and empirical investi-
gation. In Proceedings of the 30th annual confer-
ence of the Cognitive Science Society, pages 595?
600, Austin, TX.
A. Stolcke. 1995. An efficient probabilistic context-
free parsing algorithm that computes prefix proba-
bilities. Computational linguistics, 21:165?201.
J. van Berkum, C. Brown, P. Zwitserlood, V. Kooij-
man, and P. Hagoort. 2005. Anticipating upcom-
ing words in discourse: Evidence from ERPs and
reading times. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 31:443?467.
G. van Noord. 2006. At last parsing is now op-
erational. In Verbum Ex Machina. Actes de la
13e confe?rence sur le traitement automatique des
langues naturelles, pages 20?42. Presses universi-
taires de Louvain.
80
