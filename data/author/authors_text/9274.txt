A Comparative Evaluation of Data-driven Models in Translation
Selection of Machine Translation
?Yu-Seop Kim ? Jeong-Ho Chang ?Byoung-Tak Zhang
? Ewha Institute of Science and Technology, Ewha Woman?s Univ.
Seoul 120-750 Korea, yskim01@ewha.ac.kr?
? Schools of Computer Science and Engineering, Seoul National Univ.
Seoul 151-742 Korea, {jhchang, btzhang}@bi.snu.ac.kr?
Abstract
We present a comparative evaluation of two
data-driven models used in translation selec-
tion of English-Korean machine translation. La-
tent semantic analysis(LSA) and probabilistic
latent semantic analysis (PLSA) are applied for
the purpose of implementation of data-driven
models in particular. These models are able to
represent complex semantic structures of given
contexts, like text passages. Grammatical rela-
tionships, stored in dictionaries, are utilized in
translation selection essentially. We have used
k-nearest neighbor (k-NN) learning to select an
appropriate translation of the unseen instances
in the dictionary. The distance of instances in
k-NN is computed by estimating the similar-
ity measured by LSA and PLSA. For experi-
ments, we used TREC data(AP news in 1988)
for constructing latent semantic spaces of two
models and Wall Street Journal corpus for eval-
uating the translation accuracy in each model.
PLSA selected relatively more accurate transla-
tions than LSA in the experiment, irrespective
of the value of k and the types of grammatical
relationship.
1 Introduction
Construction of language associated resources
like thesaurus, annotated corpora, machine-
readable dictionary and etc. requires high de-
gree of cost, since they need much of human-
effort, which is also dependent heavily upon hu-
man intuition. A data-driven model, however,
does not demand any of human-knowledge,
knowledge bases, semantic thesaurus, syntac-
tic parser or the like. This model represents
? He is supported by Brain Korea 21 project performed
by Ewha Institute of Science and Technology.
? They are supported by Brain Tech. project controlled
by Korean Ministry of Science and Technology.
latent semantic structure of contexts like text
passages. Latent semantic analysis (LSA) (Lan-
dauer et al, 1998) and probabilistic latent se-
mantic analysis (PLSA) (Hofmann, 2001) fall
under the model.
LSA is a theory and method for extracting
and representing the contextual-usage meaning
of words. This method has been mainly used
for indexing and relevance estimation in infor-
mation retrieval area (Deerwester et al, 1990).
And LSA could be utilized to measure the co-
herence of texts (Foltz et al, 1998). By applying
the basic concept, a vector representation and
a cosine computation, to estimate relevance of
a word and/or a text and coherence of texts,
we could also estimate the semantic similarity
between words. It is claimed that LSA repre-
sents words of similar meaning in similar ways
(Landauer et al, 1998).
Probabilistic LSA (PLSA) is based on proba-
bilistic mixture decomposition while LSA is on a
linear algebra and singular value decomposition
(SVD) (Hofmann, 1999b). In contrast to LSA,
PLSA?s probabilistic variant has a sound statis-
tical foundation and defines a proper generative
model of the data. Both two techniques have
a same idea which is to map high-dimensional
vectors representing text documents, to a lower
dimensional representation, called a latent se-
mantic space (Hofmann, 1999a).
Dagan (Dagan et al, 1999) performed a com-
parative analysis of several similarity measures,
which based mainly on conditional probability
distribution. And the only elements in the dis-
tribution are words, which appeared in texts.
However, LSA and PLSA expressed the latent
semantic structures, called a topic of the con-
text.
In this paper, we comparatively evaluated
these two techniques performed in translation
selection of English-Korean machine transla-
tion. First, we built a dictionary storing tu-
ples representing the grammatical relationship
of two words, like subject-verb, object-verb, and
modifier-modified. Second, with an input tuple,
in which an input word would be translated and
the other would be used as an argument word,
translation is performed by searching the dic-
tionary with the argument word. Third, if the
argument word is not listed in the dictionary,
we used k-nearest neighbor learning method to
determine which class of translation is appro-
priate for the translation of an input word. The
distance used in discovering the nearest neigh-
bors was computed by estimating the similarity
measured on above latent semantic spaces.
In the experiment, we used 1988 AP news
corpus from TREC-7 data (Voorhees and Har-
man, 1998) for building latent semantic spaces
and Wall Street Journal (WSJ) corpus for con-
structing a dictionary and test sets. We ob-
tained 11-20% accuracy improvement, compar-
ing to a simple dictionary search method. And
PLSA has shown that its ability to select an ap-
propriate translation is superior to LSA as an
extent of up to 3%, without regard to the value
of k and grammatical relationship.
In section 2, we discuss two of data-driven
models, LSA and PLSA. Section 3 describes
ways of translation with a grammatical rela-
tion dictionary and k-nearest neighbor learning
method. Experiment is explained in Section 4
and concluding remarks are presented in Section
5.
2 Data-Driven Model
For the data-driven model which does not re-
quire additional human-knowledge in acquiring
information, Latent Semantic Analysis (LSA)
and Probabilistic LSA (PLSA) are applied to es-
timate semantic similarity among words. Next
two subsections will explain how LSA and PLSA
are to be adopted to measuring semantic simi-
larity.
2.1 Latent Semantic Analysis
The basic idea of LSA is that the aggregate of all
the word contexts in which a given word does
and does not appear provides a set of mutual
constraints that largely determines the similar-
ity of meaning of words and sets of words to each
other (Landauer et al, 1998)(Gotoh and Renals,
1997). LSA also extracts and infers relations of
expected contextual usage of words in passages
of discourse. It uses no human-made dictionar-
ies, knowledge bases, semantic thesaurus, syn-
tactic parser or the like. Only raw text parsed
into unique character strings is needed for its
input data.
The first step is to represent the text as a
matrix in which each row stands for a unique
word and each column stands for a text passage
or other context. Each cell contains the occur-
rence frequency of a word in the text passage.
Next, LSA applies singular value decomposi-
tion (SVD) to the matrix. SVD is a form of
factor analysis and is defined as
A = U?V T (1)
,where ? is a diagonal matrix composed of
nonzero eigen values of AAT or ATA, and U
and V are the orthogonal eigenvectors associ-
ated with the r nonzero eigenvalues of AAT and
ATA, respectively. One component matrix (U)
describes the original row entities as vectors of
derived orthogonal factor value, another (V ) de-
scribes the original column entities in the same
way, and the third (?) is a diagonal matrix con-
taining scaling values when the three compo-
nents are matrix-multiplied, the original matrix
is reconstructed.
The singular vectors corresponding to the
k(k ? r) largest singular values are then used
to define k-dimensional document space. Using
these vectors,m?k and n?k matrices Uk and Vk
may be redefined along with k?k singular value
matrix ?k. It is known that Ak = Uk?kV Tk is
the closest matrix of rank k to the original ma-
trix.
LSA can represent words of similar meaning
in similar ways. This can be claimed by the fact
that one compares words with similar vectors as
derived from large text corpora. The term-to-
term similarity is based on the inner products
between two row vectors of A, AAT = U?2UT .
One might think of the rows of U? as defining
coordinates for terms in the latent space. To
calculate the similarity of coordinates, V1 and
V2, cosine computation is used:
cos? = V1 ?V2? V1 ? ? ? V2 ?
(2)
2.2 Probabilistic Latent Semantic
Analysis
Probabilistic latent semantic analysis (PLSA)
is a statistical technique for the analysis of two-
mode and co-occurrence data, and has produced
some meaningful results in such applications
as language modelling (Gildea and Hofmann,
1999) and document indexing in information re-
trieval (Hofmann, 1999b). PLSA is based on
aspect model where each observation of the co-
occurrence data is associated with a latent class
variable z ? Z = {z1, z2, . . . , zK} (Hofmann,
1999a). For text documents, the observation is
an occurrence of a word w ? W in a document
d ? D, and each possible state z of the latent
class represents one semantic topic.
A word-document co-occurrence event,
(d,w), is modelled in a probabilistic way where
it is parameterized as in
P (d,w) =
?
z
P (z)P (d,w|z)
=
?
z
P (z)P (w|z)P (d|z). (3)
Here, w and d are assumed to be condition-
ally independent given a specific z. P (w|z) and
P (d|z) are topic-specific word distribution and
document distribution, respectively. The three-
way decomposition for the co-occurrence data
is similar to that of SVD in LSA. But the ob-
jective function of PLSA, unlike that of LSA,
is the likelihood function of multinomial sam-
pling. And the parameters P (z), P (w|z), and
P (d|z) are estimated by maximization of the
log-likelihood function
L =
?
d?D
?
w?W
n(d,w) logP (d,w), (4)
and this maximization is performed using the
EM algorithm as for most latent variable mod-
els. Details on the parameter estimation are
referred to (Hofmann, 1999a). To compute
the similarity of w1 and w2, P (zk|w1)P (zk|w2)
should be approximately computed with being
derived from
P (zk|w) =
P (zk)P (w|zk)
?
zk?Z P (zk)P (w|zk)
(5)
And we can evaluate similarities with the
low-dimensional representation in the semantic
topic space P (zk|w1) and P (zk|w2).
3 Translation with Grammatical
Relationship
3.1 Grammatical Relationship
We used grammatical relations stored in the
form of a dictionary for translation of words.
The structure of the dictionary is as follows
(Kim and Kim, 1998):
T (Si) =
?
?
?
?
?
?
?
T1 if Cooc(Si, S1)
T2 if Cooc(Si, S2)
. . .
Tn otherwise,
(6)
where Cooc(Si, Sj) denotes grammatical co-
occurrence of source words Si and Sj , which one
means an input word to be translated and the
other means an argument word to be used in
translation, and Tj is the translation result of
the source word. T (?) denotes the translation
process.
Table 1 shows a grammatical relationship dic-
tionary for an English verb Si =?build? and its
object nouns as an input word and an argument
word, respectively. The dictionary shows that
the word ?build? is translated into five different
translated words in Korean, depending on the
context. For example, ?build? is translated into
?geon-seol-ha-da? (?construct?) when its object
noun is a noun ?plant? (=?factory?), into ?che-
chak-ha-da? (?produce?) when co-occurring with
the object noun ?car?, and into ?seol-lip-ha-da?
(?establish?) in the context of object noun ?com-
pany? (Table 2).
One of the fundamental difficulties in co-
occurrence-based approaches to word sense dis-
ambiguation (translation selection in this case)
is the problem of data sparseness or unseen
words. For example, for an unregistered object
noun like ?vehicle? in the dictionary, the correct
translation of the verb cannot be selected us-
ing the dictionary described above. In the next
subsection, we will present k-nearest neighbor
method that resolves this problem.
3.2 k-Nearest Neighbor Learning for
Translation Selection
The similarity between two words on latent se-
mantic spaces is required when performing k-
NN search to select the translation of a word.
The nearest instance of a given word is decided
by selecting a word with the highest similarity
to the given word.
Table 1: Examples of co-occurrence word lists for a verb ?build? in the dictionary
Meaning of ?build? in Korean (Tj) Collocated Object Noun (Sj)
?geon-seol-ha-da? (= ?construct?) plant facility network . . .
?geon-chook-ha-da? (= ?design?) house center housing . . .
?che-chak-ha-da? (= ?produce?) car ship model . . .
?seol-lip-ha-da? (= ?establish?) company market empire . . .
?koo-chook-ha-da? (= ?develop?) system stake relationship . . .
Table 2: Examples of translation of ?build?
source words translated words (in Korean) sense of the verb
?build a plant? ? ?gong-jang-eul geon-seol-ha-da? ?construct?
?build a car? ? ?ja-dong-cha-reul che-chak-ha-da? ?produce?
?build a company? ? ?hoi-sa-reul seol-lip-ha-da? ?establish?
The k-nearest neighbor learning algorithm
(Cover and Hart, 1967)(Aha et al, 1991) as-
sumes all instances correspond to points in the
n-dimensional space Rn. We mapped the n-
dimensional space into the n-dimensional vector
of a word for an instance. The nearest neigh-
bors of an instance are defined in terms of the
standard Euclidean distance.
Then the distance between two instances xi
and xj , D(xi, xj), is defined to be
D(xi, xj) =
?
(a(xi)? a(xj))2 (7)
and a(xi) denotes the value of instance xi, sim-
ilarly to cosine computation between two vec-
tors. Let us consider learning discrete-valued
target functions of the form f : Rn ? V ,
where V is the finite set {v1, . . . , vs}. The k-
nearest neighbor algorithm for approximating a
discrete-valued target function is given in Table
3.
The value f?(xq) returned by this algorithm
as its estimate of f(xq) is just the most com-
mon value of f among the k training examples
nearest to xq.
4 Experiment and Evaluation
4.1 Data for Latent Space and
Dictionary
In the experiment, we used two kinds of cor-
pus data, one for constructing LSA and PLSA
spaces and the other for building a dictionary
containing grammatical relations and a test set.
79,919 texts in 1988 AP news corpus from
TREC-7 data was indexed with a stemming tool
and 19,286 words with the frequency of above 20
Table 3: The k-nearest neighbor learning algo-
rithm.
? Training
? For each training example ?x, f(x)?,
add the example to the list
training examples.
? Classification
? Given a query instance xq to be clas-
sified,
? Let x1, . . . , xk denote the k in-
stances from training examples
that are nearest to xq.
? Return
f?(xq)? argmaxv?V
k
?
i=1
?(v, f(xi)) ,
where ?(a, b) = 1 if a = b and
?(a, b) = 0 otherwise.
are extracted. We built 200 dimensions in SVD
of LSA and 128 latent dimensions of PLSA. The
difference of the numbers was caused from the
degree of computational complexity in learning
phase. Actually, PLSA of 128 latent factors re-
quired 50-fold time as much as LSA hiring 200
eigen-vector space during building latent spaces.
This was caused by 50 iterations which made
the log likelihood maximized. We utilized a sin-
Figure 1: The accuracy ration of verb-object
gle vector lanczos algorithm derived from SVD-
PACK when constructing LSA space. (Berry
et al, 1993). We generated both of LSA and
PLSA spaces, with each word having a vector
of 200 and 128 dimensions, respectively. The
similarity of any two words could be estimated
by performing cosine computation between two
vectors representing coordinates of the words in
the spaces.
Table 4 shows 5 most similar words of ran-
domly selected words from 3,443 examples. We
extracted 3,443 example sentences containing
grammatical relations, like verb-object, subject-
verb and adjective-noun, from Wall Street Jour-
nal corpus of 220,047 sentences and other
newspapers corpus of 41,750 sentences, totally
261,797 sentences. We evaluated the accu-
racy performance of each grammatical rela-
tion. 2,437, 188, and 818 examples were uti-
lized for verb-object, subject-verb, and adjective-
noun, respectively. The selection accuracy was
measured using 5-fold cross validation for each
grammatical relation. Sample sentences of each
grammatical relation were divided into five dis-
joint samples and each sample became a test
sample once in the experiment and the remain-
ing four samples were combined to make up a
collocation dictionary.
4.2 Experimental Result
Table 5 and figure 1-3 show the results of
translation selection with respect to the applied
model and to the value of k. As shown in Table
5, similarity based on data-driven model could
improve the selection accuracy up to 20% as
Figure 2: The accuracy ration of subject-verb
Figure 3: The accuracy ration of adjective-noun
contrasted with the direct matching method.
We could obtain the result that PLSA could
improve the accuracy more than LSA in almost
all cases. The amount of improvement is varied
from -0.12% to 2.96%.
As figure 1-3 show, the value of k had affec-
tion to the translation accuracy in PLSA, how-
ever, not in LSA. From this, we could not de-
clare whether the value of k and translation ac-
curacy have relationship of each other or not
in the data-driven models described in this pa-
per. However, we could also find that the degree
of accuracy was raised in accordance with the
value of k in PLSA. From this, we consequently
inferred that the latent semantic space gener-
ated by PLSA had more sound distribution with
reflection of well-structured semantic structure
than LSA. Only one of three grammatical re-
Table 4: Lists of 5 most semantically similar words for randomly selected words generated from
LSA, and PLSA. The words are stems of original words. The first row of each selected word stands
for the most similar words in LSA semantic space and the second row stands for those in the PLSA
space.
selected words most similar words
plant westinghous isocyan shutdown zinc manur
radioact hanford irradi tritium biodegrad
car buick oldsmobil chevrolet sedan corolla
highwai volkswagen sedan vehicular vehicle
home parapleg broccoli coconut liverpool jamal
memori baxter hanlei corwin headston
business entrepreneur corpor custom ventur firm
digit compat softwar blackston zayr
ship vessel sail seamen sank sailor
destroy frogmen maritim skipper vessel
Table 5: Translation accuracy in various case. The first column stands for each grammatical relation
and the second column stands for the used models, LSA or PLSA. And other three columns stand
for the accuracy ratio (rm) with respect to the value of k. The numbers in parenthesis of the first
column show the translation accuracy ratio of simple dictionary search method (rs). And numbers
in the other parenthesis were obtained by rm ? rs.
grammatical used k = 1 k = 5 k = 10
relations model
verb-object LSA 84.41(1.17) 83.01(1.16) 84.24(1.17)
(71.85) PLSA 84.53(1.18) 85.35(1.19) 86.05(1.20)
subject-verb LSA 83.99(1.11) 84.62(1.11) 84.31(1.11)
(75.93) PLSA 86.85(1.14) 87.49(1.15) 87.27(1.15)
adjective-noun LSA 80.93(1.15) 80.32(1.14) 80.93(1.15)
(70.54) PLSA 80.81(1.15) 82.27(1.17) 82.76(1.17)
lations, subj-verb, showed an exceptional case,
which seemed to be caused by the small size of
examples, 188.
Selection errors taking place in LSA and
PLSA models were caused mainly by the fol-
lowing reasons. First of all, the size of vocab-
ulary should be limited by computation com-
plexity. In this experiment, we acquired below
20,000 words for the vocabulary, which could
not cover a section of corpus data. Second, the
stemming algorithm was not robust for an in-
dexing. For example, ?house? and ?housing? are
regarded as a same word as ?hous?. This fact
brought about hardness in reflecting the seman-
tic structure more precisely. And finally, the
meaning of similar word is somewhat varied in
the machine translation field and the informa-
tion retrieval field. The selectional restriction
tends to depend a little more upon semantic
type like human-being, place and etc., than on
the context in a document.
5 Conclusion
This paper describes a comparative evaluation
of the accuracy performance in translation se-
lection based on data-driven models. LSA and
PLSA were utilized for implementation of the
models, which are mainly used in estimating
similarity between words. And a manually-
built grammatical relation dictionary was used
for the purpose of appropriate translation se-
lection of a word. To break down the data
sparseness problem occurring when the dictio-
nary is used, we utilized similarity measure-
ments schemed out from the models. When an
argument word is not included in the dictionary,
the most k similar words to the word are discov-
ered in the dictionary, and then the meaning of
the grammatically-related class for the majority
of the k words is selected as the translation of
an input word.
We evaluated the accuracy ratio of LSA and
PLSA comparatively and classified the exper-
iments with criteria of the values of k and
the grammatical relations. We acquired up to
20% accuracy improvement, compared to direct
matching to a collocation dictionary. PLSA
showed the ability to select translation better
than LSA, up to 3%. The value of k is strongly
related with PLSA in translation accuracy, not
too with LSA. That means the latent semantic
space of PLSA has more sound distribution of
latent semantics than that of LSA. Even though
longer learning time than LSA, PLSA is benefi-
cial in translation accuracy and distributional
soundness. A distributional soundness is ex-
pected to have better performance as the size
of examples is growing.
However, we should resolve several problems
raised during the experiment. First, a robust
stemming tool should be exploited for more ac-
curate morphology analysis. Second, the opti-
mal value of k should be obtained, according to
the size of examples. Finally, we should discover
more specific contextual information suited to
this type of problem. While simple text could
be used properly in IR, MT should require an-
other type of information.
The data-driven models could be applied to
other sub-fields related with semantics in ma-
chine translation. For example, to-infinitive
phrase and preposition phrase attachment dis-
ambiguation problem can also apply these mod-
els. And syntactic parser could apply the mod-
els for improvement of accurate analysis by us-
ing semantic information generated by the mod-
els.
References
D. Aha, D. Kibler, and M. Albert. 1991.
Instance-based learning algorithms. Machine
Learning, 6:37?66.
M. Berry, T. Do, G. O?Brien, V. Krishna, and
S. Varadhan. 1993. Svdpackc: Version 1.0
user?s guide. Technical Report CS?93?194,
University of Tennessee, Knoxville, TN.
T. Cover and P. Hart. 1967. Nearest neighbor
pattern classification. IEEE Transactions on
Information Theory, 13:21?27.
I. Dagan, L. Lee, and F. Fereira. 1999.
Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34:43?69.
S. Deerwester, S. Dumais, G. Furnas, T. Lan-
dauer, and R. Harshman. 1990. Indexing
by latent semantic analysis. Journal of the
American Society for Information Science,
41:391?407.
P. Foltz, W. Kintsch, and T. Landauer. 1998.
The mesurement of textual coherence with la-
tent semantic analysis. Discourse Processes,
25:285?307.
D. Gildea and T. Hofmann. 1999. Topic based
language models using em. In Proceedings of
the 6th European Conference on Speech Com-
munication and Technology (Eurospeech99).
D. Gotoh and S. Renals. 1997. Document
space models using latent semantic analysis.
In Proceedings of Eurospeech-97, pages 1443?
1446.
T. Hofmann. 1999a. Probabilistic latent se-
mantic analysis. In Proceedings of the Fif-
teenth Conference on Uncertainty in Artifi-
cial Intelligence (UAI?99).
T. Hofmann. 1999b. Probabilistic latent se-
mantic indexing. In Proceedings of the 22th
Annual International ACM SIGIR conference
on Research and Developement in Informa-
tion Retrieval (SIGIR99), pages 50?57.
T. Hofmann. 2001. Unsupervised learning by
probabilistic latent semantic analysis. Ma-
chine Learning Journal, 42(1):177?196.
Y. Kim and Y. Kim. 1998. Semantic implemen-
tation based on extended idiom for english to
korean machine translation. The Asia-Pacific
Association for Machine Translation Journal,
21:23?39.
T. K. Landauer, P. W. Foltz, and D. Laham.
1998. An introduction to latent semantic
analysis. Discourse Processes, 25:259?284.
E. Voorhees and D. Harman. 1998. Overview of
the seventh text retrieval conference (trec-7).
In Proceedings of the Seventh Text REtrieval
Conference (TREC-7), pages 1?24.
Word Sense Disambiguation by Learning from Unlabeled Data
Seong-Bae Park
y
, Byoung-Tak Zhang
y
and Yung Taek Kim
z
Articial Intelligence Lab (SCAI)
School of Computer Science and Engineering
Seoul National University
Seoul 151-742, Korea
y
fsbpark,btzhangg@scai.snu.ac.kr
z
ytkim@cse.snu.ac.kr
Abstract
Most corpus-based approaches to
natural language processing suer
from lack of training data. This
is because acquiring a large num-
ber of labeled data is expensive.
This paper describes a learning
method that exploits unlabeled data
to tackle data sparseness problem.
The method uses committee learn-
ing to predict the labels of unla-
beled data that augment the exist-
ing training data. Our experiments
on word sense disambiguation show
that predictive accuracy is signi-
cantly improved by using additional
unlabeled data.
1 Introduction
The objective of word sense disambiguation
(WSD) is to identify the correct sense of a
word in context. It is one of the most critical
tasks in most natural language applications,
including information retrieval, information
extraction, and machine translation. The
availability of large-scale corpus and various
machine learning algorithms enabled corpus-
based approach to WSD (Cho and Kim, 1995;
Hwee and Lee, 1996; Wilks and Stevenson,
1998),but a large scale sense-tagged corpus
or aligned bilingual corpus is needed for a
corpus-based approach.
However, most languages except English
do not have a large-scale sense-tagged cor-
pus. Therefore, any corpus-based approach
to WSD for such languages should consider
the following problems:
 There's no reliable and available sense-
tagged corpus.
 Most words are sense ambiguous.
 Annotating the large corpora requires
human experts, so that it is too expen-
sive.
Because it is expensive to construct sense-
tagged corpus or bilingual corpus, many re-
searchers tried to reduce the number of ex-
amples needed to learn WSD (Atsushi et al,
1998; Pedersen and Bruce, 1997). Atsushi et
al. (Atsushi et al, 1998) adopted a selec-
tive sampling method to use small number of
examples in training. They dened a train-
ing utility function to select examples with
minimum certainty, and at each training it-
eration the examples with less certainty were
saved in the example database. However, at
each iteration of training the similarity among
word property vectors must be calculated due
to their k-NN like implementation of training
utility.
While labeled examples obtained from a
sense-tagged corpus is expensive and time-
consuming, it is signicantly easier to ob-
tain the unlabeled examples. Yarowsky
(Yarowsky, 1995) presented, for the rst time,
the possibility that unlabeled examples can
be used for WSD. He used a learning algo-
rithm based on the local context under the
assumption that all instances of a word have
the same intended meaning within any xed
document and achieved good results with only
a few labeled examples and many unlabeled
ones. Nigam et al (Nigam et al, 2000) also
showed the unlabeled examples can enhance
the accuracy of text categorization.
Attribute Substance
GFUNC the grammatical function of w
PARENT the word of the node modied by w
SUBJECT whether or not PARENT of w has a subject
OBJECT whether or not PARENT of w has an object
NMODWORD the word of the noun modier of w
ADNWORD the head word of the adnominal phrase of w
ADNSUBJ whether or not the adnominal phrase of w has a subject
ADNOBJ whether or not the adnominal phrase of w has an object
Table 1: The properties used to distinguish the sense of an ambiguous Korean noun w.
In this paper, we present a new approach
to word sense disambiguation that is based
on selective sampling algorithm with commit-
tees. In this approach, the number of train-
ing examples is reduced, by determining by
weighted majority voting of multiple classi-
ers, whether a given training example should
be learned or not. The classiers of the com-
mittee are rst trained on a small set of la-
beled examples and the training set is aug-
mented by a large number of unlabeled exam-
ples. One might think that this has the pos-
sibility that the committee is misled by unla-
beled examples. But, the experimental results
conrm that the accuracy of WSD is increased
by using unlabeled examples when the mem-
bers of the committee are well trained with
labeled examples. We also theoretically show
that performance improvement is guaranteed
by a mild requirement, i.e., the base classi-
ers need to guess better than random selec-
tion. This is because the possibility misled by
unlabeled examples is reduced by integrating
outputs of multiple classiers. One advantage
of this method is that it eectively performs
WSD with only a small number of labeled ex-
amples and thus shows possibility of building
word sense disambiguators for the languages
which have no sense-tagged corpus.
The rest of this paper is organized as fol-
lows. Section 2 introduces the general proce-
dure for word sense disambiguation and the
necessity of unlabeled examples. Section 3 ex-
plains how the proposed method works using
both labeled and unlabeled examples. Section
4 presents the experimental results obtained
by using the KAIST raw corpus. Section 5
draws conclusions.
2 Word Sense Disambiguation
Let S 2 fs
1
; : : : ; s
k
g be the set of possible
senses of a word to be disambiguated. To
determine the sense of the word, we need
to consider the contextual properties. Let
x =< x
1
; : : : ; x
n
> be the vector for rep-
resenting selected contextual features. If we
have a classier f(x; ) parameterized with ,
then the sense of a word with property vec-
tor x can be determined by choosing the most
probable sense s

:
s

= argmax
s2S
f(x; ):
The parameters  are determined by training
the classier on a set of labeled examples, L =
f(x
1
; s
1
); : : : ; (x
N
; s
N
)g.
2.1 Property Sets
In general, the rst step of WSD is to extract
a set of contextual features. To select particu-
lar properties for Korean, the language of our
cencern, the following characteristics should
be considered:
 Korean is a partially free-order language.
The ordering information on the neigh-
bors of the ambiguous word, therefore,
does not give signicantly meaningful in-
formation in Korean.
 In Korean, ellipses appear very often
with a nominative case or objective case.
Therefore, it is di?cult to build a large
scale database of labeled examples with
case markers.
Considering both characteristics and re-
sults of previous work, we select eight prop-
erties for WSD of Korean nouns (Table 1).
Three of them (PARENT, NMODWORD,
ADNWORD) take morphological form as
their value, one (GFUNC) takes 11 values of
grammatical functions
1
, and others take only
true or false.
2.2 Unlabeled Data for WSD
Many researchers tried to develop automated
methods to reduce training cost in language
learning and found out that the cost can be
reduced by active learning which has control
over the training examples (Dagan and Engel-
son, 1997; Liere and Tadepalli, 1997; Zhang,
1994). Though the number of labeled exam-
ples needed is reduced by active learning, the
label of the selected examples must be given
by the human experts. Thus, active learn-
ing is still expensive and a method for auto-
matic labeling unlabeled examples is needed
to have the learner automatically gather in-
formation (Blum and Mitchell, 1998; Peder-
sen and Bruce, 1997; Yarowsky, 1995).
As the unlabeled examples can be obtained
with ease without human experts it makes
WSD robust. Yarowsky (Yarowsky, 1995)
presented the possibility of automatic label-
ing of training examples in WSD and achieved
good results with only a few labeled exam-
ples and many unlabeled examples. On the
other hand, Blum and Mitchell tried to clas-
sify Web pages, in which the description of
each example can be partitioned into distinct
views such as the words occurring on that
page and the words occurring in hyperlinks
(Blum and Mitchell, 1998). By using both
views together, they augmented a small set
of labeled examples with a lot of unlabeled
examples.
The unlabeled examples in WSD can pro-
vide information about the joint probability
1
These 11 grammatical functions are from
the parser, KEMTS (Korean-to-English Machine
Translation System) developed in Seoul National Uni-
versity, Korea.
distribution over properties but they also can
mislead the learner. However, the possibility
of being misled by the unlabeled examples is
reduced by the committee of classiers since
combining or integrating the outputs of sev-
eral classiers in general leads to improved
performance. This is why we use active learn-
ing with committees to select informative un-
labeled examples and label them.
3 Active Learning with
Committees for WSD
3.1 Active Learning Using Unlabeled
Examples
The algorithm for active learning using unla-
beled data is given in Figure 1. It takes two
sets of examples as inputs. A Set L is the one
with labeled examples and D = fx
1
; : : : ;x
T
g
is the one with unlabeled examples where x
i
is a property vector. First of all, the training
set L
(1)
j
(1  j  M) of labeled examples is
constructed for each base classier C
j
. This
is done by random resampling as in Bagging
(Breiman, 1996). Then, each base classier
C
j
is trained with the set of labeled examples
L
(1)
j
.
After the classiers are trained on labeled
examples, the training set is augmented by
the unlabeled examples. For each unlabeled
example x
t
2 D, each classier computes the
sense y
j
2 S which is the label associated with
it, where S is the set of possible sense of x
t
.
The distribution W over the base classi-
ers represents the importance weights. As
the distribution can be changed each iter-
ation, the distribution in iteration t is de-
noted by W
t
. The importance weight of clas-
sier C
j
under distribution W
t
is denoted by
W
t
(j). Initially, the base classiers have equal
weights, so that W
t
(j) = 1=M .
The sense of the unlabeled example x
t
is de-
termined by majority voting among C
j
's with
weight distribution W . Formally, the sense y
t
of x
t
is predicted by
y
t
(x
t
) = argmax
y2S
X
j:C
j
(x
t
)=y
W
t
(j):
If most classiers believe that y
t
is the correct
Given an unlabeled example set D = fx
1
; : : : ;x
T
g
and a labeled example set L
and a word sense set S 2 fs
1
; : : : ; s
k
g for x
i
,
Initialize W
1
(j) =
1
M
,
where M is the number of classiers in the
committee.
Resample L
(1)
j
from L for each classier C
j
,
where jL
(1)
j
j = jLj as done in Bagging.
Train base classier C
j
(1  j  M) from L
(1)
j
.
For t = 1; : : : ; T :
1. Each C
j
predicts the sense y
j
2 S for x
t
2 D.
Y =< y
1
; : : : ; y
M
>
2. Find the most likely sense y
t
from Y using
distribution W :
y
t
= argmax
y2S
X
j:C
j
(x
t
)=y
W
t
(j):
3. Set 
t
=
1 
t

t
, where

t
=
No. of C
j
's whose predictions are not y
t
M
:
4. If 
t
is larger than a certainty threshold ,
then update W
t
:
W
t+1
(j) =
W
t
(j)
Z
t

Text Chunking by Combining Hand-Crafted Rules and Memory-Based
Learning
Seong-Bae Park Byoung-Tak Zhang
School of Computer Science and Engineering
Seoul National University
Seoul 151-744, Korea
{sbpark,btzhang}@bi.snu.ac.kr
Abstract
This paper proposes a hybrid of hand-
crafted rules and a machine learning
method for chunking Korean. In the par-
tially free word-order languages such as
Korean and Japanese, a small number
of rules dominate the performance due
to their well-developed postpositions and
endings. Thus, the proposed method is
primarily based on the rules, and then the
residual errors are corrected by adopting a
memory-based machine learning method.
Since the memory-based learning is an
efficient method to handle exceptions in
natural language processing, it is good at
checking whether the estimates are excep-
tional cases of the rules and revising them.
An evaluation of the method yields the im-
provement in F-score over the rules or var-
ious machine learning methods alone.
1 Introduction
Text chunking has been one of the most interest-
ing problems in natural language learning commu-
nity since the first work of (Ramshaw and Marcus,
1995) using a machine learning method. The main
purpose of the machine learning methods applied to
this task is to capture the hypothesis that best deter-
mine the chunk type of a word, and such methods
have shown relatively high performance in English
(Kudo and Matsumoto, 2000; Zhang et. al, 2001).
In order to do it, various kinds of information, such
as lexical information, part-of-speech and grammat-
ical relation, of the neighboring words is used. Since
the position of a word plays an important role as a
syntactic constraint in English, the methods are suc-
cessful even with local information.
However, these methods are not appropriate for
chunking Korean and Japanese, because such lan-
guages have a characteristic of partially free word-
order. That is, there is a very weak positional con-
straint in these languages. Instead of positional con-
straints, they have overt postpositions that restrict
the syntactic relation and composition of phrases.
Thus, unless we concentrate on the postpositions,
we must enlarge the neighboring window to get
a good hypothesis. However, enlarging the win-
dow size will cause the curse of dimensionality
(Cherkassky and Mulier, 1998), which results in the
deficiency in the generalization performance.
Especially in Korean, the postpositions and the
endings provide important information for noun
phrase and verb phrase chunking respectively. With
only a few simple rules using such information,
the performance of chunking Korean is as good
as the rivaling other inference models such as ma-
chine learning algorithms and statistics-based meth-
ods (Shin, 1999). Though the rules are approxi-
mately correct for most cases drawn from the do-
main on which the rules are based, the knowledge
in the rules is not necessarily well-represented for
any given set of cases. Since chunking is usually
processed in the earlier step of natural language pro-
cessing, the errors made in this step have a fatal in-
fluence on the following steps. Therefore, the ex-
ceptions that are ignored by the rules must be com-
Training Phase
   w 1 ... w N
(PO S1 ... PO SN)
Rule Based
D eterm ination
Rule Base
For Each W ord w i
Correctly
D eterm ined?
Find Error Type
N o
Finish
Yes
E rror C ase Library
C lassification Phase
   w 1 ... w N
(PO S1 ... PO SN)
Rule Based
D eterm ination
Rule Base
For Each W ord w i
E rror C ase Library
M em ory Based
D eterm ination
 C 1 ... C N
Com bination
Figure 1: The structure of Korean chunking model. This figure describes a sentence-based learning and
classification.
pensated for by some special treatments of them for
higher performance.
To solve this problem, we have proposed a com-
bining method of the rules and the k-nearest neigh-
bor (k-NN) algorithm (Park and Zhang, 2001). The
problem in this method is that it has redundant k-
NNs because it maintains a separate k-NN for each
kind of errors made by the rules. In addition, be-
cause it applies a k-NN and the rules to each exam-
ples, it requires more computations than other infer-
ence methods.
The goal of this paper is to provide a new method
for chunking Korean by combining the hand-crafted
rules and a machine learning method. The chunk
type of a word in question is determined by the rules,
and then verified by the machine learning method.
The role of the machine learning method is to de-
termine whether the current context is an exception
of the rules. Therefore, a memory-based learning
(MBL) is used as a machine learning method that
can handle exceptions efficiently (Daelemans et. al,
1999).
The rest of the paper is organized as follows. Sec-
tion 2 explains how the proposed method works.
Section 3 describes the rule-based method for
chunking Korean and Section 4 explains chunking
by memory-based learning. Section 5 presents the
experimental results. Section 6 introduces the issues
for applying the proposed method to other problems.
Finally, Section 7 draws conclusions.
2 Chunking Korean
Figure 1 shows the structure of the chunking model
for Korean. The main idea of this model is to apply
rules to determine the chunk type of a word w
i
in a
sentence, and then to refer to a memory based clas-
sifier in order to check whether it is an exceptional
case of the rules. In the training phase, each sentence
is analyzed by the rules and the predicted chunk type
is compared with the true chunk type. In case of mis-
prediction, the error type is determined according to
the true chunk type and the predicted chunk type.
The mispredicted chunks are stored in the error case
library with their true chunk types. Since the error
case library accumulates only the exceptions of the
rules, the number of cases in the library is small if
the rules are general enough to represent the instance
space well.
The classification phase in Figure 1 is expressed
as a procedure in Figure 2. It determines the chunk
type of a word w
i
given with the context C
i
. First of
all, the rules are applied to determine the chunk type.
Then, it is checked whether C
i
is an exceptional case
of the rules. If it is, the chunk type determined by
the rules is discarded and is determined again by the
memory based reasoning. The condition to make a
decision of exceptional case is whether the similar-
ity between C
i
and the nearest instance in the error
Procedure Combine
Input : a word w
i
, a context C
i
, and the threshold t
Output : a chunk type c
[Step 1] c = Determine the chunk type of w
i
using rules.
[Step 2] e = Get the nearest instance of C
i
in error case
library.
[Step 3] If Similarity(C
i
, e) ? t,
then c = Determine chunk type of w
i
by memory-
based learning.
Figure 2: The procedure for combining the rules and
memory based learning.
case library is larger than the threshold t. Since the
library contains only the exceptional cases, the more
similar is C
i
to the nearest instance, the more prob-
able is it an exception of the rules.
3 Chunking by Rules
There are four basic phrases in Korean: noun phrase
(NP), verb phrase (VP), adverb phrase (ADVP), and
independent phrase (IP). Thus, chunking by rules is
divided into largely four components.
3.1 Noun Phrase Chunking
When the part-of-speech of w
i
is one of determiner,
noun, and pronoun, there are only seven rules to
determine the chunk type of w
i
due to the well-
developed postpositions of Korean.
1. If POS(w
i?1
) = determiner and w
i?1
does not have a
postposition Then y
i
= I-NP.
2. Else If POS(w
i?1
) = pronoun and w
i?1
does not have
a postposition Then y
i
= I-NP.
3. Else If POS(w
i?1
) = noun and w
i?1
does not have a
postposition Then y
i
= I-NP.
4. Else If POS(w
i?1
) = noun and w
i?1
has a possessive
postposition Then y
i
= I-NP.
5. Else If POS(w
i?1
) = noun andw
i?1
has a relative post-
fix Then y
i
= I-NP.
6. Else If POS(w
i?1
) = adjective and w
i?1
has a relative
ending Then y
i
= I-NP.
7. Else y
i
= B-NP.
Here, POS(w
i?1
) is the part-of-speech of w
i?1
.
B-NP represents the first word of a noun phrase,
while I-NP is given to other words in the noun
phrase.
Since determiners, nouns and pronouns play the
similar syntactic role in Korean, they form a noun
phrase when they appear in succession without post-
position (Rule 1?3). The words with postpositions
become the end of a noun phrase, but there are only
two exceptions. When the type of a postposition
is possessive, it is still in the mid of noun phrase
(Rule 4). The other exception is a relative postfix
? (jeok)? (Rule 5). Rule 6 states that a simple rela-
tive clause with no sub-constituent also constitutes a
noun phrase. Since the adjectives of Korean have no
definitive usage, this rule corresponds to the defini-
tive usage of the adjectives in English.
3.2 Verb Phrase Chunking
The verb phrase chunking has been studied for a
long time under the name of compound verb pro-
cessing in Korean and shows relatively high accu-
racy. Shin used a finite state automaton for verb
phrase chunking (Shin, 1999), while K.-C. Kim used
knowledge-based rules (Kim et. al, 1995). For the
consistency with noun phrase chunking, we use the
rules in this paper. The rules used are the ones pro-
posed by (Kim et. al, 1995) and the further explana-
tion on the rules is skipped. The number of the rules
used is 29.
3.3 Adverb Phrase Chunking
When the adverbs appear in succession, they have a
great tendency to form an adverb phrase. Though an
adverb sequence is not always one adverb phrase, it
usually forms one phrase. Table 1 shows this empiri-
cally. The usage of the successive adverbs is investi-
gated from STEP 2000 dataset1 where 270 cases are
observed. The 189 cases among them form a phrase
whereas the remaining 81 cases form two phrases in-
dependently. Thus, it can be said that the possibility
that an adverb sequence forms a phrase is far higher
than the possibility that it forms two phrases.
When the part-of-speech of w
i
is an adjective, its
chunk type is determined by the following rule.
1. If POS(w
i?1
) = adverb Then y
i
= I-ADVP.
2. Else y
i
= B-ADVP.
1This dataset will be explained in Section 5.1.
No. of Cases Probability
One Phrase 189 0.70
Two Phrases 81 0.30
Table 1: The probability that an adverb sequence
forms a chunk.
3.4 Independent Phrase Chunking
There is no special rule for independent phrase
chunking. It can be done only through knowledge
base that stores the cases where independent phrases
take place. We designed 12 rules for independent
phrases.
4 Chunking by Memory-Based Learning
Memory-based learning is a direct descent of the
k-Nearest Neighbor (k-NN) algorithm (Cover and
Hart, 1967). Since many natural language process-
ing (NLP) problems have constraints of a large num-
ber of examples and many attributes with different
relevance, memory-based learning uses more com-
plex data structure and different speedup optimiza-
tion from the k-NN.
It can be viewed with two components: a learning
component and a similarity-based performance com-
ponent. The learning component involves adding
training examples to memory, where all examples
are assumed to be fixed-length vectors of n at-
tributes. The similarity between an instance x and
all examples y in memory is computed using a dis-
tance metric, ?(x,y). The chunk type of x is then
determined by assigning the most frequent category
within the k most similar examples of x.
The distance from x and y, ?(x,y) is defined to
be
?(x,y) ?
n
?
i=1
?
i
?(x
i
, y
i
),
where ?
i
is the weight of i-th attribute and
?(x
i
, y
i
) =
{
0 if x
i
= y
i
,
1 if x
i
= y
i
.
When ?
i
is determined by information gain (Quin-
lan, 1993), the k-NN algorithm with this metric is
called IB1-IG (Daelemans et. al, 2001). All the ex-
periments performed by memory-based learning in
this paper are done with IB1-IG.
Table 2 shows the attributes of IB1-IG for chunk-
ing Korean. To determine the chunk type of a word
w
i
, the lexicons, POS tags, and chunk types of
surrounding words are used. For the surrounding
words, three words of left context and three words
of right context are used for lexicons and POS tags,
while two words of left context are used for chunk
types. Since chunking is performed sequentially, the
chunk types of the words in right context are not
known in determining the chunk type of w
i
.
5 Experiments
5.1 Dataset
For the evaluation of the proposed method, all exper-
iments are performed on STEP 2000 Korean Chunk-
ing dataset (STEP 2000 dataset)2. This dataset is
derived from the parsed corpus, which is a product
of STEP 2000 project supported by Korean govern-
ment. The corpus consists of 12,092 sentences with
111,658 phrases and 321,328 words, and the vocab-
ulary size is 16,808. Table 3 summarizes the infor-
mation on the dataset.
The format of the dataset follows that of CoNLL-
2000 dataset (CoNLL, 2000). Figure 3 shows an ex-
ample sentence in the dataset3. Each word in the
dataset has two additional tags, which are a part-of-
speech tag and a chunk tag. The part-of-speech tags
are based on KAIST tagset (Yoon and Choi, 1999).
Each phrase can have two kinds of chunk types: B-
XP and I-XP. In addition to them, there is O chunk
type that is used for words which are not part of any
chunk. Since there are four types of phrases and
one additional chunk type O, there exist nine chunk
types.
5.2 Performance of Chunking by Rules
Table 4 shows the chunking performance when only
the rules are applied. Using only the rules gives
97.99% of accuracy and 91.87 of F-score. In spite
of relatively high accuracy, F-score is somewhat low.
Because the important unit of the work in the appli-
cations of text chunking is a phrase, F-score is far
more important than accuracy. Thus, we have much
room to improve in F-score.
2The STEP 2000 Korean Chunking dataset is available in
http://bi.snu.ac.kr/?sbpark/Step2000.
3The last column of this figure, the English annotation, does
Attribute Explanation Attribute Explanation
W
i?3
word of w
i?3
POS
i?3
POS of w
i?3
W
i?2
word of w
i?2
POS
i?2
POS of w
i?2
W
i?1
word of w
i?1
POS
i?1
POS of w
i?1
W
i
word of w
i
POS
i
POS of w
i
W
i+1
word of w
i+1
POS
i+1
POS of w
i+1
W
i+2
word of w
i+2
POS
i+2
POS of w
i+2
W
i+3
word of w
i+3
POS
i+3
POS of w
i+3
C
i?3
chunk of w
i?3
C
i?2
chunk of w
i?2
C
i?1
chunk of w
i?1
Table 2: The attributes of IB1-IG for chunking Korean.
Information Value
Vocabulary Size 16,838
Number of total words 321,328
Number of chunk types 9
Number of POS tags 52
Number of sentences 12,092
Number of phrases 112,658
Table 3: The simple statistics on STEP 2000 Korean
Chunking dataset.
 nq B-NP Korea
? jcm I-NP Postposition : POSS
 nq I-NP Sejong
 ncn I-NP base
 jcj I-NP and
 mmd I-NP the
	
 ncn I-NP surrounding
 ncn I-NP base
	 jxt I-NP Postposition: TOPIC

 ncn B-NP western South Pole
 ncn B-NP south
	
 nq I-NP Shetland
? jcm I-NP Postposition : POSS
	
 nq I-NP King George Island
 jca I-NP Postposition : LOCA
 paa B-VP is located
 ef I-VP Ending : DECL
. sf O
Figure 3: An example of STEP 2000 dataset.
Type Precision Recall F-score
ADVP 98.67% 97.23% 97.94
IP 100.00% 99.63% 99.81
NP 88.96% 88.93% 88.94
VP 92.89% 96.35% 94.59
All 91.28% 92.47% 91.87
Table 4: The experimental results when the rules are
only used.
Error Type No. of Errors Ratio (%)
B-ADVP I-ADVP 89 1.38
B-ADVP I-NP 9 0.14
B-IP B-NP 9 0.14
I-IP I-NP 2 0.03
B-NP I-NP 2,376 36.76
I-NP B-NP 2,376 36.76
B-VP I-VP 3 0.05
I-VP B-VP 1,599 24.74
All 6,463 100.00
Table 5: The error distribution according to the mis-
labeled chunk type.
Table 5 shows the error types by the rules and
their distribution. For example, the error type ?B-
ADVP I-ADVP? contains the errors whose true la-
bel is B-ADVP and that are mislabeled by I-ADVP.
There are eight error types, but most errors are re-
lated with noun phrases. We found two reasons for
this:
1. It is difficult to find the beginning of noun
phrases. All nouns appearing successively
without postpositions are not a single noun
phrase. But, they are always predicted to be
single noun phrase by the rules, though they
can be more than one noun phrase.
2. The postposition representing a noun coordi-
nation, ? (wa)? is very ambiguous. When
? (wa)? is representing the coordination, the
chunk types of it and its next word should be
?I-NP I-NP?. But, when it is just an adverbial
postposition that implies ?with? in English, the
chunk types should be ?I-NP B-NP?.
Decision Tree SVM MBL
Accuracy 97.95?0.24% 98.15?0.20% 97.79?0.29%
Precision 92.29?0.94% 93.63?0.81% 91.41?1.24%
Recall 90.45?0.80% 91.48?0.70% 91.43?0.87%
F-score 91.36?0.85 92.54?0.72 91.38?1.01
Table 6: The experimental results of various ma-
chine learning algorithms.
5.3 Performance of Machine Learning
Algorithms
Table 6 gives the 10-fold cross validation result of
three machine learning algorithms. In each fold, the
corpus is divided into three parts: training (80%),
held-out (10%), test (10%). Since held-out set is
used only to find the best value for the threshold t
in the combined model, it is not used in measuring
the performance of machine learning algorithms.
The machine learning algorithms tested are (i)
memory-based learning (MBL), (ii) decision tree,
and (iii) support vector machines (SVM). We use
C4.5 release 8 (Quinlan, 1993) for decision tree in-
duction and SV Mlight (Joachims, 1998) for support
vector machines, while TiMBL (Daelemans et. al,
2001) is adopted for memory-based learning. De-
cision trees and SVMs use the same attributes with
memory-based learning (see Table 2). Two of the al-
gorithms, memory-based learning and decision tree,
show worse performance than the rules. The F-
scores of memory-based learning and decision tree
are 91.38 and 91.36 respectively, while that of the
rules is 91.87 (see Table 4). On the other hand, sup-
port vector machines present a slightly better perfor-
mance than the rules. The F-score of support vector
machine is 92.54, so the improvement over the rules
is just 0.67.
Table 7 shows the weight of attributes when
only memory-based learning is used. Each value
in this table corresponds to ?
i
in calculating
?(x,y). The more important is an attribute, the
larger is the weight of it. Thus, the most im-
portant attribute among 17 attributes is C
i?1
, the
chunk type of the previous word. On the other
hand, the least important attributes are W
i?3
and
C
i?3
. Because the words make less influence
on determining the chunk type of w
i
in ques-
tion as they become more distant from w
i
. That
not exist in the dataset. It is given for the explanation.
Attribute Weight Attribute Weight
W
i?3
0.03 POS
i?3
0.04
W
i?2
0.07 POS
i?2
0.11
W
i?1
0.17 POS
i?1
0.28
W
i
0.22 POS
i
0.38
W
i+1
0.14 POS
i+1
0.22
W
i+2
0.06 POS
i+2
0.09
W
i+3
0.04 POS
i+3
0.05
C
i?3
0.03 C
i?2
0.11
C
i?1
0.43
Table 7: The weights of the attributes in IB1-IG. The
total sum of the weights is 2.48.
fold Precision (%) Recall (%) F-score t
1 94.87 94.12 94.49 1.96
2 93.52 93.85 93.68 1.98
3 95.25 94.72 94.98 1.95
4 95.30 94.32 94.81 1.95
5 92.91 93.54 93.22 1.87
6 94.49 94.50 94.50 1.92
7 95.88 94.35 95.11 1.94
8 94.25 94.18 94.21 1.94
9 92.96 91.97 92.46 1.91
10 95.24 94.02 94.63 1.97
Avg. 94.47?1.04 93.96?0.77 94.21?0.84 1.94
Table 8: The final result of the proposed method by
combining the rules and the memory-based learning.
The average accuracy is 98.21?0.43.
is, the order of important lexical attributes is
?W
i
,W
i?1
,W
i+1
,W
i?2
,W
i+2
,W
i+3
,W
i?3
?. The
same phenomenon is found in part-of-speech
(POS) and chunk type (C). In comparing the part-
of-speech information with the lexical information,
we find out that the part-of-speech is more impor-
tant. One possible explanation for this is that the
lexical information is too sparse.
The best performance on English reported is
94.13 in F-score (Zhang et. al, 2001). The reason
why the performance on Korean is lower than that
on English is the curse of dimensionality. That is,
the wider context is required to compensate for the
free order of Korean, but it hurts the performance
(Cherkassky and Mulier, 1998).
5.4 Performance of the Hybrid Method
Table 8 shows the final result of the proposed
method. The F-score is 94.21 on the average which
is improvement of 2.34 over the rules only, 1.67 over
support vector machines, and 2.83 over memory-
based learning. In addition, this result is as high as
the performance on English (Zhang et. al, 2001).
80
82
84
86
88
90
92
94
96
98
100
ADVP IP NP VP
Phrases
F
-
s
c
o
re
Rule Only
Hybrid
Figure 4: The improvement for each kind of phrases
by combining the rules and MBL.
The threshold t is set to the value which produces
the best performance on the held-out set. The total
sum of all weights in Table 7 is 2.48. This implies
that when we set t > 2.48, only the rules are ap-
plied since there is no exception with this threshold.
When t = 0.00, only the memory-based learning is
used. Since the memory-based learning determines
the chunk type of w
i
based on the exceptional cases
of the rules in this case. the performance is poor with
t = 0.00. The best performance is obtained when t
is near 1.94.
Figure 4 shows how much F-score is improved for
each kind of phrases. The average F-score of noun
phrase is 94.54 which is far improved over that of the
rules only. This implies that the exceptional cases of
the rules for noun phrase are well handled by the
memory-based learning. The performance is much
improved for noun phrase and verb phrase, while it
remains same for adverb phrases and independent
phrases. This result can be attributed to the fact that
there are too small number of exceptions for adverb
phrases and independent phrases. Because the ac-
curacy of the rules for these phrases is already high
enough, most cases are covered by the rules. Mem-
ory based learning treats only the exceptions of the
rules, so the improvement by the proposed method
is low for the phrases.
6 Discussion
In order to make the proposed method practical and
applicable to other NLP problems, the following is-
sues are to be discussed:
1. Why are the rules applied before the
memory-based learning?
When the rules are efficient and accurate
enough to begin with, it is reasonable to ap-
ply the rules first (Golding and Rosenbloom,
1996). But, if they were deficient in some
way, we should have applied the memory-based
learning first.
2. Why don?t we use all data for the machine
learning method?
In the proposed method, memory-based learn-
ing is used not to find a hypothesis for inter-
preting whole data space but to handle the ex-
ceptions of the rules. If we use all data for both
the rules and memory-based learning, we have
to weight the methods to combine them. But, it
is difficult to know the weights of the methods.
3. Why don?t we convert the memory-based
learning to the rules?
Converting between the rules and the cases in
the memory-based learning tends to yield inef-
ficient or unreliable representation of rules.
The proposed method can be directly applied to
the problems other than chunking Korean if the
proper rules are prepared. The proposed method will
show better performance than the rules or machine
learning methods alone.
7 Conclusion
In this paper we have proposed a new method
to learn chunking Korean by combining the hand-
crafted rules and a memory-based learning. Our
method is based on the rules, and the estimates on
chunks by the rules are verified by a memory-based
learning. Since the memory-based learning is an
efficient method to handle exceptional cases of the
rules, it supports the rules by making decisions only
for the exceptions of the rules. That is, the memory-
based learning enhances the rules by efficiently han-
dling the exceptional cases of the rules.
The experiments on STEP 2000 dataset showed
that the proposed method improves the F-score of
the rules by 2.34 and of the memory-based learn-
ing by 2.83. Even compared with support vector
machines, the best machine learning algorithm in
text chunking, it achieved the improvement of 1.67.
The improvement was made mainly in noun phrases
among four kinds of phrases in Korean. This is
because the errors of the rules are mostly related
with noun phrases. With relatively many instances
for noun phrases, the memory-based learning could
compensate for the errors of the rules. We also em-
pirically found the threshold value t used to deter-
mine when to apply the rules and when to apply
memory-based learning.
We also discussed some issues in combining a
rule-based method and a memory-based learning.
These issues will help to understand how the method
works and to apply the proposed method to other
problems in natural language processing. Since the
method is general enough, it can be applied to other
problems such as POS tagging and PP attachment.
The memory-based learning showed good perfor-
mance in these problems, but did not reach the state-
of-the-art. We expect that the performance will be
improved by the proposed method.
Acknowledgement
This research was supported by the Korean Ministry
of Education under the BK21-IT program and by the
Korean Ministry of Science and Technology under
NRL and BrainTech programs.
References
V. Cherkassky and F. Mulier. 1998. Learning from Data:
Concepts, Theory, and Methods, John Wiley & Sons,
Inc.
CoNLL. 2000. Shared Task for Computational
Natural Language Learning (CoNLL), http://lcg-
www.uia.ac.be/conll2000/chunking.
T. Cover and P. Hart. 1967. Nearest Neighbor Pat-
tern Classification, IEEE Transactions on Information
Theory, Vol. 13, pp. 21?27.
W. Daelemans, A. Bosch and J. Zavrel. 1999. Forgetting
Exceptions is Harmful in Language Learning, Ma-
chine Learning, Vol. 34, No. 1, pp. 11?41.
W. Daelemans, J. Zavrel, K. Sloot and A. Bosch. 2001.
TiMBL: Tilburg Memory Based Learner, version 4.1,
Reference Guide, ILK 01-04, Tilburg University.
A. Golding and P. Rosenbloom. 1996. Improving Accu-
racy by Combining Rule-based and Case-based Rea-
soning, Artificial Intelligence, Vol. 87, pp. 215?254.
T. Joachims. 1998. Making Large-Scale SVM Learning
Practical, LS8, Universitaet Dortmund.
K.-C. Kim, K.-O. Lee, and Y.-S. Lee. 1995. Korean
Compound Verbals Processing driven by Morpholog-
ical Analysis, Journal of KISS, Vol. 22, No. 9, pp.
1384?1393.
Taku Kudo and Yuji Matsumoto. 2000. Use of Support
Vector Learning for Chunk Identification, In Proceed-
ings of the Fourth Conference on Computational Nat-
ural Language Learning, pp. 142?144.
S.-B. Park and B.-T. Zhang. 2001. Combining a Rule-
based Method and a k-NN for Chunking Korean Text,
In Proceedings of the 19th International Conference
on Computer Processing of Oriental Languages, pp.
225?230.
R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing, Morgan Kaufmann Publishers.
L. Ramshaw and M. Marcus. 1995. Text Chunking Us-
ing Transformation-Based Learning, In Proceedings
of the Third ACL Workshop on Very Large Corpora,
pp. 82?94.
H.-P. Shin. 1999. Maximally Efficient Syntatic Parsing
with Minimal Resources, In Proceedings of the Con-
ference on Hangul and Korean Language Infomration
Processing, pp. 242?244.
J.-T. Yoon and K.-S. Choi. 1999. Study on KAIST Cor-
pus, CS-TR-99-139, KAIST CS.
T. Zhang, F. Damerau and D. Johnson. 2001. Text
Chunking Using Regularized Winnow, In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics, pp. 539?546.
Reducing Parsing Complexity by Intra-Sentence Segmentation 
based on Maximum Entropy Model 
Sung Dong K im,  ByoungoTak  Zhang,  Yung Taek  K im 
School of Computer Science and Engineering, 
Seoul National University, Korea 
{sdkim,btzhang}@scai. snu. ac. kr, ytkim@cse, snu. ac. kr 
Abstract 
Long sentence analysis has been a critical 
problem because of high complexity. This pa- 
per addresses the reduction of parsing com- 
plexity by intra-sentence segmentation, and 
presents max imum entropy model for deter- 
mining segmentation positions. The model 
features lexical contexts of segmentation posi- 
tions, giving a probability to each potential 
position. Segmentation coverage and accu- 
racy of the proposed method are 96% and 
88% respectively. The parsing efficiency is im- 
proved by 77% in time and 71% in space. 
1 Introduction 
Long sentence analysis has been a critical 
problem in machine translation because of 
high complexity. In EBMT (example-based 
machine translation), the longer a sentence 
is, the less possible it is that the sentence 
has an exact match in the translation archive, 
and the less flexible an EBMT system will be 
(Cranias et al, 1994). In idiom-based ma- 
chine translation (Lee, 1993), long sentence 
parsing is difficult because more resources are 
spent during idiom recognition phase as sen- 
tence length increases. A parser is often un- 
able to analyze long sentences owing to their 
complexity, though they have no grammatical 
errors (Nasukawa, 1995). 
In English-Korean machine translation, 
idiom-based approach is adopted to overcome 
the structural differences between two lan- 
guages and to get more accurate translation. 
The parser is a chart parser with a capabil- 
ity of idiom recognition and translation, which 
is adapted to English-Korean machine trana- 
lation. Idioms are recognized prior to syn- 
tactic analysis and the part of a sentence for 
an idiom takes an edge in a chart (Winograd, 
1983). When parsing long sentences, an am- 
biguity of an idiom's range may cause more 
edges than the number of words included in 
the idiom (Yoon, 1994), which increases pars- 
ing complexity much. A parser of practical 
machine translation system should be able to 
analyze long sentences in a reasonable time. 
Most context-free parsing algorithms have 
O(n 3) parsing complexities in terms of time 
and space, where n is the length of a sen- 
tence (Tomita, 1986). Our work is moti- 
vated by the fact that parsing becomes more 
efficient, if n becomes horter. This paper 
deals with the problem of parsing complex- 
ity by way of reducing the length of sentence 
to be analyzed. This reduction is achieved 
by in t ra -sentence  segmentat ion,  which is 
distinguished from inter--sentence s gmen-  
tat ion that is used for text categorization 
(Beeferman et al, 1997) or sentence boundary 
identification (Palmer and Hearst, 1997) (Rey- 
nar and Ratnaparkhi, 1997). Intra-sentence 
segmentation plays a role as a preliminary 
step to a chart-based, context-free parser in 
English-Korean machine translation. 
There have been several methods for 
reducing parsing complexities by intra- 
sentence segmentation. In (Lyon and Frank, 
1995)(Lyon and Dickerson, 1997), they took 
advantage of the fact that the declarative 
sentences almost always consist of three seg- 
ments: \[pre-subject : subject:predicate\] .  
The complexity could be reduced by decom- 
posing a sentence into three sections. Pattern 
rules (Li et al, 1990) and sentence patterns 
(Kim and Khn, 1995) were used to segment 
long English sentences. They showed low seg- 
mentation coverage, which means that many 
of long sentences are not segmented by the 
pattern rules or sentence patterns. And they 
require much human efforts to construct pat- 
tern rules or collect sentence patterns. These 
factors may prevent hem being applicable to 
practical machine translation sYstems. 
This paper presents a trainable model for 
identifying potential segmentation positions 
164 
in a sentence and determining appropriate 
segmentation positions. Given a corpus anno- 
tated with segmentation positions, our model 
automatically earns the contextual evidences 
about segmentation positions, which relieves 
human of burden to construct pattern rules or 
sentence patterns. These evidences are com- 
bined under the maximum entropy framework 
(Jaynes, 1957) to estimate the probability for 
each position. By intra-sentence s gmenta- 
tion based on the proposed model, we achieve 
more improved parsing efficiency by 77% in 
time and 71% in space. 
In Section 2 we introduce the maximum en- 
tropy model. Section 3 describes features in- 
corporated into the model and the process of 
identifying potential segmentation positions. 
The determination schemes of segmentation 
positions are described in Section 4. Segmen- 
tation performance of the model is presented 
with the degree of contribution to efficient 
parsing by the segmentation in Section 5. We 
also compare our approach with other intra- 
sentence segmentation approaches. Section 6 
draws conclusions and presents ome further 
works. 
2 Max imum Ent ropy  Mode l ing  
Sentence patterns or pattern ruels specify the 
sub-structures of the sentences. That is, seg- 
mentation positions are determined in view of 
the global sentence structure. If there is no 
matched rules or patterns with a given sen- 
tence, the sentence could not be segmented. 
We assume that whether aword is a segmenta- 
tion position depends on its surrounding con- 
text. We try to find factors that affect the de- 
termination of segmentation positions. Maxi- 
mum entropy is a technique for automatically 
acquiring knowledge from incomplete infor- 
mation, without making any unsubstantiated 
assumptions. It masters ubtle effects o that 
we may accurately model subtle dependencies. 
It does not make any unwarranted assump- 
tions, which means that maximum entropy 
learns exactly what the data says. Therefore 
it can perform well on unseen data. 
The idea is to construct a model that as- 
signs a probability to each potential segmen- 
tation position in a sentence. We build a prob- 
ability distribution p(ylx), where y ? {0, 1} 
is a random variable specifying the potential 
segmentation position in a context x. A fea- 
tu re  of a context is a binary-valued indicator 
function \] expressing the information about a 
specific context. 
Given a training sample of size N, 
(Xl,Yl),..-, (XN,YN), an empir ica l  proba-  
bi l i ty d i s t r ibut ion  can be defined as 
y) = y) 
N 
where #(x,  y) is the number of occurrences of
(x, y). The expected value of feature fi with 
respect o the empirical distribution i~(x, y) is 
expressed as 
x,y 
and the expected value of fi with respect o 
the probability distribution p(ylx) is 
p(.fi) -~ ~(x)pCylx) .h(x ,  y), 
x~y 
where l~(x) is the empirical distribution of x 
in the corpus. We want to build probability 
distribution p(ylx) that is required to accord 
to the feature fi useful in selecting segmenta- 
tion positions: P(fi) = IS(fi) for all fi ? .T, 
where Y is the set of candidate features. This 
makes the probability distribution be built on 
only training data. 
Given a feature set .T, let C be the subset 
of all distributions P that satisfies the require- 
ment P(fi) = P(fi): 
C ~- {p ? ~ \] P(fi) = P(fi), for all fi ? .T}. 
(1) 
We choose a probability distribution consis- 
tent with all the facts, but otherwise as uni- 
form as possible. The uniformity of the prob- 
ability distribution p(ylx) is measured by the 
conditional entropy: 
H(p) = - ~p(x ,  y) logp(ylx) 
x~y 
=   (x)PCulx) logp(ylx) 
x,y 
Thus, the probability distribution with maxi- 
mum entropy is the most uniform distribution. 
In building a model, we consider the linear 
exponential family Q given as 
1 
Q(f) = {p(ylx)= ~exp(  E ~ifi(x,y))}, 
$ 
(2) 
165 
where Ai are real-valued parameters and 
ZA(x) is a normalizing constant: 
= exp(  y)). 
y i 
An intersection of the class Q of exponential 
models with the class of desired distribution 
(1) is nonempty, and the intersection contains 
the maximum entropy distribution and fur- 
thermore it is unique (Ratnaparkhi, 1994). 
Finding p. E C that maximizes H(p) is a 
problem in constrained optimization, which 
cannot be explicitly written in general. There- 
fore, we take advantage of the fact that the 
models in Q that satisfy p(fi) = 15(fi) can 
be explained under the maximum likelihood 
framework (Ratnaparkhi, 1994). Maximum 
likelihood principle also gives the unique dis- 
tribution p., the intersection of the class Q 
with C. 
We assume each occurrence of (x,y) is 
sampled independently. Thus, log-likelihood 
L#(p) of the empirical distribution i5 as pre- 
dicted by a model p can be defined as 
L~)  _= log I I  p(ylx) ~(~'y) = ~\]p(x,  y) logp(ylx ). 
x,y x ,y  
That is, the model we want to build is 
p.  = arg  xc = arg  max 
qE~ 
The parameters A~ of exponential model (2) 
are obtained by the Generalized Iterative Scal- 
ing algorithm (Darroch and Ratcliff, 1972). 
3 Const ruct ion  o f  Features  
This section describes the features. From a 
corpus, contextual evidences of segmentation 
positions are collected and combined, result- 
ing in features. The features are used in iden- 
tifying potential segmentation positions and 
included in the model. 
3.1 Segmentab le  Posit ions and  Safe 
Segmentation 
A sentence is constructed by the combina- 
tion of words, phrases, and clauses under the 
well-defined grammar. A sentence can be seg- 
mented into shorter segments that correspond 
to the constituents of the sentence. That is, 
segments correspond to the nonterminal sym- 
bols of the context-free grammar 1. The posi- 
1Nonterminal symbols include the ones for phrases, 
such as NP  (noun phrase) and VP  (verb phrase), 
tion of a word is called segmentab le  posi- 
t ion that can be a starting position of a spe- 
cific segment. 
Though the analysis complexity can be re- 
duced by segmenting a sentence, there is 
a mis-segmentation risk that causes pars- 
ing failures. A segmentation can be called 
safe segmentat ion  that results in a coherent 
blocks of words. In English-Korean transla- 
tion, safe segmentation is defined as the one 
which generates safe segments. A segment is 
safe, when there is a syntactic ategory sym- 
bol N P dominating the segment and the seg- 
ment can be combined with adjacent segments 
under a given grammar. In Figure 1, (a) is an 
unsafe segmentation because the second seg- 
ment cannot be analyzed into one syntactic 
category, resulting in parsing failure. By the 
safe segmentation (b), the first segment cor- 
responds to a noun phrase and the second to 
a verb phrase, so that we can get a correct 
analysis result. 
(a) IThe students I 
who study hard will pass the exam\] 
(b) I The students who study hard I
I will pass the exam\[ 
Figure h Examples of unsafe/safe segmenta- 
tion in English-Korean translation. 
3.2 Lexical Contextua l  Const ra in ts  
A lexical context  of a word includes even- 
word window: three to the left of a word and 
three to the right of a word and a word itself. 
It also includes the part-of-speeches of these 
words, subcategorization information for two 
words to the left, and position value. The 
position value posi_v of the ith word wi is cal- 
culated as 
pos _v = r ? R\ ] ,  
where n is the number of words and R 2 repre- 
sents the number of regions in the sentence. 
Region is the sequentially ordered block of 
and the ones for clauses like RLCL (relative clause), 
SUBCL (subordinate clause). 
sit is a heuristically set value, and we set R as 4. 
166 
words in a sentence, and posi_v represents he 
region in which a word lies. It is included to 
reflect the influence of the position of a word 
on being a segmentation position. Thus, the 
lexical context of a word is represented by 17 
attributes as shown in Figure 2. 
s_position? 
wordi 
Wi-3~ ? . . ,  Wi+3 
Pi-3, " ?, Pi+3 
8.-Cgtti-2, S-carl- 1 
posi_v 
Figure 2: The structure of lexical context. 
An example of a training data and a re- 
sulting lexical context is shown in Figure 3. 
A symbol '# '  represents a segmentation posi- 
tion marked by human annotators. Therefore, 
the lexical context of word when includes the 
value 1 for attribute s_position? and follow- 
ings: three words to the left of when (became, 
terribly, and worried) and part-of-speeches 
o f  each word (VERB ADV ADJ), three words 
to the right (they, saw, and what) and part- 
of-speeches (PRON VERB PRON), subcat- 
egorization information for two words to the 
left (0 1), and position value (2). 
Of course his parents became terribly worried 
#when they saw what was happening 
to Atzel. 
( 1 when became terribly worried they saw 
what VERB ADV ADJ PRON VERB 
PRON 0 1 2 ) 
Figure 3: An example of a training data and 
a lexical context. 
To get reliable statistics, much training 
data is required. To alleviate this prob- 
lem, we generate lexical contextua l  con- 
st ra lnts  by combining lexical contexts and 
collect statistics for them. To generate lex- 
ical contextual constraints and to identify 
segmentable positions, we define two oper- 
ations join (E9) and consistency (=). Let 
(a l , . . . ,an)  and (bl , . . . ,bn) be lexical con- 
texts and (C1,... ,On) be lexical contextual 
constraint. The operation join is defined as 
(a l , . . . ,  an) ? (bl, .- . ,  bn) = (C1,... ,  Ca), 
,.i if ai # bi 
Ci = ai if ai = bi ' 
where ' , '  is don't-care term accepting any 
value. A lexical contextual constraint is gen- 
erated as a result of jo in operation. The 
consistency is defined as 
1 if (C i=a iorC i= '* r )  fo ra l l l< i<n 
k = 0 otherwise 
The algorithm for generating lexical contex- 
tual constraints i shown in Figure 4. 
? Input: a set of active lexical contexts 
LCw = { lc l . . .  lcn} for word w, 
where lcc/= (a l , . . . ,  an). 
? Output: a set of lexical contextual 
constraints LCCw = {/ccl .../cck}, 
where lcc /= (C1,. . . ,  Cn). 
1. Initialize LCCw = 0 
2. Do the followings for each l~ E LCw 
(a) For all lcj(j # i), Count(lcj) = # of 
matched attributes with Ic/ 
(b) max_cnt = arg maxlc? eLC. Count( Icj ) 
(c) For all lcj, where Count(lcj) = max..cnt, 
Icc= lc~ ? lc~, LCCw e- LCC,  U {/cc} 
Figure 4: Algorithm for generating lexical 
contextual constraints. 
A Icc plays the role of a feature. Following 
is an example of a feature. 
f (x ,y )  = 
1 i f  Xward  = "that" and 
xi-1 = "say" and y = 1 
0 otherwise 
We collect the statistics for each Icc. The fre- 
quency of each lcc is counted as the number 
of lexical contexts that satisfy the consistency 
operation with the lcc. 
n 
i=1 
167 
Identifying segmentable positions is per- 
formed with the consistency operation with 
the lexical context of word w and lcc E LCCw. 
The word whose lexical context is consistent 
with lcc is identified as a segmentable posi- 
tion. 
4 Determinat ion  Schemes  of  
Segmentat ion  Pos i t ions  
Segmentation positions are determined 
through two steps: identifying segmentable 
positions and selecting the most appropriate 
position among them. Segmentable positions 
are identified using the consistency operation. 
Maximum entropy model in Section 2 gives a 
probability to each position. 
Segmentation performance is measured in 
terms of coverage and accuracy. Coverage is 
the ratio of the number of actually segmented 
sentences to the number of segmentation tar- 
get sentences that are longer than ot words, 
where o~ is a fixed constant distinguishing long 
sentences from short ones. Accuracy is evalu- 
ated in terms of the safe segmentation ratio. 
They are defined as follows: 
# of actually segmented Sent. 
coverage = ~ of Sent. to be segmented 
(3) 
# of Sent. with safe segmentation 
accuracy = ~ of actually segmented Sent. 
(a) 
4.1 Basel ine Scheme 
No contextual information is used in identify- 
ing segmentable positions. They are empiri- 
cally identified. A word that is tagged as a 
segmentation position more than 5 times is 
identified as a segmentable position. A set of 
segmentable positions, 9 ,  is as follows. 
~D = {wi \[ wi is tagged as segmentation position 
and #(tagged wi) >_ 5} 
In order to select the most appropriate po- 
sition, the segmentation appropriateness of
each position is evaluated by the probability 
of word wi: 
# of tagged wi 
p(Wi) = # of wi in the corpus 
p(wi) represents the tendency that word wi 
will be used as a segmentation position. A 
segmentation position w. is selected as the one 
that has highest p(wi) value: 
w. = arg max p(wi). wiE~ 
This scheme serves as a baseline for comparing 
the segmentation performance of the models. 
4.2 A Scheme using Lexical 
Contextua l  Const ra ints  
Lexical contextual constraints are used in 
identifying segmentable positions. Compared 
with the baseline scheme, this scheme con- 
siders contextual information of a word. All 
consistent words with the defined lexical con- 
textual constraints form a set of segmentable 
positions 79. 
The maximum likelihood principle gives a 
probability distribution for p(y I lcc~), where 
y E {0, 1}. Segmentation appropriateness is 
evaluated by p(1 I lcew,). A position with the 
highest p(1 I lcc~) becomes a segmentation 
position: 
w. = arg max p(1 I/CCwi). 
wi E~ 
4.3 A Scheme using Lexical 
Contextua l  Const ra ints  w i th  
Word Sets 
Due to insufficient raining samples for con- 
structing lexical contextual constraints, ome 
segmentable positions may not be identified. 
To alleviate this problem we introduce word 
sets whose elements have linguistically similar 
features. We define four word sets: coordinate 
conjunction set, subordinate conjunction set, 
interogative set, auxiliary verb set. The cate- 
gories of word sets and the examples of their 
members are shown in Table 1. 
Table 1: The word sets and examples. 
Word Set Examples 
Coordinate Conjunctions and, or, but 
Subordinate Conjunctions if, when, . . .  
Interogatives how, what, . . .  
Auxiliary Verbs can, should, . . .  
Coordinate conjunctions haveonly 3 mem- 
bers, but they frequently apprear in long sen- 
tences. Subordinate conjunctions have 25 
168 
members, interogatives 5 members, and aux- 
iliary verbs have 12 members now. The words 
belonging to each word set are treated equally. 
Lexical contextual constraints are constructed 
for words and word sets, so the statistics is 
collected for both of them. The set of seg- 
mentable positions T~ is defined somewhat dif- 
ferently as: 
:D = {wi, wsj I (Icc, v, -= lcc~,) = 1 
or (Icws~ =-- IcC.ws~) = 1}, 
where wsj denotes a word set to which the j th  
word in a sentence belongs. 
In this scheme, p(1 I Iccc,,,) or p(1 \] lccws,) 
expresses the segmentation appropriateness of 
the position. Therefore, a segmentation posi- 
tion is determined by 
w, = arg max {p(1 I lcc ,), p(1 I lcc s )}. {w,,ws~}~9 
5 Exper iments  
5.1 Corpus  and  Const ruct ion  of  the  
Max imum Ent ropy  Mode l  
We construct he corpus from two different 
domains, where the sentences longer than 15 
-words are extracted 3. The training portion is 
used to generate l xical contextual constraints 
and to collect statistics for maximum entropy 
model construction. From high school English 
texts, 1500 sentences are tagged with segmen- 
tation positions by human. Two people who 
have some knowledge about English syntactic 
structures read sentences, and marked words 
as segmentation positions where they paused. 
After generating lexical contextual con- 
straints, we constructed the maximum en- 
tropy model p(ylx), where x is a lexical con- 
textual constraint and y E {0,1}. The model 
incorporates features that occur more than 5 
times in the training data. 3626 candidate fea- 
tures were generated without word sets and 
3878 features with word sets. In Table 2, 
training time and the number of active fea- 
tures of the model are shown. 
Segmentation performance is evaluated us- 
ing test portion that consists of 1800 sentences 
ffrom two domains: high school English texts 
and the Byte Magazine. 
3The sentences with commas are excluded because 
comma is an explicit segmentation position. Segments 
resulting from a segmentation at commas may be the 
manageable-sized ones. Our work is to segment long 
sentences without explicit segmentation positions. 
Table 2: Construction of models. 
Training # of 
Time Active Features 
Without 10 rain 2720 
Word Sets 
With 12 mln 2910 
Word Sets 
5.2 Segmentat ion  Per fo rmance  
In addition to coverage and accuracy, SC 
value is also defined to express the degree of 
contribution to efficient parsing by segmenta- 
tion. It is the ratio of the sentences that can 
benefit from intra-sentence s gmentation. If a 
long sentence isnot segmented or is segmented 
at unsafe segmentation positions, the sentence 
is called a segmentat ion  er ror  sentence .  
SC value is calculated as 
# of segmentation error sentences SG= I -  
# of segmentation target sentences" 
A sentence longer than vt words is con- 
sidered as the segmentation target sentence, 
where c~ is set to 12. Table 3 compares eg- 
mentation performance for each determina- 
tion scheme. 
Table 3: Segmentation performance of the de- 
termination schemes of segmentation position. 
Determination Coverage/ 
Schemes Accuracy (%) 
Baseline 100/77.6 
LCC 90.7/89 
LCC with 95.8/87.9 
Word Sets 
SC 
0.776 
0.808 
0.865 
By the comparison of the baseline scheme 
with others, the accuracy is observed to de- 
pend on the context information. Word sets 
are helpful for increasing coverage with less 
degradation of accuracy. Each scheme has su- 
periority in terms of the different measures. 
But in terms of applicability to practical sys- 
tems, the third scheme is best for our purpose. 
Table 4 shows the segmentation performance 
of the scheme using LCC with word sets. 
SU value for the sentences from the same 
domain as training data is about 0.88, and 
169 
Table 4: Segmentation performance of LCC 
with word sets. 
Domain Sent. Coverage/ I 
Length Accuracy(%) I 
15~19 
High-School 
English Text 
Byte 
Magazine 
Total 
20~24 
25~29 
30~ 
15,-,19 
20,-~24 
25,,~29 
30,-~ 
99.0/95.9 
100/94.0 
96.0/81.3 
100/67.5 
94.0/92.6 
91.0/91.2 
92.5/94.6 
93.5/86.1 
1800 95.8/87.9 \] 
8C 
0.95 
0.94 
0.78 
0.6g 
0.87 
0.83 
0.88 
0.81 
0.87 
about 0.85 for the sentenes from the Byte 
Magazine. Though they slightly differ be- 
tween test domains, about 87% of long sen- 
tences can be parsed with less complexity and 
without causing parsing failures. It suggests 
that the intra-sentence s gmentation method 
can be utilized for efficient parsing of the long 
sentences. 
5.3  Pars ing  E f f i c iency  
Parsing efficiency is generally measured by 
the required time and memory for parsing. 
In most cases, parsing sentences longer than 
30 words could not complete without intra- 
sentence segmentation. Therefore, the parsing 
is performed for the sentences longer than 15 
and less than 30 words. Ultra-Sparc 30 ma- 
chine is used for experiments. The efficiency 
improvement was measured by 
EItime tunseg -- tseg = ,,, x 100, 
tunseg 
EImemory = rnunseg --mseg X 100, 
~T~unse9 
where $unseg and rrbanseg are time and memory 
during parsing without segmentation a d tseg, 
rnseg are for the parsing with segmentation. 
Table 5 summarizes the results. 
By segmenting long sentences into several 
manageable-sized s gments, we can parse long 
sentences with much less time and space. 
5.4  Compar i son  w i th  Re la ted  Works 
The intra-sentence segmentation method 
based on the maximum entropy model is corn- - 
pared with other approaches in terms of the 
Table 5: Comparison of parsing efficiency 
with/without segmentation. 
With 
Segmentation 
Without 
Segmentation 
Improvement 
High-School Byte 1 
English Text Magazine 
4.6 sec 5.4 sec 1 
0.9 MB 
19.6 sec 
3.4 MB 
76.5% 
73.5% 
1.1 MB 
'25.1 sec 
3.7 MB 
78.5% 
70.3% 
segmentation coverage and the improvement 
of parsing efficiency. 
In (Lyon and Frank, 1995)(Lyon and Dick- 
erson, 1997), a sentence is segmented into 
three segments. Though parsing efficiency can 
be improved by segmenting a sentence, this 
method may be applied to only simple sen- 
tences 4. Long sentences are generally coordi- 
nate sentences 5 or complex sentences 6. They 
have more than two subjects, so applying this 
method to such sentences seems to be inap- 
propriate. 
In (Kim and Kim, 1995), sentence patterns 
are used to segment long sentences. This 
method improve parsing efficiency by 30% in 
time and 58% in space. However collecting 
sentence patterns requires much hnman efforts 
and segmentation coverage isonly about 36%. 
Li's method (Li et al, 1990) for sentence 
segmentation also depends upon manual- 
intensive pattern rules. Segmentation cover- 
age seems to be unsatisfactory for practical 
machine translation system. 
The proposed method can be applied to co- 
ordinate and complex sentences a  well as sim- 
ple sentences. It shows segmentation coverage 
of about 96%. In addition, it needs no other  
human efforts except for constructing training 
data. Human ~.nnotators have only to read 
sentences and mark segmentation positions, 
which is more simple than collecting pattern 
rules or sentence patterns. We can also get 
much improved parsing efficiency: about 77% 
in time and about 71% in space. 
4A simple sentence has one subject and one predi- 
cate. 
5A coordinate sentence results ~om the combina- 
tion of several simple sentences by the coordinate con- 
junctions. - 
6A complex sentence consists of a main clause and 
several subordinate clauses. 
170 
6 Conc lus ion  and  Future  Work  
Practical machine translation systems hould 
be able to accommodate long sentences. Thus 
intra-sentence s gmentation is required as a 
means for reducing parsing complexity. This 
paper presents a method for intra-sentence 
segmentation based on the maximum entropy 
model. The method builds statistical models 
automatically from a text corpus to provide 
the segmentation appropriateness for safe seg- 
mentation. 
In the experiments with 1800 test sentences, 
about 87% of them were benefited from seg- 
mentation. The statistical intra-sentence seg- 
mentation method can also relieve human of 
the burden of constructing information, such 
as segmentation rules or sentence patterns. 
Experiments suggest that the proposed maxi- 
mum entropy models can be incorporated into 
the parser for practical machine translation 
systems. 
Further works can be done in two direc- 
tions. First, studies on recovery mecha- 
nisms for unsafe segmentation before parsing 
seem necessary since ungafe segmentation may 
cause parsing failures. Second, parsing control 
mechanisms should be studied that exploit he 
-characteristics of segmentation positions and 
the parallelism among segments. This will en- 
hance parsing efficiency further. 
Re ferences  
D. Beeferman, A. Berger, and J. Lafferty. 1997. 
Text Segmentation using Exponential Models. 
In Second Conference on Empirical Methods in 
Natural Language Processing. Providence, RI. 
Lambros Cranias, Harris Papageorgiou, and Ste- 
lios Piperdis. 1994. A Matching Technique in 
Example-Based Machine Translati on. In Pro- 
ceedings of 1995 COLING, pages 100--104. 
J.N. Darroch and D. Ratcliff. 1972. Generalized 
Iterative Scaling for Log-linear Models. The 
Annals of Mathematical Statistics, 43(5):1470- 
1480. 
E.T. Jaynes. 1957. Information Theory and Sta- 
tistical Mechanics. Physical Review, 106:620- 
630. 
Sung Dong Kim and Yung Taek Kim. 1995. 
Sentence Analysis using Pattern Matching in 
English-Korean Machine Translation. In Pro- 
ceedings of the 1995 ICCPOL, Oct. 25-28. 
Ho Suk Lee. 1993. Automatic Construction of 
Transfer Dictionary based on the Corpus for 
English-Korean Machine Translation. Ph.D. 
thesis, Seoul National University. In Korean. 
Wei-Chuan Li, Tzusheng Pei, Bing-Huang Lee, 
and Chuei-Feng Chiou. 1990. Parsing Long Ea- 
ghsh Sentences with Pattern Rules. In Proceed- 
ings of 25th Conference of COLING, pages 410-- 
412. 
Caroline Lyon and Bob Dickerson. 1997. Reduc- 
ing the Complexity of Parsing by a Method of 
Decomposition. In International Workshop on 
Parsing Technology, September. 
Caroline Lyon and Ray Frank. 1995. Neural Net- 
work Design for a Natural Language Parser. 
In International Conference on Artificial Neural 
Networks. 
Tetsura Nasukawa. 1995. Robust Parsing Based 
on Discourse Information. In 33rd Annual 
Meeting of the A CL, pages 33-46. 
David D. Palmer and Marti A. Hearst. 1997. 
Adaptive Multilingual Sentence Boundary 
Disambiguation. Computational Linguistics, 
23(2):241-265. 
A. Ratnaparkhi. 1994. A Simple Introduction 
to Maximum Entropy Models for Natural Lano 
gnage Processing. Technical report, Institute 
for Research in Cognitive Science, University of 
Pennsylvania 3401 Walnut Street, Suite 400A 
Philadelphia, PA 19104-6228, May. IRCS Re- 
port 97-08. 
J.C. Reynar and A. Ratnaparkhi. 1997. A Maxi- 
mum Entropy Approach to Identifying Sentence 
Boundaries. In Proceedings of the Fifth Confer- 
ence on Applied Natural Language Processing, 
pages 16--19. Washington D.C. 
Masaru Tomita. 1986. E~icient Parsing for Nat- 
ural Language. Kluwer Academic Publishers. 
T. Winograd. 1983. Language as a Cognitive Pro- 
cess: Syntax, volume 1. Addison-Wesley. 
Sung Hee Yoon. 1994. Efficient Parser to Find 
Bilingual Idiomatic Expressions for English- 
Korean Mvchine Translation. In Proceedings of 
the 1994 ICCPOL, pages 455-460. 
171 
