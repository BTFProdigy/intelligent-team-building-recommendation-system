Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 65?72
Manchester, August 2008
Good Neighbors Make Good Senses:
Exploiting Distributional Similarity for Unsupervised WSD
Samuel Brody
School of Informatics
University of Edinburgh
s.brody@sms.ed.ac.uk
Mirella Lapata
School of Informatics
University of Edinburgh
mlap@inf.ed.ac.uk
Abstract
We present an automatic method for sense-
labeling of text in an unsupervised manner.
The method makes use of distributionally
similar words to derive an automatically
labeled training set, which is then used to
train a standard supervised classifier for
distinguishing word senses. Experimental
results on the Senseval-2 and Senseval-3
datasets show that our approach yields sig-
nificant improvements over state-of-the-art
unsupervised methods, and is competitive
with supervised ones, while eliminating
the annotation cost.
1 Introduction
Word sense disambiguation (WSD), the task of
identifying the intended meaning (sense) of words
in context, is a long-standing problem in Natural
Language Processing. Sense disambiguation is of-
ten characterized as an intermediate task, which is
not an end in itself, but has the potential to improve
many applications. Examples include summariza-
tion (Barzilay and Elhadad, 1997), question an-
swering (Ramakrishnan et al, 2003) and machine
translation (Chan and Ng, 2007).
WSD is commonly treated as a supervised clas-
sification task. Assuming we have access to data
that has been hand-labeled with correct word
senses, we can train a classifier to assign senses
to unseen words in context. While this approach
often achieves high accuracy, adequately large
sense labeled data sets are unfortunately difficult
to obtain. For many words, domains, languages,
and sense inventories they are unavailable, and
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
in most cases it is unreasonable to expect to ac-
quire them. Ng (1997) estimates that a high accu-
racy domain-independent system for WSD would
probably need a corpus of about 3.2 million sense
tagged words. At a throughput of one word per
minute (Edmonds, 2000), this would require about
27 person-years of human annotation effort.
SemCor (Fellbaum, 1998) is one of the few cor-
pora that have been manually annotated for all
words ? it contains sense labels for 23,346 lem-
mas. In spite of being widely used, SemCor con-
tains too few tagged instances for the majority of
polysemous words (typically fewer than 10 each).
Supervised methods require much larger data sets
than this to perform adequately.
The problem of obtaining sufficient labeled
data, often referred to as the data acquisition bot-
tleneck, creates a significant barrier to the use of
supervised WSD methods in real world applica-
tions. In this work we wish to take advantage of the
high accuracy and strong capabilities of supervised
methods, while eliminating the need for human an-
notation of training data. Our approach exploits a
sense inventory such asWordNet (Fellbaum, 1998)
and corpus data to automatically create a collec-
tion of sense labeled instances which can subse-
quently serve to train any supervised classifier. The
key premise of our work is that a word?s senses
can be broadly described by semantically related
words. So, rather than laboriously annotating all
instances of a polysemous word with its senses, we
collect instances of its related words and treat them
as sense labels for the target word. The method
is inexpensive, language-independent, and can be
used to create large sense-labeled data without
human intervention. Our results demonstrate sig-
nificant improvements over state-of-the-art unsu-
pervised methods that do not make use of hand-
labeled annotations.
In the following section we provide an overview
65
of existing work on unsupervised WSD. Section 3
introduces our method for automatically creat-
ing sense annotations. We present our evaluation
framework in Section 4 and results in Section 5.
2 Related Work
The data requirements for supervisedWSD and the
current paucity of suitably annotated corpora for
many languages and text genres, has sparked con-
siderable interest in unsupervised methods. These
typically come in two flavors: (1) developing al-
gorithms that assign word senses without relying
on a sense-labeled corpus (Lesk, 1986; Galley and
McKeown, 2003) and (2) making use of pseudo-
labels, i.e., labelled data that has not been specifi-
cally annotated for sense disambiguation purposes
but contains some form of sense distinctions (Gale
et al, 1992; Leacock et al, 1998). We briefly dis-
cuss representative examples of both approaches,
with a bias to those closely related to our own
work.
Unsupervised Algorithms One of the first ap-
proaches to unsupervised WSD, and the founda-
tion of many algorithms to come, was originally
introduced by Lesk (1986). The method assigns a
sense to a target ambiguous word by comparing the
dictionary definitions of each of its senses with the
words in the surrounding context. The sense whose
definition has the highest overlap (i.e., words in
common) with the context is assumed to be the
correct one. Despite its simplicity, the algorithm
provides a good baseline for comparison. Cover-
age can be increased by augmenting the dictionary
definition (gloss) of each sense with the glosses of
related words and senses (Banerjee and Pedersen,
2003).
Although most algorithms disambiguate word
senses in context, McCarthy et al (2004) propose
a method that does not rely on contextual cues.
Their algorithm capitalizes on the fact that the dis-
tribution of word senses is highly skewed. A large
number of frequent words is often associated with
one dominant sense. Indeed, current supervised
methods rarely outperform the simple heuristic of
choosing the most common sense in the training
data (henceforth ?the first sense heuristic?), despite
taking local context into account. Rather than ob-
taining the first sense via annotating word senses
manually, McCarthy et al propose to acquire first
senses automatically and use them for disambigua-
tion. Thus, by design, their algorithm assigns the
same sense to all instances of a polysemous word.
Their approach is based on the observation that
distributionally similar neighbors often provide
cues about a word?s senses. Assuming that a set
of neighbors is available, the algorithm quantifies
the degree of similarity between the neighbors and
the sense descriptions of the polysemous word.
The sense with the highest overall similarity is the
first sense. Specifically, the approach makes use of
two similarity measures which complement each
other and provide a large amount of data regarding
the word senses. Distributional similarity indicates
the similarity between words in the distributional
feature space, whereas WordNet similarity in the
?semantic? space, is used to discover which sense
of the ambiguous word is used in the corpus, and
causing the distributional similarity.
Pseudo-labels as Training Instances Gale et al
(1992) pioneered the use of parallel corpora as a
source of sense-tagged data. Their key insight is
that different translations of an ambiguous word
can serve to distinguish its senses. Ng et al (2003)
extend this approach further and demonstrate that
it is feasible for large scale WSD. They gather
examples from English-Chinese parallel corpora
and use automatic word alignment as a means
of obtaining a translation dictionary. Translations
are next assigned to senses of English ambiguous
words. English instances corresponding to these
translations serve as training data.
It has become common to use related words
from a dictionary to learn contextual cues for WSD
(Mihalcea, 2002). Perhaps the first incarnation of
this idea is found in Leacock et al (1998), who
describe a system for acquiring topical contexts
that can be used to distinguish between senses.
For each sense, related monosemous words are ex-
tracted from WordNet using the various relation-
ship connections between sense entries (e.g., hy-
ponymy, hypernymy). Their system then queries
the Web with these related words. The contexts
surrounding the relatives of a specific sense are
presumed to be indicators of that sense, and used
for disambiguation. A similar idea, proposed by
Yarowsky (1992), is to use a thesaurus and acquire
informative contexts from words in the same cate-
gory as the target.
Our own work uses insights gained from unsu-
pervised methods with the aim of creating large
datasets of sense-labeled instances without explicit
manual coding. Unlike Ng et al (2003) our algo-
rithm works on monolingual corpora, which are
66
much more abundant than parallel ones, and is
fully automatic. In their approach translations and
their English senses must be associated manually.
Similarly to McCarthy et al (2004), we assume
that words related to the target word are useful in-
dicators of its senses. Importantly, our method dis-
ambiguates words in context and is able to assign
additional senses, besides the first one.
3 Method
As discussed earlier, our aim is to alleviate the
need for manual annotation by creating a large
dataset labeled with word senses without human
intervention. This dataset can be subsequently
used by any supervised machine learning algo-
rithm. We assume here that we have access to a
corpus and a sense inventory. We first obtain a list
of words that are semantically related to our tar-
get word. In the remainder of this paper we use the
term ?neighbors? to refer to these words. Next, we
separate the neighbors into sense-specific groups.
Finally, we replace the occurrences of each neigh-
bor in our corpus with an instance of the target
word, labeled with the matching sense for that
neighbor. The procedure has two important steps:
(1) acquiring neighbors and (2) associating them
with appropriate senses. We describe our imple-
mentation of each stage in more detail below.
Neighbor Acquisition Considerable latitude is
allowed in specifying appropriate neighbors for the
target word. Broadly speaking, the neighbors can
be extracted from a corpus or from a semantic re-
source, for example the dictionary providing the
sense inventory. A wealth of algorithms have been
proposed in the literature for acquiring distribu-
tional neighbors from a corpus (see Weeds (2003)
for an overview). They differ as to which features
they consider and how they use the distributional
statistics to calculate similarity.
Lin?s (1998) information-theoretic similarity
measure is commonly used in lexicon acquisition
tasks and has demonstrated good performance in
unsupervised WSD (McCarthy et al, 2004). It op-
erates over dependency relations. A word w is de-
scribed by a set T (w) of co-occurrence triplets
< w,r,w
?
>, which can be viewed as a sparsely
represented feature vector, where r represents
the type of relation (e.g., object-of , subject-of ,
modified-by) between w and its dependent w
?
. The
similarity between w
1
and w
2
is then defined as:
?
(r,w)?T (w
1
)?T (w
2
)
I(w
1
,r,w)+ I(w
2
,r,w)
?
(r,w)?T (w
1
)
I(w
1
,r,w)+
?
(r,w)?T (w
2
)
I(w
2
,r,w)
where I(w,r,w
?
) is the information value of w with
regard to (r,w
?
), defined as:
I(w,r,w
?
) = log
count(w,r,w
?
) ? count(r)
count(?,r,w
?
) ? count(w,r,?)
The measure is used to estimate the pairwise simi-
larity between the target word and all other words
in the corpus (with the same part of speech); the
k words most similar to the target are selected as
its neighbors.
A potential caveat with Lin?s (1998) distribu-
tional similarity measure is its reliance on syn-
tactic information for obtaining dependency rela-
tions. Parsing resources may not be available for
all languages or domains. An alternative is to use a
measure of distributional similarity which consid-
ers word collocation statistics and therefore does
not require a syntactic parser (see Weeds (2003)).
As mentioned earlier, it is also possible to ob-
tain neighbors simply by consulting a semantic
dictionary. In WordNet, for example, we can as-
sume that WordNet relations, (e.g., hypernymy,
hyponymy, synonymy) indicate words which are
semantic neighbors. An advantage of using dis-
tributional neighbors is that they reflect the char-
acteristics of the corpus we wish to disambiguate
and are potentially better suited for capturing sense
differences across genres and domains, whereas
dictionary-based neighbors are corpus-invariant.
Associating Neighbors with Senses If the
neighbors are extracted from WordNet, it is not
necessary to associate them with their senses as
they are already assigned a specific sense. Distri-
butional similarity methods, however, do not pro-
vide a way to distinguish which neighbors per-
tain to each sense of the target. For that purpose,
we adapt a method proposed by McCarthy et al
(2004). Specifically, for each acquired neighbor,
we choose the sense of the target which gives
the highest semantic similarity score to any sense
of the neighbor. There are a large number of se-
mantic similarity measures to choose from (see
Budanitsky and Hirst (2001) for an overview).
We use Lesk?s measure as modified by Banerjee
and Pedersen (2003) for two reasons. First, it has
67
been shown to perform well in the related task
of predominant sense detection (McCarthy et al,
2004). Second, it has the advantage of relying only
upon the sense definitions, rather than the complex
graph structure which is unique to WordNet. This
makes the method more suitable for use with other
sense inventories.
Note that unlike McCarthy et al (2004), we
are associating neighbors with senses, rather than
merely trying to detect the predominant sense, and
therefore we require more precision in our selec-
tion. When it is unclear which sense of the target
word is most similar to a given neighbor (when the
scores of two or more senses are close together),
that neighbor is discarded.
As an example, consider the word sense, which
has four meanings
1
in WordNet: (1) a general con-
scious awareness (e.g., a sense of security), (2) the
meaning of a word (e.g., the dictionary gave sev-
eral senses for the word), (3) sound practical judg-
ment (e.g., I can?t see the sense in doing it now),
and (4) a natural appreciation or ability (e.g., keen
musical sense). On the British National Corpus
(BNC), using Lin?s (1998) similarity method, we
retrieve the following neighbors for the first and
second sense, respectively:
1. awareness, feeling, instinct, enthusiasm, sen-
sation, vision, tradition, consciousness, anger,
panic, loyalty
2. emotion, belief, meaning, manner, necessity,
tension, motivation
No neighbors are associated with the last two
senses, indicating that they are not prevalent
enough in the BNC to be detected by this method.
Once sense-specific neighbors are acquired, the
next stage is to replace all instances of the neigh-
bors in the corpus with the target ambiguous word
labeled with the appropriate sense. For example,
when encountering the sentence ?... attempt to
state the meaning of a word?, our method would
automatically transform this to ?... attempt to state
the sense (s#2) of a word.? These pseudo-labeled
instances comprise the training instances we pro-
vide to our machine learning algorithms.
4 Experimental Setup
We evaluated the performance of our approach on
benchmark datasets. In this section we give details
1
We are using the coarse-grained representation according
to Senseval 2 annotators. The sense definitions are simplified
for the sake of brevity.
regarding our training and test data, and describe
the features and machine learners we employed.
Finally, we discuss the methods to which we com-
pare our approach.
4.1 Data
Our experiments use a subset of the data provided
for the English lexical sample task in the Sen-
seval 2 (Preiss and Yarowsky, 2001) and Sense-
val 3 (Mihalcea and Edmonds, 2004) evaluation
exercises. Since our method does not require hand
tagged training data, we merged the provided train-
ing and test data into a single test set.
As a proof of concept we focus on the disam-
biguation of nouns, since they constitute the largest
portion of content words (50% in the BNC). In ad-
dition, WordNet, which is our semantic resource
and point of comparison, has a wide coverage
of nouns. Also, for many tasks and applications
(e.g., web queries) nouns are the most frequently
encountered part-of-speech (Jansen et al, 2000).
We made use of the coarse-grained sense group-
ings provided for both Senseval datasets. For many
applications (e.g., information retrieval) coarsely
defined senses are more useful (see Snow et al
(2007) for discussion).
Our training data was created from the BNC us-
ing different ways of obtaining the neighbors of the
target word. As described in Section 3 we retrieved
neighbors using Lin?s (1998) similarity measure
on a RASP parsed (Briscoe and Carroll, 2002) ver-
sion of the BNC. We used subject and object de-
pendencies, as well as adjective and noun modifier
dependencies. We also created training data sets
using collocational neighbors. Specifically, using
the InfoMap toolkit
2
, we constructed vector-based
representations for individual words from the BNC
using a term-document matrix and the cosine sim-
ilarity measure. Vectors were initially constructed
with 1,000 dimensions, the most frequent con-
tent words. The space was reduced to 100 dimen-
sions with singular value decomposition. Finally,
we also extracted neighbors from WordNet using
first-order and sibling relations (i.e., hyponyms of
the same hypernym). A problem often encountered
when using dictionary-based neighbors is that they
are themselves polysemous, and the related sense
is often not the most prominent one in the corpus,
which leads to noisy data. We therefore experi-
mented with using all neighbors for a given word
2
http://infomap.stanford.edu/
68
?The philosophical explanation of authority is not an
attempt to state the sense of a word.?
Contextual features
?10 words explanation, of, authority, be, ...
?5 words an, attempt, to, state, of, a, ...
Collocational features
-2/+0 n-gram state the X
-1/+1 n-gram the X of
-0/+2 n-gram X of a
-2/+0 POS n-gram Verb Det X
-1/+1 POS n-gram Det X Prep
-0/+2 POS n-gram X Prep Det
Syntactic features
Object of Verb obj of state
Table 1: Example sentence and extracted features
for the word sense; X denotes the target word.
or only those which are monosemous and hope-
fully less noisy. In all cases we used 50 neighbors,
the most similar nouns to the target.
4.2 Features
We used a rich feature space based on lemmas,
part-of-speech (POS) tags and a variety of posi-
tional and syntactic relationships of the target word
capturing both immediate local context and wider
context. These feature types have been widely used
in WSD algorithms (see Lee and Ng (2002) for an
evaluation of their effectiveness). Their use is illus-
trated on a sample English sentence for the target
word sense in Table 1.
4.3 Supervised Classifiers
One of our evaluation goals was to examine the
effect of our training-data creation procedure on
different types of classifiers and determine which
ones are most suited for use with our method. We
therefore chose three supervised classifiers (sup-
port vector machines, maximum entropy, and label
propagation) which are based on different learn-
ing paradigms and have shown competitive per-
formance in WSD (Niu et al, 2005; Preiss and
Yarowsky, 2001; Mihalcea and Edmonds, 2004).
We summarize below their main characteristics
and differences.
Support Vector Machines SVMs model clas-
sification as the problem of finding a separating
hyperplane in a high dimensional vector space.
They focus on differentiating between the most
problematic cases ? instances which are close to
each other in the high dimensional space, but have
different labels. They are discriminative, rather
than generative, and do not explicitly model the
classes. SVMs have been applied successfully in
many NLP tasks. We used the multi-class bound-
constrained support vector classification (SVC)
version of SVM described in Hsu and Lin (2001)
and implemented in the BSVM package
3
. All pa-
rameters were set to their default values with the
exception of the misclassification penalty, which
was set to a high value (1,000) to penalize labeling
all instances with the most frequent sense.
Maximum Entropy Model Maximum entropy-
based classifiers are a common alternative to other
probabilistic classifiers, such as Naive Bayes, and
have received much interest in various NLP tasks
ranging from part-of-speech tagging to parsing
and text classification. They represent a probabilis-
tic, global constrained approach. They assume a
uniform, zero-knowledge model, under the con-
straints of the training dataset. The classifier finds
the (unique) maximal entropy model which con-
forms to the expected feature distribution of the
training data. In our experiments, we usedMegam
4
a publicly available maximum entropy classifier
(Daum?e III, 2004) with the default parameters.
Label Propagation The basic Label Propaga-
tion algorithm (Zhu and Ghahramani, 2002) repre-
sents labeled and unlabeled instances as nodes in
an undirected graph with weighted edges. Initially
only the known data nodes are labeled. The goal
is to propagate labels from labeled to unlabeled
points along the weighted edges. The weights are
based on distance in a high-dimensional space. At
each iteration, only the original labels are fixed,
whereas the propagated labels are ?soft?, and may
change in subsequent iterations. This property al-
lows the final labeling to be affected by more dis-
tant labels, that have propagated further, and gives
the algorithm a global aspect. We used SemiL
5
, a
publicly available implementation of label propa-
gation (all parameters were set to default values).
4.4 Comparison with State-of-the-art
As an upper bound, we considered the accuracy
of our classifiers when trained on the manually-
labeled Senseval data (using the same experimen-
tal settings and 5-fold crossvalidation). This can be
used to estimate the expected decrease in accuracy
caused solely by the use of our automatic sense la-
beling method. We also compared our approach to
other unsupervised ones. These include McCarthy
3
http://www.csie.ntu.edu.tw/?cjlin/bsvm/
4
http://www.isi.edu/?hdaume/megam/index.html
5
http://www.engineers.auckland.ac.nz/?vkec001
69
et al?s (2004) method for inferring the predomi-
nant sense and Lesk?s (1986) algorithm. We modi-
fied the latter slightly so as to increase its coverage
and used McCarthy et al?s first sense heuristic to
disambiguate unknown instances where no overlap
was found. For McCarthy et al we used parame-
ters they report as optimal.
5 Results
The evaluation of our method was motivated by
three questions: (1) How do different choices in
constructing the pseudo-labeled training data af-
fect WSD performance? Here, we would like to
assess whether the origin of the target word neigh-
bors (e.g., from a corpus or dictionary) matters.
(2) What is the degree of noise and subsequent
loss in accuracy incurred by our method? (3) How
does the proposed approach compare against other
unsupervised methods? In particular, we are in-
terested to find out whether we outperform Mc-
Carthy et al?s (2004) related method for predomi-
nant sense detection.
5.1 The Choice of Neighbors
Our results are summarized in Table 2. We re-
port accuracy (rather than F-score) since all al-
gorithms labeled all instances. The three center
columns present our results with the automatically
constructed training sets.
The best accuracies are observed when the la-
bels are created from distributionally similar words
using Lin?s (1998) dependency-based similarity
measure (Depend). We observe a small decrease in
performance (within the range of 2%?4%) when
using collocational neighbors without any syntac-
tic information.
6
Using the neighbors provided by
WordNet leads to worse results than using dis-
tributional neighbors. The differences in perfor-
mance are significant
7
(p < 0.01) on both Sense-
val datasets for all classifiers and for bothWordNet
configurations, i.e., using all neighbors (AllWN)
vs. monosemous ones (MonoWN).
This result may seem counterintuitive since
neighbors provided by a semantic resource are
based on expert knowledge and are often more ac-
curate than those obtained automatically. However,
semantic resources like WordNet are designed to
be as general as possible without a specific cor-
pus or domain in mind. They will therefore pro-
vide related words for all senses, even rare ones,
6
We omit these results from the table for brevity.
7
Throughout, we report significance using a ?
2
test.
which may not appear in our chosen corpus. Distri-
butional methods, on the other hand, are anchored
in the corpus. The extracted neighbors are usu-
ally relevant and representative of the corpus. An-
other drawback of resource-based neighbors is that
they often do not share local behavior, i.e., they
do not appear in the same immediate local con-
text and do not share the same syntax. For this rea-
son, the useful information that can be extracted
from their contexts tends to be topical (e.g., words
that are indicative of the domain), rather than lo-
cal (e.g., grammatical dependencies). Topical in-
formation is mostly useful when the difference be-
tween senses can be attributed to a specific domain.
However, for many words and senses, this is not
the case (Leacock et al, 1998).
5.2 Comparison against Manual Labels
The rightmost column of Table 2 shows the accu-
racy of our classifiers when these are trained on
the manually annotated Senseval datasets. In gen-
eral, all algorithms exhibit a similar level of per-
formance when trained on hand-coded data, with
slightly lower scores for Senseval 3. On Sense-
val 2, the SVM is significantly better than the other
two classifiers (p < 0.01). On Senseval 3, label
propagation is significantly worse than the others
(p < 0.01). The results shown here do not repre-
sent the highest achievable performance in a su-
pervised setting, but rather those obtained with-
out extensive parameter tuning. The best perform-
ing systems on coarse-grained nouns in Sense-
val 2 and 3 (Preiss and Yarowsky, 2001; Mihalcea
and Edmonds, 2004) achieved approximately 76%
and 80%, respectively. Besides being more finely
tuned, these systems employed more sophisticated
learning paradigms (e.g., ensemble learning).
When we compare the results from the manu-
ally labeled data to those achieved with the dis-
tributional neighbors, we can see that use of our
pseudo-labels results in accuracies that are ap-
proximately 8?10% lower. Since the results were
achieved using the same feature set and classi-
fier settings, the comparison provides an estimate
of the expected decrease in accuracy due only to
our unsupervised tagging method. With more de-
tailed feature engineering and more sophisticated
machine learning methods, we could probably im-
prove our classifiers? performance on the automat-
ically labeled dataset. Also note that improvements
in supervised methods can be expected to automat-
ically translate to improvements in unsupervised
70
Senseval 2 AllWN MonoWN Depend Manual
SVM 48.12 53.29 64.38 72.52
MaxEnt 40.93 52.11 62.32 71.91
LP 42.67 49.54 63.32 69.28
McCarthy 59.98
Lesk 48.12
Senseval 3 AllWN MonoWN Depend Manual
SVM 53.16 46.32 57.47 71.22
MaxEnt 49.67 44.85 57.35 71.75
LP 47.41 43.60 60.60 67.57
McCarthy 57.14
Lesk 48.66
Table 2: Accuracy (%) on Senseval 2 and 3 lexical
samples. Support vector machines (SVM), maxi-
mum entropy (MaxEnt) and label propagation (LP)
are trained on automatically and manually labeled
data sets
WSD using our method.
Interestingly, label propagation performed rela-
tively poorly on the manually labeled data. How-
ever, it ranks highly when using the automatic la-
bels. This may be due to the fact that LP is the
only algorithm that does not separate the train-
ing and test set (it is principally a semi-supervised
method), allowing the properties of both to influ-
ence the structure of the resulting graph. Since the
instances in the training data are not actual occur-
rences of the target word, it is important to learn
which instances in the training set are closest to a
given instance in the test set. The two other algo-
rithms only attempt to distinguish between classes
in the training set.
5.3 Other Unsupervised Methods
As shown in Table 2 our classifiers are signifi-
cantly better than Lesk on both Senseval datasets
(p < 0.01). They also significantly outperform the
automatically acquired predominant sense (Mc-
Carthy) on Senseval 2 (for the Maximum Entropy
classifier, the difference is significant at p < 0.05).
On Senseval 3, all classifiers quantitatively outper-
form the first sense heuristic, but the difference is
statistically significant only for label propagation
(p < 0.01). The differences in performance on the
two datasets can be explained by analyzing their
sense distributions. Senseval 3 has a higher level
of ambiguity (4.35 senses per word, on average,
compared to 3.28 for Senseval 2), and is there-
fore a more difficult dataset. Although Senseval 3
has a slightly lower percentage of first sense in-
stances, the higher ambiguity means that the skew
is, in fact, much higher than in Senseval 2. A high
Senseval 2 Depend Manual
SVM 14.3 (60.1) 16.9 (60.4)
MaxEnt 6.3 (66.9) 17.1 (56.7)
LP 8.9 (63.3) 14.8 (49.4)
Senseval 3 Depend Manual
SVM 17.6 (45.0) 23.3 (60.0)
MaxEnt 8.5 (55.0) 23.7 (60.9)
LP 5.6 (60.9) 17.8 (53.5)
Table 3: Percentage of non-first instances in auto-
matically and manually labeled training data; num-
bers in parentheses show the classifiers? accuracy
on these instances.
skew towards the predominant sense means there
are less instances from which we can learn about
the rarer senses, and that we run a higher risk when
labeling an instance as one of the rarer senses (in-
stead of defaulting to the predominant one).
Our method shares some of the principles of
McCarthy et al?s (2004) unsupervised algorithm.
However, instead of focusing on detecting a sin-
gle predominant sense throughout the corpus, we
build a dataset that will allow us to learn about and
identify all existing (prevalent) senses. Despite the
fact that the first-sense heuristic is a strong base-
line, and fall-back option in case of limited local
information, it is not a true context-specific WSD
algorithm. Any approach that ignores local con-
text, and labels all instances with a single sense
has limited effectiveness when WSD is needed
in an application. Context-indifferent methods run
the risk of completely mistaking the predominant
sense, thereby mis-labeling most of the instances,
whereas approaches that consider local context are
less prone to such large-scope errors.
We further analyzed the performance of our
method by examining instances labeled with
senses other than the most frequent one. Table 3
shows the percentage of such instances depend-
ing on the machine learner and type of training
data (automatic versus manual) being employed.
It also presents the classifiers? accuracy (figures
in parentheses) with regard to only the non-first
senses. When trained on the automatically labeled
data, our classifiers tend to be more conservative
in assigning non-first senses. Interestingly, we ob-
tain similar accuracies with the classifiers trained
on the manually labeled data, even though the lat-
ter assign more non-first senses. It is worth noting
that the SVM labels two to three times as many
instances with non-first-sense labels, yet achieves
similar levels of overall accuracy to the other clas-
71
sifiers (compare Tables 2 and 3) and only slightly
lower accuracy on the non-first senses. This would
make it a better choice when it is important to have
more data on rarer senses.
6 Conclusions and Future Work
We have presented an unsupervised approach to
WSD which retains many of the advantages of su-
pervised methods, while being free of the costly
requirement for human annotation. We demon-
strate that classifiers trained using our method can
out-perform state-of-the-art unsupervised meth-
ods, and approach the accuracy of fully-supervised
methods trained on manually-labeled data.
In the future we plan to scale our system to
the all-words task. There is nothing inherent in
our method that restricts us to the lexical sample,
which we chose primarily to assess the feasibil-
ity of our ideas. Another interesting direction con-
cerns the use of our method in a semi-supervised
setting. For example, we could automatically ac-
quire labeled instances for words whose senses are
rare in a manually tagged dataset. Finally, we could
potentially improve accuracy, at the expense of
coverage, by estimating confidence scores on the
classifiers? predictions, and assigning labels only
to instances with high confidence.
Acknowledgments
The authors acknowledge the support of EPSRC
(grant EP/C538447/1) and would like to thank
David Talbot for his insightful suggestions.
References
S. Banerjee, T. Pedersen. 2003. Extended gloss overlaps as
a measure of semantic relatedness. In Proc. of the 18th
IJCAI, 805?810, Acapulco.
R. Barzilay, M. Elhadad. 1997. Using lexical chains for text
summarization. In Proc. of the Intelligent Scalable Text
Summarization Workshop, Madrid, Spain.
T. Briscoe, J. Carroll. 2002. Robust accurate statistical an-
notation of general text. In Proc. of the 3rd LREC, 1499?
1504, Las Palmas, Gran Canaria.
A. Budanitsky, G. Hirst. 2001. Semantic distance in Word-
Net: An experimental, application-oriented evaluation of
five measures. In Proc. of the ACL Worskhop on WordNet
and Other Lexical Resources, Pittsburgh, PA.
Y. S. Chan, H. T. Ng. 2007. Word sense disambiguation
improves statistical machine translation. In Proc. of the
45th ACL, 33?40, Prague, Czech Republic.
H. Daum?e III. 2004. Notes on CG and LM-BFGS optimiza-
tion of logistic regression.
P. Edmonds. 2000. Designing a task for SENSEVAL-2, 2000.
Technical note.
C. Fellbaum, ed. 1998. WordNet: An Electronic Database.
MIT Press, Cambridge, MA.
W. Gale, K. Church, D. Yarowsky. 1992. A method for dis-
ambiguating word senses in a large corpus. Computers
and the Humanities, 26(2):415?439.
M. Galley, K. McKeown. 2003. Improving word sense dis-
ambiguation in lexical chaining. In Proc. of the 18th IJ-
CAI, 1486?1488, Acapulco.
C. Hsu, C. Lin. 2001. A comparison of methods for multi-
class support vector machines, 2001. Technical report, De-
partment of Computer Science and Information Engineer-
ing, National Taiwan University, Taipei, Taiwan.
B. J. Jansen, A. Spink, A. Pfaff. 2000. Linguistic aspects
of web queries, 2000. American Society of Information
Science, Chicago.
C. Leacock, M. Chodorow, G. A. Miller. 1998. Using cor-
pus statistics and wordnet relations for sense identification.
Computational Linguistics, 24(1):147?165.
Y. K. Lee, H. T. Ng. 2002. An empirical evaluation of knowl-
edge sources and learning algorithms for word sense dis-
ambiguation. In Proc. of the EMNLP, 41?48, NJ.
M. Lesk. 1986. Automatic sense disambiguation using ma-
chine readable dictionaries: How to tell a pine cone from
an ice cream cone. In Proc. of the 5th SIGDOC, 24?26,
New York, NY.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proc. of the ACL/COLING, 768?774, Montreal.
D. McCarthy, R. Koeling, J. Weeds, J. Carroll. 2004. Finding
predominant senses in untagged text. In Proc. of the 42th
ACL, 280?287, Barcelona, Spain.
R. F. Mihalcea, P. Edmonds, eds. 2004. Proc. of the
SENSEVAL-3, Barcelona, 2004.
R. F. Mihalcea. 2002. Word sense disambiguation with
pattern learning and automatic feature selection. Natural
Language Engineering, 8(4):343?358.
H. T. Ng, B. Wang, Y. S. Chan. 2003. Exploiting parallel
texts for word sense disambiguation: an empirical study.
In Proc. of the 41st ACL, 455?462, Sapporo, Japan.
H. T. Ng. 1997. Getting serious about word sense disam-
biguation. In Proc. of the ACL SIGLEX Workshop on Tag-
ging Text with Lexical Semantics: Why, What, and How?,
1?7, Washington, DC.
Z. Y. Niu, D. H. Ji, C. L. Tan. 2005. Word sense disam-
biguation using label propagation based semi-supervised
learning. In Proc. of the 43rd ACL, 395?402, Ann Arbor.
J. Preiss, D. Yarowsky, eds. 2001. Proc. of the 2nd Interna-
tional Workshop on Evaluating Word Sense Disambigua-
tion Systems, Toulouse, France, 2001.
G. Ramakrishnan, A. Jadhav, A. Joshi, S. Chakrabarti,
P. Bhattacharyya. 2003. Question answering via Bayesian
inference on lexical relations. In Proc. of the ACL 2003
workshop on Multilingual summarization and QA, 1?10.
R. Snow, S. Prakash, D. Jurafsky, A. Y. Ng. 2007. Learning
to merge word senses. In Proc. of the EMNLP/CoNLL,
1005?1014, Prague, Czech Republic.
J. Weeds. 2003. Measures and Applications of Lexical Dis-
tributional Similarity. Ph.D. thesis, University of Sussex.
D. Yarowsky. 1992. Word-sense disambiguation using statis-
tical models of Roget?s categories trained on large corpora.
In Proc. of the 14th COLING, 454?460, Nantes, France.
X. Zhu, Z. Ghahramani. 2002. Learning from labeled and
unlabeled data with label propagation. Technical report,
CMU-CALD-02, 2002.
72
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 97?104,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Ensemble Methods for Unsupervised WSD
Samuel Brody
School of Informatics
University of Edinburgh
s.brody@sms.ed.ac.uk
Roberto Navigli
Dipartimento di Informatica
Universita di Roma ?La Sapienza?
navigli@di.uniroma1.it
Mirella Lapata
School of Informatics
University of Edinburgh
mlap@inf.ed.ac.uk
Abstract
Combination methods are an effective way
of improving system performance. This
paper examines the benefits of system
combination for unsupervised WSD. We
investigate several voting- and arbiter-
based combination strategies over a di-
verse pool of unsupervised WSD systems.
Our combination methods rely on predom-
inant senses which are derived automati-
cally from raw text. Experiments using the
SemCor and Senseval-3 data sets demon-
strate that our ensembles yield signifi-
cantly better results when compared with
state-of-the-art.
1 Introduction
Word sense disambiguation (WSD), the task of
identifying the intended meanings (senses) of
words in context, holds promise for many NLP
applications requiring broad-coverage language
understanding. Examples include summarization,
question answering, and text simplification. Re-
cent studies have also shown that WSD can ben-
efit machine translation (Vickrey et al, 2005) and
information retrieval (Stokoe, 2005).
Given the potential of WSD for many NLP
tasks, much work has focused on the computa-
tional treatment of sense ambiguity, primarily us-
ing data-driven methods. Most accurate WSD sys-
tems to date are supervised and rely on the avail-
ability of training data, i.e., corpus occurrences of
ambiguous words marked up with labels indicat-
ing the appropriate sense given the context (see
Mihalcea and Edmonds 2004 and the references
therein). A classifier automatically learns disam-
biguation cues from these hand-labeled examples.
Although supervised methods typically achieve
better performance than unsupervised alternatives,
their applicability is limited to those words for
which sense labeled data exists, and their accu-
racy is strongly correlated with the amount of la-
beled data available (Yarowsky and Florian, 2002).
Furthermore, obtaining manually labeled corpora
with word senses is costly and the task must be
repeated for new domains, languages, or sense in-
ventories. Ng (1997) estimates that a high accu-
racy domain independent system for WSD would
probably need a corpus of about 3.2 million sense
tagged words. At a throughput of one word per
minute (Edmonds, 2000), this would require about
27 person-years of human annotation effort.
This paper focuses on unsupervised methods
which we argue are useful for broad coverage
sense disambiguation. Unsupervised WSD algo-
rithms fall into two general classes: those that per-
form token-based WSD by exploiting the simi-
larity or relatedness between an ambiguous word
and its context (e.g., Lesk 1986); and those that
perform type-based WSD, simply by assigning
all instances of an ambiguous word its most fre-
quent (i.e., predominant) sense (e.g., McCarthy
et al 2004; Galley and McKeown 2003). The pre-
dominant senses are automatically acquired from
raw text without recourse to manually annotated
data. The motivation for assigning all instances
of a word to its most prevalent sense stems from
the observation that current supervised approaches
rarely outperform the simple heuristic of choos-
ing the most common sense in the training data,
despite taking local context into account (Hoste
et al, 2002). Furthermore, the approach allows
sense inventories to be tailored to specific do-
mains.
The work presented here evaluates and com-
pares the performance of well-established unsu-
pervised WSD algorithms. We show that these
algorithms yield sufficiently diverse outputs, thus
motivating the use of combination methods for im-
proving WSD performance. While combination
approaches have been studied previously for su-
pervised WSD (Florian et al, 2002), their use
in an unsupervised setting is, to our knowledge,
novel. We examine several existing and novel
combination methods and demonstrate that our
combined systems consistently outperform the
97
state-of-the-art (e.g., McCarthy et al 2004). Im-
portantly, our WSD algorithms and combination
methods do not make use of training material in
any way, nor do they use the first sense informa-
tion available in WordNet.
In the following section, we briefly describe the
unsupervised WSD algorithms considered in this
paper. Then, we present a detailed comparison of
their performance on SemCor (Miller et al, 1993).
Next, we introduce our system combination meth-
ods and report on our evaluation experiments. We
conclude the paper by discussing our results.
2 The Disambiguation Algorithms
In this section we briefly describe the unsuper-
vised WSD algorithms used in our experiments.
We selected methods that vary along the follow-
ing dimensions: (a) the type of WSD performed
(i.e., token-based vs. type-based), (b) the represen-
tation and size of the context surrounding an am-
biguous word (i.e., graph-based vs. word-based,
document vs. sentence), and (c) the number and
type of semantic relations considered for disam-
biguation. We base most of our discussion below
on the WordNet sense inventory; however, the ap-
proaches are not limited to this particular lexicon
but could be adapted for other resources with tra-
ditional dictionary-like sense definitions and alter-
native structure.
Extended Gloss Overlap Gloss Overlap was
originally introduced by Lesk (1986) for perform-
ing token-based WSD. The method assigns a sense
to a target word by comparing the dictionary defi-
nitions of each of its senses with those of the words
in the surrounding context. The sense whose defi-
nition has the highest overlap (i.e., words in com-
mon) with the context words is assumed to be the
correct one. Banerjee and Pedersen (2003) aug-
ment the dictionary definition (gloss) of each sense
with the glosses of related words and senses. The
extended glosses increase the information avail-
able in estimating the overlap between ambiguous
words and their surrounding context.
The range of relationships used to extend the
glosses is a parameter, and can be chosen from
any combination of WordNet relations. For every
sense sk of the target word we estimate:
SenseScore(sk) = ?
Rel?Relations
Overlap(context,Rel(sk))
where context is a simple (space separated) con-
catenation of all words wi for ?n ? i ? n, i 6= 0 ina context window of length ?n around the target
word w0. The overlap scoring mechanism is also
parametrized and can be adjusted to take the into
account gloss length or to ignore function words.
Distributional and WordNet Similarity
McCarthy et al (2004) propose a method for
automatically ranking the senses of ambiguous
words from raw text. Key in their approach is the
observation that distributionally similar neighbors
often provide cues about a word?s senses. As-
suming that a set of neighbors is available, sense
ranking is equivalent to quantifying the degree
of similarity among the neighbors and the sense
descriptions of the polysemous word.
Let N(w) = {n1,n2, . . . ,nk} be the k most (dis-tributionally) similar words to an ambiguous tar-
get word w and senses(w) = {s1,s2, . . .sn} the setof senses for w. For each sense si and for eachneighbor n j, the algorithm selects the neighbor?ssense which has the highest WordNet similarity
score (wnss) with regard to si. The ranking scoreof sense si is then increased as a function of theWordNet similarity score and the distributional
similarity score (dss) between the target word and
the neighbor:
RankScore(si) = ?
n j?Nw
dss(w,n j)
wnss(si,n j)
?
s?i?senses(w)
wnss(s?i,n j)
where wnss(si,n j) = max
nsx?senses(n j)
wnss(si,nsx).
The predominant sense is simply the sense with
the highest ranking score (RankScore) and can be
consequently used to perform type-based disam-
biguation. The method presented above has four
parameters: (a) the semantic space model repre-
senting the distributional properties of the target
words (it is acquired from a large corpus repre-
sentative of the domain at hand and can be aug-
mented with syntactic relations such as subject or
object), (b) the measure of distributional similarity
for discovering neighbors (c) the number of neigh-
bors that the ranking score takes into account, and
(d) the measure of sense similarity.
Lexical Chains Lexical cohesion is often rep-
resented via lexical chains, i.e., sequences of re-
lated words spanning a topical text unit (Mor-
ris and Hirst, 1991). Algorithms for computing
lexical chains often perform WSD before infer-
ring which words are semantically related. Here
we describe one such disambiguation algorithm,
proposed by Galley and McKeown (2003), while
omitting the details of creating the lexical chains
themselves.
Galley and McKeown?s (2003) method consists
of two stages. First, a graph is built represent-
ing all possible interpretations of the target words
98
in question. The text is processed sequentially,
comparing each word against all words previously
read. If a relation exists between the senses of the
current word and any possible sense of a previous
word, a connection is formed between the appro-
priate words and senses. The strength of the con-
nection is a function of the type of relationship and
of the distance between the words in the text (in
terms of words, sentences and paragraphs). Words
are represented as nodes in the graph and seman-
tic relations as weighted edges. Again, the set of
relations being considered is a parameter that can
be tuned experimentally.
In the disambiguation stage, all occurrences of a
given word are collected together. For each sense
of a target word, the strength of all connections
involving that sense are summed, giving that sense
a unified score. The sense with the highest unified
score is chosen as the correct sense for the target
word. In subsequent stages the actual connections
comprising the winning unified score are used as a
basis for computing the lexical chains.
The algorithm is based on the ?one sense per
discourse? hypothesis and uses information from
every occurrence of the ambiguous target word in
order to decide its appropriate sense. It is there-
fore a type-based algorithm, since it tries to de-
termine the sense of the word in the entire doc-
ument/discourse at once, and not separately for
each instance.
Structural Semantic Interconnections In-
spired by lexical chains, Navigli and Velardi
(2005) developed Structural Semantic Intercon-
nections (SSI), a WSD algorithm which makes use
of an extensive lexical knowledge base. The latter
is primarily based on WordNet and its standard re-
lation set (i.e., hypernymy, meronymy, antonymy,
similarity, nominalization, pertainymy) but is also
enriched with collocation information represent-
ing semantic relatedness between sense pairs. Col-
locations are gathered from existing resources
(such as the Oxford Collocations, the Longman
Language Activator, and collocation web sites).
Each collocation is mapped to the WordNet sense
inventory in a semi-automatic manner (Navigli,
2005) and transformed into a relatedness edge.
Given a local word context C = {w1, ...,wn},SSI builds a graph G = (V,E) such that V =
n
S
i=1
senses(wi) and (s,s?) ? E if there is at least
one interconnection j between s (a sense of the
word) and s? (a sense of its context) in the lexical
knowledge base. The set of valid interconnections
is determined by a manually-created context-free
Method WSD Context Relations
LexChains types document first-order
Overlap tokens sentence first-order
Similarity types corpus higher-order
SSI tokens sentence higher-order
Table 1: Properties of the WSD algorithms
grammar consisting of a small number of rules.
Valid interconnections are computed in advance
on the lexical database, not at runtime.
Disambiguation is performed in an iterative
fashion. At each step, for each sense s of a word
in C (the set of senses of words yet to be disam-
biguated), SSI determines the degree of connectiv-
ity between s and the other senses in C :
SSIScore(s) =
?
s??C\{s} ?j?Interconn(s,s?)
1
length( j)
?
s??C\{s}
|Interconn(s,s?)|
where Interconn(s,s?) is the set of interconnec-
tions between senses s and s?. The contribution of a
single interconnection is given by the reciprocal of
its length, calculated as the number of edges con-
necting its ends. The overall degree of connectiv-
ity is then normalized by the number of contribut-
ing interconnections. The highest ranking sense s
of word wi is chosen and the senses of wi are re-moved from the context C . The procedure termi-
nates when either C is the empty set or there is no
sense such that its SSIScore exceeds a fixed thresh-
old.
Summary The properties of the different
WSD algorithms just described are summarized
in Table 1. The methods vary in the amount of
data they employ for disambiguation. SSI and Ex-
tended Gloss Overlap (Overlap) rely on sentence-
level information for disambiguation whereas Mc-
Carthy et al (2004) (Similarity) and Galley and
McKeown (2003) (LexChains) utilize the entire
document or corpus. This enables the accumula-
tion of large amounts of data regarding the am-
biguous word, but does not allow separate consid-
eration of each individual occurrence of that word.
LexChains and Overlap take into account a re-
stricted set of semantic relations (paths of length
one) between any two words in the whole docu-
ment, whereas SSI and Similarity use a wider set
of relations.
99
3 Experiment 1: Comparison of
Unsupervised Algorithms for WSD
3.1 Method
We evaluated the disambiguation algorithms out-
lined above on two tasks: predominant sense ac-
quisition and token-based WSD. As previously
explained, Overlap and SSI were not designed for
acquiring predominant senses (see Table 1), but
a token-based WSD algorithm can be trivially
modified to acquire predominant senses by dis-
ambiguating every occurrence of the target word
in context and selecting the sense which was cho-
sen most frequently. Type-based WSD algorithms
simply tag all occurrences of a target word with its
predominant sense, disregarding the surrounding
context.
Our first set of experiments was conducted on
the SemCor corpus, on the same 2,595 polyse-
mous nouns (53,674 tokens) used as a test set by
McCarthy et al (2004). These nouns were attested
in SemCor with a frequency > 2 and occurred in
the British National Corpus (BNC) more than 10
times. We used the WordNet 1.7.1 sense inventory.
The following notation describes our evaluation
measures: W is the set of all noun types in the
SemCor corpus (|W | = 2,595), and W f is the setof noun types with a dominant sense. senses(w)
is the set of senses for noun type w, while fs(w)and fm(w) refer to w?s first sense according to theSemCor gold standard and our algorithms, respec-
tively. Finally, T (w) is the set of tokens of w and
senses(t) denotes the sense assigned to token t ac-cording to SemCor.
We first measure how well our algorithms can
identify the predominant sense, if one exists:
Accps =
|{w ?W f | fs(w) = fm(w)}|
|Wf |
A baseline for this task can be easily defined for
each word type by selecting a sense at random
from its sense inventory and assuming that this is
the predominant sense:
Baselinesr =
1
|Wf | ?w ?W f
1
|senses(w)|
We evaluate the algorithms? disambiguation per-
formance by measuring the ratio of tokens for
which our models choose the right sense:
Accwsd =
?
w?W
|{t ? T (w)| fm(w) = senses(t)}|
?
w?W
|T (w)|
In the predominant sense detection task, in case of
ties in SemCor, any one of the predominant senses
was considered correct. Also, all algorithms were
designed to randomly choose from among the top
scoring options in case of a tie in the calculated
scores. This introduces a small amount of ran-
domness (less than 0.5%) in the accuracy calcu-
lation, and was done to avoid the pitfall of default-
ing to the first sense listed in WordNet, which is
usually the actual predominant sense (the order of
senses in WordNet is based primarily on the Sem-
Cor sense distribution).
3.2 Parameter Settings
We did not specifically tune the parameters of our
WSD algorithms on the SemCor corpus, as our
goal was to use hand labeled data solely for testing
purposes. We selected parameters that have been
considered ?optimal? in the literature, although
admittedly some performance gains could be ex-
pected had parameter optimization taken place.
For Overlap, we used the semantic relations
proposed by Banerjee and Pedersen (2003),
namely hypernyms, hyponyms, meronyms,
holonyms, and troponym synsets. We also
adopted their overlap scoring mechanism which
treats each gloss as a bag of words and assigns an
n word overlap the score of n2. Function words
were not considered in the overlap computation.
For LexChains, we used the relations reported
in Galley and McKeown (2003). These are all
first-order WordNet relations, with the addition of
the siblings ? two words are considered siblings
if they are both hyponyms of the same hypernym.
The relations have different weights, depending
on their type and the distance between the words
in the text. These weights were imported from
Galley and McKeown into our implementation
without modification.
Because the SemCor corpus is relatively small
(less than 700,00 words), it is not ideal for con-
structing a neighbor thesaurus appropriate for Mc-
Carthy et al?s (2004) method. The latter requires
each word to participate in a large number of co-
occurring contexts in order to obtain reliable dis-
tributional information. To overcome this prob-
lem, we followed McCarthy et al and extracted
the neighbor thesaurus from the entire BNC. We
also recreated their semantic space, using a RASP-
parsed (Briscoe and Carroll, 2002) version of the
BNC and their set of dependencies (i.e., Verb-
Object, Verb-Subject, Noun-Noun and Adjective-
Noun relations). Similarly to McCarthy et al, we
used Lin?s (1998) measure of distributional simi-
larity, and considered only the 50 highest ranked
100
Method Accps Accwsd/dir Accwsd/ps
Baseline 34.5 ? 23.0
LexChains 48.3??$ ? 40.7?#?$
Overlap 49.4??$ 36.5$ 42.5??$
Similarity 54.9? ? 46.5?$
SSI 53.7? 42.7 47.9?
UpperBnd 100 ? 68.4
Table 2: Results of individual disambiguation al-
gorithms on SemCor nouns2 (?: sig. diff. from
Baseline, ?: sig. diff. from Similarity, $: sig diff.
from SSI, #: sig. diff. from Overlap, p < 0.01)
neighbors for a given target word. Sense similar-
ity was computed using the Lesk?s (Banerjee and
Pedersen, 2003) similarity measure1.
3.3 Results
The performance of the individual algorithms is
shown in Table 2. We also include the baseline
discussed in Section 3 and the upper bound of
defaulting to the first (i.e., most frequent) sense
provided by the manually annotated SemCor. We
report predominant sense accuracy (Accps), andWSD accuracy when using the automatically ac-
quired predominant sense (Accwsd/ps). For token-based algorithms, we also report their WSD per-
formance in context, i.e., without use of the pre-
dominant sense (Accwsd/dir).As expected, the accuracy scores in the WSD
task are lower than the respective scores in the
predominant sense task, since detecting the pre-
dominant sense correctly only insures the correct
tagging of the instances of the word with that
first sense. All methods perform significantly bet-
ter than the baseline in the predominant sense de-
tection task (using a ?2-test, as indicated in Ta-
ble 2). LexChains and Overlap perform signif-
icantly worse than Similarity and SSI, whereas
LexChains is not significantly different from Over-
lap. Likewise, the difference in performance be-
tween SSI and Similarity is not significant. With
respect to WSD, all the differences in performance
are statistically significant.
1This measure is identical to the Extended gloss Overlapfrom Section 2, but instead of searching for overlap betweenan extended gloss and a word?s context, the comparison isdone between two extended glosses of two synsets.2The LexChains results presented here are not directlycomparable to those reported by Galley and McKeown(2003), since they tested on a subset of SemCor, and includedmonosemous nouns. They also used the first sense in Sem-Cor in case of ties. The results for the Similarity method areslightly better than those reported by McCarthy et al (2004)due to minor improvements in implementation.
Overlap LexChains Similarity
LexChains 28.05
Similarity 35.87 33.10
SSI 30.48 31.67 37.14
Table 3: Algorithms? pairwise agreement in de-
tecting the predominant sense (as % of all words)
Interestingly, using the predominant sense de-
tected by the Gloss Overlap and the SSI algo-
rithm to tag all instances is preferable to tagging
each instance individually (compare Accwsd/dirand Accwsd/ps for Overlap and SSI in Table 2).This means that a large part of the instances which
were not tagged individually with the predominant
sense were actually that sense.
A close examination of the performance of the
individual methods in the predominant-sense de-
tection task shows that while the accuracy of all
the methods is within a range of 7%, the actual
words for which each algorithm gives the cor-
rect predominant sense are very different. Table 3
shows the degree of overlap in assigning the ap-
propriate predominant sense among the four meth-
ods. As can be seen, the largest amount of over-
lap is between Similarity and SSI, and this cor-
responds approximately to 23 of the words theycorrectly label. This means that each of these two
methods gets more than 350 words right which the
other labels incorrectly.
If we had an ?oracle? which would tell us
which method to choose for each word, we would
achieve approximately 82.4% in the predominant
sense task, giving us 58% in the WSD task. We
see that there is a large amount of complementa-
tion between the algorithms, where the successes
of one make up for the failures of the others. This
suggests that the errors of the individual methods
are sufficiently uncorrelated, and that some advan-
tage can be gained by combining their predictions.
4 Combination Methods
An important finding in machine learning is that
a set of classifiers whose individual decisions are
combined in some way (an ensemble) can be more
accurate than any of its component classifiers, pro-
vided that the individual components are relatively
accurate and diverse (Dietterich, 1997). This sim-
ple idea has been applied to a variety of classi-
fication problems ranging from optical character
recognition to medical diagnosis, part-of-speech
tagging (see Dietterich 1997 and van Halteren
et al 2001 for overviews), and notably supervised
101
WSD (Florian et al, 2002).
Since our effort is focused exclusively on un-
supervised methods, we cannot use most ma-
chine learning approaches for creating an en-
semble (e.g., stacking, confidence-based combina-
tion), as they require a labeled training set. We
therefore examined several basic ensemble com-
bination approaches that do not require parameter
estimation from training data.
We define Score(Mi,s j) as the (normalized)score which a method Mi gives to word sense s j.The predominant sense calculated by method Mifor word w is then determined by:
PS(Mi,w) = argmax
s j?senses(w)
Score(Mi,s j)
All ensemble methods receive a set {Mi}ki=1 of in-dividual methods to combine, so we denote each
ensemble method by MethodName({Mi}ki=1).
Direct Voting Each ensemble component has
one vote for the predominant sense, and the sense
with the most votes is chosen. The scoring func-
tion for the voting ensemble is defined as:
Score(Voting({Mi}ki=1),s)) =
k
?
i=1
eq[s,PS(Mi,w)]
where eq[s,PS(Mi,w)] =
{ 1 if s = PS(Mi,w)0 otherwise
Probability Mixture Each method provides
a probability distribution over the senses. These
probabilities (normalized scores) are summed, and
the sense with the highest score is chosen:
Score(ProbMix({Mi}ki=1),s)) =
k
?
i=1
Score(Mi,s)
Rank-Based Combination Each method
provides a ranking of the senses for a given target
word. For each sense, its placements according to
each of the methods are summed and the sense
with the lowest total placement (closest to first
place) wins.
Score(Ranking({Mi}ki=1),s)) =
k
?
i=1
(?1)?Placei(s)
where Placei(s) is the number of distinct scoresthat are larger or equal to Score(Mi,s).
Arbiter-based Combination One WSD
method can act as an arbiter for adjudicating dis-
agreements among component systems. It makes
sense for the adjudicator to have reasonable
performance on its own. We therefore selected
Method Accps Accwsd/ps
Similarity 54.9 46.5
SSI 53.5 47.9
Voting 57.3?$ 49.8?$
PrMixture 57.2?$ 50.4?$?
Rank-based 58.1?$ 50.3?$?
Arbiter-based 56.3?$ 48.7?$?
UpperBnd 100 68.4
Table 4: Ensemble Combination Results (?: sig.
diff. from Similarity, $: sig. diff. from SSI, ?: sig.
diff. from Voting, p < 0.01)
SSI as the arbiter since it had the best accuracy on
the WSD task (see Table 2). For each disagreed
word w, and for each sense s of w assigned by
any of the systems in the ensemble {Mi}ki=1, wecalculate the following score:
Score(Arbiter({Mi}ki=1),s) = SSIScore?(s)
where SSIScore?(s) is a modified version of the
score introduced in Section 2 which exploits as a
context for s the set of agreed senses and the re-
maining words of each sentence. We exclude from
the context used by SSI the senses of w which were
not chosen by any of the systems in the ensem-
ble . This effectively reduces the number of senses
considered by the arbiter and can positively influ-
ence the algorithm?s performance, since it elimi-
nates noise coming from senses which are likely
to be wrong.
5 Experiment 2: Ensembles for
Unsupervised WSD
5.1 Method and Parameter Settings
We assess the performance of the different en-
semble systems on the same set of SemCor nouns
on which the individual methods were tested. For
the best ensemble, we also report results on dis-
ambiguating all nouns in the Senseval-3 data set.
We focus exclusively on nouns to allow com-
parisons with the results obtained from SemCor.
We used the same parameters as in Experiment 1
for constructing the ensembles. As discussed ear-
lier, token-based methods can disambiguate target
words either in context or using the predominant
sense. SSI was employed in the predominant sense
setting in our arbiter experiment.
5.2 Results
Our results are summarized in Table 4. As can be
seen, all ensemble methods perform significantly
102
Ensemble Accps Accwsd/ps
Rank-based 58.1 50.3
Overlap 57.6 (?0.5) 49.7 (?0.6)
LexChains 57.2 (?0.7) 50.2 (?0.1)
Similarity 56.3 (?1.8) 49.4 (?0.9)
SSI 56.3 (?1.8) 48.2 (?2.1)
Table 5: Decrease in accuracy as a result of re-
moval of each method from the rank-based ensem-
ble.
better than the best individual methods, i.e., Simi-
larity and SSI. On the WSD task, the voting, prob-
ability mixture, and rank-based ensembles signif-
icantly outperform the arbiter-based one. The per-
formances of the probability mixture, and rank-
based combinations do not differ significantly but
both ensembles are significantly better than vot-
ing. One of the factors contributing to the arbiter?s
worse performance (compared to the other ensem-
bles) is the fact that in many cases (almost 30%),
none of the senses suggested by the disagreeing
methods is correct. In these cases, there is no way
for the arbiter to select the correct sense. We also
examined the relative contribution of each compo-
nent to overall performance. Table 5 displays the
drop in performance by eliminating any particular
component from the rank-based ensemble (indi-
cated by ?). The system that contributes the most
to the ensemble is SSI. Interestingly, Overlap and
Similarity yield similar improvements in WSD ac-
curacy (0.6 and 0.9, respectively) when added to
the ensemble.
Figure 1 shows the WSD accuracy of the best
single methods and the ensembles as a function of
the noun frequency in SemCor. We can see that
there is at least one ensemble outperforming any
single method in every frequency band and that
the rank-based ensemble consistently outperforms
Similarity and SSI in all bands. Although Similar-
ity has an advantage over SSI for low and medium
frequency words, it delivers worse performance
for high frequency words. This is possibly due to
the quality of neighbors obtained for very frequent
words, which are not semantically distinct enough
to reliably discriminate between different senses.
Table 6 lists the performance of the rank-based
ensemble on the Senseval-3 (noun) corpus. We
also report results for the best individual method,
namely SSI, and compare our results with the best
unsupervised system that participated in Senseval-
3. The latter was developed by Strapparava et al
(2004) and performs domain driven disambigua-
tion (IRST-DDD). Specifically, the approach com-
1-4 5-9 10-19 20-99 100+Noun frequency bands
40
42
44
46
48
50
52
54
WS
D A
ccu
rac
y (%
)
SimilaritySSIArbiter
VotingProbMixRanking
Figure 1: WSD accuracy as a function of noun fre-
quency in SemCor
Method Precision Recall Fscore
Baseline 36.8 36.8 36.8
SSI 62.5 62.5 62.5
IRST-DDD 63.3 62.2 61.2
Rank-based 63.9 63.9 63.9
UpperBnd 68.7 68.7 68.7
Table 6: Results of individual disambiguation al-
gorithms and rank-based ensemble on Senseval-3
nouns
pares the domain of the context surrounding the
target word with the domains of its senses and uses
a version of WordNet augmented with domain la-
bels (e.g., economy, geography). Our baseline se-
lects the first sense randomly and uses it to disam-
biguate all instances of a target word. Our upper
bound defaults to the first sense from SemCor. We
report precision, recall and Fscore. In cases where
precision and recall figures coincide, the algorithm
has 100% coverage.
As can be seen the rank-based, ensemble out-
performs both SSI and the IRST-DDD system.
This is an encouraging result, suggesting that there
may be advantages in developing diverse classes
of unsupervised WSD algorithms for system com-
bination. The results in Table 6 are higher than
those reported for SemCor (see Table 4). This is
expected since the Senseval-3 data set contains
monosemous nouns as well. Taking solely polyse-
mous nouns into account, SSI?s Fscore is 53.39%
and the ranked-based ensemble?s 55.0%. We fur-
ther note that not all of the components in our en-
semble are optimal. Predominant senses for Lesk
and LexChains were estimated from the Senseval-
3 data, however a larger corpus would probably
yield more reliable estimates.
103
6 Conclusions and Discussion
In this paper we have presented an evaluation
study of four well-known approaches to unsuper-
vised WSD. Our comparison involved type- and
token-based disambiguation algorithms relying on
different kinds of WordNet relations and different
amounts of corpus data. Our experiments revealed
two important findings. First, type-based disam-
biguation yields results superior to a token-based
approach. Using predominant senses is preferable
to disambiguating instances individually, even for
token-based algorithms. Second, the outputs of
the different approaches examined here are suffi-
ciently diverse to motivate combination methods
for unsupervised WSD. We defined several ensem-
bles on the predominant sense outputs of individ-
ual methods and showed that combination systems
outperformed their best components both on the
SemCor and Senseval-3 data sets.
The work described here could be usefully em-
ployed in two tasks: (a) to create preliminary an-
notations, thus supporting the ?annotate automati-
cally, correct manually? methodology used to pro-
vide high volume annotation in the Penn Treebank
project; and (b) in combination with supervised
WSD methods that take context into account; for
instance, such methods could default to an unsu-
pervised system for unseen words or words with
uninformative contexts.
In the future we plan to integrate more com-
ponents into our ensembles. These include not
only domain driven disambiguation algorithms
(Strapparava et al, 2004) but also graph theoretic
ones (Mihalcea, 2005) as well as algorithms that
quantify the degree of association between senses
and their co-occurring contexts (Mohammad and
Hirst, 2006). Increasing the number of compo-
nents would allow us to employ more sophisti-
cated combination methods such as unsupervised
rank aggregation algorithms (Tan and Jin, 2004).
Acknowledgements
We are grateful to Diana McCarthy for her help with thiswork and to Michel Galley for making his code availableto us. Thanks to John Carroll and Rob Koeling for in-sightful comments and suggestions. The authors acknowl-edge the support of EPSRC (Brody and Lapata; grantEP/C538447/1) and the European Union (Navigli; InteropNoE (508011)).
References
Banerjee, Satanjeev and Ted Pedersen. 2003. Extended glossoverlaps as a measure of semantic relatedness. In Proceed-
ings of the 18th IJCAI. Acapulco, pages 805?810.
Briscoe, Ted and John Carroll. 2002. Robust accurate statis-tical annotation of general text. In Proceedings of the 3rd
LREC. Las Palmas, Gran Canaria, pages 1499?1504.
Dietterich, T. G. 1997. Machine learning research: Four cur-rent directions. AI Magazine 18(4):97?136.
Edmonds, Philip. 2000. Designing a task for SENSEVAL-2.Technical note.
Florian, Radu, Silviu Cucerzan, Charles Schafer, and DavidYarowsky. 2002. Combining classifiers for word sense dis-ambiguation. Natural Language Engineering 1(1):1?14.
Galley, Michel and Kathleen McKeown. 2003. Improvingword sense disambiguation in lexical chaining. In Pro-
ceedings of the 18th IJCAI. Acapulco, pages 1486?1488.Hoste, Ve?ronique, Iris Hendrickx, Walter Daelemans, andAntal van den Bosch. 2002. Parameter optimization formachine-learning of word sense disambiguation. Lan-
guage Engineering 8(4):311?325.
Lesk, Michael. 1986. Automatic sense disambiguation us-ing machine readable dictionaries: How to tell a pine conefrom an ice cream cone. In Proceedings of the 5th SIG-
DOC. New York, NY, pages 24?26.
Lin, Dekang. 1998. An information-theoretic definition ofsimilarity. In Proceedings of the 15th ICML. Madison,WI, pages 296?304.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Car-roll. 2004. Finding predominant senses in untagged text.In Proceedings of the 42th ACL. Barcelona, Spain, pages280?287.
Mihalcea, Rada. 2005. Unsupervised large-vocabulary wordsense disambiguation with graph-based algorithms for se-quence data labeling. In Proceedings of the HLT/EMNLP.Vancouver, BC, pages 411?418.
Mihalcea, Rada and Phil Edmonds, editors. 2004. Proceed-
ings of the SENSEVAL-3. Barcelona, Spain.
Miller, George A., Claudia Leacock, Randee Tengi, andRoss T. Bunker. 1993. A semantic concordance. In Pro-
ceedings of the ARPA HLT Workshop. Morgan Kaufman,pages 303?308.
Mohammad, Saif and Graeme Hirst. 2006. Determining wordsense dominance using a thesaurus. In Proceedings of the
EACL. Trento, Italy, pages 121?128.
Morris, Jane and Graeme Hirst. 1991. Lexical cohesion com-puted by thesaural relations as an indicator of the structureof text. Computational Linguistics 1(17):21?43.
Navigli, Roberto. 2005. Semi-automatic extension of large-scale linguistic knowledge bases. In Proceedings of the
18th FLAIRS. Florida.
Navigli, Roberto and Paola Velardi. 2005. Structural seman-tic interconnections: a knowledge-based approach to wordsense disambiguation. PAMI 27(7):1075?1088.
Ng, Tou Hwee. 1997. Getting serious about word sense dis-ambiguation. In Proceedings of the ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics: Why, What,
and How?. Washington, DC, pages 1?7.
Stokoe, Christopher. 2005. Differentiating homonymy andpolysemy in information retrieval. In Proceedings of the
HLT/EMNLP. Vancouver, BC, pages 403?410.
Strapparava, Carlo, Alfio Gliozzo, and Claudio Giuliano.2004. Word-sense disambiguation for machine transla-tion. In Proceedings of the SENSEVAL-3. Barcelona,Spain, pages 229?234.
Tan, Pang-Ning and Rong Jin. 2004. Ordering patterns bycombining opinions from multiple sources. In Proceed-
ings of the 10th KDD. Seattle, WA, pages 22?25.
van Halteren, Hans, Jakub Zavrel, and Walter Daelemans.2001. Improving accuracy in word class tagging throughcombination of machine learning systems. Computational
Linguistics 27(2):199?230.
Vickrey, David, Luke Biewald, Marc Teyssier, and DaphneKoller. 2005. Word-sense disambiguation for machinetranslation. In Proceedings of the HLT/EMNLP. Vancou-ver, BC, pages 771?778.
Yarowsky, David and Radu Florian. 2002. Evaluating sensedisambiguation across diverse parameter spaces. Natural
Language Engineering 9(4):293?310.
104
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 448?455,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Clustering Clauses for High-Level Relation Detection: An
Information-theoretic Approach
Samuel Brody
School of Informatics
University of Edinburgh
s.brody@sms.ed.ac.uk
Abstract
Recently, there has been a rise of in-
terest in unsupervised detection of high-
level semantic relations involving com-
plex units, such as phrases and whole
sentences. Typically such approaches are
faced with two main obstacles: data
sparseness and correctly generalizing
from the examples. In this work, we
describe the Clustered Clause represen-
tation, which utilizes information-based
clustering and inter-sentence dependen-
cies to create a simplified and generalized
representation of the grammatical clause.
We implement an algorithm which uses
this representation to detect a predefined
set of high-level relations, and demon-
strate our model?s effectiveness in over-
coming both the problems mentioned.
1 Introduction
The semantic relationship between words, and
the extraction of meaning from syntactic data
has been one of the main points of research in
the field of computational linguistics (see Sec-
tion 5 and references therein). Until recently,
the focus has remained largely either at the sin-
gle word or sentence level (for instance: depen-
dency extraction, word-to-word semantic simi-
larity from syntax, etc.) or on relations between
units at a very high context level such as the
entire paragraph or document (e.g. categorizing
documents by topic).
Recently there have been several attempts to
define frameworks for detecting and studying in-
teractions at an intermediate context level, and
involving whole clauses or sentences. Dagan
et al (2005) have emphasized the importance
of detecting textual-entailment/implication be-
tween two sentences, and its place as a key com-
ponent in many real-world applications, such as
Information Retrieval and Question Answering.
When designing such a framework, one is
faced with several obstacles. As we approach
higher levels of complexity, the problem of defin-
ing the basic units we study (e.g. words, sen-
tences etc.) and the increasing amount of in-
teractions make the task very difficult. In addi-
tion, the data sparseness problem becomes more
acute as the data units become more complex
and have an increasing number of possible val-
ues, despite the fact that many of these values
have similar or identical meaning.
In this paper we demonstrate an approach
to solving the complexity and data sparse-
ness problems in the task of detecting rela-
tions between sentences or clauses. We present
the Clustered Clause structure, which utilizes
information-based clustering and dependencies
within the sentence to create a simplified and
generalized representation of the grammatical
clause and is designed to overcome both these
problems.
The clustering method we employ is an inte-
gral part of the model. We evaluate our clusters
against semantic similarity measures defined on
the human-annotated WordNet structure (Fell-
baum, 1998). The results of these comparisons
show that our cluster members are very similar
semantically. We also define a high-level rela-
tion detection task involving relations between
clauses, evaluate our results, and demonstrate
448
the effectiveness of using our model in this task.
This work extends selected parts of Brody
(2005), where further details can be found.
2 Model Construction
When designing our framework, we must ad-
dress the complexity and sparseness problems
encountered when dealing with whole sentences.
Our solution to these issues combines two ele-
ments. First, to reduce complexity, we simplify
a grammatical clause to its primary components
- the subject, verb and object. Secondly, to pro-
vide a generalization framework which will en-
able us to overcome data-sparseness, we cluster
each part of the clause using data from within
the clause itself. By combining the simplified
clause structure and the clustering we produce
our Clustered Clause model - a triplet of clusters
representing a generalized clause.
The Simplified Clause: In order to extract
clauses from the text, we use Lin?s parser MINI-
PAR (Lin, 1994). The output of the parser is
a dependency tree of each sentence, also con-
taining lemmatized versions of the component
words. We extract the verb, subject and object
of every clause (including subordinate clauses),
and use this triplet of values, the simplified
clause, in place of the original complete clause.
As seen in Figure 1, these components make up
the top (root) triangle of the clause parse tree.
We also use the lemmatized form of the words
provided by the parser, to further reduce com-
plexity.
Figure 1: The parse tree for the sentence ?John
found a solution to the problem?. The subject-
verb-object triplet is marked with a border.
Clustering Clause Components: For our
model, we cluster the data to provide both gen-
eralization, by using a cluster to represent a
more generalized concept shared by its compo-
nent words, and a form of dimensionality reduc-
tion, by using fewer units (clusters) to represent
a much larger amount of words.
We chose to use the Sequential Information
Bottleneck algorithm (Slonim et al, 2002) for
our clustering tasks. The information Bottle-
neck principle views the clustering task as an
optimization problem, where the clustering algo-
rithm attempts to group together values of one
variable while retaining as much information as
possible regarding the values of another (target)
variable. There is a trade-off between the com-
pactness of the clustering and the amount of re-
tained information. This algorithm (and others
based on the IB principle) is especially suited for
use with graphical models or dependency struc-
tures, since the distance measure it employs in
the clustering is defined solely by the depen-
dency relation between two variables, and there-
fore required no external parameters. The val-
ues of one variable are clustered using their co-
occurrence distribution with regard to the values
of the second (target) variable in the dependency
relation. As an example, consider the following
subject-verb co-occurrence matrix:
S \ V tell scratch drink
dog 0 4 5
John 4 0 9
cat 0 6 3
man 6 1 2
The value in cell (i, j) indicates the number
of times the noun i occurred as the subject of
the verb j. Calculating the Mutual Information
between the subjects variable (S) and verbs vari-
able (V) in this table, we get MI(S, V ) = 0.52
bits. Suppose we wish to cluster the subject
nouns into two clusters while preserving the
highest Mutual Information with regard to the
verbs. The following co-occurrence matrix is the
optimal clustering, and retains a M.I. value of
0.4 bits (77% of original):
Clustered S \ V tell scratch drink
{dog,cat} 0 10 8
{John,man} 10 1 11
Notice that although the values in the drink
column are higher than in others, and we may be
449
tempted to cluster together dog and John based
on this column, the informativeness of this verb
is smaller - if we know the verb is tell we can be
sure the noun is not dog or cat, whereas if we
know it is drink, we can only say it is slightly
more probable that the noun is John or dog.
Our dependency structure consists of three
variables: subject, verb, and object, and we take
advantage of the subject-verb and verb-object
dependencies in our clustering. The clustering
was performed on each variable separately, in
a two phase procedure (see Figure 2). In the
first stage, we clustered the subject variable into
200 clusters1, using the subject-verb dependency
(i.e. the verb variable was the target). The same
was done with the object variable, using the
verb-object dependency. In the second phase, we
wish to cluster the verb values with regard to
both the subject and object variables. We could
not use all pairs of subjects and objects values as
the target variable in this task, since too many
such combinations exist. Instead, we used a vari-
able composed of all the pairs of subject and ob-
ject clusters as the target for the verb clustering.
In this fashion we produced 100 verb clusters.
Figure 2: The two clustering phases. Arrows rep-
resent dependencies between the variables which
are used in the clustering.
Combining the Model Elements: Having
obtained our three clustered variables, our orig-
inal simplified clause triplet can now be used
to produce the Clustered Clause model. This
model represents a clause in the data by a triplet
of cluster indexes, one cluster index for each
clustered variable. In order to map a clause in
1The chosen numbers of clusters are such that each
the resulting clustered variables preserved approximately
half of the co-occurrence mutual information that existed
between the original (unclustered) variable and its target.
the text to its corresponding clustered clause,
it is first parsed and lemmatized to obtain the
subject, verb and object values, as described
above, and then assigned to the clustered clause
in which the subject cluster index is that of
the cluster containing the subject word of the
clause, and the same for the verb and object
words. For example, the sentence ?The terrorist
threw the grenade? would be converted to the
triplet (terrorist, throw, grenade) and assigned
to the clustered clause composed of the three
clusters to which these words belong. Other
triplets assigned to this clustered clause might
include (fundamentalist, throw, bomb) or (mil-
itant, toss, explosive). Applying this procedure
to the entire text corpus results in a distilla-
tion of the text into a series of clustered clauses
containing the essential information about the
actions described in the text.
Technical Specifications: For this work we
chose to use the entire Reuters Corpus (En-
glish, release 2000), containing 800,000 news
articles collected uniformly from 20/8/1996 to
19/8/1997. Before clustering, several prepro-
cessing steps were taken. We had a very large
amount of word values for each of the Sub-
ject (85,563), Verb (4,593) and Object (74,842)
grammatical categories. Many of the words were
infrequent proper nouns or rare verbs and were
of little interest in the pattern recognition task.
We therefore removed the less frequent words
- those appearing in their category less than
one hundred times. We also cleaned our data
by removing all words that were one letter in
length, other than the word ?I?. These were
mostly initials in names of people or compa-
nies, which were uninformative without the sur-
rounding context. This processing step brought
us to the final count of 2,874,763 clause triplets
(75.8% of the original number), containing 3,153
distinct subjects, 1,716 distinct verbs, and 3,312
distinct objects. These values were clustered as
described above. The clusters were used to con-
vert the simplified clauses into clustered clauses.
3 Evaluating Cluster Quality
Examples of some of the resulting clusters are
provided in Table 1. When manually examin-
450
?Technical Developements? (Subject
Cluster 160): treatment, drug, method, tactic,
version, technology, software, design, device, vaccine,
ending, tool, mechanism, technique, instrument,
therapy, concept, model
?Ideals/Virtues? (Object Cluster 14):
sovereignty, dominance, logic, validity, legitimacy,
freedom, discipline, viability, referendum, wisdom,
innocence, credential, integrity, independence
?Emphasis Verbs? (Verb Cluster 92): im-
ply, signify, highlight, mirror, exacerbate, mark, sig-
nal, underscore, compound, precipitate, mask, illus-
trate, herald, reinforce, suggest, underline, aggra-
vate, reflect, demonstrate, spell, indicate, deepen
?Plans? (Object Cluster 33): journey, ar-
rangement, trip, effort, attempt, revolution, pull-
out, handover, sweep, preparation, filing, start, play,
repatriation, redeployment, landing, visit, push,
transition, process
Table 1: Example clusters (labeled manually).
ing the clusters, we noticed the ?fine-tuning?
of some of the clusters. For instance, we had
a cluster of countries involved in military con-
flicts, and another for other countries; a cluster
for winning game scores, and another for ties;
etc. The fact that the algorithm separated these
clusters indicates that the distinction between
them is important with regard to the interac-
tions within the clause. For instance, in the first
example, the context in which countries from the
first cluster appear is very different from that in-
volving countries in the second cluster.
The effect of the dependencies we use is also
strongly felt. Many clusters can be described by
labels such as ?things that are thrown? (rock,
flower, bottle, grenade and others), or ?verbs
describing attacks? (spearhead, foil, intensify,
mount, repulse and others). While such crite-
ria may not be the first choice of someone who
is asked to cluster verbs or nouns, they repre-
sent unifying themes which are very appropri-
ate to pattern detection tasks, in which we wish
to detect connections between actions described
in the clauses. For instance, we would like to
detect the relation between throwing and mil-
itary/police action (much of the throwing de-
scribed in the news reports fits this relation). In
order to do this, we must have clusters which
unite the words relevant to those actions. Other
criteria for clustering would most likely not be
suitable, since they would probably not put egg,
bottle and rock in the same category. In this re-
spect, our clustering method provides a more
effective modeling of the domain knowledge.
3.1 Evaluation via Semantic Resource
Since the success of our pattern detection task
depends to a large extent on the quality of our
clusters, we performed an experiment designed
to evaluate semantic similarity between mem-
bers of our clusters. For this purpose we made
use of the WordNet Similarity package (Peder-
sen et al, 2004). This package contains many
similarity measures, and we selected three of
them (Resnik (1995), Leacock and Chodorow
(1997), Hirst and St-Onge (1997)), which make
use of different aspects of WordNet (hierarchy
and graph structure). We measured the average
pairwise similarity between any two words ap-
pearing in the same cluster. We then performed
the same calculation on a random grouping of
the words, and compared the two scores. The re-
sults (Fig. 3) show that our clustering, based on
co-occurrence statistics and dependencies within
the sentence, correlates with a purely semantic
similarity as represented by the WordNet struc-
ture, and cannot be attributed to chance.
Figure 3: Inter-cluster similarity (average pair-
wise similarity between cluster members) in our
clustering (light) and a random one (dark). Ran-
dom clustering was performed 10 times. Aver-
age values are shown with error bars to indicate
standard deviation. Only Hirst & St-Onge mea-
sure verb similarity.
4 Relation Detection Task
Motivation: In order to demonstrate the use
of our model, we chose a relation detection task.
The workshop on entailment mentioned in the
introduction was mainly focused on detecting
whether or not an entailment relation exists be-
tween two texts. In this work we present a com-
451
plementary approach - a method designed to au-
tomatically detect relations between portions of
text and generate a knowledge base of the de-
tected relations in a generalized form. As stated
by (Dagan et al, 2005), such relations are im-
portant for IR applications. In addition, the pat-
terns we employ are likely to be useful in other
linguistic tasks involving whole clauses, such as
paraphrase acquisition.
Pattern Definition: For our relation detec-
tion task, we searched for instances of prede-
fined patterns indicating a relation between two
clustered clauses. We restricted the search to
clause pairs which co-occur within a distance of
ten clauses2 from each other. In addition to the
distance restriction, we required an anchor : a
noun that appears in both clauses, to further
strengthen the relation between them. Noun an-
chors establish the fact that the two compo-
nent actions described by the pattern involve the
same entities, implying a direct connection be-
tween them. The use of verb anchors was also
tested, but found to be less helpful in detect-
ing significant patterns, since in most cases it
simply found verbs which tend to repeat them-
selves frequently in a context. The method we
describe assumes that statistically significant co-
occurrences indicate a relationship between the
clauses, but does not attempt to determine the
type of relation.
Significance Calculation: The patterns de-
tected by the system were scored using the sta-
tistical p-value measure. This value represents
the probability of detecting a certain number
of occurrences of a given pattern in the data
under the independence assumption, i.e. assum-
ing there is no connection between the two
halves of the pattern. If the system has detected
k instances of a certain pattern, we calculate
the probability of encountering this number of
instances under the independence assumption.
The smaller the probability, the higher the sig-
nificance. We consider patterns with a chance
probability lower than 5% to be significant.
We assume a Gaussian-like distribution of oc-
2Our experiments showed that increasing the distance
beyond this point did not result in significant increase in
the number of detected patterns.
currence probability for each pattern3. In or-
der to estimate the mean and standard devia-
tion values, we created 100 simulated sequences
of triplets (representing clustered clauses) which
were independently distributed and varied only
in their overall probability of occurrence. We
then estimated the mean and standard devia-
tion for any pair of clauses in the actual data
using the simulated sequences.
(X,V C36, OC7) ?10 (X,V C57, OC85)
storm, lash, province ... storm, cross, Cuba
quake, shake, city ... quake, hit, Iran
earthquake, jolt, city ... earthquake, hit, Iran
(X,V C40, OC165) ?10 (X,V C52, OC152)
police, arrest, leader ... police, search, mosque
police, detain, leader ... police, search, mosque
police, arrest, member ... police, raid, enclave
(SC39, V C21, X) ?10 (X, beat 4, OC155)
sun, report, earnings ... earnings,beat,expectation
xerox, report, earnings ... earnings, beat, forecast
microsoft,release,result ... result, beat, forecast
(X,V C57, OC7) ?10 (X, cause 4, OC153)
storm, hit, coast ... storm, cause, damage
cyclone, near, coast ... cyclone, cause, damage
earthquake,hit,northwest ... earthquake,cause,damage
quake , hit, northwest ... quake, cause, casualty
earthquake,hit,city ... earthquake,cause,damage
Table 2: Example Patterns
4.1 Pattern Detection Results
In Table 2 we present several examples of
high ranking (i.e. significance) patterns with
different anchorings detected by our method.
The detected patterns are represented using
the notation of the form (SCi, V Cj , X) ?n
(X,V Ci? , OCj?). X indicates the anchoring
word. In the example notation, the anchoring
word is the object of the first clause and the
subject of the second (O-S for short). n indicates
the maximal distance between the two clauses.
The terms SC, V C or OC with a subscripted
index represent the cluster containing the sub-
ject, verb or object (respectively) of the appro-
priate clause. For instance, in the first example
in Table 2, V C36 indicates verb cluster no. 36,
containing the verbs lash, shake and jolt, among
others.
3Based on Gwadera et al (2003), dealing with a sim-
ilar, though simpler, case.
4In two of the patterns, instead of a cluster for the
verb, we have a single word - beat or cause. This is the
result of an automatic post-processing stage intended to
prevent over-generalization. If all the instances of the pat-
452
Anchoring Number of
System Patterns Found
Subject-Subject 428
Object-Object 291
Subject-Object 180
Object-Subject 178
Table 3: Numbers of patterns found (p < 5%)
Table 3 lists the number of patterns found,
for each anchoring system. The different anchor-
ing systems produce quantitatively different re-
sults. Anchoring between the same categories
produces more patterns than between the same
noun in different grammatical roles. This is ex-
pected, since many nouns can only play a certain
part in the clause (for instance, many verbs can-
not have an inanimate entity as their subject).
The number of instances of patterns we found
for the anchored template might be considered
low, and it is likely that some patterns were
missed simply because their occurrence proba-
bility was very low and not enough instances of
the pattern occurred in the text. In Section 4 we
stated that in this task, we were more interested
in precision than in recall. In order to detect a
wider range of patterns, a less restricted defini-
tion of the patterns, or a different significance
indicator, should be used (see Sec. 6).
Human Evaluation: In order to better de-
termine the quality of patterns detected by our
system, and confirm that the statistical signif-
icance testing is consistent with human judg-
ment, we performed an evaluation experiment
with the help of 22 human judges. We presented
each of the judges with 60 example groups, 15
for each type of anchoring. Each example group
contained three clause pairs conforming to the
anchoring relation. The clauses were presented
in a normalized form consisting only of a sub-
ject, object and verb converted to past tense,
with the addition of necessary determiners and
prepositions. For example, the triplet (police,
detain, leader) was converted to ?The police de-
tained the leader?. In half the cases (randomly
tern in the text contained the same word in a certain po-
sition (in these examples - the verb position in the second
clause), this word was placed in that position in the gen-
eralized pattern, rather than the cluster it belonged to.
Since we have no evidence for the fact that other words
in the cluster can fit that position, using the cluster in-
dicator would be over-generalizing.
selected), these clause pairs were actual exam-
ples (instances) of a pattern detected by our sys-
tem (instances group), such as those appearing
in Table 2. In the other half, we listed three
clause pairs, each of which conformed to the
anchoring specification listed in Section 4, but
which were randomly sampled from the data,
and so had no connection to one another (base-
line group). We asked the judges to rate on a
scale of 1-5 whether they thought the clause
pairs were a good set of examples of a common
relation linking the first clause in each pair to
the second one.
Instances Instances Baseline Baseline
Score StdDev Score StdDev
All 3.5461 0.4780 2.6341 0.4244
O-S 3.9266 0.6058 2.8761 0.5096
O-O 3.4938 0.5144 2.7464 0.6205
S-O 3.4746 0.7340 2.5758 0.6314
S-S 3.2398 0.4892 2.3584 0.5645
Table 4: Results for human evaluation
Table 4 reports the overall average scores for
baseline and instances groups, and for each of
the four anchoring types individually. The scores
were averaged over all examples and all judges.
An ANOVA showed the difference in scores be-
tween the baseline and instance groups to be
significant (p < 0.001) in all four cases.
Achievement of Model Goals: We em-
ployed clustering in our model to overcome data-
sparseness. The importance of this decision was
evident in our results. For example, the second
pattern shown in Table 2 appeared only four
times in the text. In these instances, verb cluster
40 was represented twice by the verb arrest and
twice by detain. Two appearances are within the
statistical deviation of all but the rarest words,
and would not have been detected as significant
without the clustering effect. This means the
pattern would have been overlooked, despite the
strongly intuitive connection it represents. The
system detected several such patterns.
The other reason for clustering was general-
ization. Even in cases where patterns involving
single words could have been detected, it would
have been impossible to unify similar patterns
into generalized ones. In addition, when encoun-
tering a new clause which differs slightly from
453
the ones we recognized in the original data, there
would be no way to recognize it and draw the ap-
propriate conclusions. For example, there would
be no way to relate the sentence ?The typhoon
approached the coast? to the fourth example pat-
tern, and the connection with the resulting dam-
age would not be recognized.
5 Comparison with Previous Work
The relationship between textual features and
semantics and the use of syntax as an indica-
tor of semantics has been widespread. Following
the idea proposed in Harris? Distributional Hy-
pothesis (Harris, 1985), that words occurring in
similar contexts are semantically similar, many
works have used different definitions of context
to identify various types of semantic similarity.
Hindle (1990) uses a mutual-information based
metric derived from the distribution of subject,
verb and object in a large corpus to classify
nouns. Pereira et al (1993) cluster nouns ac-
cording to their distribution as direct objects
of verbs, using information-theoretic tools (the
predecessors of the tools we use in this work).
They suggest that information theoretic mea-
sures can also measure semantic relatedness.
These works focus only on relatedness of indi-
vidual words and do not describe how the au-
tomatic estimation of semantic similarity can
be useful in real-world tasks. In our work we
demonstrate that using clusters as generalized
word units helps overcome the sparseness and
generalization problems typically encountered
when attempting to extract high-level patterns
from text, as required for many applications.
The DIRT system (Lin and Pantel, 2001)
deals with inference rules, and employs the no-
tion of paths between two nouns in a sentence?s
parse tree. The system extracts such path struc-
tures from text, and provides a similarity mea-
sure between two such paths by comparing the
words which fill the same slots in the two paths.
After extracting the paths, the system finds
groups of similar paths. This approach bears
several similarities to the ideas described in this
paper, since our structure can be seen as a
specific path in the parse tree (probably the
most basic one, see Fig. 1). In our setup, sim-
ilar clauses are clustered together in the same
Clustered-Clause, which could be compared to
clustering DIRT?s paths using its similarity mea-
sure. Despite these similarities, there are several
important differences between the two systems.
Our method uses only the relationships inside
the path or clause in the clustering procedure,
so the similarity is based on the structure it-
self. Furthermore, Lin and Pantel did not create
path clusters or generalized paths, so that while
their method allowed them to compare phrases
for similarity, there is no convenient way to iden-
tify high level contextual relationships between
two nearby sentences. This is one of the signifi-
cant advantages that clustering has over similar-
ity measures - it allows a group of similar objects
to be represented by a single unit.
There have been several attempts to formu-
late and detect relationships at a higher context
level. The VerbOcean project (Chklovski and
Pantel, 2004) deals with relations between verbs.
It presents an automatically acquired network
of such relations, similar to the WordNet frame-
work. Though the patterns used to acquire the
relations are usually parts of a single sentence,
the relationships themselves can also be used
to describe connections between different sen-
tences, especially the enablement and happens-
before relations. Since verbs are the central part
of the clause, VerbOcean can be viewed as de-
tecting relations between clauses as whole units,
as well as those between individual words. As
a solution to the data sparseness problem, web
queries are used. Torisawa (2006) addresses a
similar problem, but focuses on temporal re-
lations, and makes use of the phenomena of
Japanese coordinate sentences. Neither of these
approaches attempt to create generalized rela-
tions or group verbs into clusters, though in
the case of VerbOcean this could presumably
be done using the similarity and strength values
which are defined and detected by the system.
6 Future Work
The clustered clause model presents many di-
rections for further research. It may be produc-
tive to extend the model further, and include
other parts of the sentence, such as adjectives
454
and adverbs. Clustering nouns by the adjectives
that describe them may provide a more intu-
itive grouping. The addition of further elements
to the structure may also allow the detection of
new kinds of relations.
The news-oriented domain of the corpus we
used strongly influenced our results. If we were
interested in more mundane relations, involving
day-to-day actions of individuals, a literary cor-
pus would probably be more suitable.
In defining our pattern template, several ele-
ments were tailored specifically to our task. We
limited ourselves to a very restricted set of pat-
terns in order to better demonstrate the effec-
tiveness of our model. For a more general knowl-
edge acquisition task, several of these restric-
tions may be relaxed, allowing a much larger
set of relations to be detected. For example, a
less strict significance filter, such as the support
and confidence measures commonly used in data
mining, may be preferable. These can be set to
different thresholds, according to the user?s pref-
erence.
In our current work, in order to prevent over-
generalization, we employed a single step post-
processing algorithm which detected the incor-
rect use of an entire cluster in place of a single
word (see footnote for Table 2). This method
allows only two levels of generalization - sin-
gle words and whole clusters. A more appro-
priate way to handle generalization would be
to use a hierarchical clustering algorithm. The
Agglomerative Information Bottleneck (Slonim
and Tishby, 1999) is an example of such an al-
gorithm, and could be employed for this task.
Use of a hierarchical method would result in
several levels of clusters, representing different
levels of generalization. It would be relatively
easy to modify our procedure to reduce general-
ization to the level indicated by the pattern ex-
amples in the text, producing a more accurate
description of the patterns detected.
Acknowledgments
The author acknowledges the support of EPSRC grant
EP/C538447/1. The author would like to thank Naftali
Tishby and Mirella Lapata for their supervision and as-
sistance on large portions of the work presented here. I
would also like to thank the anonymous reviewers and
my friends and colleagues for their helpful comments.
References
Brody, Samuel. 2005. Cluster-Based Pattern Recognition
in Natural Language Text . Master?s thesis, Hebrew
University, Jerusalem, Israel.
Chklovski, T. and P. Pantel. 2004. Verbocean: Mining
the web for fine-grained semantic verb relations. In
Proc. of EMNLP . pages 33?40.
Dagan, I., O. Glickman, and B. Magnini. 2005. The
pascal recognising textual entailment challenge. In
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment .
Fellbaum, Christiane, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Gwadera, R., M. Atallah, and W. Szpankowski. 2003.
Reliable detection of episodes in event sequences. In
ICDM .
Harris, Z. 1985. Distributional structure. Katz, J. J.
(ed.) The Philosophy of Linguistics pages 26?47.
Hindle, Donald. 1990. Noun classification from predicate-
argument structures. In Meeting of the ACL. pages
268?275.
Hirst, G. and D. St-Onge. 1997. Lexical chains as repre-
sentation of context for the detection and correction
of malapropisms. In WordNet: An Electronic Lexical
Database, ed., Christiane Fellbaum. MIT Press.
Leacock, C. and M. Chodorow. 1997. Combining local
context and wordnet similarity for word sense identi-
fication. In WordNet: An Electronic Lexical Database,
ed., Christiane Fellbaum. MIT Press.
Lin, Dekang. 1994. Principar - an efficient, broad-
coverage, principle-based parser. In COLING. pages
482?488.
Lin, Dekang and Patrick Pantel. 2001. DIRT - discovery
of inference rules from text. In Knowledge Discovery
and Data Mining . pages 323?328.
Pedersen, T., S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity - measuring the relatedness of con-
cepts. In Proc. of AAAI-04 .
Pereira, F., N. Tishby, and L. Lee. 1993. Distributional
clustering of english words. In Meeting of the Associ-
ation for Computational Linguistics. pages 183?190.
Resnik, Philip. 1995. Using information content to eval-
uate semantic similarity in a taxonomy. In IJCAI .
pages 448?453.
Slonim, N., N. Friedman, and N. Tishby. 2002. Unsu-
pervised document classification using sequential in-
formation maximization. In Proc. of SIGIR?02 .
Slonim, N. and N. Tishby. 1999. Agglomerative informa-
tion bottleneck. In Proc. of NIPS-12 .
Torisawa, Kentaro. 2006. Acquiring inference rules with
temporal constraints by using japanese coordinated
sentences and noun-verb co-occurrences. In Proceed-
ings of NAACL. pages 57?64.
455
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 103?111,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Bayesian Word Sense Induction
Samuel Brody
Dept. of Biomedical Informatics
Columbia University
samuel.brody@dbmi.columbia.edu
Mirella Lapata
School of Informatics
University of Edinburgh
mlap@inf.ed.ac.uk
Abstract
Sense induction seeks to automatically
identify word senses directly from a cor-
pus. A key assumption underlying pre-
vious work is that the context surround-
ing an ambiguous word is indicative of
its meaning. Sense induction is thus typ-
ically viewed as an unsupervised cluster-
ing problem where the aim is to partition
a word?s contexts into different classes,
each representing a word sense. Our work
places sense induction in a Bayesian con-
text by modeling the contexts of the am-
biguous word as samples from a multi-
nomial distribution over senses which
are in turn characterized as distributions
over words. The Bayesian framework pro-
vides a principled way to incorporate a
wide range of features beyond lexical co-
occurrences and to systematically assess
their utility on the sense induction task.
The proposed approach yields improve-
ments over state-of-the-art systems on a
benchmark dataset.
1 Introduction
Sense induction is the task of discovering automat-
ically all possible senses of an ambiguous word. It
is related to, but distinct from, word sense disam-
biguation (WSD) where the senses are assumed to
be known and the aim is to identify the intended
meaning of the ambiguous word in context.
Although the bulk of previous work has been
devoted to the disambiguation problem1, there are
good reasons to believe that sense induction may
be able to overcome some of the issues associ-
ated with WSD. Since most disambiguation meth-
ods assign senses according to, and with the aid
1Approaches to WSD are too numerous to list; We refer
the interested reader to Agirre et al (2007) for an overview
of the state of the art.
of, dictionaries or other lexical resources, it is dif-
ficult to adapt them to new domains or to lan-
guages where such resources are scarce. A re-
lated problem concerns the granularity of the sense
distinctions which is fixed, and may not be en-
tirely suitable for different applications. In con-
trast, when sense distinctions are inferred directly
from the data, they are more likely to represent
the task and domain at hand. There is little risk
that an important sense will be left out, or that ir-
relevant senses will influence the results. Further-
more, recent work in machine translation (Vickrey
et al, 2005) and information retrieval (Ve?ronis,
2004) indicates that induced senses can lead to im-
proved performance in areas where methods based
on a fixed sense inventory have previously failed
(Carpuat and Wu, 2005; Voorhees, 1993).
Sense induction is typically treated as an un-
supervised clustering problem. The input to the
clustering algorithm are instances of the ambigu-
ous word with their accompanying contexts (rep-
resented by co-occurrence vectors) and the output
is a grouping of these instances into classes cor-
responding to the induced senses. In other words,
contexts that are grouped together in the same
class represent a specific word sense. In this paper
we adopt a novel Bayesian approach and formalize
the induction problem in a generative model. For
each ambiguous word we first draw a distribution
over senses, and then generate context words ac-
cording to this distribution. It is thus assumed that
different senses will correspond to distinct lexical
distributions. In this framework, sense distinctions
arise naturally through the generative process: our
model postulates that the observed data (word con-
texts) are explicitly intended to communicate a la-
tent structure (their meaning).
Our work is related to Latent Dirichlet Allo-
cation (LDA, Blei et al 2003), a probabilistic
model of text generation. LDA models each doc-
ument using a mixture over K topics, which are
in turn characterized as distributions over words.
103
The words in the document are generated by re-
peatedly sampling a topic according to the topic
distribution, and selecting a word given the chosen
topic. Whereas LDA generates words from global
topics corresponding to the whole document, our
model generates words from local topics chosen
based on a context window around the ambiguous
word. Document-level topics resemble general do-
main labels (e.g., finance, education) and cannot
faithfully model more fine-grained meaning dis-
tinctions. In our work, therefore, we create an in-
dividual model for every (ambiguous) word rather
than a global model for an entire document col-
lection. We also show how multiple information
sources can be straightforwardly integrated with-
out changing the underlying probabilistic model.
For instance, besides lexical information we may
want to consider parts of speech or dependen-
cies in our sense induction problem. This is in
marked contrast with previous LDA-based mod-
els which mostly take only word-based informa-
tion into account. We evaluate our model on a
recently released benchmark dataset (Agirre and
Soroa, 2007) and demonstrate improvements over
the state-of-the-art.
The remainder of this paper is structured as fol-
lows. We first present an overview of related work
(Section 2) and then describe our Bayesian model
in more detail (Sections 3 and 4). Section 5 de-
scribes the resources and evaluation methodology
used in our experiments. We discuss our results in
Section 6, and conclude in Section 7.
2 Related Work
Sense induction is typically treated as a cluster-
ing problem, where instances of a target word
are partitioned into classes by considering their
co-occurring contexts. Considerable latitude is
allowed in selecting and representing the co-
occurring contexts. Previous methods have used
first or second order co-occurrences (Purandare
and Pedersen, 2004; Schu?tze, 1998), parts of
speech (Purandare and Pedersen, 2004), and gram-
matical relations (Pantel and Lin, 2002; Dorow
and Widdows, 2003). The size of the context win-
dow also varies, it can be a relatively small, such as
two words before and after the target word (Gauch
and Futrelle, 1993), the sentence within which the
target is found (Bordag, 2006), or even larger, such
as the 20 surrounding words on either side of the
target (Purandare and Pedersen, 2004).
In essence, each instance of a target word
is represented as a feature vector which subse-
quently serves as input to the chosen clustering
method. A variety of clustering algorithms have
been employed ranging from k-means (Purandare
and Pedersen, 2004), to agglomerative clustering
(Schu?tze, 1998), and the Information Bottleneck
(Niu et al, 2007). Graph-based methods have also
been applied to the sense induction task. In this
framework words are represented as nodes in the
graph and vertices are drawn between the tar-
get and its co-occurrences. Senses are induced by
identifying highly dense subgraphs (hubs) in the
co-occurrence graph (Ve?ronis, 2004; Dorow and
Widdows, 2003).
Although LDA was originally developed as a
generative topic model, it has recently gained
popularity in the WSD literature. The inferred
document-level topics can help determine coarse-
grained sense distinctions. Cai et al (2007) pro-
pose to use LDA?s word-topic distributions as fea-
tures for training a supervised WSD system. In a
similar vein, Boyd-Graber and Blei (2007) infer
LDA topics from a large corpus, however for un-
supervised WSD. Here, LDA topics are integrated
with McCarthy et al?s (2004) algorithm. For each
target word, a topic is sampled from the docu-
ment?s topic distribution, and a word is generated
from that topic. Also, a distributional neighbor is
selected based on the topic and distributional sim-
ilarity to the generated word. Then, the word sense
is selected based on the word, neighbor, and topic.
Boyd-Graber et al (2007) extend the topic mod-
eling framework to include WordNet senses as a
latent variable in the word generation process. In
this case the model discovers both the topics of
the corpus and the senses assigned to each of its
words.
Our own model is also inspired by LDA but cru-
cially performs word sense induction, not disam-
biguation. Unlike the work mentioned above, we
do not rely on a pre-existing list of senses, and do
not assume a correspondence between our auto-
matically derived sense-clusters and those of any
given inventory.2 A key element in these previous
attempts at adapting LDA forWSD is the tendency
to remain at a high level, document-like, setting.
In contrast, we make use of much smaller units
of text (a few sentences, rather than a full doc-
ument), and create an individual model for each
(ambiguous) word type. Our induced senses are
few in number (typically less than ten). This is in
marked contrast to tens, and sometimes hundreds,
2Such a mapping is only performed to enable evaluation
and comparison with other approaches (see Section 5).
104
of topics commonly used in document-modeling
tasks.
Unlike many conventional clustering meth-
ods (e.g., Purandare and Pedersen 2004; Schu?tze
1998), our model is probabilistic; it specifies
a probability distribution over possible values,
which makes it easy to integrate and combine with
other systems via mixture or product models. Fur-
thermore, the Bayesian framework allows the in-
corporation of several information sources in a
principled manner. Our model can easily handle an
arbitrary number of feature classes (e.g., parts of
speech, dependencies). This functionality in turn
enables us to evaluate which linguistic informa-
tion matters for the sense induction task. Previous
attempts to handle multiple information sources
in the LDA framework (e.g., Griffiths et al 2005;
Barnard et al 2003) have been task-specific and
limited to only two layers of information. Our
model provides this utility in a general framework,
and could be applied to other tasks, besides sense
induction.
3 The Sense Induction Model
The core idea behind sense induction is that con-
textual information provides important cues re-
garding a word?s meaning. The idea dates back to
(at least) Firth (1957) (?You shall know a word by
the company it keeps?), and underlies most WSD
and lexicon acquisition work to date. Under this
premise, we should expect different senses to be
signaled by different lexical distributions.
We can place sense induction in a probabilis-
tic setting by modeling the context words around
the ambiguous target as samples from a multino-
mial sense distribution. More formally, we will
write P(s) for the distribution over senses s of
an ambiguous target in a specific context win-
dow and P(w|s) for the probability distribution
over context words w given sense s. Each word wi
in the context window is generated by first sam-
pling a sense from the sense distribution, then
choosing a word from the sense-context distribu-
tion. P(si = j) denotes the probability that the jth
sense was sampled for the ith word token and
P(wi|si = j) the probability of context word wi un-
der sense j. The model thus specifies a distribution
over words within a context window:
P(wi) =
S
?
j=1
P(wi|si = j)P(si = j) (1)
where S is the number of senses. We assume that
each target word hasC contexts and each context c
? ? s w Nc
C
?(?)
Figure 1: Bayesian sense induction model; shaded
nodes represent observed variables, unshaded
nodes indicate latent variables. Arrows indi-
cate conditional dependencies between variables,
whereas plates (the rectangles in the figure) refer
to repetitions of sampling steps. The variables in
the lower right corner refer to the number of sam-
ples.
consists of Nc word tokens. We shall write ?( j) as a
shorthand for P(wi|si = j), the multinomial distri-
bution over words for sense j, and ?(c) as a short-
hand for the distribution of senses in context c.
Following Blei et al (2003) we will assume that
the mixing proportion over senses ? is drawn from
a Dirichlet prior with parameters ?. The role of
the hyperparameter ? is to create a smoothed sense
distribution. We also place a symmetric Dirichlet ?
on ? (Griffiths and Steyvers, 2002). The hyper-
parmeter ? can be interpreted as the prior observa-
tion count on the number of times context words
are sampled from a sense before any word from
the corpus is observed. Our model is represented
in graphical notation in Figure 1.
The model sketched above only takes word in-
formation into account. Methods developed for su-
pervised WSD often use a variety of information
sources based not only on words but also on lem-
mas, parts of speech, collocations and syntactic re-
lationships (Lee and Ng, 2002). The first idea that
comes to mind, is to use the same model while
treating various features as word-like elements. In
other words, we could simply assume that the con-
texts we wish to model are the union of all our
features. Although straightforward, this solution
is undesirable. It merges the distributions of dis-
tinct feature categories into a single one, and is
therefore conceptually incorrect, and can affect the
performance of the model. For instance, parts-of-
speech (which have few values, and therefore high
probability), would share a distribution with words
(which are much sparser). Layers containing more
elements (e.g. 10 word window) would overwhelm
105
? ?
s f Nc1
C
s f Nc2...
s f Ncn
?1(?1)
?2(?2)
?n(?n)
Figure 2: Extended sense induction model; inner
rectangles represent different sources (layers) of
information. All layers share the same, instance-
specific, sense distribution (?), but each have their
own (multinomial) sense-feature distribution (?).
Shaded nodes represent observed features f ; these
can be words, parts of speech, collocations or de-
pendencies.
smaller ones (e.g. 1 word window).
Our solution is to treat each information source
(or feature type) individually and then combine
all of them together in a unified model. Our un-
derlying assumption is that the context window
around the target word can have multiple represen-
tations, all of which share the same sense distribu-
tion.We illustrate this in Figure 2 where each inner
rectangle (layer) corresponds to a distinct feature
type. We will naively assume independence be-
tween multiple layers, even though this is clearly
not the case in our task. The idea here is to model
each layer as faithfully as possible to the empirical
data while at the same time combining information
from all layers in estimating the sense distribution
of each target instance.
4 Inference
Our inference procedure is based on Gibbs sam-
pling (Geman and Geman, 1984). The procedure
begins by randomly initializing all unobserved
random variables. At each iteration, each random
variable si is sampled from the conditional distri-
bution P(si|s?i) where s?i refers to all variables
other than si. Eventually, the distribution over sam-
ples drawn from this process will converge to the
unconditional joint distribution P(s) of the unob-
served variables (provided certain criteria are ful-
filled).
In our model, each element in each layer is a
variable, and is assigned a sense label (see Fig-
ure 2, where distinct layers correspond to differ-
ent representations of the context around the tar-
get word). From these assignments, we must de-
termine the sense distribution of the instance as a
whole. This is the purpose of the Gibbs sampling
procedure. Specifically, in order to derive the up-
date function used in the Gibbs sampler, we must
provide the conditional probability of the i-th vari-
able being assigned sense si in layer l, given the
feature value fi of the context variable and the cur-
rent sense assignments of all the other variables in
the data (s?i):
p(si|s?i, f ) ? p( fi|s, f?i,?) ? p(si|s?i,?) (2)
The probability of a single sense assignment, si,
is proportional to the product of the likelihood (of
feature fi, given the rest of the data) and the prior
probability of the assignment.
(3)
p( fi|s, f?i,?) =
Z
p( fi|l,s,?) ? p(?| f?i,?l)d?=
#( fi,si)+?l
#(si)+Vl ??l
For the likelihood term p( fi|s, f?i,?), integrating
over all possible values of the multinomial feature-
sense distribution ? gives us the rightmost term in
Equation 3, which has an intuitive interpretation.
The term #( fi,si) indicates the number of times
the feature-value fi was assigned sense si in the
rest of the data. Similarly, #(si) indicates the num-
ber of times the sense assignment si was observed
in the data. ?l is the Dirichlet prior for the feature-
sense distribution ? in the current layer l, and Vl
is the size of the vocabulary of that layer, i.e., the
number of possible feature values in the layer. In-
tuitively, the probability of a feature-value given
a sense is directly proportional to the number of
times we have seen that value and that sense-
assignment together in the data, taking into ac-
count a pseudo-count prior, expressed through ?.
This can also be viewed as a form of smoothing.
A similar approach is taken with regards to the
prior probability p(si|s?i,?). In this case, how-
ever, all layers must be considered:
p(si|s?i,?) =?
l
?l ? p(si|l,s?i,?l) (4)
106
Here ?l is the weight for the contribution of layer l,
and ?l is the portion of the Dirichlet prior for the
sense distribution ? in the current layer. Treating
each layer individually, we integrate over the pos-
sible values of ?, obtaining a similar count-based
term:
(5)
p(si|l,s?i,?l) =
Z
p(si|l,s?i,?) ? p(?| f?i,?l)d?=
#l(si)+?l
#l+S ??l
where #l(si) indicates the number of elements in
layer l assigned the sense si, #l indicates the num-
ber of elements in layer l, i.e., the size of the layer
and S the number of senses.
To distribute the pseudo counts represented by
? in a reasonable fashion among the layers, we
define ?l = #l#m ?? where #m = ?l #l, i.e., the total
size of the instance. This distributes ? according
to the relative size of each layer in the instance.
p(si|l,s?i,?l)=
#l(si)+ #l#m ??
#l+S ? #l#m ??
=
#m ? #l(si)#l +?
#m+S ??
(6)
Placing these values in Equation 4 we obtain the
following:
p(si|s?i,?) =
#m ??l ?l ?
#l(si)
#l +?
#m+S ??
(7)
Putting it all together, we arrive at the final update
equation for the Gibbs sampling:
p(si|s?i, f )?
#( fi,si)+?l
#(si)+Vl ??l
?
#m ??l ?l ?
#l(si)
#l +?
#m+S ??
(8)
Note that when dealing with a single layer, Equa-
tion 8 collapses to:
p(si|s?i, f ) ?
#( fi,si)+?
#(si)+V ??
?
#m(si)+?
#m+S ??
(9)
where #m(si) indicates the number of elements
(e.g., words) in the context window assigned to
sense si. This is identical to the update equation
in the original, word-based LDA model.
The sampling algorithm gives direct estimates
of s for every context element. However, in view
of our task, we are more interested in estimating ?,
the sense-context distribution which can be ob-
tained as in Equation 7, but taking into account
all sense assignments, without removing assign-
ment i. Our system labels each instance with the
single, most probable sense.
5 Evaluation Setup
In this section we discuss our experimental set-up
for assessing the performance of the model pre-
sented above. We give details on our training pro-
cedure, describe our features, and explain how our
system output was evaluated.
Data In this work, we focus solely on inducing
senses for nouns, since they constitute the largest
portion of content words. For example, nouns rep-
resent 45% of the content words in the British Na-
tional Corpus. Moreover, for many tasks and ap-
plications (e.g., web queries, Jansen et al 2000)
nouns are the most frequent and most important
part-of-speech.
For evaluation, we used the Semeval-2007
benchmark dataset released as part of the sense
induction and discrimination task (Agirre and
Soroa, 2007). The dataset contains texts from the
Penn Treebank II corpus, a collection of articles
from the first half of the 1989 Wall Street Jour-
nal (WSJ). It is hand-annotated with OntoNotes
senses (Hovy et al, 2006) and has 35 nouns. The
average noun ambiguity is 3.9, with a high (almost
80%) skew towards the predominant sense. This is
not entirely surprising since OntoNotes senses are
less fine-grained than WordNet senses.
We used two corpora for training as we wanted
to evaluate our model?s performance across differ-
ent domains. The British National Corpus (BNC)
is a 100 million word collection of samples of
written and spoken language from a wide range of
sources including newspapers, magazines, books
(both academic and fiction), letters, and school es-
says as well as spontaneous conversations. This
served as our out-of-domain corpus, and con-
tained approximately 730 thousand instances of
the 35 target nouns in the Semeval lexical sample.
The second, in-domain, corpus was built from se-
lected portions of the Wall Street Journal. We used
all articles (excluding the Penn Treebank II por-
tion used in the Semeval dataset) from the years
1987-89 and 1994 to create a corpus of similar size
to the BNC, containing approximately 740 thou-
sand instances of the target words.
Additionally, we used the Senseval 2 and 3 lex-
ical sample data (Preiss and Yarowsky, 2001; Mi-
halcea and Edmonds, 2004) as development sets,
for experimenting with the hyper-parameters of
our model (see Section 6).
Evaluation Methodology Agirre and Soroa
(2007) present two evaluation schemes for as-
sessing sense induction methods. Under the first
107
scheme, the system output is compared to the
gold standard using standard clustering evalua-
tion metrics (e.g., purity, entropy). Here, no at-
tempt is made to match the induced senses against
the labels of the gold standard. Under the second
scheme, the gold standard is partitioned into a test
and training corpus. The latter is used to derive a
mapping of the induced senses to the gold stan-
dard labels. The mapping is then used to calculate
the system?s F-Score on the test corpus.
Unfortunately, the first scheme failed to dis-
criminate among participating systems. The one-
cluster-per-word baseline outperformed all sys-
tems, except one, which was only marginally bet-
ter. The scheme ignores the actual labeling and
due to the dominance of the first sense in the data,
encourages a single-sense approach which is fur-
ther amplified by the use of a coarse-grained sense
inventory. For the purposes of this work, there-
fore, we focused on the second evaluation scheme.
Here, most of the participating systems outper-
formed the most-frequent-sense baseline, and the
rest obtained only slightly lower scores.
Feature Space Our experiments used a feature
set designed to capture both immediate local con-
text, wider context and syntactic context. Specifi-
cally, we experimented with six feature categories:
?10-word window (10w),?5-word window (5w),
collocations (1w), word n-grams (ng), part-of-
speech n-grams (pg) and dependency relations
(dp). These features have been widely adopted in
various WSD algorithms (see Lee and Ng 2002 for
a detailed evaluation). In all cases, we use the lem-
matized version of the word(s).
The Semeval workshop organizers provided a
small amount of context for each instance (usu-
ally a sentence or two surrounding the sentence
containing the target word). This context, as well
as the text in the training corpora, was parsed us-
ing RASP (Briscoe and Carroll, 2002), to extract
part-of-speech tags, lemmas, and dependency in-
formation. For instances containing more than one
occurrence of the target word, we disambiguate
the first occurrence. Instances which were not cor-
rectly recognized by the parser (e.g., a target word
labeled with the wrong lemma or part-of-speech),
were automatically assigned to the largest sense-
cluster.3
3This was the case for less than 1% of the instances.
3 4 5 6 7 8 9Number of Senses83
84
85
86
87
88
F
-
S
c
o
r
e
 
(
%
)
In-Domain (WSJ)Out-of-Domain (BNC)
Figure 3: Model performance with varying num-
ber of senses on the WSJ and BNC corpora.
6 Experiments
Model Selection The framework presented in
Section 3 affords great flexibility in modeling the
empirical data. This however entails that several
parameters must be instantiated. More precisely,
our model is conditioned on the Dirichlet hyper-
parameters ? and ? and the number of senses S.
Additional parameters include the number of iter-
ations for the Gibbs sampler and whether or not
the layers are assigned different weights.
Our strategy in this paper is to fix ? and ?
and explore the consequences of varying S. The
value for the ? hyperparameter was set to 0.02.
This was optimized in an independent tuning ex-
periment which used the Senseval 2 (Preiss and
Yarowsky, 2001) and Senseval 3 (Mihalcea and
Edmonds, 2004) datasets. We experimented with
? values ranging from 0.005 to 1. The ? parame-
ter was set to 0.1 (in all layers). This value is often
considered optimal in LDA-related models (Grif-
fiths and Steyvers, 2002). For simplicity, we used
uniform weights for the layers. The Gibbs sampler
was run for 2,000 iterations. Due to the random-
ized nature of the inference procedure, all reported
results are average scores over ten runs.
Our experiments used the same number of
senses for all the words, since tuning this number
individually for each word would be prohibitive.
We experimented with values ranging from three
to nine senses. Figure 3 shows the results obtained
for different numbers of senses when the model is
trained on the WSJ (in-domain) and BNC (out-of-
domain) corpora, respectively. Here, we are using
the optimal combination of layers for each system
(which we discuss in the following section in de-
108
Senses of drug (WSJ)
1. U.S., administration, federal, against, war, dealer
2. patient, people, problem, doctor, company, abuse
3. company, million, sale, maker, stock, inc.
4. administration, food, company, approval, FDA
Senses of drug (BNC)
1. patient, treatment, effect, anti-inflammatory
2. alcohol, treatment, patient, therapy, addiction
3. patient, new, find, effect, choice, study
4. test, alcohol, patient, abuse, people, crime
5. trafficking, trafficker, charge, use, problem
6. abuse, against, problem, treatment, alcohol
7. people, wonder, find, prescription, drink, addict
8. company, dealer, police, enforcement, patient
Table 1: Senses inferred for the word drug from
the WSJ and BNC corpora.
tail). For the model trained on WSJ, performance
peaks at four senses, which is similar to the av-
erage ambiguity in the test data. For the model
trained on the BNC, however, the best results are
obtained using twice as many senses. Using fewer
senses with the BNC-trained system can result in
a drop in accuracy of almost 2%. This is due to
the shift in domain. As the sense-divisions of the
learning domain do not match those of the target
domain, finer granularity is required in order to en-
compass all the relevant distinctions.
Table 1 illustrates the senses inferred for the
word drug when using the in-domain and out-of-
domain corpora, respectively. The most probable
words for each sense are also shown. Firstly, note
that the model infers some plausible senses for
drug on the WSJ corpus (top half of Table 1).
Sense 1 corresponds to the ?enforcement? sense
of drug, Sense 2 refers to ?medication?, Sense 3
to the ?drug industry? and Sense 4 to ?drugs re-
search?. The inferred senses for drug on the BNC
(bottom half of Table 1) are more fine grained. For
example, the model finds distinct senses for ?med-
ication? (Sense 1 and 7) and ?illegal substance?
(Senses 2, 4, 6, 7). It also finds a separate sense
for ?drug dealing? (Sense 5) and ?enforcement?
(Sense 8). Because the BNC has a broader fo-
cus, finer distinctions are needed to cover as many
senses as possible that are relevant to the target do-
main (WSJ).
Layer Analysis We next examine which indi-
vidual feature categories are most informative
in our sense induction task. We also investigate
whether their combination, through our layered
1-Layer
10w 86.9
5w 86.8
1w 84.6
ng 83.6
pg 82.5
dp 82.2
MFS 80.9
5-Layers
-10w 83.1
-5w 83.0
-1w 83.0
-ng 83.0
-pg 82.7
-dp 84.7
all 83.3
Combination
10w+5w 87.3%
5w+pg 83.9%
1w+ng 83.2%
10w+pg 83.3%
1w+pg 84.5%
10w+pg+dep 82.2%
MFS 80.9%
Table 2: Model performance (F-score) on the WSJ
with one layer (left), five layers (middle), and se-
lected combinations of layers (right).
model (see Figure 2), yields performance im-
provements. We used 4 senses for the system
trained on WSJ and 8 for the system trained on
the BNC (? was set to 0.02 and ? to 0.1)
Table 2 (left side) shows the performance of our
model when using only one layer. The layer com-
posed of words co-occurring within a ?10-word
window (10w), and representing wider, topical, in-
formation gives the highest scores on its own. It
is followed by the ?5 (5w) and ?1 (1w) word
windows, which represent more immediate, local
context. Part-of-speech n-grams (pg) and word n-
grams (ng), on their own, achieve lower scores,
largely due to over-generalization and data sparse-
ness, respectively. The lowest-scoring single layer
is the dependency layer (dp), with performance
only slightly above the most-frequent-sense base-
line (MFS). Dependency information is very infor-
mative when present, but extremely sparse.
Table 2 (middle) also shows the results obtained
when running the layered model with all but one
of the layers as input. We can use this informa-
tion to determine the contribution of each layer by
comparing to the combined model with all layers
(all). Because we are dealing with multiple lay-
ers, there is an element of overlap involved. There-
fore, each of the word-window layers, despite rel-
atively high informativeness on its own, does not
cause as much damage when it is absent, since
the other layers compensate for the topical and lo-
cal information. The absence of the word n-gram
layer, which provides specific local information,
does not make a great impact when the 1w and pg
layers are present. Finally, we can see that the ex-
tremely sparse dependency layer is detrimental to
the multi-layer model as a whole, and its removal
increases performance. The sparsity of the data in
this layer means that there is often little informa-
tion on which to base a decision. In these cases,
the layer contributes a close-to-uniform estimation
109
1-Layer
10w 84.6
5w 84.6
1w 83.6
pg 83.1
ng 82.8
dp 81.1
MFS 80.9
5-Layers
-10w 83.3
-5w 82.8
-1w 83.5
-pg 83.2
-ng 82.9
-dp 84.7
all 84.1
Combination
10w+5w 85.5%
5w+pg 83.5%
1w+ng 83.5%
10w+pg 83.4%
1w+pg 84.1%
10w+pg+dep 81.7%
MFS 80.9%
Table 3: Model performance (F-score) on the BNC
with one layer (left), five layers (middle), and se-
lected combinations of layers (right).
of the sense distribution, which confuses the com-
bined model.
Other layer combinations obtained similar re-
sults. Table 2 (right side) shows the most informa-
tive two and three layer combinations. Again, de-
pendencies tend to decrease performance. On the
other hand, combining features that have similar
performance on their own is beneficial. We obtain
the best performance overall with a two layered
model combining topical (+10w) and local (+5w)
contexts.
Table 3 replicates the same suite of experiments
on the BNC corpus. The general trends are similar.
Some interesting differences are apparent, how-
ever. The sparser layers, notably word n-grams
and dependencies, fare comparatively worse. This
is expected, since the more precise, local, infor-
mation is likely to vary strongly across domains.
Even when both domains refer to the same sense
of a word, it is likely to be used in a different
immediate context, and local contextual informa-
tion learned in one domain will be less effective
in the other. Another observable difference is that
the combined model without the dependency layer
does slightly better than each of the single layers.
The 1w+pg combination improves over its compo-
nents, which have similar individual performance.
Finally, the best performing model on the BNC
also combines two layers capturing wider (10w)
and more local (5w) contextual information (see
Table 3, right side).
Comparison to State-of-the-Art Table 4 com-
pares our model against the two best performing
sense induction systems that participated in the
Semeval-2007 competition. IR2 (Niu et al, 2007)
performed sense induction using the Information
Bottleneck algorithm, whereas UMND2 (Peder-
sen, 2007) used k-means to cluster second order
co-occurrence vectors associated with the target
System F-Score
10w, 5w (WSJ) 87.3
I2R 86.8
UMND2 84.5
MFS 80.9
Table 4: Comparison of the best-performing
Semeval-07 systems against our model.
word. These models and our own model signif-
icantly outperform the most-frequent-sense base-
line (p < 0.01 using a ?2 test). Our best sys-
tem (10w+5w on WSJ) is significantly better than
UMND2 (p < 0.01) and quantitatively better than
IR2, although the difference is not statistically sig-
nificant.
7 Discussion
This paper presents a novel Bayesian approach to
sense induction. We formulated sense induction
in a generative framework that describes how the
contexts surrounding an ambiguous word might
be generated on the basis of latent variables. Our
model incorporates features based on lexical in-
formation, parts of speech, and dependencies in a
principled manner, and outperforms state-of-the-
art systems. Crucially, the approach is not specific
to the sense induction task and can be adapted for
other applications where it is desirable to take mul-
tiple levels of information into account. For exam-
ple, in document classification, one could consider
an accompanying image and its caption as possi-
ble additional layers to the main text.
In the future, we hope to explore more rigor-
ous parameter estimation techniques. Goldwater
and Griffiths (2007) describe a method for inte-
grating hyperparameter estimation into the Gibbs
sampling procedure using a prior over possible
values. Such an approach could be adopted in our
framework, as well, and extended to include the
layer weighting parameters, which have strong po-
tential for improving the model?s performance. In
addition, we could allow an infinite number of
senses and use an infinite Dirichlet model (Teh
et al, 2006) to automatically determine how many
senses are optimal. This provides an elegant so-
lution to the model-order problem, and eliminates
the need for external cluster-validation methods.
Acknowledgments The authors acknowledge
the support of EPSRC (grant EP/C538447/1).
We are grateful to Sharon Goldwater for her feed-
back on earlier versions of this work.
110
References
Agirre, Eneko, Llu??s Ma`rquez, and Richard Wicentowski, ed-
itors. 2007. Proceedings of the SemEval-2007. Prague,
Czech Republic.
Agirre, Eneko and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimination
systems. In Proceedings of SemEval-2007. Prague, Czech
Republic, pages 7?12.
Barnard, K., P. Duygulu, D. Forsyth, N. De Freitas, D. M.
Blei, andM. I. Jordan. 2003. Matching words and pictures.
J. of Machine Learning Research 3(6):1107?1135.
Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003.
Latent dirichlet alocation. Journal of Machine Learning
Research 3:993?1022.
Bordag, Stefan. 2006. Word sense induction: Triplet-based
clustering and automatic evaluation. In Proceedings of the
11th EACL. Trento, Italy, pages 137?144.
Boyd-Graber, Jordan and David Blei. 2007. Putop: Turning
predominant senses into a topic model for word sense dis-
ambiguation. In Proceedings of SemEval-2007. Prague,
Czech Republic, pages 277?281.
Boyd-Graber, Jordan, David Blei, and Xiaojin Zhu. 2007.
A topic model for word sense disambiguation. In Pro-
ceedings of the EMNLP-CoNLL. Prague, Czech Republic,
pages 1024?1033.
Briscoe, Ted and John Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proceedings of the 3rd
LREC. Las Palmas, Gran Canaria, pages 1499?1504.
Cai, J. F., W. S. Lee, and Y. W. Teh. 2007. Improving word
sense disambiguation using topic features. In Proceedings
of the EMNLP-CoNLL. Prague, Czech Republic, pages
1015?1023.
Carpuat, Marine and Dekai Wu. 2005. Word sense disam-
biguation vs. statistical machine translation. In Proceed-
ings of the 43rd ACL. Ann Arbor, MI, pages 387?394.
Dorow, Beate and Dominic Widdows. 2003. Discovering
corpus-specific word senses. In Proceedings of the 10th
EACL. Budapest, Hungary, pages 79?82.
Firth, J. R. 1957. A Synopsis of Linguistic Theory 1930-1955.
Oxford: Philological Society.
Gauch, Susan and Robert P. Futrelle. 1993. Experiments in
automatic word class and word sense identification for in-
formation retrieval. In Proceedings of the 3rd Annual Sym-
posium on Document Analysis and Information Retrieval.
Las Vegas, NV, pages 425?434.
Geman, S. and D. Geman. 1984. Stochastic relaxation, Gibbs
distribution, and Bayesian restoration of images. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence 6(6):721?741.
Goldwater, Sharon and Tom Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In Pro-
ceedings of the 45th ACL. Prague, Czech Republic, pages
744?751.
Griffiths, Thomas L., Mark Steyvers, David M. Blei, and
Joshua B. Tenenbaum. 2005. Integrating topics and syn-
tax. In Lawrence K. Saul, Yair Weiss, and Le?on Bottou,
editors, Advances in Neural Information Processing Sys-
tems 17, MIT Press, Cambridge, MA, pages 537?544.
Griffiths, Tom L. and Mark Steyvers. 2002. A probabilistic
approach to semantic representation. In Proeedings of the
24th Annual Conference of the Cognitive Science Society.
Fairfax, VA, pages 381?386.
Hovy, Eduard, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In Proceedings of the HLT, Companion Vol-
ume: Short Papers. Association for Computational Lin-
guistics, New York City, USA, pages 57?60.
Jansen, B. J., A. Spink, and A. Pfaff. 2000. Linguistic aspects
of web queries.
Lee, Yoong Keok and Hwee Tou Ng. 2002. An empirical
evaluation of knowledge sources and learning algorithms
for word sense disambiguation. In Proceedings of the
EMNLP. Morristown, NJ, USA, pages 41?48.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Finding predominant senses in untagged text.
In Proceedings of the 42nd ACL. Barcelona, Spain, pages
280?287.
Mihalcea, Rada and Phil Edmonds, editors. 2004. Proceed-
ings of the SENSEVAL-3. Barcelona.
Niu, Zheng-Yu, Dong-Hong Ji, and Chew-Lim Tan. 2007.
I2r: Three systems for word sense discrimination, chinese
word sense disambiguation, and english word sense dis-
ambiguation. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007). As-
sociation for Computational Linguistics, Prague, Czech
Republic, pages 177?182.
Pantel, Patrick and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of the 8th KDD. New
York, NY, pages 613?619.
Pedersen, Ted. 2007. Umnd2 : Senseclusters applied to the
sense induction task of senseval-4. In Proceedings of
SemEval-2007. Prague, Czech Republic, pages 394?397.
Preiss, Judita and David Yarowsky, editors. 2001. Proceed-
ings of the 2nd International Workshop on Evaluating
Word Sense Disambiguation Systems. Toulouse, France.
Purandare, Amruta and Ted Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and similarity
spaces. In Proceedings of the CoNLL. Boston, MA, pages
41?48.
Schu?tze, Hinrich. 1998. Automatic word sense discrimina-
tion. Computational Linguistics 24(1):97?123.
Teh, Y. W., M. I. Jordan, M. J. Beal, and D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the American
Statistical Association 101(476):1566?1581.
Ve?ronis, Jean. 2004. Hyperlex: lexical cartography for
information retrieval. Computer Speech & Language
18(3):223?252.
Vickrey, David, Luke Biewald, Marc Teyssier, and Daphne
Koller. 2005. Word-sense disambiguation for machine
translation. In Proceedings of the HLT/EMNLP. Vancou-
ver, pages 771?778.
Voorhees, Ellen M. 1993. Using wordnet to disambiguate
word senses for text retrieval. In Proceedings of the 16th
SIGIR. New York, NY, pages 171?180.
111
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1214?1222,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
It Depends on the Translation:
Unsupervised Dependency Parsing via Word Alignment
Samuel Brody
Dept. of Biomedical Informatics
Columbia University
samuel.brody@dbmi.columbia.edu
Abstract
We reveal a previously unnoticed connection
between dependency parsing and statistical
machine translation (SMT), by formulating
the dependency parsing task as a problem of
word alignment. Furthermore, we show that
two well known models for these respective
tasks (DMV and the IBM models) share com-
mon modeling assumptions. This motivates us
to develop an alignment-based framework for
unsupervised dependency parsing. The frame-
work (which will be made publicly available)
is flexible, modular and easy to extend. Us-
ing this framework, we implement several al-
gorithms based on the IBM alignment mod-
els, which prove surprisingly effective on the
dependency parsing task, and demonstrate the
potential of the alignment-based approach.
1 Introduction
Both statistical machine translation (SMT) and un-
supervised dependency parsing have seen a surge of
interest in recent years, as the need for large scale
data processing has increased. The problems ad-
dressed by each of the fields seem quite different
at first glance. However, in this paper, we reveal a
strong connection between them and show that the
problem of dependency parsing can be formulated
as one of word alignment within the sentence. Fur-
thermore, we show that the two models that are ar-
guably the most influential in their respective fields,
the IBM models 1-3 (Brown et al, 1993) and Klein
and Manning?s (2004) Dependency Model with Va-
lence (DMV), share a common set of modeling as-
sumptions.
Based on this connection, we develop a frame-
work which uses an alignment-based approach for
unsupervised dependency parsing. The framework
is flexible and modular, and allows us to explore dif-
ferent modeling assumptions. We demonstrate these
properties and the merit of the alignment-based pars-
ing approach by implementing several dependency
parsing algorithms based on the IBM alignment
models and evaluating their performance on the task.
Although the algorithms are not competitive with
state-of-the-art systems, they outperform the right-
branching baseline and approach the performance of
DMV. This is especially surprising when we con-
sider that the IBM models were not originally de-
signed for the task. These results are encourag-
ing and indicate that the alignment-based approach
could serve as the basis for competitive dependency
parsing systems, much as DMV did.
This paper offers two main contributions. First,
by revealing the connection between the two tasks,
we introduce a new approach to dependency pars-
ing, and open the way for use of SMT alignment re-
sources and tools for parsing. Our experiments with
the IBMmodels demonstrate the potential of this ap-
proach and provide a strong motivation for further
development. The second contribution is a publicly-
available framework for exploring new alignment
models. The framework uses Gibbs sampling tech-
niques and includes our sampling-based implemen-
tations of the IBM models (see Section 3.4). The
sampling approach makes it easy to modify the ex-
isting models and add new ones. The framework
can be used both for dependency parsing and for bi-
lingual word alignment.
The rest of the paper is structured as follows. In
Section 2 we present a brief overview of those works
in the fields of dependency parsing and alignment
for statistical machine translation which are directly
1214
relevant to this paper. Section 3 describes the con-
nection between the two problems, examines the
shared assumptions of the DMV and IBM models,
and describes our framework and algorithms. In
Section 4 we present our experiments and discuss
the results. We conclude in Section 5.
2 Background and Related Work
2.1 Unsupervised Dependency Parsing
In recent years, the field of supervised parsing has
advanced tremendously, to the point where highly
accurate parsers are available for many languages.
However, supervised methods require the manual
annotation of training data with parse trees, a pro-
cess which is expensive and time consuming. There-
fore, for domains and languages with minimal re-
sources, unsupervised parsing is of great impor-
tance.
Early work in the field focused on models that
made use primarily of the co-occurrence informa-
tion of the head and its argument (Yuret, 1998;
Paskin, 2001). The introduction of DMV by Klein
and Manning (2004) represented a shift in the di-
rection of research in the field. DMV is based on
a linguistically motivated generative model, which
follows common practice in supervised parsing and
takes into consideration the distance between head
and argument, as well as the valence (the capac-
ity of a head word to attach arguments). Klein and
Manning (2004) also shifted from a lexical repre-
sentation of the sentences to representing them as
part-of-speech sequences. DMV strongly outper-
formed previous models and was the first unsuper-
vised dependency induction system to achieve accu-
racy above the right-branching baseline. Much sub-
sequent work in the field has focused on modifica-
tions and extensions of DMV, and it is the basis for
today?s state-of-the-art systems (Cohen and Smith,
2009; Headden III et al, 2009).
2.2 Alignment for SMT
SMT treats translation as a machine learning prob-
lem. It attempts to learn a translation model from
a parallel corpus composed of sentences and their
translations. The IBM models (Brown et al, 1993)
represent the first generation of word-based SMT
models, and serve as a starting point for most cur-
Figure 1: An example of an alignment between an En-
glish sentence (top) and its French translation (bottom).
rent SMT systems (e.g., Moses, Koehn et al 2007;
Hiero, Chiang 2005). The models employ the notion
of alignment between individual words in the source
and translation. An example of such an alignment is
given in Figure 1.
The IBM models all seek to maximize Pr( f |e),
the probability of a French translation f of an En-
glish sentence e. This probability is broken down
by taking into account all possible alignments a be-
tween e and f , and their probabilities:
Pr( f |e) =?
a
Pr( f ,a|e) (1)
Each of the IBMmodels is based on the previous one
in the series, and adds another level of latent parame-
ters which take into account a specific characteristic
of the data.
3 Alignment-based Dependency Parsing
3.1 The Connection
The task of dependency parsing requires finding a
parse tree for a sentence, where two words are con-
nected by an edge if they participate in a syntactic
dependency relation. When dealing with unlabeled
dependencies, the exact nature of the relationship is
not determined. An example of a dependency parse
of a sentence is given in Figure 2 (left).
Another possible formulation of the problem is as
follows. Find a set of pairwise relations (si,s j) con-
necting a dependent word s j with its head word si in
the sentence. This alternate formulation allows us to
view the problem as one of alignment of a sentence
to itself, as shown in Figure 2 (right).
Given this perspective on the problem, it makes
sense to examine existing alignment models, com-
pare them to dependency parsing models, and see
if they can be successfully employed for the depen-
dency parsing task.
1215
Figure 2: Left: An example of an unlabeled dependency parse of a sentence. Right: The same parse, in the form of
an alignment between a head words (top) and their dependents (bottom).
3.2 Comparing IBM & DMV Assumptions
Lexical Association The core assumption of IBM
Model 1 is that the lexical identities of the En-
glish and French words help determine whether they
should be aligned. The same assumption is made in
all the dependency models mentioned in Section 2
regarding a head and its dependent (although DMV
uses word classes instead of the actual words).
Location IBM Model 2 adds the consideration
of difference in location between the English and
French words when considering the likelihood of
alignment. One of the improvements contributing
to the success of DMV was the notion of distance,
which was absent from previous models (see Sec-
tion 3 in Klein and Manning 2004).
Fertility IBM Model 3 adds the notion of fertil-
ity, or the idea that different words in the source lan-
guage tend to generate different numbers of words in
the target language. This corresponds to the notion
of valence, used by Klein and Manning (2004), and
the other major contributor to the success of DMV
(ibid.).
Null Source The IBM models all make use of
an additional ?null? word in every sentence, which
has special status. It is attached to words in the
translation that do not correspond to a word in the
source. It is treated separately when calculating
distance (since it has no location) and fertility. In
these characteristics, it is very similar to the ?root?
node, which is artificially added to parse trees and
used to represent the head of words which are not
dependents of any other word in the sentence.
In examining the core assumptions of the IBM
models, we note that there is a strong resemblance
to those of DMV. The similarity is at an abstract
level since the nature of the relationship that each
model attempts to detect is quite different. The
IBMmodels look for an equivalence relationship be-
tween lexical items in two languages, whereas DMV
addresses functional relationships between two el-
ements with distinct meanings. However, both at-
tempt to model a similar set of factors, which they
posit will be important to their respective tasks1.
This similarity motivates the work presented in the
rest of the paper, i.e, exploring the use of the IBM
alignment models for dependency parsing. It is im-
portant to note that the IBM models do not address
many important factors relevant to the parsing task.
For instance, they have no notion of a parse tree, a
deficit which may lead to degenerate solutions and
malformed parses. However, they serve as a good
starting point for exploring the alignment approach
to parsing, as well as discovering additional factors
that need to be addressed under this approach.
3.3 Experimental Framework
We developed a Gibbs sampling framework for
alignment-based dependency parsing2. The tradi-
tional approach to alignment uses Expectation Max-
imization (EM) to find the optimal values for the
latent variables. In each iteration, it considers all
possible alignments for each pair of sentences, and
1These abstract notions (lexical association, proximity, ten-
dencies towards few or many relations, and allowing for unasso-
ciated items) play an important role in many relation-detection
tasks (e.g., co-reference resolution, Haghighi and Klein 2010).
2Available for download at:
http://people.dbmi.columbia.edu/?sab7012
1216
chooses the optimal one based on the current pa-
rameter estimates. The sampling method, on the
other hand, only considers a small change in each
step - that of re-aligning a previously aligned target
word to a new source. The reason for our choice is
the ease of modification of such sampling models.
They allow for easy introduction of further param-
eters and more complex probabilistic functions, as
well as Bayesian priors, all of which are likely to be
helpful in development3.
Under the sampling framework, the model pro-
vides the probability of changing the alignment A[i]
of a target word i from a previously aligned source
word j to a new one j?. In all the models we consider,
this probability is proportional to the ratio between
the scores of the old sentence alignment A and the
new one A?, which differs from the old only in the
realignment of i to j?.
P(A[i] = j ? A[i] = j? ) ?
Pmodel(A?)
Pmodel(A)
(2)
As a starting point for our dependency parsing
model, we re-implemented the first three IBM mod-
els 4 in the sampling framework.
3.4 Reformulating the IBM models
IBM Model 1 According to this model, the prob-
ability of an alignment between target word i and
source word j? depends only on the lexical identities
of the two words wi and w j? respectively. This gives
us equation 3.
P(A[i] ? j? ) ?
Pmodel(A?)
Pmodel(A)
=
?kP(wk,wA[k])
?k? P(wk? ,wA?[k?])
P(A[i] ? j? ) ?
P(wi,w j?)
P(wi,w j)
(3)
In our implementation we assume the alignment
follows a Chinese Restaurant Process (CRP), where
3Preliminary experiments using the EM approach via the
GIZA++ toolkit (Och and Ney, 2003) resulted in similar per-
formance to that of the sampling method for IBM Models 1 and
2. However, we were unable to explore the use of Model 3
under that framework, since the implementation of the model
was strongly coupled to other, SMT-specific, optimizations and
heuristics.
4Our implementation, as well as some core components in
our framework, are based on code kindly provided by Chris
Dyer.
the probability of wi aligning to w j is proportional
to the number of times they have been aligned in the
past (the rest of the data), as follows:
P(wi,w j?) =
#(wi,w j)+?1/V
#(?,w j)+?1
(4)
Here, #(wi,w j) represents the number of times the
target word wi was observed to be aligned to w j in
the rest of the data, and ? stands for any word, V
is the size of the vocabulary, and ?1 is a hyperpa-
rameter of the CRP, which can also be viewed as a
smoothing factor.
IBM Model 2 The original IBM model 2 is a
distortion model that assumes that the probability
of an alignment between target word i and source
word j? depends only on the locations of the words,
i.e., the values i and j?, taking into account the dif-
ferent lengths l and m of the source and target sen-
tences, respectively. For dependency parsing, where
we align sentences to themselves, l = m. This gives
us equation 5.
P(A[i] ? j? ) ?
Pmodel(A?)
Pmodel(A)
=
P(i, j?, l)
P(i, j, l)
P(i, j?, l) =
#(i, j?, l)+?2/D
#(i,?, l)+?2
(5)
Again, we assume a CRP when choosing a dis-
tortion value, where D is the expected number of
distance values (set to 10 in our experiments), ?2
is the CRP hyperparameter, #(i, j, l) is the number
of times a target word in position i was aligned to
a source word in position j in sentences of length l,
and #(i,?, l) is the number of times word in position
i was aligned (to any source position) in sentences
of length l.
Even without the need for handling different
lengths for source and target sentences, this model
is complex and requires estimating a separate prob-
ability for each triplet (i, j, l). In addition, the as-
sumption that the distance distribution depends only
on the sentence length and is similar for all to-
kens seems unreasonable, especially when dealing
with part-of-speech tokens and dependency rela-
tions. Such concerns have been mentioned in the
SMT literature and were shown to be justified in
our experiments (see Sec. 4). For this reason, we
1217
also implemented an alternate distance model, based
loosely on Liang et al (2006). Under the alternate
model, the probability of an alignment between tar-
get word i and source word j? depends on the distance
between them, their order, the sentence length, and
the word type of the head, according to equation 6.
P(i, j?, l) =
#[wi,(i- j?), l]+?3/D
#(wi,?, l)+?3
(6)
IBM Model 3 This model handles the notion of
fertility (or valence). Under this model, the proba-
bility of an alignment depends on how many target
words are aligned to each of the source words. Each
source word type w j?, has a distribution specifying
the probability of having n aligned target words. The
probability of an alignment is proportional to the
product of the probabilities of the fertilities in the
alignment and takes into account the special status of
the null word (represented by the index j = 0). This
probability is given in Equation 7, which is based on
Equation 32 in Brown et al (1993)5.
P(A) ?
(
l??0
?0
)
pl?2?00 p
?0
1
l
?
j=1
? j!
#(w j,? j)+?4/F
#(w j,?)+?4
(7)
Here, ? j denotes the number of target words aligned
to the j-th source word in alignment A. p1 and p0
sum to 1 and are used to derive the probability that
there will be ?0 null-aligned words in a sentence
containing l words6. #(w j,? j) represents the num-
ber of times source word w j was observed to have
? j dependent target words, #(w j,?) is the number
of times w j appeared in the data, F is the expected
number of fertility values (5 in our experiments),
and ?4 is the CRP hyperparameter.
Combining the Models The original IBM mod-
els work in an incremental fashion, with each model
using the output of the previous one as a starting
point and adding a new component to the probabil-
ity distribution. The dependency parsing framework
employs a similar approach. It uses the alignments
5The transitional version of this equation depends on
whether either the old source word ( j) or the new one ( j?) are
null, and is omitted for brevity. Further details can be found in
Brown et al (1993) Section 4.4 and Equation 43.
6For details, see Brown et al (1993) Equation 31.
learned by the previous model as the starting point of
the next and combines the probability distributions
of each component via a product model. This al-
lows for the easy introduction of new models which
consider different aspects of the alignment and com-
plement each other.
Preventing Self-Alignment When adapting the
alignment approach to dependency parsing, we view
the task as that of aligning a sentence to itself. One
issue we must address is preventing the degenerate
solution of aligning each word to itself. For this pur-
pose we introduce a simple model into the product
which gives zero probability to alignments which
contain a word aligned to itself, as in equation 8.
P(A[i] = j? ) =
{
0 if i = j?
1
l?1 otherwise
(8)
4 Experiments
4.1 Data
We evaluated our model on several corpora. The first
of these was the Penn. Treebank portion of the Wall
Street Journal (WSJ). We used the Constituent-to-
Dependency Conversion Tool7 to convert the tree-
bank format into CoNLL format.
We also made use of the Danish and Dutch
datasets from the CoNLL 2006 shared task8. Since
we do not make use of annotation, we can induce a
dependency structure on the entire dataset provided
(disregarding the division into training and testing).
Following Klein and Manning (2004), we used
the gold-standard part-of-speech sequences rather
than the lexical forms and evaluated on sentences
containing 10 or fewer tokens after removal of punc-
tuation.
4.2 Results
Table 1 shows the results of the IBM Models on
the task of directed (unlabeled) dependency parsing.
We compare to the right-branching baseline used by
Klein and Manning (2004). For the WSJ10 corpus,
the authors reported 43.2% accuracy for DMV and
33.6% for the baseline. Although there are small
7nlp.cs.lth.se/software/treebank converter/
8http://nextens.uvt.nl/?conll/
1218
Corpus M 1 M2 M3 R-br
WSJ10 25.42 35.73 39.32 32.85
Dutch10 25.17 32.46 35.28 28.42
Danish10 23.12 25.96 41.94 16.05 *
Table 1: Percent accuracy of IBMModels 1-3 (M1-3) and
the right-branching baseline (R-br) on several corpora.
PoS attachment
NN DET
IN NN
NNP NNP
DET NN
JJ NN
PoS attachment
NNS JJ
RB VBZ
VBD NN
VB TO
CC NNS
Table 2: Most likely dependency attachment for the top
ten most common parts-of-speech, according to Model 1.
differences in evaluation, as evidenced by the dif-
ference between our baseline scores, IBM Models
2 and 3 outperform the baseline by a large margin
and Model 3 approaches the performance of DMV.
On the Dutch and Danish datasets, the trends are
similar. On the latter dataset, even Model 1 out-
performs the right-branching baseline. However, the
Danish dataset is unusual (see Buchholz and Marsi
2006) in that the alternate adjacency baseline of left-
branching (also mentioned by Klein and Manning
2004) is extremely strong and achieves 48.8% di-
rected accuracy.
4.3 Analysis
In order to better understand what our alignment
model was learning, we looked at each component
element individually.
Lexical Association To explore what Model 1 was
learning, we analyzed the resulting probability ta-
bles for association between tokens. Table 2 shows
the most likely dependency attachment for the top
ten most common parts-of-speech. The model is
clearly learning meaningful connections between
parts of speech (determiners and adjectives to nouns,
adverbs to verbs, etc.), but there is little notion of
directionality, and cycles can exist. For instance,
the model learns the connection between determiner
and noun, but is unsure which is the head and which
the dependent. A similar connection is learned be-
tween to and verbs in the base form (VB). This in-
consistency is, to a large extent, the result of the
deficiencies of the model, stemming from the fact
that the IBM models were designed for a different
task and are not trying to learn a well-formed tree.
However, there is a strong linguistic basis to con-
sider the directionality of these relations difficult.
There is some debate among linguists as to whether
the head of a noun phrase is the noun or the deter-
miner9 (see Abney 1987). Each can be seen as a dif-
ferent kind of head element, performing a different
function, similarly to the multiple types of depen-
dency relations identified in Hudson?s (1990) Word
Grammar. A similar case can be made regarding the
head of an infinitive phrase. The infinitive form of
the verb may be considered the lexical head, deter-
mining the predicate, while to can be seen as the
functional head, encoding inflectional features, as in
Chomsky?s (1981) Government & Binding model10.
Distance Models The original IBM distortion
model (Model 2), which does not differentiate be-
tween words types and looks only at positions, has
an accuracy of 33.43% on the WSJ10 corpus. In
addition, it tends to strongly favor left-branching at-
tachment (57.2% of target words were attached to
the word immediately to their right, 22.6% to their
left, as opposed to 31% and 25.8% in the gold stan-
dard). The alternative distance model we proposed,
which takes into account the identity of the head
word, achieves better accuracy and is closer to the
gold standard balance (43.5% right and 35.3% left).
Figure 3 shows the distribution of the location of
the dependent relative to the head word (at position
0) for several common parts-of-speech. It is inter-
esting to see that singular and plural nouns (NN,
NNS) behave similarly. They both have a strong
preference for local attachment and a tendency to-
wards a left-dependent (presumably the determiner,
see above Table 2). Pronouns (NNP), on the other
hand, are more likely to attach to the right since
they are not modified by determiners. Verbs in past
(VBZ) and present (VBD, VBP) forms have simi-
lar behavior, with a flatter distribution of dependent
locations, whereas the base form (VB) attaches al-
most exclusively to the preceding token, presumably
9In fact, the original DMV chose the determiner as the head
(see discussion in Klein and Manning 2004, Section 3).
10We thank an anonymous reviewer for elucidating this point.
1219
Figure 3: Distribution of head-to-dependent distance for several types of verbs (left) and nouns (right), as learned by
our alternate distance model.
to (see Table 2).
Fertility Figure 4 shows the distribution of fertil-
ity values for several common parts of speech. Verbs
have a relatively flat distribution with a longer tail
as compared to nouns, which means they are likely
to have a larger number of arguments. Once again,
the base form (VB) exhibits different behavior from
the other verbs forms, taking almost exclusively one
argument. This is likely an effect of the strong con-
nection between base form verbs and the preceding
word to.
Hyper-Parameters Each of our models requires a
value for its CRP hyperparameter (see Section 3.4).
In this work, since parameter estimation was not our
focus, we set the hyperparameters to be approxi-
mately 1K , where K is the number of possible val-
ues, according to the rule of thumb common in the
literature. Specifically, we chose ?1 = 0.01,?3 =
0.05,?4 = 0.1. We investigated the effect of these
choices on performance in a separate set of exper-
iments, which showed that small variations (up to
an order of magnitude) in these parameters had little
effect on the results.
In addition to the CRP parameters, Model 3 re-
quires a value for p1, the null fertility hyperparame-
ter. In our experiments, we found that this hyper-
parameter had a very strong effect on results if it
was above 0.1, creating many spurious null align-
ments. However, below that threshold, the effects
were small. In the experiments reported here, we set
p1 = 0.01.
Initialization One issue with DMV, which is of-
ten mentioned, is its sensitivity to initialization. We
tested our model with random initialization (uniform
alignment probabilities) and with an approximation
of the ad-hoc ?harmonic? initialization described in
Klein and Manning (2004) and found no noticeable
difference in accuracy.
4.4 Discussion
The accuracy achieved by the IBM models (Table 1)
is surprisingly high, given the fact that the IBM
models were not designed with dependency parsing
in mind. It is likely that customizing the models to
the task will result in even better performance. Our
findings in Section 4.3 support this hypothesis. The
analysis showed that the lack of tree structure in the
model impacted the learning, and therefore it is ex-
pected that a component which enforces tree struc-
ture (prevents cycles) will be beneficial.
Although it lacks an inherent notion of tree struc-
ture, the alignment-based approach has several ad-
vantages over the head-outward approach of DMV
and related models. It can consider the alignment as
a whole and take into account global sentence con-
straints, not just head-dependent relations. These
may also include tree-structure constraints common
to the head-outward approaches, but can be more
flexible in how they are addressed. For instance,
1220
Figure 4: Distribution of fertility values for several types of verbs (left) and nouns (right), as learned by IBM Model 3.
DMV?s method of modeling tree structure does
not allow non-projective dependencies, whereas an
alignment-based model may choose to allow or con-
strain non-projectivity, as learned from the data. An-
other advantage of our alignment-based models is
the fact that they are not strongly sensitive to ini-
tialization and can be started from a set of random
alignments.
5 Conclusions and Future Work
We have described an alternative formulation of de-
pendency parsing as a problem of word alignment.
This connection motivated us to explore the possi-
bility of using alignment tools for the task of un-
supervised dependency parsing. We chose to ex-
periment with the well-known IBM alignment mod-
els which share a set of similar modeling assump-
tions with Klein and Manning?s (2004) Dependency
Model with Valence. Our experiments showed that
the IBM models are surprisingly effective at the
dependency parsing task, outperforming the right-
branching baseline and approaching the accuracy of
DMV. Our results demonstrate that the alignment
approach can be used as a foundation for depen-
dency parsing algorithms and motivates further re-
search in this area.
There are many interesting avenues for further re-
search. These include improving and extending the
existing IBM models, as well as introducing new
models that are specifically designed for the parsing
task and represent relevant linguistic considerations
(e.g., enforcing tree structure, handling crossing de-
pendencies, learning left- or right-branching tenden-
cies).
In Spitkovsky et al (2010), the authors show that
a gradual increase in the complexity of the data can
aid the learning process. The IBM approach demon-
strated the benefit of a gradual increase of model
complexity. It would be interesting to see if the two
approaches could be successfully combined.
Finally, although we use our framework for de-
pendency parsing, the sampling approach and the
framework we developed can be used to explore new
models for bilingual word alignment. Furthermore,
an alignment-based parsing method is expected to
integrate well with SMT bi-lingual alignment mod-
els and may, therefore, be suitable for combined
models which use parse trees to improve word align-
ment (e.g., Burkett et al 2010).
Acknowledgments
I would like to thank Chris Dyer for providing the
basis for the sampling implementation. I would
also like to thank Chris, Adam Lopez, Trevor Cohn,
Adam Faulkner and the anonymous reviewers for
their time and effort and their helpful comments and
suggestions.
References
Abney, Steven. 1987. The English Noun Phrase in its
Sentential Aspect. Ph.D. thesis, Massachusetts Insti-
1221
tute of Technology.
Brown, Peter F., Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: parameter
estimation. Comput. Linguist. 19(2):263?311.
Buchholz, Sabine and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In In Proc.
of CoNLL. pages 149?164.
Burkett, David, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In North American Association for Com-
putational Linguistics. Los Angeles.
Chiang, David. 2005. A hierarchical phrase-based model
for statistical machine translation. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics. Association for Com-
putational Linguistics, Morristown, NJ, USA, pages
263?270.
Chomsky, Noam. 1981. Lectures on government and
binding : the Pisa lectures / Noam Chomsky.
Cohen, Shay B. and Noah A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In NAACL ?09: Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics. Associa-
tion for Computational Linguistics, Morristown, NJ,
USA, pages 74?82.
Haghighi, Aria and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics. Association for
Computational Linguistics, Los Angeles, California,
pages 385?393.
Headden III, William P., Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics. Asso-
ciation for Computational Linguistics, Boulder, Col-
orado, pages 101?109.
Hudson, R. 1990. English Word Grammar. Basil Black-
well, Oxford.
Klein, Dan and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL ?04: Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics. Association for Computational
Linguistics, Morristown, NJ, USA, page 478.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In ACL ?07:
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions. As-
sociation for Computational Linguistics, Morristown,
NJ, USA, pages 177?180.
Liang, Percy, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference. Association for Computational Linguis-
tics, New York City, USA, pages 104?111.
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics 29(1):19?51.
Paskin, Mark A. 2001. Grammatical bigrams. In
Thomas G. Dietterich, Suzanna Becker, and Zoubin
Ghahramani, editors, NIPS. MIT Press, pages 91?97.
Spitkovsky, Valentin I., Hiyan Alshawi, and Daniel Juraf-
sky. 2010. From Baby Steps to Leapfrog: How ?Less
is More? in unsupervised dependency parsing. In Proc.
of NAACL-HLT .
Yuret, D. 1998. Discovery of linguistic relations using
lexical attraction. Ph.D. thesis, Department of Com-
puter Science and Electrical Engineering, MIT.
1222
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 562?570,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!
Using Word Lengthening to Detect Sentiment in Microblogs
Samuel Brody
School of Communication
and Information
Rutgers University
sdbrody@gmail.com
Nicholas Diakopoulos
School of Communication
and Information
Rutgers University
diakop@rutgers.edu
Abstract
We present an automatic method which lever-
ages word lengthening to adapt a sentiment
lexicon specifically for Twitter and similar so-
cial messaging networks. The contributions of
the paper are as follows. First, we call at-
tention to lengthening as a widespread phe-
nomenon in microblogs and social messag-
ing, and demonstrate the importance of han-
dling it correctly. We then show that lengthen-
ing is strongly associated with subjectivity and
sentiment. Finally, we present an automatic
method which leverages this association to de-
tect domain-specific sentiment- and emotion-
bearing words. We evaluate our method by
comparison to human judgments, and analyze
its strengths and weaknesses. Our results are
of interest to anyone analyzing sentiment in
microblogs and social networks, whether for
research or commercial purposes.
1 Introduction
Recently, there has been a surge of interest in sen-
timent analysis of Twitter messages. Many re-
searchers (e.g., Bollen et al 2011; Kivran-Swaine
and Naaman 2011) are interested in studying struc-
ture and interactions in social networks, where senti-
ment can play an important role. Others use Twitter
as a barometer for public mood and opinion in di-
verse areas such as entertainment, politics and eco-
nomics. For example, Diakopoulos and Shamma
(2010) use Twitter messages posted in conjunction
with the live presidential debate between Barack
Obama and John McCain to gauge public opinion,
Bollen et al (2010) measure public mood on Twitter
and use it to predict upcoming stock market fluc-
tuations, and O?Connor et al (2010) connect pub-
lic opinion data from polls to sentiment expressed in
Twitter messages along a timeline.
A prerequisite of all such research is an effec-
tive method for measuring the sentiment of a post
or tweet. Due to the extremely informal nature of
the medium, and the length restriction1, the lan-
guage and jargon which is used in Twitter varies sig-
nificantly from that of commonly studied text cor-
pora. In addition, Twitter is a quickly evolving
domain, and new terms are constantly being intro-
duced. These factors pose difficulties to methods
designed for conventional domains, such as news.
One solution is to use human annotation. For exam-
ple, Kivran-Swaine and Naaman (2011) use manual
coding of tweets in several emotion categories (e.g.,
joy, sadness) for their research. Diakopoulos and
Shamma (2010) use crowd sourcing via Amazon?s
Mechanical Turk. Manual encoding usually offers
a deeper understanding and correspondingly higher
accuracy than shallow automatic methods. However,
it is expensive and labor intensive and cannot be ap-
plied in real time. Crowd-sourcing carries additional
caveats of its own, such as issues of annotator exper-
tise and reliability (see Diakopoulos and Shamma
2010).
The automatic approach to sentiment analysis is
commonly used for processing data from social net-
works and microblogs, where there is often a huge
quantity of information and a need for low latency.
Many automatic approaches (including all those
used in the work mentioned above) have at their core
a sentiment lexicon, containing a list of words la-
1Messages in Twitter are limited to 140 characters, for com-
patibility with SMS messaging
562
beled with specific associated emotions (joy, hap-
piness) or a polarity value (positive, neutral, nega-
tive). The overall sentiment of a piece of text is cal-
culated as a function of the labels of the component
words. Because Twitter messages are short, shal-
low approaches are sometimes considered sufficient
(Bermingham and Smeaton, 2010). There are also
approaches that use deeper machine learning tech-
niques to train sentiment classifiers on examples that
have been labeled for sentiment, either manually or
automatically, as described above. Recent examples
of this approach are Barbosa and Feng (2010) and
Pak and Paroubek (2010).
Most established sentiment lexicons (e.g., Wilson
et al 2005, see Section 5) were created for a gen-
eral domain, and suffer from limited coverage and
inaccuracies when applied to the highly informal do-
main of social networks communication. By cre-
ating a sentiment lexicon which is specifically tai-
lored to the microblogging domain, or adapting an
existing one, we can expect to achieve higher accu-
racy and increased coverage. Recent work in this
area includes Velikovich et al (2010), who devel-
oped a method for automatically deriving an exten-
sive sentiment lexicon from the web as a whole.
The resulting lexicon has greatly increased cover-
age compared to existing dictionaries and can handle
spelling errors and web-specific jargon. Bollen et al
(2010) expand an existing well-vetted psychometric
instrument - Profile of Mood States (POMS) (Mc-
Nair et al, 1971) that associates terms with moods
(e.g. calm, happy). The authors use co-occurrence
information from the Google n-gram corpus (Brants
and Franz, 2006) to enlarge the original list of 72
terms to 964. They use this expanded emotion lexi-
con (named GPOS) in conjunction with the lexicon
of Wilson et al (2005) to estimate public mood from
Twitter posts2.
The method we present in this paper leverages a
phenomenon that is specific to informal social com-
munication to enable the extension of an existing
lexicon in a domain specific manner.
2Although the authors state that all data and methods will
be made available on a public website, it was not present at the
time of the writing of this article.
2 Methodology
Prosodic indicators (such as high pitch, prolonged
duration, intensity, vowel quality, etc.) have long
been know (Bolinger, 1965) as ways for a speaker to
emphasize or accent an important word. The ways
in which they are used in speech are the subject of
ongoing linguistic research (see, for example, Cal-
houn 2010). In written text, many of these indica-
tors are lost. However, there exist some orthographic
conventions which are used to mark or substitute
for prosody, including punctuation and typographic
styling (italic, bold, and underlined text). In purely
text-based domains, such as Twitter, styling is not
always available, and is replaced by capitalization
or other conventions (e.g., enclosing the word in as-
terisks). Additionally, the informal nature of the do-
main leads to an orthographic style which is much
closer to the spoken form than in other, more formal,
domains. In this work, we hypothesize that the com-
monly observed phenomenon of lengthening words
by repeating letters is a substitute for prosodic em-
phasis (increased duration or change of pitch). As
such, it can be used as an indicator of important
words and, in particular, ones that bear strong in-
dication of sentiment.
Our experiments are designed to analyze the phe-
nomenon of lengthening and its implications to sen-
timent detection. First, in Experiment I, we show
the pervasiveness of the phenomenon in our dataset,
and measure the potential gains in coverage that can
be achieved by considering lengthening when pro-
cessing Twitter data. Experiment II substantiates
the claim that word lengthening is not arbitrary, and
is used for emphasis of important words, including
those conveying sentiment and emotion. In the first
part of Experiment III we demonstrate the implica-
tions of this connection for the purpose of sentiment
detection using an existing sentiment lexicon. In
the second part, we present an unsupervised method
for using the lengthening phenomenon to expand an
existing sentiment lexicon and tailor it to our do-
main. We evaluate the method through compari-
son to human judgments, analyze our results, and
demonstrate some of the benefits of our automatic
method.
563
1. For every word in the vocabulary, extract the condensed form, where sequences of a repeated
letter are replaced with a single instance of that letter.
E.g., niiiice? nice, realllly ? realy ...
2. Create sets of words sharing the same condensed form.
E.g., {nice, niiice, niccccceee...}, {realy, really, realllly, ...} ...
3. Remove sets which do not contain at least one repeat of length three.
E.g.,{committee, committe, commitee}
4. Find the most frequently occurring form in the group, and mark it as the canonical form.
E.g., {nice, niiice, niccccceee...}, {realy, really, realllly, ...} ...
Figure 1: Procedure for detecting lengthened words and associating them with a canonical form.
3 Data
Half a million tweets were sampled from the Twit-
ter Streaming API on March 9th 2011. The tweets
were sampled to cover a diverse geographic distri-
bution within the U.S. such that regional variation in
language use should not bias the data. Some tweets
were also sampled from Britain to provide a more
diverse sampling of English. We restricted our sam-
ple to tweets from accounts which indicated their
primary language as English. However, there may
be some foreign language messages in our dataset,
since multi-lingual users may tweet in other lan-
guages even though their account is marked as ?En-
glish?.
The tweets were tokenized and converted to
lower-case. Punctuation, as well as links, hash-
tags, and username mentions were removed. The
resulting corpus consists of approximately 6.5 mil-
lion words, with a vocabulary of 22 thousand words
occurring 10 times or more.
4 Experiment I - Detection
To detect and analyze lengthened words, we employ
the procedure described in Figure 1. We find sets
of words in our data which share a common form
and differ only in the number of times each letter is
repeated (Steps 1 & 2). In Step 3 we remove sets
where all the different forms are likely to be the re-
sult of misspelling, rather than lengthening. Finally,
in Step 4, we associate all the forms in a single set
with a canonical form, which is the most common
one observed in the data.
The procedure resulted in 4,359 sets of size > 1.
To reduce noise resulting from typos and mis-
spellings, we do not consider words containing non-
alphabetic characters, or sets where the canonical
form is a single character or occurs less than 10
times. This left us with 3,727 sets.
Analysis Table 1 lists the canonical forms of the
20 largest sets in our list (in terms of the number of
variations). Most of the examples are used to ex-
press emotion or emphasis. Onomatopoeic words
expressing emotion (e.g., ow, ugh, yay) are often
lengthened and, for some, the combined frequency
of the different lengthened forms is actually greater
than that of the canonical (single most frequent) one.
Lengthening is a common phenomenon in our
dataset. Out of half-a-million tweets, containing
roughly 6.5 million words, our procedure identifies
108,762 word occurrences which are lengthenings
of a canonical form. These words occur in 87,187
tweets (17.44% or approximately one out of every
six, on average). The wide-spread use of length-
ening is surprising in light of the length restriction
of Twitter. Grinter and Eldridge (2003) point out
several conventions that are used in text messages
specifically to deal with this restriction. The fact that
lengthening is used in spite of the need for brevity
suggests that it conveys important information.
Canonical Assumption We validate the assump-
tion that the most frequent form in the set is the
canonical form by examining sets containing one or
more word forms that were identified in a standard
564
Can. Form Card. # Can. # Non-Can.
nice 76 3847 348
ugh 75 1912 1057
lmao 70 10085 3727
lmfao 67 2615 1619
ah 61 767 1603
love 59 16360 359
crazy 59 3530 253
yeah 57 4562 373
sheesh 56 247 131
damn 52 5706 299
shit 51 10332 372
really 51 9142 142
oh 51 7114 1617
yay 45 1370 375
wow 45 3767 223
good 45 21042 3171
ow 44 116 499
mad 44 3627 827
hey 44 4669 445
please 43 4014 157
Table 1: The canonical forms of the 20 largest sets (in
terms of cardinality), with the number of occurrences of
the canonical and non-canonical forms.
English dictionary3. This was the case for 2,092 of
the sets (56.13%). Of these, in only 55 (2.63%) the
most frequent form was not recognized by the dic-
tionary. This indicates that the strategy of choosing
the most frequent form as the canonical one is reli-
able and highly accurate (> 97%).
Implications for NLP To examine the effects of
lengthening on analyzing Twitter data, we look at
the difference in coverage of a standard English dic-
tionary when we explicitly handle lengthened words
by mapping them to the canonical form. Cov-
erage with a standard dictionary is important for
many NLP applications, such as information re-
trieval, translation, part-of-speech tagging and pars-
ing. The canonical form for 2,037 word-sets are
identified by our dictionary. We searched for oc-
currences of these words which were lengthened by
two or more characters, meaning they would not
be identified using standard lemmatization methods
or spell-correction techniques that are based on edit
3We use the standard dictionary for U.S. English included in
the Aspell Unix utility.
distance. We detected 25,101 occurrences of these,
appearing in 22,064 (4.4%) tweets. This implies that
a lengthening-aware stemming method can be used
to increase coverage substantially.
5 Experiment II - Relation to Sentiment
At the beginning of Section 2 we presented the hy-
pothesis that lengthening represents a textual substi-
tute for prosodic indicators in speech. As such, it is
not used arbitrarily, but rather applied to subjective
words to strengthen the sentiment or emotion they
convey. The examples presented in Table 1 in the
previous section appear to support this hypothesis.
In this section we wish to provide experimental evi-
dence for our hypothesis, by demonstrating a signif-
icant degree of association between lengthening and
subjectivity.
For this purpose we use an existing sentiment lex-
icon (Wilson et al, 2005), which is commonly used
in the literature (see Section 1) and is at the core
of OpinionFinder4, a popular sentiment analysis tool
designed to determine opinion in a general domain.
The lexicon provides a list of subjective words, each
annotated with its degree of subjectivity (strongly
subjective, weakly subjective), as well as its sen-
timent polarity (positive, negative, or neutral). In
these experiments, we use the presence of a word
(canonical form) in the lexicon as an indicator of
subjectivity. It should be noted that the reverse is
not true, i.e., the fact that a word is absent from the
lexicon does not indicate it is objective.
As a measure of tendency to lengthen a word, we
look at the number of distinct forms of that word ap-
pearing in our dataset (the cardinality of the set to
which it belongs). We group the words according to
this statistic, and compare to the vocabulary of our
dataset (all words appearing in our data ten times
or more, and consisting of two or more alphabetic
characters, see Section 4). Figure 2 shows the per-
centage of subjective words (those in the lexicon) in
each of the groups. As noted previously, this is a
lower bound, since it is possible (in fact, very likely)
that other words in the group are subjective, despite
being absent from the lexicon. The graph shows a
clear trend - the more lengthening forms a word has,
4http://www.cs.pitt.edu/mpqa/
opinionfinderrelease/
565
the more likely it is to be subjective (as measured by
the percentage of words in the lexicon).
The reverse also holds - if a word is used to con-
vey sentiment, it is more likely to be lengthened. We
can verify this by calculating the average number of
distinct forms for words in our data that are sub-
jective and comparing to the rest. This calculation
yields an average of 2.41 forms for words appearing
in our sentiment lexicon (our proxy for subjectiv-
ity), compared to an average of 1.79 for those that
aren?t5. This difference is statistically significant at
p < 0.01%, using a student t-test.
The lexicon we use was designed for a general
domain, and suffers from limited coverage (see be-
low) and inaccuracies (see O?Connor et al 2010 and
below Section 6.2 for examples), due to the domain
shift. The sentiment lexicon contains 6,878 words,
but only 4,939 occur in our data, and only 2,446 ap-
pear more than 10 times. Of those appearing in our
data, only 485 words (7% of the lexicon vocabulary)
are lengthened (the bar for group 2+ in Figure 2),
but these are extremely salient. They encompass
701,607 instances (79% of total instances of words
from the lexicon), and 339,895 tweets. This pro-
vides further evidence that lengthening is used with
salient sentiment words.
These results also demonstrates the limitations of
using a sentiment lexicon which is not tailored to
the domain. Only a small fraction of the lexicon is
represented in our data, and it is likely that there are
many sentiment words that are commonly used but
are absent from it. We address this issue in the next
section.
6 Experiment III - Adapting the Sentiment
Lexicon
The previous experiment showed the connection be-
tween lengthening and sentiment-bearing words. It
also demonstrated some of the shortcomings of a
lexicon which is not specifically tailored to our do-
main. There are two steps we can take to use
the lengthening phenomenon to adapt an existing
sentiment lexicon. The first of these is simply
to take lengthening into account when identifying
sentiment-bearing words in our corpus. The second
5This, too, is a conservative estimate, since the later group
also includes subjective words, as mentioned.
  0%
  5%
  10%
  15%
  20%
  25%
  30%
All 2+ 3+ 4+ 5+ 6+ 7+ 8+ 9+ 10+
% S
ubje
ctiv
e
Number of Variations
All 2+ 3+ 4+ 5+ 6+ 7+ 8+ 9+ 10+
18,817 3,727 2,451 1,540 1,077 778 615 487 406 335
Figure 2: The percentage of subjective word-sets (those
whose canonical form appears in the lexicon) as a func-
tion of cardinality (number of lengthening variations).
The accompanying table provides the total number of sets
in each cardinality group.
is to exploit the connection between lengthening and
sentiment to expand the lexicon itself.
6.1 Expanding Coverage of Existing Words
We can assess the effect of specifically consider-
ing lengthening in our domain by measuring the
increase of coverage of the existing sentiment lex-
icon. Similarly to Experiment I (Section 4), we
searched for occurrences of words from the lexi-
con which were lengthened by two or more charac-
ters, and would therefore not be detected using edit-
distance. We found 12,105 instances, occurring in
11,439 tweets (2.29% of the total). This increase in
coverage is relatively small6, but comes at almost no
cost, by simply considering lengthening in the anal-
ysis.
A much greater benefit of lengthening, however,
results from using it as an aid in expanding the sen-
timent lexicon and detecting new sentiment-bearing
words. This is the subject of the following section.
6.2 Expanding the Sentiment Vocabulary
In Experiment II (Section 5) we showed that length-
ening is strongly associated with sentiment. There-
fore, words which are lengthened can provide us
with good candidates for inclusion in the lexicon.
We can employ existing sentiment-detection meth-
6Note that almost half of the increase in coverage calculated
in Experiment I (Section 4) comes from subjective words!
566
ods to decide which candidates to include, and de-
termine their polarity.
Choosing a Candidate Set The first step in ex-
panding the lexicon is to choose a set of candidate
words for inclusion. For this purpose we start with
words that have 5 or more distinct forms. There
are 1,077 of these, of which only 217 (20.15%)
are currently in our lexicon (see Figure 2). Since
we are looking for commonly lengthened words,
we disregard those where the combined frequency
of the non-canonical forms is less than 1% that of
the canonical one. We also remove stop words,
even though some are often lengthened for emphasis
(e.g., me, and, so), since they are too frequent, and
introduce many spurious edges in our co-occurrence
graph. Finally, we filter words based on weight, as
described below. This leaves us with 720 candidate
words.
Graph Approach We examine two methods for
sentiment detection - that of Brody and Elhadad
(2010) for detecting sentiment in reviews, and that of
Velikovich et al (2010) for finding sentiment terms
in a giga-scale web corpus. Both of these employ
a graph-based approach, where candidate terms are
nodes, and sentiments is propagated from a set of
seed words of known sentiment polarity. We calcu-
lated the frequency in our corpus of all strongly pos-
itive and strongly negative words in the Wilson et al
(2005) lexicon, and chose the 100 most frequent in
each category as our seed sets.
Graph Construction Brody and Elhadad (2010)
considered all frequent adjectives as candidates and
weighted the edge between two adjectives by a func-
tion of the number of times they both modified a
single noun. Velikovich et al (2010) constructed
a graph where the nodes were 20 million candidate
words or phrases, selected using a set of heuristics
including frequency and mutual information of word
boundaries. Context vectors were constructed for
each candidate from all its mentions in a corpus of
4 billion documents, and the edge between two can-
didates was weighted by the cosine similarity be-
tween their context vectors.
Due to the nature of the domain, which is highly
informal and unstructured, accurate parsing is dif-
ficult. Therefore we cannot employ the exact con-
struction method of Brody and Elhadad (2010). On
the other hand, the method of Velikovich et al
(2010) is based on huge amounts of data, and takes
advantage of the abundance of contextual informa-
tion available in full documents, whereas our do-
main is closer to that of Brody and Elhadad (2010),
who dealt with a small number of candidates and
short documents typical to online reviews. There-
fore, we adapt their construction method. We con-
sider all our candidates words as nodes, along with
the words in our positive and negative seed sets. As a
proxy for syntactic relationship, edges are weighted
as a function of the number of times two words oc-
curred within a three-word window of each other in
our dataset. We remove nodes whose neighboring
edges have a combined weight of less than 20, mean-
ing they participate in relatively few co-occurrence
relations with the other words in the graph.
Algorithm Once the graph is constructed, we can
use either of the propagation algorithms of Brody
and Elhadad (2010) and Velikovich et al (2010),
which we will denote Reviews and Web, respec-
tively. The Reviews propagation method is based on
Zhu and Ghahramani (2002). The words in the posi-
tive and negative seed groups are assigned a polarity
score of 1 and 0, respectively. All the rest start with
a score of 0.5. Then, an update step is repeated. In
update iteration t, for each word x that is not in the
seed, the following update rule is applied:
pt(x) =
?y?N(x)w(y, x) ? pt?1(y)
?y?N(x)w(y, x)
(1)
Where pt(x) is the polarity of word x at step t,N(x)
is the set of the neighbors of x, and w(y, x) is the
weight of the edge connecting x and y. Following
Brody and Elhadad (2010), we set this weight to be
1 + log(#co(y, x)), where #co(y, x) is the number
of times y and x co-occurred within a three-word
window. The update step is repeated to convergence.
Velikovich et al (2010) employed a different
label propagation method, as described in Fig-
ure 3. Rather than relying on diffusion along the
whole graph, this method considers only the sin-
gle strongest path between each candidate and each
seed word. In their paper, the authors claim that
their algorithm is more suitable than that of Zhu and
Ghahramani (2002) to a web-based dataset, which
567
Input: G = (V,E), wij ? [0, 1]
P,N, ? ? R, T ? N
Output: poli ? R|V |
Initialize: poli, pol+i , pol-i = 0 for all i
pol+i = 1.0 for all vi ? P and
pol-i = 1.0 for all vi ? N
1: ?ij = 0 for all i 6= j, ?ii = 1 for all i
2: for vi ? P
3: F = {vi}
4: for t : 1...T
5: for (vk, vj) ? E such that vk ? F
6: ?ij = max(?ij , ?ik ? wk,j)
F = F ? {vj}
7: for vj ? V
8: pol+j =
?
vi?P ?ij
9: Repeat steps 1-8 using N to compute pol-
10: ? =?i pol+i /
?
i pol-i
11: poli = pol+i ? ?pol-i , for all i
12: if |poli| < ? then poli = 0.0 for all i
Figure 3: Web algorithm from Velikovich et al (2010).
P and N are the positive and negative seed sets, respec-
tively, wij are the weights, and T and ? are parameters9.
contained many dense subgraphs and unreliable as-
sociations based only on co-occurrence statistics.
We ran both algorithms in our experiment7, and
compared the results.
Evaluation We evaluated the output of the algo-
rithms by comparison to human judgments. For
words appearing in the sentiment lexicon, we used
the polarity label provided. For the rest, similarly
to Brody and Elhadad (2010), we asked volunteers
to rate the words on a five-point scale: strongly-
negative, weakly-negative, neutral, weakly-positive,
or strongly-positive. We also provided a N/A option
if the meaning of the word was unknown. Each word
was rated by two volunteers. Words which were la-
beled N/A by one or more annotators were consid-
ered unknown. For the rest, exact inter-rater agree-
7We normalize the weights described above when using the
Web algorithm.
9In Velikovich et al (2010), the parameters T and ? were
tuned on a held out dataset. Since our graphs are comparatively
small, we do not need to limit the path length T in our search.
We do not use the threshold ?, but rather employ a simple cutoff
of the top 50 words.
Human Judgment
Pos. Neg. Neu. Unk.
Web Pos. 18 2 26 2Neg. 8 19 17 8
Reviews Pos. 21 6 21 2Neg. 9 14 11 16
Table 2: Evaluation of the top 50 positive and negative
words retrieved by the two algorithms through compari-
son to human judgment.
Web
pos. neg.
see shit
win niggas
way dis
gotta gettin
summer smh
lets tight
haha fuckin
birthday fuck
tomorrow sick
ever holy
school smfh
peace outta
soon odee
stuff wack
canes nigga
Reviews
pos. neg.
kidding rell
justin whore
win rocks
feel ugg
finale naw
totally yea
awh headache
boys whack
pls yuck
ever shawty
yer yeah
lord sus
mike sleepy
three hunni
agreed sick
Table 3: Top fifteen negative and positive words for the
algorithms of Brody and Elhadad (2010) (Reviews) and
Velikovich et al (2010) (Web).
ment was 67.6%, but rose to 93% when considering
adjacent ratings as equivalent10. This is compara-
ble with the agreement reported by Brody and El-
hadad (2010). We assigned values 1 (strongly nega-
tive) to 5 (strongly positive) to the ratings, and cal-
culated the average between the two ratings for each
word. Words with an average rating of 3 were con-
sidered neutral, and those with lower and higher rat-
ings were considered negative and positive, respec-
tively.
Results Table 2 shows the distribution of the hu-
man labels among the top 50 most positive and most
negative words as determined by the two algorithms.
Table 3 lists the top 15 of these as examples.
10Cohen?s Kappa ? = 0.853
568
From Table 2 we can see that both algorithms do
better on positive words (fewer words with reversed
polarity)11, and that the Web algorithm is more accu-
rate than the Reviews method. The difference in per-
formance can be explained by the associations used
by the algorithms. The Web algorithm takes into ac-
count the strongest path to every seed word, while
the Reviews algorithm propagates from the each
seed to its neighbors and then onward. This makes
the Reviews algorithm sensitive to strong associa-
tions between a word and a single seed. Because our
graph is constructed with co-occurrence edges be-
tween words, rather than syntactic relations between
adjectives, noisy edges are introduced, causing mis-
taken associations. The Web algorithm, on the other
hand, finds words that have a strong association with
the positive or negative seed group as a whole, thus
making it more robust. This explains some of the ex-
amples in Table 3. The words yeah and yea, which
often follow the negative seed word hell, are consid-
ered negative by the Reviews algorithm. The word
Justin refers to Justin Beiber, and is closely associ-
ated with the positive seed word love. Although the
Web algorithm is more robust to associations with
a single seed, it still misclassifies the word holy as
negative, presumably because it appears frequently
before several different expletives.
Detailed analysis shows that the numbers reported
in Table 2 are only rough estimates of performance.
For instance, several of the words in the unknown
category were correctly identified by the algorithm.
Examples include sm(f)h, which stands for ?shak-
ing my (fucking) head? and expresses disgust or dis-
dain, sus, which is short for suspicious (as in ?i hate
susssss ass cars that follow me/us when i?m/we walk-
inggg?), and odee, which means overdose and is
usually negative (though it does not always refers
to drugs, and is sometimes used as an intensifier,
e.g., ?aint shit on tv odee bored?).
There were also cases were the human labels were
incorrect in the context of our domain. For exam-
ple, the word bull is listed as positive in the sen-
timent lexicon, presumably because of its financial
sense. In our domain it is (usually) short for bull-
shit. The word canes was rated as negative by one of
11This trend is not apparent from the top 15 results presented
in Table 3, but becomes noticeable when considering the larger
group.
the annotators, but in our data it refers to the Miami
Hurricanes, who won a game on the day our dataset
was sampled, and were the subject of many posi-
tive tweets. This example also demonstrates that our
method is capable of detecting terms which are asso-
ciated with sentiment at different time points, some-
thing that is not possible with a fixed lexicon.
7 Conclusion
In this paper we explored the phenomenon of length-
ening words by repeating a single letter. We showed
that this is a common phenomenon in Twitter, oc-
curring in one of every six tweets, on average, in our
dataset. Correctly detecting these cases is important
for comprehensive coverage. We also demonstrated
that lengthening is not arbitrary, and is often used
with subjective words, presumably to emphasize the
sentiment they convey. This finding leads us to de-
velop an unsupervised method based on lengthening
for detecting new sentiment bearing words that are
not in the existing lexicon, and discovering their po-
larity. In the rapidly-changing domain of microblog-
ging and net-speak, such a method is essential for
up-to-date sentiment detection.
8 Future Work
This paper examined one aspect of the lengthening
phenomenon. There are other aspects of lengthen-
ing that merit research, such as the connection be-
tween the amount of lengthening and the strength of
emphasis in individual instances of a word. In addi-
tion to sentiment-bearing words, we saw other word
classes that were commonly lengthened, including
intensifiers (e.g., very, so, odee), and named enti-
ties associated with sentiment (e.g., Justin, ?Canes).
These present interesting targets for further study.
Also, in this work we focused on data in English,
and it would be interesting to examine the phe-
nomenon in other languages. Another direction of
research is the connection between lengthening and
other orthographic conventions associated with sen-
timent and emphasis, such as emoticons, punctua-
tion, and capitalization. Finally, we plan to integrate
lengthening and its related phenomena into an accu-
rate, Twitter-specific, sentiment classifier.
569
Acknowledgements
The authors would like to thank Paul Kantor and
Mor Naaman for their support and assistance in this
project. We would also like to thank Mark Steedman
for his help, and the anonymous reviewers for their
comments and suggestions.
References
Barbosa, Luciano and Junlan Feng. 2010. Robust
sentiment detection on twitter from biased and
noisy data. In Chu-Ren Huang and Dan Jurafsky,
editors, COLING (Posters). Chinese Information
Processing Society of China, pages 36?44.
Bermingham, Adam and Alan F. Smeaton. 2010.
Classifying sentiment in microblogs: is brevity an
advantage? In Proceedings of the 19th ACM in-
ternational conference on Information and knowl-
edge management. ACM, New York, NY, USA,
CIKM ?10, pages 1833?1836.
Bolinger, Dwight. 1965. Forms of English: Accent,
Morpheme, Order. Harvard University Press,
Cambridge, Massachusetts, USA.
Bollen, J., H. Mao, and X.-J. Zeng. 2010. Twitter
mood predicts the stock market. ArXiv e-prints .
Bollen, Johan, Bruno Gonalves, Guangchen Ruan,
and Huina Mao. 2011. Happiness is assortative in
online social networks. Artificial Life 0(0):1?15.
Brants, Thorsten and Alex Franz. 2006. Google web
1T 5-gram corpus, version 1. Linguistic Data
Consortium, Catalog Number LDC2006T13.
Brody, Samuel and Noemie Elhadad. 2010. An un-
supervised aspect-sentiment model for online re-
views. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational
Linguistics (NAACL 2010). ACL, Los Angeles,
CA, pages 804?812.
Calhoun, Sasha. 2010. The centrality of metrical
structure in signaling information structure: A
probabilistic perspective. Language 86:1?42.
Diakopoulos, Nicholas A. and David A. Shamma.
2010. Characterizing debate performance via ag-
gregated twitter sentiment. In Proceedings of
the 28th international conference on Human fac-
tors in computing systems. ACM, New York, NY,
USA, CHI ?10, pages 1195?1198.
Grinter, Rebecca and Margery Eldridge. 2003.
Wan2tlk?: everyday text messaging. In Proceed-
ings of the SIGCHI conference on Human fac-
tors in computing systems. ACM, New York, NY,
USA, CHI ?03, pages 441?448.
Kivran-Swaine, Funda and Mor Naaman. 2011.
Network properties and social sharing of emo-
tions in social awareness streams. In Proceed-
ings of the 2011 ACM Conference on Com-
puter Supported Cooperative Work (CSCW 2011).
Hangzhou, China.
McNair, D. M., M. Lorr, and L. F. Droppleman.
1971. Profile of Mood States (POMS). Educa-
tional and Industrial Testing Service.
O?Connor, Brendan, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
International AAAI Conference on Weblogs and
Social Media.
Pak, Alexander and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Proceedings of the Seventh conference
on International Language Resources and Evalu-
ation (LREC?10). ELRA, Valletta, Malta.
Velikovich, Leonid, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The via-
bility of web-derived polarity lexicons. In Hu-
man Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics. ACL,
Stroudsburg, PA, USA, HLT ?10, pages 777?785.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the
conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing. ACL, Stroudsburg, PA, USA, HLT ?05, pages
347?354.
Zhu, X. and Z. Ghahramani. 2002. Learning from
labeled and unlabeled data with label propagation.
Technical report, CMU-CALD-02.
570
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 804?812,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An Unsupervised Aspect-Sentiment Model for Online Reviews
Samuel Brody
Dept. of Biomedical Informatics
Columbia University
samuel.brody@dbmi.columbia.edu
Noemie Elhadad
Dept. of Biomedical Informatics
Columbia University
noemie@dbmi.columbia.edu
Abstract
With the increase in popularity of online re-
view sites comes a corresponding need for
tools capable of extracting the information
most important to the user from the plain text
data. Due to the diversity in products and ser-
vices being reviewed, supervised methods are
often not practical. We present an unsuper-
vised system for extracting aspects and deter-
mining sentiment in review text. The method
is simple and flexible with regard to domain
and language, and takes into account the in-
fluence of aspect on sentiment polarity, an is-
sue largely ignored in previous literature. We
demonstrate its effectiveness on both compo-
nent tasks, where it achieves similar results to
more complex semi-supervised methods that
are restricted by their reliance on manual an-
notation and extensive knowledge sources.
1 Introduction
Online review sites continue to grow in popularity as
more people seek the advice of fellow users regard-
ing services and products. Unfortunately, users are
often forced to wade through large quantities of writ-
ten data in order to find the information they want.
This has led to an increase in research in the areas
of opinion mining and sentiment analysis, with the
aim of providing systems that can automatically an-
alyze user reviews and extract the information most
relevant to the user.
One example of such an application is generat-
ing a summary of the important factors mentioned
in the reviews of a product (see Lerman et al 2009).
Another application is comparing two similar prod-
ucts. In this case, it is important to present to the
user the aspects in which the products differ, rather
than just provide a general star rating. A third exam-
ple is systems for generating automatic recommen-
dations, based on similarity between products, user
reviews, and history of previous purchases. These
types of application require an underlying frame-
work to identify the important aspects of the prod-
uct (also known as features or attributes), and the
sentiment expressed by the review writer.
Unsupervised Methods are desirable for this task,
for two reasons. First, due to the wide range and va-
riety of products and services being reviewed, the
framework must be robust and easily transferable
between domains. The second reason is the nature of
the data. Online reviews are often short and unstruc-
tured, and may contain many spelling and gram-
matical errors, as well as slang or specialized jar-
gon. These factors often present a problem to meth-
ods relying exclusively on dictionaries, manually-
constructed knowledge resources, and gazetteers, as
they may miss out on an important aspect of the
product or an indicator of sentiment. Unsupervised
methods, on the other hand, are not influenced by
the lexical form, and can handle unknown words or
word-forms, provided they occur frequently enough.
This insures that any emergent topic that is salient in
the data will be addressed by the system.
In this paper, we present an unsupervised system
which addresses the core tasks necessary to enable
advanced applications to handle review data. We in-
troduce a local topic model, which works at the sen-
tence level and employs a small number of topics, to
automatically infer the aspects. For sentiment detec-
tion, we present a method for automatically deriving
an unsupervised seed set of positive and negative ad-
jectives that replaces the manually constructed ones
commonly used in the literature. Our approach is
specifically designed to take into account the inter-
804
action between the two tasks.
The rest of the paper is structured as follows. In
Sec. 2 we provide relevant background, and place
our method in the context of previous work in the
field. We describe the data we used in Sec. 3, and our
experiments on the aspect and sentiment-polarity
components in Sec. 4 and 5, respectively. We con-
clude in Sec. 6 with a discussion of our results and
findings and directions for future research.
2 Previous Approaches
In this paper, we focus on the detection of two prin-
ciple elements in the review text: aspects and sen-
timent. In previous work these elements have been
treated, for the most part, as two separate tasks.
Aspect The earliest attempts at aspect detection
were based on the classic information extraction (IE)
approach of using frequently occurring noun phrases
(e.g., Hu and Liu 2004). Such approaches work well
in detecting aspects that are strongly associated with
a single noun, but are less useful when aspects en-
compass many low frequency terms (e.g., the food
aspect of restaurants, which involves many differ-
ent dishes), or are abstract (e.g. ambiance can be
described without using any concrete nouns at all).
Common solutions to this problem involve cluster-
ing with the help of knowledge-rich methods, in-
volving manually-constructed rules, semantic hier-
archies, or both (e.g., Popescu and Etzioni 2005,
Fahrni and Klenner 2008). Titov and McDonald
(2008b) underline the need for unsupervised meth-
ods for aspect detection. However, according to the
authors, existing topic models, such as standard La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003),
are not suited to the task of aspect detection in re-
views, because they tend to capture global topics
in the data, rather than rateable aspects pertinent
to the review. To address this problem, they con-
struct a multi-grain topic model (MG-LDA), which
attempts to capture two layers of topics - global and
local, where the local topics correspond to rateable
aspects. MG-LDA distinguishes tens of local top-
ics, but the many-to-one mapping between these and
rateable aspects is not explicit in the system. To re-
solve this issue, the authors extend their model in
Titov and McDonald (2008a) and attempt to infer
such a mapping with the help of aspect-specific rat-
ings provided along with the review text.
Sentiment Sentiment analysis has been the fo-
cus of much previous research. In this discussion,
we will only mention work directly related to our
own. For a comprehensive survey of the subject, the
reader is directed to Pang and Lee (2008).
Most previous approaches rely on a manually
constructed lexicon of terms which are strongly pos-
itive or negative regardless of context. This informa-
tion on its own is usually insufficient, due to lack
of coverage and the fact that sentiment is often ex-
pressed through words whose polarity is highly do-
main and context specific. If a sentiment lexicon is
available for one domain, domain adaptation can be
used, provided the domains are sufficiently similar
(Blitzer et al, 2007). Another common solution is
through bootstrapping - using a seed group of terms
with known polarity to infer the polarity of domain
specific terms (e.g., Fahrni and Klenner 2008; Jijk-
oun and Hofmann 2009). The most minimalist ex-
ample of this approach is Turney (2002), who used
only a single pair of adjectives (good and poor) to
determine the polarity of other terms through mu-
tual information. For Chinese, Zagibalov and Carroll
(2008) use a single seed word meaning good, and six
common indicators of negation in their bootstrap-
ping approach. Often, when using a context indepen-
dent seed, large amounts of domain-specific data are
required, in order to obtain sufficient co-occurrence
statistics. Commonly, web queries are used to obtain
such data.
Independently of any specific task, Hatzivas-
siloglou and McKeown (1997) present a completely
unsupervised method for determining the polarity of
adjectives in a large corpus. A graph is created, in
which adjectives are nodes, and edges between them
are weighted according to a (dis)similarity function
based primarily on whether the two adjectives oc-
curred in a conjunction or disjunction in the corpus.
A heuristic approach is then used to split the graph
in two. The group containing the adjectives with the
higher average frequency is labeled as positive, and
the other as negative.
Combined Approaches Aspects can influence
sentiment polarity within a single domain. For ex-
ample, in the restaurant domain, cheap is usually
positive when discussing food, but negative when
discussing the decor or ambiance. Many otherwise
neutral terms (e.g., warm, heavy, soft) acquire a sen-
timent polarity in the context of a specific aspect.
805
Recent work has addressed this interaction in differ-
ent ways. Mei et al (2007) present a form of do-
main adaptation using an LDA model which treats
positive and negative sentiment as two additional
topics. Fahrni and Klenner (2008) directly address
the specificity of sentiment to the word it is modi-
fying. Aspects are defined by a manually specified
subset of the Wikipedia category hierarchy. For sen-
timent, the authors use a seed set of positive and
negative adjectives, and iteratively propagate sen-
timent polarity through conjunction relations (like
those used by Hatzivassiloglou and McKeown 1997,
above). Web queries are used to overcome the spar-
sity issue of these highly-specific patterns. In the IE
setting, Popescu and Etzioni (2005) extract frequent
terms, and cluster them into aspects. The sentiment
detection task is formulated as a Relaxation Label-
ing problem of finding the most likely sentiment la-
bels for opinion-bearing terms, while satisfying as
many local constraints as possible. The authors use
a variety of knowledge sources, web queries, and
hand crafted rules to detect relations between terms
(e.g., meronymy). These relations are used both for
the clustering, and as a basis for the constraints.
Our approach is designed to be as unsupervised
and knowledge-lean as possible, so as to make it
transferable across different types of products and
services, as well as across languages. Aspects are
determined via a local version of LDA, which oper-
ates on sentences, rather than documents, and em-
ploys a small number of topics that correspond di-
rectly to aspects. This approach overcomes the prob-
lems of frequent-term methods, as well as the issues
raised by Titov andMcDonald (2008b). We use mor-
phological negation indicators to automatically cre-
ate a seed set of highly relevant positive and nega-
tive adjectives, which are guaranteed to be pertinent
to the aspect at hand. These automatically-derived
seed sets achieve comparable results to the use of
manual ones, and the work of Zagibalov and Car-
roll (2008) suggests that the use of negation can be
easily transfered to other languages.
3 Data
Our primary dataset is the publicly available corpus
used in Ganu et al (2009). It contains over 50,000
restaurant reviews from Citysearch New York1. Ad-
1http://newyork.citysearch.com/
ditionally, to demonstrate the domain independence
of our system, we collected 1086 reviews for four
leading netbook computers from Amazon.com.
For evaluation purposes, we used the annotated
dataset from Ganu et al (2009), which is a sub-
set of 3,400 sentences from the Citysearch corpus.
These sentences were manually labeled for aspect
and sentiment. There were six manually defined as-
pect labels - Food & Drink, Service, Price, Atmo-
sphere, Anecdotes and Miscellaneous. A sentence
could contain multiple aspects, but, for our evalua-
tion, we used only sentences with a single label. For
sentiment, each sentence was given a single value -
Positive, Negative, Neutral or Conflict (indicating a
mixture of positive and negative sentiment).
We were also provided with a seed set of 128 pos-
itive and 88 negative adjectives used by Fahrni and
Klenner (2008), which were specifically selected to
be domain and target independent.
For the purpose of the experiments presented
here, we focused on sentences containing noun-
adjective pairs. Such pairs are one of the most com-
mon way of expressing sentiment about an aspect
and allow us to capture the interaction between the
two.
4 Aspect
4.1 Methodology
In order to infer the salient aspects in the data, we
employed the following steps:
Local LDA We used a standard implementation2
of LDA. In order to prevent the inference of global
topics and direct the model towards rateable aspects
(see Sec. 2), we treated each sentence as a separate
document. The output of the model is a distribution
over inferred aspects for each sentence in the data.
The parameters we employed were standard, out-of-
the-box settings (? = 0.1,? = 0.1, 3000 iterations),
with no specific tuning to our data. We ran the algo-
rithm with the number of aspects ranging from 10 to
20, and employed a cluster validation scheme (see
below) to determine the optimal number.
Model Order The issue of model order, i.e., deter-
mining the correct number of clusters, is an impor-
tant element in unsupervised learning. A common
2GibbsLDA++, by Xuan-Hieu Phan. Available at http://
gibbslda.sourceforge.net/.
806
approach (Levine and Domany, 2001; Lange et al,
2004; Niu et al, 2007) is to use a cluster validation
procedure. In such a procedure, different model or-
ders are compared, and the one with the most con-
sistent clustering is chosen. For the purpose of the
validation procedure, we have a cluster correspond-
ing to each aspect, and we label each sentence as
belonging to the cluster of the most probable aspect.
Given the collection of sentences in our data, D,
and two connectivity matricesC and C?, where a cell
i, j contains 1 if sentences di and d j belong to the
same cluster, we define a consistency function F
(following Niu et al 2007):
F(C,C?) =
?i, j 1{Ci, j = C?i, j = 1,di,d j ? D?}
?i, j 1{Ci, j = 1,di,d j ? D?}
(1)
We then employ the following procedure:
1. Run the LDA model with k topics on D to ob-
tain connectivity matrixCk.
2. Create a comparison connectivity matrix Rk
based on uniformly drawn random assignments
of the instances.
3. Sample random subset Di of size ?|D| from D.
4. Run the LDA model on Di to obtain connectiv-
ity matrixCik.
5. Create a comparison matrix Rik based on uni-
formly drawn random assignments of the in-
stances in Di.
6. Calculate scorei(k) = F(C?,C)?F(R?,R) where
F is given in Eq. 1.
7. Repeat steps 3 to 6 q times.
8. Return the average score over q iterations.
This procedure calculates the consistency of our
clustering solution, using a similar sized random as-
signment for comparison. It does this on q subsets to
reduce the effects of chance. The k with the high-
est score is chosen. In our experiments, we used
q= 5,?= 0.9. For both our datasets (restaurants and
netbooks), the highest-scoring k was 14.
Determining Representative Words For each as-
pect, we list all the nouns in the data according to a
score based on their mutual information with regard
to that aspect.
Scorea(w) = p(w,a) ? log
p(w,a)
p(w) ? p(a)
(2)
Where p(w), p(a), p(w,a) are the probabilities, ac-
cording to the LDA model, of the word w, the aspect
a, and the wordw labeled with aspect a, respectively.
We then select, for each aspect, the top ka rank-
ing words, such that they cover 75% of the word-
instances labeled by the LDA model with aspect la-
bel a. Due to the skewed frequency distribution of
words, this is a relatively small portion of the words
(typically 100-200). This set of representative words
for each aspect is used in the sentiment component
of our system (see Sec. 5.1).
4.2 Inferred Aspects
Table 1 presents the aspects inferred by our system
for the restaurant domain. The inferred aspects cover
all those defined in the manual annotation, but also
distinguish between a finer granularity of aspects,
based solely on the review text, e.g., between phys-
ical environment and ambiance, and between the at-
titude of the staff and the quality of the service.
In order to demonstrate that our method can be
transfered between very different domains and cat-
egories of products, we also ran our algorithm on
our set of netbook reviews. The inferred aspects
are presented in Table 2. The system identifies im-
portant aspects relevant to our data. Some of these
(e.g., software, hardware) might be suggested by hu-
man annotators, but some would probably be missed
unless the annotators carefully read through all the
reviews, e.g., theMemory aspect, which includes ad-
vice about upgrading specific models. This capabil-
ity of our system is important, as it demonstrates that
our method can be used to produce customized com-
parisons for the user and will take into account the
important common factors, as well as the unique as-
pects of each item.
4.3 Evaluation
To determine the quality of our automatically in-
ferred aspects, we compared the output of our sys-
tem to the sentence-level manual annotation of Ganu
et al (2009). To each sentence in the data, the LDA
model assigns a distribution {P(a)}a?A over the set
A of inferred aspects. By defining a threshold ta for
each aspect, we can label a sentence as belonging
to aspect a if P(a) > ta. By varying the threshold ta
we created precision-recall curves for the top three
rateable aspects in the restaurant domain, shown in
807
Inferred Aspect Representative Words Manual Aspect
Main Dishes chicken, sauce, rice, cheese, spicy, salad,
Food & Drink
Bakery hot, delicious, dessert, bagels, bread, chocolate
Food - General menu, fresh, sushi, fish, chef, cuisine
Wine & Drinks wine, list, glass, drinks, beer, bottle
Ambiance / Mood great, atmosphere, wonderful, music, experience, relaxed
Atmosphere
Physical Atmosphere bar, room, outside, seating, tables, cozy, loud
Staff service, staff, friendly, attentive, busy, slow
Staff
Service table, order, wait, minutes, reservation, forgot
Value portions, quality, worth, size, cheap Price
Anecdotes dinner, night, group, friends, date, family
Anecdotes
Anecdotes out, back, definitely, around, walk, block
General best, top, favorite, city, NYC
Misc.Misc. - Location never, restaurant, found, Paris, (New) York, location
Misc. place, eat, enjoy, big, often, stuff
Table 1: List of automatically inferred aspects for the restaurant domain, with some representative words for each
aspect (middle), and the corresponding aspect label from the manual annotation (right). Labels (left) were assigned by
the authors.
Aspect Representative Words
Performance power, performance, mode, fan, quiet
Hardware drive, wireless, bluetooth, usb, speakers, webcam
Memory ram, 2GB, upgrade, extra, 1GB, speed
Software using, office, software, installed, works, programs
Usability internet, video, web, movies, music, email, play
Portability around, light, work, portable, weight, travel
Comparison netbooks, best, reviews, read, decided, research
Aspect Representative Words
Mouse mouse, right, touchpad, pad, buttons, left
General great, little, machine, price, netbook, happy
Purchase amazon, purchased, bought, weeks, ordered
Looks looks, feel, white, finish, blue, solid, glossy
OS windows, xp, system, boot, linux, vista, os
Battery battery, life, hours, time, cell, last
Size screen, keyboard, size, small, enough, big
Table 2: List of automatically inferred aspects for the netbook dataset, with representative words for each aspect .
Figure 13. Although the data used in Titov and Mc-
Donald (2008a) was unavailable for direct compar-
ison, our method exhibits similar behavior and per-
formance (compare Fig. 4, there) on a domain with
similar characteristics (abstract aspects which en-
compass many low frequency words). This demon-
strates that our local version of LDA with few top-
ics overcomes the issues which confronted the au-
thors of that work (i.e., global topics and many-to-
one mapping of topics to aspects), without requiring
specially designed models or additional information
in the form of user-provided aspect-specific ratings
(see Sec. 2).
We believe the reason for this stems from the
composition of online reviews. Since many reviews
have similar mixtures of local topics (e.g., food, ser-
vice), standard LDA prefers global topics, which
3We combined the probabilities of all the inferred aspects
that match a single manually assigned aspect, according to the
mapping in Table 1.
distinguish more strongly between reviews (e.g., cui-
sine type, restaurant type). However, when em-
ployed at the sentence level, local topics (corre-
sponding to rateable aspects) provide a stronger way
to distinguish between individual sentences.
5 Sentiment
5.1 Methodology
For determining sentiment polarity, we developed
the following procedure. For each aspect, we ex-
tracted the relevant adjectives, built a conjunction
graph, automatically determined the seed set (or
used a manual one, for comparison), and propagated
the polarity scores to the rest of the adjectives. De-
tails of each step are described below.
Extracting Adjectives As a pre-processing step,
we parsed our data (using RASP, Briscoe and Car-
roll 2002). The parsed output was used to detect
negation and conjunction. If an adjective A partic-
808
(a) (b) (c)
Figure 1: Precision / Recall curves for the top three rateable aspects: (a) Food, (b) Service, and (c) Atmosphere.
ipated in a negation in the sentence, it was replaced
by a new adjective not-A. We then extract all cases
where an adjective modified a noun. For example,
from the sentence ?The food was tasty and hot, but
our waiter was not friendly.? we can extract the pairs
(tasty, food), (hot, food), (not-friendly, waiter).
Building the Graph Our method for determin-
ing sentiment polarity is based on an adaptation of
Hatzivassiloglou and McKeown (1997) (see Sec. 2).
Several issues confronted us when attempting to
adapt their method to our task. In the original arti-
cle, adjectives with no orientation were ignored. It
is unclear how this can be easily done in an unsu-
pervised fashion, and such sentiment-neutral adjec-
tives are ubiquitous in real-world data. Furthermore,
adjectives whose orientation depended on the con-
text were also ignored. These are of particular in-
terest in our task, and are likely to be missing or
incorrectly labeled in standard sentiment dictionar-
ies. For our purposes, since we need to handle ad-
jectives expressing various shades of sentiment, not
only strongly positive or negative ones, we are inter-
ested in a scoring method, rather than a binary label-
ing. Also, we do not want to use a general corpus,
but rather the text from the reviews themselves. This
usually means a much smaller corpus than the one
used in the original paper, but has the advantage of
being domain specific.
Our method of building the polarity graph differed
in several ways from the original. First, we did not
use disjunctions (e.g., ?but?) as indicators of opposite
polarity. The reason for this was that, in our domain
of online reviews, disjunctions often did not convey
contrast in polarity, but rather in perceived expecta-
tions, e.g., ?dainty but strong necklace?, and ?cheap
but delicious food?.
Instead of using regular expressions to capture ex-
plicit conjunctions, we retrieved all cases where our
parser indicated that two adjectives modified a sin-
gle noun in the same sentence.
To ensure that aspect-specific adjectives are han-
dled correctly, we built a separate graph for each as-
pect, by selecting the cases where the modified noun
was one of the representative words for that aspect
(see Sec. 4.1).
Constructing a Seed Set We used morphologi-
cal information and explicit negation to find pairs of
opposite polarity. Specifically, adjective pairs which
were distinguished only by one of the prefixes ?un?,
?in?, ?dis?, ?non?, or by the negation marker ?not-?
were selected for the seed set. Starting with the most
frequent pair, we assigned a positive polarity to the
more frequent member of the pair.
Then, in order of decreasing frequency, we as-
signed polarity to the other seed pairs, based on the
shortest path either of the members had to a previ-
ously labeled adjective. That member received its
neighbor?s polarity, and the other member of the pair
received the opposite polarity. When all pairs were
labeled, we corrected for misclassifications by iter-
ating through the pairs and reversing the polarity if
that improved consistency, i.e., if it caused the mem-
bers of the pair to match the polarities of more of
their neighbors. Finally, we reverse the polarity of
the seed groups if the negative group has a higher
total frequency.
Propagating Polarity Our propagation method is
based on the label propagation algorithm of Zhu
and Ghahramani (2002). The adjectives in the posi-
tive and negative seed groups are assigned a polarity
809
score of 1 and 0, respectively. All the rest start with
a score of 0.5. Then, an update step is repeated. In
update iteration t, for each adjective x that is not in
the seed, the following update rule is applied:
pt(x) =
?y?N(x)w(y,x) ? p
t?1(y)
?y?N(x)w(y,x)
(3)
Where pt(x) is the polarity of adjective x at step t,
N(x) is the set of the neighbors of x, andw(y,x) is the
weight of the edge connecting x and y. We set this
weight to be 1+ log(#mod(y,x)) where #mod(y,x)
is the number of times y and x both modified a single
noun. The update step is repeated to convergence.
5.2 Aspect-Specific Gold Standard
To evaluate the performance of the sentiment com-
ponent of our system, we created an aspect-specific
gold standard. For each of the top eight automati-
cally inferred aspects (corresponding to the Food,
Service and Atmosphere aspects in the annotation),
we constructed a polarity graph, as described in
Sec. 5.1. We retrieved a list of all adjectives that
participated in five or more modifications of nouns
from that specific aspect). Table 3 lists the number of
such adjectives in each aspect. We split the data into
ten portions and, for each portion, asked two volun-
teers to rate each adjective according to the polar-
ity of the sentiment it expresses in the context of the
specified aspect. The judges could select from the
following ratings: Strongly Negative, Weakly Nega-
tive, Neutral, Weakly Positive, Strongly Positive, and
N/A. As expected, exact inter-annotator agreement
was low - only 54%, but when considering two ad-
jacent ratings as equivalent (i.e, Strongly vs. Weakly
Negative or Positive, and Neutral vs. Weakly Neg-
ative or Positive), agreement was 93.3%. This indi-
cates there is some difficulty distinguishing between
the fine-grained categories we specified, but high
agreement at a coarser level, which advocates us-
ing a ranking approach for evaluation (see also Pang
and Lee 2005). We therefore translated the annota-
tor ratings to a numerical scale, from ?2 (Strongly
Negative) to +2 (Strongly Positive) at unit intervals.
After discarding adjectives where one or more anno-
tators gave a ?N/A? tag, we averaged the two annota-
tor numerical scores, and used this data as the gold
standard for our evaluation.
Aspect # Adj. # Rated % Neu.
Mood 293 206 17%
Staff 155 122 3%
Main Dishes 287 185 25%
Physical Atmo. 161 103 21%
Bakery 180 129 23%
Food - General 192 144 28%
Wine & Drinks 111 75 18%
Service 89 57 5%
Total 1468 1021 ?
Table 3: For each aspect, the number of frequently oc-
curring adjectives for each aspect (# Adj.), number of
adjectives remaining after removing those labeled ?N/A?
(# Rated), and percent of rated adjectives labeled ?Neu-
tral? by both annotators (% Neu.).
Auto. Manual
Aspect ?k Dk ?k Dk
Mood 0.53 0.23 0.56 0.22
Staff 0.57 0.22 0.60 0.20
Main Dishes 0.19 0.40 0.38 0.31
Physical Atmo. 0.34 0.33 0.25 0.37
Bakery 0.33 0.33 0.35 0.33
Food - General 0.19 0.41 0.41 0.30
Wine & Drinks 0.32 0.34 0.52 0.24
Service 0.41 0.30 0.54 0.23
Average 0.36 0.32 0.45 0.27
Table 4: Kendall coefficient and distance scores for eight
inferred aspects.
5.3 Evaluation Measures
Kendall?s tau coefficient (?k) and Kendall?s distance
(Dk) are commonly used (e.g., Jijkoun and Hofmann
2009) to compare rankings. These measures look at
the number of pairs of ranked items that agree or
disagree with the ordering in the gold standard. The
value of ?k ranges from -1 (perfect disagreement) to
1 (perfect agreement), with 0 indicating an almost
random ranking. The value ofDk ranges from 0 (per-
fect agreement) to 1 (perfect disagreement). It is im-
portant to note that only pairs that are ordered in the
gold standard are used in the comparison.
5.4 Evaluation Results
Table 4 reports Kendall?s coefficient (?k) and dis-
tance (Dk) values for our method when using our
automatically derived seed set (Auto.). For com-
parison, we ran our procedure using the manually
compiled seed set (Manual) of Fahrni and Klenner
810
Food - General: Mexican, French, Eastern, Turkish,
European, Tuscan, Mediterranean, American, Cuban,
Thai, Peruvian, Spanish, Korean, Vietnamese, Indian,
African, Japanese, Italian, Chinese, Asian
Mood: Vietnamese, Brazilian, Turkish, Eastern,
Caribbean, Cuban, Italian, Spanish, Japanese, Euro-
pean, Mediterranean, Colombian, Mexican, Asian,
Indian, Thai, British, American, French, Korean,
Chinese, Russian, Moroccan
Staff: British, European, Chinese, Indian, American,
Spanish, Asian, Italian, French
Table 5: Polarity ranking of cuisine adjectives (from most
positive) for three aspects.
(2008). Using the manual seed set obtains results
that correspond better to our gold standard. Our au-
tomatic method also achieves good results, and can
be used when a manual seed set is not available.
More importantly, correlation with the gold standard
may not indicate better suitability to the sentiment
detection task in reviews. For instance, it is interest-
ing to note that the worst correlation scores were on
the Main Dishes and Food - General aspects. If we
compare to Table 3, we can see these aspects have
the highest percentage of adjectives rated as neutral
by the annotators. However, in many cases, these ad-
jectives actually carry some sentiment in their con-
text. An example of this are adjectives describing
the type of cuisine, which are objective, and there-
fore usually considered neutral by annotators. Ta-
ble 5 shows the automatic ranking of cuisine type
from positive to negative in three aspects. It is inter-
esting to see that the rankings change according to
the aspect, and certain cuisines are strongly associ-
ated with specific aspects and not with others. This
is supported by Ganu et al (2009), who observed
during the annotation that, in the restaurant corpus,
French and Italian restaurants were strongly associ-
ated with the service aspect. This trend can be iden-
tified automatically by our method, and at a much
more detailed level than that noticed by a human an-
alyzing the data.
6 Discussion & Future Work
Our experiments confirm the value of a fully un-
supervised approach to the tasks of aspect detec-
tion and sentiment analysis. The aspects are inferred
from the data, and are more representative than
manually derived ones. For instance, in our restau-
rant domain, the manually constructed aspect list
omitted or over-generalized some important aspects,
while over-representing others. There was no sep-
arate Drinks category, even though it was strongly
present in the data. The Service aspect, dealing with
waiting time, reservations, and mistaken orders, was
an important emergent aspect on its own, but was
grouped under Staff in the manual annotation.
Adjectives can convey different sentiments de-
pending on the aspect being discussed. For exam-
ple, the adjective ?warm? was ranked very positive in
the Staff aspect, but slightly negative in the General
Food aspect. A knowledge-rich approach might ig-
nore such adjectives, thereby missing important ele-
ments of the review.
Finally, as online reviews belong to an informal
genre, with inventive spelling and specialized jar-
gon, it may be insufficient, for both aspect and
sentiment, to rely only on lexicons. For example,
our restaurant reviews included spelling errors such
as desert, decour/decore, anti-pasta, creme-brule,
sandwhich, omlette, exelent, tastey, as well as at
least six different common misspellings of restau-
rant. There were also specialized terms, such as Ko-
rma, Edamame, Dosa and Pho, all of which do not
appear in common dictionaries, and creative use of
adjectives, such as orgasmic and New-Yorky.
This work has opened many avenues for future re-
search and improvements. So far, we focused on ad-
jectives as sentiment indicators, however, there have
been studies showing that other parts of speech can
be very helpful for this task (e.g., Pang et al 2002;
Benamara et al 2007). Also, it would be interesting
to take a closer look at the interactions between as-
pect and sentiment, especially at a multiple-sentence
level (see Snyder and Barzilay 2007). Finally, we
feel that the true test of the usability of our system
should be through an application, and intend to pro-
ceed in that direction.
Acknowledgments
We?d like to thank Angela Fahrni and Manfred Klenner
for kindly allowing us access to their data and annotation.
We also wish to thank the volunteer annotators. This work
was partially supported by a Google Research Award.
References
Benamara, Farah, Carmine Cesarano, Antonio Picariello,
Diego Reforgiato, and V. S. Subrahmanian. 2007. Sen-
timent analysis: Adjectives and adverbs are better than
811
adjectives alone. In Proc. of the International Confer-
ence on Weblogs and Social Media (ICWSM).
Blei, David M., Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research 3:993?1022.
Blitzer, John, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proc. of the 45th Annual Meeting of the Association of
Computational Linguistics. ACL, Prague, Czech Re-
public, pages 440?447.
Briscoe, Ted and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proc. of the 3rd
LREC. Las Palmas, Gran Canaria, pages 1499?1504.
Fahrni, Angela and Manfred Klenner. 2008. Old Wine
or Warm Beer: Target-Specific Sentiment Analysis of
Adjectives. In Proc.of the Symposium on Affective
Language in Human and Machine, AISB 2008 Con-
vention. pages 60 ? 63.
Ganu, Gayatree, Noemie Elhadad, and Amelie Marian.
2009. Beyond the stars: Improving rating predictions
using review text content. In WebDB.
Hatzivassiloglou, Vasileios and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proc. of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics. ACL, Madrid,
Spain, pages 174?181.
Hu, Minqing and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD ?04: Proc. of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining. ACM, NewYork, NY,
USA, pages 168?177.
Jijkoun, Valentin and Katja Hofmann. 2009. Generating a
non-english subjectivity lexicon: Relations that matter.
In Proc. of the 12th Conference of the European Chap-
ter of the ACL (EACL 2009). ACL, Athens, Greece,
pages 398?405.
Lange, Tilman, Volker Roth, Mikio L. Braun, and
Joachim M. Buhmann. 2004. Stability-based val-
idation of clustering solutions. Neural Comput.
16(6):1299?1323.
Lerman, Kevin, Sasha Blair-Goldensohn, and Ryan Mc-
Donald. 2009. Sentiment summarization: evaluating
and learning user preferences. In EACL ?09: Proc. of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics. ACL,Mor-
ristown, NJ, USA, pages 514?522.
Levine, Erel and Eytan Domany. 2001. Resampling
method for unsupervised estimation of cluster validity.
Neural Comput. 13(11):2573?2593.
Mei, Qiaozhu, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In WWW
?07: Proc. of the 16th international conference on
World Wide Web. ACM, New York, NY, USA, pages
171?180.
Niu, Zheng-Yu, Dong-Hong Ji, and Chew-Lim Tan. 2007.
I2r: three systems for word sense discrimination, chi-
nese word sense disambiguation, and english word
sense disambiguation. In SemEval ?07: Proc. of the
4th International Workshop on Semantic Evaluations.
ACL, Morristown, NJ, USA, pages 177?182.
Pang, Bo and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proc. of the ACL. pages
115?124.
Pang, Bo and Lillian Lee. 2008. Opinion mining and sen-
timent analysis. Foundations and Trends in Informa-
tion Retrieval 2(1-2):1?135.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In EMNLP ?02: Proc. of the
conference on Empirical methods in natural language
processing. ACL, Morristown, NJ, USA, pages 79?86.
Popescu, Ana-Maria and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
HLT ?05: Proc. of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing. ACL, Morristown, NJ, USA, pages
339?346.
Snyder, Benjamin and Regina Barzilay. 2007. Multi-
ple aspect ranking using the good grief algorithm. In
Candace L. Sidner, Tanja Schultz, Matthew Stone, and
ChengXiang Zhai, editors, HLT-NAACL. The Associa-
tion for Computational Linguistics, pages 300?307.
Titov, Ivan and Ryan McDonald. 2008a. A joint model of
text and aspect ratings for sentiment summarization. In
Proc. of ACL-08: HLT . ACL, Columbus, Ohio, pages
308?316.
Titov, Ivan and RyanMcDonald. 2008b. Modeling online
reviews with multi-grain topic models. In WWW ?08:
Proc. of the 17th international conference on World
Wide Web. ACM, New York, NY, pages 111?120.
Turney, Peter. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of 40th Annual Meeting of
the Association for Computational Linguistics. ACL,
Philadelphia, Pennsylvania, USA, pages 417?424.
Zagibalov, Taras and John Carroll. 2008. Automatic seed
word selection for unsupervised sentiment classifica-
tion of chinese text. In COLING ?08: Proc. of the 22nd
International Conference on Computational Linguis-
tics. ACL, Morristown, NJ, USA, pages 1073?1080.
Zhu, X. and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with label propagation. Technical
report, CMU-CALD-02.
812
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 491?495,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatic Assessment of Coverage Quality in Intelligence
Reports
Samuel Brody
School of Communication
and Information
Rutgers University
sdbrody@gmail.com
Paul Kantor
School of Communication
and Information
Rutgers University
paul.kantor@rutgers.edu
Abstract
Common approaches to assessing docu-
ment quality look at shallow aspects, such
as grammar and vocabulary. For many
real-world applications, deeper notions of
quality are needed. This work represents
a first step in a project aimed at devel-
oping computational methods for deep as-
sessment of quality in the domain of intel-
ligence reports. We present an automated
system for ranking intelligence reports
with regard to coverage of relevant mate-
rial. The system employs methodologies
from the field of automatic summarization,
and achieves performance on a par with
human judges, even in the absence of the
underlying information sources.
1 Introduction
Distinguishing between high- and low-quality
documents is an important skill for humans, and
a challenging task for machines. The majority of
previous research on the subject has focused on
low-level measures of quality, such as spelling,
vocabulary and grammar. However, in many
real-world situations, it is necessary to employ
deeper criteria, which look at the content of the
document and the structure of argumentation.
One example where such criteria are essential
is decision-making in the intelligence commu-
nity. This is also a domain where computational
methods can play an important role. In a typi-
cal situation, an intelligence officer faced with an
important decision receives reports from a team
of analysts on a specific topic of interest. Each
decision may involve several areas of interest,
resulting in several collections of reports. Addi-
tionally, the officer may be engaged in many de-
cision processes within a small window of time.
Given the nature of the task, it is vital that
the limited time be used effectively, i.e., that
the highest-quality information be handled first.
Our project aims to provide a system that will
assist intelligence officers in the decision making
process by quickly and accurately ranking re-
ports according to the most important criteria
for the task.
In this paper, as a first step in the project,
we focus on content-related criteria. In particu-
lar, we chose to start with the aspect of ?cover-
age?. Coverage is perhaps the most important
element in a time-sensitive scenario, where an
intelligence officer may need to choose among
several reports while ensuring no relevant and
important topics are overlooked.
2 Related Work
Much of the work on automatic assessment of
document quality has focused on student essays
(e.g., Larkey 1998; Shermis and Burstein 2002;
Burstein et al 2004), for the purpose of grad-
ing or assisting the writers (e.g., ESL students).
This research looks primarily at issues of gram-
mar, lexical selection, etc. For the purpose of
judging the quality of intelligence reports, these
aspects are relatively peripheral, and relevant
mostly through their effect on the overall read-
ability of the document. The criteria judged
most important for determining the quality of
an intelligence report (see Sec. 2.1) are more
complex and deal with a deeper level of repre-
sentation.
In this work, we chose to start with crite-
ria related to content choice. For this task,
491
we propose that the most closely related prior
research is that on automatic summarization,
specifically multi-document extractive summa-
rization. Extractive summarization works along
the following lines (Goldstein et al, 2000): (1)
analyze the input document(s) for important
themes; (2) select the best sentences to include
in the summary, taking into account the sum-
marization aspects (coverage, relevance, redun-
dancy) and generation aspects (grammaticality,
sentence flow, etc.). Since we are interested in
content choice, we focus on the summarization
aspects, starting with coverage. Effective ways
of representing content and ensuring coverage
are the subject of ongoing research in the field
(e.g., Gillick et al 2009, Haghighi and Vander-
wende 2009). In our work, we draw on ele-
ments from this research. However, they must
be adapted to our task of quality assessment and
must take into account the specific characteris-
tics of our domain of intelligence reports. More
detail is provided in Sec. 3.1.
2.1 The ARDA Challenge Workshop
Given the nature of our domain, real-world data
and gold standard evaluations are difficult to ob-
tain. We were fortunate to gain access to the
reports and evaluations from the ARDA work-
shop (Morse et al, 2004), which was conducted
by NIST in 2004. The workshop was designed to
demonstrate the feasibility of assessing the effec-
tiveness of information retrieval systems. Dur-
ing the workshop, seven intelligence analysts
were each asked to use one of several IR sys-
tems to obtain information about eight different
scenarios and write a report about each. This
resulted in 56 individual reports.
The same seven analysts were then asked to
judge each of the 56 reports (including their
own) on several criteria on a scale of 0 (worst)
to 5 (best). These criteria, listed in Table 1,
were chosen by the researchers as desirable in
a ?high-quality? intelligence report. From an
NLP perspective they can be divided into three
broad categories: content selection, structure,
and readability. The written reports, along with
their associated human quality judgments, form
the dataset used in our experiments. As men-
tioned, this work focuses on coverage. When as-
Content
COVER covers the material relevant to the query
NO-IRR avoids irrelevant material
NO-RED avoids redundancy
Structure
ORG organized presentation of material
Readability
CLEAR clear and easy to read and understand
Table 1: Quality criteria used in the ARDA work-
shop, divided into broad categories.
sessing coverage, it is only meaningful to com-
pare reports on the same scenario. Therefore,
we regard our dataset as 8 collections (Scenario
A to Scenario H), each containing 7 reports.
3 Experiments
3.1 Methodology
In the ARDA workshop, the analysts were
tasked to extract and present the information
which was relevant to the query subject. This
can be viewed as a summarization task. In fact,
a high quality report shares many of the charac-
teristics of a good document summary. In par-
ticular, it seeks to cover as much of the impor-
tant information as possible, while avoiding re-
dundancy and irrelevant information.
When seeking to assess these qualities, we
can treat the analysts? reports as output from
(human) summarization systems, and employ
methods from automatic summarization to eval-
uate how well they did.
One challenge to our analysis is that we do
not have access to the information sources used
by the analysts. This limitation is inherent to
the domain, and will necessarily impact the as-
sessment of coverage, since we have no means of
determining whether an analyst has included all
the relevant information to which she, in partic-
ular, had access. We can only assess coverage
with respect to what was included in the other
analysts? reports. For our task, however, this
is sufficient, since our purpose is to identify, for
the person who must choose among them, the
report which is most comprehensive in its cover-
age, or indicate a subset of reports which cover
all topics discussed in the collection as a whole1.
1The absence of the sources also means the system
is only able to compare reports on the same subject, as
opposed to humans, who might rank the coverage quality
492
As a first step in modeling relevant concepts
we employ a word-gram representation, and use
frequency as a measure of relevance. Exam-
ination of high-quality human summaries has
shown that frequency is an important factor
(Nenkova et al, 2006), and word-gram repre-
sentations are employed in many summariza-
tion systems (e.g., Radev et al 2004, Gillick and
Favre 2009). Following Gillick and Favre (2009),
we use a bigram representation of concepts2. For
each document collection D, we calculate the av-
erage prevalence of every bigram concept in the
collection:
prevD(c) =
1
|D|
?
r?D
Countr(c) (1)
Where r labels a report in the collection, and
Countr(c) is the number of times the concept c
appears in report r.
This scoring function gives higher weight to
concepts which many reports mentioned many
times. These are, presumably, the terms consid-
ered important to the subject of interest. We
ignore concepts (bigrams) composed entirely of
stop words. To model the coverage of a report,
we calculate a weighted sum of the concepts it
mentions (multiple mentions do not increase this
score), using the prevalence score as the weight,
as shown in Equation 2.
CoverScore(r ? D) =
?
c?Concepts(r)
prevD(c)
(2)
Here, Concepts(r) is the set of concepts ap-
pearing at least once in report r. The system
produces a ranking of the reports in order of
their coverage score (where highest is considered
best).
3.2 Evaluation
As a gold standard, we use the average of
the scores given to each report by the human
of two reports on completely different subjects, based on
external knowledge. For our usage scenario, this is not
an issue.
2We also experimented with unigram and trigram rep-
resentations, which did not do as well as the bigram rep-
resentation (as suggested by Gillick and Favre 2009).
judges3. Since we are interested in ranking re-
ports by coverage, we convert the scores from
the original numerical scale to a ranked list.
We evaluate the performance of the algorithms
(and of the individual judges) using Kendall?s
Tau to measure concordance with the gold stan-
dard. Kendall?s Tau coefficient (?k) is com-
monly used (e.g., Jijkoun and Hofmann 2009)
to compare rankings, and looks at the number
of pairs of ranked items that agree or disagree
with the ordering in the gold standard. Let
T = {(ai, aj) : ai ?g aj} denote the set of pairs
ordered in the gold standard (ai precedes aj).
Let R = {(al, am) : al ?r am} denote the set of
pairs ordered by a ranking algorithm. C = T?R
is the set of concordant pairs, i.e., pairs ordered
the same way in the gold standard and in the
ranking, and D = T ?R is the set of discordant
pairs. Kendall?s rank correlation coefficient ?k is
defined as follows:
?k =
|C| ? |D|
|T |
(3)
The value of ?k ranges from -1 (reversed rank-
ing) to 1 (perfect agreement), with 0 being
equivalent to a random ranking (50% agree-
ment). As a simple baseline system, we rank the
reports according to their length in words, which
asserts that a longer document has ?more cov-
erage?. For comparison, we also examine agree-
ment between individual human judges and the
gold standard. In each scenario, we calculate
the average agreement (Tau value) between an
individual judge and the gold standard, and also
look at the highest and lowest Tau value from
among the individual judges.
3.3 Results
Figure 1 presents the results of our ranking ex-
periments on each of the eight scenarios.
Human Performance There is a relatively
wide range of performance among the human
3Since the judges in the NIST experiment were also
the writers of the documents, and the workshop report
(Morse et al, 2004) identified a bias of the individual
judges when evaluating their own reports, we did not
include the score given by the report?s author in this
average. I.e, the gold standard score was the average of
the scores given by the 6 judges who were not the author.
493
-0.2
 0
 0.2
 0.4
 0.6
 0.8
 1
HGFEDCBA
A
gr
ee
m
en
t
Scenario
Num. Words
Judges
Concepts
Figure 1: Agreement scores (Kendall?s Tau) for the word-count baseline (Num. Words), the concept-based
algorithm (Concepts). Scores for the individual human judges (Judges) are given as a range from lowest to
highest individual agreement score, with ?x? indicating the average.
judges. This is indicative of the cognitive com-
plexity of the notion of coverage. We can see
that some human judges are better than oth-
ers at assessing this quality (as represented by
the gold standard). It is interesting to note that
there was not a single individual judge who was
worst or best across all cases. A system that out-
performs some individual human judge on this
task can be considered successful, and one that
surpasses the average individual agreement even
more so.
Baseline The experiments bear out the intu-
ition that led to our choice of baseline. The num-
ber of words in a document is significantly corre-
lated with its gold-standard coverage rank. This
simple baseline is surprisingly effective, outper-
forming the worst human judge in seven out of
eight scenarios, and doing better than the aver-
age individual in two of them.
System Performance Our concept-based
ranking system exhibits very strong perfor-
mance4. It is as good or better than the
baseline in all scenarios. It outperforms the
worst individual human judge in seven of the
eight cases, and does better than the average
individual agreement in four. This is in spite of
the fact that the system had no access to the
4Our conclusions are based on the observed differences
in performance, although statistical significance is diffi-
cult to assess, due to the small sample size.
sources of information available to the writers
(and judges) of the reports.
When calculating the overall agreement with
the gold-standard over all the scenarios, our
concept-based system came in second, outper-
forming all but one of the human judges. The
word-count baseline was in the last place, close
behind a human judge. A unigram-based sys-
tem (which was our first attempt at modeling
concepts) tied for third place with two human
judges.
3.4 Discussion and Future Work
We have presented a system for assessing the
relative quality of intelligence reports with re-
gard to their coverage. Our method makes use
of ideas from the summarization literature de-
signed to capture the notion of content units and
relevance. Our system is as accurate as individ-
ual human judges for this concept.
The bigram representation we employ is only
a rough approximation of actual concepts or
themes. We are in the process of obtaining more
documents in the domain, which will allow the
use of more complex models and more sophis-
ticated representations. In particular, we are
considering clusters of terms and probabilistic
topic models such as LDA (Blei et al, 2003).
However, the limitations of our domain, primar-
494
ily the small amount of relatively short docu-
ments, may restrict their applicability, and ad-
vocate instead the use of semantic knowledge
and resources.
This work represents a first step in the com-
plex task of assessing the quality of intelligence
reports. In this paper we focused on coverage -
perhaps the most important aspect in determin-
ing which single report to read among several.
There are many other important factors in as-
sessing quality, as described in Section 2.1. We
will address these in future stages of the quality
assessment project.
4 ACKNOWLEDGMENTS
The authors were funded by an IC Postdoc
Grant (HM 1582-09-01-0022). The second
author also acknowledges the support of the
AQUAINT program, and the KDD program un-
der NSF Grants SES 05-18543 and CCR 00-
87022. We would like to thank Dr. Emile
Morse of NIST for her generosity in providing
the documents and set of judgments from the
ARDA Challenge Workshop project, and Prof.
Dragomir Radev for his assistance and advice.
We would also like to thank the anonymous re-
viewers for their helpful comments.
References
Blei, David M., Andrew Y. Ng, and Michael I.
Jordan. 2003. Latent dirichlet alocation.
Journal of Machine Learning Research 3:993?
1022.
Burstein, Jill, Martin Chodorow, and Claudia
Leacock. 2004. Automated essay evaluation:
the criterion online writing service. AI Mag.
25:27?36.
Gillick, Dan and Benoit Favre. 2009. A scal-
able global model for summarization. In Proc.
of the Workshop on Integer Linear Program-
ming for Natural Language Processing . ACL,
Stroudsburg, PA, USA, ILP ?09, pages 10?18.
Gillick, Daniel, Benoit Favre, Dilek Hakkani-
Tur, Berndt Bohnet, Yang Liu, and Shasha
Xie. 2009. The ICSI/UTD Summarization
System at TAC 2009. In Proc. of the Text
Analysis Conference workshop, Gaithersburg,
MD (USA).
Goldstein, Jade, Vibhu Mittal, Jaime Carbonell,
and Mark Kantrowitz. 2000. Multi-document
summarization by sentence extraction. In
Proc. of the 2000 NAACL-ANLP Work-
shop on Automatic summarization - Volume
4 . Association for Computational Linguis-
tics, Stroudsburg, PA, USA, NAACL-ANLP-
AutoSum ?00, pages 40?48.
Haghighi, Aria and Lucy Vanderwende. 2009.
Exploring content models for multi-document
summarization. In Proc. of Human Language
Technologies: The 2009 Annual Conference
of the North American Chapter of the Asso-
ciation for Computational Linguistics. ACL,
Boulder, Colorado, pages 362?370.
Jijkoun, Valentin and Katja Hofmann. 2009.
Generating a non-english subjectivity lexicon:
Relations that matter. In Proc. of the 12th
Conference of the European Chapter of the
ACL (EACL 2009). ACL, Athens, Greece,
pages 398?405.
Larkey, Leah S. 1998. Automatic essay grad-
ing using text categorization techniques. In
SIGIR ?98: Proceedings of the 21st annual
international ACM SIGIR conference on Re-
search and development in information re-
trieval . ACM, New York, NY, USA, pages 90?
95.
Morse, Emile L., Jean Scholtz, Paul Kantor, Di-
ane Kelly, and Ying Sun. 2004. An investi-
gation of evaluation metrics for analytic ques-
tion answering. Available by request from the
first author.
Nenkova, Ani, Lucy Vanderwende, and Kath-
leen McKeown. 2006. A compositional context
sensitive multi-document summarizer: ex-
ploring the factors that influence summariza-
tion. In SIGIR. ACM, pages 573?580.
Radev, Dragomir R., Hongyan Jing, Ma lgorzata
Stys?, and Daniel Tam. 2004. Centroid-based
summarization of multiple documents. Inf.
Process. Manage. 40:919?938.
Shermis, Mark D. and Jill C. Burstein, editors.
2002. Automated Essay Scoring: A Cross-
disciplinary Perspective. Routledge, 1 edition.
495
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 496?501,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Putting it Simply: a Context-Aware Approach to Lexical Simplification
Or Biran
Computer Science
Columbia University
New York, NY 10027
ob2008@columbia.edu
Samuel Brody
Communication & Information
Rutgers University
New Brunswick, NJ 08901
sdbrody@gmail.com
Noe?mie Elhadad
Biomedical Informatics
Columbia University
New York, NY 10032
noemie@dbmi.columbia.edu
Abstract
We present a method for lexical simplifica-
tion. Simplification rules are learned from a
comparable corpus, and the rules are applied
in a context-aware fashion to input sentences.
Our method is unsupervised. Furthermore, it
does not require any alignment or correspon-
dence among the complex and simple corpora.
We evaluate the simplification according to
three criteria: preservation of grammaticality,
preservation of meaning, and degree of sim-
plification. Results show that our method out-
performs an established simplification base-
line for both meaning preservation and sim-
plification, while maintaining a high level of
grammaticality.
1 Introduction
The task of simplification consists of editing an in-
put text into a version that is less complex linguisti-
cally or more readable. Automated sentence sim-
plification has been investigated mostly as a pre-
processing step with the goal of improving NLP
tasks, such as parsing (Chandrasekar et al, 1996;
Siddharthan, 2004; Jonnalagadda et al, 2009), se-
mantic role labeling (Vickrey and Koller, 2008) and
summarization (Blake et al, 2007). Automated sim-
plification can also be considered as a way to help
end users access relevant information, which would
be too complex to understand if left unedited. As
such, it was proposed as a tool for adults with
aphasia (Carroll et al, 1998; Devlin and Unthank,
2006), hearing-impaired people (Daelemans et al,
2004), readers with low-literacy skills (Williams and
Reiter, 2005), individuals with intellectual disabil-
ities (Huenerfauth et al, 2009), as well as health
INPUT: In 1900, Omaha was the center of a national
uproar over the kidnapping of Edward Cudahy, Jr., the
son of a local meatpacking magnate.
CANDIDATE RULES:
{magnate? king} {magnate? businessman}
OUTPUT: In 1900, Omaha was the center of a national
uproar over the kidnapping of Edward Cudahy, Jr., the
son of a local meatpacking businessman.
Figure 1: Input sentence, candidate simplification rules,
and output sentence.
consumers looking for medical information (El-
hadad and Sutaria, 2007; Dele?ger and Zweigen-
baum, 2009).
Simplification can take place at different levels of
a text ? its overall document structure, the syntax
of its sentences, and the individual phrases or words
in a sentence. In this paper, we present a sentence
simplification approach, which focuses on lexical
simplification.1 The key contributions of our work
are (i) an unsupervised method for learning pairs of
complex and simpler synonyms; and (ii) a context-
aware method for substituting one for the other.
Figure 1 shows an example input sentence. The
word magnate is determined as a candidate for sim-
plification. Two learned rules are available to the
simplification system (substitute magnate with king
or with businessman). In the context of this sen-
tence, the second rule is selected, resulting in the
simpler output sentence.
Our method contributes to research on lexical
simplification (both learning of rules and actual sen-
tence simplification), a topic little investigated thus
far. From a technical perspective, the task of lexi-
cal simplification bears similarity with that of para-
1Our resulting system is available for download at
http://www.cs.columbia.edu/ ob2008/
496
phrase identification (Androutsopoulos and Malaka-
siotis, 2010) and the SemEval-2007 English Lexi-
cal Substitution Task (McCarthy and Navigli, 2007).
However, these do not consider issues of readabil-
ity and linguistic complexity. Our methods lever-
age a large comparable collection of texts: En-
glish Wikipedia2 and Simple English Wikipedia3.
Napoles and Dredze (2010) examined Wikipedia
Simple articles looking for features that characterize
a simple text, with the hope of informing research
in automatic simplification methods. Yatskar et al
(2010) learn lexical simplification rules from the edit
histories of Wikipedia Simple articles. Our method
differs from theirs, as we rely on the two corpora as a
whole, and do not require any aligned or designated
simple/complex sentences when learning simplifica-
tion rules.4
2 Data
We rely on two collections ? English Wikipedia
(EW) and Simple English Wikipedia (SEW). SEW
is a Wikipedia project providing articles in Sim-
ple English, a version of English which uses fewer
words and easier grammar, and which aims to be
easier to read for children, people who are learning
English and people with learning difficulties. Due to
the labor involved in simplifying Wikipedia articles,
only about 2% of the EW articles have been simpli-
fied.
Our method does not assume any specific align-
ment or correspondance between individual EW and
SEW articles. Rather, we leverage SEW only as
an example of an in-domain simple corpus, in or-
der to extract word frequency estimates. Further-
more, we do not make use of any special properties
of Wikipedia (e.g., edit histories). In practice, this
means that our method is suitable for other cases
where there exists a simplified corpus in the same
domain.
The corpora are a snapshot as of April 23, 2010.
EW contains 3,266,245 articles, and SEW contains
60,100 articles. The articles were preprocessed as
follows: all comments, HTML tags, and Wiki links
were removed. Text contained in tables and figures
2http://en.wikipedia.org
3http://simple.wikipedia.org
4Aligning sentences in monolingual comparable corpora has
been investigated (Barzilay and Elhadad, 2003; Nelken and
Shieber, 2006), but is not a focus for this work.
was excluded, leaving only the main body text of
the article. Further preprocessing was carried out
with the Stanford NLP Package5 to tokenize the text,
transform all words to lower case, and identify sen-
tence boundaries.
3 Method
Our sentence simplification system consists of two
main stages: rule extraction and simplification. In
the first stage, simplification rules are extracted from
the corpora. Each rule consists of an ordered word
pair {original? simplified} along with a score indi-
cating the similarity between the words. In the sec-
ond stage, the system decides whether to apply a rule
(i.e., transform the original word into the simplified
one), based on the contextual information.
3.1 Stage 1: Learning Simplification Rules
3.1.1 Obtaining Word Pairs
All content words in the English Wikipedia Cor-
pus (excluding stop words, numbers, and punctua-
tion) were considered as candidates for simplifica-
tion. For each candidate word w, we constructed a
context vectorCVw, containing co-occurrence infor-
mation within a 10-token window. Each dimension
i in the vector corresponds to a single word wi in
the vocabulary, and a single dimension was added to
represent any number token. The value in each di-
mension CVw[i] of the vector was the number of oc-
currences of the corresponding wordwi within a ten-
token window surrounding an instance of the candi-
date word w. Values below a cutoff (2 in our exper-
iments) were discarded to reduce noise and increase
performance.
Next, we consider candidates for substitution.
From all possible word pairs (the Cartesian product
of all words in the corpus vocabulary), we first re-
move pairs of morphological variants. For this pur-
pose, we use MorphAdorner6 for lemmatization, re-
moving words which share a common lemma. We
also prune pairs where one word is a prefix of the
other and the suffix is in {s, es, ed, ly, er, ing}. This
handles some cases which are not covered by Mor-
phAdorner. We use WordNet (Fellbaum, 1998) as
a primary semantic filter. From all remaining word
pairs, we select those in which the second word, in
5http://nlp.stanford.edu/software/index.shtml
6http://morphadorner.northwestern.edu
497
its first sense (as listed in WordNet)7 is a synonym
or hypernym of the first.
Finally, we compute the cosine similarity scores
for the remaining pairs using their context vectors.
3.1.2 Ensuring Simplification
From among our remaining candidate word pairs,
we want to identify those that represent a complex
word which can be replaced by a simpler one. Our
definition of the complexity of a word is based on
two measures: the corpus complexity and the lexical
complexity. Specifically, we define the corpus com-
plexity of a word as
Cw =
fw,English
fw,Simple
where fw,c is the frequency of word w in corpus c,
and the lexical complexity as Lw = |w|, the length
of the word. The final complexity ?w for the word
is given by the product of the two.
?w = Cw ? Lw
After calculating the complexity of all words par-
ticipating in the word pairs, we discard the pairs for
which the first word?s complexity is lower than that
of the second. The remaining pairs constitute the
final list of substitution candidates.
3.1.3 Ensuring Grammaticality
To ensure that our simplification substitutions
maintain the grammaticality of the original sentence,
we generate grammatically consistent rules from
the substitution candidate list. For each candidate
pair (original, simplified), we generate all consis-
tent forms (fi(original), fi(substitute)) of the two
words using MorphAdorner. For verbs, we create
the forms for all possible combinations of tenses and
persons, and for nouns we create forms for both sin-
gular and plural.
For example, the word pair (stride, walk) will gen-
erate the form pairs (stride, walk), (striding, walk-
ing), (strode, walked) and (strides, walks). Signifi-
cantly, the word pair (stride, walked) will generate
7Senses in WordNet are listed in order of frequency. Rather
than attempting explicit disambiguation and adding complex-
ity to the model, we rely on the first sense heuristic, which is
know to be very strong, along with contextual information, as
described in Section 3.2.
exactly the same list of form pairs, eliminating the
original ungrammatical pair.
Finally, each pair (fi(original), fi(substitute)) be-
comes a rule {fi(original) ? fi(substitute)},
with weight Similarity(original, substitute).
3.2 Stage 2: Sentence Simplification
Given an input sentence and the set of rules learned
in the first stage, this stage determines which words
in the sentence should be simplified, and applies
the corresponding rules. The rules are not applied
blindly, however; the context of the input sentence
influences the simplification in two ways:
Word-Sentence Similarity First, we want to en-
sure that the more complex word, which we are at-
tempting to simplify, was not used precisely because
of its complexity - to emphasize a nuance or for its
specific shade of meaning. For example, suppose we
have a rule {Han? Chinese}. We would want to
apply it to a sentence such as ?In 1368 Han rebels
drove out the Mongols?, but to avoid applying it to
a sentence like ?The history of the Han ethnic group
is closely tied to that of China?. The existence of
related words like ethnic and China are clues that
the latter sentence is in a specific, rather than gen-
eral, context and therefore a more general and sim-
pler hypernym is unsuitable. To identify such cases,
we calculate the similarity between the target word
(the candidate for replacement) and the input sen-
tence as a whole. If this similarity is too high, it
might be better not to simplify the original word.
Context Similarity The second factor has to do
with ambiguity. We wish to detect and avoid cases
where a word appears in the sentence with a differ-
ent sense than the one originally considered when
creating the simplification rule. For this purpose, we
examine the similarity between the rule as a whole
(including both the original and the substitute words,
and their associated context vectors) and the context
of the input sentence. If the similarity is high, it is
likely the original word in the sentence and the rule
are about the same sense.
3.2.1 Simplification Procedure
Both factors described above require sufficient
context in the input sentence. Therefore, our sys-
tem does not attempt to simplify sentences with less
than seven content words.
498
Type Gram. Mean. Simp.
Baseline 70.23(+13.10)% 55.95% 46.43%
System 77.91(+8.14)% 62.79% 75.58%
Table 1: Average scores in three categories: grammatical-
ity (Gram.), meaning preservation (Mean.) and simplifi-
cation (Simp.). For grammaticality, we show percent of
examples judged as good, with ok percent in parentheses.
For all other sentences, each content word is ex-
amined in order, ignoring words inside quotation
marks or parentheses. For each word w, the set of
relevant simplification rules {w ? x} is retrieved.
For each rule {w ? x}, unless the replacement
word x already appears in the sentence, our system
does the following:
? Build the vector of sentence context SCVs,w in a
similar manner to that described in Section 3.1,
using the words in a 10-token window surround-
ing w in the input sentence.
? Calculate the cosine similarity of CVw and
SCVs,w. If this value is larger than a manually
specified threshold (0.1 in our experiments), do
not use this rule.
? Create a common context vector CCVw,x for the
rule {w ? x}. The vector contains all fea-
tures common to both words, with the feature
values that are the minimum between them. In
other words, CCVw,x[i] = min(CVw[i], CVx[i]).
We calculate the cosine similarity of the common
context vector and the sentence context vector:
ContextSim = cosine(CCVw,x, SCVs,w)
If the context similarity is larger than a threshold
(0.01), we use this rule to simplify.
If multiple rules apply for the same word, we use
the one with the highest context similarity.
4 Experimental Setup
Baseline We employ the method of Devlin and
Unthank (2006) which replaces a word with its most
frequent synonym (presumed to be the simplest) as
our baseline. To provide a fairer comparison to our
system, we add the restriction that the synonyms
should not share a prefix of four or more letters
(a baseline version of lemmatization) and use Mor-
phAdorner to produce a form that agrees with that
of the original word.
Type Freq. Gram. Mean. Simp.
Base High 63.33(+20)% 46.67% 50%
Sys. High 76.67(+6.66)% 63.33% 73.33%
Base Med 75(+7.14)% 67.86% 42.86%
Sys. Med 72.41(+17.25)% 75.86% 82.76%
Base Low 73.08(+11.54)% 53.85% 46.15%
Sys. Low 85.19(+0)% 48.15% 70.37%
Table 2: Average scores by frequency band
Evaluation Dataset We sampled simplification
examples for manual evaluation with the following
criteria. Among all sentences in English Wikipedia,
we first extracted those where our system chose to
simplify exactly one word, to provide a straightfor-
ward example for the human judges. Of these, we
chose the sentences where the baseline could also
be used to simplify the target word (i.e., the word
had a more frequent synonym), and the baseline re-
placement was different from the system choice. We
included only a single example (simplified sentence)
for each rule.
The evaluation dataset contained 65 sentences.
Each was simplified by our system and the baseline,
resulting in 130 simplification examples (consisting
of an original and a simplified sentence).
Frequency Bands Although we included only a
single example of each rule, some rules could be
applied much more frequently than others, as the
words and associated contexts were common in the
dataset. Since this factor strongly influences the
utility of the system, we examined the performance
along different frequency bands. We split the eval-
uation dataset into three frequency bands of roughly
equal size, resulting in 46 high, 44 med and 40 low.
Judgment Guidelines We divided the simplifica-
tion examples among three annotators 8 and ensured
that no annotator saw both the system and baseline
examples for the same sentence. Each simplification
example was rated on three scales: Grammaticality
- bad, ok, or good; Meaning - did the transforma-
tion preserve the original meaning of the sentence;
and Simplification - did the transformation result in
8The annotators were native English speakers and were not
the authors of this paper. A small portion of the sentence pairs
were duplicated among annotators to calculate pairwise inter-
annotator agreement. Agreement was moderate in all categories
(Cohen?s Kappa = .350? .455 for Simplicity, .475? .530 for
Meaning and .415? .425 for Grammaticality).
499
a simpler sentence.
5 Results and Discussion
Table 1 shows the overall results for the experiment.
Our method is quantitatively better than the base-
line at both grammaticality and meaning preserva-
tion, although the difference is not statistically sig-
nificant. For our main goal of simplification, our
method significantly (p < 0.001) outperforms the
baseline, which represents the established simplifi-
cation strategy of substituting a word with its most
frequent WordNet synonym. The results demon-
strate the value of correctly representing and ad-
dressing content when attempting automatic simpli-
fication.
Table 2 contains the results for each of the fre-
quency bands. Grammaticality is not strongly influ-
enced by frequency, and remains between 80-85%
for both the baseline and our system (considering
the ok judgment as positive). This is not surpris-
ing, since the method for ensuring grammaticality is
largely independent of context, and relies mostly on
a morphological engine. Simplification varies some-
what with frequency, with the best results for the
medium frequency band. In all bands, our system is
significantly better than the baseline. The most no-
ticeable effect is for preservation of meaning. Here,
the performance of the system (and the baseline) is
the best for the medium frequency group. However,
the performance drops significantly for the low fre-
quency band. This is most likely due to sparsity of
data. Since there are few examples from which to
learn, the system is unable to effectively distinguish
between different contexts and meanings of the word
being simplified, and applies the simplification rule
incorrectly.
These results indicate our system can be effec-
tively used for simplification of words that occur
frequently in the domain. In many scenarios, these
are precisely the cases where simplification is most
desirable. For rare words, it may be advisable to
maintain the more complex form, to ensure that the
meaning is preserved.
Future Work Because the method does not place
any restrictions on the complex and simple corpora,
we plan to validate it on different domains and ex-
pect it to be easily portable. We also plan to extend
our method to larger spans of texts, beyond individ-
ual words.
References
Androutsopoulos, Ion and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. Journal of Artificial Intelligence
Research 38:135?187.
Barzilay, Regina and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable cor-
pora. In Proc. EMNLP. pages 25?32.
Blake, Catherine, Julia Kampov, Andreas Or-
phanides, David West, and Cory Lown. 2007.
Query expansion, lexical simplification, and sen-
tence selection strategies for multi-document
summarization. In Proc. DUC.
Carroll, John, Guido Minnen, Yvonne Canning,
Siobhan Devlin, and John Tait. 1998. Practical
simplication of english newspaper text to assist
aphasic readers. In Proc. AAAI Workshop on Inte-
grating Artificial Intelligence and Assistive Tech-
nology.
Chandrasekar, R., Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simplifi-
cation. In Proc. COLING.
Daelemans, Walter, Anja Hthker, and Erik
Tjong Kim Sang. 2004. Automatic sentence
simplification for subtitling in Dutch and English.
In Proc. LREC. pages 1045?1048.
Dele?ger, Louise and Pierre Zweigenbaum. 2009.
Extracting lay paraphrases of specialized expres-
sions from monolingual comparable medical cor-
pora. In Proc. Workshop on Building and Using
Comparable Corpora. pages 2?10.
Devlin, Siobhan and Gary Unthank. 2006. Help-
ing aphasic people process online information. In
Proc. ASSETS. pages 225?226.
Elhadad, Noemie and Komal Sutaria. 2007. Mining
a lexicon of technical terms and lay equivalents.
In Proc. ACL BioNLP Workshop. pages 49?56.
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Database. MIT Press, Cambridge,
MA.
Huenerfauth, Matt, Lijun Feng, and Noe?mie El-
hadad. 2009. Comparing evaluation techniques
500
for text readability software for adults with intel-
lectual disabilities. In Proc. ASSETS. pages 3?10.
Jonnalagadda, Siddhartha, Luis Tari, Jo?rg Haken-
berg, Chitta Baral, and Graciela Gonzalez. 2009.
Towards effective sentence simplification for au-
tomatic processing of biomedical text. In Proc.
NAACL-HLT . pages 177?180.
McCarthy, Diana and Roberto Navigli. 2007.
Semeval-2007 task 10: English lexical substitu-
tion task. In Proc. SemEval. pages 48?53.
Napoles, Courtney and Mark Dredze. 2010. Learn-
ing simple wikipedia: a cogitation in ascertaining
abecedarian language. In Proc. of the NAACL-
HLT Workshop on Computational Linguistics and
Writing. pages 42?50.
Nelken, Rani and Stuart Shieber. 2006. Towards
robust context-sensitive sentence alignment for
monolingual corpora. In Proc. EACL. pages 161?
166.
Siddharthan, Advaith. 2004. Syntactic simplifica-
tion and text cohesion. Technical Report UCAM-
CL-TR-597, University of Cambridge, Computer
Laboratory.
Vickrey, David and Daphne Koller. 2008. Apply-
ing sentence simplification to the CoNLL-2008
shared task. In Proc. CoNLL. pages 268?272.
Williams, Sandra and Ehud Reiter. 2005. Generating
readable texts for readers with low basic skills. In
Proc. ENLG. pages 127?132.
Yatskar, Mark, Bo Pang, Cristian Danescu-
Niculescu-Mizil, and Lillian Lee. 2010. For the
sake of simplicity: Unsupervised extraction of
lexical simplifications from wikipedia. In Proc.
NAACL-HLT . pages 365?368.
501
