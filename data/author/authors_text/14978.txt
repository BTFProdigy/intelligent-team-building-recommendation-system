Predicting Automatic Speech Recognition Performance Using 
Prosodic Cues 
Diane  J .  L i tman and Ju l ia  B .  H i rschberg  Marc  Swer ts  
AT&T Labs - -  Research IPO,  Center for User-System Interact ion 
F lo rham Park,  NJ 07932-0971 USA Eindhoven, The Nether lands 
{ diane,julia} @research. att.com swerts@ipo.tue.nl 
Abst rac t  
In spoken dialogue systems, it is important for a 
system to know how likely a speech recognition hy- 
pothesis is to be correct, so it can reprompt for 
fresh input, or, in cases where many errors have 
occurred, change its interaction strategy or switch 
the caller to a human attendant. We have discov- 
ered prosodic features which more accurately predict 
when a recognition hypothesis contains a word error 
than the acoustic onfidence score thresholds tradi- 
tionally used in automatic speech recognition. We 
present analytic results indicating that there are sig- 
nificant prosodic differences between correctly and 
incorrectly recognized turns in the TOOT train in- 
formation corpus. We then present machine learn- 
ing results howing how the use of prosodic features 
to automatically predict correct versus incorrectly 
recognized turns improves over the use of acoustic 
confidence scores alone. 
1 I n t roduct ion  
One of the central tasks of the dialogue manager 
in most current spoken dialogue systems (SDSs) is 
error handling. The automatic speech recognition 
(ASR) component of such systems i prone to error, 
especially when the system has to operate in noisy 
conditions or when the domain of the system is large. 
Given that it is impossible to fully prevent ASR er- 
rors, it is important for a system to know how likely 
a speech recognition hypothesis to be correct, so 
it can take appropriate action, since users have con- 
siderable difficulty correcting incorrect information 
that is presented by the system as true (Krahmer 
et al, 1999). Such action may include verifying the 
user's input, reprompting for fresh input, or, in cases 
where many errors have occurred, changing the in- 
teraction strategy or switching the caller to a human 
attendant (Smith, 1998; Litman et al, 1999; Langk- 
ilde et al, 1999). Traditionally, the decision to re- 
ject a recognition hypothesis i based on acoustic 
confidence score thresholds, which provide a relia- 
bility measure on the hypothesis and are set in the 
application (Zeljkovic, 1996). However, this process 
often fails, as there is no simple one-to-one mapping 
between low confidence scores and incorrect recog- 
nitions, and the setting of a rejection threshold is 
a matter of trial and error (Bouwman et al, 1999). 
Also, some incorrect recognitions do not necessarily 
lead to misunderstandings at aconceptual level (e.g. 
"a.m." recognized as "in the morning"). 
The current paper looks at prosody as one possible 
predictor of ASR performance. ASR performance 
is known to vary based upon speaking style (Wein- 
traub et al, 1996), speaker gender and age, na- 
tive versus non-native speaker status, and, in gen- 
eral, the deviation of new speech from the training 
data. Some of this variation is linked to prosody, as 
prosodic differences have been found to character- 
ize differences in speaking style (Blaauw, 1992) and 
idiosyncratic differences (Kraayeveld, 1997). Sev- 
eral other studies (Wade et al, 1992; Oviatt et al, 
1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell 
and Gustafson, 1999) report that hyperarticulated 
speech, characterized by careful enunciation, slowed 
speaking rate, and increase in pitch and loudness, 
often occurs when users in human-machine interac- 
tions try to correct system errors. Still others have 
shown that such speech also decreases recognition 
performance (Soltau and Waibel, 1998). Prosodic 
features have also been shown to be effective in 
ranking recognition hypotheses, asa post-processing 
filter to score ASR hypotheses (Hirschberg, 1991; 
Veilleux, 1994; Hirose, 1997). 
In this paper we present results of empirical stud- 
ies testing the hypothesis that prosodic features pro- 
vide an important clue to ASR performance. We 
first present results comparing prosodic analyses of 
correctly and incorrectly recognized speaker turns 
in TOOT, an experimental SDS for obtaining train 
information over the phone. We then describe ma- 
chine learning experiments based on these results 
that explore the predictive power of prosodic fea- 
tures alone and in combination with other automat- 
ically available information, including ASR confi- 
dence scores and recognized string. Our results in- 
dicate that there are significant prosodic differences 
between correctly and incorrectly recognized utter- 
ances. These differences can in fact be used to pre- 
218 
dict whether an utterance has been misrecognized, 
with a high degree of accuracy. 
2 The  TOOT Corpus  
Our corpus consists of a set of dialogues between 
humans and TOOT, an SDS for accessing train 
schedules from the web via telephone, which was 
collected to study both variations in SDS strat- 
egy and user-adapted interaction (Litman and Pan, 
1999). TOOT is implemented on a platform com- 
bining ASR, text-to-speech, a phone interface, a 
finite-state dialogue manager, and application func- 
tions (Kamm et al, 1997). The speech recognizer is 
a speaker-independent hidden Markov model system 
with context-dependent phone models for telephone 
speech and constrained grammars for each dialogue 
state. Confidence scores for recognition were avail- 
able only at the turn, not the word, level (Zeljkovic, 
1996). An example TOOT dialogue is shown in Fig- 
ure 1. 
Subjects performed four tasks with one of sev- 
eral versions of TOOT, that differed in terms of locus 
of initiative (system, user, or mixed), confirmation 
strategy (explicit, implicit, or none), and whether 
these conditions could be changed by the user during 
the task. Subjects were 39 students, 20 native speak- 
ers of standard American English and 19 non-native 
speakers; 16 subjects were female and 23 male. Dia- 
logues were recorded and system and user behavior 
logged automatically. The concept accuracy (CA) of 
each turn was manually labeled by one of the exper- 
imenters. If the ASR output correctly captured all 
the task-related information in the turn (e.g. time, 
departure and arrival cities), the turn was given a 
CA score of 1 (a semantically correct recognition). 
Otherwise, the CA score reflected the percentage of 
correctly recognized task information in the turn. 
The dialogues were also transcribed by hand and 
these transcriptions automatically compared to the 
ASR recognized string to produce a word error rate 
(WEPt) for each turn. Note that a concept can be 
correctly recognized even though all words are not, 
so the CA metric does not penalize for errors that 
are unimportant to overall utterance interpretation. 
For the study described below, we examined 1994 
user turns from 152 dialogues in this corpus. The 
speech recognizer was able to generate a recognized 
string and an associated acoustic confidence score 
per turn for 1975 of these turns. 1 1410 of these 1975 
turns had a CA score of 1 (for an overall conceptual 
accuracy score of 71%) and 961 had a WER of 0 (for 
an overall transcription accuracy score of 49%, with 
a mean WER per turn of 47%). 
1For the remaining turns, ASR output "no speech" (and 
TOOT played a timeout message) or "garbage" (TOOT played 
a rejection message). 
3 D is t ingu ish ing  Cor rect  f rom 
Incor rect  Recogn i t ions  
We first looked for distinguishing prosodic charac- 
teristics of misrecognitions, defining misrecognitions 
in two ways: a) as turns with WER>0; and b) as 
turns with CA<I.  As noted in Section 1, previous 
studies have speculated that hyperarticulated speech 
(slower and louder speech which contains wider pitch 
excursions) may be associated with recognition fail- 
ure. So, we examined the following features for each 
user turn: 2 
? maximum and mean fundamental frequency 
values (F0 Max, F0 Mean) 
? maximum and mean energy values (RMS Max, 
RMS Mean) 
? total duration 
? length of pause preceding the turn (Prior Pause) 
* speaking rate (Tempo) 
? amount of silence within the turn (% Silence) 
F0 and I:LMS values, representing measures of pitch 
excursion and loudness, were calculated from the 
output of Entropic Research Laboratory's pitch 
tracker, get_fO, with no post-correction. Timing vari- 
ation was represented by four features. Duration 
within and length of pause between turns was com- 
puted from the temporal labels associated with each 
turn's beginning and end. Speaking rate was ap- 
proximated in terms of syllables in the recognized 
string per second, while % Silence was defined as the 
percentage of zero frames in the turn, i.e., roughly 
the percentage of time within the turn that the 
speaker was silent. These features were chosen based 
upon previous findings (see Section 1) and observa- 
tions from our data. 
To ensure that our results were speaker indepen- 
dent, we calculated mean values for each speaker's 
recognized turns and their misrecognized turns for 
every feature. Then, for each feature, we created 
vectors of speaker means for recognized and misrec- 
ognized turns and performed paired t-tests on the 
vectors. For example, for the feature "F0 max", 
we calculated mean maxima for misrecognized turns 
and for correctly recognized turns for each of our 
thirty-nine speakers. We then performed a paired 
t-test on these thirty-nine pairs of means to de- 
rive speaker-independent results for differences in F0 
maxima between correct and incorrect recognitions. 
Tables 1 and 2 show results of these compar- 
isons when we calculate misrecognition i terms of 
2While the features were automatically computed, turn 
beginnings and endings were hand segmented in dialogue-level 
speech files, as the turn-level files created by TOOT were not 
available. 
219 
Toot: 
User: 
Toot: 
User: 
Toot: 
Hi, this is AT&T Amtrak schedule system. This is TOOT. How may I help you? 
I want the trains from New York City to Washington DC on Monday at 9:30 in the evening. 
Do you want me to find the trains from New York City to Washington DC on Monday 
approximately at 9:30 in the evening now? 
Yes. 
I am going to get the train schedule for you ...  
Figure 1: Example Dialogue Excerpt with TOOT. 
Table 1: Comparison of Misrecognized (WER>0) 
vs. Recognized Turns by Prosodic Feature Across 
Speakers. Fe tur0 I st tlMeanMisrecdRocd PII 
*F0 Max 7.83 30.31 Hz 0 
*F0 Mean 3.66 ~I.12 Hz 0 
*RMS Max 5.70 235.93 0 
RMS Mean -.57 -8.50 .57 
*Duration 10.30 2.20 sec 0 
*Prior Pause 5.55 .35 sec 0 
Tempo -.05 .15 sps .13 
*% Silence -5.15 -.06% 0 
*significant at a 95% confidence level 
Table 2: Comparison of Misrecognized (CA<I)  
vs. Recognized Turns by Prosodic Feature Across 
Speakers. Fe turo I st t  ? nMisrecdl rlq ecd 
*F0 Max 5.60 29.64 Hz 0 
F0 Mean 1.70 2.10 Hz .10 
*RMS Max 2.86 173.87 .007 
RMS Mean -1.85 -27.75 .07 
*Duration 9.80 2.15 sec 0 
*Prior Pause 4.05 .38 sec 0 
*Tempo -4.21 -.58 sps 0 
% Silence -1.42 -.02% .16 
*significant at a 95% confidence level (p< .05) 
WER>0 and CA<l ,  respectively. These results in- 
dicate that misrecognized turns do differ from cor- 
rectly recognized ones in terms of prosodic features, 
although the features on which they differ vary 
slightly, depending upon the way "misrecognition" 
is defined. Whether defined by WER or CA, mis- 
recognized turns exhibit significantly higher F0 and 
RMS maxima, longer durations, and longer preced- 
ing pauses than correctly recognized speaker turns. 
For a traditional WER definition of misrecognition, 
misrecognitions are slightly higher in mean F0 and 
contain a lower percentage of internal silence. For a 
CA definition, on the other hand, tempo is a signif- 
icant factor, with misrecognitions spoken at a faster 
rate than correct recognitions - - contrary to our hy- 
pothesis about the role of hyperarticulation in recog- 
nition error. 
While the comparisons in Tables 1 and 2 were 
made on the means of raw values for all prosodic fea- 
tures, little difference is found when values are nor- 
malized by value of first or preceding turn, or by con- 
verting to z scores. 3 From this similarity between the 
performance of raw and normalized values, it would 
seem to be relative differences in speakers' prosodic 
values, not deviation from some 'acceptable' range, 
that distinguishes recognition failures from success- 
ful recognitions. A given speaker's turns that are 
The only differences occur for CA defined misrecognition, 
where normalizing by first utterance results in significant dif- 
ferences in mean RMS, and normalizing by preceding turn 
results in no significant differences in tempo. 
higher in pitch or loudness, or that are longer, or 
that follow longer pauses, are less likely to be recog- 
nized correctly than that same speaker's turns that 
are lower in pitch or loudness, shorter, and follow 
shorter pauses - -  however correct recognition is de- 
fined. 
It is interesting to note that the features we found 
to be significant indicators of failed recognitions (F0 
excursion, loudness, long prior pause, and longer du- 
ration) are all features previously associated with 
hyperarticulated speech. Since prior research has 
suggested that speakers may respond to failed recog- 
nition attempts by hyperarticulating, which itself 
may lead to more recognition failures, had we in fact 
simply identified a means of characterizing and iden- 
tifying hyperarticulated speech prosodically? 
Since we had independently labeled all speaker 
turns for evidence of hyperarticulation (two of the 
authors labeled each turn as "not hyperarticulated", 
"some hyperarticulation in the turn", and "hyperar- 
ticulated", following Wade et al (1992)), we were 
able to test this possibility. We excluded any turn 
either labeler had labeled as partially or fully hy- 
perarticulated, and again performed paired t-tests 
on mean values of misrecognized versus recognized 
turns for each speaker. Results show that for both 
WER-defined and CA-defined misrecognitions, not 
only are the same features ignificant differentiators 
when hyperarticulated turns are excluded from the 
analysis, but in addition, tempo also is significant 
for WER-defined misrecognition. So, our findings 
220 
for the prosodic characteristics of recognized and of 
misrecognized turns hold even when perceptibly hy- 
perarticulated turns are excluded from the corpus. 
4 P red ic t ing  M is recogn i t ions  Us ing  
Mach ine  Learn ing  
Given the prosodic differences between misrecog- 
nized and correctly recognized utterances in our 
corpus, is it possible to predict accurately when a 
particular utterance will be misrecognized or not? 
This section describes experiments using the ma- 
chine learning program RIPPER (Cohen, 1996) to au- 
tomatically induce prediction models, using prosodic 
as well as additional features. Like many learning 
programs, RIPPER takes as input the classes to be 
learned, a set of feature names and possible values, 
and training data specifying the class and feature 
values for each training example. RIPPER outputs 
a classification model for predicting the class of fu- 
ture examples. The model is learned using greedy 
search guided by an information gain metric, and is 
expressed as an ordered set of if-then rules. 
Our predicted classes correspond to correct recog- 
nition (T) or not (F). As in Section 3, we examine 
both WER-defined and CA-defined notions of cor- 
rect recognition, and represent each user turn as a 
set of features. The features used in our learning 
experiments include the raw prosodic features in Ta- 
bles 1 and 2 (which we will refer to as the feature set 
"Prosody"), the hyperarticulation score discussed in 
Section 3, and the following additional potential pre- 
dictors of misrecognition (described in Section 2): 
? ASR grammar 
? ASR confidence 
? ASR string 
? system adaptability 
? dialogue strategy 
? task number 
? subject 
? gender 
? native speaker 
The first three features are derived from the ASR 
process (the context-dependent grammar used to 
recognize the turn, the turn-level acoustic onfidence 
score output by the recognizer, and the recognized 
string). We included these features as a baseline 
against which to test new methods of predicting 
misrecognitions, although, currently, we know of no 
ASR system that includes recognized string in its 
rejection calculations. 4 TOOT itself used only the 
4Note that, while the entire recognized string is provided 
to the learning algorithm, RIPPER rules test for the presence 
of particular words in the string. 
first two features to calculate rejections and ask the 
user to repeat the utterance, whenever the confi- 
dence score fell below a pre-defined grammar-specific 
threshold. The other features represent he exper- 
imental conditions under which the data was col- 
lected (whether users could adapt TOOT's dialogue 
strategies, TOOT's initial initiative and confirmation 
strategies, experimental task, speaker's name and 
characteristics). We included these features to de- 
termine the extent o which particulars of task, sub- 
ject, or interaction influenced ASR success rates or 
our ability to predict them; previous work showed 
that these factors impact TOOT's performance (Lit- 
man and Pan, 1999; Hirschberg et al, 1999). Except 
for the task, subject, gender, native language, and 
hyperarticulation scores, all of our features are au- 
tomatically available. 
Table 3 shows the relative performance of a num- 
ber of the feature sets we examined; results here 
are for misrecognition defined in terms of WER. 5 A 
baseline classifier for misrecognition, predicting that 
ASR is always wrong (the majority class of F), has 
an error of 48.66%. The best performing feature 
set includes only the raw prosodic and ASR features 
and reduces this error to an impressive 6.53% +/ -  
.63%. Note that this performance is not improved 
by adding manually labeled features or experimen- 
tal conditions: the feature set corresponding to ALL 
features yielded the statistically equivalent 6.68% 
+/ -  0.63%. 
With respect o the performance of prosodic fea- 
tures, Table 3 shows that using them in conjunction 
with ASR features (error of 6.53%) significantly out- 
performs prosodic features alone (error of 12.76%), 
which, in turn, significantly outperforms any single 
prosodic feature; duration, with an error of 17.42%, 
is the best such feature. Although not shown in 
the table, the unnormalized prosodic features ig- 
nificantly outperform the normalized versions by 7- 
13%. Recall that prosodic features normalized by 
first task utterance, by previous utterance, or by 
z scores showed little performance difference in the 
analyses performed in Section 3. This difference may 
indicate that there are indeed limits on the ranges 
in features uch as F0 and RMS maxima, duration 
and preceding pause within which recognition per- 
formance is optimal. It seems reasonable that ex- 
treme deviation from characteristics of the acoustic 
training material should in fact impact ASR perfor- 
mance, and our experiments may have uncovered, if
not the critical variants, at least important acoustic 
correlates of them. However, it is difficult to com- 
SThe errors and standard errors (SE) result from 25-fold 
cross-validation the 1975 turns where ASR yielded a string 
and confidence. When two errors plus or minus twice the stan- 
dard error do not overlap, they are statistically significantly 
different. 
221 
Table 3: Estimated Error for Predicting Misrecognized Turns (WER>0). 
Features Used Error \] SE 
Prosody, ASR Confidence, ASR String, ASR Grammar 6.53% .63 
ALL 6.68% .63 
Prosody, ASR String 7.34% .75 
ASR Confidence, ASR String, ASR Grammar 9.01% .70 
Prosody, ASR Confidence, ASR Grammar 10.63% .88 
Prosody, ASR Confidence 10.99% .87 
Prosody 12.76% .79 
ASR String 15.24% 1.11 
Duration 17.42% .88 
ASR Confidence, ASR Grammar 17.77% .72 
ASR Confidence 22.23% 1.16 
ASR Grammar 26.28% .84 
Tempo 32.76% 1.03 
Hyperarticulation 35.24% 1.46 
% Silence 36.46% .79 
Prior Pause 36.61% .97 
F0 Max 38.73% .82 
RMS Max 42.23% .96 
F0 Mean 46.33% 1.10 
RMS Mean 48.35% 1.15 
II Majority Baseline J. 48.66%_%_\[___~ 
pare our machine learning results with the statisti- 
cal analyses, since a) the statistical analyses looked 
at only a single prosodic variable at a time, and b) 
data points for that analysis were means calculated 
per speaker, while the learning algorithm operated 
on all utterances, allowing for unequal contributions 
by speaker. 
We now address the issue of what prosodic fea- 
tures are contributing to misrecognition identifica- 
tion, relative to the more traditional ASR tech- 
niques. Do our prosodic features imply correlate 
with information already in use by ASR systems 
(e.g., confidence score, grammar), or at least avail- 
able to them (e.g., recognized string)? First, the 
error using ASR confidence score alone (22.23%) 
is significantly worse than the error when prosodic 
features are combined with ASR confidence scores 
(10.99%) - -  and is also significantly worse than 
the use of prosodic features alone (12.76%). Simi- 
larly, the error using ASR confidence scores and the 
ASR grammar (17.77%) is significantly worse than 
prosodic features alone (12.76%). Thus, prosodic 
features, either alone or in conjunction with tradi- 
tional ASR features, significantly outperform these 
traditional features alone for predicting WER-based 
misrecognitions. 
Another interesting finding from our experiments 
is the predictive power of information available to 
current ASR systems but not made use of in calcu- 
lating rejection likelihoods, the identity of the recog- 
nized string. This feature is in fact the best perform- 
ing single feature in predicting our data (15.24%). 
And, at a 95% confidence level, the error using 
ASR confidence scores, the recognized string, and 
grammar (9.01%) matches the performance of our 
best performing feature set (6.53%). It seems that, 
at least in our task and for our ASR system, the 
appearance of particular words in the recognized 
strings is an extremely useful cue to recognition ac- 
curacy. So, even by making use of information cur- 
rently available from the traditional ASR process, 
ASR systems could improve their performance on 
identifying rejections by a considerable margin. A 
caveat here is that this feature, like grammar state, 
is unlikely to generalize from task to task or recog- 
nizer to recognizer, but these findings suggest hat 
both should be considered as a means of improving 
rejection performance in stable systems. 
The classification model earned from the best per- 
forming feature set in Table 3 is shown in Figure 2. 6 
The first rule RIPPER finds with this feature set is 
that if the user turn is less than .9 seconds and the 
recognized string contains the word "yes" (and possi- 
bly other words as well), with an acoustic onfidence 
score > -2.6, then predict hat the turn will be cor- 
rectly recognized.7 Note that all of the prosodic fea- 
6Rules are presented in order of importance in classifying 
data. When multiple rules are applicable, RIPPER uses the 
first rule. 
7The confidence scores observed in our data ranged from 
a high of -0.087662 to a low of-9.884418. 
222 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
if (duration 
else F 
< 0.897073) A (confidence > -2.62744 ) A (string contains 'yes') then T 
< 1.03872 ) A (confidence > -2.69775) A (string contains 'no') then T 
< 0.982051) A (confidence > -1.99705) A (tempo > 3.1147) then T 
< 0.813633) A (duration > 0.642652) A (confidence > -3.33945) A (F0 Mean > 176.794) then T 
< 1.30312) A (confidence > -3.37301) A (% silences ~_ 0.647059) then T 
0.610734) A (confidence > -3.37301) A (% silences > 0.521739) then T 
< 1.09537) A (string contains 'Baltimore') then T 
< 0.982051) A (string contains 'no') then T 
< 1.1803) A (confidence > -2.93085) A (grammar ---- date) then T 
< 1.09537) A (confidence > -2.30717) A (% silences > 0.356436) A (F0 Max > 249.225) then T 
< 0.868743) A (confidence > -4.14926 ) A (% silences > 0.51923) A (F0 Max > 205.296) then T 
< 1.18036) A (string contains 'Philadelphia') then T 
Figure 2: Ruleset for Predicting Correctly Recognized Turns (WER = 0) from Prosodic and ASR Features. 
tures except for RMS mean, max, and prior pause 
appear in at least one rule, and that the features 
shown to be significant in our statistical analyses 
(Section 3) are not the same features as in the rules. 
But, as noted above, our data points in these two 
experiments differ. It is useful to note though, that 
while this ruleset contains all three ASR features, 
none of the experimental parameters was found to 
be a useful predictor, suggesting that our results are 
not specific to the particular conditions of and par- 
ticipants in the corpus collection, although they are 
specific to the lexicon and grammars. 
Results of our learning experiments with mis- 
recognition defined in terms of CA rather than WER 
show the overall role of the features which predict 
WER-defined misrecognition to be less successful 
in predicting CA-defined error. Table 4 shows the 
relative performance of the same feature sets dis- 
cussed above, with misrecognition ow defined in 
terms of CA<I.  As with the WER experiments, the 
best performing feature set makes use of prosodic 
and ASR-derived features. However, the predictive 
power of prosodic over ASR features decreases when 
misrecognition is defined in terms of CA - -  which is 
particularly interesting since ASR confidence scores 
are intended to predict WER rather than CA; the er- 
ror rate using ASR confidence scores alone (13.52%) 
is now significantly lower than the error obtained 
using prosody (18.18%). However, prosodic features 
still improve the predictive power of ASR confidence 
scores, to 11.34%, although this difference is not sig- 
nificant at a 95% confidence level. And the error 
rate of the three ASR features combined (11.70%) is 
reduced to the lowest error rate in our table when 
prosodic features are added (10.43%); this error rate 
is (just) significantly different from the use of ASR 
confidence scores alone. Thus, for CA-defined mis- 
recognitions, our experiments have uncovered only 
minor improvements over traditional ASR rejection 
calculation procedures. 
5 D iscuss ion  
A statistical comparison of recognized versus mis- 
recognized utterances indicates that F0 excursion, 
loudness, longer prior pause, and longer duration 
are significant prosodic haracteristics of both WER 
and CA-defined failed recognition attempts. Results 
from a set of machine learning experiments show 
that prosodic differences can in fact be used to im- 
prove the prediction of misrecognitions with a high 
degree of accuracy (12.76% error) for WER-based 
misrecognit ions-  and an even higher degree (6.53% 
error) when combined with information currently 
available from ASR systems. The use of ASR confi- 
dence scores alone had a predicted WER of 22.23%, 
so the improvement over traditional methods is quite 
considerable. For CA-defined misrecognitions, the 
improvement provided by prosodic features is con- 
siderably less. One of our future research directions 
will be to understand this difference. 
Another future direction will be to address the is- 
sue of just why  prosodic features provide such use- 
ful indicators of recognition failure. Do the features 
themselves make recognition difficult, or are they 
instead indirect correlates of other phenomena not 
captured in our study? While the negative influence 
of speaking rate variation on ASR has been reported 
before (e.g. (Ostendorf et al, 1996), it is tradition- 
ally assumed that ASR is impervious to differences 
in F0 and RMS; yet, it is known that F0 and RMS 
variations co-vary to some extent with spectral char- 
acteristics (e.g. (Swerts and Veldhuis, 1997; Fant et 
al., 1995)), so that it is not unlikely that utterances 
with extreme values for these may differ critically 
from the training data. Other prosodic features may 
be more indirect indicators of errors. Longer ut- 
terances may simply provide more chance for error 
than shorter ones, while speakers who pause longer 
before utterances and take more time making them 
may also produce more disfluencies than others. 
We are currently replicating our experiment on a 
new domain with a new speech recognizer. We are 
examining the W99 corpus, which was collected in a 
223 
Table 4: Estimated Error for Predicting Misrecognized Turns (CA<l). 
Features Used \[ Error 
Prosody, ASR Confidence~ ASR String, ASR Grammar 10.43% .63 
ALL 10.68% .71 
Prosody, ASR Confidence, ASR Grammar 11.24% .68 
Prosody, ASR Confidence 11.34% .64 
ASR Confidence, ASR String, ASR Grammar 11.70% .68 
ASR Confidence 13.52% .82 
ASR Confidence, ASR Grammar 13.52% .84 
ASR String 13.62% .83 
Prosody, ASR String 15.04% .84 
Prosody 18.18% .85 
Duration 18.38% .90 
ASR Grammar 22.73% .96 
Tempo 24.61% 1.28 
Hyperarticulation 25.27% 1.05 
F0 Mean 28.61% 1.19 
F0 Max 28.76% .90 
RMS Mean 28.86% 1.17 
% Silence 28.91% 1.23 
RMS Max 29.01% 1.16 
Prior Pause 29.22% 1.26 
Majority Baseline \[ 28.61% 
spoken dialogue system that supported registration, 
checking paper status, and information access for the 
IEEE Automatic Speech Recognition and Under- 
standing Workshop (ASRU99) (Rahim et al, 1999). 
This system employed the AT&T WATSON speech 
recognition technology (Sharp et al, 1997). Prelim- 
inary results indicate that our TOOT results do in 
fact hold up across recognizers. We also are extend- 
ing our TOOT corpus analysis to include prosodic 
analyses of turns in which users become aware of 
misrecognitions and correct them. In addition, we 
are exploring whether prosodic differences can help 
explain the "goat" phenomenon - -  the fact that 
some voices are recognized much more poorly than 
others (Doddington et al, 1998; Hirschberg et al, 
1999). Our ultimate goal is to provide prosodically- 
based mechanisms for identifying and reacting to 
ASR failures in SDS systems. 
Acknowledgements  
We would like to thank Jennifer Chu-Carroll, Candy 
Kamm, participants in the AT&T "SLUG" seminar 
series, and participants in the 1999 JHU Summer 
Language Engineering Workshop, for providing us 
with useful comments on this research and on earlier 
versions of this paper. 
References 
Linda Bell and Joakim Gustafson. 1999. Repe- 
tition and its phonetic realizations: Investigat- 
ing a Swedish database of spontaneous computer- 
directed speech. In Proceedings of ICPhS-99, San 
Francisco. International Congress of Phonetic Sci- 
ences. 
E. Blaauw. 1992. Phonetic differences between read 
and spontaneous speech. In Proceedings of IC- 
SLP92, volume 1, pages 751-758, Banff. 
A. G. Bouwman, J. Sturm, and L. Boves. 1999. 
Incorporating confidence measures in the dutch 
train timetable information system developed in 
the ARISE project. In Proc. International Con- 
ference on Acoustics, Speech and Signal Process- 
ing, volume 1, pages 493-496, Phoenix. 
William Cohen. 1996. Learning trees and rules with 
set-valued features. In l$th Conference of the 
American Association of Artificial Intelligence, 
AAAI. 
George Doddington, Walter Liggett, Alvin Martin, 
Mark Przybocki, and Douglas Reynolds. 1998. 
Sheep, goats, lambs and wolves: A statistical anal- 
ysis of speaker performance in the NIST 1998 
speaker ecognition evaluation. In Proceedings of 
ICSLP-98. 
G. Fant, J. Liljencrants, I. Karlsson, and 
M. B?veg?rd. 1995. Time and frequency do- 
main aspects of voice source modelling. BR 
Speechmaps 6975, ESPRIT. Deliverable 27 WP 
1.3. 
Keikichi Hirose. 1997. Disambiguating recogni- 
tion results by prosodic features. In Computing 
224 
Prosody: Computational Models for Processing 
Spontaneous Speech, pages 327-342. Springer. 
Julia Hirschberg, Diane Litman, and Marc Swerts. 
1999. Prosodic cues to recognition errors. In Pro- 
ceedings of the Automatic Speech Recognition and 
Understandin9 Workshop (ASRU'99). 
Julia Hirschberg. 1991. Using text analysis to pre- 
dict intonational boundaries. In Proceedings of the 
Second European Conference on Speech Commu- 
nication and Technology, Genova. ESCA. 
C. Kamm, S. Narayanan, D. Dutton, and R. Rite- 
nour. 1997. Evaluating spoken dialog systems 
for telecommunication services. In 5th European 
Conference on Speech Technology and Communi- 
cation, EUROSPEECH 97. 
Hans Kraayeveld. 1997. Idiosyncrasy in prosody. 
Speaker and speaker group identification i  Dutch 
using melodic and temporal information. Ph.D. 
thesis, Nijmegen University. 
E. Krahmer, M. Swerts, M. Theune, and 
M. Weegels. 1999. Error spotting in human- 
machine interactions. In Proceedings of 
E UR OSPEECH- 99. 
Irene Langkilde, Marilyn Walker, Jerry Wright, 
A1 Gorin, and Diane Litman. 1999. Automatic 
prediction of problematic human-computer dia- 
logues in 'how may i help you?'. In Proceedings 
of the Automatic Speech Recognition and Under- 
standin 9 Workshop (ASRU'99). 
Gina-Anne Levow. 1998. Characterizing and recog- 
nizing spoken corrections in human-computer dia- 
logue. In Proceedings of the 36th Annual Meeting 
of the Association of Computational Linguistics, 
COLING/ACL 98, pages 736-742. 
Diane J. Litman and Shimei Pan. 1999. Empirically 
evaluating an adaptable spoken dialogue system. 
In Proceedings of the 7th International Conference 
on User Modeling (UM). 
Diane J. Litman, Marilyn A. Walker, and Michael J. 
Kearns. 1999. Automatic detection of poor 
speech recognition at the dialogue level. In Pro- 
ceedings of the 37th Annual Meeting of the As- 
sociation of Computational Linguistics , ACL99, 
pages 309-316. 
M. Ostendorf, B. Byrne, M. Bacchiani, M. Finke, 
A. Gunawardana, K. Ross, S. Roweis, E. Shriberg, 
D. Talkin, A. Waibel, B. Wheatley, and T. Zep- 
penfeld. 1996. Modeling systematic variations 
in pronunciation via a language-dependent hid- 
den speaking mode. Report on 1996 CLSP/JHU 
Workshop on Innovative Techniques for Large Vo- 
cabulary Continuous Speech Recognition. 
S. L. Oviatt, G. Levow, M. MacEarchern, and 
K. Kuhn. 1996. Modeling hyperarticulate speech 
during human-computer error resolution. In Pro- 
ceedings of ICSLP-96, pages 801-804, Philadel- 
phia. 
M. Rahim, R. Pieracini, W. Eckert, E. Levin, G. Di 
Fabbrizio, G. Riccardi, C. Lin, and C. Kamm. 
1999. W99 - a spoken dialogue system for the 
asru'99 workshop. In Proc. ASRU'99. 
R.D. Sharp, E. Bocchieri, C. Castillo, 
S. Parthasarathy, C. Rath, M. Riley, and 
J Rowland. 1997. The watson speech recognition 
engine. In Proc. ICASSP97, pages 4065-4068. 
Ronnie W. Smith. 1998. An evaluation of strate- 
gies for selectively verifying utterance meanings 
in spoken natural language dialog. International 
Journal of Human- Computer Studies, 48:627-647. 
Hagen Soltau and Alex Waibel. 1998. On the in- 
fluence of hyperarticulated speech on recognition 
performance. In Proceedings of ICSLP-98, Syd- 
ney. International Conference on Spoken Lan- 
guage Processing. 
M. Swerts and M. Ostendorf. 1997. Prosodic 
and lexical indications of discourse structure in 
human-machine interactions. Speech Communica- 
tion, 22:25-41. 
Marc Swerts and Raymond Veldhuis. 1997. Interac- 
tions between intonation and glottal-pulse char- 
acteristics. In A. Botinis, G. Kouroupetroglou, 
and G. Carayiannis, editors, Intonation: Theory, 
Models and Applications, pages 297-300, Athens. 
ESCA. 
Nanette Veilleux. 1994. Computational Models of 
the Prosody/Syntax Mapping for Spoken Language 
Systems. Ph.D. thesis, Boston University. 
E. Wade, E. E. Shriberg, and P. J. Price. 1992. User 
behaviors affecting speech recognition. In Pro- 
ceedings of ICSLP-92, volume 2, pages 995-998, 
Banff. 
M. Weintraub, K. Taussig, K. Hunicke-Smith, and 
A. Snodgrass. 1996. Effect of speaking style on 
LVCSR performance. In Proceedings of ICSLP- 
96, Philadelphia. International Conference on 
Spoken Language Processing. 
Ilija Zeljkovic. 1996. Decoding optimal state se- 
quences with smooth state likelihoods. In Interna- 
tional Conference on Acoustics, Speech, and Sig- 
nal Processing, ICASSP 96, pages 129-132. 
225 
SCANMail: Audio Navigation in the Voicemail Domain
Michiel Bacchiani Julia Hirschberg Aaron Rosenberg Steve Whittaker
Donald Hindle Phil Isenhour Mark Jones Litza Stark
Gary Zamchick
fmichiel,julia,aer,stevewg@research.att.com, dhindle@answerlogic.com, isenhour@vt.edu,
jones@research.att.com, litza@udel.edu, zamchick@attlabs.att.com
AT&T Labs ? Research
180 Park Avenue
Florham Park, NJ 07932-0971, USA
ABSTRACT
This paper describes SCANMail, a system that allows users to
browse and search their voicemail messages by content through
a GUI. Content based navigation is realized by use of automatic
speech recognition, information retrieval, information extraction
and human computer interaction technology. In addition to the
browsing and querying functionalities, acoustics-based caller ID
technology is used to proposes caller names from existing caller
acoustic models trained from user feedback. The GUI browser also
provides a note-taking capability. Comparing SCANMail to a regu-
lar voicemail interface in a user study, SCANMail performed better
both in terms of objective (time to and quality of solutions) as well
as subjective objectives.
1. INTRODUCTION
Increasing amounts of public, corporate, and private audio present
a major challenge to speech, information retrieval, and human-
computer interaction research: how can we help people to take ad-
vantage of these resources when current techniques for navigating
them fall far short of text-based search methods? In this paper,
we describe SCANMail, a system that employs automatic speech
recognition (ASR), information retrieval (IR), information extrac-
tion (IE), and human computer interaction (HCI) technology to per-
mit users to browse and search their voicemail messages by content
through a GUI interface. A CallerId server also proposes caller
names from existing caller acoustic models and is trained from
user feedback. An Email server sends the original message plus
its ASR transcription to a mailing address specified in the user?s
profile. The SCANMail GUI also provides note-taking capabilities
as well as browsing and querying features. Access to messages and
information about them is presented to the user via a Java applet
running under Netscape. Figure 1 shows the SCANMail GUI.
.
2. SYSTEM DESCRIPTION
In SCANMail, messages are first retrieved from a voicemail server,
then processed by the ASR server that provides a transcription. The
message audio and/or transcription are then passed to the IE, IR,
Email, and CallerId servers. The acoustic and language model of
the recognizer, and the IE and IR servers are trained on 60 hours of
a 100 hour voicemail corpus, transcribed and hand labeled for tele-
phone numbers, caller names, times, dates, greetings and closings.
The corpus includes approximately 10,000 messages from approx-
imately 2500 speakers. About 90% of the messages were recorded
from regular handsets, the rest from cellular and speaker-phones.
The corpus is approximately gender balanced and approximately
12% of the messages were from non-native speakers. The mean
duration of the messages was 36.4 seconds; the median was 30.0
seconds.
2.1 Automatic Speech Recognition
The baseline ASR system is a decision-tree based state-clustered
triphone system with 8k tied states. The emission probabilities of
the states are modeled by 12 component Gaussian mixture distribu-
tions. The system uses a 14k vocabulary, automatically generated
by the AT&T Labs NextGen Text To Speech system. The language
model is a Katz-style backoff trigram trained on 700k words from
the transcriptions of the 60 hour training set. The word-error rate
of this system on a 40 hour test set is 34.9%.
Since the messages come from a highly variable source both in
terms of speaker as well as channel characteristics, transcription ac-
curacy is significantly improved by application of various normal-
ization techniques, developed for Switchboard evaluations [9]. The
ASR server uses Vocal Tract Length Normalization (VTLN) [5],
Constrained Modelspace Adaptation (CMA) [3], Maximum Like-
lihood Linear Regression (MLLR) [6] and Semi-Tied Covariances
(STC) [4] to obtain progressively more accurate acoustic models
and uses these in a rescoring framework. In contrast to Switch-
board, voicemail messages are generally too short too allow direct
application of the normalization techniques. A novel message clus-
tering algorithm based on MLLR likelihood [1] is used to guarantee
sufficient data for normalization. The final transcripts, obtained af-
ter 6 recognition passes, have a word error rate of 28.7% ? a 6.2%
accuracy improvement. Gender dependency provides 1.6% of this
gain. VTLN then additively improves accuracy with 1.0% when
applied only on the test data and an additional 0.3% when sub-
sequently applied with a VTLN trained model. The use of STC
further improves accuracy with 1.2%. Finally CMA and MLLR
provide additive gains of 1.5% and 0.6% respectively. The ASR
Figure 1: The SCANMail User Interface
server, running on a 667 MHz 21264 Alpha processor, produces
the final transcripts in approximately 20 times real-time.
2.2 Information Retrieval
Messages transcripts are indexed by the IR server using the SMART
IR [8, 2] engine. SMART is based on the vector space model
of information retrieval. It generates weighted term (word) vec-
tors for the automatic transcriptions of the messages. SMART pre-
processes the automatic transcriptions of each new message by to-
kenizing the text into words, removing common words that appear
on its stop-list, and performing stemming on the remaining words
to derive a set of terms, against which later user queries can be
compared. When the IR server is used to execute a user query, the
query terms are also converted into weighted term vectors. Vector
inner-product similarity computation is then used to rank messages
in decreasing order of their similarity to the user query.
2.3 Information Extraction
Key information is extracted from the ASR transcription by the
IE server, which currently extracts any phone numbers identified
in the message. Currently, this is done by recognizing digit strings
and scoring them based on the sequence length. An improved ex-
traction algorithm, trained on our hand-labeled voicemail corpus,
employs a digit string recognizer combined with a trigram language
model, to recognize strings in their lexical contexts, e.g. <word>
<digit string> <word>.
2.4 Caller Identification
The CallerID server proposes caller names by matching mes-
sages against existing caller models; this module is trained from
user feedback. The caller identification capability is based on text
independent speaker recognition techniques applied to the processed
speech in the voicemail messages. A user may elect to label a mes-
sage he/she has reviewed with a caller name for the purpose of
creating a speaker model for that caller. When the cumulative du-
ration of such user-labeled messages is sufficient, a caller model
is constructed. Subsequent messages will be processed and scored
against this caller model and models for other callers the user may
have designated. If the best matching model score for an incom-
ing message exceeds a decision threshold, a caller name hypothesis
is sent to the GUI client; if there is no PBX-supplied identifica-
tion (i.e. caller name supplied from the owner of the extension for
calls internal to the PBX), the CallerId hypothesis is presented in
the message header, for either accepting or editing by the user; if
there is a PBX identification, the CallerId hypothesis appears as the
first item in a user ?contact menu?, together with all previously id?d
callers for that user. To optimize the use of the available speech
data, and to speed model-building, caller models are shared among
users. Details and a performance evaluation of the CallerId process
are described in [7].
2.5 Graphical User Interface
In the SCANMail GUI, users see message headers (callerid, time
and date, length in seconds, first line of any attached note, and
presence of extracted phone numbers) as well as a thumbnail and
the ASR transcription of the current message. Any note attached
to the current message is also displayed. A search panel permits
users to search the contents of their mailbox by inputting any text
query. Results are presented in a new search window, with key-
words color-coded in the query, transcript, and thumbnail.
2.6 User Studies
User studies compared SCANMail with a standard over-the-phone
voicemail access. Eight subjects performed a series of fact-finding,
relevance ranking, and summarization tasks on artificial mailboxes
of twenty messages each, using either SCANMail or phone access.
SCANMail showed advantages for fact-finding and relevance rank-
ing tasks in quality of solution normalized by time to solution, for
fact-finding in time to solution and in overall user preference. Nor-
malized performance scores are higher when subjects employ IR
searches that are successful (i.e. the queries they choose contain
words correctly recognized by the recognizer) and for subjects who
listen to less audio and rely more upon the transcripts. However, we
also found that SCANMail?s search capability can be misleading,
causing subjects to assume that they have found all relevant doc-
uments when in fact some are NOT retrieved, and that when sub-
jects rely upon the accuracy of the ASR transcript, they can miss
crucial but unrecognized information. A trial of 10 friendly users
is currently underway, with modifications to access functionality
suggested by our subject users. A larger trial of the system is be-
ing prepared, for more extensive testing of user behavior with their
own mailboxes over time.
Acknowledgements
The authors would like to thank Andrej Ljolje, S. Parthasarathy,
Fernando Pereira, and Amit Singhal for their help in developing
this application.
3. REFERENCES
[1] M. Bacchiani. Using maximum likelihood linear regression
for segment clustering and speaker identification. In
Proceedings of the Sixth International Conference on Spoken
Language Processing, volume 4, pages 536?539, Beijing,
2000.
[2] C. Buckley. Implementation of the SMART information
retrieval system. Technical Report TR85-686, Department of
Computer Science, Cornell University, Ithaca, NY 14853,
May 1985.
[3] M. J. F. Gales. Maximum likelihood linear transformations for
hmm-based speech recognition. Computer Speech and
Language, pages 75?90, 1998.
[4] M. J. F. Gales. Semi-tied covariance matrices for hidden
markov models. IEEE Transactions on Acoustics, Speech, and
Signal Processing, 7(3), 1999.
[5] T. Kamm, G. Andreou, and J. Cohen. Vocal tract
normalization in speech recognition: Compensating for
systematic speaker variability. In Proceedings of the 15th
Annual Speech Research Symposium, pages 161?167, Johns
Hopkins University, Baltimore, MD, 1995.
[6] C. J. Legetter and P. C. Woodland. Maximum likelihood linear
regression for speaker adaptation of continuous density hidden
markov models. Computer Speech and Language, pages
171?185, 1995.
[7] A. Rosenberg, S. Parthasarathy, J. Hirschberg, and
S. Whittaker. Foldering voicemail messages by caller using
text independent speaker recognition. In Proceedings of the
Sixth International Conference on Spoken Language
Processing, Beijing, 2000.
[8] G. Salton, editor. The SMART Retrieval System?Experiments
in Automatic Document Retrieval. Prentice Hall Inc.,
Englewood Cliffs, NJ, 1971.
[9] Proceedings of the Speech Transcription Workshop,
University of Maryland, May 2000.
 
		Modeling Local Context for Pitch Accent Prediction
Shimei Pan
Department of Computer Science
Columbia University
New York, NY, 10027, USA
pan@cs.columbia.edu
Julia Hirschberg
AT&T Labs-Research
Florham Park, NJ, 07932-0971, USA
julia@research.att.com
Abstract
Pitch accent placement is a major
topic in intonational phonology re-
search and its application to speech
synthesis. What factors inuence
whether or not a word is made
intonationally prominent or not is
an open question. In this paper,
we investigate how one aspect of a
word's local context | its colloca-
tion with neighboring words | inu-
ences whether it is accented or not.
Results of experiments on two tran-
scribed speech corpora in a medical
domain show that such collocation
information is a useful predictor of
pitch accent placement.
1 Introduction
In English, speakers make some words more
intonationally prominent than others. These
words are said to be accented or to bear
pitch accents. Accented words are typically
louder and longer than their unaccented coun-
terparts, and their stressable syllable is usu-
ally aligned with an excursion in the funda-
mental frequency. This excursion will dier
in shape according to the type of pitch ac-
cent. Pitch accent type, in turn, inuences
listeners' interpretation of the accented word
or its larger syntactic constituent. Previous
research has associated pitch accent with vari-
ation in various types of information status,
including the given/new distinction, focus, and
contrastiveness, inter alia. Assigning pitch ac-
cent in speech generation systems which em-
ploy speech synthesizers for output is thus crit-
ical to system performance: not only must one
convey meaning naturally, as humans would,
but one must avoid conveying mis-information
which reliance on the synthesizers' defaults
may result in.
The speech generation work discussed here
is part of a larger eort in developing an intel-
ligent multimedia presentation generation sys-
tem called MAGIC (Medical Abstract Gen-
eration for Intensive Care) (Dalal et al,
1996). In MAGIC, given a patient's medical
record stored at Columbia Presbyterian Medi-
cal Center (CPMC)'s on-line database system,
the system automatically generates a post-
operative status report for a patient who has
just undergone bypass surgery. There are two
media-specic generators in MAGIC: a graph-
ics generator which automatically produces
graphical presentations from database entities,
and a spoken language generator which auto-
matically produces coherent spoken language
presentations from these entities. The graph-
ical and the speech generators communicate
with each other on the y to ensure that the
nal multimedia output is synchronized.
In order to produce natural and coherent
speech output, MAGIC's spoken language gen-
erator models a collection of speech features,
such as accenting and intonational phrasing,
which are critical to the naturalness and intel-
ligibility of output speech. In order to assign
these features accurately, the system needs to
identify useful correlates of accent and phrase
boundary location to use as predictors. This
work represents part of our eorts in identi-
fying useful predictors for pitch accent place-
ment.
Pitch accent placement has long been a re-
search focus for scientists working on phonol-
ogy, speech analysis and synthesis (Bolinger,
1989; Ladd, 1996). In general, syntactic fea-
tures are the most widely used features in
pitch accent predication. For example, part-
of-speech is traditionally the most useful sin-
gle pitch accent predictor (Hirschberg, 1993).
Function words, such as prepositions and ar-
ticles, are less likely to be accented, while
content words, such as nouns and adjectives,
are more likely to be accented. Other lin-
guistic features, such as inferred given/new
status (Hirschberg, 1993; Brown, 1983), con-
trastiveness (Bolinger, 1961), and discourse
structure (Nakatani, 1998), have also been ex-
amined to explain accent assignment in large
speech corpora. In a previous study (Pan and
McKeown, 1998; Pan andMcKeown, 1999), we
investigated how features such as deep syntac-
tic/semantic structure and word informative-
ness correlate with accent placement. In this
paper, we focus on how local context inuences
accent patterns. More specically, we investi-
gate how word collocation inuences whether
nouns are accented or not.
Determining which nouns are accented and
which are not is challenging, since part-of-
speech information cannot help here. So, other
accent predictors must be found. There are
some advantages in looking only at one word
class. We eliminate the interaction between
part-of-speech and collocation, so that the in-
uence of collocation is easier to identify. It
also seems likely that collocation may have a
greater impact on content words, like nouns,
than on function words, like prepositions.
Previous researchers have speculated that
word collocation aects stress assignment of
noun phrases in English. For example, James
Marchand (1993) notes how familiar colloca-
tions change their stress, witness the American
pronunciation of `Little House' [in the televi-
sion series Little House on the Prairie], where
stress used to be on HOUSE, but now, since the
series is so familiar, is placed on the LITTLE.
That is, for collocated words, stress shifts to
the left element of the compound. However,
there are numerous counter-examples: con-
sider apple PIE, which retains a right stress
pattern, despite the collocation. So, the ex-
tent to which collocational status aects ac-
cent patterns is still unclear.
Despite some preliminary investigation
(Liberman and Sproat, 1992), word colloca-
tion information has not, to our knowledge,
been successfully used to model pitch accent
assignment; nor has it been incorporated into
any existing speech synthesis systems. In this
paper, we empirically verify the usefulness of
word collocation for accent prediction. In Sec-
tion 2, we describe our annotated speech cor-
pora. In Section 3, we present a description of
the collocation measures we investigated. Sec-
tion 4 to 7 describe our analyses and machine
learning experiments in which we attempt to
predict accent location. In Section 8 we sum
up our results and discuss plans for further re-
search.
2 Speech Corpora
From the medical domain described in Section
1, we collected two speech corpora and one text
corpus for pitch accent modeling. The speech
corpora consist of one multi-speaker sponta-
neous corpus, containing twenty segments and
totaling fty minutes, and one read corpus of
ve segments, read by a single speaker and to-
taling eleven minutes of speech. The text cor-
pus consists of 3.5 million words from 7375 dis-
charge summaries of patients who had under-
gone surgery. The speech corpora only cover
cardiac patients, while the text corpus covers
a larger group of patients and the majority of
them have also undergone cardiac surgery.
The speech corpora were rst transcribed or-
thographically and then intonationally, using
the ToBI convention for prosodic labeling of
standard American English (Silverman et al,
1992). For this study, we used only binary ac-
cented/deaccented decisions derived from the
ToBI tonal tier, in which location and type
of pitch accent is marked. After ToBI label-
ing, each word in the corpora was tagged with
part-of-speech, from a nine-element set: noun,
verb, adjective, adverb, article, conjunction,
pronoun, cardinal, and preposition. The spon-
taneous corpus was tagged by hand and the
read tagged automatically. As noted above,
we focus here on predicting whether nouns are
accented or not.
3 Collocation Measures
We used three measures of word collocation to
examine the relationship between collocation
and accent placement: word bigram pre-
dictability, mutual information, and the
Dice coefficient. While word predictabil-
ity is not typically used to measure collocation,
there is some correlation between word collo-
cation and predictability. For example, if two
words are collocated, then it will be easy to
predict the second word from the rst. Sim-
ilarly, if one word is highly predictable given
another word, then there is a higher possibility
that these two words are collocated. Mutual
information (Fano, 1961) and the Dice coe?-
cient (Dice, 1945) are two standard measures
of collocation. In general, mutual information
measures uncertainty reduction or departure
from independence. The Dice coe?cient is a
collocation measure widely used in information
retrieval. In the following, we will give a more
detailed denitions of each.
Statistically, bigram word predictability is
dened as the log conditional probability of
word w
i
, given the previous word w
i 1
:
Pred(w
i
) = log(Prob(w
i
jw
i 1
))
Bigram predictability directly measures the
likelihood of seeing one word, given the
occurrence of the previous word. Bi-
gram predictability has two forms: abso-
lute and relative. Absolute predictability is
the value directly computed from the for-
mula. For example, given four adjacent
words w
i 1
; w
i
; w
i+1
and w
i+2
, if we assume
Prob(w
i
jw
i 1
) = 0:0001, Prob(w
i+1
jw
i
) =
0:001, and Prob(w
i+2
jw
i+1
) = 0:01, the abso-
lute bigram predictability will be -4, -3 and
-2 for w
i
; w
i+1
and w
i+2
. The relative pre-
dictability is dened as the rank of absolute
predictability among words in a constituent.
In the same example, the relative predictabil-
ity will be 1, 2 and 3 for w
i
; w
i+1
and w
i+2
,
where 1 is associated with the word with the
lowest absolute predictability. In general, the
higher the rank, the higher the absolute pre-
dictability. Except in Section 7, all the pre-
dictability measures mentioned in this paper
use the absolute form.
We used our text corpus to compute bigram
word predictability for our domain. When cal-
culating the word bigram predictability, we
rst ltered uncommon words (words occur-
ring 5 times or fewer in the corpus) then used
the Good-Turing discount strategy to smooth
the bigram. Finally we calculated the log con-
ditional probability of each word as the mea-
sure of its bigram predictability.
Two measures of mutual information were
used for word collocation: pointwise mu-
tual information, which is dened as :
I
1
(w
i 1
;w
i
) = log
P
r
(w
i 1
; w
i
)
P
r
(w
i 1
)P
r
(w
i
)
and average mutual information, which
is dened as:
I
2
(w
i 1
;w
i
) =
P
r
(w
i 1
; w
i
) log
P
r
(w
i 1
; w
i
)
P
r
(w
i 1
)P
r
(w
i
)
+P
r
(w
i 1
; w
i
) log
P
r
(w
i 1
; w
i
)
P
r
(w
i 1
)P
r
(w
i
)
+P
r
(w
i 1
; w
i
) log
P
r
(w
i 1
; w
i
)
P
r
(w
i 1
)P
r
(w
i
)
+P
r
(w
i 1
; w
i
) log
P
r
(w
i 1
; w
i
)
P
r
(w
i 1
)P
r
(w
i
)
The same text corpus was used to compute
both mutual information measures. Only word
pairs with bigram frequency greater than ve
were retained.
The Dice coe?cient is dened as:
Dice(w
i 1
; w
i
) =
2 P
r
(w
i 1
; w
i
)
P
r
(w
i 1
) + P
r
(w
i
)
Here, we also use a cut o threshold of ve to
lter uncommon bigrams.
Although all these measures are correlated,
one measure can score word pairs quite dier-
ently from another. Table 1 shows the top ten
collocations for each metric.
In the predictability top ten list, we have
pairs like scarlet fever where fever is very pre-
dictable from scarlet (in our corpus, scarlet is
always followed by fever), thus, it ranks high-
est in the predictability list. Since scarlet can
be di?cult to predict from fever, these types
of pairs will not receive a very high score us-
ing mutual information (in the top 5% in I
1
sorted list and in the top 20% in I
2
list) and
Dice coe?cient (top 22%). From this table, it
is also quite clear that I
1
tends to rank un-
common words high. All the words in the top
ten I
1
list have a frequency less than or equal
Pred I
1
I
2
Dice
chief complaint polymyalgia rheumatica The patient greeneld lter
cerebrospinal uid hemiside stepper present illness Guillain Barre
folic acid Pepto Bismol hospital course Viet Nam
periprocedural complications Glen Cove p o Neo Synephrine
normoactive bowel hydrogen peroxide physical exam polymyalgia rheumatica
uric acid Viet Nam i d hemiside stepper
postpericardiotomy syndrome Neo Synephrine coronary artery Pepto Bismol
Staten Island otitis media postoperative day Glen Cove
scarlet fever Lo Gerfo saphenous vein present illness
pericardiotomy syndrome Chlor Trimeton medical history chief complaint
Table 1: Top Ten Most Collocated Words for Each Measure
to seven (we lter all the pairs occurring fewer
than six times).
Of the dierent metrics, only bigram pre-
dictability is a unidirectional measure. It cap-
tures how the appearance of one word aects
the appearance of the following word. In con-
trast, the other measures are all bidirectional
measures, making no distinction between the
relative position of elements of a pair of col-
located items. Among the bidirectional mea-
sures, point-wise mutual information is sensi-
tive to marginal probabilities P
r
(word
i 1
) and
P
r
(word
i
). It tends to give higher values as
these probabilities decrease, independently of
the distribution of their co-occurrence. The
Dice coe?cient, however, is not sensitive to
marginal probability. It computes conditional
probabilities which are equally weighted in
both directions.
Average mutual information measures the
reduction in the uncertainty, of one word,
given another, and is totally symmetric. Since
I
2
(word
i 1
; word
i
)=I
2
(word
i
;word
i 1
), the
uncertainty reduction of the rst word, given
the second word, is equal to the uncer-
tainty reduction of the second word, given the
rst word. Further more, because I
2
(word
i
;
word
i 1
) = I
2
(word
i
;word
i 1
), the uncer-
tainty reduction of one word, given another,
is also equal to the uncertainty reduction of
failing to see one word, having failed to see
the other.
Since there is considerable evidence that
prior discourse context, such as previous men-
tion of a word, aects pitch accent decisions,
it is possible that symmetric measures, such
as mutual information and the Dice coe?-
cient, may not model accent placement as
well as asymmetric measures, such as bigram
predictability. Also, the bias of point-wise
mutual information toward uncommon words
can aect its ability to model accent assign-
ment, since, in general, uncommon words are
more likely to be accented (Pan and McKe-
own, 1999). Since this metric disproportion-
ately raises the mutual information for un-
common words, making them more predictable
than their appearance in the corpus warrants,
it may predict that uncommon words are more
likely to be deaccented than they really are.
4 Statistical Analyses
In order to determine whether word collo-
cation is useful for pitch accent prediction,
we rst employed Spearman's rank correlation
test (Conover, 1980).
In this experiment, we employed a unigram
predictability-based baseline model. The un-
igram predictability of a word is dened as
the log probability of a word in the text cor-
pus. The maximum likelihood estimation of
this measure is:
log
Freq(w
i
)
P
i
Freq(w
i
)
The reason for choosing this as the baseline
model is not only because it is context inde-
pendent, but also because it is eective. In
a previous study (Pan and McKeown, 1999),
we showed that when this feature is used, it
is as powerful a predictor as part-of-speech.
When jointly used with part-of-speech infor-
mation, the combined model can perform sig-
nicantly better than each individual model.
When tested on a similar medical corpus, this
combined model also outperforms a compre-
hensive pitch accent model employed by the
Bell Labs' TTS system (Sproat et al, 1992;
Hirschberg, 1993; Sproat, 1998), where dis-
course information, such as given/new, syntac-
tic information, such as POS, and surface in-
formation, such as word distance, are incorpo-
rated. Since unigram predictability is context
independent. By comparing other predictors
to this baseline model, we can demonstrate the
impact of context, measured by word colloca-
tion, on pitch accent assignment.
Table 2 shows that for our read speech
corpus, unigram predictability, bigram pre-
dictability and mutual information are all sig-
nicantly correlated (p < 0:001) with pitch ac-
cent decision.
1
However, the Dice coe?cient
shows only a trend toward correlation (p <
0:07). In addition, both bigram predictabil-
ity and (pointwise) mutual information show a
slightly stronger correlation with pitch accent
than the baseline. When we conducted a sim-
ilar test on the spontaneous corpus, we found
that all but the baseline model are signicantly
correlated with pitch accent placement. Since
all three models incorporate a context word
while the baseline model does not, these re-
sults suggest the usefulness of context in ac-
cent prediction. Overall, for all the dierent
measures of collocation, bigram predictability
explains the largest amount of variation in ac-
cent status for both corpora. We conducted a
similar test using trigram predictability, where
two context words, instead of one, were used
to predict the current word. The results are
slightly worse than bigram predictability (for
the read corpus r =  0:167, p < 0:0001; for
the spontaneous r =  0:355, p < 0:0001).
The failure of the trigram model to improve
over the bigram model may be due to sparse
data. Thus, in the following analysis, we focus
on bigram predictability. In order to further
verify the eectiveness of word predictability
in accent prediction, we will show some exam-
ples in our speech corpora rst. Then we will
describe how machine learning helps to derive
pitch accent prediction models using this fea-
ture. Finally, we show that both absolute pre-
dictability and relative predictability are use-
ful for pitch accent prediction.
1
Since pointwise mutual information performed con-
sistently better than average mutual information in our
experiment, we present results only for the former.
5 Word Predictability and Accent
In general, nouns, especially head nouns, are
very likely to be accented. However, cer-
tain nouns consistently do not get accented.
For example, Table 3 shows some collocations
containing the word cell in our speech cor-
pus. For each context, we list the collocated
pair, its most frequent accent pattern in our
corpus (upper case indicates that the word
was accented and lower case indicates that
it was deaccented), its bigram predictability
(the larger the number is, the more predictable
the word is), and the frequency of this ac-
cent pattern, as well as the total occurrence
of the bigram in the corpus. In the rst ex-
Word Pair Pred(cell) Freq
[of] CELL -3.11 7/7
[RED] CELL -1.119 2/2
[PACKED] cell -0.5759 4/6
[BLOOD] cell -0.067 2/2
Table 3: cell Collocations
ample, cell in [of ] CELL is very unpredictable
from the occurrence of of and always receives a
pitch accent. In [RED] CELL, [PACKED] cell,
and [BLOOD] cell, cell has the same semantic
meaning, but dierent accent patterns: cell in
[PACKED] cell and [BLOOD] cell is more pre-
dictable and deaccented, while in [RED] CELL
it is less predictable and is accented. These
examples show the inuence of context and
its usefulness for bigram predictability. Other
predictable nouns, such as saver in CELL
saver usually are not accented even when they
function as head nouns. Saver is deaccented in
ten of the eleven instances in our speech cor-
pus. Its bigram score is -1.5517, which is much
higher than that of CELL (-4.6394{3.1083 de-
pending upon context). Without collocation
information, a typical accent prediction sys-
tem is likely to accent saver, which would be
inappropriate in this domain.
6 Accent Prediction Models
Both the correlation test results and direct ob-
servations provide some evidence on the useful-
ness of word predictability. But we still need to
demonstrate that we can successfully use this
feature in automatic accent prediction. In or-
der to achieve this, we used machine learning
Corpus Read Spontaneous
r p-value r p-value
Baseline (Unigram) r =  0:166 p = 0:0002 r =  0:02 p = 0:39
Bigram Predictability r =  0:236 p < 0:0001 r =  0:36 p < 0:0001
Pointwise Mutual Information r =  0:185 p < 0:0001 r =  0:177 p < 0:0001
Dice Coe?cient r =  0:079 p = 0:066 r =  0:094 p < 0:0001
Table 2: Correlation of Dierent Collocation Measures with Accent Decision
techniques to automatically build accent pre-
diction models using bigram word predictabil-
ity scores.
We used RIPPER (Cohen, 1995b) to ex-
plore the relations between predictability and
accent placement. RIPPER is a classication-
based rule induction system. From annotated
examples, it derives a set of ordered if-then
rules, describing how input features can be
used to predict an output feature. In order
to avoid overtting, we use 5-fold cross valida-
tion. The training data include all the nouns in
the speech corpora. The independent variables
used to predict accent status are the unigram
and bigram predictability measures, and the
dependent variable is pitch accent status. We
used a majority-based predictability model as
our baseline (i.e. predict accented).
In the combined model, both unigram and
bigram predictability are used together for ac-
cent prediction. From the results in Table 4,
we see that the bigram model consistently out-
performs the unigram model, and the com-
bined model achieves the best performance.
To evaluate the signicance of the improve-
ments achieved by incorporating a context
word, we use the standard error produced by
RIPPER. Two results are statistically signif-
icant when the results plus or minus twice
the standard error do not overlap (Cohen,
1995a). As shown in Table 4, for the read
corpus, except for the unigram model, all the
models with bigram predictability performed
signicantly better than the baseline model.
However, the bigram model and the combined
model failed to improve signicantly over the
unigram model. This may result from too
small a corpus. For the spontaneous corpus,
the unigram, bigram and the combined model
all achieved signicant improvement over the
baseline. The bigram also performed signi-
cantly better than the unigram model. The
combined model had the best performance. It
also achieved signicant improvement over the
unigram model.
The improvement of the combined model
over both unigram and bigram models may
be due to the fact that some accent patterns
that are not captured by one are indeed cap-
tured by the other. For example, accent pat-
terns for street names have been extensively
discussed in the literature (Ladd, 1996). For
example, street in phrases like (e.g. FIFTH
street) is typically deaccented while avenue
(e.g. Fifth AVENUE) is accented. While it
seems likely that the conditional probability
of P
r
(StreetjFifth) is no higher than that of
P
r
(AvenuejFifth), the unigram probability of
P
r
(street) is probably higher than that of av-
enue P
r
(avenue).
2
. So, incorporating both
predictability measures may tease apart these
and similar cases.
7 Relative Predictability
In the our previous analysis, we showed the ef-
fectiveness of absolute word predictability. We
now consider whether relative predictability is
correlated with a larger constituent's accent
pattern. The following analysis focuses on ac-
cent patterns of non-trivial base NPs.
3
For
this study we labeled base NPs by hand for
the corpora described in Section 2. For each
base NP, we calculate which word is the most
predictable and which is the least. We want
to see, when comparing with its neighboring
2
For example, in a 7.5M word general news corpus
(from CNN and Reuters), street occurs 2115 times and
avenue just 194. Therefore, the unigram predictabil-
ity of street is higher than that of avenue. The most
common bigram with street is Wall Street which occurs
116 times and the most common bigram with avenue is
Pennsylvania Avenue which occurs 97. In this domain,
the bigram predictability for street in Fifth Street is ex-
tremely low because this combination never occurred,
while that for avenue in Fifth Avenue is -3.0995 which
is the third most predictable bigrams with avenue as
the second word.
3
Non-recursive noun phrases containing at least two
elements.
Corpus Predictability Model Performance Standard Error
baseline model 81.98%
unigram model 82.86%  0.93
Read bigram predictability model 84.41%  1.10
unigram+bigram model 85.03%  1.04
baseline model 70.03%
unigram model 72.22%  0.62
Spontaneous bigram model 74.46%  0.30
unigram+bigram model 77.43%  0.51
Table 4: Ripper Results for Accent Status Prediction
Model Predictability Total Accented Word Not Accented Accentability
unigram Least Predictable 1206 877 329 72.72%
Most Predictable 1198 485 713 40.48%
bigram Least Predictable 1205 965 240 80.08%
Most Predictable 1194 488 706 40.87%
Table 5: Relative Predictability and Accent Status
words, whether the most predictable word is
more likely to be deaccented. As shown in Ta-
ble 5, the \total" column represents the total
number of most (or least) predictable words
in all baseNPs
4
. The next two columns indi-
cate how many of them are accented and deac-
cented. The last column is the percentage of
words that are accented. Table 5 shows that
the probability of accenting a most predictable
word is between 40:48% and 45:96% and that
of a least predictable word is between 72:72%
and 80:08%. This result indicates that rela-
tive predictability is also a useful predictor for
a word's accentability.
8 Discussion
It is di?cult to directly compare our results
with previous accent prediction studies, to
determine the general utility of bigram pre-
dictability in accent assignment, due to dif-
ferences in domain and the scope of our task.
For example, Hirschberg (1993) built a com-
prehensive accent prediction model using ma-
chine learning techniques for predicting ac-
cent status for all word classes for a text-to-
speech system, employing part-of-speech, var-
ious types of information status inferred from
the text, and a number of distance metrics,
as well as a complex nominal predictor devel-
oped by Sproat (1992). An algorithm making
use of these features achieved 76.5%-80% ac-
cent prediction accuracy for a broadcast news
4
The total number of most predictable words is not
equal to that of least predictable words due to ties.
corpus, 85% for sentences from the ATIS cor-
pus of spontaneous elicited speech, and 98.3%
success on a corpus of laboratory read sen-
tences. Liberman and Sproat's (1992) success
in predicting accent patterns for complex nom-
inals alone, using rules combining a number
of features, achieved considerably higher suc-
cess rates (91% correct, 5.4% acceptable, 3.6%
unacceptable when rated by human subjects)
for 500 complex nominals of 2 or more ele-
ments chosen from the AP Newswire. Our re-
sults, using bigram predictability alone, 77%
for the spontaneous corpus and 85% for the
read corpus, and using a dierent success es-
timate, while not as impressive as (Liberman
and Sproat, 1992)'s, nonetheless demonstrate
the utility of a relatively untested feature for
this task.
In this paper, we have investigated several
collocation-based measures for pitch accent
prediction. Our initial hypothesis was that
word collocation aects pitch accent place-
ment, and that the more predictable a word
is in terms of its local lexical context, the
more likely it is to be deaccented. In order
to verify this claim, we estimated three col-
location measures: word predictability, mu-
tual information and the Dice coe?cient. We
then used statistical techniques to analyze the
correlation between our dierent word collo-
cation metrics and pitch accent assignment
for nouns. Our results show that, of all the
collocation measures we investigated, bigram
word predictability has the strongest correla-
tion with pitch accent assignment. Based on
this nding, we built several pitch accent mod-
els, assessing the usefulness of unigram and
bigram word predictability {as well as a com-
bined model{ in accent predication. Our re-
sults show that the bigram model performs
consistently better than the unigram model,
which does not incorporate local context in-
formation. However, our combined model per-
forms best of all, suggesting that both con-
textual and non-contextual features of a word
are important in determining whether or not
it should be accented.
These results are particularly important for
the development of future accent assignment
algorithms for text-to-speech. For our contin-
uing research, we will focus on two directions.
The rst is to combine our word predictability
feature with other pitch accent predictors that
have been previously used for automatic accent
prediction. Features such as information sta-
tus, grammatical function, and part-of-speech,
have also been shown to be important deter-
minants of accent assignment. So, our nal
pitch accent model should include many other
features. Second, we hope to test whether the
utility of bigram predictability can be gener-
alized across dierent domains. For this pur-
pose, we have collected an annotated AP news
speech corpus and an AP news text corpus,
and we will carry out a similar experiment in
this domain.
9 Acknowledgments
Thanks for C. Jin, K. McKeown, R. Barzi-
lay, J. Shaw, N. Elhadad, M. Kan, D. Jor-
dan, and anonymous reviewers for the help on
data preparation and useful comments. This
research is supported in part by the NSF Grant
IRI 9528998, the NLM Grant R01 LM06593-01
and the Columbia University Center for Ad-
vanced Technology in High Performance Com-
puting and Communications in Healthcare.
References
D. Bolinger. 1961. Contrastive accent and con-
trastive stress. language, 37:83{96.
D. Bolinger. 1989. Intonation and Its Uses. Stan-
ford University Press.
G. Brown. 1983. Prosodic structure and the
given/new distinction. In A. Cutler and D.R.
Ladd, ed., Prosody: Models and Measurements,
pages 67{78. Springer-Verlag, Berlin.
P. Cohen. 1995a. Empirical methods for articial
intelligence. MIT press, Cambridge, MA.
W. Cohen. 1995b. Fast eective rule induction.
In Proc. of the 12th International Conference on
Machine Learning.
W. J. Conover. 1980. Practical Nonparametric
Statistics. Wiley, New York, 2nd edition.
M. Dalal, S. Feiner, K. McKeown, S. Pan, M. Zhou,
T. Hoellerer, J. Shaw, Y. Feng, and J. Fromer.
1996. Negotiation for automated generation of
temporal multimedia presentations. In Proc. of
ACM Multimedia 96, pages 55{64.
Lee R. Dice. 1945. Measures of the amount of
ecologic association between species. Journal of
Ecology, 26:297{302.
Robert M. Fano. 1961. Transmission of Informa-
tion: A Statistical Theory of Communications.
MIT Press, Cambridge, MA.
J. Hirschberg. 1993. Pitch accent in context: pre-
dicting intonational prominence from text. Ar-
ticial Intelligence, 63:305{340.
D. Robert Ladd. 1996. Intonational Phonology.
Cambridge University Press, Cambridge.
M. Liberman and R. Sproat. 1992. The stress and
structure of modied noun phrases in English.
In I. Sag, ed., Lexical Matters, pages 131{182.
University of Chicago Press.
J. Marchand. 1993. Message posted on HUMAN-
IST mailing list, April.
C. Nakatani. 1998. Constituent-based accent pre-
diction. In Proc. of COLING/ACL'98, pages
939{945, Montreal, Canada.
S. Pan and K. McKeown. 1998. Learning intona-
tion rules for concept to speech generation. In
Proc. of COLING/ACL'98, Montreal, Canada.
S. Pan and K. McKeown. 1999. Word informa-
tiveness and automatic pitch accent modeling.
In Proc. of the Joint SIGDAT Conference on
EMNLP and VLC, pages 148{157.
K. Silverman, M. Beckman, J. Pitrelli, M. Osten-
dorf, C. Wightman, P. Price, J. Pierrehumbert,
and J. Hirschberg. 1992. ToBI: a standard for
labeling English prosody. In Proc. of ICSLP92.
R. Sproat, J. Hirschberg, and D. Yarowsky. 1992.
A corpus-based synthesizer. In Proc. of IC-
SLP92, pages 563{566, Ban.
R. Sproat, ed. 1998. Multilingual Text-to-Speech
Synthesis: The Bell Labs Approach. Kluwer.
Predicting User Reactions to System Error
Diane Litman and Julia Hirschberg
AT&T Labs?Research
Florham Park, NJ, 07932 USA
 
diane/julia  @research.att.com
Marc Swerts
IPO, Eindhoven, The Netherlands,
and CNTS, Antwerp, Belgium
m.g.j.swerts@tue.nl
Abstract
This paper focuses on the analysis and
prediction of so-called aware sites,
defined as turns where a user of a
spoken dialogue system first becomes
aware that the system has made a
speech recognition error. We describe
statistical comparisons of features of
these aware sites in a train timetable
spoken dialogue corpus, which re-
veal significant prosodic differences
between such turns, compared with
turns that ?correct? speech recogni-
tion errors as well as with ?normal?
turns that are neither aware sites nor
corrections. We then present machine
learning results in which we show how
prosodic features in combination with
other automatically available features
can predict whether or not a user turn
was a normal turn, a correction, and/or
an aware site.
1 Introduction
This paper describes new results in our continu-
ing investigation of prosodic information as a po-
tential resource for error recovery in interactions
between a user and a spoken dialogue system. In
human-human interaction, dialogue partners ap-
ply sophisticated strategies to detect and correct
communication failures so that errors of recog-
nition and understanding rarely lead to a com-
plete breakdown of the interaction (Clark and
Wilkes-Gibbs, 1986). In particular, various stud-
ies have shown that prosody is an important cue
in avoiding such breakdown, e.g. (Shimojima et
al., 1999). Human-machine interactions between
a user and a spoken dialogue system (SDS) ex-
hibit more frequent communication breakdowns,
due mainly to errors in the Automatic Speech Re-
cognition (ASR) component of these systems. In
such interactions, however, there is also evidence
showing prosodic information may be used as a
resource for error recovery. In previous work,
we identified new procedures to detect recogni-
tion errors. In particular, we found that pros-
odic features, in combination with other inform-
ation already available to the recognizer, can dis-
tinguish user turns that are misrecognized by the
system far better than traditional methods used in
ASR rejection (Litman et al, 2000; Hirschberg et
al., 2000). We also found that user corrections
of system misrecognitions exhibit certain typical
prosodic features, which can be used to identify
such turns (Swerts et al, 2000; Hirschberg et al,
2001). These findings are consistent with previ-
ous research showing that corrections tend to be
hyperarticulated ? higher, louder, longer . . . than
other turns (Wade et al, 1992; Oviatt et al, 1996;
Levow, 1998; Bell and Gustafson, 1999).
In the current study, we focus on another turn
category that is potentially useful in error hand-
ling. In particular, we examine what we term
aware sites ? turns where a user, while interact-
ing with a machine, first becomes aware that the
system has misrecognized a previous user turn.
Note that such aware sites may or may not also be
corrections (another type of post-misrecognition
turn), since a user may not immediately provide
correcting information. We will refer to turns
that are both aware sites and corrections as corr-
awares, to turns that are only corrections as corrs,
to turns that are only aware sites as awares, and to
turns that are neither aware sites nor corrections as
norm.
We believe that it would be useful for the
dialogue manager in an SDS to be able to de-
tect aware sites for several reasons. First, if
aware sites are detectable, they can function as
backward-looking error-signaling devices, mak-
ing it clear to the system that something has gone
wrong in the preceding context, so that, for ex-
ample, the system can reprompt for information.
In this way, they are similar to what others have
termed ?go-back? signals (Krahmer et al, 1999).
Second, aware sites can be used as forward-
looking signals, indicating upcoming corrections
or more drastic changes in user behavior, such
as complete restarts of the task. Given that, in
current systems, both corrections and restarts of-
ten lead to recognition error (Swerts et al, 2000),
aware sites may be useful in preparing systems to
deal with such problems.
In this paper, we investigate whether aware
sites share acoustic properties that set them apart
from normal turns, from corrections, and from
turns which are both aware sites and corrections.
We also want to test whether these different turn
categories can be distinguished automatically, via
their prosodic features or from other features
known to or automatically detectible by a spoken
dialogue system. Our domain is the TOOT spoken
dialogue corpus, which we describe in Section 2.
In Section 3, we present some descriptive findings
on different turn categories in TOOT. Section 4
presents results of our machine learning experi-
ments on distinguishing the different turn classes.
In Section 5 we summarize our conclusions.
2 Data
The TOOT corpus was collected using an experi-
mental SDS developed for the purpose of compar-
ing differences in dialogue strategy. It provides
access to train information over the phone and
is implemented using an internal platform com-
bining ASR, text-to-speech, a phone interface,
and modules for specifying a finite-state dialogue
manager, and application functions. Subjects per-
formed four tasks with versions of TOOT, which
varied confirmation type and locus of initiative
(system initiative with explicit system confirma-
tion, user initiative with no system confirmation
until the end of the task, mixed initiative with im-
plicit system confirmation), as well as whether
the user could change versions at will using voice
commands. Subjects were 39 students, 20 nat-
ive speakers of standard American English and
19 non-native speakers; 16 subjects were female
and 23 male. The exchanges were recorded and
the system and user behavior logged automatic-
ally. Dialogues were manually transcribed and
user turns automatically compared to the corres-
ponding ASR (one-best) recognized string to pro-
duce a word accuracy score (WA) for each turn.
Each turn?s concept accuracy (CA) was labeled
by the experimenters from the dialogue recordings
and the system log; if the recognizer correctly cap-
tured all the task-related information given in the
user?s original input (e.g. date, time, departure or
arrival cities), the turn was given a CA score of
1, indicating a semantically correct recognition.
Otherwise, the CA score reflected the percentage
of correctly recognized task concepts in the turn.
For the study described below, we examined 2328
user turns from 152 dialogues generated during
these experiments. 194 of the 2320 turns were re-
jected by the system.
To identify the different turn categories in the
corpus, two authors independently labeled each
turn as to whether or not it constituted a correction
of a prior system failure (a CA error or a rejection)
and what turn was being corrected, and whether
or not it represented an aware site for a prior fail-
ure, and, if so, the turn which the system had failed
on. Labeler disagreement was subsequently re-
solved by consensus. The fragment in Figure 1,
produced with a version of TOOT in which the
user has the initiative with no confirmation until
the end of the task, illustrates these labels. This
example illustrates cases of corraware, in which
both the user?s awareness and correction of a mis-
recognition occur in the same turn (e.g. turns
1159 and 1160, after system prompts for informa-
tion already given in turn 1158). It also illustrates
cases in which aware sites and corrections occur
in different turns. For example, after the immedi-
ate explicit system confirmation of turn 1162, the
user first becomes aware of the system errors (turn
1163), then separately corrects them (turn 1164);
turn 1163 is thus an aware turn and turn 1164 a
corr. When no immediate confirmation of an ut-
terance occurs (as with turn 1158), it may take sev-
eral turns before the user becomes aware of any
Turn Turn ID Aware of Corr of Type
S: How may I help you?
U: Get me the train to New York City from
Baltimore on Sunday at eight thirty pm 1158 NA NA NORM
S: Which city do you want to go to?
U: New York City 1159 1158 1158 CORRAWARE
S: Which city do you want to leave from?
U: Baltimore 1160 1158 1158 CORRAWARE
S: Do you want me to find the trains from
Baltimore to New York City today at anytime now?
U: No 1161 1158 NA AWARE
S: How may I help you?
U: Get me the train to New York City from
Baltimore on Sunday at eight thirty pm 1162 NA 1158 CORR
S: Do you want me to find the trains from
Baltimore to New York City today at anytime now?
U: No 1163 1162 NA AWARE
S: How may I help you?
U: Get me the train on Sunday at eight thirty pm 1164 NA 1162 CORR
Figure 1: Dialogue Fragment with Aware and Correction Labels.
misrecognition errors. For example, it is not un-
til turn 1161 that the user first becomes aware of
the error in date and time from 1158; the user then
corrects the error in 1162. So, 1161 is classified as
an aware and 1162 as a corr. Note that corr turns
represent 13% of the turns in our corpus, awares
represent 14%, corrawares account for 16%, and
norm turns represent 57% of the turns in the cor-
pus.
3 Descriptive Analysis and Results
We examined prosodic features for each user turn
which had previously been shown to be useful for
predicting misrecognized turns and corrections:
maximum and mean fundamental frequency val-
ues (F0 Max, F0 Mean), maximum and mean en-
ergy values (RMS Max, RMS Mean), total dur-
ation (Dur), length of pause preceding the turn
(Ppau), speaking rate (Tempo) and amount of si-
lence within the turn (%Sil). F0 and RMS val-
ues, representing measures of pitch excursion and
loudness, were calculated from the output of En-
tropic Research Laboratory?s pitch tracker, get f0,
with no post-correction. Timing variation was
represented by four features. Duration within and
length of pause between turns was computed from
the temporal labels associated with each turn?s be-

While the features were automatically computed, begin-
nings and endings were hand segmented from recordings of
the entire dialogue, as the turn-level speech files used as in-
put in the original recognition process created by TOOT were
unavailable.
ginning and end. Speaking rate was approximated
in terms of syllables in the recognized string per
second, while %Sil was defined as the percentage
of zero frames in the turn, i.e., roughly the per-
centage of time within the turn that the speaker
was silent.
To see whether the different turn categories
were prosodically distinct from one another, we
applied the following procedure. We first calcu-
lated mean values for each prosodic feature for
each of the four turn categories produced by each
individual speaker. So, for speaker A, we divided
all turns produced into four classes. For each
class, we then calculated mean F0 Max, mean F0
Mean, and so on. After this step had been repeated
for each speaker and for each feature, we then cre-
ated four vectors of speaker means for each indi-
vidual prosodic feature. Then, for each prosodic
feature, we ran a one-factor within subjects anova
on the means to learn whether there was an overall
effect of turn category.
Table 1 shows that, overall, the turn categor-
ies do indeed differ significantly with respect to
different prosodic features; there is a signific-
ant, overall effect of category on F0 Max, RMS
Max, RMS Mean, Duration, Tempo and %Sil. To
identify which pairs of turns were significantly
different where there was an overall significant ef-
fect, we performed posthoc paired t-tests using the
Bonferroni method to adjust the p-level to 0.008
(on the basis of the number of possible pairs that
Turn categories
Feature Normal Correction Aware Corraware  -stat
***F0 Max (Hz) 220.05 263.40 216.87 229.00 
	  =10.477
F0 Mean (Hz) 161.78 173.43 162.61 158.24 
	  =1.575
***RMS Max (dB) 1484.14 1833.62 1538.91 1925.38 
	  =7.548
*RMS Mean (dB) 372.47 379.65 425.96 464.16  
	  =3.190
***Dur (sec) 1.43 4.39 1.12 2.33 
	  =34.418
Ppau (sec) 0.60 0.93 0.87 0.80 
	  =1.325
**Tempo (syls/sec) 2.59 2.38 2.16 2.43  
	  =4.206
*%Sil (sec) 0.46 0.41 0.44 0.42  
	  =3.182
Significance level: *(p  .05), **(p  .01), ***(p  .001)
Table 1: Mean Values of Prosodic Features for Turn Categories.
Prosodic features
Classes F0 max F0 mean RMS max RMS mean Dur Ppau Tempo %Sil
norm/corr ? ? ? +
norm/aware +
norm/corraware ? ?
aware/corr ? ? ? ?
aware/corraware ? ? ?
corraware/corr ? ?
Table 2: Pairwise Comparisons of Different Turn Categories by Prosodic Feature.
can be drawn from an array of 4 means). Res-
ults are summarized in Table 2, where ? + ? or
? ? ? indicates that the feature value of the first cat-
egory is either significantly higher or lower than
the second. Note that, for each of the pairs, there
is at least one prosodic feature that distinguishes
the categories significantly, though it is clear that
some pairs, like aware vs. corr and norm vs. corr
appear to have more distinguishing features than
others, like norm vs. aware. It is also interesting to
see that the three types of post-error turns are in-
deed prosodically different: awares are less prom-
inent in terms of F0 and RMS maximum than cor-
rawares, which, in turn, are less prominent than
corrections, for example. In fact, awares, except
for duration, are prosodically similar to normal
turns.
4 Predictive Results
We next wanted to determine whether the pros-
odic features described above could, alone or
in combination with other automatically avail-
able features, be used to predict our turn categor-
ies automatically. This section describes experi-
ments using the machine learning program RIP-
PER (Cohen, 1996) to automatically induce pre-
diction models from our data. Like many learn-
ing programs, RIPPER takes as input the classes
to be learned, a set of feature names and possible
values, and training data specifying the class and
feature values for each training example. RIPPER
outputs a classification model for predicting the
class of future examples, expressed as an ordered
set of if-then rules. The main advantages of RIP-
PER for our experiments are that RIPPER supports
?set-valued? features (which allows us to repres-
ent the speech recognizer?s best hypothesis as a set
of words), and that rule output is an intuitive way
to gain insight into our data.
In the current experiments, we used 10-fold
cross-validation to estimate the accuracy of the
rulesets learned. Our predicted classes corres-
pond to the turn categories described in Section
2 and variations described below. We repres-
ent each user turn using the feature set shown in
Figure 2, which we previously found useful for
predicting corrections (Hirschberg et al, 2001).
A subset of the features includes the automatic-
ally computable raw prosodic features shown in
Table 1 (Raw), and normalized versions of these
features, where normalization was done by first
turn (Norm1) or by previous turn (Norm2) in a
dialogue. The set labeled ?ASR? contains stand-
ard input and output of the speech recognition pro-
cess, which grammar was used for the dialogue
state the system believed the user to be in (gram),
Raw: f0 max, f0 mean, rms max, rms mean, dur, ppau,
tempo, %sil;
Norm1: f0 max1, f0 mean1, rms max1, rms mean1, dur1,
ppau1, tempo1, %sil1;
Norm2: f0 max2, f0 mean2, rms max2, rms mean2, dur2,
ppau2, tempo2, %sil2;
ASR: gram, str, conf, ynstr, nofeat, canc, help, wordsstr,
syls, rejbool;
System Experimental: inittype, conftype, adapt, realstrat;
Dialogue Position: diadist;
PreTurn: features for preceding turn (e.g., pref0max);
PrepreTurn: features for preceding preceding turn (e.g.,
ppref0max);
Prior: for each boolean-valued feature (ynstr, nofeat,
canc, help, rejbool), the number/percentage of
prior turns exhibiting the feature (e.g., prioryn-
strnum/priorynstrpct);
PMean: for each continuous-valued feature, the mean of the
feature?s value over all prior turns (e.g., pmnf0max);
Figure 2: Feature Set.
the system?s best hypothesis for the user input
(str), and the acoustic confidence score produced
by the recognizer for the turn (conf). As subcases
of the str feature, we also included whether or not
the recognized string included the strings yes or no
(ynstr), some variant of no such as nope (nofeat),
cancel (canc), or help (help), as these lexical items
were often used to signal problems in our sys-
tem. We also derived features to approximate the
length of the user turn in words (wordsstr) and in
syllables (syls) from the str features. And we ad-
ded a boolean feature identifying whether or not
the turn had been rejected by the system (rejbool).
Next, we include a set of features representing
the system?s dialogue strategy when each turn was
produced. These include the system?s current ini-
tiative and confirmation strategies (inittype, conf-
type), whether users could adapt the system?s dia-
logue strategies (adapt), and the combined initiat-
ive/confirmation strategy in effect at the time of
the turn (realstrat). Finally, given that our previ-
ous studies showed that preceding dialogue con-
text may affect correction behavior (Swerts et al,
2000; Hirschberg et al, 2001), we included a fea-
ture (diadist) reflecting the distance of the current
turn from the beginning of the dialogue, and a set
of features summarizing aspects of the prior dia-
logue: for the latter features, we included both the
number of times prior turns exhibited certain char-
acteristics (e.g. priorcancnum) and the percent-
age of the prior dialogue containing one of these
features (e.g. priorcancpct). We also examined
means for all raw and normalized prosodic fea-
tures and some word-based features over the en-
tire dialogue preceding the turn to be predicted
(pmn ). Finally, we examined more local con-
texts, including all features of the preceding turn
(pre ) and for the turn preceding that (ppre ).
We provided all of the above features to the
learning algorithm first to predict the four-way
classification of turns into normal, aware, corr and
corraware. A baseline for this classification (al-
ways predicting norm, the majority class) has a
success rate of 57%. Compared to this, our fea-
tures improve classification accuracy to 74.23%
(+/? 0.96%). Figure 3 presents the rules learned
for this classification. Of the features that appear
in the ruleset, about half are features of current
turn and half features of the prior context. Only
once does a system feature appear, suggesting that
the rules generalize beyond the experimental con-
ditions of the data collection. Of the features spe-
cific to the current turn, prosodic features domin-
ate, and, overall, timing features (dur and tempo
especially) appear most frequently in the rules.
About half of the contextual features are prosodic
ones and half are ASR features, with ASR confid-
ence score appearing to be most useful. ASR fea-
tures of the current turn which appear most often
are string-based features and the grammar state
the system used for recognizing the turn. There
appear to be no differences in which type of fea-
tures are chosen to predict the different classes.
If we express the prediction results in terms of
precision and recall, we see how our classification
accuracy varies for the different turn categories
(Table 3). From Table 3, we see that the majority
class (normal) is most accurately classified. Pre-
dictions for the other three categories, which oc-
cur about equally often in our corpus, vary consid-
erably, with modest results for corr and corraware,
and rather poor results for aware. Table 4 shows a
confusion matrix for the four classes, produced by
if (gram=universal)  (dur2  7.31) then CORR
if (dur2  2.19)  (priornofeatpct  0.09)  (tempo  1.50)  (pmntempo  2.39) then CORR
if (dur2  1.53)  (pmnwordsstr  2.06)  (tempo1  1.07)  (predur  0.80)  (prenofeat=F)  (presyls  4) then CORR
if (predur1  0.26)  (dur  0.79)  (rmsmean2  1.51)  (f0mean  173.49) then CORR
if (dur2  1.41)  (prenofeat=T)  (str contains word ?eight?) then CORR
if (predur1  0.18)  (dur2  4.21)  (dur1  0.50)  (f0mean  276.43) then CORR
if (predur1  0.19)  (ppregram=cityname)  (rmsmax1  1.10)  (pmntempo2  1.64) then CORR
if (realstrat=SystemImplicit)  (gram=cityname)  (pmnf0mean1  0.96) then CORR
if (preconf  -2.66)  (dur2  0.31)  (pprenofeat=T)  (tempo2  0.61) then AWARE
if (preconf  -2.85)  (syls  2)  (predur  1.05)  (pref0max  4.82)  (tempo2  0.58)  (pmn%sil  0.53) then AWARE
if (preconf  -3.34)  (syls  2)  (ppau  0.57)  (conf  -3.07)  (preppau  0.72) then AWARE
if (dur  0.74)  (pmndur  2.57)  (preconf  -4.36)  (f0mean2  0.90) then CORRAWARE
if (preconf  -2.80)  (pretempo  2.16)  (preconf  -3.95)  (tempo1  4.67) then CORRAWARE
if (preconf  -2.80)  (dur  0.66)  (rmsmean  488.56) then CORRAWARE
if (preconf  -3.56)  (dur2  0.64)  (prerejbool=T) then CORRAWARE
if (pretempo  0.71)  (tempo  3.31) then CORRAWARE
if (preconf  -3.01)  (tempo2  0.78)  (pmndur  2.83)  (pmnf0mean  199.84) then CORRAWARE
if (pmnconf  -3.10)  (prestr contains the word ?help?)  (pmndur2  2.01)  (ppau  0.98) then CORRAWARE
if (pmnconf  -3.10)  (gram=universal)  (pregram=universal)  ( %sil  0.39) then CORRAWARE
else NORM
Figure 3: Rules for Predicting 4 Turn Categories.
Precision (%) Recall (%)
norm 80.09 89.39
corr 72.86 61.66
aware 61.01 39.79
corraware 61.76 61.72
Accuracy: 74.23% (  0.96%); baseline: 57%
Table 3: 4-way Classification Performance.
applying our best ruleset to the whole corpus. This
Classified as
norm corr aware corraware
norm 1263 14 11 38
corr 68 219 0 7
aware 149 1 130 47
corraware 53 5 8 315
Table 4: Confusion Matrix, 4-way Classification.
matrix clearly shows a tendency for the minority
classes, aware, corr and corraware, to be falsely
classified as normal. It also shows that aware and
corraware are more often confused than the other
categories.
These confusability results motivated us to col-
lapse the aware and corraware into one class,
which we will label isaware; this class thus rep-
resents all turns in which users become aware of
a problem. From a system perspective, such a
3-way classification would be useful in identify-
ing the existence of a prior system failure and in
further identifying those turns which simply rep-
resent corrections; such information might be as
useful, potentially, as the 4-way distinction, if we
could achieve it with greater accuracy.
Indeed, when we predict the three classes
(isaware, corr, and norm) instead of four, we
do improve in predictive power ? from 74.23%
to 81.14% (+/? 0.83%) classification success.
Again, this compares to the baseline (predicting
norm, which is still the majority class) of 57%. We
also get a corresponding improvement in terms of
precision and recall, as shown in Table 5, with
the isaware category considerably better distin-
guished than either aware or corraware in Table 3.
The ruleset for the 3-class predictions is given in
Precision (%) Recall(%)
norm 84.49 87.48
corr 72.07 67.38
isaware 80.52 77.07
Accuracy: 81.14% (  0.83%); baseline: 57%
Table 5: 3-way Classification Performance.
Figure 4. The distribution of features in this rule-
set is quite similar to that in Figure 3. However,
there appear to be clear differences in which fea-
tures best predict which classes. First, the features
used to predict corrections are balanced between
those from the current turn and features from the
preceding context, whereas isaware rules primar-
ily make use of features of the preceding context.
Second, the features appearing most often in the
rules predicting corrections are durational features
(dur2, predur1, dur), while duration is used only
if (gram=universal)  (dur2  7.31) then CORR
if (dur2  2.25)  (priornofeatpct  0.11)  (%sil  0.55)
 (wordsstr  4) then CORR
if (dur2  2.75)  (gram=universal)  (pre%sil1  1.17)
then CORR
if (predur1  0.24)  (dur  0.85)  (priornofeatnum  2)
 (pmnconf  -3.11)  (pmn%sil  0.45) then CORR
if (predur1  0.19)  (dur  1.21)  (pmnf0mean2  0.99)
 (predur2  0.90)  (%sil  0.70)  (tempo  3.25) then
CORR
if (predur1  0.20)  (ynstr=F)  (pregram=cityname) 
(ppref0mean  171.58) then CORR
if (dur2  0.75)  (gram=cityname)  (pmnsyls  3.67) 
(pmnconf  -3.23)  (%sil  0.41) then CORR
if (prenofeat=T)  (preconf  -0.72) then CORR
if (preconf  -4.07) then ISAWARE
if (preconf  -2.76)  (pmntempo  2.39)  (tempo2 
1.56)  (preynstr=F) then ISAWARE
if (preconf  -2.75)  (ppau  0.46)  (tempo  1.20) then
ISAWARE
if (pretempo  0.23) then ISAWARE
if (pmnconf  -3.10)  (ppregram=universal)  (ppre%sil 
0.34)  (tempo1  2.94) then ISAWARE
if (predur  1.27)  (pretempo  2.36)  (prermsmean 
229.33)  (tempo2  0.83) then ISAWARE
if (preconf  -2.80)  (nofeat=T)  (f0mean  205.56) then
ISAWARE
else NORM
Figure 4: Rules for Predicting 3 Turn Categories.
once in isaware rules. Instead, these rules make
considerable use of the ASR confidence score of
the preceding turn; in cases where aware turns im-
mediately follow a rejection or recognition error,
one would expect this to be true. Isaware rules
also appear distinct from correction rules in that
they make frequent use of the tempo feature. It
is also interesting to note that rules for predicting
isaware turns make only limited use of the nofeat
feature, i.e. whether or not a variant of the word
no appears in the turn. We might expect this lex-
ical item to be a more useful predictor, since in
the explicit confirmation condition, users should
become aware of errors while responding to a re-
quest for confirmation.
Note that corrections, now the minority class,
are more poorly distinguished than other classes in
our 3-way classification task (Table 5). In a third
set of experiments, we merged corrections with
normal turns to form a 2-way distinction over all
between aware turns and all others. Thus, we only
distinguish turns in which a user first becomes
aware of an ASR failure (our original isaware and
corraware categories) from those that are not (our
original corr and norm categories). Such a dis-
tinction could be useful in flagging a prior sys-
tem problem, even though it fails to target the ma-
terial intended to correct that problem. For this
new 2-way distinction, we obtain a higher de-
gree of classification accuracy than for the 3-way
classification ? 87.80% (+/? 0.61%) compared to
81.14%. Note, however, that the baseline (predict
majority class of !isaware) for this new classifica-
tion is 70%, considerably higher than the previous
baseline. Table 6 shows the improvement in terms
of accuracy, precision, and recall.
Precision (%) Recall (%)
!isaware 91.7 91.6
isaware 80.7 81.1
Accuracy: 87.80% (  0.61%); baseline: 70%
Table 6: 2-way Classification Performance.
The ruleset for the 2-way distinction is shown in
Figure 5. The features appearing most frequently
if (preconf  -4.06)  (pretempo  2.65)  (ppau  0.25)
then T
if (preconf  -3.59)  (prerejbool=T) then T
if (preconf  -2.85)  (predur  1.039)  (tempo2  1.04)
 (preppau  0.57)  (pretempo  2.18) then T
if (preconf  -3.78)  (pmnsyls  4.04) then T
if (preconf  -2.75)  (prestr contains the word ?help?) then
T
if (pregram=universal)  (pprewordsstr  2) then T
if (preconf  -2.60)  (predur  1.04)  (%sil1  1.06) 
(prermsmean  370.65) then T
if (pretempo  0.13) then T
if (predur  1.27)  (pretempo  2.36)  (prermsmean 
245.36) then T
if (pretempo  0.80)  (pmntempo  1.75)  (ppretempo2
 1.39) then T
then F
Figure 5: Rules for Predicting 2 Turn Categories:
ISAWARE (T) versus the rest (F).
in these rules are similar to those in the previous
two rulesets in some ways, but quite different in
others. Like the rules in Figures 3 and 4, they ap-
pear independent of system characteristics. Also,
of the contextual features appearing in the rules,
about half are prosodic features and half ASR-
related; and, of the current turn features, pros-
odic features dominate. And timing features again
(especially tempo) dominate the prosodic features
that appear in the rules. However, in contrast to
previous classification rulesets, very few features
of the current turn appear in the rules at all. So,
it would seem that, for the broader classification
task, contextual features are far more important
than for the more fine-grained distinctions.
5 Conclusion
Continuing our earlier research into the use of
prosodic information to identify system misrecog-
nitions and user corrections in a SDS, we have
studied aware sites, turns in which a user first no-
tices a system error. We find first that these sites
have prosodic properties which distinguish them
from other turns, such as corrections and normal
turns. Subsequent machine learning experiments
distinguishing aware sites from corrections and
from normal turns show that aware sites can be
classified as such automatically, with a consid-
erable degree of accuracy. In particular, in a 2-
way classification of aware sites vs. all other turns
we achieve an estimated success rate of 87.8%.
Such classification, we believe, will be especially
useful in error-handling for SDS. We have pre-
viously shown that misrecognitions can be clas-
sified with considerable accuracy, using prosodic
and other automatically available features. With
our new success in identifying aware sites, we
acquire another potentially powerful indicator of
prior error. Using these two indicators together,
we hope to target system errors considerably more
accurately than current SDS can do and to hypo-
thesize likely locations of user attempts to correct
these errors. Our future research will focus upon
combining these sources of information identify-
ing system errors and user corrections, and invest-
igating strategies to make use of this information,
including changes in dialogue strategy (e.g. from
user or mixed initiative to system initiative after
errors) and the use of specially trained acoustic
models to better recognize corrections.
References
L. Bell and J. Gustafson. 1999. Repetition and its
phonetic realizations: Investigating a Swedish data-
base of spontaneous computer-directed speech. In
Proceedings of ICPhS-99, San Francisco. Interna-
tional Congress of Phonetic Sciences.
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as
a collaborative process. Cognition, 22:1?39.
W. Cohen. 1996. Learning trees and rules with set-
valued features. In 14th Conference of the American
Association of Artificial Intelligence, AAAI.
J. Hirschberg, D. Litman, and M. Swerts. 2000.
Generalizing prosodic prediction of speech recog-
nition errors. In Proceedings of the Sixth Interna-
tional Conference on Spoken Language Processing,
Beijing.
J. Hirschberg, D. Litman, and M. Swerts. 2001.
Identifying user corrections automatically in spoken
dialogue systems. In Proceedings of NAACL-2001,
Pittsburgh.
E. Krahmer, M. Swerts, M. Theune, and M. Weegels.
1999. Error spotting in human-machine interac-
tions. In Proceedings of EUROSPEECH-99.
G. Levow. 1998. Characterizing and recognizing
spoken corrections in human-computer dialogue.
In Proceedings of the 36th Annual Meeting of the
Association of Computational Linguistics, COL-
ING/ACL 98, pages 736?742.
D. Litman, J. Hirschberg, and M. Swerts. 2000. Pre-
dicting automatic speech recognition performance
using prosodic cues. In Proceedings of NAACL-00,
Seattle, May.
S. L. Oviatt, G. Levow, M. MacEarchern, and K. Kuhn.
1996. Modeling hyperarticulate speech during
human-computer error resolution. In Proceedings
of ICSLP-96, pages 801?804, Philadelphia.
A. Shimojima, K. Katagiri, H. Koiso, and M. Swerts.
1999. An experimental study on the informational
and grounding functions of prosodic features of Ja-
panese echoic responses. In Proceedings of the
ESCA Workshop on Dialogue and Prosody, pages
187?192, Veldhoven.
M. Swerts, D. Litman, and J. Hirschberg. 2000.
Corrections in spoken dialogue systems. In Pro-
ceedings of the Sixth International Conference on
Spoken Language Processing, Beijing.
E. Wade, E. E. Shriberg, and P. J. Price. 1992. User
behaviors affecting speech recognition. In Proceed-
ings of ICSLP-92, volume 2, pages 995?998, Banff.
Labeling Corrections and Aware Sites
in Spoken Dialogue Systems
Julia Hirschbergy and Marc Swertsz and Diane Litmany
y AT&T Labs{Research z IPO, Eindhoven, The Netherlands,
Florham Park, NJ, 07932 USA and CNTS, Antwerp, Belgium
fjulia/dianeg@research.att.com m.g.j.swerts@tue.nl
Abstract
This paper deals with user correc-
tions and aware sites of system er-
rors in the TOOT spoken dialogue
system. We rst describe our cor-
pus, and give details on our proced-
ure to label corrections and aware
sites. Then, we show that correc-
tions and aware sites exhibit some
prosodic and other properties which
set them apart from `normal' utter-
ances. It appears that some correc-
tion types, such as simple repeats,
are more likely to be correctly recog-
nized than other types, such as para-
phrases. We also present evidence
that system dialogue strategy aects
users' choice of correction type, sug-
gesting that strategy-specic meth-
ods of detecting or coaching users on
corrections may be useful. Aware
sites tend to be shorter than other
utterances, and are also more dif-
cult to recognize correctly for the
ASR system.
1 Introduction
Compared to many other systems, spoken
dialogue systems (SDS) tend to have more
diculties in correctly interpreting user in-
put. Whereas a car will normally go left if
the driver turns the steering wheel in that
direction or a vacuum cleaner will start work-
ing if one pushes the on-button, interactions
between a user and a spoken dialogue system
are often hampered by mismatches between
the action intended by the user and the action
executed by the system. Such mismatches
are mainly due to errors in the Automatic
Speech Recognition (ASR) and/or the Nat-
ural Language Understanding (NLU) com-
ponent of these systems. To solve these mis-
matches, users often have to put considerable
eort in trying to make it clear to the system
that there was a problem, and trying to cor-
rect it by re-entering misrecognized or misin-
terpreted information. Previous research has
already brought to light that it is not always
easy for users to determine whether their in-
tended actions were carried out correctly or
not, in particular when the dialogue system
does not give appropriate feedback about its
internal representation at the right moment.
In addition, users' corrections may miss their
goal, because corrections themselves are more
dicult for the system to recognize and in-
terpret correctly, which may lead to so-called
cyclic (or spiral) errors. That corrections
are dicult for ASR systems is generally ex-
plained by the fact that they tend to be hyper-
articulated | higher, louder, longer . . . than
other turns (Wade et al, 1992; Oviatt et al,
1996; Levow, 1998; Bell and Gustafson, 1999;
Shimojima et al, 1999), where ASR models
are not well adapted to handle this special
speaking style.
The current paper focuses on user correc-
tions, and looks at places where people rst
become aware of a system problem (\aware
sites"). In other papers (Swerts et al, 2000;
Hirschberg et al, 2001; Litman et al, 2001),
we have already given some descriptive stat-
istics on corrections and aware sites and we
have been looking at methods to automatic-
ally predict these two utterance categories.
One of our major ndings is that prosody,
which had already been shown to be a good
predictor of misrecognitions (Litman et al,
2000; Hirschberg et al, 2000), is also useful to
correctly classify corrections and aware sites.
In this paper, we will elaborate more on the
exact labeling scheme we used, and add fur-
ther descriptive statistics. More in particular,
we address the question whether there is much
variance in the way people react to system er-
rors, and if so, to what extent this variance
can be explained on the basis of particular
properties of the dialogue system. In the fol-
lowing section we rst provide details on the
TOOT corpus that we used for our analyses.
Then we give information on the labels for
corrections and aware sites, and on the actual
labeling procedure. The next section gives
the results of some descriptive statistics on
properties of corrections and aware sites and
on their distributions. We will end the paper
with a general discussion of our ndings.
2 The data
2.1 The TOOT corpus
Our corpus consists of dialogues between hu-
man subjects and TOOT, a spoken dialogue
system that allows access to train information
from the web via telephone. TOOT was col-
lected to study variations in dialogue strategy
and in user-adapted interaction (Litman and
Pan, 1999). It is implemented using an
IVR (interactive voice response) platform de-
veloped at AT&T, combining ASR and text-
to-speech with a phone interface (Kamm et
al., 1997). The system's speech recognizer is
a speaker-independent hidden Markov model
system with context-dependent phone models
for telephone speech and constrained gram-
mars dening vocabulary at any dialogue
state. The platform supports barge-in. Sub-
jects performed four tasks with one of several
versions of the system that diered in terms
of locus of initiative (system, user, or mixed),
conrmation strategy (explicit, implicit, or
none), and whether these conditions could
be changed by the user during the task (ad-
aptive vs. non-adaptive). TOOT's initiative
System Initiative, Explicit Conrmation
T: Which city do you want to go to?
U: Chicago.
S: Do you want to go to Chicago?
U: Yes.
User Initiative, No Conrmation
S: How may I help you?
U: I want to go to Chicago from Baltimore.
S: On which day of the week do you want
to leave?
U: I want a train at 8:00.
Mixed Initiative, Implicit Conrmation
S: How may I help you?
U: I want to go to Chicago.
S: I heard you say go to Chicago.
Which city do you want to leave from?
U: Baltimore.
Figure 1: Illustrations of various dialogue
strategies in TOOT
strategy species who has control of the dia-
logue, while TOOT's conrmation strategy
species how and whether TOOT lets the user
know what it just understood. The fragments
in Figure 1 provide some illustrations of how
dialogues vary with strategy. Subjects were
39 students; 20 native speakers and 19 non-
native, 16 female and 23 male. Dialogues
were recorded and system and user behavior
logged automatically. The concept accuracy
(CA) of each turn was manually labeled. If
the ASR correctly captured all task-related
information in the turn (e.g. time, departure
and arrival cities), the turn's CA score was
1 (semantically correct). Otherwise, the CA
score reected the percentage of correctly re-
cognized task information in the turn. The
dialogues were also transcribed and automat-
ically scored in comparison to the ASR re-
cognized string to produce a word error rate
(WER) for each turn. For the study described
below, we examined 2328 user turns (all user
input between two system inputs) from 152
dialogues.
2.2 Dening Corrections and Aware
Sites
To identify corrections
1
in the corpus two au-
thors independently labeled each turn as to
whether or not it constituted a correction of
a prior system failure (a rejection or CA er-
ror, which were the only system failure sub-
jects were aware of) and subsequently de-
cided upon a consensus label. Note that much
of the discrepancies between labels were due
to tiredness or incidental sloppiness of indi-
vidual annotators, rather than true disagree-
ment. Each turn labeled `correction' was fur-
ther classied as belonging to one of the fol-
lowing categories: REP (repetition, includ-
ing repetitions with dierences in pronunci-
ation or uency), PAR (paraphrase), ADD
(task-relevant content added, OMIT (content
omitted), and ADD/OMIT (content both ad-
ded and omitted). Repetitions were further
divided into repetitions with pronunciation
variation (PRON) (e.g. yes correcting yeah),
and repetitions where the correction was pro-
nounced using the same pronunciation as the
original turn, but this distinction was di-
cult to make and turned out not to be useful.
User turns which included both corrections
and other speech acts were so distinguished by
labeling them \2+". For user turns contain-
ing a correction plus one or more additional
dialogue acts, only the correction is used for
purposes of analysis below. We also labeled as
restarts user corrections which followed non-
initial system-initial prompts (e.g. \How may
I help you?" or \What city do you want to
go to?"); in such cases system and user es-
sentially started the dialogue over from the
beginning. Figure 2 shows examples of each
correction type and additional label for cor-
rections of system failures on I want to go
to Boston on Sunday. Note that the utter-
ance on the last line of this gure is labeled
2+PAR, given that this turn consist of two
speech acts: the goal of the no-part of this
1
The labels discussed in this section for corrections
and aware sites may well be related to more general
dialogue acts, like the ones proposed by (Allen and
Core, 1997), but this needs to be explored in more
detail in the future.
turn is to signal a problem, whereas the re-
mainder of this turn serves to correct a prior
error.
Corr Type Correction
REP I want to go to Boston on Sunday
PAR To Boston on Sunday
OMIT I want to go to Boston
ADD To Boston on Sunday at 8pm
ADD/ I want to arrive Sunday at 8pm
OMIT
2+PAR No, to Boston on Sunday
Figure 2: Examples of Correction Types
Each correction was also indexed with an
identier representing the closest prior turn
it was correcting, so that we could investigate
\chains" of corrections of a single failed turn,
by tracing back through subsequent correc-
tions of that turn. Figure 3 shows a fragment
of a TOOT dialogue with corrections labeled
as discussed above.
We also identied aware sites in our cor-
pus | turns where a user, while interact-
ing with a machine, rst becomes aware that
the system has misrecognized a previous user
turn. For our corpus, we tried to determ-
ine whether there was some evidence in the
user turn indicating that the user had be-
come aware of a mistake in the system's un-
derstanding of a previous user turn, and, if
so, which previous turn had occasioned that
error. Note that such aware sites may or
may not also be corrections (another type of
post-misrecognition turn), since a user may
not immediately provide correcting informa-
tion. Also, it may take a while before the
user is able to notice a system error. Fig-
ure 4 shows an example that illustrates cases
in which both the user's awareness and cor-
rection of a misrecognition occur in the same
turn (e.g. turns 1159 and 1160, after sys-
tem prompts for information already given in
turn 1158). It also illustrates cases in which
aware sites and corrections occur in dierent
turns. For example, after the immediate ex-
plicit system conrmation of turn 1162, the
user rst becomes aware of the system errors
Turn Turn ID Corr of Corr Type
S: On which day of the week do you want to leave?
S: Sorry, I didn't hear anything from you.
Please tell me your departure date.
U: Um tomorrow 776 NA NA
S: You can say the departure date. For example,
you can say today or tomorrow. Or you can say
Monday, Tuesday, Wednesday...
U: Tomorrow 777 776 REP
S: You can say the departure date. For example,
you can say today or tomorrow. Or you can say
Monday, Tuesday, Wednesday, Thursday, Friday...
U: Saturday 778 777 PAR
S: Do you want to leave on Saturday?
U: Yes 779 NA NA
S: What time do you want to leave?
U: Eight o'clock A M 780 NA NA
S: Do you want to leave around 10:00pm?
U: Eight o'clock 781 780 OMIT
S: Do you want to leave around 10:00pm?
U: No 782 NA NA
S: What time do you want to leave?
U: 8:00am 783 781 ADD
Figure 3: Toot Dialogue Fragment with Correction Labels
(turn 1163), then separately corrects them
(turn 1164). When no immediate conrm-
ation of an utterance occurs (as with turn
1158), it may take several turns before the
user becomes aware of any misrecognition er-
rors. For example, it is not until turn 1161
that the user rst becomes aware of the er-
ror in date and time from 1158; the user then
corrects the error in 1162. Of all turns in
our corpus, 13% represent cases of turns that
are only corrections, 14% are only aware sites,
and 16% are turns where aware sites and cor-
rections co-occur. Also, note that turns 1162
and 1164 in this dialogue fragment represent
cases of restarts after a system-initial, non-
initial prompt (\How may I help you?").
3 Characteristics of corrections
and aware sites in TOOT
3.1 Corrections
We identied 29% of the turns in the corpus
as corrections. These were a serious prob-
lem for the system, being misrecognized in
terms of transcription error about twice as
often (61%) as non-corrections (33%) and in
terms of concept error in 53% of cases for cor-
rections compared with 21% concept error for
non-corrections.
Corrections diered from non-corrections
in other ways as well, which are suggest-
ive of how they might be identied by an
SDS. We have previously discussed the pros-
odic and acoustic dierences between correc-
tions and non-correction (Swerts et al, 2000;
Hirschberg et al, 2001). In particular, in
line with previous studies, we found that cor-
rections more often tend to be hyperarticu-
lated (longer, louder, with great pitch excur-
sions,...) than other turns. More recently we
have also discovered dierences in other di-
mensions.
First, corrections appear to be longer than
non-corrections in numbers of words, when
we examine actual human transcriptions of
Turn Turn ID Aware of Corr of
S: How may I help you?
U: Get me the train to New York City from
Baltimore on Sunday at eight thirty pm 1158 NA NA
S: Which city do you want to go to?
U: New York City 1159 1158 1158
S: Which city do you want to leave from?
U: Baltimore 1160 1158 1158
S: Do you want me to nd the trains from
Baltimore to New York City today at anytime now?
U: No 1161 1158 NA
S: How may I help you?
U: Get me the train to New York City from
Baltimore on Sunday at eight thirty pm 1162 NA 1158
S: Do you want me to nd the trains from
Baltimore to New York City today at anytime now?
U: No 1163 1162 NA
S: How may I help you?
U: Get me the train on Sunday at eight thirty pm 1164 NA 1162
Figure 4: Dialogue Fragment with Aware and Correction Labels.
them, both in absolute terms (T=17.68;
df=2326; p=0) and also controlling for
speaker (T=5.32; df=38; p=0). Even the
ASR hypotheses show this dierence, with
hypotheses of corrections being longer in ab-
solute terms (T=13.72; df=2326; p=0) and
across speakers (T=5.18; df=38; p=0).
Of the correction types we labeled, the
largest number were REPs and OMITs, as
shown in Table 1, which shows over-all dis-
tribution of correction types, and distribu-
tions for each type of system failure corrected.
Table 1 shows that 39% of TOOT corrections
were simple repetitions of the previously mis-
recognized turn. While this strategy is often
suboptimal in correcting ASR errors (Levow,
1998), REPs (45% error) and OMITs (52% er-
ror) were better recognized than ADDs (90%
error) and PARs (72% error). Thus, over-
all, users tend to have a preference for correc-
tion types that are more likely to be succes-
ful. That REPs and OMITs are more often
correctly recognized can be linked to the ob-
servation that they tend to be realized with
prosody which is less marked than the pros-
ody on ADDs and PARs. Table 2 shows that
REPs and OMITs are closer to normal utter-
ances in terms of their prosodic features than
ADDs, which are considerably higher, longer
and slower. This is in line with our previous
observations that marked settings for these
prosodic features more often lead to recogni-
tion errors.
What the user was correcting also inu-
enced the type of correction chosen. Table
1 shows that corrections of misrecognitions
(Post-Mrec) were more likely to omit inform-
ation present in the original turn (OMITs),
while corrections of rejections (Post-Rej) were
more likely to be simple repetitions. The
latter nding is not surprising, since the re-
jection message for tasks was always a close
paraphrase of \Sorry, I can't understand
you. Can you please repeat your utterance?"
However, it does suggest the surprising power
of system directions, and how important it is
to craft prompts to favor the type of correc-
tion most easily recognized by the system.
Corrections following system restarts
diered in type somewhat from other correc-
tions, with more turns adding new material
to the correction and fewer of them repeating
ADD ADD/OMIT OMIT PAR REP
All 8% 2% 32% 19% 39%
% Mrec(WER) 90% 93% 52% 72% 45%
% Mrec(CA) 88% 71% 47% 65% 45%
Post-Mrec 7% 3% 40% 18% 32%
Post-Rej 6% 0% 7% 28% 59%
Table 1: Distribution of Correction Types
Feature Normal ADD ADD/OMIT OMIT PAR REP
F0max (Hz) 219.4 286.3 252.9 236.7 252.1 239.9
rmsmax 1495.0 1868.1 2646.3 1698.0 1852.4 2024.6
dur (s) 1.4 6.8 4.1 2.3 4.7 2.5
tempo (sylls/s) 2.5 1.7 1.6 2.9 2.1 2.3
Table 2: Averages for dierent prosodic features of dierent Correction Types
the original turn.
Dialogue strategy clearly aected the type
of correction users made. For example, users
more frequently repeat their misrecognized
utterance in the SystemExplicit condition,
than in the MixedImplicit or UserNoConrm;
the latter conditions have larger proportions
of OMITs and ADDs. This is an important
observation given that this suggests that some
dialogue strategies lead to correction types,
such as ADDs, which are more likely to be
misrecognized than correction types elicited
by other strategies.
As noted above, corrections in the TOOT
corpus often take the form of chains of correc-
tions of a single original error. Looking back
at Figure 3, for example, we see two chains
of corrections: In the rst, which begins with
the misrecognition of turn 776 (\Um, tomor-
row"), the user repeats the original phrase
and then provides a paraphrase (\Saturday"),
which is correctly recognized. In the second,
beginning with turn 780, the time of depar-
ture is misrecognized. The user omits some
information (\am") in turn 781, but without
success; an ADD correction follows, with the
previously omitted information restored, in
turn 783. Elsewhere (Swerts et al 2000),
we have shown that chain position has an in-
uence on correction behaviour in the sense
that more distant corrections tend to be mis-
recognized more often than corrections closer
to the original error.
3.2 Aware Sites
708 (30%) of the turns in our corpus were
labeled aware sites. The majority of these
turns (89%) immediately follow the system
failures they react to, unlike the more com-
plex cases in Figure 4 above. If a system
would be able to detect aware sites with a
reasonable accuracy, this would be useful,
given that the system would then be able to
correctly guess in the majority of the cases
that the problem occurred in the preceding
turn. Aware turns, like corrections, tend to
be misrecognized at a higher rate than other
turns; in terms of transcription accuracy, 50%
of awares are misrecognized vs. 35% of other
turns, and in terms of concept accuracy, 39%
of awares are misrecognized compared to 27%
of other turns. In other words, both types
of post-error utterances, i.e., corrections and
aware sites, share the fact that they tend to
lead to additional errors. But whereas we
have shown above that for corrections this is
probably caused by the fact that these utter-
ances are uttered in a hyperarticulated speak-
ing style, we do not nd dierences in hyper-
articulation between aware sites and `normal
utterances' (T= 0.9085; df=38; p=0.3693).
This could mean that these sites are real-
ized in a speaking style which is not per-
ceptibly dierent from normal speaking style
ADD ADD/OMIT OMIT PAR REP
MixedExplicit 1 0 4 1 4
MixedImplicit 16 8 58 44 64
MixedNoConrm 0 0 2 0 1
SystemExplicit 2 2 8 31 67
SystemImplicit 0 1 18 0 20
SystemNoConrm 0 0 5 0 4
UserExplicit 0 0 0 1 1
UserImplicit 1 0 4 3 6
UserNoConrm 31 3 116 47 98
Table 3: Number of Correction Types for dierent dialogue strategies
Single no Other Turns
Aware site 162 546
Not Aware site 122 1498
Table 4: Distribution of single no utterances
and other turns for aware sites versus other
utterances
when judged by human labelers, but which
is still suciently dierent to cause problems
for an ASR system.
In terms of distinguishing features which
might explain or help to identify these turns,
we have previously examined the acoustic
and prosodic features of aware sites (Lit-
man et al, 2001). Here we present some
additional features. Aware sites appear to
be signicantly shorter, in general, than
other turns, both in absolute terms and con-
trolling for speaker variation, and whether
we examine the ASR transcription (absolute:
T=4.86; df=2326; p=0; speaker-controlled:
T=5.37; df=38; p=0) or the human one (ab-
solute: T=3.45; df=2326; p<.0001; speaker-
controlled: T=4.69; df=38; p=0). A sizable
but not overwhelming number of aware sites
in fact consist of a simple negation (i.e., a vari-
ant of the word `no') (see Table 4). This at
the same time shows that a simple no-detector
will not be sucient as an indicator of aware
sites (see also (Krahmer et al, 1999; Krahmer
et al, to appear)), given that most aware sites
are more complex than that, such as turns
1159 and 1160 in the example of Figure 4.
More concretely, Table 4 shows that a single
no would correctly predict that the turn is an
aware site with a precision of only 57% and a
recall of only 23%.
4 Discussion
This paper has dealt with user corrections and
aware sites of system errors in the TOOT
spoken dialogue system. We have described
our corpus, and have given details on our pro-
cedure to label corrections and aware sites.
Then, we have shown that corrections and
aware sites exhibit some prosodic and other
properties which set them apart from `normal'
utterances. It appears that some correction
types, such as simple repeats, are more likely
to be correctly recognized than other types,
such as paraphrases. We have also presen-
ted evidence that system dialogue strategy
aects users' choice of correction type, sug-
gesting that strategy-specic methods of de-
tecting or coaching users on corrections may
be useful. Aware sites tend to be shorter than
other utterances, and are also more dicult
to recognize correctly for the ASR system.
In addition to the descriptive study presen-
ted in this paper, we have also tried to auto-
matically predict corrections and aware sites
using the machine learning program RIP-
PER (Cohen, 1996). These experiments show
that corrections and aware sites can be clas-
sied as such automatically, with a consider-
able degree of accuracy (Litman et al, 2001;
Hirschberg et al, 2001). Such classication,
we believe, will be especially useful in error-
handling for SDS. If aware sites are detect-
able, they can function as backward-looking
error-signaling devices, making it clear to the
system that something has gone wrong in
the preceding context, so that, for example,
the system can reprompt for information. In
this way, they are similar to what others
have termed `go-back' signals (Krahmer et
al., 1999). Aware sites can also be used as
forward-looking signals, indicating upcoming
corrections or more drastic changes in user be-
havior, such as complete restarts of the task.
Given that, in current systems, both correc-
tions and restarts often lead to recognition er-
ror (Swerts et al, 2000), aware sites may be
useful in preparing systems to deal with such
problems. An accurate detection of turns that
are corrections may trigger the use of specially
trained ASR models to better recognize cor-
rections, or can be used to change dialogue
strategy (e.g. from user or mixed initiative to
system initiative after errors).
References
J. Allen and M. Core. 1997. Dialogue markup in
several layers. Draft contribution for the Dis-
course Resource Initiative.
L. Bell and J. Gustafson. 1999. Repetition
and its phonetic realizations: Investigating a
Swedish database of spontaneous computer-
directed speech. In Proceedings of ICPhS-99,
San Francisco. International Congress of Phon-
etic Sciences.
W. Cohen. 1996. Learning trees and rules with
set-valued features. In 14th Conference of the
American Association of Articial Intelligence,
AAAI.
J. Hirschberg, D. Litman, and M. Swerts. 2000.
Generalizing prosodic prediction of speech re-
cognition errors. In Proceedings of the Sixth
International Conference on Spoken Language
Processing, Beijing.
J. Hirschberg, D. Litman, and M. Swerts. 2001.
Identifying user corrections automatically in
spoken dialogue systems. In Proceedings of
NAACL-2001, Pittsburgh.
C. Kamm, S. Narayanan, D. Dutton, and R.
Ritenour. 1997. Evaluating spoken dialogue
systems for telecommunication services. In
Proc. EUROSPEECH-97, Rhodes.
E. Krahmer, M. Swerts, M. Theune, and M. Wee-
gels. 1999. Error spotting in human-
machine interactions. In Proceedings of
EUROSPEECH-99.
E. Krahmer, M. Swerts, M. Theune, and M. Wee-
gels. to appear. The dual of denial: Two uses
of disconrmations in dialogue and their pros-
odic correlates. Accepted for Speech Commu-
nication.
G. Levow. 1998. Characterizing and recogniz-
ing spoken corrections in human-computer dia-
logue. In Proceedings of the 36th Annual Meet-
ing of the Association of Computational Lin-
guistics, COLING/ACL 98, pages 736{742.
D. Litman, J. Hirschberg, and M. Swerts. 2000.
Predicting automatic speech recognition per-
formance using prosodic cues. In Proceedings
of NAACL-00, Seattle, May.
D. Litman, J. Hirschberg, and M. Swerts. 2001.
Predicting User Reactions to System Error. In
Proceedings of ACL-01, Toulouse, July.
D. Litman and S. Pan. 1999. Empirically eval-
uating an adaptable spoken dialogue system.
In Proceedings tth International conference on
User Modeling.
S. L. Oviatt, G. Levow, M. MacEarchern, and
K. Kuhn. 1996. Modeling hyperarticulate
speech during human-computer error resolu-
tion. In Proceedings of ICSLP-96, pages 801{
804, Philadelphia.
A. Shimojima, K. Katagiri, H. Koiso, and
M. Swerts. 1999. An experimental study on
the informational and grounding functions of
prosodic features of Japanese echoic responses.
In Proceedings of the ESCA Workshop on Dia-
logue and Prosody, pages 187{192, Veldhoven.
M. Swerts, D. Litman, and J. Hirschberg. 2000.
Corrections in spoken dialogue systems. In Pro-
ceedings of the Sixth International Conference
on Spoken Language Processing, Beijing.
E. Wade, E. E. Shriberg, and P. J. Price. 1992.
User behaviors aecting speech recognition. In
Proceedings of ICSLP-92, volume 2, pages 995{
998, Ban.
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 410?420, Prague, June 2007. c?2007 Association for Computational Linguistics
V-Measure: A conditional entropy-based external cluster evaluation
measure
Andrew Rosenberg and Julia Hirschberg
Department of Computer Science
Columbia University
New York, NY 10027
{amaxwell,julia}@cs.columbia.edu
Abstract
We present V-measure, an external entropy-
based cluster evaluation measure. V-
measure provides an elegant solution to
many problems that affect previously de-
fined cluster evaluation measures includ-
ing 1) dependence on clustering algorithm
or data set, 2) the ?problem of matching?,
where the clustering of only a portion of data
points are evaluated and 3) accurate evalu-
ation and combination of two desirable as-
pects of clustering, homogeneity and com-
pleteness. We compare V-measure to a num-
ber of popular cluster evaluation measures
and demonstrate that it satisfies several de-
sirable properties of clustering solutions, us-
ing simulated clustering results. Finally, we
use V-measure to evaluate two clustering
tasks: document clustering and pitch accent
type clustering.
1 Introduction
Clustering techniques have been used successfully
for many natural language processing tasks, such
as document clustering (Willett, 1988; Zamir and
Etzioni, 1998; Cutting et al, 1992; Vempala and
Wang, 2005), word sense disambiguation (Shin and
Choi, 2004), semantic role labeling (Baldewein et
al., 2004), pitch accent type disambiguation (Levow,
2006). They are particularly appealing for tasks
in which there is an abundance of language data
available, but manual annotation of this data is
very resource-intensive. Unsupervised clustering
can eliminate the need for (full) manual annotation
of the data into desired classes, but often at the cost
of making evaluation of success more difficult.
External evaluation measures for clustering can
be applied when class labels for each data point in
some evaluation set can be determined a priori. The
clustering task is then to assign these data points to
any number of clusters such that each cluster con-
tains all and only those data points that are members
of the same class Given the ground truth class la-
bels, it is trivial to determine whether this perfect
clustering has been achieved. However, evaluating
how far from perfect an incorrect clustering solution
is a more difficult task (Oakes, 1998) and proposed
approaches often lack rigor (Meila, 2007).
In this paper, we describe a new entropy-based
external cluster evaluation measure, V-MEASURE1 ,
designed to address the problem of quantifying such
imperfection. Like all external measures, V-measure
compares a target clustering ? e.g., a manually an-
notated representative subset of the available data ?
against an automatically generated clustering to de-
termine now similar the two are. We introduce two
complementary concepts, completeness and homo-
geneity, to capture desirable properties in clustering
tasks.
In Section 2, we describe V-measure and how it
is calculated in terms of homogeneity and complete-
ness. We describe several popular external cluster
evaluation measures and draw some comparisons to
V-measure in Section 3. In Section 4, we discuss
how some desirable properties for clustering are sat-
isfied by V-measure vs. other measures. In Sec-
tion 5, we present two applications of V-measure, on
document clustering and on pitch accent type clus-
tering.
2 V-Measure and Its Calculation
V-measure is an entropy-based measure which ex-
plicitly measures how successfully the criteria of ho-
mogeneity and completeness have been satisfied. V-
measure is computed as the harmonic mean of dis-
tinct homogeneity and completeness scores, just as
1The ?V? stands for ?validity?, a common term used to de-
scribe the goodness of a clustering solution.
410
precision and recall are commonly combined into
F-measure (Van Rijsbergen, 1979). As F-measure
scores can be weighted, V-measure can be weighted
to favor the contributions of homogeneity or com-
pleteness.
For the purposes of the following discussion, as-
sume a data set comprising N data points, and two
partitions of these: a set of classes, C = {ci|i =
1, . . . , n} and a set of clusters, K = {ki|1, . . . ,m}.
Let A be the contingency table produced by the clus-
tering algorithm representing the clustering solution,
such that A = {aij} where aij is the number of data
points that are members of class ci and elements of
cluster kj .
To discuss cluster evaluation measures we intro-
duce two criteria for a clustering solution: homo-
geneity and completeness. A clustering result sat-
isfies homogeneity if all of its clusters contain only
data points which are members of a single class. A
clustering result satisfies completeness if all the data
points that are members of a given class are elements
of the same cluster. The homogenity and complete-
ness of a clustering solution run roughly in opposi-
tion: Increasing the homogeneity of a clustering so-
lution often results in decreasing its completeness.
Consider, two degenerate clustering solutions. In
one, assigning every datapoint into a single cluster,
guarantees perfect completeness ? all of the data
points that are members of the same class are triv-
ially elements of the same cluster. However, this
cluster is as unhomogeneous as possible, since all
classes are included in this single cluster. In an-
other solution, assigning each data point to a dis-
tinct cluster guarantees perfect homogeneity ? each
cluster trivially contains only members of a single
class. However, in terms of completeness, this so-
lution scores very poorly, unless indeed each class
contains only a single member. We define the dis-
tance from a perfect clustering is measured as the
weighted harmonic mean of measures of homogene-
ity and completeness.
Homogeneity:
In order to satisfy our homogeneity criteria, a
clustering must assign only those datapoints that are
members of a single class to a single cluster. That is,
the class distribution within each cluster should be
skewed to a single class, that is, zero entropy. We de-
termine how close a given clustering is to this ideal
by examining the conditional entropy of the class
distribution given the proposed clustering. In the
perfectly homogeneous case, this value, H(C|K),
is 0. However, in an imperfect situation, the size of
this value, in bits, is dependent on the size of the
dataset and the distribution of class sizes. There-
fore, instead of taking the raw conditional entropy,
we normalize this value by the maximum reduction
in entropy the clustering information could provide,
specifically, H(C).
Note that H(C|K) is maximal (and equals H(C))
when the clustering provides no new information ?
the class distribution within each cluster is equal to
the overall class distribiution. H(C|K) is 0 when
each cluster contains only members of a single class,
a perfectly homogenous clustering. In the degen-
erate case where H(C) = 0, when there is only a
single class, we define homogeneity to be 1. For a
perfectly homogenous solution, this normalization,
H(C|K)
H(C) , equals 0. Thus, to adhere to the convention
of 1 being desirable and 0 undesirable, we define ho-
mogeneity as:
h =
{
1 if H(C,K) = 0
1? H(C|K)H(C) else
(1)
where
H(C|K) = ?
|K|
?
k=1
|C|
?
c=1
ack
N log
ack
?|C|
c=1 ack
H(C) = ?
|C|
?
c=1
?|K|
k=1 ack
n log
?|K|
k=1 ack
n
Completeness:
Completeness is symmetrical to homogeneity. In
order to satisfy the completeness criteria, a cluster-
ing must assign all of those datapoints that are mem-
bers of a single class to a single cluster. To eval-
uate completeness, we examine the distribution of
cluster assignments within each class. In a perfectly
complete clustering solution, each of these distribu-
tions will be completely skewed to a single cluster.
We can evaluate this degree of skew by calculat-
ing the conditional entropy of the proposed cluster
distribution given the class of the component dat-
apoints, H(K|C). In the perfectly complete case,
H(K|C) = 0. However, in the worst case scenario,
411
each class is represented by every cluster with a dis-
tribution equal to the distribution of cluster sizes,
H(K|C) is maximal and equals H(K). Finally, in
the degenerate case where H(K) = 0, when there
is a single cluster, we define completeness to be 1.
Therefore, symmetric to the calculation above, we
define completeness as:
c =
{
1 if H(K,C) = 0
1 ? H(K|C)H(K) else
(2)
where
H(K|C) = ?
|C|
?
c=1
|K|
?
k=1
ack
N log
ack
?|K|
k=1 ack
H(K) = ?
|K|
?
k=1
?|C|
c=1 ack
n log
?|C|
c=1 ack
n
Based upon these calculations of homogeneity
and completeness, we then calculate a clustering
solution?s V-measure by computing the weighted
harmonic mean of homogeneity and completeness,
V? = (1+?)?h?c(??h)+c . Similarly to the familiar F-
measure, if ? is greater than 1 completeness is
weighted more strongly in the calculation, if ? is less
than 1, homogeneity is weighted more strongly.
Notice that the computations of homogeneity,
completeness and V-measure are completely inde-
pendent of the number of classes, the number of
clusters, the size of the data set and the clustering al-
gorithm used. Thus these measures can be applied to
and compared across any clustering solution, regard-
less of the number of data points (n-invariance), the
number of classes or the number of clusters. More-
over, by calculating homogeneity and completeness
separately, a more precise evaluation of the perfor-
mance of the clustering can be obtained.
3 Existing Evaluation Measures
Clustering algorithms divide an input data set into
a number of partitions, or clusters. For tasks where
some target partition can be defined for testing pur-
poses, we define a ?clustering solution? as a map-
ping from each data point to its cluster assignments
in both the target and hypothesized clustering. In the
context of this discussion, we will refer to the target
partitions, or clusters, as CLASSES, referring only to
hypothesized clusters as CLUSTERS.
Two commonly used external measures for as-
sessing clustering success are Purity and Entropy
(Zhao and Karypis, 2001), defined as,
Purity = ?kr=1 1n maxi(nir)
Entropy = ?kr=1 nrn (? 1log q
?q
i=1
nir
nr log
nir
nr )
where q is the number of classes, k the number
of clusters, nr is the size of cluster r, and nir is the
number of data points in class i clustered in cluster
r.
Both these approaches represent plausable ways
to evaluate the homogeneity of a clustering solution.
However, our completeness criterion is not mea-
sured at all. That is, they do not address the ques-
tion of whether all members of a given class are in-
cluded in a single cluster. Therefore the Purity and
Entropy measures are likely to improve (increased
Purity, decreased Entropy) monotonically with
the number of clusters in the result, up to a degen-
erate maximum where there are as many clusters as
data points. However, clustering solutions rated high
by either measure may still be far from ideal.
Another frequently used external clustering eval-
uation measure is commonly refered to as ?cluster-
ing accuracy?. The calculation of this accuracy is
inspired by the information retrieval metric of F-
Measure (Van Rijsbergen, 1979). The formula for
this clustering F-measure as described in (Fung et
al., 2003) is shown in Figure 3.
Let N be the number of data points, C the set of classes, K
the set of clusters and nij be the number of members of class
ci ? C that are elements of cluster kj ? K.
F (C, K) =
X
ci?C
|ci|
N maxkj?K
{F (ci, kj)} (3)
F (ci, kj) =
2 ? R(ci, kj) ? P (ci, kj)
R(ci, kj) + P (ci, kj)
R(ci, kj) =
nij
|ci|
P (ci, kj) =
nij
|kj |
Figure 1: Calculation of clustering F-measure
This measure has a significant advantage over
Purity and Entropy, in that it does measure both
the homogeneity and the completeness of a cluster-
ing solution. Recall is calculated as the portion of
items from class i that are present in cluster j, thus
measuring how complete cluster j is with respect to
class i. Similarly, Precision is calculated as the por-
412
Solution A Solution B
F-Measure=0.5 F-Measure=0.5
V-Measure=0.14 V-Measure=0.39
Solution C Solution D
F-Measure=0.6 F-Measure=0.6
V-Measure=0.30 V-Measure=0.41
Figure 2: Examples of the Problem of Matching
tion of cluster j that is a member of class i, thus mea-
suring how homogenous cluster j is with respect to
class i.
Like some other external cluster evaluation tech-
niques (misclassification index (MI) (Zeng et al,
2002), H (Meila and Heckerman, 2001), L (Larsen
and Aone, 1999), D (van Dongen, 2000), micro-
averaged precision and recall (Dhillon et al, 2003)),
F-measure relies on a post-processing step in which
each cluster is assigned to a class. These techniques
share certain problems. First, they calculate the
goodness not only of the given clustering solution,
but also of the cluster-class matching. Therefore, in
order for the goodness of two clustering solutions to
be compared using one these measures, an identical
post-processing algorithm must be used. This prob-
lem can be trivially addressed by fixing the class-
cluster matching function and including it in the def-
inition of the measure as in H . However, a second
and more critical problem is the ?problem of match-
ing? (Meila, 2007). In calculating the similarity be-
tween a hypothesized clustering and a ?true? cluster-
ing, these measures only consider the contributions
from those clusters that are matched to a target class.
This is a major problem, as two significantly differ-
ent clusterings can result in identical scores.
In figure 2, we present some illustrative examples
of the problem of matching. For the purposes of this
discussion we will be using F-Measure as the mea-
sure to describe the problem of matching, however,
these problems affect any measure which requires a
mapping from clusters to classes for evaluation.
In the figures, the shaded regions represent CLUS-
TERS, the shapes represent CLASSES. In a perfect
clustering, each shaded region would contain all and
only the same shapes. The problem of matching
can manifest itself either by not evaluating the en-
tire membership of a cluster, or by not evaluating
every cluster. The former situation is presented in
the figures A and B in figure 2. The F-Measure of
both of these clustering solutions in 0.6. (The preci-
sion and recall for each class is 35 .) That is, for each
class, the best or ?matched? cluster contains 3 of 5
elements of the class (Recall) and 3 of 5 elements of
the cluster are members of the class (Precision). The
make up of the clusters beyond the majority class is
not evaluated by F-Measure. Solution B is a better
clustering solution than solution A, in terms of both
homogeneity (crudely, ?each cluster contains fewer2
classes?) and completeness (?each class is contained
in fewer clusters?). Indeed, the V-Measure of so-
lution B (0.387) is greater than that of solution A
(0.135). Solutions C and D represent a case in which
not every cluster is considered in the evaluation of
F-Measure. In this example, the F-Measure of both
solutions is 0.5 (the harmonic mean of 35 and 37 ). The
small ?unmatched? clusters are not measured at all
in the calculation of F-Measure. Solution D is a bet-
ter clustering than solution C ? there are no incorrect
clusterings of different classes in the small clusters.
V-Measure reflects this, solution C has a V-measure
of 0.30 while the V-measure of solution D is 0.41.
A second class of clustering evaluation techniques
is based on a combinatorial approach which exam-
ines the number of pairs of data points that are clus-
tered similarly in the target and hypothesized clus-
tering. That is, each pair of points can either be 1)
clustered together in both clusterings (N11), 2) clus-
tered separately in both clusterings (N00), 3) clus-
tered together in the hypothesized but not the tar-
get clustering (N01) or 4) clustered together in the
target but not in the hypothesized clustering (N10).
Based on these 4 values, a number of measures have
been proposed, including Rand Index (Rand, 1971),
2Homogeneity is not measured by V-measure as a count of
the number of classes contained by a cluster but ?fewer? is an
acceptable way to conceptualize this criterion for the purposes
of these examples.
413
Adjusted Rand Index (Hubert and Arabie, 1985), ?
statistic (Hubert and Schultz, 1976), Jaccard (Mil-
ligan et al, 1983), Fowlkes-Mallows (Fowlkes and
Mallows, 1983) and Mirkin (Mirkin, 1996). We il-
lustrate this class of measures with the calculation of
Rand Index. Rand(C,K) = N11+N00n(n?1)/2 Rand Index
can be interpreted as the probability that a pair of
points is clustered similarly (together or separately)
in C and K .
Meila (2007) describes a number of poten-
tial problems of this class of measures posed by
(Fowlkes and Mallows, 1983) and (Wallace, 1983).
The most basic is that these measures tend not to
vary over the interval of [0, 1]. Transformations like
those applied by the adjusted Rand Index and a mi-
nor adjustment to the Mirkin measure (see Section
4) can address this problem. However, pair match-
ing measures also suffer from distributional prob-
lems. The baseline for Fowlkes-Mallows varies sig-
nificantly between 0.6 and 0 when the ratio of data
points to clusters is greater than 3 ? thus includ-
ing nearly all real-world clustering problems. Simi-
larly, the Adjusted Rand Index, as demonstrated us-
ing Monte Carlo simulations in (Fowlkes and Mal-
lows, 1983), varies from 0.5 to 0.95. This variance
in the measure?s baseline prompts Meila to ask if the
assumption of linearity following normalization can
be maintained. If the behavior of the measure is so
unstable before normalization can users reasonably
expect stable behavior following normalization?
A final class of cluster evaluation measures are
based on information theory. These measures an-
alyze the distribution of class and cluster member-
ship in order to determine how successful a given
clustering solution is or how different two parti-
tions of a data set are. We have already examined
one member of this class of measures, Entropy.
From a coding theory perspective, Entropy is the
weighted average of the code lengths of each clus-
ter. Our V-measure is a member of this class of clus-
tering measures. One significant advantage that in-
formation theoretic evaluation measures have is that
they provide an elegant solution to the ?problem of
matching?. By examining the relative sizes of the
classes and clusters being evaluated, these measures
all evaluate the entire membership of each cluster ?
not just a ?matched? portion.
Dom?s Q0 measure (Dom, 2001) uses conditional
entropy, H(C|K) to calculate the goodness of a
clustering solution. That is, given the hypothesized
partition, what is the number of bits necessary to
represent the true clustering?
However, this term ? like the Purity and
Entropy measures ? only evaluates the homogene-
ity of a solution. To measure the completeness of the
hypothesized clustering, Dom includes a model cost
term calculated using a coding theory argument. The
overall clustering quality measure presented is the
sum of the costs of representing the data (H(C|K))
and the model. The motivation for this approach
is an appeal to parsimony: Given identical condi-
tional entropies, H(C|K), the clustering solution
with the fewest clusters should be preferred. Dom
also presents a normalized version of this term, Q2,
which has a range of (0, 1] with greater scores being
representing more preferred clusterings.
Q0(C,K) = H(C|K)+
1
n
|K|
?
k=1
log
(h(k) + |C| ? 1
|C| ? 1
)
where C is the target partition, K is the hypothe-
sized partition and h(k) is the size of cluster k.
Q2(C,K) =
1
n
?|C|
c=1 log
(h(c)+|C|?1
|C|?1
)
Q0(C,K)
We believe that V-measure provides two significant
advantages over Q0 that make it a more useful diag-
nostic tool. First, Q0 does not explicitly calculate the
degree of completeness of the clustering solution.
The cost term captures some of this information,
since a partition with fewer clusters is likely to be
more complete than a clustering solution with more
clusters. However, Q0 does not explicitly address
the interaction between the conditional entropy and
the cost of representing the model. While this is
an application of the minimum description length
(MDL) principle (Rissanen, 1978; Rissanen, 1989),
it does not provide an intuitive manner for assessing
our two competing criteria of homogeneity and com-
pleteness. That is, at what point does an increase in
conditional entropy (homogeneity) justify a reduc-
tion in the number of clusters (completeness).
Another information-based clustering measure
is variation of information (V I) (Meila, 2007),
V I(C,K) = H(C|K)+H(K|C). V I is presented
414
as a distance measure for comparing partitions (or
clusterings) of the same data. It therefore does not
distinguish between hypothesized and target cluster-
ings. V I has a number of useful properties. First,
it satisfies the metric axioms. This quality allows
users to intuitively understand how V I values com-
bine and relate to one another. Secondly, it is ?con-
vexly additive?. That is to say, if a cluster is split,
the distance from the new cluster to the original is
the distance induced by the split times the size of
the cluster. This property guarantees that all changes
to the metric are ?local?: the impact of splitting or
merging clusters is limited to only those clusters in-
volved, and its size is relative to the size of these
clusters. Third, VI is n-invariant: the number of
data points in the cluster do not affect the value of
the measure. V I depends on the relative sizes of the
partitions of C and K , not on the number of points
in these partitions. However, V I is bounded by the
maximum number of clusters in C or K , k?. With-
out manual modification however, k? = n, where
each cluster contains only a single data point. Thus,
while technically n-invariant, the possible values of
V I are heavily dependent on the number of data
points being clustered. Thus, it is difficult to com-
pare V I values across data sets and clustering algo-
rithms without fixing k?, as V I will vary over differ-
ent ranges. It is a trivial modification to modify V I
such that it varies over [0,1]. Normalizing, V I by
log n or 1/2 log k? guarantee this range. However,
Meila (2007) raises two potential problems with this
modification. The normalization should not be ap-
plied if data sets of different sizes are to be com-
pared ? it negates the n-invariance of the measure.
Additionally, if two authors apply the latter normal-
ization and do not use the same value for k?, their
results will not be comparable.
While V I has a number of very useful distance
properties when analyzing a single data set across a
number of settings, it has limited utility as a general
purpose clustering evaluation metric for use across
disparate clusterings of disparate data sets. Our
homogeneity (h) and completeness (c) terms both
range over [0,1] and are completely n-invariant and
k?-invariant. Furthermore, measuring each as a ra-
tio of bit lengths has greater intuitive appeal than a
more opportunistic normalization.
V-measure has another advantage as a clustering
evaluation measure over V I and Q0. By evaluat-
ing homogeneity and completeness in a symmetri-
cal, complementary manner, the calculation of V-
measure makes their relationship clearly observable.
Separate analyses of homogeneity and complete-
ness are not possible with any other cluster evalu-
ation measure. Moreover, by using the harmonic
mean to combine homogeneity and completeness,
V-measure is unique in that it can also prioritize one
criterion over another, depending on the clustering
task and goals.
4 Comparing Evaluation Measures
Dom (2001) describes a parametric technique for
generating example clustering solutions. He then
proceeds to define five ?desirable properties? that
clustering accuracy measures should display, based
on the parameters used to generate the clustering so-
lution. To compare V-measure more directly to alter-
native clustering measures, we evaluate V-measure
and other measures against these and two additional
desirable properties.
The parameters used in generating a clustering so-
lution are as follows.
? |C| The number of classes
? |K| The number of clusters
? |Knoise| Number of ?noise? clusters;
|Knoise| < |K|
? |Cnoise| Number of ?noise? classes; |Cnoise| <
|C|
? ? Error probability; ? = ?1 + ?2 + ?3.
? ?1 The error mass within ?useful? class-cluster
pairs
? ?2 The error mass within noise clusters
? ?3 The error mass within noise classes
The construction of a clustering solution begins
with a matching of ?useful? clusters to ?useful?
classes3. There are |Ku| = |K| ? |Knoise| ?useful?
clusters and |Cu| = |C| ? |Cnoise| ?useful? classes.
The claim is useful classes and clusters are matched
to each other and matched pairs contain more data
points than unmatched pairs. Probability mass of
1 ? ? is evenly distributed across each match. Er-
ror mass of ?1 is evenly distributed across each pair
3The operation of this matching is omitted in the interest of
space. Interested readers should see (Dom, 2001).
415
of non-matching useful class/cluster pairs. Noise
clusters are those that contain data points equally
from each cluster. Error mass of ?2 is distributed
across every ?noise?-cluster/ ?useful?-class pair. We
extend the parameterization technique described in
(Dom, 2001) in with |Cnoise| and ?3. Noise classes
are those that contain data points equally from each
cluster. Error mass of ?3 is distributed across every
?useful?-cluster/?noise?-class pair. An example so-
lution, along with its generating parameters is given
in Figure 3.
C1 C2 C3 Cnoise1
K1 12 12 2 3
K2 2 2 12 3
Knoise1 4 4 4 0
Figure 3: Sample parametric clustering solution
with n = 60, |K| = 3, |Knoise| = 1, |C| =
3, |Cnoise| = 1, ?1 = .1, ?2 = .2, ?3 = .1
The desirable properties proposed by Dom are
given as P1-P5 in Table 1. We include two addi-
tional properties (P6,P7) relating the examined mea-
sure value to the number of ?noise? classes and ?3.
P1 For |Ku| < |C| and ?|Ku| ? (|C| ? |Ku|),
?M
?|Ku| > 0
P2 For |Ku| ? |C|, ?M?|Ku| < 0
P3 ?M?|Knoise| < 0, if ?2 > 0
P4 ?M??1 ? 0, with equality only if |Ku| = 1
P5 ?M??2 ? 0, with equality only if |Knoise| = 0
P6 ?M?|Cnoise| < 0, if ?3 > 0
P7 ?M??3 ? 0, with equality only if |Cnoise| = 0
Table 1: Desirable Properties of a cluster evaluation
measure M
To evaluate how different clustering measures sat-
isfy each of these properties, we systematically var-
ied each parameter, keeping |C| = 5 fixed.
? |Ku|: 10 values: 2, 3,. . . , 11
? |Knoise|: 7 values: 0, 1,. . . , 6
? |Cnoise|: 7 values: 0, 1,. . . , 6
? ?1: 4 values: 0, 0.033, 0.066, 0.1
? ?2: 4 values: 0, 0.066, 0.133, 0.2
? ?3: 4 values: 0, 0.066, 0.133, 0.2
We evaluated the behavior of V-Measure, Rand,
Mirkin, Fowlkes-Mallows, Gamma, Jaccard, VI,
Q0, F-Measure against the desirable properties P1-
P74. Based on the described systematic modification
of each parameter, only V-measure, VI and Q0 em-
pirically satisfy all of P1-P7 in all experimental con-
ditions. Full results reporting how frequently each
evaluated measure satisfied the properties based on
these experiments can be found in table 2.
All evaluated measures satisfy P4 and P7. How-
ever, Rand, Mirkin, Fowlkes-Mallows, Gamma, Jac-
card and F-Measure all fail to satisfy P3 and P6 in
at least one experimental configuration. This indi-
cates that the number of ?noise? classes or clusters
can be increased without reducing any of these mea-
sures. This implies a computational obliviousness to
potentially significant aspects of an evaluated clus-
tering solution.
5 Applying V-measure
In this section, we present two clustering experi-
ments. We describe a document clustering experi-
ment and evaluate its results using V-measure, high-
lighting the interaction between homogeneity and
completeness. Second, we present a pitch accent
type clustering experiment. We present results from
both of these experiments in order to show how V-
measure can be used to drawn comparisons across
data sets.
5.1 Document Clustering
Clustering techniques have been used widely to sort
documents into topic clusters. We reproduce such
an experiment here to demonstrate the usefulness
of V-measure. Using a subset of the TDT-4 cor-
pus (Strassel and Glenn, 2003) (1884 English news
wire and broadcast news documents manually la-
beled with one of 12 topics), we ran clustering
experiments using k-means clustering (McQueen,
1967) and evaluated the results using V-Measure,
VI and Q0 ? those measures that satisfied the de-
sirable properties defined in section 4. The top-
ics and relative distributions are as follows: Acts
4The inequalities in the desirable properties are inverted in
the evaluation of VI, Q0 and Mirkin as they are defined as dis-
tance, as opposed to similarity, measures.
416
Property Rand Mirkin Fowlkes ? Jaccard F-measure Q0 VI V-Measure
P1 0.18 0.22 1.0 1.0 1.0 1.0 1.0 1.0 1.0
P2 1.0 1.0 0.76 1.0 0.89 0.98 1.0 1.0 1.0
P3 0.0 0.0 0.30 0.19 0.21 0.0 1.0 1.0 1.0
P4 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
P5 0.50 0.57 1.0 1.0 1.0 1.0 1.0 1.0 1.0
P6 0.20 0.20 0.41 0.26 0.52 0.87 1.0 1.0 1.0
P7 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
Table 2: Rates of satisfaction of desirable properties
of Violence/War (22.3%), Elections (14.4%), Diplo-
matic Meetings (12.9%), Accidents (8.75%), Natu-
ral Disasters (7.4%), Human Interest (6.7%), Scan-
dals (6.5%), Legal Cases (6.4%), Miscellaneous
(5.3%), Sports (4.7), New Laws (3.2%), Science and
Discovery (1.4%).
We employed stemmed (Porter, 1980), tf*idf-
weighted term vectors extracted for each document
as the clustering space for these experiments, which
yielded a very high dimension space. To reduce
this dimensionality, we performed a simple feature
selection procedure including in the feature vector
only those terms that represented the highest tf*idf
value for at least one data point. This resulted in a
feature vector containing 484 tf*idf values for each
document. Results from k-means clustering are are
shown in Figure 4.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 1  10  100  1000
 3
 3.5
 4
 4.5
 5
 5.5
V-
m
ea
su
re
 a
nd
 Q
2 
va
lue
s
VI
 v
al
ue
s
number of clusters
V-Measure
VI
Q2
Figure 4: Results of document clustering measured
by V-Measure, VI and Q2
The first observation that can be drawn from these
results is the degree to which VI is dependent on the
number of clusters (k). This dependency severely
limits the usefulness of VI: it is inappropriate in se-
lecting an appropriate parameter for k or for evalu-
ating the distance between clustering solutions gen-
erated using different values of k.
V-measure and Q2 demonstrate similar behavior
in evaluating these experimental results. They both
reach a maximal value with 35 clusters, however, Q2
shows a greater descent as the number of clusters in-
creases. We will discuss this quality in greater detail
in section 5.2.
5.2 Pitch Accent Clustering
Pitch accent is how speakers of many languages
make a word intonational prominent. In most
pitch accent languages, words can also be ac-
cented in different ways to convey different mean-
ings (Hirschberg, 2002). In the ToBI labeling con-
ventions for Standard American English (Silverman
et al, 1992), for example, there are five different ac-
cent types (H*, L*, H+!H*, L+H*, L*+H).
We extracted a number of acoustic features from
accented words within the read portion of the Boston
Directions Corpus (BDC) (Nakatani et al, 1995) and
examined how well clustering in these acoustic di-
mensions correlates to manually annotated pitch ac-
cent types. We obtained a very skewed distribution,
with a majority of H* pitch accents.5 We there-
fore included only a randomly selected 10% sample
of H* accents, providing a more even distribution
of pitch accent types for clustering: H* (54.4%),
L*(32.1%), L+H* (26.5%), L*+H (2.8%), H+!H*
(2.1%).
We extracted ten acoustic features from each ac-
cented word to serve as the clustering space for
this experiment. Using Praat?s (Boersma, 2001) Get
Pitch (ac)... function, we calculated the mean F0
and ?F0, as well as z-score speaker normalized ver-
sions of the same. We included in the feature vector
the relative location of the maximum pitch value in
the word as well as the distance between this max-
5Pitch accents containing a high tone may also be down-
stepped, or spoken in a compressed pitch range. Here we col-
lapsed all DOWNSTEPPED instances of each pitch accent with
the corresponding non-downstepped instances.
417
imum and the point of maximum intensity. Finally,
we calculated the raw and speaker normalized slope
from the start of the word to the maximum pitch, and
from the maximum pitch to the end of the word.
Using this feature vector, we performed k-means
clustering and evaluate how successfully these di-
mensions represent differences between pitch accent
types. The resulting V-measure, VI and Q0 calcula-
tions are shown in Figure 5.
 0
 0.05
 0.1
 0.15
 0.2
 1  10  100  1000
 2
 3
 4
 5
 6
 7
 8
V-
m
ea
su
re
 a
nd
 Q
2 
va
lue
s
VI
 v
al
ue
s
number of clusters
VI
V-measure
Q2
Figure 5: Results of pitch accent clustering mea-
sured by V-Measure, VI and Q0
In evaluating the results from these experiments,
Q2 and V-measure reveal considerably different be-
haviors. Q2 shows a maximum at k = 10, and de-
scends at k increases. This is an artifact of the MDL
principle. Q2 makes the claim that a clustering so-
lution based on fewer clusters is preferable to one
using more clusters, and that the balance between
the number of clusters and the conditional entropy,
H(C|K), should be measured in terms of coding
length. With V-measure, we present a different argu-
ment. We contend that the a high value of k does not
inherently reduce the goodness of a clustering solu-
tion. Using these results as an example, we find that
at approximately 30 clusters an increase of clusters
translates to an increase in V-Measure. This is due to
an increased homogeneity (H(C|K)H(C) ) and a relatively
stable completeness (H(K|C)H(K) ). That is, inclusion of
more clusters leads to clusters with a more skewed
within-cluster distribution and a equivalent distribu-
tion of cluster memberships within classes. This is
intuitively preferable ? one criterion is improved, the
other is not reduced ? despite requiring additional
clusters. This is an instance in which the MDL prin-
ciple limits the usefulness of Q2. We again (see sec-
tion 5.1) observe the close dependency of VI and k.
Moreover, in considering figures 5 and 4, simulta-
neously, we see considerably higher values achieved
by the document clustering experiments. Given the
na??ve approaches taken in these experiments, this is
expected ? and even desired ? given the previous
work on these tasks: document clustering has been
notably more successfully applied than pitch accent
clustering. These examples allow us to observe how
transparently V-measure can be used to compare the
behavior across distinct data sets.
6 Conclusion
We have presented a new external cluster evaluation
measure, V-measure, and compared it with existing
clustering evaluation measures. V-measure is based
upon two criteria for clustering usefulness, homo-
geneity and completeness, which capture a cluster-
ing solution?s success in including all and only data-
points from a given class in a given cluster. We have
also demonstrated V-measure?s usefulness in com-
paring clustering success across different domains
by evaluating document and pitch accent cluster-
ing solutions. We believe that V-measure addresses
some of the problems that affect other cluster mea-
sures. 1) It evaluates a clustering solution indepen-
dent of the clustering algorithm, size of the data set,
number of classes and number of clusters. 2) It does
not require its user to map each cluster to a class.
Therefore, it only evaluates the quality of the cluster-
ing, not a post-hoc class-cluster mapping. 3) It eval-
uates the clustering of every data point, avoiding the
?problem of matching?. 4) By evaluating the criteria
of both homogeneity and completeness, V-measure
is more comprehensive than those that evaluate only
one. 5) Moreover, by evaluating these criteria sepa-
rately and explicitly, V-measure can serve as an el-
egant diagnositic tool providing greater insight into
clustering behavior.
Acknowledgments
The authors thank Kapil Thadani, Martin Jansche
and Sasha Blair-Goldensohn and for their feedback.
This work was funded in part by the DARPA GALE
program under a subcontract to SRI International.
418
References
Ulrike Baldewein, Katrin Erk, Sebastian Pado, and Detlef
Prescher. 2004. Semantic role labelling with similarity-
based generalization using EM-based clustering. In Pro-
ceedings of Senseval?04, Barcelona.
Paul Boersma. 2001. Praat, a system for doing phonetics by
computer. Glot International, 5(9-10):341?345.
Douglass R. Cutting, Jan O. Pedersen, David Karger, and
John W. Tukey. 1992. Scatter/gather: A cluster-based ap-
proach to browsing large document collections. In Proceed-
ings of the Fifteenth Annual International ACM SIGIR Con-
ference on Research and Development in Information Re-
trieval, pages 318?329.
I. S. Dhillon, S. Mallela, and D. S. Modha. 2003. Information-
theoretic co-clustering. In Proceedings of The Ninth ACM
SIGKDD International Conference on Knowledge Discovery
and Data Mining(KDD-2003), pages 89?98.
Byron E. Dom. 2001. An information-theoretic external
cluster-validity measure. Technical Report RJ10219, IBM,
October.
E. B. Fowlkes and C. L. Mallows. 1983. A method for com-
paring two hierarchical clusterings. Journal of the American
Statistical Association, 78:553?569.
Benjamin C. M. Fung, Ke Wang, and Martin Ester. 2003. Hi-
erarchical document clustering using frequent itemsets. In
Proc. of the SIAM International Conference on Data Min-
ing.
Julia Hirschberg. 2002. The pragmatics of intonational mean-
ing. In Proc. Speech Prosody, pages 65?68.
L. Hubert and P. Arabie. 1985. Comparing partitions. Journal
of Classification, 2:193?218.
L. Hubert and J. Schultz. 1976. Quadratic assignment as a gen-
eral data analysis strategy. British Journal of Mathematical
and Statistical Psychology, 29:190?241.
Bjornar Larsen and Chinatsu Aone. 1999. Fast and effective
text mining using linear-time document clustering. In KDD
?99: Proceedings of the fifth ACM SIGKDD international
conference on Knowledge discovery and data mining, pages
16?22, New York, NY, USA. ACM Press.
Gina-Anne Levow. 2006. Unsupervised and semi-supervised
learning of tone and pitch accent. In Proceedings of the main
conference on Human Language Technology Conference of
the North American Chapter of the Association of Compu-
tational Linguistics, pages 224?231, Morristown, NJ, USA.
Association for Computational Linguistics.
J. McQueen. 1967. Some methods for classification and analy-
sis of multivariate observations. In Proc. of the Fifty Berke-
ley Symposium on Mathematical Statistics and Probability,
pages 281?297.
Marina Meila and David Heckerman. 2001. An experimen-
tal comparison of model-based clustering methods. Mach.
Learn., 42(1/2):9?29.
Marina Meila. 2007. Comparing clusterings ? an information
based distance. Journal of Multivariate Analysis, 98:873?
895.
G. W. Milligan, S. C. Soon, and L. M. Sokol. 1983. The ef-
fect of cluster size, dimensionality and the number of clustes
on recovery of true cluster structure. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 5:40?47.
Boris G. Mirkin. 1996. Mathematical classification and clus-
tering. Kluwer Academic Press.
Christine Nakatani, Julia Hirschberg, and Barbara Grosz. 1995.
Discourse structure in spoken language: Studies on speech
corpora. In Working Notes of AAAI-95 Spring Symposiom
on Empirical Methods in Discourse Interpretation.
Michael P. Oakes. 1998. Statistics for Corpus Linguistics. Ed-
inburgh University Press.
M. Porter. 1980. An algorithm for suffix stripping. Program,
14(3):130?137.
William M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. Journal of the American Statistical
Association, 66(336):846?850, Dec.
J. Rissanen. 1978. Modeling by shortest data description. Au-
tomatica, 14:465?471.
J. Rissanen. 1989. Stochastic complexity in statistical inquiry.
World Scientific Series in Computer Science, 15.
Sa-Im Shin and Key-Sun Choi. 2004. Automatic word sense
clustering using collocation for sense adaptation. In The Sec-
ond Global Wordnet Conference.
K. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf, C. Wight-
man, P. Price, J. Pierrehumbert, and J. Hirschberg. 1992.
Tobi: A standard for labeling english prosody. In Proc. of
the 1992 International Conference on Spoken Language Pro-
cessing, volume 2, pages 12?16.
S. Strassel and M. Glenn. 2003. Creating
the annotated tdt-4 y2003 evaluation corpus.
http://www.nist.gov/speech/tests/tdt/tdt2003/papers/ldc.ppt.
Stijn van Dongen. 2000. Performance criteria for graph cluster-
ing and markov cluster experiments. Technical report, CWI
(Centre for Mathematics and Computer Science), Amster-
dam, The Netherlands, The Netherlands.
C. J. Van Rijsbergen. 1979. Information Retrieval, 2nd edition.
Dept. of Computer Science, University of Glasgow.
Santosh Vempala and Grant Wang. 2005. The benefit of
spectral projection for document clustering. In Workshop
on Clustering High Dimensional Data and its Applications
Held in conjunction with Fifth SIAM International Confer-
ence on Data Mining (SDM 2005).
D. L. Wallace. 1983. Comment. Journal of the American Sta-
tistical Association, 78:569?576.
Peter Willett. 1988. Recent trends in hierarchic document clus-
tering: a critical review. Inf. Process. Manage., 24(5):577?
597.
419
Oren Zamir and Oren Etzioni. 1998. Web document clustering:
A feasibility demonstration. In Research and Development
in Information Retrieval, pages 46?54.
Yujing Zeng, Jianshan Tang, Javier Garcia-Frias, and Guang R.
Gao. 2002. An adaptive meta-clustering approach: Com-
bining the information from different clustering results. csb,
00:276.
Ying Zhao and George Karypis. 2001. Criterion functions for
ducument clustering: Experiments and analysis. Technical
Report TR 01?40, Department of Computer Science, Uni-
versity of Minnesota.
420
Characterizing and Predicting Corrections
in Spoken Dialogue Systems
Diane Litman?
University of Pittsburgh
Julia Hirschberg?
Columbia University
Marc Swerts?
Tilburg University
This article focuses on the analysis and prediction of corrections, defined as turns where a
user tries to correct a prior error made by a spoken dialogue system. We describe our labeling
procedure of various corrections types and statistical analyses of their features in a corpus
collected from a train information spoken dialogue system. We then present results of machine-
learning experiments designed to identify user corrections of speech recognition errors. We
investigate the predictive power of features automatically computable from the prosody of the
turn, the speech recognition process, experimental conditions, and the dialogue history. Our
best-performing features reduce classification error from baselines of 25.70?28.99% to 15.72%.
1. Introduction
Compared to many other systems, spoken dialogue systems (SDS) tend to have more
difficulties in correctly interpreting user input. Whereas a car normally goes left if the
driver turns the steering wheel in that direction or a vacuum cleaner starts working if
one pushes the on button, interactions between a user and a spoken dialogue system
are often hampered by mismatches between the action intended by the user and the
action executed by the system. Such mismatches are mainly due to errors in the Auto-
matic Speech Recognition (ASR) and/or the Natural Language Understanding (NLU)
component of these systems; they can also be due to wrong default assumptions of the
system or the fact that a user asks out-of-topic questions for which the system was not
designed. To solve these mismatches, users often have to put considerable effort into
trying to make it clear to the system that there was a problem, and trying to correct it by
reentering misrecognized or misinterpreted information. Previous research has already
brought to light that it is not always easy for users to determine whether their intended
actions were carried out correctly or not, in particular when the dialogue system does
not give appropriate feedback about its internal representation at the right moment.
In addition, users? corrections may miss their goal because corrections themselves are
more difficult for the system to recognize and interpret correctly, which may lead to
so-called cyclic (or spiral) errors.
? E-mail: litman@cs.pitt.edu.
? E-mail: julia@cs.columbia.edu.
? E-mail: m.g.j.swerts@uvt.nl.
Submission received: 12 January 2005; revised submission received: 3 April 2006; accepted for publication:
4 May 2006
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 3
Given that current spoken dialogue systems are not sufficiently robust, there is
need for sophisticated error-handling strategies to gracefully solve communication
problems between the system and the user. Ideally, apart from strategies to prevent
errors, error handling would consist of steps to immediately detect an error when
it occurs and to interact with the user to correct the error in subsequent exchanges.
To date, attempts to improve system performance have largely focused on improv-
ing ASR accuracy or simplifying the task, either by further constraining the domain
and functionality of the system or by further restricting the vocabulary the system
must recognize. Such studies include work on improved acoustic and semantic con-
fidence scores (Ammicht, Potamianos, and Fosler-Lussier 2001; Andorno, Laface, and
Gemello 2002; Bouwman, Sturm, and Boves 1999; Falavigna, Gretter, and Riccardi
2002; Guillevic, Gandrabur, and Normandin 2002; Moreno, Logan, and Raj 2001; Wang
and Lin 2002; Zhang and Rudnicky 2001), on new system architectures for error han-
dling (McTear et al 2005; Prodanov and Drygajlo 2005; Torres et al 2005), on new
interfaces that are more user-friendly for error recovery (Bulyko et al 2005; Karsenty
and Botherel 2005; Sturm and Boves 2005), and on the use of error-recovery strategies
that are based on analyses of human?human dialogues (Skantze 2005), including the
use of facial expressions (Barkhuysen, Krahmer, and Swerts 2005).
However, as ASR accuracy improves, dialogue systems will be called upon to
handle ever more complex tasks and ever less restricted vocabularies. So, it seems likely
that spoken dialogue systems will, for the foreseeable future, always require effective
error detection and repair strategies. In previous research (Hirschberg, Litman, and
Swerts 1999, 2004), we identified new procedures to detect recognition errors, which
perform well when tested on two different corpora, the TOOT and W99 corpora (train
information and conference registration dialogues) collected using two different ASR
systems (Sharp et al 1997; Kamm et al 1997). We found that prosodic features, in
combination with information already available to the recognizer, such as acoustic
confidence scores, grammar, and recognized string, can distinguish speaker turns that
are misrecognized far better than traditional methods for ASR rejection (the system
decision that its hypothesis is so weak that it should reprompt for fresh input), which
use acoustic confidence scores alone. Related work has been done by Lendvai (2004)
and Batliner et al (2003). In the current study, we turn to the question of how people try
to correct ASR errors in their interactions with machines and the role that prosody may
play in identifying user corrections and in helping to analyze them.
Understanding how users attempt to correct system failures and why their attempts
succeed or fail is important to improve the design of future spoken dialogue systems.
For example, knowing whether they are more likely to repeat or rephrase their utter-
ances, add new information or shorten their input, and how system behavior influences
these choices can suggest appropriate on-line modifications to the system?s interaction
strategy or to the recognition procedure it employs. Determining which speaker behav-
iors are more successful in correcting system errors can also lead to improvements in the
help information such systems provide. There is growing evidence that there is much
variance in the way people react to system errors and that the variance can be explained
on the basis of particular properties of the dialogue system or the dialogue context. In
particular, dialogue confirmation strategies may hinder users? ability to correct system
error. For instance, if a system wrongly presents information as being correct, as when it
verifies information implicitly, users become confused about how to respond (Krahmer
et al 2001). Other studies have shown that speakers tend to switch to a prosodically
?marked? speaking style after communication errors, comparing repetition corrections
with the speech being repeated (Wade, Shriberg, and Price 1992; Oviatt et al 1996;
418
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
Levow 1998; Bell and Gustafson 1999). Although this speaking style may be effective in
problematic human?human communicative settings, there is evidence that suggests it
leads to further errors in human?machine interactions (Levow 1998; Soltau and Waibel
2000). That corrections are difficult for ASR systems is generally explained by the fact
that they tend to be hyperarticulated?higher, louder, longer?than other turns (Wade,
Shriberg, and Price 1992; Oviatt et al 1996; Levow 1998; Bell and Gustafson 1999;
Shimojima, et al 2001; Soltau and Waibel 1998, 2000; Soltau, Metze, and Waibel 2002),
where ASR models are not well adapted to handle this special speaking style, although
recent studies suggest that ASR systems are becoming less vulnerable to hyperarticula-
tion (Bulyko et al 2005).
So, repair strategies in human?machine interactions can be more or less effective.
Therefore, increased knowledge about the efficiency of different correction strategies
can lead to a number of possible courses of action. System strategy might be chosen to
favor the type(s) of correction the system can most easily process. Or, having chosen a
particular interaction strategy, the system repair strategy might be tuned to handle the
correction types that that strategy is likely to produce. Alternatively, the system?s dia-
logue manager might use the detection of corrections as a signal that it should modify its
interaction strategy, either locally, by beginning a subdialogue for faster error recovery,
or globally, by changing its initiative or confirmation strategies, or even directing the
user to a human operator. Or, since corrections are often hyperarticulated, detection of
a correction could serve as a signal to the ASR engine to run a recognizer trained on
hyperarticulated speech in parallel with its normal processor, to better transcribe the
speech. All of these possibilities, however, assume that user corrections can be detected
by the system reliably during the dialogue.
In this article, we describe an analysis of user corrections of system error collected in
the TOOT spoken dialogue system. In the next section, we describe the corpus itself and
how it was collected and labeled. The corpus is suitable to gain insight into the different
correction strategies that speakers exploit in different dialogue contexts and interaction
styles. Then, we characterize the nature of corrections in this corpus in terms of when
they occur, how well they are handled by the system, what distinguishes their prosody
from other utterances, their relationship to the utterances they correct, and how they
differ according to dialogue strategy. Then we present results of some machine-learning
experiments designed to automatically distinguish corrections from other user input,
using features that we derived as potentially useful from our descriptive analyses.
2. The Data
2.1 The TOOT Corpus
Our corpus consists of dialogues between human subjects and TOOT, a spoken dialogue
system that allows access to train information from the Web via telephone. TOOT
was collected to study variations in dialogue strategy and in user-adapted interac-
tion (Litman and Pan 1999). It is implemented using an interactive voice response
(IVR) platform developed at AT&T, combining ASR and text-to-speech with a phone
interface (Kamm et al 1997). The system?s speech recognizer is a speaker-independent,
hidden Markov model system with context-dependent phone models for telephone
speech and constrained (rule-based) grammars defining vocabulary at any dialogue
state. Whereas a ?universal? grammar specifying all legal utterances was used at some
points in the dialogue, seven smaller grammars were also used at many points in the
dialogue (e.g., to recognize city names, days of the week, answers to yes/no questions,
419
Computational Linguistics Volume 32, Number 3
etc.). Grammars were only written for originally expected answers; in other words,
no specific grammar for corrections was built in.1 Confidence scores for recognition
were available only at the turn level and were based on acoustic likelihoods; thresh-
olds for rejecting an utterance based on confidence scores were specified manually by
the system designers and were set differently for different grammars. The platform
supports barge-in. Subjects performed four tasks with one of several versions of the
system that differed in terms of locus of initiative (system, user, or mixed), confirmation
strategy (explicit, implicit, or none), and whether these conditions could be changed
by the user during the task (adaptive vs. non-adaptive). In the adaptive version of
the system, users were allowed to say change strategy at any point(s) in the dialogue.
TOOT would then ask the user to specify new initiative and confirmation strategies,
for example, You are using the no confirmation strategy. Which confirmation strategy do
you want to change to? No confirmation, implicit confirmation, or explicit confirmation?
TOOT?s initiative strategy specifies who has control of the dialogue, whereas TOOT?s
confirmation strategy specifies how and whether TOOT lets the user know what it just
understood. The fragments in Figure 1 provide some illustrations of how dialogues
vary with strategy. For example, in user initiative mode, the system allows the user
to specify any number of attributes in a single utterance. Thus, the system will let the
user ignore specific questions. In the example in Figure 1, although the system asks for
the day of the week, the user answers with the time, which can be recognized due to the
use of the ?universal grammar.? In contrast, in both mixed and system initiative mode,
when a specific question is asked, one of the restricted grammars is used to recognize
the response. Finally, in universal and mixed but not system initiative mode, the sys-
tem can ask both specific questions and open-ended questions (e.g., How may I help
you?). Subjects were 39 students: 20 native speakers and 19 non-native, 16 women
and 23 men. Dialogues were recorded and system and user behavior were logged
automatically. The concept accuracy (CA) of each turn was manually labeled. If the
ASR correctly captured all task-related information in the turn (e.g., time, departure,
and arrival cities), the turn?s CA score was 1 (semantically correct). Otherwise, the CA
score reflected the percentage of correctly recognized task information in the turn. The
dialogues were also transcribed and automatically scored in comparison to the ASR
recognized string (the best hypothesis output by the ASR engine) to produce a word
error rate (WER) for each turn. For the study described below, we examined 2,328 user
turns (all user input between two system inputs) from 152 dialogues.
2.2 Labeling
To identify corrections in the corpus two authors independently labeled each turn as
to whether or not it constituted a correction of a prior system failure (a rejection or CA
error, which were the only system failure subjects were aware of) and subsequently
decided upon a consensus label. Note that many of the discrepancies between labels
were due to tiredness or incidental sloppiness of individual annotators, rather than true
disagreement. Each turn labeled ?correction? was further classified as belonging to one
of the following categories: REP (repetition, including repetitions with differences in
pronunciation or fluency), PAR (paraphrase), ADD (task-relevant content added), OMIT
1 Thus, if the system prompted for a single city, but the user also included a correction of a prior utterance
(e.g., No, at 10:30 p.m. I want to go to New York City), the turn would be out of grammar. Coding our
corpus for out of vocabulary turns and examining whether corrections are more likely to be out of
grammar is an area for future work.
420
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
Figure 1
Illustrations of various dialogue strategies in TOOT.
(content omitted), and ADD/OMIT (content both added and omitted). Repetitions
were further divided into repetitions with pronunciation variation (PRON) (e.g., yes
correcting yeah) and repetitions where the correction was pronounced using the same
pronunciation as the original turn, but this distinction was difficult to make and turned
out not to be useful. User turns that included both corrections and other speech acts
were so distinguished by labeling them ?2+.? For example, the turn I would like to go
to Chicago from Baltimore change strategies system contains not only an ADD correction,
but also a request to adapt the system?s dialogue strategies, followed by an inform
of the desired initiative value. As another example, the turn yes help contains a REP
correction, followed by a request for help. For user turns containing a correction plus
one or more additional dialogue acts, only the correction is used for purposes of analysis
below. We also labeled as ?restarts? user corrections that followed non-initial system-
initial prompts (e.g., How may I help you? or What city do you want to go to?); in such
cases, system and user essentially started the dialogue over from the beginning.2 Table 1
shows examples of each correction type and additional label for corrections of system
failures on I want to go to Boston on Sunday. Note that the utterance on the last line of this
figure is labeled 2+PAR, given that this turn consists of two speech acts: The goal of the
no-part of this turn is to signal a problem, whereas the remainder of this turn serves to
correct a prior error.
Each correction was also indexed with an identifier representing the closest prior
turn it was correcting, so that we could investigate ?chains? of corrections of a single
failed turn by tracing back through subsequent corrections of that turn. Figure 2 shows
a fragment of a TOOT dialogue with corrections labeled as discussed above.
3. Descriptive Analyses
This section presents the results of some descriptive analyses of the corrections we
labeled in the TOOT corpus. We provide data on the distribution of different correction
2 Restarts occurred when either the user said the phrase I?m done here at any point in the dialogue, or
answered no to the system?s request to perform a database query (e.g., Do you want me to find the trains
from Baltimore to Chicago on Tuesday around 8:45 now?).
421
Computational Linguistics Volume 32, Number 3
Table 1
Example corrections of I want to go to Boston on Sunday.
Corr Type Correction
REP I want to go to Boston on Sunday
PAR To Boston on Sunday
OMIT I want to go to Boston
ADD To Boston on Sunday at 8 p.m.
ADD/OMIT I want to arrive Sunday at 8 p.m.
2+PAR No, to Boston on Sunday
types, prosodic features of corrections, characteristics of correction chains, and variation
in features of corrections as a function of dialogue strategy.
3.1 Correction Types
Of the correction types we labeled, the largest numbers were REPs and OMITs, as
shown in Table 2, which shows overall distribution of correction types, and distri-
butions for each type of system failure corrected, following either a misrecognized
turn (with respect to concept accuracy) (Post-Misrec) or a rejected turn (Post-Rej) or
correcting an earlier system failure (Non-Immed). (The last group includes corrections
of earlier utterances that do not immediately follow a rejection or misrecognition.)
Table 2 shows that 39% of TOOT corrections were simple repetitions of a previously
rejected or misrecognized turn. Although this strategy is often suboptimal in correcting
ASR errors (Levow 1998), REPs (45% WER error) and OMITs (52% error) were better
recognized than ADDs (90% WER error) and PARs (72% WER error).
Figure 2
Toot dialogue fragment with correction labels.
422
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
Table 2
Distribution of correction types.
Type ADD ADD/OMIT OMIT PAR REP N
Total 51 8% 14 2% 215 32% 127 19% 265 39% 672
Post-Misrec 39 7% 13 3% 203 40% 90 18% 173 32% 518
Post-Rej 8 6% 0 0% 9 7% 36 28% 75 59% 128
Non-Immed 4 15% 1 4% 3 12% 1 4% 17 65% 26
There was no significant difference either in the number of corrections produced
(? = 2.44, p = .12) or in correction type (?2 = 5.07, p = .28) between our native speaker
subjects and non-native speakers. However, what the user was correcting did influence
the type of correction chosen. Table 2 shows that corrections of misrecognitions (Post-
Misrec) were more likely to omit information present in the original turn (OMITs),
whereas corrections of rejections (Post-Rej) were more likely to be simple repetitions.
The latter finding is not surprising because the rejection message for tasks was always
a close paraphrase of Sorry, I can?t understand you. Can you please repeat your utterance?
However, it does suggest the surprising power of system directions and how impor-
tant it is to craft prompts to favor the type of correction most easily recognized by
the system.
3.2 Prosodic Features of Corrections
In part to test the hypothesis that corrections tend to be hyperarticulated (slower and
louder speech that contains wider pitch excursions and more internal silence), we
examined the following features for each user turn: maximum and mean fundamental
frequency values (f0 Max, f0 Mean); maximum and mean energy values (RMS Max,
RMS Mean); total duration; length of pause preceding the turn (Prior Pause); speaking
rate (Tempo), calculated in syllables per second (sps); and amount of silence within
the turn (% Silence).3 f0 and RMS values, representing measures of pitch excursion
and loudness, were calculated from the output of Entropic Research Laboratory?s pitch
tracker, get f0 (Talkin 1995), with no postcorrection. Timing variation was represented
by four features: Duration of turn and length of pause between turns was hand labeled.
Speaking rate was approximated in terms of syllables in the recognized string per
second. % Silence was defined as the percentage of zero frames in the turn, calculated
from the pitch track; this feature approximates the percentage of time within the turn
that the speaker was silent.
To ensure that our results were speaker independent, we calculated mean val-
ues for each speaker?s corrections and non-corrections for every feature. Then, for
each feature, we created vectors of speaker means for correction and non-correction
turns and performed paired t tests on the paired vectors. For example, for the feature
?f0 Max,? we calculated mean maxima for correction turns and for non-corrections
for each of our thirty-nine speakers. We then performed a paired t test on these
3 Although the features were automatically computed, turn beginnings and endings were hand segmented
in dialogue-level speech files, as the turn-level files created by TOOT were not available. Because of some
system/user overlap in the recordings, we were able to calculate prosodic features for only 1,975 user
turns.
423
Computational Linguistics Volume 32, Number 3
thirty-nine pairs of means to derive speaker-independent results for differences in
f0 maxima between corrections and non-corrections. Note, however, that there were
overall differences in the corrections produced by native and non-native speakers,
normalized by value of first turn in task: mean f0 was higher for native speakers than
for non-native speakers (t stat = ?2.72, df = 602, p = .0067), tempo was faster (t stat =
?3.18, df = 670, p = .0015), and duration was shorter (t stat = 2.20, df = 670, p = .028).
These differences do not occur in non-correction utterances.
Our results provide some explanation for why corrections are more poorly re-
cognized than non-corrections because they indicate that corrections are indeed
characterized by prosodic features associated with hyperarticulation. Table 3 shows
that corrections differ from other turns in that they are longer, louder, higher in
pitch excursion, follow longer pauses, and contain less internal silence than non-
corrections. All but the latter difference supports the hypothesis that corrections tend
to be hyperarticulated.
To confirm this hypothesis, two of the authors labeled each turn in the corpus for
evidence of perceptual hyperarticulation, following (Wade, Shriberg, and Price 1992).
Fifty-two percent of corrections in the corpus have some perceptual hyperarticulation,
compared with only 12% of other turns. Too, hyperarticulated corrections are more
likely to be misrecognized than other corrections (70% misrecognitions vs. 52%). How-
ever, it is important to note that only 59% of misrecognized corrections in the corpus are
also hyperarticulated, so recognition failure for a considerable portion of corrections
must be explained in some other way. There is still a large number of misrecognized
corrections that show no perceptual evidence of hyperarticulation.
In our earlier analysis of prosodic differences between correct and incorrectly
recognized turns (Hirschberg, Litman, and Swerts 2004), we also found that misrecog-
nized turns differed from correctly recognized turns in f0, loudness, duration, and
timing?all features associated with hyperarticulation. In addition, more misrecogni-
tions are hyperarticulated than are correctly recognized turns. But when we excluded
perceptually hyperarticulated turns from our prosodic analysis, we found that mis-
recognized turns were still prosodically different from correctly recognized turns, in
the same ways. We hypothesized there that misrecognitions might exhibit tendencies
toward hyperarticulation that are imperceptible to human listeners, but not to ASR
engines. The same may also be true of non-hyperarticulated, but still prosodically
distinct corrections. When we exclude hyperarticulated utterances from our corpus
Table 3
Corrections versus non-corrections by prosodic feature.
Feature t stat Mean corr p
- non-corr
f0 Max* 3.79 17.76 Hz < .001
f0 Mean 0.23 ?4.12 Hz .823
RMS Max* 4.88 347.75 < .001
RMS Mean* 2.57 63.44 .014
Duration* 6.68 1.16 sec < .001
Prior pause* 2.17 0.186 sec .036
Tempo 1.78 ?0.15 sps .246
% Silence* 4.75 ?0.05% < .001
*Significant at a 95% confidence level (p ? .05)
424
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
and reanalyze prosodic features of corrections versus non-corrections, we find signif-
icant differences in duration, rms maximum, rms mean, tempo, and amount of turn-
internal silence as we did with the corpus as a whole. So, again, even when corrections
are not perceptibly hyperarticulated, they share some acoustic tendencies with turns
that are.
3.3 Correction Chains
As noted above, corrections in the TOOT corpus often take the form of chains of
corrections of a single original error. Looking back at Figure 2, for example, we see
two chains of corrections: In the first, which begins with the misrecognition of turn 776
(Um, tomorrow), the user repeats the original phrase and then provides a paraphrase
(Saturday), which is correctly recognized. In the second, beginning with turn 780, the
time of departure is misrecognized. The user omits some information (a.m.) in turn
781, but without success; an ADD correction follows, with the previously omitted
information restored, in turn 783.
Distance of a correction from the original misrecognized turn?whether calculated
as position in chain (e.g., Saturday in Figure 2 is the second in the chain correcting
turn 776) or further in number of turns from that original error (e.g., Saturday here is
also two turns from the original error)?correlates significantly with prosodic variation.
An analysis of the relationship between both distance measures and our prosodic
features (using Pearson?s product?moment correlation) shows significant correlations
of distance in chain or from original error with f0 maximum (r = .20, p < .001; r = .21,
p < .001) and mean (r = .27, p < .001; r = .29, p < .001), rms maximum (r =?.09, p < .02;
r = ?.12, p < .005) and mean (r = ?.12, p < .0025; r = ?.16, p < .001), absolute duration
(r = .14, p < 0; r = .16, p < .001) and duration in number of words (r = .11, p < .01; r =
.12, p < .005), length of preceding pause (r = .11, p < .005; r = .10, p < .01), and speaking
rate (r = ?.05, p < .01; r = ?.10, p < .02). The more distant a correction is, in short, the
higher it is in pitch, the softer it is, the longer it is, the greater is its preceding pause, and
the more slowly it is spoken. In addition, more distant corrections are also more likely
to be misrecognized; for distance in turns there is a (negative) significant correlation for
concept accuracy (r = ?.13, p < .001), whereas both word and concept accuracy decline
significantly by position in chain (r = ?.08, p < .05; r = ?.15, p < .001). Table 4 shows
the mean concept accuracy of corrections for chain position through 8 (higher numbers
are very small) in the corpus. So, as speakers must try again and again to correct an
error, their attempts appear to become ever less likely to succeed, perhaps because their
prosodic behavior changes in ways that do not help the recognition process. Curiously,
however, our perceptual measure of hyperarticulation is not significantly correlated
with either of these distance measures. So, although speakers modify their speech in
ways generally consistent with hyperarticulation, their corrections do not necessarily
become more hyperarticulated as their attempts to correct continue. Another curious
Table 4
Mean concept accuracy by correction position in Chain.
Position 1 2 3 4 5 6 7 8
N 311 143 84 49 25 15 10 4
Error .43 .57 .63 .51 .60 .87 .70 1.00
425
Computational Linguistics Volume 32, Number 3
finding is that corrections that are more distant from the turn they immediately correct
(e.g., in Figure 2, turn 783 is more distant from the turn it corrects (781) than turn 781 is
from the turn it corrects, which is 780) tend to be more accurately recognized than turns
that are closer. Yet, prosodically, these turns are very like distant turns in a chain or from
the original error, being higher in f0 maximum and mean, lower in rms maximum and
mean, and longer in seconds and number of words. So, in the one case these prosodic
changes might be thought to lead to recognition error, where in the other they occur
with better recognized corrections.
3.4 Variation by Dialogue Strategy
Dialogue strategy clearly affects the type of correction users make and whether it
is successful or not. For example, users more frequently repeat their misrecognized
utterance in the SystemExplicit (75% of corrections are repetitions) condition than in
the MixedImplicit or UserNoConfirm (both 37% REP); the latter conditions have larger
proportions of OMITs and paraphrases. Perhaps this disparity is partly explained by the
larger proportion of corrections that follow rejections in the SystemExplicit condition
(39% vs. 22% and 19%). Overall, SystemExplicit turns are rejected 6% of the time,
whereas the other conditions have about 10% rejections. Table 5 shows differences in
mean length of tasks, number of corrections, number of misrecognitions, and number
of misrecognized corrections by dialogue strategy. Again, misrecognitions were defined
in terms of concept accuracy (turns with CA < 1); misrecognized corrections refer to
the intersection of user terms that were coded as both corrections and misrecognitions.
The fewer misrecognitions, corrections, and misrecognized corrections per task in the
SystemExplicit condition may well explain user ratings of the various systems (non-
adapt) in the original experiments (Litman and Pan 1999): When asked to say whether
they would be likely to use such a system in the future, on a 1?5 scale, subjects scored
SystemExplicit 3.5, MixedImplicit 2.6, and UserNoConfirm 1.7. User satisfaction scores
were similar: Where 40 is the highest score, users gave SystemExplicit 31.25, Mixed-
Implicit 24, and UserNoConfirm 22.1. So, SystemExplicit is preferred by users, even
though MixedImplicit on average takes fewer turns to accomplish a task, suggesting
that the large number of misrecognitions and consequent need for correction has a large
impact on user preferences. This is consistent with performance functions derived from
evaluations of TOOT (Litman and Pan 1999).
Perhaps because correction chains often end unsuccessfully, users frequently
?restart? a task within a session. Most restarts occurred in the MixedImplicit and
UserNoConfirm conditions and were rarely successful. In non-adaptive tasks, 42% of
corrections in the MixedImplicit condition were restarts and 31% in the UserNoConfirm,
Table 5
Corrections by system strategy.
Means SystemExplicit MixedImplicit UserNoConfirm
per task
# Turns 13.4 11.7 16.2
# Corrs 1.3 4.6 7.1
# Misrecs 2.8 6.4 9.4
# Misrec?d Corrs 0.3 3.2 4.8
426
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
whereas none occurred in the SystemExplicit condition. Restarts were misrecognized
77% of the time, compared to 65% of first turns in task. They thus seem to have been
a worse strategy than initiating a new task and might prove a useful diagnostic for
changing system strategy?or summoning a human operator.
4. Predicting Corrections
The previous section showed that corrections differ significantly from non-corrections
prosodically, being higher in pitch, louder, longer, with longer pauses preceding them
and less internal silence. In addition, they are misrecognized more frequently than
non-corrections?although they are no more likely to be rejected by the system. And
corrections more distant from the error they correct tend to exhibit greater prosodic
differences and are recognized more poorly, suggesting that users are not learning to
modify their own behavior to improve system performance. So, dealing with corrections
is a particularly difficult task for both users and systems. We also found that system dia-
logue strategy?the amount of initiative users are allowed to exercise in controlling the
flow of the dialogue and the type of confirmation strategy the system adopts?affects
users? choice of correction type (e.g., directly repeating vs. paraphrasing misrecognized
information). In the following, we turn to the question of identifying user corrections
automatically, from prosodic features as well as other features that are readily available
to a spoken dialogue system. In Section 4.1, we describe the features we use for our
machine-learning experiments. Section 4.2 presents the results of those experiments.
Section 4.3 presents further experiments using additional classifications and features,
motivated by our descriptive results. In the final section, we summarize our conclusions
and describe future research directions.
4.1 Features
In this section we describe the features used in the machine-learning experiments
and the motivation behind their selection. The entire feature set is presented in
Figure 3 and includes only features that could be automatically available to a dialogue
system.
4.1.1 Prosodic Features. Above we showed that corrections were significantly longer,
louder, higher in pitch excursion, and followed longer pauses than other turns. Thus,
we expected that these features would be useful in identifying corrections automatically.
We examined maximum and mean fundamental frequency values (f0max, f0mn) as
indicators of pitch range; maximum and mean energy values (rmsmax, rmsmn) as in-
dicators of loudness; total duration of the speaker turn (dur); length of pause preceding
the turn (ppau); speaking rate (tempo); and amount of silence within the turn (zeros).
The features were measured as indicated above. Table 6 shows the overall means and
standard deviations for these features over the corpus.
4.1.2 ASR Features. Since corrections in our corpus were misrecognized more frequently
than non-corrections (Swerts, Litman, and Hirschberg 2000), we included a set of ASR
features that were derived from TOOT?s speech recognition component and its outputs:
the grammar used as the ASR language model at each dialogue state (gram), the string
recognized by the ASR system as its best hypothesis (str), and the turn-level acoustic
427
Computational Linguistics Volume 32, Number 3
Figure 3
Feature set for predicting corrections.
Table 6
Means and standard deviations for prosodic features over all turns.
f0max f0mn rmsmax rmsmn dur ppau tempo zeros
(Hz) (Hz) (sec) (sec) (sps) (%)
Mean 227 163 1612 396 1.92 .71 2.48 44
S.D. 77 44 1020 261 2.44 .79 1.37 17
confidence score it produced (conf).4 As subcases of the str feature, we included Boolean
features representing whether or not the recognized string included the strings yes or no
(ynstr), some variant of no, such as nope (nofeat), cancel (canc), or help (help), as these
lexical items often occurred during problem resolution. To estimate durational features,
we approximated the length of the user turn in words (wordsstr) and in syllables (syls)
from the str feature, and we added a Boolean feature identifying whether or not the turn
had been rejected by the system (rejbool).
4.1.3 System Experimental Features. Our descriptive study showed that differences in
dialogue strategy affect the type and success of user corrections. For example, TOOT
users more frequently repeat their misrecognized turns and produce the fewest cor-
rections per task when TOOT has the initiative and explicitly confirms all user input.
So, we hypothesized that system conditions might prove important in our learning
experiments. We thus include features representing the system?s current initiative
and confirmation strategies (inittype, conftype), whether users could adapt the sys-
tem?s dialogue strategies (adapt), and the combined initiative and confirmation setting
(realstrat).
4 Confidence scores ranged from ?0.087662 to ?9.884418.
428
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
4.1.4 Dialogue Position and History Features. We also showed that the further a cor-
rection is from the original error, the less likely it is to be recognized correctly, and the
stronger the correlation with prosodic deviation from the mean values over a speaker?s
turns (e.g., more distant corrections are higher in pitch than closer corrections). As a first
approximation of this distance feature, we included the feature diadist?distance of the
current turn from the beginning of the dialogue.
In addition, previous research (Litman, Walker, and Kearns 1999; Walker et al
2000) has shown that features of the dialogue as a whole and features of more local
context can be helpful in predicting ?problematic? dialogues. So we looked at a set
of features summarizing aspects of the prior dialogue for both the absolute number
of times prior turns exhibited certain characteristics (e.g., contained a key word like
cancel?priorcancnum) and the percentage of the prior dialogue containing one of these
features (e.g., priorcancpct). We also examined means for all our continuous-valued
features over the entire dialogue preceding the turn to be predicted (pmn ), such as
pmnsyls, the mean length of prior turns calculated in number of syllables per turn.
Finally, we examined more local contexts, including all features of the preceding turn
(pre ) and for the turn preceding that (ppre ).
It seemed particularly likely that lexical features of the local context?such as
whether a user had asked for help recently, or tried to cancel out of an exchange, or
replied no to a system query?might prove useful in identifying corrections.5 Also,
whether a prior turn had been rejected was clearly a useful cue to the identification
of the current turn as a correction, since users generally supplied a correction when
explicitly asked.
4.2 Machine-learning Experiments
In this section we investigate whether the features described in Section 4.1 (or inter-
esting subsets of them) can in fact be used to accurately predict whether a turn will
be a correction or not. We describe experiments using the machine-learning program
RIPPER (Cohen 1996) to automatically induce such prediction models. RIPPER takes as
input the classes to be learned, the names and possible values of a set of features, and
training data specifying the class and feature values for each training example. For our
experiments, the features presented in Figure 3 comprise the independent variables for
our learning experiments. The dependent variable to be learned, correction (T) ver-
sus non-correction (F), corresponds to the hand-labeled observations described above.
Given a vector of values for the independent and dependent variables for each speaker
turn, RIPPER outputs a classification model for classifying future examples. The model
is learned using greedy search guided by an information gain metric and is expressed as
an ordered set of if-then rules. When multiple rules are applicable, RIPPER uses the first
rule it finds. When no rules are applicable, RIPPER classifies the turn as a non-correction
(F) by default.
Table 7 shows the performance of the learned classification models for some
of the feature sets we examined; all performance figures are estimated using 25-fold
cross-validation on the 2,328 turns in our corpus. The Features column identifies the
set of features (as defined in Figure 3) used to learn the model. The second column,
DIA, indicates which type of dialogue history features (PreTurn, PrepreTurn, Prior,
5 Recall that these are lexical features from the recognized string, not from the actual user transcript.
429
Computational Linguistics Volume 32, Number 3
Table 7
Estimated error, recall, precision, and F? = 1 for predicting corrections.
class = T class = F
Features DIA Error ? SE Rec. Prec. F? = 1 Rec. Prec. F? = 1
Raw+ASR+SYS+POS PreTurn 15.72 ? 0.80 70.61 74.96 .72 89.95 88.28 .89
Raw+ASR+SYS+POS all 16.16 ? 0.58 69.80 74.65 .72 90.12 87.82 .89
PROS+ASR+SYS+POS all 16.38 ? 0.61 69.01 74.05 .71 89.60 87.61 .88
ASR all 16.41 ? 0.93 69.93 72.39 .70 88.76 87.7 .88
ASR+SYS+POS all 17.01 ? 0.78 73.73 73.38 .73 88.68 89.00 .89
ASR+SYS+POS none 18.60 ? 0.81 56.48 72.79 .63 91.33 83.76 .87
Raw+ASR+SYS+POS none 18.68 ? 0.67 58.45 71.64 .64 90.37 84.17 .87
ASR+PROS none 19.29 ? 0.78 54.54 69.97 .61 90.25 82.90 .86
POS+PROS none 19.59 ? 0.73 52.96 69.70 .60 90.38 82.47 .86
Raw all 19.68 ? 0.78 55.62 70.89 .62 90.64 83.33 .87
PROS all 20.33 ? 0.90 56.45 69.23 .61 89.43 83.42 .86
ASR+POS none 20.40 ? 0.79 52.20 71.99 .60 91.43 82.41 .87
PROS none 20.53 ? 0.81 54.86 71.72 .62 90.78 83.07 .87
conf+rejbool all 21.23 ? 0.93 59.70 65.97 .62 87.05 84.05 .85
ASR+SYS none 23.46 ? 0.72 51.55 63.40 .56 87.53 81.65 .84
ASR none 24.19 ? 0.84 45.93 60.99 .52 87.80 79.90 .84
Raw none 25.35 ? 0.93 42.26 59.46 .48 88.29 78.97 .83
POS none 29.00 ? 1.02 0.00 ? ? 99.94 70.99 .83
SYS none 29.00 ? 1.02 0.00 ? ? 100.00 71.00 .83
Prerejbool baseline error = 25.70; majority baseline error = 28.99
and/or PMean) were also included in the feature set; these features represent the
same types of information (e.g., f0max) that the Features column denotes, but for one
or more previous turns in the dialogue. The third column shows the mean error and
standard error (SE) predicted for the model specified by the first two columns. When
error estimates in different rows differ by plus or minus twice the standard error,
they are significantly different (Cohen 1995). The remaining columns show the mean
recall, precision, and F? = 1 for corrections (focus class = T) and non-corrections (focus
class = F), respectively.6 For comparison purposes, we compare our predictions to two
potential baselines. The Majority baseline predicts that all turns are non-corrections
(the majority class of F), and has a classification error of 28.99%. The Prerejbool base-
line predicts that all turns following rejected turns (prerejbool = T) are corrections?
since after rejections, TOOT asks users to repeat their turn7?and all others are non-
corrections; this baseline gives a classification error of 25.70%.
The first question addressed in our experiments is whether or not corrections can be
predicted significantly better than our baselines. Table 7 shows that in fact they can. Our
best-performing feature set (Raw+ASR+SYS+POS, DIA = PreTurn) cuts the majority
baseline error almost in half, from 28.99% to 15.72%, and predicts significantly better
than the rejection-based baseline as well. This feature set includes raw versions of all our
prosodic features and all of the non-prosodic features, for both the turn being classified
6 Recall is the percentage of actual members of a class that are identified, whereas precision is the percentage
of predicted class members that are in fact members. The definition of F? is
(?2+1)PrecisionRecall
?2Precicison+Recall
; ? = 1
equally weights precision and recall. These values are computed using our own cross-validation
program, while error is computed using RIPPER?s cross-validation option.
7 Although users are asked to repeat their turn, 29% of the turns after rejections are not in fact corrections
(e.g., the user instead asks for help or asks the system to repeat the prompt).
430
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
and the immediately prior turn. Note that even if all of the available features are used
for learning (i.e., the normalized versions of prosodic features and all of the various
history features (PROS+ASR+SYS+POS, DIA = all, error = 16.38%)), performance is
statistically comparable to this model.8 In addition, the recall, precision and F? = 1
values in Table 7 show that corrections are generally predicted with better precision
than recall whereas the reverse holds for non-corrections, and that non-corrections (the
majority class) are easier to accurately predict than corrections.
We next turn to an examination of the contribution of the different types of features
we used for prediction. First, we consider the utility of our non-prosodic features.
Table 7 shows that, using only non-prosodic features (ASR, SYS, POS), corrections can
still be predicted with an accuracy statistically equivalent to our best results. That is,
using all feature types (PROS+ASR+SYS+POS, DIA = all, error = 16.38%) is equivalent
to using only non-prosodic features (ASR+SYS+POS, DIA = all, error = 17.01%). Simi-
larly, restricting our feature set to the ASR-derived subset of our non-prosodic features
(ASR, DIA = all, error = 16.41%) or removing all dialogue history (ASR+SYS+POS,
DIA = none, error = 18.60%) yields results equivalent to our best-performing classifier.
However, when only those ASR features derived from the acoustic confidence score
(i.e., conf, preconf, ppreconf, pmnconf, rejbool, prerejbool, pprerejbool, priorrejbool-
num, priorrejboolpct) are used for prediction, then performance does significantly
degrade (conf+rejbool, DIA = all, error = 21.23%). So, it appears that there are numerous
ways to classify corrections successfully, using various combinations of feature types.
This finding is an important one because it suggests that systems that have access to
restricted kinds of information can still hope to identify user corrections with some
confidence. In particular, simply using information available to current ASR systems,
such as acoustic confidence score, recognized string, grammar, and features derived
from these, produces classification results equivalent to our best-performing classifier.
A caveat here is that some of the features in this ASR feature set (e.g., grammar and
recognized string) are less likely to generalize from task to task.
Turning now to the role of prosodic features in classifying corrections, Table 7 shows
that use of only non-prosodic features (ASR+SYS+POS, DIA = all, error = 17.01%)9
slightly (but not quite significantly) outperforms use of only raw prosodic features (Raw,
DIA = all, error = 19.68%). However, using raw prosodic features alone (error = 19.68%)
is comparable to using only ASR features alone (ASR, DIA = all, error = 16.41%). And
both significantly outperform the majority class and rejection-based baselines. Note
also that prediction from raw prosodic features alone (19.68%) is not improved by the
inclusion of their normalized versions (PROS, DIA = all, error = 20.33%). Thus, ASR-
derived features and prosodic features seem to provide equally successful classifications
of user corrections. Since ASR-derived features, in particular, acoustic confidence score,
are currently used by spoken dialogue systems to determine when to reject a turn,
our results suggest that such features can also be useful for identifying corrections.
Although prosodic features are rarely made use of in spoken dialogue systems, they
would, in fact, seem more likely to generalize across tasks and recognizers than the
ASR features.
Now we turn to the issue of how useful features of the dialogue history are in
classifying corrections. Recall that our best-performing ruleset used only a limited dia-
8 Note that removing features sometimes changes performance, which might indicate a weakness in
RIPPER?s feature selection process.
9 Recall that DIA = all includes only the same type of features as for the current utterance, in this case only
non-prosodic history features.
431
Computational Linguistics Volume 32, Number 3
Figure 4
Best performing ruleset (Raw+ASR+SYS+POS, DIA = PreTurn).
logue history?features from the preceding turn (Raw+ASR+SYS+POS, DIA = PreTurn,
error = 15.72%). While adding features of the turn two turns back (PrepreTurn ) and
of the dialogue as a whole (Prior and PMean ) does not significantly change the
error (Raw+ASR+SYS+POS, DIA = all, error = 16.16%), removing the features of the
immediately previous turn from the dialogue history does in fact cause a significant
increase in error rate (Raw+ASR+SYS+POS, DIA = none, error = 18.68%). However,
as discussed above, when only non-prosodic features are considered (ASR+SYS+POS),
there is no significant difference between DIA = all and DIA = none. So, it seems that
features of the immediate local context can improve our ability to classify corrections
accurately when prosodic features are included, but adding a larger local context win-
dow and a global context does not improve over these results. Contextual features seem
particularly important to performance when only raw prosodic features are considered
(Raw, DIA = all, error = 19.68%). When the raw prosodic features of the dialogue
history are removed, the error rate dramatically increases (Raw, DIA = none, error =
25.35%). However, if the normalized prosodic features (which themselves encode much
of the historical information) are also included, then removing the DIA versions of these
features does not significantly degrade performance (PROS, DIA = all, error = 20.33%
vs. PROS, DIA = none, error = 20.53%). We might explain the larger role that prosodic
context plays in classification by returning to the differences we found between prosodic
features of corrections and non-corrections, described in Section 3. In our descriptive
analyses we found that prosodic features such as pitch, duration, and loudness reliably
distinguish corrections based on relative differences between the two types of turns,
not absolute differences. In prediction also, it seems that some form of normalization by
context improves the performance of prosodic features.
When we examine which class of features performs best in the absence of contextual
information, we see that the prosodic features (PROS, DIA = none, error = 20.53%)
significantly outperform the ASR-derived features (ASR, DIA = none, error = 24.19%),
which in turn significantly outperform either of the remaining feature types (POS and
SYS). Table 7 also shows the cases in which the addition of new sources of knowl-
edge improves prediction performance. For DIA = none, the statistically significant
improvements involve adding the feature diadist (distance of the current turn from
the beginning of the dialogue): For example, ASR+POS (error = 20.4%) outperforms
both ASR (error = 24.19%) and POS (error = 29%), and ASR+SYS+POS (error = 18.6%)
outperforms ASR+SYS (error = 23.46%). Again, these are features that are easily made
available to current spoken dialogue systems.
The classification model learned from the best-performing feature set in Table 7 is
shown in Figure 4. Rules are presented in order of importance in classifying data. The
432
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
first rule RIPPER finds with this feature set specifies that if the duration of the current
turn is ? 3.89046 seconds, and if the acoustic confidence score of the prior turn is
? ?0.645234, and if the percentage of silence in the current turn is ? 53.9474%, then
predict that the turn is a correction; this rule correctly predicts 153 corrections and
incorrectly predicts that 10 non-corrections are corrections. So, this rule applies when
the previous turn has a low confidence score and the current turn exhibits some marked
prosodic features. The fourth rule predicts a correction after a previous rejection, but
only when the rejected turn was relatively short with a low confidence score. The
fifth rule predicts a correction when TOOT uses a particular confirmation strategy for
turns that are relatively long and far from the beginning of the dialogue. The sixth
rule predicts a correction when the previous turn is spoken soon after the prompt,
and contains the problem indicator help. Note that this use of the domain-independent
help is the only reference to a lexical item in this ruleset. This ruleset includes features
from all of the feature subsets in our inventory (PROS, ASR, SYS, POS, DIA). For the
current turn, the feature types that appear in the rules are PROS (dur, zeros), ASR (conf,
gram, syls), SYS (conftype), and POS (diadist). Of the previous turn?s features, only
two feature sets emerge as important: PROS (pref0mn, predur, preppau, pretempo)
and ASR (preconf, prestr, prewordstr, prerejbool). Furthermore, within a feature set
such as PROS, the useful features of the current and previous turns differ somewhat
(e.g., zeros is useful for the current turn, whereas tempo is useful for the prior turn),
suggesting important differences in the prosodic characteristics of corrections versus
the turns they follow.
When we look at a ruleset produced using only features commonly available to
current dialogue systems, such as ASR+SYS+POS (DIA = all), we see that creative use
of these features could in fact support correction classification (Figure 5). For example,
the fourth rule predicts that the current turn is a correction when it is not too short, and
when the pre turn indicates awareness (evidenced by the presence of no) of a problem
in the ppre turn (which was recognized with low confidence). This ruleset uses both
ASR (gram, nofeat, syls) and SYS (conftype) features of the current turn; although only
one rule in fact makes use of SYS features. For the contextual DIA features, only the
ASR features occur in the rule-set: PreTurn (preconf, prestr, prenofeat, prerejbool),
PrepreTurn (ppreconf, ppreynstr), and Prior and PMean (pmnconf, priorynstrpct,
pmnwordsstr, priorrejnum). Comparing this ruleset to the previous one (Figure 4), we
see that where timing features (dur, predur, zeros, pretempo, preppau) appear often
when prosodic features are available, related features such as syls and wordsstr (from
which, e.g., tempo is estimated) may be compensating in this ruleset. And of course the
rejection feature (prerejbool) itself is a function of the confidence score of the prior turn.
Note also that lexical features of the recognized string (nofeat, prenofeat, ppreynstr,
Figure 5
Ruleset for non-prosodic features (ASR+SYS+POS, DIA = all).
433
Computational Linguistics Volume 32, Number 3
prestr, priorynstrpct) emerge as quite useful in this ruleset?especially as contextual
features. So, what the system has recognized in prior turns is a good predictor of
whether the current turn is a correction. Also note that the overall verbosity of the
previous dialogue (pmnwordsstr) appears in two of the rules.
An example of a ruleset learned from only prosodic features (Raw, DIA = all, from
Table 7) is shown in Figure 6. This ruleset is notably terser than those shown in Figures 4
and 5 and includes primarily timing-based features (current turn features dur, zeros,
and tempo; local contextual feature pretempo; and dialogue-level features pmndur and
pmnppau). However, all prosodic feature types but f0 appear at least once in the ruleset,
and features specific to the current turn differ from those relevant to different types of
dialogue history. As with our previous descriptive findings discussed in Section 3, this
ruleset shows that corrections are longer, louder, follow longer pauses, and contain less
internal silence than non-corrections, and that these features can be used successfully to
identify them.
4.3 Other Experiments
The machine-learning experiments described in Section 4.2 were motivated by our
long-term goal to incorporate a correction predictor into future versions of our spoken
dialogue system. As such, the experiments were limited to a binary prediction task
(predicting whether a turn was a correction or a non-correction) and only considered
features readily available to our dialogue system. In this section we present additional
experiments removing some of these restrictions, with the goal of further investigating
some of the descriptive findings discussed in Section 3.
Recall from Section 3.2 that there were some differences in the prosodic features of
corrections produced by native versus non-native speakers when such features were
normalized by the first turn in the dialogue. We thus investigated whether adding a
native speaker feature (currently manually labeled) to the prosodic feature set Norm1
(DIA = all) would improve prediction accuracy. Although the error was reduced from
24.32% to 22.68%, this difference was not statistically significant. Furthermore, when
we added the native speaker feature to both the best-performing ruleset in Table 7
(Raw+ASR+SYS+POS, DIA = PreTurn) and the best-performing prosodic feature set
(Raw, DIA = all), the error rates actually increased; again, however, the differences were
not statistically significant.
Also, in Sections 3.1 and 3.4, we identified differences between different types of cor-
rections, which suggests that our features might be more effectively used to predict each
correction type differently. In other words, what would happen if instead of predicting
whether a turn was a correction (T) or not (F) (the binary classification task investigated
above), we predicted whether a turn was ADD, ADD/OMIT, OMIT, PAR, REP, or F (i.e.,
not a correction). Because, as Table 2 shows, we only have limited amounts of data for
Figure 6
Ruleset for raw prosodic features (Raw, DIA = all).
434
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
several of our classes (e.g., only 2% of our corrections are ADD/OMIT); we performed a
simpler version of this experiment, combining our three lowest frequency classes (ADD,
ADD/OMIT, and PAR) into the single class MISC.
Using the best feature set from Table 7 (Raw+ASR+SYS+POS, DIA = PreTurn),
Table 8 shows our results using 25-fold cross-validation. First, note that our overall
estimated error is now 24.13% ? 0.89%. Although this is a huge increase compared to
the 15.72% error rate of our original binary classifier, it should be noted that considering
correction types separately makes our class distribution quite skewed, with the data
for our three correction classes much smaller than the majority class. Nevertheless,
our classifier yields a slight but significant decrease compared to the majority baseline
error, and a nonsignificant decrease compared to the Prerejbool baseline error (both
baselines remain the same as in Table 7). With respect to precision and recall, although
the absolute numbers for corrections are much lower than in Table 7, we again see
that predicting corrections yields higher precision than recall, whereas predicting non-
corrections yields higher recall than precision. Finally, an examination of the learned
ruleset (which contains four rules for predicting MISC, two rules for predicting OMIT,
and seven rules for predicting REP) does show that features are used differently across
correction types. For example, the feature prestr is only used to predict repetition correc-
tions (in particular, after a turn containing help). Our rules also show some overlap with
our earlier descriptive findings. For example, we noted that corrections of rejections
were more likely to be repetitions, and find the feature prerejbool in two of the rules
for predicting repetitions. These findings suggest that if more data were available,
predicting corrections by type might prove a useful strategy.
5. Conclusions
In this article we have presented results of an analysis of corrections in the TOOT
spoken dialogue corpus. We first introduced the TOOT spoken dialogue corpus and
our labeling scheme to identify different types of corrections. The TOOT corpus is
representative of many current research and commercial dialog systems in focusing on
the travel domain. Also, since data were collected using a variety of dialog strategies
with different types of initiative and confirmation, results obtained with this corpus
are more likely to have general usefulness for builders of other spoken dialogue
systems.
We next presented a statistical description of the corrections we labeled. In general,
it appears that corrections are a serious problem for ASR, being recognized much
more poorly than non-corrections but not being rejected any more frequently. Some
corrections types are more difficult to handle for systems than others, with repetitions
Table 8
Predicting correction types (error ? SE = 24.13 ? 0.89)
Class Recall Precision F? = 1
FALSE 93.30 82.37 .87
REP 33.86 56.00 .41
MISC 36.17 48.36 .38
OMIT 25.47 50.13 .32
435
Computational Linguistics Volume 32, Number 3
and corrections that omit information from the original turn being much better recog-
nized than corrections that add or paraphrase such information. Confirming previous
studies of repetition corrections, we found that corrections in general differ from non-
corrections prosodically: They are higher in f0, softer, longer, follow longer pauses, and
contain less internal silence than non-corrections. Also, corrections more distant from
the error they are correcting are louder, higher in pitch, longer, slower, and follow
longer pauses than closer corrections. Both findings suggest a correlation between
corrections and hyperarticulation; however, most prosodic differences persist even
when perceptually hyperarticulated turns are removed from the sample, and perceptual
hyperarticulation is not significantly correlated with distance from original error. We
hypothesize that recognizers may be more sensitive to hyperarticulatory tendencies
than humans.
The second part of this article discusses results of machine-learning experiments
designed to evaluate how well we can distinguish user corrections from non-corrections
using features automatically available to dialogue systems. Clearly, new techniques
must be developed to interpret such corrections, but such techniques can only be
effective if corrections can be reliably identified as such for special handling. Using a
large set of prosodic, ASR-derived, and system-specific features, both for the current
turn and for contextual windows, and using summary features of the prior dialogue, we
have demonstrated that it is possible to classify user corrections significantly better than
either of two baseline classifiers (15.72% error vs. 25.70?28.99%). More usefully perhaps
for current spoken dialogue systems, we have found that we can derive classifiers
that perform equivalently well using only features currently available to most speech
recognizers, such as acoustic confidence score, recognized string, grammar, and features
easily derived from these data. For example, using only such features, we can classify
user corrections with an estimated success rate of 16.41%. So, it does, in fact, seem quite
feasible for current systems to identify user corrections using data they typically do not
make use of.
Given that our findings show that corrections can be classified well using quite dis-
tinct feature sets, a possible future line of research would be to try classification combi-
nation schemes. For instance, one could envision a form of metalearning or boosting that
combines classifications using different feature sets (e.g., ASR vs. prosodic vs. context),
or that combines the output of different learning algorithms (e.g., Ripper combined
with memory-based learning; see, e.g., Lendvai 2004). Kirchhoff (2001) presents some
results of classifier combination schemes, showing some improvements in detection of
corrections when using cascading, but especially when using boosting.
The next steps, developing techniques to interpret these turns more accurately
and to use correction prediction to drive modifications in dialogue strategy, are both
subjects of our future research. Also, whereas our analyses so far have given us overall
information about the relative contribution of various feature sets for the automatic
classification of corrections, one interesting problem for the future is to get more specific
information about the cues that characterize corrections, especially for the development
of on-line error-correction detection. In this respect, an interesting observation has been
made by Kirchhoff (2001), who reports that correction classification using only features
of the first half of a turn performs equally well as a classification using features of the
turn as a whole; this could be explained by the fact that speakers tend to put character-
istic cue phrases, such as ?no? or ?help,? in the beginning of a turn. Additional research
is needed to find strategies that use the detection of corrections to look back in the
dialogue history to identify the utterance being corrected or even the actual problematic
words in these turns. Finally, it would be worthwhile to investigate speaker-specific
436
Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems
correction strategies in more detail, the possible effect on such strategies of the user?s
experience with a system, and his or her linguistic background.
Acknowledgments
Marc Swerts is also affiliated with the
University of Antwerp. His research is
sponsored by the Netherlands Organisation
for Scientific Research (NWO). This work
was performed when the authors were at
AT&T Labs?Research.
References
Ammicht, E., A. Potamianos, and
E. Fosler-Lussier. 2001. Ambiguity
representation and resolution in spoken
dialogue systems. In Proceedings of
EUROSPEECH-01, pages 2217?2220,
Aalborg.
Andorno, M., P. Laface, and R. Gemello.
2002. Experiments in confidence scoring
for word and sentence verification. In
Proceedings of International Conference on
Spoken Language Processing-02,
pages 1377?1381, Denver.
Barkhuysen, P., E. Krahmer, and M. Swerts.
2005. Problem detection in
human?machine interactions based on
facial expressions of users. Speech
Communication, 45:343?359.
Batliner, A., K. Fischer, R. Huber, J. Spilker,
and E. No?th. 2003. How to find trouble in
communication. Speech Communication,
40:117?143.
Bell, L. and J. Gustafson. 1999. Repetition
and its phonetic realizations: Investigating
a Swedish database of spontaneous
computer-directed speech. In Proceedings
of International Congress of Phonetic
Sciences-99, pages 1221?1224, San
Francisco.
Bouwman, A. G., J. Sturm, and L. Boves.
1999. Incorporating confidence
measures in the Dutch train timetable
information system developed
in the ARISE project. In Proceedings
International Conference on Acoustics,
Speech and Signal Processing, volume 1,
pages 493?496, Phoenix.
Bulyko, I., K. Kirchhoff, M. Ostendorf, and
J. Goldberg. 2005. Error-correction
detection and response generation in a
spoken dialogue system. Speech
Communication, 45:271?288.
Cohen, P. 1995. Empirical Methods for Artificial
Intelligence. MIT Press, Boston.
Cohen, W. 1996. Learning trees and rules
with set-valued features. In 14th Conference
of the American Association of Artificial
Intelligence, AAAI, pages 709?716,
Portland.
Falavigna, D., R. Gretter, and G. Riccardi.
2002. Acoustic and word lattice based
algorithms for confidence scores. In
Proceedings of International Conference
on Spoken Language Processing-02,
pages 1621?1624, Denver.
Guillevic, D., S. Gandrabur, and
Y. Normandin. 2002. Robust semantic
confidence scoring. In Proceedings of
International Conference on Spoken Language
Processing-02, pages 853?856, Denver.
Hirschberg, J., D. Litman, and M. Swerts.
1999. Prosodic cues to recognition errors.
In Proceedings of the Automatic Speech
Recognition and Understanding Workshop
(ASRU?99), pages 349?352, Keystone.
Hirschberg, J., D. Litman, and M. Swerts.
2004. Prosodic and other cues to speech
recognition failures. Speech Communication,
43:155?175.
Kamm, C., S. Narayanan, D. Dutton, and
R. Ritenour. 1997. Evaluating spoken
dialog systems for telecommunication
services. In Proceedings of
EUROSPEECH-97, pages 2203?2206,
Rhodes.
Karsenty, L. and V. Botherel. 2005.
Transparency strategies to help users
handle system errors. Speech
Communication, 45:305?324.
Kirchhoff, Katrin. 2001. A comparison
of classification techniques for the
automatic detection of error corrections
in human?computer dialogues. In
Proceedings of the NAACL Workshop
on Adaptation in Dialogue Systems,
pages 33?40, Pittsburgh, PA.
Krahmer, E., M. Swerts, M. Theune, and
M. Weegels. 2001. Error detection in
spoken human?machine interaction.
International Journal of Speech Technology,
4(1):19?30.
Levow, Gina-Anne. 1998. Characterizing
and recognizing spoken corrections in
human?computer dialogue. In Proceedings
of the 36th Annual Meeting of the Association
of Computational Linguistics, COLING/ACL
98, pages 736?742, Montreal.
Lendvai, Piroska. 2004. Extracting
information from spoken user input. A
machine-learning approach. Unpublished
Ph.D. dissertation, Tilburg University.
Litman, D. and S. Pan. 1999. Empirically
evaluating an adaptable spoken
437
Computational Linguistics Volume 32, Number 3
dialogue system. In Proceedings of the
7th International Conference on User
Modeling (UM), pages 55?64, Banff.
Litman, D., M. Walker, and M. Kearns. 1999.
Automatic detection of poor speech
recognition at the dialogue level. In
Proceedings of the 37th Annual Meeting of the
Association of Computational Linguistics,
ACL99, pages 309?316, College Park.
McTear, M., I. A. O?Neill, P. Hanna, and
X. Liu. 2005. Handling errors and
determining confirmation strategies?an
object-based approach. Speech
Communication, 45:249?269.
Moreno, P. J., B. Logan, and B. Raj. 2001. A
boosting approach for confidence scoring.
In Proceedings of EUROSPEECH-01,
pages 2109?2112, Aalborg.
Oviatt, S. L., G. Levow, M. MacEarchern, and
K. Kuhn. 1996. Modeling hyperarticulate
speech during human-computer error
resolution. In Proceedings of International
Conference on Spoken Language
Processing-96, pages 801?804, Philadelphia.
Prodanov, P. and A. Drygajlo. 2005. Bayesian
networks based multimodality fusion for
error handling in human?robot dialogues
under noisy conditions. Speech
Communication, 45:231?248.
Sharp, R. D., E. Bocchieri, C. Castillo,
S. Parthasarathy, C. Rath, M. Riley,
and J. Rowland. 1997. The Watson
speech recognition engine. In
Proceedings International Conference on
Acoustics, Speech and Signal Processing97,
pages 4065?4068, Munich.
Shimojima, A., Y. Katagiri, H. Koiso, and
M. Swerts. 2001. An experimental study on
the informational and grounding functions
of prosodic features of Japanese echoic
responses. Speech Communication,
43:155?175.
Skantze, G. 2005. Exploring human error
recovery strategies: Implications for
spoken dialogue systems. Speech
Communication, 45:325?341.
Soltau, Hagen and Alex Waibel. 1998. On the
influence of hyperarticulated speech on
recognition performance. In Proceedings of
International Conference on Spoken Language
Processing-98, pages 225?228, Sydney.
Soltau, Hagen and Alex Waibel. 2000.
Specialized acoustic models for
hyperarticulated speech. In Proceedings
of International Conference on Acoustics,
Speech and Signal Processing 2000,
pages 1779?1782, Istanbul.
Soltau, H., H. Metze, and A. Waibel. 2002.
Compensating for hyperarticulation by
modeling articulatory properties. In
Proceedings of International Conference
on Spoken Language Processing-02,
pages 83?86, Denver.
Sturm, J. and L. Boves. 2005. Effective error
recovery strategies for multimodal
form-filling applications. Speech
Communication, 45:289?303.
Swerts, M., D. Litman, and J. Hirschberg.
2000. Corrections in spoken dialogue
systems. In Proceedings of International
Conference on Spoken Language
Processing-00, pages 615?618, Beijing.
Talkin, D. 1995. A Robust algorithm for
pitch tracking (RAPT). In W. B. Klein
and K. K. Paliwal, editors, Speech
Coding and Synthesis. Elsevier Science,
Athens, pages 495?518.
Torres, F., L. Hurtado, F. Garc??a, E. Sanchis,
and E. Segarra. 2005. Error handling in a
stochastic dialog system through
confidence measures. Speech
Communication, 45:211?229.
Wade, E., E. E. Shriberg, and P. J. Price.
1992. User behaviors affecting
speech recognition. In Proceedings of
International Conference on Spoken
Language Processing-92, volume 2,
pages 995?998, Banff.
Walker, M., I. Langkilde, J. Wright,
A. Gorin, and D. Litman. 2000.
Learning to predict problematic
situations in a spoken dialogue
system: Experiments with How may
I help you? In Proceedings of NAACL-00,
pages 210?217, Seattle.
Wang, H.-M. and Y.-C. Lin. 2002. Error-
tolerant spoken language understanding
with confidence measuring. In Proceedings
of International Conference on Spoken
Language Processing-02, pages 1625?1628,
Denver.
Zhang, R. and A. Rudnicky. 2001. Word
level confidence annotation using
combinations of features. In Proceedings
of EUROSPEECH-01, pages 2105?2108,
Aalborg.
438
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 89?92,
New York, June 2006. c?2006 Association for Computational Linguistics
Summarizing Speech Without Text Using Hidden Markov Models
Sameer Maskey, Julia Hirschberg
Dept. of Computer Science
Columbia University
New York, NY
{smaskey, julia}@cs.columbia.edu
Abstract
We present a method for summarizing
speech documents without using any type
of transcript/text in a Hidden Markov
Model framework. The hidden variables
or states in the model represent whether
a sentence is to be included in a sum-
mary or not, and the acoustic/prosodic fea-
tures are the observation vectors. The
model predicts the optimal sequence of
segments that best summarize the docu-
ment. We evaluate our method by compar-
ing the predicted summary with one gen-
erated by a human summarizer. Our re-
sults indicate that we can generate ?good?
summaries even when using only acous-
tic/prosodic information, which points to-
ward the possibility of text-independent
summarization for spoken documents.
1 Introduction
The goal of single document text or speech sum-
marization is to identify information from a text
or spoken document that summarizes, or conveys
the essence of a document. EXTRACTIVE SUM-
MARIZATION identifies portions of the original doc-
ument and concatenates these segments to form a
summary. How these segments are selected is thus
critical to the summarization adequacy.
Many classifier-based methods have been exam-
ined for extractive summarization of text and of
speech (Maskey and Hirschberg, 2005; Christensen
et. al., 2004; Kupiec et. al., 1995). These ap-
proaches attempt to classify segments as to whether
they should or should not be included in a summary.
However, the classifiers used in these methods im-
plicitly assume that the posterior probability for the
inclusion of a sentence in the summary is only de-
pendent on the observations for that sentence, and
is not affected by previous decisions. Some of these
(Kupiec et. al., 1995; Maskey and Hirschberg, 2005)
also assume that the features themselves are inde-
pendent. Such an independence assumption simpli-
fies the training procedure of the models, but it does
not appear to model the factors human beings appear
to use in generating summaries. In particular, human
summarizers seem to take previous decisions into
account when deciding if a sentence in the source
document should be in the document?s summary.
In this paper, we examine a Hidden Markov
Model (HMM) approach to the selection of seg-
ments to be included in a summary that we believe
better models the interaction between extracted seg-
ments and their features, for the domain of Broad-
cast News (BN). In Section 2 we describe related
work on the use of HMMs in summarization. We
present our own approach in Section 3 and discuss
our results in Section 3.1. We conclude in Section 5
and discuss future research.
2 Related Work
Most speech summarization systems (Christensen
et. al., 2004; Hori et. al., 2002; Zechner, 2001) use
lexical features derived from human or Automatic
Speech Recognition (ASR) transcripts as features to
select words or sentences to be included in a sum-
mary. However, human transcripts are not gener-
ally available for spoken documents, and ASR tran-
scripts are errorful. So, lexical features have prac-
tical limits as a means of choosing important seg-
ments for summarization. Other research efforts
have focussed on text-independent approaches to ex-
tractive summarization (Ohtake et. al., 2003), which
rely upon acoustic/prosodic cues. However, none
of these efforts allow for the context-dependence of
extractive summarization, such that the inclusion of
89
one word or sentence in a summary depends upon
prior selection decisions. While HMMs are used in
many language processing tasks, they have not been
employed frequently in summarization. A signifi-
cant exception is the work of Conroy and O?Leary
(2001), which employs an HMM model with pivoted
QR decomposition for text summarization. How-
ever, the structure of their model is constrained by
identifying a fixed number of ?lead? sentences to be
extracted for a summary. In the work we present
below, we introduce a new HMM approach to ex-
tractive summarization which addresses some of the
deficiencies of work done to date.
3 Using Continuous HMM for Speech
Summarization
We define our HMM by the following parameters:
? = 1..N : The state space, representing a set of
states where N is the total number of states in the
model; O = o1k, o2k, o3k, ...oMk : The set of obser-
vation vectors, where each vector is of size k; A =
{aij} : The transition probability matrix, where aij
is the probability of transition from state i to state j;
bj(ojk) : The observation probability density func-
tion, estimated by ?Mk=1cjkN(ojk, ?jk,?jk), where
ojk denotes the feature vector; N(ojk, ?jk,?jk) de-
notes a single Gaussian density function with mean
of ?jk and covariance matrix ?jk for the state j,
with M the number of mixture components and cjk
the weight of the kth mixture component; ? = pii :
The initial state probability distribution. For conve-
nience, we define the parameters for our HMM by
a set ? that represents A, B and ?. We can use the
parameter set ? to evaluate P (O|?), i.e. to measure
the maximum likelihood performance of the output
observables O. In order to evaluate P (O|?), how-
ever, we first need to compute the probabilities in
the matrices in the parameter set ?
The Markov assumption that state durations have
a geometric distribution defined by the probability
of self transitions makes it difficult to model dura-
tions in an HMM. If we introduce an explicit du-
ration probability to replace self transition proba-
bilities, the Markov assumption no longer holds.
Yet, HMMs have been extended by defining state
duration distributions called Hidden Semi-Markov
Model (HSMM) that has been succesfully used
(Tweed et. al., 2005). Similar to (Tweed et. al.,
1
2
3
4
L-1
L
Figure 1: L state position-sensitive HMM
2005)?s use of HSMMs, we want to model the po-
sition of a sentence in the source document explic-
itly. But instead of building an HSMM, we model
this positional information by building our position-
sensitive HMM in the following way:
We first discretize the position feature into L num-
ber of bins, where the number of sentences in each
bin is proportional to the length of the document.
We build 2 states for each bin where the second
state models the probability of the sentence being
included in the document?s summary and the other
models the exclusion probability. Hence, for L bins
we have 2L states. For any bin lth where 2l and
2l ? 1 are the corresponding states, we remove all
transitions from these states to other states except
2(l+1) and 2(l+1)?1. This converts our ergodic L
state HMM to an almost Left-to-Right HMM though
l states can go back to l ? 1. This models sentence
position in that decisions at the lth state can be ar-
rived at only after decisions at the (l ? 1)th state
have been made. For example, if we discretize sen-
tence position in document into 10 bins, such that
10% of sentences in the document fall into each bin,
then states 13 and 14, corresponding to the seventh
bin (.i.e. all positions between 0.6 to 0.7 of the text)
can be reached only from states 11, 12, 13 and 14.
The topology of our HMM is shown in Figure 1.
3.1 Features and Training
We trained and tested our model on a portion of
the TDT-2 corpus previously used in (Maskey and
Hirschberg, 2005). This subset includes 216 stories
from 20 CNN shows, comprising 10 hours of audio
data and corresponding manual transcript. An an-
notator generated a summary for each story by ex-
tracting sentences. While we thus rely upon human-
90
identified sentence boundaries, automatic sentence
detection procedures have been found to perform
with reasonable accuracy compared to human per-
formance (Shriberg et. al., 2000).
For these experiments, we extracted only acous-
tic/prosodic features from the corpus. The intu-
ition behind using acoustic/prosodic features for
speech summarization is based on research in speech
prosody (Hirschberg, 2002) that humans use acous-
tic/prosodic variation ? expanded pitch range,
greater intensity, and timing variation ? to indi-
cate the importance of particular segments of their
speech. In BN, we note that a change in pitch, am-
plitude or speaking rate may signal differences in
the relative importance of the speech segments pro-
duced by anchors and reporters ? the professional
speakers in our corpus. There is also considerable
evidence that topic shift is marked by changes in
pitch, intensity, speaking rate and duration of pause
(Shriberg et. al., 2000), and new topics or stories
in BN are often introduced with content-laden sen-
tences which, in turn, often are included in story
summaries.
Our acoustic feature-set consists of 12 features,
similar to those used in (Inoue et. al., 2004; Chris-
tensen et. al., 2004; Maskey and Hirschberg, 2005).
It includes speaking rate (the ratio of voiced/total
frames); F0 minimum, maximum, and mean; F0
range and slope; minimum, maximum, and mean
RMS energy (minDB, maxDB, meanDB); RMS
slope (slopeDB); sentence duration (timeLen =
endtime - starttime). We extract these features by
automatically aligning the annotated manual tran-
scripts with the audio source. We then employ Praat
(Boersma, 2001) to extract the features from the
audio and produce normalized and raw versions of
each. Normalized features were produced by divid-
ing each feature by the average of the feature values
for each speaker, where speaker identify was deter-
mined from the Dragon speaker segmentation of the
TDT-2 corpus. In general, the normalized acoustic
features performed better than the raw values.
We used 197 stories from this labeled corpus to
train our HMM. We computed the transition proba-
bilities for the matrix ANXN by computing the rel-
ative frequency of the transitions made from each
state to the other valid states. We had to compute
four transition probabilities for each state, i.e. aij
where j = i, i + 1, i + 2, i + 3 if i is odd and
j = i ? 1, i, i + 1, i + 2 if i is even. Odd states
signify that the sentence should not be included in
the summary, while even states signify sentence in-
clusion. Observation probabilities were estimated
using a mixture of Gaussians where the number of
mixtures was 12. We computed a 12X1 matrix for
the mean ? and 12X12 matrices for the covariance
matrix ? for each state. We then computed the max-
imum likelihood estimates and found the optimal
sequence of states to predict the selection of docu-
ment summaries using the Viterbi algorithm. This
approach maximizes the probability of inclusion of
sentences at each stage incrementally.
4 Results and Evaluation
We tested our resulting model on a held-out test set
of 19 stories. For each sentence in the test set we ex-
tracted the 12 acoustic/prosodic features. We built a
12XN matrix using these features for N sentences
in the story where N was the total length of the
story. We then computed the optimal sequence of
sentences to include in the summary by decoding
our sentence state lattice using the Viterbi algorithm.
For all the even states in this sequence we extracted
the corresponding segments and concatenated them
to produce the summary.
Evaluating summarizers is a difficult problem,
since there is great disagreement between humans
over what should be included in a summary. Speech
summaries are even harder to evaluate because most
objective evaluation metrics are based on word over-
lap. The metric we will use here is the standard
information retrieval measure of Precision, Recall
and F-measure on sentences. This is a strict met-
ric, since it requires exact matching with sentences
in the human summary; we are penalized if we iden-
tify sentences similar in meaning but not identical to
the gold standard.
We first computed the F-measure of a baseline
system which randomly extracts sentences for the
summary; this method produces an F-measure of
0.24. To determine whether the positional informa-
tion captured in our position-sensitive HMM model
was useful, we first built a 2-state HMM that models
only inclusion/exclusion of sentences from a sum-
mary, without modeling sentence position in the
document. We trained this HMM on the train-
91
ing corpus described above. We then trained a
position-sensitive HMM by first discretizing posi-
tion into 4 bins, such that each bin includes one-
quarter of the sentences in the story. We built an
8-state HMM that captures this positional informa-
tion. We tested both on our held-out test set. Re-
sults are shown in Table 1. Note that recall for
the 8-state position-sensitive HMM is 16% better
than recall for the 2-state HMM, although precision
for the 2-state model is slightly (1%) better than
for the 8-state model. The F-measure for the 8-
state position-sensitive model represents a slight im-
provement over the 2-state model, of 1%. These re-
sults are encouraging, since, in skewed datasets like
documents with their summaries, only a few sen-
tences from a document are usually included in the
summary; thus, recall is generally more important
than precision in extractive summarization. And,
compared to the baseline, the position-sensitive 8-
state HMM obtains an F-measure of 0.41, which is
17% higher than the baseline.
ModelType Precision Recall F-Meas
HMM-8state 0.26 0.95 0.41
HMM-2state 0.27 0.79 0.40
Baseline 0.23 0.24 0.24
Table 1: Speech Summarization Results
5 Conclusion
We have shown a novel way of using continuous
HMMs for summarizing speech documents without
using any lexical information. Our model generates
an optimal summary by decoding the state lattice,
where states represent whether a sentence should
be included in the summary or not. This model is
able to take the context and the previous decisions
into account generating better summaries. Our re-
sults also show that speech can be summarized fairly
well using acoustic/prosodic features alone, without
lexical features, suggesting that the effect of ASR
transcription errors on summarization may be mini-
mized by techniques such as ours.
6 Acknowledgement
We would like to thank Yang Liu, Michel Galley and
Fadi Biadsy for helpful comments. This work was
funded in part by the DARPA GALE program under
a subcontract to SRI International.
References
Boersma P. Praat, a system for doing phonetics by com-
puter Glot International 5:9/10, 341-345. 2001.
Christensen H., Kolluru B., Gotoh Y., Renals S. From
text summarisation to style-specific summarisation for
broadcast news Proc. ECIR-2004, 2004
Conroy J. and Leary D.O Text Summarization via Hidden
Markov Models and Pivoted QR Matrix Decomposi-
tion Technical report, University of Maryland, March
2001
Hirschberg J Communication and Prosody: Functional
Aspects of Prosody Speech Communication, Vol 36,
pp 31-43, 2002.
Hori C., Furui S., Malkin R., Yu H., Waibel A.. Au-
tomatic Speech Summarization Applied to English
Broadcast News Speech Proc. of ICASSP 2002, pp.
9-12 .
Inoue A., Mikami T., Yamashita Y. Improvement of
Speech Summarization Using Prosodic Information
Proc. of Speech Prosody 2004, Japan
Kupiec J., Pedersen J.O., Chen F. A Trainable Document
Summarizer Proc. of SIGIR 1995
Language Data Consortium ?TDT-2 Corpus Univ. of
Pennsylvania.
Maskey S. and Hirschberg J. 2005. Comparing Lexical,
Acoustic/Prosodic, Structural and Discourse Features
Proc. of ICSLP, Lisbon, Portugal.
Ohtake K., Yamamoto K., Toma y., Sado S., Ma-
suyama S. Newscast Speech Summarization via Sen-
tence Shortening Based on Prosodic Features Proc. of
SSPR pp.167-170. 2003
Shriberg E., Stolcke A., Hakkani-Tur D., Tur G. Prosody
Based Automatic Segmentation of Speech into Sen-
tences and Topics? Speech Communication 32(1-2)
September 2000
Tweed D., Fisher R., Bins J., List T, Efficient Hidden
Semi-Markov Model Inference for Structured Video
Sequences Proc. of (VS-PETS), pp 247-254, Beijing,
Oct 2005.
Witbrock M.J. and Mittal V.O. Ultra-Summarization: A
Statistical Approach to Generating Highly Condensed
Non-Extractive Summaries Proc. of SIGIR 1999
Zechner K. Automatic Generation of Concise Sum-
maries of Spoken Dialogues in Unrestricted Domains
Research and Development in Information Retrieval,
199-207, 2001.
92
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 125?128,
New York, June 2006. c?2006 Association for Computational Linguistics
Story Segmentation of Brodcast News in English, Mandarin and Arabic
Andrew Rosenberg
Computer Science Department
Columbia University
New York City, N.Y. 10027
amaxwell@cs.columbia.edu
Julia Hirschberg
Computer Science Department
Columbia University
New York City, N.Y. 10027
julia@cs.columbia.edu
Abstract
In this paper, we present results from a
Broadcast News story segmentation sys-
tem developed for the SRI NIGHTIN-
GALE system operating on English, Ara-
bic and Mandarin news shows to provide
input to subsequent question-answering
processes. Using a rule-induction algo-
rithm with automatically extracted acous-
tic and lexical features, we report success
rates that are competitive with state-of-
the-art systems on each input language.
We further demonstrate that features use-
ful for English and Mandarin are not dis-
criminative for Arabic.
1 Introduction
Broadcast News (BN) shows typically include
multiple unrelated stories, interspersed with anchor
presentations of headlines and commercials. Tran-
sitions between each story are frequently marked
by changes in speaking style, speaker participation,
and lexical choice. Despite receiving a consider-
able amount of attention through the Spoken Doc-
ument Retrieval (SDR), Topic Detection and Track-
ing (TDT), and Text Retrieval Conference: Video
(TRECVID) research programs, automatic detec-
tion of story boundaries remains an elusive prob-
lem. State-of-the-art story segmentation error rates
on English and Mandarin BN remain fairly high and
Arabic is largely unstudied. The NIGHTINGALE
system searches a diverse news corpus to return an-
swers to user queries. For audio sources, the iden-
tification of story boundaries is crucial, to segment
material to be searched and to provide interpretable
results to the user.
2 Related work
Previous approaches to story segmentation have
largely focused lexical features, such as word sim-
ilarily (Kozima, 1993), cue phrases (Passonneau
and Litman, 1997), cosine similarity of lexical win-
dows (Hearst, 1997; Galley et al, 2003), and adap-
tive language modeling (Beeferman et al, 1999).
Segmentation of stories in BN have included some
acoustic features (Shriberg et al, 2000; Tu?r et al,
2001). Work on non-English BN, generally use
this combination of lexical and acoustic measures,
such as (Wayne, 2000; Levow, 2004) on Mandarin.
And (Palmer et al, 2004) report results from feature
selection experiments that include Arabic sources,
though they do not report on accuracy. TRECVID
has also identified visual cues to story segmentation
of video BN (cf. (Hsu et al, 2004; Hsieh et al, 2003;
Chaisorn et al, 2003; Maybury, 1998)).
3 The NIGHTINGALE Corpus
The training data used for NIGHTINGALE in-
cludes the TDT-4 and TDT5 corpora (Strassel and
Glenn, 2003; Strassel et al, 2004). TDT-4 in-
cludes newswire text and broadcast news audio
in English, Arabic and Mandarin; TDT-5 contains
only text data, and is therefore not used by our
system. The TDT-4 audio corpus includes 312.5
hours of English Broadcast News from 450 shows,
88.5 hours of Arabic news from 109 shows, and
134 hours of Mandarin broadcasts from 205 shows.
This material was drawn from six English news
shows ? ABC ?World News Tonight?, CNN ?Head-
line News?, NBC ?Nightly News?, Public Radio
International ?The World?, MS-NBC ?News with
Brian Williams?, and Voice of America, English
three Mandarin newscasts ? China National Ra-
dio, China Television System, and Voice of Amer-
ica, Mandarin Chinese ? and two Arabic newscasts
? Nile TV and Voice of America, Modern Standard
Arabic. All of these shows aired between Oct. 1,
2000 and Jan. 31, 2001.
4 Our Features and Approach
Our story segmentation system procedure is es-
sentially one of binary classification, trained on a
variety of acoustic and lexical cues to the presence
or absence of story boundaries in BN. Our classi-
fier was trained using the JRip machine learning al-
125
gorithm, a Java implementation of the RIPPER al-
gorithm of (Cohen, 1995).1 All of the cues we
use are automatically extracted. We use as input
to our classifier three types of automatic annotation
produced by other components of the NIGHTIN-
GALE system, speech recognition (ASR) transcrip-
tion, speaker diarization, sentence segmentation.
Currently, we assume that story boundaries occur
only at these hypothesized sentence boundaries. For
our English corpus, this assumption is true for only
47% of story boundaries; the average reference story
boundary is 9.88 words from an automatically rec-
ognized sentence boundary2 . This errorful input im-
mediately limits our overall performance.
For each such hypothesized sentence boundary,
we extract a set of features based on the previous
and following hypothesized sentences. The classi-
fier then outputs a prediction of whether or not this
sentence boundary coincides with a story boundary.
The features we use for story boundary prediction
are divided into three types: lexical, acoustic and
speaker-dependent.
The value of even errorful lexical information in
identifying story boundaries has been confirmed for
many previous story segmentation systems (Beefer-
man et al, 1999; Stokes, 2003)). We include some
previously-tested types of lexical features in our own
system, as well as identifying our own ?cue-word?
features from our training corpus. Our lexical fea-
tures are extracted from ASR transcripts produced
by the NIGHTINGALE system. They include lexi-
cal similarity scores calculated from the TextTiling
algorithm.(Hearst, 1997), which determines the lex-
ical similarity of blocks of text by analyzing the co-
sine similarity of a sequence of sentences; this al-
gorithm tests the likelihood of a topic boundary be-
tween blocks, preferring locations between blocks
which have minimal lexical similarity. For En-
glish, we stem the input before calculating these fea-
tures, using an implementation of the Porter stem-
mer (Porter, 1980); we have not yet attempted to
identify root forms for Mandarin or Arabic. We also
calculate scores from (Galley et al, 2003)?s LCseg
1JRip is implemented in the Weka (Witten et al, 1999) ma-
chine learning environment.
2For Mandarin and Arabic respectively, true for 69% and
62% with the average distance between sentence and story
boundary of 1.97 and 2.91 words.
method, a TextTiling-like approach which weights
the cosine-similarity of a text window by an addi-
tional measure of its component LEXICAL CHAINS,
repetitions of stemmed content words. We also iden-
tify ?cue-words? from our training data that we find
to be significantly more likely (determined by ?2) to
occur at story boundaries within a window preceed-
ing or following a story boundary. We include as
features the number of such words observed within
3, 5, 7 and 10 word windows before and after the
candidate sentence boundary. For English, we in-
clude the number of pronouns contained in the sen-
tence, on the assumption that speakers would use
more pronouns at the end of stories than at the be-
ginning. We have not yet obtained reliable part-of-
speech tagging for Arabic or Mandarin. Finally, for
all three languages, we include features that repre-
sent the sentence length in words, and the relative
sentence position in the broadcast.
Acoustic/prosodic information has been shown to
be indicative of topic boundaries in both sponta-
neous dialogs and more structured speech, such as,
broadcast news (cf. (Hirschberg and Nakatani, 1998;
Shriberg et al, 2000; Levow, 2004)). The acous-
tic features we extract include, for the current sen-
tence, the minimum, maximum, mean, and standard
deviation of F0 and intensity, and the median and
mean absolute slope of F0 calculated over the en-
tire sentence. Additionally, we compute the first-
order difference from the previous sentence of each
of these. As a approximation of each sentence?s
speaking rate, we include the ratio of voiced 10ms
frames to the total number of frames in the sentence.
These acoustic values were extracted from the audio
input using Praat speech analysis software(Boersma,
2001). Also, using the phone alignment information
derived from the ASR process, we calculate speak-
ing rate in terms of the number of vowels per second
as an additional feature. Under the hypothesis that
topic-ending sentences may exhibit some additional
phrase-final lenghthening, we compare the length of
the sentence-final vowel and of the sentence-final
rhyme to average durations for that vowel and rhyme
for the speaker, where speaker identify is available
from the NIGHTINGALE diarization component;
otherwise we use unnormalized values.
We also use speaker identification information
from the diarization component to extract some fea-
126
tures indicative of a speaker?s participation in the
broadcast as a whole. We hypothesize that partici-
pants in a broadcast may have different roles, such
as an anchor providing transitions between stories
and reporters beginning new stories (Barzilay et al,
2000) and thus that speaker identity may serve as
a story boundary indicator. To capture such infor-
mation, we include binary features answering the
questions: ?Is the speaker preceeding this boundary
the first speaker in the show??, ?Is this the first time
the speaker has spoken in this broadcast??, ?The last
time??, and ?Does a speaker boundary occur at this
sentence boundary??. Also, we include the percent-
age of sentences in the broadcast spoken by the cur-
rent speaker.
We assumed in the development of this system
that the source of the broadcast is known, specif-
ically the source language and the show identity
(e. g. ABC ?World News Tonight?, CNN ?Head-
line News?). Given this information, we constructed
different classifiers for each show. This type of
source-specific modeling was shown to improve per-
formance by Tu?r (2001).
5 Results and Discussion
We report the results of our system on En-
glish, Mandarin and Arabic in Table 5. All results
use show-specific modeling, which consistently im-
proved our results across all metrics, reducing er-
rors by between 10% and 30%. In these tables, we
report the F-measure of identifying the precise lo-
cation of a story boundary as well as three metrics
designed specifically for this type of segmentation
task: the pk metric (Beeferman et al, 1999), Win-
dowDiff (Pevzner and Hearst, 2002) and Cseg (Pseg= 0.3) (Doddington, 1998). All three are derived
from the pk metric (Beeferman et al, 1999), and for
all, lower values imply better performance. For each
of these three metrics we let k = 5, as prescribed in
(Beeferman et al, 1999).
In every system, the best peforming results are
achieved by including all features from the lexical,
acoustic and speaker-dependent feature sets. Across
all languages, our precision?and false alarm rates?
are better than recall?and miss rates. We believe
that inserting erroneous story boundaries will lead
to more serious downstream errors in anaphora res-
olution and summarization than a boundary omis-
sion will. Therefore, high precision is more impor-
tant than high recall for a helpful story segmentation
system. In the English and Mandarin systems, the
lexical and acoustic feature sets perform similarly,
and combine to yield improved results. However,
on the Arabic data, the acoustic feature set performs
quite poorly, suggesting that the use of vocal cues to
topic transitions may be fundamentally different in
Arabic. Moreover, these differences are not simply
differences of degree or direction. Rather, the acous-
tic indicators of topic shifts in English and Man-
darin are, simply, not discriminative when applied
to Arabic. This difference may be due to the style of
Arabic newscasts or to the language itself. Across
configurations, we find that the inclusion of features
derived from automatic speaker identification (fea-
ture set S), errorful as it is, significantly improves
performance. This improvement is particularly pro-
nounced on the Mandarin material; in China News
Radio broadcasts, story boundaries are very strongly
correlated with speaker transitions.
It is difficult to determine how well our system
performs against state-of-the-art story segmentation.
There are no comparable results for the TDT-4 cor-
pus. On the English TDT-2 corpus, (Shriberg et al,
2000) report a Cseg score of 0.1438. While our scoreof .0670 is half that, we hesitate to conclude that
our system is significantly better than this system;
since the (Shriberg et al, 2000) results are based on a
word-level segmentation, the discrepancy may be in-
fluenced by the disparate datasets as well as the per-
formance of the two systems. On CNN and Reuters
stories from the TDT-1 corpus, (Stokes, 2003) re-
port a Pk score of 0.25 and a WD score of 0.253.
Our Pk score is better than this on TDT-4, while
our WD score is worse. (Chaisorn et al, 2003) re-
port an F-measure of 0.532 using only audio-based
features on the TRECVID 2003 corpus , which is
higher than our system, however, this allows for
?correct? boundaries to fall within 5 seconds of ref-
erence boundaries. (Franz et al, 2000) present a sys-
tem which achieves Cseg scores of 0.067 and Man-darin BN and 0.081 on English audio in TDT-3. This
suggests that their system may be better than ours
on Mandarin, and worse on English, although we
trained and tested on different corpora. Finally, we
are unaware of any reported story segmentation re-
sults on Arabic BN.
127
Table 1: TDT-4 segmentation results. (L=lexical feature set, A=acoustic, S=speaker-dependent)
English Mandarin Arabic
F1(p,r) Pk WD Cseg F1(p,r) Pk WD Cseg F1(p,r) Pk WD CsegL+A+S .421(.67,.31) .194 .318 .0670 .592(.73,.50) .179 .245 .0679 .300(.65,.19) .264 .353 .0850
A+S .346(.65,.24) .220 .349 .0721 .586(.72,.49) .178 .252 .0680 .0487(.81,.03) .333 .426 .0999
L+S .342(.66,.23) .231 .362 .074 .575(.72,.48) .200 .278 .0742 .285(.68,.18) .286 .372 .0884
L+A .319(.66,.21) .240 .376 .0787 .294(.72,.18) .277 .354 .0886 .284(.64,.18) .257 .344 .0851
L .257(.68,.16) .261 .399 .0840 .226(.74,.13) .309 .391 .0979 .286(.68,.18) .283 .349 .0849
A .194(.63,.11) .271 .412 .0850 .252(.72,.18) .291 .377 .0904 .0526(.81,.03) .332 .422 .0996
6 Conclusion
In this paper we have presented results of our
story boundary detection procedures on English,
Mandarin, and Arabic Broadcast News from the
TDT-4 corpus. All features are obtained automati-
cally, except for the identity of the news show and
the source language, information which is, however,
available from the data itself, and could be automat-
ically obtained. Our performance on TDT-4 BN ap-
pears to be better than previous work on earlier cor-
pora of BN for English, and slightly worse than pre-
vious efforts on Mandarin, again for a different cor-
pus. We believe our Arabic results to be the first
reported evaluation for BN in that language. One
important observation from our study is that acous-
tic/prosodic features that correlate with story bound-
aries in English and in Mandarin, do not correlate
with Arabic boundaries. Our further research will
adress the study of vocal cues to segmentation in
Arabic BN.
Acknowledgments
This research was partially supported by the De-
fese Advanced Research Projects Agency (DARPA)
under Contract No. HR0011-06-C-0023.
References
R. Barzilay, M. Collins, J. Hirschberg, and S. Whittaker. 2000.
The rules behind roles: Identifying speaker role in radio
broadcasts. In AAAI/IAAI, 679?684.
D. Beeferman, A. Berger, and J. Lafferty. 1999. Statistical mod-
els for text segmentation. Machine Learning, 31:177?210.
P. Boersma. 2001. Praat, a system for doing phonetics by com-
puter. Glot International, 5(9-10):341?345.
L. Chaisorn, T. Chua, C. Koh, Y. Zhao, H. Xu, H. Feng, and
Q. Tian. 2003. A two-level multi-modeal approach for story
segmentation of large news video corpus. In TRECVID.
W. Cohen. 1995. Fast effective rule induction. In Machine
Learning: Proc. of the Twelfth International Conference,
115?123.
G. Doddington. 1998. The topic detection and tracking phase
2 (tdt2) evaluation plan. In Proccedings DARPA Broadcast
News Transcription and Understanding Workshop, 223?229.
M. Franz, J. S. McCarley, T. Ward, and W. J. Zhu. 2000. Seg-
mentation and detection at ibm: Hybrid statstical models and
two-tiered clustering. In Proc. of TDT-3 Workshop.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing. 2003.
Discourse segmentation of multi-party conversation. In 41st
Annual Meeting of ACL, 562?569.M. A. Hearst. 1997. Texttiling: Segmenting text into multi-
paragraph subtopic passages. Computational Linguistics,
23(1):33?64.J. Hirschberg and C. Nakatani. 1998. Acoustic indicators of
topic segmentation. In Proc. of ICSLP, 1255?1258.J. H. Hsieh, C. H. Wu, and K. A. Fung. 2003. Two-stage story
segmentation and detection on broadcast news using genetic
algorithm. In Proc. of the 2003 ISCA Workshop on Multilin-
gual Spoken Document Retrieval (MSDR2003), 55?60.W. Hsu, L. Kennedy, C. W. Huang, S. F. Chang, C. Y. Lin, and
G. Iyengar. 2004. News video story segmentation using fu-
sion of multi-level multi-modal features in trecvid 2003. In
ICASSP.H. Kozima. 1993. Text segmentation based on similarity be-
tween words. In 31st Annual Meeting of the ACL, 286?288.G. A. Levow. 2004. Assessing prosodic and text features for
segmentation of mandarin broadcast news. In HLT-NAACL.M. T. Maybury. 1998. Discourse cues for broadcast news seg-
mentation. In COLING-ACL, 819?822.D. D. Palmer, M. Reichman, and E. Yaich. 2004. Feature selec-
tion for trainable multilingual broadcast news segmentation.
In HLT/NAACL.R. J. Passonneau and D. J. Litman. 1997. Discourse segmenta-
tion by human and automated means. Computational Liun-
guistics, 23(1):103?109.L. Pevzner and M. Hearst. 2002. A critique and improvement
of an evaluation metric for text segmentation. Computational
Linguistics, 28(1):19?36.M. Porter. 1980. An algorithm for suffix stripping. Program,
14(3):130?137.E. Shriberg, A. Stolcke, D. Hakkani-Tu?r, and G. Tu?r. 2000.
Prosody based automatic segmentation of speech into sen-
tences and topics. Speech Comm., 32(1-2):127?154.N. Stokes. 2003. Spoken and written news story segmentation
using lexical chains. In Proc. of the Student Workshop at
HLT-NAACL2003, 49?53.S. Strassel and M. Glenn. 2003. Creating
the annotated tdt-4 y2003 evaluation corpus.
http://www.nist.gov/speech/tests/tdt/tdt2003/papers/ldc.ppt.S. Strassel, M. Glenn, and J. Kong. 2004. Creating
the tdt5 corpus and 2004 evalutation topics at ldc.
http://www.nist.gov/speech/tests/tdt/tdt2004/papers/LDC-
TDT5.ppt.G. Tu?r, D. Hakkani-Tu?r, A. Stolcke, and E. Shriberg. 2001. In-
tegrating prosodic and lexical cues for automatic topic seg-
mentation. Computational Linguistics, 27:31?57.C. L. Wayne. 2000. Multilingual topic detection and tracking:
Successful research enabled by corpora and evaluation. In
LREC, 1487?1494.I. Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, and
S. Cunningham. 1999. Weka: Practical machine learn-
ing tools and techniques with java implementation. In
ICONIP/ANZIIS/ANNES, 192?196.
128
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 397?405,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving the Arabic Pronunciation Dictionary for Phone and Word
Recognition with Linguistically-Based Pronunciation Rules
Fadi Biadsy? and Nizar Habash? and Julia Hirschberg?
?Department of Computer Science, Columbia University, New York, USA
{fadi,julia}@cs.columbia.edu
?Center for Computational Learning Systems, Columbia University, New York, USA
habash@ccls.columbia.edu
Abstract
In this paper, we show that linguistically mo-
tivated pronunciation rules can improve phone
and word recognition results for Modern Stan-
dard Arabic (MSA). Using these rules and
the MADA morphological analysis and dis-
ambiguation tool, multiple pronunciations per
word are automatically generated to build two
pronunciation dictionaries; one for training
and another for decoding. We demonstrate
that the use of these rules can significantly
improve both MSA phone recognition and
MSA word recognition accuracies over a base-
line system using pronunciation rules typi-
cally employed in previous work on MSA Au-
tomatic Speech Recognition (ASR). We ob-
tain a significant improvement in absolute ac-
curacy in phone recognition of 3.77%?7.29%
and a significant improvement of 4.1% in ab-
solute accuracy in ASR.
1 Introduction
The correspondence between orthography and pro-
nunciation in Modern Standard Arabic (MSA) falls
somewhere between that of languages such as Span-
ish and Finnish, which have an almost one-to-one
mapping between letters and sounds, and languages
such as English and French, which exhibit a more
complex letter-to-sound mapping (El-Imam, 2004).
The more complex this mapping is, the more diffi-
cult the language is for Automatic Speech Recogni-
tion (ASR).
An essential component of an ASR system is its
pronunciation dictionary (lexicon), which maps the
orthographic representation of words to their pho-
netic or phonemic pronunciation variants. For lan-
guages with complex letter-to-sound mappings, such
dictionaries are typically written by hand. However,
for morphologically rich languages, such as MSA,1
pronunciation dictionaries are difficult to create by
hand, because of the large number of word forms,
each of which has a large number of possible pro-
nunciations. Fortunately, the relationship between
orthography and pronunciation is relatively regu-
lar and well understood for MSA. Moreover, re-
cent automatic techniques for morphological anal-
ysis and disambiguation (MADA) can also be useful
in automating part of the dictionary creation process
(Habash and Rambow, 2005; Habash and Rambow,
2007) Nonetheless, most documented Arabic ASR
systems appear to handle only a subset of Arabic
phonetic phenomena; very few use morphological
disambiguation tools.
In Section 2, we briefly describe related work, in-
cluding the baseline system we use. In Section 3, we
outline the linguistic phenomena we believe are crit-
ical to improving MSA pronunciation dictionaries.
In Section 4, we describe the pronunciation rules we
have developed based upon these linguistic phenom-
ena. In Section 5, we describe how these rules are
used, together with MADA, to build our pronuncia-
tion dictionaries for training and decoding automat-
ically. In Section 6, we present results of our eval-
uations of our phone- and word-recognition systems
(XPR and XWR) on MSA comparing these systems
to two baseline systems, BASEPR and BASEWR.
1MSA words have fourteen features: part-of-speech, person,
number, gender, voice, aspect, determiner proclitic, conjunctive
proclitic, particle proclitic, pronominal enclitic, nominal case,
nunation, idafa (possessed), and mood. MSA features are real-
ized using both concatenative (affixes and stems) and templatic
(root and patterns) morphology with a variety of morphological
and phonological adjustments that appear in word orthography
and interact with orthographic variations.
397
We conclude in Section 7 and identify directions for
future research.
2 Related Work
Most recent work on ASR for MSA uses a sin-
gle pronunciation dictionary constructed by map-
ping every undiacritized word in the training cor-
pus to all of the diacritized Buckwalter analyses and
the diacritized versions of this word in the Arabic
Treebank (Maamouri et al, 2003; Afify et al, 2005;
Messaoudi et al, 2006; Soltau et al, 2007). In these
papers, each diacritized word is converted to a sin-
gle pronunciation with a one-to-one mapping using
?very few? unspecified rules. None of these systems
use morphological disambiguation to determine the
most likely pronunciation of the word given its con-
text. Vergyri et al (2008) do use morphological in-
formation to predict word pronunciation. They se-
lect the top choice from the MADA (Morphological
Analysis and Disambiguation for Arabic) system for
each word to train their acoustic models. For the test
lexicon they used the undiacritized orthography, as
well as all diacritizations found for each word in the
training data as possible pronunciation variants. We
use this system as our baseline for comparison.
3 Arabic Orthography and Pronunciation
MSA is written in a morpho-phonemic orthographic
representation using the Arabic script, an alphabet
accented with optional diacritical marks.2 MSA has
34 phonemes (28 consonants, 3 long vowels and 3
short vowels). The Arabic script has 36 basic let-
ters (ignoring ligatures) and 9 diacritics. Most Ara-
bic letters have a one-to-one mapping to an MSA
phoneme; however, there are a small number of
common exceptions (Habash et al, 2007; El-Imam,
2004) which we summarize next.
3.1 Optional Diacritics
Arabic script commonly uses nine optional diacrit-
ics: (a) three short-vowel diacritics representing the
vowels /a/, /u/ and /i/; (b) one long-vowel diacritic
(Dagger Alif ?) representing the long vowel /A/ in a
2We provide Arabic script orthographic transliteration in
the Buckwalter transliteration scheme (Buckwalter, 2004). For
Modern Standard Arabic phonological transcription, we use a
variant of the Buckwalter transliteration with the following ex-
ceptions: glottal stops are represented as /G/ and long vowels as
/A/, /U/ and /I/. All Arabic script diacritics are phonologically
spelled out.
small number of words; (c) three nunation diacrit-
ics (F /an/, N /un/, K /in/) representing a combina-
tion of a short vowel and the nominal indefiniteness
marker /n/ in MSA; (d) one consonant lengthening
diacritic (called Shadda ?) which repeats/elongates
the previous consonant (e.g., kat?ab is pronounced
/kattab/); and (e) one diacritic for marking when
there is no diacritic (called Sukun o).
Arabic diacritics can only appear after a let-
ter. Word-initial diacritics (in practice, only short
vowels) are handled by adding an extra Alif A A
(also called Hamzat-Wasl) at the beginning of the
word. Sentence/utterance initial Hamzat-Wasl is
pronounced like a glottal stop preceding the short
vowel; however, the sentence medial Hamzat-Wasl
is silent except for the short vowel. For exam-
ple, Ainkataba kitAbN is /Ginkataba kitAbun/ but
kitAbN Ainkataba is /kitAbun inkataba/. A ?real?
Hamza (glottal stop) is always pronounced as a glot-
tal stop. The Hamzat-Wasl appears most commonly
as the Alif of the definite article Al. It also appears
in specific words and word classes such as relative
pronouns (e.g., Aly ?who? and verbs in pattern VII
(Ain1a2a3).
Arabic short vowel diacritics are used together
with the glide consonant letters w and y to denote
the long vowels /U/ (as uw) and /I/ (iy). This makes
these two letters ambiguous.
Diacritics are largely restricted to religious texts
and Arabic language school textbooks. In other
texts, fewer than 1.5% of words contain a diacritic.
Some diacritics are lexical (where word meaning
varies) and others are inflectional (where nominal
case or verbal mood varies). Inflectional diacritics
are typically word final. Since nominal case, verbal
mood and nunation have all disappeared in spoken
dialectal Arabic, Arabic speakers do not always pro-
duce these inflections correctly or at all.
Much work has been done on automatic Arabic
diacritization (Vergyri and Kirchhoff, 2004; Anan-
thakrishnan et al, 2005; Zitouni et al, 2006; Habash
and Rambow, 2007). In this paper, we use the
MADA (Morphological Analysis and Disambigua-
tion for Arabic) system to diacritize Arabic (Habash
and Rambow, 2005; Habash and Rambow, 2007).
MADA, which uses the Buckwalter Arabic mor-
phological Analyzer databases (Buckwalter, 2004),
provides the necessary information to determine
Hamzat-Wasl through morphologically tagging the
definite article; in most other cases it outputs the spe-
cial symbol ?{? for Hamzat-Wasl.
398
3.2 Hamza Spelling
The consonant Hamza (glottal stop /G/) has multi-
ple forms in Arabic script:  ?,  >,  <,  &, ?
}, Proceedings of NAACL HLT 2009: Short Papers, pages 81?84,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Detecting Pitch Accents at the Word, Syllable and Vowel Level
Andrew Rosenberg
Columbia University
Department of Computer Science
amaxwell@cs.columbia.edu
Julia Hirschberg
Columbia University
Department of Computer Science
julia@cs.columbia.edu
Abstract
The automatic identification of prosodic
events such as pitch accent in English has long
been a topic of interest to speech researchers,
with applications to a variety of spoken lan-
guage processing tasks. However, much re-
mains to be understood about the best meth-
ods for obtaining high accuracy detection. We
describe experiments examining the optimal
domain for accent analysis. Specifically, we
compare pitch accent identification at the syl-
lable, vowel or word level as domains for anal-
ysis of acoustic indicators of accent. Our re-
sults indicate that a word-based approach is
superior to syllable- or vowel-based detection,
achieving an accuracy of 84.2%.
1 Introduction
Prosody in a language like Standard American En-
glish can be used by speakers to convey semantic,
pragmatic and paralinguistic information. Words are
made intonationall prominent, or accented to convey
information such as contrast, focus, topic, and in-
formation status. The communicative implications
of accenting influence the interpretation of a word
or phrase. However, the acoustic excursions associ-
ated with accent are typically aligned with the lex-
ically stressed syllable of the accented word. This
disparity between the domains of acoustic proper-
ties and communicative impact has led to different
approaches to pitch accent detection, and to the use
of different domains of analysis.
In this paper, we compare automatic pitch accent
detection at the vowel, syllable, and word level to
determine which approach is optimal. While lex-
ical and syntactic information has been shown to
contribute to the detection of pitch accent, we only
explore acoustic features. This decision allows us
to closely examine the indicators of accent that are
present in the speech signal in isolation from lin-
guistic effects that may indicate that a word or syl-
lable may be accented. The choice of domain for
automatic pitch accent prediction it also related to
how that prediction is to be used and impacts how
it can be evaluated in comparison with other re-
search efforts. While some downstream spoken lan-
guage processing tasks benefit by knowing which
syllable in a word is accented, such as clarifica-
tion of communication misunderstandings, such as
?I said unlock the door ? not lock it!?, most appli-
cations care only about which word is intonation-
ally prominent. For the identification of contrast,
given/new status, or focus, only word-level informa-
tion is required. While the performance of nucleus-
or syllable-based predictions can be translated to
word predictions, such a translation is rarely per-
formed, making it difficult to compare performance
and thus determine which approach is best.
In this paper, we describe experiments in pitch ac-
cent detection comparing the use of vowel nuclei,
syllables and words as units of analysis. In Section
2, we discuss related work. We describe the ma-
terials in Section 3, the experiments themselves in
Section 4 and conclude in Section 5.
2 Related Work
Acoustic-based approaches to pitch accent detection
have explored prediction at the word, syllable, and
81
vowel level, but have rarely compared prediction
accuracies across these different domains. An ex-
ception is the work of Ross and Ostendorf (1996),
who detect accent on the Boston University Radio
News Corpus (BURNC) at both the syllable and
word level. Using CART predictions as input to an
HMM, they detect pitch accents on syllables spoken
by a single speaker from BURNC with 87.7% accu-
racy, corresponding to 82.5% word-based accuracy,
using both lexical and acoustic features. In compar-
ing the discriminative usefulness of syllables vs. syl-
lable nuclei for accent detection, Tamburini (2003)
finds syllable nuclei (vowel) duration to be as useful
to full syllables. Rosenberg and Hirschberg (2007)
used an energy-based ensemble technique to detect
pitch accents with 84.1% accuracy on the read por-
tion of the Boston Directions Corpus, without us-
ing lexical information. Sridhar et al (2008) ob-
tain 86.0% word-based accuracy using maximum
entropy models from acoustic and syntactic infor-
mation on the BURNC. Syllable-based detection
by Ananthakrishnan and Narayanan (2008) com-
bines acoustic, lexical and syntactic FSM models
to achieve a detection rate of 86.75%. Similar
suprasegmental features have also been explored in
work at SRI/ICSI which employs a hidden event
model to model intonational information for a va-
riety of tasks including punctuation and disfluency
detection (Baron et al, 2002). However, while
progress has been made in accent detection perfor-
mance in the past 15 years, with both word and syl-
lable accuracy at about 86%, these accuracies have
been achieved with different methods and some have
included lexico-syntactic as well as acoustic fea-
tures. It is still not clear which domain of acoustic
analysis provides the most accurate cues for accent
prediction. To address this issue, our work compares
accent detection at the syllable nucleus, full syllable,
and word levels, using a common modeling tech-
nique and a common corpus, to focus on the ques-
tion of which domain of acoustic analysis is most
useful for pitch accent prediction.
3 Boston University Radio News Corpus
Our experiments use 157.9 minutes (29,578 words)
from six speakers in the BURNC (Ostendorf et al,
1995) recordings of professionally read radio news.
This corpus has been prosodically annotated with
full ToBI labeling (Silverman et al, 1992), includ-
ing the presence and type of accents; these are an-
notated at the syllable level and 54.7% (16,178) of
words are accented. Time-aligned phone boundaries
generated by forced alignment are used to identify
vowel regions for analysis. There are 48,359 vow-
els in the corpus and 34.8 of these are accented. To
generate time-aligned syllable boundaries, we align
the forced-aligned phones with a syllabified lexicon
included with the corpus.
The use of BURNC for comparative accent pre-
diction in our three domains is not straightforward,
due to anomalies in the corpus. First, the lexicon
and forced-alignment output in BURNC use distinct
phonetic inventories; to align these, we have em-
ployed a minimum edit distance procedure where
aligning any two vowels incurs zero cost. This guar-
antees that, at a minimum the vowels will be aligned
correctly. Also, the number of syllables per word
in the lexicon does not always match the number
of vowels in the forced alignment. This leads to
114 syllables containing two forced-aligned vowels,
and 8 containing none. Instead of performing post
hoc correction of the syllabification results, we in-
clude all of the automatically identified syllables in
the data set. This syllabification approach generates
48,253 syllables, 16,781 (34.8%) bearing accent.
4 Pitch Accent Detection Experiments
We train logistic regression models to detect the
presence of pitch accent using acoustic features
drawn from each word, syllable and vowel, using
Weka (Witten et al, 1999). The features we use in-
cluded pitch (f0), energy and duration, which have
been shown to correlate with pitch accent in En-
glish. To model these, we calculate pitch and en-
ergy contours for each token using Praat (Boersma,
2001). Duration information is derived using the
vowel, syllable or word segmentation described in
Section 3. The feature vectors we construct include
features derived from both raw and speaker z-score
normalized1 pitch and energy contours. The feature
vector used in all three analysis scenarios is com-
prised of minimum, maximum, mean, standard de-
1Z-score normalization:xnorm = x??? , where x is a valueto normalize, ? and ? are mean and standard deviation. These
are estimated from all pitch or intensity values for a speaker.
82
viation and the z-score of the maximum of these raw
and normalized acoustic contours. The duration of
the region in seconds is also included.
The results of ten-fold cross validation classifica-
tion experiments are shown in Table 1. Note that,
when running ten-fold cross validation on syllables
and vowels, we divide the folds by words, so that
each syllable within a word is a member of the
same fold. To allow for direct comparison of the
three approaches, we generate word-based results
from vowel- and syllable-based experiments. If any
syllable or vowel in a word is hypothesized as ac-
cented, the containing word is predicted to be ac-
cented. Vowel/syllable accuracies should be higher
Region Accuracy (%) F-Measure
Vowel 68.5? 0.319 0.651? 0.00329
Syllable 75.6? 0.125 0.756? 0.00188
Word 82.9? 0.168 0.845? 0.00162
Table 1: Word-level accuracy and F-Measure
than word-based accuracies since the baseline is sig-
nificantly higher. However, we find that the F-
measure for detecting accent is consistently higher
for word-based results. A prediction of accented
on any component syllable is sufficient to generate
a correct word prediction.
Our results suggest, first of all, that there is dis-
criminative information beyond the syllable nucleus.
Syllable-based classification is significantly better
than vowel-based classification, whether we com-
pare accuracy or F-measure. It is possible that the
narrow region of analysis offered by syllable and
vowel-based analysis makes the aggregated features
more susceptible to the effects of noise. Moreover,
errors in the forced-alignment phone boundaries
and syllabification may negatively impact the per-
formance of vowel- and syllable-based approaches.
Until automatic phone alignment improves, word-
based prediction appears to be more reliable. An
automatic, acoustic syllable-nucleus detection ap-
proach may be able generate more discriminative re-
gions of analysis for pitch accent detection than the
forced-alignment and lexicon alignment technique
used here. This remains an area for future study.
However, if we accept that the feature represen-
tations accurately model the acoustic information
contained in the regions of analysis and that the
BURNC annotation is accurate, the most likely ex-
planation for the superiority of word-based predic-
tion over syllable- or vowel-based strategiesis is that
the acoustic excursions correlated with accent occur
outside a word?s lexically stressed syllable. In par-
ticular, complex pitch accents in English are gener-
ally realized on multiple syllables. To examine this
possibility, we looked at the distribution of misses
from the three classification scenarios. The distribu-
tion of pitch accent types of missed detections using
evaluation of the three scenarios is shown in Table
2. In the ToBI framework, the complex pitch ac-
cents include L+H*, L*+H, H+!H* and their down-
stepped variants. As we suspected, larger units of
analysis lead to improved performance on complex
tones; ?2 analysis of the difference between the er-
ror distributions yields a ?2 of 42.108, p< 0.0001.
Since accenting is the perception of a word as
more prominent than surrounding words, features
that incorporate local contextual acoustic informa-
tion should improve detection accuracy at all lev-
els. To represent surrounding acoustic context in
feature vectors, we calculate the z-score of the max-
imum and mean pitch and energy over six regions.
Three of these are ?short range? regions: one pre-
vious region, one following region, and both the
previous and following region. The other three are
?long range? regions. For words, these regions
are defined as two previous words, two following
words, and both two previous and two following
words. To give syllable- and vowel-based classifi-
cation scenarios access to a comparable amount of
acoustic context, the ?long range? regions covered
ranges of three syllables or vowels. There are ap-
proximately 1.63 syllables/vowels per word in the
BURNC corpus; thus, on balance, a window of two
words is equivalent to one of three syllables. Du-
ration is also normalized relative to the duration of
regions within the contextual regions. Accuracy and
f-measure results from ten-fold cross validation ex-
periments are shown in Table 3. We find dramatic
Analysis Region Accuracy (%) F-Measure
Vowel 77.4? 0.264 0.774? 0.00370
Syllable 81.9? 0.197 0.829? 0.00195
Word 84.2? 0.247 0.858? 0.00276
Table 3: Word-level accuracy and F-Measure with Con-
textual Features
increases in the performance of vowel- and syllable-
83
Region H* L* Complex Total Misses
Vowel .6825 (3732) .0686 (375) .2489 (1361) 1.0 (5468)
Syllable .7033 (2422) .0851 (293) .2117 (729) 1.0 (3444)
Word .7422 (2002) .0610 (165) .1986 (537) 1.0 (2704)
Table 2: Distribution of missed detections organized by H*, L* and complex pitch accents.
based performance when we include contextual fea-
tures. Vowel-based classification shows nearly 10%
absolute increase accuracy when translated to the
word level. The improvements in word-based clas-
sification, however, are less dramatic. It may be
that word-based analysis already incorporates much
the contextual information that is helpful for detect-
ing pitch accents. The feature representations in
each of these three experiments include a compara-
ble amount of acoustic context. This suggests that
the superiority of word-based detection is not sim-
ply due to the access to more contextual informa-
tion, but rather that there is discriminative informa-
tion outside the accent-bearing syllable.
5 Conclusion and Future Work
In this paper, we describe experiments comparing
the detection of pitch accents on three acoustic do-
mains ? words, syllables and vowels ? using acous-
tic features alone. To permit direct comparison be-
tween accent prediction in these three domains of
analysis, we generate word-, syllable-, and vowel-
based results directly, and then transfer syllable- and
nucleus-based predictions to word predictions.
Our experiments show that word-based accent
detection significantly outperforms syllable- and
vowel-based approaches. Extracting features that
incorporate acoustic information from surrounding
context improves performance in all three domains.
We find that there is, in fact, acoustic information
discriminative to pitch accent that is found within
accented words, outside the accent-bearing sylla-
ble. We achieve 84.2% word-based accuracy ?
significantly below the 86.0% reported by Sridhar
et al (2008) using syntactic and acoustic compo-
nents. However, our experiments use only acoustic
features, since we are concerned with comparing do-
mains of acoustic analysis within the larger task of
accent identification. Our 84.2% accuracy is signifi-
cantly higher than the 80.09% accuracy obtained by
the 10ms frame-based acoustic modeling described
in (Sridhar et al, 2008). Our aggregations of pitch
and energy contours over a region of analysis appear
to be more helpful than short frame modeling.
In future work, we will explore a number of tech-
niques to transfer word based predictions to sylla-
bles. This will allow us to compare word-based de-
tection to published syllable-based results. Prelimi-
nary results suggest that word-based detection is su-
perior regardless of the domain of evaluation.
References
S. Ananthakrishnan and S. Narayanan. 2008. Auto-
matic prosodic event detection using acoustic, lexical
and syntactic evidence. IEEE Transactions on Audio,
Speech & Language Processing, 16(1):216?228.
D. Baron, E. Shriberg, and A. Stolcke. 2002. Auto-
matic punctuation and disfluency detection in multi-
party meetings using prosodic and lexical cues. In IC-
SLP.
P. Boersma. 2001. Praat, a system for doing phonetics
by computer. Glot International, 5(9-10):341?345.
M. Ostendorf, P. Price, and S. Shattuck-Hufnagel. 1995.
The boston university radio news corpus. Technical
Report ECS-95-001, Boston University, March.
A. Rosenberg and J. Hirschberg. 2007. Detecting pitch
accent using pitch-corrected energy-based predictors.
In Interspeech.
K. Ross and M. Ostendorf. 1996. Prediction of ab-
stract prosodic labels for speech synthesis. Computer
Speech & Language, 10(3):155?185.
K. Silverman, M. Beckman, J. Pitrelli, M. Osten-
dorf, C. Wightman, P. Price, J. Pierrehumbert, and
J. Hirschberg. 1992. Tobi: A standard for labeling en-
glish prosody. In Proc. of the 1992 International Con-
ference on Spoken Language Processing, volume 2,
pages 12?16.
V. R. Sridhar, S. Bangalore, and S. Narayanan. 2008.
Exploiting acoustic and syntactic features for prosody
labeling in a maximum entropy framework. IEEE
Transactions on Audio, Speech & Language Process-
ing, 16(4):797?811.
F. Tamburini. 2003. Prosodic prominence detection in
speech. In ISSPA2003, pages 385?388.
I. Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, and
S. Cunningham. 1999. Weka: Practical machine
learning tools and techniques with java implementa-
tion. In ICONIP/ANZIIS/ANNES International Work-
shop, pages 192?196.
84
Identifying Agreement and Disagreement in Conversational Speech:
Use of Bayesian Networks to Model Pragmatic Dependencies
Michel Galley   , Kathleen McKeown   , Julia Hirschberg   ,
  Columbia University
Computer Science Department
1214 Amsterdam Avenue
New York, NY 10027, USA

galley,kathy,julia  @cs.columbia.edu
and Elizabeth Shriberg 
 SRI International
Speech Technology and Research Laboratory
333 Ravenswood Avenue
Menlo Park, CA 94025, USA
ees@speech.sri.com
Abstract
We describe a statistical approach for modeling
agreements and disagreements in conversational in-
teraction. Our approach first identifies adjacency
pairs using maximum entropy ranking based on a
set of lexical, durational, and structural features that
look both forward and backward in the discourse.
We then classify utterances as agreement or dis-
agreement using these adjacency pairs and features
that represent various pragmatic influences of pre-
vious agreement or disagreement on the current ut-
terance. Our approach achieves 86.9% accuracy, a
4.9% increase over previous work.
1 Introduction
One of the main features of meetings is the occur-
rence of agreement and disagreement among par-
ticipants. Often meetings include long stretches
of controversial discussion before some consensus
decision is reached. Our ultimate goal is auto-
mated summarization of multi-participant meetings
and we hypothesize that the ability to automatically
identify agreement and disagreement between par-
ticipants will help us in the summarization task.
For example, a summary might resemble minutes of
meetings with major decisions reached (consensus)
along with highlighted points of the pros and cons
for each decision. In this paper, we present a method
to automatically classify utterances as agreement,
disagreement, or neither.
Previous work in automatic identification of
agreement/disagreement (Hillard et al, 2003)
demonstrates that this is a feasible task when var-
ious textual, durational, and acoustic features are
available. We build on their approach and show
that we can get an improvement in accuracy when
contextual information is taken into account. Our
approach first identifies adjacency pairs using maxi-
mum entropy ranking based on a set of lexical, dura-
tional and structural features that look both forward
and backward in the discourse. This allows us to ac-
quire, and subsequently process, knowledge about
who speaks to whom. We hypothesize that prag-
matic features that center around previous agree-
ment between speakers in the dialog will influence
the determination of agreement/disagreement. For
example, if a speaker disagrees with another per-
son once in the conversation, is he more likely to
disagree with him again? We model context using
Bayesian networks that allows capturing of these
pragmatic dependencies. Our accuracy for classify-
ing agreements and disagreements is 86.9%, which
is a 4.9% improvement over (Hillard et al, 2003).
In the following sections, we begin by describ-
ing the annotated corpus that we used for our ex-
periments. We then turn to our work on identify-
ing adjacency pairs. In the section on identification
of agreement/disagreement, we describe the contex-
tual features that we model and the implementation
of the classifier. We close with a discussion of future
work.
2 Corpus
The ICSI Meeting corpus (Janin et al, 2003) is
a collection of 75 meetings collected at the In-
ternational Computer Science Institute (ICSI), one
among the growing number of corpora of human-
to-human multi-party conversations. These are nat-
urally occurring, regular weekly meetings of vari-
ous ICSI research teams. Meetings in general run
just under an hour each; they have an average of 6.5
participants.
These meetings have been labeled with adja-
cency pairs (AP), which provide information about
speaker interaction. They reflect the structure of
conversations as paired utterances such as question-
answer and offer-acceptance, and their labeling is
used in our work to determine who are the ad-
dressees in agreements and disagreements. The an-
notation of the corpus with adjacency pairs is de-
scribed in (Shriberg et al, 2004; Dhillon et al,
2004).
Seven of those meetings were segmented into
spurts, defined as periods of speech that have no
pauses greater than .5 second, and each spurt was
labeled with one of the four categories: agreement,
disagreement, backchannel, and other.1 We used
spurt segmentation as our unit of analysis instead of
sentence segmentation, because our ultimate goal is
to build a system that can be fully automated, and
in that respect, spurt segmentation is easy to ob-
tain. Backchannels (e.g. ?uhhuh? and ?okay?) were
treated as a separate category, since they are gener-
ally used by listeners to indicate they are following
along, while not necessarily indicating agreement.
The proportion of classes is the following: 11.9%
are agreements, 6.8% are disagreements, 23.2% are
backchannels, and 58.1% are others. Inter-labeler
reliability estimated on 500 spurts with 2 labelers
was considered quite acceptable, since the kappa
coefficient was .63 (Cohen, 1960).
3 Adjacency Pairs
3.1 Overview
Adjacency pairs (AP) are considered fundamental
units of conversational organization (Schegloff and
Sacks, 1973). Their identification is central to our
problem, since we need to know the identity of
addressees in agreements and disagreements, and
adjacency pairs provide a means of acquiring this
knowledge. An adjacency pair is said to consist of
two parts (later referred to as A and B) that are or-
dered, adjacent, and produced by different speakers.
The first part makes the second one immediately rel-
evant, as a question does with an answer, or an offer
does with an acceptance. Extensive work in con-
versational analysis uses a less restrictive definition
of adjacency pair that does not impose any actual
adjacency requirement; this requirement is prob-
lematic in many respects (Levinson, 1983). Even
when APs are not directly adjacent, the same con-
straints between pairs and mechanisms for select-
ing the next speaker remain in place (e.g. the case
of embedded question and answer pairs). This re-
laxation on a strict adjacency requirement is partic-
ularly important in interactions of multiple speak-
ers since other speakers have more opportunities to
insert utterances between the two elements of the
AP construction (e.g. interrupted, abandoned or ig-
nored utterances; backchannels; APs with multiple
second elements, e.g. a question followed by an-
swers of multiple speakers).2
Information provided by adjacency pairs can be
used to identify the target of an agreeing or dis-
agreeing utterance. We define the problem of AP
1Part of these annotated meetings were provided by the au-
thors of (Hillard et al, 2003).
2The percentage of APs labeled in our data that have non-
contiguous parts is about 21%.
identification as follows: given the second element
(B) of an adjacency pair, determine who is the
speaker of the first element (A). A quite effective
baseline algorithm is to select as speaker of utter-
ance A the most recent speaker before the occur-
rence of utterance B. This strategy selects the right
speaker in 79.8% of the cases in the 50 meetings that
were annotated with adjacency pairs. The next sub-
section describes the machine learning framework
used to significantly outperform this already quite
effective baseline algorithm.
3.2 Maximum Entropy Ranking
We view the problem as an instance of statisti-
cal ranking, a general machine learning paradigm
used for example in statistical parsing (Collins,
2000) and question answering (Ravichandran et al,
2003).3 The problem is to select, given a set of  
possible candidates 			
 (in our case, po-
tential A speakers), the one candidate  that maxi-
mizes a given conditional probability distribution.
We use maximum entropy modeling (Berger et
al., 1996) to directly model the conditional proba-
bility  Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 800?807,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
On the role of context and prosody in the interpretation of ?okay?
Agust??n Gravano, Stefan Benus, He?ctor Cha?vez, Julia Hirschberg, Lauren Wilcox
Department of Computer Science
Columbia University, New York, NY, USA
{agus,sbenus,hrc2009,julia,lgw23}@cs.columbia.edu
Abstract
We examine the effect of contextual and
acoustic cues in the disambiguation of three
discourse-pragmatic functions of the word
okay. Results of a perception study show
that contextual cues are stronger predictors
of discourse function than acoustic cues.
However, acoustic features capturing the
pitch excursion at the right edge of okay fea-
ture prominently in disambiguation, whether
other contextual cues are present or not.
1 Introduction
CUE PHRASES (also known as DISCOURSE MARK-
ERS) are linguistic expressions that can be used to
convey explicit information about the structure of
a discourse or to convey a semantic contribution
(Grosz and Sidner, 1986; Reichman, 1985; Cohen,
1984). For example, the word okay can be used to
convey a ?satisfactory? evaluation of some entity in
the discourse (the movie was okay); as a backchan-
nel in a dialogue to indicate that one interlocutor
is still attending to another; to convey acknowledg-
ment or agreement; or, in its ?cue? use, to start or fin-
ish a discourse segment (Jefferson, 1972; Schegloff
and Sacks, 1973; Kowtko, 1997; Ward and Tsuka-
hara, 2000). A major question is how speakers indi-
cate and listeners interpret such variation in mean-
ing. From a practical perspective, understanding
how speakers and listeners disambiguate cue phrases
is important to spoken dialogue systems, so that sys-
tems can convey potentially ambiguous terms with
their intended meaning and can interpret user input
correctly.
There is considerable evidence that the different
uses of individual cue phrases can be distinguished
by variation in the prosody with which they are re-
alized. For example, (Hirschberg and Litman, 1993)
found that cue phrases in general could be disam-
biguated between their ?semantic? and their ?dis-
course marker? uses in terms of the type of pitch
accent borne by the cue phrase, the position of the
phrase in the intonational phrase, and the amount
of additional information in the phrase. Despite the
frequence of the word okay in natural dialogues,
relatively little attention has been paid to the rela-
tionship between its use and its prosodic realization.
(Hockey, 1993) did find that okay differs in terms of
the pitch contour speakers use in uttering it, suggest-
ing that a final rising pitch contour ?categorically
marks a turn change,? while a downstepped falling
pitch contour usually indicates a discourse segment
boundary. However, it is not clear which, if any, of
the prosodic differences identified in this study are
actually used by listeners in interpreting these po-
tentially ambiguous items.
In this study, we address the question of how hear-
ers disambiguate the interpretation of okay. Our goal
is to identify the acoustic, prosodic and phonetic fea-
tures of okay tokens for which listeners assign differ-
ent meanings. Additionally, we want to determine
the role that discourse context plays in this classi-
fication: i.e., can subjects classify okay tokens reli-
ably from the word alone or do they require addi-
tional context?
Below we describe a perception study in which
listeners were presented with a number of spoken
productions of okay, taken from a corpus of dia-
logues between subjects playing a computer game.
The tokens were presented both in isolation and in
context. Users were asked to select the meaning
800
of each token from three of the meanings that okay
can take on: ACKNOWLEDGEMENT/AGREEMENT,
BACKCHANNEL, and CUE OF AN INITIAL DIS-
COURSE SEGMENT. Subsequently, we examined the
acoustic, prosodic and phonetic correlates of these
classifications to try to infer what cues listeners used
to interpret the tokens, and how these varied by con-
text condition. Section 2 describes our corpus. Sec-
tion 3 describes the perception experiment. In Sec-
tion 4 we analyze inter-subject agreement, introduce
a novel representation of subject judgments, and ex-
amine the acoustic, prosodic, phonetic and contex-
tual correlates of subject classification of okays. In
Section 5 we discuss our results and future work.
2 Corpus
The materials for our perception study were selected
from a portion of the Columbia Games Corpus, a
collection of 12 spontaneous task-oriented dyadic
conversations elicited from speakers of Standard
American English. The corpus was collected and
annotated jointly by the Spoken Language Group
at Columbia University and the Department of Lin-
guistics at Northwestern University.
Subjects were paid to play two series of com-
puter games (the CARDS GAMES and the OBJECTS
GAMES), requiring collaboration between partners
to achieve a common goal. Participants sat in front
of laptops in a soundproof booth with a curtain be-
tween them, so that all communication would be ver-
bal. Each player played with two different partners
in two different sessions. On average, each session
took 45m 39s, totalling 9h 8m of dialogue for the
whole corpus. All interactions were recorded, digi-
tized, and downsampled to 16K.
The recordings were orthographically transcribed
and words were aligned by hand by trained annota-
tors in a ToBI (Beckman and Hirschberg, 1994) or-
thographic tier using Praat (Boersma and Weenink,
2001) to manipulate waveforms. The corpus con-
tains 2239 unique words, with 73,831 words in total.
Nearly all of the Objects Games part of the corpus
has been intonationally transcribed, using the ToBI
conventions. Pitch, energy and duration information
has been extracted for the entire corpus automati-
cally, using Praat.
In the Objects Games portion of the corpus each
player?s laptop displayed a gameboard containing 5?
7 objects (Figure 1). In each segment of the game,
both players saw the same set of objects at the same
position on each screen, except for one object (the
TARGET). For one player (the DESCRIBER), this tar-
get appeared in a random location among other ob-
jects on the screen. For the other player (the FOL-
LOWER), the target object appeared at the bottom of
the screen. The describer was instructed to describe
the position of the target object on their screen so
that the follower could move their representation of
the target to the same location on their own screen.
After the players had negotiated what they deter-
mined to be the best location, they were awarded
up to 100 points based on the actual match of the
target location on the two screens. The game pro-
ceeded in this way through 14 tasks, with describer
and follower alternating roles. On average, the Ob-
jects Games portion of each session took 21m 36s,
resulting in 4h 19m of dialogue for the twelve ses-
sions in the corpus. There are 1484 unique words in
this portion of the corpus, and 36,503 words in total.
Figure 1: Sample screen of the Objects Games.
Throughout the Objects Games, we noted that
subjects made frequent use of affirmative cue words,
such as okay, yeah, alright, which appeared to vary
in meaning. To investigate the discourse functions
of such words, we first asked three labelers to inde-
pendently classify all occurrences of alright, gotcha,
huh, mmhm, okay, right, uhhuh, yeah, yep, yes, yup
in the entire Games Corpus into one of ten cate-
gories, including acknowledgment/agreement, cue
beginning or ending discourse segment, backchan-
nel, and literal modifier. Labelers were asked to
801
choose the most appropriate category for each to-
ken, or indicate with ??? if they could not make a
decision. They were allowed to read the transcripts
and listen to the speech as they labeled.
For our perception experiment we chose materials
from the tokens of the most frequent of our labeled
affirmative words, okay, from the Objects Games,
which contained most of these tokens. Altogether,
there are 1151 instances of okay in this part of the
corpus; it is the third most frequent word, follow-
ing the, with 4565 instances, and of, with 1534.
At least two labelers agreed on the functional cat-
egory of 902 (78%) okay tokens. Of those tokens,
286 (32%) were classified as BACKCHANNEL, 255
(28%) as ACKNOWLEDGEMENT/AGREEMENT, 141
(16%) as CUE BEGINNING, 116 (13%) as PIVOT
BEGINNING (a function that combines Acknowl-
edgement/agreement and Cue beginning), and 104
(11%) as one of the other functions. We sampled
from tokens the annotators had labeled as Cue be-
ginning discourse segment, Backchannel, and Ac-
knowledgement/agreement, the most frequent cate-
gories in the corpus; we will refer to these below
simply as ?C?, ?B?, and ?A? classes, respectively.
3 Experiment
We next designed a perception experiment to ex-
amine naive subjects? perception of these tokens of
okay. To obtain good coverage both of the (labeled)
A, B, and C classes, as well as the degrees of po-
tential ambiguity among these classes, we identified
9 categories of okay tokens to include in the experi-
ment: 3 classes (A, B, C) ? 3 levels of labeler agree-
ment (UNANIMOUS, MAJORITY, NO-AGREEMENT).
?Unanimous? refers to tokens assigned to a particu-
lar class label by all 3 labelers, ?majority? to tokens
assigned to this class by 2 of the 3 labelers, and ?no-
agreement? to tokens assigned to this class by only
1 labeler. To decrease variability in the stimuli, we
selected tokens only from speakers who produced at
least one token for each of the 9 conditions. There
were 6 such speakers (3 female, 3 male), which gave
us a total of 54 tokens.
To see whether subjects? classifications of okay
were dependent upon contextual information or not,
we prepared two versions of each token. The iso-
lated versions consisted of only the word okay ex-
tracted from the waveform. For the contextualized
versions, we extracted two full speaker turns for
each okay including the full turn1 containing the tar-
get okay plus the full turn of the previous speaker. In
the following three sample contexts, pauses are indi-
cated with ?#?, and the target okays are underlined:
Speaker A: yeah # um there?s like there?s some space there?s
Speaker B: okay # I think I got it
Speaker A: but it?s gonna be below the onion
Speaker B: okay
Speaker A: okay # alright # I?ll try it # okay
Speaker B: okay the owl is blinking
The isolated okay tokens were single channel au-
dio files; the contextualized okay tokens were for-
matted so that each speaker was presented to sub-
jects on a different channel, with the speaker uttering
the target okay consistently on the same channel.
The perception study was divided into two parts.
In the first part, each subject was presented with
the 54 isolated okay tokens, in a different ran-
dom ordering for each subject. They were given
a forced choice task to classify them as A, B, or
C, with the corresponding labels (Acknowledge-
ment/agreement, Backchannel, and Cue beginning)
also presented in a random order for each token. In
the second part, the same subject was given 54 con-
textualized tokens, presented in a different random
order, and asked to make the same choice.
We recruited 20 (paid) subjects for the study, 10
female, and 10 male, all between the ages of 20 and
60. All subjects were native speakers of Standard
American English, except for one subject who was
born in Jamaica but a native speaker of English. All
subjects reported no hearing problems. Subjects per-
formed the study in a quiet lab using headphones to
listen to the tokens and indicating their classification
decisions in a GUI interface on a lab workstation.
They were given instructions on how to use the in-
terface before each of the two sections of the study.
For the study itself, for each token in the isolated
condition, subjects were shown a screen with the
three randomly ordered classes and a link to the to-
ken?s sound file. They could listen to the sound files
as many times as they wished but were instructed
not to be concerned with answering the questions
1We define a TURN as a maximal sequence of words spoken
by the same speaker during which the speaker holds the floor.
802
?correctly?, but to answer with their immediate re-
sponse if possible. However, they were allowed to
change their selection as many times as they liked
before moving to the next screen. In the contex-
tualized condition, they were also shown an ortho-
graphic transcription of part of the contextualized to-
ken, to help them identify the target okay. The mean
duration of the first part of the study was 25 minutes,
and of the second part, 27 minutes.
4 Results
4.1 Subject ratings
The distribution of class labels in each experimental
condition is shown in Table 1. While this distribu-
tion roughly mirrors our selection of equal numbers
of tokens from each previously-labeled class, in both
parts of the study more tokens were labeled as A
(acknowledgment/agreement) than as B (backchan-
nel) or C (cue to topic beginning). This supports
the hypothesis that acknowledgment/agreement may
function as the default interpretation of okay.
Isolated Contextualized
A 426 (39%) 452 (42%)
B 324 (30%) 306 (28%)
C 330 (31%) 322 (30%)
Total 1080 (100%) 1080 (100%)
Table 1: Distribution of label classes in each
study condition.
We examined inter-subject agreement using
Fleiss? ? measure of inter-rater agreement for mul-
tiple raters (Fleiss, 1971).2 Table 2 shows Fleiss? ?
calculated for each individual label vs. the other two
labels and for all three labels, in both study condi-
tions. From this table we see that, while there is very
little overall agreement among subjects about how
to classify tokens in the isolated condition, agree-
ment is higher in the contextualized condition, with
a moderate agreement for class C (? score of .497).
This suggests that context helps distinguish the cue
beginning discourse segment function more than the
other two functions of okay.
2 This measure of agreement above chance is interpreted as
follows: 0 = None, 0 - 0.2 = Small, 0.2 - 0.4 = Fair, 0.4 - 0.6 =
Moderate, 0.6 - 0.8 = Substantial, 0.8 - 1 = Almost perfect.
Isolated Contextualized
A vs. rest .089 .227
B vs. rest .118 .164
C vs. rest .157 .497
all .120 .293
Table 2: Fleiss? ? for each label class
in each study condition.
Recall from Section 3 that the okay tokens were
chosen in equal numbers from three classes accord-
ing to the level of agreement of our three original
labelers (unanimous, majority, and no-agreement),
who had the full dialogue context to use in making
their decisions. Table 3 shows Fleiss? ? measure
now grouped by amount of agreement of the orig-
inal labelers, again presented for each context con-
dition. We see here that the inter-subject agreement
Isolated Context. OL
no-agreement .085 .104 -
majority .092 .299 -
unanimous .158 .452 -
all .120 .293 .312
Table 3: Fleiss? ? in each study condition, grouped
by agreement of the three original labelers (?OL?).
also mirrors the agreement of the three original la-
belers. In both study conditions, tokens which the
original labelers agreed on also had the highest ?
scores, followed by tokens in the majority and no-
agreement classes, in that order. In all cases, tokens
which subjects heard in context showed more agree-
ment than those they heard in isolation.
The overall ? is small at .120 for the isolated con-
dition, and fair at .293 for the contextualized con-
dition. The three original labelers also achieved fair
agreement at .312.3 The similarity between the lat-
ter two ? scores suggests that the full context avail-
able to the original labelers and the limited context
presented to the experiment subjets offer compara-
ble amounts of information to disambiguate between
the three functions, although lack of any context
clearly affected subjects? decisions. We conclude
3 For the calculation of this ?, we considered four label
classes: A, B, C, and a fourth class ?other? that comprises the
remaining 7 word functions mentioned in Section 2. In conse-
quence, these ? scores should be compared with caution.
803
from these results that context is of considerable im-
portance in the interpretation of the word okay, al-
though even a very limited context appears to suf-
fice.
4.2 Representing subject judgments
In this section, we present a graphical representa-
tion of subject decisions, useful for interpreting, vi-
sualizing, and comparing the way our subjects in-
terpreted the different tokens of okay. For each in-
dividual okay in the study, we define an associated
three-dimensional VOTE VECTOR, whose compo-
nents are the proportions of subjects that classified
the token as A, B or C. For example, if a particu-
lar okay was labeled as A by 5 subjects, as B by 3,
and as C by 12, then its associated vote vector is
( 5
20 ,
3
20 ,
12
20
)
= (0.25, 0.15, 0.6). Following this def-
inition, the vectors A = (1, 0, 0), B = (0, 1, 0) and
C = (0, 0, 1) correspond to the ideal situations in
which all 20 subjects agreed on the label. We call
these vectors the UNANIMOUS-VOTE VECTORS.
Figure 2.i shows a two-dimensional representa-
tion that illustrates these definitions. The black dot
Figure 2: 2D representation of a vote vector (i)
and of the cluster centroids (ii).
represents the vote vector for our example okay,
the vertices of the triangle correspond to the three
unanimous-vote vectors (A, B and C), and the cross
in the center of the triangle represents the vote vector
of a three-way tie between the labelers (13 , 13 , 13
).
We are thus able to calculate the Euclidean dis-
tance of a vote vector to each of the unanimous-vote
vectors. The shortest of these distances corresponds
to the label assigned by the plurality4 of subjects.
Also, the smaller that distance, the higher the inter-
subject agreement for that particular token. For our
4Plurality is also known as simple majority: the candidate
who gets more votes than any other candidate is the winner.
example okay, the distances to A, B and C are 0.972,
1.070 and 0.495, respectively; its plurality label is C.
In our experiment, each okay has two associated
vote vectors, one for each context condition. To
illustrate the relationship between decisions in the
isolated and the contextualized conditions, we first
grouped each condition?s 54 vote vectors into three
clusters, according to their plurality label. Figure
2.ii shows the cluster centroids in a two-dimensional
representation of vote vectors. The filled dots corre-
spond to the cluster centroids of the isolated condi-
tion, and the empty dots, to the centroids of the con-
textualized condition. Table 4 shows the distances
in each condition from the cluster centroids (denoted
Ac, Bc, Cc) to the respective unanimous-vote vec-tors (A, B, C), and also the distance between each
pair of cluster centroids.
Isolated Contextualized
d(Ac,A) .54 .44 (?18%)
d(Bc,B) .57 .52 (?10%)
d(Cc, C) .52 .28 (?47%)
d(Ac, Bc) .41 .48 (+17%)
d(Ac, Cc) .49 .86 (+75%)
d(Bc, Cc) .54 .91 (+69%)
Table 4: Distances from the cluster centroids (Ac,
Bc, Cc) to the unanimous-vote vectors (A, B, C)and between cluster centroids, in each condition.
In the isolated condition, the three cluster cen-
troids are approximately equidistant from each other
?that is, the three word functions appear to be
equally confusable. In the contextualized condi-
tion, while Cc is further apart from the other twocentroids, the distance between Ac and Bc remainspractically the same. This suggests that, with some
context available, A and B tokens are still fairly con-
fusable, while both are more easily distinguished
from C tokens. We posit two possible explanations
for this: First, C is the only function for which
the speaker uttering the okay necessarily continues
speaking; thus the role of context in disambiguat-
ing seems quite clear. Second, both A and B have a
common element of ?acknowledgement? that might
affect inter-subject agreement.
804
4.3 Features of the okay tokens
In this section, we describe a set of acoustic,
prosodic, phonetic and contextual features which
may help to explain why subjects interpret okay dif-
ferently. Acoustic features were extracted automat-
ically using Praat. Phonetic and prosodic features
were hand-labeled by expert annotators. Contextual
features were considered only in the analysis of the
contextualized condition, since they were not avail-
able to subjects in the isolated condition.
We examined a number of phonetic features to de-
termine whether these correlated with subject clas-
sifications. We first looked at the production of the
three phonemes in the target okay (/oU/, /k/, /eI/),
noting the following possible variations:
? /oU/: [], [A], [5], [O], [OU], [m], [N], [@], [@U].
? /k/: [G], [k], [kx], [q], [x].
? /eI/: [e], [eI], [E], [e@].
We also calculated the duration of each phone and
of the velar closure. Whether the target okay was at
least partially whispered or not, and whether there
was glottalization in the target okay were also noted.
For each target okay, we also examined its du-
ration and its maximum, mean and minimum pitch
and intensity, as well as the speaker-normalized ver-
sions of these values.5 We considered its pitch slope,
intensity slope, and stylized pitch slope, calculated
over the whole target okay, its last 50, 80 and 100
milliseconds, its second half, its second syllable, and
the second half of its second syllable, as well.
We used the ToBI labeling scheme (Pitrelli et al,
1994) to label the prosody of the target okays and
their surrounding context.
? Pitch accent, if any, of the target okay (e.g., H*,
H+!H*, L*).
? Break index after the target okay (0-4).
? Phrase accent and boundary tone, if any, fol-
lowing the target okay (e.g., L-L%, !H-H%).
For contextualized tokens, we included several fea-
tures related to the exchange between the speaker
uttering the target okay (Speaker B) and the other
speaker (Speaker A).
5Speaker-normalized features were normalized by comput-
ing z-scores (z = (X ?mean)/st.dev) for the feature, where
mean and st.dev were calculated from all okays uttered by the
speaker in the session.
? Number of words uttered by Speaker A in the
context, before and after the target okay. Same
for Speaker B.
? Latency of Speaker A before Speaker B?s turn.
? Duration of silence of Speaker B before and af-
ter the target okay.
? Duration of speech by Speaker B immediately
before and after the target okay and up to a si-
lence.
4.4 Cues to interpretation
We conducted a series of Pearson?s tests to look for
correlations between the proportion of subjects that
chose each label and the numeric features described
in Section 4.3, together with two-sided t-tests to find
whether such correlations differed significantly from
zero. Tables 5 and 6 show the significant results
(two-sided t-tests, p < 0.05) for the isolated and
contextualized conditions, respectively.
Acknowledgement/agreement r
duration of realization of /k/ ?0.299
Backchannel r
stylized pitch slope over 2nd half 2nd syl. 0.752
pitch slope over 2nd half of 2nd syllable 0.409
speaker-normalized maximum intensity ?0.372
pitch slope over last 80 ms 0.349
speaker-normalized mean intensity ?0.327
duration of realization of /eI/ 0.278
word duration 0.277
Cue to discourse segment beginning r
stylized pitch slope over the whole word ?0.380
pitch slope over the whole word ?0.342
pitch slope over 2nd half of 2nd syllable ?0.319
Table 5: Features correlated to the proportion of
votes for each label. Isolated condition.
Table 5 shows that in the isolated condition, sub-
jects tended to classify tokens of okay as Acknowl-
edgment/agreement (A) which had a longer realiza-
tion of the /k/ phoneme. They tended to classify
tokens as Backchannels (B) which had a lower in-
tensity, a longer duration, a longer realization of the
/eI/ phoneme, and a final rising pitch. They tended
to classify tokens as C (cue to topic beginning) that
ended with falling pitch.
805
Acknowledgement/agreement r
latency of Spkr A before Spkr B?s turn ?0.528
duration of silence by Spkr B before okay ?0.404
number of words by Spkr B after okay ?0.277
Backchannel r
pitch slope over 2nd half of 2nd syllable 0.520
pitch slope over last 80 ms 0.455
number of words by Spkr A before okay 0.451
number of words by Spkr B after okay ?0.433
duration of speech by Spkr B after okay ?0.413
latency of Spkr A before Spkr B?s turn ?0.385
duration of silence by Spkr B before okay 0.295
intensity slope over 2nd syllable ?0.279
Cue to discourse segment beginning r
latency of Spkr A before Spkr B?s turn 0.645
number of words by Spkr B after okay 0.481
number of words by Spkr A before okay ?0.426
pitch slope over 2nd half of 2nd syllable ?0.385
pitch slope over last 80 ms ?0.377
duration of speech by Spkr B after okay 0.338
Table 6: Features correlated to the proportion of
votes for each label. Contextualized condition.
In the contextualized condition, we find very dif-
ferent correlations. Table 6 shows that nearly all of
the strong correlations in this condition involve con-
textual features, such as the latency before Speaker
B?s turn, or the number of words by each speaker be-
fore and after the target okay. Notably, only one of
the features that show strong correlations in the iso-
lated condition shows the same strong correlation in
the contextualized condition: the pitch slope at the
end of the word. In both conditions subjects tended
to label tokens with a final rising pitch contour as
B, and tokens with a final falling pitch contour as C.
This supports (Hockey, 1993)?s findings on the role
of pitch contour in disambiguating okay.
We next conducted a series of two-sided Fisher?s
exact tests to find correlations between subjects? la-
belings of okay and the nominal features described
in Section 4.3. We found significant associations be-
tween the realization of the /oU/ phoneme and the
okay function in the isolated condition (p < 0.005).
Table 7 shows that, in particular, [m] seems to be the
preferred realization for B okays, while [@] seems to
be the preferred one for A okays, and [OU] and [O]
for A and C okays.
? [A] [5] [OU] [O] [N] [@U] [@] [] [m]
A 0 0 5 6 4 0 0 8 0 0
B 2 0 4 1 0 1 0 1 1 5
C 1 1 2 3 4 0 1 3 0 0
Table 7: Realization of the /oU/ phoneme, grouped
by subject plurality label. Isolated condition only.
Notably, we did not find such significant asso-
ciations in the contextualized condition. We did
find significant correlations in both conditions, how-
ever, between okay classifications and the type of
phrase accent and boundary tone following the target
(Fisher?s Exact Test, p < 0.05 for the isolated con-
dition, p < 0.005 for the contextualized condition).
Table 8 shows that L-L% tends to be associated with
A and C classes, H-H% with B classes, and L-H%
with A and B classes. In this case, such correlations
are present in the isolated condition, and sustained
or enhanced in the contextualized condition.
H-H% H-L% L-H% L-L% other
Isolated
A 0 2 4 8 9
B 3 3 1 5 3
C 1 1 0 8 5
Context.
A 0 2 3 10 10
B 4 3 2 1 2
C 0 1 0 10 5
Table 8: Phrase accent and boundary tone, grouped
by subject plurality label.
Summing up, when subjects listened to the okay
tokens in isolation, with only their acoustic, prosodic
and phonetic properties available, a few features
seem to strongly correlate with the perception of
word function; for example, maximum intensity,
word duration, and realizing the /oU/ phoneme as
[m] tend to be associated with backchannel, while
the duration of the realization of the /k/ phoneme,
and realizing the /oU/ phoneme as [@] tend to be as-
sociated with acknowledgment/agreement.
In the second part of the study, when subjects
listened to contextualized versions of the same to-
kens of okay, most of the strong correlations of word
function with acoustic, prosodic and phonetic fea-
tures were replaced by correlations with contextual
features, like latency and turn duration. In other
words, these results suggest that contextual features
806
might override the effect of most acoustic, prosodic
and phonetic features of okay. There is nonethe-
less one notable exception: word final intonation ?
captured by the pitch slope and the ToBI labels for
phrase accent and boundary tone ? seems to play a
central role in the interpretation of both isolated and
contextualized okays.
5 Conclusion and future work
In this study, we have presented evidence of differ-
ences in the interpretation of the function of isolated
and contextualized okays. We have shown that word
final intonation strongly correlates with the subjects?
classification of okays in both conditions. Addition-
ally, the higher degree of inter-subject agreement in
the contextualized condition, along with the strong
correlations found for contextualized features, sug-
gests that context, when available, plays a central
role in the disambiguation of okay. (Note, how-
ever, that further research is needed in order to assess
whether these features are indeed, in fact, perceptu-
ally important, both individually and combined.)
We have also presented results suggesting that ac-
knowledgment/agreement acts as a default function
for both isolated an contextualized okays. Further-
more, while that function remains confusable with
backchannel in both conditions, the availability of
some context helps in distinguishing those two func-
tions from cue to topic beginning.
These results are relevant to spoken dialogue sys-
tems in suggesting how systems can convey the cue
word okay with the intended meaning and can inter-
pret users? productions of okay correctly. How these
results extend to other cue words and to other word
functions remains an open question.
As future work, we will extend this study to in-
clude the over 5800 occurrences of alright, gotcha,
huh, mmhm, okay, right, uhhuh, yeah, yep, yes, yup
in the entire Games Corpus, and all 10 discourse
functions mentioned in Section 2, as annotated by
our three original labelers. Since we have observed
considerable differences in conversation style in the
two parts of the corpus (the Objects Games elicited
more ?dynamic? conversations, with more overlaps
and interruptions than the Cards Games), we will
compare cue phrase usage in these two settings. Fi-
nally, we are also interested in examining speaker
entrainment in cue phrase usage, or how subjects
adapt their choice and production of cue phrases to
their conversation partner?s.
Acknowledgments
This work was funded in part by NSF IIS-0307905.
We thank Gregory Ward, Elisa Sneed, and Michael
Mulley for their valuable help in collecting and la-
beling the data, and the anonymous reviewers for
helpful comments and suggestions.
References
Mary E. Beckman and Julia Hirschberg. 1994. The ToBIannotation conventions. Ohio State University.
Paul Boersma and David Weenink. 2001. Praat: Doingphonetics by computer. http://www.praat.org.
Robin Cohen. 1984. A computational theory of the func-
tion of clue words in argument understanding. 22nd
Conference of the ACL, pages 251?258.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-ment among many raters. Psychological Bulletin,76(5):378?382.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, Intentions, and the Structure of Discourse. Com-
putational Linguistics, 12(3):175?204.
Julia Hirschberg and Diane Litman. 1993. EmpiricalStudies on the Disambiguation of Cue Phrases. Com-
putational Linguistics, 19(3):501?530.
Beth Ann Hockey. 1993. Prosody and the role of okay
and uh-huh in discourse. Proceedings of the Eastern
States Conference on Linguistics, pages 128?136.
Gail Jefferson. 1972. Side sequences. Studies in social
interaction, 294:338.
Jacqueline C. Kowtko. 1997. The function of intonation
in task-oriented dialogue. Ph.D. thesis, University of
Edinburgh.
John Pitrelli, Mary Beckman, and Julia Hirschberg.1994. Evaluation of prosodic transcription labelingreliability in the ToBI framework. In ICSLP94, vol-ume 2, pages 123?126, Yokohama, Japan.
Rachel Reichman. 1985. Getting Computers to Talk Like
You and Me: Discourse Context, Focus, and Seman-
tics: (an ATN Model). MIT Press.
Emanuel A. Schegloff and Harvey Sacks. 1973. Openingup closings. Semiotica, 8(4):289?327.
Nigel Ward and Wataru Tsukahara. 2000. Prosodic fea-tures which cue back-channel responses in English and
Japanese. Journal of Pragmatics, 23:1177?1207.
807
Proceedings of ACL-08: HLT, pages 807?815,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Unsupervised Approach to Biography Production using Wikipedia
Fadi Biadsy,? Julia Hirschberg? and Elena Filatova*
?Department of Computer Science
Columbia University, New York, NY 10027, USA
{fadi,julia}@cs.columbia.edu
*InforSense LLC
Cambridge, MA 02141, USA
efilatova@inforsense.com
Abstract
We describe an unsupervised approach to
multi-document sentence-extraction based
summarization for the task of producing
biographies. We utilize Wikipedia to auto-
matically construct a corpus of biographical
sentences and TDT4 to construct a corpus
of non-biographical sentences. We build a
biographical-sentence classifier from these
corpora and an SVM regression model for
sentence ordering from the Wikipedia corpus.
We evaluate our work on the DUC2004
evaluation data and with human judges.
Overall, our system significantly outperforms
all systems that participated in DUC2004,
according to the ROUGE-L metric, and is
preferred by human subjects.
1 Introduction
Producing biographies by hand is a labor-intensive
task, generally done only for famous individuals.
The process is particularly difficult when persons of
interest are not well known and when information
must be gathered from a wide variety of sources. We
present an automatic, unsupervised, multi-document
summarization (MDS) approach based on extractive
techniques to producing biographies, answering the
question ?Who is X??
There is growing interest in automatic MDS in
general due in part to the explosion of multilingual
and multimedia data available online. The goal of
MDS is to automatically produce a concise, well-
organized, and fluent summary of a set of docu-
ments on the same topic. MDS strategies have been
employed to produce both generic summaries and
query-focused summaries. Due to the complexity
of text generation, most summarization systems em-
ploy sentence-extraction techniques, in which the
most relevant sentences from one or more docu-
ments are selected to represent the summary. This
approach is guaranteed to produce grammatical sen-
tences, although they must subsequently be ordered
appropriately to produce a coherent summary.
In this paper we describe a sentence-extraction
based MDS procedure to produce biographies from
online resources automatically. We make use of
Wikipedia, the largest free multilingual encyclope-
dia on the internet, to build a biographical-sentence
classifier and a component for ordering sentences in
the output summary. Section 2 presents an overview
of our system. In Section 3 we describe our cor-
pus and in Section 4 we discuss the components of
our system in more detail. In Section 5, we present
an evaluation of our work on the Document Under-
standing Conference of 2004 (DUC2004), the biog-
raphy task (task 5) test set. In Section 6 we com-
pare our research with previous work on biography
generation. We conclude in Section 7 and identify
directions for future research.
2 System Overview
In this section, we present an overview of our biog-
raphy extraction system. We assume as input a set of
documents retrieved by an information retrieval en-
gine from a query consisting of the name of the per-
son for whom the biography is desired. We further
assume that these documents have been tagged with
Named Entities (NE)s with coreferences resolved
807
using a system such as NYU?s 2005 ACE system
(Grishman et al, 2005), which we used for our ex-
periments. Our task is to produce a concise biogra-
phy from these documents.
First, we need to select the most ?important? bio-
graphical sentences for the target person. To do so,
we first extract from the input documents all sen-
tences that contain some reference to the target per-
son according to the coreference assignment algo-
rithm; this reference may be the target?s name or
a coreferential full NP or pronominal referring ex-
pression, such as the President or he. We call these
sentences hypothesis sentences. We hypothesize that
most ?biographical? sentences will contain a refer-
ence to the target. However, some of these sentences
may be irrelevant to a biography; therefore, we filter
them using a binary classifier that retains only ?bio-
graphical? sentences. These biographical sentences
may also include redundant information; therefore,
we cluster them and choose one sentence from each
cluster to represent the information in that cluster.
Since some of these sentences have more salient bi-
ographical information than others and since manu-
ally produced biographies tend to include informa-
tion in a certain order, we reorder our summary sen-
tences using an SVM regression model trained on
biographies. Finally, the first reference to the tar-
get person in the initial sentence in the reordering
is rewritten using the longest coreference in our hy-
pothesis sentences which contains the target?s full
name. We then trim the output to a threshold to pro-
duce a biography of a certain length for evaluation
against the DUC2004 systems.
3 Training Data
One of the difficulties inherent in automatic biog-
raphy generation is the lack of training data. One
might collect training data by manually annotating
a suitable corpus containing biographical and non-
biographical data about a person, as in (Zhou et al,
2004). However, such annotation is labor intensive.
To avoid this problem, we adopt an unsupervised ap-
proach. We use Wikipedia biographies as our corpus
of ?biographical? sentences. We collect our ?non-
biographical? sentences from the English newswire
documents in the TDT4 corpus.1 While each corpus
1http://projects.ldc.upenn.edu/TDT4
may contain positive and negative examples, we as-
sume that most sentences in Wikipedia biographies
are biographical and that the majority of TDT4 sen-
tences are non-biographical.
3.1 Constructing the Biographical Corpus
To automatically collect our biographical sentences,
we first download the xml version of Wikipedia
and extract only the documents whose authors used
the Wikipedia biography template when creating
their biography. There are 16,906 biographies in
Wikipedia that used this template. We next apply
simple text processing techniques to clean the text.
We select at most the first 150 sentences from each
page, to avoid sentences that are not critically impor-
tant to the biography. For each of these sentences we
perform the following steps:
1. We identify the biography?s subject from its ti-
tle, terming this name the ?target person.?
2. We run NYU?s 2005 ACE system (Grish-
man et al, 2005) to tag NEs and do coref-
erence resolution. There are 43 unique NE
tags in our corpora, including PER Individual,
ORG Educational, and so on, and TIMEX tags
for all dates.
3. For each sentence, we replace each NE by its
tag name and type ([name-type subtype]) as as-
signed by the NYU tagger. This modified sen-
tence we term a class-based/lexical sentence.
4. Each non-pronominal referring expression
(e.g., George W. Bush, the US president) that
is tagged as coreferential with the target per-
son is replaced by our own [TARGET PER] tag
and every pronoun P that refers to the target
person is replaced by [TARGET P], where P is
the pronoun itself. This allows us to general-
ize our sentences while retaining a) the essen-
tial distinction between this NE (and its role in
the sentence) and all other NEs in the sentence,
and b) the form of referring expressions.
5. Sentences containing no reference to the tar-
get person are assumed to be irrelevant and re-
moved from the corpus, as are sentences with
808
fewer than 4 tokens; short sentences are un-
likely to contain useful information beyond the
target reference.
For example, given sentences from the Wikipedia
biography of Martin Luther King, Jr. we produce
class-based/lexical sentences as follows:
Martin Luther King, Jr., was born on January 15, 1929, in Atlanta,
Georgia. He was the son of Reverend Martin Luther King, Sr. and
Alberta Williams King. He had an older sister, Willie Christine
(September 11, 1927) and a younger brother, Albert Daniel.
[TARGET PER], was born on [TIMEX], in [GPE PopulationCenter].
[TARGET HE] was the son of [PER Individual] and [PER Individual].
[TARGET HE] had an older sister, [PER Individual] ([TIMEX]) and a
younger brother, [PER Individual].
3.2 Constructing the Non-Biographical Corpus
We use the TDT4 corpus to identify non-
biographical sentences. Again, we run NYU?s 2005
ACE system to tag NEs and do coreference resolu-
tion on each news story in TDT4. Since we have
no target name for these stories, we select an NE
tagged as PER Individual at random from all NEs in
the story to represent the target person. We exclude
any sentence with no reference to this target person
and produce class-based/lexical sentences as above.
4 Our Biography Extraction System
4.1 Classifying Biographical Sentences
Using the biographical and non-biographical cor-
pora described in Section 3, we train a binary classi-
fier to determine whether a new sentence should be
included in a biography or not. For our experiments
we extracted 30,002 sentences from Wikipedia bi-
ographies and held out 2,108 sentences for test-
ing. Similarly. we extracted 23,424 sentences from
TDT4, and held out 2,108 sentences for testing.
For each sentence, we then extract the frequency of
three class-based/lexical features ? unigram, bia-
gram, and trigram ? and two POS features ? the
frequency of unigram and bigram POS. To reduce
the dimensionality of our feature space, we first sort
the features in decreasing order of Chi-square statis-
tics computed from the contingency tables of the ob-
served frequencies from the training data. We then
take the highest 30-80% features, where the num-
ber of features used is determined empirically for
Classifier Accuracy F-Measure
SVM 87.6% 0.87
M. na??ve Bayes 84.1% 0.84
C4.5 81.8% 0.82
Table 1: Binary classification results: Wikipedia bi-
ography class-based/lexical sentences vs. TDT4 class-
based/lexical sentences
each feature type. This process identifies features
that significantly contribute to the classification task.
We extract 3K class-based/lexical unigrams, 5.5K
bigrams, 3K trigrams, 20 POS unigrams, and 166
POS bigrams.
Using the training data described above, we ex-
perimented with three different classification algo-
rithms using the Weka machine learning toolkit
(Witten et al, 1999): multinomial na??ve Bayes,
SVM with linear kernel, and C4.5. Weka also pro-
vides a classification confidence score that repre-
sents how confident the classifier is on each classi-
fied sample, which we will make use of as well.
Table 1 presents the classification results on our
4,216 held-out test-set sentences. These results are
quite promising. However, we should note that they
may not necessarily represent the successful clas-
sification of biographical vs. non-biographical sen-
tences but rather the classification of Wikipedia sen-
tences vs. TDT4 sentences. We will validate these
results for our full systems in Section 5.
4.2 Removing Redundant Sentences
Typically, redundancy removal is a standard com-
ponent in MDS systems. In sentence-extraction
based summarizers, redundant sentences are defined
as those which include the same information with-
out introducing new information and identified by
some form of lexically-based clustering. We use
an implementation of a single-link nearest neighbor
clustering technique based on stem-overlap (Blair-
Goldensohn et al, 2004b) to cluster the sentences
classified as biographical by our classifier, and then
select the sentence from each cluster that maximizes
the confidence score returned by the classifier as the
representative for that cluster.
4.3 Sentence Reordering
It is essential for MDS systems in the extraction
framework to choose the order in which sentences
809
should be presented in the final summary. Present-
ing more important information earlier in a sum-
mary is a general strategy for most domains, al-
though importance may be difficult to determine re-
liably. Similar to (Barzilay and Lee, 2004), we au-
tomatically learn how to order our biographical sen-
tences by observing the typical order of presentation
of information in a particular domain. We observe
that our Wikipedia biographies tend to follow a gen-
eral presentation template, in which birth informa-
tion is mentioned before death information, infor-
mation about current professional position and af-
filiations usually appear early in the biography, and
nuclear family members are typically mentioned be-
fore more distant relations. Learning how to order
information from these biographies however would
require that we learn to identify particular types of
biographical information in sentences.
We directly use the position of each sentence in
each Wikipedia biography as a way of determin-
ing where sentences containing similar information
about different target individuals should appear in
their biographies. We represent the absolute posi-
tion of each sentence in its biography as an inte-
ger and train an SVM regression model with RBF
kernel, from the class/lexical features of the sen-
tence to its position. We represent each sentence by
a feature vector whose elements correspond to the
frequency of unigrams and bigrams of class-based
items (e.g., GPE, PER) (cf. Section 3) and lexical
items; for example, the unigrams born, became, and
[GPE State-or-Province], and the bigrams was born,
[TARGET PER] died and [TARGET PER] joined
would be good candidates for such features.
To minimize the dimensionality of our regres-
sion space, we constrained our feature choice to
those features that are important to distinguish bi-
ographical sentences, which we term biographical
terms. Since we want these biographical terms to
impact the regression function, we define these to
be phrases that consist of at least one lexical item
that occurs in many biographies but rarely more than
once in any given biography. We compute the bio-
graphical term score as in the following equation:
bio score(t)= | Dt || D | ?
?
d?Dt(1?
n(t)d
maxt(n(t)d) )
| D | (1)
where D is the set of 16,906 Wikipedia biographies,
n(t)d is the number of occurrences of term t in doc-
ument d, and Dt = {d ? D : t ? d}. The left factor
represents the document frequency of term t, and the
right factor calculates how infrequent the term is in
each biography that contains t at least once.2 We or-
der the unigrams and bigrams in the biographies by
their biographical term scores and select the high-
est 1K unigrams and 500 bigrams; these thresholds
were determined empirically.
4.4 Reference Rewriting
We observe that news articles typically mention bio-
graphical information that occurs early in Wikipedia
biographies when they mention individuals for the
first time in a story (e.g. Stephen Hawking, the Cam-
bridge University physicist). We take advantage of
the fact that the coreference resolution system we
use tags full noun phrases including appositives as
part of NEs. Therefore, we initially search for the
sentence that contains the longest identified NE (of
type PER) that includes the target person?s full name
and is coreferential with the target according to the
reference resolution system; we denote this NE NE-
NP. If this sentence has already been classified as
a biographical sentence by our classifier, we simply
boost its rank in the summary to first. Otherwise,
when we order our sentences, we replace the refer-
ence to the target person in the first sentence by NE-
NP. For example, if the first sentence in the biogra-
phy we have produced for Jimmy Carter is He was
born in 1947 and a sentence not chosen for inclusion
in our biography Jimmy Carter, former U.S. Presi-
dent, visited the University of California last year.
contains the NE-NP, and Jimmy Carter and He are
coreferential, then the first sentence in our biography
will be rewritten as Jimmy Carter, former U.S. Presi-
dent, was born in 1947. Note that, in the evaluations
presented in Section 5, sentence order was modified
by this process in only eight summaries.
5 Evaluation
To evaluate our biography generation system, we use
the document sets created for the biography evalua-
2We considered various approaches to feature selection here,
such as comparing term frequency between our biographical
and non-biographical corpora. However, terms such as killed
and died, which are useful biographical terms, also occur fre-
quently in our non-biographical corpus.
810
ROUGE-L Average_F
0.25
0.275
0.3
0.325
0.35
0 1 2 3 4 5 6 7 8 9 10 11 12
SVM 
reg. onlytop-DUC2004 C4.5 SVM SVM  + SVM reg. MNB +SVM reg
 
MNBC4.5  +  SVM reg. SVM + baseline orderC4.5 +   baseline order MNB +   baseline order
Figure 1: Comparing our approaches against the top performing system in DUC2004 according to ROUGE-L (dia-
mond).
tion (task 5) of DUC2004.3 The task for systems
participating in this evalution was ? Given each doc-
ument cluster and a question of the form ?Who is
X??, where X is the name of a person or group of
people, create a short summary (no longer than 665
bytes) of the cluster that responds to the question.?
NIST assessors chose 50 clusters of TREC docu-
ments such that all the documents in a given cluster
provide at least part of the answer to this question.
Each cluster contained on average 10 documents.
NIST had 4 human summaries written for each clus-
ter. A baseline summary was also created for each
cluster by extracting the first 665 bytes of the most
recent document in the cluster. 22 systems partici-
pated in the competition, producing a total of 22 au-
tomatic summaries (restricted to 665 bytes) for each
cluster. We evaluate our system against the top per-
forming of these 22 systems, according to ROUGE-
L, which we denote top-DUC2004.4
5.1 Automatic Evaluation Using ROUGE
As noted in Section 4.1, we experimented with a
number of learning algorithms when building our
biographical-sentence classifier. For each machine
learning algorithm tested, we build a system that ini-
tially classifies the input list of sentences into bio-
graphical and non-biographical sentences and then
3http://duc.nist.gov/duc2004
4Note that this system out-performed 19 of the 22 systems
on ROUGE-1 and 20 of 22 on ROUGE-L and ROUGE-W-1.2
(p < .05) (Blair-Goldensohn et al, 2004a). No ROUGE metric
produced scores where this system scored significantly worse
than any other system. See Figure 2 below for a comparison
of all DUC2004 systems with our top system where all systems
are evaluated using ROUGE-L-1.5.5.
removes redundant sentences. Next, we produce
three versions of each system: one which imple-
ments a baseline ordering procedure, in which sen-
tences from the clusters are ordered by their ap-
pearance in their source document (e.g. any sen-
tence which occurred first in its original document
is placed first in the summary, with ties ordered ran-
domly within the set), a second which orders the
biographical sentences by the confidence score ob-
tained from the classifier, and a third which uses the
SVM regression as the reordering component. Fi-
nally, we run our reference rewriting component on
each and trim the output to 665 bytes.
We evaluate first using the ROUGE-L metric (Lin
and Hovy, 2003) with a 95% (ROUGE computed)
confidence interval for all systems and compared
these to the ROUGE-L score of the best-performing
DUC2004 system.5 The higher the ROUGE score,
the closer the summary is to the DUC2004 human
reference summaries. As shown in Figure 1, our
best performing system is the multinomial na??ve
Bayes classifier (MNB) using the classifier confi-
dence scores to order the sentences in the biography.
This system significantly outperforms the top ranked
DUC2004 system (top-DUC2004).6 The success of
this particularly learning algorithm on our task may
be due to: (1) the nature of our feature space ? n-
gram frequencies are modeled properly by a multi-
nomial distribution; (2) the simplicity of this classi-
fier particularly given our large feature dimensional-
5We used the same version (1.5.5) of the ROUGE metric to
compute scores for the DUC systems and baseline also.
6Significance for each pair of systems was determined by
paired t-test and calculated at the .05 significance level.
811
ity; and (3) the robustness of na??ve Bayes with re-
spect to noisy data: Not all sentences in Wikipedia
biographies are biographical sentences and some
sentences in TDT4 are biographical.
While the SVM regression reordering component
has a slight negative impact on the performance
of the MNB system, the difference between the
two versions is not significant. Note however, that
both the C4.5 and the SVM versions of our system
are improved by the SVM regression sentence re-
ordering. While neither performs better than top-
DUC2004 without this component, the C4.5 system
with SVM reordering is significantly better than top-
DUC2004 and the performance of the SVM sys-
tem with SVM regression is comparable to top-
DUC2004. In fact, when we use only the SVM
regression model to rank the hypothesis sentences,
without employing any classifier, then remove re-
dundant sentences, rewrite and trim the results, we
find that, interestingly, this approach also outper-
forms top-DUC2004, although the difference is not
statistically significant. However, we believe that
this is an area worth pursuing in future, with more
sophisticated features.
The following biography of Brian Jones was pro-
duced by our MNB system and then the sentences
were ordered using the SVM regression model:
Born in Bristol in 1947, Brian Jones, the co-pilot on the
Breitling mission, learned to fly at 16, dropping out of
school a year later to join the Royal Air Force. After earn-
ing his commercial balloon flying license, Jones became
a ballooning instructor in 1989 and was certified as an ex-
aminer for balloon flight licenses by the British Civil Avi-
ation Authority. He helped organize Breitling?s most re-
cent around-the-world attempts, in 1997 and 1998. Jones,
52, replaces fellow British flight engineer Tony Brown.
Jones, who is to turn 52 next week, is actually the team?s
third co-pilot. After 13 years of service, he joined a cater-
ing business and, in the 1980s,...
Figure 2 illustrates the performance of our MNB
system with classifier confidence score sentence or-
dering when compared to mean ROUGE-L-1.5.5
scores of DUC2004 human-generated summaries
and the 22 DUC2004 systems? summaries across all
summary tasks. Human summaries are labeled A-
H, DUC2004 systems 1-22, and our MNB system
is marked by the rectangle. Results are sorted by
mean ROUGE-L score. Note that our system perfor-
mance is actually comparable in ROUGE-L score to
one of the human summary generators and is signif-
icantly better that all DUC2004 systems, including
top-DUC2004, which is System 1 in the figure.
5.2 Manual Evaluation
ROUGE evaluation is based on n-gram overlap be-
tween the automatically produced summary and the
human reference summaries. Thus, it is not able to
measure how fluent or coherent a summary is. Sen-
tence ordering is one factor in determining fluency
and coherence. So, we conducted two experiments
to measure these qualities, one comparing our top-
performing system according to ROUGE-L score
(MNB) vs. the top-performing DUC2004 system
(top-DUC2004) and another comparing our top sys-
tem with two different ordering methods, classifier-
based and SVM regression.7 In each experiment,
summaries were trimmed to 665 bytes.
In the first experiment, three native American En-
glish speakers were presented with the 50 questions
(Who is X?). For each question they were given a
pair of summaries (presented in random order): one
was the output of our MNB system and the other
was the summary produced by the top-DUC2004
system. Subjects were asked to decide which sum-
mary was more responsive in form and content to the
question or whether both were equally responsive.
85.3% (128/150) of subject judgments preferred one
summary over the other. 100/128 (78.1%) of these
judgments preferred the summaries produced by our
MNB system over those produced by top-DUC2004.
If we compute the majority vote, there were 42/50
summaries in which at least two subjects made the
same choice. 37/42 (88.1%) of these majority judg-
ments preferred our system?s summary (using bino-
mial test, p = 4.4e?7). We used the weighted kappa
statistic with quadratic weighting (Cohen, 1968)
to determine the inter-rater agreement, obtaining a
mean pairwise ? of 0.441.
Recall from Section 5.1 that our SVM regression
reordering component slightly decreases the aver-
age ROUGE score (although not significantly) for
our MNB system. For our human evaluations, we
decided to evaluate the quality of the presentation
of our summaries with and without this compo-
7Note that top-DUC2004 was ranked sixth in the DUC 2004
manual evaluation, with no system performing significantly
better for coverage and only 1 system performing significantly
better for responsiveness.
812
ROUGE-L Average_F
0.2
0.25
0.3
0.35
0.4
0.45
B E H G F A D C * 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 BL 18 19 20 21 22
Figure 2: ROUGE-L scores for DUC2004 human summaries (A-H), our MNB system (rectangle), and the DUC2004
competing systems (1-22 anonymized), with the baseline system labeled BL.
nent to see if this reordering component affected hu-
man judgments even if it did not improve ROUGE
scores. For each question, we produced two sum-
maries from the sentences classified as biographi-
cal by the MNB classifier, one ordered by the con-
fidence score obtained by the MNB, in decreasing
order, and the other ordered by the SVM regression
values, in increasing order. Note that, in three cases,
the summary sentences were ordered identically by
both procedures, so we used only 47 summaries
for this evaluation. Three (different) native Amer-
ican English speakers were presented with the 47
questions for which sentence ordering differed. For
each question they were given the two summaries
(presented in random order) and asked to determine
which biography they preferred.
We found inter-rater agreement for these judg-
ments using Fleiss? kappa (Fleiss, 1971) to be only
moderate (?=0.362). However, when we computed
the majority vote for each question, we found that
61.7% (29/47) preferred the SVM regression order-
ing over the MNB classifier confidence score order-
ing. Although this difference is not statistically sig-
nificant, again we find the SVM regression ordering
results encouraging enough to motivate our further
research on improving such ordering procedures.
6 Related Work
The DUC2004 system achieving the highest over-
all ROUGE score, our top-DUC2004 in Section 5,
was Blair-Goldensohn et al (2004a)?s DefScriber,
which treats ?Who is X?? as a definition question
and targets definitional themes (e.g. genus-species)
found in the input document collections which in-
clude references to the target person. Extracted sen-
tences are then rewritten using a reference rewriting
system (Nenkova and McKeown, 2003) which at-
tempts to shorten subsequent references to the tar-
get. Sentences are ordered in the summary based
on a weighted combination of topic centrality, lex-
ical cohesion, and topic coverage scores. A simi-
lar approach is explored in Biryukov et al (2005),
which uses Topic Signatures (Lin and Hovy, 2000)
constructed around the target individual?s name to
identify sentences to be included in the biography.
Zhou et al (2004)?s biography generation system,
like ours, trains biographical and non-biographical
sentence classifiers to select sentences to be included
in the biography. Their system is trained on a hand-
annotated corpus of 130 biographies of 12 people,
tagged with 9 biographical elements (e.g., bio, ed-
ucation, nationality) and uses binary unigram and
bigram lexical and unigram part-of-speech features
for classification. Duboue et al (2003) also ad-
dress the problem of learning content selection rules
for biography. They learn rules from two corpora,
a semi-structured corpus with lists of biographical
facts about show business celebrities and a corpus
of free-text biographies about the same celebrities.
Filatova et al (2005) learn text features typical
of biographical descriptions by deducing biograph-
ical and occupation-related activities automatically
by compariing descriptions of people with differ-
ent occupations. Weischedel et al (2004) models
kernel-fact features typical for biographies using lin-
guistic and semantic processing. Linguistic features
813
are derived from predicate-argument structures de-
duced from parse trees, and semantic features are the
set of biography-related relations and events defined
in the ACE guidelines (Doddington et al, 2004).
Sentences containing kernel facts are ranked using
probabilities estimated from a corpus of manually
created biographies, including Wikipedia, to esti-
mate the conditional distribution of relevant material
given a kernel fact and a background corpus.
The problem of ordering sentences and preserv-
ing coherence in MDS is addressed by Barzi-
lay et al (2001), who combine chronological order-
ing of events with cohesion metrics. SVM regres-
sion has recently been used by (Li et al, 2007) for
sentence ranking for general MDS. The authors cal-
culated a similarity score for each sentence to the
human summaries and then regress numeric features
(e.g., the centroid) from each sentence to this score.
Barzilay and Lee (2004) use HMMs to capture topic
shift within a particular domain; sequence of topic
shifts then guides the subsequent ordering of sen-
tences within the summary.
7 Discussion and Future Work
In this paper, we describe a MDS system for produc-
ing biographies, given a target name. We present an
unsupervised approach using Wikipedia biography
pages and a general news corpus (TDT4) to automat-
ically construct training data for our system. We em-
ploy a NE tagger and a coreference resolution sys-
tem to extract class-based and lexical features from
each sentence which we use to train a binary classi-
fier to identify biographical sentences. We also train
an SVM regression model to reorder the sentences
and then employ a rewriting heuristic to create the
final summary.
We compare versions of our system based upon
three machine learning algorithms and two sentence
reordering strategies plus a baseline. Our best per-
forming system uses the multinomial na??ve Bayes
(MNB) classifier with classifier confidence score re-
ordering. However, our SVM regression reorder-
ing improves summaries produced by the other two
classifiers and is preferred by human judges. We
compare our MNB system on the DUC2004 bi-
ography task (task 5) to other DUC2004 systems
and to human-generated summaries. Our system
out-performs all DUC2004 systems significantly,
according to ROUGE-L-1.5.5. When presented
with summaries produced by our system and sum-
maries produced by the best-performing (according
to ROUGE scores) of the DUC2004 systems, human
judges (majority vote of 3) prefer our system?s bi-
ographies in 88.1% of cases.
In addition to its high performance, our approach
has the following advantages: It employs no manual
annotation but relies upon identifying appropriately
different corpora to represent our training corpus.
It employs class-based as well as lexical features
where the classes are obtained automatically from
an ACE NE tagger. It utilizes automatic corefer-
ence resolution to identify sentences containing ref-
erences to the target person. Our sentence reorder-
ing approaches make use of either classifier confi-
dence scores or ordering learned automatically from
the actual ordering of sentences in Wikipedia biogra-
phies to determine the order of presentation of sen-
tences in our summaries.
Since our task is to produce concise summaries,
one focus of our future research will be to simplify
the sentences we extract before classifying them
as biographical or non-biographical. This proce-
dure should also help to remove irrelevant informa-
tion from sentences. Recall that our SVM regres-
sion model for sentence ordering was trained using
only biographical class-based/lexical items. In fu-
ture, we would also like to experiment with more
linguistically-informed features. While Wikipedia
does not enforce any particular ordering of infor-
mation in biographies, and while different biogra-
phies may emphasize different types of information,
it would appear that the success of our automatically
derived ordering procedures may capture some un-
derlying shared view of how biographies are written.
The same underlying views may also apply to do-
mains such as organization descriptions or types of
historical events. In future we plan to explore such a
generalization of our procedures to such domains.
Acknowledgments
We thank Kathy McKeown, Andrew Rosenberg, Wisam Dakka, and the
Speech and NLP groups at Columbia for useful discussions. This mate-
rial is based upon work supported by the Defense Advanced Research
Projects Agency (DARPA) under Contract No. HR001106C0023 (ap-
proved for public release, distribution unlimited). Any opinions, find-
ings and conclusions or recommendations expressed in this material are
those of the authors and do not necessarily reflect the views of DARPA.
814
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL-HLT.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2001. Sentence ordering in multidocument sum-
marization. In Proceedings of the First Human Lan-
guage Technology Conference, San Diego, California.
Maria Biryukov, Roxana Angheluta, and Marie-Francine
Moens. 2005. Multidocument question answering
text summarization using topic signatures. In Pro-
ceedings of the 5th Dutch-Belgium Information Re-
trieval Workshop, Utrecht, the Netherlands.
Sasha Blair-Goldensohn, David Evans, Vasileios Hatzi-
vassiloglou, Kathleen McKeown, Ani Nenkova, Re-
becca Passonneau, Barry Schiffman, Andrew Schlaik-
jer, Advaith Siddharthan, and Sergey Siegelman.
2004a. Columbia University at DUC 2004. In Pro-
ceedings of the 4th Document Understanding Confer-
ence, Boston, Massachusetts, USA.
Sasha Blair-Goldensohn, Kathy McKeown, and Andrew
Schlaikjer. 2004b. Answering definitional questions:
A hybrid approach. In Mark Maybury, editor, New
Directions In Question Answering, chapter 4. AAAI
Press.
J. Cohen. 1968. Weighted kappa: Nominal scale agree-
ment with provision for scaled disagreement or partial
credit. volume 70, pages 213?220.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
program - tasks, data, and evaluation. In Proceedings
of the LREC Conference, Canary Islands, Spain, July.
Pablo Duboue and Kathleen McKeown. 2003. Statistical
acquisition of content selection rules for natural lan-
guage generation. In Proceedings of the Conference
on Empirical Methods for Natural Language Process-
ing, pages 121?128, Sapporo, Japan, July.
Elena Filatova and John Prager. 2005. Tell me what
you do and I?ll tell you what you are: Learning
occupation-related activities for biographies. In Pro-
ceedings of the Joint Human Language Technology
Conference and Conference on Empirical Methods in
Natural Language Processing, pages 113?120, Van-
couver, Canada, October.
J. L. Fleiss. 1971. Measuring nominal scale agreement
among many raters. volume 76, No. 5, pages 378?382.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. Nyu?s english ace 2005 system description. In
ACE 05 Evaluation Workshop, Gaithersburg, MD.
Sujian Li, You Ouyang, Wei Wang, and Bin Sun. 2007.
Multi-document summarization using support vector
regression. In http://duc.nist.gov/pubs/2007papers.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text summa-
rization. In Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 495?501,
Saarbru?cken, Germany, July.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Language Technology
Conference, Edmonton, Canada.
Ani Nenkova and Kathleen McKeown. 2003. References
to named entities: A corpus study. In Proceedings
of the Joint Human Language Technology Conference
and North American chapter of the Association for
Computational Linguistics Annual Meeting, Edmon-
ton, Canada, May.
Ralph Weischedel, Jinxi Xu, and Ana Licuanan. 2004. A
hybrid approach to answering biographical questions.
In Mark Maybury, editor, New Directions In Question
Answering, chapter 5. AAAI Press.
I. Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, and
S. Cunningham. 1999. Weka: Practical machine
learning tools and techniques with java implementa-
tion. In International Workshop: Emerging Knowl-
edge Engineering and Connectionist-Based Informa-
tion Systems, pages 192?196.
Liang Zhou, Miruna Ticrea, and Eduard Hovy. 2004.
Multi-document biography summarization. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 434?441,
Barcelona, Spain.
815
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 169?172,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
High Frequency Word Entrainment in Spoken Dialogue
Ani Nenkova
Dept. of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
nenkova@seas.upenn.edu
Agust??n Gravano
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
agus@cs.columbia.edu
Julia Hirschberg
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
julia@cs.columbia.edu
Abstract
Cognitive theories of dialogue hold that en-
trainment, the automatic alignment between
dialogue partners at many levels of linguistic
representation, is key to facilitating both pro-
duction and comprehension in dialogue. In
this paper we examine novel types of entrain-
ment in two corpora?Switchboard and the
Columbia Games corpus. We examine en-
trainment in use of high-frequency words (the
most common words in the corpus), and its as-
sociation with dialogue naturalness and flow,
as well as with task success. Our results show
that such entrainment is predictive of the per-
ceived naturalness of dialogues and is signifi-
cantly correlated with task success; in overall
interaction flow, higher degrees of entrainment
are associated with more overlaps and fewer
interruptions.
1 Introduction
When people engage in conversation, they adapt the
way they speak to their conversational partner. For
example, they often adopt a certain way of describ-
ing something based upon the way their conversa-
tional partner describes it, negotiating a common
description, particularly for items that may be un-
familiar to them (Brennan, 1996). They also alter
their amplitude, if the person they are speaking with
speaks louder than they do (Coulston et al, 2002),
or reuse syntactic constructions employed earlier in
the conversation (Reitter et al, 2006). This phe-
nomenon is known in the literature as entrainment,
accommodation, adaptation, or alignment.
There is a considerable body of literature which
posits that entrainment may be crucial to human per-
ception of dialogue success and overall quality, as
well as to participants? evaluation of their conversa-
tional partners. Pickering and Garrod (2004) pro-
pose that the automatic alignment at many levels of
linguistic representation (lexical, syntactic and se-
mantic) is key for both production and comprehen-
sion in dialogue, and facilitates interaction. Gole-
man (2006) also claims that a key to successful com-
munication is human ability to synchronize their
communicative behavior with that of their conver-
sational partner. For example, in laboratory stud-
ies of non-verbal entrainment (mimicry of manner-
isms and facial expressions between subjects and
a confederate), Chartrand and Bargh (1999) found
not only that subjects displayed a strong uninten-
tional entrainment, but also that greater entrain-
ment/mimicry led subjects to feel that they liked the
confederate more and that the overall interaction was
progressing more smoothly. People who had a high
inclination for empathy (understanding the point of
view of the other) entrained to a greater extent than
others. Reitter et al (2007) also found that degree of
entrainment in lexical and syntactic repetitions that
occurred in only the first five minutes of each dia-
logue significantly predicted task success in studies
of the HCRC Map Task Corpus.
In this paper we examine a novel dimension of
entrainment between conversation partners: the use
of high-frequency words, the most frequent words in
the dialogue or corpus. In Section 2 we describe ex-
periments on high-frequency word entrainment and
perceived dialogue naturalness in Switchboard dia-
169
logues. The degree of high-frequency word entrain-
ment predicts naturalness with an accuracy of 67%
over a 50% baseline. In Section 3 we discuss experi-
ments on the association of high-frequency word en-
trainment with task success and turn-taking. Results
show that degree of high-frequency word entrain-
ment is positively and significantly correlated with
task success and proportion of overlaps in these di-
alogues, and negatively and significantly correlated
with proportion of interruptions.
2 Predicting perceived naturalness
2.1 The Switchboard Corpus
The Switchboard Corpus (Godfrey et al, 1992) is
a collection of recordings of spontaneous telephone
conversations between speakers of many varieties of
American English who were asked to discuss a pre-
assigned topic from a set including favorite types of
music or the new roles of women in society. The
corpus consists of 2430 conversations with an aver-
age duration of 6 minutes, for a total of 240 hours
and three million words. The corpus has been ortho-
graphically transcribed and annotated for degree of
naturalness on Likert scales from 1 (very natural) to
5 (not natural at all).
2.2 Entrainment and perceived naturalness
Previous studies (Niederhoffer and Pennebaker,
2002) have suggested that adaptation in overall word
count as well as words of particular parts of speech,
or words associated with emotion or with various
cognitive states, can predict the degree of coordi-
nation and engagement of conversational partners.
Here, we examine conversational partners? similar-
ity in high-frequency word usage in the Switchboard
corpus as a predictor of the hand-annotated natural-
ness scores for their conversation. Using entrain-
ment over the most frequent words in the entire cor-
pus has the advantage of avoiding sparsity problems;
we hypothesize that it will be more general and ro-
bust than attempting to measure lexical entrainment
over the high-frequency words that occur in a partic-
ular conversation.
Our measure of entrainment entr(w) is defined as
the negated absolute value of the difference between
the fraction of times a particular word w is used by
the two speakers S1 and S2. More formally,
entr(w) = ?
?
?
?
?
countS1(w)
ALLS1
? countS2(w)ALLS2
?
?
?
?
Here, ALLSi is the number of all words ut-
tered by speaker Si in the given conversation, and
countSi(w) is the number of times Si used word w.
The entr(w) statistic was computed for the 100
most common words in the entire Switchboard cor-
pus and feature selection was used to determine the
25 most predictive words used for later classifica-
tion: um, how, okay, go, I?ve, all, very, as, or, up, a,
no, more, something, from, this, what, too, got, can,
he, in, things, you, and.
The data for the experiments was a balanced set of
250 conversations rated ?1? (very natural) and 250
examples of problematic conversations with ratings
of 3, 4 or 5. The accuracy of predicting the binary
naturalness (ratings of 1 or 3-5) of each conversa-
tion from a logistic regression model is 63.76%, sig-
nificantly over a 50% random baseline. This result
confirms the hypothesis that entrainment in high-
frequency word usage is a good indicator of the per-
ceived naturalness of a conversation.
Some of our 25 high-frequency words are in fact
cue phrases, which are important indicators of dia-
logue structure. This suggests that a more focused
examination of this class of words might be useful.
3 Association with task success and
dialogue flow
3.1 The Columbia Games Corpus
The Columbia Games Corpus (Benus et al, 2007) is
a collection of 12 spontaneous task-oriented dyadic
conversations elicited from native speakers of Stan-
dard American English. Subjects played a series
of computer games requiring verbal communication
between partners to achieve a common goal, ei-
ther identifying matching cards appearing on each
of their screens, or moving an object on one screen
to the same location in which it appeared on the
other, where each subject could see only their own
screen. The games were designed to encourage fre-
quent and natural conversation by engaging the sub-
jects in competitive yet collaborative tasks. For ex-
ample, players could receive points in the games in a
variety of ways and had to negotiate the best strategy
170
for matching cards; in other games, they received
more points if they could place objects in exactly
the same location. Subjects were scored on each
game and their overall score determined the addi-
tional monetary compensation they would receive.
A total of 9h 8m (?73,800 words) of dialogue were
recorded. All files in the corpus were orthograph-
ically transcribed and words were hand-aligned by
trained annotators. A subset of the corpus was also
labeled for different types of turn-taking behavior.
These include (i) smooth turn exchanges?speaker
S2 takes the floor after speaker S1 has completed her
turn, with no overlap; (ii) overlaps?S2 starts his
turn before S1 has completely finished her turn, but
S1 does complete her turn; (iii) interruptions?S2
starts talking before S1 completes her turn, and as a
result S1 does not complete her utterance. We used
these annotations to study the association between
entrainment and turn-taking behavior.
3.2 Entrainment and task success
In the Columbia Games Corpus, we hypothesize that
the game score achieved by the participants is a good
measure of the effectiveness of the dialogue. To de-
termine the extent to which task success is related
to the degree of entrainment in high-frequency word
usage, we examined 48 dialogues. We computed the
correlation coefficient between the game score (nor-
malized by the highest achieved score for the game
type) and two different ways of quantifying the de-
gree of entrainment between the speakers (S1 and
S2) in several word classes. In addition to overall
high-frequency words, we looked at two subclasses
of words often used in dialogue:
25MF-G The 25 most frequent words in the game.
25MF-C The 25 most frequent words over the entire
corpus: the, a, okay, and, of, I, on, right, is, it, that, have,
yeah, like, in, left, it?s, uh, so, top, um, bottom, with, you, to.
ACW Affirmative cue words: alright, gotcha, huh,
mm-hm, okay, right, uh-huh, yeah, yep, yes, yup. There
are 5831 instances in the corpus (7.9% of all words).
FP Filled pauses: uh, um, mm. The corpus contains
1845 instances of filled pauses (2.5% of all tokens).
We generalize our measure of word entrainment
entr(w) to each of these classes of words c:
ENTR1(c) =
?
w?c
entr(w)
ENTR1 ranges from 0 to ??, with 0 meaning per-
fect match on usage of lexical items in class c. An
alternative measure of entrainment that we experi-
mented with is defined as
ENTR2(c) = ?
?
w?c
|countS1(w)? countS2(w)|
?
w?c
(countS1(w) + countS2(w))
The entrainment score defined in this way ranges
from 0 to ?1, with 0 meaning perfect match on lex-
ical usage and ?1 meaning perfect mismatch.
The correlations between the normalized game
score and these measures of entrainment are shown
in Table 1. ENTR1 for the 25 most frequent words,
both corpus-wide and game-specific, is highly and
significantly correlated with task success, with
stronger results for game-specific words. For the
ENTR1 ENTR2
Word class cor p cor p
25MF-C 0.341 0.018 0.187 0.202
25MF-G 0.376 0.008 0.260 0.074
ACW 0.230 0.116 0.372 0.009
FP ?0.080 0.591 ?0.007 0.964
Table 1: Pearson?s correlation with game score.
filled pauses class, there is essentially no correlation
between entrainment and task success, while for af-
firmative cue words there is association only under
the ENTR2 definition of entrainment. The differ-
ence in results between ENTR1 and ENTR2 sug-
gests that the two measures of entrainment capture
different aspects of dialogue coordination and that
exploring various formulations of entrainment de-
serves future attention.
3.3 Dialogue coordination
The coordination of turn-taking in dialogue is espe-
cially important for successful interaction. Speech
overlaps (O), might indicate a lively, highly coor-
dinated conversation, with participants anticipating
the end of their interlocutor?s speaking turn. Smooth
switches of turns (S) with no overlapping speech
are also characteristic of good coordination, in cases
where these are not accompanied by long pauses be-
tween turns. On the other hand, interruptions (I)
and long inter-turn latency (L)?long simultaneous
pauses by the speakers? are generally perceived as
a sign of poorly coordinated dialogues.
171
To determine the relationship between entrain-
ment and dialogue coordination, we examined the
correlation between entrainment types and the pro-
portion of interruptions, smooth switches and over-
laps, for which we have manual annotations for a
subset of 12 dialogues. We also looked at the cor-
relation of entrainment with mean latency in each
dialogue. Table 2 summarizes our major findings.
cor p
ENTR1(25MF-C) I ?0.612 0.035
ENTR1(25MF-G) I ?0.514 0.087
ENTR1(ACW) O 0.636 0.026
ENTR2(ACW) O 0.606 0.037
ENTR1(FP) O 0.750 0.005
ENTR2(25MF-G) O 0.605 0.037
ENTR2(25MF-G) S ?0.663 0.019
ENTR2(ACW) L ?0.757 0.004
ENTR2(25MF-G) L ?0.523 0.081
Table 2: Pearson?s correlation with proportion of over-
laps, interruptions, smooth switches, and mean latency.
The two measures that were significantly cor-
related with task success?ENTR1(25MF-C) and
ENTR1(25MF-G)?also correlated negatively with
the proportion of interruptions in the dialogue. This
finding could have important implications for the de-
velopment of spoken dialog systems (SDS). For ex-
ample, a measure of entrainment might be used to
anticipate the user?s propensity to interrupt the sys-
tem, signalling the need to change dialogue strategy.
It also suggests that if the system entrains to users it
might help to reduce such interruptions. While our
study is of association, not causality, this suggests
future areas of investigation.
Our other correlations reveal that turn exchanges
characterized by overlaps are reliably associated
with entrainment in usage of affirmative cue word,
filled pauses and game-specific most frequent
words. Long latency is negatively associated with
entrainment in affirmative cue words and game-
specific most frequent words. Overall, the more
entrainment, the more engaged the participants and
the better coordination there is between them, with
shorter latencies and more overlaps.
Unexpectedly, smooth switches correlate nega-
tively with entrainment in game-specific most fre-
quent words. This result might be confounded by the
presence of long latencies in some switches. While
smooth switches are desirable, especially in SDS,
long latencies between turns can indicate lack of co-
ordination.
4 Conclusion
We present a corpus study relating dialogue natural-
ness, success and coordination with speaker entrain-
ment on common words: most frequent words over-
all, most frequent words in a dialogue, filled pauses,
and affirmative cue words. We find that degree of
entrainment with respect to most frequent words can
distinguish dialogues rated most natural from those
rated less natural. Entrainment over classes of com-
mon words also strongly correlates with task success
and highly engaged and coordinated turn-taking be-
havior. Entrainment over corpus-wide most frequent
words significantly correlates with task success and
minimal interruptions?important goals of SDS. In
future work we will explore the consequences of
system entrainment to SDS users in helping systems
achieve these goals, and the use of simple measures
of entrainment to modify dialogue strategies in order
to decrease the occurrence of user interruptions.
Acknowledgments
This work was funded in part by NSF IIS-0307905.
References
S. Benus, A. Gravano, and J. Hirschberg. 2007.
The prosody of backchannels in American English.
ICPhS?07.
S.E. Brennan. 1996. Lexical entrainment in spontaneous
dialog. ISSD?96.
T. Chartrand and J. Bargh. 1999. The chameleon ef-
fect: the perception-behavior link and social interac-
tion. J. of Personality & Social Psych., 76(6):893?910.
R. Coulston, S. Oviatt, and C. Darves. 2002. Amplitude
convergence in children?s conversational speech with
animated personas. ICSLP?02.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. ICASSP?92.
Daniel Goleman. 2006. Social Intelligence. Bantam.
K. Niederhoffer and J. Pennebaker. 2002. Linguistic
style matching in social interaction.
M. J. Pickering and S. Garrod. 2004. Toward a mecha-
nistic psychology of dialogue. Behavioral and Brain
Sciences, 27:169?226.
D. Reitter and J. Moore. 2007. Predicting success in
dialogue. ACL?07.
D. Reitter, F. Keller, and J.D. Moore. 2006. Compu-
tational Modelling of Structural Priming in Dialogue.
HLT-NAACL?06.
172
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, page 128,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Invited Talk
Speaking More Like You: Lexical,
Acoustic/Prosodic, and Discourse Entrainment
in Spoken Dialogue Systems
Julia Hirschberg
Department of Computer Science
Columbia University
julia@cs.columbia.edu
Abstract
When people engage in conversation, they adapt the way they speak to
the speaking style of their conversational partner in a variety of ways. For
example, they may adopt a certain way of describing something based upon
the way their conversational partner describes it, or adapt their pitch range
or speaking rate to a conversational partner?s. They may even align their
turn-taking style or use of cue phrases to match their partner?s. These types
of entrainment have been shown to correlate with various measures of task
success and dialogue naturalness. While there is considerable evidence for
lexical entrainment from laboratory experiments, much less is known about
other types of acoustic-prosodic and discourse-level entrainment and little
work has been done to examine entrainments in multiple modalities for the
same dialogue. We will discuss work on entrainment in multiple dimensions
in the Columbia Games Corpus. Our goal is to understand how the different
varieties of entrainment correlate with one another and to determine which
types of entrainment will be both useful and feasible for Spoken Dialogue
Systems.
128
Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 53?61,
Athens, Greece, 31 March, 2009. c?2009 Association for Computational Linguistics
Spoken Arabic Dialect Identification Using Phonotactic Modeling
Fadi Biadsy and Julia Hirschberg
Department of Computer Science
Columbia University, New York, USA
{fadi,julia}@cs.columbia.edu
Nizar Habash
Center for Computational Learning Systems
Columbia University, New York, USA
habash@ccls.columbia.edu
Abstract
The Arabic language is a collection of
multiple variants, among which Modern
Standard Arabic (MSA) has a special sta-
tus as the formal written standard language
of the media, culture and education across
the Arab world. The other variants are in-
formal spoken dialects that are the media
of communication for daily life. Arabic di-
alects differ substantially from MSA and
each other in terms of phonology, mor-
phology, lexical choice and syntax. In this
paper, we describe a system that automat-
ically identifies the Arabic dialect (Gulf,
Iraqi, Levantine, Egyptian and MSA) of a
speaker given a sample of his/her speech.
The phonotactic approach we use proves
to be effective in identifying these di-
alects with considerable overall accuracy
? 81.60% using 30s test utterances.
1 Introduction
For the past three decades, there has been a great
deal of work on the automatic identification (ID)
of languages from the speech signal alone. Re-
cently, accent and dialect identification have be-
gun to receive attention from the speech science
and technology communities. The task of dialect
identification is the recognition of a speaker?s re-
gional dialect, within a predetermined language,
given a sample of his/her speech. The dialect-
identification problem has been viewed as more
challenging than that of language ID due to the
greater similarity between dialects of the same lan-
guage. Our goal in this paper is to analyze the ef-
fectiveness of a phonotactic approach, i.e. making
use primarily of the rules that govern phonemes
and their sequences in a language ? a techniques
which has often been employed by the language
ID community ? for the identification of Arabic
dialects.
The Arabic language has multiple variants, in-
cluding Modern Standard Arabic (MSA), the for-
mal written standard language of the media, cul-
ture and education, and the informal spoken di-
alects that are the preferred method of communi-
cation in daily life. While there are commercially
available Automatic Speech Recognition (ASR)
systems for recognizing MSA with low error rates
(typically trained on Broadcast News), these rec-
ognizers fail when a native Arabic speaker speaks
in his/her regional dialect. Even in news broad-
casts, speakers often code switch between MSA
and dialect, especially in conversational speech,
such as that found in interviews and talk shows.
Being able to identify dialect vs. MSA as well as to
identify which dialect is spoken during the recog-
nition process will enable ASR engines to adapt
their acoustic, pronunciation, morphological, and
language models appropriately and thus improve
recognition accuracy.
Identifying the regional dialect of a speaker will
also provide important benefits for speech tech-
nology beyond improving speech recognition. It
will allow us to infer the speaker?s regional origin
and ethnicity and to adapt features used in speaker
identification to regional original. It should also
prove useful in adapting the output of text-to-
speech synthesis to produce regional speech as
well as MSA ? important for spoken dialogue sys-
tems? development.
In Section 2, we describe related work. In Sec-
tion 3, we discuss some linguistic aspects of Ara-
bic dialects which are important to dialect iden-
tification. In Section 4, we describe the Arabic
dialect corpora employed in our experiments. In
Section 5, we explain our approach to the identifi-
cation of Arabic dialects. We present our experi-
mental results in Section 6. Finally, we conclude
in Section 7 and identify directions for future re-
search.
2 Related Work
A variety of cues by which humans and machines
distinguish one language from another have been
explored in previous research on language identi-
53
fication. Examples of such cues include phone in-
ventory and phonotactics, prosody, lexicon, mor-
phology, and syntax. Some of the most suc-
cessful approaches to language ID have made
use of phonotactic variation. For example, the
Phone Recognition followed by Language Model-
ing (PRLM) approach uses phonotactic informa-
tion to identify languages from the acoustic sig-
nal alone (Zissman, 1996). In this approach, a
phone recognizer (not necessarily trained on a re-
lated language) is used to tokenize training data for
each language to be classified. Phonotactic lan-
guage models generated from this tokenized train-
ing speech are used during testing to compute lan-
guage ID likelihoods for unknown utterances.
Similar cues have successfully been used for
the identification of regional dialects. Zisssman
et al (1996) show that the PRLM approach yields
good results classifying Cuban and Peruvian di-
alects of Spanish, using an English phone recog-
nizer trained on TIMIT (Garofolo et al, 1993).
The recognition accuracy of this system on these
two dialects is 84%, using up to 3 minutes of test
utterances. Torres-Carrasquillo et al (2004) devel-
oped an alternate system that identifies these two
Spanish dialects using Gaussian Mixture Models
(GMM) with shifted-delta-cepstral features. This
system performs less accurately (accuracy of 70%)
than that of (Zissman et al, 1996). Alorfi (2008)
uses an ergodic HMM to model phonetic dif-
ferences between two Arabic dialects (Gulf and
Egyptian Arabic) employing standard MFCC (Mel
Frequency Cepstral Coefficients) and delta fea-
tures. With the best parameter settings, this system
achieves high accuracy of 96.67% on these two
dialects. Ma et al (2006) use multi-dimensional
pitch flux features and MFCC features to distin-
guish three Chinese dialects. In this system the
pitch flux features reduce the error rate by more
than 30% when added to a GMM based MFCC
system. Given 15s of test-utterances, the system
achieves an accuracy of 90% on the three dialects.
Intonational cues have been shown to be good
indicators to human subjects identifying regional
dialects. Peters et al (2002) show that human sub-
jects rely on intonational cues to identify two Ger-
man dialects (Hamburg urban dialects vs. North-
ern Standard German). Similarly, Barakat et
al. (1999) show that subjects distinguish between
Western vs. Eastern Arabic dialects significantly
above chance based on intonation alone.
Hamdi et al (2004) show that rhythmic dif-
ferences exist between Western and Eastern Ara-
bic. The analysis of these differences is done by
comparing percentages of vocalic intervals (%V)
and the standard deviation of intervocalic inter-
vals (?C) across the two groups. These features
have been shown to capture the complexity of the
syllabic structure of a language/dialect in addition
to the existence of vowel reduction. The com-
plexity of syllabic structure of a language/dialect
and the existence of vowel reduction in a language
are good correlates with the rhythmic structure of
the language/dialect, hence the importance of such
a cue for language/dialect identification (Ramus,
2002).
As far as we could determine, there is no
previous work that analyzes the effectiveness of
a phonotactic approach, particularly the parallel
PRLM, for identifying Arabic dialects. In this pa-
per, we build a system based on this approach and
evaluate its performance on five Arabic dialects
(four regional dialects and MSA). In addition, we
experiment with six phone recognizers trained on
six languages as well as three MSA phone recog-
nizers and analyze their contribution to this classi-
fication task. Moreover, we make use of a discrim-
inative classifier that takes all the perplexities of
the language models on the phone sequences and
outputs the hypothesized dialect. This classifier
turns out to be an important component, although
it has not been a standard component in previous
work.
3 Linguistic Aspects of Arabic Dialects
3.1 Arabic and its Dialects
MSA is the official language of the Arab world.
It is the primary language of the media and cul-
ture. MSA is syntactically, morphologically and
phonologically based on Classical Arabic, the lan-
guage of the Qur?an (Islam?s Holy Book). Lexi-
cally, however, it is much more modern. It is not
a native language of any Arabs but is the language
of education across the Arab world. MSA is pri-
marily written not spoken.
The Arabic dialects, in contrast, are the true na-
tive language forms. They are generally restricted
in use to informal daily communication. They
are not taught in schools or even standardized, al-
though there is a rich popular dialect culture of
folktales, songs, movies, and TV shows. Dialects
are primarily spoken, not written. However, this
is changing as more Arabs gain access to elec-
54
tronic media such as emails and newsgroups. Ara-
bic dialects are loosely related to Classical Ara-
bic. They are the result of the interaction between
different ancient dialects of Classical Arabic and
other languages that existed in, neighbored and/or
colonized what is today the Arab world. For ex-
ample, Algerian Arabic has many influences from
Berber as well as French.
Arabic dialects vary on many dimensions ?
primarily, geography and social class. Geo-
linguistically, the Arab world can be divided in
many different ways. The following is only one
of many that covers the main Arabic dialects:
? Gulf Arabic (GLF) includes the dialects of
Kuwait, Saudi Arabia, Bahrain, Qatar, United
Arab Emirates, and Oman.
? Iraqi Arabic (IRQ) is the dialect of Iraq. In
some dialect classifications, Iraqi Arabic is
considered a sub-dialect of Gulf Arabic.
? Levantine Arabic (LEV) includes the di-
alects of Lebanon, Syria, Jordan, Palestine
and Israel.
? Egyptian Arabic (EGY) covers the dialects
of the Nile valley: Egypt and Sudan.
? Maghrebi Arabic covers the dialects of
Morocco, Algeria, Tunisia and Mauritania.
Libya is sometimes included.
Yemenite Arabic is often considered its own
class. Maltese Arabic is not always consid-
ered an Arabic dialect. It is the only Arabic
variant that is considered a separate language
and is written with Latin script.
Socially, it is common to distinguish three sub-
dialects within each dialect region: city dwellers,
peasants/farmers and Bedouins. The three degrees
are often associated with a class hierarchy from
rich, settled city-dwellers down to Bedouins. Dif-
ferent social associations exist as is common in
many other languages around the world.
The relationship between MSA and the dialect
in a specific region is complex. Arabs do not think
of these two as separate languages. This particular
perception leads to a special kind of coexistence
between the two forms of language that serve dif-
ferent purposes. This kind of situation is what lin-
guists term diglossia. Although the two variants
have clear domains of prevalence: formal written
(MSA) versus informal spoken (dialect), there is
a large gray area in between and it is often filled
with a mixing of the two forms.
In this paper, we focus on classifying the di-
alect of audio recordings into one of five varieties:
MSA, GLF, IRQ, LEV, and EGY. We do not ad-
dress other dialects or diglossia.
3.2 Phonological Variations among Arabic
Dialects
Although Arabic dialects and MSA vary on many
different levels ? phonology, orthography, mor-
phology, lexical choice and syntax ? we will
focus on phonological difference in this paper.1
MSA?s phonological profile includes 28 conso-
nants, three short vowels, three long vowels and
two diphthongs (/ay/ and /aw/). Arabic dialects
vary phonologically from standard Arabic and
each other. Some of the common variations in-
clude the following (Holes, 2004; Habash, 2006):
The MSA consonant (/q/) is realized as a glot-
tal stop /?/ in EGY and LEV and as /g/ in GLF and
IRQ. For example, the MSA word /t
?
ari:q/ ?road?
appears as /t
?
ari:?/ (EGY and LEV) and /t
?
ari:g/ (GLF
and IRQ). Other variants also are found in sub di-
alects such as /k/ in rural Palestinian (LEV) and
/dj/ in some GLF dialects. These changes do not
apply to modern and religious borrowings from
MSA. For instance, the word for ?Qur?an? is never
pronounced as anything but /qur?a:n/.
The MSA alveolar affricate (/dj/) is realized as
/g/ in EGY, as /j/ in LEV and as /y/ in GLF. IRQ
preserves the MSA pronunciation. For example,
the word for ?handsome? is /djami:l/ (MSA, IRQ),
/gami:l/ (EGY), /jami:l/ (LEV) and /yami:l/ (GLF).
The MSA consonant (/k/) is generally realized
as /k/ in Arabic dialects with the exception of GLF,
IRQ and the Palestinian rural sub-dialect of LEV,
which allow a /c?/ pronunciation in certain con-
texts. For example, the word for ?fish? is /samak/
in MSA, EGY and most of LEV but /simac?/ in IRQ
and GLF.
The MSA consonant /?/ is pronounced as /t/ in
LEV and EGY (or /s/ in more recent borrowings
from MSA), e.g., the MSA word /?ala:?a/ ?three?
is pronounced /tala:ta/ in EGY and /tla:te/ in LEV.
IRQ and GLF generally preserve the MSA pronun-
ciation.
1It is important to point out that since Arabic dialects are
not standardized, their orthography may not always be con-
sistent. However, this is not a relevant point to this paper
since we are interested in dialect identification using audio
recordings and without using the dialectal transcripts at all.
55
The MSA consonant /?/ is pronounced as /d/
in LEV and EGY (or /z/ in more recent borrow-
ings from MSA), e.g., the word for ?this? is pro-
nounced /ha:?a/ in MSA versus /ha:da/ (LEV) and
/da/ EGY. IRQ and GLF generally preserve the
MSA pronunciation.
The MSA consonants /d
?
/ (emphatic/velarized
d) and /?
?
/ (emphatic /?/) are both normalized to
/d
?
/ in EGY and LEV and to /?
?
/ in GLF and IRQ.
For example, the MSA sentence /?
?
alla yad
?
rubu/
?he continued to hit? is pronounced /d
?
all yud
?
rub/
(LEV) and /?
?
all yu?
?
rub/ (GLF). In modern bor-
rowings from MSA, /?
?
/ is pronounced as /z
?
/ (em-
phatic z) in EGY and LEV. For instance, the word
for ?police officer? is /?
?
a:bit
?
/ in MSA but /z
?
a:bit
?
/
in EGY and LEV.
In some dialects, a loss of the emphatic feature
of some MSA consonants occurs, e.g., the MSA
word /lat
?
i:f/ ?pleasant? is pronounced as /lati:f/ in
the Lebanese city sub-dialect of LEV. Empha-
sis typically spreads to neighboring vowels: if a
vowel is preceded or succeeded directly by an em-
phatic consonant (/d
?
/, /s
?
/, /t
?
/, /?
?
/) then the vowel
becomes an emphatic vowel. As a result, the loss
of the emphatic feature does not affect the conso-
nants only, but also their neighboring vowels.
Other vocalic differences among MSA and the
dialects include the following: First, short vow-
els change or are completely dropped, e.g., the
MSA word /yaktubu/ ?he writes? is pronounced
/yiktib/ (EGY and IRQ) or /yoktob/ (LEV). Sec-
ond, final and unstressed long vowels are short-
ened, e.g., the word /mat
?
a:ra:t/ ?airports? in MSA
becomes /mat
?
ara:t/ in many dialects. Third, the
MSA diphthongs /aw/ and /ay/ have mostly be-
come /o:/ and /e:/, respectively. These vocalic
changes, particularly vowel drop lead to different
syllabic structures. MSA syllables are primarily
light (CV, CV:, CVC) but can also be (CV:C and
CVCC) in utterance-final positions. EGY sylla-
bles are the same as MSA?s although without the
utterance-final restriction. LEV, IRQ and GLF al-
low heavier syllables including word initial clus-
ters such as CCV:C and CCVCC.
4 Corpora
When training a system intended to classify lan-
guages or dialects, it is of course important to use
training and testing corpora recorded under simi-
lar acoustic conditions. We are able to obtain cor-
pora from the Linguistic Data Consortium (LDC)
with similar recording conditions for four Arabic
dialects: Gulf Arabic, Iraqi Arabic, Egyptian Ara-
bic, and Levantine Arabic. These are corpora of
spontaneous telephone conversations produced by
native speakers of the dialects, speaking with fam-
ily members, friends, and unrelated individuals,
sometimes about predetermined topics. Although,
the data have been annotated phonetically and/or
orthographically by LDC, in this paper, we do not
make use of any of annotations.
We use the speech files of 965 speakers (about
41.02 hours of speech) from the Gulf Arabic
conversational telephone Speech database for our
Gulf Arabic data (Appen Pty Ltd, 2006a).2 From
these speakers we hold out 150 speakers for test-
ing (about 6.06 hours of speech).3 We use the Iraqi
Arabic Conversational Telephone Speech database
(Appen Pty Ltd, 2006b) for the Iraqi dialect, se-
lecting 475 Iraqi Arabic speakers with a total du-
ration of about 25.73 hours of speech. From
these speakers we hold out 150 speakers4 for test-
ing (about 7.33 hours of speech). Our Levan-
tine data consists of 1258 speakers from the Ara-
bic CTS Levantine Fisher Training Data Set 1-3
(Maamouri, 2006). This set contains about 78.79
hours of speech in total. We hold out 150 speakers
for testing (about 10 hours of speech) from Set 1.5
For our Egyptian data, we use CallHome Egyp-
tian and its Supplement (Canavan et al, 1997)
and CallFriend Egyptian (Canavan and Zipperlen,
1996). We use 398 speakers from these corpora
(75.7 hours of speech), holding out 150 speakers
for testing.6 (about 28.7 hours of speech.)
Unfortunately, as far as we can determine, there
is no data with similar recording conditions for
MSA. Therefore, we obtain our MSA training data
from TDT4 Arabic broadcast news. We use about
47.6 hours of speech. The acoustic signal was pro-
cessed using forced-alignment with the transcript
to remove non-speech data, such as music. For
testing we again use 150 speakers, this time iden-
tified automatically from the GALE Year 2 Dis-
tillation evaluation corpus (about 12.06 hours of
speech). Non-speech data (e.g., music) in the test
2We excluded very short speech files from the corpora.
3The 24 speakers in devtest folder and the last 63 files,
after sorting by file name, in train2c folder (126 speakers).
The sorting is done to make our experiments reproducible by
other researchers.
4Similar to the Gulf corpus, the 24 speakers in devtest
folder and the last 63 files (after sorting by filename) in
train2c folder (126 speakers)
5We use the last 75 files in Set 1, after sorting by name.
6The test speakers were from evaltest and devtest folders
in CallHome and CallFriend.
56
corpus was removed manually. It should be noted
that the data includes read speech by anchors and
reporters as well as spontaneous speech spoken in
interviews in studios and though the phone.
5 Our Dialect ID Approach
Since, as described in Section 3, Arabic dialects
differ in many respects, such as phonology, lex-
icon, and morphology, it is highly likely that
they differ in terms of phone-sequence distribu-
tion and phonotactic constraints. Thus, we adopt
the phonotactic approach to distinguishing among
Arabic dialects.
5.1 PRLM for dialect ID
As mentioned in Section 2, the PRLM approach to
language identification (Zissman, 1996) has had
considerable success. Recall that, in the PRLM
approach, the phones of the training utterances of
a dialect are first identified using a single phone
recognizer.7 Then an n-gram language model is
trained on the resulting phone sequences for this
dialect. This process results in an n-gram lan-
guage model for each dialect to model the dialect
distribution of phone sequence occurrences. Dur-
ing recognition, given a test speech segment, we
run the phone recognizer to obtain the phone se-
quence for this segment and then compute the per-
plexity of each dialect n-gram model on the se-
quence. The dialect with the n-gram model that
minimizes the perplexity is hypothesized to be the
dialect from which the segment comes.
Parallel PRLM is an extension to the PRLM ap-
proach, in which multiple (k) parallel phone rec-
ognizers, each trained on a different language, are
used instead of a single phone recognizer (Ziss-
man, 1996). For training, we run all phone recog-
nizers in parallel on the set of training utterances
of each dialect. An n-gram model on the outputs of
each phone recognizer is trained for each dialect.
Thus if we have m dialects, k x m n-gram models
are trained. During testing, given a test utterance,
we run all phone recognizers on this utterance and
compute the perplexity of each n-gram model on
the corresponding output phone sequence. Finally,
the perplexities are fed to a combiner to determine
the hypothesized dialect. In our implementation,
7The phone recognizer is typically trained on one of the
languages being identified. Nonetheless, a phone recognize
trained on any language might be a good approximation,
since languages typically share many phones in their phonetic
inventory.
we employ a logistic regression classifier as our
back-end combiner. We have experimented with
different classifiers such as SVM, and neural net-
works, but logistic regression classifier was supe-
rior. The system is illustrated in Figure 1.
We hypothesize that using multiple phone rec-
ognizers as opposed to only one allows the system
to capture subtle phonetic differences that might
be crucial to distinguish dialects. Particularly,
since the phone recognizers are trained on differ-
ent languages, they may be able to model different
vocalic and consonantal systems, hence a different
phonetic inventory. For example, an MSA phone
recognizer typically does not model the phoneme
/g/; however, an English phone recognizer does.
As described in Section 3, this phoneme is an
important cue to distinguishing Egyptian Arabic
from other Arabic dialects. Moreover, phone rec-
ognizers are prone to many errors; relying upon
multiple phone streams rather than one may lead
to a more robust model overall.
5.2 Phone Recognizers
In our experiments, we have used phone recogniz-
ers for English, German, Japanese, Hindi, Man-
darin, and Spanish, from a toolkit developed by
Brno University of Technology.8 These phone rec-
ognizers were trained on the OGI multilanguage
database (Muthusamy et al, 1992) using a hybrid
approach based on Neural Networks and Viterbi
decoding without language models (open-loop)
(Matejka et al, 2005).
Since Arabic dialect identification is our goal,
we hypothesize that an Arabic phone recognizer
would also be useful, particularly since other
phone recognizers do not cover all Arabic con-
sonants, such as pharyngeals and emphatic alveo-
lars. Therefore, we have built our own MSA phone
recognizer using the HMM toolkit (HTK) (Young
et al, 2006). The monophone acoustic models
are built using 3-state continuous HMMs without
state-skipping, with a mixture of 12 Gaussians per
state. We extract standard Mel Frequency Cepstral
Coefficients (MFCC) features from 25 ms frames,
with a frame shift of 10 ms. Each feature vec-
tor is 39D: 13 features (12 cepstral features plus
energy), 13 deltas, and 13 double-deltas. The fea-
tures are normalized using cepstral mean normal-
ization. We use the Broadcast News TDT4 corpus
(Arabic Set 1; 47.61 hours of speech; downsam-
pled to 8Khz) to train our acoustic models. The
8www.fit.vutbr.cz/research/groups/speech/sw/phnrec
57
???????????
????????
?????????? ?
???????????????
??????????????
?????????
??????????????
?????????????
????????????
??????????????
? ??????????
???????????????
????????????
??????? ?
?????? ?
?????????? ?
??????????? ?
????? ?
?????????
?????? ?
?????????? ?
??????????? ?
????? ?
????????????????
?????????
?????? ?
?????????? ?
??????????? ?
????? ?
?????????
????????????
Figure 1: Parallel Phone Recognition Followed by Language Modeling (PRLM) for Arabic Dialect Identification.
pronunciation dictionary is generated as described
in (Biadsy et al, 2009). Using these settings we
build three MSA phone recognizers: (1) an open-
loop phone recognizer which does not distinguish
emphatic vowels from non-emphatic (ArbO), (2)
an open-loop with emphatic vowels (ArbOE), and
(3) a phone recognizer with emphatic vowels and
with a bi-gram phone language model (ArbLME).
We add a new pronunciation rule to the set of
rules described in (Biadsy et al, 2009) to distin-
guish emphatic vowels from non-emphatic ones
(see Section 3) when generating our pronunciation
dictionary for training the acoustic models for the
the phone recognizers. In total we build 9 (Arabic
and non-Arabic) phone recognizers.
6 Experiments and Results
In this section, we evaluate the effectiveness of the
parallel PRLM approach on distinguishing Ara-
bic dialects. We first run the nine phone recog-
nizers described in Section 5 on the training data
described in Section 4, for each dialect. This pro-
cess produces nine sets of phone sequences for
each dialect. In our implementation, we train a
tri-gram language model on each phone set using
the SRILM toolkit (Stolcke, 2002). Thus, in total,
we have 9 x (number of dialects) tri-grams.
In all our experiments, the 150 test speakers of
each dialect are first decoded using the phone rec-
ognizers. Then the perplexities of the correspond-
ing tri-gram models on these sequences are com-
puted, and are given to the logistic regression clas-
sifier. Instead of splitting our held-out data into
test and training sets, we report our results with
10-fold cross validation.
We have conducted three experiments to eval-
uate our system. The first is to compare the per-
formance of our system to Alorfi?s (2008) on the
same two dialects (Gulf and Egyptian Arabic).
The second is to attempt to classify four collo-
quial Arabic dialects. In the third experiment, we
include MSA as well in a five-way classification
task.
6.1 Gulf vs. Egyptian Dialect ID
To our knowledge, Alorfi?s (2008) work is the
only work dealing with the automatic identifica-
tion of Arabic dialects. In this work, an Ergodic
HMM is used to model phonetic differences be-
tween Gulf and Egyptian Arabic using MFCC and
delta features. The test and training data used in
this work was collected from TV soap operas con-
taining both the Egyptian and Gulf dialects and
from twenty speakers from CallHome Egyptian
database. The best accuracy reported by Alorfi
(2008) on identifying the dialect of 40 utterances
of duration of 30 seconds each of 40 male speakers
(20 Egyptians and 20 Gulf speakers) is 96.67%.
Since we do not have access to the test collec-
tion used in (Alorfi, 2008), we test a version of our
system which identifies these two dialects only on
our 150 Gulf and 150 Egyptian speakers, as de-
scribed in Section 4. Our best result is 97.00%
(Egyptian and Gulf F-Measure = 0.97) when us-
ing only the features from the ArbOE, English,
Japanese, and Mandarin phone recognizers. While
our accuracy might not be significantly higher than
that of Alorfi?s, we note a few advantages of our
experiments. First, the test sets of both dialects
are from telephone conversations, with the same
recording conditions, as opposed to a mix of dif-
ferent genres. Second, in our system we test 300
speakers as oppose to 40, so our results may be
more reliable. Third, our test data includes female
58
4 dialectsseconds accuracy Gulf Iraqi Levantine Egyptian5 60.833 49.2 52.7 58.1 8315 72.83 60.8 61.2 77.6 91.930 78.5 68.7 67.3 84 9445 81.5 72.6 72.4 86.9 93.760 83.33 75.1 75.7 87.9 94.6120 84 75.1 75.4 89.5 96
??
??
??
??
??
??
??
??
??
??
??
??
? ?? ?? ?? ?? ??
?????
????????
????????
??????????
?????????
?
?????????????????
Figure 2: The accuracies and F-Measures of the four-way
classification task with different test-utterance durations
speakers as well as male, so our results are more
general.
6.2 Four Colloquial Arabic Dialect ID
In our second experiment, we test our system on
four colloquial Arabic dialects (Gulf, Iraqi, Levan-
tine, and Egyptian). As mentioned above, we use
the phone recognizers to decode the training data
to train the 9 tri-gram models per dialect (9x4=36
tri-gram models). We report our 10-fold cross val-
idation results on the test data in Figure 2. To
analyze how dependent our system is on the du-
ration of the test utterance, we report the system
accuracy and the F-measure of each class for dif-
ferent durations (5s ? 2m). The longer the ut-
terance, the better we expect the system to per-
form. We can observe from these results that re-
gardless of the test-utterance duration, the best dis-
tinguished dialect among the four dialects is Egyp-
tian (F-Measure of 94% with 30s test utterances),
followed by Levantine (F-Measure of 84% with
30s), and the most confusable dialects, according
to the classification confusion matrix, are those of
the Gulf and Iraqi Arabic (F-Measure of 68.7%,
67.3%, respectively with 30s). This confusion is
consistent with dialect classifications that consider
Iraqi a sub-dialect of Gulf Arabic, as mentioned in
Section 3.
We were also interested in testing which phone
recognizers contribute the most to the classifica-
tion task. We observe that employing a subset of
the phone recognizers as opposed to all of them
provides us with better results. Table 1 shows
which phone recognizers are selected empirically,
for each test-utterance duration condition.9
9Starting from all phone recognizers, we remove one rec-
ognizer at a time; if the cross-validation accuracy decreases,
Dur. Acc. (%) Phone Recognizers
5s 60.83 ArbOE+ArbLME+G+H+M+S
15s 72.83 ArbOE+ArbLME+G+H+M
30s 78.50 ArbO+H+S
45s 81.5 ArbE+ArbLME+H+G+S
60s 83.33 ArbOE+ArbLME+E+G+H+M
120s 84.00 ArbOE+ArbLME+G+M
Table 1: Accuracy of the four-way classification (four col-
loquial Arabic dialects) and the best combination of phone
recognizers used per test-utterances duration; The phone
recognizers used are: E=English, G=German, H=Hindi,
M=Mandarin, S=Spanish, ArbO=open-loop MSA without
emphatic vowels, ArbOE=open-loop MSA with emphatic
vowels, ArbLME=MSA with emphatic vowels and bi-gram
phone LM
We observe that the MSA phone recognizers are
the most important phone recognizers for this task,
usually when emphatic vowels are modeled. In all
scenarios, removing all MSA phone recognizers
leads to a significant drop in accuracy. German,
Mandarin, Hindi, and Spanish typically contribute
to the classification task, but English, and Japanese
phone recognizers are less helpful. It is possible
that the more useful recognizers are able to cap-
ture more of the distinctions among the Arabic di-
alects; however, it might also be that the overall
quality of the recognizers also varies.
6.3 Dialect ID with MSA
Considering MSA as a dialectal variant of Ara-
bic, we are also interested in analyzing the perfor-
mance of our system when including it in our clas-
sification task. In this experiment, we add MSA as
the fifth dialect. We perform the same steps de-
scribed above for training, using the MSA corpus
described in Section 4. For testing, we use also
our 150 hypothesized MSA speakers as our test
set. Interestingly, in this five-way classification,
we observe that the F-Measure for the MSA class
in the cross-validation task is always above 98%
regardless of the test-utterance duration, as shown
in Figure 3.
It would seem that MSA is rarely confused with
any of the colloquial dialects: it appears to have a
distinct phonotactic distribution. This explanation
is supported by linguists, who note that MSA dif-
fers from Arabic dialects in terms of its phonology,
lexicon, syntax and morphology, which appears to
lead to a profound impact on its phonotactic distri-
bution. Similar to the four-way classification task,
we add it back. We have experimented with an automatic
feature selection methods, but with the empirical (?greedy?)
selection we typically obtain higher accuracy.
59
4 dialectsseconds accuracy Gulf Iraqi Levantine Egyptian5 68.6667 54.5 50.7 60 77.915 76.6667 57.3 62.6 73.8 90.730 81.6 68.3 71.7 79.4 90.245 84.8 69.9 73.6 86.2 94.960 86.933 76.8 76.5 85.4 96.3120 87.86 79.1 77.4 90.1 93.6
??
??
??
??
??
??
??
??
??
??
??
??
? ?? ?? ?? ?? ??
?????
????????
????????
??????????
?????????
???????
?
?????????????????
Figure 3: The accuracies and F-Measures of the five-way
classification task with different test-utterance durations
Dur. Acc. (%) Phone Recognizers
5s 68.67 ArbO+ArbLME+H+M
15s 76.67 ArbLME+G+H+J+M
30s 81.60 ArbO+ArbOE+E+G+H+J+M+S
45s 84.80 ArbOE+ArbLME+E+G+H+J+M+S
60s 86.93 ArbOE+ArbLME+G+J+M+S
120s 87.86 ArbO+ArbLME+E+S
Table 2: Accuracy of the five-way classification (4 colloquial
Arabic dialects + MSA) and the best combination of phone
recognizers used per test-utterances duration; The phone
recognizers used are: E=English, G=German, H=Hindi,
J=Japanese, M=Mandarin, S=Spanish, ArbO=open-loop
MSA without emphatic vowels, ArbOE=open-loop MSA
with emphatic vowels, ArbLME=MSA with emphatic vow-
els and bi-gram phone LM
Egyptian was the most easily distinguished dialect
(F-Measure=90.2%, with 30s test utterance) fol-
lowed by Levantine (79.4%), and then Iraqi and
Gulf (71.7% and 68.3%, respectively). Due to the
high MSA F-Measure, the five-way classifier can
also be used as a binary classifier to distinguish
MSA from colloquial Arabic (Gulf, Iraqi, Levan-
tine, and Egyption) reliably.
It should be noted that our classification results
for MSA might be inflated for several reasons: (1)
The MSA test data were collected from Broad-
cast News, which includes read (anchor and re-
porter) speech, as well as telephone speech (for in-
terviews). (2) The identities of the test speakers in
the MSA corpus were determined automatically,
and so might not be as accurate.
As a result of the high identification rate of
MSA, the overall accuracy in the five-way clas-
sification task is higher than that of the four-way
classification. Table 2 presents the phone recog-
nizers selected the accuracy for each test utterance
duration. We observe here that the most impor-
tant phone recognizers are those trained on MSA
(ArbO, ArbOE, and/or ArbLME). Removing them
completely leads to a significant drop in accu-
racy. In this classification task, we observe that all
phone recognizers play a role in the classification
task in some of the conditions.
7 Conclusions and Future Work
In this paper, we have shown that four Arabic
colloquial dialects (Gulf, Iraqi, Levantine, and
Egyptian) plus MSA can be distinguished using
a phonotactic approach with good accuracy. The
parallel PRLM approach we employ thus appears
to be effective not only for language identification
but also for Arabic dialect ID.
We have found that the most distinguishable
dialect among the five variants we consider here
is MSA, independent of the duration of the test-
utterance (F-Measure is always above 98.00%).
Egyptian Arabic is second (F-Measure of 90.2%
with 30s test-utterances), followed by Levantine
(F-Measure of 79.4%, with 30s test). The most
confusable dialects are Iraqi and Gulf (F-Measure
of 71.7% and 68.3%, respectively, with 30s test-
utterances). This high degree of Iraqi-Gulf confu-
sion is consistent with some classifications of Iraqi
Arabic as a sub-dialect of Gulf Arabic. We have
obtained a total accuracy of 81.60% in this five-
way classification task when given 30s-duration
utterances. We have also observed that the most
useful phone streams for classification are those
of our Arabic phone recognizers ? typically those
with emphatic vowels.
As mentioned above, the high F-measure for
MSA may be due to the MSA corpora we have
used, which differs in genre from the dialect cor-
pora. Therefore, one focus of our future research
will be to collect MSA data with similar record-
ing conditions to the other dialects to validate
our results. We are also interested in including
prosodic features, such as intonational, durational,
and rhythmic features in our classification. A more
long-term and general goal is to use our results to
improve ASR for cases in which code-switching
occurs between MSA and other dialects.
Acknowledgments
We thank Dan Ellis, Michael Mandel, and Andrew Rosenberg
for useful discussions. This material is based upon work sup-
ported by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023 (approved
for public release, distribution unlimited). Any opinions,
findings and conclusions or recommendations expressed in
this material are those of the authors and do not necessarily
reflect the views of DARPA.
60
References
F. S. Alorfi. 2008. PhD Dissertation: Automatic Identifica-
tion Of Arabic Dialects Using Hidden Markov Models. In
University of Pittsburgh.
Appen Pty Ltd. 2006a. Gulf Arabic Conversational Tele-
phone Speech Linguistic Data Consortium, Philadelphia.
Appen Pty Ltd. 2006b. Iraqi Arabic Conversational Tele-
phone Speech Linguistic Data Consortium, Philadelphia.
M. Barkat, J. Ohala, and F. Pellegrino. 1999. Prosody as a
Distinctive Feature for the Discrimination of Arabic Di-
alects. In Proceedings of Eurospeech?99.
F. Biadsy, N. Habash, and J. Hirschberg. 2009. Improv-
ing the Arabic Pronunciation Dictionary for Phone and
Word Recognition with Linguistically-Based Pronuncia-
tion Rules. In Proceedings of NAACL/HLT 2009, Col-
orado, USA.
A. Canavan and G. Zipperlen. 1996. CALLFRIEND Egyp-
tian Arabic Speech Linguistic Data Consortium, Philadel-
phia.
A. Canavan, G. Zipperlen, and D. Graff. 1997. CALL-
HOME Egyptian Arabic Speech Linguistic Data Consor-
tium, Philadelphia.
J. S. Garofolo et al 1993. TIMIT Acoustic-Phonetic
Continuous Speech Corpus Linguistic Data Consortium,
Philadelphia.
N. Habash. 2006. On Arabic and its Dialects. Multilingual
Magazine, 17(81).
R. Hamdi, M. Barkat-Defradas, E. Ferragne, and F. Pelle-
grino. 2004. Speech Timing and Rhythmic Structure in
Arabic Dialects: A Comparison of Two Approaches. In
Proceedings of Interspeech?04.
C. Holes. 2004. Modern Arabic: Structures, Functions, and
Varieties. Georgetown University Press. Revised Edition.
B. Ma, D. Zhu, and R. Tong. 2006. Chinese Dialect Iden-
tification Using Tone Features Based On Pitch Flux. In
Proceedings of ICASP?06.
M. Maamouri. 2006. Levantine Arabic QT Training Data
Set 5, Speech Linguistic Data Consortium, Philadelphia.
P. Matejka, P. Schwarz, J. Cernocky, and P. Chytil. 2005.
Phonotactic Language Identification using High Quality
Phoneme Recognition. In Proceedings of Eurospeech?05.
Y. K. Muthusamy, R.A. Cole, and B.T. Oshika. 1992. The
OGI Multi-Language Telephone Speech Corpus. In Pro-
ceedings of ICSLP?92.
J. Peters, P. Gilles, P. Auer, and M. Selting. 2002. Iden-
tification of Regional Varieties by Intonational Cues. An
Experimental Study on Hamburg and Berlin German.
45(2):115?139.
F. Ramus. 2002. Acoustic Correlates of Linguistic Rhythm:
Perspectives. In Speech Prosody.
A. Stolcke. 2002. SRILM - an Extensible Language Model-
ing Toolkit. In ICASP?02, pages 901?904.
P. Torres-Carrasquillo, T. P. Gleason, and D. A. Reynolds.
2004. Dialect identification using Gaussian Mixture Mod-
els. In Proceedings of the Speaker and Language Recog-
nition Workshop, Spain.
S. Young, G. Evermann, M. Gales, D. Kershaw, G. Moore,
J. Odell, D. Ollason, D. Povey, V. Valtchev, and P. Wood-
land. 2006. The HTK Book, version 3.4.
M. A. Zissman, T. Gleason, D. Rekart, and B. Losiewicz.
1996. Automatic Dialect Identification of Extempora-
neous Conversational, Latin American Spanish Speech.
In Proceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing, Atlanta, USA.
M. A. Zissman. 1996. Comparison of Four Approaches to
Automatic Language Identification of Telephone Speech.
IEEE Transactions of Speech and Audio Processing, 4(1).
61
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 253?261,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Turn-Yielding Cues in Task-Oriented Dialogue
Agust??n Gravano
Department of Computer Science
Columbia University
New York, NY, USA
agus@cs.columbia.edu
Julia Hirschberg
Department of Computer Science
Columbia University
New York, NY, USA
julia@cs.columbia.edu
Abstract
We examine a number of objective, au-
tomatically computable TURN-YIELDING
CUES ? distinct prosodic, acoustic and
syntactic events in a speaker?s speech that
tend to precede a smooth turn exchange ?
in the Columbia Games Corpus, a large
corpus of task-oriented dialogues. We
show that the likelihood of occurrence of
a turn-taking attempt from the interlocu-
tor increases linearly with the number of
cues conjointly displayed by the speaker.
Our results are important for improving
the coordination of speaking turns in in-
teractive voice-response systems, so that
systems can correctly estimate when the
user is willing to yield the conversational
floor, and so that they can produce their
own turn-yielding cues appropriately.
1 Introduction and Previous Research
Users of state-of-the-art interactive voice response
(IVR) systems often find interactions with these
systems to be unsatisfactory. Part of this reac-
tion is due to deficiencies in speech recognition
and synthesis technologies, but some can also be
traced to coordination problems in the exchange
of speaking turns between system and user (Ward
et al, 2005; Raux et al, 2006). Users are not sure
when the system is ready to end its turn, and sys-
tems are not sure when users are ready to relin-
quish theirs. Currently, the standard method for
determining when a user is willing to yield the
conversational floor is to wait for a silence longer
than a prespecified threshold, typically ranging
from 0.5 to 1 second (Ferrer et al, 2003). How-
ever, this strategy is rarely used by humans, who
rely instead on cues from sources such as syntax,
acoustics and prosody to anticipate turn transitions
(Yngve, 1970). If such TURN-YIELDING CUES
could be modeled and incorporated in IVR sys-
tems, it should be possible to make faster, more
accurate turn-taking decisions, thus leading to a
more fluent interaction. Additionally, a better un-
derstanding of the mechanics of turn-taking could
be used to vary the speech output of IVR systems
to (i) produce turn-yielding cues when the sys-
tem is finished speaking and the user is expected
to speak next, and (ii) avoid producing such cues
when the system has more things to say. In this
paper we examine the existence of turn-yielding
cues in a large corpus of task-oriented dialogues
in Standard American English (SAE).
The question of what types of cues humans ex-
ploit for engaging in synchronized conversation
has been addressed by several studies. Duncan
(1972, inter alia) conjectures that speakers dis-
play complex signals at turn endings, composed
of one or more discrete turn-yielding cues, such
as the completion of a grammatical clause, or any
phrase-final intonation other than a plateau. Dun-
can also hypothesizes that the likelihood of a turn-
taking attempt by the listener increases linearly
with the number of such cues conjointly displayed
by the speaker. Subsequent studies have investi-
gated some of these hypotheses (Ford and Thomp-
son, 1996; Wennerstrom and Siegel, 2003). More
recent studies have investigated how to improve
IVR system?s the turn-taking decisions by incor-
porating some of the features found to correlate
with turn endings (Ferrer et al, 2003; Atterer et
al., 2008; Raux and Eskenazi, 2008). All of these
models are shown to improve over silence-based
techniques for predicting turn endings, motivating
further research. In this paper we present results
253
of a large, corpus-based study of turn-yielding
cues in the Columbia Games Corpus which veri-
fies some of Duncan?s hypotheses and adds addi-
tional cues to turn-taking behavior.
2 Materials and Method
The materials for our study are taken from the
Columbia Games Corpus (Gravano, 2009), a col-
lection of 12 spontaneous task-oriented dyadic
conversations elicited from 13 native speakers of
SAE. In each session, two subjects were paid to
play a series of computer games requiring verbal
communication to achieve joint goals of identify-
ing and moving images on the screen, while seated
in a soundproof booth divided by a curtain to en-
sure that all communication was verbal. The sub-
jects? speech was not restricted in any way, and
the games were not timed. The corpus contains
9 hours of dialogue, which were orthographically
transcribed; words were time-aligned to the source
by hand. Around 5.4 hours have also been into-
nationally transcribed using the ToBI framework
(Beckman and Hirschberg, 1994).
We automatically extracted a number of acous-
tic features from the corpus using the Praat toolkit
(Boersma and Weenink, 2001), including pitch,
intensity and voice quality features. Pitch slopes
were computed by fitting least-squares linear re-
gression models to the F0 track extracted from
given portions of the signal. Part-of-speech
(POS) tags were labeled automatically using Rat-
naparkhi?s maxent tagger trained on a subset of the
Switchboard corpus in lower-case with all punctu-
ation removed, to simulate spoken language tran-
scripts. All speaker normalizations were calcu-
lated using z-scores: z = (x ? ?)/?, where x
is a raw measurement, and ? and ? are the mean
and standard deviation for a speaker.
For our turn-taking studies, we define an
INTER-PAUSAL UNIT (IPU) as a maximal se-
quence of words surrounded by silence longer than
50 ms.1 A TURN then is defined as a maximal se-
quence of IPUs from one speaker, such that be-
tween any two adjacent IPUs there is no speech
from the interlocutor. Boundaries of IPUs and
turns are computed automatically from the time-
aligned transcriptions. Two trained annotators
classified each turn transition in the corpus using a
labeling scheme adapted from Beattie (1982) that
identifies, inter alia, SMOOTH SWITCHES ? tran-
150 ms was identified empirically to avoid stopgaps.
sitions from speaker A to speaker B such that (i)
A manages to complete her utterance, and (ii) no
overlapping speech occurs between the two con-
versational turns. Additionally, all continuations
from one IPU to the next within the same turn
were labeled automatically as HOLD transitions.
The complete labeling scheme is shown in the Ap-
pendix.
Our general approach consists in contrasting
IPUs immediately preceding smooth switches
(S) with IPUs immediately preceding holds (H).
(Note that in this paper we consider only non-
overlapping exchanges.) We hypothesize that
turn-yielding cues are more likely to occur before
S than before H. It is important to emphasize the
optionality of all turn-taking phenomena and de-
cisions: For H, turn-yielding cues ? whatever
their nature ? may still be present; and for S, they
may sometimes be absent. However, we hypothe-
size that their likelihood of occurrence should be
much higher before S. Finally, note that we do
not make claims regarding whether speakers con-
sciously produce turn-yielding cues, or whether
listeners consciously perceive and/or use them to
aid their turn-taking decisions.
3 Individual Turn-Yielding Cues
Figures 1 and 2 show the speaker-normalized
mean of a number of objective, automatically
computed variables for IPUs preceding S and H.
In all cases, one-way ANOVA and Kruskal-Wallis
tests reveal significant differences (at p < 0.001)
between the two groups. We we discuss these re-
sults in detail below.
3.1 Intonation
The literature contains frequent mention of the
propensity of speaking turns to end in any into-
nation contour other than a plateau (a sustained
pitch level, neither rising nor falling). We first
analyze the categorical prosodic labels in the por-
tion of the Columbia Games Corpus annotated us-
ing the ToBI annotations. We tabulate the phrase
S H
H-H% 484 22.1% 513 9.1%
[!]H-L% 289 13.2% 1680 29.9%
L-H% 309 14.1% 646 11.5%
L-L% 1032 47.2% 1387 24.7%
No boundary tone 16 0.7% 1261 22.4%
Other 56 2.6% 136 2.4%
Total 2186 100% 5623 100%
Table 1: ToBI phrase accents and boundary tones.
254
Figure 1: Individual turn-yielding cues: intonation, speaking rate and IPU duration.
accent and boundary tone labels assigned to the
end of each IPU, and compare their distribution
for the S and H turn exchange types, as shown
in Table 1. A chi-square test indicates that there
is a significant departure from a random distribu-
tion (?2=1102.5, df=5, p?0). Only 13.2% of
all IPUs immediately preceding a smooth switch
(S) ? where turn-yielding cues are most likely
present ? end in a plateau ([!]H-L%); most of the
remaining ones end in either a falling pitch (L-L%)
or a high rise (H-H%). For IPUs preceding a hold
(H) the counts approximate a uniform distribution,
with the plateau contours being the most common,
supporting the hypothesis that this contour func-
tions as a TURN-HOLDING CUE (that is, a cue that
typically prevents turn-taking attempts from the
listener). The high counts for the falling contour
preceding a hold (24.7%) may be explained by the
fact that, as discussed above, taking the turn is
optional for the listener, who may choose not to
act despite hearing some turn-yielding cues. It is
not entirely clear what the role is of the low-rising
contour (L-H%), as it occurs in similar proportions
before S and before H. Finally, we note that the ab-
sence of a boundary tone works as a strong indi-
cation that the speaker has not finished speaking,
since nearly all (98%) IPUs without a boundary
tone precede a hold transition.
Next, we examine four objective acoustic ap-
proximations of this perceptual feature: the ab-
solute value of the speaker-normalized F0 slope,
both raw and stylized, computed over the final 200
and 300 ms of each IPU. The case of a plateau
corresponds to a value of F0 slope close to zero;
the other case, of either a rising or a falling pitch,
corresponds to a high absolute value of F0 slope.
As shown in Figure 1, we find that the final slope
before S is significantly higher than before H in
all four cases. These findings provide additional
support to the hypothesis that turns tend to end in
falling and high-rising final intonations, and pro-
vide automatically identifiable indicators of this
turn-yielding cue.
3.2 Speaking rate
Duncan (1972) hypothesizes a ?drawl on the fi-
nal syllable or on the stressed syllable of a termi-
nal clause? [p. 287] as a turn-yielding cue, which
would probably correspond to a noticeable de-
crease in speaking rate. We examine this hypothe-
sis in our corpus using two common definitions of
speaking rate: syllables per second and phonemes
per second. Syllable and phoneme counts were
estimated from dictionary lookup, and word dura-
tions were extracted from the manual orthographic
alignments. Figure 1 shows that both measures,
computed over either the whole IPU or its final
word, are significantly higher before S than be-
fore H, which indicates an increase in speaking
rate before turn boundaries rather than Duncan?s
hypothesized drawl.
Furthermore, the speaking rate is, in both cases
(before S and before H), significantly slower on
the final word than over the whole IPU, a finding
that is in line with phonological theories that pre-
dict a segmental lengthening near prosodic phrase
boundaries (Wightman et al, 1992). This finding
may indeed correspond to the drawl or lengthen-
ing described by Duncan before turn boundaries.
However, it seems to be the case ? at least for
our corpus ? that the final lengthening tends to
occur at all phrase final positions, not just at turn
endings. In fact, our results indicate that the fi-
nal lengthening is more prominent in turn-medial
IPUs than in turn-final ones.
255
Figure 2: Individual turn-yielding cues: intensity, pitch and voice quality.
3.3 IPU duration and acoustic cues
In the Columbia Games Corpus, we find that turn-
final IPUs tend to be significantly longer than turn-
medial ones, both when measured in seconds and
in number of words (Figure 1). This suggests that
IPU duration could function as a turn-yielding cue,
supporting similar findings in perceptual experi-
ments by Cutler and Pearson (1986).
We also find that IPUs followed by S have a
mean intensity significantly lower than those fol-
lowed by H (computed over the IPU-final 500 and
1000 ms, see Figure 2). Also, the differences in-
crease when moving towards the end of the IPU.
This suggests that speakers tend to lower their
voices when approaching potential turn bound-
aries, whereas they reach turn-internal pauses with
a higher intensity.
Phonological theories conjecture a declination
in the pitch level, which tends to decrease grad-
ually within utterances, and across utterances
within the same discourse segment, as a conse-
quence of a gradual compression of the pitch range
(Pierrehumbert and Hirschberg, 1990). For con-
versational turns, then, we would expect to find
that speakers tend to lower their pitch level as they
reach potential turn boundaries. This hypothesis
is verified by the dialogues in our corpus, where
we find that IPUs preceding S have a significantly
lower mean pitch than those preceding H (Figure
2). In consequence, pitch level may also work as a
turn-yielding cue.
Next we examine three acoustic features asso-
ciated with the perception of voice quality: jit-
ter, shimmer and noise-to-harmonics ratio (NHR)
(Bhuta et al, 2004), computed over the IPU-final
500 and 1000 ms (Figure 2). We compute jit-
ter and shimmer only over voiced frames for im-
proved robustness. For all three features, the mean
value for IPUs preceding S is significantly higher
than for IPUs preceding H, with the difference in-
creasing towards the end of the IPU. Therefore,
voice quality seems to play a clear role as a turn-
yielding cue.
3.4 Lexical cues
Stereotyped expressions such as you know or I
think have been proposed in the literature as lex-
ical turn-yielding cues. However, in the Games
Corpus we find that none of the most frequent
IPU-final unigrams and bigrams, both preceding
S and H, correspond to such expressions (see Ta-
ble A.1 in the Appendix). Instead, such unigrams
and bigrams are specific to the computer games
in which the subjects participated. For example,
the game objects tended to be spontaneously de-
scribed by subjects from top to bottom and from
left to right, as shown in the following excerpt
(pauses are indicated with #):
A: I have a blue lion on top # with a lemon
in the bottom left # and a yellow crescent
moon in- # i- # in the bottom right
B: oh okay [...]
In consequence, bigrams such as lower right and
bottom right are common before S, while on top
or bottom left are common before H. These are all
task-specific lexical constructions and do not con-
stitute stereotyped expressions in the traditional
sense.
Also very common among the most frequent
IPU-final expressions are AFFIRMATIVE CUE
WORDS ? heavily overloaded words, such as
okay or yeah, that are used both to initiate and
to end discourse segments, among other functions
(Gravano et al, 2007). The occurrence of these
words does not constitute a turn-yielding or turn-
holding cue per se; rather, additional contextual,
acoustic and prosodic information is needed to dis-
ambiguate their meaning.
256
While we do not find clear examples of lexical
turn-yielding cues in our task-oriented corpus, we
do find two lexical turn-holding cues: word frag-
ments (e.g., incompl-) and filled pauses (e.g., uh,
um). Of the 8123 IPUs preceding H, 6.7% end
in a word fragment, and 9.4% in a filled pause.
By constrast, only 0.3% of the 3246 IPUs preced-
ing S end in a word fragment, and 1% in a filled
pause. These differences suggest that, after either
a word fragment or a filled pause, the speaker is
much more likely to intend to continue holding
the floor. This notion of disfluencies functioning
as a turn-taking cue has been studied by Goodwin
(1981), who shows that they may be used to secure
the listener?s attention at turn beginnings.
3.5 Textual completion
Several authors (Duncan, 1972; Ford and Thomp-
son, 1996; Wennerstrom and Siegel, 2003) claim
that some form of syntactic or semantic comple-
tion, independent of intonation and interactional
import, functions as a turn-yielding cue. Although
some call this syntactic completion, since all au-
thors acknowledge the need for semantic and dis-
course information in judging it, we choose the
more neutral term TEXTUAL COMPLETION for
this phenomenon. We annotated a portion of
our corpus with respect to textual completion and
trained a machine learning (ML) classifier to auto-
matically label the whole corpus. From these an-
notations we then examined how textual comple-
tion labels relate to turn-taking categories in the
corpus.
3.5.1. Manual labeling: In conversation, lis-
teners judge textual completion incrementally and
without access to later material. To simulate these
conditions in the labeling task, annotators were
asked to judge the textual completion of a turn up
to a target pause from the written transcript alone,
without listening to the speech. They were al-
lowed to read the transcript of the full previous
turn by the other speaker (if any), but they were
not given access to anything after the target pause.
These are two sample tokens:
A: the lion?s left paw our front
B: yeah and it?s th- right so the
A: and then a tea kettle and then the wine
B: okay well I have the big shoe and the wine
We selected 400 tokens at random from the Games
Corpus; the target pauses were also chosen at ran-
dom. Three annotators labeled each token inde-
pendently as either complete or incomplete ac-
cording to these guidelines: Determine whether
you believe what speaker B has said up to this
point could constitute a complete response to what
speaker A has said in the previous turn/segment.
Note: If there are no words by A, then B is begin-
ning a new task, such as describing a card or the
location of an object. To avoid biasing the results,
annotators were not given the turn-taking labels of
the tokens. Inter-annotator reliability is measured
by Fleiss? ? at 0.814, which corresponds to the
?almost perfect? agreement category. The mean
pairwise agreement between the three subjects is
90.8%. For the cases in which there is disagree-
ment between the three annotators, we adopt the
MAJORITY LABEL as our gold standard; that is,
the label chosen by two annotators.
3.5.2. Automatic classification: Next, we
trained a ML model using the 400 manually anno-
tated tokens as training data to automatically clas-
sify all IPUs in the corpus as either complete or in-
complete. For each IPU we extracted a number of
lexical and syntactic features from the current turn
up to the IPU itself: lexical identity of the IPU-
final word (w); POS tags and simplified POS tags
(N, V, Adj, Adv, Other) of w and of the IPU-final
bigram; number of words in the IPU; a binary flag
indicating if w is a word fragment; size and type of
the biggest (bp) and smallest (sp) phrase that end
in w; binary flags indicating if each of bp and sp is
a major phrase (NP, VP, PP, ADJP, ADVP); binary
flags indicating if w is the head of each of bp and
sp. We chose these features in order to capture as
much lexical and syntactic information as possible
from the transcripts. The syntactic features were
computed using two different parsers: the Collins
statistical parser (Collins, 2003) and CASS, a par-
tial parser especially designed for use with noisy
text (Abney, 1996). We experimented with the
learners listed in Table 2, using the implementa-
tions provided in the WEKA ML toolkit (Witten
and Frank, 2000). Table 2 shows the accuracy of
the majority-class baseline and of each classifier,
using 10-fold cross validation on the 400 train-
ing data points, and the mean pairwise agreement
by the three human labelers. The linear-kernel
support-vector-machine (SVM) classifier achieves
the highest accuracy, significantly outperforming
the baseline, and approaching the mean agreement
of human labelers.
257
Classifier Accuracy
Majority-class (?complete?) 55.2%
C4.5 (decision trees) 55.2%
Ripper (propositional rules) 68.2%
Bayesian networks 75.7%
SVM, RBF kernel (c = 1, ? = 10?12) 78.2%
SVM, linear kernel (c = 1, ? = 10?12) 80.0%
Human labelers (mean agreement) 90.8%
Table 2: Textual completion: ML results.
3.5.3. Results: First we examine the tokens that
were manually labeled by the human annotators.
Of the 100 tokens followed by S, 91 were labeled
textually complete, a significantly higher propor-
tion than the 42% followed by H that were labeled
complete (?2=51.7, df=1, p?0). Next, we used
our highest performing classifier, the linear-kernel
SVM, to automatically label all IPUs in the cor-
pus. Of the 3246 IPUs preceding S, 2649 (81.6%)
were labeled textually complete, and about half of
all IPUs preceding H (4272/8123, or 52.6%) were
labeled complete. The difference is also signifi-
cant (?2 =818.7, df = 1, p? 0). These results
suggest that textual completion as defined above
constitutes a necessary, but not sufficient, turn-
yielding cue.
4 Combining Turn-Yielding Cues
So far, we have shown strong evidence supporting
the existence of individual acoustic, prosodic and
textual turn-yielding cues. Now we shift our atten-
tion to the manner in which they combine together
to form more complex turn-yielding signals. For
each individual cue type, we choose two or three
features shown to correlate strongly with smooth
switches, as shown in Table 3 (e.g., the speaking
rate cue is represented by two automatic features:
syllables and phonemes per second over the whole
IPU). We consider a cue c to be PRESENT on IPU
u if, for any feature f modeling c, the value of f
on u is closer to fS than to fH , where fS and fH
are the mean values of f across all IPUs preced-
ing S and H, respectively. Otherwise, we say c is
ABSENT on u. Also, we automatically annotate all
IPUs in the corpus for textual completion using the
linear-kernel SVM classifier described in Section
3.5. IPUs classified as complete are considered to
bear the textual completion turn-yielding cue.
We first analyze the frequency of occurrence
of conjoined individual turn-yielding cues. Ta-
ble 4 shows the top frequencies of complex turn-
yielding cues for IPUs immediately before smooth
Individual cues Automatic features
Intonation
Abs(F0 slope) over IPU-final 200 ms
Abs(F0 slope) over IPU-final 300 ms
Speaking rate
Syllables per second over whole IPU
Phonemes per second over whole IPU
Intensity level
Mean intensity over IPU-final 500 ms
Mean intensity over IPU-final 1000 ms
Pitch level
Mean pitch over IPU-final 500 ms
Mean pitch over IPU-final 1000 ms
IPU duration
IPU duration in ms
Number of words in IPU
Voice quality
Jitter over IPU-final 500 ms
Shimmer over IPU-final 500 ms
NHR over IPU-final 500 ms
Table 3: Features used to estimate the presence of
individual turn-yielding cues.
switches (S) and holds (H). The most frequent
cases before S correspond to all, or almost all, cues
present at once. For IPUs preceding a hold (H),
the opposite is true: those with no cues, or with
just one or two, represent the most frequent cases.
S H
Cues Count Cues Count
1234567 267 ...4... 392
.234567 226 ......7 247
1234.67 138 ....... 223
.234.67 109 ...4..7 218
.23..67 98 ...45.. 178
..34567 94 .2....7 166
123..67 93 1234.67 163
.2.4567 73 .2..5.7 157
... ...
Total 3246 Total 8123
Table 4: Top frequencies of complex turn-yielding
cues for IPUs preceding S and H. A digit indicates
the presence of a specific cue; a dot, its absence.
1: Intonation; 2: Speaking rate; 3: Intensity level;
4: Pitch level; 5: IPU duration; 6: Voice quality;
7: Textual completion.
Table 5 shows the same results, now grouping
together all IPUs with the same number of cues,
independently of the cue types. Again, we observe
that larger proportions of IPUs preceding S present
more conjoined cues than IPUs preceding H.
Next we look at how the likelihood of a turn-
taking attempt varies with respect to the number
of individual cues displayed by the speaker, a rela-
tion hypothesized to be linear by Duncan (1972).
Figure 3 shows the proportion of IPUs with 0-7
cues present that are followed by a turn-taking at-
tempt from the interlocutor.2 The dashed line cor-
2 The proportion of turn-taking attempts is computed for
each cue count as the number of S and PI divided by the num-
ber of S, PI, H and BC, according to our labeling scheme.
258
Cue count S H
0 4 0.1% 223 2.7%
1 52 1.6% 970 11.9%
2 241 7.4% 1552 19.1%
3 518 16.0% 1829 22.5%
4 740 22.8% 1666 20.5%
5 830 25.6% 1142 14.1%
6 594 18.3% 611 7.5%
7 267 8.2% 130 1.6%
Total 3246 100% 8123 100%
Table 5: Distribution of the number of turn-
yielding cues displayed in IPUs preceding smooth
switches (S) and hold transitions (H).
Figure 3: Percentage of turn-taking attempts from
the listener (either S or PI) following IPUs con-
taining 0-7 turn-yielding cues.
responds to a linear model fitted to the data (Pear-
son?s correlation test: r2 = 0.969), and the contin-
uous line, to a quadratic model (r2 = 0.995). The
high correlation coefficient of the linear model
supports Duncan?s hypothesis, that the likelihood
of a turn-taking attempt by the interlocutor in-
creases linearly with the number of individual cues
displayed by the speaker. However, an ANOVA test
reveals that the quadratic model fits the data sig-
nificantly better than the linear model (F (1, 5) =
23.01; p = 0.005), even though the curvature of
the quadratic model is only moderate, as can be
observed in the figure.
5 Speaker Variation
To investigate possible speaker dependence in our
turn-yielding cues, we examine evidence for each
cue for each of our thirteen speakers. Table 6
summarizes this data. For each speaker, a check
(
?
) indicates that there is significant evidence of
the speaker producing the corresponding individ-
ual turn-yielding cue (at p < 0.05, using the same
statistical tests described in the previous sections).
Five speakers show evidence of all seven cues,
Speaker 101 102 103 104 105 106 107 108 109 110 111 112 113
Intonation
? ? ? ? ? ? ? ? ? ? ?
Spk. rate
? ? ? ? ? ? ? ? ? ? ? ? ?
Intensity
? ? ? ? ? ? ? ? ? ? ? ?
Pitch
? ? ? ? ? ? ?
Completion
? ? ? ? ? ? ? ? ? ? ? ? ?
Voice quality
? ? ? ? ? ? ? ? ? ? ? ? ?
IPU duration
? ? ? ? ? ? ? ? ? ? ? ? ?
LM r2 .92 .93 .82 .88 .97 .96 .95 .95 .97 .91 .95 .97 .89
QM r2 .98 .95 .95 .92 .98 .98 .96 .95 .99 .94 .98 .99 .90
Table 6: Summary of results for each individual
speaker.
while the remaining eight speakers show either
five or six cues. Pitch level is the least reliable
cue, present only for seven subjects. Notably, the
cues related to speaking rate, textual completion,
voice quality, and IPU duration are present for all
thirteen speakers.
The two bottom rows in Table 6 show the cor-
relation coefficients (r2) of linear and quadratic
regressions performed on the data from each
speaker. In all cases, the coefficients are very high.
The fit of the quadratic model is significantly bet-
ter for six speakers (shown in bold typeface); for
the remaining seven speakers, both models pro-
vide statistically indistinguishable explanations of
the data.
6 Discussion
We have examined seven turn-yielding cues ?
i.e., seven measurable events that take place with
a significantly higher frequency on IPUs preced-
ing smooth turn switches than on IPUs preceding
hold transitions. These events may be summarized
as follows: (i) a falling or high-rising intonation at
the end of the IPU; (ii) an increased speaking rate;
(iii) a lower intensity level; (iv) a lower pitch level;
(v) a longer IPU duration; (vi) a higher value of
three voice quality features: jitter, shimmer, and
NHR; and (vii) a point of textual completion. We
have also shown that, when several turn-yielding
cues occur simultaneously, the likelihood of a sub-
sequent turn-taking attempt by the interlocutor in-
creases in an almost linear fashion.
We propose that these findings can be used to
improve some turn-taking decisions of state-of-
the-art IVR systems. For example, if a system
wishes to yield the floor to a user, it should in-
clude in its output as many of the described cues
as possible. Conversely, when the user is speak-
ing, the system may detect appropriate moments
to take the turn by estimating the presence of turn-
259
yielding cues at every silence. If the number of de-
tected cues is high enough, then the system should
take the turn; otherwise, it should remain silent.
Two assumptions of our study are that turn-
yielding cues are binary and all contribute equally
to the overall ?count?. In future research we
will explore alternative methods of combining and
weighting the different features ? by means of
multiple linear regression, for example ? in or-
der to experiment with more sophisticated models
of turn-yielding behavior. We also plan to exam-
ine new turn-yielding cues, paying special atten-
tion to additional voice quality features, given the
promising results obtained for jitter, shimmer and
noise-to-harmonics ratio.
7 Acknowledgements
This work was funded in part by NSF IIS-
0307905. We thank Stefan Benus, Enrique Hen-
estroza, Elisa Sneed and Gregory Ward, for valu-
able discussion and for their help in collecting and
labeling the data, and the anonymous reviewers for
helpful comments and suggestions.
References
S. Abney. 1996. Partial parsing via finite-state cas-
cades. Journal of Natural Language Engineering,
2(4):337?344.
M. Atterer, T. Baumann, and D. Schlangen. 2008. To-
wards incremental end-of-utterance detection in di-
alogue systems. In Proceedings of Coling, Manch-
ester, UK.
G. W. Beattie. 1982. Turn-taking and interruption
in political interviews: Margaret Thatcher and Jim
Callaghan compared and contrasted. Semiotica,
39(1/2):93?114.
M. E. Beckman and J. Hirschberg. 1994. The ToBI
annotation conventions. Ohio State University.
T. Bhuta, L. Patrick, and J. D. Garnett. 2004. Per-
ceptual evaluation of voice quality and its correla-
tion with acoustic measurements. Journal of Voice,
18(3):299?304.
P. Boersma and D. Weenink. 2001. Praat: Doing pho-
netics by computer. http://www.praat.org.
M. J. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics, 29(4):589?637.
E. A. Cutler and M. Pearson. 1986. On the analysis of
prosodic turn-taking cues. In C. Johns-Lewis, Ed.,
Intonation in Discourse, pp. 139?156. College-Hill.
S. Duncan. 1972. Some signals and rules for taking
speaking turns in conversations. Journal of Person-
ality and Social Psychology, 23(2):283?292.
L. Ferrer, E. Shriberg, and A. Stolcke. 2003. A
prosody-based approach to end-of-utterance detec-
tion that does not require speech recognition. In
Proceedings of ICASSP.
C. E. Ford and S. A. Thompson. 1996. Interactional
units in conversation: Syntactic, intonational and
pragmatic resources for the management of turns. In
E. Ochs, E. A. Schegloff, and S. A. Thompson, Eds.,
Interaction and Grammar, pp. 134?184. Cambridge
University Press.
C. Goodwin. 1981. Conversational Organization:
Interaction between Speakers and Hearers. Aca-
demic Press.
A. Gravano, S. Benus, J. Hirschberg, S. Mitchell, and
I. Vovsha. 2007. Classification of discourse func-
tions of affirmative words in spoken dialogue. In
Proceedings of Interspeech.
A. Gravano. 2009. Turn-Taking and Affirmative Cue
Words in Task-Oriented Dialogue. Ph.D. thesis,
Columbia University, New York.
J. Pierrehumbert and J. Hirschberg. 1990. The mean-
ing of intonational contours in the interpretation of
discourse. In P. Cohen, J. Morgan, and M. Pol-
lack, Eds., Intentions in Communication, pp. 271?
311. MIT Pr.
A. Raux and M. Eskenazi. 2008. Optimizing endpoint-
ing thresholds using dialogue features in a spoken
dialogue system. In Proceedings of SIGdial.
A. Raux, D. Bohus, B. Langner, A. W. Black, and
M. Eskenazi. 2006. Doing research on a deployed
spoken dialogue system: One year of Let?s Go! ex-
perience. In Proceedings of Interspeech.
N. G. Ward, A. G. Rivera, K. Ward, and D. G. Novick.
2005. Root causes of lost time and user stress in
a simple dialog system. In Proceedings of Inter-
speech.
A. Wennerstrom and A. F. Siegel. 2003. Keeping the
floor in multiparty conversations: Intonation, syn-
tax, and pause. Discourse Processes, 36(2):77?107.
C. W. Wightman, S. Shattuck-Hufnagel, M. Ostendorf,
and P. J. Price. 1992. Segmental durations in the
vicinity of prosodic phrase boundaries. The Journal
of the Acoustical Society of America, 91:1707.
I. H. Witten and E. Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
V. H. Yngve. 1970. On getting a word in edgewise.
Sixth Regional Meeting of the Chicago Linguistic
Society, 6:657?677.
260
For each turn by speaker S2, where S1 is the other speaker, label S2?s turn as follows:
Is S2?s utterance in response to S1?s utterance and indicates only
?I?m still here / I hear you and please continue??(1)


no
Simultaneous speech present?


yes
S2 is successful?


yes
S1?s utterance
complete?(2)

yes
Overlap
(O)
HHH
Hj
no
Interruption
(I)
H
HHHj
no
Butting-in
(BI)
HH
HHj
no
S1?s utterance
complete?(2)


yes
Smooth
switch (S)
HH
HHj
no
Pause interruption
(PI)
HH
HHj
yes
Simultaneous speech present?


yes
Backchannel
with overlap
(BC O)
HHH
Hj
no
Backchannel
(BC)
Figure A.1: Turn-taking labeling scheme.
Appendix: Turn-Taking Labeling Scheme
We adopt a slightly modified version of Beat-
tie?s (1982) labeling scheme, depicted in Fig-
ure A.1. We incorporate backchannels (excluded
from Beattie?s study) by adding the decision
marked (1) at the root of the decision tree, for
which we use the annotations described in Gra-
vano et al (2007). For the decision marked (2), we
use Beattie?s informal definition of utterance com-
pleteness: ?Completeness [is] judged intuitively,
taking into account the intonation, syntax, and
meaning of the utterance? [p. 100]. All continu-
ations from one IPU to the next within the same
turn are labeled automatically H, for ?hold?. Also,
we identify three special cases that do not corre-
spond to actual turn exchanges:
Task beginnings: Turns beginning a new game
task are labeled X1.
Continuations after BC or BC O: If a turn t is a
continuation after a backchannel b from the other
speaker, it is labeled X2 O if t and b overlap, or
X2 if not.
Simultaneous starts: Fry (1975) reports that hu-
mans require at least 210 ms to react verbally to a
verbal stimulus.3 Thus, if two turns begin within
210 ms of each other, they are most probably con-
nected to preceding events than to one another. In
Figure A.2, A1, A2 and B1 represent turns from
speakers A and B. Most likely, A2 is simply a
continuation from A1, and B1 occurs in response
3D. B. Fry. 1975. Simple reaction-times to speech and
non-speech stimuli. Cortex, 11(4):355-60.
to A1. Thus, B1 is labeled with respect to A1 (not
A2), and A2 is labeled X3.
A1 A2x
B1y
Figure A.2: Simultaneous start (|y?x| < 210ms).
S Count H Count
okay 241 okay 402
yeah 167 on top 172
lower right 85 um 136
bottom right 74 the top 117
the right 59 of the 67
hand corner 52 blue lion 57
lower left 43 bottom left 56
the iron 37 with the 54
the onion 33 the um 54
bottom left 31 yeah 53
the ruler 30 the left 48
mm-hm 30 and 48
right 28 lower left 46
right corner 27 uh 45
the bottom 26 oh 45
the left 24 and a 45
crescent moon 23 alright 44
the lemon 22 okay um 43
the moon 20 the uh 42
tennis racket 20 the right 41
blue lion 19 the bottom 39
the whale 18 I have 39
the crescent 18 yellow lion 37
the middle 17 the middle 37
of it 17 I?ve got 34
... ...
Total 3246 Total 8123
Table A.1: 25 most frequent final bigrams preced-
ing smooth turn switches (S) and hold transitions
(H). (See Section 3.4.)
261
Affirmative Cue Words in Task-Oriented Dialogue
Agust?n Gravano?
Universidad de Buenos Aires
Julia Hirschberg??
Columbia University
?tefan Ben?u??
Constantine the Philosopher University
and Institute of Informatics,
Slovak Academy of Sciences
We present a series of studies of affirmative cue words?a family of cue words such as ?okay? or
?alright" that speakers use frequently in conversation. These words pose a challenge for spoken
dialogue systems because of their ambiguity: They may be used for agreeing with what the in-
terlocutor has said, indicating continued attention, or for cueing the start of a new topic, among
other meanings. We describe differences in the acoustic/prosodic realization of such functions in
a corpus of spontaneous, task-oriented dialogues in Standard American English. These results
are important both for interpretation and for production in spoken language applications. We
also assess the predictive power of computational methods for the automatic disambiguation of
these words. We find that contextual information and final intonation figure as the most salient
cues to automatic disambiguation.
1. Introduction
CUE PHRASES are linguistic expressions that may be used to convey explicit information
about the discourse or dialogue, or to convey a more literal, semantic contribution.
They aid speakers and writers in organizing the discourse, and listeners and readers in
processing it. In previous literature, these constructions have also been termed discourse
markers, pragmatic connectives, discourse operators, and clue words. Examples of cue
phrases include now, well, so, and, but, then, after all, furthermore, however, in consequence,
as a matter of fact, in fact, actually, okay, alright, for example, and incidentally.
The ability to correctly determine the function of cue phrases is critical for important
natural language processing tasks, including anaphora resolution (Grosz and Sidner
1986), argument understanding (Cohen 1984), plan recognition (Grosz and Sidner 1986;
Litman and Allen 1987), and discourse segmentation (Litman and Passonneau 1995).
? Departamento de Computaci?n, FCEyN, Universidad de Buenos Aires, Pabell?n I, Ciudad Universitaria,
(C1428EGA) Buenos Aires, ARGENTINA. E-mail: gravano@dc.uba.ar.
?? E-mail: julia@cs.columbia.edu.
? E-mail: sbenus@ukf.sk.
Submission received: 9 December 2009; revised submission received: 2 February 2011; accepted for
publication: 13 March 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 1
Furthermore, correctly determining the function of cue phrases using features of the
surrounding text can be used to improve the naturalness of synthetic speech in text-to-
speech systems (Hirschberg 1990).
In this study, we focus on a subclass of cue phrases that we term affirmative
cue words (hereafter, ACWs), and that include alright, mm-hm, okay, right, and
uh-huh, inter alia. These words are frequent in spontaneous conversation, especially in
task-oriented dialogue, and are heavily overloaded: Their possible discourse/pragmatic
functions include agreeing with what the interlocutor has said, displaying interest and
continued attention, and cueing the start of a new topic. Some ACWs (e.g., alright, okay)
are capable of conveying as many as ten different functions, as described in Section 3.
Whereas ACWs thus form a subset of more general classes of utterances which have
been studied in more general studies of cue words, cue phrases, discourse markers, feedback
utterances, linguistic feedback, acknowledgments, grounding acts, our focus is on this par-
ticular subset of lexical items which may convey an affirmative response?but which
may also convey many different meanings. The disambiguation of these meanings we
believe is critical to the success of spoken dialogue systems.
In the studies presented here, our goal is to extend our understanding of ACWs,
in particular by finding descriptions of the acoustic/prosodic characteristics of their
different functions, and by assessing the predictive power of computational methods for
their automatic disambiguation. This knowledge should be helpful in spoken language
generation and understanding tasks, including interactive spoken dialogue systems and
applications doing off-line analyses of conversational data, such as meeting segmenta-
tion and summarization. For example, spoken dialogue systems lacking a model of the
appropriate realization of different uses of these words are likely to have difficulty in
understanding and communicating with their users, either by producing cue phrases
in a way that does not convey the intended meaning or by misunderstanding users?
productions.
This article is organized as follows. Section 2 reviews previous literature. In Sec-
tion 3 we describe the materials used in the present study from the Columbia Games
Corpus. Section 4 presents a statistical description of the acoustic, prosodic, and con-
textual characteristics of the functions of ACWs in this corpus. In Section 5 we describe
results from a number of machine learning experiments aimed at investigating how
accurately ACWs may be automatically classified into their various functions. Finally,
in Section 6 we summarize and discuss our main findings.
2. Previous Work
Cue phrases have received extensive attention in the computational linguistics litera-
ture. Early work by Cohen (1984) presents a computational justification for the impor-
tance of cue phrases in discourse processing. Using a simple propositional framework
for analyzing discourse, Cohen claims that, in some cases, cue phrases decrease the
number of operations required by the listener to process ?coherent transmissions?; in
other cases, cue phrases are necessary to allow the recognition of ?transmissions which
would be incoherent (too complex to reconstruct) in the absence of clues? (page 251).
Reichman (1985) proposes a model of discourse structure in which discourse com-
prises a collection of basic constituents called context spaces, organized hierarchically
according to semantic and logical relations called conversational moves. In Reichman?s
model, cue phrases are portrayed as mechanisms that signal context space boundaries,
specifying the kind of conversational move about to take place. Grosz and Sidner (1986)
introduce an alternative model of discourse structure formed by three interrelated
2
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
components: a linguistic structure, an intentional structure, and an attentional state. In
this model, cue phrases play a central role, allowing the speaker to provide information
about all of the following to the listener:
1) that a change of attention is imminent; 2) whether the change returns to a previous
focus space or creates a new one; 3) how the intention is related to other intentions;
4) what precedence relationships, if any, are relevant (page 196).
In a corpus study of spontaneous conversations, Schiffrin (1987) describes cue phrases
as syntactically detachable from a sentence, commonly used in initial position within
utterances, capable of operating at both local and global levels of discourse, and having
a range of prosodic contours. As other authors, Schiffrin observes that cue phrases
provide contextual coordinates for an utterance in the discourse?that is, they indicate
the discourse segment to which an utterance belongs. However, she suggests that cue
phrases only display discourse structure relations; they do not create them. In a critique
of Schiffrin?s work, Redeker (1991) proposes defining cue phrases as phrases ?uttered
with the primary function of bringing to the listener?s attention a particular kind of
linkage of the upcoming utterance with the immediate discourse context? (page 1169).
Prior work on the automatic classification of cue phrases includes a series of studies
performed by Hirschberg and Litman (Hirschberg and Litman 1987, 1993; Litman and
Hirschberg 1990), which focus on differentiating between the discourse and sentential
senses of single-word cue phrases such as now, well, okay, say, and so in American
English. When used in a discourse sense, a cue phrase explicitly conveys information
about the discourse structure; when used in a sentential sense, a cue phrase instead
conveys semantic information. Hirschberg and Litman present two manually devel-
oped classification models, one based on prosodic features, and one based on textual
features. This line of research is further pursued by Litman (1994, 1996), who incorpo-
rates machine learning techniques to derive classification models automatically. Litman
uses different combinations of prosodic and text-based features to train decision-tree
and rule learners, and shows that machine learning constitutes a powerful tool for
developing automatic classifiers of cue phrases into their sentential and discourse uses.
Zufferey and Popescu-Belis (2004) present a similar study on the automatic classification
of like and well into discourse and sentential senses, achieving a performance close to
that of human annotators.
Besides the binary division of cue phrases into discourse vs. sentential meanings,
the Conversational Analysis (CA) literature describes items it terms linguistic feedback
or acknowledgments. These include not only the computational linguists? cue phrases
but also expressions such as I see or oh wow, which CA research describes in terms of
attention, understanding, and acceptance by the speaker of a proposition uttered by
another conversation participant (Kendon 1967; Yngve 1970; Duncan 1972; Schegloff
1982; Jefferson 1984). Such items typically occur at the second position in common
adjacency pairs and include backchannels (also referred to as continuers), which
?exhibit on the part of [their] producer an understanding that an extended unit of talk
is underway by another, and that it is not yet, or may not be (even ought not yet be)
complete; [they take] the stance that the speaker of that extended unit should continue
talking? (Schegloff 1982, page 81), and agreements, which indicate the speaker?s
agreement with a statement or opinion expressed by another speaker. Allwood,
Nivre, and Ahlsen (1992) distinguish four basic communicative functions of linguistic
feedback which enable conversational partners to exchange information: contact,
perception, understanding, and attitudinal reactions. These correspond respectively
3
Computational Linguistics Volume 38, Number 1
to whether the interlocutor is willing and able to continue the interaction, perceive the
message, understand the message, and react and respond to the message. Allwood,
Nivre, and Ahlsen posit that ?simple feedback words, like yes, [...] involve a high
degree of context dependence? (page 5), and suggest that their basic communicative
function strongly depends on the type of speech act, factual polarity, and information
status of the immediately preceding communicative act. Novick and Sutton (1994)
propose an alternative categorization of linguistic feedback in task-oriented dialogue,
which is based on the structural context of exchanges rather than on the characteristics
of the preceding utterance. The three main classes in Novick and Sutton?s catalogue
are: (i) other ? ackn, where an acknowledgment immediately follows a contribution by
other speaker; (ii) self ? other ? ackn, where self initiates an exchange, other eventually
completes it, and self utters an acknowledgment; and (iii) self + ackn, where self includes
an acknowledgment in an utterance independently of other?s previous contribution.
Substantial attention has been paid to subsets and supersets of words we include
in our class of ACWs in the psycholinguistic literature in studies of grounding?
the process by which conversants obtain and maintain a common ground of mutual
knowledge, mutual beliefs, and mutual assumptions over the course of a conversation
(Clark and Schaefer 1989; Clark and Brennan 1991). Computational work on grounding
has been pursued for a number of years by Traum and colleagues (e.g., Traum and Allen
1992; Traum 1994), who recently have described a corpus-based study of lexical and
semantic evidence supporting different degrees of grounding (Roque and Traum 2009).
Our ACWs often occur in the process of establishing such common ground.
Prosodic characteristics of the responses involved in grounding have been studied in
the Australian English Map Task corpus by Mushin et al (2003), who find that these
utterances often consist of acknowledgment contributions such as okay or yeh produced
with a ?non-final? intonational contour, and followed by speech by the same speaker
which appears to continue the intonational phrase. Studies by Walker of informa-
tionally redundant utterances (IRUs) (Walker 1992, 1996), utterances which express
?a proposition already entailed, presupposed or implicated by a previous utterance
in the same discourse situation? (Walker 1993a, page 12), also include some of our
ACWs, such as IRU prompts (e.g., uh-huh), which, according to Walker, ?add no new
propositional content to the common ground? (Walker 1993a, page 32). Walker adopts
the term ?continuer? from the Conversational Analysis school to further describe these
prompts (Walker 1993a). Walker describes some intonational contours which are used
to realize IRUs in generation in Walker (1993a) and in Walker (1993b), examining 63 IRU
tokens and finding five different types of contour used among them.
As part of a larger project on automatically detecting discourse structure for speech
recognition and understanding tasks in American English, Jurafsky et al (1998) present
a study of four particular discourse/pragmatic functions, or dialog acts (Bunt 1989;
Core and Allen 1997), closely related to ACWs: backchannel, agreement, incipient
speakership (indicating an intention to take the floor), and yes-answer (affirmative
answer to a yes?no question). The authors examine 1,155 conversations from the Switch-
board database (Godfrey, Holliman, and McDaniel 1992), and report that the vast ma-
jority of these four dialogue acts are realized with words like yeah, okay, or uh-huh. They
find that the lexical realization of the dialogue act is the strongest cue to its identity (e.g.,
backchannel is the preferred function for uh-huh and mm-hm), and report preliminary
results on some prosodic differences across dialogue acts: Backchannels are shorter in
duration, have lower pitch and intensity, and are more likely to end in a rising intonation
than agreements. Two related studies, part of the same project, address the automatic
classification of dialogue acts in conversational speech (Shriberg et al 1998; Stolcke
4
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
et al 2000). The results of their machine learning experiments, conducted on the same
subset of Switchboard used previously, indicate a high degree of confusion between
agreements and backchannels, because both classes share words such as yeah and right.
They also show that prosodic features (including duration, pause, and intensity) can aid
the automatic disambiguation between these two classes: A classifier trained using both
lexical and prosodic features slightly yet significantly outperforms one trained using
just lexical features.
There is also considerable evidence that linguistic feedback does not take place
at arbitrary locations in conversation; rather, it mostly occurs at or near transition-
relevance places for turn-taking (Sacks, Schegloff, and Jefferson 1974; Goodwin 1981).
Ward and Tsukahara (2000) describe, in both Japanese and American English, a region
of low pitch lasting at least 110 msec which may function as a prosodic cue inviting
the realization of a backchannel response from the interlocutor. In a corpus study
of Japanese dialogues, Koiso et al (1998) find that both syntax and prosody play a
central role in predicting the occurrence of backchannels. Cathcart, Carletta, and Klein
(2003) propose a method for automatically predicting the placement of backchannels
in Scottish English conversation, based on pause durations and part-of-speech tags,
that outperforms a random baseline model. Recently, Gravano and Hirschberg (2009a,
2009b, 2011) describe six distinct prosodic, acoustic, and lexical events in American
English speech that tend to precede the occurrence of a backchannel by the interlocutor.
Despite their high frequency in spontaneous conversation, the set of ACWs we
examine here have seldom, if ever, been an object of study in themselves, as a separate
subclass of cue phrases or dialogue acts. Some have attempted to model other types
of cue phrases (e.g., well, like) or cue phrases in general; others discuss discourse/
pragmatic functions that may be conveyed through ACWs, but which may also be
conveyed through other types of expressions (e.g., agreements may be communicated
by single words such as yes or longer cue phrases such as that?s correct). Subsets of ACWs
have been studied in very small corpora, with some proposals about their prosodic
and functional variations. For example, Hockey (1993) examines the prosodic variation
of two ACWs, okay and uh-huh (66 and 77 data points, respectively) produced as full
intonational phrases in two spontaneous task-oriented dialogues. She groups the F0
contours visually and auditorily, and shows that instances of okay produced with a
high-rise contour are significantly more likely to be followed by speech from the other
speaker than from the same speaker. The results of a perception experiment conducted
by Gravano et al (2007) suggest that, in task-oriented American English dialogue,
contextual information (e.g., duration of surrounding silence, number of surrounding
words) as well as word-final intonation figure as the most salient cues to disambiguation
of the function of the word okay by human listeners. Also, in a study of the function of
intonation in Scottish English task-oriented dialogue, Kowtko (1996) examines a corpus
of 273 instances of single-word utterances, including affirmative cue words such as mm-
hm, okay, right, uh-huh, and yes. Kowtko finds a significant correlation between discourse
function and intonational contour: The align function (which checks that the listener?s
understanding aligns with that of the speaker) is shown to correlate with rising in-
tonational contours; the ready function (which cues the speaker?s intention to begin
a new task) and the reply-y function (which ?has an affirmative surface and usually
indicates agreement?; Kowtko 1996, page 59) correlate with a non-rising intonation;
and the acknowledge function (which indicates having heard and understood) presents
all types of final intonation. It is important to note, however, that different dialects
and different languages have distinct ways of realizing different discourse/pragmatic
functions, so it is unclear how useful these results are for American English.
5
Computational Linguistics Volume 38, Number 1
Although broader studies focusing on the pragmatic function of cue phrases, dis-
course markers, linguistic feedback, and dialogue acts do shed light on the particular
subset of utterances we are studying, and although there is some information on par-
ticular lexical items we include here in our study, the class of ACWs itself has received
little attention. Particularly given the frequency of ACWs in dialogue, it is important
to identify reliable and automatically extractable cues to their disambiguation, so that
spoken dialogue systems can recognize the pragmatic function of ACWs in user input
and can produce ACWs that are less likely to be misinterpreted in system output.
3. Materials
The materials for all experiments in this study were taken from the Columbia Games
Corpus, a collection of 12 spontaneous task-oriented dyadic conversations elicited from
13 native speakers (6 female, 7 male) of Standard American English (SAE). A detailed
description of this corpus is given in Appendix A. In each session, two subjects were
paid to play a series of computer games requiring verbal communication to achieve
joint goals of identifying and moving images on the screen. Each subject used a separate
laptop computer; they sat facing each other in a soundproof booth, with an opaque
curtain hanging between to allow only verbal communication.
Each session contains an average of 45 minutes of dialogue, totaling roughly 9 hours
of dialogue in the corpus. Trained annotators orthographically transcribed the re-
cordings and manually aligned the words to the speech signal, yielding a total of
70,259 words and 2,037 unique words in the corpus. Additionally, self repairs and
certain non-word vocalizations were marked, including laughs, coughs, and breaths.
For roughly two thirds of the corpus, intonational patterns and other aspects of the
prosody were identified by trained annotators using the ToBI transcription framework
(Beckman and Hirschberg 1994; Pitrelli, Beckman, and Hirschberg 1994).
3.1 Affirmative Cue Words in the Games Corpus
Throughout the Games Corpus, subjects made frequent use of affirmative cue words:
The 5,456 instances of affirmative cue words alright, gotcha, huh, mm-hm, okay, right,
uh-huh, yeah, yep, yes, and yup account for 7.8% of the total words in the corpus. Because
the usage of these words seems to vary significantly in meaning, we asked three labelers
to independently classify all occurrences of these 11 words in the entire corpus into the
ten discourse/pragmatic functions listed in Table 1.
Among the distinctions we make in these pragmatic functions, we note particularly
that our categories of Agr and BC differ primarily in that Agr is defined as indicating
belief in or agreement with the interlocutor (e.g., a response to a yes?no question),
whereas BC indicates only continued attention.1
1 Our definition of BC is similar to definitions of backchannel and continuer as discussed by a number
of authors in the Conversational Analysis and spoken language processing communities (e.g.,
Stolcke et al?s [2000] ?a short utterance that plays discourse structuring roles, e.g., indicating that the
speaker should go on talking? [page 345]; and Cathcart et al?s [2003] ?utterances, with minimal content,
used to clearly signal that the speaker should continue with her current turn? [page 51]). Although some
definitions of BC also include the notion that the speaker is indicating understanding, we did not ask
annotators to make this distinction. We note further that, although it is also possible (cf. Clark and
Schaefer 1989) to signal understanding without agreement, in the process of designing the labeling
scheme we did not find instances of ACWs that seemed to us to have this function in our corpus; nor
did our labelers find such cases. Hence we did not include this distinction among our classes.
6
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
Table 1
Labeled discourse/pragmatic functions of affirmative cue words.
Agr Agreement. Indicates I believe what you said, and/or I agree with what you say.
BC Backchannel. Indicates only I hear you and please continue, in response to another
speaker?s utterance.
CBeg Cue beginning discourse segment. Marks a new segment of a discourse or a new
topic.
CEnd Cue ending discourse segment. Marks the end of a current segment of a discourse
or a current topic.
PBeg Pivot beginning (Agr+CBeg). Functions both to agree and to cue a beginning
segment.
PEnd Pivot ending (Agr+CEnd). Functions both to agree and to cue the end of the
current segment.
Mod Literal modifier. Examples: I think that?s okay; to the right of the lion.
BTsk Back from a task. Indicates I?ve just finished what I was doing and I?m back.
Chk Check. Used with the meaning Is that okay?
Stl Stall. Used to stall for time while keeping the floor.
? Cannot decide.
Labelers were given examples of each category, and annotated with access to both
transcript and speech source. The guidelines used by the annotators are presented in
Appendix B. Appendix C includes some examples of each class of ACWs, as labeled
by our annotators. Inter-labeler reliability was measured by Fleiss?s ? (Fleiss 1971) as
Substantial at 0.745.2 We define the majority label of a token as the label chosen for that
token by at least two of the three labelers; we assign the ??? label to a token either when
its majority label is ???, or when it was assigned a different label by each labeler. Of the
5,456 affirmative cue words in the corpus, 5,185 (95%) have a majority label other than
??.? Table 2 shows the distribution of discourse/pragmatic functions over ACWs in the
whole corpus.
3.2 Data Downsampling
Some of the word/function pairs in Table 2 are skewed to contributions from a few
speakers. For example, for backchannel (BC) uh-huh, as many as 65 instances (44%) are
from one single speaker, and the remaining 83 are from seven other speakers. In cases
like this, using the whole sample would pose the risk of drawing false conclusions on
the usage of ACWs, possibly influenced by stylistic properties of individual speakers.
Therefore, we downsampled the tokens of ACWs in the Games Corpus to obtain a
balanced data set, with instances of each word and function coming in similar propor-
tions from as many speakers as possible. Specifically, we downsampled our data using
the following procedure: First, we discarded all word/function pairs with tokens from
fewer than four different speakers; second, for each of the remaining word/function
pairs, we discarded tokens (at random) from speakers who contributed more than 25%
of its tokens. In other words, the resulting data set meets two conditions: For each word/
2 The ? measure of agreement above chance is interpreted as follows: 0 = None, 0?0.2 = Small,
0.2?0.4 = Fair, 0.4?0.6 = Moderate, 0.6?0.8 = Substantial, 0.8?1 = Almost perfect.
7
Computational Linguistics Volume 38, Number 1
Table 2
Distribution of function over ACW. Rest = {gotcha, huh, yep, yes, yup}.
alright mm-hm okay right uh-huh yeah Rest Total
Agr 76 58 1,092 111 18 754 116 2,225
BC 6 395 120 14 148 69 5 757
CBeg 83 0 543 2 0 2 0 630
CEnd 6 0 6 0 0 0 0 12
PBeg 4 0 65 0 0 0 0 69
PEnd 11 12 218 2 0 20 15 278
Mod 5 0 18 1,069 0 0 0 1,092
BTsk 7 1 32 0 0 0 0 40
Chk 1 0 6 49 0 1 6 63
Stl 1 0 15 1 0 2 0 19
? 36 12 150 10 3 55 5 271
Total 236 478 2,265 1,258 169 903 147 5,456
function pair, (a) tokens come from at least four different speakers, and (b) no single
speaker contributes more than 25% of the tokens. The two thresholds were found via
a grid search, and were chosen as a trade-off between size and representativeness of
the data set. With this procedure we discarded 506 tokens of ACWs, or 9.3% of such
words in the corpus. Table 3 shows the resulting distribution of discourse/pragmatic
functions over ACWs in the whole corpus after downsampling the data. The ? measure
of inter-labeler reliability was practically identical for the downsampled data, at 0.751.
3.3 Feature Extraction
We extracted a number of lexical, discourse, timing, phonetic, acoustic, and prosodic
features for each target ACW, which we use in the statistical analysis and machine
learning experiments presented in the following sections. Tables 4 through 8 summarize
the full feature set. For simplicity, in those tables each line may describe one or more
features. Features that may be extracted by on-line applications are marked with letter
O; this is further explained later in this section.
Table 3
Distribution of function over ACW, after downsampling. Rest = {gotcha, huh, yep, yes, yup}.
alright mm-hm okay right uh-huh yeah Rest Total
Agr 76 58 1,092 74 16 754 87 2,157
BC 0 395 120 0 101 58 0 674
CBeg 61 0 543 0 0 0 0 604
CEnd 0 0 4 0 0 0 0 4
PBeg 0 0 64 0 0 0 0 64
PEnd 10 4 218 0 0 18 0 250
Mod 4 0 18 1,069 0 0 0 1,091
BTsk 5 0 28 0 0 0 0 33
Chk 0 0 5 49 0 0 4 58
Stl 0 0 15 0 0 0 0 15
Total 156 457 2,107 1,192 117 830 91 4,950
8
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
Table 4
Lexical and discourse features. Each line may describe one or more features. Features marked O
may be available in on-line conditions.
Lexical features
O Lexical identity of the target word (w).
O Part-of-speech tag of w, original and simplified.
O Word immediately preceding w, and its original and simplified POS tags. If w is preceded
by silence, this feature takes value ?#?.
O Word immediately following w, and its original and simplified POS tags. If w is followed
by silence, this feature takes value ?#?.
Discourse features
O Number of words in w?s IPU.
O Number and proportion of words in w?s IPU before and after w.
O Number of words uttered by the other speaker during w?s IPU.
O Number of words in the previous turn by the other speaker.
Number of words in w?s turn.
Number and proportion of words and IPUs in w?s turn before and after w.
Number and proportion of turns in w?s task before and after w.
Number of words uttered by the other speaker during w?s turn.
Number of words in the following turn by the other speaker.
Number of ACWs in w?s turn other than w.
Our lexical features consist of the lexical identity and the part-of-speech (POS) tag
of the target word (w), the word immediately preceding w, and the word immediately
following w (see Table 4). POS tags were labeled automatically for the whole corpus
using Ratnaparkhi, Brill, and Church?s (1996) maxent tagger trained on a subset of the
Switchboard corpus (Charniak and Johnson 2001) in lower-case with all punctuation
removed, to simulate spoken language transcripts. Each word had an associated POS
tag from the full Penn Treebank tag set (Marcus, Marcinkiewicz, and Santorini 1993),
and one of the following simplified tags: noun, verb, adjective, adverb, contraction,
or other.
For our discourse features, listed in Table 4, we define an inter-pausal unit (IPU) as
a maximal sequence of words surrounded by silence longer than 50 msec. A turn is a
maximal sequence of IPUs from one speaker, such that between any two adjacent IPUs
there is no speech from the interlocutor.3,4 Boundaries of IPUs and turns are computed
automatically from the time-aligned transcriptions. A task in the Games Corpus cor-
responds to a simple game played by the subjects, requiring verbal communication to
achieve a joint goal of identifying and moving images on the screen (see Appendix A for
a description of these game tasks). Task boundaries are extracted from the logs collected
automatically during the sessions, and subsequently checked by hand. Our discourse
features are intended to capture discrete positional information of the target word, in
relation to its containing IPU, turn, and task.
3 Here ?between? refers strictly to the time after the end point of the former IPU and before the start point
of the latter.
4 Note that our operational definition of ?turn? here includes all speaker utterances, including
backchannels, which are typically not counted as turn-taking behaviors. We use this more inclusive
definition of ?turn? here to avoid inventing a new term to encompass ?turns and backchannels?.
9
Computational Linguistics Volume 38, Number 1
Our timing features (Table 5) are intended to capture positional information of a
temporal nature, such as the duration (in milliseconds) of w and its containing IPU
and turn, or the duration of any silence before and after w. These features also contain
information about the target word relative to the other speaker?s speech, including the
duration of any overlapping speech, and the latencies between w?s conversational turn
and the other speaker?s preceding and subsequent turns.
Prosody was annotated following the ToBI system (Beckman and Hirschberg 1994;
Pitrelli, Beckman, and Hirschberg 1994), which consists of annotations at four time-
linked levels of analysis: an orthographic tier of time-aligned words; a tonal tier describing
targets in the fundamental frequency (F0) contour; a break index tier indicating degrees
of juncture between words; and a miscellaneous tier, in which phenomena such as
disfluencies may be optionally marked. The tonal tier describes events such as pitch
accents, which make words intonationally prominent and are realized by increased F0
height, loudness, and duration of accented syllables. A given word may be accented
or not and, if accented, may bear different tones, or different degrees of prominence,
with respect to other words. Five types of pitch accent are distinguished in the ToBI
system for American English: two simple accents H* and L*, and three complex ones,
L*+H, L+H*, and H+!H*. An L indicates a low tone and an H, a high tone; the
asterisk indicates which tone of the accent is aligned with the stressable syllable of
the lexical item bearing the accent. Some pitch accents may be downstepped, such that
the pitch range of the accent is compressed in comparison to a non-downstepped accent.
Downsteps are indicated by the ?!? diacritic (e.g., !H*, L+!H*). Break indices define two
levels of phrasing: Level 3 corresponds to Pierrehumbert?s (1980) intermediate phrase
and level 4 to Pierrehumbert?s intonational phrase. Level 4 phrases consist of one or
more level 3 phrases, plus a high or low boundary tone (H% or L%) indicated in the
tonal tier at the right edge of the phrase. Level 3 phrases consist of one or more pitch
accents, aligned with the stressed syllable of lexical items, plus a phrase accent, which
also may be high (H-) or low (L-). For example, a standard declarative contour consists
Table 5
Timing features. Each line may describe one or more features. Features marked O may be
available in on-line conditions.
Timing features
O Duration (in msec) of w (raw, normalized with respect to all occurrences of the same word
by the same speaker, and normalized with respect to all words with the same number of
syllables and phonemes uttered by the same speaker).
O Flag indicating whether there was any overlapping speech from the other speaker.
O Duration of w?s IPU.
O Latency (in msec) between w?s turn and the previous turn by the other speaker.
O Duration of the silence before w (or 0 if the w is not preceded by silence), its IPU, and its
turn.
O Duration and proportion of w?s IPU elapsed before and after w.
O Duration of w?s turn before w.
O Duration of any overlapping speech from the other speaker during w?s IPU.
O Duration of the previous turn by the other speaker.
Duration of the silence after w (or 0 if w is not followed by silence), its IPU, and its turn.
Latency between w?s turn and the following turn by the other speaker.
Duration of w?s turn, as a whole and after w.
Duration of any overlapping speech from the other speaker during w?s turn.
Duration of the following turn by the other speaker.
10
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
Figure 1
A standard declarative contour (left), and a standard yes?no question contour. The top panes
show the waveform and the fundamental frequency (F0) track.
of a sequence of H* pitch accents ending in a low phrase accent and low boundary
tone (L-L%); likewise, a standard yes?no question contour consists of a sequence of L*
pitch accents ending in H-H%. These are illustrated in Figure 1. In our study, prosodic
features include the ToBI labels as specified by the annotators, and also a simplified
version of the labels, considering only high and low pitch targets (i.e., H* vs. L* for
pitch accents, H- vs. L- for phrase accents, and H% vs. L% for boundary tones), and
simplified break indices (0?4). These are listed in Table 6.
All acoustic features were extracted automatically for the whole corpus using the
Praat toolkit (Boersma and Weenink 2001). These include pitch, intensity, stylized pitch,
ratio of voiced frames to total frames, jitter, shimmer, and noise-to-harmonics ratio
(NHR) (see Table 7). Pitch features capture how high the speaker?s voice sounds or how
low. Intensity is correlated with how loud the speaker sounds to a hearer. The voiced-
frames ratio roughly approximates the speaking rate. Jitter and shimmer correspond to
variability in the frequency and amplitude of vocal-fold vibration, respectively. NHR
is the energy ratio of noise to harmonic components in the voiced speech signal. Jitter,
shimmer, and NHR correlate with perceptual evaluations of voice quality, such as harsh,
whispery, creaky, and nasalized, inter alia. Pitch slope features capture elements of
the intonational contour, and were computed by fitting least-squares linear regression
models to the F0 data points extracted from given portions of the signal, such as a
full word or its last 200 msec. This procedure is illustrated in Figure 2, which shows
the pitch track of a sample utterance (blue dots) with three linear regressions, com-
puted over the whole utterance (solid black line), and over the final 300 and 200 msec
(?A? and ?B? dashed lines, respectively). We used a similar procedure to compute
Table 6
Prosodic features. In all cases, both original and simplified ToBI labels were considered. Each line
may describe one or more features. Features marked O may be available in on-line conditions.
ToBI prosodic features
? Phrase accent, boundary tone, break index, and pitch accent on w.
? Phrase accent, boundary tone, break index, and final pitch accent on the final intonational
phrase of the previous turn by the other speaker (these features are defined only when w
is turn initial).
11
Computational Linguistics Volume 38, Number 1
Table 7
Acoustic features. Each line may describe one or more features. Features marked O may be
available in on-line conditions.
Acoustic features
O w?s mean, maximum, and minimum pitch and intensity (raw and speaker normalized).
O Jitter and shimmer, computed over the whole word and over the first and second
syllables, computed over just the voiced frames (raw and speaker normalized).
O Noise-to-harmonics ratio (NHR), computed over the whole word and over the first and
second syllables (raw and speaker normalized).
O w?s ratio of voiced frames to total frames (raw and speaker normalized).
O Pitch slope, intensity slope, and stylized pitch slope, computed over the whole word, its
first and second halves, its first and second syllables, the first and second halves of each
syllable, and the word?s final 100, 200, and 300 msec (raw and normalized with respect to
all other occurrences of the same word by the same speaker).
O w?s mean, maximum, and minimum pitch and intensity, normalized with respect to three
types of context: w?s IPU, w?s immediately preceding word by the same speaker, and w?s
immediately following word by the same speaker.
O Voiced-frames ratio, jitter, and shimmer, normalized with respect to the same three types
of context.
O Mean, maximum, and minimum pitch and intensity, ratio of voiced frames, (all raw and
speaker normalized), jitter, and shimmer, calculated over the final 500, 1,000, 1,500 and
2,000 msec of the previous turn by the other speaker (only defined when w is turn initial
but not task initial).
O Pitch slope, intensity slope, and stylized pitch slope, calculated over the final 100, 200,
300, 500, 1,000, 1,500, and 2,000 msec of the previous turn by the other speaker (only
defined when w is turn initial but not task initial).
intensity slopes (which capture changes in perceived loudness) and stylized pitch
slopes (which capture more coarse-grained characteristics of the intonational contour).
Stylized pitch curves were obtained using the algorithm provided in Praat: Look up
the pitch point p that is closest to the straight line L that connects its two neighboring
points; if p is further than four semitones away from L, end; otherwise, remove p and
start over.
Figure 2
Sample pitch track with three linear regressions: computed over the whole IPU (bold line), and
over the final 300 msec (A) and 200 msec (B).
12
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
All features related to absolute (i.e., unnormalized) pitch values, such as maximum
pitch or final pitch slope, are not comparable across genders because of the different
pitch ranges of female and male speakers?roughly 75?500 kHz and 50?300 kHz, respec-
tively. Therefore, before computing those features we applied a linear transformation
to the pitch track values, thus making the pitch range of speakers of both genders
approximately equivalent. We refer to this process as gender normalization. All other
normalizations were calculated using z-scores: z = (x ? ?)/?, where x is a raw mea-
surement to be normalized (e.g., the duration of a particular word), and ? and ? are
the mean and standard deviation of a certain population (e.g., all instances of the same
word by the same speaker in the whole conversation).
For our phonetic features (listed in Table 8), we trained an automatic phone rec-
ognizer based on the Hidden Markov Model Toolkit (HTK) (Young et al 2006), using
three corpora as training data: the TIMIT Acoustic-Phonetic Continuous Speech Corpus
(Garofolo et al 1993), the Boston Directions Corpus (Hirschberg and Nakatani 1996),
and the Columbia Games Corpus. With this recognizer, we obtained automatic time-
aligned phonetic transcriptions of each instance of alright, mm-hm, okay, right, uh-huh,
and yeah in the corpus. To improve accuracy, we restricted the recognizer?s grammar to
accept only the most frequent variations of each word, as shown in Table 9. We extracted
our phonetic features, such as phone and syllable durations, from the resulting time-
aligned phonetic transcriptions. The remaining five ACWs in our corpus (gotcha, huh,
yep, yes, and yup) had too low counts to contain meaningful phonetic variation; thus, we
did not compute phonetic features for those words.
Finally, our session-specific features include the session of the Games Corpus in
which the target word was produced, along with the identity and gender of both
Table 8
Phonetic and session-specific features. Each line may describe one or more features. Features
marked O may be available in on-line conditions.
Phonetic features
O Identity of each of w?s phones.
O Absolute and relative duration of each phone.
O Absolute and relative duration of each syllable.
Session-specific features
? Session number.
? Identity and gender of both speakers.
Table 9
Restricted grammars for the automatic speech recognizer. Phones in square brackets are optional.
ACW ARPAbet Grammar
alright (aa|ao|ax) r (ay|eh) [t]
mm-hm m hh m
okay [aa|ao|ax|m|ow] k (ax|eh|ey)
right r (ay|eh) [t]
uh-huh (aa|ax) hh (aa|ax)
yeah y (aa|ae|ah|ax|ea|eh)
13
Computational Linguistics Volume 38, Number 1
speakers (Table 8). These features were solely intended for searching for speaker or
dialogue dependencies.
Also, to simulate the conditions of on-line applications, which process speech as
it is produced by the user, we distinguish a subset of features that may typically be
extracted from the speech signal only up to the IPU containing the target ACW. In
Tables 4 through 8 these features are marked with letter O (for on-line). All on-line fea-
tures can be computed automatically in real time by state-of-the-art speech processing
applications, although it should be noted that all of our lexical and discourse features
strongly rely on a speech recognizer output, which typically has a high error rate for
spontaneous productions. All on-line features are also available in off-line conditions;
the remaining features (those not tagged O in Tables 4 through 8) are normally available
only in offline conditions. We distinguish online features for the machine learning exper-
iments described in Section 5, in which we assess, among other things, the usefulness
of information contained in different feature sets, simulating the conditions of actual
on-line and off-line applications.
In the following sections, we use the features described here in several ways. We
first perform a series of statistical tests to find differences across the various func-
tions of ACWs. Subsequently, we experiment with machine learning techniques for the
automatic classification of the function of ACWs, training the models with different
combinations of features.
4. Characterizing Affirmative Cue Words
In this section we present results of a series of statistical tests aimed at identifying con-
textual, acoustic, and prosodic differences in the production of the various discourse/
pragmatic functions of affirmative cue words. This kind of characterization is important
both for interpretation and for production in spoken language applications: If we can
find reliable features that effectively distinguish the various uses of these words, we can
hope to interpret them automatically and generate them appropriately.
4.1 Position in IPU and Turn
We begin this analysis by looking at the discourse position of the various discourse/
pragmatic functions of ACWs. Because these words help shape, or at least reflect,
the structure of conversations, we expect to find positional differences between their
functions. Figure 3 shows the distribution of the six most frequent ACWs in the corpus
(alright, okay, yeah, mm-hm, uh-huh, and right) with respect to their position in their IPU.
An IPU-initial word is one that occurs in the first position in its corresponding IPU; that
is, it is preceded by at least 50 msec of silence and followed by another word. An IPU-
final word occurs last in its IPU. An IPU-medial word is both immediately preceded
and followed by other words. Lastly, a single-word IPU is an individual word both
preceded and followed by silence. Figure 3 also depicts the distribution of discourse/
pragmatic functions within each of these four categories. For example, roughly 40% of
all tokens of alright in the corpus occur as IPU initial; of those, about half are agreements
(Agr), half are cues to beginning discourse segments (CBeg), and a marginal number
convey other functions.
Similarly, Figure 4 shows the distribution of the same six ACWs with respect to their
position in the corresponding conversational turn. Turn-initial, turn-medial, and turn-
final words, and single-word turns are defined analogously to the four IPU-related
categories defined previously, but considering conversational turns instead of IPUs.
14
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
Figure 3
Position of the target word in its IPU.
Figure 4
Position of the target word in its turn.
From these figures we observe several interesting aspects of the discourse position
of ACWs in the Games Corpus. Only a minority of these words occur as IPU medial
or IPU final. The only exception appears to be right, for which a high proportion of
instances do occur in such positions?mainly tokens with the literal modifier (Mod)
meaning, but also tokens used to check with the interlocutor (Chk), which take place at
the end of a turn (and thus, of an IPU).
The default function of ACWs, agreement (Agr), occurs for alright, okay, yeah, and
right in all possible positions within the IPU and the turn; for mm-hm and uh-huh,
agreements occur mostly as full conversational turns. Nearly all backchannels (BC)
occur as separate turns, with only a handful of exceptions: In four cases, the backchannel
is followed by a pause in which the interlocutor chooses not to continue speaking, and
the utterer of the backchannel takes the turn; in two other cases, two backchannels are
uttered in fast repetition (e.g., uh-huh uh-huh).
From the six lexical items analyzed in Tables 3 and 4, two pairs of words seem to
pattern similarly. The first such pair consists of mm-hm and uh-huh, which show very
similar distributions and are realized almost always as single-word turns, as either Agr
or BC. The second pair of words with comparable patterns of IPU and turn position
are alright and okay. These are precisely the only two ACWs used to convey all ten
discourse/pragmatic functions in the Games Corpus (recall from Table 2). This result
suggests that the lexical items in these two pairs may be used interchangeably in
conversation. The word yeah presents a pattern analogous to that of alright and okay,
albeit with fewer meanings.
In all, these findings confirm the existence of large differences in the discourse
position of ACWs between their functional types, as well as between their lexical
types. We will revisit this topic in Section 5, where we discuss the predictive power
15
Computational Linguistics Volume 38, Number 1
Figure 5
ToBI phrase accents and boundary tones. The ?other? category consists of cases with no phrase
accent and/or boundary tone present at the target word.
of discourse features in the automatic classification of the function of ACWs. Given the
observed positional differences, we expect these features to play a prominent role in
such a task.
4.2 Word-Final Intonation
Shifting our attention to acoustic/prosodic characteristics of ACWs, we examine next
the manner in which word-final intonation varies across ACW functions. First we look
at two categorical variables in the ToBI framework which capture the final pitch incur-
sion: phrase accent and boundary tone. Figure 5 shows the distribution of ToBI labels for
each of the six most frequent ACWs and their corresponding functions (see Section 3.3
for a description of the ToBI labeling conventions). The distributions for alright, okay,
right, and yeah depart significantly from random (alright: Fisher?s Exact test, p = 0.0483;
okay: Pearson?s Chi-squared test, ?2(24) = 261, p ? 0; right: Pearson, ?2(8) = 220, p ? 0;
yeah: Fisher, p ? 0).5,6 For right, considering just its discourse/pragmatic functions (i.e.,
excluding its Mod instances), the distribution also significantly differs from random
(Fisher, p ? 0). On the other hand, the distributions for mm-hm and uh-huh do not depart
significantly from random.
5 Fisher?s Exact test was used whenever the accuracy of Pearson?s Chi-squared test was compromised by
data sparsity.
6 We performed statistical tests for approximately 35 variables on the same data set. Applying the
Bonferroni correction, the alpha value should be lowered from the standard 0.05 to 0.05/35 ? 0.0014 to
maintain the familywise error rate. Thus, a result would be significant when p < 0.0014. According to
this, most tests are still significant in the current section; however, the Tukey post hoc tests following our
ANOVA tests are not: most of these have a confidence level of 95%, and significant differences begin to
disappear when considering a confidence level of 99%.
16
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
The first clear pattern we find is that the backchannel function (BC) shows a marked
preference for a high-rising (H-H% in the ToBI conventions) or low-rising (L-H%) pitch
contour towards the end of the word. Those two contours account for more than 60%
of the backchannel instances of mm-hm, okay, uh-huh, and yeah. For the other ACWs
there are not enough instances labeled BC in the corpus for statistical comparison.
The predominance of H% found for backchannels is consistent with the openness that
such boundary tone has been hypothesized to indicate (Hobbs 1990; Pierrehumbert
and Hirschberg 1990). The utterer of a backchannel understands that (i) there is
more to be said, and (ii) it is the speaker holding the conversational turn who must
say it.
The default function of ACWs, agreement (Agr) is produced most often with falling
(L-L%) or plateau final intonation ([!]H-L%) in the case of alright, okay, right, and yeah.
The L% boundary tone is believed to indicate the opposite of H%, a sense of closure,
separating the current phrase from a subsequent one (Pierrehumbert and Hirschberg
1990). In our case, by agreeing with what the speaker has said, the listener indicates that
enough information has been provided and that any subsequent phrases may refer to a
different topic. In other words, such closure might mean that the proposition preceding
the ACW has been added to the current context space (Reichman 1985), or that a new
focus space is about to be created (Grosz and Sidner 1986).
Notably, Agr instances of mm-hm and uh-huh present a very different behavior from
the other lexical items, with a distribution of final intonations that closely resembles
that of backchannels. In particular, over 60% of the Agr tokens of mm-hm and uh-huh
are produced with final rising intonation (either L-H% or H-H%). As we will see in the
following sections, the realization of mm-hm and uh-huh as Agr or BC seems to be very
similar along several dimensions besides intonation.
Alright and okay are the only two ACWs in the corpus that are used to cue the
beginning of a new discourse segment, either combined with an agreement function
(PBeg) or in its pure form (CBeg). These two functions typically have a falling (L-L%) or
sustained ([!]H-L%) final pitch contour. Additionally, the instances of okay and yeah used
to cue a discourse segment ending (PEnd) tend to be produced with a L-L% contour, and
also with [!]H-L% in the case of okay. This predominance of L% for ACWs conveying a
discourse boundary function is consistent with the previously mentioned closure that
such boundary tone is believed to indicate.
Lastly, the only ACW used frequently in the corpus for checking with the interlocu-
tor (the Chk function) is right, as illustrated in the following exchange:
A: and the top?s not either, right?
B: no
A: okay
Such instances of right in the corpus normally end in a high-rising pitch contour,
or H-H%. This fact is probably explained by the close semantic resemblance of
this construction to yes?no questions, which typically end in the same contour type
(Pierrehumbert and Hirschberg 1990).
In addition to the categorical prosodic variables described previously, word final
intonation may also be studied by measuring the slope of the word-final pitch track
(see Section 3.3 for a description of how pitch slopes are calculated). A high positive
value of pitch slope corresponds to a rising intonation; a value close to zero, to a flat
intonation; a high negative value, to a falling intonation. Final pitch slope has the ad-
vantage of being automatically computable; ToBI labels, on the other hand, still must be
17
Computational Linguistics Volume 38, Number 1
Figure 6
Final pitch slope, computed over the second half and over the final 100 and 200 msec of the
target word. In all cases, the vertical axis represents the change in Hertz per second. Significant
differences: For okay: BC>all; CBeg<Agr, BC, PEnd. For right: Chk>Agr. For yeah: BC>Agr.
manually annotated?although ongoing research may change this fact in the near future
(Rosenberg and Hirschberg 2009; Rosenberg 2010a, 2010b). Therefore, it is important to
verify that the results obtained using ToBI labels?if they are to be of practical use?
are also observable when considering numeric measures such as pitch slope. Figure 6
shows, for the same ACWs and functions discussed earlier,7 the mean pitch slope
computed over the second half of the word and over its final 100 and 200 msec, and
gender-normalized as described in Section 3.
The comparison of these numeric acoustic features across discourse/pragmatic
functions confirms that the observations made previously for categorical prosodic fea-
tures also hold when considering numeric features such as pitch slope, thus making the
likelihood that such observations will be of practical use in actual systems. For okay,
the three measures of word-final pitch slope are significantly higher for backchannels
(BC) than for all other functions, and significantly lower for CBeg than for Agr, BC,
and PEnd (RMANOVA for each of the three variables: between-subjects p > 0.3, within
subjects p ? 0; Tukey test confidence: 95%).8 BC tokens of yeah are also significantly
higher than Agr, with similar p-values. Figure 6 shows that BC instances of mm-hm
7 For PEnd instances of yeah and Agr instances of uh-huh, the number of tokens with no errors in the pitch
track and pitch slope computations is too low for statistical consideration.
8 Repeated-measures analysis of variance (RMANOVA) tests estimate the existence of both within-subjects
effects (i.e., differences between discourse/pragmatic functions) and between-subjects effects (i.e.,
differences between speakers). When the between-subjects effects are negligible, we may safely draw
conclusions across multiple speakers in the corpus, with low risk of a bias from the behavior of a
particular subset of speakers.
18
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
and uh-huh also have comparably high final pitch slopes. Again, for mm-hm we find no
significant difference in final pitch slope between agreements and backchannels.
Although Figure 6 shows that Chk tokens of right tend to end in a steeply rising
pitch, the RMANOVA tests yield between-subjects p-values of 0.01 or lower, indicating
substantial speaker effects. In other words, even though the general tendency for these
tokens, as indicated by both the numeric and categorical variables, seems to be to end
in a high-rising intonation, there is evidence of different behavior for some individual
speakers, which keeps us from drawing general conclusions about this pragmatic func-
tion of right.
4.3 Intensity
The next feature we find to vary significantly with the discourse/pragmatic function
of ACWs is word intensity. Commonly referred to as loudness or volume, intensity
generally functions to make words more salient or prominent. Figure 7 shows the
maximum and mean intensity for the most frequent ACWs and functions, computed
over the whole word and speaker normalized using z-scores.
The two types of differences we find are related to the discourse functions of
ACWs. For okay and yeah, both maximum and mean intensity are significantly lower
for instances cueing the end of a discourse segment (PEnd) than instances of all other
functions (for both variables and both words, RMANOVA tests report between-subjects
p > 0.4 and within-subjects p ? 0; Tukey 95%). For ACWs cueing a beginning discourse
segment, the opposite is true. Instances of alright and okay labeled CBeg or PBeg have
a maximum and mean intensity significantly higher than all other functions (for alright,
Figure 7
Word maximum and mean intensity, speaker normalized using z-scores. All vertical axes
represent z-scores. Significant differences: For alright: Agr<CBeg. For okay: PEnd<all;
Agr<CBeg, PBeg, BC; BC<CBeg. For yeah: PEnd<Agr, BC.
19
Computational Linguistics Volume 38, Number 1
a RMANOVA test reports between-subjects p > 0.12 and within-subjects p ? 0). These
results are consistent with previous studies of prosodic variation relative to discourse
structure, which find intensity to increase at the start of a new topic and decrease at the
end (Brown, Currie, and Kenworthy 1980; Hirschberg and Nakatani 1996). Because by
definition CBeg/PBeg ACWs begin a new topic and CEnd/PEnd end one, it is then not
surprising to find that the former tend to be produced with higher intensity, and the
latter with lower.
Finally, for mm-hm and uh-huh we find no significant differences in intensity be-
tween their unique functions, agreement (Agr) and backchannel (BC). Recall from the
previous section that we find no differences in final intonation either. This further
suggests that these two lexical types tend to be produced with similar acoustic/prosodic
features, independently of their function.
4.4 Other Features
For the remaining acoustic/prosodic features analyzed, we find a small number of sig-
nificant or approaching-significance differences between the functions of ACWs. These
differences are related to duration, mean pitch, and voice quality. The first set of findings
corresponds to the duration of ACWs, normalized with respect to all words with the
same number of syllables and phonemes uttered by the same speaker. For alright and
okay, instances cueing a beginning (CBeg) tend to be shorter than the other functions
(for both words, RMANOVA: between-subjects p > 0.5, within-subjects p < 0.05, Tukey
95%). We also find tokens of right used to check with the interlocutor (Chk) to be
on average shorter than the other two functions of right (RMANOVA, between-subjects
p > 0.7, within-subjects p = 0.001; Tukey 95%). Note that these two functions are rel-
atively simple: CBeg calls for the listener?s attention, and is frequently conveyed with
a filled pause (uh, um); Chk asks the interlocutor for confirmation, which may alter-
natively be achieved via a high-rising intonation. Thus, it is not surprising that these
functions take less time to be realized than other more pragmatically loaded functions,
such as agreement.
Speaker-normalized mean pitch over the whole word also presents significant
differences for okay and yeah. Instances labeled PEnd (agreement and cue ending
discourse segment) present a higher mean pitch than the other functions (for both
words, RMANOVA: between-subjects p > 0.6, within-subjects p < 0.01; Tukey 95%). This
is rather unexpected, because as noted in Section 4.2 around 70% of PEnd ACWs in the
corpus end in a L% boundary tone, and thus they would plausibly be uttered with a
low pitch level. What our data indicate, however, is that speakers tend to reset and raise
their pitch range when producing PEnd instances of ACWs.
Finally, we find some evidence of differences in voice quality. Both alright and
okay show a lower shimmer over voiced portions when starting a new segment (CBeg)
(RMANOVA: between-subjects p > 0.9 for alright, p = 0.09 for okay; within-subjects
p < 0.001 for both words). Also, okay and yeah present a lower noise-to-harmonics ratio
(NHR) for backchannels (RMANOVA: between-subjects p > 0.3 for okay, p = 0.04 for
yeah; within-subjects p < 0.005 for both words). A lower value of shimmer and NHR
has been associated with the perception of a better voice quality (Eskenazi, Childers,
and Hicks 1990; Bhuta, Patrick, and Garnett 2004). Our results suggest, then, that voice
quality may constitute another dimension along which speakers vary their productions
to convey the intended discourse/pragmatic meaning. Notice though that for these two
variables some of the between-subjects p-values are low enough to suggest significant
20
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
speaker effects. Therefore, our results related to differences in voice quality should be
considered preliminary.
5. Automatic Classification of Affirmative Cue Words
In this section we present results from machine learning (ML) experiments aimed at
investigating how accurately affirmative cue words may be classified automatically
into their various discourse/pragmatic functions. If spoken dialogue systems are to
interpret and generate ACWs reliably, we must identify reliable cues. With this goal
in mind, we explore several dimensions of the problem: We consider three classification
tasks, simulating the conditions of different speech applications, and study the perfor-
mance of different ML algorithms and feature sets on each task. We note that previous
studies have attempted to disambiguate between the sentential and discourse uses of
cue phrases such as now, well, and like, in corpora containing comparable numbers of
instances of each class. For ACWs in the Games Corpus dialogues, sentential uses are
rare, with the sole exception of right. Therefore, disambiguating between discourse and
sentential uses appears to be less useful than distinguishing among different discourse
functions.
The first ML task we consider consists in the general classification of any ACW
(alright, gotcha, huh, mm-hm, okay, right, uh-huh, yeah, yep, yes, yup) into any function
(Agr, BC, CBeg, PBeg, CEnd, PEnd, Mod, BTsk, Chk, Stl; see Table 1), a critical task
for spoken dialogue systems seeking to interpret user input in general. The second task
involves identifying instances of these words used to signal the beginning (CBeg, PBeg
in our labeling scheme) or ending (CEnd, PEnd) of a discourse segment, which is im-
portant for applications that must segment speech into coherent units, such as meeting
browsing systems and turn-taking components of spoken dialogue systems. The third
task consists in identifying tokens conveying some degree of acknowledgment: (Agr,
BC, PBeg, and PEnd), a function especially important to spoken dialogue systems, for
which it is critical to know that a user has heard the system?s output.
Speech processing applications operate in disparate conditions. On-line applica-
tions such as spoken dialogue systems process information as it is generated, having
access to a limited amount of context, normally up to the last IPU uttered by the user.
On the other hand, off-line applications, such as meeting transcription and browsing
systems, have the whole audio file available for processing. We simulate these two
conditions in our experiments, assessing how the limitations of online systems af-
fect performance. We also group the features described in Section 3.3 into five sets?
lexical (LX), discourse (DS), timing (TM), acoustic (AC), and phonetic (PH); see Tables 4
through 8?to determine the relative importance of each feature set in the various
classification tasks. For example, this approach allows us to simulate the conditions
of the understanding component of a spoken dialogue system, which can use only the
information up through the current IPU to detect the function of a user?s ACW. Such
a system may have access only to ASR transcription or it may have access to acoustic
and prosodic information; we note that our analysis does not take into account the pos-
sibility that transcriptions are likely to contain some errors. Our approach also allows
us to simulate a text-to-speech (TTS) system which might be used to produce a spoken
version of an on-line chat room. In order to choose the appropriate acoustic/prosodic
realization of each ACW, the TTS system will first need to determine its function based
on features extracted solely from the input text (in our taxonomy, LX and DS).
We conduct our ML experiments using three well-known algorithms with very
different characteristics: the decision tree learner C4.5 (Quinlan 1993), the propositional
21
Computational Linguistics Volume 38, Number 1
Table 10
Error rate of each classifier on the general task using different feature sets; F-measures of
the SVM classifier; and error rate and F-measures of two baselines and human labelers.
For the classifier error rates: ? Significantly different from full model. ? Significantly different
from SVM. (Wilcoxon signed rank sum test, p < 0.05.) Significance was not tested for the
classifier F-measures.
Error Rate SVM F-Measure
Feature Set C4.5 Ripper SVM Agr BC CBeg PEnd Mod Chk
LX DS TM AC PH 16.6% ? 16.3% ? 14.3% .86 .81 .89 .50 .97 .39
DS TM AC PH 21.3% ?? 17.2% ? 16.5% ? .84 .82 .87 .44 .94 0
LX TM AC PH 20.3% ?? 20.1% ? 17.0% ? .84 .80 .83 .16 .97 .21
LX DS AC PH 17.1% ? 18.1% ?? 14.8% ? .86 .81 .89 .38 .97 .35
LX DS TM PH 15.2% ? 16.3% 16.2% ? .85 .80 .86 .16 .97 .33
LX DS TM AC 17.0% ? 16.9% ? 14.7% .86 .80 .89 .48 .97 .35
LX 23.7% ?? 22.7% ? 22.3% ? .79 .80 .65 0 .96 .03
DS 22.8% ?? 24.0% ?? 25.3% ? .76 .67 .82 0 .87 0
TM 29.5% ?? 27.3% ?? 36.2% ? .70 0 .57 0 .83 0
AC 44.8% ?? 29.8% ?? 41.3% ? .67 .66 .14 0 .58 0
PH 56.4% ?? 26.5% ?? 45.4% ? .65 .08 .13 0 .64 0
Majority class baseline ER 56.4% .61 0 0 0 0 0
Word-based baseline ER 27.7% .75 .79 0 0 .94 .13
Human labelers ER (estimate 1) 9.3% .92 .91 .94 .51 .99 .67
Human labelers ER (estimate 2) 11.0% .90 .89 .93 ? .99 ?
rule learner RIPPER (Cohen 1995), and support vector machines (SVM) (Cortes and
Vapnik 1995; Vapnik 1995). We use the implementation of these algorithms provided
in the WEKA machine learning toolkit (Witten and Frank 2000), known respectively as
J48, JRIP, and SMO. We also use 10-fold cross-validation in all experiments.9
5.1 Classifiers and Feature Types
To assess the predictive power of the five feature types (LX, DS, TM, AC, and PH) we
exclude one type at a time and compare the performance of the resulting set to that
of the full model. Table 10 displays the error rate of each ML classifier on the general
task, classifying any ACW into any of the most frequent discourse/pragmatic functions
(Agr, BC, CBeg, PEnd, Mod, Chk). Table 11 shows the same results for the other two
tasks: the detection of a discourse boundary function?cue beginning (CBeg PBeg),
cue ending (CEnd, PEnd), or no-boundary (all other labels); and the detection of an
acknowledgment function?Agr, BC, PBeg, or PEnd, vs. all other labels.10
9 In the case of SVM, prior to the actual tests we experimented with two kernel types: polynomial
(K(x, y) = (x + y)d) and Gaussian radial basis function (RBF) (K(x, y) = exp(??||x ? y||2) for ? > 0). We
performed a grid search for the optimal arguments for either kernel using the data portion left out after
downsampling the corpus (see Section 3.2). The best results were obtained using a polynomial kernel
with exponent d = 1.0 (i.e., a linear kernel) and model complexity C = 1.0.
10 We note that performance on new data may be somewhat worse than the results reported here, because
we did exclude approximately 5% of tokens in our corpus due to lack of annotator agreement on labels.
22
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
Table 11
Error rate of each classifier on the detection of discourse boundary functions and
acknowledgment functions, using different feature sets. ? Significantly different from
full model. ? Significantly different from SVM. (Wilcoxon signed rank sum test, p < 0.05.)
Disc. Boundary Acknowledgment
{CBeg, PBeg} vs. {CEnd, PEnd} vs. Rest {Agr, BC, PBeg, PEnd} vs. Rest
Feature Set C4.5 Ripper SVM C4.5 Ripper SVM
LX DS TM AC PH 6.9% 8.1% ? 6.9% 5.8% 5.9% ? 4.5%
DS TM AC PH 7.6% ? 8.0% 7.6% ? 8.5% ?? 5.5% ? 6.4% ?
LX TM AC PH 10.4% ? 10.1% ? 9.5% ? 8.7% ?? 8.7% ?? 6.5% ?
LX DS AC PH 8.0% ? 8.7% ? 7.5% ? 5.3% 5.7% ? 4.9%
LX DS TM PH 6.6% ? 7.9% 8.9% ? 5.4% 5.4% 5.1%
LX DS TM AC 7.1% 8.3% ? 7.0% 5.8% ? 5.6% ? 4.6%
LX 14.2% ? 14.5% ?? 13.9% ? 11.4% ? 11.4% ? 11.7% ?
DS 7.8% ? 8.6% ? 10.9% ? 8.4% ?? 8.9% ? 9.4% ?
TM 12.2% ?? 11.2% ?? 14.7% ? 12.8% ?? 13.5% ? 14.5% ?
AC 17.3% ?? 14.3% ?? 18.5% ? 26.7% ? 16.6% ?? 28.4% ?
PH 18.6% ? 17.6% ? 18.6% ? 36.5% ?? 14.1% ?? 25.4% ?
Majority class baseline ER 18.6% 36.5%
Word-based baseline ER 18.6% 15.3%
Human labelers ER (est. 1) 5.3% 2.9%
Human labelers ER (est. 2) 5.6% 3.0%
In both tables, the first line corresponds to the full model, with all five feature
types. The subsequent five lines show the performance of models with just four fea-
ture types, excluding one feature type at a time, and the following five lines show
the performance of models with exactly one feature type?these are two methods for
assessing the predictive power of each feature set. For the error rates of our classifiers,
the ? symbol indicates that the given classifier performs significantly worse when
trained on a particular feature set than when trained on the full set.11 The ? symbol
indicates that the difference between SVM and the given classifier, either C4.5 or Ripper,
is significant. For example, the second line (DS TM AC PH) in Table 10 indicates that,
for the general classification task, the three models trained on all but lexical features
perform significantly worse than the respective full models; also, the performance of
C4.5 is significantly worse than SVM, and the difference between Ripper and SVM is
not significant.
The bottom parts of Tables 10 and 11 show the error rate of two baselines, as
well as two estimates of the error rate of human labelers. We consider two types of
baseline: one a majority-class baseline, and one that employs a simple rule based
11 All accuracy comparisons discussed in this section are tested for significance with the Wilcoxon signed
rank sum test (a non-parametric alternative to Student?s t-test) at the p < 0.05 level, computed over
the error rates of the classifiers on the ten cross-validation folds. These tests provide evidence that
the observed differences in mean accuracy over cross-validation folds across two models are not
attributable to chance.
23
Computational Linguistics Volume 38, Number 1
on word identity. In the general classification task, the majority class is Agr, and the
best performing word-based rule is huh?Chk, mm-hm?Mod, uh-huh?BC, right?
Mod, others?Agr. For the identification of a discourse boundary function, the
majority class is no-boundary, and the word-based rule also assigns no-boundary to
all tokens. For the detection of an acknowledgment function, the majority class
is acknowledgment, and the word-based rule is right, huh?no-acknowledgment;
others?acknowledgment.
The error rates of human labelers are estimated using two different approaches.
Our first estimate compares the labels assigned by each labeler and the majority labels
as defined in Section 3.1. Because each labeler?s labels are used for calculating both the
error rate and the gold standard, this estimate is likely to be over-optimistic. Our second
estimate considers the subset of cases in which two annotators agree, and compares
those labels with the third labeler?s. Tables 10 and 11 show that these two estimates
yield similar results; for PEnd and Chk, there are not enough counts for computing the
F-measure of estimate 2.
The right half of Table 10 shows the F-measure of the SVM classifier for each
individual ACW function, for the general task. The highest F-measures correspond to
Agr, BC, CBeg, and Mod, precisely the four functions with the highest counts in the
Games Corpus. For PBeg and Chk the F-measures are much lower (and equal to zero
for the four remaining functions, not included in the table) due very likely to their low
counts, which prevent a better generalization during the learning stage. Future research
could investigate incorporating boosting and bootstrapping techniques to reduce the
negative effect on classification of low counts for some of the discourse/pragmatic
functions of ACWs.
For the three classification tasks, SVM outperforms, or performs at least comparably
to, the other two classifiers whenever acoustic features (AC) are taken into account
together with other feature types. When used alone, though, acoustic features per-
form poorly in all three tasks. Moreover, when acoustic features are excluded, SVM?s
accuracy is comparable to, or worse than, C4.5 and Ripper. This is probably due to
the fact that SVM?s mathematical model is better suited to exploit larger amounts of
continuous numerical variables, and thus makes a difference when including acoustic
features.
For the first two tasks, the SVM classifier seems to take advantage of all but one
feature type, as shown by the significantly lower performance resulting from removing
any of the feature types from the full model?the sole exception is the phonetic type
(PH), whose removal in no case negatively affects the accuracy of any classifier. C4.5
and Ripper, on the other hand, appear to take more advantage of some feature types
than others. For the third task, lexical (LX) and discourse (DS) features apparently have
more predictive power for both C4.5 and SVM than the other types. Note also that
for the second and third tasks, the error rates of our full-model SVM classifiers closely
approximate the estimated error rates of human labelers.
For the general task of classifying any ACW into any discourse/pragmatic function,
our full-model SVM classifier achieves the best overall results. To take a closer look at
the performance of this model, we compute its F-measure for the discourse/pragmatic
functions of each individual lexical item, as shown in Table 12. We observe that the
classifier achieves better results for word?function pairs with higher counts in the
Games Corpus, such as yeah-Agr or right-Mod (cf. Table 2). Again, the low counts
for the remaining word?function pairs may prevent a better generalization during the
learning stage, a problem that could be attenuated in future work with boosting and
bootstrapping techniques.
24
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
5.2 Session-Specific and ToBI Prosodic Features
When including session-specific features in the full model, such as identity and gender
of both speakers (see Table 8), the error rate of the SVM classifier is significantly reduced
for the general task (13.3%) and for the discourse boundary function identification task
(6.4%) (Wilcoxon, p < 0.05). For the detection of an acknowledgment function, the error
rate is not modified when including those features (4.5%). This suggests the existence
of speaker differences in the production of at least some functions of ACWs that may be
exploited by ML classifiers. Finally, the inclusion of categorical prosodic features based
on the ToBI framework, such as type of pitch accent and break index on the target word
(see Table 5), does not improve the performance of the SVM-based full models in any of
the classification tasks.
5.3 Individual Features
To estimate the importance of individual features in our classification tasks, we rank
them according to an information-gain metric. We find that for the three tasks, lexical
(LX), discourse (DS), and timing (TM) features dominate. The highest ranked features are
the ones capturing the position of the target word in its IPU and in its turn. Lexical
identity and POS tags of the previous, target, and following words, and duration of
the target word, are also ranked high. Acoustic features appear lower in the ranking;
the best performing ones are word intensity (range, mean, and standard deviation),
pitch (maximum and mean), pitch slope over the final part of the word (200 msec and
second half), voiced-frames ratio, and noise-to-harmonics ratio. All phonetic features
are ranked very low. Note that, whereas durational features at the word level are
ranked high, durational features at the phonetic level are not, because the latter only
capture the duration of each phone relative to the word duration?apparently not an
informative attribute for these classification tasks. These results confirm the existence of
large positional differences across functions of ACWs, as seen in Section 4. Additionally,
whereas several acoustic/prosodic features extracted from the target word contain use-
ful information for the automatic disambiguation of ACWs, it is positional information
that provides the most predictive power.
5.4 Online and Offline Tasks
To simulate the conditions of online applications, which process speech as it is produced
by the user, we consider a subset of features that may typically be extracted from the
Table 12
F-measure achieved by our full-model SVM classifier for the different discourse/pragmatic
functions of each lexical item.
Agr BC CBeg PBeg PEnd Mod BTsk Chk Stl
alright .88 ? .93 ? .33 ? ? ? ?
mm-hm .35 .94 ? ? ? ? ? ? ?
okay .82 .51 .88 .27 .63 .53 0 ? .18
right .84 ? ? ? ? .98 ? .53 ?
uh-huh .35 .93 ? ? ? ? ? ? ?
yeah .96 .54 ? ? .17 ? ? ? ?
25
Computational Linguistics Volume 38, Number 1
speech signal only up to the IPU containing the target ACW. These features are marked
in Tables 4 through 8 with letter O. With these features, we train and evaluate an SVM
classifier for the three tasks described previously. Table 13 shows the results, comparing
the performance of each classifier to that of the models trained on the full feature set,
which simulate the conditions of off-line applications. In all three cases the on-line
model performs significantly worse than its offline correlate, but also significantly better
than the baseline (Wilcoxon, p < 0.05).
Table 13 also shows the error rates of on-line and off-line classifiers trained using
solely text-based features?that is, only features of lexical (LX) or discourse (DS) types.
Text-based models simulate the conditions of spoken dialogue systems with no access
to acoustic and prosodic information, or generation systems attempting to realize text-
based exchanges in speech. They reflect the importance of text information alone in
training such systems to recognize the function of ACWs on-line and off-line and to
produce appropriate realizations from limited or full transcription.
Our on-line and off-line text-based models perform significantly worse than the
corresponding models that use the whole feature set, but they still outperform the
baseline models in all cases (Wilcoxon, p < 0.05). Finally, the off-line text-based models
also outperform their on-line correlates in all three tasks (Wilcoxon, p < 0.05). These
results indicate the important role that other classes of cues play in recognition, while
indicating the level of performance we can expect from TTS systems which have only
text available.
5.5 Backchannel Detection
The correct identification of backchannels is a desirable capability for speech processing
systems, as it would allow us to distinguish between two quite distinct speaker inten-
tions: the intention to take the conversational floor, and the intention to backchannel.
We first consider an off-line binary classification task?namely, classifying all
ACWs in the corpus into backchannels vs. the rest, using information from the whole
conversation. In such a task, an SVM classifier achieves a 4.91% error rate, slightly
yet significantly outperforming a word-based baseline (mm-hm, uh-huh?BC; others?
no-BC), with 5.17% (Wilcoxon, p < 0.05).
On-line applications such as spoken dialogue systems need to classify every new
speaker contribution immediately after (or even while) it is uttered, and certainly
without access to any subsequent context. The Games Corpus contains approximately
6,700 turns following speech from the other speaker, all of which begin as potential
backchannels and need to be disambiguated by the listener. Most of these candidates
can be trivially discarded using a simple observation about backchannels: By definition
Table 13
Error rate of the SVM classifier on online and offline tasks.
All Functions Disc. Boundary Acknowledgment
Feature Set Online Offline Online Offline Online Offline
LX DS TM AC PH (Full model) 17.4% 14.3% 10.1% 6.9% 6.7% 4.5%
LX DS (Text-based) 21.4% 16.8% 13.5% 9.1% 10.0% 5.9%
Word-based baseline 27.7% 18.6% 15.3%
26
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
they are short, isolated utterances, and consist normally in just one ACW. Of the 6,700
candidate turns in the corpus, only 2,351 (35%) begin with an isolated ACW, including
753 of the 757 backchannels in the corpus.12 Thus, an on-line classification procedure
would only need to identify backchannels in those 2,351 turns. At this point, we ex-
plore using a binary classifier for this task. The same word-based majority baseline
described earlier achieves an error rate of 11.56%. An SVM classifier trained on features
extracted from up to the current IPU (to simulate the on-line condition of a spoken
dialogue system) fails to improve over this baseline, achieving an error rate of 11.51%,
not significantly different from the baseline. A possible explanation for this might be
that backchannels seem to be difficult to distinguish from agreements in many cases,
leading to an increase in the error rate. Recall, from the statistical analyses in the
previous section, the positional and acoustic/prosodic similarities of tokens with these
two functions for mm-hm and uh-huh, for example. Shriberg et al (1998) report the same
difficulty in distinguishing these two word functions. We conclude that further research
is needed to develop novel approaches to this crucial problem of spoken dialogue
systems.
5.6 Comparison with Previous Work
In an effort to provide a general frame of reference for our results, we discuss here
what we believe to be the most relevant results from related studies. Note, however,
that comparing these results directly to the results of our classification experiments
is difficult because the type of corpora, definitions used, features examined, and/or
methodology employed vary greatly among the studies. The current study focuses
exclusively on the discourse/pragmatic functions of ACWs whereas other studies have
either a broader or narrower scope.
Among the cue words tested in Litman (1996) is okay, one of the ACWs we also
investigate. Litman describes the automatic classification of cue words in general (in-
cluding, e.g., now, well, say, and so), classifying these into discourse and sentential uses
using a corpus of monologue. In this classification task, which is not performed in our
study, the best results are reached by decision-tree learners trained on prosodic and
text-based features, with an error rate of 13.8%.
The most relevant study to ours is that of Stolcke et al (2000), which presents
experiments on the automatic disambiguation of dialogue acts (DA) on 1,155 sponta-
neous telephone conversations from the Switchboard corpus, labeled using the DAMSL
(Dialogue Act Markup in Several Layers) annotation scheme (Core and Allen 1997). For
the subtask of identifying the Agreement and Backchannels tags collapsed together, the
authors report an error rate of 27.1% when using prosodic features, 19.0% when using
features extracted from the text, and 15.3% when using all features. Other DA classi-
fications also include some of the functions of ACWs discussed in our current study.
For instance, Reithinger and Klesen (1997) employ a Bayesian approach for classifying
18 classes of DAs in transcripts of 437 German dialogues from the VERBMOBIL corpus
(Jekat et al 1995). The DA tags examined include Accept, Confirm, and Feedback, all
of which are related to the functions of ACWs discussed here. For the Accept DA tag,
the authors report an F-measure of 0.69; for Feedback, 0.48; and for Confirm, 0.40. These
12 The four remaining backchannels correspond to a rare phenomenon in which the speaker overlaps the
interlocutor?s last phrase with a short agreement, followed by an optional short pause and a backchannel.
Example: A: but it doesn?t overlap *them. B: right* yeah yeah # okay.
27
Computational Linguistics Volume 38, Number 1
experiments are repeated on transcripts of 163 English dialogues from the same corpus,
yielding an F-measure of 0.78 for the Accept DA tag, and 0 for the other two tags due to
data sparsity.
As part of a study aimed at assessing the effectiveness of machine learning for this
type of task, Core (1998) experiments with hand-coded decision trees for classifying
five high-level dialogue act classes, including AGREEMENT and UNDERSTANDING,
following the DAMSL annotation scheme. On 19 dialogues from the TRAINs corpus
(discussions related to solving transportation problems), Core reports an accuracy of
70% for both the Agreement and the Understanding DA classes, using only the previous
utterance?s DAMSL tag as a feature in the decision trees. This use of DA context in
classifying ACWs would appear to be promising, assuming an accurate automatic
classification of all DAs in the corpus.
Finally, Lampert, Dale, and Paris (2006) describe a statistical classifier trained on
text-based features for automatically predicting eight different speech acts derived from
a taxonomy called Verbal Response Modes (VRM). The experiments are conducted
on transcripts of 1,368 utterances from 14 dialogues in English. For the Acknowledg-
ment speech act (which ?conveys receipt of or receptiveness to other?s communication;
simple acceptance, salutations; e.g., yes? [page 37]), the classifier yields an F-measure
of 0.75.
Again, all of these studies differ significantly from our own, in their task definition,
in their methodology, and in the domain they examine. However, we expect this brief
summary to serve as a general frame of reference for our own classification results.
6. Discussion
In this work we have undertaken a comprehensive study of affirmative cue words, a
subset of cue phrases such as okay, yeah, or alright that may be utilized to convey as
many as ten different discourse/pragmatic functions, such as indicating continued
attention to the interlocutor or cueing the beginning of a new topic. Considering
the high frequency of ACWs in task-oriented dialogue, it is critical for some spoken
language processing applications such as spoken dialogue systems to model the usage
of these words correctly, from both an understanding and a generation perspective.
Section 4 presents statistical evidence of a number of differences in the production
of the various discourse/pragmatic functions of ACWs. The most notable contrasts in
acoustic/prosodic features relate to word final intonation and word intensity. Backchan-
nels typically end in a rising intonation; agreements and cue beginnings, in a falling
intonation. Cue beginnings tend to be produced with a high intensity, and cue endings
with a very low one. Other acoustic/prosodic features?duration, mean pitch, and
voice quality?also seem to vary with the word usage. Our findings related to final
intonation are consistent with previous results obtained by Hockey (1993) and Jurafsky
et al (1998) for American English. For Scottish English, Kowtko (1996) reports a non-
rising intonation for cue beginnings and for her ?reply-y? function, a subclass of our
agreement function. Kowtko also reports observing all types of final intonation in her
?acknowledge? function, whose definition overlaps both our agreements and backchan-
nels. Thus, we find no apparent contradictions between Kowtko?s results for Scottish
English and ours for American English.
The word okay is the most heavily overloaded ACW in our corpus. Our corpus
includes instances conveying each of the ten identified meanings, and this item shows
the highest degree of variation along the acoustic/prosodic features we have examined.
28
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
We speculate from this finding that the more ambiguous an ACW, the more a speaker
needs to vary acoustic/prosodic features to differentiate its meaning.
Our statistical analysis of ACWs also shows that these words display substantial
positional differences across functions, such as the position of the word in its con-
versational turn, or whether the word is preceded and/or followed by silence. Such
large differences bring support to Novick and Sutton?s (1994) claim that the discourse/
pragmatic role of these expressions strongly depends on their basic structural context.
For example, in Novick and Sutton?s words, an ACW in turn-initial position is ?clearly
not serving as a prompt for the other speaker to continue? (page 97).
Previous studies on the automatic disambiguation of other types of cue words, such
as now, well, or like, present the problem as a binary classification task: Each cue word has
either a discourse or a sentential sense (e.g., Litman 1996; Zufferey and Popescu-Belis
2004). In the study of automatic classification of ACWs presented in Section 5 we show
that for spoken task-oriented dialogue, the simple discourse/sentential distinction is
insufficient. In consequence, we define two new classification tasks besides the general
task of classifying any ACW into any function. Our first task, the detection of an
acknowledgment function, has important implications for the language management
component in spoken dialogue systems, which must keep track of which material has
reached mutual belief in a conversation (Bunt, Morante, and Keizer 2007; Roque and
Traum 2009). Our second task, the detection of a discourse segment boundary func-
tion, should help in discourse segmentation and meeting processing tasks (Litman and
Passonneau 1995). Our SVM models based on lexical, discourse, timing, and acoustic
features approach the error rate of trained human labelers in all tasks, while our auto-
matically computed phonetic features offer no improvement. Previous studies indicate
that the pragmatic function of ambiguous expressions may be effectively predicted by
models that combine information extracted from various sources, including lexical and
prosodic (e.g., Litman 1996; Stolcke et al 2000). Our results support this, and extend the
list of useful information sources to include discourse and timing features that may be
easily extracted from the time-aligned transcripts.
Additionally, our machine learning study includes experiments with several combi-
nations of feature sets, in an attempt to simulate the conditions of different applications.
Models that are trained using features extracted only from the speech signal up to the
IPU containing the target word simulate on-line applications such as spoken dialogue
systems with access to acoustic/prosodic features. Although such models perform
worse than ?off-line? models, which make use of left and right context, they still sig-
nificantly outperform our baseline classifiers. Models that simulate the conditions of
current spoken dialogue systems with access only to lexical features (although perhaps
errorful) and TTS systems synthesizing spoken conversations, which have access only
to features extracted from the input text, also significantly outperform our baseline
classifiers.
Interactions between state-of-the-art spoken dialogue systems and their users ap-
pear to contain very few instances of backchannel responses from either conversational
partner. On the system?s side, the absence of this important element of spoken com-
munication may be due to the difficulty of detecting appropriate moments where a
backchannel response would be welcome by the user. Recent advances on that research
topic (Ward and Tsukahara 2000; Cathcart, Carletta, and Klein 2003; Gravano and
Hirschberg 2009a) have encouraged research on ways to equip systems with the ability
to signal to the user that the system is still listening (Maatman, Gratch, and Marsella
2005; Bevacqua, Mancini, and Pelachaud 2008; Morency, de Kok, and Gratch 2008)?
for example, when the user is asked to enter large amounts of information. On the
29
Computational Linguistics Volume 38, Number 1
user?s side, an important reason for not backchanneling may lie in the unnaturalness
of such systems, often described as ?confusing? or even ?intimidating? by users, as
well as their inability to recognize backchannels as such. Nonetheless, recent Wizard-
of-Oz experiments conducted by Hjalmarsson (2009, 2011) show that humans appear
to react to turn-management cues produced by a synthetic voice in the same way that
they react to cues produced by another human. This important finding suggests that
users of spoken dialogue systems could be cued to produce backchannel responses, for
example to determine if they are still paying attention. In that case, it will be crucial for
systems to be able to distinguish backchannels from other pragmatic functions (Shriberg
et al 1998). In Section 5.5 we present results on the task of automatically identifying
backchannel ACWs from the other possible functions. Our models improve over the
baseline in an off-line condition (e.g., for meeting processing tasks), but fail to do so
in an on-line setting (e.g., for spoken dialogue systems). Practically all of the confusion
of this on-line model comes from misclassifying agreements (Agr) as backchannels (BC)
and vice versa. The reliability of our human labelers for distinguishing these two classes
was measured by Fleiss?s ? at 0.570, a level considerably lower than the 0.745 achieved
for the general labeling task, which indicates that the backchannel identification task is
difficult for humans as well, at least when they are not engaged in the conversation itself
but only listening to it after the fact. Although we asked our annotators to distinguish
the agreement function of ACWs from ?continued attention,? there are clearly cases
where people disagree about whether speakers are indicating agreement or not. In
future research we will investigate this issue in more detail, given the relevance of
on-line identification of backchannels in spoken dialogue systems.
In summary, in this study we have identified a number of characterizations of
affirmative cue words in a large corpus of SAE task-oriented dialogue. The corpus
on which our experiments were conducted, rich in ACWs conveying a wide range
of discourse/pragmatic functions, has allowed us to systematically investigate many
dimensions of these words, including their production and automatic disambiguation.
Besides the value of our findings from a linguistic modeling perspective, we believe
that incorporating these results into the production and understanding components of
spoken dialogue systems should improve their performance and increase user satisfac-
tion levels accordingly, getting us one step closer to the long-term goal of effectively
emulating human behavior in dialogue systems.
Appendix A: The COLUMBIA GAMES CORPUS
The COLUMBIA GAMES CORPUS is a collection of 12 spontaneous task-oriented dyadic
conversations elicited from native speakers of Standard American English. The cor-
pus was collected and annotated jointly by the Spoken Language Group at Columbia
University and the Department of Linguistics at Northwestern University. In each of
the 12 sessions, two subjects were paid to play a series of computer games requiring
verbal communication to achieve joint goals of identifying and moving images on the
screen. Each subject used a separate laptop computer and could not see the screen of the
other subject. They sat facing each other in a soundproof booth, with an opaque curtain
hanging between them, so that all communication was verbal. The subjects? speech was
not restricted in any way, and it was emphasized at the session beginning that the game
was not timed. Subjects were told that their goal was to accumulate as many points as
possible over the entire session, since they would be paid additional money for each
point they earned.
30
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
A.1 Game Tasks
Subjects were first asked to play three instances of the CARDS game, where they were
shown cards with one to four images on them. Images were of two sizes (small or large)
and various colors, and were selected to contain primarily voiced consonants, which
facilitates pitch track computation (e.g., yellow lion, blue mermaid). There were two parts
to each Cards game, designed to vary genre from primarily monologue to dialogue.
In the first part of the Cards game, each player?s screen displayed a stack of 9 or
10 cards (Figure A1a). Player A was asked to describe the top card on her pile, while
Player B was asked to search through his pile to find the same card, clicking a button
when he found it. This process was repeated until all cards in Player A?s deck were
matched. In all cases, Player B?s deck contained one additional card that had no match
in Player A?s deck, to ensure that she would need to describe all cards.
In the second part of the Cards game, each player saw a board of 12 cards on the
screen (Figure A1b), all initially face down. As the game began, the first card on one
player?s (the DESCRIBER?s) board was automatically turned face up. The Describer
was told to describe this card to the other player (the SEARCHER), who was to find a
matching card from the cards on his board. If the Searcher could not find a card exactly
matching the Describer?s card, but could find a card depicting one or more of the objects
on that card, the players could decide whether to declare a partial match and receive
points proportional to the numbers of objects matched on the cards. At most three cards
were visible to each player at any time, with cards seen earlier being automatically
turned face down as the game progressed. Players switched roles after each card was
described and the process continued until all cards had been described. The players
were given additional opportunities to earn points, based on other characteristics of the
matched cards, to make the game more interesting and to encourage discussion.
After completing all three instances of the Cards game, subjects were asked to play
a final game, the OBJECTS game. As in the Cards game, all images were selected to
have likely descriptions which were as voiced and sonorant as possible. In the Objects
game, each player?s laptop displayed a game board with 5 to 7 objects (Figure A1c).
Both players saw the same set of objects at the same position on the screen, except for
one (the TARGET). For the DESCRIBER, the target object appeared in a random location
among other objects on the screen; for the FOLLOWER, the target object appeared at the
bottom of the screen. The Describer was instructed to describe the position of the target
object on her screen so that the Follower could move his representation to the same
location on his own screen. After players negotiated what they believed to be their best
Figure A1
Sample screens from the Cards games (a, b) and Objects games (c).
31
Computational Linguistics Volume 38, Number 1
location match, they were awarded 1 to 100 points based on how well the Follower?s
target location matched the Describer?s.
The Objects game proceeded through 14 tasks. In the initial four tasks, one of
the subjects always acted as the Describer, and the other one as the Follower. In the
following four tasks their roles were inverted: The subject who played the Describer
role in the initial four tasks was now the Follower, and vice versa. In the final six tasks,
they alternated the roles with each new task.
A.2 Subjects and Sessions
Thirteen subjects (six women, seven men) participated in the study, which took place in
October 2004 in the Speech Lab at Columbia University. Eleven of the subjects partici-
pated in two sessions on different days, each time with a different partner. All subjects
reported being native speakers of Standard American English and having no hearing
impairments. Their ages ranged from 20 to 50 years (mean, 30.0; standard deviation,
10.9), and all subjects lived in the New York City area at the time of the study. They
were contacted through the classified advertisements Web site craigslist.org.
We recorded twelve sessions, each containing an average of 45 minutes of dialogue,
totaling roughly 9 hours of dialogue in the corpus. Of those, 70 minutes correspond to
the first part of the Cards game, 207 minutes to the second part of the Cards game, and
258 minutes to the Objects game. Each subject was recorded on a separate channel of
a DAT recorder, at a sample rate of 48 kHz with 16-bit precision, using a Crown head-
mounted close-talking microphone. Each session was later downsampled to 16 kHz,
16-bit precision, and saved as one stereo .wav file with one player per channel, and also
as two separate mono .wav files, one for each player.
Trained annotators orthographically transcribed the recordings of the Games Cor-
pus and manually aligned the words to the speech signal, yielding a total of 70,259
words and 2,037 unique words in the corpus. Additionally, self repairs and certain non-
word vocalizations were marked, including laughs, coughs, and breaths. Intonational
patterns and other aspects of the prosody were identified using the ToBI transcription
framework (Beckman and Hirschberg 1994; Pitrelli, Beckman, and Hirschberg 1994):
Trained annotators intonationally transcribed all of the Objects portion of the corpus
(258 minutes of dialogue) and roughly one third of the Cards portion (90 minutes).
Appendix B: ACW Labeling Guidelines
These guidelines for labeling the discourse/pragmatic functions of affirmative cue
words were developed by Julia Hirschberg, ?tefan Ben?u?, Agust?n Gravano, and
Michael Mulley at Columbia University.
Classification Scheme
Most of the labels are defined using okay, but the definitions hold for all of these words:
alright, gotcha, huh, mm-hm, okay, right, uh-huh, yeah, yep, yes, yup. If you really have no
clue about the function of a word, label it as ?.
[Mod] Literal Modifiers: In this case the words are used as modifiers. Examples:
?I think that?s okay.?
?It?s right between the mermaid and the car.?
?Yeah, that?s right.?
32
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
[Agr] Acknowledge/Agreement: The function of okay that indicates ?I believe what you
said?, and/or ?I agree with what you say.? This label should also be used for okay after
another okay or after an evaluative comment like ?Great? or ?Fine? in its role as an
acknowledgment.13 Examples:
A: Do you have a blue moon?
B: Yeah.
A: Then move it to the left of the yellow mermaid.
B: Okay, gotcha. Let?s see... (Here, both okay and gotcha are labeled Agr.)
[CBeg] Cue Beginning: The function of okay that marks a new segment of a discourse
or a new topic. Test: could this use of okay be replaced by ?Now??
[PBeg] Pivot Beginning: (Agr+CBeg) When okay functions as both a cue word and as an
Acknowledge/Agreement. Test: Can okay be replaced by ?Okay now? with the same
pragmatic meaning?
[CEnd] Cue Ending: The function of okay that marks the end of a current segment of a
discourse or a current topic. Example: ?So that?s done. Okay.?
[PEnd] Pivot Ending: (Agr+CEnd) When okay functions as both a cue word and as an
Acknowledge/Agreement, but ends a discourse segment.
[BC] Backchannel: The function of okay in response to another speaker?s utterance that
indicates only ?I?m still here / I hear you and please continue.?
[Stl] Stall: Okay used to stall for time while keeping the floor. Test: Can okay be replaced
by an elongated ?Um? or ?Uh? with the same pragmatic meaning? ?So I yeah I think
we should go together.?
[Chk] Check: Okay used with the meaning ?Is that okay?? or ?Is everything okay?? For
example, ?I?m stopping now, okay??
[BTsk] Back from a task: ?I?ve just finished what I was doing and I?m back.? Typical
case: One subject spends some time thinking, and then signals s/he is ready to continue
the discourse.
Special Cases
(1) ?okay so? / ?okay now? / ?okay then? / and so forth, where both words are uttered
together, okay seems to convey Agr, and so / now / then seems to convey CBeg. Because
we do not label words like so, now, or then, we label okay as PBeg.
(2) If you encounter a rapid sequence of the same word several times in a row, all of them
uttered in one ?burst? of breath, mark only the first one the corresponding label, and
label the others with ???. Example: ?okay yeah yeah yeah? should be labeled as: ?okay:Agr
yeah:Agr yeah:? yeah:??.
13 Throughout this article we have used the term ?agreement? to avoid confusion with other definitions of
?acknowledgment? found in the literature.
33
Computational Linguistics Volume 38, Number 1
Appendix C: ACW Labeling Examples
This appendix lists a number of examples of each type of ACWs from the Columbia
Games Corpus, as labeled by our annotators. Each ACW is highlighted and annotated
with its majority label. Overlapping speech segments are embraced by square brackets,
and additional notes are given in parentheses.
A: it?s aligned to the f- to the foot of the M&M guy like to the bottom of the iron
B: okayAgr lines up
A: yeahAgr it?s it?s almost it?s just barely like over
B: okayAgr
A: the tail
B: mm-hmBC
A: of the iron
B: mm-hmBC
A: is past the it?s a little bit past the mermaid?s body
A: when you look at the lower left corner of the iron
B: [okayBC]
A: [where] the turquoise stuff is [and you]
B: [mm-hmBC]
A: know the bottom point out to the farthest left for that region
A: the blinking image is a lawnmower
B: okayBC
A: and it?s gonna go below the yellow lion and above the bl- blue lion
B: mm-hmBC
A: the bottom black part is almost aligned to the white feet of the M&M guy
B: [okayAgr]
A: [yeahPEnd] (end-of-task)
A: okayCBeg um the blinking image is the iron
A: okayCBeg it?s uh the l- I guess the lime that?s blinking
A: nothing lined up real well
B: yeahAgr that?s rightMod
A: that was good okayCEnd
A: that?s awesome
B: you?re still the ace alrightCEnd
A: his beak?s kinda orange rightChk
B: uh-huhAgr
A: you can?t see any of that
34
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
A: that?s like a smaller amount than it is on the rightMod side to the ear [rightChk]
B: [rightAgr]
A: okayAgr
A: the lower rightMod corner
B: yeahAgr the lower rightMod corner
A: let?s start over
B: okayAgr
A: okayPBeg so you have your crescent moon
A: but not any of the yellow [part]
B: [okayPBeg] so would the top of the ear be aligned to like where
A: the like head of the lion to like the where the grass shoots out there?s that?s a significant
difference
B: okayPBeg so there?s definitely a bigger space from the blue lion to the lawnmower than there
is from the handle to the feet of the yellow
A: alright? I?ll try it (7.81 sec) okayBTsk
B: okayCBeg the owl is blinking
A: that thing is gonna be like (0.99 sec) okayStl (0.61 sec) one pixel to the rightMod of the edge
Acknowledgments
This work was supported in part by
NSF IIS-0307905, NSF IIS-0803148,
ANPCYT PICT-2009-0026, UBACYT
20020090300087, CONICET, and the Slovak
Agency for Science and Research Support
(APVV-0369-07). We thank Fadi Biadsy,
H?ctor Ch?vez, Enrique Henestroza,
Jackson Liscombe, Shira Mitchell, Michael
Mulley, Andrew Rosenberg, Elisa Sneed
German, Ilia Vovsha, Gregory Ward, and
Lauren Wilcox for valuable discussions
and for their help in collecting, labeling,
and processing the data.
References
Allwood, J., J. Nivre, and E. Ahlsen. 1992. On
the semantics and pragmatics of linguistic
feedback. Journal of Semantics, 9(1):1?30.
Beckman, Mary E. and Julia Hirschberg.
1994. The ToBI annotation conventions.
Available on-line at http://www.ling.
ohio-state.edu/?tobi/ame_tobi/
annotation_conventions.html.
Bevacqua, E., M. Mancini, and C. Pelachaud.
2008. A listening agent exhibiting variable
behavior. In B. H. Prendinger, J. Lester, and
M. Ishizuka, editors, Intelligent Virtual
Agents, pages 262?269. Springer, Berlin.
Bhuta, T., L. Patrick, and J. D. Garnett.
2004. Perceptual evaluation of voice
quality and its correlation with
acoustic measurements. Journal of
Voice, 18(3):299?304.
Boersma, Paul and David Weenink. 2001.
Praat: Doing phonetics by computer.
Available at http://www.praat.org.
Brown, G., K. L. Currie, and J. Kenworthy.
1980. Questions of Intonation. University
Park Press, Baltimore, MD.
Bunt, H. C. 1989. Information dialogues as
communicative actions in relation to user
modelling and information processing. In
M. M. Taylor, F. Neel, and D. G. Bouwhuis,
editors, The Structure of Multimodal
Dialogue, pages 47?73. Elsevier,
Amsterdam.
Bunt, H. C., R. Morante, and S. Keizer. 2007.
An empirically based computational
model of grounding in dialogue. In
35
Computational Linguistics Volume 38, Number 1
Proceedings of the 8th SIGdial Workshop on
Discourse and Dialogue, pages 283?290,
Antwerp.
Cathcart, N., J. Carletta, and E. Klein. 2003. A
shallow model of backchannel continuers
in spoken dialogue. In Proceedings of the
10th Conference of the European Chapter of the
Association for Computational Linguistics
(EACL), pages 51?58, Budapest.
Charniak, Eugene and Mark Johnson. 2001.
Edit detection and parsing for transcribed
speech. In Proceedings of the 2nd Meeting of
the North American Chapter of the Association
for Computational Linguistics (NAACL),
pages 118?126, Pittsburgh, PA.
Clark, H. H. and Susan Brennan. 1991.
Grounding in communication. In L.
Resnick, J. Levine, and S. Teasley, editors,
Perspectives on Socially Shared Cognition,
pages 127?149. American Psychological
Association (APA), Hyattsville, MD.
Clark, H. H. and E. F. Schaefer. 1989.
Contributing to discourse. Cognitive
Science, 13:259?294.
Cohen, Robin. 1984. A computational theory
of the function of clue words in argument
understanding. In Proceedings of the
22nd Annual Meeting Association for
Computational Linguistics (ACL),
pages 251?258, Stanford, CA.
Cohen, William C. 1995. Fast effective rule
induction. In Proceedings of the 12th
International Conference on Machine
Learning, pages 115?123, Tahoe City, CA.
Core, Mark G. 1998. Analyzing and
predicting patterns of DAMSL utterance
tags. In Working Notes of the AAAI Spring
Symposium on Applying Machine Learning
to Discourse Processing, pages 18?24,
Stanford, CA.
Core, M. G. and J. Allen. 1997. Coding
dialogs with the damsl annotation scheme.
In Proceedings of the AAAI Fall Symposium
on Communicative Action in Humans and
Machines, pages 28?35, Cambridge, MA.
Cortes, Corinna and Vladimir Vapnik. 1995.
Support vector networks. Machine
Learning, 20(3):273?297.
Duncan, Starkey. 1972. Some signals and
rules for taking speaking turns in
conversations. Journal of Personality
and Social Psychology, 23(2):283?292.
Eskenazi, L., D. G. Childers, and D. M.
Hicks. 1990. Acoustic correlates of vocal
quality. Journal of Speech, Language and
Hearing Research, 33(2):298?306.
Fleiss, J. L. 1971. Measuring nominal
scale agreement among many raters.
Psychological Bulletin, 76(5):378?382.
Garofolo, John S., Lori F. Lamel, William M.
Fisher, Jonathan G. Fiscus, David S. Pallett,
Nancy L. Dahlgren, and Victor Zue.
1993. Ldc93s1: Timit acoustic-phonetic
continuous speech corpus. Linguistic Data
Consortium, University of Pennsylvania,
Philadelphia.
Godfrey, J. J., E. C. Holliman, and
J. McDaniel. 1992. SWITCHBOARD:
Telephone speech corpus for research and
development. In Proceedings of the IEEE
International Conference on Acoustics, Speech,
and Signal Processing, pages 517?520,
San Francisco, CA.
Goodwin, C. 1981. Conversational
Organization: Interaction Between Speakers
and Hearers. Academic Press, New York.
Gravano, Agust?n, Stefan Benus, H?ctor
Ch?vez, Julia Hirschberg, and Lauren
Wilcox. 2007. On the role of context
and prosody in the interpretation of
?okay?. In Proceedings of the 45th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 800?807,
Prague.
Gravano, Agust?n and Julia Hirschberg.
2009a. Backchannel-inviting cues in
task-oriented dialogue. In Proceedings of
Interspeech, pages 1019?1022, Brighton.
Gravano, Agust?n and Julia Hirschberg.
2009b. Turn-yielding cues in task-oriented
dialogue. In Proceedings of the 10th SIGdial
Workshop on Discourse and Dialogue,
pages 253?261, London.
Gravano, Agust?n and Julia Hirschberg.
2011. Turn-taking cues in task-oriented
dialogue. Computer Speech and Language,
25(3):601?634.
Grosz, Barbara and Candace Sidner. 1986.
Attention, intention, and the structure
of discourse. Computational Linguistics,
12(3):175?204.
Hirschberg, J. 1990. Accent and discourse
context: Assigning pitch accent in
synthetic speech. In Proceedings of the 8th
National Conference on Artificial Intelligence,
volume 2, pages 952?957, Boston, MA.
Hirschberg, Julia and Diane Litman. 1987.
Now let?s talk about now: Identifying
cue phrases intonationally. In Proceedings
of the 25th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 163?171, Stanford, CA.
Hirschberg, Julia and Diane Litman. 1993.
Empirical studies on the disambiguation
of cue phrases. Computational Linguistics,
19(3):501?530.
Hirschberg, Julia and Christine Nakatani.
1996. A prosodic analysis of discourse
36
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
segments in direction-giving monologues.
In Proceedings of the 34th Annual Meeting
Association for Computational Linguistics
(ACL), pages 286?293, Santa Cruz, CA.
Hjalmarsson, Anna. 2009. On cue?Additive
effects of turn-regulating phenomena in
dialogue. In Proceedings of Diaholmia?
13th Workshop on the Semantics and
Pragmatics of Dialogue, pages 27?34,
Stockholm.
Hjalmarsson, Anna. 2011. The additive
effect of turn-taking cues in human and
synthetic voice. Speech Communication,
53(1):23?35.
Hobbs, Jerry R. 1990. The
Pierrehumbert-Hirschberg theory of
intonational meaning made simple:
Comments on Pierrehumbert and
Hirschberg. In P. R. Cohen, J. Morgan,
and M. E. Pollack, editors, Intentions in
Communication. MIT Press, Cambridge,
MA, pages 313?323.
Hockey, B. A. 1993. Prosody and the role
of ?okay? and ?uh-huh? in discourse.
In Proceedings of the Eastern States
Conference on Linguistics, pages 128?136,
Columbus, OH.
Jefferson, G. 1984. Notes on a systematic
deployment of the acknowledgement
tokens ?yeah?; and ?mm hm?. Research
on Language & Social Interaction,
17(2):197?216.
Jekat, S., A. Klein, E. Maier, I. Maleck,
M. Mast, and J. J. Quantz. 1995. Dialogue
acts in VERBMOBIL. Technical report
Verbmobil-Report 65, Universitaet
Erlangen, Berlin.
Jurafsky, Daniel, Elizabeth Shriberg,
Barbara Fox, and Traci Curl. 1998. Lexical,
prosodic, and syntactic cues for dialog
acts. In Proceedings of ACL/COLING,
Workshop on Discourse Relations and
Discourse Markers, pages 114?120,
Montreal.
Kendon, A. 1967. Some functions of
gaze-direction in social interaction.
Acta Psychologica, 26:22?63.
Koiso, H., Y. Horiuchi, S. Tutiya, A. Ichikawa,
and Y. Den. 1998. An analysis of
turn-taking and backchannels based on
prosodic and syntactic features in Japanese
Map Task dialogs. Language and Speech:
Special Issue on Prosody and Conversation,
41(3-4):295?321.
Kowtko, Jacqueline C. 1996. The Function
of Intonation in Task-Oriented Dialogue.
Ph.D. thesis, University of Edinburgh.
Lampert, A., R. Dale, and C. Paris. 2006.
Classifying speech acts using verbal
response modes. In Proceedings of the
Australasian Language Technology
Workshop, pages 34?41, Sydney.
Litman, Diane. 1994. Classifying cue
phrases in text and speech using machine
learning. In Proceedings of the 12th National
Conference on Artificial Intelligence - AAAI,
pages 806?813, Seattle, WA.
Litman, Diane. 1996. Cue phrase
classification using machine learning.
Journal of Artificial Intelligence, 5:53?94.
Litman, Diane and Julia Hirschberg. 1990.
Disambiguating cue phrases in text and
speech. In Proceedings of the 13th
International Conference on Computational
Linguistics, pages 251?256, Helsinki.
Litman, D. J. and J. F. Allen. 1987. A plan
recognition model for subdialogues
in conversations. Cognitive Science,
11(2):163?200.
Litman, D. J. and R. J. Passonneau. 1995.
Combining multiple knowledge
sources for discourse segmentation.
In Proceedings of the 33rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 108?115,
Cambridge, MA.
Maatman, R. M., J. Gratch, and S. Marsella.
2005. Natural behavior of a listening
agent. In 5th International Conference on
Intelligent Virtual Agents, pages 25?36, Kos.
Marcus, M. P., M. A. Marcinkiewicz, and
B. Santorini. 1993. Building a large
annotated corpus of English: The Penn
Treebank. Computational Linguistics,
19(2):313?330.
Morency, L. P., I. de Kok, and J. Gratch.
2008. Predicting listener backchannels:
A probabilistic multimodal approach.
In Proceedings of the 8th International
Conference on Intelligent Virtual Agents,
pages 176?190, Tokyo.
Mushin, I., L. Stirling, J. Fletcher, and
R. Wales. 2003. Discourse structure,
grounding, and prosody in task-oriented
dialogue. Discourse Processes, 35(1):1?31.
Novick, D. G. and S. Sutton. 1994. An
empirical model of acknowledgment for
spoken-language systems. In Proceedings
of the 32nd Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 96?101, Morristown, NJ.
Pierrehumbert, Janet and Julia Hirschberg.
1990. The meaning of intonational
contours in the interpretation of
discourse. In P. R. Cohen, J. Morgan,
and M. E. Pollack, editors, Intentions in
Communication. MIT Press, Cambridge,
MA, pages 271?311.
37
Computational Linguistics Volume 38, Number 1
Pierrehumbert, J. B. 1980. The Phonology and
Phonetics of English Intonation. Ph.D. thesis,
Massachusetts Institute of Technology,
Cambridge, MA.
Pitrelli, John F., Mary E. Beckman, and Julia
Hirschberg. 1994. Evaluation of prosodic
transcription labeling reliability in the
ToBI framework. In Proceedings of the
International Conference of Spoken Language
Processing (ICSLP), pages 123?126,
Yokohama.
Quinlan, John Ross. 1993. C4.5: Programs for
Machine Learning. Morgan Kaufmann,
Waltham, MA.
Ratnaparkhi, A., E. Brill, and K. Church.
1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 133?142,
Philadelphia, PA.
Redeker, G. 1991. Review article: Linguistic
markers of linguistic structure. Linguistics,
29(6):1139?1172.
Reichman, Rachel. 1985. Getting Computers
to Talk like You and Me. MIT Press,
Cambridge, MA.
Reithinger, N. and M. Klesen. 1997. Dialogue
act classification using language models.
In Proceedings of the 5th European Conference
on Speech Communication and Technology,
pages 2235?2238, Rhodes.
Roque, A. and D. Traum. 2009. Improving a
virtual human using a model of degrees
of grounding. In Proceedings of the 21st
International Joint Conferences on Artificial
Intelligence (IJCAI), pages 1537?1542,
Pasadena, CA.
Rosenberg, A. and J. Hirschberg. 2009.
Detecting pitch accent at the word,
syllable and vowel level. In Proceedings
of the North American Chapter of
the Association for Computational
Linguistics ? Human Language
Technologies (NAACL-HLT) Conference,
pages 81?84, Boulder, CO.
Rosenberg, Andrew. 2010a. AuToBI ? A
tool for automatic ToBI annotation. In
Proceedings of Interspeech, pages 146?149,
Makuhari.
Rosenberg, Andrew. 2010b. Classification
of prosodic events using quantized
contour modeling. In Proceedings of the
North American Chapter of the Association
for Computational Linguistics ? Human
Language Technologies (NAACL-
HLT) Conference, pages 721?724,
Los Angeles, CA.
Sacks, H., E. A. Schegloff, and G. Jefferson.
1974. A simplest systematics for the
organization of turn-taking for
conversation. Language, 50:696?735.
Schegloff, E. A. 1982. Discourse as an
interactional achievement: Some
uses of ?uh huh? and other things
that come between sentences. In
Tannen D, editor, Analyzing Discourse:
Text and Talk, pages 71?93. APA,
Hyattsville, MD.
Schiffrin, Deborah. 1987. Discourse
Markers. Cambridge University Press,
Cambridge, UK.
Shriberg, E., R. Bates, A. Stolcke, P. Taylor,
D. Jurafsky, K. Ries, N. Coccaro, R. Martin,
M. Meteer, and C. Van Ess-Dykema.
1998. Can prosody aid the automatic
classification of dialog acts in
conversational speech? Language
and Speech, 41(3-4):443?492.
Stolcke, A., K. Ries, N. Coccaro, E. Shriberg,
R. Bates, D. Jurafsky, P. Taylor, R. Martin,
C. V. Ess-Dykema, and M. Meteer. 2000.
Dialogue act modeling for automatic
tagging and recognition of conversational
speech. Computational Linguistics,
26(3):339?373.
Traum, David. 1994. A Computational
Theory of Grounding in Natural Language
Conversation. Ph.D. thesis, Rochester
University, Rochester, NY.
Traum, David and James Allen. 1992.
A speech acts approach to grounding
in conversation. In Proceedings of the
International Conference on Spoken
Language Processing (ICSLP),
pages 137?140, Banff.
Vapnik, Vladimir N. 1995. The Nature of
Statistical Learning Theory. Springer-Verlag,
New York.
Walker, M. A. 1992. Redundancy in
collaborative dialogue. In Proceedings
of the 14th Conference on Computational
Linguistics, pages 345?351, Morristown, NJ.
Walker, M. A. 1993a. Informational
Redundancy and Resource Bounds in
Dialogue. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Walker, M. A. 1993b. When given
information is accented: Repetition,
paraphrase and inference in dialogue.
In LSA Annual Meeting, pages 231?240,
Los Angeles, CA.
Walker, M. A. 1996. Inferring acceptance and
rejection in dialogue. Language and Speech,
39(2-3).
Ward, N. and W. Tsukahara. 2000. Prosodic
features which cue back-channel responses
in English and Japanese. Journal of
Pragmatics, 32(8):1177?1207.
38
Gravano, Hirschberg, and Ben?u? Affirmative Cue Words in Task-Oriented Dialogue
Witten, I. H. and E. Frank. 2000. Data Mining:
Practical Machine Learning Tools and
Techniques with Java Implementations.
Morgan Kaufmann, Waltham, MA.
Yngve, V. H. 1970. On getting a word in
edgewise. In Proceedings of the 6th Regional
Meeting of the Chicago Linguistic Society,
volume 6, pages 657?677, Chicago, IL.
Young, S., G. Evermann, M. Gales,
D. Kershaw, G. Moore, J. Odell,
D. Ollason, D. Povey, V. Valtchev, and
P. Woodland. 2006. The HTK Book,
version 3.4. Available on-line at
http://htk.eng.cam.ac.uk.
Zufferey, S. and A. Popescu-Belis. 2004.
Towards automatic identification of
discourse markers in dialogs: The case of
?like?. In Proceedings of the 5th SIGdial
Workshop on Discourse and Dialogue,
pages 63?71, Boston, MA.
39
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 11?19,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Acoustic-Prosodic Entrainment and Social Behavior
Rivka Levitan1, Agust??n Gravano2, Laura Willson1,
S?tefan Ben?us?3, Julia Hirschberg1, Ani Nenkova4
1 Dept. of Computer Science, Columbia University, New York, NY 10027, USA
2 Departamento de Computacio?n (FCEyN), Universidad de Buenos Aires, Argentina
3 Constantine the Philosopher University & Institute of Informatics, Slovak Academy of Sciences, Slovakia
4 Dept. of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104, USA
rlevitan@cs.columbia.edu, gravano@dc.uba.ar, law2142@barnard.edu,
sbenus@ukf.sk, julia@cs.columbia.edu, nenkova@seas.upenn.edu
Abstract
In conversation, speakers have been shown
to entrain, or become more similar to each
other, in various ways. We measure entrain-
ment on eight acoustic features extracted from
the speech of subjects playing a cooperative
computer game and associate the degree of en-
trainment with a number of manually-labeled
social variables acquired using Amazon Me-
chanical Turk, as well as objective measures
of dialogue success. We find that male-female
pairs entrain on all features, while male-male
pairs entrain only on particular acoustic fea-
tures (intensity mean, intensity maximum and
syllables per second). We further determine
that entrainment is more important to the per-
ception of female-male social behavior than it
is for same-gender pairs, and it is more impor-
tant to the smoothness and flow of male-male
dialogue than it is for female-female or mixed-
gender pairs. Finally, we find that entrainment
is more pronounced when intensity or speak-
ing rate is especially high or low.
1 Introduction
Entrainment, also termed alignment, adaptation,
priming or coordination, is the phenomenon of
conversational partners becoming more similar to
each other in what they say, how they say it,
and other behavioral phenomena. Entrainment has
been shown to occur for numerous aspects of spo-
ken language, including speakers? choice of re-
ferring expressions (Brennan & Clark, 1996); lin-
guistic style (Niederhoffer & Pennebaker, 2002;
Danescu-Niculescu-Mizil et al, 2011); syntactic
structure (Reitter et al, 2006); speaking rate (Lev-
itan & Hirschberg, 2011); acoustic/prosodic fea-
tures such as fundamental frequency, intensity, voice
quality (Levitan & Hirschberg, 2011); and phonet-
ics (Pardo, 2006).
Entrainment in many of these dimensions has also
been associated with different measures of dialogue
success. For example, Chartrand and Bargh (1999)
demonstrated that mimicry of posture and behavior
led to increased liking between the dialogue par-
ticipants as well as a smoother interaction. They
also found that naturally empathetic individuals ex-
hibited a greater degree of mimicry than did oth-
ers. Nenkova et al (2008) found that entrainment
on high-frequency words was correlated with nat-
uralness, task success, and coordinated turn-taking
behavior. Natale (1975) showed that an individ-
ual?s social desirability, or ?propensity to act in
a social manner,? can predict the degree to which
that individual will match her partner?s vocal inten-
sity. Levitan et al (2011) showed that entrainment
on backchannel-preceding cues is correlated with
shorter latency between turns, fewer interruptions,
and a higher degree of task success. In a study of
married couples discussing problems in their rela-
tionships, Lee et al (2010) found that entrainment
measures derived from pitch features were signifi-
cantly higher in positive interactions than in nega-
tive interactions and were predictive of the polarity
of the participants? attitudes.
These studies have been motivated by theoreti-
cal models such as Giles? Communication Accom-
modation Theory (Giles & Coupland, 1991), which
proposes that speakers promote social approval or
11
efficient communication by adapting to their inter-
locutors? communicative behavior. Another theory
informing the association of entrainment and dia-
logue success is the coordination-rapport hypoth-
esis (Tickle-Degnen & Rosenthal, 1990), which
posits that the degree of liking between conversa-
tional partners should be correlated with the degree
of nonverbal coordination between them.
Motivated by such theoretical proposals and em-
pirical findings, we hypothesized that entrainment
on acoustic/prosodic dimensions such as pitch, in-
tensity, voice quality and speaking rate might also
be correlated with positive aspects of perceived
social behaviors as well as other perceived char-
acteristics of efficient, well-coordinated conversa-
tions. In this paper we describe a series of ex-
periments investigating the relationship between ob-
jective acoustic/prosodic dimensions of entrainment
and manually-annotated perception of a set of so-
cial variables designed to capture important as-
pects of conversational partners? social behaviors.
Since prior research on other dimensions of entrain-
ment has sometimes observed differences in degree
of entrainment between female-female, male-male
and mixed gender groups (Bilous & Krauss, 1988;
Pardo, 2006; Namy et al, 2002), we also exam-
ined our data for variation by gender pair, consid-
ering female-female, male-male, and female-male
pairs of speakers separately. If previous findings
extend to acoustic/prosodic entrainment, we would
expect female-female pairs to entrain to a greater
degree than male-male pairs and female partners in
mixed gender pairs to entrain more than their male
counterparts. Since prior findings posit that entrain-
ment leads to smoother and more natural conversa-
tions, we would also expect degree of entrainment
to correlate with perception of other characteristics
descriptive of such conversations.
Below we describe the corpus and annotations
used in this study and how our social annotations
were obtained in Sections 2 and 3. We next discuss
our method and results for the prevalence of entrain-
ment among different gender groups (Section 4). In
Sections 5 and 6, we present the results of correlat-
ing acoustic entrainment with social variables and
objective success measures, respectively. Finally, in
Section 7, we explore entrainment in cases of outlier
feature values.
2 The Columbia Games Corpus
The Columbia Games Corpus (Gravano & Hirsch-
berg, 2011) consists of approximately nine hours
of spontaneous dialogue between pairs of subjects
playing a series of computer games. Six females and
seven males participated in the collection of the cor-
pus; eleven of the subjects returned on a different
day for another session with a new partner.
During the course of each session, a pair of speak-
ers played three Cards games and one Objects game.
The work described here was carried out on the Ob-
jects games. This section of each session took 7m
12s on average. We have a total of 4h 19m of Ob-
jects game speech in the corpus.
For each task in an Objects game, the players
saw identical collections of objects on their screens.
However, one player (the Describer) had an addi-
tional target object positioned among the other ob-
jects, while the other (the Follower) had the same
object at the bottom of her screen. The Describer
was instructed to describe the position of the target
object so that the Follower could place it in exactly
the same location on her screen. Points (up to 100)
were awarded based on how well the Follower?s tar-
get location matched the describers. Each pair of
partners completed 14 such tasks, alternating roles
with each task. The partners were separated by a
curtain to ensure that all communication was oral.
The entire corpus has been orthographically tran-
scribed and words aligned with the speech source. It
has also been ToBI-labeled (Silverman et al, 1992)
for prosodic events, as well as labeled for turn-
taking behaviors.
3 Annotation of Social Variables
In order to study how entrainment in various dimen-
sions correlated with perceived social behaviors of
our subjects, we asked Amazon Mechanical Turk1
annotators to label the 168 Objects games in our cor-
pus for an array of social behaviors perceived for
each of the speakers, which we term here ?social
variables.?
Each Human Intelligence Task (HIT) presented to
the AMT workers for annotation consisted of a sin-
gle Objects game task. To be eligible for our HITs,
1http://www.mturk.com
12
annotators had to have a 95% success rate on pre-
vious AMT HITs and to be located in the United
States. They also had to complete a survey estab-
lishing that they were native English speakers with
no hearing impairments. The annotators were paid
$0.30 for each HIT they completed. Over half of the
annotators completed fewer than five hits, and only
four completed more than twenty.
The annotators listened to an audio clip of the
task, which was accompanied by an animation that
displayed a blue square or a green circle depending
on which speaker was currently talking. They were
then asked to answer a series of questions about each
speaker: Does Person A/B believe s/he is better than
his/her partner? Make it difficult for his/her partner
to speak? Seem engaged in the game? Seem to dis-
like his/her partner? Is s/he bored with the game?
Directing the conversation? Frustrated with his/her
partner? Encouraging his/her partner? Making
him/herself clear? Planning what s/he is going to
say? Polite? Trying to be liked? Trying to domi-
nate the conversation? They were also asked ques-
tions about the dialogue as a whole: Does it flow
naturally? Are the participants having trouble un-
derstanding each other? Which person do you like
more? Who would you rather have as a partner?
A series of check questions with objectively de-
terminable answers (e.g. ?Which speaker is the De-
scriber??) were included among the target questions
to ensure that the annotators were completing the
task with integrity. HITs for which the annotator
failed to answer the check questions correctly were
disqualified.
Each task was rated by five unique annotators who
answered ?yes? or ?no? to each question, yielding
a score ranging from 0 to 5 for each social vari-
able, representing the number of annotators who an-
swered ?yes.? A fuller description of the annotation
for social variables can be found in (Gravano et al,
2011).
In this study, we focus our analysis on annotations
of four social variables:
? Is the speaker trying to be liked?
? Is the speaker trying to dominate the conversa-
tion?
? Is the speaker giving encouragement to his/her
partner?
? Is the conversation awkward?
We correlated annotations of these variables with
an array of acoustic/prosodic features.
4 Acoustic entrainment
We examined entrainment in this study in eight
acoustic/prosodic features:
? Intensity mean
? Intensity max
? Pitch mean
? Pitch max
? Jitter
? Shimmer
? Noise-to-harmonics ratio (NHR)
? Syllables per second
Intensity is an acoustic measure correlated with
perceived loudness. Jitter, shimmer, and noise-to-
harmonics ratios are three measures of voice quality.
Jitter describes varying pitch in the voice, which is
perceived as a rough sound. Shimmer describes fluc-
tuation of loudness in the voice. Noise-to-harmonics
ratio is associated with perceived hoarseness. All
features were speaker-normalized using z-scores.
For each task, we define entrainment between
partners on each feature f as
ENTp = ?|speaker1f ? speaker2f |
where speaker[1,2]f represents the corresponding
speaker?s mean for that feature over the task.
We say that the corpus shows evidence of en-
trainment on feature f if ENTp, the similarities be-
tween partners, are significantly greater than ENTx,
the similarities between non-partners:
ENTx = ?
?
i |speaker1f ?Xi,f |
|X|
where X is the set of speakers of same gender and
role as the speaker?s partner who are not paired with
the speaker in any session. We restrict the compar-
isons to speakers of the same gender and role as the
speaker?s partner to control for the fact that differ-
ences may simply be due to differences in gender or
role. The results of a series of paired t-tests compar-
ing ENTp and ENTx for each feature are summarized
in Table 1.
13
Feature FF MM FM
Intensity mean X X X
Intensity max X X X
Pitch mean X
Pitch max X
Jitter X X
Shimmer X X
NHR X
Syllables per sec X X X
Table 1: Evidence of entrainment for gender pairs. A tick
indicates that the data shows evidence of entrainment on
that row?s feature for that column?s gender pair.
We find that female-female pairs in our corpus
entrain on, in descending order of significance, jitter,
intensity max, intensity mean, syllables per second
and shimmer. They do not entrain on pitch mean
or max or NHR. Male-male pairs show the least
evidence of entrainment, entraining only on inten-
sity mean, intensity max, and syllables per second,
supporting the hypothesis that entrainment is less
prevalent among males. Female-male pairs entrain
on, again in descending order of significance, inten-
sity mean, intensity max, jitter, syllables per second,
pitch mean, NHR, shimmer, and pitch max ? in fact,
on every feature we examine, with significance val-
ues in each case of p<0.01.
To look more closely at the entrainment behavior
of males and females in mixed-gender pairs, we de-
fine ENT2p as follows:
ENT2p = ?
?
i |Pi,f ? Ti,f |
|T|
where T is the set of the pause-free chunks of speech
that begin a speaker?s turns, and P is the correspond-
ing set of pause-free chunks that end the interlocu-
tor?s preceding turns. Unlike ENTp, this measure is
asymmetric, allowing us to consider each member
of a pair separately.
We compare ENT2p for each feature for males and
females of mixed gender pairs. Contrary to our hy-
pothesis that females in mixed-gender pairs would
entrain more, we found no significant differences
in partner gender. Females in mixed-gender pairs
do not match their interlocutor?s previous turn any
more than do males. This may be due to the fact
Feature FM MM F p
Intensity mean ? ? 3.83 0.02
Intensity max ? ? 4.01 0.02
Syllables per sec ? ? 2.56 0.08
Table 2: Effects of gender pair on entrainment. An arrow
pointing up indicates that the group?s normalized entrain-
ment for that feature is greater than that of female-female
pairs; an arrow pointing down indicates that it is smaller.
that, as shown in Table 1, the overall differences be-
tween partners in mixed-gender pairs are quite low,
and so neither partner may be doing much turn-by-
turn matching.
However, as we expected, entrainment is least
prevalent among male-male pairs. Although we ex-
pected female-female pairs to exhibit the highest
prevalence of entrainment, they do not show evi-
dence of entrainment on pitch mean, pitch max or
NHR, while female-male pairs entrain on every fea-
ture. In fact, although ENTp for these features is not
significantly smaller between female-female pairs
than between female-male pairs, ENTx, the overall
similarity among non-partners for these features, is
significantly larger between females than between
females and males. The degree of similarity between
female-female partners is therefore attributable to
the overall similarity between females rather than
the effect of entrainment.
All three types of pairs exhibit entrainment on in-
tensity mean, intensity max, and syllables per sec-
ond. We look more closely into the gender-based
differences in entrainment behavior with an ANOVA
with the ratio of ENTp to ENTx as the dependent
variable and gender pair as the independent variable.
Normalizing ENTp by ENTx allows us to compare
the degree of entrainment across gender pairs. Re-
sults are shown in Table 2. Male-male pairs have
lower entrainment than female-female pairs for ev-
ery feature; female-male pairs have higher entrain-
ment than female-female pairs for intensity mean
and max and lower for syllables per second (p <
0.1). These results are consistent with the general
finding that male-male pairs entrain the least and
female-male pairs entrain the most.
14
5 Entrainment and social behavior
We next correlate each of the social variables de-
scribed in Section 3 with ENTp for our eight acous-
tic features. Based on Communication Accommo-
dation Theory, we would expect gives encourage-
ment, a variable representing a desirable social char-
acteristic, to be positively correlated with entrain-
ment. Conversely, conversation awkward should be
negatively correlated with entrainment. We note that
Trying to be liked is negatively correlated with the
like more variable in our data ? that is, annotators
were less likely to prefer speakers whom they per-
ceived as trying to be liked. This reflects the in-
tuition that someone overly eager to be liked may
be perceived as annoying and socially inept. How-
ever, similarity-attraction theory states that similar-
ity promotes attraction, and someone might there-
fore entrain in order to obtain his partner?s social
approval. This idea is supported by Natale?s find-
ing that the need for social approval is predictive
of the degree of a speaker?s convergence on inten-
sity (Natale, 1975). We can therefore expect trying
to be liked to positively correlate with entrainment.
Speakers who are perceived as trying to dominate
may be overly entraining to their interlocutors in
what is sometimes called ?dependency overaccom-
modation.? Dependency overaccommodation causes
the interlocutor to appear dependent on the speaker
and gives the impression that the speaker is control-
ling the conversation (West & Turner, 2009).
The results of our correlations of social vari-
ables with acoustic/prosodic entrainment are gen-
erally consonant with these intuitions. Although it
is not straightforward to compare correlation coeffi-
cients of groups for which we have varying amounts
of data, for purposes of assessing trends, we will
consider a correlation strong if it is significant at the
p < 0.00001 level, moderate at the p < 0.01 level,
and weak at the p < 0.05 level. The results are sum-
marized in Table 3; we present only the significant
results for space considerations.
For female-female pairs, giving encouragement
is weakly correlated with entrainment on intensity
max and shimmer. Conversation awkward is weakly
correlated with entrainment on jitter. For male-male
pairs, trying to be liked is moderately correlated
with entrainment on intensity mean and weakly cor-
related with entrainment on jitter and NHR. Giv-
ing encouragement is moderately correlated with
entrainment on intensity mean, intensity max, and
NHR. For female-male pairs, trying to be liked
is moderately correlated with entrainment on pitch
mean. Giving encouragement is strongly corre-
lated with entrainment on intensity mean and max
and moderately correlated with entrainment on pitch
mean and shimmer. However, it is negatively cor-
related with entrainment on jitter, although the cor-
relation is weak. Conversation awkward is weakly
correlated with entrainment on jitter.
As we expected, giving encouragement is corre-
lated with entrainment for all three gender groups,
and trying to be liked is correlated with entrainment
for male-male and female-male groups. However,
trying to dominate is not correlated with entrainment
on any feature, and conversation awkward is actu-
ally positively correlated with entrainment on jitter.
Entrainment on jitter is a clear outlier here, with
all of its correlations contrary to our hypotheses. In
addition to being positively correlated with conver-
sation awkward, it is the only feature to be nega-
tively correlated with giving encouragement.
Entrainment is correlated with the most social
variables for female-male pairs; these correlations
are also the strongest. We therefore conclude that
acoustic entrainment is not only most prevalent for
mixed-gender pairs, it is also more important to the
perception of female-male social behavior than it is
for same-gender pairs.
6 Entrainment and objective measures of
dialogue success
We now examine acoustic/prosodic entrainment in
our corpus according to four objective measures of
dialogue success: the mean latency between turns,
the percentage of turns that are interruptions, the
percentage of turns that are overlaps, and the number
of turns in a task.
High latency between turns can be considered a
sign of an unsuccessful conversation, with poor turn-
taking behavior indicating a possible lack of rapport
and difficulty in communication between the part-
ners. A high percentage of interruptions, another ex-
ample of poor turn-taking behavior, may be a symp-
tom of or a reason for hostility or awkwardness be-
15
Social Acoustic df r p
Female-Female
Giving Int. max -0.24 0.03
enc. Shimmer -0.24 0.03
Conv. Jitter -0.23 0.03
awkward
Male-Male
Trying to Int. mean -0.30 0.006
be liked Jitter -0.27 0.01
NHR -0.23 0.03
Giving Int. mean -0.39 0.0003
enc. Int. max -0.31 0.005
NHR -0.30 0.005
Female-Male
Trying to Pitch mean -0.26 0.001
be liked
Giving Int. mean -0.36 2.8e-06
enc. Int. max -0.31 7.7e-05
Pitch mean -0.23 0.003
Jitter 0.19 0.02
Shimmer -0.16 0.04
Conv. Jitter -0.17 0.04
awkward
Table 3: Correlations between entrainment and social
variables.
tween partners. We expect these measures to be neg-
atively correlated with entrainment. Conversely, a
high percentage of overlaps may be a symptom of
a well-coordinated conversation that is flowing eas-
ily. In the guidelines for the turn-taking annotation
of the Games Corpus (Gravano, 2009), overlaps are
defined as cases in which Speaker 2 takes the floor,
overlapping with the completion of Speaker 1?s ut-
terance. Overlaps require the successful reading of
turn-taking cues and by definition preclude awkward
pauses. We expect a high percentage of overlaps to
correlate positively with entrainment.
The number of turns in a task can be interpreted
either positively or negatively. A high number is
negative in that it is the sign of an inefficient dia-
logue, one which takes many turn exchanges to ac-
complish the objective. However, it may also be
the sign of easy, flowing dialogue between the part-
ners. In our domain, it may also be a sign of a high-
achieving pair who are placing the object meticu-
Objective Acoustic df r p
Female-Female
Latency Int. mean 0.22 0.04
Int. max 0.31 0.005
Pitch mean 0.24 0.02
Jitter 0.29 0.007
Shimmer 0.33 0.002
Syllables/sec 0.39 0.0002
# Turns Int. max -0.30 0.006
Shimmer -0.34 0.002
NHR -0.24 0.03
Syllables/sec -0.28 0.01
% Overlaps Int. max -0.23 0.04
Shimmer -0.30 0.005
% Interruptions Shimmer -0.33 0.005
Male-Male
Latency Int. mean 0.57 8.8e-08
Int. max 0.43 0.0001
Pitch mean 0.52 2.4e-06
Pitch max 0.61 5.7e-09
Jitter 0.65 4.5e-10
NHR 0.40 0.0004
# Turns Int. mean -0.29 0.0002
Pitch mean -0.32 0.003
Pitch max -0.29 0.007
NHR -0.47 7.9e-06
Syllables/sec -0.25 0.02
% Overlaps Int. mean -0.39 0.0002
Int. max -0.39 0.0002
% Interruptions NHR -0.33 0.002
Female-Male
# Turns Int. mean -0.24 0.003
Int. max -0.19 0.02
Shimmer -0.16 0.04
% Overlaps Shimmer -0.26 0.001
Table 4: Correlations between entrainment and objective
variables.
lously in order to secure every single point. We
therefore expect the number of turns to be positively
correlated with entrainment. As before, we con-
sider a correlation strong if it is significant at the
p < 0.00001 level, moderate at the p < 0.01 level,
and weak at the p < 0.05 level. The significant cor-
relations are presented in Table 4.
For female-female pairs, mean latency between
16
turns is negatively correlated with entrainment on all
variables except pitch max and NHR. The correla-
tions are weak for intensity mean and pitch mean
and moderate for intensity max, jitter, shimmer, and
syllables per second. The number of turns is moder-
ately correlated with entrainment on intensity max
and shimmer and weakly correlated with entrain-
ment on syllables per second. Contrary to our expec-
tations, the percentage of interruptions is positively
(though moderately) correlated with entrainment on
shimmer; the percentage of overlaps is moderately
correlated with entrainment on shimmer and weakly
correlated with entrainment on intensity max.
Male-male pairs show the most correlations be-
tween entrainment and objective measures of dia-
logue success. The latency between turns is neg-
atively correlated with entrainment on all variables
except shimmer and syllables per second; the corre-
lations are moderate for intensity max and NHR and
strong for the rest. The number of turns in a task
is positively correlated with entrainment on every
variable except intensity mean, jitter and shimmer:
strongly for NHR; moderately for intensity mean,
pitch mean, and pitch max; and weakly for syllables
per second.. The percentage of overlaps is moder-
ately correlated with entrainment on intensity mean
and max. The percentage of interruptions is moder-
ately correlated with entrainment on NHR.
For female-male pairs, the number of turns is
moderately correlated with entrainment on intensity
mean and weakly correlated with entrainment on in-
tensity max and shimmer. The percentage of over-
laps is moderately correlated with entrainment on
shimmer.
For the most part, the directions of the correla-
tions we have found are in accordance with our hy-
potheses. Latency is negatively correlated with en-
trainment and overlaps and the number of turns are
positively correlated. A puzzling exception is the
percentage of interruptions, which is positively cor-
related with entrainment on shimmer (for female-
female pairs) and NHR (for male-male pairs).
While the strongest correlations were for mixed-
gender pairs for the social variables, we find that
the strongest correlations for objective variables are
for male-male pairs, which also have the great-
est number of correlations. It therefore seems that
while entrainment is more important to the percep-
tion of social behavior for mixed-gender pairs than
it is for same-gender pairs, it is more important to
the smoothness and flow of dialogue for male-male
pairs than it is for female-female or female-male
pairs.
7 Entrainment in outliers
Since acoustic entrainment is generally considered
an unconscious phenomenon, it is interesting to con-
sider tasks in which a particular feature of a person?s
speech is particularly salient. This will occur when a
feature differs significantly from the norm ? for ex-
ample, when a person?s voice is unusually loud or
soft. Chartrand and Bargh (1999) suggest that the
psychological mechanism behind the entrainment is
the perception-behavior link, the finding that the act
of observing another?s behavior increases the like-
lihood of the observer?s engaging in that behavior.
Based on this finding, we hypothesize that a part-
ner pair containing one ?outlier? speaker will exhibit
more entrainment on the salient feature, since that
feature is more likely to be observed and therefore
imitated.
We consider values in the 10th or 90th percentile
for a feature ?outliers.? We can consider ENTx, the
similarity between a speaker and the speakers of her
partner?s role and gender with whom she is never
paired, the ?baseline? value for the similarity be-
tween a speaker and her interlocutor when no en-
trainment occurs. ENTp ? ENTx, the difference be-
tween the similarity existing between partners and
the baseline similarity, is then a measure of how
much entrainment exists relative to baseline.
We compare ENTp ? ENTx for ?normal? versus
?outlier? speakers. ENTp should be smaller for out-
lier speakers, since their interlocutors are not likely
to be similarly unusual. However, ENTx should also
be lower for outlier speakers, since by definition they
diverge from the norm, while the normal speakers
by definition represent the norm. It is therefore rea-
sonable to expect ENTp ? ENTx to be the same for
outlier speakers and normal speakers.
If ENTp ? ENTx is higher for outlier speakers,
that means that ENTp is higher than we expect, and
entrainment is greater relative to baseline for pairs
containing an outlier speaker. If ENTp ? ENTx is
lower for outlier speakers, that means that ENTp is
17
Acoustic t df p
Intensity mean 5.66 94.26 1.7e-07
Intensity max 8.29 152.05 5.5e-14
Pitch mean -1.20 76.82 N.S.
Pitch max -0.84 76.76 N.S.
Jitter 0.36 70.23 N.S.
Shimmer 2.64 102.23 0.02
NHR -0.92 137.34 N.S.
Syllables per sec 2.41 72.60 0.02
Table 5: T-tests for relative entrainment for outlier vs.
normal speakers.
lower than we expect, and pairs containing an outlier
speaker entrain less than do pairs of normal speak-
ers, even allowing for the fact that their usual values
should be further apart to begin with.
The results for t-tests comparing ENTp ? ENTx
for ?normal? versus ?outlier? speakers are shown
in Table 5. Outlier pairs have higher relative en-
trainment than do normal pairs for intensity mean
and max, shimmer, and syllables per second. This
means that speakers confronted with an interlocutor
who diverges widely from the norm for those four
features make a larger adjustment to their speech in
order to converge to that interlocutor.
An ANOVA shows that relative entrainment on
intensity max is higher in outlier cases for male-
male pairs than for female-female pairs and even
higher for female-male pairs (F=11.33, p=5.3e-05).
Relative entrainment on NHR in these cases is lower
for male-male pairs than for female-female pairs
and higher for female-male pairs (F=11.41, p=6.5e-
05). Relative entrainment on syllables per second
is lower for male-male pairs and higher for female-
male pairs (F=5.73, p=0.005). These results differ
slightly from the results in Table 2 for differences
in entrainment in the general case among gender
pairs, reinforcing the idea that cases in which fea-
ture values diverge widely from the norm are unique
in terms of entrainment behavior.
8 Conclusion
Our study of entrainment on acoustic/prosodic vari-
ables yields new findings about entrainment be-
havior for female-female, male-male, and mixed-
gender dyads, as well as the association of entrain-
ment with perceived social characteristics and ob-
jective measures of dialogue smoothness and effi-
ciency. We find that entrainment is the most preva-
lent for mixed-gender pairs, followed by female-
female pairs, with male-male pairs entraining the
least. Entrainment is the most important to the per-
ception of social behavior of mixed-gender pairs,
and it is the most important to the efficiency and flow
of male-male dialogues.
For the most part, the directions of the correla-
tions of entrainment with success variables accord
with hypotheses motivated by the relevant literature.
Giving encouragement and trying to be liked are
positively correlated with entrainment, as are per-
centage of overlaps and number of turns. Mean la-
tency, a symptom of a poorly-run conversation, is
negatively associated with entrainment. However,
several exceptions suggest that the associations are
not straightforward and further research must be
done to fully understand the relationship between
entrainment, social characteristics and dialogue suc-
cess. In particular, the explanation behind the as-
sociations of entrainment on certain variables with
certain social and objective measures is an interest-
ing direction for future work.
Finally, we find that in ?outlier? cases where a
particular speaker diverges widely from the norm for
intensity mean, intensity max, or syllables per sec-
ond, entrainment is more pronounced. This supports
the theory that the perception-behavior link is the
mechanism behind entrainment and provides a pos-
sible direction for research into why speakers entrain
on certain features and not others. In future work we
will explore this direction and go more thoroughly
into individual differences in entrainment behavior.
Acknowledgments
This material is based upon work supported in
part by NSF IIS-0307905, NSF IIS-0803148,
UBACYT 20020090300087, ANPCYT PICT-2009-
0026, CONICET, VEGA No. 2/0202/11; and the
EUSF (ITMS 26240220060).
References
Amazon Mechanical Turk, http://www.mturk.com.
Frances R. Bilous and Robert M. Krauss 1988. Dom-
inance and accommodation in the conversational be-
18
haviours of same- and mixed-gender dyads. Language
and Communication, 8(3/4):183?194.
Susan E. Brennan and Herbert H. Clark. 1996. Concep-
tual Pacts and Lexical Choice in Conversation. Jour-
nal of Experimental Psychology: Learning, Memory
and Cognition, 22(6):1482?1493.
Tanya L. Chartrand and John A. Bargh. 1999. The
Chameleon Effect: The Perception-Behavior Link and
Social Interaction. Journal of Personality and Social
Psychology, 76(6):893?910.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and
Susan Dumais. 2011. Mark My Words! Linguistic
Style Accommodation in Social Media. Proceedings
of WWW 2011.
H. Giles and N. Coupland. 1991. Language: Contexts
and Consequences. Pacific Grove, CA: Brooks/Cole.
Agust??n Gravano. 2009. Turn-Taking and Affirmative
Cue Words in Task-Oriented Dialogue. Ph.D. thesis,
Columbia University, New York.
Agust??n Gravano and Julia Hirschberg. 2011. Turn-
taking cues in task-oriented dialogue. Computer
Speech and Language, 25(3):601?634.
Agust??n Gravano, Rivka Levitan, Laura Willson, S?tefan
Ben?us?, Julia Hirschberg, Ani Nenkova. 2011. Acous-
tic and Prosodic Correlates of Social Behavior. Inter-
speech 2011.
Chi-Chun Lee, Matthew Black, Athanasios Katsama-
nis, Adam Lammert, Brian Baucom, Andrew Chris-
tensen, Panayiotis G. Georgiou, Shrikanth Narayanan.
2010. Quantification of Prosodic Entrainment in Af-
fective Spontaneous Spoken Interactions of Married
Couples. Eleventh Annual Conference of the Interna-
tional Speech Communication Association.
Rivka Levitan, Agust??n Gravano, and Julia Hirschberg.
2011. Entrainment in Speech Preceding Backchan-
nels. Proceedings of ACL/HLT 2011.
Rivka Levitan and Julia Hirschberg. 2011. Measuring
acoustic-prosodic entrainment with respect to multi-
ple levels and dimensions. Proceedings of Interspeech
2011.
Laura L. Namy, Lynne C. Nygaard, Denise Sauerteig.
2002. Gender differences in vocal accommodation:
the role of perception. Journal of Language and So-
cial Psychology, 21(4):422?432.
Michael Natale. 1975. Convergence of Mean Vocal In-
tensity in Dyadic Communication as a Function of So-
cial Desirability. Journal of Personality and Social
Psychology, 32(5):790?804.
Ani Nenkova, Agust??n Gravano, and Julia Hirschberg.
2008. High-frequency word entrainment in spoken di-
alogue. Proceedings of ACL/HLT 2008.
Kate G. Niederhoffer and James W. Pennebaker. 2002.
Linguistic style matching in social interaction. Jour-
nal of Language and Social Psychology, 21(4):337?
360.
Jennifer S. Pardo. 2006. On phonetic convergence dur-
ing conversational interaction. Journal of the Acousti-
cal Society of America, 119(4):2382?2393.
David Reitter, Johanna D. Moore, and Frank Keller.
1996. Priming of Syntactic Rules in Task-Oriented Di-
alogue and Spontaneous Conversation. Proceedings of
the 28th Annual Conference of the Cognitive Science
Society.
Kim Silverman, Mary Beckman, John Pitrelli, Mori Os-
tendorf, Colin Wightman, Patti Price, Janet Pierrehum-
bert, Julia Hirschberg. 1992. TOBI: A Standard for
Labeling English Prosody. ICSLP-1992, 867-870.
Linda Tickle-Degnen and Robert Rosenthal. 1990. The
Nature of Rapport and its Nonverbal Correlates. Psy-
chological Inquiry, 1(4):285?293.
Richard West & Lynn Turner. 2009. Introducing
Communication Theory: Analysis and Application.
McGraw-Hill Humanities/Social Sciences/Languages,
4th edition.
19
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 113?117,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Entrainment in Speech Preceding Backchannels
Rivka Levitan
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
rlevitan@cs.columbia.edu
Agust??n Gravano
DC-FCEyN & LIS
Universidad de Buenos Aires
Buenos Aires, Argentina
gravano@dc.uba.ar
Julia Hirschberg
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
julia@cs.columbia.edu
Abstract
In conversation, when speech is followed by
a backchannel, evidence of continued engage-
ment by one?s dialogue partner, that speech
displays a combination of cues that appear to
signal to one?s interlocutor that a backchan-
nel is appropriate. We term these cues back-
channel-preceding cues (BPC)s, and examine
the Columbia Games Corpus for evidence of
entrainment on such cues. Entrainment, the
phenomenon of dialogue partners becoming
more similar to each other, is widely believed
to be crucial to conversation quality and suc-
cess. Our results show that speaking partners
entrain on BPCs; that is, they tend to use simi-
lar sets of BPCs; this similarity increases over
the course of a dialogue; and this similarity is
associated with measures of dialogue coordi-
nation and task success.
1 Introduction
In conversation, dialogue partners often become
more similar to each other. This phenomenon,
known in the literature as entrainment, alignment,
accommodation, or adaptation has been found to
occur along many acoustic, prosodic, syntactic and
lexical dimensions in both human-human interac-
tions (Brennan and Clark, 1996; Coulston et al,
2002; Reitter et al, 2006; Ward and Litman,
2007; Niederhoffer and Pennebaker, 2002; Ward and
Mamidipally, 2008; Buder et al, 2010) and human-
computer interactions (Brennan, 1996; Bell et al,
2000; Stoyanchev and Stent, 2009; Bell et al, 2003)
and has been associated with dialogue success and
naturalness (Pickering and Garrod, 2004; Goleman,
2006; Nenkova et al, 2008). That is, interlocutors
who entrain achieve better communication. How-
ever, the question of how best to measure this phe-
nomenon has not been well established. Most re-
search has examined similarity of behavior over a
conversation, or has compared similarity in early
and later phases of a conversation; more recent work
has proposed new metrics of synchrony and conver-
gence (Edlund et al, 2009) and measures of similar-
ity at a more local level (Heldner et al, 2010).
While a number of dimensions of potential en-
trainment have been studied in the literature, en-
trainment in turn-taking behaviors has received lit-
tle attention. In this paper we examine entrainment
in a novel turn-taking dimension: backchannel-
preceding cues (BPC)s.1 Backchannels are short
segments of speech uttered to signal continued in-
terest and understanding without taking the floor
(Schegloff, 1982). In a study of the Columbia
Games Corpus, Gravano and Hirschberg (2009;
2011) identify five speech phenomena that are
significantly correlated with speech followed by
backchannels. However, they also note that indi-
vidual speakers produced different combinations of
these cues and varied the way cues were expressed.
In our work, we look for evidence that speaker pairs
negotiate the choice of such cues and their realiza-
tions in a conversation ? that is, they entrain to one
another in their choice and production of such cues.
We test for evidence both at the global and at the
local level.
1Prior studies termed cues that precede backchannels, back-
channel-inviting cues. To avoid suggesting that such cues are a
speaker?s conscious decision, we adopt a more neutral term.
113
In Section 2, we describe the Columbia Games
Corpus, on which the current analysis was con-
ducted. In Section 3, we present three measures of
BPC entrainment. In Section 4, we further show that
two of these measures also correlate with dialogue
coordination and task success.
2 The Columbia Games Corpus
The Columbia Games Corpus is a collection of 12
spontaneous dyadic conversations elicited from na-
tive speakers of Standard American English. 13 peo-
ple participated in the collection of the corpus. 11
participated in two sessions, each time with a dif-
ferent partner. Subjects were separated by a curtain
to ensure that all communication was verbal. They
played a series of computer games requiring collab-
oration in order to achieve a high score.
The corpus consists of 9h 8m of speech. It is
orthographically transcribed and annotated for var-
ious types of turn-taking behavior, including smooth
switches (cases in which one speaker completes her
turn and another speaker takes the floor), interrup-
tions (cases in which one speaker breaks in, leaving
the interlocutor?s turn incomplete), and backchan-
nels. There are 5641 exchanges in the corpus; of
these, approximately 58% are smooth switches, 2%
are interruptions, and 11% are backchannels. Other
turn types include overlaps and pause interruptions;
a full description of the Columbia Games Corpus?
annotation for turn-taking behavior can be found in
(Gravano and Hirschberg, 2011).
3 Evidence of entrainment
Gravano and Hirschberg (2009; 2011) identify five
cues that tend to be present in speech preceding
backchannels. These cues, and the features that
model them, are listed in Table 1. The likelihood
that a segment of speech will be followed by a
backchannel increases quadratically with the num-
ber of cues present in the speech. However, they
note that individual speakers may display different
combinations of cues. Furthermore, the realization
of a cue may differ from speaker to speaker. We hy-
pothesize that speaker pairs adopt a common set of
cues to which each will respond with a backchan-
nel. We look for evidence for this hypothesis us-
ing three different measures of entrainment. Two of
Cue Feature
Intonation pitch slope over the IPU-
final 200 and 300 ms
Pitch mean pitch over the final
500 and 1000 ms
Intensity mean intensity over the
final 500 and 1000 ms
Duration IPU duration in seconds
and word count
Voice quality NHR over the final 500
and 1000 ms
Table 1: Features modeling each of the five cues.
these measures capture entrainment globally, over
the course of an entire dialogue, while the third
looks at entrainment on a local level. The unit of
analysis we employ for each experiment is an inter-
pausal unit (IPU), defined as a pause-free segment
of speech from a single speaker, where pause is de-
fined as a silence of 50ms or more from the same
speaker. We term consecutive pairs of IPUs from
a single speaker holds, and contrast hold-preceding
IPUs with backchannel-preceding IPUs to isolate
cues that are significant in preceding backchannels.
That is, when a speaker pauses without giving up
the turn, which IPUs are followed by backchannels
and which are not? We consider a speaker to use
a certain BPC if, for any of the features model-
ing that cue, the difference between backchannel-
preceding IPUs and hold-preceding IPUs is signif-
icant (ANOVA, p < 0.05).
3.1 Entrainment measure 1: Common cues
For our first entrainment metric, we measure the
similarity of two speakers? cue sets by simply count-
ing the number of cues that they have in common
over the entire conversation. We hypothesize that
speaker pairs will use similar sets of cues.
The speakers in our corpus each displayed 0 to 5
of the BPCs described in Table 1 (mean = 2.17). The
number of cues speaker pairs had in common ranged
from 0 to 4 (out of a maximum of 5). Let S1 and S2
be two speakers in a given dialogue, and n1,2 the
number of BPCs they had in common. Let alo n1,?
and n?,2 be the mean number of cues S1 and S2 had
in common with all other speakers in the corpus not
partnered with them in any session. For all 12 dia-
114
logues in the corpus, we pair n1,2 both with n1,? and
with n?,2, and run a paired t-test. The results indi-
cate that, on average, the speakers had significantly
more cues in common with their interlocutors than
with other speakers in the corpus (t = 2.1, df = 23,
p < 0.05).
These findings support our hypothesis that speak-
er pairs negotiate common sets of cues, and suggest
that, like other aspects of conversation, speaker vari-
ation in use of BPCs is not simply an expression of
personal behavior, but is at least partially the result
of coordination with a conversational partner.
3.2 Entrainment measure 2: BPC realization
With our second measure, we look for evidence that
the speakers? actual values for the cue features are
similar: that not only do they alter their production
of similar feature sets when preceding a backchan-
nel, they also alter their productions in similar ways.
We measure how similarly two speakers S1 and
S2 in a conversation realize a BPC as follows:
First, we compute the difference (df1,2) between both
speakers for the mean value of a feature f over
all backchannel-preceding IPUs. Second, we com-
pute the same difference between each of S1 and S2
and the averaged values of all other speakers in the
corpus who are not partnered with that speaker in
any session (df1,? and df?,2). Finally, if for any fea-
ture f modeling a given cue, it holds that df1,2 <
min(df1,?, d
f
?,2), we say that that session exhibits
mutual entrainment on that cue.
Eleven out of 12 sessions exhibit mutual entrain-
ment on pitch and intensity, 9 exhibit mutual entrain-
ment on voice quality, 8 on intonation, and 7 on du-
ration. Interestingly, the only session not entrain-
ing on intensity is the only session not entraining
on pitch, but the relationships between the different
types of entrainment is not readily observable.
For each of the 10 features associated with
backchannel invitation, we compare the differences
between conversational partners (df1,2) and the aver-
aged differences between each speaker and the other
speakers in the corpus (df1,? and df?,2). Paired t-tests
(Table 2) show that the differences in intensity, pitch
and voice quality in backchannel-preceding IPUs
are smaller between conversational partners than be-
tween speakers and their non-partners in the corpus.
Feature t df p-value Sig.
Intensity 500 -4.73 23 9.09e-05 *
Intensity 1000 -2.80 23 0.01 *
Pitch 500 -3.38 23 0.002 *
Pitch 1000 -3.28 23 0.003 *
Pitch slope 200 -1.77 23 0.09 .
Pitch slope 300 -0.93 23 N.S.
Duration 0.50 23 N.S.
# Words 1.39 23 N.S.
NHR 500 -2.00 23 0.06 .
NHR 1000 -2.30 23 0.03 *
Table 2: T -tests between partners and their non-partners
in the corpus.
The differences between interlocutor and their
non-partners in features modeling pitch show that
there is no single ?optimal? value for a pitch level
that precedes a backchannel; this value is coordi-
nated between partners on a pair-by-pair basis. Sim-
ilarly, while varying intensity or voice quality may
be considered a universal cue for a backchannel, the
specific values of the production appear to be a mat-
ter of coordination between individual speaker pairs.
While some views of entrainment hold that coor-
dination takes place at the very beginning of a dia-
logue, others hypothesize that coordination contin-
ues to improve over the course of the conversation.
T -tests for difference of means show that indeed
the differences between conversational partners in
mean pitch and intensity in the final 1000 millisec-
onds of backchannel-preceding IPUs are smaller in
the second half of the conversation than in the first
(t = 3.44, 2.17; df = 23; p < 0.05, 0.01), indicat-
ing that entrainment in this dimension is an ongoing
process that results in closer alignment after the in-
terlocutors have been speaking for some time.
3.3 Measure 3: Local BPC entrainment
Measures 1 and 2 capture global entrainment and
can be used to characterize an entire dialogue with
respect to entrainment. We now look for evidence
to support the hypothesis that a speaker?s realization
of BPCs influences how her interlocutor produces
BPCs. To capture this, we compile a list of pairs
of backchannel-preceding IPUs, in which the second
member of each pair follows the first in the conver-
115
sation and is produced by a different speaker. For
each feature, we calculate the Pearson?s correlation
between acoustic variables extracted from the first
element of each pair and the second.
The correlations for mean pitch and intensity are
significant (r = 0.3, two-sided t-test: p < 0.05, in
both cases). Other correlations are not significant.
These results suggest that entrainment on pitch and
intensity at least is a localized phenomenon. Spoken
dialogue systems may exploit this information, mod-
ifying their output to invite a backchannel similar to
the user?s own previous backchannel invitation.
4 Correlation with dialogue coordination
and task success
Entrainment is widely believed to be crucial to dia-
logue coordination. In the specific case of BPC en-
trainment, it seems intuitive that some consensus on
BPCs should be integral to the successful coordina-
tion of a conversation. Long latencies (periods of si-
lence) before backchannels can be considered a sign
of poor coordination, as when a speaker is waiting
for an indication that his partner is still attending,
and the partner is slow to realize this. Similarly,
interruptions signal poor coordination, as when a
speaker has not finished what he has to say, but his
partner thinks it is her turn to speak. We thus use
mean backchannel latency and proportion of inter-
ruptions as measures of coordination of whole ses-
sions. We use the combined score of the games the
subjects played as a measure of task success. We
correlate all three with our two global entrainment
scores and report correlation coefficients in Table 3.
Entrain. Success/coord. r p-value
measure measure
1 Latency -0.33 0.06
Interruptions -0.50 0.01
Score 0.22 N.S.
2 Latency -0.61 0.002
Interruptions -0.22 N.S.
Score 0.72 6.9e-05
Table 3: Correlations with success and coordination.
Our first metric for identifying entrainment, Mea-
sure 1, the number of cues the speaker pair has in
common, is negatively correlated with mean latency
and proportion of interruptions, our two measures of
poor coordination. Its correlation with score, though
not significant, is positive. So, more entrainment in
BPCs under Measure 1 means smaller latency before
backchannels and fewer interruptions, while there
is a tendency for such entrainment to be associated
with higher scores.
Our second entrainment metric, Measure 2, cap-
tures the similarities between speaker means of the
10 features associated with BPCs. To test correla-
tions of this measure with task success, we collapse
the ten features into a single measure by taking the
negated Euclidean distance between each speaker
pair?s 2 vectors of means; this measure tells us how
close these speakers are across all features exam-
ined. Under this analysis, we find that Measure 2
is negatively correlated with mean latency and pos-
itively correlated with score. Both correlations are
strong and highly significant. Again, the correlation
with interruptions is negative, although not signifi-
cant. Thus, more entrainment defined by this metric
means shorter latency between turns, fewer interrup-
tions, and again and more strongly, higher scores.
We thus find that, the more entrainment at the
global level, the better the coordination between the
partners and the better their performance on their
joint task. These results provide evidence of the im-
portance of BPC entrainment to dialogue.
5 Conclusion
In this paper we discuss the role of entrainment
in turn-taking behavior and its impact on conversa-
tional coordination and task success in the Columbia
Games Corpus. We examine a novel form of en-
trainment, entrainment in BPCs ? characteristics of
speech segments that are followed by backchannels
from the interlocutor. We employ three measures
of entrainment ? two global and one local ? and
find evidence of entrainment in all three. We also
find correlations between our two global entrain-
ment measures and conversational coordination and
task success. In future, we will extend this analysis
to the complementary turn-taking category of turn-
yielding cues and explore how a spoken dialogue
system may take advantage of information about en-
trainment to improve dialogue coordination and the
user experience.
116
6 Acknowledgments
This material is based on work supported in
part by the National Science Foundation under
Grant No. IIS-0803148 and by UBACYT No.
20020090300087.
References
L. Bell, J. Boye, J. Gustafson, and M. Wiren. 2000.
Modality convergence in a multimodal dialogue sys-
tem. In Proceedings of 4th Workshop on the Semantics
and Pragmatics of Dialogue (GOTALOG).
L. Bell, J. Gustafson, and M. Heldner. 2003. Prosodic
adaptation in human-computer interaction. In Pro-
ceedings of the 15th International Congress of Pho-
netic Sciences (ICPhS).
S.E. Brennan and H.H. Clark. 1996. Conceptual pacts
and lexical choice in conversation. Journal of Exper-
imental Psychology: Learning, Memory, and Cogni-
tion, 22(6):1482?1493.
S.E. Brennan. 1996. Lexical entrainment in spontaneous
dialog. In Proceedings of the International Sympo-
sium on Spoken Dialog (ISSD).
E.H. Buder, A.S. Warlaumont, D.K. Oller, and L.B.
Chorna. 2010. Dynamic indicators of Mother-Infant
Prosodic and Illocutionary Coordination. In Proceed-
ings of the 5th International Conference on Speech
Prosody.
R. Coulston, S. Oviatt, and C. Darves. 2002. Amplitude
convergence in children?s conversational speech with
animated personas. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing
(ICSLP).
J. Edlund, M. Heldner, and J. Hirschberg. 2009. Pause
and gap length in face-to-face interaction. In Proceed-
ings of Interspeech.
D. Goleman. 2006. Social Intelligence: The New Sci-
ence of Human Relationships. Bantam.
A. Gravano and J. Hirschberg. 2009. Backchannel-
inviting cues in task-oriented dialogue. In Proceedings
of SigDial.
A. Gravano and J. Hirschberg. 2011. Turn-taking cues
in task-oriented dialogue. Computer Speech and Lan-
guage, 25(33):601?634.
M. Heldner, J. Edlund, and J. Hirschberg. 2010. Pitch
similarity in the vicinity of backchannels. In Proceed-
ings of Interspeech.
A. Nenkova, A. Gravano, and J. Hirschberg. 2008. High
frequency word entrainment in spoken dialogue. In
Proceedings of ACL/HLT.
K. Niederhoffer and J. Pennebaker. 2002. Linguistic
style matching in social interaction. Journal of Lan-
guage and Social Psychology, 21(4):337?360.
M. J. Pickering and S. Garrod. 2004. Toward a mecha-
nistic psychology of dialogue. Behavioral and Brain
Sciences, 27:169?226.
D. Reitter, F. Keller, and J.D. Moore. 2006. Computa-
tional modelling of structural priming in dialogue. In
Proceedings of HLT/NAACL.
E. Schegloff. 1982. Discourse as an interactional
achievement: Some uses of ?uh huh? and other things
that come between sentences. In D. Tannen, editor,
Analyzing Discourse: Text and Talk, pages 71?93.
Georgetown University Press.
S. Stoyanchev and A. Stent. 2009. Lexical and syntactic
priming and their impact in deployed spoken dialogue
systems. In Proceedings of NAACL.
A. Ward and D. Litman. 2007. Automatically measuring
lexical and acoustic/prosodic convergence in tutorial
dialog corpora. In Proceedings of the SLaTE Work-
shop on Speech and Language Technology in Educa-
tion.
N.G. Ward and S.K. Mamidipally. 2008. Factors Affect-
ing Speaking-Rate Adaptation in Task-Oriented Di-
alogs. In Proceedings of the 4th International Con-
ference on Speech Prosody.
117
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 49?54,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
WELT: Using Graphics Generation in Linguistic Fieldwork
Morgan Ulinski
?
mulinski@cs.columbia.edu
Anusha Balakrishnan
?
ab3596@columbia.edu
Daniel Bauer
?
bauer@cs.columbia.edu
Bob Coyne
?
coyne@cs.columbia.edu
Julia Hirschberg
?
julia@cs.columbia.edu
Owen Rambow
?
rambow@ccls.columbia.edu
?
Department of Computer Science
?
CCLS
Columbia University
New York, NY, USA
Abstract
We describe the WordsEye Linguistics
tool (WELT), a novel tool for the docu-
mentation and preservation of endangered
languages. WELT is based on Words-
Eye (Coyne and Sproat, 2001), a text-to-
scene tool that automatically generates 3D
scenes from written input. WELT has two
modes of operation. In the first mode, En-
glish input automatically generates a pic-
ture which can be used to elicit a de-
scription in the target language. In the
second mode, the linguist formally docu-
ments the grammar of an endangered lan-
guage, thereby creating a system that takes
input in the endangered language and gen-
erates a picture according to the grammar;
the picture can then be used to verify the
grammar with native speakers. We will
demonstrate WELT?s use on scenarios in-
volving Arrernte and Nahuatl.
1 Introduction
Although languages have appeared and disap-
peared throughout history, today languages are
facing extinction at an unprecedented pace. Over
40% of the estimated 7,000 languages in the world
are at risk of disappearing. When languages die
out, we lose access to an invaluable resource for
studying the culture, history, and experience of
peoples around the world (Alliance for Linguistic
Diversity, 2013). Efforts to document languages
and develop tools in support of collecting data on
them become even more important with the in-
creasing rate of extinction. Bird (2009) empha-
sizes a particular need to make use of computa-
tional linguistics during fieldwork.
To address this issue, we are developing the
WordsEye Linguistics Tool, or WELT. In the first
mode of operation, we provide a field linguist with
tools for running custom elicitation sessions based
on a collection of 3D scenes. In the second, input
in an endangered language generates a picture rep-
resenting the input?s meaning according to a for-
mal grammar.
WELT provides important advantages for elic-
itation over the pre-fabricated sets of static pic-
tures commonly used by field linguists today. The
field worker is not limited to a fixed set of pictures
but can, instead, create and modify scenes in real
time, based on the informants? answers. This al-
lows them to create additional follow-up scenes
and questions on the fly. In addition, since the
pictures are 3D scenes, the viewpoint can easily
be changed, allowing exploration of linguistic de-
scriptions based on different frames of reference.
This will be particularly useful in eliciting spatial
descriptions. Finally, since scenes and objects can
easily be added in the field, the linguist can cus-
tomize the images used for elicitation to be maxi-
mally relevant to the current informants.
WELT also provides a means to document the
semantics of a language in a formal way. Lin-
guists can customize their studies to be as deep or
shallow as they wish; however, we believe that a
major advantage of documenting a language with
WELT is that it enables such studies to be much
more precise. The fully functioning text-to-scene
system created as a result of this documentation
will let linguists easily test the theories they de-
velop with native speakers, making changes to
grammars and semantics in real time. The result-
ing text-to-scene system can be an important tool
for language preservation, spreading interest in the
language among younger generations of the com-
munity and recruiting new speakers.
We will demonstrate the features of WELT
for use in fieldwork, including designing elic-
itation sessions, building scenes, recording au-
dio, and adding descriptions and glosses to a
scene. We will use examples from sessions we
49
have conducted with a native speaker of Nahu-
atl, an endangered language spoken in Mexico.
We will demonstrate how to document seman-
tics with WELT, using examples from Arrernte,
an Australian aboriginal language spoken in Alice
Springs. We will also demonstrate a basic Arrernte
text-to-scene system created in WELT.
In the following sections, we will mention re-
lated work (Section 2), discuss the WordsEye sys-
tem that WELT is based on (Section 3), describe
WELT in more detail, highlighting the functional-
ity that will appear in our demonstration (Section
4), and briefly mention our future plans for WELT
(Section 5).
2 Related Work
One of the most widely-used computer toolkits for
field linguistics is SIL Fieldworks. FieldWorks is
a collection of software tools; the most relevant
for our research is FLEx, Fieldworks Language
Explorer. FLEx includes tools for eliciting and
recording lexical information, dictionary develop-
ment, interlinearization of texts, analysis of dis-
course features, and morphological analysis. An
important part of FLEx is its ?linguist-friendly?
morphological parser (Black and Simons, 2006),
which uses an underlying model of morphology
familiar to linguists, is fully integrated into lexicon
development and interlinear text analysis, and pro-
duces a human-readable grammar sketch as well
as a machine-interpretable parser.
Several computational tools aim to simplify the
formal documentation of syntax by eliminating
the need to master particular grammar formalisms.
First is the PAWS starter kit (Black and Black,
2012), a system that prompts linguists with a series
of guided questions about the target language and
uses their answers to produce a PC-PATR gram-
mar (McConnel, 1995). The LinGO Grammar
Matrix (Bender et al., 2002) is a similar tool de-
veloped for HPSG that uses a type hierarchy to
represent cross-linguistic generalizations.
The most commonly used resource for for-
mally documenting semantics across languages
is FrameNet (Filmore et al., 2003). FrameNets
have been developed for many languages, includ-
ing Spanish, Japanese, and Portuguese. Most
start with English FrameNet and adapt it for the
new language; a large portion of the frames end
up being substantially the same across languages
(Baker, 2008). ParSem (Butt et al., 2002) is a
collaboration to develop parallel semantic repre-
sentations across languages, by developing seman-
tic structures based on LFG. Neither of these re-
sources, however, are targeted at helping non-
computational linguists formally document a lan-
guage, as compared to the morphological parser in
FLEx or the syntactic documentation in PAWS.
3 WordsEye Text-to-Scene System
WordsEye (Coyne and Sproat, 2001) is a system
for automatically converting natural language text
into 3D scenes representing the meaning of that
text. WordsEye supports language-based control
of spatial relations, textures and colors, collec-
tions, facial expressions, and poses; it handles
simple anaphora and coreference resolution, al-
lowing for a variety of ways of referring to ob-
jects. The system assembles scenes from a library
of 2,500 3D objects and 10,000 images tied to an
English lexicon of about 15,000 nouns.
The system includes a user interface where the
user can type simple sentences that are processed
to produce a 3D scene. The user can then modify
the text to refine the scene. In addition, individual
objects and their parts can be selected and high-
lighted with a bounding box to focus attention.
Several thousand real-world people have used
WordsEye online (http://www.wordseye.com). It
has also been used as a tool in education, to en-
hance literacy (Coyne et al., 2011b). In this paper,
we describe how we are using WordsEye to create
a comprehensive tool for field linguistics.
Vignette Semantics and VigNet To interpret in-
put text, WordsEye uses a lexical resource called
VigNet (Coyne et al., 2011a). VigNet is inspired
by and based on FrameNet (Baker et al., 1998),
a resource for lexical semantics. In FrameNet,
lexical items are grouped together in frames ac-
cording to their shared semantic structure. Every
frame contains a number of frame elements (se-
mantic roles) which are participants in this struc-
ture. The English FrameNet defines the mapping
between syntax and semantics for a lexical item by
providing lists of valence patterns that map syntac-
tic functions to frame elements.
VigNet extends FrameNet in two ways in or-
der to capture ?graphical semantics?,? the knowl-
edge needed to generate graphical scenes from
language. First, graphical semantics are added
to the frames by adding primitive graphical (typ-
ically, spatial) relations between the frame ele-
50
ment fillers. Second, VigNet distinguishes be-
tween meanings of words that are distinguished
graphically. For example, the specific objects
and spatial relations in the graphical semantics for
cook depend on the object being cooked and on
the culture in which it is being cooked (cooking
turkey in Baltimore vs. cooking an egg in Alice
Springs), even though at an abstract level cook an
egg in Alice Springs and cook a turkey in Bal-
timore are perfectly compositional semantically.
Frames augmented with graphical semantics are
called vignettes.
4 WordsEye Linguistics Tool (WELT)
In this section, we describe the two modes of
WELT, focusing on the aspects of our system that
will appear in our demonstration.
4.1 Tools for Linguistic Fieldwork
WELT includes tools that allow linguists to elicit
language with WordsEye. Each elicitation session
is organized around a set of WordsEye scenes. We
will demonstrate how a linguist would use WELT
in fieldwork, including (1) creating an elicitation
session, either starting from scratch, or by import-
ing scenes from a previous session; (2) building
scenes in WordsEye, saving them to a WELT ses-
sion, and modifying scenes previously added to
the session, either overwriting the original scene or
saving the changes as a new scene; (3) adding tex-
tual descriptions, glosses, and notes to a scene; and
(4) recording audio, which is automatically synced
to open scenes, and playingit back tto review any
given scene. A screen shot of the scene annotation
window is included in Figure 1.
To test the fieldwork capabilities of WELT,
we created a set of scenes based on the Max
Planck topological relations picture series (Bower-
man and Pederson, 1992). We used these scenes to
elicit descriptions from a native Nahuatl speaker;
some examples of scenes and descriptions are in-
cluded in Figure 2.
4.2 Formal Documentation of a Language
WELT also provides the means to formally doc-
ument the semantics of a language and create a
text-to-scene system for that language. The formal
documentation allows precise description of the
lexical semantics of a language. We will demon-
strate both the user interface for documenting se-
mantics, as well as a text-to-scene system for Ar-
Figure 1: WELT interface for annotating a scene
rernte created with WELT.
When a sentence is processed by WordsEye, it
goes through three main stages: (1) morphological
analysis and syntactic parsing, (2) semantic anal-
ysis, and (3) graphical realization. We will walk
through these modules in the context of WELT,
discussing (a) the formal documentation required
for that component, (b) the processing of an ex-
ample sentence through that component, and (c)
the parts of that component that will feature in our
demonstration. We will use the Arrernte sentence
shown in (1) as a running example.
(1) artwe le goal arrerneme
man ERG goal put.nonpast
The man kicks a goal.
Morphology and Syntax WELT first parses a
sentence into its morphology and syntax. Since
the focus of WELT is documentation of semantics,
the exact mechanisms for parsing the morphology
and syntax may vary. To document Arrernte, we
are using XFST (Karttunen et al., 1997) to model
the morphology and XLE (Crouch et al., 2006) to
model the syntax in the LFG formalism (Kaplan
and Bresnan, 1982). These are mature systems
that we believe are sufficient for the formal doc-
umentation of morphology and syntax. In future,
we will provide interfaces to the third-party tools
so that common information, like the lexicon, can
51
(a) in amat? t?akentija se kutSara
the paper cover one spoon
(b) in kwawit? t?apanawi t?akoja se mansana
the stick pass.thru in.middle one apple
Figure 2: Nahuatl examples elicited with WELT
be shared.
Running each word of the sentence through
the morphological analyzer in XFST transforms
the verb arrerneme into ?arrerne+NONPAST.? The
other tokens in the sentence remain unchanged.
Parsing the sentence with XLE gives the c-
structure shown in Figure 3(a) and the f-structure
shown in Figure 3(b). The f-structure will be
passed on to the semantics module.
(a)
(b)
Figure 3: C-structure (a) and f-structure (b) for
artwe le goal arrerneme.
We have added one additional feature to the
morphology and syntax module of WELT?s text-
to-scene system: an interface for selecting an f-
structure from multiple options produced by XLE,
in case the grammar is ambiguous. This way, a
linguist can use the WELT text-to-scene system
to verify their semantic documentation even if the
syntactic documentation is fairly rough. We will
demonstrate this feature when demonstrating the
Arrernte text-to-scene system.
Semantics The WELT semantics is represented
using VigNet, which has been developed for
WordsEye based on English. We will assume that
large parts of VigNet are language-independent
(for instance, the set of low-level graphical rela-
tions used to express the graphical semantics is
based on physics and human anatomy and does not
depend on language). Therefore, it should not be
necessary to create a completely new VigNet for
every language that will be used in WELT. In fu-
ture, we will develop tools for modifying VigNet
to handle linguistic and cultural differences as they
occur.
In order to use VigNet with other languages,
we need to map between the formal syntax of the
language being studied and the (English) lexical
semantics required currently by VigNet. One in-
stance showing why this is necessary occurs in our
example Arrrente sentence. When discussing foot-
ball in English, one would say that someone kicks
a goal or makes a goal. In Arrente, one would say
goal arrerneme, which translates literally to ?put
a goal.? Although the semantics of both sentences
are the same, the entry for ?put? in the English
VigNet does not include this meaning, but the Ar-
rernte text-to-scene system needs to account for it.
To address such instances, we have created an
interface for a linguist to specify a set of rules that
map from syntax to semantics. The rules take syn-
tactic f-structures as input and output a high-level
semantic representation compatible with VigNet.
The left-hand side of a rule consists of a set of con-
ditions on the f-structure elements and the right-
hand side consists of the semantic structure that
should be returned. Figure 4(a) is an example of
a rule mapping Arrernte syntax to semantics, cre-
ated in WELT.
In addition to these rules, the linguist creates a
simple table mapping lexical items into VigNet se-
mantic concepts, so that nouns can be converted to
graphical objects. We have created a mapping for
the lexical items in the Arrernte grammar; a partial
mapping is shown in Table 1.
We now describe the semantic processing of our
example Arrernte sentence, assuming a set of rules
consisting solely of the one in Figure 4(a) and the
noun mapping in Table 1. The f-structure in Fig-
52
(a) (b)
Figure 4: Syntax-semantics rule (a) and semantic category browser (b) from WELT
Lexical Item artwe panikane angepe akngwelye apwerte tipwele
VigNet Concept PERSON.N CUP.N CROW.N DOG.N ROCK-ITEM.N TABLE.N
Table 1: A mapping from nouns (lexical items) to VigNet semantic concepts
ure 3(b) has main predicate arrerne with two ar-
guments; the object is goal. Therefore, it matches
the left-hand-side of our rule. The output of
the rule specifies predicate CAUSE MOTION.KICK
with three arguments. The latter two are straight-
forward; the Theme is the VigNet object FOOTY-
BALL.N, and the Goal is FOOTYGOAL.N. To deter-
mine the Agent, we need to find the VigNet con-
cept corresponding to var-1, which occupies the
subject position in the f-structure. The subject in
our f-structure is artwe, and according to Table 1,
it maps to the VigNet concept PERSON.N. The re-
sulting semantic representation is augmented with
its graphical semantics, taken from the vignette
for CAUSE MOTION.KICK (vignette definition not
shown for lack of space). The final representation
is shown in Figure 5, with lexical semantics at the
top and graphical semantics below. The Words-
Eye system then builds the scene from these con-
straints and renders it in 3D.
CAUSE_MOTION.KICK
FOOTYBALL
Theme
FOOTYGOAL
Goal
PERSON
Agent
20 ft
FRONT-OF
Dist
ORIENT-TOPOSITION-BETWEEN
Figure GroundGoal Ground
IN-POSE
FigureSource SubjectFigure
kick
Value
Figure 5: The semantics (lexical and graphical) for
sentence (1)
WELT provides an interface for creating rules
by defining the tree structures for the left-hand-
side and right-hand-side of the rule. Every node on
the left-hand-side can optionally contain boolean
logic, if for example we want to allow the sub-
ject to be [(artwe ?man? OR arhele ?woman?) AND
NOT ampe ?child?]; so rules can be as simple or
complex as desired. Rules need not specify lexical
items directly; it is also possible to refer to more
general semantic categories. For example, a rule
could select for all verbs of motion, or specify a
particular constraint on the subject or object. In
figure 4(a), for instance, we may want to only al-
low animate subjects.
Semantic categories are chosen through a
browser that allows the user to search through all
the semantic categories defined in VigNet. For ex-
ample, if we want to find the semantic category
to use as a constraint on our example subject, we
might start by searching for human. This takes us
to a portion of a tree of semantic concepts cen-
tered around HUMAN.N. The semantic categories
are displayed one level at a time, so we initially
see only the concepts directly above and directly
below the word we searched for. From there, it?s
easy to select the concepts we are interested in,
and go up or down the tree until we find the one we
want. Below HUMAN.N are HUMAN-FEMALE.N
and HUMAN-MALE.N, but we are more interested
in the more general categories above the node. A
screen shot showing the result of this search is
shown in Figure 4(b). Above HUMAN.N is HU-
MANOID.N; above that, ANIMATE-BEING.N. Do-
ing a quick check of further parents and chil-
dren, we can see that for the subject of ?put goal,?
we would probably want to choose ANIMATE-
BEING.N over LIVING-THING.N.
The table mapping lexical items to VigNet con-
cepts is built in a similar way; the lexicon is au-
tomatically extracted from the LFG grammar, and
the user can search and browse semantic concepts
to find the appropriate node for each lexical item.
We will demonstrate the WELT user inter-
53
face which supports the creation of syntax-to-
semantics rules, creates the mapping between
nouns in the lexicon and VigNet concepts, and ver-
ifies the rules using the WELT text-to-scene sys-
tem. We will show examples from our documenta-
tion of Arrernte and demonstrate entering text into
the Arrernte text-to-scene system to generate pic-
tures.
5 Summary and Future Work
We have described a novel tool for linguists work-
ing with endangered languages. It provides a new
way to elicit data from informants, an interface
for formally documenting the lexical semantics of
a language, and allows the creation of a text-to-
scene system for any language.
This project is in its early stages, so we are plan-
ning many additional features and improvements.
For both modes of WELT, we want to generate pic-
tures appropriate for the target culture. To han-
dle this, we will add the ability to include cus-
tom objects and modify VigNet with new vignettes
or new graphical semantics for existing vignettes.
We also plan to build tools to import and export
the work done in WELT in order to facilitate col-
laboration among linguists working on similar lan-
guages or cultures. Sharing sets of scenes will al-
low linguists to reuse work and avoid duplicated
effort. Importing different versions of VigNet will
make it easier to start out with WELT on a new
language if it is similar to one that has already
been studied. We might expect, for instance, that
other Australian aboriginal languages will require
the same kinds of cultural modifications to VigNet
that we make for Arrernte, or that two languages
in the same family might also have similar syntax
to semantics rules.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
1160700.
References
Alliance for Linguistic Diversity. 2013. The En-
dangered Languages Project. http://www.
endangeredlanguages.com/.
C. Baker, J. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet project. In 36th Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics
(COLING-ACL?98), pages 86?90, Montr?eal.
C. Baker. 2008. FrameNet, present and future. In The
First International Conference on Global Interoper-
ability for Language Resources, pages 12?17.
E. Bender, D. Flickinger, and S. Oepen. 2002. The
Grammar Matrix. In J. Carroll, N. Oostdijk, and
R. Sutcliffe, editors, Workshop on Grammar En-
gineering and Evaluation at the 19th International
Conference on Computational Linguistics, pages 8?
14, Taipei, Taiwan.
S. Bird. 2009. Natural language processing and
linguistic fieldwork. Computational Linguistics,
35(3):469?474.
C. Black and H.A. Black. 2012. Grammars for the
people, by the people, made easier using PAWS and
XLingPaper. In Sebastian Nordoff, editor, Elec-
tronic Grammaticography, pages 103?128. Univer-
sity of Hawaii Press, Honolulu.
H.A. Black and G.F. Simons. 2006. The SIL Field-
Works Language Explorer approach to morphologi-
cal parsing. In Computational Linguistics for Less-
studied Languages: Texas Linguistics Society 10,
Austin, TX, November.
M. Bowerman and E. Pederson. 1992. Topological re-
lations picture series. In S. Levinson, editor, Space
stimuli kit 1.2, page 51, Nijmegen. Max Planck In-
stitute for Psycholinguistics.
M. Butt, H. Dyvik, T.H. King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project. In
2002 Workshop on Grammar Engineering and Eval-
uation - Volume 15, COLING-GEE ?02, pages 1?
7, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
B. Coyne and R. Sproat. 2001. WordsEye: An au-
tomatic text-to-scene conversion system. In SIG-
GRAPH.
B. Coyne, D. Bauer, and O. Rambow. 2011a. Vignet:
Grounding language in graphics using frame seman-
tics. In ACL Workshop on Relational Models of Se-
mantics (RELMS), Portland, OR.
B. Coyne, C. Schudel, M. Bitz, and J. Hirschberg.
2011b. Evaluating a text-to-scene generation system
as an aid to literacy. In SlaTE (Speech and Language
Technology in Education) Workshop at Interspeech,
Venice.
D. Crouch, M. Dalrymple, R. Kaplan, T. King,
J. Maxwell, and P. Newman, 2006. XLE Doc-
umentation. http://www2.parc.com/isl/
groups/nltt/xle/doc/xle.
C. Filmore, C. Johnson, and M. Petruck. 2003. Back-
ground to FrameNet. In International Journal of
Lexicography, pages 235?250.
R.M. Kaplan and J.W. Bresnan. 1982. Lexical-
functional grammar: A formal system for grammat-
ical representation. In J.W. Bresnan, editor, The
Mental Representation of Grammatical Relations.
MIT Press, Cambridge, Mass., December.
L. Karttunen, T. Ga?al, and A. Kempe. 1997. Xerox
finite-state tool. Technical report, Xerox Research
Centre Europe, Grenoble.
S. McConnel, 1995. PC-PATR Reference Manual.
Summer Institute for Linguistics.
54
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 152?161,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Detecting Levels of Interest from Spoken Dialog with Multistream
Prediction Feedback and Similarity Based Hierarchical Fusion Learning
William Yang Wang
Department of Computer Science
Columbia University
New York, NY 10027
yw2347@columbia.edu
Julia Hirschberg
Department of Computer Science
Columbia University
New York, NY 10027
julia@cs.columbia.edu
Abstract
Detecting levels of interest from speakers
is a new problem in Spoken Dialog Under-
standing with significant impact on real world
business applications. Previous work has fo-
cused on the analysis of traditional acous-
tic signals and shallow lexical features. In
this paper, we present a novel hierarchical fu-
sion learning model that takes feedback from
previous multistream predictions of promi-
nent seed samples into account and uses a
mean cosine similarity measure to learn rules
that improve reclassification. Our method is
domain-independent and can be adapted to
other speech and language processing areas
where domain adaptation is expensive to per-
form. Incorporating Discriminative Term Fre-
quency and Inverse Document Frequency (D-
TFIDF), lexical affect scoring, and low and
high level prosodic and acoustic features, our
experiments outperform the published results
of all systems participating in the 2010 Inter-
speech Paralinguistic Affect Subchallenge.
1 Introduction
In recent years, there has been growing interest in
identifying speakers? emotional state from speech
(Devillers and Vidrascu, 2006; Ai et al, 2006; Lis-
combe et al, 2005). For Spoken Dialog Systems
(SDS), the motivation has been to provide users with
improved over-the-phone services by recognizing
emotions such as anger and frustration and direct-
ing users to a human attendant. Other forms of par-
alinguistic information which researchers have at-
tempted to detect automatically include other classic
emotions, charismatic speech (Biadsy et al, 2008),
and deceptive speech (Hirschberg et al, 2005).
More recently, the 2010 Interspeech Paralinguisic
Affect Subchallenge sparked interest in detecting a
speaker?s level of interest (LOI), including both the
speaker?s interest in the topic and his/her willingness
to participating in the dialog (Schuller et al, 2010).
Sensing users? LOI in SDS should be useful in sales
domains, political polling, or service subscription.
In this paper, we present a similarity-based hi-
erarchical regression approach to predicting speak-
ers? LOI. The system has been developed based on
the hierarchical fusion learning of lexical and acous-
tic cues from speech. We investigate the contri-
bution of a novel source of information, Discrimi-
native TFIDF; lexical affect scoring; and prosodic
event features. Inspired by the successful use of
Pseudo Relevance Feedback (Tao and Zhai, 2006)
techniques in Information Retrieval and the cosine
similarity measure (Salton, 1989) in Data Mining,
we design a novel learning model which takes the
multistream prediction feedback that is initially re-
turned from seed samples 1 and uses a mean cosine
similarity measure to calculate the distance between
the new instance and prominent seed data points in
the Euclidean Space. We then add this similarity
measure as a new feature to perform a reclassifi-
cation. Our main contributions in this paper are:
(1) the novel Discriminative TFIDF approach for
lexical modeling and keywords spotting; (2) using
lexical affect scoring and language modeling tech-
niques to augment lexical modeling; (3) combin-
1Seed samples are from a random small subset in the test
set.
152
ing (1) and (2) with additional low-level prosodic
features together with voice quality and high-level
prosodic event features; and (4) introducing a mul-
tistream prediction feedback and mean cosine simi-
larity based fusion learning approach.
We outline related work in Section 2. The corpus,
system features, and machine learning approaches
are described in Section 3. We describe our experi-
mental results in Section 4 and conclude in Section
5.
2 Related Work
Schuller et al (2006) were among the first to study
LOI from conversational speech. They framed this
task as either a three-way or binary classification,
extracting standard acoustic features and building a
bag-of-words vector space model for lexical anal-
ysis. By linearly combining lexical features with
acoustic features, they achieved high F-measures
when using Support Vector Machine (SVM). Since
a bag-of-words model is a naive model, there may
be more valuable lexical information that it cannot
capture. Moreover, as lexical and acoustic features
are extracted from different domains, a single layer
linear combination may not yield the optimal results.
In 2010, Interspeech launched a Paralinguistic
Challenge (Schuller et al, 2010) that included the
task of detecting LOI from speech as a subchallenge.
Competitors were given conversational speech cor-
pora with annotated LOI, baseline acoustic features,
and two baseline results. The evaluation metric used
for the challenge was primarily the cross correlation
2 (CC) measure (Grimm et al, 2008), with mean
linear error 3 (MLE) also taken into consideration.
The baseline was built only on acoustic features, and
the CC and MLE for Training vs. Development sets
were 0.604 and 0.118. For the test data, CC and
MLE scores of 0.421 and 0.146 were observed.
Gajsek et al (2010) participated in this challenge
and proposed the use of Gaussian Mixture Models
as Universal Background Model (GMM-UBM) with
relevance MAP estimation for the acoustic data.
This is based on the success of GMM-UBM mod-
2Pearson product-moment correlation coefficient is a mea-
sure of the linear dependence that is widely used in regression
settings.
3MLE is a regression performance measure for the mean ab-
solute error between an estimator and the true value.
eling in the speaker identification tasks (Reynolds et
al., 2000). They achieved CC and MLE of 0.630 and
0.123 in the training vs. development condition, but
CC and MLE of only 0.390 and 0.143 in the testing
condition. This performance may have been due to
the fact that different subsets of the corpus include
different speakers: acoustic features alone may not
be robust enough to capture the speaker variation.
Jeon et al (2010) approach won the 2010 Sub-
challenge for this task. In addition to the baseline
acoustic features provided, they used term frequency
and a subjectivity dictionary to mine the lexical in-
formation. In addition to a linear combination of
all lexical and acoustic features, they designed a hi-
erarchical regression framework with multiple level
of combinations. Its first two combiners tackle the
prediction problems from different acoustic classi-
fiers and then uses a final stage SVM classifier to
combine the overall acoustic predictions with lexi-
cal predictions to form the final output. They report
a result of 0.622 for CC and 0.115 for MLE. On the
test set, they report CC and MLE of 0.428 and 0.146
respectively.
3 Our System
Unlike previous approaches, we emphasize lexical
modeling, to counter problems of speaker variation
in acoustic features (Jeon et al, 2010). We propose
an improved version of standard TFIDF (Spa?rck
Jones, 1972) ? Discriminative TFIDF ? which
computes the IDF score of the target word by dis-
criminating its different mean LOI score tags during
training to produce more informative keyword spot-
ting in testing.
In addition to Discriminative TFIDF, we uti-
lize the Dictionary of Affect in Language (DAL)
(Whissell, 1989) to detect lexical affect and com-
pute an utterance-level affect score. To maximize
the coverage of lexical cues, we also train trigram
language models on the training data to capture con-
textual information and use the test output log like-
lihoods and perplexities as features. Besides these
lexical features and the 1582 baseline acoustic fea-
tures from the Interspeech Paralinguistic Challenge,
we extract 32 additional prosodic and voice quality
features using Praat (Boersma, 2001). In order to
model sentence-level prosodic events, we use Au-
153
ToBI (Rosenberg, 2010) to extract pitch accent and
phrase-based features. These features are described
in detail in Section 3.2.
The simplest approach to classification is to in-
clude all features in a single classifier. However,
different features streams include different number
of features, extracted and represented in different
domains. The Sum Rule approach (Kittler et al,
1998) is an early solution to this classifier combi-
nation problem. Instead, we train 1st-tier classi-
fiers for each of the feature streams and then train
a 2nd-tier classifier to weight the posterior predic-
tions of the 1st-tier classifiers. We further improve
this method by integrating a novel model which con-
siders the 1st-tier multistream prediction feedback
from the seed samples and uses a mean cosine simi-
larity method to measure the distance between a new
instance and prominent seed samples. We use this
similarity measure to improve classification.
3.1 Corpus
The corpus we use in our experiments is the 2010
Paralinguistic Challenge Affect Subchallenge cor-
pus Technische Universta?t Munche?n Audiovisual In-
terest Corpus (TUM AVIC), provided by Schuller
(2010). The corpus consists of 10 hours of audio-
visual recordings of interviews in which an inter-
viewer provides commercial presentations of vari-
ous products to a subject. The subject and inter-
viewer discuss the product, and the subject com-
ments on his/her interest in it. Subjects were in-
structed to relax and not to worry about politeness
in the conversation. 21 subjects participated (11
male, 10 female), including three Asians and the rest
of European background. All interviews were con-
ducted in English; while none of the subjects were
native speakers, all were fluent. 11 subjects were
younger than 30; 7 were between 30-40; and 3 were
over 40. The subject portions of the recordings were
segmented into speaker turns (continuous speech by
one speaker with backchannels by the interviewer
ignored). These were further segmented into sub-
speaker turns at grammatical phrase boundaries such
that each segment is shorter than 2sec.
These smaller segments were annotated by four
male undergraduate psychology students for subject
LOI, using a 5-point scale as follows: (-2) Disin-
terest (subject is totally tired of discussing this topic
and totally passive); (-1) Indifference (subject is pas-
sive and does not want to give feedback); (0) Neu-
trality (subject follows and participates in the dialog,
but it is not recognized if she/he is interested in the
topic); (1) Interest (subject wants to talk about the
topic, follows the interviewer and asks questions);
(2) Curiosity (subject is strongly interest in the topic
and wants to learn more.) A normalized mean LOI
is then derived from mean LOI/2, to map the scores
into [-1, +1]. (Note that no negative scores occur
for this corpus.) In our experiments, we consider
the normalized mean LOI score as the label for each
sub-speaker turn segment; we refer to this as ?mean
LOI? below. The corpus was divided for the Sub-
challenge into training, development, and test cor-
pora; we use these divisions in our experimens.
3.2 Feature Sets
Table 1 provides an overview of the feature sets in
our system.
Discriminative TFIDF
In the standard vector space model, each word
is associated with its Term Frequency (TF) in the
utterance. The Inverse Document Frequency (IDF)
provides information on how rare the word is over
all utterances. The standard TFIDF vector of a term
t in an utterance u is represented as V(t,u):
V (t, u) = TF ? IDF =
C(t, u)
C(v, u)
? log
|U |
?
u(t)
TF is calculated by dividing the number of occur-
rences of term t in the utterance u by the total num-
ber of tokens v in the utterance u. IDF is the log of
the total number of utterances U in the training set,
divided by the number of utterances in the training
set in which the term t appears. u(t) can be viewed
as a simple function: if t appears in utterance u, then
it returns 1, otherwise 0.
In Discriminative TFIDF we add additional infor-
mation to the TFIDF metrics. When calculating IDF,
we weight each word by the distribution of its labels
in the training set. This helps us to weight words by
the LOI of the utterances they are uttered in. An in-
tuitive example is this: Although the words ?chaos?
and ?Audi? both appear once in the corpus, the oc-
currence of ?Audi? is in an utterance with a Mean
LOI score of 0.9, while ?chaos? appears in an utter-
ance with a label of 0.1. A standard TFIDF approach
154
Feature Sets Features
Discriminative TFIDF Sum of word-level Discriminative TFIDF scores
Lexical Affect Scoring Sum of word-level lexical affect scores
Language Modeling Trigram language model log-likelihood and perplexity
Acoustic Features 1582 acoustic features. Detail see Schuller et. al, (2010)
Pulses # Pulses, # Periods, Mean Periods, SDev Period
Voicing Fraction, # Voice Breaks, Degree, Voiced2total Frames
Jitter Local, Local (absolute), RAP, PPQ5
Shimmer Local, Local (dB), APQ3, APQ5, APQ11
Harmonicity Mean Autocorrelation, Mean NHR, Mean NHR (dB)
Duration Seconds
Fundamental Frequency Min, Max, Mean, Median, SDev, MAS
Energy Min, Max, Mean, SDev
Prosodic Events Pitch accents, intermediate phrase, and intonational boundaries.
Table 1: Feature Sets. RAP: Relative Average Perturbation. PPQ5: five-point Period Perturbation Quotient. APQn:
n-point Amplitude Perturbation Quotient. NHR: Noise-to-Harmonics Ratio. MAS: Mean Absolute Slope.
will give these two terms the same score. To differ-
entiate the importance of these two words, we define
our Discriminative TFIDF measure as follow:
V (t, u) =
C(t, u)
C(v, u)
?log
|U |
?
u(t) ? (1? |MeanLOI|)
Here, the Mean LOI score ranging from (0,1) is
the label of each utterance. Instead of summing
the u(t) scores directly, we now assign a weight to
each utterance. The weight is (1? |MeanLOI|) in
our task. The overall IDF score of words important
to identifying the LOI of an utterance will thus be
boosted, as the denominator of the IDF metric de-
creases compared to the standard TFIDF. Discrimi-
native TFIDF can be viewed as a generalized version
of Delta TFIDF (Martineau and Finin, 2009) that can
be used in various regression settings.
Wang and McKeown (2010) show that adding
Part-of-Speech (POS) information to text can be
helpful in similar classification tasks. So we have
also used the Stanford POS tagger (Toutanova and
Manning, 2000) to tag these transcripts before cal-
culating the Discriminative TFIDF score.
Lexical Affect Scoring
Whissell?s Dictionary of Affect in Language
(DAL) (Whissell, 1989) attempts to quantify emo-
tional language by asking raters to judge 8742 words
collected from various sources including college es-
says, interviews, and teenagers descriptions of their
own emotional state. Its pleasantness (EE) score in-
dicates the negative or positive valence of a word,
rated on a scale from 1 to 3. For example, ?aban-
don? scores 1.0, implying a fairly low level of pleas-
antness. A previous study (Agarwal et al, 2009)
notes that one of the advantages of this dictionary
is that it has different scores for various forms of a
root word. For example, the words ?affect? and ?af-
fection? have very different meanings; if they were
given the same score, the lexical affect quantifica-
tion might not be discriminative. To calculate an
utterance?s lexical affect score, we first remove the
stopwords and then sum up 4 the EE score of each
word in the utterance.
Statistical Language Modeling
In order to capture the contextual information and
maximize the use of lexical information, we also
train a statistical language model to augment the
Discriminative TFIDF and lexical affect scores. We
train trigram language models on the training set
using the SRI Language Modeling Tookit (Stolcke,
2002). In the testing stage, the log likelihood and
perplexity scores are used as language modeling fea-
tures. Due to the data sparsity issue, we are not able
to train language models on subsets of training data
that correspond to different LOI scores.
4We have experimented with Min, Max and Mean scores,
but the results were poor.
155
Acoustic, Prosodic and Voice Quality Features
As noted above, the TUM AVIC corpus includes
acoustic features (Schuller et al, 2010) for all of the
data sets. These include: PCM loudness, MFCC[0-
14], log Mel Frequency Band[0-7], Line Spectral
Pairs Frequency [0-7], F0 by Sub-Harmonic Sum.,
F0 Envelope, Voicing Probability, Jitter Local, Jit-
ter Difference of Difference of Periods, and Shim-
mer local. We have extracted an additional 32 stan-
dard prosodic and voice quality features to aug-
ment these, including Glottal Pulses, Voicing, Jitter,
Shimmer, Harmonicity, Duration, Fundamental Fre-
quency, and Energy (See Table 1).
Prosodic Event Features
To examine the contribution of higher-level
prosodic events, we have also experimented with
AuToBI (Rosenberg, 2010) to automatically de-
tect pitch accents, word boundaries, intermedi-
ate phrase boundaries, and intonational bound-
aries in utterances. AuToBI requires annotated
word boundary information; since we do not have
hand-annotated boundaries, we use the Penn Pho-
netics Lab Forced Aligner (Yuan and Liberman,
2008) to align each utterance with its transcription.
We use AuToBI?s models, which were trained on
the spontaneous speech Boston Directions Corpus
(BDC) (Hirschberg and Nakatani, 1996), to identify
prosodic events in our corpus.
3.3 Fusion Learning Approaches
Assuming that our various lexical, acoustic and
prosodic feature streams are informative to some ex-
tent when tested separately, we want to combine in-
formation from the streams in different domains to
improve prediction. We experimented with several
approaches, including Bag-of-Features, Sum Rule
combination, Hierarchical Fusion, and a new ap-
proach. We present here results of each on our LOI
prediction task. In the Bag-of-Features approach,
a simple classification method includes all features
in a single classifier. A potential problem with this
method is that, when combining 1582 acoustic fea-
tures with 10 lexical features, the classifier will treat
them equally, so potentially more useful lexical fea-
tures will not be evaluated properly. A second prob-
lem is that our features are extracted from differ-
ent domains using different methods, and normal-
ization across domains is not possible in a bag-of-
features classification/regression approach. Another
possible approach is the Sum Rule Combiner, which
uses product or sum rules to combine the predictions
from 1st-tier classifiers. Kittler et al (1998) show
that the Sum Rule approach outperforms the product
rule, max rule and mean rule approaches when com-
bining classifiers. Their sensitivity analysis shows
that this approach is most resilient to estimation er-
rors.
A third method of combining features is the Hier-
archical Fusion approach of fusing multistream in-
formation, which involves multiple classifiers and
performs classification/regression in multiple stages.
This can be implemented by first training 1st-tier
classifiers for each single stream of features, collect-
ing the predictions from these classifiers, and train-
ing a 2nd-tier supervector classifier to weight the im-
portance of predictions from the different streams
and make a final prediction. The rationale behind
this approach is to solve the cross-domain issue by
letting the 2nd-tier classifier weight the streams, as
the predictions from 1st-tier classifiers will be in a
unified/normalized form (e.g. 0 to 1 in this task).
The Multistream Prediction Feedback and Mean
Cosine Similarity based Hierarchical Fusion
Our Multistream Prediction Feedback and Mean
Cosine Similarity based Hierarchical Fusion ap-
proach combines a similarity based two-stage ap-
proach with a multistream feedback approach. Fig-
ure 1 shows the architecture of this system. It is
based on the intuition that, if we can identify the
prominent samples (e.g. the samples that all 1st-tier
classifiers assign high average prediction scores),
then we can measure the average distance between
a new sample and all these prominent samples in the
Euclidean Space. Furthermore, we can use this av-
erage distance (average similarity) as a new feature
to improve the 2nd-tier classifier?s final prediction.
To implement this process, we first train five
1st-tier Additive Logistic Regression (Friedman et
al., 2000) classifiers and a Random Subspace meta
learning (Ho, 1998) 1st-tier classifier (for the acous-
tic stream), using six different feature streams in our
training data. In the testing stage, we use a random
subset of the test set as seed samples. Next, we run
the seed samples for each of these 1st-tier classifiers
156
Seed 
Samples
Discriminative 
TFIDF
2nd-Tier 
Classifier:
RBF Kernel SVM
1st-Tier 
Addictive Regression and 
Random Subspace  
Classifiers:
New 
Samples
Lexical Affect 
Scoring
Language 
Modeling
Prosodic and 
Voice Quality
Acoustic
Prosodic Events
1st-Tier Predictions (seed)
S1: 0.8, 0.9, 0.6, 0.5, 0.7, 0.8S2: 0.3, 0.5, 0.4, 0.3, 0.2, 0.4S3: 0.4, 0.1, 0.3, 0.3, 0.1, 0.5?????
Maxn (Mean(Si))
Top-N
Prominent Samples
Avg. Cosine Similarly
1st-Tier Predictions (new)
S4: 0.7, 0.8, 0.6, 0.5, 0.8, 0.8, 99%S5: 0.6, 0.5, 0.4, 0.3, 0.7, 0.4, 72%?????
Final Prediction
Figure 1: The Overview of Multistream Prediction Feedback and Mean Cosine Similarity based Hierarchical
Fusion Learning
to obtain prediction scores ranging from 0 to 1. Now,
we take the mean of these predicted scores for each
sample, and use the following method to select the
top n samples from the seed samples S as ?promi-
nent samples?:
Prominent(S, n) = Maxn(Mean(S))
Recall that the cosine similarity (Salton, 1989) of
two utterances Ui, Uj in the vector-space model is:
cos(Ui, Uj) =
Ui ? Uj
||Ui||2 ? ||Uj ||2
where ??? indicates ?dot product?. Now, given our
hypothesized prominent samples, for each of these
samples and new samples, we choose the original
Discriminative TFIDF, Lexical Affect Scoring, Lan-
guage Modeling, Prosodic and Voice Quality, and
Prosodic Event features as k vectors to represent all
the samples in Euclidean Space. The reason we drop
the acoustic features from the vector space model is
because of the dimensionality issue ? 1582 acous-
tic features. We substitute our 32 standard prosodic
features instead. Now we use the mean cosine simi-
larity score to represent how far a new sample Un is
from the prominent samples US in the space:
Sim(Un, US) = Mean
?
?
?k
i=1 Vn ? Vs?
?k
i=1 V
2
n ?
?
?k
i=1 V
2
s
?
?
In the next step, we add this mean cosine sim-
ilarity measure as a new feature and include it in
the 2nd-tier classifier for reclassification. Now, in
the reclassification stage, all 1st-tier feature stream
predictions will be re-weighted by the new 2nd-tier
classifier that incorporated with Multistream Feed-
back information.
The reason why the Multistream Prediction Feed-
back is useful in this task is that, like many spoken
language understanding tasks, in LOI detection, if
we have a different set of speakers with different
genders, ages, and speaker styles, the overall feature
distribution for lexical, prosodic, and acoustic cues
in the test set can be very different from the training
set. Traditional speaker adaptation techniques typi-
157
cally focus only on the acoustic stream and may be
very expensive to perform. So, by extracting more
knowledge about the lexical, prosodic, and acoustic
features distributions in test set using our novel ap-
proach, we will have a better understanding about
the skewed distributions in the test set. In addition,
our approach is inexpensive and does not require ex-
tra unlabeled data.
4 Experiments and Results
We conduct our experiments in three parts. First, we
examine how well the Discriminative TFIDF feature
performs, compared with standard TFIDF feature.
Secondly, we look at how different feature sets influ-
ence our results. For the first two parts, we evaluate
our features using the Subchallenge training vs. de-
velopment sets only. Finally, we compare our sim-
ilarity based multistream fusion feedback approach
to other feature-combining approaches. We exam-
ine our final system first comparing training vs. de-
velopment performance, and then combined training
and development sets vs. the test set. WEKA (Wit-
ten and Frank, 2005) and LIBSVM (Chang and Lin,
2001) are used for regression.
4.1 TFIDF v.s. Discriminative TFIDF
Method CC MLE
TFIDF 0.296 0.142
D-TFIDF 0.368 0.140
S-D-TFIDF 0.381 0.136
Table 2: Single TFIDF Feature Stream Single Re-
gression Results (Train vs. Develop, Additive Logis-
tic Regression). D-TFIDF: Discriminative TFIDF. S-D-
TFIDF: the POS tagged version of D-TFIDF. CC: Cross
Correlation. MLE: Mean Linear Error.
When working with the training and develop-
ment sets, we are able to access the label and tran-
scriptions of each set to calculate the Discrimina-
tive TFIDF scores. For the testing scenario dis-
cussed in in Section 4.3, we do not have these anno-
tations. So, we redefine the task as a keyword spot-
ting task, where we can use the identified keywords
in the training and development sets as keyword fea-
tures in testing. We also sum up the word-level
TFIDF scores and use the sentence-level TFIDF as
a single feature for the classification experiment.
The regression algorithm we use is Additive Logis-
tic Regression with 50 iterations. Table 2 shows
how different approaches perform in the experiment.
We see that the Syntactic Discriminative TFIDF ap-
proach is much more informative than the standard
TFIDF approach. Note that, after calculating the
global IDF score, the standard TFIDF approach se-
lects 732 terms as top-1 level keywords. In contrast,
our Discriminative TFIDF has stronger discrimina-
tive power and picks a total number of 59 truly rare
terms as top-1 level keywords.
4.2 Regression with Different Feature Streams
Table 3 shows performance using different feature
streams in our system. We see that the acoustic
Feature Streams CC MLE
S-D-TFIDF 0.394 0.132
Language Modeling 0.404 0.141
Prosodic Events 0.458 0.133
Lexical Affect Scoring 0.459 0.132
Standard Prosody + VQ 0.591 0.122
Acoustic 0.607 0.118
Multistream Feedback (n=3) 0.234 0.150
Multistream Feedback (n=10) 0.262 0.149
Multistream Feedback (n=20) 0.290 0.146
Table 3: Comparing Contributions of Different Fea-
ture Streams in the 2nd-tier Classifier (Training vs. De-
velopmen, Random Subspace for the 1st-tier classifier of
Acoustic Stream, and Additive Logistic Regression for
other 1st-tier classifiers. Radial Basis Function (RBF)
Kernel SVM as 2nd-tier Classifier.) S-D-TFIDF: the POS
tagged version of D-TFIDF. VQ: Voice Quality. n: Top-n
Feedback. CC: Cross Correlation. MLE: Mean Linear
Error.
and prosodic features are the dominating features in
this task. The Prosodic Events feature stream also
emerges as a new informative high-level prosodic
feature in this task.
When testing the multistream feedback informa-
tion as a single feature stream, we see in the bottom
half of Table 3 that CC and MLE are improved when
we increase the number of prominent samples. Dis-
criminative TFIDF and Language Modeling are also
158
important, as seen from these results, but the Lexi-
cal Affect Scoring feature performs best among the
lexical features in this task. We suspect that the rea-
son may be a data sparsity issue, as we do not have a
large amount of data for training robust global Dis-
criminative IDF scores, language models, and the
feedback stream. In contrast, the DAL is trained on
much larger amounts of data.
4.3 Comparing with State-of-the-Art Systems
Table 4 compares our approach to alternative learn-
ing approaches. The first half of this table reports
results on training vs. development sets, and the sec-
ond half compares combined training and develope-
men vs. test set result.
Method CC MLE
Shuller et al,(2010) 0.604 0.118
Jeon et al, (2010) 0.622 0.115
Gajsek et al (2010) 0.630 0.123
Bag-of-features Fusion 0.602 0.118
Sum Rule Combination 0.617 0.117
SVM Hierarchical Fusion 0.628 0.115
Feedback + Hierarchical Fusion 0.640 0.113
Gajsek et al (2010) 0.390 0.143
Shuller et al,(2010) 0.421 0.146
Jeon et al, (2010) 0.428 0.146
Bag-of-features Fusion 0.420 0.145
Sum Rule Combination 0.422 0.138
SVM Hierarchical Fusion 0.450 0.131
Feedback + Hierarchical Fusion 0.480 0.131
Table 4: Comparing Different Systems. Above: Train-
ing vs. Development. Bottom: Combined Training+ De-
velopment vs. Test. CC: Cross Correlation. MLE: Mean
Linear Error.
Note that, in order to transcribe the test data, we
have trained a 20 Gaussian per state 39 MFCC Hid-
den Markov Model speech recognizer with HTK, us-
ing the training and development sets together with
TIMIT (Fisher et al, 1986), the Boston Directions
Corpus (BDC) (Hirschberg and Nakatani, 1996),
and the Columbia Game Corpus (Hirschberg et al,
2005). The word error rate (WER) is 29% on the
development set.
Note that a Bag-of-Features approach combin-
ing all features results in poorer performance than
the use of acoustic features alone. The Sum Rule
approach improves over this method by achieving
CC score of 0.422. Although the improvement of
CC seems small, it is extremely statistically signifi-
cant (Paired t-test with two-tailed P-value less than
0.0001), comparing to the Bag-of-features model.
However, when using the SVM as the 2nd-tier su-
pervector classifier to weight different prediction
streams, we achieve 0.628 CC and 0.115 MLE in
training vs. development data, and 0.450 CC and
0.131 MLE on the test set; this result is significantly
different from the Bag-of-features baseline (paired
t-test, p < 0.0001), but it is not significantly differ-
ent from the Sum Rule Combination approach.
Augmenting the SVM hierarchical fusion learn-
ing approach with multistream feedback, we observe
a significant improvement over all other systems and
methods. We obtain a final CC of 0.480 and MLE of
0.131 in the test mode, which is sigificantly differ-
ent from the Bag-of-features approach (paired t-test
p < 0.0001), but does not differ significantly from
the SVM hierarchical fusion approach.
5 Conclusion
Detecting levels of interest from speakers is an im-
portant problem for Spoken Dialog Understanding.
While earlier work, done in the 2010 Interspeech
Paralinguistic Affect Subchallenge, employing tra-
ditional acoustic features and shallow lexical fea-
tures, achieved good results, our new features ?
Discriminative TFIDF, lexical affect scoring, lan-
guage modeling, prosodic event ? when used with
standard prosodic features and our new Multistream
Prediction Feedback and Mean Cosine Similarity
heuristic-based Hierarchical Learning method im-
proves over all published results on the LOI cor-
pus. Our method is domain-independent and can
be adapted to other speech and language process-
ing areas where domain adaptation is expensive to
perform. In the future, we would like to experiment
with different distributional similarity measures and
bootstrapping strategies.
159
Acknowledgments
The first author was funded by Kathleen McKeown
while conducting the research. We would also like
to thank Andrew Rosenberg and three anonymous
reviewers for their useful comments.
References
Agarwal, Apoorv and Biadsy, Fadi and Mckeown, Kath-
leen R. 2009. Contextual Phrase-Level Polarity Anal-
ysis Using Lexical Affect Scoring And Syntactic N-
Grams. in EACL 2009.
Ai, Hua and Litman, Diane J. and Forbes-Riley, Kate and
Rotaru, Mihai and Tetreault, Joel and Purandare, Am-
ruta 2006. Using System and User Performance Fea-
tures to Improve Emotion Detection in Spoken Tutor-
ing Dialogs. in INTERSPEECH 2006.
Biadsy, Fadi and Rosenberg, Andrew and Carlson, Rolf
and Hirschberg, Julia and Strangert, Eva. 2008. A
Cross-Cultural Comparison of American, Palestinian,
and Swedish Perception of Charismatic Speech. in
Proceedings of the Speech Prosody 2008.
Boersma, Paul. 2001. Praat, a system for doing phonet-
ics by computer. in Glot International.
Chang,Chih-Chung and Lin, Chih-Jen. 2001. LIBSVM:
a library for support vector machines. Software avail-
able at www.csie.ntu.edu.tw/? cjlin/libsvm.
Devillers, Laurence and Vidrascu, Laurence 2006. Real-
life Emotions Detection with Lexical and Paralinguis-
tic Cues on Human-Human Call Center Dialogs. in
INTERSPEECH 2006.
Fisher, William M. and Doddington, George R. and
Goudie-Marshall, Kathleen M. 1986. The DARPA
Speech Recognition Research Database: Specifica-
tions and Status. in DARPA Workshop on Speech
Recognition.
Friedman, Jerome and Hastie, Trevor and Tibshirani,
Robert. 2000. Additive logistic regression: a statis-
tical view of boosting. in Ann. Statist..
Gajs?ek, Rok and Z?ibert, Janez and Justin, Tadej and
S?truc, Vitomir and Vesnicer, Bos?tjan and Mihelic?,
France. 2010. Gender and Affect Recognition Based
on GMM and GMMUBM modeling with relevance
MAP estimation. in INTERSPEECH 2010.
Grimm, Michael and Kroschel, Kristian and Narayana,
Shrikanth. 2008. The Vera am Mittag German Audio-
Visual Emotional Speech Database. in IEEE ICME.
Hirschberg, Julia and Nakatani, Christine H. 1996. A
prosodic analysis of discourse segments in direction-
giving monologues. in ACL 1996.
Hirschberg, Julia and Benus, Stefan and Brenier, Jason
M. and Enos, Frank and Friedman, Sarah and Gilman,
Sarah and Gir, Cynthia and Graciarena, Martin and
Kathol, Andreas and Michaelis, Laura. 2005. Distin-
guishing Deceptive from Non-Deceptive Speech. in
INTERSPEECH 2005.
Ho, Tin Kam. 1998. The Random Subspace Method for
Constructing Decision Forests. IEEE Transactions on
PAMI.
Jeon, Je Hun and Xia, Rui and Liu, Yang. 2010. Level of
Interest Sensing in Spoken Dialog Using Multi-level
Fusion of acoustic and Lexical Evidence. in INTER-
SPEECH 2010.
Kittler, Josef and Hatef, Mohamad and Duin, Robert P.
W. and Matas, Jiri. 1998. On combining classifiers.
IEEE Transactions on PAMI.
Laskowski, Kornel and Burger, Susanne. 2007. Analysis
of the Occurrence of Laughter in Meetings. in INTER-
SPEECH 2007.
Liscombe, Jackson and Hirschberg, Julia and Venditti,
Jennifer J.. 2005. Detecting Certainness in Spoken
Tutorial Dialogues. in Eurospeech.
Martineau, Justin and Finin, Tim. 2009. Delta TFIDF:
An Improved Feature Space for Sentiment Analysis.
in ICWSM.
Reynolds, Douglas A. and Quatieri, Thomas F. and Dunn,
Robert B. 2000. Speaker verication using adapted
gaussian mixture models. in Digital Signal Process-
ing.
Rosenberg, Andrew. 2010. AuToBI - A Tool for Auto-
matic ToBI Annotation. in INTERSPEECH 2010.
Salto, Gerard 1989. Automatic Text Processing: The
Transformation, Analysis, and Retrieval of Informa-
tion by Computer.
Schuller, Bjo?ern, and Ko?hler, Niels and Mu?eller, Ronald
and Rigoll, Gerhard. 2006. Recognition of Interest
in Human Conversational Speech. in INTERSPEECH
2006.
Schuller, Bjo?ern, and Steidl, Stefan and Batliner, An-
ton and Burkhardt, Felix and Devillers, Laurence and
Mu?eller, Christian and Narayanan, Shrikanth. 2010.
The INTERSPEECH 2010 Paralinguistic Challenge.
in INTERSPEECH 2010.
Spa?rck Jones, Karen. 1972. A statistical interpretation
of term specificity and its application in retrieval. in
Journal of Documentation.
Stolcke, Andreas. 2002. SRILM-an extensible language
modeling toolkit. in ICSLP 2002.
Toutanova, Kristina and Manning, Christopher D..
2000. Enriching the Knowledge Sources Used in
a Maximum Entropy Part-of-Speech Tagger. in
EMNLP/VLC-2000.
Tao, Tao and Zhai, ChengXiang 2006. Regularized esti-
mation of mixture models for robust pseudo-relevance
feedback. in SIGIR 2006.
160
Wang, William Yang and McKeown, Kathleen. 2010.
?Got You!?: Automatic Vandalism Detection in
Wikipedia with Web-based Shallow Syntactic-
Semantic Modeling. in COLING 2010.
Wang, Chingning and Zhang, Ping and Choi, Risook and
DEredita, Michael. 2002. Understanding consumers
attitude toward advertising. in Eighth Americas conf.
on Information System.
Witten, Ian H. and Frank, Eibe 2005. Data mining:
Practical machine learning tools and techniques, 2nd
Edition. San Francisco: Morgan Kaufmann.
Whissell, Cynthia. 1989. The Dictionary of Affect in
Language. in R. Plutchik and H. Kellerman, Editors,
Emotion: Theory Research and Experience.
Yuan, Jiahong and Liberman, Mark. 2008. Speaker iden-
tification on the SCOTUS corpus. in Proceedings of
acoustics ?08.
161
Proceedings of the 2012 Workshop on Language in Social Media (LSM 2012), pages 19?26,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Detecting Hate Speech on the World Wide Web
William Warner and Julia Hirschberg
Columbia University
Department of Computer Science
New York, NY 10027
whw2108@columbia.edu, julia@cs.columbia.edu
Abstract
We present an approach to detecting hate
speech in online text, where hate speech is
defined as abusive speech targeting specific
group characteristics, such as ethnic origin, re-
ligion, gender, or sexual orientation. While
hate speech against any group may exhibit
some common characteristics, we have ob-
served that hatred against each different group
is typically characterized by the use of a small
set of high frequency stereotypical words;
however, such words may be used in either
a positive or a negative sense, making our
task similar to that of words sense disambigua-
tion. In this paper we describe our definition
of hate speech, the collection and annotation
of our hate speech corpus, and a mechanism
for detecting some commonly used methods
of evading common ?dirty word? filters. We
describe pilot classification experiments in
which we classify anti-semitic speech reach-
ing an accuracy 94%, precision of 68% and
recall at 60%, for an F1 measure of .6375.
1 Introduction
Hate speech is a particular form of offensive lan-
guage that makes use of stereotypes to express an
ideology of hate. Nockleby (Nockleby, 2000) de-
fines hate speech as ?any communication that dis-
parages a person or a group on the basis of some
characteristic such as race, color, ethnicity, gen-
der, sexual orientation, nationality, religion, or other
characteristic.? In the United States, most hate
speech is protected by the First Amendment of
the U. S. Constitution, which, except for obscen-
ity, ?fighting words? and incitement, guarantees the
right to free speech, and internet commentators exer-
cise this right in online forums such as blogs, news-
groups, Twitter and Facebook. However, terms of
service for such hosted services typically prohibit
hate speech. Yahoo! Terms Of Service 1 prohibits
posting ?Content that is unlawful, harmful, threat-
ening, abusive, harassing, tortuous, defamatory, vul-
gar, obscene, libelous, invasive of another?s privacy,
hateful, or racially, ethnically or otherwise objec-
tionable.? Facebook?s terms 2 are similar, forbid-
ding ?content that: is hateful, threatening, or porno-
graphic; incites violence.? While user submissions
are typically filtered for a fixed list of offensive
words, no publicly available automatic classifier cur-
rently exists to identify hate speech itself.
In this paper we describe the small amount of ex-
isting literature relevant to our topic in Section 2. In
Section 3 we motivate our working definition of hate
speech. In Section 4 we describe the resources and
corpora of hate and non-hate speech we have used
in our experiments. In Section 5 we describe the an-
notation scheme we have developed and interlabeler
reliability of the labeling process. In Section 6 we
describe our approach to the classification problem
and the features we used. We present preliminary
results in Section 7, follow with an analysis of clas-
sification errors in 8 and conclude in Section 9 with
an outline of further work.
1Yahoo TOS, paragraph 9a
http://info.yahoo.com/legal/us/yahoo/utos/utos-173.html
2Facebook TOS, paragraph 3.7
https://www.facebook.com/legal/terms
19
2 Previous Literature
There is little previous literature on identifying hate
speech.
In (A Razavi, Diana Inkpen, Sasha Uritsky, Stan
Matwin, 2010), the authors look for Internet
?flames? in newsgroup messages using a three-stage
classifier. The language of flames is significantly
different from hate speech, but their method could
inform our work. Their primary contribution is a
dictionary of 2700 hand-labeled words and phrases.
In (Xu and Zhu, 2010), the authors look for offen-
sive language in YouTube comments and replaces
all but the first letter of each word with asterisks.
Again, while the language and the goal is different,
the method may have some value for detecting hate
speech. Their detection method parses the text and
arranges it into a hierarchy of clauses, phrases and
individual words. Both the annotation and the clas-
sification strategies found in this paper are based on
the sentiment analysis work found in (Pang and Lee,
2008) and (Pang, Lee and Vaithyanathan, 2002).
3 Defining Hate Speech
There are numerous issues involved in defining what
constitutes hate speech, which need to be resolved in
order to annotate a corpus and develop a consistent
language model. First, merely mentioning, or even
praising, an organization associated with hate crimes
does not by itself constitute hate speech. The name
?Ku Klux Klan? by itself is not hateful, as it may ap-
pear in historical articles, legal documents, or other
legitimate communication. Even an endorsement of
the organization does not constitute a verbal attack
on another group. While one may hypothesize that
such endorsements are made by authors who would
also be comfortable with hateful language, by them-
selves, we do not consider these statements to be
hate speech.
For the same reason, an author?s excessive pride
in his own race or group doesn?t constitute hate
speech. While such boasting may seem offensive
and likely to co-occur with hateful language, a dis-
paragement of others is required to satisfy the defi-
nition.
For example, the following sentence does not con-
stitute hate speech, even though it uses the word
?Aryan?.
And then Aryan pride will be true because
humility will come easily to Aryans who
will all by then have tasted death.
On the other hand, we believe that unnecessary
labeling of an individual as belonging to a group of-
ten should be categorized as hate speech. In the fol-
lowing example, hate is conveyed when the author
unnecessarily modifies bankers and workers with
?jew? and ?white.?
The next new item is a bumper sticker that
reads: ?Jew Bankers Get Bailouts, White
Workers Get Jewed!? These are only 10
cents each and require a minimum of a
$5.00 order
Unnecessarily calling attention to the race or eth-
nicity of an individual appears to be a way for an
author to invoke a well known, disparaging stereo-
type.
While disparaging terms and racial epithets when
used with the intent to harm always constitute hate-
ful language, there are some contexts in which such
terms are acceptable. For example, such words
might be acceptable in a discussion of the words
themselves. For example:
Kike is a word often used when trying to
offend a jew.
Sometimes such words are used by a speaker who
belongs to the targeted group, and these may be hard
to classify without that knowledge. For example:
Shit still happenin and no one is hearin
about it, but niggas livin it everyday.
African American authors appear to use the ?N?
word with a particular variant spelling, replacing
?er? with ?a?, to indicate group solidarity (Stephens-
Davidowitz, 2011). Such uses must be distin-
guished from hate speech mentions. For our pur-
poses, if the identity of the speaker cannot be as-
certained, and if no orthographic or other contextual
cues are present, such terms are categorized as hate-
ful.
20
4 Resources and Corpora
We received data from Yahoo! and the American
Jewish Congress (AJC) to conduct our research on
hate speech. Yahoo! provided data from its news
group posts that readers had found offensive. The
AJC provided pointers to websites identified as of-
fensive.
Through our partnership with the American Jew-
ish Congress, we received a list of 452 URLs previ-
ously obtained from Josh Attenberg (Attenberg and
Provost, 2010) which were originally collected to
classify websites that advertisers might find unsuit-
able. After downloading and examining the text
from these sites, we found a significant number that
contained hate speech according to our working def-
inition; in particular, a significant number were anti-
semitic. We noted, however, that sites which which
appeared to be anti-semitic rarely contained explic-
itly pejorative terms. Instead, they presented sci-
entifically worded essays presenting extremely anti-
semitic ideologies and conclusions. Some texts con-
tained frequent references to a well known hate
group, but did not themselves constitute examples of
hate speech. There were also examples containing
only defensive statements or declarations of pride,
rather than attacks directed toward a specific group.
In addition to the data we collected from these
URLs, Yahoo! provided us with several thou-
sand comments from Yahoo! groups that had been
flagged by readers as offensive, and subsequently
purged by administrators. These comments are
short, with an average of length of 31 words, and
lacked the contextual setting in which they were
originally found. Often, these purged comments
contained one or more offensive words, but obscured
with an intentional misspelling, presumably to evade
a filter employed by the site. For common racial ep-
ithets, often a single character substitution was used,
as in ?nagger?, or a homophone was employed, such
as ?joo.? Often an expanded spelling was employed,
in which each character was separated by a space
or punctuation mark, so that ?jew? would become
?j@e@w@.?
The two sources of data were quite different, but
complementary.
The Yahoo! Comment data contained many ex-
amples of offensive language that was sometimes
hateful and sometimes not, leading to our hypoth-
esis that hate speech resembles a word sense dis-
ambiguation task, since, a single word may appear
quite frequently in hate and non-speech texts. An
example is the word ?jew?. In addition, it provided
useful examples of techniques used to evade simple
lexical filters (in case such exist for a particular fo-
rum). Such evasive behavior generally constitutes a
positive indicator of offensive speech.
Web data captured from Attenberg?s URLs tended
to include longer texts, giving us more context,
and contained additional lower frequency offensive
terms. After examining this corpus, we decided
to attempt our first classification experiments at the
paragraph level, to make use of contextual features.
The data sets we received were considered offen-
sive, but neither was labeled for hate speech per se.
So we developed a labeling manual for annotating
hate speech and asked annotators to label a corpus
drawn from the web data set.
5 Corpus Collection and Annotation
We hypothesize that hate speech often employs well
known stereotypes to disparage an individual or
group. With that assumption, we may be further sub-
divide such speech by stereotype, and we can distin-
guish one form of hate speech from another by iden-
tifying the stereotype in the text. Each stereotype
has a language all its own, with one-word epithets,
phrases, concepts, metaphors and juxtapositions that
convey hateful intent. Anti-hispanic speech might
make reference to border crossing or legal identi-
fication. Anti-African American speech often ref-
erences unemployment or single parent upbringing.
And anti-semitic language often refers to money,
banking and media.
Given this, we find that creating a language model
for each stereotype is a necessary prerequisite for
building a model for all hate speech. We decided to
begin by building a classifier for anti-semitic speech,
which is rich with references to well known stereo-
types.
The use of stereotypes also means that some lan-
guage may be regarded as hateful even though no
single word in the passage is hateful by itself. Of-
ten there is a relationship between two or more sen-
tences that show the hateful intent of the author.
21
Using the website data, we captured paragraphs
that matched a general regular expression of words
relating to Judaism and Israel 3. This resulted in
about 9,000 paragraphs. Of those, we rejected those
that did not contain a complete sentence, contained
more than two unicode characters in a row, were
only one word long or longer than 64 words.
Next we identified seven categories to which
labelers would assign each paragraph. Annota-
tors could label a paragraph as anti-semitic, anti-
black, anti-asian, anti-woman, anti-muslim, anti-
immigrant or other-hate. These categories were de-
signed for annotation along the anti-semitic/not anti-
semitic axis, with the identification of other stereo-
types capturing mutual information between anti-
semitism and other hate speech. We were interested
in the correlation of anti-semitism with other stereo-
types. The categories we chose reflect the content
we encountered in the paragraphs that matched the
regular expression.
We created a simple interface to allow labelers
to assign one or more of the seven labels to each
paragraph. We instructed the labelers to lump to-
gether South Asia, Southeast Asia, China and the
rest of Asia into the category of anti-asian. The
anti-immigrant category was used to label xenopho-
bic speech in Europe and the United States. Other-
hate was most often used for anti-gay and anti-white
speech, whose frequency did not warrant categories
of their own.
5.1 Interlabeler Agreement and Labeling
Quality
We examined interlabeler agreement only for the
anti-semitic vs. other distinction. We had a set of
1000 paragraphs labeled by three different annota-
tors. The Fleiss kappa interlabeler agreement for
anti-semitic paragraphs vs. other was 0.63. We cre-
ated two corpora from this same set of 1000 para-
graphs. First, the majority corpus was generated
from the three labeled sets by selecting the label
with on which the majority agreed. Upon examin-
ing this corpus with the annotators, we found some
cases in which annotators had agreed upon labels
that seemed inconsistent with their other annotations
3jewish|jew|zionist|holocaust|denier|rabbi|
israel|semitic|semite
? often they had missed instances of hate speech
which they subsequently felt were clear cases. One
of the authors checked and corrected these apparent
?errors? in annotator labeling to create a gold cor-
pus. Results for both the original majority class an-
notations and the ?gold? annotations are presented
in Section 7.
As a way of gauging the performance of human
annotators, we compared two of the annotators? la-
bels to the gold corpus by treating their labeled para-
graphs as input to a two fold cross validation of
the classifier constructed from the gold corpus. We
computed a precision of 59% and recall of 68% for
the two annotators. This sets an upper bound on the
performance we should expect from a classifier.
6 Classification Approach
We used the template-based strategy presented in
(Yarowsky, 1994) to generate features from the cor-
pus. Each template was centered around a single
word as shown in Table 1. Literal words in an or-
dered two word window on either side of a given
word were used exactly as described in (Yarowsky,
1994). In addition, a part-of-speech tagging of each
sentence provided the similar part-of-speech win-
dows as features. Brown clusters as described in
(Koo, Carreras and Collins, 2008) were also utilized
in the same window. We also used the occurrence of
words in a ten word window. Finally, we associated
each word with the other labels that might have been
applied to the paragraph, so that if a paragraph con-
taining the word ?god? were labeled ?other-hate?, a
feature would be generated associating ?god? with
other-hate: ?RES:other-hate W+0:god?.
We adapted the hate-speech problem to the prob-
lem of word sense disambiguation. We say that
words have a stereotype sense, in that they either
anti-semitic or not, and we can learn the sense of
all words in the corpus from the paragraph labels.
We used a process similar to the one Yarowsky de-
scribed when he constructed his decisions lists, but
we expand the feature set. What is termed log-
likelihood in (Yarowsky, 1994) we will call log-
odds, and it is calculated in the following way. All
templates were generated for every paragraph in the
corpus, and a count of positive and negative occur-
rences for each template was maintained. The ab-
22
solute value of the ratio of positive to negative oc-
currences yielded the log-odds. Because log-odds is
based on a ratio, templates that do not occur at least
once as both positive and negative are discarded. A
feature is comprised of the template, its log-odds,
and its sense. This process produced 4379 features.
Next, we fed these features to an SVM classifier.
In this model, each feature is dimension in a fea-
ture vector. We treated the sense as a sign, 1 for
anti-semitic and -1 otherwise, and the weight of each
feature was the log-odds times the sense. The task
of classification is sensitive to weights that are large
relative to other weights in the feature space. To
address this, we eliminated the features whose log-
odds fell below a threshold of 1.5. The resulting val-
ues passed to the SVM ranged from -3.99 to -1.5 and
from +1.5 to +3.2. To find the threshold, we gener-
ated 40 models over an evenly distributed range of
thresholds and selected the value that optimized the
model?s f-measure using leave-1-out validation. We
conducted this procedure for two sets of indepen-
dent data and in both cases ended up with a log-odds
threshold of 1.5. After the elimination process, we
were left with 3537 features.
The most significant negative feature was the un-
igram literal ?black,?, with log-odds 3.99.
The most significant positive feature was the part-
of-speech trigram ?DT jewish NN?, or a determiner
followed by jewish followed by a noun. It was as-
signed a log-odds of 3.22.
In an attempt to avoid setting a threshold, we
also experimented with binary features, assigning -1
to negative feature weights and +1 to positive fea-
ture weights, but this had little effect, and are not
recorded in this paper. Similarly, adjusting the SVM
soft margin parameter C had no effect.
We also created two additional feature sets. The
all unigram set contains only templates that are
comprised of a single word literal. This set con-
tained 272 features, and the most significant re-
mained ?black.? The most significant anti-semitic
feature of this set was ?television,? with a log-odds
of 2.28. In the corpus we developed, television fig-
ures prominently in conspiracy theories our labelers
found anti-semitic.
The positive unigram set contained only unigram
templates with a positive (indicating anti-semitism)
log-odds. This set contained only 13 features, and
the most significant remained ?television.?
7 Preliminary Results
7.1 Baseline Accuracy
We established a baseline by computing the ac-
curacy of always assuming the majority (not anti-
semitic) classification. If N is the number of sam-
ples and Np is the number of positive (anti-semitic)
samples, accuracy is given by (N ? Np)/N , which
yielded a baseline accuracy of 0.910.
7.2 Classifiers
For each of the majority and gold corpora, we
generated a model for each type of feature tem-
plate strategy, resulting in six classifiers. We used
SVM light (Joachims, 1999) with a linear kernel
function. We performed 10 fold cross validation for
each classifier and recorded the results in Table 2.
As expected, our results on the majority corpus were
not as accurate as those on the gold corpus. Perhaps
surprising is that unigram feature sets out performed
the full set, with the smallest feature set, comprised
of only positive unigrams, performing the best.
8 Error Analysis
Table 3 contains a summary of errors made by all
the classifiers. For each classifier, the table reports
the two kinds of errors a binary classifier can make:
false negatives (which drive down recall), and false
positives (which drive down precision).
The following paragraph is clearly anti-semitic,
and all three annotators agreed. Since the classifier
failed to detect the anti-semitism, we use look at this
example of a false negative for hints to improve re-
call.
4. That the zionists and their american
sympathizers, in and out of the american
media and motion picture industry, who
constantly use the figure of ?six million?
have failed to offer even a shred of evi-
dence to prove their charge.
23
Table 1: Example Feature Templates
unigram ?W+0:america?
template literal ?W-1:you W+0:know?
template literal ?W-1:go W+0:back W+1:to?
template part of speech ?POS-1:DT W+0:age POS+1:IN?
template Brown sub-path ?W+0:karma BRO+1:0x3fc00:0x9c00 BRO+2:0x3fc00:0x13000?
occurs in ?10 word window ?WIN10:lost W+0:war?
other labels ?RES:anti-muslim W+0:jokes?
Table 2: Classification Performance
Accuracy Precision Recall F1
Majority All Unigram 0.94 0.00 0.00 0.00
Majority Positive Unigram 0.94 0.67 0.07 0.12
Majority Full Classifier 0.94 0.45 0.08 0.14
Gold All Unigram 0.94 0.71 0.51 0.59
Gold Positive Unigram 0.94 0.68 0.60 0.63
Gold Full Classifier 0.93 0.67 0.36 0.47
Human Annotators 0.96 0.59 0.68 0.63
Table 3: Error Report
False Negative False Positive
Majority All Unigram 6.0% 0.1%
Majority Positive Unigram 5.6% 0.2%
Majority Full Classifier 5.5% 0.6%
Gold All Unigram 4.4% 1.8%
Gold Positive Unigram 3.6% 2.5%
Gold Full Classifier 5.7% 1.6%
24
The linguistic features that clearly flag this para-
graph as anti-semitic are the noun phrase containing
zionist ... sympathizers, the gratuitous inclusion of
media and motion picture industry and the skepti-
cism indicated by quoting the phrase ?six million?.
It is possible that the first feature could have been de-
tected by adding parts of speech and Brown Cluster
paths to the 10 word occurrence window. A method
for detecting redundancy might also be employed to
detect the second feature. Recent work on emotional
speech might be used to detect the third.
The following paragraph is more ambiguous. The
annotator knew that GT stood for gentile, which left
the impression of an intentional misspelling. With
the word spelled out, the sentence might not be anti-
semitic.
18 ) A jew and a GT mustn?t be buried side
by side.
Specialized knowledge of stereotypical language
and the various ways that its authors mask it could
make a classifier?s performance superior to that of
the average human reader.
The following sentence was labeled negative by
annotators but the classifier predicted an anti-semitic
label.
What do knowledgeable jews say?
This false positive is nothing more than a case of
over fitting. Accumulating more data containing the
word ?jews? in the absence of anti-semitism would
fix this problem.
9 Conclusions and Future Work
Using the feature templates described by Yarowsky
we successfully modeled hate speech as a classifica-
tion problem. In terms of f-measure, our best classi-
fier equaled the performance of our volunteer anno-
tators. However, bigram and trigram templates de-
graded the performance of the classifier. The learn-
ing phase of the classifier is sensitive to features that
ought to cancel each other out. Further research on
classification methods, parameter selection and op-
timal kernel functions for our data is necessary.
Our definition of the labeling problem could have
been more clearly stated to our annotators. The anti-
immigrant category in particular may have confused
some.
The recall of the system is low. This suggests
there are larger linguistic patterns that our shallow
parses cannot detect. A deeper parse and an analysis
of the resulting tree might reveal significant phrase
patterns. Looking for patterns of emotional speech,
as in (Lipscombe, Venditti and Hirschberg, 2003)
could also improve our recall.
The order of the paragraphs in their original con-
text could be used as input into a latent variable
learning model. McDonald (McDonald et al 2007)
has reported some success mixing fine and course
labeling in sentiment analysis.
Acknowledgments
The authors are grateful for the data and the sup-
port of Matthew Holtzman of the American Jewish
Congress. We would also like to thank Belle Tseng,
Kim Capps-Tanaka, Evgeniy Gabrilovich and Mar-
tin Zinkevich of Yahoo! for providing data as well
as for their financial support. Without their support,
this research would not have been possible.
References
[Choi et al2005] Yejin Choi, Claire Cardie, Ellen Riloff,
Siddharth Patwardhan, Identifying Sources of Opin-
ions with Conditional Random Fields and Extraction
Patterns. In HLT ?05 Association for Computational
Linguistics Stroudsburg, PA, USA, pp. 355-362, 2005
[Yarowsky 1994] David Yarowsky, Decision Lists for
Lexical Ambiguity Resolution: Application to Ac-
cent Restoration in Spanish and French. In ACL-94,
Stroudsburg, PA, pp. 88-95, 1994
[Yarowsky 1995] David Yarowsky, Unsupervised Word
Sense Disambiguation Rivaling Supervised Methods.
In ACL-95, Cambridge, MA, pp. 189-196, 1995.
[Nockleby 2000] John T. Nockleby, Hate Speech. In
Encyclopedia of the American Constitution (2nd ed.,
edited by Leonard W. Levy, Kenneth L. Karst et al,
New York: Macmillan, 2000), pp. 1277-1279 (see
http://www.jiffynotes.com/a_study_
guides/book_notes/eamc_03/eamc_03_
01193.html)
[Stephens-Davidowitz 2011] Seth Stephens-Davidowitz,
The Effects of Racial Animus on Voting: Evidence Us-
ing Google Search Data http://www.people.
fas.harvard.edu/?sstephen/papers/
RacialAnimusAndVotingSethStephensDavidowitz.
pdf
25
[McDonald et al2007] McDonald, R. Hannan, K. Ney-
lon, T. Wells, M. Reynar, J. Structured Models
for Fine-to-Coarse Sentiment Analysis. In ANNUAL
MEETING- ASSOCIATION FOR COMPUTATIONAL
LINGUISTICS 2007, CONF 45; VOL 1, pages 432-
439
[Pang and Lee 2008] Pang, Bo and Lee, Lillian, Opinion
Mining and Sentiment Analysis. In Foundations and
Trends in Information Retrieval, issue 1-2, vol. 2, Now
Publishers Inc., Hanover, MA, USA, 2008 pp. 1?135
[Pang, Lee and Vaithyanathan 2002] Pang, Bo and Lee,
Lillian and Vaithyanathan, Shivakumar Thumbs up?:
sentiment classification using machine learning tech-
niques. In Proceedings of the ACL-02 conference on
Empirical methods in natural language processing -
Volume 10, Association for Computational Linguis-
tics, Stroudsburg, PA, USA, 2002 pp. 79-86
[Qiu et al2009] Qiu, Guang and Liu, Bing and Bu, Jia-
jun and Chen, Chun Expanding domain sentiment lexi-
con through double propagation. In Proceedings of the
21st international jont conference on Artificial intelli-
gence, Morgan Kaufmann Publishers Inc. San Fran-
cisco, CA, USA, 2009 pp. 1199-1204
[Joachims 1999] Making large-Scale SVM Learning
Practical. Advances in Kernel Methods - Support
Vector Learning, B. Schlkopf and C. Burges and A.
Smola (ed.), MIT-Press, 1999.
[Koo, Carreras and Collins 2008] Simple Semi-
supervised Dependency Parsing In Proc. ACL/HLT
2008
[Xu and Zhu 2010] Filtering Offensive Language in On-
line Communities using Grammatical Relations
[A Razavi, Diana Inkpen, Sasha Uritsky, Stan Matwin 2010]
Offensive Language Detection Using Multi-level Clas-
sification In Advances in Artificial Intelligence
Springer, 2010, pp. 1627
[Attenberg and Provost 2010] Why Label When You Can
Search?: Alternatives to active learning for applying
human resources to build classification models under
extreme class imbalance, KDD 2010
[Lipscombe, Venditti and Hirschberg 2003] Classifying
Subject Ratings of Emotional Speech Using Acoustic
Features. In Proceedings of Eurospeech 2003, Geneva.
26
Proceedings of the SIGDIAL 2013 Conference, pages 132?136,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Exploring Features For Localized Detection of Speech Recognition Errors
Eli Pincus and Svetlana Stoyanchev and Julia Hirschberg
Department of Computer Science, Columbia University, USA
elipincus@gmail.com & sstoyanchev@cs.columbia.edu
& julia@cs.columbia.edu
Abstract
We address the problem of localized error
detection in Automatic Speech Recognition
(ASR) output to support the generation of tar-
geted clarifications in spoken dialogue sys-
tems. Localized error detection finds specific
mis-recognized words in a user utterance. Tar-
geted clarifications, in contrast with generic
?please repeat/rephrase? clarifications, target
a specific mis-recognized word in an utter-
ance (Stoyanchev et al, 2012a) and require
accurate detection of such words. We extend
and modify work presented in (Stoyanchev et
al., 2012b) by experimenting with a new set
of features for predicting the likelihood of a
local error in an ASR hypothesis on an un-
sifted version of the original dataset. We im-
prove over baseline results, where only ASR-
generated features are used, by constructing
optimal feature sets for utterance and word
mis-recognition prediction. The f-measure for
identifying incorrect utterances improves by
2.2% and by 3.9% for identifiying incorrect
words.
1 Introduction
Spoken Dialogue Systems typically indicate their lack
of understanding of user input by simple requests for
repetition or rephrasing ? ?I?m sorry, I didn?t under-
stand you.?, or ?Can you please repeat??. However
human conversational partners generally provide more
targeted clarification requests. Corpus analysis of hu-
man conversations have shown that people are more
likely to indicate what they have understood and what
they have not understood by producing reprise clar-
ification questions (Purver, 2004; Stoyanchev et al,
2012a), as illustrated in the following exchange where
XXX indicates a word misunderstood by speaker B:
A: Do you have any XXX in your bag?
B: Do I have any what in my bag?
A reprise clarification question targets a specific mis-
recognized word and incorporates recognized context
into a clarification question.
We investigate replacing generic please repeat clari-
fications with more natural targeted clarifications in au-
tomatic spoken systems. Targeted clarifications allow
users to provide a concise response to a clarification
question which is beneficial for spoken systems accept-
ing broad vocabulary and flexible syntax. Examples of
such systems include tutoring systems, intelligent as-
sistants, and spoken translation systems (Litman and
Silliman, 2004; Dzikovska et al, 2009; Akbacak et al,
2009).
To enable Spoken Dialogue Systems (SDS) to gen-
erate targeted clarification questions, we must first be
able to identify mis-recognized words with high accu-
racy. We term such mis-recognition detection localized
error detection. Accurate distinction between correctly
and incorrectly recognized words is essential to the cre-
ation of appropriate targeted clarification questions.
In previous research on recognition error detection in
dialogue systems, researchers have addressed error de-
tection at the utterance level (Hirschberg et al, 2004;
Komatani and Okuno, 2010). In this paper we present
results of classification experiments designed to de-
tect localized errors within the utterance. Our base-
line results are obtained from a classifier trained only
on word posterior probabilities generated by an Auto-
matic Speech Recognition (ASR) engine. ASR confi-
dence score computation is an active research area, re-
lying upon acoustic and lexical collocation information
to compute confidence scores. We determine whether
improvement over baseline can be achieved by training
a classifier for utterance and word mis-recognition pre-
diction on an expanded feature set that includes lexical,
positional, prosodic, semantic, syntactic as well as ad-
ditional ASR score features. All of the features we ex-
periment with can be computed from an ASR hypothe-
sis without affecting the performance of a SDS materi-
ally. After determining optimal feature sets we experi-
ment with one- and two-stage approaches for localized
error detection. The first simply identifies whether a
word is correctly recognized or not. The second first
classifies an utterance as incorrect or correct and then
classifies errors only on utterances labeled incorrect.
132
This work extends earlier work in which we eval-
uated a smaller set of syntactic and prosodic fea-
tures (Stoyanchev et al, 2012b). In addition to im-
provements implemented in the ASR engine that we
use to produce ASR hypotheses, our current work re-
ports results on a larger dataset which includes com-
mands to the system and utterances containing disflu-
encies. Here, we propose a framework for localized
error detection that does not rely upon pre-filtering of
the dataset.
In Section 2 we describe our corpus. In Section 3
we discuss our classification experiments. In Section
4 we discuss our results. In Section 5 we present our
conclusions and discuss future research.
2 Data
We conduct our machine learning experiments on the
DARPA TRANSTAC corpus (Weiss et al, 2008). The
TRANSTAC corpus is comprised of staged conversa-
tions between American military personnel and Ara-
bic interviewees utilizing IraqComm speech-to-speech
translation system (Akbacak et al, 2009). This data
was collected by NIST between 2005 and 2008 in eval-
uation exercises. The dataset contains audio record-
ings and manual transcript of English and Arabic utter-
ances. We used SRI?s DynaSpeak (Franco et al, 2002)
speech recognition system to recognize the English ut-
terances and use posterior probabilities from DynaS-
peak as our baseline feature. We create a corpus from
this dataset that contains over 99% of the English ut-
terances. 38 utterances were removed from the dataset
either for lack of actual speech data or errors in refer-
ence transcription. 26.2% of our cleaned corpus con-
sist of mis-recognized instances and 6.4% of the total
words in it are incorrectly recognized by DynaSpeak
(see Table 1). We are using an unsifted version of the
corpus used in our previous work (Stoyanchev et al,
2012b) whose hypotheses were produced with a new
version of the DynaSpeak ASR system. In our previous
work utterances containing disfluencies and commands
to the system were excluded. We seek to avoid the cas-
cading errors that would follow from implementing a
2-step framework for localized error detection where
the first step is command and disfluency detection and
the second step is localized error detection. The 1-step
framework also has the advantage of working for all ut-
terances including ones that contain commands or dis-
fluencies. Due to these differences, our current results
are not directly comparable with our previous results.
Table 1: Corpus statistics
Overall Correct ASR Incorr ASR
All utts. 3,952 2,914 (73.7%) 1,038 (26.2%)
All wrds. 25,333 23,705 (93.6%) 1,628 (6.4%)
wrds in err utts 7,888 6,260 (79.4%) 1,628 (20.6%)
3 Method
We analyze how the performance of predicting mis-
recognized utterances and words is affected by the
use of lexical, positional, prosodic, semantic, and syn-
tactic features in addition to ASR confidence scores.
We perform machine learning experiments using the
Weka Machine Learning Library to construct a J48
decision tree classifier boosted with MultiBoostAB
method (Witten and Eibe, 2005).
Baseline confidence features We use ASR posterior
scores extracted from the log files output by Dynaspeak
as a baseline feature set in our experiments. In the utter-
ance mis-recognition prediction experiment, we calcu-
late the average of the logarithm of the ASR posterior
scores over all words in the hypothesis. In the word
mis-recognition prediction experiment we use the log-
arithm of the posterior score of a given word.
Feature selection We run a heuristic feature ex-
ploration experiment to identify optimal feature sets
for predicting mis-recognized utterances and mis-
recognized words. We first use a greedy approach
adding one feature at a time to the baseline ASR feature
set and only keep a feature in the set if it improves F-
measure predicting mis-recognition. We then use an al-
ternate greedy approach in which we begin with a fea-
ture set composed of all extracted features and proceed
to remove one feature at a time and only leave it out
of the set if incorrect F-measure improved or remained
the same with its absence. The second approach yields
the optimal feature sets for both utterance and word
mis-recognition prediction. Table 2 lists the features
that make up these optimal sets. For incorrect utter-
ance prediction, we run a 10-fold cross validation on
all utterances. For incorrect word prediction, we run a
10-fold cross validation on all words in mis-recognized
utterances.1 We next describe the features we found to
be useful in prediction and those that did not improve
performance.
3.1 Useful Features
ASR context features We use the logarithm of the pos-
terior score of a given word and the average of the log-
arithm of the posterior scores for both a given word and
its surrounding context. We use one word context be-
fore and after the given word. We also use the average
of the logarithm of the posterior scores for all words in
the utterance.
Lexical features We hypothesize that properties of
words such as length and frequency are predictive of
whether a word is correctly recognized. In particular,
noting that words of greater length are often better rec-
ognized by an ASR engine, we examine the length, fre-
quency, and posterior score of the maximum and min-
1Because of the size limitations of our dataset feature se-
lection and evaluation are performed on the same dataset.
133
imum words in an utterance. For mis-recognized ut-
terance prediction, we find that the average length of a
word in the utterance are useful features for predicting
both mis-recognized utterances and words. For mis-
recognized word prediction, we find the word length of
the surrounding words, the current word, and the fre-
quency of the longest word in an utterance are useful.
We also find that utterance length calculated in words is
a useful feature for predicting both utterance and word
mis-recognition.
Positional features Motivated by the use of dialogue
history features in Lopes et al(2011), we find that the
location of the hypothesis relative to the speaker?s first
utterance in the dialogue (utterance location) is a use-
ful feature. Similarly, we obtain improvement from the
word index feature, the distance of the word from the
first word in the utterance.
Syntactic POS tags were shown to be helpful in our
previous work and we find that these tags improve the
current results as well. We obtain these from the Stan-
ford POS tagger (et al, 2003). In mis-recognized ut-
terance prediction, we use unigram and bigram counts
of POS tags as a feature. For mis-recognized word pre-
diction, we use the word?s POS tag as well as the POS
tag for the surrounding one or two words.
We obtain a binary Func/Content feature using a
function word list to distinguish function from content
words. The list includes certain adverbs, conjunctions,
determiners, modal verbs, primary verbs such as be,
prepositions, pronouns, and WP-pronouns. These tags
also boost our ability to identify mis-recognized words.
The feature Func/Tot ratio is the fraction of function
words to total words in an ASR hypothesis. We hy-
pothesize that an extreme value of the Func/Tot ratio
may indicate a potential mis-recognition, and it does
improve both utterance and word mis-recognition pre-
diction.
3.2 Less Useful Features
Features we do not find helpful include information as-
sociated with the minimum length word in the utter-
ance, the fraction of words in an utterance that pos-
sess greater length than the average length word in the
corpus, as well as syntactic features such as a depen-
dency tag assigned to the word. Additional unhelp-
ful features include prosodic features, such as shimmer
and jitter identified by PRAAT (Boersma and Weenink,
2013) and pitch and phrase information extracted from
AuToBI(Rosenberg, 2010) software. Performing a se-
mantic role label of our hypotheses with the software
SENNA (Collobert et al, 2011) also did not provide
helpful semantic features.
System Performance To evaluate performance of
our mis-recognized word classifier, we use the selected
features in 1-stage and 2-stage approaches. First, we
train models for utterance and word classification sep-
Table 2: Features
Cat Specific In Optimal Utt
Feature Set
In Optimal
Wrd Feature
Set
ASR Log Post Score Yes (avg of all
wrds in utt)
Yes (curr wrd)
ASR-
CTX
Log Post Score No Yes (avg of curr
wrd, curr wrd
context, avg of
all wrds in utt)
Lex Wrd length Yes (avg wrd
length in utt)
Yes
(curr,prev,next)
Max Wrd freq No Yes
Utt length Yes Yes
POS Utt location Yes Yes
Word Index No Yes (curr)
Syn POS Tag Yes (unigram
and bigram
count)
Yes
(curr,prev,next)
Func/Cont tag No Yes (curr, prev,
next)
Func/Tot ratio Yes Yes
arately on 80% of the dataset with up-sampling (35%)2
of the incorrect instances as well as with the actual dis-
tribution of incorrect instances in the corpus (20.6% ut-
terances, 6.4% words). We then test these models on
the remaining 20% of the dataset using the 1-stage and
2-stage approach. In the 1-stage approach we test on
20% of the total words in the corpus. In the 2-stage ap-
proach we first test on 20% of the total utterances in the
corpus and then only test on the words in the utterances
labeled as mis-recognized.
4 Results
New Feature Experiments Using our newly con-
structed utterance feature set we are able to boost incor-
rect utterance classification F-measure by 2.2% from
.597 to .610 (see Table 3). The increase in F-measure
for incorrect utterance mis-recognition is due to an in-
crease in incorrect utterance recall from .531 to .555.
There is a slight decrease in incorrect utterance pre-
cision from .682 to .678. Overall classification accu-
racy improves by 2.1% points (absolute) from 81.2%
to 83.3%. Using our newly constructed word feature
set we are able to improve incorrect word classification
F-measure by 3.9% from .620 to .644 (see Table 4). For
incorrect word classification there is an increase in both
mis-recognized word precision and recall; the former
increasing from .678 to .719 and the latter increasing
from .571 to .584. The results for incorrect word clas-
sification represent a statistically significant improve-
2This percentage was derived empirically.
134
Table 3: Utterance new feature experiment results
Feature Correct Incorrect % F-Measure Incorr Imp Accuracy
P ? R ? F P ? R ? F over ASR Only
ASR .845 ? .912 ? .877 .682 ? .531 ? .597 - 81.2%
ASR+LEX+POS+SYN .851 ? .906 ? .878 .678 ? .555 ? .610 2.2% 83.3%
Table 4: Word new feature experiment results
Feature Correct Incorrect % F-Measure Incorr Imp Accuracy
P ? R ? F P ? R ? F over ASR only
ASR .893 ? .930 ? .911 .678 ? .571 ? .620 - 85.5%
ASR+LEX+POS+SYN .897 ? .941 ? .918 .719 ? .584 ? .644 3.9% 86.7%
Table 5: 1-stage and 2-stage approach results
Experiment Correct Incorrect Accuracy
P ? R ? F P ? R ? F
Maj. Baseline .94 ? 1.00 ? .97 - ? 0 ? - 94%
1-stage original .97 ? .94 ? .96 .39 ? .57 ? .46 92%
1-stage (35% upsample) .98 ? .90 ? .94 .31 ? .72 ? .44 89%
2-stage original .96 ? .98 ? .97 .51 ? .34 ? .41 94%
2-stage (35% upsample) .96 ? .96 ? .96 .41 ? .46 ? .43 93%
ment3. Overall classification accuracy improves by
1.2% points (absolute) from 85.5% to 86.7%.
1-stage and 2-stage experiments To estimate how
well a dialogue system could perform incorrect word
classification we run our 1-stage and 2-stage ap-
proaches. The 1-stage approaches (with and without
up-sampling) are able to achieve higher recall; while
the 2-stage approaches (with and without up-sampling)
are able to achieve higher precision. The 2-stage re-
sult?s higher precision is not surprising given that this
approach has two chances to filter out correct words
? first with utterance classification and then with
word classification. In our 1-stage approach with up-
sampling we are able to identify almost 3/4 (72%) of
the incorrect words in the corpus (see Table 5). In
our 2-stage approach without up-sampling we are able
to accurately label just over 1/2 (51%) of the total in-
stances we identify as incorrect. In future work we will
experiment with additional features in order to boost
precision for incorrect word classification to a level
suitable for use in the construction of reprise clarifi-
cation questions.
5 Conclusions
We have presented results of machine learning exper-
iments that utilize new features to improve localized
detection of ASR errors to assist spoken dialogue sys-
tem?s production of reprise clarification questions. We
conducted feature selection experiments to find optimal
feature sets to train classifiers for utterance and word
mis-recognition prediction. We find that certain lexi-
cal, positional, and syntactic features improve classi-
fication results over a baseline feature set containing
only ASR posterior score features. We improve incor-
rect F-measure for utterance mis-recognition prediction
by 2.2% by adding utterance length, location, fraction
3?2test(p < .01)
of function words to total words, average word length,
and unigram and bigram count to the baseline feature
set. By removing average word length as well as uni-
gram and bigram count from this optimal set for utter-
ances and adding the current word?s ASR-context fea-
tures, length, distance from first word, POS tag, Con-
tent/Function tag as well as the length of the current?s
words surrounding 1 or 2 word contexts, we improve
incorrect F-measure for word mis-recognition predic-
tion by 3.9% . We then employ these feature sets in
1-stage and 2-stage approached to obtain our final re-
sults. The 2-stage (no up-sampling) approach yields the
highest precision for detection of word mis-recognition
at 51% while the 1-stage (with 35% up-sampling) ap-
proach yields the highest recall for detection of word
mis-recognition at 72%.
In order to implement this approach in a working
dialog system we would need to increase our word
mis-recognition precision. The presence of false pos-
itives in mis-recognition prediction (correctly recog-
nized words classified as mis-recognized) could lead
to unnecessary clarification requests ? potentially de-
railing the dialogue.
In future work we will experiment with additional
corpora as well as with an even more fine-grained ap-
proach to local error detection, looking for deletions,
insertions, and substitutions. Potentially, optimal clas-
sifiers could be found for each of these types of mis-
recognition. If we are able to identify the type of ASR
error as well as its location, we should be able to im-
prove our construction of clarifications questions.
We will also continue our investigation of how to use
reprise clarification questions in SDS. Once we have
detected localized ASR errors we must still refine our
strategies for constructing clarification questions using
this information. We are also studying how appropri-
ate and inappropriate reprise clarification questions are
handled by SDS users.
135
References
M. Akbacak, H. Franco, M. Frandsen, S. Hasan,
H. Jameel, A. Kathol, S. Khadivi, X. Lei, A. Man-
dal, S. Mansour, K. Precoda, C. Richey, D. Ver-
gyri, W. Wang, M. Yang, and J. Zheng. 2009. Re-
cent advances in sri?s iraqcomm; iraqi arabic-english
speech-to-speech translation system. In Acoustics,
Speech and Signal Processing, 2009. ICASSP 2009.
IEEE International Conference on, pages 4809?
4812.
P. Boersma and D. Weenink. 2013. Praat: do-
ing phonetics by computer [computer program].
http://www.fon.hum.uva.nl/praat/.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch.
M. O. Dzikovska, C. B. Callaway, E. Farrow, J. D.
Moore, N. Steinhauser, and G. Campbell. 2009.
Dealing with interpretation errors in tutorial dia-
logue. In Proceedings of the SIGDIAL 2009 Con-
ference: The 10th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue, SIGDIAL
?09, pages 38?45, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
K. Toutanova et al 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1.
H. Franco, J. Zheng, J. Butzberger, F. Cesari, M. Fr,
J. Arnold, V. Ramana, A. Stolcke R. Gadde, and
V. Abrash. 2002. Dynaspeak: Sri?s scalable speech
recognizer for embedded and mobile systems. In
Proceedings of the second international conference
on Human Language Technology Research, HLT
?02, pages 25?30, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
J. Hirschberg, D. J. Litman, and M. Swerts. 2004.
Prosodic and other cues to speech recognition fail-
ures. Speech Communication, 43(1-2):155?175.
K. Komatani and H. G. Okuno. 2010. Online error
detection of barge-in utterances by using individual
users utterance histories in spoken dialogue system.
D. J. Litman and S. Silliman. 2004. Itspoke: an
intelligent tutoring spoken dialogue system. In
Demonstration Papers at HLT-NAACL 2004, HLT-
NAACL?Demonstrations ?04, pages 5?8, Strouds-
burg, PA, USA.
J. Lopes, M. Eskenazi, and I. Trancoso. 2011. To-
wards choosing better primes for spoken dialog sys-
tems. In Proceedings of the IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU), Keystone, CO.
M. Purver. 2004. The Theory and Use of Clarification
Requests in Dialogue. Ph.D. thesis, King?s College,
University of London.
A. Rosenberg. 2010. Autobi - a tool for auto-
matic tobi annotation. In Takao Kobayashi, Kei-
kichi Hirose, and Satoshi Nakamura, editors, IN-
TERSPEECH, pages 146?149. ISCA.
S. Stoyanchev, A. Liu, and J. Hirschberg. 2012a. Clar-
ification questions with feedback 2012. In Interdis-
ciplinary Workshop on Feedback Behaviors in Dia-
log.
S. Stoyanchev, P. Salletmayr, J. Yang, and
J. Hirschberg. 2012b. Localized detection of
speech recognition errors. In Spoken Language
Technology Workshop (SLT), 2012 IEEE, pages
25?30.
B. Weiss, C. Schlenoff, G. Sanders, M. Steves, S. Con-
don, J. Phillips, and D. Parvaz. 2008. Perfor-
mance evaluation of speech translation systems. In
Nicoletta Calzolari (Conference Chair) et al, editor,
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC?08),
Marrakech, Morocco, may. European Language
Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
I. Witten and F. Eibe. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
136
Proceedings of the SIGDIAL 2013 Conference, pages 137?141,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Modelling Human Clarification Strategies
Svetlana Stoyanchev, Alex Liu, Julia Hirschberg
Columbia University, New York NY 10027
sstoyanchev, al3037, julia @cs.columbia.edu
Abstract
We model human responses to speech recog-
nition errors from a corpus of human clarifi-
cation strategies. We employ learning tech-
niques to study 1) the decision to either stop
and ask a clarification question or to continue
the dialogue without clarification, and 2) the
decision to ask a targeted clarification question
or a more generic question. Targeted clarifi-
cation questions focus specifically on the part
of an utterance that is misrecognized, in con-
trast with generic requests to ?please repeat?
or ?please rephrase?. Our goal is to generate
targeted clarification strategies for handling er-
rors in spoken dialogue systems, when appro-
priate. Our experiments show that linguis-
tic features, in particular the inferred part-of-
speech of a misrecognized word are predictive
of human clarification decisions. A combina-
tion of linguistic features predicts a user?s de-
cision to continue or stop a dialogue with ac-
curacy of 72.8% over a majority baseline accu-
racy of 59.1%. The same set of features predict
the decision to ask a targeted question with ac-
curacy of 74.6% compared with the majority
baseline of 71.8%.1
1 Introduction
Clarification questions are common in human-human
dialogue. They help dialogue participants main-
tain dialogue flow and resolve misunderstandings.
Purver (2004) finds that in human-human dialogue
speakers most frequently use reprise clarification ques-
tions to resolve recognition errors. Reprise clarification
questions use portions of the misunderstood utterance
which are thought to be correctly recognized to target
the part of an utterance that was misheard or misunder-
stood. In the following example from (Purver, 2004),
Speaker B has failed to hear the word toast and so con-
structs a clarification question using a portion of the
correctly understood utterance ? the word some ? to
query the portion of the utterance B has failed to under-
stand:
1This work was partially funded by DARPA HR0011-12-
C-0016 as a Columbia University subcontract to SRI Interna-
tional.
A: Can I have some toast please?
B: Some?
A: Toast.
Unlike human conversational partners, most di-
alogue systems today employ generic ?please re-
peat/rephrase? questions asking a speaker to repeat or
rephrase an entire utterance. Our goal is to introduce
reprise, or targeted, clarifications into an automatic
spoken system. Targeted clarifications can be espe-
cially useful for systems accepting unrestricted speech,
such as tutoring systems, intelligent agents, and speech
translation systems. Using a reprise question, a user
can correct an error by repeating only a portion of
an utterance. Targeted questions also provide natural
grounding and implicit confirmation by signalling to
the conversation partner which parts of an utterance
have been recognized.
In order to handle a misrecognition, the system must
first identify misrecognized words (Stoyanchev et al,
2012), determine the type of question to ask, and con-
struct the question. In this work, we address two points
necessary for determining the type of question to ask:
? Is it appropriate for a system to ask a clarification
question when a misrecognized word is detected?
? Is it possible to ask a targeted clarification ques-
tion for a given sentence and an error segment?
To answer these questions, we analyze a corpus of hu-
man responses to transcribed utterances with missing
information which we collected using Amazon Me-
chanical Turk (2012). Although the data collection
was text-based, we asked annotators to respond as they
would in a dialogue. In Section 2, we describe related
work on error recovery strategies in dialogue systems.
In Section 3, we describe the corpus used in this exper-
iment. In Section 4, we describe our experiments on
human clarification strategy modelling. We conclude
in Section 5 with our plan for applying our models in
spoken systems.
2 Related work
To handle errors in speech recognition, slot-filling di-
alogue systems typically use simple rejection (?I?m
sorry. I didn?t understand you.?) when they have
low confidence in a recognition hypothesis and ex-
plicit or implicit confirmation when confidence scores
137
are higher. Machine learning approaches have been
successfully employed to determine dialogue strate-
gies (Bohus and Rudnicky, 2005; Bohus et al, 2006;
Rieser and Lemon, 2006), such as when to provide
help, repeat a previous prompt, or move on to the next
prompt. Reiser and Lemon (2006) use machine learn-
ing to determine an optimal clarification strategy in
multimodal dialogue. Komatani et al (2006) propose a
method to generate a help message based on perceived
user expertise. Corpus studies on human clarifications
in dialogue indicate that users ask task-related ques-
tions and provide feedback confirming their hypothesis
instead of giving direct indication of their misunder-
standing (Skantze, 2005; Williams and Young, 2004;
Koulouri and Lauria, 2009). In our work, we model
human strategies with the goal of building a dialogue
system which can generate targeted clarification ques-
tions for recognition errors that require additional user
input but which can also recover from other errors au-
tomatically, as humans do.
3 Data
In our experiments, we use a dataset of human re-
sponses to missing information, which we collected
with Amazon Mechanical Turk (AMT). Each AMT an-
notator was given a set of Automatic Speech Recog-
nition (ASR) transcriptions of an English utterance
with a single misrecognized segment. 925 such utter-
ances were taken from acted dialogues between En-
glish and Arabic speakers conversing through SRI?s
IraqComm speech-to-speech translation system (Akba-
cak et al, 2009). Misrecognized segments were re-
placed by ?XXX? to indicate the missing information,
simulating a dialogue system?s automatic detection of
misrecognized words (Stoyanchev et al, 2012). For
each sentence, AMT workers were asked to 1) indi-
cate whether other information in the sentence made
its meaning clear despite the error, 2) guess the miss-
ing word if possible, 3) guess the missing word?s part-
of-speech (POS) if possible, and 4) create a targeted
clarification question if possible. Three annotators an-
notated each sentence. Table 1 summarizes the results.
In 668 (72%) of the sentences an error segment corre-
sponds to a single word while in 276 (28%) of them, an
error segment corresponds to multiple words. For mul-
tiple word error segments, subjects had the option of
guessing multiple words and POS tags. We scored their
guess correct if any of their guesses matched the syn-
tactic head word of an error segment determined from
an automatically assigned dependency parse structure.
We manually corrected annotators? POS tags if the
hypothesized word was itself correct. After this post-
processing, we see that AMT workers hypothesized
POS correctly in 57.7% of single-word and 60.2% of
multi-word error cases. They guessed words correctly
in 34.9% and 19.3% of single- and multi-word error
cases. They choose to ask a clarification question in
38.3% /47.9% of cases and 76.1%/62.3% of these ques-
tions were targeted clarification questions. These re-
Single-word Agree Multi-word
error error
Total sent 668 (72%) - 276 (28%)
Correct POS 57.7% 62% 60.2%
Correct word 34.9% 25% 19.3%
Ask a question 38.3% 39% 47.9%
Targeted question 76.1% 25% 62.3%
Table 1: Annotation summary for single-word and
multi-word error cases. Absolute annotator agreement
is shown for single-word error cases.
sults indicate that people are often able to guess a POS
tag and sometimes an actual word. We observe that 1)
in a single-word error segment, subjects are better at
guessing an actual word than they are in a multi-word
error segment; and 2) in a multi-word error segment,
subjects are more likely to ask a clarification question
and less likely to ask a targeted question. All three an-
notators agree on POS tags in 62% of cases and on hy-
pothesized words in 25%. Annotators? agreement on
response type is low ? not surprising since there is
more than one appropriate and natural way to respond
in dialogue. In 39% of cases, all three annotators agree
on the decision to stop/continue and in only 25% of
cases all three annotators agree on asking a targeted
clarification question. Figure 1 shows the annotator
Figure 1: Distribution of decisions to ask a question or
continue dialogue without a question.
distribution for asking a clarification question vs. con-
tinuing the dialogue based on hypothesized POS tag. It
indicates that annotators are more likely to ask a ques-
tion than continue without a question when they hy-
pothesize a missing word to be a content word (noun
or adjective) or when they are unsure of the POS of the
missing word. They are more likely to continue when
they believe a missing word is a function word. How-
ever, when they believe a missing word is a verb, they
are more likely to continue, and they are also likely to
identify the missing verb correctly.
Figure 2 shows a distribution of annotator decisions
as to the type of question they would ask. The pro-
portion of targeted question types varies with hypoth-
esized POS. It is more prevalent than confirm and
generic questions combined for all POS tags except
preposition and question word, indicating that annota-
tors are generally able to construct a targeted clarifica-
tion question based on their analysis of the error seg-
ment.
138
Figure 2: Distribution of decisions for targeted, confir-
mation, and generic question types.
4 Experiment
We use our AMT annotations to build classifiers for 1)
choice of action: stop and engage in clarification vs.
continue dialogue; and 2) type of clarification ques-
tion (targeted vs. non-targeted) to ask. For the con-
tinue/stop experiment, we aim to determine whether a
system should stop and ask a clarification question. For
the targeted vs. non-targeted experiment, we aim to de-
termine whether it is possible to ask a targeted clarifi-
cation question.2
Using the Weka (Witten and Eibe, 2005) machine
learning framework, we build classifiers to predict
AMT decisions. We automatically assign POS tags to
transcripts using the Stanford tagger (Toutanova and
others, 2003). We compare models built with an au-
tomatically tagged POS for an error word (POS-auto)
with one built with POS guessed by a user (POS-
guess). Although a dialogue manager may not have
access to a correct POS, it may simulate this by pre-
dicting POS of the error. We assign dependency tags
using the AMU dependency parser (Nasr et al, 2011)
which has been optimized on the Transtac dataset.
We hypothesize that a user?s dialogue move depends
on the syntactic structure of a sentence as well as
on syntactic and semantic information about the er-
ror word and its syntactic parent. To capture sentence
structure, we use features associated with the whole
sentence: POS ngram, all pairs of parent-child depen-
dency tags in a sentence (Dep-pair), and all semantic
roles (Sem-presence) in a sentence. To capture the syn-
tactic and semantic role of a misrecognized word, we
use features associated with this word: POS tag, depen-
dency tag (Dep-tag), POS of the parent word (Parent-
POS), and semantic role of an error word (Sem-role).
We first model individual annotators? decisions for
each of the three annotation instances. We measure
the value that each feature adds to a model, using an-
notators? POS guess (POS-guess). Next, we model a
joint annotators? decision using the automatically as-
signed POS-auto feature. This model simulates a sys-
tem behaviour in a dialogue with a user where a system
chooses a single dialogue move for each situation. We
run 10-fold cross validation using the Weka J48 Deci-
2If any annotators asked a targeted question, we assign a
positive label to this instance, and negative otherwise.
sion Tree algorithm.
Feature Description
Count
Word-position beginning if a misrecognized word is
the first word in the sentence, end if it
is the last word, middle otherwise.
Utterance-length number of words in the sentence
Part-of-speech (compare)
POS-auto POS tag of the misrecognized word au-
tomatically assigned on a transcript
POS-guess POS tag of the misrecognized word
guessed by a user
POS ngrams
POS ngrams all bigrams and trigrams of POS tags in
a sentence
Syntactic Dependency
Dep-tag dependency tag of the misrecognized
word automatically assigned on a tran-
script
Dep-pair dependency tags of all (parent, child)
pairs in the sentence
Parent-POS POS tag of the syntactic parent of the
misrecognized word
Semantic
Sem-role semantic role of the misrecognized
word
Sem-presence all semantic roles present in a sentence
Table 2: Features
4.1 Stop/Continue Experiment
In this experiment, we classify each instance in the
dataset into a binary continue or stop decision. Since
each instance is annotated by three annotators, we first
predict individual annotators? decisions. The absolute
agreement on continue/stop is 39% which means that
61% of sentences are classified into both classes. We
explore the role of each feature in predicting these de-
cisions. All features used in this experiment, except for
the POS-guess feature, are extracted from the sentences
automatically. Variation in the POS-guess feature may
explain some of the difference between annotator deci-
sions.
Features Acc F-measure %Diff
Majority baseline 59.1%
All features 72.8% ? 0.726 0.0%
less utt length 72.9% ? 0.727 +0.1%
less POS ngrams 72.8% ? 0.727 +0.1%
less Semantic 72.6% ? 0.724 -0.3%
less Syn. Depend. 71.5% ? 0.712 -1.9%
less Position 71.2% ? 0.711 -2.0%
less POS 67.9% ? 0.677 -6.7%
POS only 70.1% ? 0.690 -5.0%
Table 3: Stop/Continue experiment predicting individ-
ual annotator?s decision with POS-guess. Accuracy, F-
measure and Difference of f-measure from All feature.
?indicates statistically significant difference from the
majority baseline (p<.01)
Table 3 shows the results of continue/stop classifica-
tion. A majority baseline method predicts the most fre-
quent class continue and has 59.1% accuracy. In com-
parison, our classifier, built with all features, achieves
72.8% accuracy.
139
Next, we evaluate the utility of each feature by re-
moving it from the feature set and comparing the model
built without it with a model built on all features. POS
is the most useful feature, as we expected: when it is
removed from the feature set, the f-measure decreases
by 6.7%. A model trained on the POS-guess feature
alone outperforms a model trained on all other features.
Word position in the sentence is the next most salient
feature, contributing 2% to the f-measure. The syntac-
tic dependency features Syn-Dep, Dep-pair, and Parent
POS together contribute 1.9%.3
Next, we predict a majority decision for each sen-
tence. Table 4 shows the accuracy of this prediction.
A majority baseline has an accuracy of 59.9%. When
we use a model trained on the POS-auto feature alone,
accuracy rises to 66.1%, while a combination of all fea-
tures further increases it to 69.2%.
Features Acc F-measure
Majority baseline 59.9%
POS 66.1% ? 0.655
All features 69.2% ? 0.687
Table 4: Stop/Continue experiment predicting majority
decision, using POS-auto. ?indicates statistically sig-
nificant difference from the majority baseline (p<.01).
4.2 Targeted Clarification Experiment
In this experiment, we classify each instance into tar-
geted or not targeted categories. The targeted category
comprises the cases in which an annotator chooses to
stop and ask a targeted question. We are interested in
identifying these cases in order to determine whether a
system should try to ask a targeted clarification ques-
tion. Table 5 shows the results of this experiment. The
majority baseline predicts not targeted and has a 71.8%
accuracy because in most cases, no question is asked.
A model trained on all features increases accuracy to
74.6%. POS is the most salient feature, contributing
3.8% to the f-measure. All models that use POS fea-
ture are significantly different from the baseline. The
next most salient features are POS ngram and a com-
bination of syntactic dependency features contributing
1% and .5% to the f-measure respectively.
Table 6 shows system performance in predicting a
joint annotators? decision of whether a targeted ques-
tion can be asked. A joint decision in this experiment
is considered not targeted when none of the annotators
chooses to ask a targeted question. We aim at identi-
fying the cases where position of an error word makes
it difficult to ask a clarification question, such as for a
sentence XXX somebody steal these supplies. Using the
automatically assigned POS (POS-auto) feature alone
achieves an accuracy of 62.2%, which is almost 10%
above the baseline. A combination of all features, sur-
prisingly, lowers the accuracy to 59.4%. Interestingly, a
combination of all features less POS increases accuracy
3All trained models are significantly different from the
baseline. None of the trained models are significantly dif-
ferent from each other.
Features Acc F-measure %Diff
Majority baseline 71.8%
All features 74.6% ? 0.734 0.0%
All feature (POS guess)
less Utt length 74.8% ? 0.736 +0.3%
less Position 74.9% ? 0.731 -0.4%
less Semantic 74.8% ? 0.737 +0.4%
less Syn. Depend. 74.2% ? 0.730 -0.5%
less POS ngram 74.2% ? 0.727 -1.0%
less POS 74.0% 0.706 -3.8%
POS 74.1% ? 0.731 -0.4%
Table 5: Targeted/not experiment predicting individ-
ual annotator?s decision with POS-guess. Accuracy, F-
measure and Difference of f-measure from All feature.
?indicates statistically significant difference from the
majority baseline (p<.05)
above the baseline by 7.6% points to 60.1% accuracy.
Features Acc F-measure
Majority baseline 52.5%
POS only 62.2% ? 0.622
All features 59.4% ? 0.594
All features less POS 60.1% ? 0.600
Table 6: Targeted/not experiment predicting majority
decision, using POS tag feature POS-auto. ?indicates
statistically significant difference from the majority
baseline.
5 Conclusions and Future Work
In this paper we have described experiments modelling
human strategies in response to ASR errors. We have
used machine learning techniques on a corpus anno-
tated by AMT workers asked to respond to missing in-
formation in an utterance. Although annotation agree-
ment in this task is low, we aim to learn natural strate-
gies for a dialogue system by combining the judge-
ments of several annotators. In a dialogue, as in other
natural language tasks, there is more than one appro-
priate response in each situation. A user does not judge
the system (or another speaker) by a single response.
Over a dialogue session, appropriateness, or lack of it
in system actions, becomes evident. We have shown
that by using linguistic features we can predict the de-
cision to either ask a clarification question or continue
dialogue with an accuracy of 72.8% in comparison with
the 59.1% baseline. The same linguistic features pre-
dict a targeted clarification question with an accuracy
of 74.6% compared to the baseline of 71.8%.
In future work, we will apply modelling of a clari-
fication choice strategy in a speech-to-speech transla-
tion task. In our related work, we have addressed the
problem of automatic correction of some ASR errors
for cases when humans believe a dialogue can continue
without clarification In other work, we have addressed
the creation of targeted clarification questions for han-
dling the cases when such questions are appropriate.
Combining these research directions, we are develop-
ing a clarification component for a speech-to-speech
translation system that responds naturally to speech
recognition errors.
140
References
M. Akbacak, Franco, H., M. Frandsen, S. Hasan,
H. Jameel, A. Kathol, S. Khadivi, X. Lei, A. Man-
dal, S. Mansour, K. Precoda, C. Richey, D. Vergyri,
W. Wang, M. Yang, and J. Zheng. 2009. Recent ad-
vances in SRI?s IraqCommtm Iraqi Arabic-English
speech-to-speech translation system. In ICASSP,
pages 4809?4812.
Amazon Mechanical Turk. 2012.
http://aws.amazon.com/mturk/, accessed on 28
may, 2012.
D. Bohus and A. I. Rudnicky. 2005. A principled ap-
proach for rejection threshold optimization in spoken
dialog systems. In INTERSPEECH, pages 2781?
2784.
D. Bohus, B. Langner, A. Raux, A. Black, M. Eske-
nazi, and A. Rudnicky. 2006. Online supervised
learning of non-understanding recovery policies. In
Proceedings of SLT.
Y. Fukubayashi, K. Komatani, T. Ogata, and H. Okuno.
2006. Dynamic help generation by estimating user?s
mental model in spoken dialogue systems. In Pro-
ceedings of the International Conference on Spoken
Language Processing (ICSLP).
T. Koulouri and S. Lauria. 2009. Exploring miscom-
munication and collaborative behaviour in human-
robot interaction. In SIGDIAL Conference, pages
111?119.
A. Nasr, F. Be?chet, J.F. Rey, B. Favre, and J. Le Roux.
2011. Macaon: an nlp tool suite for processing
word lattices. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: Systems
Demonstrations, pages 86?91. Association for Com-
putational Linguistics.
M. Purver. 2004. The Theory and Use of Clarification
Requests in Dialogue. Ph.D. thesis, King?s College,
University of London.
V. Rieser and O. Lemon. 2006. Using machine learn-
ing to explore human multimodal clarification strate-
gies. In ACL.
G. Skantze. 2005. Exploring human error recov-
ery strategies: Implications for spoken dialogue sys-
tems. Speech Communication, 45(2-3):325?341.
Svetlana Stoyanchev, Philipp Salletmayr, Jingbo Yang,
and Julia Hirschberg. 2012. Localized detection
of speech recognition errors. In SLT, pages 25?30.
IEEE.
K. Toutanova et al 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1. Association for Computational Linguistics.
J. D. Williams and S. Young. 2004. Characterizing
task-oriented dialog using a simulated ASR channel.
In Proceedings of the ICSLP, Jeju, South Korea.
I. Witten and F. Eibe. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
141
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 6?14,
Baltimore, Maryland, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Documenting Endangered Languages with the WordsEye Linguistics Tool
Morgan Ulinski
?
mulinski@cs.columbia.edu
Anusha Balakrishnan
?
ab3596@columbia.edu
Daniel Bauer
?
bauer@cs.columbia.edu
Bob Coyne
?
coyne@cs.columbia.edu
Julia Hirschberg
?
julia@cs.columbia.edu
Owen Rambow
?
rambow@ccls.columbia.edu
?
Department of Computer Science
?
CCLS
Columbia University
New York, NY, USA
Abstract
In this paper, we describe how field lin-
guists can use the WordsEye Linguistics
Tool (WELT) to study endangered lan-
guages. WELT is a tool under devel-
opment for eliciting endangered language
data and formally documenting a lan-
guage, based on WordsEye (Coyne and
Sproat, 2001), a text-to-scene generation
tool that produces 3D scenes from text in-
put. First, a linguist uses WELT to create
elicitation materials and collect language
data. Next, he or she uses WELT to for-
mally document the language. Finally, the
formal models are used to create a text-
to-scene system that takes input in the en-
dangered language and generates a picture
representing its meaning.
1 Introduction
Although languages have appeared and disap-
peared throughout history, today languages are
facing extinction at an unprecedented pace. Over
40% of the estimated 7,000 languages in the world
are at risk of disappearing. When languages die,
we lose access to an invaluable resource for study-
ing the culture, history, and experience of people
who spoke them (Alliance for Linguistic Diversity,
2013). Efforts to document languages and develop
tools to support these efforts become even more
important with the increasing rate of extinction.
Bird (2009) emphasizes a particular need to make
use of computational linguistics during fieldwork.
To address this issue, we are developing the
WordsEye Linguistics Tool, WELT. In one mode
of operation, we provide field linguists with tools
for building elicitation sessions based on custom
3D scenes. In another, we provide a way to for-
mally document the endangered language. For-
mal hypotheses can be verified using a text-to-
scene system that takes input in the endangered
language, analyzes it based on the formal model,
and generates a picture representing the meaning.
WELT provides important advantages to field
linguists for elicitation over the current practice of
using a set of pre-fabricated static pictures. Using
WELT the linguist can create and modify scenes
in real time, based on informants? responses, cre-
ating follow-up questions and scenes to support
them. Since the pictures WELT supports are 3D
scenes, the viewpoint can easily be changed, al-
lowing exploration of linguistic descriptions based
on different frames of reference, as for elicitations
of spatial descriptions. Finally, since scenes and
objects can easily be added in the field, the lin-
guist can customize the images used for elicitation
to be maximally relevant to the current informants.
Creating a text-to-scene system for an endan-
gered language with WELT also has advantages.
First, WELT allows documentation of the seman-
tics of a language in a formal way. Linguists can
customize the focus of their studies to be as deep
or shallow as they wish; however, we believe that a
major advantage of documenting a language with
WELT is that it enables studies that are much more
precise. The fact that a text-to-scene system is cre-
ated from this documentation will allow linguists
to test the theories they develop with native speak-
ers, making changes to grammars and semantics
in real time. The resulting text-to-scene system
can also be an important tool for language preser-
vation, spreading interest in the language among
younger generations of the community and re-
cruiting new speakers.
In this paper, we discuss the WELT toolkit and
its intended use, with examples from Arrernte and
Nahuatl. In Section 2 we discuss prior work on
field linguistics computational tools. In Section 3
we present an overview of the WELT system. We
describe using WELT for elicitation in Section 4
and describe the tools for language documentation
in Section 5. We conclude in Section 6.
6
2 Related Work
Computational tools for field linguistics fall into
two categories: tools for native speakers to use
directly, without substantial linguist intervention,
and tools for field linguists to use. Tools intended
for native speakers include the PAWS starter kit
(Black and Black, 2009), which uses the answers
to a series of guided questions to produce a draft
of a grammar. Similarly, Bird and Chiang (2012)
describe a simplified workflow and supporting MT
software that lets native speakers produce useable
documentation of their language on their own.
One of the most widely-used toolkits in the lat-
ter category is SIL FieldWorks (SIL FieldWorks,
2014), or specifically, FieldWorks Language Ex-
plorer (FLEx). FLEx includes tools for elicit-
ing and recording lexical information, dictionary
development, interlinearization of texts, analysis
of discourse features, and morphological analy-
sis. An important part of FLEx is its ?linguist-
friendly? morphological parser (Black and Si-
mons, 2006), which uses an underlying model
of morphology familiar to linguists, is fully in-
tegrated into lexicon development and interlin-
ear text analysis, and produces a human-readable
grammar sketch as well as a machine-interpretable
parser. The morphological parser is constructed
?stealthily? in the background, and can help a lin-
guist by predicting glosses for interlinear texts.
Linguist?s Assistant (Beale, 2011) provides a
corpus of semantic representations for linguists to
use as a guide for elicitation. After eliciting the
language data, a linguist writes rules translating
these semantic representations into surface forms.
The result is a description of the language that can
be used to generate text from documents that have
been converted into the semantic representation.
Linguists are encouraged to collect their own elic-
itations and naturally occurring texts and translate
them into the semantic representation.
The LinGO Grammar Matrix (Bender et al.,
2002) facilitates formal modeling of syntax by
generating basic HPSG ?starter grammars? for
languages from the answers to a typological ques-
tionnaire. Extending a grammar beyond the proto-
type, however, does require extensive knowledge
of HPSG, making this tool more feasibly used by
grammar engineers and computational linguists.
For semantics, the most common resource for for-
mal documentation across languages is FrameNet
(Filmore et al., 2003); FrameNets have been de-
veloped for many languages, including Spanish,
Japanese, and Portuguese. However, FrameNet is
also targeted toward computational linguists.
In general, we also lack tools for creating cus-
tom elicitation materials. With WELT, we hope to
fill some of the gaps in the range of available field
linguistics tools. WELT will enable the creation of
custom elicitation material and facilitate the man-
agement sessions with an informant. WELT will
also enable formal documentation of the semantics
of a language without knowledge of specific com-
putational formalisms. This is similar to the way
FLEx allows linguists to create a formal model of
morphology while also documenting the lexicon
of a language and glossing interlinear texts.
3 Overview of WELT Workflow
In this section, we briefly describe the workflow
for using WELT; a visual representation is pro-
vided in Figure 1. Since we are still in the early
stages of our project, this workflow has not been
tested in practice. The tools for scene creation and
elicitation are currently useable, although more
features will be added in the future. The tools for
modeling and documentation are still in develop-
ment; although some functionality has been imple-
mented, we are still testing it with toy grammars.
First, WELT will be used to prepare a set of 3D
scenes to be used to elicit targeted descriptions or
narratives. An important part of this phase will be
the cultural adaptation of the graphical semantics
used in WordsEye, so that scenes will be relevant
to the native speakers a linguist works with. We
will discuss cultural adaptation in more detail in
Section 4.1. Next, the linguist will work with an
informant to generate language data based on pre-
pared 3D scenes. This can be a dynamic process;
as new questions come up, a linguist can easily
modify existing scenes or create new ones. WELT
also automatically syncs recorded audio with open
scenes and provides an interface for the linguist to
write notes, textual descriptions, and glosses. We
will discuss creating scenes and eliciting data with
WELT in Section 4.2. After the elicitation session,
the linguist can use WELT to review the data col-
lected, listen to the audio recorded for each scene,
and revise notes and glosses. The linguist can then
create additional scenes to elicit more data or be-
gin the formal documentation of the language.
Creating a text-to-scene system with WELT re-
quires formal models of the morphology, syntax,
7
Define	 ?Lexicon	 ?
Cultural	 ?
Adapta?on	 ?of	 ?
VigNet	 ?
Create	 ?Scenes	 ? Collect	 ?Data	 ?from	 ?informant	 ?
Clean-??up	 ?notes/
glosses	 ?
Modify	 ?&	 ?add	 ?vigne?es	 ?
Define	 ?syntax	 ?to	 ?seman?cs	 ?rules	 ?
Define	 ?Morphology	 ?
Define	 ?Syntax	 ?
L2	 ?
Lexicon	 ?
L2	 ?Syntax-??
Seman?cs	 ?rules	 ?VigNet	 ?Resources 
Output	 ?&	 ?
Collabora?on	 ? Prepare	 ?L2	 ?scenes	 ?
Verify	 ?with	 ?
informant	 ?
XLE	 ? FieldWorks	 ?Tools WELT	 ?
Figure 1: WELT workflow
and semantics of a language. Since the focus
of WELT is on semantics, the formalisms used
to model morphology and syntax may vary. We
are using FieldWorks to document Nahuatl mor-
phology, XFST (Beesley and Karttunen, 2003) to
model Arrernte morphology, and XLE (Crouch et
al., 2011) to model syntax in the LFG formal-
ism (Kaplan and Bresnan, 1982). We will provide
tools to export WELT descriptions and glosses
into FLEx format and to export the lexicon cre-
ated during documentation into FLEx and XLE.
WELT will provide user interfaces for modeling
the syntax-semantics interface, lexical semantics,
and graphical semantics of a language. We will
discuss these in more detail in Section 5.3.
Once models of morphology, syntax, and se-
mantics are in place (note that these can be work-
ing models, and need not be complete), WELT
puts the components together into a text-to-scene
system that takes input in the endangered language
and uses the formal models to generate pictures.
This system can be used to verify theories with in-
formants and revise grammars. As new questions
arise, WELT can also continue to be used to create
elicitation materials and collect linguistic data.
Finally, we will create a website for WELT so
linguists can share resources such as modified ver-
sions of VigNet, 3D scenes, language data col-
lected, and formal grammars. This will allow
comparison of analyses across languages, as well
as facilitate the documentation of other languages
that are similar linguistically or spoken by cul-
turally similar communities. In addition, sharing
the resulting text-to-scene systems with a wider
audience can generate interest in endangered lan-
guages and, if shared with endangered-language-
speaking communities, encourage younger mem-
bers of the community to use the language.
4 Elicitation with WELT
WELT organizes elicitation sessions around a set
of 3D scenes, which are created by inputting En-
glish text into WordsEye. Scenes can be imported
and exported between sessions, so that useful
scenes can be reused and data compared. WELT
also provides tools for recording audio (which is
automatically synced with open scenes), textual
descriptions, glosses, and notes during a session.
Screenshots are included in Figure 2.
4.1 Cultural Adaptation of VigNet
To interpret input text, WordsEye uses VigNet
(Coyne et al., 2011), a lexical resource based on
FrameNet (Baker et al., 1998). As in FrameNet,
lexical items are grouped in frames according to
shared semantic structure. A frame contains a set
of frame elements (semantic roles). FrameNet de-
fines the mapping between syntax and semantics
for a lexical item with valence patterns that map
syntactic functions to frame elements.
VigNet extends FrameNet in order to capture
?graphical semantics?, a set of graphical con-
straints representing the position, orientation, size,
color, texture, and poses of objects in the scene,
8
Figure 2: Screenshots of WELT elicitation interfaces
which is used to construct and render a 3D
scene. Graphical semantics are added to frames by
adding primitive graphical (typically, spatial) rela-
tions between frame element fillers. VigNet distin-
guishes between meanings of words that are dis-
tinguished graphically. For example, the specific
objects (e.g., implements) and spatial relations in
the graphical semantics for cook depend on the
object being cooked and on the culture in which
it is being cooked (cooking turkey in Baltimore
vs. cooking an egg in Alice Springs), even though
at an abstract level cook an egg in Alice Springs
and cook a turkey in Baltimore are perfectly com-
positional semantically. Frames augmented with
graphical semantics are called vignettes.
Vignette Tailoring: Without digressing into a dis-
cussion on linguistic relativity, we assume that
large parts of VigNet are language- and culture-
independent. The low-level graphical relations
used to express graphical semantics are based on
physics and human anatomy and do not depend on
language. However, the graphical semantics for a
vignette may be culture-specific, and some new vi-
gnettes will need to be added for a culture. In the
U.S., for example, the sentence The woman boiled
the water might invoke a scene with a pot of wa-
ter on a stove in a kitchen. Among the Arrernte
people, it would instead invoke a woman sitting
on the ground in front of a kettle on a campfire.
Figure 3 shows an illustration from the Eastern
and Central Arrernte Picture Dictionary (Broad,
2008) of the sentence Ipmenhe-ipmenhele kwatye
urinpe-ilemele iteme, ?My grandmother is boiling
the water.? The lexical semantics for the English
verb boil and the Arrente verb urinpe-ileme are
the same, the relation APPLY-HEAT.BOIL. How-
ever, the vignettes map to different, culture-typical
graphical semantics. The vignettes for our exam-
ple are shown in Figure 4.
Figure 3: Illustration from Broad (2008).
To handle cultural differences like these, a lin-
guist will use WELT to extend VigNet with new
9
Figure 4: Vignettes for the woman boils the water.
The high-level semantics of APPLY-HEAT.BOIL
are decomposed into sets of objects and primitive
graphical relations that depend on cultural context.
graphical semantics for existing vignettes that
need to be modified, and new vignettes for scenar-
ios not already covered. We will create interfaces
so that VigNet can easily be adapted.
Custom WordsEye Objects: Another way to
adapt WordsEye to a culture or region is to add rel-
evant 3D objects to the database. WordsEye also
supports 2D-cutout images, which is an easy way
to add new material without 3D modeling. We
have created a corpus of 2D and 3D models for
WordsEye that are specifically relevant to aborig-
inal speakers of Arrernte, including native Aus-
tralian plants and animals and culturally relevant
objects and gestures. Many of the pictures we cre-
ated are based on images from IAD Press, used
with permission, which we enhanced and cropped
in PhotoShop. Some scenes that use these images
are included in Figure 5. Currently, each new ob-
ject has to be manually incorporated into Words-
Eye, but we will create tools to allow WELT users
to easily add pictures and objects.
New objects will also need to be incorporated
into the semantic ontology. VigNet?s ontology
consists of semantic concepts that are linked to-
gether with ISA relations. The ontology supports
multiple inheritance, allowing a given concept to
be a sub-type of more than one concept. For exam-
ple, a PRINCESS.N is a subtype of both FEMALE.N
and ARISTOCRAT.N, and a BLACK-WIDOW.N is a
subtype of SPIDER.N and POISONOUS-ENTITY.N.
Concepts are often linked to corresponding lexi-
cal items. If a lexical item has more than one
word sense, the different word senses would be
represented by different concepts. In addition, ev-
ery graphical object in VigNet is represented by
a unique concept. For example, a particular 3D
model of a dog would be a linked to the general
DOG.N concept by the ISA relation. The semantic
concepts in VigNet include the graphical objects
available in WordsEye as well as concepts tied to
related lexical items. While WordsEye might only
have a handful of graphical objects for dogs, Vi-
gNet will have concepts representing all common
types of dogs, even if there is no graphical object
associated with them. We will provide interfaces
both for adding new objects and for modifying the
semantic concepts in VigNet to reflect the differ-
ing lexical semantics of a new language.
4.2 Preparing Scenes and Eliciting Data
The next step in the workflow is the preparation
of scenes and elicitation of descriptions. To test
creating elicitation materials with WELT, we built
a set of scenes based on the Max Planck topolog-
ical relations picture series (Bowerman and Ped-
erson, 1992). In creating these, we used a feature
of WordsEye that allows highlighting specific ob-
jects (or parts of objects) in a scene. We used these
scenes to elicit descriptions from a native Nahuatl
speaker; some examples are included in Figure 6.
(a) in tapamet? t?atsakwa se kali
the fence/wall around the house
(b) in tsopelik katsekotok t?atsint?a in t?apetS
the candy sticking under the table
Figure 6: Nahuatl examples elicited with WELT
One topic we will explore with WELT is the re-
lationship in Arrernte between case and semantic
interpretation of a sentence. It is possible to signif-
icantly alter a sentence?s meaning by changing the
case on an argument. For example, the sentences
in (1) from Wilkins (1989) show that adding dative
10
Figure 5: WordsEye scenes using custom 2D gum tree and dingo from our corpus
case to the direct object of the sentence changes
the meaning from shooting and hitting the kanga-
roo to shooting at the kangaroo and not hitting it.
Wilkins calls this the ?dative of attempt.?
(1) a. re aherre tyerre-ke
he kangaroo shot-pc
He shot the kangaroo.
b. re aherre-ke tyerre-ke
he kangaroo-DAT shot-pc
He shot at the kangaroo (but missed).
In order to see how this example generalizes,
we will create pairs of pictures, one in which the
object of the sentence is acted upon, and one in
which the object fails to be acted upon. Figure 7
shows a pair of scenes contrasting an Australian
football player scoring a goal with a player aiming
at the goal but missing the shot. Sentences (2) and
(3) are two ways of saying ?score a goal? in Ar-
rernte; we want to see if a native Arrernte speaker
would use goal-ke in place of goal in this context.
(2) artwe le goal arrerne-me
man ERG goal put-NP
The man kicks a goal.
(3) artwe le goal kick-eme-ile-ke
man ERG goal kick-VF-TV-PST
The man kicked a goal.
5 Modeling a Language with WELT
WELT includes tools for documenting the seman-
tics of the language. It also uses this documenta-
tion to automatically generate a text-to-scene sys-
tem for the language. Because WELT is centered
around the idea of 3D scenes, the formal docu-
mentation will tend to focus on the parts of the se-
mantics that can be represented graphically. Note
that this can include figurative concepts as well,
although the visual representation of these may be
culture-specific. However, linguists do not need
to be limited by the graphical output; WELT can
be used to document other aspects of semantics as
well, but linguists will not be able to verify these
theories using the text-to-scene system.
To explain the necessary documentation, we
briefly describe the underlying architecture of
WordsEye, and how we are adapting it to sup-
port text-to-scene systems for other languages.
The WordsEye system parses each input sentence
into a labeled syntactic dependency structure, then
converts it into a lexical-semantic structure using
lexical valence patterns and other lexical and se-
mantic information. The resulting set of seman-
tic relations is converted to a ?graphical seman-
tics?, the knowledge needed to generate graphical
scenes from language.
To produce a text-to-scene system for a new lan-
guage, WELT must replace the English linguistic
processing modules with models for the new lan-
guage. The WELT processing pipeline is illus-
trated in Figure 8, with stages of the pipeline on
top and required resources below. In this section,
we will discuss creating the lexicon, morphologi-
cal and syntactic parsers, and syntax-to-semantics
rules. The vignettes and 3D objects will largely
have been done during cultural adaptation of Vi-
gNet; additional modifications needed to handle
the semantics can be defined using the same tools.
5.1 The Lexicon
The lexicon in WELT is a list of word forms
mapped to semantic concepts. The process of
building the lexicon begins during elicitation.
WELT?s elicitation interface includes an option to
display each object in the scene individually be-
fore progressing to the full scene. When an object
is labeled and glossed in this way, the word and
the semantic concept represented by the 3D ob-
ject are immediately added to the lexicon. Word
forms glossed in scene descriptions will also be
added to the lexicon, but will need to be mapped
to semantic concepts later. WELT will provide
11
Figure 7: WordsEye scenes to elicit the ?dative of attempt.?
Morph	 ? Lexical	 ?Seman?cs	 ?
Graphical	 ?
Seman?cs	 ? Scene	 ?
Input	 ?
Text	 ?
Processing	 ?Pipeline	 ?
VigNet	 ?
Vigne?s	 ? 2D/3D	 ?objects	 ?Lexicon	 ?
Syntax	 ?
Morphological	 ?
Analyzer	 ?
Syntac?c	 ?
Parser	 ?
Syntax-??Seman?cs	 ?
Rules	 ?
Figure 8: WELT architecture
tools for completing the lexicon by modifying
the automatically-added items, adding new lexical
items, and mapping each lexical item to a seman-
tic concept in VigNet. Figure 9(a) shows a partial
mapping of the nouns in our Arrernte lexicon.
WELT includes a visual interface for search-
ing VigNet?s ontology for semantic concepts and
browsing through the hierarchy to select a partic-
ular category. Figure 9(b) shows a portion of the
ontology that results from searching for cup. Here,
we have decided to map panikane to CUP.N. Se-
mantic categories are displayed one level at a time,
so initially only the concepts directly above and
below the search term are shown. From there, it is
simple to click on relevant concepts and navigate
the graph to find an appropriate semantic category.
To facilitate the modeling of morphology and syn-
tax, WELT will also export the lexicon into for-
mats compatible with FieldWorks and XLE, so the
list of word forms can be used as a starting point.
5.2 Morphology and Syntax
As mentioned earlier, the focus of our work on
WELT is on modeling the interface between syn-
tax, lexical semantics, and graphical semantics.
Therefore, although WELT requires models of
morphology and syntax to generate a text-to-scene
system, we are relying on third-party tools to build
those models. For morphology, a very good tool
already exists in FLEx, which allows the creation
Lexical VigNet
Item Concept
artwe PERSON.N
panikane CUP.N
angepe CROW.N
akngwelye DOG.N
tipwele TABLE.N
(a) (b)
Figure 9: (a) Arrernte lexical items mapped to Vi-
gNet concepts; (b) part of the VigNet ontology
of a morphological parser without knowledge of
any particular grammatical formalism. For syn-
tax, we are using XLE for our own work while
researching other options that would be more ac-
cessible to non-computational linguists. It is im-
portant to note, though, that the modeling done in
WELT does not require a perfect syntactic parser.
In fact, one can vastly over-generate syntax and
still accurately model semantics. Therefore, the
syntactic grammars provided as models do not
need to be complex. However, the question of syn-
tax is still an open area of research in our project.
5.3 Semantics
To use the WordsEye architecture, the system
needs to be able to map between the formal syntax
of the endangered language and a representation of
semantics compatible with VigNet. To accomplish
12
Figure 10: Creating syntax-semantics rules in WELT
this, WELT includes an interface for the linguist to
specify a set of rules that map from syntax to (lex-
ical) semantics. Since we are modeling Arrernte
syntax with LFG, the rules currently take syntactic
f-structures as input, but the system could easily be
modified to accommodate other formalisms. The
left-hand side of a rule consists of a set of con-
ditions on the f-structure elements and the right-
hand side is the desired semantic structure. Rules
are specified by defining a tree structure for the
left-hand (syntax) side and a DAG for the right-
hand (semantics) side.
As an example, we will construct a rule to
process sentence (2) from Section 4.2, artwe le
goal arrerneme. For this sentence, our Arrernte
grammar produces the f-structure in Figure 11.
We create a rule that selects for predicate ar-
rerne with object goal and any subject. Figure
10 shows the construction of this rule in WELT.
Note that var-1 on the left-hand side becomes
VIGNET(var-1) on the right-hand side; this in-
dicates that the lexical item found in the input is
mapped into a semantic concept using the lexicon.
Figure 11: F-structure for sentence 2, Section 4.2.
The rule shown in Figure 10 is a very sim-
ple example. Nodes on the left-hand side of
the rule can also contain boolean logic, if we
wanted to allow the subject to be [(artwe ?man? OR
arhele ?woman?) AND NOT ampe ?child?]. Rules
need not specify lexical items directly but may
refer to more general semantic categories. For
example, our rule could require a particular se-
mantic category for VIGNET(var-1), such as
ANIMATE-BEING.N. These categories are chosen
through the same ontology browser used to cre-
ate the lexicon. Finally, to ensure that our sen-
tence can be converted into graphics, we need
to make sure that a vignette definition exists for
CAUSE MOTION.KICK so that the lexical seman-
tics on the right-hand side of our rule can be aug-
mented with graphical semantics; the vignette def-
inition is given in Figure 12. The WordsEye sys-
tem will use the graphical constraints in the vi-
gnette to build a scene and render it in 3D.
Figure 12: Vignette definition for
CAUSE MOTION.KICK
6 Summary
We have described a novel tool under develop-
ment for linguists working with endangered lan-
guages. It will provide a new way to elicit data
from informants, an interface for formally docu-
menting the lexical semantics of a language, and
allow the creation of a text-to-scene system for any
language. In this paper, we have focused specifi-
cally on the workflow that a linguist would fol-
low while studying an endangered language with
WELT. WELT will provide useful tools for field
linguistics and language documentation, from cre-
ating elicitation materials, to eliciting data, to for-
mally documenting a language. In addition, the
text-to-scene system that results from document-
ing an endangered language with WELT will be
valuable for language preservation, generating in-
terest in the wider world, as well as encouraging
younger members of endangered language com-
munities to use the language.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
1160700.
13
References
Alliance for Linguistic Diversity. 2013. The En-
dangered Languages Project. http://www.
endangeredlanguages.com.
C. Baker, J. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet project. In 36th Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics
(COLING-ACL?98), pages 86?90, Montr?eal.
Stephen Beale. 2011. Using Linguist?s Assistant for
Language Description and Translation. In IJCNLP
2011 System Demonstrations, pages 5?8.
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite-
State Morphology Homepage. http://www.
fsmbook.com.
E. Bender, D. Flickinger, and S. Oepen. 2002. The
Grammar Matrix. In J. Carroll, N. Oostdijk, and
R. Sutcliffe, editors, Workshop on Grammar En-
gineering and Evaluation at the 19th International
Conference on Computational Linguistics, pages 8?
14, Taipei, Taiwan.
S. Bird and D. Chiang. 2012. Machine translation for
language preservation. In COLING 2012: Posters,
pages 125?134, Mumbai, December.
S. Bird. 2009. Natural language processing and
linguistic fieldwork. Computational Linguistics,
35(3):469?474.
Cheryl A Black and H Andrew Black. 2009. PAWS:
Parser and Writer for Syntax. In SIL Forum for Lan-
guage Fieldwork 2009-002.
H.A. Black and G.F. Simons. 2006. The SIL Field-
Works Language Explorer approach to morphologi-
cal parsing. In Computational Linguistics for Less-
studied Languages: Texas Linguistics Society 10,
Austin, TX, November.
M. Bowerman and E. Pederson. 1992. Topological re-
lations picture series. In S. Levinson, editor, Space
stimuli kit 1.2, page 51, Nijmegen. Max Planck In-
stitute for Psycholinguistics.
N. Broad. 2008. Eastern and Central Arrernte Picture
Dictionary. IAD Press.
B. Coyne and R. Sproat. 2001. WordsEye: An au-
tomatic text-to-scene conversion system. In SIG-
GRAPH.
B. Coyne, D. Bauer, and O. Rambow. 2011. Vignet:
Grounding language in graphics using frame seman-
tics. In ACL Workshop on Relational Models of Se-
mantics (RELMS), Portland, OR.
D. Crouch, M. Dalrymple, R. Kaplan, T. King,
J. Maxwell, and P. Newman. 2011. XLE Doc-
umentation. http://www2.parc.com/isl/
groups/nltt/xle/doc/xle_toc.html.
C. Filmore, C. Johnson, and M. Petruck. 2003. Back-
ground to FrameNet. In International Journal of
Lexicography, pages 235?250.
R.M. Kaplan and J.W. Bresnan. 1982. Lexical-
functional grammar: A formal system for grammat-
ical representation. In J.W. Bresnan, editor, The
Mental Representation of Grammatical Relations.
MIT Press, Cambridge, Mass., December.
SIL FieldWorks. 2014. SIL FieldWorks. http://
fieldworks.sil.org.
D. Wilkins. 1989. Mparntwe Arrernte (Aranda): Stud-
ies in the structure and semantics of grammar. Ph.D.
thesis, Australian National University.
14
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 62?72,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Overview for the First Shared Task on
Language Identification in Code-Switched Data
Thamar Solorio
Dept. of Computer Science
University of Houston
Houston, TX, 77004
solorio@cs.uh.edu
Elizabeth Blair, Suraj Maharjan, Steven Bethard
Dept. of Computer and Information Sciences
University of Alabama at Birmingham
Birmingham, AL, 35294
{eablair,suraj,bethard}@uab.edu
Mona Diab, Mahmoud Gohneim, Abdelati Hawwari, Fahad AlGhamdi
Dept. of Computer Science
George Washington University
Washington, DC 20052
{mtdiab,mghoneim,abhawwari,fghamdi}@gwu.edu
Julia Hirschberg and Alison Chang
Dept. of Computer Science
Columbia University
New York, NY 10027
julia@cs.columbia.edu
ayc2135@columbia.edu
Pascale Fung
Dept. of Electronic & Computer Engineering
Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong
pascale@ece.ust.hk
Abstract
We present an overview of the first shared
task on language identification on code-
switched data. The shared task in-
cluded code-switched data from four lan-
guage pairs: Modern Standard Arabic-
Dialectal Arabic (MSA-DA), Mandarin-
English (MAN-EN), Nepali-English (NEP-
EN), and Spanish-English (SPA-EN). A to-
tal of seven teams participated in the task
and submitted 42 system runs. The evalua-
tion showed that language identification at
the token level is more difficult when the
languages present are closely related, as in
the case of MSA-DA, where the prediction
performance was the lowest among all lan-
guage pairs. In contrast, the language pairs
with the higest F-measure where SPA-EN
and NEP-EN. The task made evident that
language identification in code-switched
data is still far from solved and warrants
further research.
1 Introduction
The main goal of this language identification shared
task is to increase awareness of the outstanding
challenges in the automated processing of Code-
Switched (CS) data and motivate more research in
this direction. We define CS broadly as a commu-
nication act, whether spoken or written, where two
or more languages are being used interchangeably.
In its spoken form, CS has probably been around
ever since different languages first came in contact.
Linguists have studied this phenomenon since the
mid 1900s. In contrast, the Natural Language Pro-
cessing (NLP) community has only recently started
to pay attention to CS, with the earliest work in
this area dating back to Joshi?s theoretical work
proposing an approach to parsing CS data (Joshi,
1982) based on the Matrix and Embedded language
framework. With the wide-spread use of social me-
dia, CS is now being used more and more in written
language and thus we are seeing an increase in pub-
lished papers dealing with CS. We are specifically
interested in intrasentential code switched phenom-
ena. As a result of this task, we have successfully
created the first set of annotated data for several
language pairs with a coherent set of labels across
the languages. As the shared task results show,
CS poses new research questions that warrant new
NLP approaches, and thus we expect to see a sig-
nificant increase in NLP work in the coming years
addressing CS phenomena in data.
The shared task covers four language pairs and
is focused on social media data. We provided par-
ticipants with annotated data from Twitter for the
62
language pairs: Modern Standard Arabic-Arabic
dialects (MSA-DA), Mandarin-English (MAN-
EN), NEP-EN (NEP-EN), and SPA-EN (SPA-EN).
These language pairs represent a good variety in
terms of language typology and relatedness among
pairs. They also cover languages with different rep-
resentation in terms of number of speakers world
wide. Participants were asked to make predictions
on unseen Twitter data for each language pair. We
also provided participants with test data from a
?surprise genre? with the objective of assessing the
robustness of language identification systems to
genre variation.
2 Task Description
The task consists of labeling each token/word in
the input file with one of six labels: lang1, lang2,
other, ambiguous, mixed, and named entities NE.
The lang1, lang2 labels refer to the two languages
addressed in the subtask, for example for the lan-
guage pair MSA-DA, lang1 would be an MSA and
lang2 is DA. The other category is a label used to
tag all punctuation marks, emoticons, numbers, and
similar tokens that do not represent actual words in
any of the given languages. The ambiguous label
is for instances where it is not possible to assign
a language with certainty, for example, a lexical
form that belongs to both languages, appearing in a
context that does not indicate one language over the
other. The mixed category is for words composed
of CS morphemes, such as the word snapchateando
?to chat? from SPA-EN, the word overai from NEP-
EN, or the word hayqwlwn
1
?they will say?, from
MSA-DA, where the ?ha? is a DA future morpheme
and the stem ?yqwlwn? is MSA.The NE label is
included in this task in an effort to allow for a more
focused analysis of CS data with the exclusion of
proper nouns. NEs have a very different behavior
than most other words in a language vocabulary
and thus from our perspective they need to be iden-
tified to be handled properly.
Table 1 shows Twitter examples taken from the
training data. The annotation guidelines are posted
on the workshop website
2
. We post the ones used
for SPA-EN as for the other language pairs the only
differences are the examples provided.
1
We use Buckwalter transliteration scheme http://
www.qamus.org/transliteration.htm
2
http://emnlp2014.org/workshops/
CodeSwitch/call.html
Language Pair Example
MSA-DA AlnhArdp AlsAEp 11 hAkwn Dyf >.
HAfZ AlmyrAzy ElY qnAp drym llHdyv
En >wlwyAt Alvwrp fy AlmrHlp Al-
HAlyp wqDyp tSHyH msAr Alvwrp
Al<ElAmy
(Today O?Clock 11 I will be
[a ]guest[ of] Mr. Hafez
AlMirazi on Channel Dream
to talk about [the ]priorities[ of]
the revolution in the stage the current
and [the ]issue[ of] correcting
[the ]path[ of] the revolution Media)
NEP-EN My car at the workshop for a much
needed repairs... ABA pocket khali
hune bho
(My car at the workshop for a much
needed repairs. . . now my pocket will
be empty)
SPA-EN Por primera vez veo a @username ac-
tually being hateful! it was beautiful:)
(For the first time I get to see @user-
name actually being hateful! it was
beautiful:)
Table 1: Examples of Twitter data used in the
shared task.
3 Related Work
In the past, most language identification research
has been done at the document level. Some re-
searchers, however, have developed methods to
identify languages within multilingual documents
(Singh and Gorla, 2007; Nguyen and Do?gru?oz,
2013; King and Abney, 2013). Their test data
comes from a variety of sources, including web
pages, bilingual forum posts, and jumbled data
from monolingual sources, but none of them are
trained on code-switched data, opting instead for a
monolingual training set per language. This could
prove to be a problem when working on code-
switched data, particularly in shorter samples such
as social media data, as the code-switching context
is not present in training material.
One system tackled both the problems of code-
switching and social media in language and code-
switched status identification (Lignos and Marcus,
2013). Lignos and Marcus gathered millions of
monolingual tweets in both English and Spanish in
order to model the two languages, and used crowd-
sourcing to annotate tens of thousands of Span-
ish tweets, approximately 11% of which contained
code-switched content. This system was able to
achieve 96.9% word-level accuracy and a 0.936
F-measure in identifying code-switched tweets.
The issue still stands that relatively little code-
switching data, such as that used in Lignos and
63
Marcus? research, is readily available. Even in
their data, the percentage of code-switched tweets
was barely over a tenth of the total test data. There
have been other corpora built, particularly for other
language pairs such as Mandarin-English (Li et
al., 2012; Lyu et al., 2010), but the amount of data
available and the percentage of code-switching data
within that data are not up to the standards of other
areas of the natural language processing field. With
this in mind, we sought to provide corpora for mul-
tiple language pairs, each with a better distribution
of code-switching phenomena.
4 Data Sets
Most of the data for the shared task comes form
Twitter. However, we also collected and annotated
data from other social media sources, including
Facebook, web forums, and blogs. These additional
sources of data were used as the surprise data. In
this section we describe briefly the corpora curated
for the shared task.
Language-pair Training Test Surprise
MAN-EN 1000 313 n/a
MSA-DA 5,838 2332, 1,777 12,017
NEP-EN 9,993 3,018 (2,874) 1,087
SPA-EN 11,400 3,060 (1,626) 1,102
Table 2: Statistics of the shared task data sets
per language pairs. The numbers are according to
what was actually annotated, numbers in parenthe-
sis show what the participating systems were able
to crawl from Twitter. The Surprise genre comes
from various sources, other than Twitter.
Table 2 shows some statistics about the differ-
ent datasets used in this task. We strive to provide
dataset sizes that would allow a robust analysis of
results. However, an unexpected challenge was
the rate at which tweets became unavailable. Dif-
ferent language pairs had different attrition rates
with SPA-EN being the most affected language and
MSA-DA and NEP-EN the least affected. Note
that we provided two test datasets for MSA-DA.
Since we separated the data on a per user basis, the
first test set had a highly skewed distribution. The
second test set was distributed to participants to
allow a comparison with a data set having a class
distribution more similar to the training set.
4.1 SPA-EN data
Developing the corpus involved two primary steps:
locating code-switching tweets and using crowd-
sourcing to annotate their tokens with language
tags. A small portion of the tweets were annotated
in-lab and this was used as the gold data for quality
control in the crowdsourcing annotation.
To avoid biasing the data used in this task, we
used a two step process to select the tweets: first we
identified CS tweets by doing a keyword search on
Twitter?s API. We selected a few frequently used
English words and restricted the search to tweets
identified by Twitter as Spanish from users in Cali-
fornia and Texas. An additional set of tweets was
then collected by using frequent Spanish words in
an all English tweet, from users in the same loca-
tions. We filtered these tweets to remove tweets
containing URLs, duplicates, spam tweets and
retweets.
In-lab annotators labeled the filtered tweets using
the guidelines referenced above. From this set of
labeled data we then ranked the users in this set by
the percentage of CS tweets. We selected the 12
most prolific CS users and then pulled all of their
available tweets. These 12 users contributed the
tweets used in the shared task. The tweets were
labeled using CrowdFlower
3
. After analyzing the
number and content distribution of the tweets, the
SPA-EN data was split into a 11,400 tweet training
set and a 3,014 tweet test set.
The SPA-EN Surprise Genre (SPA-EN-SG) in-
cluded Facebook comments from the Veteranas
community
4
and the Chicanas community
5
and
blog data from the Albino Bean
6
. Data was col-
lected using Python scripts that implemented the
Beautiful Soup library and the third-party Python
Facebook SDK (for Blogger and Facebook respec-
tively). Post and comment IDs were used to iden-
tify Facebook posts, and URLs were used to iden-
tify Blogger posts. The collected posts were format-
ted to match those collected from Twitter. In-lab
annotators were used to annotate approximately 1K
tokens. All the data we collected in this manner
was released as surprise data to all participants.
4.2 NEP-EN data
The collection of NEP-EN data followed a simi-
lar approach to that of SPA-EN. We first focused
on finding users that switched frequently between
3
http://www.crowdflower.com/
4
https://www.facebook.com/
VeteranaPinup
5
https://www.facebook.com/pages/
Chicanas/444483772293893
6
http://thealbinobean.blogspot.com/
64
Nepali and English. In addition, the users must
not be using Devnagari script as done by Nepalese
to write Nepali, but must have used its Roman-
ized form. We started by manually reading tweets
from some of our Nepali friends. We then crawled
their followers who corresponded with them using
code-switched tweets or replies. We found that
a lot of these users were regular code-switchers
themselves. We repeated the same process with the
followers and collected nearly 30 such users. We
then collected about 2,000 tweets each from these
users using the Twitter API. We filtered out all the
retweets and the tweets with URLs, following the
same process that was used for SPA-EN.
For the surprise test data, we crawled code-
switched data from Facebook comments and posts.
We found that most Nepalese comments had a rich
amount of code-switched data. However, we could
not crawl their data because of privacy issues. Nev-
ertheless, we could crawl data from public Face-
book pages. We identified some public Nepali Face-
book pages where anyone could comment. These
pages include FM, news and public figures? public
Facebook pages. We crawled the latest 10 feeds
from these public pages using the Facebook API
and gathered about 12,000 comments and posts for
the shared task.
Initially, we sought out help from Nepali gradu-
ate students at the University of Alabama at Birm-
ingham to annotate 100 tweets (1739 tokens). We
gave the same annotation file to two annotators to
do the annotation. We found that they agreed with
an accuracy of 95.34%. These tweets were then re-
viewed and used as initial gold data in Crowdflower
to annotate the first 1000 tweets. The annotation
job was enabled only in Nepal and Bhutan. We
disabled India, even though people living in some
regions of India (Darjeeling, Sikkim) also speak
and write in Nepali, as most spammers were com-
ing from India. We then ran two batches of 5000
tweets and one batch of 3000 tweets along with the
initial 1,000 tweets as the gold data. This NEP-EN
data was then split into a 9,993 tweet training set
and a 2,874 tweet test set. No Twitter user appeared
in both sets.
4.3 MAN-EN data
The MAN-EN tweets were collected from Twitter
with the Twitter API. Users were selected from
lists of most followed Twitter accounts in Taiwan
(where Mandarin Chinese is the official language).
These users? tweets were checked for Mandarin En-
glish bilingualism and added to our data collection
if they contained both languages.
The next round of usernames came from the
lists of users that our original top accounts were
following. The tweets written by this new set of
users were then examined for Mandarin English
code switching and stored as data if they matched
the criteria.
The jieba tokenizer
7
was used to segment the
Mandarin sections of the tweets and compute off-
sets of each segment. We format the code switch-
ing tweets into columns including language type,
labels, and offsets. Named entities were labeled
manually by a single annotator.
The data was split by user into 1000 tweets for
training and 313 for testing. No MAN-EN surprise
data for the current shared task.
4.4 MSA-DA data
For the MSA-DA language pair, we selected Egyp-
tian Arabic (EGY) as the Arabic dialect. We har-
vested data from two social media sources: Twitter
[TWT] and Blog commentaries [COM]. The TWT
data served as the main gold standard data for the
task where we provided fully annotated data for
Training/Tuning and Test. We provided two TWT
data sets for the test data that exemplified different
tag distributions. The COM data set comprised
only test data and it served as the Arabic surprise
data set.
To reduce the potential of TWT data attrition
from users deleting their accounts or tweets, we
selected tweets that are less prone to deletion and/or
change. Thereby we harvested tweets by a select
set of Egyptian Public Figures. The percentage
of deleted tweets and deactivated accounts among
those users is significantly lower if we compare it
to the tweets crawled from random Egyptian users.
We used the ?Tweepy? library to crawl the time-
lines of 12 Public Figures. Similar to other lan-
guage pairs, we excluded all re-tweets, tweets with
URLs, tweets mentioning other users, and tweets
containing Latin characters. We accepted 9,947
tweets, for each we extracted the tweet-id and user-
id. Using these IDs, we retrieved the tweets text,
tokenized it and assigned character offsets. To guar-
antee consistency and avoid any misalignment is-
sues, we compiled the full pipeline into the ?Arabic
Tweets Token Assigner? package which is made
7
https://github.com/fxsjy/jieba
65
available through the workshop website
8
.
For COM, we selected 6723 commentaries (half
MSA and half DA) from ?youm7?
9
commen-
taries provided by the Arabic Online Commentary
Dataset (Zaidan and Callison-Burch, 2011). The
COM data set was processed (12017 total tokens)
using the same pipeline created for the task. We
also provided the participants with the data format-
ted with character offsets to maintain consistency
across data sets in the Arabic subtask.
The annotation of MSA-DA language pair data
is based on two sets of guidelines. The first set
is a generic set of guidelines for code switching
in general across different language pairs. These
guidelines provide the overarching framework for
annotating code switched data on the morpholog-
ical, lexical, syntactic, and pragmatic levels. The
second set of guidelines is language pair specific.
We created the guidelines for the Arabic language
specifically. We enlisted the help of 3 annotators
in addition to a super annotator, hence resulting
in 4 annotators overall for the whole collection of
the data. All the annotators are native speakers
of Egyptian Arabic with excellent proficiency in
MSA. The super annotator only annotated 10% of
the overall data and served as the adjudicator. The
annotation process was iterative with several repe-
titions of the cycle of training, annotation, revision,
adjudication until we approached a stable Inter An-
notator Agreement (IAA) of over 90% pairwise
agreement.
5 Survey of Shared Task Systems
We received submissions from seven different
teams. Each participating system had the freedom
to submit responses to any of the language pairs
covered in the shared task. All seven participants
submitted system responses for SPA-EN, making
this language pair the most popular in this shared
task and MAN-EN the least popular.
All but one participating system used a machine
learning algorithm or language models, or even a
combination of both, as part of their configuration.
A couple of the participating systems used hand-
crafted rules of some sort, either at the intermediate
steps or as the final post-processing step. We also
observed a good number of systems using exter-
nal resources, in the form of labeled monolingual
8
http://emnlp2014.org/workshops/
CodeSwitch/call.html
9
An Egyptian newspaper, www.youm7.com
corpora, language specific gazetteers, off the shelf
tools (NE recognizers, language id systems, or mor-
phological analyzers) and even unsupervised data
crawled from the same users present in the data
sets provided. Affixes were also used in some form
by different systems.
The architecture of the different systems ranged
from a simple approach based on frequencies of
character n-grams combined in a rule-based system,
to more complex approaches using word embed-
dings, extended Markov Models, and CRF autoen-
coders. The majority of the systems that partici-
pated in more than one language pair did little to no
customization to account for the morphological dif-
ferences of the specific language pairs beyond lan-
guage specific parameter-tuning, which probably
reflects participants? goal to develop a multilingual
id system.
Due to the presence of the NE label, several
systems included a component for NE recognition
where there was one available for the specific lan-
guage. In addition, many systems also included
case information. One unexpected finding from
the shared task was that no participating system
tried to embed in their models some form of lin-
guistic theory or framework about CS. Only one
system made an explicit reference to CS theories
(Chittaranjan et al., 2014) in their motivation to use
contextual information, which can be considered
as a loose embedding of CS theory. While system
performance was competitive (see next section),
there is still room for improvement and perhaps
some of that improvement can come out of adding
this kind of knowledge into the models. Lastly, we
were surprised to see that not all systems made use
of character encoding information, even though for
Mandarin-English that would have been a strong
indicator. In Table 3 we present a summary high-
lighting some of the design choices of participating
systems.
6 Results
We used the following evaluation metrics: Accu-
racy, Precision, Recall, and F-measure. We use
F-measure to provide a ranking of systems. In the
evaluation at the tweet level we use the standard
f-measure. For the evaluation at the token level
we use instead the average weighted f-measure to
account for the highly imbalanced distribution of
classes.
To provide a fair evaluation, we only scored pre-
66
System
Machine
Learning
Rules Case
Character
Encoding
External Resources LM Affixes Context
(Chittaranjan et al., 2014)
CRF
4
4 dbpedia dumps, online sources
? 3
(Shrestha, 2014) 4
4 spell checker
(Jain and Bhat, 2014)
CRF
4
4 English dictionary
4 4 ? 2
(King et al., 2014)
eMM
ANERgazet, TwitterNLP, Stan-
ford NER
4 4 4
(Bar and Dershowitz, 2014)
SVM
4
Illocution Twitter Lexicon,
monolingual corpora (NE lists)
4 4 ? 2
(Lin et al., 2014)
CRF
4
4
Hindi-Nepali Wikipedia, JRC,
CoNLL 2003 shared task, lang
id predictors: cld2 and ldig
4 4
(Barman et al., 2014)
kNN, SVM
4 4
BNC, LexNorm
4 ? 1
Table 3: Comparison of shared task participating system algorithm choices. CRF stands for Conditional
Random Fields, SVM for Support Vector Machines and LM for Language Models.
dictions on tweets submitted by all teams. All
systems were compared to a simple lexicon-based
baseline. The lexicon was gathered from the train-
ing data for classes lang1 and lang2 only. Emoti-
cons, punctuation marks, usernames and URLs are
by default tagged as other. In the case of a tie or a
new token, the baseline system assigns the majority
class for that language pair.
Figure 1 shows prediction performance on the
Twitter test data for each language pair at the tweet
level. The system predictions for this task are taken
directly from the individual token predictions in
the following manner: if the system predictions for
the same tweet contain at least one tag from each
language (lang1 and lang2), the tweet is labeled
as code-switched, otherwise it is labeled as mono-
lingual. As illustrated, each language pair shows
different patterns. Comparing the systems that par-
ticipated in all language pairs, there is no clear
winner across the board. However, (Chittaranjan et
al., 2014) was in the top three places in at least one
test file for each language pair. Table 4 shows the
results at the token level by label. Here again the
figures show F-measure per class label and the last
column is the weighted average f-measure (Avg-F).
One of the few general trends on these results is
that most participating systems were not able to
correctly identify the minority classes ?ambiguous?
and ?other?. There are only few instances of these
labels in the training set and some test sets did not
have one of these classes present. The impact on
final system performance from these classes is not
significant. However, to study CS patterns we will
need to have these labels identified properly.
The MAN-EN pair received four system re-
sponses and all four of them reached an F-measure
>80% and outperformed the simple baseline by a
considerable margin. We expected this language
pair to be the easiest one for the shared task since
each language uses a different encoding script. A
very rough but accurate distinction between Man-
darin and English could be achieved by looking
at the character encoding. However, according to
the system descriptions provided, not all systems
used encoding information. The best performing
systems for MAN-EN are (King et al., 2014) and
(Chittaranjan et al., 2014). The former slightly
outperformed the latter at the Tweet level (see Fig-
ure 1a) task while the opposite was true at the token
level (see Table 4 rows 4 and 5).
In the case of SPA-EN, all seven systems out-
performed the simple baseline. The best perform-
ing system in all SPA-EN tasks was (Bar and
Dershowitz, 2014). This system achieved an F-
measure of 82.2%, 2.9 percentage points above the
second best system (Lin et al., 2014) on the tweet
level task (see Figure 1(d)). In the token level
evaluation, (Bar and Dershowitz, 2014) reached
an Avg. F-measure of 94%. This top performing
system uses a sequential classification approach
where the labels from the preceding words are used
as features in the model. Another design choice
that might have given the edge to this system is the
fact that their model combines character- and word-
based language models in what the authors call
?intra- and inter-word level? features. Both types
of language models are trained on large amounts
of monolingual data and NE lists, which again pro-
vides additional knowledge that other systems are
not exploiting. For instance, the NE lexicons might
account for the best results in the NE class in both
the Twitter data and the Surprise genre (see Table 4
last row for SPA-EN and second to last for SPA-
EN Surprise). Most systems showed considerable
67
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Chittaranjan
et al., 2014)
(King et
al., 2014)
0.6
0.7
0.8
0.9
1
0.838
0.888
0.892
0.894
F
-
m
e
a
s
u
r
e
Baseline
(a) MAN-EN
(Chittaranjan
et al., 2014)
(King et
al., 2014)
(Jain and
Bhat, 2014)
(Elfardy et
al., 2014)
(Lin et
al., 2014)
0.1
0.2
0.3
0.4
0.152
0.118
0.048
0.044
0.095
0.196
0.260
0.338
0.360
0.417
F
-
m
e
a
s
u
r
e
Baseline Test1
Baseline Test2
(b) MSA-DA. Dark gray bars show performance on Test1 and light gray bars show performance for Test2
(King et
al., 2014)
(Lin et
al., 2014)
(Jain and
Bhat, 2014)
(Shrestha,
2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
0.8
0.9
1
0.952
0.962
0.972
0.974
0.975
0.977
F
-
m
e
a
s
u
r
e
Baseline
(c) NEP-EN
(Shrestha,
2014)
(King et
al., 2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Bar and
Dershowitz,
2014)
0.6
0.7
0.8
0.9
1
0.634
0.703
0.753
0.754
0.783
0.793
0.822
F
-
m
e
a
s
u
r
e
Baseline
(d) SPA-EN
Figure 1: Prediction results on language identification at the tweet level. This is a binary task to distinguish
between a monolingual and a CS tweet. We show performance of participating systems using F-measure
as the evaluation metric. The solid line shows the lexicon baseline performance.
differences in prediction performance in both gen-
res. In all cases the Avg. F-measure was higher
on the Twitter test data than on the surprise genre.
Although the surprise genre is too small to draw
strong conclusions, all language pairs with surprise
genre test data showed a decrease in performance
of around 10%.
We analyzed system outputs and found some
consistent sources of error. Lexical forms that exist
in both languages were frequently mislabeled by
68
most systems. For example the word for ?he? was
frequently mislabeled by at least one system. In
most of the cases systems were predicting EN as
label when the target language was SPA. Cases like
this were even more prone to errors when these
words fell in the CS point, as in this tweet: ni el
header he hecho (I haven?t even done the header).
Tweets like this one, with just one token from the
other language, were difficult for most systems.
Named entities were also frequent sources of error,
especially when they were spelled with lower cases
letters.
By far the hardest language pair in this shared
task was MSA-DA, as anticipated. Especially when
considering the typological similarities between
MSA and DA. This is mainly due to the fact that
DA and MSA are close variants of one another and
hence they share considerable amount of lexical
items. The shared lexical items could be simple
cognates of one another, or faux amis where they
are homographs or homophones, but have com-
pletely different meaning. Both categories con-
stitute a significant challenge. Accordingly, the
baseline system had the lowest performance from
all language pairs in both test sets. We note chal-
lenges in this language pair on each linguistic level
where CS occurs especially for the shared lexical
items.
On the phonological level, DA writers tend to
mimic the MSA script for DA words even if they
are pronounced differently. For example: ?heart? is
pronounced in DA Alob and in MSA as qalob but
commonly written in MSA as ?qalob? in DA data.
Also many phonological differences are in short
vowels that are underspecified in written Arabic,
adding another layer of ambiguity.
On the morphological level, there is no avail-
able morphological analyzer able to recognize such
shared words and hence they are mostly misclassi-
fied. Language identification for MSA-DA CS text
highly depends on the context. Typically some Ara-
bic variety word serves as a marker for a context
switch such as mElh$ for DA or mn? for MSA. But
if shared lexical items are used, it is challenging
to identify the Arabic variant. An example from
the training data is qlb meaning either heart as a
noun or change as a verb in the phrase lw qlb mjrm,
corresponding to ?If the heart of a criminal? or ?if
he changes into a criminal?. These challenges ren-
der language identification for CS MSA-DA data
far from solved as evident by the fact that the high-
est scoring system reached an F-measure of only
41.7% in Test2 for CS identification. Moreover,
this is the only language pair where at least one
system was not able to outperform the baseline and
in the case of Test2 only one system (Lin et al.,
2014) outperformed the baseline.
Most teams did well for the NEP-EN shared task,
and all teams outperformed the baseline. The rea-
son for the high performance might be the high
number of codeswitched tweets in the training and
test data for NEP-EN (much higher than other lan-
guage pairs). This allowed systems to have more
samples of CS instances. The other reason for good
performance by most participants in both evalua-
tions might be that Nepali and English are two very
different languages. The structure of the words and
syntax of word formation are very different. We
suspect, for instance, that there is a much lower
overlap of character n-grams in this language pair
than in SPA-EN, which makes for an easier task. At
the Tweet level, system performance ranged over
a small set of values, the lowest F-measure was
95.2% while the highest was 97.7%. Looking at
the numbers in Table 4, we can see that even NE
recognition seemed to be a much easier task for this
language pair than for SPA-EN (compare results
for the NE category in both SPA-EN sets to those
of both NEP-EN data sets). The best performing
system for the Twitter test data is (Barman et al.,
2014) with an F-measure of 97.7%. The results
trend in the surprise genre is not consistent with
what we observed for the Twitter test data. The
top ranked system for Twitter sunk to the 4th place
with an F-measure or 59.6%, a considerable drop
of almost 40 percentage points. In this case, the
overall numbers indicate a much wider difference
in the genres than what we observed for other lan-
guages, such as SPA-EN, for example. We should
note that the class distribution in the surprise data is
considerably different from what the models used
for training, and from that of the test data as well.
In the Twitter data there was a larger number of CS
tweets than monolingual ones, while in the surprise
genre the majority class was monolingual. This
will account for a good portion of the differences
in performance. But here as well, the small number
of labeled instances makes it hard to draw strong
conclusions.
69
Test Set System lang1 lang2 NE other ambiguous mixed Avg-F
MAN-EN
Baseline 0.9 0.47 0 0.29 - 0 0.761
(Jain and Bhat, 2014) 0.97 0.66 0.52 0.33 - 0 0.871
(Lin et al., 2014) 0.98 0.73 0.62 0.34 - 0 0.886
(King et al., 2014) 0.98 0.74 0.58 0.30 - 0 0.884
(Chittaranjan et al., 2014) 0.98 0.76 0.66 0.34 - 0 0.892
MSA-DA Test 1
(King et al., 2014) 0.88 0.14 0.05 0 0 - 0.720
Baseline 0.92 0.06 0 0.89 0 - 0.819
(Chittaranjan et al., 2014) 0.94 0.15 0.57 0.91 0 - 0.898
(Jain and Bhat, 2014) 0.93 0.05 0.73 0.87 0 - 0.909
(Lin et al., 2014) 0.94 0.09 0.74 0.98 0 - 0.922
(Elfardy et al., 2014)* 0.94 0.05 0.85 0.99 0 - 0.936
MSA-DA Test 2
Baseline 0.54 0.27 0 0.94 0 0 0.385
(King et al., 2014) 0.59 0.59 0.13 0.01 0 0 0.477
(Chittaranjan et al., 2014) 0.58 0.50 0.42 0.43 0.01 0 0.513
(Jain and Bhat, 2014) 0.62 0.49 0.67 0.75 0 0 0.580
(Elfardy et al., 2014)* 0.73 0.73 0.91 0.98 0 0.01 0.777
(Lin et al., 2014) 0.76 0.81 0.73 0.98 0 0 0.799
MSA-DA Surprise
(King et al., 2014) 0.48 0.60 0.05 0.02 0 0 0.467
(Jain and Bhat, 2014) 0.53 0.61 0.62 0.96 0 0 0.626
(Chittaranjan et al., 2014) 0.56 0.69 0.33 0.96 0 0 0.654
(Lin et al., 2014) 0.68 0.82 0.61 0.97 0 0 0.778
(Elfardy et al., 2014)* 0.66 0.81 0.87 0.99 0 0 0.801
NEP-EN
Baseline 0.67 0.76 0 0.61 - 0 0.678
(King et al., 2014) 0.87 0.80 0.51 0.34 - 0.03 0.707
(Lin et al., 2014) 0.93 0.91 0.49 0.95 - 0.02 0.917
(Jain and Bhat, 2014) 0.94 0.96 0.52 0.94 - 0 0.942
(Shrestha, 2014) 0.94 0.96 0.57 0.95 - 0 0.944
(Chittaranjan et al., 2014) 0.94 0.96 0.45 0.97 - 0 0.948
(Barman et al., 2014) 0.96 0.97 0.58 0.97 - 0.06 0.959
NEP-EN Surprise
(Lin et al., 2014) 0.83 0.73 0.46 0.65 - - 0.712
(King et al., 2014) 0.82 0.88 0.43 0.12 - - 0.761
(Chittaranjan et al., 2014) 0.78 0.87 0.37 0.80 - - 0.796
(Jain and Bhat, 2014) 0.83 0.91 0.50 0.87 - - 0.850
(Barman et al., 2014) 0.87 0.90 0.61 0.74 - - 0.853
(Shrestha, 2014) 0.85 0.92 0.53 0.78 - - 0.855
SPA-EN
Baseline 0.72 0.56 0 0.75 0 0 0.704
(Shrestha, 2014) 0.88 0.85 0.35 0.92 0 0 0.873
(Jain and Bhat, 2014) 0.92 0.92 0.36 0.90 0 0 0.905
(Lin et al., 2014) 0.93 0.93 0.32 0.91 0.03 0 0.913
(Barman et al., 2014) 0.93 0.92 0.47 0.93 0.03 0 0.921
(King et al., 2014) 0.94 0.93 0.54 0.92 0 0 0.923
(Chittaranjan et al., 2014) 0.94 0.93 0.28 0.95 0 0 0.926
(Bar and Dershowitz, 2014) 0.95 0.95 0.56 0.94 0.04 0 0.940
SPA-EN Surprise
(Shrestha, 2014) 0.80 0.78 0.23 0.81 0 0 0.778
(Jain and Bhat, 2014) 0.83 0.84 0.22 0.79 0 0 0.811
(Lin et al., 2014) 0.83 0.86 0.19 0.80 0.03 0 0.816
(Barman et al., 2014) 0.84 0.85 0.31 0.82 0.03 0 0.823
(Chittaranjan et al., 2014) 0.94 0.86 0.14 0.83 0 0 0.824
(King et al., 2014) 0.84 0.85 0.35 0.81 0 0 0.828
(Bar and Dershowitz, 2014) 0.85 0.87 0.37 0.83 0.03 0 0.839
Table 4: Performance results on language identification at the token level. A ?-? indicates there were no
tokens of this class in the test set. We ranked systems using weighted averaged f-measure (Avg-F). The ?*?
marks the system by (Elfardy et al., 2014). This system was not considered in the ranking for the shared
task as it was developed by co-organizers of the task.
7 Lessons Learned
Among the things we want to improve for future
shared tasks is the issue of data loss due to re-
moval of tweets or users deleting their accounts.
We decided to use Twitter data to have a relevant
corpus. However, the trade-off is the lack of rights
to distribute the data ourselves. This is not just a
burden for the participants. It is an awful waste of
resources as the data that was expensive to gather
and label is not being used beyond the small group
of researchers involved in the creation of the cor-
pus. This will deter us from using Twitter data for
future shared tasks, at least until a better solution
is identified.
70
(Chittaranjan
et al., 2014)
(King et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Elfardy et
al., 2014)
0
0.1
0.2
0.3
0.170
0.194
0.222
0.276
0.277
F
-
m
e
a
s
u
r
e
(a) MSA-DA Surprise Genre Results
(Chittaranjan
et al., 2014)
(Jain and
Bhat, 2014)
(Barman et
al., 2014)
(King et
al., 2014)
(Shrestha,
2014)
(Lin et
al., 2014)
0.4
0.5
0.6
0.7
0.554
0.571
0.596
0.604
0.632
0.702
F
-
m
e
a
s
u
r
e
(b) NEP-EN Surprise Genre Results
(Shrestha,
2014)
(King et
al., 2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Bar and
Dershowitz,
2014)
0.4
0.5
0.6
0.7
0.8
0.633
0.640
0.704
0.710
0.725
0.727
0.753
F
-
m
e
a
s
u
r
e
(c) SPA-EN Surprise Genre Results
Figure 2: Prediction results on language identification at the document level for the surprise genre. This
is a binary task to distinguish between a monolingual and a code-switched text. We show performance of
participating systems using F-measure as the evaluation metric.
Using crowdsourcing for annotating the data is a
cheap and easy way for generating resources. But
we found out that even when following best prac-
tices for quality control, there was a substantial
amount of noise in the gold data. We plan to con-
tinue working on refining the annotation guidelines
and quality control processes to reduce the amount
of noise in gold annotations.
8 Conclusion
This is the first shared task on language identifica-
tion in CS data. Yet, the response was quite positive
as we received 42 system runs from seven different
teams, plus submissions for MSA-AD from a sub
group of the task organizers (Elfardy et al., 2014).
The systems presented are overall robust and with
interesting differences from one another. Although
we did not see a single system ranking in the top
places across all language pairs and tasks, we did
see systems showing robust performance indicat-
ing some level of language independence. But the
results are not consistent at the tweet/document
level. The language pair that proved to be the most
difficult for the task was MSA-DA, where the lexi-
con baseline system was hard to beat even with an
F-measure of 47.1%.
This shared task showed that language identifica-
tion in code-switched data is still an open problem
that warrants further investigation. Perhaps in the
near future we will see systems that embed some
form of linguistic theory about CS and maybe that
would result in more accurate predictions.
Our goal is to support new research addressing
CS data. Discussions about the challenge for the
next shared task are already underway. One pos-
sibility might be parsing. We plan to investigate
the challenges in parsing CS data and we will start
by exploring the hardships in manually annotating
CS with syntactic information. We would also like
to explore the possibility of classifying CS points
according to their socio-pragmatic role.
71
Acknowledgments
We would like to thank all shared task partici-
pants. We also thank Brian Hester and Mohamed
Elbadrashiny for their invaluable support in the
development of the gold standard data and analy-
sis of results. We also thank the in-lab annotators
and the CrowdFlower contributors. This work was
partly funded by NSF under awards 1205475 and
1205556.
References
Kfir Bar and Nachum Dershowitz. 2014. Tel Aviv Uni-
versity system description for the code-switching
workshop shared task. In Proceedings of the First
Workshop on Computational Approaches to Code-
Switching, Doha, Qatar, October. ACL.
Utsab Barman, Joachim Wagner, Grzegorz Chrupala,
and Jennifer Foster. 2014. DCU-UVT: Word-
level language classification with code-mixed data.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code-Switching, Doha, Qatar,
October. ACL.
Gokul Chittaranjan, Yogarshi Vyas, Kalika Bali, and
Monojit Choudhury. 2014. A framework to label
code-mixed sentences in social media. In Proceed-
ings of the First Workshop on Computational Ap-
proaches to Code-Switching, Doha, Qatar, October.
ACL.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2014. AIDA: Identifying code switching in
informal Arabic text. In Proceedings of the First
Workshop on Computational Approaches to Code-
Switching, Doha, Qatar, October. ACL.
Naman Jain and Riyaz Ahmad Bhat. 2014. Language
identification in codeswitching scenario. In Pro-
ceedings of the First Workshop on Computational
Approaches to Code-Switching, Doha, Qatar, Octo-
ber. ACL.
A. Joshi. 1982. Processing of sentences with in-
trasentential code-switching. In J?an Horeck?y, editor,
COLING-82, pages 145?150, Prague, July.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110?
1119, Atlanta, Georgia, June. Association for Com-
putational Linguistics.
Levi King, Eric Baucom, Tim Gilmanov, Sandra
K?ubler, Dan Whyatt, Wolfgang Maier, and Paul Ro-
drigues. 2014. The IUCL+ system: Word-level
language identification via extended Markov models.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code-Switching, Doha, Qatar,
October. ACL.
Ying Li, Yue Yu, and Pascale Fung. 2012. A
Mandarin-English code-switching corpus. In Nico-
letta Calzolari, Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 2515?2519, Istanbul, Turkey, May. European
Language Resources Association (ELRA). ACL An-
thology Identifier: L12-1573.
Constantine Lignos and Mitch Marcus. 2013. Toward
web-scale analysis of codeswitching. In 87th An-
nual Meeting of the Linguistic Society of America.
Chu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori
Levin. 2014. The CMU submission for the shared
task on language identification in code-switched
data. In Proceedings of the First Workshop on Com-
putational Approaches to Code-Switching, Doha,
Qatar, October. ACL.
D.C. Lyu, T.P. Tan, E. Chng, and H. Li. 2010. SEAME:
a Mandarin-English code-switching speech corpus
in South-East Asia. In INTERSPEECH, volume 10,
pages 1986?1989.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word level
language identification in online multilingual com-
munication. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-
ing, pages 857?862, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Prajwol Shrestha. 2014. An incremental approach for
language identification in codeswitched text. In Pro-
ceedings of the First Workshop on Computational
Approaches to Code-Switching, Doha, Qatar, Octo-
ber. ACL.
Anil Kumar Singh and Jagadeesh Gorla. 2007. Identifi-
cation of languages and encodings in a multilingual
document. In Proceedings of ACL-SIGWAC?s Web
As Corpus3, Belgium.
Omar F. Zaidan and Chris Callison-Burch. 2011. The
Arabic online commentary dataset: An annotated
dataset of informal Arabic with high dialectal con-
tent. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: Short Papers - Volume
2, HLT ?11, pages 37?41, Stroudsburg, PA, USA.
Association for Computational Linguistics.
72
Proceedings of the SIGDIAL 2014 Conference, pages 238?242,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Detecting Inappropriate Clarification Requests in Spoken Dialogue
Systems
Alex Liu
1
, Rose Sloan
2
, Mei-Vern Then
1
, Svetlana Stoyanchev
3
,
Julia Hirschberg
1
, Elizabeth Shriberg
4
Columbia University
1
, Yale University
2
, AT&T Labs Research
3
, SRI International
4
{al3037@columbia.edu, rose.sloan@yale.edu,
mt2837@columbia.edu, sveta@research.att.com,
julia@cs.columbia.edu, elizabeth.shriberg@sri.com}
Abstract
Spoken Dialogue Systems ask for clarifi-
cation when they think they have misun-
derstood users. Such requests may dif-
fer depending on the information the sys-
tem believes it needs to clarify. However,
when the error type or location is misiden-
tified, clarification requests appear confus-
ing or inappropriate. We describe a clas-
sifier that identifies inappropriate requests,
trained on features extracted from user re-
sponses in laboratory studies. This classi-
fier achieves 88.5% accuracy and .885 F-
measure in detecting such requests.
1 Introduction
When Spoken Dialogue Systems (SDS) believe
they have not understood a user, they generate re-
quests for clarification. For example, in the fol-
lowing exchange, the System believes it has mis-
understood the word Washington in the user?s ut-
terance and asks a clarification question, prompt-
ing the user to repeat the misrecognized word.
User: I?d like a ticket to Washington.
System: A ticket to where?
User: Washington.
Clarification requests may be generic or specific
to the type and location of the information the sys-
tem believes it has not recognized. Targeted clar-
ifications focus on a specific part of an utterance,
as in the system?s question above. They use under-
stood portions of an utterance (?I?d like a ticket
to?) to query a misunderstood portion (?Wash-
ington?). Targeted clarification is a type of task-
related request, which has been shown to be more
effective and prevalent in human-human dialogues
than more general clarification requests (Skantze,
2005). Such generic clarifications signal mis-
understanding without identifying the type or lo-
cation of the misunderstanding. They often take
the form of a request to repeat or rephrase, e.g.
?please repeat?, ?please rephrase?, ?what did
you say??.
Questions that address a particular type of mis-
recognition come in several varieties. Systems
may ask reprise clarification questions, by repeat-
ing a recognized portion of an utterance (Ginzburg
and Cooper, 2004; Purver, 2004). Systems may
also request that users spell a word if they be-
lieve the misrecognized word is a proper name,
especially one that is not in its vocabulary (OOV).
They may ask the user to provide a synonym for
OOV terms that are not proper names. Systems
may also ask users to disambiguate homophones
(e.g. ?Did you mean ?right? as in correct or ?rite? as
in a ritual??). They may request confirmation ex-
plicitly (e.g. ?I heard you say Washington. Is that
correct??), or implicitly, by repeating the recog-
nized information while asking a follow-up query
(e.g. ?When do you want to go to Washington??).
Each request type may be appropriate in different
circumstances. However, when systems make in-
appropriate requests to users, such as to rephrase
a proper name or to confirm a statement that con-
tains a misrecognized word, dialogues often go
awry. Therefore, it is extremely important for sys-
tems to know when a request is inappropriate, so
that they can provide a different clarification re-
quest or fall back to a more generic strategy.
In this work, we develop a data-driven method
for detecting inappropriate clarification requests.
We have defined a list of inappropriate request
types and have collected a corpus of speaker re-
sponses to both appropriate and inappropriate re-
quests under laboratory conditions. We use this
corpus to train an inappropriate clarification clas-
sifier to be used by a system after a user responds
to a system request, in order to determine whether
the question was appropriate or not. In Section 2,
we describe previous research on error handling in
dialogue. We describe our data set in Section 3 and
238
our approach in Section 4. We present our evalua-
tion results in Section 5. We conclude in Section 6
and discuss future directions.
2 Related Work
Today?s SDS use generic approaches to clarifica-
tion, asking the user to repeat or rephrase an en-
tire utterance when the system believes it has not
been understood correctly. They use confidence
scores on the ASR hypothesis to decide whether
to accept, reject, or ask for clarification (Bohus
and Rudnicky, 2005). Hypotheses with low scores
may be confirmed and those with lower scores will
trigger a generic request for repetition or rephras-
ing. Researchers have found that the formulation
of system prompts has a significant effect on the
success of SDS interaction. Goldberg et al. (2003)
find that form of a clarification question affects
user frustration and the consequent success of clar-
ification subdialogue. In previous work, we ex-
plored the use of targeted reprise clarifications to
improve naturalness (Stoyanchev et al., 2014).
Lendvai et al. (2002) apply machine learning
methods to detect errors in human-machine di-
alogue, focusing on predicting when a user ut-
terance causes a misunderstanding. Litman et
al. (2006) identify user corrections of the system?s
recognition errors from speech prosody, ASR con-
fidence scores, and the dialogue history. In con-
trast, we focus here on detecting when a system
clarification request is the cause of dialogue prob-
lems. We employ only lexical features here, as
well as the type of system request, to investigate
user responses to a wide variety of system re-
quests, and to identify system errors in request for-
mulation from user reactions. In future work we
will include acoustic and prosodic features as well.
3 Data
Our data consists of spoken answers to clarifica-
tion requests collected at Columbia University us-
ing a simulated dialogue system in order to control
recognition results and type of system response.
The system displays a sentence and asks the user
to read it. The system then issues a pre-prepared
clarification request, which may be appropriate or
inappropriate, to which the user responds. For ex-
ample, in the following exchange, the system sim-
ulates a misunderstanding of the word furor by
asking a targeted reprise clarification question.
User: We hope this won?t create a furor.
System: Create a what?
User: A furor, an uproar.
The system issued six different types of clari-
fication requests: confirmation; rephrase, spell, or
disambiguate part of the utterance; targeted reprise
clarification; and a targeted-reprise-rephrase com-
bination. These request types were chosen based
on the types of requests made by the SRI Thunder-
BOLT speech-to-speech translation system (Ayan
and others, 2013). Confirmation questions sim-
ply ask the user to confirm an ASR hypothesis.
Rephrase-part requests ask users to rephrase a spe-
cific part of an utterance which is played back
to the user. Spell questions ask users to spell a
word or phrase using the NATO alphabet. Disam-
biguate questions clarify ambiguous terms. Tar-
geted reprise clarification questions make use of
the recognized portion of an utterance to query the
part that has been misrecognized based on the sys-
tem?s assessment. Targeted-reprise-rephrase re-
quests are similar, with the additional request for
the user to rephrase a portion of the utterance
believed to have been misrecognized, which is
played to the user.
Inappropriate requests in this study were de-
fined as those that resulted from the Thunder-
BOLT system?s incorrect identification of an er-
ror segment or an error type. For example, the
clarification request ?Please say a different word
for Afdhal? is inappropriate since it asks for a
rephrasal of a proper name. A request to spell
a very long phrase is also identified as inappro-
priate since users have found this difficult, espe-
cially when using the NATO alphabet. Requests
to disambiguate in the system provide two possi-
ble senses of the ambiguous word and are inap-
propriate when the correct sense is not one of the
two provided. Targeted reprise clarification ques-
tions are inappropriate when the error segment is
not correctly recognized and an errorful segment
is included in the question (e.g. ?The okay I zoo
would like what??). An appropriate question cor-
rectly identifies the error segment or ambiguous
term and the error type. For example, the ques-
tion ?I think ?Afdhal? is a name. Please spell it?,
would be appropriate when ?Afdhal? is OOV be-
cause it correctly targets the error and its type.
For each clarification request type, except for
confirmation questions, which are always appro-
priate, we created one or more types of inappro-
priate requests for each of the conditions we ob-
239
served in dialogues collected with the Thunder-
BOLT system. For example, when the system
asks the user to rephrase a part of their utter-
ance which the system believes to be a misrecog-
nized non-proper-name, the question is appropri-
ate when indeed that non-proper-name has been
misrecognized. However, the request will be in-
appropriate when the hypothesized error segment
played back to the user is a partial word, a proper
name, an extended segment including a name, or
a function word. We created instances of each
of these conditions for our users to respond to in
our experiment. A full list of the system question
types and their appropriate and inappropriate con-
ditions is provided in Table 3, in the Appendix.
We prepared 228 clarification requests (84 appro-
priate and 144 inappropriate), 12 for each of the 19
categories listed in Table 3 in the Appendix, based
on data in the TRANSTAC dataset (Akbacak and
others, 2009). Our subjects were 17 native Ameri-
can English speakers, each of whom answered 114
requests. We recorded speakers? answers to 714
appropriate and 1224 inappropriate requests. As
most request types have more than one inappro-
priate version, 63% of the requests in the data set
are inappropriate.
4 Experiment
We used the Weka machine learning library (Wit-
ten and Eibe, 2005) to train classifiers to predict
whether a clarification request was appropriate or
inappropriate. Our features were extracted from
transcripts of user utterances, and included lexical,
syntactic, numeric, and features from the output of
Linguistic Inquiry and Word Count (LIWC) (Pen-
nebaker et al., 2007) as described in Table 1.
We included unigram and bigram features, ex-
cluding unigrams that appeared fewer than 3 times
in the dataset (11% of the unigrams), and bi-
grams that appeared fewer than 2 times (25%),
with thresholds set empirically. LIWC features
were extracted using the LIWC 2007 software,
which includes lexical categories, such as articles
and negations, and psychological constructs, such
as affect and cognition. In one version of the
corpus, we replaced sequences of user spellings
with the tag ?SPELL? and disfluencies with the
symbol ?DISF?. We used the Stanford POS tag-
ger (Toutanova and others, 2003) to tag both
the original corpus as well as the modified ver-
sion. In the latter, we replaced the ?SPELL? and
Feature Description
word unigrams
(Lexical)
Count of unigrams
word bigrams
(Lexical)
Count of bigrams
pos bigrams
(Syntactic)
Bigrams of POS assigned by Stanford
tagger
liwc LIWC Output
func ratio Proportion of function words in re-
sponse
len spell Total length of spelling sequences in
response
request type Type of request preceding response
Table 1: Features used in Classification.
?DISF? tags with the symbols themselves. We
also mapped nine of the most frequent unigrams
to their own POS classes, such as ?no?, ?not?,
and ?neither? to ?NO? and ?word? to ?WORD?.
We then used counts of POS bigrams as a syn-
tactic feature. Additionally, as we observed that
responses to inappropriate requests contained a
higher proportion of function words, we added this
as a numeric feature. We also observed that aver-
age length of responses to inappropriate requests
was greater than responses to appropriate ones,
and we hypothesized this was in part due to in-
appropriate requests to spell long phrases. There-
fore, we also used the length of the total spelling
sequences, or the count of letters spelled out, as a
numeric feature. We also added type of clarifica-
tion request as a feature since some requests are
less likely to be inappropriate than others. For ex-
ample, we consider confirmation questions (?Did
you say . . . ??) to always be appropriate.
5 Results
We report classification results using Weka?s J48
decision tree classifier with 10-fold cross valida-
tion in Table 2, which outperformed JRip and
LibSVM in our experiments. Compared to the
majority baseline of 63.2% accuracy and .489 F-
measure, our classifier which uses all of the fea-
tures in Table 1 achieves a significant improve-
ment, with an accuracy of 88.5% and an F-
measure of .885. A baseline method that uses
only system request type feature (Req. type base-
line) achieves accuracy of 73.7% and F-measure
of .686, which is significantly below the perfor-
mance of the trained classifier. To identify the
most important features in predicting inappropri-
ate requests, we iteratively removed a single fea-
ture from the full feature set and re-evaluated pre-
diction accuracy. Table 2 shows absolute decrease
240
Features Acc (%) P/R/F-Measure
Majority baseline 63.2 * 0.399/0.632/0.489
Req. type baseline 73.7 * 0.814/0.737/0.686
All Features 88.5 0.885/0.885/0.885
less request type ?7.6 * ?0.076
less liwc ?2.3 ?0.023
less pos bigrams ?2.0 ?0.020
less word unigrams ?0.4 ?0.004
less func ratio ?0.1 ?0.001
less len spell ?0.05 ?0.0005
less word bigrams +0.05 +0.0007
Table 2: Classifying Inappropriate Requests: All
Features vs. Baseline vs. Leave-One-Out Classi-
fiers, where * indicates statistically significant dif-
ference from All Features (p < 0.01)
in percentage points and in F-measure when each
feature is removed in turn compared to the clas-
sifier trained on the full features set. We found
that system request type was the most important
feature, as performance decreased by 7.6 percent-
age points without it. This makes sense in light of
the fact that the ratio of inappropriate to appropri-
ate requests varied for the different request types
represented in our dataset. The next most useful
features were the output of LIWC and the POS
bigrams. We had hypothesized that, since LIWC
captures the presence of negations and assents, it
could capture negative user responses to the sys-
tem such as yes or no. As for the POS bigrams, we
modified the POS tags to mark common words and
included start and end markers in the bigrams be-
cause we hypothesized that the first words and last
words in the responses might be particularly infor-
mative. Looking at the decision tree created with
all our features, we find that the first five branches
involve decisions regarding the unigrams ?name?
and ?SPELL? (a collapsed spelling sequence), the
?START, ?neither?? bigram, the LIWC ingestion-
word feature, and the type of request, in that order.
Not only do these findings confirm our hypothe-
ses, they also confirm that the unigrams ?name?,
?SPELL?, and ?neither? which we had mapped to
special POS classes are particularly useful.
After training our model, we used it to classify
our entire dataset to see which responses it per-
formed well on and which it tended to misclassify.
Responses to targeted reprise and targeted-reprise-
rephrase questions together accounted for around
half of the misclassified instances. Many easily
identifiable responses to inappropriate requests in-
volved the user correcting the system, as in the fol-
lowing example:
User: You are going to need to dole out
punishment.
System: I think this is a name: ?dole out
punishment?. Please spell that name.
User: It is not a name, it is a phrase, dole
out punishment.
However, when the users did not correct the sys-
tem after an inappropriate request, their responses
appeared no different from answers to appropri-
ate requests. In the following example, the system
misrecognizes ?hyperbaric? and interprets it as the
word ?hyper? followed by an unknown phrase, but
the user simply ignores the request and repeats.
User: We are going to put you in a
hyperbaric chamber.
System: Put you in a high what? Please
give me another word or phrase
for ?perbaric?.
User: Hyperbaric chamber.
Many cases in which appropriate requests were
misclassified as inappropriate involved users re-
sponding correctly to targeted or targeted-rephrase
questions. We hypothesize that these are also due
primarily to users ignoring the inappropriate sys-
tem request and providing the information the sys-
tem should have asked for. As a result, those cases
make it difficult to distinguish between responses
to appropriate and inappropriate targeted ques-
tions. Of course, users may be giving prosodic
cues to indicate confusion or uncertainty or hyper-
articulating in their responses. We will address the
use of prosodic features in predicting inappropri-
ate requests in future work.
6 Conclusions
In this work, we have addressed a novel task of
identifying inappropriate clarification requests us-
ing features extracted from user responses. We
collected responses to inappropriate clarification
requests based on six request types in a simulated
SDS environment. The classifier trained on this
dataset detects inappropriate requests with accu-
racy of 88.5%, which is 25.3 percentage points
above the majority baseline, and an F-measure of
.885, which is .396 points above the majority F-
measure. In future work, we will include acoustic
and prosodic features as well as lexical features
and we will evaluate the use of an inappropriate
clarification request component in our speech-to-
speech translation system.
241
References
M. Akbacak et al. 2009. Recent advances in SRI?s
IraqComm
tm
Iraqi Arabic-English speech-to-speech
translation system. In ICASSP, pages 4809?4812.
N. F. Ayan et al. 2013. ?Can you give me another word
for hyperbaric??: Improving speech translation using
targeted clarification questions. In Acoustics, Speech
and Signal Processing (ICASSP), 2013 IEEE Interna-
tional Conference on, pages 8391?8395. IEEE.
D. Bohus and A. I. Rudnicky. 2005. A principled ap-
proach for rejection threshold optimization in spoken
dialog systems. In INTERSPEECH, pages 2781?2784.
J. Ginzburg and R. Cooper. 2004. Clarification, ellip-
sism and the nature of contextual updates. Linguistics
and Philosophy, 27(3).
J. Goldberg, M. Ostendorf, and K. Kirchhoff. 2003.
The impact of response wording in error correction
subdialogs. In ISCA Tutorial and Research Workshop
on Error Handling in Spoken Dialogue Systems.
P. Lendvai, A. van den Bosch, E. Krahmer, and
M. Swerts. 2002. Improving machine-learned de-
tection of miscommunications in human-machine di-
alogues through informed data splitting. In Proceed-
ings of the ESSLLI Workshop on Machine Learning Ap-
proaches in Computational Linguistics, pages 1?15.
D. Litman, J. Hirschberg, and M. Swerts. 2006. Char-
acterizing and predicting corrections in spoken dia-
logue systems. Computational linguistics, 32(3):417?
438.
J. W. Pennebaker, C. K. Chung, M. Ireland, A. Gon-
zales, and R. J. Booth, 2007. The development and
psychometric properties of LIWC2007. Austin, TX.
M. Purver. 2004. The Theory and Use of Clarification
Requests in Dialogue. Ph.D. thesis, King?s College,
University of London.
G. Skantze. 2005. Exploring human error recovery
strategies: Implications for spoken dialogue systems.
Speech Communication, 45(2-3):325?341.
S. Stoyanchev, A. Liu, and J. Hirschberg. 2014. To-
wards natural clarification questions in dialogue sys-
tems. In Proceedings of AISB2014.
K. Toutanova et al. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1.
Association for Computational Linguistics.
I. Witten and F. Eibe. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kauf-
mann, San Francisco, 2nd edition.
Appendix
ID Simulation Appro. Example
1. Confirmation
1 Correctly recognized utterance yes Did you say ?place this on the pane??
2 Misrecognized utterance yes Did you say ?these are in um searches will cause the insur-
gents to priest buyer??
2. Rephrase-part
1 Full non-name word or phrase yes Please say a different word for ?surmise?.
2 Partial word no Please say a different word for ?nouncing?.
3 Name no Please say a different word for ?Afdhal?.
4 Extended segment including name no Please say a different word for ?checkpoint at Betirma?.
5 Function word no Please say a different word for ?off over?.
3. Disambiguate
1 One choice is correct yes Did you mean fliers as in handouts or fliers as in pilots?
2 Neither choice is correct no Did you mean plane as in aircraft or plain as in simple?
3 Word being disambiguated was not said no Did you mean sight as in vision or site as in location?
4. Spell
1 Name yes Please spell ?Hadi Al Hemdani?.
2 Non-name no I think this is a name: ?eluding?. Please spell that name.
3 Extended segment no Please spell ?staff are stealing themselves?.
5. Reprise
1 Error segment correctly recognized and
no other errors
yes We will search some of the what?
2 Recognition error right before ?what?
word
no Supplies of I see them what?
3 Recognition error which is not the last
word before ?what?
no Ask if they are for eating for what?
6. Reprise rephrase
1 No errors outside of the error segment yes Use a what? Please say another word for ?bristled?.
2 Error segment is a partial word no Are there any my what? Please say another word for ?nors?.
3 Error outside the targeted segment no Be a right is what? Please say another word for ?rain?.
Table 3: Clarification Requests and Contexts in which they are Appropriate and Inappropriate.
242
