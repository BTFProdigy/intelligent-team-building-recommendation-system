Multilingual Speech Recognition for Information Retrieval                        
in Indian context 
 
 
Udhyakumar.N, Swaminathan.R and Ramakrishnan.S.K 
Dept. of Electronics and Communication Engineering 
Amrita Institute of Technology and Science 
Coimbatore, Tamilnadu ? 641105, INDIA. 
udhay_ece@rediffmail.com,{rswami_ece,skram_ece}@yahoo.com. 
 
 
Abstract 
This paper analyzes various issues in building 
a HMM based multilingual speech recognizer 
for Indian languages. The system is originally 
designed for Hindi and Tamil languages and 
adapted to incorporate Indian accented Eng-
lish. Language-specific characteristics in 
speech recognition framework are highlighted. 
The recognizer is embedded in information re-
trieval applications and hence several issues 
like handling spontaneous telephony speech in 
real-time, integrated language identification 
for interactive response and automatic graph-
eme to phoneme conversion to handle Out Of 
Vocabulary words are addressed. Experiments 
to study relative effectiveness of different al-
gorithms have been performed and the results 
are investigated. 
1 Introduction 
Human preference for speech communication has led to 
the growth of spoken language systems for information 
exchange. Such systems need a robust and versatile 
speech recognizer at its front-end, capable of decoding 
the speech utterances. A recognizer developed for spo-
ken language information retrieval in Indian languages 
should have the following features: 
 It must be insensitive to spontaneous speech effects 
and telephone channel noise. 
 Language Switching is common in India where 
there is a general familiarity of more than one lan-
guage. This demands for a multilingual speech rec-
ognition system to decode sentences with words 
from several languages. 
 Integrated language identification should be possi-
ble which helps later stages like speech synthesis to 
interact in user?s native language. 
This paper reports our work in building a multilingual 
speech recognizer for Tamil, Hindi and accented Eng-
lish. To handle sparseness in speech data and linguistic 
knowledge for Indian languages, we have addressed 
techniques like cross-lingual bootstrapping, automatic 
grapheme to phoneme conversion and adaptation of 
phonetic decision trees.  
In the area of Indian language speech recogni-
tion, various issues in building Hindi LVCSR systems 
have been dealt in (Nitendra Rajput, et al 2002). For 
Tamil language (Yegnanarayana.B et al 2001) attempts 
to develop a speaker independent recognition system for 
restricted vocabulary tasks. Speech recognizer for rail-
way enquiry task in Hindi is developed by 
(Samudravijaya.K 2001). 
 The paper is organized as follows. In sections 2, 
3 and 4 we discuss the steps involved in building a mul-
tilingual recognizer for Hindi and Tamil. In section 5 
automatic generation of phonetic baseforms from or-
thography is explained. Section 6 presents the results of 
adapting the system for accented English. Techniques to 
incorporate Language Identification are described in 
Section 7. Finally we conclude with a note on future 
work in section 8. 
2 Monolingual Baseline systems 
Monolingual baseline systems are designed for Tamil 
and Hindi using HTK as the first step towards multilin-
gual recognition. We have used OGI Multilanguage 
telephone speech corpus for our experiments (Yeshwant 
K. Muthusamy et al1992). The database is initially 
cleaned up and transcribed both at word and phone 
level. The phoneme sets for Hindi and Tamil are ob-
tained from (Rajaram.S 1990) and (Nitendra Rajput, et 
al. 2002). The spontaneous speech effects like filled 
pauses (ah, uh, hm), laughter, breathing, sighing etc. are 
modeled with explicit words. The background noises 
from radio, fan and crosstalk are pooled together and 
represented by a single model to ensure sufficient train-
ing. Front-end features consist of 39 dimensional 
Melscale cepstral coefficients. Vocal Tract Length 
Normalization (VTLN) is used to reduce inter and intra-
speaker variability. 
2.1 Train and Test sets 
The OGI Multilanguage corpus consists up to nine sepa-
rate responses from each caller, ranging from single 
words to short-topic specific descriptions to 60 seconds 
of unconstrained spontaneous speech. Tamil data totaled 
around 3 hours and Hindi data around 2.5 hours of con-
tinuous speech. The details of training and test data used 
for our experiments are shown in Table.1. 
 
Lang Data Sent Words Spkrs 
Train 900 7320 250 Tamil Test 300 1700 15 
Train 125 10500 125 Hindi Test 200 1250 12 
Table.1: Details of Training and Test Corpus. 
2.2 Context Independent Training 
The context independent monophones are modeled by 
individual HMMs. They are three state strict left-to-
right models with a single Gaussian output probability 
density function for each state. Baum-Welch training is 
carried out to estimate the HMM parameters. The re-
sults of the monolingual baseline systems are shown in 
Table.2. 
 
Accuracy Language Word Level Sentence Level 
Hindi 49.7% 46.2% 
Tamil 50.3% 48.7% 
Table.2: Recognition Accuracy of Monophone Models. 
 
The difference in accuracy cannot be attributed to the 
language difficulties because there are significant varia-
tions in database quality, vocabulary and quantity be-
tween both the languages.  
TAMIL: The recognition result for monophones shows 
that prominent errors are due to substitution between 
phones, which are acoustic variants of the same alpha-
bet (eg.ch and s, th and dh, etc.). Hence the lexicon is 
updated with alternate pronunciations for these words. 
As a result the accuracy improved to 56%.  
HINDI: Consonant clusters are the main sources of er-
rors in Hindi. They are replaced with a single consonant 
followed by a short spelled ?h? phone in the lexicon. 
This increased the accuracy to 52.9%. 
2.3 Context Dependent Training 
Each monophone encountered in the training data is 
cloned into a triphone with left and right contexts. All 
triphones that have the same central phone end up with 
different HMMs that have the same initial parameter 
values. HMMs that share the same central phone are 
clustered using decision trees and incrementally trained. 
The phonetic questions (nasals, sibilants, etc.) for tree 
based state tying require linguistic knowledge about the 
acoustic realization of the phones. Hence the decision 
tree built for American English is modified to model 
context-dependency in Hindi and Tamil. Further unsu-
pervised adaptation using Maximum Likelihood Linear 
Regression (MLLR) is used to handle calls from non-
native speakers. Environment adaptation is analyzed for 
handling background noise. 
3 Multilingual Recognition System 
Multilingual phoneme set is obtained from monolingual 
models by combining acoustically similar phones. The 
model combination is based on the assumption that the 
articulatory representations of phones are so similar 
across languages that they can be considered as units 
that are independent from the underlying language. 
Such combination has the following benefits (Schultz.T 
et al1998): 
 Model sharing across languages makes the system 
compact by reducing the complexity of the system. 
 Data sharing results in reliable estimation of model 
parameters especially for less frequent phonemes. 
 Multilingual models, bootstrapped as seed models 
for an unseen target language improve the recogni-
tion accuracy considerably. 
 Global phoneme pool allows accurate modeling of 
OOV (Out Of Vocabulary) words. 
International Phonetic Association has classified sounds 
based on the phonetic knowledge, which is independent 
of languages. Hence IPA mapping is used to form the 
global phoneme pool for multilingual recognizer. In this 
scheme, phones of Tamil and Hindi having the same 
IPA representation are combined and trained with data 
from both the languages (IPA 1999).  
The phonetic inventory of the multilingual rec-
ognizer ML?  can be expressed as a group of language 
independent phones LI? unified with a set of language 
dependent phones LD? that are unique to Hindi or 
Tamil. 
LDHLDTLIML ?????=?  
where LDT?   is the set of Tamil dependent models. 
           LDH?   is the set of Hindi dependent models 
The share factor SF is calculated as 
5.1
70
4359
?
+
=
?
?+?
=
ML
HTSF  
which implies a sharing rate of 75% between both 
the languages. The share factor is a measure of relation 
between the sum of language specific phones and the 
size of the global phoneme set. The high overlap of 
Hindi and Tamil phonetic space is evident from the 
value of SF. This property has been a motivating factor 
to develop a multilingual system for these languages. 
After merging the monophone models, context depend-
ent triphones are created as stated earlier. Alternate 
data-driven techniques can also be used for acoustic 
model combination, but they are shown to be outper-
formed by IPA mapping (Schultz.T et al1998). 
4 Cross Language Adaptation 
One major time and cost limitation in developing 
LVCSR systems in Indian languages is the need for 
large training data. Cross-lingual bootstrapping is ad-
dressed to overcome these drawbacks. The key idea in 
this approach is to initialize a recognizer in the target 
language by using already developed acoustic models 
from other language as seed models. After the initializa-
tion the resulting system is rebuilt using training data of 
the target language. The cross-language seed models 
perform better than flat starts or random models. Hence 
the phonetic space of Hindi and Tamil ML?  is popu-
lated with English models E?  in the following steps.  
 The English phones are trained with Network 
speech database (NTIMIT), which is the telephone 
bandwidth version of widely used TIMIT database. 
 To suit Indian telephony conditions, 16KHz 
NTIMIT speech data is down-sampled to 8KHz. 
 A heuristic IPA mapping combined with data-
driven approach is used to map English models 
with multilingual models. The mappings are shown 
in Table.3.  
 If any phone in E?  maps to two or more phones in 
ML?  the vectors are randomly divided between the 
phones since duplication reduces classification rate. 
 After bootstrapping, the models are trained with 
data from both the languages. 
 
Hindi Tamil English Hindi Tamil English 
  AA  - KD 
 - AA+N  - KH 
  AY 	  L 
 - AY+N 
  M 
  AW   N 
 - AW+N   NG 
 	 AX  
 OW 
 - AX+N  - OW+N 
  B   P 
 - BD  - PD 
 - BD+HH  - F 
  CH   R 
 - CH+HH   S 
  D  - K+SH 
 - DD   T 
