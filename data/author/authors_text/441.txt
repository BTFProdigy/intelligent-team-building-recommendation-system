Proceedings of the Workshop on Linguistic Distances, pages 8?15,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Similarity judgments: philosophical, psychological and mathematical 
investigations 
 
Claude St-Jacques 
Institute for Information Technology 
National Research Council of Canada 
Gatineau, QC, Canada 
Claude.St-Jacques@nrc.gc.ca 
Caroline Barri?re 
Institute for Information Technology 
National Research Council of Canada 
Gatineau, QC, Canada 
Caroline.Barriere@nrc.gc.ca 
 
  
 
Abstract 
This study investigates similarity judg-
ments from two angles.  First, we look at 
models suggested in the psychology and 
philosophy literature which capture the 
essence of concept similarity evaluation 
for humans.  Second, we analyze the 
properties of many metrics which simu-
late such evaluation capabilities. The first 
angle reveals that non-experts can judge 
similarity and that their judgments need 
not be based on predefined traits.  We use 
such conclusions to inform us on how 
gold standards for word sense disam-
biguation tasks could be established.  
From the second angle, we conclude that 
more attention should be paid to metric 
properties before assigning them to per-
form a particular task. 
1 Introduction 
The task of word sense disambiguation has 
been at the heart of Natural Language Processing 
(NLP) for many years.  Recent Senseval compe-
titions (Mihalcea and Edmonds, 2004; Preiss and 
Yarowsky, 2001) have stimulated the develop-
ment of algorithms to tackle different lexical dis-
ambiguation tasks. Such tasks require at their 
core a judgment of similarity as a word?s multi-
ple definitions and its contexts of occurrences are 
compared.  Similarity judgment algorithms come 
in many different forms.  One angle of this arti-
cle is to analyze the assumptions behind such 
similarity metrics by looking at different shared 
or non-shared properties. Among the interesting 
properties we note symmetry and transitivity, 
which are fundamental to the understanding of 
similarity. This angle is investigated in Section 4 
and 5, looking respectively at two broad classes 
of mathematical models of similarity and then 
more closely at different similarity metrics. 
As Senseval and other similar competitions 
need a gold standard for evaluating the compet-
ing systems, the second angle of our research 
looks into literature in philosophy and psychol-
ogy to gain insight on the human capability in 
performing a similarity judgment. From the first 
discipline explored in Section 2, we discover that 
philosophers have divergent views on concept 
identification, ranging from scientific definitions 
to human perception of concepts.  From the sec-
ond discipline, explored in Section 3, we dis-
cover different psychological models for concept 
identification and implicitly concept comparison, 
this time ranging from continuous concepts being 
positioned in multi-dimensional spaces to con-
crete concepts being grasped as entities. 
The two angles (metrics and humans) con-
verge in the conclusion of Section 6 with general 
observations and future work. 
2 Philosophical evidence 
Children have a natural eagerness to recognize 
regularities in the world and to mimic the behav-
ior of competent members of their linguistic 
community. It is in these words that Wittgenstein 
(1980) simply expresses how infants acquire the 
community?s language. What underlies the ac-
tivities surrounding a common use of language is 
similar to our usage of words to express some-
thing: ?Consider for example the proceedings 
that we call games. I mean board-games, card-
games, ball-games, Olympic games, and so on. 
What is common to them all?? (Wittgenstein, 
1968: 66). Wittgenstein answers that these ex-
pressions are characterized by similarities he 
calls family resemblances. 
8
Given that a dictionary?s purpose is to define 
concepts, we could hope to see such family re-
semblances among its definitions.  Contrarily to 
this intuition, Table 1 shows definitions and ex-
amples for a few senses of game in Wordnet1, 
from which resemblance cannot be found in 
terms of common words in the definitions or ex-
amples.  Nevertheless, humans are able to give 
different judgments of similarity between differ-
ent senses of the word game.  For example, simi-
larity between sense 1 and sense 3 is intuitively 
larger than between sense 1 and sense 4.   
 
Table 1: Some senses of game in Wordnet 
 Definition + Example 
1 A single play of a sport or other contest. The 
game lasted two hours. 
2 A contest with rules to determine a winner. You 
need four people to play this game. 
3 The game equipment needed in order to play a 
particular game. The child received several 
games for his birthday. 
4 Your occupation or line of work He's in the 
plumbing game. 
5 A secret scheme to do something (especially 
something underhand or illegal). [?] I saw 
through his little game from the start. 
 
Before being tempted to call up gigabytes of 
corpus evidence data and computational strength 
to help us identify the family of resemblance 
emerging here, let us further look at the nature of 
that notion from a philosophical point of view. 
Possible senses of individual things could be 
traced back to Aristotle?s work and identified 
?without qualification? as the primary substance 
of a thing (Cassam, 1986). What accounts for the 
substance of an object, for Aristotle, was the 
thing itself, namely its essence. Taking a slightly 
different view on the notion of family of objects, 
Putnam (1977) instead pursues a quest for natu-
ral kinds and according to him, the distinguish-
ing characteristics that ?hold together? natural 
kinds are the ?core facts [?] conveying the use 
of words of that kind? (Putnam, 1977: 118). Put-
nam disagrees with any analytical approaches 
sustaining that the meaning of a word X is given 
by a conjunction of properties P = {P1, P2,? Pn} 
in such a way that P is the essence of X. The 
problem is that a ?natural kind may have abnor-
mal members? (Putnam, 1977: 103). For instance, 
normal lemons have a yellow peel but let?s sup-
pose in accordance with Putnam, that a new en-
vironmental condition makes lemon peel become 
                                                
1
 See http://wordnet.princeton.edu/ 
blue. An analytical view will be unable to state 
which one amongst the yellow or the blue ones is 
now the normal member of the natural class of 
lemons. Putnam rather relies on a ?scientific the-
ory construction? to define what an object of 
natural kind is, and therefore, does not see that 
dictionaries ?are cluttered up [?] with pieces of 
empirical information? (Putnam, 1977: 118) as a 
defect to convey core facts about a natural class.  
In contrast to Putnam, Fodor (1998) is a viru-
lent opponent to a mind-independent similarity 
semantics subject to scientific discoveries. With 
his ostentatious doorknob example, Fodor shows 
that there is not any natural kind, hidden essence 
or peculiar structure that makes a doorknob a 
doorknob. ?No doubt, some engineer might con-
struct a counter-example?a mindless doorknob 
detector; and we might even come to rely on 
such a thing when groping for a doorknob in the 
dark? (Fodor, 1998: 147). However, the con-
struct will have to be done on what strikes us as  
doorknobhood or satisfying the doorknob stereo-
type, i.e. ?the gadget would have to be calibrated 
to us since there is nothing else in nature that 
responds selectively to doorknobs? (Fodor, 1998: 
147). According to Fodor, our capacity to ac-
quire the concept of doorknob involves a similar-
ity metric, and it is the human innate capacity to 
determine the concepts similar to doorknob that 
allow the characterization of doorknobhood. 
Therefore, Fodor states that the meaning of con-
cepts is mind-dependent and that individuation is 
not intractable since members of a language 
community, although experiencing diverse forms 
of a concept will tend to acquire similar stereo-
types of such a concept.  
This brief exploration into philosophical ap-
proaches for concept representation and delimita-
tion can inform us on the establishment of a gold 
standard by humans for the word sense disam-
biguation (WSD) task.  In fact, the adherence to 
one model rather than another has an impact on 
who should be performing the evaluation2.  Sen-
seval-2 was in line with Putnam?s view of ?divi-
sion of linguistic labour? by relying on lexicog-
raphers? judgments to build a gold standard (Kil-
garrif, 1998). On the other hand, Senseval-3 col-
lected data via Open-Mind Initiative3, which was 
much more in line with Fodor?s view that any 
common people can use their own similarity 
                                                
2
 The evaluation consists in performing sense tagging of 
word occurrences in context.  
3
 See http://www.openmind.org/, a web site where anyone 
can perform the sense tagging ?games?. 
9
metric to disambiguate polysemous terms. Inter-
estingly, a recent empirical study (Murray and 
Green 2004) showed how judgments by ordinary 
people were consistent among themselves but 
different from the one of lexicographers. It is 
important to decide who the best judges are; a 
decision which can certainly be based on the 
foreseen application, but also, as we suggest here, 
on some theoretical grounds. 
3 Psychological Evidence 
We pursue our quest for insights in the 
establishment of gold standards by humans for 
the WSD task, now trying to answer the ?how? 
question rather then the ?who? question. Indeed, 
Fodor?s view might influence us in deciding that 
non-experts can perform similarity judgments, 
but this does not tell us how these judgments 
should be performed.  Different psychological 
models will give possible answers.  In fact, 
similarity judgments have been largely studied 
by experimental psychologists and distinctive 
theories give some evidence about the existence 
of a human internal cognitive mechanism for 
such judgments. In this section, we present three 
approaches: subjective scaling and objective 
scaling (Voinov, 2002), and semantic differential 
(Osgood et al 1957).  
3.1 Subjective Scaling 
In subjective scaling (Voinov, 2002), the 
subjective human judgment is considered as a 
convenient raw material to make comparison 
between empirical studies of similarity. Subjects 
are asked to point out the ?similarities among n 
objects of interest ? whether concepts, persons, 
traits, symptoms, cultures or species? (Shepard, 
1974: 373). Then the similarity judgments are 
represented in an n ? n matrix of objects by a 
multidimensional scaling (MDS) of the distance 
between each object.  Equation 1 shows the 
evaluation of similarity, where ),( jkik xxd stands 
for the distance between objects ix and jx   on 
stimulus (dimension) k and kw  is the 
psychological salience of that stimulus k: 
( ) )),((,
1
 
=
=
m
k
jkikkji xxdwxxD .                      (1) 
Shepard?s MDS theory assumes that a 
monotonic transformation should be done from a 
nonmetric psychological salience of a stimulus to 
a metric space model. By definition, the resulting 
metric function over a set X should fullfill the 
following conditions: 
Xzyx ?? ,, : 
1. 0),(),( =? xxdyxd  (minimality), 
2. ),(),( xydyxd =  (symmetry), 
3. ),(),(),( yzdzxdyxd +?  (triangle ineq.). 
Accordingly to Shepard (1974), the distance in 
equation (1) can be computed with different 
metrics. Some of these metrics are given in 
Lebart and Rajman (2000). The Euclidean metric 
is the best known: 
2
1
1
2)(),( 


?= 
=
m
k
jkikkjiE xxwxxd .          (2)      
The city block metric is another one: 

=
?=
m
k
jkikkjiC xxwxxd
1
),( .                     (3) 
Another yet is the Minkowski metric: 
( )nm
k
n
jkikkjiN xxwxxd
1
1
)(),( 	
=
?= .            (4)             
There is a main concern with the MDS model. 
Tversky (1977) criticized the adequacy of the 
metric distance functions as he showed that the 
three conditions of minimality, symmetry and 
triangle inequality are sometimes empirically 
violated. For instance, Tversky and Gati showed 
empirically that assessment of the similarity 
between pairs of countries was asymetric when 
they asked  for ?the degree to which Red China 
is similar to North Korea? (1978: 87) and in the 
reverse order, i.e. similarity between North 
Korea and Red China. 
3.2 Objective Scaling 
The second approach is called objective scaling 
by Voinov ?though this term is not widely ac-
cepted? (Voinov, 2002). According to him, the 
objectivity of the method comes from the fact 
that similarity measures are calculated from the 
ratio of objective features that describe objects 
under analysis. So, subjects are asked to make 
qualitative judgments on common or distinctive 
features of objects and the comparison is then 
made by any distance axioms. Tversky?s (1977) 
contrast model (CM) is the best known formal-
ization of this approach. In his model, the meas-
ure of similarity is computed by: 
 
)()(),( BAfBAfBAS ??= ?? 
  
)( ABf ?? ?                     (5) 
10
where )( BAf   represents a function of the 
common features of both entities A and B,  
)( BAf ? is the function of the features belong-
ing to A but not B, )( ABf ? is the function of 
the features belonging to B but not A and 
??? ,, are their respective weighting parame-
ters. Equation (5) is the matching axiom of the 
CM. A second fundamental property of that 
model is given by the axiom of monotonicity: 
 
),(),( CASBAS ?                      (6) 
If BACA  ? , ,CABA ???  and 
ACAB ??? ,  then (6) is satisfied. With these 
two axioms (5-6), Tversky (1977) defined the 
basis of what he called the matching function 
using the theoretical notion of feature sets rather 
then the geometric concept of similarity distance. 
Interesting empirical studies followed this re-
search on CM and aimed at finding the correla-
tion between human judgments of similarity and 
difference. Although some results show a corre-
lation between these judgments, there is limita-
tion to their complementarity: ?the relative 
weights of the common and distinctive features 
vary with the nature of the task and support the 
focusing hypothesis that people attend more to 
the common features in judgments of similarity 
than in judgments of the difference? (Tverski and 
Gati, 1978: 84). Later on, Medin et al (1990) 
also reported cases when judgments of similarity 
and difference are not inverses: first, when enti-
ties differ in their number of features, and second 
when similarity/difference judgments involve 
distinction of both attributes and relations. ?Al-
though sameness judgments are typically de-
scribed as more global or non-analytic than dif-
ference judgments, an alternative possibility is 
that they focus on relations rather than attributes? 
(Medin et al, 1990: 68). 
3.3 Semantic Differential 
One standard psycholinguistic method to 
measure the similarity of meaning combines the 
use of subjective scaling transposed in a 
semantic space. One well-known method is 
Semantic Differential (SD) developed by Osgood 
et al (1957). 
The SD methodology measures the meanings 
that individual subjects grant to words and 
concepts according to a series of factor analyses. 
These factor analyses are bipolar adjectives put 
at each end of a Likert scale (Likert, 1932) 
devised to rate the individual reaction to the 
contrasted stimulus. For instance, the SD of a 
concept can be rated with two stimuli of 
goodness and temperature: 
BadGood
3
:
2
:
1
:
0
:
1
:
2
:
3
?
 
HotCold
3
:
2
:
1
:
0
:
1
:
2
:
3
?
 
If the subject feels that the observed concept is 
neutral with regards to the polar terms, his 
check-mark should be at the position 0. In our 
example, the mark on the good-bad scale being 
at the 1 on the left side of the neutral point 0, the 
judgment means slighthy good. Positions 2 and 3 
on that same side would be respectively quite 
good and extremely good. A similar analysis 
applies for the cold-hot scale shown. 
The theoretical background of that 
methodology, which tries to standardize across 
subjects the meaning of the same linguistic 
stimulus, relies on psychological research on 
synestesia. Simply explained, synestesia is 
similar to a double reaction to a stimulus. For 
example, when presented with images of 
concepts, subjects do not only have a 
spontaneous reaction to the images, but they are 
also able to characterize the associated concept 
in terms of almost any bipolar adjective pairs 
(hot-cold, pleasant-unpleasant, simple-complex, 
vague-precise, dull-sharp, static-dynamic, sweet-
bitter, emotional-rational, etc.). According to 
Osgood et al ?the imagery found in synesthesia 
is intimately tied up with language metaphor, and 
both represent semantic relations? (1957: 23). 
In SD, bipolar adjectives used in succession 
can mediate a generalization to the meaning of a 
sign, as uncertainty on each scale is reduced with 
the successive process of elicitation. By 
postulating representation in a semantic space, 
each orthogonal axis of selection produces a 
semantic differentiation when the subjects rate 
the semantic alternatives on a bipolar scale. 
Although that space could be multidimensional, 
empirical studies (Osgood et al, 1957) on factor 
analysis showed stability and relative importance 
of three particular dimensions labeled as 
Evaluation, Potency, and Activity (EPA). We 
refer the reader to Osgood et al (1957) for 
further explanation on these EPA dimensions. 
3.4 WSD and human judgments 
Table 2 emphasizes commonalities and differ-
ences between the three psychological models 
explored.   
11
Table 2 ? Psychological Models 
 Continuous Prede-
fined traits 
Similarity/ 
Difference 
MDS Yes Yes No 
CM No Yes Yes 
SD No No Possible 
 
In Table 2, we show that both MDS (Shepard, 
1974) and CM (Tversky, 1977) rely on a set of 
predefined traits.  This is a major problem, as it 
leads to the necessity of defining in advance such 
a set of traits on which to judge similarity be-
tween objects.  On the other hand, SD (Osgood 
et al 1957), although using a few bipolar scales 
for positioning concepts, argues that these scales 
are not concept-dependent, but rather they can be 
used for grasping the meaning of all concepts.  A 
second major difference highlighted in Table 2 is 
that MDS is the only approach looking at con-
tinuous perceptual dimensions of stimulus, con-
trarily to CM in which the scaling procedes with 
discrete conceptual traits, and even more in op-
position to SD which considers entities as primi-
tives. Finally, Table 2 shows the interesting ob-
servation brought forth by Tversky and later em-
pirical studies of Medin et al (1980) of the non-
equivalence between the notion of similarity and 
difference. 
Coming back to the question of ?how? human 
evaluation could be performed to provide a gold 
standard for the WSD task, considering the pros 
and cons of the different models lead us to sug-
gest a particular strategy of sense attribution.  
Combining the similarity/difference of Tversky 
with the successive elucidation of Osgood et al, 
two bipolar Likert scales could be used to delimit 
a similarity concept: a resembling axis and a con-
trasting axis. In this approach, the similarity con-
cept still stays general, avoiding the problems of 
finding specific traits for each instance on which 
to have a judgment. 
Already in the empirical studies of Murray and 
Green (2004), a Likert scale is used, but on an 
?applying? axis.  Subjects are asked for each 
definition of a word to decide whether it ?applies 
perfectly? or rather ?barely applies? to a context 
containing the word.  The choice of such an axis 
has limitations in its applicability for mapping 
senses on examples.  More general resembling 
and contrasting axis would allow for similarity 
judgments on any statements whether they are 
two sense definitions, two examples or a sense 
definition with an example. 
4 Mathematical Models of Similarity 
Logic and mathematics are extremely prolific 
in similarity measurement models. According to 
Dubois et al(1997), they are used for cognitive 
tasks like classification, case-based reasoning 
and interpolation. In the present study, we re-
strict our investigation to the classification task 
as representative on the unsupervised WSD task.  
The other approaches are inferential strategies, 
using already solved problems to extrapolate or 
interpolate solutions to new problems. Those 
would be appropriate for WSD in a supervised 
context (provided training data), but due to space 
constraints, we postpone discussion of those 
models to a later study. Our present analysis di-
vides classification models into two criteria: the 
cardinality of sets and the proximity-based simi-
larity measures. 
4.1 Cardinality of sets 
In line with De Baets et al (2001), similarity 
measures can be investigated under a rational 
cardinality-based criterion of sets. In an exten-
sive study of 28 similarity measures for ordinary 
sets, this research showed that measures can be 
classified on the basis of only a few properties. 
They proposed at first to build the class of cardi-
nality-based similarity measures from one ge-
neric formula: 
 
YXYXYXYX
YXYXYXYX
zyxw
zyxw
YXS
,,,,
,,,,
''''
),( ????
????
+++
+++
= ,  
                                        (8) 
 where { })(#),(#min
,
XYYXYX ??=? , 
{ })(#),(#max
,
XYYXYX ??=? , 
)(#
,
YXYX  =?  and cYX YX )(#, =? , and 
all w , x , y , z , 'w , 'x , 'y , 'z  { }1,0? . It 
follows that )(# YX   is the number of couples 
(1,1) and YX ?  denotes the sets difference 
)()( cYXYX =? . 
The classification of these 28 similarity meas-
ures (which can all be linked to the general for-
mula) becomes possible by borrowing from the 
framework of fuzzy sets the concepts of T for t-
norm (fuzzy intersection) operators and T-
equivalence for the property of T-
indistinguishability (De Baets et al, 2001). So, a 
typical measure M of T-equivalence under the 
universe U  must satisfy the following condi-
tions for any (x, y, z) U? : (i) 1),( =xxM  (re-
flexivity); (ii) ),(),( xyMyxM =  (Symmetry); 
12
(iii) ),()),(),,(( zxMzyMyxMT ?  (T-
transitivity). 
All 28 measures show reflexivity and symme-
try but they vary on the type of transitivity they 
achieve. In fact, studying boundary and 
monotonicity behavior of the different measures, 
De Baets et al (2001) group them under four 
types corresponding to four different formulas of 
fuzzy intersections (t-norms): the standard inter-
section ),min(),( babaZ = ,  the Lukasiewicz t-
norm )1,0max(),( ?+= babaL , the algebraic 
product abbaP =),(  and the drastic intersec-
tion abaD (),( =  when 1=b , b  when 1=a  
and 0  otherwise). We refer the reader to De 
Baets et al (2001) to get the full scope of their 
results. Accordingly, Jaccard?s coefficient J 
(equation 9) and Russel-Rao?s coefficient R 
(equation 10) are both, for example, L-transivive 
(Lukasiewicz? type): 
( )
( )YX
YXYXSJ  

#
#),( =                      (9) 
( )
n
YXYXSR

#),( =     .              (10) 
On the other hand, the overlapping coefficient O 
(equation 11) is not even D-transitive, knowing 
that D is the lower transitive condition 
)( ZPLD ??? in the framework: 
( )
( )YX
YXYXSO #,#min
#),(

=  .          (11) 
4.2 Proximity-based 
Following our second criterion of classifica-
tion, mathematics also uses diverse proximity-
based similarity measures. We subdivide these 
mathematical measures into three groups: the 
distance model, the probabilistic model, and the 
angular coefficients. The first one, the distance 
model, overlaps in part with the subjective scal-
ing of similarity as presented in the psychologi-
cal approaches (section 3.1). The mathematical 
model is the same with a metric of distance 
),( yxd computed between the objects in a space. 
Algorithms like formulae (2), (3) and (4) of sec-
tion 3.1 are amongst the proximity-based similar-
ity measures. 
Second, the probabilistic model is based on 
the statistical analysis of objects and their attrib-
utes in a data space. Lebart & Rajman (2000) 
gave many examples of that kind of proximity 
measures, such as the Kullback-Leiber distance 
KD  between two documents A and B, given the 
probability distribution { }npppP ,...,, 21= : 

??
??=
0
)log)(log(),(
bkak pp
bkakbkakK ppppBAD
 (12) 
The third mathematical model is also a metric 
space model but it uses angular measures be-
tween vectors of features to determine the simi-
larity between objects. A well-known measure 
from that group is the cosine-correlation: 


	


	

= 

==
=
n
k
k
n
k
k
n
k
kk
C
yx
yx
yxS
1
2
1
2
1),( .             (13) 
 
Although conditions applying on proximity-
based measures are shortly described in Cross 
and Sudkamp (2002) and Miyamoto (1990) for 
fuzzy sets, we are not aware of an extensive re-
search such as the one by De Baets et al (2001), 
presented in section 4.1, for classifying cardinal-
ity of sets types. We make such an attempt in the 
following section. 
5 Analysis of similarity metrics 
In this section, we perform a classification and 
analysis exercise for similarity measure4, possi-
bly used for WSD, but more generally used in 
any task where similarity between words is re-
quired. Table 3 shows the measures classified in 
the four categories of the mathematical model 
presented in section 4: measures of cardinality 
(Card), of distance (Dist), of probability (Prob) 
and of angle (Ang).   
We sustain that these groupings can be further 
justified based on two criteria: the psychological 
model of meaning (Table 2) and the typical 
properties of the classes (Table 4). The first crite-
rion refers to the representation of concepts dis-
tinguishing between the dense-state and the dis-
crete-state5 of concept (meaning) attributes. That 
psychological distinction is helpful to categorize 
some metrics, like Gotoh, which seems hybrid 
(Card and Dist). In such a metric, the penalty for 
the gap between two concepts applies on the de-
fect of the dense-state, such as for a blurred im-
                                                
4
 We use  the list of the following web page: http:// 
www.dcs.shef.ac.uk/~sam/stringmetrics.html#sellers  
5
 This differentiation is based on Tenenbaum?s (1996) idea 
that MDS better suits continuous perceptual domains and 
set-theoretic accommodate discrete features like in the CM. 
13
age rather then the absence of the discrete-state, 
i.e. of a feature; it is therefore classified in the 
Dist category. 
 
Table 3: Classification of Similarity Metrics 
Metric Card Dist Prob Ang 
Hamming distance  X   
Levenshtein distance  X   
Needleman-Wunch  X   
Smith-Waterman  X   
Gotoh distance  X   
Block distance  X   
Monge Elkan dist.  X   
Jaro distance   X  
Jaro Winkler   X  
SoundEx distance   X  
Matching coefficient X    
Dice?s coefficient X    
Jaccard similarity X    
Overlap coefficient X    
Euclidean distance  X   
Cosine similarity    X 
Variational distance   X  
Hellinger distance   X  
Information radius   X  
Harmonic mean   X  
Skew divergence   X  
Confusion probability   X  
Tau   X  
Fellegi & Sunters   X  
TFIDF     X 
FastA   X  
BlastP   X  
Maximal matches   X  
q-gram   X  
Ukkonen algorithms   X  
 
The second criterion is a study on shared 
properties for each category of the mathematical 
model. Table 4 summarizes the properties using 
the following schema: (m) minimality, (r) reflex-
ivity, (s) symmetry, (ti) triangle inequality, (tr) 
transitivity. 
 
Table 4 ? Typical Properties of Metrics 
 (m) (r) (s) (ti) (tr) 
Card  Yes Yes  Yes 
Dist Yes  Yes Yes Possible 
Prob  No Possible  Yes 
Ang Yes  Yes  Yes 
 
From Table 4, we see for instance that reflex-
ivity is a basic property for cardinality measures 
because we wish to regularly count discrete ob-
jects in a set. On the opposite side, the minimal-
ity property is a characteristic of a distance 
measure, since it is noticeable by the displace-
ment or the change, for example, in distinctive 
images. According to Fodor (1998), we say that 
statistical or probabilistic approaches exhibit 
several necessary and sufficient conditions for 
the inclusion of elements in the extension of a 
concept, but the dominant element, such as the 
pattern of comparison (in Maximal matches for 
instance) is anti-reflexive and asymmetric with 
the resulting elements. However, there is symme-
try in the resultant, but there is still anti-
reflexivity. 
We also single out the angular metrics from 
distance measures even though they use a similar 
analysis of the qualitative variation of entities. 
According to Ekman & Sj?berg (1965), a method 
using similarity converted into cosine representa-
tion has the advantage to reveal two components 
of percepts, i.e. the two-dimensional vector is a 
modeling in magnitude and direction. Thus, an-
gular metrics can be a means used to contrast 
two semantic features of entities. 
5.1 A closer look at properties 
Finding out that different sets of properties can 
serve as dividing lines between groups of metrics 
is interesting in itself, but does not answer the 
question as to which set is more appropriate than 
others.  We do not wish to answer this question 
here as we believe it is application-dependent, 
but we do wish to emphasize that a questioning 
should take place before choosing a particular 
measure. In fact, for each property, there is an 
appropriate question that can be asked, as is 
summarized in Table 5. 
 
Table 5 ? Questioning for Measure Selection 
Property Question 
Minimality Is the minimal distance between objects the 
distance of an object with itself? 
Symmetry Is it true that the distance between x and y is 
always the same as the distance between y 
and x? 
Triangle 
Inequality 
Is it appropriate that a direct distance be-
tween x and z is always smaller than a com-
posed distance from x to y and y to z? 
Reflexivity  Is it true that the relation that it holds be-
tween an object and itself is always the 
same? 
Transitivity Is it necessarily the case that when x is 
similar to y and y is similar to z, that x be 
similar to z? 
 
For the task of WSD investigated in this paper, 
we hope to open the debate as to which proper-
ties are to be taken into consideration. 
6 Conclusion and future work 
This paper presented some ideas from two angles 
of study (human and metrics) into the intricate 
problem of similarity judgments.  A larger study 
14
is under way on both angles.  First, we suggested, 
based on some psychological and philosophical 
model analysis, a two-axis Osgood-like bench-
marking approach for ?ordinary human? word-
sense judgments.  We intend to perform an em-
pirical experiment to validate this idea by look-
ing at inter-judge agreement.  
On the algorithm side, although the ap-
proaches based on the cardinality of sets are not 
central to WSD, we presented them first as we 
find it inspiring to see an effort of classification 
on those measures.  We then attempted a some-
what more broad classification by emphasizing 
properties of different groups of similarity meas-
ures: cardinality of sets, distance, probabilistic 
measures and angular metrics.  Although each 
group has a particular subset of properties, we 
noted that all of them share a property of transi-
tivity.  This is interestingly different from the 
psychological contrast model of Tversky where 
differences and similarities are measured differ-
ently on different criteria.  We think investiga-
tions into similarity measures which reproduce 
such a non-transitive differentiation approach 
should be performed.  We are on that path in our 
larger study.  We also suggest that any proposal 
of a measure for a task should be preceded by a 
study of which properties seem adequate for such 
a task.  We conclude by opening up the debate 
for the WSD task. 
References 
Bernard De Baets, Hans De Meyer and Helga Naes-
sens. 2001. A class of rational cardinality-based 
similarity measures. Journal of Computational and 
Applied Mathematics, 132:51-69. 
Quassim Cassam. 1986. Science and Essence. Phi-
losophy, 61:95-107. 
Valerie V. Cross and Thomas A. Sudkamp. 2002. 
Similarity and Compatibility in Fuzzy Set Theory. 
Heidelberg, Germany: Physica-Verlag.. 
Didier Dubois, Henri Prade, Francesc Esteva, Pere 
Garcia and Lluis Godo. 1997. A Logical Approach 
to Interpolation Based on Similarity Relations. In-
ternational Journal of Approximate Reasoning, 
17:1-36. 
C?sta Ekman and Lennart Sj?berg. 1965. Scaling. 
Annual Review of Psychology, 16, 451-474. 
Jerry A. Fodor. 1998. Concepts. Where Cognitive 
Science Went Wrong. Oxford: Clarendon Press. 
Adam Kilgarriff. 1998. Gold standard datasets for 
evaluating word sense disambiguation programs. 
Computer Speech and Language, 12:453-472. 
Ludovic Lebart and Martin Rajman. 2000. Computing 
Similarity in R. Dale, H. Moisl & H. Somers eds. 
Handbook of Natural Language Processing. New 
York: Marcel Dekker, Inc., 477-505. 
Rensis Likert. 1932. A technique for the measurement 
of attitudes.  Archives of Psychology 140, 5-53. 
Douglas L. Medin, Robert L. Goldstone and Dedre 
Gentner. 1990. Similarity Involving Attributes and 
Relations: Judgments of Similarity and Difference 
are not Inverses. Psychological Science, 1(1):64-69 
Rada Mihalcea and Phil Edmonds. 2004. Proceedings 
of SENSEVAL-3, Association for Computational 
Linguistics Workshop, Barcelona, Spain. 
Sadaaki Miyamoto. 1990. Fuzzy Sets in Information 
Retrieval and Cluster Analysis. Dordrecht: Kluwer 
Academic Publisher. 
G. Craig Murray and Rebecca Green. 2004. Lexical 
knowledge and human disagreement on a WSD 
task, Computer Speech and Language 18, 209-222. 
Charles E. Osgood, George J. Suci and Percy H. Tan-
nenbaum. 1957. The measurement of meaning. Ur-
bana: University of Illinois Press 
Judita Preiss and David Yarowsky (eds). 2001. Pro-
ceedings of SENSEVAL-2, Association for Compu-
tational Linguistics Workshop, Toulouse, France. 
Hilary Putnam. 1977. Is Semantics Possible? in 
Stephen P. Schwartz ed. Naming, Necessity, and 
Natural Kinds. Ithaca and London: Cornell Univer-
sity Press, 102-118. 
Roger N. Shepard. 1974. Representation of structure 
in similarity data: Problems and prospects. Psy-
chometrika, 39(4):373-421. 
Joshua B. Tenenbaum. 1996. Learning the structure of 
similarity. In D. S. Touretzky, M. C. Mozer and M. 
E. Hasselmo (Eds), Advances in neural information 
processing systems, (Vol. 8, pp. 3-9), Cambridge, 
MA: MIT Press. 
Amos Tversky. 1977. Features of Similarity. Psycho-
logical Review, 84, 79-98. 
Amos Tversky and Itamar Gati. 1978. Studies of 
Similarity in E. Rosch & B. B. Lloyd eds. Cogni-
tion and Categorization. New York: John Wiley & 
Sons, Inc., 79-98. 
Alexander V. Voinov. 2002. The Role of Similarity 
Judgment in Intuitive Problem Solving and its 
Modeling in a Sheaf-Theoretic Framework. Pro-
ceedings of the 1st Int. Conf. on FSKD?02, 1:753-
757. 
Ludwig Wittgenstein. 1968. Philosophical Investiga-
tions. Oxford: Basil Blackwell. 
Ludwig Wittgenstein. 1980. Remarks on the Philoso-
phy of Psychology. Chicago: University of Chicago 
Press; Oxford: Basil Blackwell. 
15
Proceedings of the First Workshop on Computational Approaches to Compound Analysis, pages 72?80,
Dublin, Ireland, August 24 2014.
Multiword noun compound bracketing using Wikipedia
Caroline Barri
`
ere Pierre Andr
?
e M
?
enard
Centre de Recherche Informatique de Montr?eal (CRIM)
Montr?eal, QC, Canada
{caroline.barriere;pierre-andre.menard}@crim.ca
Abstract
This research suggests two contributions in relation to the multiword noun compound bracketing
problem: first, demonstrate the usefulness of Wikipedia for the task, and second, present a novel
bracketing method relying on a word association model. The intent of the association model
is to represent combined evidence about the possibly lexical, relational or coordinate nature of
links between all pairs of words within a compound. As for Wikipedia, it is promoted for its
encyclopedic nature, meaning it describes terms and named entities, as well as for its size, large
enough for corpus-based statistical analysis. Both types of information will be used in measuring
evidence about lexical units, noun relations and noun coordinates in order to feed the associa-
tion model in the bracketing algorithm. Using a gold standard of around 4800 multiword noun
compounds, we show performances of 73% in a strict match evaluation, comparing favourably
to results reported in the literature using unsupervised approaches.
1 Introduction
The noun compound bracketing task consists in determining related subgroups of nouns within a larger
compound. For example (from Lauer (1995)), (woman (aid worker)) requires a right-bracketing inter-
pretation, contrarily to ((copper alloy) rod) requiring a left-bracketing interpretation. When only three
words are used, n1 n2 n3, bracketing is defined as a binary decision between grouping (n1,n2) or group-
ing (n2,n3). Two models, described in early work by Lauer (1995), are commonly used to inform such
decision: the adjacency model and the dependency model. The former compares probabilities (or more
loosely, strength of association) of two alternative adjacent noun compounds, that of n1 n2 and of n2 n3.
The latter compares probabilities of two alternative dependencies, either between n1 and n3 or between
n2 and n3.
Most compound bracketing research has focused on three-noun compounds as described above. Some
recent work (Pitler et al. (2010), Vadas and Curran (2007b)) looks at larger compounds, experimenting
with a dataset created by Vadas and Curran (2007a) which we also use in our research. For larger
noun compounds, the adjacency model alone will not allow longer range dependencies to be taken into
account. This had been noted much earlier in Barker (1998) using examples such as (wooden (((French
(onion soup)) bowl) handle)) to show a long-range dependency between wooden and handle.
To allow for such long-range dependencies, our bracketing algorithm looks at all possible word as-
sociations within the full expression to make its decisions. The word associations are captured within
an association model which goes beyond the adjacency and dependency models. The association model
represents combined evidence about the possibly lexical, relational or coordinate nature of the links be-
tween all word pairs. In its current implementation, our association model relies on Wikipedia as a
resource for obtaining all three types of evidence. Wikipedia is used in two forms: first as a list of terms
and named entities (Wikipedia page titles), and second, as a large corpus obtained from the merging of
all its pages. The resulting corpus is large enough to be used for statistical measures. The most current
version contains 14,466,099 pages in English for an uncompressed file size of 47 gigabytes (including
This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/
72
some metadata). To the best of our knowledge, no previous research has used Wikipedia for the noun
bracketing task, and this research will explore its usefulness.
The reminder of this article will unfold as follows. Section 2 presents a brief literature review. Sec-
tion 3 describes the dataset used in our experiments. Section 4 presents the bracketing algorithm, and
Section 5 the implementation of a word association model using Wikipedia. Section 6 describes our
evaluation approach, while results are presented and analysed in Section 7. Section 8 concludes and
suggests future work.
2 Related work
Noun compound bracketing has not received as much attention as many other Natural Language Pro-
cessing (NLP) tasks. Nakov and Hearst (2005) call it an understudied language analysis problem. Early
work by Lauer (1995) took inspiration in even earlier linguistic work by Levi (1978). Lauer (1995) hav-
ing devised a small dataset of 3-word noun compounds, his dataset was reused by various researchers
(Lapata et al. (2004), Girju et al. (2005), Nakov and Hearst (2005)) who promoted the use of corpus-
based empirical methods for the task.
To address the noun compound bracketing task, different authors use different datasets, different views
on the problem (adjacency, dependency), different methods of resolution (supervised, unsupervised)
and different constraints on the problem (compound seen in isolation or in context). Independently
of such differences, all researchers have an interest in evaluating word-pair associations. Most recent
research uses the Web for providing word pair association scores to their bracketing algorithm. The work
of Lapata et al. (2004) shows usefulness of web counts for different tasks, including noun compound
bracketing. The work of Pitler et al. (2010) intensively uses web-scale ngrams in a supervised task for
large NP bracketing, showing that coverage impacts on accuracy. Beyond bigram counts on the web,
varied and clever searches (Nakov and Hearst, 2005) have been suggested such as the use of paraphrases
(n1 causes n2) or simpler possessive markers (n1?s n2) or even the presence of an hyphen between words
(n1-n2). All variations are to provide better word association estimates and improve bracketing. The use
of web counts is sometimes complemented by the use of more structured resources, such as in Vadas and
Curran (2007b) who combines web counts with features from Wordnet.
In our research, instead of web counts, we rely on a community-based encyclopedic resource,
Wikipedia, for corpus-based evidence. We rely on the same resource to access a list of terms and entities.
Although not much of the structure of Wikipedia is used in our current implementation, such as its cate-
gories or page links, we can envisage to use it in future work. Similarly to other researchers mentioned
above, our goal is to gather evidence for word-pair association, although an important contribution of
our work is to refine this notion of word-pair association into three subtypes of association: lexical, re-
lational and coordinate. We suggest that a better characterization of the possible links among word pairs
in a large compound will better inform the bracketing algorithm.
3 Dataset
Vadas and Curran (2007a) manually went through the Penn Treebank (Marcus et al., 1994) to further
annotate large NPs. They openly published a diff file of the Penn Treebank to show their annotations
which differ from the original. From this available file, we constructed our gold-standard dataset by
extracting large NPs (three or more words) which only include relevant items (common and proper
nouns, adverbs and adjectives), removing determiners, numbers, punctuations and conjunctions. The
expressions were then verified for completeness, so that the opening bracket should be closed within the
length of text defined in the differential file. Finally, tags and single words enclosing parentheses were
removed to produce simplified versions of the bracketed expressions (e.g (NML (NNP Nesbitt) (NNP
Thomson) (NNP Deacon) ) becomes (Nesbitt (Thomson Deacon)) ).
Vadas and Curran (2007a) used a Named Entity annotator to suggest bracketing to the human anno-
tators (who could accept or reject them). The entity types used were the ones defined by Weischedel
and Ada Brunstein (2005) (e.g. Person, Facility, Organization, Nationality, Product, Event, etc). Named
73
entities could be kept as-is by the annotators or could be bracketed if deemed compositional. Annotators
were also instructed to use a default right-bracketing (implicit in Penn Treebank) for difficult decision.
In our dataset, we transformed the ones left as-is into right-bracketed in order to have all expressions
fully bracketed. This process might seem controversial, as it assumes compositionality of all named
entities, which for sure, is a wrong hypothesis. The alternative, though, would require the bracketing al-
gorithm to recognize named entities, which we consider outside the scope of this research. Furthermore,
it would also be wrong to assume all named entities are non-compositional. For example New York Stock
Exchange is clearly compositional, and a Named Entity Tagger based on Wikipedia would easily iden-
tify it as a named entity (although the use of Wikipedia as a source of named entities is also debatable).
Clearly, no solution is satisfying. We opted for the approximation which provided a fully bracketed gold
standard to which our results could be compared. We are aware that this will have a negative impact, in
some cases, on our results.
The extraction produced a total 6,600 examples from which we removed duplicate expressions, yield-
ing a corpus of 4,749 unique expressions. Among those unique expressions, 2,889 (60.95%) were three
words long (e.g. Mary Washington College), 1,270 (26.79%) had four words (e.g. standardized achieve-
ment tests scores), 413 (8.71%) with five words (e.g. annual gross domestic product growth) and the
remaining longer expressions (up to nine words) covered around 3.5% of the dataset
1
.
4 Bracketing method
As in the work of Pitler et al. (2010), our bracketing algorithm takes into account all possible word
pairs within the noun compound. This differs from Barker?s algorithm Barker (1998) used in Vadas
and Curran (2007b) which only uses local information, three-words at a time, in a right-to-left moving
window. We briefly present our algorithm below and refer the reader to M?enard and Barri`ere (2014) for
a more detailed explanation.
First, a list (L1) is created to contain every word pair that can be generated, in order, from an ex-
pression. For example, a list L1 {(A,B), (A,C), (A,D), (B,C), (B,D), (C,D)} would be created from
expression ?A B C D?. Second, a dependency score needs to be assigned to each pair. Our bracketing
algorithm actually builds a dependency tree and requires these dependency scores. We make the assump-
tion that dependencies are implicitly directed left-to-right. This is an oversimplification, as there are a
few cases, such as Vitamin C or Cafe Vienna, pointed in (Nakov, 2013), where the direction is reversed.
Furthermore, this hypothesis is valid only for English and renders our algorithm less applicable to other
languages. Although fair for English, this hypothesis should be revisited in future work.
The next step is building a final list of dependencies (L2) to represent the full dependency tree. To do
so, the algorithm repeatedly selects from L1 the word pair with the maximum score and adds it to L2
only if both (a) the modifier has not already been used, and (b) the new pair does not create a crossing
of modifier/head pairs in the expression. For example, if L2 already contains (AB)(C(DE)), then (BD)
would create an invalid crossing and is not accepted. The selection of pairs from L1 ends when all words
from the expression, except for the right-most one, are used as modifiers in L2.
Our algorithm is greedy and considers only the best score at every step. We have experimented with
randomized greedy algorithms as well, choosing randomly between top N scores at each step, but since
results did not improve, we do not report on them in the current article. The bracketing algorithm favours
high dependency scores without consideration for the actual distance between word pairs in the source
expression. This helps linking far reaching dependencies in noun compounds, but might also force some
strong association between two distant words without regard to the soundness of using nearer words.
5 Implementing an association model using Wikipedia
Our association model contains three types of association: lexical, relational and coordinate. Each one
will be measured using Wikipedia through different approximation strategies. The challenge is the inte-
gration of the association model with the bracketing algorithm. We mainly explore a solution of score
1
We describe our dataset in more details in M?enard and Barri`ere (2014), and our extraction method is published as part of
the LREC resources sharing effort.
74
modulation which does not require the bracketing algorithm to be modified but rather use the three asso-
ciation scores to modulate the dependency score required by the bracketing algorithm. We present below
a basic dependency score, and then different strategies to transform the three types of association into
modulation factors on that dependency score.
Basic dependency association: Based simply on the co-occurrence of two words in a corpus, this basic
association will be influenced by the actual corpus (domain and size), and the association measure used.
In our current experiment, Wikipedia pages are merged into a large corpus (47 Gigabytes) covering
multiple domains. As for the association measure, we compare Dice and Point-Wise Mutual
Information (PMI), although many more exist in the literature. Co-occurrence is not a direct measure of
dependency, it is an approximation. A true dependency measure would require a syntactic analysis
(using a link parser) of the whole corpus. We will explore this idea in future work.
Relational association: The relational association is a refinement to the dependency association. In
semantic analysis of noun compounds, an important goal is to characterize the nature of the dependence
between its words, such as cause, purpose, location, etc (see work by Girju et al. (2005), Nakov and
Hearst (2005), Nastase et al. (2013) among many). Here, we do not require the identity relations, but
rather search for indications of the relational status of a word pair. In our current implementation,
relational association is na??vely determined by the presence of a preposition between two nouns. We
use the prepositions: about, at, by, for, from, in, of, on, to, with. We search in the corpus for patterns
such as ?N1 at N2? and ?N1 for N2?, etc. The frequency of these will be used to boost the basic
dependency association scores.
Coordinate association: Proximity sometimes refers implicitly to coordination, as for example the
words cotton and polyester in the expression cotton polyester shirt. Explicit external evidence that these
words often co-occur in a coordination relation could lower their dependency association in expressions
such as cotton polyester shirt. To gather such evidence, we measure the frequency of explicit
coordination between word pairs in Wikipedia. The common conjunctions: or, and, nor are used. We
search in the corpus for patterns such as ?N1 or N2? and ?N1 and N2?, etc. Contrarily to relational
associations boosting the basic dependency association scores, coordinate associations should attenuate
the dependency scores.
Lexical association: Based on the idea that many compounds, even named entities, are compositional,
we want to determine the likeliness that a subexpression in a compound forms itself a lexical unit with a
meaning of its own. To do so, we use a first approach requiring a set of corpus-based statistical
approximations and a second approach requiring Wikipedia page titles.
? Statistical approximation: The presence of determiners (a, an, the) and plural forms are used as
statistical evidence of lexical association. For example, starting with expression cotton polyester
shirt, corpus analysis shows that the cotton shirts is frequent, which can be used to boost the depen-
dency score between cotton and shirt. On the other hand, the cotton polyesters will be much less
frequent. The presence of indicators (determiners and plurals) can be used independently, search-
ing for patterns such as ?the N1 N2? and ?N1 plural(N2)?, or together for patterns such as ?a N1
plural(N2)?.
? Presence in Wikipedia: A second strong indicator of lexical association for a word pair is its pres-
ence in an encyclopedic resource (Wikipedia). In fact, not only word pairs, but for any subcompound
of two or more words are considered for look-up as Wikipedia entries. Since we now have lexical
units of any length, rather than word pairs, our score modulation is not as straight forward. We
thought of two different strategies.
The first strategy, in line with score modulation, uses all word pairs found in the lexical units to
boost dependency scores. For example, assuming the compound ABCDE, with [BCD] found as
a lexical unit in Wikipedia. Then, the association scores of pairs [BC],[CD],[BD] are boosted
equally (uniform boost). This will not help for any internal bracketing of [BCD], but will reinforce
the fact that [BCD] should stay together within the larger compound. A variant to uniform boost
75
Gold Evaluated
Gold elements
Strict
Lenient
Subexpression Binary tree Subexpression Binary tree
(a b) c (a b) c (a b) a-b, b-c 100% 100% 100%
(a b) c a (b c) (a b) a-b, b-c 0% 0% 50%
(a b) (c d) (a b) (c d) (a b), (c d) a-b, c-d, b-d 100% 100% 100%
(a b) (c d) a (b (c d)) (a b), (c d) a-b, b-d, c-d 0% 50% 66.6%
(((a b) c) d) (e f) a (b (c (d (e f)))) (a b), (a b c), (a b
c d), (e f)
a-b, b-c, c-d, d-f, e-f 0% 25% 40%
Average: 40% 55% 71.3%
Table 1: Applied examples of evaluation metrics.
is a right-attachment boost to mimic the default right bracketing in the gold standard for the longer
units.
The second strategy is one of compound segmentation, in which lexical units found become seg-
mentation constraints on the bracketing algorithm. Association scores are them measured between
pairs of lexical units instead of between words pairs. We also try to minimize the number of en-
tities within the compound. For example, assuming again we wish to bracket compound ABCDE,
and find the possible three segmentations into lexical units using Wikipedia: (1)[AB][CDE], (2)
[AB][CD][E], (3) [ABC][DE]. Only segmentations (1) and (3) are kept since they have two lexical
units and not three. The association scores must then be calculated between pairs of lexical units,
and within each lexical unit containing three words or more (to perform full bracketing). Bracketing
within a lexical unit will be performed using the same bracketing methods described above. Brack-
eting between lexical units requires association scores between these units. For doing so, using the
example above, we will search in corpus for cooccurrences of [AB] with [CDE] for segmentation
(1), and [ABC] with [DE] for segmentation (3). Since statistics on longer units will be sparse in the
corpus, we will also measure association scores between heads of the lexical units. For example, in
segmentation (1) the association between heads [B] and [E] would be measured.
6 Evaluation metrics
Three methods are used to evaluate performances: strict, lenient binary tree and lenient sub-expression.
The strict evaluation verifies that all bracketed groups of the gold-standard expression are exactly the
same as those found in the evaluated expression, providing a score of 1 or 0. The two lenient evaluations
compute the ratio between the number of matching groups from a gold expression with those found in
the evaluated expression. In other words, lenient is the recall score based on the gold elements.
In lenient binary tree, each fully bracketed expression is parsed as a binary tree. From that tree, each
modifier/head pair becomes a basic evaluation element. For example, in (A (B C)), two elements A-C and
B-C are used for the evaluation process. This method boosts the performance level on most expressions,
but especially those composed of three words, for which a minimum 50% is always obtained.
In lenient sub-expression, evaluation elements are rather sub-expressions to provide a more balanced
score. The method extracts each bracketed group except the top-level group and removes all internal
parentheses from each one. Thus, from the expression (((A B) C) D), the method extracts (A B) and (A B
C). The two resulting sub-expressions become gold elements for comparison with those obtained from
the evaluated expression. Table 1 shows five examples illustrating score variations using the different
methods on expressions of different length.
7 Results
In section 5, we described various approaches to capture, using Wikipedia, the different types of asso-
ciation proposed in our model: lexical, relational and coordinate. We also presented two solutions for
combining this more complex model with the bracketing algorithm of section 4 which expects a single
type of association, that of dependency. Below, using a dataset of 4749 compound nouns, presented in
section 3, we report on some interesting results.
76
Resource Algorithm Strict Lenient
Wikipedia
Dice 55.00% 67.63%
PMI 56.25% 68.98%
Google Web Ngram
Dice 51.80% 63.90%
PMI 60.41% 72.47%
Table 2: Comparing basic association scores in Wikipedia and Google Web.
7.1 Baseline
To measure the impact of combining different types of associations, we first establish our baseline as
the bracketing results obtained solely with the basic dependency association scores, as measured on
Wikipedia. To further validate our baseline, we wish to compare it to the literature. The closest research
providing comparable results on large compounds are Vadas and Curran (2007b) and Pitler et al. (2010),
although both focus on supervised approaches, and furthermore, Vadas and Curran (2007b) use con-
textual features, assuming the noun compounds are to be bracketed in context. Still, Vadas and Curran
(2007b) give some baseline results for an unsupervised approach (the supervised approach was promoted
in their article) to which we compare our baseline. Far from an ideal comparison (which would be with
the exact same dataset and setting), it still provides some indication of the performance of our baseline.
They report exact match for complex NPs to be 54.66% for default right branching, 32.66% chi-square
dependency and 35.86% chi-square adjacency. As we obtain around 55% for strict matches (see Table 2,
first row), we seem above the unsupervised approach they used, which combined their association scores
within an implementation of Barker?s algorithm.
To confirm that merged Wikipedia pages form a large enough corpus in comparison to most recent
work on noun bracketing using web counts (see section 2), we use the English Google Web Ngrams (Lin
et al., 2010) (GWN), a 1T corpus contains n-gram counts collected from 1 trillion words of web text, and
performed our bracketing algorithm with Wikipedia basic dependency scores, and GWN bigram scores.
As shown in Table 2, results are comparable, slightly higher for Dice (55.0% compared to 51.8%) and
slightly lower for PMI (56.25% compared to 60.41%).
Throughout our experiments, we have continued using both association measures (Dice and PMI),
as well as performing both Barker?s algorithm and our bracketing algorithm, but since our algorithm
with Dice always gave better results (contrarily to the baseline in which PMI performed better), we only
present those results in the following sections.
7.2 Corpus-based improvements
In Section 5, we described how the use of stop words (conjunctions, prepositions, determiners) com-
bined with word pairs of interest could respectively modulate the basic dependency association scores to
emphasize coordinate, relational, or lexical association.
For lexical association, word pairs preceded by determiners were searched for in the corpus. We tried
different ways of combining association scores between the form with the determiner (?the N1 N2?) and
the word pair only (N1 N2), such as adding scores, keeping the maximum or minimum score. As well,
we tried different ways of combining the scores obtained with the different determiners (a, the, an), again
adding, keeping the maximum or the minimum score. Unfortunately, none of these variations helped.
We also experimented with searching for plural forms in corpus to emphasize lexical association, which
provided a small increase to the baseline as shown in Table 3.
For relational association, we searched for noun pairs with prepositions. The same merging strategies
given above for the use of determiners we tried. The best configuration uses a relational boosting strategy
of adding scores and a preposition merging strategy of using the minimum score among all prepositions.
Even with the best combination, overall, the improvement is marginal as shown in Table 3.
For coordinate association, we searched for noun pairs with conjunctions. Similarly to determiners
and prepositions, we tried different merging strategies. Since we are interested in an attenuation of the
dependency score with the coordinate score, our merging strategies were of subtracting scores or using
77
Option Strict Lenient Binary
Baseline 0.5500 0.6763 0.8132
Only including lexical association 0.5842 0.7106 0.8321
Only including relational association 0.5854 0.7093 0.8314
Only including coordinate association 0.5867 0.7110 0.8325
Table 3: Impact of corpus-based statistics (lexical, relational, coordinate association)
Option Strict Lenient Binary
Baseline 0.5500 0.6763 0.8132
Using entity-based refinement (uniform distribution) 0.6020 0.7257 0.8408
Using entity-based compound segmentation 0.7316 0.8213 0.8940
Table 4: Use of entities
the minimum. Again, unfortunately, improvement is marginal, as shown in Table 3.
7.3 Entity-based improvements
Our second approach to promote the lexical unit association score is to find which sub-expressions of
the compound are Wikipedia page titles. In Section 5, we suggested two strategies of using these entries,
either score modulation or compound segmentation.
In score modulation, we tried uniform boosting and right boosting as explained in Section 5, with
different boosting factors arbitrarily set between 10 and 100. The best result, obtained using a uniform
boost with a factor of 50 is presented in Table 4. There is a small improvement using this method. The
second strategy of compound segmentation is the one providing the most significant gain. An increase of
13% is obtained for the strict evaluation as shown in the last row of Table 4. For the sake of completeness,
we reran all the different variations and parameters which are used for performing the within and between
lexical units bracketing. The best configuration required that (1) basic dependency scores were actually
replaced by scores obtained by finding plural forms in the corpus (lexical association), (2) determiners
were not used, (3) the negative modulation from conjunctions (coordinate association) is obtained by
subtracting their frequency from the basic scores, (4) the positive modulation of prepositions (relational
association) is obtained by adding their frequency to the basic scores, (5) as different prepositions are
searched in corpus, the one with minimum frequency should be taken to alter basic scores, same for
conjunctions (6) the head of lexical units is used to measure the ?between units? association scores.
7.4 Result analysis
We first note some aspects of the gold standard that would affect the adequacy of our algorithm, and our
results.
? Noun compound status: A few examples in the dataset contain very generic adjectives, such as:
(certain ((natural resource) assets)), (such ((gas management) contracts)),(most (structural engi-
neers)), or ((too much) attention). These are not problematic in themselves, but our statistical
approximations for lexical, relational and coordinate associations are not adequate for these cases.
? Abbreviations: Some examples in the gold standard contain abbreviations, for example, (republican
(u.s. sen.)), ((american president) cos.) or (((el dorado) investment) co.). Again, these are not
problematic in themselves, but we have not yet implemented anything in our algorithm to manage
such cases.
? Ambiguity: Some examples found in the gold standard, such as ((sun ((life assurance) society)) plc)
or ((magnetic (resonance imaging)) equipment) are not obvious to us as being correct.
? Compositional examples: On the positive side, the dataset certainly contains many interesting ex-
amples, such as ((new england) ((medical center) hospitals)), ((northern california) (home prices)),
78
(world-wide ((advanced materials) operations)), (((lone star) spokesman) (michael london)), or
((magnetic (resonance imaging)) equipment). These examples are interesting because they show a
variety of right and left bracketing needed and a variety of named entities and terms of different
compositional nature. Research on compound bracketing is required for those examples, as they
will probably never end-up in even the most extensive lists of terms and named entities.
To better understand this dataset and the adequacy of our algorithm to its content, we intend, in future
work, to perform a manual sampling to determine the types of compounds, and the possible ambiguities.
As for Wikipedia as a resource, it is very valuable and contains many named entities (places, corpora-
tions, persons, etc), but it can never contain all entities. For example, we will find tadeusz mazowiecki to
help in bracketing (polish (prime minister)) (tadeusz mazowiecki), but we will not find bruno lucisano,
and wrongly bracket (((rome (film producer)) bruno) lucisano).
Independently of the gold standard and the resource used, our method has multiple limitations and
peculiarities. We believe that the general approach presented in this research is quite valid: a proposal
for the refinement of generic association scores into three subtypes of associations: lexical, relational
and coordinate associations. Nevertheless, the statistical approximations used for evaluating the different
association types should be revisited and refined.
8 Conclusion
Although bracketing of three-word expressions has been performed quite successfully using unsuper-
vised approaches with web-corpus resources ((Nakov and Hearst, 2005), (Vadas and Curran, 2007b)),
compound bracketing of large expressions remains a challenge.
One research direction, taken by Vadas and Curran (2007b) and Pitler et al. (2010) is to investigate
supervised learning approaches which will be able to build on the redundancy within the dataset. We
take a different direction, that of developing a more complex association model and exploring Wikipedia
in an unsupervised manner. Our research presents a noun compound bracketing algorithm which goes
beyond the adjacency / dependency models presented so far in the literature. We suggest a method that
takes into account different meaning of the proximity of two words, that of being part of the same lexical
unit, or being coordinates, or being in a relation.
Our current implementation of our association model certainly provides improvement on the basic
association scores, but it does not give a clear view of whether our corpus-based approximations are
correct or not. This deserves future investigation into how to best approximate with statistical measures
the notions of relational, coordinate and lexical associations. On the other hand, the use of Wikipedia
as an encyclopedic resource to help determine lexical units certainly provides the most gain and the best
results. On the dataset of 4749 compounds, our best results are 73.16% strict, 82.13% lenient and 89.40%
binary tree evaluation. Further use of the structure of Wikipedia can be investigated to help characterize
the different types of associations.
An important future goal is to refine the association model, and better anchor it in both linguistic and
computational linguistic traditions of noun compound analysis. The model deserves to be studied in its
own, regardless of its implementation, which here was performed using Wikipedia. A better understand-
ing of the model and its impact on noun compound bracketing might direct us to better choices for the
implementation of the association measures.
Lastly, similarly to other researchers who look at noun compound bracketing as the first step of se-
mantic analysis of NPs to elicit semantic relations (purpose, cause, location, etc) between subgroups of
words (Girju et al. (2005), Nastase et al. (2013)), we want to pursue our work into a more fine-grained un-
derstanding of noun compounds (Nakov, 2013), combining bracketing with the identification of specific
noun relations.
9 Acknowledgements
This research project is partly funded by an NSERC grant RDCPJ417968-11, titled Toward a second
generation of an automatic product coding system.
79
References
Ken Barker. 1998. A Trainable Bracketer for Noun Modifiers. In Twelfth Canadian Conference on Artificial
Intelligence (LNAI 1418).
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe. 2005. On the semantics of noun compounds.
Computer Speech & Language, 19(4):479?496, October.
Mirella Lapata, Portobello St, S Sheffield, and Frank Keller. 2004. The Web as a Baseline : Evaluating the
Performance of Unsupervised Web-based Models for a Range of NLP Tasks. In Proceedings of the HLT-
NAACL, pages 121?128.
Mark Lauer. 1995. Corpus statistics meet the noun compound: some empirical results. Proceedings of the 33rd
annual meeting on Association for Computational Linguistics, pages 47?54.
Judith Levi. 1978. The syntax and semantics of complex nominals.
D Lin, KW Church, H Ji, and S Sekine. 2010. New Tools for Web-Scale N-grams. LREC.
Mitchell P Marcus, Santorini Beatrice, and Mary A Marcinkiewicz. 1994. Building a large annotated corpus of
English: the Penn Treebank. Computational Linguistics, 19:313?330.
Pierre Andr?e M?enard and Caroline Barri`ere. 2014. Linked Open Data and Web Corpus Data for noun compound
bracketing. In Proceedings of the Ninth International Conference on Language Resources and Evaluation
(LREC?14), pages 702?709, Reykjavik, Iceland.
Preslav Nakov and M Hearst. 2005. Search engine statistics beyond the n-gram: Application to noun compound
bracketing. Proceedings of the Ninth Conference on Computational Natural Language Learning, (June):17?24.
Preslav Nakov. 2013. On the interpretation of noun compounds: Syntax, semantics, and entailment. Natural
Language Engineering, 19(03):291?330, May.
Vivi Nastase, Preslav Nakov, Diarmuid O Seaghdha, and Stan Szpakowicz. 2013. Semantic Relations Between
Nominals. Morgan and Claypool Publishers.
Emily Pitler, Shane Bergsma, Dekang Lin, and Kenneth Church. 2010. Using web-scale N-grams to improve
base NP parsing performance. Proceedings of the 23rd International Conference on Computational Linguistics,
(August):886?894.
David Vadas and JR Curran. 2007a. Adding noun phrase structure to the Penn Treebank. 45th Annual Meeting of
the Association of Computational Linguistics, (June):240?247.
David Vadas and JR Curran. 2007b. Large-scale supervised models for noun phrase bracketing. 10th Conference
of the Pacific Association for Computational Linguistics, (2004):104?112.
Ralph Weischedel and Ada Brunstein. 2005. BBN pronoun coreference and entity type corpus. Technical report,
Linguistic Data Consortium.
80
