Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 769?779,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploiting language models for visual recognition
Dieu-Thu Le
DISI, University of Trento
Povo, 38123, Italy
dle@disi.unitn.it
Jasper Uijlings
DISI, University of Trento
Povo, 38123, Italy
jrr@disi.unitn.it
Raffaella Bernardi
DISI, University of Trento
Povo, 38123, Italy
bernardi@disi.unitn.it
Abstract
The problem of learning language models
from large text corpora has been widely stud-
ied within the computational linguistic com-
munity. However, little is known about the
performance of these language models when
applied to the computer vision domain. In this
work, we compare representative models: a
window-based model, a topic model, a distri-
butional memory and a commonsense knowl-
edge database, ConceptNet, in two visual
recognition scenarios: human action recog-
nition and object prediction. We examine
whether the knowledge extracted from texts
through these models are compatible to the
knowledge represented in images. We de-
termine the usefulness of different language
models in aiding the two visual recognition
tasks. The study shows that the language
models built from general text corpora can be
used instead of expensive annotated images
and even outperform the image model when
testing on a big general dataset.
1 Introduction
Computational linguistics have created many tools
for automatic knowledge acquisition which have
been successfully applied in many tasks inside the
language domain, such as question answering, ma-
chine translation, semantic web, etc. In this paper
we ask whether such knowledge generalizes to the
observed reality outside the language domain, where
we use well-known image datasets as a proxy for ob-
served reality.
In particular, we aim to determine which language
model yields knowledge that is most suitable for use
in Computer Vision. Therefore we test a variety of
language models and a linguistically mined knowl-
edge base within two computer vision scenarios:
Human action recognition : Recognizing
<subject, verb, object> triples based on
objects (e.g., car, horse) and scenes (the place
that the actions occur, e.g., countryside, forest,
office) recognized in images. In this scenario,
we only consider images with human actions
so the ?human? subject is always present.
Objects in context : Predicting the most likely
identity of an object given its context as ex-
pressed in terms of co-occurring objects.
Computer vision can greatly benefit from natural
language processing as learning from images re-
quires a prohibitively expensive annotation effort. A
major goal of natural language processing is to ob-
tain general knowledge from text and in this paper
we test which model provides the best knowledge
for use in the visual domain.
Within the two visual scenarios, we compare three
state-of-the-art language models and a knowledge
base: (1) A window-based model, which counts
co-occurrence frequencies within a fixed window;
(2) R-LDA (Se?aghdha, 2010), an extension of LDA
that enables generation of joint probabilities; (3)
TypeDM (Baroni and Lenci, 2010), a strong Distri-
butional Memory model; (4) ConceptNet (Speer and
Havasi, 2013), an automatically generated semantic
graph containing concepts with their relations.
We test the language models in two ways: (1) We
directly compare the statistics of the linguistic mod-
els with statistics extracted from the visual domain.
769
(2) We compare the linguistic models inside the two
computer vision applications, leading to a direct es-
timation of their usefulness.
To summarize, our main research questions are:
(1) Is the knowledge from language compatible with
the knowledge from vision? (2) Can the knowl-
edge extracted from language help in computer vi-
sion scenarios?
2 Related Work
Using high level knowledge to aid image under-
standing has become a recent interest in the com-
puter vision community. Objects, actions and scenes
are detected and localized in images using low-
level features. This detection and localization pro-
cess is guided by reasoning and knowledge. Such
knowledge is employed to disambiguate locations
between objects in (Gupta and Davis, 2008). From
the defined relationships between nouns (e.g., above,
below, brighter, smaller), the system constrains
which region in an image corresponds to which ob-
ject/noun. Similarly, (Srikanth et al, 2005) ex-
ploit ontologies extracted from WordNet to asso-
ciate words and images and image regions. (Yu
et al, 2011) employ relations between scenes and
objects introducing an active model to recognize
scenes through objects. The reasoning knowledge
limits the detector to search for an object within a
particular region rather than on the whole image.
Language models have also been employed to
generate descriptive sentences for images. (Ushiku
et al, 2012) introduce an online learning method for
multi-keyphrase estimation to generate a sentence
using a grammar model to describe an image. Simi-
larly, from objects and scenes detected in an image,
(Yang et al, 2011) estimated a sentence structure to
generate a sentence description composed of a noun,
verb, scene and preposition.
The studies most similar to ours are (Teo et al,
2012) and (Lampert et al, 2009). In (Teo et al,
2012), the Gigaword corpus is used to extract rela-
tionships between tools and actions (e.g., knife - cut,
cup - drink) by counting their co-occurences. These
relationships are used to constrain and select the
most plausible actions within a predefined set of ac-
tions in cooking videos. Instead of using this knowl-
edge as a guidance during recognition, we compare
different language models and build a general frame-
work that is able to detect unseen actions through
their components (verb - object - scene), hence our
method does not limit the number of actions in im-
ages. (Lampert et al, 2009) use attributes of nouns
(e.g., an animal: white, eat fish, water, etc.). They
can detect animals without having seen training ex-
amples by manually defining the attributes of the tar-
get animal. In this work, rather than relying on man-
ual definitions, our aim is to find the best language
models built automatically from available corpora to
extract relations from natural language.
Currently, human action recognition is popular
and mostly studied in video using the Bag-of-Visual-
Words method (Delaitre et al, 2010; Everts et al,
2013; Kuehne et al, 2012; Reddy and Shah, 2012;
Wang et al, 2013). In this method one extracts small
local visual patches of, say, 24 by 24 pixels by 10
frames at every 12th pixel at every 5th frame. For
each patch local gradients or local movement (opti-
cal flow) histograms are calculated. Then these local
visual features are mapped to abstract, predefined
?visual words?, previously obtained using k-means
clustering on a set of random features. While results
are good, there are two main drawbacks with this
approach. First of all, human actions are semantic
and more naturally recognized through their compo-
nents (human, objects, scene) rather than through a
bag of local gradient/motion patterns. Hence we use
a component-based method for human action recog-
nition. Second, the number of possible human ac-
tions is huge (the number of objects times the num-
ber of verbs). Obtaining annotated visual examples
for each action is therefore prohibitively expensive.
So we learn from language models how components
combine into human actions.
3 Two Visual Recognition Scenarios
We now describe the two computer vision scenarios:
human action recognition and objects in context.
3.1 Human Action Recognition
We want to identify a human action, defined as a
<subject, verb, object> triple. We do this by recog-
nizing the human, the object, and the scene and then
determine the most likely verb based on these com-
ponents. Scenes are only used here as features for
770
predicting/disambiguating the human action and the
final task is to define the human action triple. As in
most work in human action recognition, we simplify
the problem by considering only images in which
human actions occur. This means that a human is
always present, leaving the problem of predicting
the verb given the object and the scene. While this
may seem like a strong assumption, the possibility
of having no action in the image at all is largely un-
explored in computer vision due to its difficulty.
	

	
		


 



	

Proceedings of the 2012 Student Research Workshop, pages 19?24,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Query classification using topic models and support vector machine
Dieu-Thu Le
University of Trento, Italy
dieuthu.le@disi.unitn.it
Raffaella Bernardi
University of Trento, Italy
bernardi@disi.unitn.it
Abstract
This paper describes a query classification
system for a specialized domain. We take as
a case study queries asked to a search engine
of an art, cultural and history library and clas-
sify them against the library cataloguing cate-
gories. We show how click-through links, i.e.,
the links that a user clicks after submitting a
query, can be exploited for extracting informa-
tion useful to enrich the query as well as for
creating the training set for a machine learn-
ing based classifier. Moreover, we show how
Topic Model can be exploited to further enrich
the query with hidden topics induced from the
library meta-data. The experimental evalua-
tions show that this system considerably out-
performs a matching and ranking classifica-
tion approach, where queries (and categories)
were also enriched with similar information.
1 Introduction
Query classification (QC) is the task of automati-
cally labeling user queries into a given target tax-
onomy. Providing query classification can help the
information providers understand users? needs based
on the categories that the users are searching for.
The main challenges of this task come from the na-
ture of user queries, which are usually very short and
ambiguous. Since queries contain only several to a
dozen words, a QC system often requires either a
rather large training set or an enrichment of queries
with other information (Shen et al, 2006a), (Broder
et al, 2007).
This study will focus on QC in art, culture and
history domain, using the Bridgeman art library1, al-
though our framework is general enough to be used
in different domains. Manually creating a training
1http://www.bridgemanart.com/
set of queries to build a classifier in a specific do-
main is very time-consuming. In this study, we
will describe our method of automatically creating
a training set based on the click-through links and
how we build an SVM (Support Vector Machine)
classifier with the integration of enriched informa-
tion. In (Le et al, 2011), it has been shown that
click-through information and topic models are use-
ful for query enrichment when the ultimate goal is
query classification. We will follow this enrichment
step, but integrate this information into a SVM clas-
sifier instead of using matching and ranking between
queries and categories as in (Le et al, 2011).
The purpose of this paper is to determine (1)
whether the query enrichment with click-though in-
formation and hidden topics is useful for a machine
learning query classification system using SVM; and
(2) whether integrating this enriched information
into a machine learning classifier can perform bet-
ter than the matching and ranking system.
In the next section, we will briefly review the
main streams of related work in QC. In section 3,
we will describe the Bridgeman art library. Sec-
tion 4 accounts for our proposed query classifica-
tion framework. In section 5, we will present our
experiment and evaluation. Section 6 concludes by
discussing our main achievements and proposing fu-
ture work.
2 Related work
Initial studies in QC classify queries into several
different types based on the information needed by
the user. (Broder, 2002) considered three different
types of queries: informational queries, navigational
queries and transactional queries. This stream of
study focuses on the type of the queries, rather than
topical classification of the queries.
Another stream of work deals with the problem
19
of classifying queries into a more complex taxon-
omy containing different topics. Our study falls into
this second stream. To classify queries consider-
ing their meaning, some work considered only in-
formation available in queries (e.g., (Beitzel et al,
2005) only used terms in queries). Some other work
has attempted to enrich queries with information
from external online dataset, e.g., web pages (Shen
et al, 2006a; Broder et al, 2007) and web direc-
tories (Shen et al, 2006b). Our work is similar
to their in the idea of exploiting additional dataset.
However, instead of using search engines as a way
of collecting relevant documents, we use the meta-
data of the library itself as a reference set. Further-
more, we employ topic models to analyze topics for
queries, rather than enriching queries with words se-
lected from those webpages directly as in (Shen et
al., 2006a; Broder et al, 2007).
The context of a given query can provide use-
ful information to determine its categories. Previ-
ous studies have confirmed the importance of search
context in QC. (Cao et al, 2009) considered the con-
text to be both previous queries within the same ses-
sion and pages of the clicked urls. In our approach,
we will also consider click through information to
enrich the queries and analyze topics.
In (Le et al, 2011), queries and categories are en-
riched with both information mined from the click-
through links as well as topics derived from a topic
model estimated from the library metadata. Sub-
sequently, the queries are mapped to the categories
based on their cosine similarity. Our proposed ap-
proach differs from (Le et al, 2011) in three re-
spects: (1) we enrich the queries, but not the cat-
egories (2) we employ a machine learning system
and integrate this enriched information as features to
learn an SVM classifier (3) we assume that the cate-
gory of a query is closely related to the category of
the corresponding click-through link, hence we au-
tomatically create a training data for the SVM clas-
sifier by analyzing the query log.
3 Bridgeman Art Library
Bridgeman Art Library (BAL)2 is one of the world?s
top image libraries for art, culture and history. It
contains images from over 8,000 collections and
2http://www.bridgemanart.com
more than 29,000 artists, providing a central source
of fine art for image users.
Works of art in the library have been annotated
with titles and keywords. Some of them are catego-
rized into a two-level taxonomy, a more fine-grained
classification of the Bridgeman browse menu. In our
study, we do not use the image itself but only the in-
formation associated with it, i.e., the title, keywords
and categories. We will take the 55 top-level cate-
gories from this taxonomy, which have been orga-
nized by a domain expert, as our target taxonomy.
4 Building QC using topic models and
SVM
Following (Le et al, 2011), we enrich queries both
with the information mined from the library via
click-through links and the information collected
from the library metadata via topic modeling. To
perform the query enrichment with topics derived
from the library metadata, there are several impor-
tant steps:
? Collecting and organizing the library metadata as
a reference set: the library metadata contains the in-
formation about artworks that have been annotated
by experts. To take advantage of this information
automatically, we collected all annotated artworks
and organized them by their given categories.
? Estimating a topic model for this reference set:
This step is performed using hidden topic analysis
models. In this framework, we choose to use latent
dirichlet alocation, LDA (Blei et al, 2003b).
? Analyzing topics for queries and integrating topics
into data for both the training set and new queries:
After the reference set has been analyzed using topic
models, it will be used to infer topics for queries.
The topic model will then be integrated into the data
to build a classifier.
4.1 Query enrichment via click-through links
We automatically extracted click-through links from
the query log (which provides us with the title of
the image that the user clicks) to enrich the query,
represented as a vector ??qi , with the title of one
randomly-chosen click-through associated with it.
To further exploit the click-through link, we find
the corresponding artwork and extract its keywords:
??qi ?
??
ti ?
???
kwi, where
??
ti ,
???
kwi are the vectors of words
20
in the title and keywords respectively.
4.2 Hidden Topic Models
The underlying idea is based upon a probabilis-
tic procedure of generating a new set of artworks,
where each set refers to titles and keywords of
all artworks in a category: First, each set ??wm
= (wm,n)
Nm
n=1 is generated by sampling a distribu-
tion over topics
??
?m from a Dirichlet distribution
(Dir(??? )), where Nm is the number of words in
that set m. After that, the topic assignment for
each observed word wm,n is performed by sam-
pling a word place holder zm,n from a multino-
mial distribution (Mult(
??
?m)). Then a word wm,n is
picked by sampling from the multinomial distribu-
tion (Mult(??? zm,n)). This process is repeated until
all K topics have been generated for the whole col-
lection.
Table 1: Generation process for LDA
?M : the total number of artwork sets
?K: the number of (hidden/latent) topics
? V : vocabulary size
? ??? ,
??
? : Dirichlet parameters
?
??
?m: topic distribution for document m
? ??? k: word distribution for topic k
? Nm: the length of document m
? zm,n: topic index of nth word in document m
? wm,n: a particular word for word placeholder [m, n]
? ? = {
??
?m}Mm=1: a M ?K matrix
? ? = {??? k}Kk=1: a K ? V matrix
In order to estimate parameters for LDA (i.e.,
the set of topics and their word probabilities ?
and the particular topic mixture of each document
?), different inference techniques can be used,
such as variational Bayes (Blei et al, 2003b), or
Gibbs sampling (Heinrich, 2004). In this work,
we will use Gibbs sampling following the descrip-
tion given in (Heinrich, 2004). Generally, the topic
assignment of a particular word t is computed as:
p(zi=k|
??z ?i,
??w)=
n(t)k,?i + ?t
[
?V
v=1 n
(v)
k +?v]?1
n(k)m,?i + ?k
[
?K
j=1 n
(j)
m +?j ]?1
(1)
where n(t)k,?i is the number of times the word t is
assigned to topic k except the current assignment;
?V
v=1 n
(v)
k ?1 is the total number of words assigned
to topic k except the current assignment; n(k)m,?i is the
number of words in set m assigned to topic k except
the current assignment; and
?K
j=1 n
(j)
m ? 1 is the
total number of words in set m except the current
word t. In normal cases, Dirichlet parameters ??? ,
and
??
? are symmetric, that is, all ?k (k = 1..K) are
the same, and similarly for ?v (v = 1..V ).
4.3 Hidden topic analysis of the Bridgeman
metadata
The Bridgeman metadata contains information
about artworks in the library that have been anno-
tated by the librarians. We extracted titles and key-
words of each artwork, those for which we had a
query with a click-through link corresponding to it,
and grouped them together by their sub-categories.
Each group is considered as a document ??wm =
(wm,n)
Nm
n=1, with the number of total documents M
= 732 and the vocabulary size V = 136K words. In
this experiment, we fix the number of topics K =
100. We used the GibbsLDA++ implementation3 to
estimate this topic model.
4.4 Building query classifier with hidden topics
Let Q? = {??qi ?}i=Ni=1 be the set of all queries en-
riched via the click-through links, where each en-
riched query is ??qi ? =
??qi ?
??
ti ?
???
kwi. We also per-
formed Gibbs sampling for all ??qi ? in order to esti-
mate its topic distribution
??
? i = {?i,1, . . . , ?i,K}
where the probability ?i,k of topic k in
??qi ? is com-
puted as:
?i,k =
n(k)i + ?k
?K
j=1 n
(j)
i + ?j
(2)
where n(k)i is the number of words in query i as-
signed to topic k and n(j)i is the total number of
words appearing in the enriched query i.
In order to integrate the topic distribution
??
?i =
{?i,1, . . . , ?i,K} into the vector of words
??qi ?
= {wi,1, wi,2, . . . , wi,Ni}, following (Phan et al,
2010), we only keep topics whose ?i,k is larger than
a threshold cut-off and use a scale parameter to do
the discretization for topics: the number of times
topic k integrated to ??qi ? is round(?i? scale). After
that, we build a Support Vector Machine classifier
using SVM light V2.204.
3http://gibbslda.sourceforge.net/
4http://svmlight.joachims.org/
21
5 Evaluation
In this section, we will describe our training set, gold
standard and the performance of our system in com-
parison with the one in (Le et al, 2011).
5.1 Training set
Manually annotating queries to create a training set
in this domain is a difficult task (e.g., it requires the
expert to search the query and look at the picture cor-
responding to the query, etc.). Therefore, we have
automatically generated a training set by exploiting
a 6-month query log as follow.
First, each query has been mapped to its click-
through information to extract the sub-category as-
sociated to the corresponding image. Then, from
this sub-category, we obtained its corresponding
top-cateogry (among the 55 we consider) as defined
in BAL taxonomy. The distribution of queries in
different categories varies quite a lot among the 55
target categories reflecting the artwork distribution
(e.g., there are many more artworks in the library be-
longing to the category ?Religion and Belief? than
to the category?Costume and Fashion?). We have
preserved such distribution over the target categories
when selecting randomly the 15,490 queries to build
our training set. After removing all punctuations and
stop words, we obtained a training set containing
50,337 words in total. Each word in this set serves
as a feature for the SVM classifier.
5.2 Test set
We used the test set of 1,049 queries used in (Le et
al., 2011), which is separate from the training set.
These queries have been manually annotated by a
BAL expert (up to 3 categories per query). Note that
these queries have also been selected automatically
while preserving the distribution over the target cat-
egories observed in the 6-month query log. We call
this the ?manual? gold standard. In addition, we also
made use of another gold standard obtained by map-
ping the click-through information of these queries
with their categories, similar to the way in which we
obtain the training set. We call this the ?via-CT?
gold standard.
5.3 Experimental settings
To evaluate the impact of click-though information
and topics in the classifier, we designed the follow-
ing experiments, where QR is the method without
any enrichment andQR-CT -HT is with the enrich-
ment via both click-through and hidden topics.
Setting Query enrichment
QR ??q
QR-HT ??q ?HT
QR-CT ??q ? = ??q +
??
t +
??
kw
QR-CT -HT ??q ? ?HT
? ??q : query
? ??q ?: query enriched with click-through information
?
??
t : click-through image?s title
?
??
kw: click-through image?s keywords
?HT : hidden topics from Bridgeman metadata
Table 2: Experimental Setting
Setting
Hits
Manual GS via-CT
# 1 # 2 # 3
?
Top 3 GS
QR 207 80 24 311 231
QR-HT 212 81 25 318 235
QR - CT 243 107 38 388 266
QR - CT - HT 289 136 49 474 323
Table 3: Results of query classification: number of cor-
rect categories found (for 1,049 queries)
Figure 1: The impact of click-through information with
matching-ranking (mr) and our approach (svm)
To answer our first research question, namely
whether click-through information and hidden top-
ics are useful for this query classifier, we examine
the number of correct categories found by the classi-
fier built both with and without the enrichment. The
results of the experiment are reported in Table 3. As
can be seen from the table, we notice that the click-
through information plays an important role. In par-
22
ticular, it increases the number of correct categories
found from 311 to 388 (compared with the manual
GS) and from 231 to 266 (using the via-CT GS).
To answer our second research question, namely
whether integrating the enriched information into a
machine learning classifier can perform better than
the matching and ranking method, we also compare
the results of our approach with the one in (Le et
al., 2011). Figure 1 shows the impact of the click-
through information for the SVM classifier (svm) in
comparison with the matching and ranking approach
(mr). Figure 2 shows the impact of the hidden topics
in both cases. We can see that in both cases our clas-
sifier outperforms the matching-ranking one con-
siderably (e.g., from 183 to 388 correct categories
found in the QR-CT-HT method).
Figure 2: The impact of hidden topics with matching-
ranking (mr) and our approach (svm)
However, in the case where we use only queries
without click-through information, we can see that
hidden topics do not bring a very strong impact (the
number of correct categories found only slightly in-
creases by 7 - using the ?manual? gold standard).
The result might come from the fact that this topic
model was built from the metadata, using only click-
through information, but has not been learned with
queries.
6 Conclusion
In this study, we have presented a machine learn-
ing classifier for query classification in an art im-
age archive. Since queries are usually very short,
thus difficult to classify, we first extend them with
their click-through information. Then, these queries
are further enriched with topics learned from the
BAL metadata following (Le et al, 2011). The re-
sult from this study has confirmed again the effect
of click-through information and hidden topics in
the query classification task using SVM. We have
also described our method of automatically creat-
ing a training set based on the selection of queries
mapped to the click-through links and their corre-
sponding available categories using a 6-month query
log. The result of this study has shown a consid-
erable increase in the performance of this approach
over the matching-ranking system reported in (Le et
al., 2011).
7 Future work
For future work, we are in the process of enhancing
our experimentation in several directions:
Considering more than one click-through image
per query: In this work, we have considered only
one category per query to create the training set,
while it might be more reasonable to take into ac-
count all click-through images of a given query. In
future work, we plan to enrich the queries with either
all click-through images or with the most relevant
one instead of randomly picking one click-through
image. In many cases, a click-through link is not
necessarily related to the meaning of a query (e.g.,
when users just randomly click on an image that they
find interesting). Thus, it might be useful to filter out
those click-through images that are not relevant.
Enriching queries with top hits returned by the
BAL search engine: In the query logs, there are
many queries that do not have an associated click-
through link. Hence, we plan to exploit other en-
richment method that do not rely on those links, in
particular we will try to exploit the information com-
ing from the top returned hits given by the library
search engine.
Analyzing queries in the same session: It has been
shown in some studies (Cao et al, 2009) that analyz-
ing queries in the same session can help determine
their categories. Our next step is to enrich a new
query with the information coming from the other
previous queries in the same session.
Optimizing LDA hyperparameters and topic
number selection: Currently, we fixed the num-
ber of topics K = 100, the Dirichlet hyperparame-
ters ? = 50/K = 0.5 and ? = 0.1 as in (Griffiths and
23
Steyvers, 2004). In the future, we will explore ways
to optimize these input values to see the effect of dif-
ferent topic models in our query classification task.
Exploiting visual features from the BAL images:
The BAL dataset provides an interesting case study
in which we plan to further analyze images to enrich
queries with their visual features. Combining text
and visual features has drawn a lot of attention in the
IR research community. We believe that exploiting
visual features from this art archive could lead to in-
teresting results in this specific domain. A possible
approach would be extracting visual features from
the click-through images and representing them to-
gether with textual features in a joint topic distribu-
tion (e.g., (Blei et al, 2003a; Li et al, 2010)).
Comparing system with other approaches: In the
future, we plan to compare our system with other
query classification systems and similar techniques
for query expansion in general. Furthermore, the
evaluation phase has not been carried out thoroughly
since it was difficult to compare the one-class output
with the gold-standard, where the number of correct
categories per query is not fixed. In the future, we
plan to exploit the output of our multi-class classi-
fier to assign up to three categories for each query
and compute the precision at n.
Acknowledgments
This work has been partially supported by the
GALATEAS project (http://www.galateas.eu/ ?
CIP-ICT PSP-2009-3-25430) funded by the Euro-
pean Union under the ICT PSP program.
References
Steven M. Beitzel, Eric C. Jensen, Ophir Frieder, and
David Grossman. 2005. Automatic web query clas-
sification using labeled and unlabeled training data. In
In Proceedings of the 28th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 581?582. ACM Press.
David M. Blei, Michael I, David M. Blei, and Michael I.
2003a. Modeling annotated data. In In Proc. of the
26th Intl. ACM SIGIR Conference.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003b. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Andrei Z. Broder, Marcus Fontoura, Evgeniy
Gabrilovich, Amruta Joshi, Vanja Josifovski, and
Tong Zhang. 2007. Robust classification of rare
queries using web knowledge. In Proceedings of the
30th annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ?07, pages 231?238, New York, NY, USA.
ACM.
Andrei Broder. 2002. A taxonomy of web search. SIGIR
Forum, 36:3?10, September.
Huanhuan Cao, Derek Hao Hu, Dou Shen, Daxi Jiang,
Jian-Tao Sun, Enhong Chen, and Qiang Yang. 2009.
Context-aware query classification. In SIGIR?09, The
32nd Annual ACM SIGIR Conference.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101 Suppl 1(Suppl 1):5228?5235.
Gregor Heinrich. 2004. Parameter estimation for text
analysis. Technical report.
Dieu-Thu Le, Raffaella Bernardi, and Edwin Vald. 2011.
Query classification via topic models for an art im-
age archive. In Recent Advances in Natural Language
Processing, RANLP, Bulgaria.
Li-Jia Li, Chong Wang, Yongwhan Lim, David Blei, and
Li Fei-Fei. 2010. Building and using a semantivisual
image hierarchy. In The Twenty-Third IEEE Confer-
ence on Computer Vision and Pattern Recognition, San
Francisco, CA, June.
Xuan-Hieu Phan, Cam-Tu Nguyen, Dieu-Thu Le, Le-
Minh Nguyen, Susumu Horiguchi, and Quang-Thuy
Ha. 2010. A hidden topic-based framework towards
building applications with short web documents. IEEE
Transactions on Knowledge and Data Engineering,
99(PrePrints).
Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey Junfeng Pan,
Kangheng Wu, Jie Yin, and Giang Yang. 2006a.
Query enrichment for web-query classification. ACM
Transactions on Information Systems, 24(3):320?352.
Dou Shen, Jian-Tao Sun, Qiang Yang, and Zheng Chen.
2006b. Building bridges for web query classification.
In SIGIR?06.
24
Proceedings of the 25th International Conference on Computational Linguistics, pages 17?24,
Dublin, Ireland, August 23-29 2014.
TUHOI: Trento Universal Human Object Interaction Dataset
Dieu-Thu Le
DISI, University of Trento
Povo, 38123, Italy
dle@disi.unitn.it
Jasper Uijlings
University of Trento, Italy
University of Edinburgh, Scotland
jrr.uijlings@ed.ac.uk
Raffaella Bernardi
DISI, University of Trento
Povo, 38123, Italy
bernardi@disi.unitn.it
Abstract
This paper describes the Trento Universal Human Object Interaction dataset, TUHOI, which is
dedicated to human object interactions in images.
1
Recognizing human actions is an important
yet challenging task. Most available datasets in this field are limited in numbers of actions and
objects. A large dataset with various actions and human object interactions is needed for training
and evaluating complicated and robust human action recognition systems, especially systems
that combine knowledge learned from language and vision. We introduce an image collection
with more than two thousand actions which have been annotated through crowdsourcing. We
review publicly available datasets, describe the annotation process of our image collection and
some statistics of this dataset. Finally, experimental results on the dataset including human action
recognition based on objects and an analysis of the relation between human-object positions in
images and prepositions in language are presented.
1 Introduction
Visual action recognition is generally studied on datasets with a limited number of predefined actions
represented in many training images or videos (Ikizler et al., 2008; Delaitre et al., 2011; Yao and Li,
2010; Yao et al., 2011). Common methods using holistic image or video representation such as Bag-
of-Words have achieved successful results in retrieval settings (Ayache and Quenot, 2008). Though
these predefined lists of actions are good for many computer vision problems, this cannot work when
one wants to recognize all possible actions. Firstly, the same action can be phrased in several ways.
Secondly, the number of actions that such systems would have to recognize in real life data is huge: the
number of possible interactions with all possible objects is bounded by the cartesian product of numbers
of verbs and objects. Therefore, the task of collecting images or videos of each individual action becomes
infeasible with this growing number. By necessity this means that for some actions only few examples
will be available. In this paper we want to enable studies in the direction of recognizing all possible
actions, for which we provide a new, suitable human-object interaction dataset.
A human action can be defined as a human, object, and the relation between them. Therefore, an action
is naturally recognized through its individual components. Recent advances in computer vision have led
to reasonable accuracy for object and human recognition, which makes recognizing the components
feasible. Additionally, language can help determining how components are combined. Furthermore,
the relative position between human and object can be used to disambiguate different human actions.
Perhaps prepositions in natural language can be linked to this relative position between the object and
human (e.g., step out of a car). To transfer this knowledge from language to vision, it is important that
the distribution of the visual actions are sampled similarly as the language data. This requirement is
fulfilled when the action frequencies in the dataset mirror the frequencies in which they occur in real life.
To sum up, we aim at building an image dataset which can (1) capture the distribution of human
interactions with objects in reality (if an action is more common that the other actions, that action is
also observed more frequently in the dataset than the others), (2) provide different ways of describing
1
Our dataset is available to download at http://disi.unitn.it/ dle/dataset/TUHOI.html
This work is licensed under a Creative Commons Attribution 4.0 International Licence. License details:
http://creativecommons.org/licenses/by/4.0/
17
an action for each image (there are many actions that can be phrased in several ways, for example: fix a
bike or repair a bike), (3) help with identifying different verb meanings (for example, the word ?riding?
has different implications for ?riding a horse?, ?riding a car?, and ?riding a skateboard?).
2 Available image datasets for human action recognition
A common approach to human action recognition is to exploit visual features using bag-of-features or
part-based representation and treat action recognition as a general classification problem (Delaitre et al.,
2010; Yao and Li, 2010; Wang et al., ; Laptev, 2005). For common actions, it has been shown that
learning the joint appearance of the human-object interaction can be beneficial (Sadeghi and Farhadi,
2011). Other studies recognize actions by their components such as objects, human poses, scenes (Gupta
et al., 2009; Yao et al., 2011): (Yao et al., 2011) jointly models attributes and parts, where attributes
are verbs and parts are objects and local body parts. These studies rely on suitable training data for
a set of predefined actions: (Gupta et al., 2009) tests on a 6 sport action dataset, (Yao and Li, 2010)
attempts to distinguish images where a human plays a musical instrument from images where he/she
does not, (Delaitre et al., 2010) classifies images to one of the seven every day actions, and (Yao et al.,
2011) introduces a dataset containing 40 human actions. Most of these datasets were obtained using web
search results such as Google, Bing, Flickr, etc. The number of images varies from 300 to more than 9K
images. A comparison of the publicly available datasets with respect to the number of actions and their
related objects is given in Table 1.
Dataset #images #objects #actions Examples of actions
Ikirler (Ikizler et al., 2008) 467 0 6 running, walking, throwing, crouching and kicking
Willow (Delaitre et al., 2011) 968 5 7 interaction with computer, photographing, riding bike
Sport dataset (Gupta et al., 2009) 300 4 6 tennis-forehand, tennis-serve, cricket bowling
Stanford 40 (Yao et al., 2011) 9532 31 40 ride horse, row boat, ride bike, cut vegetables
PPMI (Yao and Li, 2010) 4800 7 7 play violin, play guitar, play flute, play french horn
PASCAL (Everingham et al., 2012) 1221 6 10 jumping, playing instrument, riding horse
89 action dataset (Le et al., 2013) 2038 19 89 drive bus, sail boat, ride bike, fix bike, watch TV
TUHOI dataset 10805 189 2974 sit on chair, use computer, ride horse, play with dog
Table 1: A comparison of available human action datasets in terms of number of objects and actions
As can be seen in Table 1, the Stanford 40 action dataset contains quite a big number of images with
40 different actions. This dataset is good for visually training action recognizers since there are enough
images collected for each actions divided into training and test sets. There are some dataset in which
human action does not involved any object, these actions are for instance running, walking, or actions
where objects are not specified such as catching, throwing. These types of actions are not the target
domain of our dataset. We aim at recognizing the human object interactions based on objects. With the
same object, some actions are also more common than other actions: for example, sitting on a chair is
more commonly observed than standing on a chair. We want to capture such information in our dataset
which can reflect the human action distributions on common objects, aiming to sample human actions
related to objects in the visual world. Furthermore, how actions can be phrased in different ways, or how
verbs can have different meanings when interacting with different objects should also be considered.
Some actions can only be performed on some particular objects and are not applicable to some other
objects: a person can ride a horse, ride a bike, can feed a horse, but cannot feed a bike. This problem
of ambiguity and different word uses have been widely studied in computational linguistics, but have
received little attention from the computer vision community.
With the aim of creating a dataset that covers these requirements, we collect our dataset starting from
images where humans and objects co-occur together and define the actions we observe in each image
instead of collecting images for some predefined human actions. This way of annotating actions in
images is more natural and helps creating a more realistic dataset with various human actions that can
occur in images generally.
Recently, some good works attempted to generate descriptive sentences from images (Farhadi et al.,
2010; Kulkarni et al., 2011). In our dataset we focus on human actions, which, if present, are often the
main topic of interest within an image. As such, our dataset can be used as an important stepping stone
18
for generating full image descriptions as it allows for more rigorous evaluation than free-form text.
3 TUHOI, the new human action dataset
ImageNet is a hierarchical image database built upon the WordNet structure. The DET dataset in the
ImageNet large scale object recognition challenge 2013
2
contains 200 objects for training and evaluation.
With the idea of starting from images with humans and common objects, we chose to use this DET dataset
as a starting point to build our human action data.
3.1 The DET dataset: Object categories and labels
The 200 objects in the DET dataset are general, basic-level categories (e.g., monitor, waffle iron, sofa,
spatula, starfish). Each object corresponds to a synset (set of synonymous nouns) in WordNet. The
DET training set consists of single topic images where only the target object is annotated. As such,
most images only contain primarily the object of interest and few actions. It is good for learning object
classifiers but is not suitable for learning action recognition. In contrast, the validation dataset contains
various images where all object instances are annotated with a bounding box. Many of these images
contain actions. Therefore we start the annotation from the validation set.
Dataset #images #images #object #instances/object #?person?
having ?person? instances (min-max-median) instances
Training 395,909 9,877 345,854 438 - 73,799 - 660 18,258
Validation 20,121 5,791 55,502 31 - 12,823 - 111 12,823
Table 2: The statistics of the DET dataset
As can be seen in Table 2, there are 15,668 images having human and 31,081 human instances in
these images. We select only images having human since we want to annotate this dataset with human
object interactions. Objects related to clothes such as bathing cap, miniskirt, tie, etc. are not interesting
for human actions (most of the time, the action associated with these objects is ?to wear?). Therefore,
we excluded all these objects from the list of 200 objects above, which are: bathing cap, bow tie, bow,
brassiere, hat with a wide brim, helmet, maillot, miniskirt, neck brace, sunglasses, tie.
3.2 Human action annotation
Goal Our goal is to annotate these selected images containing humans and objects with their interac-
tions. Each human action is required to be associated with at least one of the given 200 object categories.
We used the Crowdflower, a crowdsourcing service for annotating these images. The Crowdflower anno-
tators are required to be English native speakers and they can use any vocabulary to describe the actions
as they wish. Every action is composed of a verb and an object (possibly with a preposition).
Annotation guideline For each image, given all object instances appearing in that image (together
with their bounding boxes), the annotator has been be asked to assign all human actions associated to
each of the object instance in the image (where ?no action? is also possible). Every human actions need
to have as object one of the object instances given in that image. For example, if the image has a bike
and a dog, the annotator will assign every human actions associated to ?bike? and ?dog?. Every image
has been annotated by at least 3 annotators, so that each action in the image can be described differently
by different people. Some examples of annotated images in our dataset are given in Figure 1.
3.3 Results of the annotation and some statistics
In total, there are 10,805 images, which have been annotated with 58,808 actions, of which 6,826 times
it has been annotated with ?no action? (11.6%). On average, there are 4.8 actions annotated for each
image (excluding ?no action?), of which there are 1.97 unique action/image. Some other statistics of the
dataset are given in Table 3: The number of unique verbs per object ranges from 1 (starfish, otter) to 158
(dog). As dogs occur very often in this image dataset (4,671 times), the number of actions associated to
it is also larger than other objects.
2
http://www.image-net.org/challenges/LSVRC/2013/
19
Figure 1: Examples of annotated images: Left: (1) play ping-pong, hold racket; (2) use laptop, hold computer mouse; (3)
use microphone, play accordion, play guitar, play violin; (4) talk on microphone, sit on sofa, pour pitcher; (5) play trombone;
(6) eat/suck popsicle; (7) listen/use/hear stethoscope; (8) ride bicycle, wear backpack; (9) swing/hold racket, hit tennis ball;
Right: (1) sit on chair, play violin; (2) wear diaper, sit on chair, squeeze/apply cream; (3) sit on chair, play cello; (4) hold/shake
maraca; (5) ride watercraft, wear swimming trunks; (6) cook/use stove, stir mushroom, hold spatula; (7) drive/row watercraft;
(8) sit on chair, pet dog, lay on sofa; (9) click/type on computer keyboard
Number of unique actions (verb + object): 2,974 actions
Number of unique verbs: 860 verbs
Verbs that are used most frequently (verb (#occurrences)): play (13043), hold (7731), ride (4765), sit (3535), sit on (1501)
drive (1491), wear (1441), eat (1175), hit (1168), pet (970), use (897), walk (787), stand (756)
touch (509), carry (507), blow (384), sail (323), kick (297), lead (290), throw (246), strum (239)
stand on (223), run (223)
Verbs that are used least frequently (occur only once): dirty, swing over, twist, beats, walks, ay, curl
face, shit, sail in, n?, see by, forge, draw, tag10, sling, rides, walk across, no image available, waving
drag, award, preform, strumb, died, land, unload, tricks, cooked, time, fasten, fall over, holed, leap over, pull up
Objects go with the largest number of verbs (object (#unique verbs)): dog (158), car (80), table (79), watercraft (68)
horizontal bar (56), chair (54), cart (52), whale (50), bicycle (48), cattle (42), soccer ball (41), balance beam (38)
band aid (38), motorcycle (37), flower pot (35), ladle (35), guitar (35), horse (35), ski (34), bus (34)
Objects that go with the least number of verbs (object (#unique verbs)): milk can (5), pitcher (5), scorpion (4), bear (4),
pretzel (4), sheep (4), frog (4), mushroom (4), printer (4), pineapple (4), ruler (3), guacamole (3), isopod (3), chime (3),
plate (rack (3), strawberry (3), porcupine (3), ant (3), toaster (3), bagel (3), jellyfish (3), dragonfly (2), lion (2), zebra (2),
goldfish (2), hamster (2), fig (2), squirrel (2), bee (2), centipede (2), koala (bear (2), snail (2), pomegranate (2), armadillo
(2), otter (1), starfish (1)
Table 3: Some statistics of the human action dataset
For some images, the annotators find many different ways to describe the action in the image. In our
data, a set of images was selected to be annotated by more than three people in order to facilitate sanity
checks. An example of such image which has been annotated by many people is given in Figure 2.
The annotators have found many verbs to describe the action: feeding, leading, running with, touching,
giving a treat to, etc.
Splitting training and test set For each object in our human action dataset, we split half of the images
for training and the other half is used for testing. The splitting process is done such that actions that
occur in test set also occur in training set to guarantee that the training set contains at least one image for
each action occurring in the test set.
Figure 2: Many different ways to describe an action in an image
Evaluating human action classification in our dataset To evaluate the performance of the human
action classification on this dataset, we use two different measurements: the accuracy and the traditional
20
precision, recall and F1 score. The accuracy reflects the percentage of predictions that are correct. We
calculate within how many images, the classifier assigns the correct actions for a given object i:
Accuracy
i
=
number of images that the classifier predicts correctly
total number of images
(1)
If the output of the classifier is one of the three annotated actions by human, then the action predicted
is considered to be correct. The accuracy of the whole system is the average accuracy over all objects,
with n is the total number of objects.
Accuracy =
?
n
i=1
Accuracy
i
n
(2)
This metric gives us the general performance of the system and easy to interpret. However, it gives
higher weights to actions that occur more often in the dataset. For example, if there are many actions
?ride bike? occurring in the dataset, the accuracy of the whole system depends mostly on the performance
of the class ?ride bike?. For actions that occur more rarely such as ?fix bike?, then the accuracy of the
class ?fix bike? will have little effect to the accuracy of the whole system.
To better analyze the results of the system and evaluate each action individually, we use the precision,
recall and F1 score for each class in the classifier. More specifically, as this classifier is the multi-class
classifier, these metrics are computed using a confusion matrix:
Precision
i
=
M
ii
?
j
M
ji
;Recall
i
=
M
ii
?
j
M
ij
(3)
where M
ij
is the value of the row i, column j in the confusion matrix. The confusion matrix is oriented
such that a given row of the matrix corresponds to the value of the ?truth?, i.e., correct actions assigned
by human, and a given column corresponds to the value of action assigned by the classifier. Finally, the
precision, recall and F1 score of the whole system are calculated as the average score over all actions.
4 Experiments
In this part, we use our newly collected dataset for building a general human action classifier based
on objects. We analyze the relative positions between humans and objects in each image and use this
information to help classifying human actions. Finally, we discuss the relations between human-object
positions with prepositions that are used in language for describing human actions.
4.1 Classifying human actions based on human-object positions
In this experiment, we used Forest Random classification method to classify an image to an action given
an object. The features used for this classifier are positions of the object and the person appearing in that
image. We compare this classifier when using position with a classifier using no position information to
see whether position information helps in classifying human actions and in which cases.
Extracting features To extract the features of objects and persons? positions in the images, we take
the bounding box of the first object instance annotated in that image. There are images with more than
one object instance (for example, there are several ?bike? in an image, so we do not know what ?bike?
we are talking about). We use the four coordinates of the bounding boxes of the object and person in the
image as features for the classifier.
Results of the classifier To compare whether position information can help in recognizing actions or
not, we design a naive classifier which learns from the probability of a verb given an object to assign an
action for each image from the training image dataset.
Accuracy Precision Recall F1
Without position 74.2% 0.40 0.26 0.29
With position 72.1% 0.65 0.29 0.36
Table 4: Results of the classifier with and without position information
21
Object Without position With position Object Without position With position
baseball 0.36 0.52 bus 0.57 0.73
face powder 0.33 1 hair spray 0.73 0.74
harmonica 0.07 0.97 horizontal bar 0.42 0.45
hotdog 0.29 0.57 motorcycle 0.80 0.82
turtle 0.43 0.71 water bottle 0.56 0.65
Table 5: Objects with higher accuracy when using position information
The results of the systems with and without position are report in Table 4. It shows that the accuracy of
the classifier without position is higher than when including the position (74.2% in compared to 72.1%).
However the precision, recall and F1 of the classifier using position are all higher than without position.
It?s due to the fact that the classifier without position blindly assigns each image to the most probable
action (i.e., actions that occurs most often with a given object learned from the training set), so it obtains
better overall accuracy when testing on all images. However, for other possible actions, this classifier
is unable to disambiguate actions and the performance of this classifier on less frequent actions is worst
than when including position information into the classifier. Generally, when taking into account all
possible actions, the position-based classifier has better average precision, recall and F1 score (28.6%
without position in compared with 35.8% using position).
To further analyze which objects and actions, the position information helps better, we compare the
accuracy of each individual objects. Table 5 reports main objects that have higher accuracy when using
position. We want to be able to predict which kind of actions that positions will help in recognizing them
through the knowledge we learn from language. This prediction will help us to learn how to include the
position information inside our human action recognizer since not all actions can be disambiguated by
positions. We divide the actions into two groups: one group for which we found position information
increase the classification results. Another group for which we found position information to decrease
the classification results.
4.2 From prepositions in language to relative positions between human and object in images
In this section, we want to learn how prepositions in language can be used to determine which positions
are useful in action classification, i.e., if they belong to the first group or the second group in the previous
experiment.
The relative positions between human and object in images are useful in analyzing their interactions.
For example, when a person is riding a horse, the person is usually on the top of the horse, and when
a person is feeding a horse, then the person is usually standing next to the horse. In spoken English,
sometimes prepositions can be used as an indicator to the relations between human and object positions.
We want to exploit the connection between human-object positions in images and prepositions that
link human, verb and object in language. Intuitively, if an action implies a strong positional relation
between the human and the object, we expect to find specific, distinguishing prepositions in language.
For example, in language you usually say ?sit on chair?, where the preposition on suggests a specific
spatial relation between the human and the chair. When an action does not imply a strong positional
relation, such as ?play?, we expect no specific prepositions.
Links in language models To test this hypothesis, we use TypeDM (Baroni and Lenci, 2010), a dis-
tributional memory that has been built from large scale text corpora. This model contains weighted
<word-link-word> tuples extracted from a dependency parse of corpora. The relations between words
are characterized by their ?link?. Some of these links are prepositions that connect verbs and objects
together. Examples of some tuples with word-link-word and their weights are provided in Figure 3.
Number of links and link entropy We want to determine whether there is any correlation between
human-object relative positions in images and the associated prepositions from language models. To do
this, we record two metrics: the number of links, where we count how many different links that connect
verbs and objects in the language model; and the entropy of each action A
i
verb-object pair (where the
human is implicit) is H(A
i
) defined by: H(A
i
) = ?
?
l
j
?L
i
p(l
j
)? log p(l
j
)
where L
i
is the set of all links that occur between verb and object of action i; p(l
j
) is the probability
of the link l
j
of the action A
i
:
22
Figure 3: Examples of word-link-word and their weights in the distributional memory
p(l
j
) =
weight(l
j
)
?
l
k
?L
i
weight(l
k
)
(4)
where weight(l
j
) is the weight given by the TypeDM of link j in action i.
Generally, the entropy for each action allows seeing whether a link is predictable for a given pair of
verb-object or not: when a link is predictable, the entropy is expected to be low (contain little infor-
mation), which might correspond to the case that the position information will be useful in predicting
actions and the other way around.
Number of links Entropy
Group 1 (position helps) 8 1.05
Group 2 (position doesn?t help) 15.3 1.36
Table 6: Actions that can be disambiguated by positions (Group 1) vs. actions that cannot be disam-
biguated by positions (Group 2) and their links in the language model
Results The result shown in Table 6: for the first group (with position is better), the average number
of links per relation (verb - object) is 8 and the average entropy is 1.05; the average number of links
per relation for the second group is almost twice more, 15.3, and their average entropy is also higher,
1.36. It shows that verbs which have many different ways of linking to an object might not have a
representative relative position between the person and object, hence more difficult to be classified based
on their positions. Verbs that have less links to an object tend to have more fixed relative positions
between persons and objects, hence it might be helpful to use position information in classification.
A qualitative analysis We further examine actions where this statement does not apply, i.e., actions
with high number of links and high entropy but belong to group 1 (position information helps) and
actions with low number of links and entropy belonging to group 2. For the first case, typical actions
which have high number of links/entropy are: ride car, ride bus, ride train, pull cart, light lamp. The large
number of links of these actions seem to come from relations which do not describe the human/object
interaction itself. For example, the links associated with ?ride bus? do not all actually refer to ?ride a
bus? but to ride another object in a position with respect to the bus: ride after bus, ride behind bus,
ride before bus. These cause extra links which are not related to the action itself. Similarly, actions
pull of/around/behind/below/on cart, there is another object which is moved to a specific position with
regards to the cart.
For the second case, examples of typical actions with low links but for which positions information
doesn?t help are hold harmonica, wear diaper, hold ladle, spread cream, hold racket, apply lipstick.
These actions are related to objects, for which their positions depends a lot on the human pose (e.g., hold
something). These actions in the language model do not contain many links as we expected: the most
possible link between hold, harmonica is in, which probably means hold harmonica in your hand.
Instead of looking at actions, we look into typical verbs where position information helps in classifying
actions and verbs where position information doesn?t help. For the first group, the most frequent verbs
are: chop, cut, drink, feed, lean, sit on, sleep, look at, put on, shake, shoot, wash, catch. For the second
group, the most frequent verbs are: clean, cook, lift, punch, sing, spray, spread. It can be observed that
verbs related to some particular poses or relative positions between human and object are better with
23
the position information (chop, drink, sit on, sleep), and verbs related to more various human poses and
unspecific are not helped by the position information (cook, sing, spray, clean).
Generally, there is a relation between prepositions in language and the relative positions between
human-object in images. Although this statement does not hold in every cases, for example when the
prepositions refer to the positions between another action (e.g., ride) and that object (e.g., after a bike),
this can be potentially solved by better NLP parsing and analyses of verb phrases. Furthermore, actions
that cannot be disambiguated by positions are usually related to different human poses, while actions that
have some particular human poses can be classified using position information.
5 Conclusion
In this paper, we have introduced the Trento Universal Human Object Interaction image dataset, TUHOI.
This dataset contains more than two thousand human actions associated with 189 common objects in
images. The main characteristics of this dataset are that it follows the actual human action distribution
observed in images, it captures different ways of describing an action and it enables the study of how
verbs are used differently with different objects in images. Additionally, we performed some prelimi-
nary experiments in which we show that action recognition can benefit from using position information.
Finally, we showed that this position information is related to prepositions that can be extracted from a
general language model.
References
Stephane Ayache and Georges Quenot. 2008. Video Corpus Annotation using Active Learning. In European
Conference on Information Retrieval (ECIR), pages 187?198, Glasgow, Scotland, mar.
Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based
semantics. Computational Linguistics.
Vincent Delaitre, Ivan Laptev, and Josef Sivic. 2010. Recognizing human actions in still images: a study of
bag-of-features and part-based representations. In BMVC. BMVA Press.
Vincent Delaitre, Josef Sivic, and Ivan Laptev. 2011. Learning person-object interactions for action recognition in
still images. In NIPS.
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. 2012. The PASCAL Visual Object
Classes Challenge 2012 (VOC2012) Results.
Ali Farhadi, Mohsen Hejrati, Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David
Forsyth. 2010. Every picture tells a story: Generating sentences for images. In ECCV.
Abhinav Gupta, Aniruddha Kembhavi, and Larry S. Davis. 2009. Observing human-object interactions: Using
spatial and functional compatibility for recognition. IEEE Trans. Pattern Anal. Mach. Intell., 31(10), October.
Nazli Ikizler, Ramazan Gokberk Cinbis, Selen Pehlivan, and Pinar Duygulu. 2008. Recognizing actions from still
images. In ICPR, pages 1?4. IEEE.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander Berg, and Tamara Berg. 2011.
Babytalk: Understanding and generating simple image descriptions. In CVPR.
Ivan Laptev. 2005. On space-time interest points. Int. J. Comput. Vision, 64(2-3):107?123, September.
Dieu Thu Le, Raffaella Bernardi, and Jasper Uijlings. 2013. Exploiting language models to recognize unseen
actions. In ICMR.
Amin Sadeghi and Ali Farhadi. 2011. Recognition using visual phrases. In CVPR.
Heng Wang, Alexander Kl?aser, Cordelia Schmid, and Cheng-Lin Liu. Dense trajectories and motion boundary
descriptors for action recognition.
Bangpeng Yao and Fei-Fei Li. 2010. Grouplet: A structured image representation for recognizing human and
object interactions. In CVPR, pages 9?16.
Bangpeng Yao, Xiaoye Jiang, Aditya Khosla, Andy Lai Lin, Leonidas J. Guibas, and Li Fei-Fei. 2011. Action
recognition by learning bases of action attributes and parts. In ICCV.
24
