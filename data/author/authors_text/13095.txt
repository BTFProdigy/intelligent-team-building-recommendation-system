Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1144?1153, Dublin, Ireland, August 23-29 2014.
A Lexicalized Reordering Model  
for Hierarchical Phrase-based Translation
1
 
 
Hailong Cao1, Dongdong Zhang2, Mu Li2, Ming Zhou2 and Tiejun Zhao1 
1Harbin Institute of Technology, Harbin, P.R. China 
2Microsoft Research Asia, Beijing, P.R. China 
{hailong, tjzhao}@mtlab.hit.edu.cn 
{Dongdong.Zhang, muli, mingzhou}@microsoft.com 
Abstract 
Lexicalized reordering model plays a central role in phrase-based statistical machine translation sys-
tems. The reordering model specifies the orientation for each phrase and calculates its probability con-
ditioned on the phrase. In this paper, we describe the necessity and the challenge of introducing such a 
reordering model for hierarchical phrase-based translation. To deal with the challenge, we propose a 
novel lexicalized reordering model which is built directly on synchronous rules. For each target phrase 
contained in a rule, we calculate its orientation probability conditioned on the rule. We test our model 
on both small and large scale data. On NIST machine translation test sets, our reordering model 
achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong baseline 
hierarchical phrase-based system. 
1 Introduction 
In statistical machine translation, the problem of reordering source language into the word order of the 
target language remains a central research topic. Statistical phrase-based translation models (Och and 
Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the 
phrase, since the order is specified by phrasal translations. However, phrase-based models remain 
weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the 
phrases, two types of models have been developed. 
The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and 
Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Gal-
ley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical 
information. The model in (Koehn et al., 2007) distinguishes three orientations with respect to the pre-
vious and the next phrase?monotone (M), swap (S) and discontinuous (D). For example, we can ex-
tract a phrase pair ?xiayou ||| the lower reach of? whose orientations with respect to the previous and 
the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, 
and has become a standard component of phrase-based systems such as MOSES.  
 
 
Figure 1. Phrase orientations for Chinese-English translation. 
 
The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchro-
nous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) 
and nonterminals (sub-phrases). The order of terminals and nonterminal are specified by the rule. For 
                                                 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
1144
example, the translation rule <X xiayou, the lower reach of X > specifies that the translation of sub 
phrase X before ?xiayou? should be put after ?the lower reach of?. 
One problem with the HPB model is that the application of a rule is independent of the actual sub 
phrase. For example, the rule <X xiayou, the lower reach of X > will always swap the translation of X 
and ?xiayou?, no matter what is covered by X. This is an over-generalization problem. Much work has 
been done to solve this issue. For example, Zollmann and Venugopal (2006) annotate non-terminals 
by syntactic categories. He et al. (2008) proposes maximum entropy models which combine rich con-
text information for selecting translation rules during decoding. Huang et al. (2010) automatically in-
duce a set of latent syntactic categories to annotate nonterminals. These works alleviate the over-
generalization problem by considering the content of X. In this paper, we try to solve it from an alter-
native view by modeling whether the phrases covered by X prefer the order specified by the rule. This 
has led us to borrow the lexicalized reordering model from the phrase-based model for the HPB model. 
We propose a novel lexicalized reordering model for hierarchical phrase-based translation and 
achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong HPB 
baseline system. 
2 Related work 
In this section, we briefly review two types of related work which are a nonterminal-based lexicalized 
reordering models and a path-based lexicalized reordering model. Both of them calculate the orienta-
tion for HPB translation. 
2.1 Nonterminal-based lexicalized reordering models 
Xiao et al. (2011) proposed an orientation model for HPB translation. The orientation probability of a 
derivation is calculated as the product of orientation probabilities of all nonterminals except the root.  
In order to define the relative orders of nonterminals and their adjacent phrase, they expand the align-
ment in a rule to include both terminals and nonterminals. There may be multiple ways to segment a 
rule into phrases; they use the maximum adjacent phrase similar to Galley and Manning (2008). They 
significantly outperformed the HPB system on both Chinese-English and German-English translation.  
Xiao et al. (2011) use the boundary word feature of nonterminals without considering their internal 
structure. For example, in Figure 1, suppose nonterminal X1 is not the root node and the orientation 
probability of X1 will condition on ?zhe, xiayou, this, river?.  
In this paper, we will consider how the words covered by the nonterminal X1 are reordered. Rather 
than using ?xiayou? as a feature to determine the orientation of X1 with respect to the next phrase, we 
think the immediately translated source word ?huanghe? could be more informative through it is not 
on the boundary of X1 , since ?huanghe? is the exact starting point from where we search for the next 
phrase to translate. 
Huck et al. (2013) proposed a very effective phrase orientation model for HPB translation. The 
model is also based on nonterminal. They extracted phrase orientation probabilities from word-aligned 
training data for use with hierarchical phrase inventories, and scored orientations in hierarchical de-
coding.  
2.2 Path-based lexicalized reordering model 
The most recent related work is Nguyen and Vogel (2013). They map a HPB derivation into a discon-
tinuous phrase-based translation path in the following two steps: 
1) Represent each rule as a sequence of phrase pairs and non-terminals.  
2) The rules? sequences are used to find the corresponding phrase-based path of a HPB derivation 
and calculate the phrase-based reordering features. 
 
 
Figure 2. The phrase-based path of the derivation in Figure 1. 
1145
A phrase-based path is the sequence of phrase pairs, whose source sides covers the source sentences 
and whose target sides generated the target sentences from left to right. For example, the phrase-based 
path of the derivation in Figure 1 is shown in Figure 2. 
The phrase-based reordering features for the above phrase-based path are: 
 
>)is  thisshi, zhe|<(log DPnext ,                      >)of reachlower   thexiayou,|<(log DPprevious , 
>)of reachlower   thexiayou,|<(log SPnext , >)river yellow  thehuanghe,|<(log SPprevious . 
 
Nguyen and Vogel (2013) achieved significant improvement over both phrase-based and HPB models 
on three language pairs respectively.  
One problem with the above work is that they did not use rules with unaligned source or target 
phrases. Though this can get faster and better Arabic-English translation, it leads to a 0.49 BLEU point 
loss for Chinese-English translation. 
Another problem with path-based model is: there are many forms of HPB rules which we cannot 
map into a reasonable sequence of phrase pairs and non-terminals. We will show this with an example 
derivation shown in Figure 3. The main difference between Figure 3 and Figure 1 is there is such a 
rule <fangzhi X, prevent X from> that a source phrase ?fangzhi? is aligned with a discontinuous target 
phrase ?prevent?from?. This makes it hard to find the corresponding phrase-based path because we 
do not know what is the right order of ?fangzhi ||| prevent?from? and ?daozei ||| the thieves? in the 
discontinuous phrase-based path. We face the following dilemmas: 
 
? If ?fangzhi ||| prevent?from? goes first, then the discontinuous phrase-based path is as shown in 
Figure 4(a). On such a path, we will consider the orientation of ?the thieves? with respect to 
?breaking in?. This is unreasonable because ?the thieves? and ?breaking in? are not adjacent in the 
target side. It does not satisfy the definition of the phrase-based reordering model which predicts 
the orientation with respect to previous or next adjacent target phrase.  
? If ?daozei ||| the thieves? goes first, then the discontinuous phrase-based path is as shown in Figure 
4(b). This is unreasonable because ?The policeman? and ?the thieves? are not adjacent on the tar-
get side. 
 
 
Figure 3. Example of Chinese-English translation and its derivation. 
 
 
          
(a)                                                                                         (b) 
                  
 Figure 4. Two discontinuous phrase-based path candidates of the HPB derivation. 
 
From the above example, we can see that if a target phrase is aligned to a discontinuous target 
phrase in a HPB rule, then it is hard to find a reasonable path whose target sides can generate the tar-
get sentence from left to right.  
1146
3 Our lexicalized reordering model 
Rather than mapping a HPB derivation into a discontinuous phrase-based path and applying reordering 
model built on phrases, we propose a lexicalized reordering model which is built directly on HPB 
rules. For each target phrase contained in a HPB rule, we calculate its orientation probability condi-
tioned on the rule. For the example derivation in Figure 3, we represent it by the structure shown in the 
following figure: 
 
 
Figure 5.  Our representation of the HPB derivation in Figure 3.  
 
Different from Figure 4(a) and Figure 4(b) which contain a discontinuous phrase ?prevent?from?, we 
represent ?prevent?from? as two individual target phrases: ?prevent? and ?from?. Instead of consid-
ering the orientation of ?prevent?from?, we consider the orientation of ?prevent? and ?from? respec-
tively. For example, we will consider the orientation of ?prevent? with respect the previous phrase 
?the policeman? 
prevent)(previousO
, and the orientation of ?prevent? with respect the next phrase ?the 
thieves? prevent)(nextO . The probabilities of both prevent)(previousO
and prevent)(nextO are conditioned 
on the rule <fangzhi X, prevent X from>. 
In Figure 5, every two neighboring target phrases are adjacent in the original target side. In this way, 
we can borrow the phrase-based reordering model which calculates the orientation with respect to pre-
vious and next adjacent phrase.  
More formally, we represent a HPB rule in the general form of: 
 
??? ?,X...XX,X...XX 2211022110 nnnn ttttssssr  
 
where n is the number of nonterminals. ...n,isi 1? , is the source phrase which is a continuous source 
word sequences. ...n,iti 1? , is the target phrase which is a continuous target word sequences. We use 
?  to represent the alignment of words and nonterminals in the rule. Note that is or it can be empty if 
there are adjacent nonterminals or there is nonterminal on the boundary. The lexicalized reordering 
probability of rule r is defined as the product of each target phrase?s orientation probabilities condi-
tioned on the rule r: 
 
)|)(()|)((
0
r,itOPr,itOP inextnext
n
i
ipreviousprevious?
?
 
 
In the above equation, each probability is conditioned on the whole rule. In this way, we avoid the 
problem of mapping a HPB derivation into a discontinuous phrase-based path. There are two ad-
vantages for our reordering model: 
? It is compatible with HPB rules which contain unaligned phrases. 
? It is compatible with HPB rules in which a source phrase is aligned to a discontinuous target 
phrase. 
Actually, our model is compatible with any kind of HPB rules since it is defined on the general 
form of rule. 
Now we describe how to define )( iprevious tO and )( inext tO in the model. Suppose it  contains ik target 
words and we write 
it as )()1-()2()1( ... ii kikiiii wwwwt ?
. Then we define: 
 
),()()( )1(1-)1()1( iiipreviousiprevious wwOwOtO ?? ,          ),()(( 1)()()( ??? iii kikikinextinext wwOwOtO
 
 
1147
where ),( 1?jj wwO is the orientation of two adjacent target words and is determined as follows: 
If ( )(1)( 1??? jj wlmwrm
 )           MwwO jj ?? ),( 1
; 
Else if (  )(1)( 1 jj wlmwrm ???
 )  SwO jj ?? ),( 1
; 
Else                                                         DwwO jj ?? ),( 1
; 
)(wrm is the position of the right most source word aligned to target word w; )(wlm is the position of 
the left most source word aligned to target word w. 
Above is our lexicalized reordering model which is built upon HPB rules. We complete its descrip-
tion using an example. For the rule <fangzhi X, prevent X from>, n=1, 
0 prevent? ??t  and 
1 from? ??t , the lexicalized reordering probability is: 
 
( (prevent)|<fangzhi X, prevent X from>,0 ) ( (prevent)|<fangzhi X, prevent X from> ,0)
( (from)| f i , r t  fr ,1) ( (from)|<fangzhi X,
?
? ?
previous previous next next
previous previous next next
P O P O
O P O  prevent X from> ,1)
 
 
Note that we calculate the orientation of plain phrase pairs in the same way as for HPB rules. We 
can represent a phrase pair in the form of ??? ?,, 00 tsr , which is a rule that does not contain any 
nonterminal. Then we can apply our above model which is general enough to cover both HPB rules 
and plain phrase pairs. 
4 Training and decoding 
The training of our model is similar to the reordering model of Moses. During the standard phrase pair 
extraction and rule extraction, besides the nonterminal alignment in rules, we also keep the lexical 
alignments and orientations. If a phrase pair or a rule is observed with more than one set of alignment, 
we only keep the most frequent one and only count the orientations corresponding to the most frequent 
alignment.  
Following Moses, we use relative frequency and add 0.5 smoothing technique to estimate the orien-
tation probability based on all samples collected from the training corpus. Generally, given a rule r 
with n target phrases, we estimated the reordering probability for each 
it as follows: 
 
0.5 # ( )( ( )| ) 1.5 #( )
? ???? ?
previous i
previous previous i
O t rP O t r, i r
,         0 5 ( ( ), )( ( ) | ) 1 5 ( )
?? ?
next inext next i
. # O t rP O t r, i . # r
 
 
For each parallel sentences pair, we add a start and an end mark on both sides. They are aligned re-
spectively. 
Our phrase pairs and rules are extracted from word aligned parallel sentences. There are many 
phrase pairs and rules which contain unaligned target or source words. How to deal with them is quite 
important for our reordering model. We will describe how to process them in the following two sub-
sections. 
4.1 The processing of unaligned target words 
Our main principle for processing an unaligned word is to: skip it and use the nearest aligned word. 
For example in Figure 3, the orientation of ?prevent? with respect to the next phrase is determined by: 
 
)  the(prevent,prevent)( OOnext ? 
 
If the target word ?the? is unaligned and ?thieves? is aligned with ?daozei?, we will define: 
 
(prevent) (prevent, the) (prevent, thieves)? ? ?nextO O O M 
 
Similarly, in Figure 1, the orientation of ?the lower reach of? with respect with ?the yellow river? is 
determined by O(of, the). Suppose both ?of? and ?the? are unaligned and there are alignments for 
?reach-xiayou? and ?yellow-huanghe?, we will have: 
1148
 SOO = yellow)(reach, = the)(of,  
 
We believe this orientation is consistent with our intuitions.  
More formally, before we determine the orientation of two adjacent target words ),( qp wwO ?we 
apply the following processing procedure: 
 
While (target word
pw is unaligned) p--; 
While (target word 
qw is unaligned) q++; 
 
If all words in a target phrase 
it  are unaligned, we do not need to consider its orientation since it  
does not trigger any movement along the source words at all. Actually, it will be skipped when we de-
termine the orientation of the previous and next aligned target phrases. (See also the decoding algo-
rithm in Section 4.3) 
4.2 The processing of unaligned source words 
The processing of Section 4.1 can guarantee that the orientation is determined based on two aligned 
target words, namely
pw and qw ,which must be continuous or separated by unaligned target words.  
Now we introduce the processing of unaligned source words. Before we determine the orientation 
of two target words ),( qp wwO ?we apply the following procedure to modify the position index of 
the left most source word aligned to 
pw and qw respectively: 
 
While (the 
th1)-)(( pwlm  source word is unaligned) --)( pwlm ; 
While (the thqwlm )1-)((  source word is unaligned) --)( qwlm ; 
 
For the example shown in the Figure 6, initially we have 1)( 1 ?wrm and 4)( 4 ?wlm . Since the 
source words 
3w  and 2w  are unaligned, our procedure will modify the value of )( 4wlm from 4 to 2. 
Finally, since )(1)( 41 wlmwrm ?? , the orientation of the two phrases marked by rectangular boxes in 
Figure 6 is: 
 
MwwOwwO ?? ),(),( 4132  
 
Again, we believe this result is consistent with our intuition. 
 
 
Figure 6. An example of phrases contain unaligned words 
 
Note that during decoding, both the unaligned source and target words are also processed in the 
same way as in the training step. This makes our lexicalized reordering model consistent. 
4.3 Decoding  
Now we introduce how to integrate our reordering model into the HPB system during the standard 
CYK bottom-up decoding.   
During decoding, if we just apply a plain phrase, we do not need to consider the orientation at once. 
It will be triggered when the phrase is used to compose a larger translation hypothesis together with 
other phrases or rules. 
We need to calculate the reordering features whenever we apply a HPB rule or a glue rule during 
the CYK decoding. Generally, given a rule ??? ?,X...XX,X...XX 2211022110 nnnn ttttssssr  defined in 
section 3, we calculate the reordering probability for the span covered by r with algorithm 1. In the 
algorithm, LL(X) represents the lowest rule which covers the left most word of X; LR(X) is the lowest 
1149
rule which covers the right most word of X; Both LL(X) and LR(X) can be found by traversing the 
derivation tree top to down recursively. LI(r) is the index of the last target phrase of rule r.  
As in the example shown in Figure 3, for the rule r2=<X2 jinru, X2 breaking in>, the orientation of 
X2 and ?breaking in? is: 
 
(breaking in) (from, breaking)? ? ?previousO O O D 
 
The right most target word of X2 is ?from?, the lowest rule covering ?from? is r3=<fangzhi X4, prevent 
X4 from> and the index of the last target phrase of r3 is 1. So the reordering probability is: 
 
)1,0, 31 (D|rP)(D|rPprob nextprevious ??  
 
Note that, for readability, we use the product of probabilities to demonstrate the decoding process. 
Actually in practice, we use a linear model which sums the weighted log probabilities. 
 
prob=1; 
for (int i=1; i<=n; i++) 
{ 
   if (
1?it  is not empty and contains aligned words) 
   { 
            ;0,*
;1,*
;1
))(O|LL(XPprob
)i(O|rPprob
)(tOO
iprevious
next
inext
?
??
? ?
 
    } 
   if (
it  is not empty and contains aligned words) 
   { 
            
);|(*
;)(,*
);(
r,iOPprob
))LR(XLI)(O|LR(XPprob
tOO
previous
iinext
iprevious
?
?
?  
    } 
   else if (i<n)  
   {             
          //
iX  and 1?iX  are continuous 
          //or all words between them is unaligned  
      
1
??
??
??
 ( );
 ( );
the firs  phra
??
?
se of ;
( );
* ,LI( ))?
??
?
* ;,0)
;?
?
?
?
?
?
?
?
? ?
p i
q i
q
previous
next p p
previous q
rule r LR X
rule r LL X
t r
O O t
prob P (O | r r
prob P (O | r
i
 
    } 
} 
 
Algorithm 1. Calculating the reordering probability for a span covered by a rule:
??? ?,X...XX,X...XX 2211022110 nnnn ttttssssr . 
 
As shown in Algorithm 1, the reordering probability depends on the lowest rules which cover the 
left/right most word. Therefore, we keep the lowest rules which cover the left/right most word for each 
partial translation. If two partial translations are same in everything but differ in the lowest rule, we 
need to keep both of them, rather than only keep the one with higher score. This will increase the 
complexity of the searching. 
4.4 Discussion 
Orientation can be determined based on word, phrase and hierarchical phrase (Galley and Manning, 
2008). What we adopt in this paper is word based orientation. It is based on the following considera-
tions: 
? Our baseline is a HPB system, which can capture hierarchical orientation. We use word based ori-
entation with the aim to complement the HPB system. 
? Word based orientation is consistent during training and decoding; phrase based orientation is 
prone to inconsistent between training and decoding.  
Galley and Manning (2008) has pointed out an inconsistency in Moses between training and decod-
ing. Here we would like to note that phrase based orientation depends on phrase segmentation. For 
example, in Figure 1, the orientation of phrase ?this is? with respect to next phrase could be either: 
? D, if we think the next phrase is ?the lower reach of ? which is what Figure 1 shows. 
? or S, if the next phrase is ?the lower reach of the yellow river? which can compose a legal phrase 
pair with ?huanghe xiayou? according to the standard phrase pair extraction algorithm.  
1150
The decision to adopt word-based orientation makes our work similar with Hayashi et al. (2010) who 
proposed a word-based reordering model for HPB system. The difference between our work and 
Hayashi et al. (2010) is: they adopt the reordering model proposed by Tromble and Eisner (2009) for 
the preprocessing approach, while we borrow the idea of lexicalized  reordering models which are 
originally proposed for phrase-based machine translation. 
5 Experiments 
5.1 Experimental settings 
Our baseline system is re-implementation of Hiero, a hierarchical phrase-based system (Chiang, 
2007). Besides the standard features of a HPB model, there are six reordering features in our reorder-
ing model which are M, S and D with respect to the previous and next phrase respectively. They are 
integrated into the log-linear model of the HPB system. The Minimum Error Rate Training (MERT) 
(Och, 2003) algorithm is adopted to tune feature weights for translation systems. 
We test our reordering model on a Chinese-English translation task. The NIST evaluation set MT06 
was used as our development set to tune the feature weights, and the test data are MT04, MT 05 and 
MT08. We first conduct experiments by using the FBIS parallel training corpus, and then further test 
the effect of our method on a large scale parallel training corpus. 
Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default 
setting. The language model is a 4-gram model trained with the Xinhua portion of LDC English Gi-
gaword Version 3.0 and the English part of the bilingual training data. Translation performances are 
measured with case-insensitive BLEU4 score (Papineni et al., 2002). 
5.2 Experimental results on FBIS corpus 
We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline 
and our lexicalized reordering model. After pre-processing, the statistics of FBIS corpus is shown in 
table 1. 
 
 #sentences #words 
Chinese 128832 3016570 
English 128832 3922816 
Table 1. The statistics of FBIS corpus 
 
Table 2 summarizes the translation performance. The first row shows the results of baseline HPB 
system, and the second row shows the results when we integrated our lexicalized reordering model 
(LRM). We get 1.2, 0.8 and 0.7 BLEU point improvements over the baseline HPB system on three test 
sets respectively. 
 
 MT04 MT05 MT08 
HPB 33.53 32.97 25.08 
HPB+LRM 34.71 33.77 25.84 
Table 2. Translation performance on the FBIS corpus. 
5.3 Experimental results on large scale  corpus 
To further test the effect of our reordering model, we use a large scale corpus released by LDC. The 
catalog number of them is LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, LDC2005E83, 
LDC2006E26, LDC2006E34, LDC2006E85 and LDC2006E92. There are 498K sentence pairs, 12.1M 
Chinese words and 13.8M English words. Table 3 summarizes the translation performance on the 
large scale of corpus.  
 
 MT04 MT05 MT08 
HPB 38.72 37.59 29.03 
HPB+LRM 39.81 38.24 29.63 
Table 3. Translation performance on a large scale parallel corpus. 
1151
Our model is still effective when we train the translation system on large scale data. We get 1.1, 0.7 
and 0.6 BLEU point improvements over the baseline HPB system on three test sets respectively. 
6 Conclusion and future work 
We proposed a novel lexicalized reordering model for hierarchical phrase based machine translation. 
The model is compatible with any kind of HPB rules no matter how complex the alignments are. We 
tested our reordering model on both small and large scale data. On NIST machine translation test sets, 
our reordering model achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation 
over a strong baseline hierarchical phrase-based system. 
In future work, we will further test our model on other language pairs and compare it with other re-
ordering models for HPB translation. 
Acknowledgments 
We thank anonymous reviewers for insightful comments. The work of Hailong Cao is sponsored by 
Microsoft Research Asia Star Track Visiting Young Faculty Program. The work of HIT is also funded 
by the project of National Natural Science Foundation of China (No. 61173073) and International Sci-
ence & Technology Cooperation Program of China (No. 2014DFA11350). 
Reference 
Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion Models for Statistical Machine Translation. In Pro-
ceedings of ACL. 
Colin Cherry, Robert C. Moore and Chris Quirk. 2012. On Hierarchical Re-ordering and Permutation Parsing for 
Phrase-based Decoding. In Proceedings of  NAACL Workshop on SMT. 
David Chiang. 2007. Hierarchical Phrase-based Translation. Computational Linguistics, 33(2):201?228. 
Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Mod-
el. In Proceedings of EMNLP. 
Katsuhiko Hayashi, Hajime Tsukada, Katsuhito Sudoh, Kevin Duh and Seiichi Yamamoto. 2010. Hierarchical 
Phrase-based Machine Translation with Word-based Reordering Model. In Proceedings of COLING. 
Zhongjun He, Qun Liu, Shouxun Lin. 2008. Improving Statistical Machine Translation using Lexicalized Rule 
Selection. In Proceedings of COLING. 
Liang Huang, Hao Zhang and Daniel Gildea. 2005. Machine Translation as Lexicalized Parsing with Hooks. In 
Proceedings of IWPT. 
Zhongqiang Huang, Martin ?mejrek, and Bowen Zhou. 2010. Soft Syntactic Constraints for Hierarchical Phrase-
based Translation Using Latent Syntactic Distributions. In Proceedings of EMNLP.  
Matthias Huck, Joern Wuebker, Felix Rietig, and Hermann Ney. 2013. A Phrase Orientation Model for Hierar-
chical Machine Translation. In Proceedings of ACL Workshop on SMT.  
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne, Christopher Callison-Burch, Marcello Federico, Nicola 
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst.. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL 
demonstration session. 
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto and Kazuteru Ohashi. 2006. A Clustered Global Phrase 
Reordering Model for Statistical Machine Translation. In Proceedings of ACL. 
Thuylinh Nguyen and Stephan Vogel. 2013. Integrating Phrase-based Reordering Features into Chart-based De-
coder for Machine Translation. In Proceedings of ACL. 
Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of ACL. 
Franz Josef Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. 
Computational Linguistics, 30(4):417?449. 
Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL. 
1152
 Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evalua-
tion of Machine Translation. In Proceedings of ACL. 
Christoph Tillmann. 2004. A Unigram Orientation Model for Statistical Machine Translation. In Proceedings of 
HLT-NAACL. 
Roy Tromble, Jason Eisner. 2009. Learning Linear Ordering Problems for Better Translation. In Proceedings of 
EMNLP. 
Xinyan Xiao, Jinsong Su, Yang Liu, Qun Liu, and Shouxun Lin. 2011. An Orientation Model for Hierarchical 
Phrase-based Translation. In Proceedings of IALP.  
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statisti-
cal Machine Translation. In Proceedings of ACL. 
Richard Zens and Hermann Ney. 2006. Discriminative Reordering Models for Statistical Machine Translation. 
In Proceedings of Workshop on SMT. 
Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing. In 
Proceedings of NAACL Workshop on SMT. 
1153
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2227?2236, Dublin, Ireland, August 23-29 2014.
 Soft Dependency Matching                                                                               
for Hierarchical Phrase-based Machine Translation
1
 
Hailong Cao1, Dongdong Zhang2, Ming Zhou2 and Tiejun Zhao1 
1Harbin Institute of Technology, Harbin, P.R. China 
2Microsoft Research Asia, Beijing, P.R. China 
{hailong, tjzhao}@mtlab.hit.edu.cn 
{Dongdong.Zhang, mingzhou}@microsoft.com 
Abstract 
This paper proposes a soft dependency matching model for hierarchical phrase-based (HPB) machine 
translation. When a HPB rule is extracted, we enrich it with dependency knowledge automatically learnt 
from the training data. The dependency knowledge not only encodes the dependency relations between 
the components inside the rule, but also contains the dependency relations between the rule and its con-
text. When a rule is applied to translate a sentence, the dependency knowledge is used to compute the 
syntactic structural consistency of the rule against the dependency tree of the sentence. We characterize 
the structure consistency by three features and integrate them into the standard SMT log-linear model to 
guide the translation process. Our method is evaluated on multiple Chinese-to-English machine transla-
tion test sets. The experimental results show that our soft matching model achieves 0.7-1.4 BLEU points 
improvements over a strong baseline of an in-house implemented HPB translation system. 
1 Introduction 
HPB model (Chiang, 2007) is widely used and has consistently delivered state-of-the-art performance. 
This model extends the phrase-based model (Koehn et al., 2003) by using the formal synchronous 
grammar to well capture the recursiveness of language during translation. In a formal synchronous 
grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which 
may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of 
translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. 
To generate grammatical translations, lots of syntax-based models have been proposed by Galley et 
al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), 
Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic 
structure of either the source sentence or the target sentence. These approaches can generate more 
grammatical translations by capturing the structural difference between language pairs. However, 
these models need special efforts to capture non-syntactic translation knowledge to improve the trans-
lation performance.  
It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 
2010). There has been much work trying to improve HPB model by incorporating syntax information. 
Marton and Resnik (2008) leverage linguistic constituents to constrain the decoding softly. Some work 
go further to augment the non-terminals in HPB rules with syntactic tags which depend on the syntac-
tic structure covered by the non-terminals (Zollmann and Venugopal, 2006; Chiang, 2010; Li et al., 
2012; Huang et al., 2013). For example, given below HPB rules (1-4), the source non-terminal X 
could be refined into NP or PP as shown in rules (5-8) respectively.  
 
(1) <? ? X, borrowed X>                  (2) <? ? X, lent X>  
(3) <X1 ? ? X2, borrowed X2 X1>     (4) <X1 ? ? X2, X1 borrowed X2>  
 
(5) <?? NP, borrowed X>                  (6) <?? NP, lent X> 
(7) <PP ?? NP, borrow X2 X1>          (8) <NP ?? NP, X1 lent X2> 
 
Although augmenting the non-terminals with syntactic tags in these methods achieved better results 
for HPB model, they have limitations that the syntax information on the non-terminals are not discrim-
                                                 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
2227
inative enough due to the limited context covered by the HPB rule. For example, rule (5) and (6) are 
still not discriminative when translating below two sentences (9) and (10). 
 
(9) ????????(I borrowed a book from him)    (10) ????????(I lent a book to him) 
 
where the common phrase ??????? appear in both sentences. Obviously, although rule (5) and 
(6) share same source sides, rule (5) can only be applied to the translation of sentence (9) and rule (6) 
to sentence (10). Otherwise, inappropriate application will lead to wrong translations. Rule (5) and (6) 
are not discriminative due to no consideration of their outside context during the translation.  
Motivated by such observation, we proposed an alternative approach, called soft dependency 
matching model, to incorporate into each HPB rule the source syntactic dependencies connecting the 
contents inside the rule with the context outside the rule. The dependency knowledge associated with 
HPB rules is automatically learnt from bilingual training corpus. They make HPB rules discriminative 
according to global context.  
 
 
Figure 1.  Dependency information associated with two rules. LC and RC mean the source context on 
the left and right of the rule respectively. 
 
Figure 1 shows two rules associated with different dependencies. The first one is applicable to the case 
when some word on the left side depends on the word ??? in the rule, and the second one is applica-
ble to the case when the word ??? in the rule depends on some word on the right side. 
During SMT decoding, first we parse the source sentence to get the dependency tree. When a HPB 
rule is applied to translate the sentence, we calculate structural consistency between the dependency 
knowledge associated with the rule and dependency tree structure of the source sentence. The con-
sistency degree is integrated into the SMT log-linear model as features to encourage syntactic hypoth-
eses and penalize the hypotheses violating syntactic constraints. 
Compared with previous work that incorporate syntax knowledge into HPB model, the advantage of 
our soft dependency matching model is: 
? It not only captures the dependency relations between the components inside the rule, but also 
models the dependency relations between the rule and its context from a global view. 
? Without increasing the amount of rules or the searching space, our model can capture the syntactic 
variation for all of the rules (syntactic or non-syntactic, well-formed or ill-formed). 
? Our model can take advantage of the dependency knowledge on both terminals and non-terminals.  
We evaluate the performance of our soft dependency matching model on Chinese-to-English trans-
lation task. Experimental results show that our method can achieve the improvements of 0.7-1.4 
BLEU points over the baseline HPB model on multiple NIST MT evaluation test sets. 
2 Related Work 
Ever since the invention of phrase-based model, a lot of efforts have been made to incorporate linguis-
tic syntax. Cherry(2008) and Marton and Resnik (2008) leverage linguistic constituent to constrain the 
decoding softly. In their methods, a translation hypothesis gets an extra credit if it respects the parse 
tree but may incur a cost if it violates a constituent boundary. The soft constrain based methods 
achieved promising results on various language pairs. One problem of these methods is that exactly 
matching syntactic constraints cannot always guarantee a good translation, and violating syntactic 
structure does not always induce a poor translation. It could be more reasonable if the credit and penal-
ty is learnt from the parallel training data. In this work, we learn this kind of constrain knowledge di-
rectly from the syntactic structures over the training corpus.  
Xiong et al. (2009) present a method that automatically learns syntactic constraints from training 
data for the ITG based translation (Wu, 1997; Xiong et al., 2006).  They utilize the syntactic con-
straints to estimates the extent to which a span is bracketable. Though the effect was demonstrated on 
the ITG based model, the method is also applicable to the HPB model. The main difference between 
Xiong et al. (2009) and our work is that we try to estimate the structural consistency of each rule 
2228
against the source syntax tree. For rules which are same in the source side but different in the target 
side, our method will distinguish the inconsistency degree for different rules. While, for such rules, 
Xiong et al. (2009) will give a same score which will be used to compete with rules in other spans. 
More recently, Huang et al. (2013) associate each non-terminal with the distribution of tags that is 
used to measure the consistency of syntactic compatibility of the translation rule on source spans. Our 
work is similar to Huang et al. (2013) since we also represent the syntactic variation of translation 
rules in the form of distribution. The main difference is that they annotate non-terminals with head 
POS tags while we use dependency triples (over both terminals and non-terminals) to explicitly repre-
sent both the dependency relations inside the rule, and that between the rule and its context. 
Both above related work and our work need parse the source sentence to get syntactic context be-
fore decoding. There are also some methods incorporating syntax information without the need of 
online parsing the source sentences (Zollmann and Venugopal, 2006; Shen et al, 2009; Chiang, 2010). 
They parse the training data to label the non-terminals with syntactic tags. During the bottom-up de-
coding, the tags are used to model the substitution of non-terminals in a soft way (Shen et al, 2009; 
Chiang, 2010) or in a hard way (Zollmann and Venugopal, 2006).  
Gao et al. (2011) derive soft constraints from the source dependency parsing for the HPB translation. 
They focus on the relative order of each dependent word and its head word after translation, while our 
method models whether the dependency information of a rule matches the context or not.  
Our work utilizes contextual information around translation rules. In this sense, it is similar to He et 
al. (2008) and Liu et al. (2008). The main difference between their work and our work is that they lev-
erage lexical context for rule selection while we focus on the syntactic contextual information. 
3 Hierarchical Phrase based Machine Translation 
Our model proposed in this paper is an extension of the HPB model (Chiang, 2007). Formally, HPB 
model is a weighted synchronous context free grammar. It employs a generalization of the standard 
plain phrase extraction approach in order to acquire the synchronous rules of the grammar directly 
from word-aligned parallel text. Rules have the form of: 
          
where X is a nonterminal,   and   are both strings of terminals and non-terminals from source and tar-
get side respectively, and ? is a one-to-one correspondence between nonterminal occurrences in   and 
 . Associated with each rule is a set of feature functions with the form        . These feature functions 
are combined into a log-linear model. When a rule is applied during SMT decoding, its score is calcu-
lated as: 
?          
 
 
where    is the weight associated with feature function         . The feature weights are typically op-
timized using minimum error rate training algorithm (Och, 2003).  
4 Soft Dependency Matching Model 
In order to incorporate syntactic knowledge to refine both the word ordering and word sense disam-
biguation for HPB model, we propose a soft dependency matching model (SDMM). It extends HPB 
rule into a form which is named as SDMM rule: 
 
              
 
where RDT(rule?s dependency triples) is a set of dependency triples defined on source string   . Each 
element in RDT is a triple representing dependency knowledge in the form: 
 
{m-h-l} 
 
where m and h are the dependent and head respectively, l is the label of the dependency relation type. 
m and h could be any of terminals, non-terminals, LC and RC, where LC denotes the left context and 
RC the right context. 
In the following two sub-sections, we will explain the details of SDMM rule extensions for both 
plain phrases (i.e., there are no non-terminals in both         ) and hierarchical rules (i.e., there are at 
2229
least one non-terminal in both         )  respectively. For simplicity, we ignore the correspondence   
in the representations of both HPB rules and SDMM rules. 
 
 
Figure 2: An illustration of a dependency parse tree for the source side of a word-aligned parallel sen-
tences pair. 
4.1 SDMM Over Plain Phrase Rules 
Figure 2 illustrates a parallel sentence together with word alignments and source dependency parse 
tree, from which we can extract the phrase pairs of HPB rules like: 
 
(11) < ? ? ?, a book >        (12) < ? ? ? ? ?, borrowed a book > 
 
By incorporating syntactic knowledge, we can extend these HPB rules into SDMM rules as shown 
in Figure 3(a) and Figure 3(b) respectively.  
 
Figure 3: An illustration of two phrase pairs annotated with a set of dependency triples. 
 
Formally, the RDT corresponding to phrase pair (11) is {?-LC-dobj}. The RDT corresponding to 
phrase pair (12) is {LC-?-nsubj, LC-?-prep}. 
Now we describe how to build the RDT when a phrase pair is extracted from a sentence pair during 
the training step. First, we initialize RDT to be empty. Then, for each dependency triple ?m-h-l? in the 
parse tree of the source sentence, if either m or h is covered by the source phrase in the rule, we add it 
to RDT. However, if both m and h are covered by the source phrase, we will ignore it because it holds 
less syntactic information beyond HPB rule itself. For example, the dependency triple ?? -? -
nummod? is excluded from RDT for both phrase pair (11) and phrase pair (12). In addition, we do not 
add the dependency triple ?m-h-l? into RDT if both m and h are not contained in source phrase, be-
cause it is not related to phrase pair at all. The dependency triple ??-?-pobj? is such a case for both 
phrase pair (11) and phrase pair (12).  
Finally, we normalize the word in RDT that is not covered by the source phrase with either LC 
(stands for the left context) or RC (stands for the right context) according to its relative position to the 
source phrase. For example, in the RDT for phrase pair (11), we normalize ??-?-dobj? as ??-LC-
dobj? since the word??? is not covered by the source phrase and it is treated as left context.  
Note that for each context word outside the source phrase, we only record whether it is on the left or 
on the right of phrase. We do not further consider its lexical form and its distance to the source phrase. 
For example, in the two dependency triples in Figure 3(b), both the dependent word ??? and ??? are 
normalized into LC. In this way, we can generalize the dependency triples in RDT and alleviate the 
data sparseness problem. In fact, there might be duplicated dependency triples for a phrase pair. In this 
case, we only keep one of them. 
4.2 SDMM over Hierarchical Rules 
Hierarchical rules are usually generated by substituting sub-phrases with non-terminals from plain 
phrase pairs. For example, given the parallel sentence and the two phrase pairs in Section 4.1, we can 
get a hierarchical rule like:  
<? ? X, borrowed X> 
To extend hierarchical rules into SDMM rules, we add dependency information to source terminals 
or non-terminals in RDT. Figure 4 shows an example representing an SDMM rule: 
2230
 
Figure 4: An illustration of a hierarchical rule annotated with a set of dependency triples. 
 
The generation of SDMM rules over hierarchical rules is similar to that of plain phrase rules. The only 
difference lies in processing the non-terminals, whose dependencies are inferred from the words they 
covered. For example, the RDT of the above SDMM rule would be:{LC-?-nsubj, LC-?-prep, X-?-
dobj} 
Similarly, any dependencies over two terminals contained in the source rule are not included in 
RDT, and dependencies inferred from same non-terminals are excluded as well. In addition, depend-
encies between two non-terminals are ignored.   
4.3 SDMM Rule Composing 
A same HPB rule (either plain phrase pair or a hierarchical rule) can be extracted from different bilin-
gual sentences. Therefore, the same HPB rule could be extended into multiple SDMM rules. For ex-
ample, given a parallel sentence pair shown in Figure 5, 
 
 
Figure 5: An example of a dependency tree over the source sentence together with the word-aligned 
target sentence. 
 
we might get a SDMM rule as shown in Figure 6. Compared to the SDMM rule in Figure 4, there is an 
additional dependency triple ?LC-?-tmod? in RDT. 
 
 
Figure 6: An illustration of dependency triples associated to a hierarchical rule. 
 
Intuitively, we can process SDMM rules independently although they share the same information of 
HPB rules. However, this will exacerbate the data sparseness problem and make the computation inef-
ficient due to dramatically increased model size. An alternative way is only to keep the most frequent 
RDT information for the same HPB rules. Though this can get a very concise model, a lot of useful 
syntactic information might be lost. 
We propose a balanced composing method to make a trade-off between knowledge representation 
and computation efficiency of SDMM rules. Suppose there are more than one SDMM rules with dif-
ferent      but the same HPB rule, we compose them by the union and get the new form of RDT as: 
 
    ?    
 
 
 
In addition, we record the frequency of HPB rule as well as that of each dependency triple in RDT 
as: 
             ,                 
 
where              is the number of times that HPB rule           is extracted from the 
training data, and                 is the frequency that    and           co-occur. For ex-
ample, suppose SDMM rules in Figure 4 and Figure 6 occurs 9 and 1 times respectively, we can com-
pose them into the form as shown in Figure 7.  
 
2231
 
Figure 7: Composed form of the dependency annotation of a rule. The integers following the colons 
denote occurring times. 
 
Therefore, the composed SDMM rule will be represented by the original HPB rule <? ? X, bor-
rowed X> together with RDT and its frequency information shown in Table 1. 
 
RDT # 
{ LC-?-tmod, 
   LC-?-nsubj, 
   LC-?-prep, 
    X - ?-dobj } 
1 
10 
10 
10 
Table 1. The RDT and its frequency information of a composed SDMM rule. 
4.4 Consistency of SDMM Rules 
So far we have described how to enrich a rule with RDT in the training step. Now we introduce how to 
use the RDT of each rule to guide the translation process. 
In the decoding, we parse the source sentence to get the dependency parse tree as shown in Figure 8. 
When we apply a rule to get a partial translation for a span, we also extract a set of dependency triples 
based on the parse tree in the exact same way that is used in the training step. We denote this by CDT 
(context dependency triples). Suppose the rule <? ? X, borrowed X> is applied to translate the un-
derlined span in Figure 8, then the CDT for the rule is: {LC-?-nsubj, LC-?- dep, X-?-dobj}. 
 
 
Figure 8: A sentence to be translated and its dependency parse tree. 
 
In order to evaluate whether a SDMM rule is applicable to translate a sentence or not from the syn-
tactic view, we model the structural consistency of SDMM rule against source dependency tree by cal-
culating the matching degree between RDT and CDT. The example in Figure 9 illustrates how we 
compute the matching degree between the SDMM rule in Figure 7 and CDT over the source depend-
ency tree in Figure 8. We estimate the matching degree based on three sets including the relative com-
plement set of CDT in RDT, the intersection set of RDT and CDT, and the relative complement set of 
RDT in CDT. 
 
 
Figure 9: Three different sets of dependency triples to model the structural consistency of syntactic 
matching. 
 
The statistics over above three sets are leveraged to design three features which are incorporated into 
SMT log-linear model to encourage and penalize various syntactic motivated hypotheses. The first 
feature is called as the lost dependency triple feature   . It is calculated based on the set RDT\CDT as: 
 
   ?                   
         
              
2232
 where   is the indicator function whose value is one if and only if the condition is true, otherwise its 
value is zero. The motivation of     is that: if a dependency triple which always co-occur with the HPB 
rule is not observed in CDT, it indicates the current SDMM rule may mismatch with the source sen-
tence and therefore we need to penalize its application. In Figure 9, ?LC-?-prep? is such a dependen-
cy triple. However, for the less frequent dependency triples in RDT such as ?LC-?-tmod? in Figure 8, 
there is no penalty on it although it is not found in CDT.  
The second feature is the unexpected dependency triple feature   , which is computed as :   
   |       | 
 
This feature is the number of dependency triples in CDT that never co-occur with the rule in the train-
ing data. In Figure 9, ?LC-?-dep? is such a case. Intuitively, the higher the value    is, the higher in-
consistency degree is, because it means that many dependency triples in CDT are never observed in 
the training corpus. We should discourage the application of the corresponding SDMM rule.  
The third feature is the matched dependency triple feature  
 
 which is calculated based on 
RDT?CDT. It is directly used to model the structural consistency over all the dependency triples in 
RDT?CDT for the application of HPB rule          . Formally,  
 
 is defined as the sum of log 
probability of each dependency triple in RDT?CDT conditioned on the HPB rule: 
 
   ?        |           
         
 
 
where    |           is the probability of a dependency triple  associated to a HPB rule    
       . We estimate it based on the relative frequency and experimentally use the adding 0.5 
smoothing. 
5 Experiments 
5.1 Experimental Settings 
Our baseline is the re-implementation of the Hiero system (Chiang, 2007). When our soft dependency 
matching model is integrated, the HPB rule is extended into the form of  
              and the score is calculated by: 
 
?           
 
                                              
 
where the additional three features are defined in Section 4.3,   ,    and    are corresponding feature 
weights. 
We test our soft dependency matching model on a Chinese-English translation task. The NIST06 
evaluation data was used as our development set to tune the feature weights, and NIST04, NIST05 and 
NIST08 evaluation data are our test sets. We first conduct experiments by using the FBIS parallel cor-
pus, and then further test the performance of our method on a large scale training corpus. 
Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default set-
ting. 4-gram language model is trained over the Xinhua portion of LDC English Gigaword Version 3.0 
and the English part of the bilingual training data. Feature weights are tuned with the minimum error 
rate training algorithm (Och, 2003).Translation performance is measured with case-insensitive BLEU4 
score (Papineni et al., 2002). 
All the Chinese sentences in the training set, development set and test set are parsed by an in-house 
developed dependency parser based on shift-reduce algorithm (Zhang and Nivre, 2011). There are 45 
named grammatical relations plus a default relation representing unknown cases. The detailed descrip-
tions about dependency parsing are explained in Chang et al. (2009). 
5.2 Experimental Results on FBIS Corpus 
We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline 
and the soft dependency matching model. Table 2 shows the statistics of FBIS corpus after the pre-
processing. 
 
2233
  #sentences #words 
Chinese 128,832 3,016,570 
English 128,832 3,922,816 
Table 2. The statistics of FBIS corpus 
 
The evaluation results over FBIS corpus are reported in Table 3. The first row shows the results of 
baseline, the next three rows show the effect of three features respectively and the last row gives the 
result when all features are integrated together. Based on Table 3, we can see that each individual fea-
ture improves the performance. Among all integrated features, the third feature  
 
 is the most effec-
tive one. The best performance is achieved when using all three features, where we get 1.4, 0.9 and 1.2 
BLEU points improvements respectively over the baseline on three test sets. 
 
 NIST04 NIST05 NIST08 
Baseline 33.53 32.97 25.08 
Baseline+fl 34.59 33.44 25.69 
Baseline+fu 34.48 33.59 25.51 
Baseline+fm 34.73 33.74 25.76 
Baseline+fl+fu+fm 34.96 33.91 26.28 
Table 3. Translation performance over BLEU% when models are trained on the FBIS corpus. 
5.3 Experimental Results on Large Scale  Corpus 
To further test the effect of our soft dependency matching model, we use a large scale corpus released 
by LDC. The catalog number of them is LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, 
LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85 and LDC2006E92. There are 498K sen-
tence pairs, 12.1M Chinese words and 13.8M English words. Table 4 summarizes the translation per-
formance on the large scale of corpus. Our model is still effective when we train the translation system 
on large scale data. We get 1.3, 0.7 and 1.0 BLEU point improvements over the baseline on three test 
sets respectively, which shows that our method can consistently improve HPB system over different 
sized training corpus. 
 
 NIST04 NIST05 NIST08 
Baseline 38.72 37.59 29.03 
Baseline+fl+fu+fm 40.00 38.34 30.06 
Table 4. Translation performance over BLEU% when models are trained on a large scale parallel 
 corpus. 
5.4 Decoding Cost 
Incorporating syntax can improve the translation performance, but it might increase the SMT decoding 
complexity. One advantage of our method is that it does not increase the amount of translation rules, 
so the searching space is not enlarged. Table 5 shows the decoding time comparison with the baseline 
when models are trained on the FBIS corpus. The average decoding time per sentence is only in-
creased by about 12% due to the parsing of source sentences and the computation of the features. We 
believe that this is acceptable given the performance gain. 
 
 NIST04 NIST05 NIST08 
Baseline 0.67sec 0.78sec 0.50sec 
Baseline+fl+fu+fm 0.88sec 0.87sec 0.56sec 
Table 5.  The average decoding time per sentence, measured in second/sentence. 
6 Conclusion and Future Work 
We proposed a soft dependency matching model for HPB machine translation. We enrich the HPB 
rule with dependency knowledge learnt from the training data. The dependency knowledge allows our 
model to capture the both the dependency relations inside the rule and the dependency relations be-
tween the rule and its context from a global view. During decoding, the syntax structural consistency 
of rules against source dependency tree is calculated and converted into SMT log-linear model fea-
2234
tures to guide the translation process. The experimental results show that our soft matching model 
achieves significant improvements over a strong baseline of an in-house implemented HPB system. 
In future work, there is much room to improve the performance via our method. First, we can dis-
criminatively learn the contribution of the dependency knowledge of each rule based on the training 
data. Second, we can go beyond the current ?bag of dependency triples? representation by composing 
them hierarchically to capture deep syntactic information. Third, section 2 has discussed the theoreti-
cal difference with related work on adding source syntax into the HPB model, we are interested in 
empirically comparing our method with them and combining it with them to get further improvement. 
Acknowledgments 
We thank anonymous reviewers for insightful comments. The work of Hailong Cao is sponsored by 
Microsoft Research Asia Star Track Visiting Young Faculty Program. The work of HIT is also funded 
by the project of National Natural Science Foundation of China (No. 61173073) and International Sci-
ence & Technology Cooperation Program of China (No. 2014DFA11350). 
Reference 
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D. Manning.  2009. Discriminative Reordering 
with Chinese Grammatical Relations Features. In Proceedings of NAACL Workshop on SSST. 
Colin Cherry. 2008. Cohesive Phrase-based Decoding for Statistical Machine Translation. In Proceedings of 
ACL. 
David Chiang. 2007. Hierarchical Phrase-based Translation. Computational Linguistics, 33(2):201?228. 
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011. Soft Dependency Constraints for Reordering in Hierar-
chical Phrase-Based Translation. In Proceedings of EMNLP.  
Zhongjun He, Qun Liu, Shouxun Lin. 2008. Improving Statistical Machine Translation using Lexicalized Rule 
Selection. In Proceedings of COLING. 
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended-
domain of locality. In Proceedings of AMTA. 
Zhongqiang Huang, Martin ?mejrek, and Bowen Zhou. 2010. Soft Syntactic Constraints for Hierarchical Phrase-
based Translation Using Latent Syntactic Distributions. In Proceedings of EMNLP. 
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib. 2013. Factored Soft Syntactic Contraints for Hierarchical 
Machine Translation. In Proceedings of EMNLP.  
Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of ACL. 
Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL. 
Philipp Koehn, Franz Josef Och, Daniel Marcu. 2003. Statistical phrase based translation. In Proceedings of 
NAACL. 
Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef van Genabith. 2012. Using Syntactic Head Information in 
Hierarchical Phrase-based Translation. In Proceedings of WMT. 
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree to-string alignment template for statistical machine translation. 
In Proceedings of ACL. 
Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In 
Proceedings of ACL.  
Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of EMNLP.  
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evalua-
tion of Machine Translation. In Proceedings of ACL. 
 Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm 
with a target dependency language model. In Proceedings of ACL. 
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas, and Ralph Weischedel. 2009. Effective use of linguistic 
and contextual information for statistical machine translation. In Proceedings of EMNLP.  
2235
Daniel Stein, Stephan Peitz, David Vilar, and Hermann Ney. 2010. A Cocktail of Deep Syntactic Features for 
Hierarchical Machine Translation. In Conference of the Association for Machine Translation in the Americas. 
Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377?404. 
Jun Xie, Haitao Mi and Qun Liu. 2011. A Novel Dependency-to-String Model for Statistical Machine Transla-
tion. In Proceedings of EMNLP. 
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statisti-
cal Machine Translation. In Proceedings of ACL. 
Deyi Xiong, Min Zhang, Aiti Aw, Haizhou Li. 2009. A Syntax-Driven Bracketing Model for Phrase-Based 
Translation.  In Proceedings of ACL. 
Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing. In 
Proceedings of NAACL Workshop on SMT.  
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence align-
ment-based tree-to-tree translation model. In Proceedings of ACL. 
Yue Zhang and Joakim Nivre. 2011. Transition-based Dependency Parsing with Rich Non-local Features In Pro-
ceedings of ACL. 
2236
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 402?411, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Locally Training the Log-Linear Model for SMT
Lemao Liu1, Hailong Cao1, Taro Watanabe2, Tiejun Zhao1, Mo Yu1, CongHui Zhu1
1School of Computer Science and Technology
Harbin Institute of Technology, Harbin, China
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
{lmliu,hailong,tjzhao,yumo,chzhu}@mtlab.hit.edu.cn
taro.watanabe@nict.go.jp
Abstract
In statistical machine translation, minimum
error rate training (MERT) is a standard
method for tuning a single weight with regard
to a given development data. However, due to
the diversity and uneven distribution of source
sentences, there are two problems suffered by
this method. First, its performance is highly
dependent on the choice of a development set,
which may lead to an unstable performance
for testing. Second, translations become in-
consistent at the sentence level since tuning is
performed globally on a document level. In
this paper, we propose a novel local training
method to address these two problems. Un-
like a global training method, such as MERT,
in which a single weight is learned and used
for all the input sentences, we perform training
and testing in one step by learning a sentence-
wise weight for each input sentence. We pro-
pose efficient incremental training methods to
put the local training into practice. In NIST
Chinese-to-English translation tasks, our lo-
cal training method significantly outperforms
MERT with the maximal improvements up to
2.0 BLEU points, meanwhile its efficiency is
comparable to that of the global method.
1 Introduction
Och and Ney (2002) introduced the log-linear model
for statistical machine translation (SMT), in which
translation is considered as the following optimiza-
tion problem:
e?(f ;W ) = arg max
e
P(e|f ;W )
= arg max
e
exp
{
W ? h(f, e)
}
?
e? exp
{
W ? h(f, e?)
}
= arg max
e
{
W ? h(f, e)
}
, (1)
where f and e (e?) are source and target sentences,
respectively. h is a feature vector which is scaled
by a weight W . Parameter estimation is one of
the most important components in SMT, and var-
ious training methods have been proposed to tune
W . Some methods are based on likelihood (Och and
Ney, 2002; Blunsom et al2008), error rate (Och,
2003; Zhao and Chen, 2009; Pauls et al2009; Gal-
ley and Quirk, 2011), margin (Watanabe et al2007;
Chiang et al2008) and ranking (Hopkins and May,
2011), and among which minimum error rate train-
ing (MERT) (Och, 2003) is the most popular one.
All these training methods follow the same
pipeline: they train only a single weight on a given
development set, and then use it to translate all the
sentences in a test set. We call them a global train-
ing method. One of its advantages is that it allows us
to train a single weight offline and thereby it is effi-
cient. However, due to the diversity and uneven dis-
tribution of source sentences(Li et al2010), there
are some shortcomings in this pipeline.
Firstly, on the document level, the performance of
these methods is dependent on the choice of a devel-
opment set, which may potentially lead to an unsta-
ble translation performance for testing. As referred
in our experiment, the BLEU points on NIST08 are
402
 Source  Candidate Translation   
i  
i
f  j  
ij
e  h  score  
1 ? ? ?? ? 1 I am students . <2, 1> 0.5 
  2 I was students . <1,1> 0.2 
2 ?? ?? ? ? 1 week several today ? <1,2> 0.3 
  2 today several weeks . <3,2> 0.1 
 
(a) (b)
2 21 2 222,0 ( , ) ( , )h f e h f e? ? ?? ?
2 22 2 212,0 ( , ) ( , )h f e h f e? ?? ?1 11 1 11, 0 ( , ) ( , )h f e h f e? ?? ?
1 12 1 111,0 ( , ) ( , )h f e h f e? ? ?? ?
2 22 2 21( , ) ( , )h f e h f e?
1 11 1 12( , ) ( , )h f e h f e?
<-2,0>
<-1,0>
<1,0>
<2,0>
0h1h
. .* *
2 21 2 22( , ) ( , )h f e h f e?
1 12 1 11( , ) ( , )h f e h f e?
Figure 1: (a). An Example candidate space of dimensionality two. score is a evaluation metric of e. (b). The non-
linearly separable classification problem transformed from (a) via tuning as ranking (Hopkins and May, 2011). Since
score of e11 is greater than that of e12, ?1, 0? corresponds to a possitive example denoted as ???, and ??1, 0? corre-
sponds to a negative example denoted as ?*?. Since the transformed classification problem is not linearly separable,
there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile. However, one can
obtain e11 and e21 with weights: ?1, 1? and ??1, 1?, respectively.
19.04 when the Moses system is tuned on NIST02
by MERT. However, its performance is improved to
21.28 points when tuned on NIST06. The automatic
selection of a development set may partially address
the problem. However it is inefficient since tuning
requires iteratively decoding an entire development
set, which is impractical for an online service.
Secondly, translation becomes inconsistent on the
sentence level (Ma et al2011). Global training
method such as MERT tries to optimize the weight
towards the best performance for the whole set, and
it can not necessarily always obtain good translation
for every sentence in the development set. The rea-
son is that different sentences may need different
optimal weights, and MERT can not find a single
weight to satisfy all of the sentences. Figure 1(a)
shows such an example, in which a development set
contains two sentences f1 and f2 with translations e
and feature vectors h. When we tune examples in
Figure 1(a) by MERT, it can be regarded as a non-
linearly separable classification problem illustrated
in Figure 1(b). Therefore, there exists no single
weightW which simultaneously obtains e11 and e21
as translation for f1 and f2 via Equation (1). How-
ever, we can achieve this with two weights: ?1, 1?
for f1 and ??1, 1? for f2.
In this paper, inspired by KNN-SVM (Zhang et
al., 2006), we propose a local training method,
which trains sentence-wise weights instead of a sin-
gle weight, to address the above two problems.
Compared with global training methods, such as
MERT, in which training and testing are separated,
our method works in an online fashion, in which
training is performed during testing. This online
fashion has an advantage in that it can adapt the
weights for each of the test sentences, by dynam-
ically tuning the weights on translation examples
which are similar to these test sentences. Similar
to the method of development set automatical selec-
tion, the local training method may also suffer the
problem of efficiency. To put it into practice, we
propose incremental training methods which avoid
retraining and iterative decoding on a development
set.
Our local training method has two advantages:
firstly, it significantly outperforms MERT, especially
when test set is different from the development set;
secondly, it improves the translation consistency.
Experiments on NIST Chinese-to-English transla-
tion tasks show that our local training method sig-
nificantly gains over MERT, with the maximum im-
provements up to 2.0 BLEU, and its efficiency is
comparable to that of the global training method.
2 Local Training and Testing
The local training method (Bottou and Vapnik,
1992) is widely employed in computer vision
(Zhang et al2006; Cheng et al2010). Compared
with the global training method which tries to fit
a single weight on the training data, the local one
learns weights based on the local neighborhood in-
formation for each test example. It is superior to
403
the global one when the data sets are not evenly
distributed (Bottou and Vapnik, 1992; Zhang et al
2006).
Algorithm 1 Naive Local Training Method
Input: T = {ti}Ni=1(test set), K (retrieval size),
Dev(development set), D(retrieval data)
Output: Translation results of T
1: for all sentence ti such that 1 ? i ? N do
2: Retrieve the training examples Di with size
K for ti from D according to a similarity;
3: Train a local weight W i based on Dev and
Di;
4: Decode ti with W i;
5: end for
Suppose T be a test set, Dev a development set,
and D a retrieval data. The local training in SMT
is described in the Algorithm 1. For each sentence
ti in test set, training examples Di is retrieved from
D using a similarity measure (line 2), a weight W i
is optimized on Dev and Di (line 3)1, and, finally,
ti is decoded with W i for testing (line 4). At the
end of this algorithm, it returns the translation re-
sults for T . Note that weights are adapted for each
test sentence ti in line 3 by utilizing the translation
examples Di which are similar to ti. Thus, our local
training method can be considered as an adaptation
of translation weights.
Algorithm 1 suffers a problem of training effi-
ciency in line 3. It is impractical to train a weight
W i on Dev and Di from scratch for every sen-
tence, since iteratively decodingDev andDi is time
consuming when we apply MERT. To address this
problem, we propose a novel incremental approach
which is based on a two-phase training.
On the first phase, we use a global training
method, like MERT, to tune a baseline weight on
the development set Dev in an offline manner. On
the second phase, we utilize the retrieved examples
to incrementally tune sentence-wise local weights
based on the baseline weight. This method can
not only consider the common characteristics learnt
from the Dev, but also take into account the knowl-
1Usually, the quality of development set Dev is high, since
it is manually produced with multiple references. This is the
main reason why Dev is used as a part of new development set
to train W i.
edge for each individual sentence learnt from sim-
ilar examples during testing. On the phase of in-
cremental training, we perform decoding only once
for retrieved examples Di, though several rounds of
decoding are possible and potentially better if one
does not seriously care about training speed. Fur-
thermore, instead of on-the-fly decoding, we decode
the retrieval data D offline using the parameter from
our baseline weight and its nbest translation candi-
dates are saved with training examples to increase
the training efficiency.
Algorithm 2 Local Training Method Based on In-
cremental Training
Input: T = {ti}Ni=1 (test set), K (retrieval size),
Dev (development set),
D = {?fs, rs?}s=Ss=1 (retrieval data),
Output: Translation results of T
1: Run global Training (such as MERT) on Dev to
get a baseline weight Wb; // Phase 1
2: Decode each sentence in D to get
D = {?fs, cs, rs?}s=Ss=1 ;
3: for all sentence ti such that 1 ? i ? N do
4: Retrieve K training examples Di =
{?f ij , c
i
j , r
i
j?}
j=K
j=1 for ti from D according to
a similarity;
5: Incrementally train a local weight W i based
on Wb and Di; // Phase 2
6: Decode ti with W i;
7: end for
The two-phase local training algorithm is de-
scribed in Algorithm 2, where cs and rs denote the
translation candidate set and reference set for each
sentence fs in retrieval data, respectively, and K is
the retrieval size. It globally trains a baseline weight
Wb (line 1), and decodes each sentence in retrieval
data D with the weight Wb (line 2). For each sen-
tence ti in test set T , it first retrieves training exam-
ples Di from D (line 4), and then it runs local train-
ing to tune a local weight W i (line 5) and performs
testing with W i for ti (line 6). Please note that the
two-phase training contains global training in line 1
and local training in line 5.
From Algorithm 2, one can see that our method is
effective even if the test set is unknow, for example,
in the scenario of online translation services, since
the global training on development set and decoding
404
on retrieval data can be performed offline.
In the next two sections, we will discuss the de-
tails about the similarity metric in line 4 and the in-
cremental training in line 5 of Algorithm 2.
3 Acquiring Training Examples
In line 4 of Algorithm 2, to retrieve training exam-
ples for the sentence ti , we first need a metric to
retrieve similar translation examples. We assume
that the metric satisfy the property: more similar the
test sentence and translation examples are, the better
translation result one obtains when decoding the test
sentence with the weight trained on the translation
examples.
The metric we consider here is derived from
an example-based machine translation. To retrieve
translation examples for a test sentence, (Watanabe
and Sumita, 2003) defined a metric based on the
combination of edit distance and TF-IDF (Manning
and Schu?tze, 1999) as follows:
dist(f1, f2) = ? ? edit-dist(f1, f2)+
(1? ?)? tf-idf(f1, f2), (2)
where ?(0 ? ? ? 1) is an interpolation weight,
fi(i = 1, 2) is a word sequence and can be also
considered as a document. In this paper, we extract
similar examples from training data. Like example-
based translation in which similar source sentences
have similar translations, we assume that the optimal
translation weights of the similar source sentences
are closer.
4 Incremental Training Based on
Ultraconservative Update
Compared with retraining mode, incremental train-
ing can improve the training efficiency. In the field
of machine learning research, incremental training
has been employed in the work (Cauwenberghs and
Poggio, 2001; Shilton et al2005), but there is lit-
tle work for tuning parameters of statistical machine
translation. The biggest difficulty lies in that the fea-
ture vector of a given training example, i.e. transla-
tion example, is unavailable until actually decoding
the example, since the derivation is a latent variable.
In this section, we will investigate the incremental
training methods in SMT scenario.
Following the notations in Algorithm 2, Wb is
the baseline weight, Di = {?f ij , c
i
j , r
i
j?}
K
j=1 denotes
training examples for ti. For the sake of brevity, we
will drop the index i, Di = {?fj , cj , rj?}Kj=1, in the
rest of this paper. Our goal is to find an optimal
weight, denoted by W i, which is a local weight and
used for decoding the sentence ti. Unlike the global
method which performs tuning on the whole devel-
opment set Dev +Di as in Algorithm 1, W i can be
incrementally learned by optimizing onDi based on
Wb. We employ the idea of ultraconservative update
(Crammer and Singer, 2003; Crammer et al2006)
to propose two incremental methods for local train-
ing in Algorithm 2 as follows.
Ultraconservative update is an efficient way to
consider the trade-off between the progress made on
development set Dev and the progress made on Di.
It desires that the optimal weight W i is not only
close to the baseline weight Wb, but also achieves
the low loss over the retrieved examples Di. The
idea of ultraconservative update can be formalized
as follows:
min
W
{
d(W,Wb) + ? ? Loss(D
i,W )
}
, (3)
where d(W,Wb) is a distance metric over a pair
of weights W and Wb. It penalizes the weights
far away from Wb and it is L2 norm in this paper.
Loss(Di,W ) is a loss function of W defined on Di
and it evaluates the performance of W over Di. ?
is a positive hyperparameter. If Di is more similar
to the test sentence ti, the better performance will be
achieved for the larger ?. In particular, ifDi consists
of only a single sentence ti, the best performance
will be obtained when ? goes to infinity.
4.1 Margin Based Ultraconservative Update
MIRA(Crammer and Singer, 2003; Crammer et al
2006) is a form of ultraconservative update in (3)
whoseLoss is defined as hinge loss based on margin
over the pairwise translation candiates in Di. It tries
to minimize the following quadratic program:
1
2
||W ?Wb||
2+
?
K
K?
j=1
max
1?n?|cj |
(
`jn?W ??h(fj , ejn)
)
with
?h(fj , ejn) = h(fj , ej?)? h(fj , ejn), (4)
405
where h(fj , e) is the feature vector of candidate e,
ejn is a translation member of fj in cj , ej? is the
oracle one in cj , `jn is a loss between ej? and ejn
and it is the same as referred in (Chiang et al2008),
and |cj | denotes the number of members in cj .
Different from (Watanabe et al2007; Chiang
et al2008) employing the MIRA to globally train
SMT, in this paper, we apply MIRA as one of local
training method for SMT and we call it as margin
based ultraconservative update (MBUU for shortly)
to highlight its advantage of incremental training in
line 5 of Algorithm 2.
Further, there is another difference between
MBUU and MIRA in (Watanabe et al2007; Chi-
ang et al2008). MBUU is a batch update mode
which updates the weight with all training examples,
but MIRA is an online one which updates with each
example (Watanabe et al2007) or part of examples
(Chiang et al2008). Therefore, MBUU is more ul-
traconservative.
4.2 Error Rate Based Ultraconservative
Update
Instead of taking into account the margin-based
hinge loss between a pair of translations as the Loss
in (3), we directly optimize the error rate of trans-
lation candidates with respect to their references in
Di. Formally, the objective function of error rate
based ultraconservative update (EBUU) is as fol-
lows:
1
2
?W ?Wb?
2 +
?
K
K?
j=1
Error(rj ; e?(fj ;W )), (5)
where e?(fj ;W ) is defined in Equation (1), and
Error(rj , e) is the sentence-wise minus BLEU (Pa-
pineni et al2002) of a candidate e with respect to
rj .
Due to the existence of L2 norm in objective
function (5), the optimization algorithm MERT can
not be applied for this question since the exact line
search routine does not hold here. Motivated by
(Och, 2003; Smith and Eisner, 2006), we approxi-
mate the Error in (5) by the expected loss, and then
derive the following function:
1
2
?W?Wb?
2+
?
K
K?
j=1
?
e
Error(rj ; e)P?(e|fj ;W ),
(6)
Systems NIST02 NIST05 NIST06 NIST08
Moses 30.39 26.31 25.34 19.07
Moses hier 33.68 26.94 26.28 18.65
In-Hiero 31.24 27.07 26.32 19.03
Table 1: The performance comparison of the baseline In-
Hiero VS Moses and Moses hier.
with
P?(e|fj ;W ) =
exp[?W ? h(fj , e)]
?
e??cj exp[?W ? h(fj , e
?)]
, (7)
where ? > 0 is a real number valued smoother. One
can see that, in the extreme case, for ? ? ?, (6)
converges to (5).
We apply the gradient decent method to minimize
the function (6), as it is smooth with respect to ?.
Since the function (6) is non-convex, the solution
obtained by gradient descent method may depend on
the initial point. In this paper, we set the initial point
as Wb in order to achieve a desirable solution.
5 Experiments and Results
5.1 Setting
We conduct our experiments on the Chinese-to-
English translation task. The training data is FBIS
corpus consisting of about 240k sentence pairs. The
development set is NIST02 evaluation data, and the
test datasets are NIST05, NIST06,and NIST08.
We run GIZA++ (Och and Ney, 2000) on the
training corpus in both directions (Koehn et al
2003) to obtain the word alignment for each sen-
tence pair. We train a 4-gram language model on
the Xinhua portion of the English Gigaword cor-
pus using the SRILM Toolkits (Stolcke, 2002) with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998). In our experiments the translation per-
formances are measured by case-insensitive BLEU4
metric (Papineni et al2002) and we use mteval-
v13a.pl as the evaluation tool. The significance test-
ing is performed by paired bootstrap re-sampling
(Koehn, 2004).
We use an in-house developed hierarchical
phrase-based translation (Chiang, 2005) as our base-
line system, and we denote it as In-Hiero. To ob-
tain satisfactory baseline performance, we tune In-
Hiero system for 5 times using MERT, and then se-
406
Methods Steps Seconds
Global method Decoding 2.0
Local method Retrieval +0.6
Local training +0.3
Table 2: The efficiency of the local training and testing
measured by sentence averaged runtime.
Methods NIST05 NIST06 NIST08
Global MERT 27.07 26.32 19.03
Local MBUU 27.75+ 27.88+ 20.84+
EBUU 27.85+ 27.99+ 21.08+
Table 3: The performance comparison of local train-
ing methods (MBUU and EBUU) and a global method
(MERT). NIST05 is the set used to tune ? for MBUU and
EBUU, and NIST06 and NIST08 are test sets. + means
the local method is significantly better than MERT with
p < 0.05.
lect the best-performing one as our baseline for the
following experiments. As Table 1 indicates, our
baseline In-Hiero is comparable to the phrase-based
MT (Moses) and the hierarchical phrase-based MT
(Moses hier) implemented in Moses, an open source
MT toolkit2 (Koehn et al2007). Both of these sys-
tems are with default setting. All three systems are
trained by MERT with 100 best candidates.
To compare the local training method in Algo-
rithm 2, we use a standard global training method,
MERT, as the baseline training method. We do not
compare with Algorithm 1, in which retraining is
performed for each input sentence, since retraining
for the whole test set is impractical given that each
sentence-wise retraining may take some hours or
even days. Therefore, we just compare Algorithm
2 with MERT.
5.2 Runtime Results
To run the Algorithm 2, we tune the baseline weight
Wb on NIST02 by MERT3. The retrieval data is set
as the training data, i.e. FBIS corpus, and the re-
trieval size is 100. We translate retrieval data with
Wb to obtain their 100 best translation candidates.
We use the simple linear interpolated TF-IDF met-
ric with ? = 0.1 in Section 3 as the retrieval metric.
2See web: http://www.statmt.org
3Wb is exactly the weight of In-Hiero in Table 1.
NIST05 NIST06 NIST08
NIST02 0.665 0.571 0.506
Table 4: The similarity of development and three test
datasets.
For an efficient tuning, the retrieval process is par-
allelized as follows: the examples are assigned to 4
CPUs so that each CPU accepts a query and returns
its top-100 results, then all these top-100 results are
merged into the final top-100 retrieved examples to-
gether with their translation candidates. In our ex-
periments, we employ the two incremental training
methods, i.e. MBUU and EBUU. Both of the hyper-
parameters ? are tuned on NIST05 and set as 0.018
and 0.06 for MBUU and EBUU, respectively. In
the incremental training step, only one CPU is em-
ployed.
Table 2 depicts that testing each sentence with lo-
cal training method takes 2.9 seconds, which is com-
parable to the testing time 2.0 seconds with global
training method4. This shows that the local method
is efficient. Further, compared to the retrieval, the
local training is not the bottleneck. Actually, if we
use LSH technique (Andoni and Indyk, 2008) in re-
trieval process, the local method can be easily scaled
to a larger training data.
5.3 Results and Analysis
Table 3 shows the main results of our local train-
ing methods. The EBUU training method signifi-
cantly outperforms the MERT baseline, and the im-
provement even achieves up to 2.0 BLEU points on
NIST08. We can also see that EBUU and MBUU are
comparable on these three test sets. Both of these
two local training methods achieve significant im-
provements over the MERT baseline, which proves
the effectiveness of our local training method over
global training method.
Although both local methods MBUU and EBUU
achieved improvements on all the datasets, their
gains on NIST06 and NIST08 are significantly
higher than those achieved on NIST05 test dataset.
We conjecture that, the more different a test set and
a development set are, the more potential improvem-
4The runtime excludes the time of tuning and decoding on D
in Algorithm 2, since both of them can be performanced offline.
407
0 . 0 0 0 . 0 2 0 . 0 4 0 . 0 6 0 . 0 8 0 . 1 01 82 02 2
2 42 62 8  
 
 N I S T 0 5 N I S T 0 6 N I S T 0 8BLEU l
Figure 2: The peformance of EBUU for different ? over
all the test datasets. The horizontal axis denotes the val-
ues of ? in function (6), and the vertical one denotes the
BLEU points.
Metthods Dev NIST08
NIST02 19.03
MERT NIST05 20.06
NIST06 21.28
EBUU NIST02 21.08
Table 5: The comparison of MERT with different de-
velopment datasets and local training method based on
EBUU.
nts local training has for the sentences in this test set.
To test our hypothesis, we measured the similarity
between the development set and a test set by the
average value5 of accumulated TF-IDF scores of de-
velopment dataset and each sentence in test datasets.
Table 4 shows that NIST06 and NIST08 are more
different from NIS02 than NIST05, thus, this is po-
tentially the reason why local training is more effec-
tive on NIST06 and NIST08.
As mentioned in Section 1, the global training
methods such as MERT are highly dependent on de-
velopment sets, which can be seen in Table 5. There-
fore, the translation performance will be degraded if
one chooses a development data which is not close
5Instead of using the similarity between two documents de-
velopment and test datasets, we define the similarity as the av-
erage similarity of the development set and the sentences in test
set. The reason is that it reduces its dependency on the number
of sentences in test dataset, which may cause a bias.
Methods Number Percents
MERT 1735 42.3%
EBUU 1606 39.1%
Table 6: The statistics of sentences with 0.0 sentence-
level BLEU points over three test datasets.
to the test data. We can see that, with the help of the
local training, we still gain much even if we selected
an unsatisfactory development data.
As also mentioned in Section 1, the global meth-
ods do not care about the sentence level perfor-
mance. Table 6 depicts that there are 1735 sentences
with zero BLEU points in all the three test datasets
for MERT. Besides obtaining improvements on doc-
ument level as referred in Table 3, the local training
methods can also achieve consistent improvements
on sentence level and thus can improve the users?
experiences.
The hyperparameters ? in both MBUU (4) and
EBUU (6) has an important influence on transla-
tion performance. Figure 2 shows such influence
for EBUU on the test datasets. We can see that, the
performances on all these datasets improve as ? be-
comes closer to 0.06 from 0, and the performance
continues improving when ? passes over 0.06 on
NIST08 test set, where the performance constantly
improves up to 2.6 BLEU points over baseline. As
mentioned in Section 4, if the retrieved examples are
very similar to the test sentence, the better perfor-
mance will be achieved with the larger ?. There-
fore, it is reasonable that the performances improved
when ? increased from 0 to 0.06. Further, the turn-
ing point appearing at 0.06 proves that the ultra-
conservative update is necessary. We can also see
that the performance on NIST08 consistently im-
proves and achieves the maximum gain when ? ar-
rives at 0.1, but those on both NIST05 and NIST06
achieves the best when it arrives at 0.06. This
phenomenon can also be interpreted in Table 4 as
the lowest similarity between the development and
NIST08 datasets.
Generally, the better performance may be
achieved when more examples are retrieved. Actu-
ally, in Table 7 there seems to be little dependency
between the numbers of examples retrieved and the
translation qualities, although they are positively re-
408
Retrieval Size NIST05 NIST06 NIST08
40 27.66 27.81 20.87
70 27.77 27.93 21.08
100 27.85 27.99 21.08
Table 7: The performance comparison by varying re-
trieval size in Algorithm 2 based on EBUU.
Methods NIST05 NIST06 NIST08
MERT 27.07 26.32 19.03
EBUU 27.85 27.99 21.08
Oracle 29.46 29.35 22.09
Table 8: The performance of Oracle of 2-best results
which consist of 1-best resluts of MERT and 1-best
resluts of EBUU.
lated approximately.
Table 8 presents the performance of the oracle
translations selected from the 1-best translation re-
sults of MERT and EBUU. Clearly, there exists more
potential improvement for local training method.
6 Related Work
Several works have proposed discriminative tech-
niques to train log-linear model for SMT. (Och and
Ney, 2002; Blunsom et al2008) used maximum
likelihood estimation to learn weights for MT. (Och,
2003; Moore and Quirk, 2008; Zhao and Chen,
2009; Galley and Quirk, 2011) employed an eval-
uation metric as a loss function and directly opti-
mized it. (Watanabe et al2007; Chiang et al2008;
Hopkins and May, 2011) proposed other optimiza-
tion objectives by introducing a margin-based and
ranking-based indirect loss functions.
All the methods mentioned above train a single
weight for the whole development set, whereas our
local training method learns a weight for each sen-
tence. Further, our translation framework integrates
the training and testing into one unit, instead of treat-
ing them separately. One of the advantages is that it
can adapt the weights for each of the test sentences.
Our method resorts to some translation exam-
ples, which is similar as example-based translation
or translation memory (Watanabe and Sumita, 2003;
He et al2010; Ma et al2011). Instead of using
translation examples to construct translation rules
for enlarging the decoding space, we employed them
to discriminatively learn local weights.
Similar to (Hildebrand et al2005; Lu? et al
2007), our method also employes IR methods to re-
trieve examples for a given test set. Their methods
utilize the retrieved examples to acquire translation
model and can be seen as the adaptation of trans-
lation model. However, ours uses the retrieved ex-
amples to tune the weights and thus can be consid-
ered as the adaptation of tuning. Furthermore, since
ours does not change the translation model which
needs to run GIZA++ and it incrementally trains lo-
cal weights, our method can be applied for online
translation service.
7 Conclusion and Future Work
This paper proposes a novel local training frame-
work for SMT. It has two characteristics, which
are different from global training methods such as
MERT. First, instead of training only one weight for
document level, it trains a single weight for sentence
level. Second, instead of considering the training
and testing as two separate units, we unify the train-
ing and testing into one unit, which can employ the
information of test sentences and perform sentence-
wise local adaptation of weights.
Local training can not only alleviate the prob-
lem of the development data selection, but also re-
duce the risk of sentence-wise bad translation re-
sults, thus consistently improve the translation per-
formance. Experiments show gains up to 2.0 BLEU
points compared with a MERT baseline. With the
help of incremental training methods, the time in-
curred by local training was negligible and the local
training and testing totally took 2.9 seconds for each
sentence.
In the future work, we will further investigate the
local training method, since there are more room for
improvements as observed in our experiments. We
will test our method on other translation models and
larger training data6.
Acknowledgments
We would like to thank Hongfei Jiang and Shujie
Liu for many valuable discussions and thank three
6Intuitionally, when the corpus of translation examples is
larger, the retrieval results in Algorithm 2 are much similar as
the test sentence. Therefore our method may favor this.
409
anonymous reviewers for many valuable comments
and helpful suggestions. This work was supported
by National Natural Science Foundation of China
(61173073,61100093), and the Key Project of the
National High Technology Research and Develop-
ment Program of China (2011AA01A207), and the
Fundamental Research Funds for Central Univer-
sites (HIT.NSRIF.2013065).
References
Alexandr Andoni and Piotr Indyk. 2008. Near-optimal
hashing algorithms for approximate nearest neighbor
in high dimensions. Commun. ACM, 51(1):117?122,
January.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL,
pages 200?208, Columbus, Ohio, June. Association
for Computational Linguistics.
Le?on Bottou and Vladimir Vapnik. 1992. Local learning
algorithms. Neural Comput., 4:888?900, November.
G. Cauwenberghs and T. Poggio. 2001. Incremental
and decremental support vector machine learning. In
Advances in Neural Information Processing Systems
(NIPS*2000), volume 13.
Stanley F Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. In Technical Report TR-10-98. Harvard Univer-
sity.
Haibin Cheng, Pang-Ning Tan, and Rong Jin. 2010. Ef-
ficient algorithm for localized support vector machine.
IEEE Trans. on Knowl. and Data Eng., 22:537?549,
April.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 224?233, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?05, pages 263?270, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551?
585, December.
Michel Galley and Chris Quirk. 2011. Optimal search
for minimum error rate training. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 38?49, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging smt and tm with translation
recommendation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 622?630, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
S. Hildebrand, M. Eck, S. Vogel, and Alex Waibel. 2005.
Adaptation of the translation model for statistical ma-
chine translation based on information retrieval. In
Proceedings of EAMT. Association for Computational
Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL. ACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP.
ACL.
Mu Li, Yinggong Zhao, Dongdong Zhang, and Ming
Zhou. 2010. Adaptive development data selection for
log-linear model in statistical machine translation. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ?10, pages 662?
670, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
410
343?350, Prague, Czech Republic, June. Association
for Computational Linguistics.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent translation using discrim-
inative learning - a translation memory-inspired ap-
proach. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1239?1248, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statistical
machine translation. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics -
Volume 1, COLING ?08, pages 585?592, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?00, pages 440?447, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 295?302, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167, Sapporo, Japan,
July. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Adam Pauls, John Denero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1418?1427, Singapore, August. Association for
Computational Linguistics.
Alistair Shilton, Marimuthu Palaniswami, Daniel Ralph,
and Ah Chung Tsoi. 2005. Incremental training of
support vector machines. IEEE Transactions on Neu-
ral Networks, 16(1):114?131.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP.
Taro Watanabe and Eiichiro Sumita. 2003. Example-
based decoding for statistical machine translation. In
Proc. of MT Summit IX, pages 410?417.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic, June. Association for
Computational Linguistics.
Hao Zhang, Alexander C. Berg, Michael Maire, and Ji-
tendra Malik. 2006. Svm-knn: Discriminative near-
est neighbor classification for visual category recog-
nition. In Proceedings of the 2006 IEEE Computer
Society Conference on Computer Vision and Pattern
Recognition - Volume 2, CVPR ?06, pages 2126?2136,
Washington, DC, USA. IEEE Computer Society.
Bing Zhao and Shengyuan Chen. 2009. A simplex
armijo downhill algorithm for optimizing statistical
machine translation decoding parameters. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, Compan-
ion Volume: Short Papers, NAACL-Short ?09, pages
21?24, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
411
Proceedings of the ACL 2010 Conference Short Papers, pages 17?21,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Filtering Syntactic Constraints for Statistical Machine Translation 
 
 
Hailong Cao and Eiichiro Sumita 
Language Translation Group, MASTAR Project 
National Institute of Information and Communications Technology 
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289 
{hlcao, eiichiro.sumita }@nict.go.jp 
 
  
 
Abstract 
Source language parse trees offer very useful 
but imperfect reordering constraints for statis-
tical machine translation. A lot of effort has 
been made for soft applications of syntactic 
constraints. We alternatively propose the se-
lective use of syntactic constraints. A classifier 
is built automatically to decide whether a node 
in the parse trees should be used as a reorder-
ing constraint or not. Using this information 
yields a 0.8 BLEU point improvement over a 
full constraint-based system. 
1 Introduction 
In statistical machine translation (SMT), the 
search problem is NP-hard if arbitrary reordering 
is allowed (Knight, 1999). Therefore, we need to 
restrict the possible reordering in an appropriate 
way for both efficiency and translation quality. 
The most widely used reordering constraints are 
IBM constraints (Berger et al, 1996), ITG con-
straints (Wu, 1995) and syntactic constraints 
(Yamada et al, 2000; Galley et al, 2004; Liu et 
al., 2006; Marcu et al, 2006; Zollmann and 
Venugopal 2006; and numerous others). Syntac-
tic constraints can be imposed from the source 
side or target side. This work will focus on syn-
tactic constraints from source parse trees. 
Linguistic parse trees can provide very useful 
reordering constraints for SMT. However, they 
are far from perfect because of both parsing er-
rors and the crossing of the constituents and for-
mal phrases extracted from parallel training data. 
The key challenge is how to take advantage of 
the prior knowledge in the linguistic parse trees 
without affecting the strengths of formal phrases. 
Recent efforts attack this problem by using the 
constraints softly (Cherry, 2008; Marton and 
Resnik, 2008). In their methods, a candidate 
translation gets an extra credit if it respects the 
parse tree but may incur a cost if it violates a 
constituent boundary. 
In this paper, we address this challenge from a 
less explored direction. Rather than use all con-
straints offered by the parse trees, we propose 
using them selectively. Based on parallel training 
data, a classifier is built automatically to decide 
whether a node in the parse trees should be used 
as a reordering constraint or not. As a result, we 
obtain a 0.8 BLEU point improvement over a full 
constraint-based system.  
2 Reordering Constraints from Source 
Parse Trees 
In this section we briefly review a constraint-
based system named IST-ITG (Imposing Source 
Tree on Inversion Transduction Grammar, Ya-
mamoto et al, 2008) upon which this work 
builds. 
When using ITG constraints during decoding, 
the source-side parse tree structure is not consid-
ered. The reordering process can be more tightly 
constrained if constraints from the source parse 
tree are integrated with the ITG constraints. IST-
ITG constraints directly apply source sentence 
tree structure to generate the target with the 
following constraint: the target sentence is ob-
tained by rotating any node of the source sen-
tence tree structure. 
After parsing the source sentence, a bracketed 
sentence is obtained by removing the node 
syntactic labels; this bracketed sentence can then 
be directly expressed as a tree structure. For 
example1, the parse tree ?(S1 (S (NP (DT This)) 
(VP (AUX is) (NP (DT a) (NN pen)))))? is 
obtained from the source sentence ?This is a 
pen?, which consists of four words. By removing 
                                                 
1 We use English examples for the sake of readability. 
17
the node syntactic labels, the bracketed sentence 
?((This) ((is) ((a) (pen))))? is obtained. Such a 
bracketed sentence can be used to produce 
constraints.  
For example, for the source-side bracketed 
tree ?((f1 f2) (f3 f4)) ?, eight target sequences [e1, 
e2, e3, e4], [e2, e1, e3, e4], [e1, e2, e4, e3], [e2, 
e1, e4, e3], [e3, e4, e1, e2], [e3, e4, e2, e1], [e4, 
e3, e1, e2], and [e4, e3, e2, e1] are possible. For 
the source-side bracketed tree ?(((f1f2) f3) f4),? 
eight sequences [e1, e2, e3, e4], [e2, e1, e3, e4], 
[e3, e1, e2, e4], [e3, e2, e1, e4], [e4, e1, e2, e3], 
[e4, e2, e1, e3], [e4, e3, e1, e2], and [e4, e3, e2, 
e1] are possible. When the source sentence tree 
structure is a binary tree, the number of word 
orderings is reduced to 2N-1 where N is the length 
of the source sentence.  
The parsing results sometimes do not produce 
binary trees. In this case, some subtrees have 
more than two child nodes. For a non-binary sub-
tree, any reordering of child nodes is allowed. 
For example, if a subtree has three child nodes, 
six reorderings of the nodes are possible. 
3 Learning to Classify Parse Tree 
Nodes 
In IST-ITG and many other methods which use 
syntactic constraints, all of the nodes in the parse 
trees are utilized. Though many nodes in the 
parse trees are useful, we would argue that some 
nodes are not trustworthy. For example, if we 
constrain the translation of ?f1 f2 f3 f4? with 
node N2 illustrated in Figure 1, then word ?e1? 
will never be put in the middle the other three 
words. If we want to obtain the translation ?e2 e1 
e4 e3?, node N3 can offer a good constraint 
while node N2 should be filtered out. In real cor-
pora, cases such as node N2 are frequent enough 
to be noticeable (see Fox (2002) or section 4.1 in 
this paper). 
Therefore, we use the definitions in Galley et 
al. (2004) to classify the nodes in parse trees into 
two types: frontier nodes and interior nodes. 
Though the definitions were originally made for 
target language parse trees, they can be straight-
forwardly applied to the source side. A node 
which satisfies both of the following two condi-
tions is referred as a frontier node: 
 
? All the words covered by the node can be 
translated separately. That is to say, these 
words do not share a translation with any 
word outside the coverage of the node. 
? All the words covered by the node remain 
contiguous after translation. 
 
Otherwise the node is an interior node. 
For example, in Figure 1, both node N1 and 
node N3 are frontier nodes. Node N2 is an inte-
rior node because the source words f2, f3 and f4 
are translated into e2, e3 and e4, which are not 
contiguous in the target side. 
Clearly, only frontier nodes should be used as 
reordering constraints while interior nodes are 
not suitable for this. However, little work has 
been done on how to explicitly distinguish these 
two kinds of nodes in the source parse trees. In 
this section, we will explore building a classifier 
which can label the nodes in the parse trees as 
frontier nodes or interior nodes.  
 
Figure 1: An example parse tree and align-
ments 
3.1 Training 
Ideally, we would have a human-annotated cor-
pus in which each sentence is parsed and each 
node in the parse trees is labeled as a frontier 
node or an interior node. But such a target lan-
guage specific corpus is hard to come by, and 
never in the quantity we would like. 
Instead, we generate such a corpus automati-
cally. We begin with a parallel corpus which will 
be used to train our SMT model. In our case, it is 
the FBIS Chinese-English corpus.  
Firstly, the Chinese sentences are segmented, 
POS tagged and parsed by the tools described in 
Kruengkrai et al (2009) and Cao et al (2007), 
both of which are trained on the Penn Chinese 
Treebank 6.0. 
Secondly, we use GIZA++ to align the sen-
tences in both the Chinese-English and English-
Chinese directions. We combine the alignments 
using the ?grow-diag-final-and? procedure pro-
vided with MOSES (Koehn, 2007). Because 
there are many errors in the alignment, we re-
move the links if the alignment count is less than 
three for the source or the target word. Addition-
ally, we also remove notoriously bad links in 
  f1        f2      f3   f4 
 
  e2       e1      e4   e3 
N3 
N2
N1
18
{de, le} ? {the, a, an} following Fossum and 
Knight (2008).  
Thirdly, given the parse trees and the align-
ment information, we label each node as a fron-
tier node or an interior node according to the 
definition introduced in this section. Using the 
labeled nodes as training data, we can build a 
classifier. In theory, a broad class of machine 
learning tools can be used; however, due to the 
scale of the task (see section 4), we utilize the 
Pegasos 2  which is a very fast SVM solver 
(Shalev-Shwartz et al 2007).  
3.2 Features 
For each node in the parse trees, we use the fol-
lowing feature templates: 
? A context-free grammar rule which rewrites 
the current node (In this and all the following 
grammar based features, a mark is used to 
indicate which non terminal is the current 
node.) 
? A context-free grammar rule which rewrites 
the current node?s father 
? The combination of the above two rules  
? A lexicalized context-free grammar rule 
which rewrites the current node 
? A lexicalized context-free grammar rule 
which rewrites the current node?s father 
? Syntactic label, head word, and head POS 
tag of the current node 
? Syntactic label, head word, and head POS 
tag of the current node?s left child 
? Syntactic label, head word, and head POS 
tag of the current node?s right child 
? Syntactic label, head word, and head POS 
tag of the current node?s left brother  
? Syntactic label, head word, and head POS 
tag of the current node?s right brother  
? Syntactic label, head word, and head POS 
tag of the current node?s father 
? The leftmost word covered by the current 
node and the word before it 
? The rightmost word covered by the current 
node and the word after it 
 
4 Experiments 
Our SMT system is based on a fairly typical 
phrase-based model (Finch and Sumita, 2008). 
For the training of our SMT model, we use a 
modified training toolkit adapted from the 
                                                 
2 http://www.cs.huji.ac.il/~shais/code/index.html 
MOSES decoder. Our decoder can operate on the 
same principles as the MOSES decoder. Mini-
mum error rate training (MERT) with respect to 
BLEU score is used to tune the decoder?s pa-
rameters, and it is performed using the standard 
technique of Och (2003). A lexical reordering 
model was used in our experiments.  
The translation model was created from the 
FBIS corpus. We used a 5-gram language model 
trained with modified Knesser-Ney smoothing. 
The language model was trained on the target 
side of FBIS corpus and the Xinhua news in GI-
GAWORD corpus. The development and test 
sets are from NIST MT08 evaluation campaign. 
Table 1 shows the statistics of the corpora used 
in our experiments. 
 
Data Sentences Chinese 
words 
English 
words 
Training set 243,698 7,933,133 10,343,140 
Development set 1664 38,779 46,387 
Test set 1357 32377 42,444 
GIGAWORD 19,049,757 - 306,221,306 
 
Table 1: Corpora statistics 
 
4.1 Experiments on Nodes Classification 
We extracted about 3.9 million example nodes 
from the training data, i.e. the FBIS corpus. 
There were 2.37 million frontier nodes and 1.59 
million interior nodes in these examples, give 
rise to about 4.4 million features. To test the per-
formance of our classifier, we simply use the last 
ten thousand examples as a test set, and the rest 
being used as Pegasos training data. All the pa-
rameters in Pegasos were set as default values. In 
this way, the accuracy of the classifier was 
71.59%. 
Then we retrained our classifier by using all of 
the examples. The nodes in the automatically 
parsed NIST MT08 test set were labeled by the 
classifier. As a result, 17,240 nodes were labeled 
as frontier nodes and 5,736 nodes were labeled 
as interior nodes. 
4.2 Experiments on Chinese-English SMT 
In order to confirm that it is advantageous to dis-
tinguish between frontier nodes and interior 
nodes, we performed four translation experi-
ments.  
The first one was a typical beam search decod-
ing without any syntactic constraints.  
All the other three experiments were based on 
the IST-ITG method which makes use of syntac-
19
tic constraints. The difference between these 
three experiments lies in what constraints are 
used. In detail, the second one used all nodes 
recognized by the parser; the third one only used 
frontier nodes labeled by the classifier; the fourth 
one only used interior nodes labeled by the clas-
sifier.  
With the exception of the above differences, 
all the other settings were the same in the four 
experiments. Table 2 summarizes the SMT per-
formance. 
 
Syntactic Constraints BLEU 
none 17.26 
all nodes 16.83 
frontier nodes 17.63 
interior nodes 16.59 
 
Table 2: Comparison of different constraints by 
SMT quality 
 
Clearly, we obtain the best performance if we 
constrain the search with only frontier nodes. 
Using just frontier yields a 0.8 BLEU point im-
provement over the baseline constraint-based 
system which uses all the constraints. 
On the other hand, constraints from interior 
nodes result in the worst performance. This com-
parison shows it is necessary to explicitly distin-
guish nodes in the source parse trees when they 
are used as reordering constraints.  
The improvement over the system without 
constraints is only modest. It may be too coarse 
to use pare trees as hard constraints. We believe 
a greater improvement can be expected if we ap-
ply our idea to finer-grained approaches that use 
constraints softly (Marton and Resnik (2008) and 
Cherry (2008)).  
5 Conclusion and Future Work 
We propose a selectively approach to syntactic 
constraints during decoding. A classifier is built 
automatically to decide whether a node in the 
parse trees should be used as a reordering con-
straint or not. Preliminary results show that it is 
not only advantageous but necessary to explicitly 
distinguish between frontier nodes and interior 
nodes. 
The idea of selecting syntactic constraints is 
compatible with the idea of using constraints 
softly; we plan to combine the two ideas and ob-
tain further improvements in future work.  
Acknowledgments 
We would like to thank Taro Watanabe and 
Andrew Finch for insightful discussions. We also 
would like to thank the anonymous reviewers for 
their constructive comments. 
Reference 
A.L. Berger, P.F. Brown, S.A.D. Pietra, V.J.D. Pietra, 
J.R. Gillett, A.S. Kehler, and R.L. Mercer. 1996. 
Language translation apparatus and method of us-
ing context-based translation models. United States 
patent, patent number 5510981, April. 
Hailong Cao, Yujie Zhang and Hitoshi Isahara. Em-
pirical study on parsing Chinese based on Collins' 
model. 2007. In PACLING. 
Colin Cherry. 2008. Cohesive phrase-Based decoding 
for statistical machine translation. In ACL- HLT. 
Andrew Finch and Eiichiro Sumita. 2008. Dynamic 
model interpolation for statistical machine transla-
tion. In SMT Workshop. 
Victoria Fossum and Kevin Knight. 2008. Using bi-
lingual Chinese-English word alignments to re-
solve PP attachment ambiguity in English. In 
AMTA Student Workshop. 
Heidi J. Fox. 2002. Phrasal cohesion and statistical 
machine translation. In EMNLP. 
Michel Galley, Mark Hopkins, Kevin Knight, and 
Daniel Marcu. 2004. What's in a translation rule? 
In HLT-NAACL. 
Kevin Knight. 1999. Decoding complexity in word 
replacement translation models. Computational 
Linguistics, 25(4):607?615. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine 
Moran, Richard Zens, Chris Dyer, Ondrej Bojar, 
Alexandra Constantin, Evan Herbst. 2007. Moses: 
Open Source Toolkit for Statistical Machine Trans-
lation. In ACL demo and poster sessions. 
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun'ichi 
Kazama, Yiou Wang, Kentaro Torisawa and Hito-
shi Isahara. 2009. An error-driven word-character 
hybrid model for joint Chinese word segmentation 
and POS tagging. In ACL-IJCNLP. 
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine 
translation. In ACL-COLING. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language 
phrases. In EMNLP. 
20
Yuval Marton and Philip Resnik. 2008. Soft syntactic 
constraints for hierarchical phrased-based transla-
tion. In ACL-HLT. 
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In ACL. 
Shai Shalev-Shwartz, Yoram Singer and Nathan Sre-
bro. 2007. Pegasos: Primal estimated sub-gradient 
solver for SVM. In ICML. 
Dekai Wu. 1995. Stochastic inversion transduction 
grammars with application to segmentation, brack-
eting, and alignment of parallel corpora. In IJCAI. 
Kenji Yamada and Kevin Knight. 2000. A syntax-
based statistical translation model. In ACL. 
Hirofumi Yamamoto, Hideo Okuma and Eiichiro 
Sumita. 2008. Imposing constraints from the 
source tree on ITG constraints for SMT. In Work-
shop on syntax and structure in statistical transla-
tion.  
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In SMT Workshop, HLT-NAACL. 
21
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 28?33,
COLING 2010, Beijing, August 2010.
Syntactic Constraints on Phrase Extraction for Phrase-Based 
Machine Translation 
Hailong Cao, Andrew Finch and Eiichiro Sumita 
Language Translation Group, MASTAR Project 
National Institute of Information and Communications Technology 
{hlcao,andrew.finch,eiichiro.sumita }@nict.go.jp
 
Abstract 
A typical phrase-based machine transla-
tion (PBMT) system uses phrase pairs 
extracted from word-aligned parallel 
corpora. All phrase pairs that are consis-
tent with word alignments are collected. 
The resulting phrase table is very large 
and includes many non-syntactic phrases 
which may not be necessary. We propose 
to filter the phrase table based on source 
language syntactic constraints. Rather 
than filter out all non-syntactic phrases, 
we only apply syntactic constraints when 
there is phrase segmentation ambiguity 
arising from unaligned words. Our 
method is very simple and yields a 
24.38% phrase pair reduction and a 0.52 
BLEU point improvement when com-
pared to a baseline PBMT system with 
full-size tables. 
1 Introduction 
Both PBMT models (Koehn et al, 2003; Chiang, 
2005) and syntax-based machine translation 
models (Yamada et al, 2000; Quirk et al, 2005; 
Galley et al, 2006; Liu et al, 2006; Marcu et al, 
2006; and numerous others) are the state-of-the- 
art statistical machine translation (SMT) meth-
ods. Over the last several years, an increasing 
amount of work has been done to combine the 
advantages of the two approaches. DeNeefe et al 
(2007) made a quantitative comparison of the 
phrase pairs that each model has to work with 
and found it is useful to improve the phrasal 
coverage of their string-to-tree model. Liu et al 
(2007) proposed forest-to-string rules to capture 
the non-syntactic phrases in their tree-to-string 
model. Zhang et al (2008) proposed a tree se-
quence based tree-to-tree model which can de-
scribe non-syntactic phrases with syntactic struc-
ture information. 
The converse of the above methods is to in-
corporate syntactic information into the PBMT 
model. Zollmann and Venugopal (2006) started 
with a complete set of phrases as extracted by 
traditional PBMT heuristics, and then annotated 
the target side of each phrasal entry with the la-
bel of the constituent node in the target-side 
parse tree that subsumes the span. Marton and 
Resnik (2008) and Cherry (2008) imposed syn-
tactic constraints on the PBMT system by mak-
ing use of prior linguistic knowledge in the form 
of syntax analysis. In their PBMT decoders, a 
candidate translation gets an extra credit if it re-
spects the source side syntactic parse tree but 
may incur a cost if it violates a constituent 
boundary. Xiong et al (2009) proposed a syn-
tax-driven bracketing model to predict whether a 
phrase (a sequence of contiguous words) is 
bracketable or not using rich syntactic con-
straints. 
In this paper, we try to utilize syntactic 
knowledge to constrain the phrase extraction 
from word-based alignments for PBMT system. 
Rather than filter out all non-syntactic phrases, 
we only apply syntactic constraints when there is 
phrase segmentation ambiguity arising from un-
aligned words. Our method is very simple and 
yields a 24.38% phrase pair reduction and a 0.52 
BLEU point improvement when compared to the 
baseline PBMT system with full-size tables. 
2 Extracting Phrase Pairs from Word-
based Alignments 
In this section, we briefly review a simple and 
effective phrase pair extraction algorithm upon 
which this work builds. 
28
The basic translation unit of a PBMT model is 
the phrase pair, which consists of a sequence of 
source words, a sequence of target words and a 
vector of feature values which represents this 
pair?s contribution to the translation model. In 
typical PBMT systems such as MOSES (Koehn, 
2007), phrase pairs are extracted from word-
aligned parallel corpora. Figure 1 shows the 
form of training example. 
 
 
 
 
 
Figure 1: An example parallel sentence pair 
and word alignment 
 
Since there is no phrase segmentation infor-
mation in the word-aligned sentence pair, in 
practice all pairs of ?source word sequence ||| 
target word sequence? that are consistent with 
word alignments are collected. The words in a 
legal phrase pair are only aligned to each other, 
and not to words outside (Och et al, 1999). For 
example, given a sentence pair and its word 
alignments shown in Figure1, the following nine 
phrase pairs will be extracted: 
 
Source phrase ||| Target phrase 
f1 ||| e1 
f2 ||| e2 
f4 ||| e3 
f1 f2 ||| e1 e2 
f2 f3 ||| e2 
f3 f4 ||| e3 
f1 f2  f3 ||| e1 e2 
f2 f3 f4 ||| e2 e3 
f1 f2 f3 f4 ||| e1 e2 e3 
 
Table 1: Phrase pairs extracted from the example 
in Figure 1 
 
Note that neither the source phrase nor the 
target phrase can be empty. So ?f3 ||| EMPTY? is 
not a legal phrase pair. 
Phrase pairs are extracted over the entire 
training corpus. Given all the collected phrase 
pairs, we can estimate the phrase translation 
probability distribution by relative frequency. 
The collected phrase pairs will also be used to 
build the lexicalized reordering model. For more 
details of the lexicalized reordering model, 
please refer to Tillmann and Zhang (2005) and 
section 2.7.2 of the MOSES?s manual1. 
The main problem of such a phrase pair ex-
traction procedure is the resulting phrase transla-
tion table is very large, especially when a large 
quantity of parallel data is available. This is not 
desirable in real application where speed and 
memory consumption are often critical concerns. 
In addition, some phrase translation pairs are 
generated from training data errors and word 
alignment noise. Therefore, we need to filter the 
phrase table in an appropriate way for both effi-
ciency and translation quality (Johnson et al, 
2007; Yang and Zheng, 2009).  
  f1        f2      f3   f4 
        |           |               | 
  e1        e2            e3 
 
3 Syntactic Constraints on Phrase Pair 
Extraction 
We can divide all the possible phrases into two 
types: syntactic phrases and non-syntactic 
phrases. A ?syntactic phrase? is defined as a 
word sequence that is covered by a single sub-
tree in a syntactic parse tree (Imamura, 2002). 
Intuitively, we would think syntactic phrases are 
much more reliable while the non-syntactic 
phrases are useless. However, (Koehn et al, 
2003) showed that restricting phrasal translation 
to only syntactic phrases yields poor translation 
performance ? the ability to translate non-
syntactic phrases (such as ?there are?, ?note 
that?, and ?according to?) turns out to be critical 
and pervasive. 
 (Koehn et al, 2003) uses syntactic constraints 
from both the source and target languages, and 
over 80% of all phrase pairs are eliminated. In 
this section, we try to use syntactic knowledge in 
a less restrictive way.  
Firstly, instead of using syntactic restriction 
on both source phrases and target phrases, we 
only apply syntactic restriction to the source 
language side. 
Secondly, we only apply syntactic restriction 
to the source phrase whose first or last word is 
unaligned. 
For example, given a parse tree illustrated in 
Figure 2, we will filter out the phrase pair ?f2 f3 
||| e2? since the source phrase ?f2 f3? is a non-
syntactic phrase and its last word ?f3? is not 
                                                 
1 http://www.statmt.org/moses/ 
29
aligned to any target word. The phrase pair ?f1 
f2  f3 ||| e1 e2? will  also be eliminated for the 
same reason. But we do keep phrase pairs such 
as ?f1 f2 ||| e1 e2? even if its source phrase ?f1 
f2? is a non-syntactic phrase. Also, we keep ?f3 
f4 ||| e3? since ?f3 f4? is a syntactic phrase. Ta-
ble 2 shows the completed set of phrase pairs 
that are extracted with our constraint-based 
method. 
 
Source phrase ||| Target phrase 
f1 ||| e1 
f2 ||| e2 
f4 ||| e3 
f1 f2 ||| e1 e2 
f3 f4 ||| e3 
f2 f3 f4 ||| e2 e3 
f1 f2 f3 f4 ||| e1 e2 e3 
 
Table 2: Phrase pairs extracted from the example 
in Figure 2 
 
 
 
Figure 2: An example parse tree and word-
based alignments 
 
The state-of-the-art alignment tool such as 
GIZA++ 2  can not always find alignments for 
every word in the sentence pair. The possible 
reasons could be: its frequency is too low, noisy 
data, auxiliary words or function words which 
have no obvious correspondence in the opposite 
language. 
In the automatically aligned parallel corpus, 
unaligned words are frequent enough to be no-
ticeable (see section 4.1 in this paper). How to 
decide the translation of unaligned word is left to 
the phrase extraction algorithm. An unaligned 
                                                 
2 http://fjoch.com/GIZA++.html 
source word should be translated together with 
the words on the right of it or the words on the 
left of it. The existing algorithm considers both 
of the two directions. So both ?f2 f3 ||| e2? and 
?f3 f4 ||| e3? are extracted. However, it is 
unlikely that ?f3? can be translated into both 
?e2? and ?e3?.  So our algorithm uses prior syn-
tactic knowledge to keep ?f3 f4 ||| e3? and ex-
clude ?f2 f3 ||| e2?. 
4 Experiments 
Our SMT system is based on a fairly typical 
phrase-based model (Finch and Sumita, 2008). 
For the training of our SMT model, we use a 
modified training toolkit adapted from the 
MOSES decoder. Our decoder can operate on 
the same principles as the MOSES decoder. 
Minimum error rate training (MERT) with re-
spect to BLEU score is used to tune the de-
coder?s parameters, and it is performed using the 
standard technique of Och (2003). A lexicalized 
reordering model was built by using the ?msd-
bidirectional-fe? configuration in our experi-
ments. 
The translation model was created from the 
FBIS parallel corpus. We used a 5-gram lan-
guage model trained with modified Kneser-Ney 
smoothing. The language model was trained on 
the target side of the FBIS corpus and the Xin-
hua news in the GIGAWORD corpus. The de-
velopment and test sets are from the NIST MT08 
evaluation campaign. Table 3 shows the statis-
tics of the corpora used in our experiments. 
N3 
N2
N1 
  f1        f2      f3   f4 
 
  e1       e2             e3 
 
 
Data Sentences Chinese words 
English 
words 
Training set 221,994 6,251,554 8,065,629 
Development set 1,664 38,779 46,387 
Test set 1,357 32,377 42,444 
GIGAWORD 19,049,757 - 306,221,306
 
Table 3: Corpora statistics 
 
The Chinese sentences are segmented, POS 
tagged and parsed by the tools described in Kru-
engkrai et al (2009) and Cao et al (2007), both 
of which are trained on the Penn Chinese Tree-
bank 6.0.  
30
4.1 Experiments on Word Alignments 
We use GIZA++ to align the sentences in both 
the Chinese-English and English-Chinese direc-
tions. Then we combine the alignments using the 
standard ?grow-diag-final-and? procedure pro-
vided with MOSES. 
In the combined word alignments, 614,369 or 
9.82% of the Chinese words are unaligned. Ta-
ble 4 shows the top 10 most frequently un-
aligned words. Basically, these words are auxil-
iary words or function words whose usage is 
very flexible. So it would be difficult to auto-
matically align them to the target words.  
 
Unaligned word Frequency 
? 77776 
, 29051 
? 9414 
? 8768 
? 8543 
? 7471 
? 7365 
? 6155 
? 5945 
? 5450 
 
Table 4: Frequently unaligned words from the 
training corpus 
4.2 Experiments on Chinese-English SMT 
In order to confirm that it is advantageous to 
apply appropriate syntactic constraints on phrase 
extraction, we performed three translation ex-
periments by using different ways of phrase ex-
traction.  
In the first experiment, we used the method 
introduced in Section 2 to extract all possible 
phrase translation pairs without using any con-
straints arising from knowledge of syntax.  
The second experiment used source language 
syntactic constraints to filter out all non-
syntactic phrases during phrase pair extraction. 
The third experiment used source language 
syntactic constraints to filter out only non-
syntactic phrases whose first or last source word 
was unaligned.  
With the exception of the above differences in 
phrase translation pair extraction, all the other 
settings were the identical in the three 
experiments. Table 5 summarizes the SMT per-
formance. The evaluation metric is case-
sensitive BLEU-4 (Papineni et al, 2002) which 
estimates the accuracy of translation output with 
respect to a set of reference translations. 
 
Syntactic Con-
straints 
Number of 
distinct phrase pairs BLEU
None 14,195,686 17.26
Full constraint 4,855,108 16.51
Selectively 
constraint 10,733,731 17.78
 
Table 5: Comparison of different constraints on 
phrase pair extraction by translation quality 
 
As shown in the table, it is harmful to fully 
apply syntactic constraints on phrase extraction, 
even just on the source language side. This is 
consistent with the observation of (Koehn et al, 
2003) who applied both source and target con-
straints in German to English translation ex-
periments. 
Clearly, we obtained the best performance if 
we use source language syntactic constraints 
only on phrases whose first or last source word 
is unaligned. In addition, we reduced the number 
of distinct phrase pairs by 24.38% over the base-
line full-size phrase table. 
The results in table 5 show that while some 
non-syntactic phrases are very important to 
maintain the performance of a PBMT system, 
not all of them are necessary. We can achieve 
better performance and a smaller phrase table by 
applying syntactic constraints when there is 
phrase segmentation ambiguity arising from un-
aligned words. 
5 Related Work 
To some extent, our idea is similar to Ma et al 
(2008), who used an anchor word alignment 
model to find a set of high-precision anchor 
links and then aligned the remaining words rely-
ing on dependency information invoked by the 
acquired anchor links. The similarity is that both 
Ma et al (2008) and this work utilize structure 
information to find appropriate translations for 
words which are difficult to align. The differ-
31
ence is that they used dependency information in 
the word alignment stage while our method uses 
syntactic information during the phrase pair ex-
traction stage. There are also many works which 
leverage syntax information to improve word 
alignments (e.g., Cherry and Lin, 2006; DeNero 
and Klein, 2007; Fossum et al, 2008; Hermja-
kob, 2009). 
Johnson et al, (2007) presented a technique 
for pruning the phrase table in a PBMT system 
using Fisher?s exact test. They compute the sig-
nificance value of each phrase pair and prune the 
table by deleting phrase pairs with significance 
values smaller than a certain threshold. Yang 
and Zheng (2008) extended the work in Johnson 
et al, (2007) to a hierarchical PBMT model, 
which is built on synchronous context free 
grammars (SCFG). Tomeh et al, (2009) de-
scribed an approach for filtering phrase tables in 
a statistical machine translation system, which 
relies on a statistical independence measure 
called Noise, first introduced in (Moore, 2004). 
The difference between the above research and 
this work is they took advantage of some statis-
tical measures while we use syntactic knowledge 
to filter phrase tables. 
6 Conclusion and Future Work 
Phrase pair extraction plays a very important 
role on the performance of PBMT systems. We 
utilize syntactic knowledge to constrain the 
phrase extraction from word-based alignments 
for a PBMT system. Rather than filter out all 
non-syntactic phrases, we only filter out non-
syntactic phrases whose first or last source word 
is unaligned. Our method is very simple and 
yields a 24.38% phrase pair reduction and a 0.52 
BLEU point improvement when compared to the 
baseline PBMT system with full-size tables. 
In the future work, we will use other language 
pairs to test our phrase extraction method so that 
we can discover whether or not it is language 
independent. 
References 
Robert C. Moore. 2004. On log-likelihood-ratios and 
the significance of rare events. In EMNLP. 
Hailong Cao, Yujie Zhang and Hitoshi Isahara. Em-
pirical study on parsing Chinese based on Collins' 
model. 2007. In PACLING. 
Colin Cherry and Dekang Lin. 2006. Soft syntactic 
constraints for word alignment through discrimina-
tive training. In ACL. 
Colin Cherry. 2008. Cohesive phrase-Based decoding 
for statistical machine translation. In ACL-HLT. 
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In ACL. 
Steve DeNeefe, Kevin Knight, Wei Wang, and 
Daniel Marcu. 2007. What can syntax-based MT 
learn from phrase-based MT? In EMNLP-CoNLL. 
John DeNero and Dan Klein. 2007. Tailoring word 
alignments to syntactic machine translation. In 
ACL. 
Andrew Finch and Eiichiro Sumita. 2008. Dynamic 
model interpolation for statistical machine transla-
tion. In SMT Workshop. 
Victoria Fossum, Kevin Knight and Steven Abney. 
2008. Using syntax to improve word alignment 
precision for syntax-based machine translation. In 
SMT Workshop, ACL. 
Michel Galley, Jonathan Graehl, Kevin Knight, 
Daniel Marcu, Steve Deneefe, Wei Wang and 
Ignacio Thayer. 2006. Scalable inference and 
training of context-rich syntactic translation mod-
els. In ACL. 
Ulf Hermjakob. 2009. Improved word alignment with 
statistics and linguistic heuristics. In EMNLP. 
Kenji Imamura. 2002. Application of translation 
knowledge acquired by hierarchical phrase align-
ment for pattern-based MT. In TMI. 
Howard Johnson, Joel Martin, George Foster and 
Roland Kuhn. 2007. Improving translation quality 
by discarding most of the phrase table. In EMNLP-
CoNLL. 
Franz Josef Och, Christoph Tillmann and Hermann 
Ney. 1999. Improved alignment models for statis-
tical machine translation. In EMNLP-VLC. 
Franz Josef Och. 2003. Minimum error rate training 
in statistical machine translation. In ACL. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. In HLT-
NAACL. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine 
Moran, Richard Zens, Chris Dyer, Ondrej Bojar, 
Alexandra Constantin, Evan Herbst. 2007. Moses: 
Open Source Toolkit for Statistical Machine 
Translation. In ACL demo and poster sessions. 
32
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun'ichi 
Kazama, Yiou Wang, Kentaro Torisawa and Hito-
shi Isahara. 2009. An error-driven word-character 
hybrid model for joint Chinese word segmentation 
and POS tagging. In ACL-IJCNLP. 
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine 
translation. In ACL-COLING. 
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 
2007. Forest-to-string statistical translation rules. 
In ACL. 
Yanjun Ma, Sylwia Ozdowska, Yanli Sun and Andy 
Way. 2008. Improving word alignment using syn-
tactic dependencies. In SSST. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi, 
and Kevin Knight. 2006. SPMT: Statistical ma-
chine translation with syntactified target language 
phrases. In EMNLP. 
Yuval Marton and Philip Resnik. 2008. Soft syntactic 
constraints for hierarchical phrased-based transla-
tion. In ACL-HLT. 
Kishore Papineni, Salim Roukos, Todd Ward and 
WeiJing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In ACL. 
Chris Quirk and Arul Menezes and Colin Cherry. 
2005. Dependency treelet translation: Syntactically 
informed phrasal SMT. In ACL. 
Christoph Tillmann and Tong Zhang. 2005. A local-
ized prediction model for statistical machine trans-
lation. In ACL. 
Nadi Tomeh, Nicola Cancedda and Marc Dymetman. 
2009. Complexity-based phrase-table filtering for 
statistical machine translation. In MT Summit. 
Deyi Xiong, Min Zhang, Aiti Aw and Haizhou Li. 
2009. A syntax-driven bracketing model for 
phrase-based translation. In ACL-IJCNLP. 
Kenji Yamada and Kevin Knight. 2000. A syntax-
based statistical translation model. In ACL. 
Mei Yang and Jing Zheng. 2009. Toward smaller, 
faster, and better hierarchical phrase-based SMT. 
In ACL. 
Min Zhang, Hongfei Jiang, Aiti Aw, Chew Lim Tan 
and Sheng Li. 2008. A tree sequence alignment-
based tree-to-tree translation model. In ACL- HLT. 
Andreas Zollmann and Ashish Venugopal. 2006. 
Syntax augmented machine translation via chart 
parsing. In SMT Workshop, HLT-NAACL. 
33
