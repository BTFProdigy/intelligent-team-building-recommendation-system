Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1179?1190, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Document-Wide Decoding for
Phrase-Based Statistical Machine Translation
Christian Hardmeier Joakim Nivre Jo?rg Tiedemann
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
Independence between sentences is an as-
sumption deeply entrenched in the models and
algorithms used for statistical machine trans-
lation (SMT), particularly in the popular dy-
namic programming beam search decoding al-
gorithm. This restriction is an obstacle to re-
search on more sophisticated discourse-level
models for SMT. We propose a stochastic lo-
cal search decoding method for phrase-based
SMT, which permits free document-wide de-
pendencies in the models. We explore the sta-
bility and the search parameters of this method
and demonstrate that it can be successfully
used to optimise a document-level semantic
language model.
1 Motivation
In the field of translation studies, it is undisputed that
discourse-wide context must be considered care-
fully for good translation results (Hatim and Mason,
1990). By contrast, the state of the art in statistical
machine translation (SMT), despite significant ad-
vances in the last twenty years, still assumes that
texts can be translated sentence by sentence under
strict independence assumptions, even though it is
well known that certain linguistic phenomena such
as pronominal anaphora cannot be translated cor-
rectly without referring to extra-sentential context.
This is true both for the phrase-based and the syntax-
based approach to SMT. In the rest of this paper, we
shall concentrate on phrase-based SMT.
One reason why it is difficult to experiment
with document-wide models for phrase-based SMT
is that the dynamic programming (DP) algorithm
which has been used almost exclusively for decod-
ing SMT models in the recent literature has very
strong assumptions of locality built into it. DP
beam search for phrase-based SMT was described
by Koehn et al(2003), extending earlier work on
word-based SMT (Tillmann et al 1997; Och et al
2001; Tillmann and Ney, 2003). This algorithm con-
structs output sentences by starting with an empty
hypothesis and adding output words at the end until
translations for all source words have been gener-
ated. The core models of phrase-based SMT, in par-
ticular the n-gram language model (LM), only de-
pend on a constant number of output words to the
left of the word being generated. This fact is ex-
ploited by the search algorithm with a DP technique
called hypothesis recombination (Och et al 2001),
which permits the elimination of hypotheses from
the search space if they coincide in a certain number
of final words with a better hypothesis and no future
expansion can possibly invert the relative ranking of
the two hypotheses under the given models. Hypoth-
esis recombination achieves a substantial reduction
of the search space without affecting search optimal-
ity and makes it possible to use aggressive pruning
techniques for fast search while still obtaining good
results.
The downside of this otherwise excellent ap-
proach is that it only works well with models that
have a local dependency structure similar to that
of an n-gram language model, so they only de-
pend on a small context window for each target
word. Sentence-local models with longer dependen-
cies can be added, but doing so greatly increases
the risk for search errors by inhibiting hypothesis
recombination. Cross-sentence dependencies can-
not be directly integrated into DP SMT decoding in
1179
any obvious way, especially if joint optimisation of
a number of interdependent decisions over an entire
document is required. Research into models with
a more varied, non-local dependency structure is to
some extent stifled by the difficulty of decoding such
models effectively, as can be seen by the problems
some researchers encountered when they attempted
to solve discourse-level problems. Consider, for in-
stance, the work on cache-based language models
by Tiedemann (2010) and Gong et al(2011), where
error propagation was a serious issue, or the works
on pronominal anaphora by Le Nagard and Koehn
(2010), who implemented cross-sentence dependen-
cies with an ad-hoc two-pass decoding strategy, and
Hardmeier and Federico (2010) with the use of an
external decoder driver to manage backward-only
dependencies between sentences.
In this paper, we present a method for decoding
complete documents in phrase-based SMT. Our de-
coder uses a local search approach whose state con-
sists of a complete translation of an entire document
at any time. The initial state is improved by the ap-
plication of a series of operations using a hill climb-
ing strategy to find a (local) maximum of the score
function. This setup gives us complete freedom to
define scoring functions over the entire document.
Moreover, by optionally initialising the state with
the output of a traditional DP decoder, we can en-
sure that the final hypothesis is no worse than what
would have been found by DP search alone. We start
by describing the decoding algorithm and the state
operations used by our decoder, then we present em-
pirical results demonstrating the effectiveness of our
approach and its usability with a document-level se-
mantic language model, and finally we discuss some
related work.
2 SMT Decoding by Hill Climbing
In this section, we formally describe the phrase-
based SMT model implemented by our decoder as
well as the decoding algorithm we use.
2.1 SMT Model
Our decoder is based on local search, so its state at
any time is a representation of a complete translation
of the entire document. Even though the decoder op-
erates at the document level, it is important to keep
track of sentence boundaries, and the individual op-
erations that are applied to the state are still confined
to sentence scope, so it is useful to decompose the
state of a document into the state of its sentences,
and we define the overall state S as a sequence of
sentence states:
S = S1S2 . . .SN , (1)
where N is the number of sentences. This implies
that we constrain the decoder to emit exactly one
output sentence per input sentence.
Let i be the number of a sentence and mi the num-
ber of input tokens of this sentence, p and q (with
1 ? p ? q ? mi) be positions in the input sentence
and [p;q] denote the set of positions from p up to and
including q. We say that [p;q] precedes [p?;q?], or
[p;q]? [p?;q?], if q < p?. Let ?i([p;q]) be the set of
translations for the source phrase covering positions
[p;q] in the input sentence i as given by the phrase
table. We call A = ?[p;q],?? an anchored phrase
pair with coverage C(A) = [p;q] if ? ? ?i([p;q]) is
a target phrase translating the source words at posi-
tions [p;q]. Then a sequence of ni anchored phrase
pairs
Si = A1A2 . . .Ani (2)
is a valid sentence state for sentence i if the follow-
ing two conditions hold:
1. The coverage sets C(A j) for j in 1, . . . ,ni are
mutually disjoint, and
2. the anchored phrase pairs jointly cover the
complete input sentence, or
ni?
j=1
C(A j) = [1;mi]. (3)
Let f (S) be a scoring function mapping a state S
to a real number. As usual in SMT, it is assumed that
the scoring function can be decomposed into a linear
combination of K feature functions hk(S), each with
a constant weight ?k, so
f (S) =
K
?
k=1
?khk(S). (4)
The problem addressed by the decoder is the search
for the state S? with maximal score, such that
S? = argmax
S
f (S). (5)
1180
The feature functions implemented in our baseline
system are identical to the ones found in the popular
Moses SMT system (Koehn et al 2007). In particu-
lar, our decoder has the following feature functions:
1. phrase translation scores provided by the
phrase table including forward and backward
conditional probabilities, lexical weights and a
phrase penalty (Koehn et al 2003),
2. n-gram language model scores implemented
with the KenLM toolkit (Heafield, 2011),
3. a word penalty score,
4. a distortion model with geometric decay
(Koehn et al 2003), and
5. a feature indicating the number of times a given
distortion limit is exceeded in the current state.
In our experiments, the last feature is used with a
fixed weight of negative infinity in order to limit the
gaps between the coverage sets of adjacent anchored
phrase pairs to a maximum value. In DP search, the
distortion limit is usually enforced directly by the
search algorithm and is not added as a feature. In
our decoder, however, this restriction is not required
to limit complexity, so we decided to add it among
the scoring models.
2.2 Decoding Algorithm
The decoding algorithm we use (algorithm 1) is
very simple. It starts with a given initial document
state. In the main loop, which extends from line 3
to line 12, it generates a successor state S? for the
current state S by calling the function Neighbour,
which non-deterministically applies one of the oper-
ations described in section 3 of this paper to S. The
score of the new state is compared to that of the pre-
vious one. If it meets a given acceptance criterion,
S? becomes the current state, else search continues
from the previous state S. For the experiments in
this paper, we use the hill climbing acceptance cri-
terion, which simply accepts a new state if its score
is higher than that of the current state. Other accep-
tance criteria are possible and could be used to en-
dow the search algorithm with stochastic behaviour.
The main loop is repeated until a maximum num-
ber of steps (step limit) is reached or until a maxi-
mum number of moves are rejected in a row (rejec-
tion limit).
Algorithm 1 Decoding algorithm
Input: an initial document state S;
search parameters maxsteps and maxrejected
Output: a modified document state
1: nsteps? 0
2: nrejected? 0
3: while nsteps < maxsteps and
nrejected < maxrejected do
4: S?? Neighbour(S)
5: if Accept( f (S?), f (S)) then
6: S? S?
7: nrejected? 0
8: else
9: nrejected? nrejected+1
10: end if
11: nsteps? nsteps+1
12: end while
13: return S
A notable difference between this algorithm and
other hill climbing algorithms that have been used
for SMT decoding (Germann et al 2004; Langlais
et al 2007) is its non-determinism. Previous work
for sentence-level decoding employed a steepest as-
cent strategy which amounts to enumerating the
complete neighbourhood of the current state as de-
fined by the state operations and selecting the next
state to be the best state found in the neighbourhood
of the current one. Enumerating all neighbours of
a given state, costly as it is, has the advantage that
it makes it easy to prove local optimality of a state
by recognising that all possible successor states have
lower scores. It can be rather inefficient, since at
every step only one modification will be adopted;
many of the modifications that are discarded will
very likely be generated anew in the next iteration.
As we extend the decoder to the document level,
the size of the neighbourhood that would have to be
explored in this way increases considerably. More-
over, the inefficiency of the steepest ascent approach
potentially increases as well. Very likely, a promis-
ing move in one sentence will remain promising af-
ter a modification has been applied to another sen-
1181
tence, even though this is not guaranteed to be true
in the presence of cross-sentence models. We there-
fore adopt a first-choice hill climbing strategy that
non-deterministically generates successor states and
accepts the first one that meets the acceptance cri-
terion. This frees us from the necessity of gener-
ating the full set of successors for each state. On
the downside, if the full successor set is not known,
it is no longer possible to prove local optimality of a
state, so we are forced to use a different condition for
halting the search. We use a combination of two lim-
its: The step limit is a hard limit on the resources the
user is willing to expend on the search problem. The
value of the rejection limit determines how much of
the neighbourhood is searched for better successors
before a state is accepted as a solution; it is related
to the probability that a state returned as a solution
is in fact locally optimal.
To simplify notations in the description of the in-
dividual state operations, we write
Si ?? S
?
i (6)
to signify that a state operation, when presented with
a document state as in equation 1 and acting on sen-
tence i, returns a new document state of
S? = S1 . . .Si?1 S
?
i Si+1 . . .SN . (7)
Similarly,
Si : A j . . .A j+h?1 ?? A
?
1 . . .A
?
h? (8)
is equivalent to
Si ?? A1 . . .A j?1 A
?
1 . . .A
?
h? A j+h . . .Ani (9)
and indicates that the operation returns a state in
which a sequence of h consecutive anchored phrase
pairs has been replaced by another sequence of h?
anchored phrase pairs.
2.3 Efficiency Considerations
When implementing the feature functions for the de-
coder, we have to exercise some care to avoid re-
computing scores for the whole document at every
iteration. To achieve this, the scores are computed
completely only once, at the beginning of the de-
coding run. In subsequent iterations, scoring func-
tions are presented with the scores of the previous
iteration and a list of modifications produced by the
state operation, a set of tuples ?i,r,s,A?1 . . .A
?
h??, each
indicating that the document should be modified as
described by
Si : Ar . . .As ?? A
?
1 . . .A
?
h? . (10)
If a feature function is decomposable in some way,
as all the standard features developed under the con-
straints of DP search are, it can then update the state
simply by subtracting and adding score components
pertaining to the modified parts of the document.
Feature functions have the possibility to store their
own state information along with the document state
to make sure the required information is available.
Thus, the framework makes it possible to exploit de-
composability for efficient scoring without impos-
ing any particular decomposition on the features as
beam search does.
To make scoring even more efficient, scores are
computed in two passes: First, every feature func-
tion is asked to provide an upper bound on the score
that will be obtained for the new state. In some
cases, it is possible to calculate reasonable upper
bounds much more efficiently than computing the
exact feature value. If the upper bound fails to meet
the acceptance criterion, the new state is discarded
right away; if not, the full score is computed and the
acceptance criterion is tested again.
Among the basic SMT models, this two-pass
strategy is only used for the n-gram LM, which re-
quires fairly expensive parameter lookups for scor-
ing. The scores of all the other baseline models are
fully computed during the first scoring pass. The
n-gram model is more complex. In its state informa-
tion, it keeps track of the LM score and LM library
state for each word. The first scoring pass then iden-
tifies the words whose LM scores are affected by the
current search step. This includes the words changed
by the search operation as well as the words whose
LM history is modified. The range of the history de-
pendencies can be determined precisely by consider-
ing the ?valid state length? information provided by
the KenLM library. In the first pass, the LM scores
of the affected words are subtracted from the total
score. The model only looks up the new LM scores
for the affected words and updates the total score
if the new search state passes the first acceptance
check. This two-pass scoring approach allows us
1182
to avoid LM lookups altogether for states that will
be rejected anyhow because of low scores from the
other models, e. g. because the distortion limit is vi-
olated.
Model score updates become more complex and
slower as the number of dependencies of a model in-
creases. While our decoding algorithm does not im-
pose any formal restrictions on the number or type
of dependencies that can be handled, there will be
practical limits beyond which decoding becomes un-
acceptably slow or the scoring code becomes very
difficult to maintain. These limits are however fairly
independent of the types of dependencies handled
by a model, which permits the exploration of more
varied model types than those handled by DP search.
2.4 State Initialisation
Before the hill climbing decoding algorithm can be
run, an initial state must be generated. The closer the
initial state is to an optimum, the less work remains
to be done for the algorithm. If the algorithm is to be
self-contained, initialisation must be relatively unin-
formed and can only rely on some general prior as-
sumptions about what might be a good initial guess.
On the other hand, if optimal results are sought after,
it pays off to invest some effort into a good starting
point. One way to do this is to run DP search first.
For uninformed initialisation, we chose to imple-
ment a very simple procedure based only on the ob-
servation that, at least for language pairs involving
the major European languages, it is usually a good
guess to keep the word order of the output very sim-
ilar to that of the input. We therefore create the ini-
tial state by selecting, for each sentence in the docu-
ment, a sequence of anchored phrase pairs covering
the input sentence in monotonic order, that is, such
that for all pairs of adjacent anchored phrase pairs
A j and A j+1, we have that C(A j)?C(A j+1).
For initialisation with DP search, we first run the
Moses decoder (Koehn et al 2007) with default
search parameters and the same models as those
used by our decoder. Then we extract the best output
hypothesis from the search graph of the decoder and
map it into a sequence of anchored phrase pairs in
the obvious way. When the document-level decoder
is used with models that are incompatible with beam
search, Moses can be run with a subset of the mod-
els in order to find an approximation of the solution
which is then refined with the complete feature set.
3 State Operations
Given a document state S, the decoder uses a neigh-
bourhood function Neighbour to simulate a move
in the state space. The neighbourhood function non-
deterministically selects a type of state operation and
a location in the document to apply it to and returns
the resulting new state. We use a set of three opera-
tions that has the property that every possible docu-
ment state can be reached from every other state in
a sequence of moves.
Designing operations for state transitions in lo-
cal search for phrase-based SMT is a problem that
has been addressed in the literature (Langlais et
al., 2007; Arun et al 2010). Our decoder?s first-
choice hill climbing strategy never enumerates the
full neighbourhood of a state. We therefore place
less emphasis than previous work on defining a com-
pact neighbourhood, but allow the decoder to make
quite extensive changes to a state in a single step
with a certain probability. Otherwise our operations
are similar to those used by Arun et al(2010).
All of the operations described in this paper make
changes to a single sentence only. Each time it is
called, the Neighbour function selects a sentence
in the document with a probability proportional to
the number of input tokens in each sentence to en-
sure a fair distribution of the decoder?s attention over
the words in the document regardless of varying sen-
tence lengths.
3.1 Changing Phrase Translations
The change-phrase-translation operation re-
places the translation of a single phrase with a ran-
dom translation with the same coverage taken from
the phrase table. Formally, the operation selects an
anchored phrase pair A j by drawing uniformly from
the elements of Si and then draws a new translation
? ? uniformly from the set ?i(C(A j)). The new state
is given by
Si : A j ?? ?C(A j),? ??. (11)
3.2 Changing Word Order
The swap-phrases operation affects the output
word order without changing the phrase translations.
1183
It exchanges two anchored phrase pairs A j and A j+h,
resulting in an output state of
Si : A j . . .A j+h ?? A j+h A j+1 . . .A j+h?1 A j. (12)
The start location j is drawn uniformly from the el-
igible sentence positions; the swap range h comes
from a geometric distribution with configurable de-
cay. Other word-order changes such as a one-way
move operation that does not require another move-
ment in exchange or more advanced permutations
can easily be defined.
3.3 Resegmentation
The most complex operation is resegment, which
allows the decoder to modify the segmentation of the
source phrase. It takes a number of anchored phrase
pairs that form a contiguous block both in the input
and in the output and replaces them with a new set
of phrase pairs covering the same span of the input
sentence. Formally,
Si : A j . . .A j+h?1 ?? A
?
1 . . .A
?
h? (13)
such that
j+h?1?
j?= j
C(A j?) =
h??
j?=1
C(A?j?) = [p;q] (14)
for some p and q, where, for j? = 1, . . . ,h?, we
have that A?j? = ?[p j? ;q j? ],? j??, all [p j? ;q j? ] are mu-
tually disjoint and each ? j? is randomly drawn from
?i([p j? ;q j? ]).
Regardless of the ordering of A j . . .A j+h?1, the
resegment operation always generates a sequence
of anchored phrase pairs in linear order, such that
C(A?j?)?C(A
?
j?+1) for j
? = 1, . . . ,h??1.
As for the other operations, j is generated uni-
formly and h is drawn from a geometric distribution
with a decay parameter. The new segmentation is
generated by extending the sequence of anchored
phrase pairs with random elements starting at the
next free position, proceeding from left to right until
the whole range [p;q] is covered.
4 Experimental Results
In this section, we present the results of a series
of experiments with our document decoder. The
goal of our experiments is to demonstrate the be-
haviour of the decoder and characterise its response
to changes in the fundamental search parameters.
The SMT models for our experiments were cre-
ated with a subset of the training data for the
English-French shared task at the WMT 2011 work-
shop (Callison-Burch et al 2011). The phrase ta-
ble was trained on Europarl, news-commentary and
UN data. To reduce the training data to a manage-
able size, singleton phrase pairs were removed be-
fore the phrase scoring step. Significance-based fil-
tering (Johnson et al 2007) was applied to the re-
sulting phrase table. The language model was a 5-
gram model with Kneser-Ney smoothing trained on
the monolingual News corpus with IRSTLM (Fed-
erico et al 2008). Feature weights were trained with
Minimum Error-Rate Training (MERT) (Och, 2003)
on the news-test2008 development set using the DP
beam search decoder and the MERT implementation
of the Moses toolkit (Koehn et al 2007). Experi-
mental results are reported for the newstest2009 test
set, a corpus of 111 newswire documents totalling
2,525 sentences or 65,595 English input tokens.
4.1 Stability
An important difference between our decoder and
the classical DP decoder as well as previous work in
SMT decoding with local search is that our decoder
is inherently non-deterministic. This implies that re-
peated runs of the decoder with the same search pa-
rameters, input and models will not, in general, find
the same local maximum of the score space. The
first empirical question we ask is therefore how dif-
ferent the results are under repeated runs. The re-
sults in this and the next section were obtained with
random state initialisation, i. e. without running the
DP beam search decoder.
Figure 1 shows the results of 7 decoder runs with
the models described above, translating the news-
test2009 test set, with a step limit of 227 and a rejec-
tion limit of 100,000. The x-axis of both plots shows
the number of decoding steps on a logarithmic scale,
so the number of steps is doubled between two adja-
cent points on the same curve. In the left plot, the
y-axis indicates the model score optimised by the
decoder summed over all 2525 sentences of the doc-
ument. In the right plot, the case-sensitive BLEU
score (Papineni et al 2002) of the current decoder
1184
Figure 1: Score stability in repeated decoder runs
state against a reference translation is displayed.
We note, as expected, that the decoder achieves
a considerable improvement of the initial state with
diminishing returns as decoding continues. Be-
tween 28 and 214 steps, the score increases at a
roughly logarithmic pace, then the curve flattens out,
which is partly due to the fact that decoding for
some documents effectively stopped when the max-
imum number of rejections was reached. The BLEU
score curve shows a similar increase, from an initial
score below 5 % to a maximum of around 21.5 %.
This is below the score of 22.45 % achieved by the
beam search decoder with the same models, which
is not surprising considering that our decoder ap-
proximates a more difficult search problem, from
which a number of strong independence assump-
tions have been lifted, without, at the moment, hav-
ing any stronger models at its disposal to exploit this
additional freedom for better translation.
In terms of stability, there are no dramatic differ-
ences between the decoder runs. Indeed, the small
differences that exist are hardly discernible in the
plots. The model scores at the end of the decod-
ing run range between ?158767.9 and ?158716.9,
a relative difference of only about 0.03 %. Final
BLEU scores range from 21.41 % to 21.63 %, an in-
terval that is not negligible, but comparable to the
variance observed when, e. g., feature weights from
repeated MERT runs are used with one and the same
SMT system. Note that these results were obtained
with random state initialisation. With DP initialisa-
tion, score differences between repeated runs rarely
exceed 0.02 absolute BLEU percentage points.
Overall, we conclude that the decoding results of
our algorithm are reasonably stable despite the non-
determinism inherent in the procedure. In our sub-
sequent experiments, the evaluation scores reported
are calculated as the mean of three runs for each ex-
periment.
4.2 Search Algorithm Parameters
The hill climbing algorithm we use has two param-
eters which govern the trade-off between decoding
time and the accuracy with which a local maximum
is identified: The step limit stops the search pro-
cess after a certain number of steps regardless of the
search progress made or lack thereof. The rejection
limit stops the search after a certain number of un-
successful attempts to make a step, when continued
search does not seem to be promising. In most of our
experiments, we used a step limit of 227 ? 1.3 ? 108
and a rejection limit of 105. In practice, decoding
terminates by reaching the rejection limit for the vast
majority of documents. We therefore examined the
effect of different rejection limits on the learning
curves. The results are shown in figure 2.
The results show that continued search does pay
off to a certain extent. Indeed, the curve for re-
jection limit 107 seems to indicate that the model
score increases roughly logarithmically, albeit to a
higher base, even after the curve has started to flat-
ten out at 214 steps. At a certain point, however, the
probability of finding a good successor state drops
rather sharply by about two orders of magnitude, as
1185
Figure 2: Search performance at different rejection limits
evidenced by the fact that a rejection limit of 106
does not give a large improvement over one of 105,
while one of 107 does. The continued model score
improvement also results in an increase in BLEU
scores, and with a BLEU score of 22.1 % the system
with rejection limit 107 is fairly close to the score of
22.45 % obtained by DP beam search.
Obviously, more exact search comes at a cost, and
in this case, it comes at a considerable cost, which is
an explosion of the time required to decode the test
set from 4 minutes at rejection limit 103 to 224 min-
utes at rejection limit 105 and 38 hours 45 minutes
at limit 107. The DP decoder takes 31 minutes for
the same task. We conclude that the rejection limit
of 105 selected for our experiments, while techni-
cally suboptimal, realises a good trade-off between
decoding time and accuracy.
4.3 A Semantic Document Language Model
In this section, we present the results of the applica-
tion of our decoder to an actual SMT model with
cross-sentence features. Our model addresses the
problem of lexical cohesion. In particular, it rewards
the use of semantically related words in the trans-
lation output by the decoder, where semantic dis-
tance is measured with a word space model based
on Latent Semantic Analysis (LSA). LSA has been
applied to semantic language modelling in previous
research with some success (Coccaro and Jurafsky,
1998; Bellegarda, 2000; Wandmacher and Antoine,
2007). In SMT, it has mostly been used for domain
adaptation (Kim and Khudanpur, 2004; Tam et al
2007), or to measure sentence similarities (Banchs
and Costa-jussa`, 2011).
The model we use is inspired by Bellegarda
(2000). It is a Markov model, similar to a stan-
dard n-gram model, and assigns to each content
word a score given a history of n preceding content
words, where n = 30 below. Scoring relies on a 30-
dimensional LSA word vector space trained with the
S-Space software (Jurgens and Stevens, 2010). The
score is defined based on the cosine similarity be-
tween the word vector of the predicted word and the
mean word vector of the words in the history, which
is converted to a probability by histogram lookup
as suggested by Bellegarda (2000). The model is
structurally different from a regular n-gram model
in that word vector n-grams are defined over content
words occurring in the word vector model only and
can cross sentence boundaries. Stop words, identi-
fied by an extensive stop word list and amounting to
around 60 % of the tokens, are scored by a different
mechanism based on their relative frequency (undis-
counted unigram probability) in the training corpus.
In sum, the score produced by the semantic docu-
ment LM has the following form:
h(w|h)=
?
??
??
punigr(w) if w is a stop word, else
? pcos(w|h) if w is known, else
? if w is unknown,
(15)
where ? is the proportion of content words in the
training corpus and ? is a small fixed probability.
It is integrated into the decoder as an extra feature
function. Since we lack an automatic method for
1186
training the feature weights of document-wide fea-
tures, its weight was selected by grid search over
a number of values, comparing translation perfor-
mance for the newstest2009 test set.
In these experiments, we used DP beam search
to initialise the state of our local search decoder.
Three results are presented (table 1): The first table
row shows the baseline performance using DP beam
search with standard sentence-local features only.
The scores in the second row were obtained by run-
ning the hill climbing decoder with DP initialisation,
but without adding any models. A marginal increase
in scores for all three test sets demonstrates that the
hill climbing decoder manages to fix some of the
search errors made by the DP search. The last row
contains the scores obtained by adding in the seman-
tic language model. Scores are presented for three
publicly available test sets from recent WMT Ma-
chine Translation shared tasks, of which one (news-
test2009) was used to monitor progress during de-
velopment and select the final model.
Adding the semantic language model results in a
small increase in NIST scores (Doddington, 2002)
for all three test sets as well as a small BLEU score
gain (Papineni et al 2002) for two out of three cor-
pora. We note that the NIST score turned out to re-
act more sensitively to improvements due to the se-
mantic LM in all our experiments, which is reason-
able because the model specifically targets content
words, which benefit from the information weight-
ing done by the NIST score. While the results
we present do not constitute compelling evidence
in favour of our semantic LM in its current form,
they do suggest that this model could be improved
to realise higher gains from cross-sentence seman-
tic information. They support our claim that cross-
sentence models should be examined more closely
and that existing methods should be adapted to deal
with them, a problem addressed by our main contri-
bution, the local search document decoder.
5 Related Work
Even though DP beam search (Koehn et al 2003)
has been the dominant approach to SMT decoding
in recent years, methods based on local search have
been explored at various times. For word-based
SMT, greedy hill-climbing techniques were advo-
cated as a faster replacement for beam search (Ger-
mann et al 2001; Germann, 2003; Germann et al
2004), and a problem formulation specifically tar-
geting word reordering with an efficient word re-
ordering algorithm has been proposed (Eisner and
Tromble, 2006).
A local search decoder has been advanced as a
faster alternative to beam search also for phrase-
based SMT (Langlais et al 2007; Langlais et al
2008). That work anticipates many of the features
found in our decoder, including the use of local
search to refine an initial hypothesis produced by
DP beam search. The possibility of using models
that do not fit well into the beam search paradigm is
mentioned and illustrated with the example of a re-
versed n-gram language model, which the authors
claim would be difficult to implement in a beam
search decoder. Similarly to the work by Germann
et al(2001), their decoder is deterministic and ex-
plores the entire neighbourhood of a state in order
to identify the most promising step. Our main con-
tribution with respect to the work by Langlais et al
(2007) is the introduction of the possibility of han-
dling document-level models by lifting the assump-
tion of sentence independence. As a consequence,
enumerating the entire neighbourhood becomes too
expensive, which is why we resort to a ?first-choice?
strategy that non-deterministically generates states
and accepts the first one encountered that meets the
acceptance criterion.
More recently, Gibbs sampling was proposed as
a way to generate samples from the posterior distri-
bution of a phrase-based SMT decoder (Arun et al
2009; Arun et al 2010), a process that resembles
local search in its use of a set of state-modifying
operators to generate a sequence of decoder states.
Where local search seeks for the best state attainable
from a given initial state, Gibbs sampling produces
a representative sample from the posterior. Like all
work on SMT decoding that we know of, the Gibbs
sampler presented by Arun et al(2010) assumes in-
dependence of sentences and considers the complete
neighbourhood of each state before taking a sample.
6 Conclusion
In the last twenty years of SMT research, there has
been a strong assumption that sentences in a text
1187
newstest2009 newstest2010 newstest2011
BLEU NIST BLEU NIST BLEU NIST
DP search only 22.56 6.513 27.27 7.034 24.94 7.170
DP + hill climbing 22.60 6.518 27.33 7.046 24.97 7.169
with semantic LM 22.71 6.549 27.53 7.087 24.90 7.199
Table 1: Experimental results with a cross-sentence semantic language model
are independent of one another, and discourse con-
text has been largely neglected. Several factors have
contributed to this. Developing good discourse-level
models is difficult, and considering the modest trans-
lation quality that has long been achieved by SMT,
there have been more pressing problems to solve and
lower hanging fruit to pick. However, we argue that
the popular DP beam search algorithm, which deliv-
ers excellent decoding performance, but imposes a
particular kind of local dependency structure on the
feature models, has also had its share in driving re-
searchers away from discourse-level problems.
In this paper, we have presented a decoding pro-
cedure for phrase-based SMT that makes it possi-
ble to define feature models with cross-sentence de-
pendencies. Our algorithm can be combined with
DP beam search to leverage the quality of the tradi-
tional approach with increased flexibility for models
at the discourse level. We have presented prelimi-
nary results on a cross-sentence semantic language
model addressing the problem of lexical cohesion to
demonstrate that this kind of models is worth explor-
ing further. Besides lexical cohesion, cross-sentence
models are relevant for other linguistic phenomena
such as pronominal anaphora or verb tense selection.
We believe that SMT research has reached a point of
maturity where discourse phenomena should not be
ignored any longer, and we consider our decoder to
be a step towards this goal.
References
Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blun-
som, Adam Lopez, and Philipp Koehn. 2009. Monte
carlo inference and maximization for phrase-based
translation. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 102?110, Boulder, Colorado,
June. Association for Computational Linguistics.
Abhishek Arun, Barry Haddow, Philipp Koehn, Adam
Lopez, Chris Dyer, and Phil Blunsom. 2010. Monte
Carlo techniques for phrase-based translation. Ma-
chine translation, 24(2):103?121.
Rafael E. Banchs and Marta R. Costa-jussa`. 2011. A se-
mantic feature for Statistical Machine Translation. In
Proceedings of Fifth Workshop on Syntax, Semantics
and Structure in Statistical Translation, pages 126?
134, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Jerome R. Bellegarda. 2000. Exploiting latent semantic
information in statistical language modeling. Proceed-
ings of the IEEE, 88(8):1279?1296.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 Work-
shop on Statistical Machine Translation. In Proceed-
ings of the Sixth Workshop on Statistical Machine
Translation, pages 22?64, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Noah Coccaro and Daniel Jurafsky. 1998. Towards bet-
ter integration of semantic predictors in statistical lan-
guage modeling. In Proceedings of the 5th Interna-
tional Conference on Spoken Language Processing,
Sydney.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second Interna-
tional conference on Human Language Technology
Research, pages 138?145, San Diego.
Jason Eisner and Roy W. Tromble. 2006. Local search
with very large-scale neighborhoods for optimal per-
mutations in machine translation. In Proceedings of
the HLT-NAACL Workshop on Computationally Hard
Problems and Joint Inference in Speech and Language
Processing, pages 57?75.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an open source toolkit for handling
large scale language models. In Interspeech 2008,
pages 1618?1621. ISCA.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of 39th Annual Meeting of the Association for
Computational Linguistics, pages 228?235, Toulouse,
France, July. Association for Computational Linguis-
tics.
1188
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2004. Fast and optimal de-
coding for machine translation. Artificial Intelligence,
154(1?2):127?143.
Ulrich Germann. 2003. Greedy decoding for Statis-
tical Machine Translation in almost linear time. In
Proceedings of the 2003 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level Statistical Ma-
chine Translation. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 909?919, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Christian Hardmeier and Marcello Federico. 2010. Mod-
elling Pronominal Anaphora in Statistical Machine
Translation. In Proceedings of the seventh Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 283?289.
Basil Hatim and Ian Mason. 1990. Discourse and the
Translator. Language in Social Life Series. Longman,
London.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, June. Association for
Computational Linguistics.
David Jurgens and Keith Stevens. 2010. The S-Space
package: An open source package for word space
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30?35, Uppsala, Sweden, July.
Association for Computational Linguistics.
Woosung Kim and Sanjeev Khudanpur. 2004. Cross-
lingual latent semantic analysis for language model-
ing. In IEEE international conference on acoustics,
speech, and signal processing (ICASSP), volume 1,
pages 257?260, Montre?al.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 conference of the North Ameri-
can chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 48?
54, Edmonton.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for Statistical Ma-
chine Translation. In Annual meeting of the Associ-
ation for Computational Linguistics: Demonstration
session, pages 177?180, Prague.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2007. A greedy decoder for phrase-based statistical
machine translation. In TMI-2007: Proceedings of
the 11th International Conference on Theoretical and
Methodological Issues in Machine Translation, pages
104?113, Sko?vde.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2008. Recherche locale pour la traduction statistique
par segments. In TALN 2008, pages 119?128, Avi-
gnon, France, June. ATALA.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding pro-
noun translation with co-reference resolution. In Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 252?261,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for Statisti-
cal Machine Translation. In Proceedings of the Data-
Driven Machine Translation Workshop, 39th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 55?62, Toulouse.
Franz Josef Och. 2003. Minimum error rate training in
Statistical Machine Translation. In Proceedings of the
41st annual meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo (Japan).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of Machine Translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia. ACL.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for Statistical Ma-
chine Translation. Machine Translation, 21(4):187?
207.
Jo?rg Tiedemann. 2010. To cache or not to cache? Ex-
periments with adaptive models in Statistical Machine
Translation. In Proceedings of the ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 189?194, Uppsala, Sweden. As-
sociation for Computational Linguistics.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a Dynamic Programming beam search al-
gorithm for Statistical Machine Translation. Compu-
tational linguistics, 29(1):97?133.
Christoph Tillmann, Stephan Vogel, Hermann Ney, and
Alex Zubiaga. 1997. A DP-based search using mono-
tone alignments in Statistical Translation. In Proceed-
ings of the 35th Annual Meeting of the Association for
1189
Computational Linguistics, pages 289?296, Madrid,
Spain, July. Association for Computational Linguis-
tics.
Tonio Wandmacher and Jean-Yves Antoine. 2007.
Methods to integrate a language model with seman-
tic information for a word prediction component. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 506?513, Prague, Czech Republic,
June. Association for Computational Linguistics.
1190
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 380?391,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Latent Anaphora Resolution for Cross-Lingual Pronoun Prediction
Christian Hardmeier J?rg Tiedemann Joakim Nivre
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
This paper addresses the task of predicting the
correct French translations of third-person sub-
ject pronouns in English discourse, a problem
that is relevant as a prerequisite for machine
translation and that requires anaphora resolu-
tion. We present an approach based on neu-
ral networks that models anaphoric links as
latent variables and show that its performance
is competitive with that of a system with sep-
arate anaphora resolution while not requiring
any coreference-annotated training data. This
demonstrates that the information contained in
parallel bitexts can successfully be used to ac-
quire knowledge about pronominal anaphora
in an unsupervised way.
1 Motivation
When texts are translated from one language into
another, the translation reconstructs the meaning or
function of the source text with the means of the
target language. Generally, this has the effect that
the entities occurring in the translation and their mu-
tual relations will display similar patterns as the enti-
ties in the source text. In particular, coreference pat-
terns tend to be very similar in translations of a text,
and this fact has been exploited with good results to
project coreference annotations from one language
into another by using word alignments (Postolache
et al, 2006; Rahman and Ng, 2012).
On the other hand, what is true in general need
not be true for all types of linguistic elements. For
instance, a substantial percentage of the English third-
person subject pronouns he, she, it and they does
not get realised as pronouns in French translations
(Hardmeier, 2012). Moreover, it has been recognised
by various authors in the statistical machine transla-
tion (SMT) community (Le Nagard and Koehn, 2010;
Hardmeier and Federico, 2010; Guillou, 2012) that
pronoun translation is a difficult problem because,
even when a pronoun does get translated as a pro-
noun, it may require choosing the correct word form
based on agreement features that are not easily pre-
dictable from the source text.
The work presented in this paper investigates
the problem of cross-lingual pronoun prediction for
English-French. Given an English pronoun and its
discourse context as well as a French translation of
the same discourse and word alignments between
the two languages, we attempt to predict the French
word aligned to the English pronoun. As far as we
know, this task has not been addressed in the litera-
ture before. In our opinion, it is interesting for several
reasons. By studying pronoun prediction as a task in
its own right, we hope to contribute towards a better
understanding of pronoun translation with a long-
term view to improving the performance of SMT
systems. Moreover, we believe that this task can lead
to interesting insights about anaphora resolution in a
multi-lingual context. In particular, we show in this
paper that the pronoun prediction task makes it possi-
ble to model the resolution of pronominal anaphora
as a latent variable and opens up a way to solve a
task relying on anaphora resolution without using
any data annotated for anaphora. This is what we
consider the main contribution of our present work.
We start by modelling cross-lingual pronoun pre-
diction as an independent machine learning task after
doing anaphora resolution in the source language
(English) using the BART software (Broscheit et
al., 2010). We show that it is difficult to achieve
satisfactory performance with standard maximum-
380
The latest version released in March is equipped with ... It is sold at ...
La derni?re version lanc?e en mars est dot?e de ... ? est vendue ...
Figure 1: Task setup
entropy classifiers especially for low-frequency pro-
nouns such as the French feminine plural pronoun
elles. We propose a neural network classifier that
achieves better precision and recall and manages to
make reasonable predictions for all pronoun cate-
gories in many cases.
We then go on to extend our neural network archi-
tecture to include anaphoric links as latent variables.
We demonstrate that our classifier, now with its own
source language anaphora resolver, can be trained
successfully with backpropagation. In this setup, we
no longer use the machine learning component in-
cluded in the external coreference resolution system
(BART) to predict anaphoric links. Anaphora reso-
lution is done by our neural network classifier and
requires only some quantity of word-aligned parallel
data for training, completely obviating the need for a
coreference-annotated training set.
2 Task Setup
The overall setup of the classification task we address
in this paper is shown in Figure 1. We are given an
English discourse containing a pronoun along with
its French translation and word alignments between
the two languages, which in our case were computed
automatically using a standard SMT pipeline with
GIZA++ (Och and Ney, 2003). We focus on the four
English third-person subject pronouns he, she, it and
they. The output of the classifier is a multinomial
distribution over six classes: the four French subject
pronouns il, elle, ils and elles, corresponding to mas-
culine and feminine singular and plural, respectively;
the impersonal pronoun ce/c?, which occurs in some
very frequent constructions such as c?est (it is); and
a sixth class OTHER, which indicates that none of
these pronouns was used. In general, a pronoun may
be aligned to multiple words; in this case, a training
example is counted as a positive example for a class
if the target word occurs among the words aligned
to the pronoun, irrespective of the presence of other
0 0 0 1 0version
0 1 0 0 0la
0 0 1 0 0elle
0 .5 0 .5 0
0 0 1 0 0
0 .05 .9 .05 0
p1 = .9
p2 =
.1
word candidate training ex.
Figure 2: Antecedent feature aggregation
aligned tokens.
This task setup resembles the problem that an
SMT system would have to solve to make informed
choices when translating pronouns, an aspect of trans-
lation neglected by most existing SMT systems. An
important difference between the SMT setup and our
own classifiers is that we use context from human-
made translations for prediction. This potentially
makes the task both easier and more difficult; easier,
because the context can be relied on to be correctly
translated, and more difficult, because human transla-
tors frequently create less literal translations than an
SMT system would. Integrating pronoun prediction
into the translation process would require significant
changes to the standard SMT decoding setup in order
to take long-range dependencies in the target lan-
guage into account, which is why we do not address
this issue in our current work.
In all the experiments presented in this paper, we
used features from two different sources:
? Anaphora context features describe the source
language pronoun and its immediate context
consisting of three words to its left and three
words to its right. They are encoded as vec-
tors whose dimensionality is equal to the source
vocabulary size with a single non-zero compo-
nent indicating the word referred to (one-hot
vectors).
? Antecedent features describe an antecedent can-
didate. Antecedent candidates are represented
by the target language words aligned to the syn-
tactic head of the source language markable
381
TED News
ce 16.3 % 6.4 %
elle 7.1 % 10.1 %
elles 3.0 % 3.9 %
il 17.1 % 26.5 %
ils 15.6 % 15.1 %
OTHER 40.9 % 38.0 %
Table 1: Distribution of classes in the training data
noun phrase as identified by the Collins head
finder (Collins, 1999).
The different handling of anaphora context features
and antecedent features is due to the fact that we al-
ways consider a constant number of context words
on the source side, whereas the number of word
vectors to be considered depends on the number of
antecedent candidates and on the number of target
words aligned to each antecedent.
The encoding of the antecedent features is illus-
trated in Figure 2 for a training example with two
antecedent candidates translated to elle and la ver-
sion, respectively. The target words are represented
as one-hot vectors with the dimensionality of the tar-
get language vocabulary. These vectors are then av-
eraged to yield a single vector per antecedent candi-
date. Finally, the vectors of all candidates for a given
training example are weighted by the probabilities
assigned to them by the anaphora resolver (p1 and
p2) and summed to yield a single vector per training
example.
3 Data Sets and External Tools
We run experiments with two different test sets. The
TED data set consists of around 2.6 million tokens of
lecture subtitles released in the WIT3 corpus (Cet-
tolo et al, 2012). The WIT3 training data yields
71,052 examples, which were randomly partitioned
into a training set of 63,228 examples and a test set
of 7,824 examples. The official WIT3 development
and test sets were not used in our experiments. The
news-commentary data set is version 6 of the parallel
news-commentary corpus released as a part of the
WMT 2011 training data1. It contains around 2.8 mil-
lion tokens of news text and yields 31,017 data points,
1http://www.statmt.org/wmt11/translation-task.
html (3 July 2013).
which were randomly split into 27,900 training exam-
ples and 3,117 test instances. The distribution of the
classes in the two training sets is shown in Table 1.
One thing to note is the dominance of the OTHER
class, which pools together such different phenom-
ena as translations with other pronouns not in our list
(e. g., celui-ci) and translations with full noun phrases
instead of pronouns. Splitting this group into more
meaningful subcategories is not straightforward and
must be left to future work.
The feature setup of all our classifiers requires
the detection of potential antecedents and the extrac-
tion of features pairing anaphoric pronouns with an-
tecedent candidates. Some of our experiments also
rely on an external anaphora resolution component.
We use the open-source anaphora resolver BART to
generate this information. BART (Broscheit et al,
2010) is an anaphora resolution toolkit consisting of
a markable detection and feature extraction pipeline
based on a variety of standard natural language pro-
cessing (NLP) tools and a machine learning com-
ponent to predict coreference links including both
pronominal anaphora and noun-noun coreference. In
our experiments, we always use BART?s markable
detection and feature extraction machinery. Mark-
able detection is based on the identification of noun
phrases in constituency parses generated with the
Stanford parser (Klein and Manning, 2003). The set
of features extracted by BART is an extension of the
widely used mention-pair anaphora resolution feature
set by Soon et al (2001) (see below, Section 6).
In the experiments of the next two sections, we
also use BART to predict anaphoric links for pro-
nouns. The model used with BART is a maximum
entropy ranker trained on the ACE02-npaper corpus
(LDC2003T11). In order to obtain a probability dis-
tribution over antecedent candidates rather than one-
best predictions or coreference sets, we modified the
ranking component with which BART resolves pro-
nouns to normalise and output the scores assigned
by the ranker to all candidates instead of picking the
highest-scoring candidate.
4 Baseline Classifiers
In order to create a simple, but reasonable baseline
for our task, we trained a maximum entropy (ME)
382
TED
(Accuracy: 0.685)
P R F
ce 0.593 0.728 0.654
elle 0.798 0.523 0.632
elles 0.812 0.164 0.273
il 0.764 0.550 0.639
ils 0.632 0.949 0.759
OTHER 0.724 0.692 0.708
News commentary
(Accuracy: 0.576)
P R F
ce 0.508 0.294 0.373
elle 0.530 0.312 0.393
elles 0.538 0.062 0.111
il 0.600 0.666 0.631
ils 0.593 0.769 0.670
OTHER 0.564 0.609 0.586
Table 2: Maximum entropy classifier results
TED
(Accuracy: 0.700)
P R F
ce 0.634 0.747 0.686
elle 0.756 0.617 0.679
elles 0.679 0.319 0.434
il 0.719 0.591 0.649
ils 0.663 0.940 0.778
OTHER 0.743 0.678 0.709
News commentary
(Accuracy: 0.576)
P R F
ce 0.477 0.344 0.400
elle 0.498 0.401 0.444
elles 0.565 0.116 0.193
il 0.655 0.626 0.640
ils 0.570 0.834 0.677
OTHER 0.567 0.573 0.570
Table 3: Neural network classifier with anaphoras resolved by BART
classifier with the MegaM software package2 using
the features described in the previous section and the
anaphora links found by BART. Results are shown
in Table 2. The baseline results show an overall
higher accuracy for the TED data than for the news-
commentary data. While the precision is above 50 %
in all categories and considerably higher in some,
recall varies widely.
The pronoun elles is particularly interesting. This
is the feminine plural of the personal pronoun, and
it usually corresponds to the English pronoun they,
which is not marked for gender. In French, elles is a
marked choice which is only used if the antecedent
exclusively refers to females or feminine-gendered
objects. The presence of a single item with mascu-
line grammatical gender in the antecedent will trigger
the use of the masculine plural pronoun ils instead.
This distinction cannot be predicted from the English
source pronoun or its context; making correct pre-
dictions requires knowledge about the antecedent of
the pronoun. Moreover, elles is a low-frequency pro-
noun. There are only 1,909 occurrences of this pro-
2http://www.umiacs.umd.edu/~hal/megam/ (20 June
2013).
noun in the TED training data, and 1,077 in the news-
commentary training set. Because of these special
properties of the feminine plural class, we argue that
the performance of a classifier on elles is a good indi-
cator of how well it can represent relevant knowledge
about pronominal anaphora as opposed to overfitting
to source contexts or acting on prior assumptions
about class frequencies.
In accordance with the general linguistic prefer-
ence for ils, the classifier tends to predict ils much
more often than elles when encountering an English
plural pronoun. This is reflected in the fact that elles
has much lower recall than ils. Clearly, the classifier
achieves a good part of its accuracy by making ma-
jority choices without exploiting deeper knowledge
about the antecedents of pronouns.
An additional experiment with a subset of 27,900
training examples from the TED data confirms that
the difference between TED and news commentaries
is not just an effect of training data size, but that TED
data is genuinely easier to predict than news com-
mentaries. In the reduced data TED condition, the
classifier achieves an accuracy of 0.673. Precision
and recall of all classifiers are much closer to the
383
E
P
R1
L1
R2
L2
R3
L3
p3p2p1
321
H
S
A
Figure 3: Neural network for pronoun prediction
large-data TED condition than to the news commen-
tary experiments, except for elles, where we obtain
an F-score of 0.072 (P 0.818, R 0.038), indicating
that small training data size is a serious problem for
this low-frequency class.
5 Neural Network Classifier
In the previous section, we saw that a simple multi-
class maximum entropy classifier, while making cor-
rect predictions for much of the data set, has a signifi-
cant bias towards making majority class decisions, re-
lying more on prior assumptions about the frequency
distribution of the classes than on antecedent features
when handling examples of less frequent classes. In
order to create a system that can be trained to rely
more explicitly on antecedent information, we cre-
ated a neural network classifier for our task. The intro-
duction of a hidden layer should enable the classifier
to learn abstract concepts such as gender and number
that are useful across multiple output categories, so
that the performance of sparsely represented classes
can benefit from the training examples of the more
frequent classes.
The overall structure of the network is shown in
Figure 3. As inputs, the network takes the same fea-
tures that were available to the baseline ME classifier,
based on the source pronoun (P) with three words
of context to its left (L1 to L3) and three words to
its right (R1 to R3) as well as the words aligned to
the syntactic head words of all possible antecedent
candidates as found by BART (A). All words are
encoded as one-hot vectors whose dimensionality is
equal to the vocabulary size. If multiple words are
aligned to the syntactic head of an antecedent candi-
date, their word vectors are averaged with uniform
weights. The resulting vectors for each antecedent
are then averaged with weights defined by the pos-
terior distribution of the anaphora resolver in BART
(p1 to p3).
The network has two hidden layers. The first layer
(E) maps the input word vectors to a low-dimensional
representation. In this layer, the embedding weights
for all the source language vectors (the pronoun
and its 6 context words) are tied, so if two words
are the same, they are mapped to the same lower-
dimensional embedding irrespective of their position
relative to the pronoun. The embedding of the an-
tecedent word vectors is independent, as these word
vectors represent target language words. The entire
embedding layer is then mapped to another hidden
layer (H), which is in turn connected to a softmax out-
put layer (S) with 6 outputs representing the classes
ce, elle, elles, il, ils and OTHER. The non-linearity of
both hidden layers is the logistic sigmoid function,
f (x) = 1/(1+ e?x).
In all experiments reported in this paper, the dimen-
sionality of the source and target language word em-
beddings is 20, resulting in a total embedding layer
size of 160, and the size of the last hidden layer is
equal to 50. These sizes are fairly small. In experi-
ments with larger layer sizes, we were able to obtain
similar, but no better results.
384
The neural network is trained with mini-batch
stochastic gradient descent with backpropagated gra-
dients using the RMSPROP algorithm with cross-
entropy as the objective function.3 In contrast to
standard gradient descent, RMSPROP normalises the
magnitude of the gradient components by dividing
them by a root-mean-square moving average. We
found this led to faster convergence. Other features
of our training algorithm include the use of momen-
tum to even out gradient oscillations, adaptive learn-
ing rates for each weight as well as adaptation of
the global learning rate as a function of current train-
ing progress. The network is regularised with an `2
weight penalty. Good settings of the initial learning
rate and the weight cost parameter (both around 0.001
in most experiments) were found by manual experi-
mentation. Generally, we train our networks for 300
epochs, compute the validation error on a held-out
set of some 10 % of the training data after each epoch
and use the model that achieved the lowest validation
error for testing.
Since the source context features are very infor-
mative and it is comparatively more difficult to learn
from the antecedents, the network sometimes had a
tendency to overfit to the source features and disre-
gard antecedent information. We found that this prob-
lem can be solved effectively by presenting a part of
the training without any source features, forcing the
network to learn from the information contained in
the antecedents. In all experiments in this paper, we
zero out all source features (input layers P, L1 to L3
and R1 to R3) with a probability of 50 % in each
training example. At test time, no information is ze-
roed out.
Classification results with this network are shown
in Table 3. We note that the accuracy has increased
slightly for the TED test set and remains exactly the
same for the news commentary corpus. However, a
closer look on the results for individual classes re-
veals that the neural network makes better predictions
for almost all classes. In terms of F-score, the only
class that becomes slightly worse is the OTHER class
for the news commentary corpus because of lower
recall, indicating that the neural network classifier is
less biased towards using the uninformative OTHER
3Our training procedure is greatly inspired by a series of on-
line lectures held by Geoffrey Hinton in 2012 (https://www.
coursera.org/course/neuralnets, 10 September 2013).
category. Recall for elle and elles increases consider-
ably, but especially for elles it is still quite low. The
increase in recall comes with some loss in precision,
but the net effect on F-score is clearly positive.
6 Latent Anaphora Resolution
Considering Figure 1 again, we note that the bilin-
gual setting of our classification task adds some in-
formation not available to the monolingual anaphora
resolver that can be helpful when determining the
correct antecedent for a given pronoun. Knowing the
gender of the translation of a pronoun limits the set
of possible antecedents to those whose translation is
morphologically compatible with the target language
pronoun. We can exploit this fact to learn how to
resolve anaphoric pronouns without requiring data
with manually annotated anaphoric links.
To achieve this, we extend our neural network with
a component to predict the probability of each an-
tecedent candidate to be the correct antecedent (Fig-
ure 4). The extended network is identical to the previ-
ous version except for the upper left part dealing with
anaphoric link features. The only difference between
the two networks is the fact that anaphora resolution
is now performed by a part of our neural network
itself instead of being done by an external module
and provided to the classifier as an input.
In this setup, we still use some parts of the BART
toolkit to extract markables and compute features.
However, we do not make use of the machine learn-
ing component in BART that makes the actual pre-
dictions. Since this is the only component trained on
coreference-annotated data in a typical BART con-
figuration, no coreference annotations are used any-
where in our system even though we continue to rely
on the external anaphora resolver for preprocessing
to avoid implementing our own markable and feature
extractors and to make comparison easier.
For each candidate markable identified by BART?s
preprocessing pipeline, the anaphora resolution
model receives as input a link feature vector (T) de-
scribing relevant aspects of the antecedent candidate-
anaphora pair. This feature vector is generated by the
feature extraction machinery in BART and includes
a standard feature set for coreference resolution par-
tially based on work by Soon et al (2001). We use
the following feature extractors in BART, each of
385
E
H
S
1
2
3
L3
R3
L2
R2
L1
R1
P
T
U
V
1 2 3
A
Figure 4: Neural network with latent anaphora resolution
which can generate multiple features:
? Anaphora mention type
? Gender match
? Number match
? String match
? Alias feature (Soon et al, 2001)
? Appositive position feature (Soon et al, 2001)
? Semantic class (Soon et al, 2001)
? Semantic class match
? Binary distance feature
? Antecedent is first mention in sentence
Our baseline set of features was borrowed whole-
sale from a working coreference system and includes
some features that are not relevant to the task at hand,
e. g., features indicating that the anaphora is a pro-
noun, is not a named entity, etc. After removing all
features that assume constant values in the training
set when resolving antecedents for the set of pro-
nouns we consider, we are left with a basic set of 37
anaphoric link features that are fed as inputs to our
network. These features are exactly the same as those
available to the anaphora resolution classifier in the
BART system used in the previous section.
Each training example for our network can have
an arbitrary number of antecedent candidates, each of
which is described by an antecedent word vector (A)
and by an anaphoric link vector (T). The anaphoric
link features are first mapped to a regular hidden layer
with logistic sigmoid units (U). The activations of the
hidden units are then mapped to a single value, which
functions as an element in a softmax layer over all an-
tecedent candidates (V). This softmax layer assigns
a probability to each antecedent candidate, which we
then use to compute a weighted average over the an-
tecedent word vector, replacing the probabilities pi
in Figures 2 and 3.
At training time, the network?s anaphora resolu-
tion component is trained in exactly the same way as
the rest of the network. The error signal from the em-
bedding layer is backpropagated both to the weight
matrix defining the antecedent word embedding and
to the anaphora resolution subnetwork. Note that the
number of weights in the network is the same for
all training examples even though the number of an-
tecedent candidates varies because all weights related
to antecedent word features and anaphoric link fea-
tures are shared between all antecedent candidates.
One slightly uncommon feature of our neural net-
work is that it contains an internal softmax layer to
generate normalised probabilities over all possible
antecedent candidates. Moreover, weights are shared
between all antecedent candidates, so the inputs of
our internal softmax layer share dependencies on
the same weight variables. When computing deriva-
tives with backpropagation, these shared dependen-
cies must be taken into account. In particular, the
outputs yi of the antecedent resolution layer are the re-
sult of a softmax applied to functions of some shared
variables q:
yi =
exp fi(q)
?k exp fk(q)
(1)
386
The derivatives of any yi with respect to q, which
can be any of the weights in the anaphora resolution
subnetwork, have dependencies on the derivatives of
the other softmax inputs with respect to q:
?yi
?q = yi
(
? fi(q)
?q ??k
yk
? fk(q)
?q
)
(2)
This makes the implementation of backpropagation
for this part of the network somewhat more compli-
cated, but in the case of our networks, it has no major
impact on training time.
Experimental results for this network are shown
in Table 4. Compared with Table 3, we note that the
overall accuracy is only very slightly lower for TED,
and for the news commentaries it is actually better.
When it comes to F-scores, the performance for elles
improves by a small amount, while the effect on the
other classes is a bit more mixed. Even where it gets
worse, the differences are not dramatic considering
that we eliminated a very knowledge-rich resource
from the training process. This demonstrates that it
is possible, in our classification task, to obtain good
results without using any data manually annotated for
anaphora and to rely entirely on unsupervised latent
anaphora resolution.
7 Further Improvements
The results presented in the preceding section repre-
sent a clear improvement over the ME classifiers in
Table 2, even though the overall accuracy increased
only slightly. Not only does our neural network clas-
sifier achieve better results on the classification task
at hand without requiring an anaphora resolution clas-
sifier trained on manually annotated data, but it per-
forms clearly better for the feminine categories that
reflect minority choices requiring knowledge about
the antecedents. Nevertheless, the performance is still
not entirely satisfactory.
By subjecting the output of our classifier on a de-
velopment set to a manual error analysis, we found
that a fairly large number of errors belong to two error
types: On the one hand, the preprocessing pipeline
used to identify antecedent candidates does not al-
ways include the correct antecedent in the set pre-
sented to the neural network. Whenever this occurs,
it is obvious that the classifier cannot possibly find
the correct antecedent. Out of 76 examples of the cat-
egory elles that had been mistakenly predicted as ils,
we found that 43 suffered from this problem. In other
classes, the problem seems to be somewhat less com-
mon, but it still exists. On the other hand, in many
cases (23 out of 76 for the category mentioned be-
fore) the anaphora resolution subnetwork does iden-
tify an antecedent manually recognised to belong to
the right gender/number group, but still predicts an in-
correct pronoun. This may indicate that the network
has difficulties learning a correct gender/number rep-
resentation for all words in the vocabulary.
7.1 Relaxing Markable Extraction
The pipeline we use to extract potential antecedent
candidates is borrowed from the BART anaphora
resolution toolkit. BART uses a syntactic parser to
identify noun phrases as markables. When extract-
ing antecedent candidates for coreference prediction,
it starts by considering a window consisting of the
sentence in which the anaphoric pronoun is located
and the two immediately preceding sentences. Mark-
ables in this window are checked for morphological
compatibility in terms of gender and number with the
anaphoric pronoun, and only compatible markables
are extracted as antecedent candidates. If no compat-
ible markables are found in the initial window, the
window is successively enlarged one sentence at a
time until at least one suitable markable is found.
Our error analysis shows that this procedure
misses some relevant markables both because the ini-
tial two-sentence extraction window is too small and
because the morphological compatibility check incor-
rectly filters away some markables that should have
been considered as candidates. By contrast, the ex-
traction procedure does extract quite a number of first
and second person noun phrases (I, we, you and their
oblique forms) in the TED talks which are extremely
unlikely to be the antecedent of a later occurrence of
he, she, it or they. As a first step, we therefore adjust
the extraction criteria to our task by increasing the
initial extraction window to five sentences, exclud-
ing first and second person markables and removing
the morphological compatibility requirement. The
compatibility check is still used to control expansion
of the extraction window, but it is no longer applied
to filter the extracted markables. This increases the
accuracy to 0.701 for TED and 0.602 for the news
387
TED
(Accuracy: 0.696)
P R F
ce 0.618 0.722 0.666
elle 0.754 0.548 0.635
elles 0.737 0.340 0.465
il 0.718 0.629 0.670
ils 0.652 0.916 0.761
OTHER 0.741 0.682 0.711
News commentary
(Accuracy: 0.597)
P R F
ce 0.419 0.368 0.392
elle 0.547 0.460 0.500
elles 0.539 0.135 0.215
il 0.623 0.719 0.667
ils 0.596 0.783 0.677
OTHER 0.614 0.544 0.577
Table 4: Neural network classifier with latent anaphora resolution
TED
(Accuracy: 0.713)
P R F
ce 0.611 0.723 0.662
elle 0.749 0.596 0.664
elles 0.602 0.616 0.609
il 0.733 0.638 0.682
ils 0.710 0.884 0.788
OTHER 0.760 0.704 0.731
News commentary
(Accuracy: 0.626)
P R F
ce 0.492 0.324 0.391
elle 0.526 0.439 0.478
elles 0.547 0.558 0.552
il 0.599 0.757 0.669
ils 0.671 0.878 0.761
OTHER 0.681 0.526 0.594
Table 5: Final classifier results
commentaries, while the performance for elles im-
proves to F-scores of 0.531 (TED; P 0.690, R 0.432)
and 0.304 (News commentaries; P 0.444, R 0.231),
respectively. Note that these and all the following re-
sults are not directly comparable to the ME baseline
results in Table 2, since they include modifications
and improvements to the training data extraction pro-
cedure that might possibly lead to benefits in the ME
setting as well.
7.2 Adding Lexicon Knowledge
In order to make it easier for the classifier to iden-
tify the gender and number properties of infrequent
words, we extend the word vectors with features indi-
cating possible morphological features for each word.
In early experiments with ME classifiers, we found
that our attempts to do proper gender and number
tagging in French text did not improve classification
performance noticeably, presumably because the an-
notation was too noisy. In more recent experiments,
we just add features indicating all possible morpho-
logical interpretations of each word, rather than try-
ing to disambiguate them. To do this, we look up
the morphological annotations of the French words
in the Lefff dictionary (Sagot et al, 2006) and intro-
duce a set of new binary features to indicate whether
a particular reading of a word occurs in that dictio-
nary. These features are then added to the one-hot
representation of the antecedent words. Doing so im-
proves the classifier accuracy to 0.711 (TED) and
0.604 (News commentaries), while the F-scores for
elles reach 0.589 (TED; P 0.649, R 0.539) and 0.500
(News commentaries; P 0.545, R 0.462), respectively.
7.3 More Anaphoric Link Features
Even though the modified antecedent candidate ex-
traction with its larger context window and without
the morphological filter results in better performance
on both test sets, additional error analysis reveals
that the classifiers has greater problems identifying
the correct markable in this setting. One reason for
this may be that the baseline anaphoric link feature
set described above (Section 6) only includes two
very rough binary distance features which indicate
whether or not the anaphora and the antecedent can-
didate occur in the same or in immediately adjacent
sentences. With the larger context window, this may
be too unspecific. In our final experiment, we there-
fore enable some additional features which are avail-
able in BART, but disabled in the baseline system:
388
? Distance in number of markables
? Distance in number of sentences
? Sentence distance, log-transformed
? Distance in number of words
? Part of speech of head word
Most of these encode the distance between the
anaphora and the antecedent candidate in more pre-
cise ways. Complete results for this final system are
presented in Table 5.
Including these additional features leads to another
slight increase in accuracy for both corpora, with sim-
ilar or increased classifier F-scores for most classes
except elle in the news commentary condition. In par-
ticular, we should like to point out the performance
of our benchmark classifier for elles, which suffered
from extremely low recall in the first classifiers and
approaches the performance of the other classes, with
nearly balanced precision and recall, in this final sys-
tem. Since elles is a low-frequency class and cannot
be reliably predicted using source context alone, we
interpret this as evidence that our final neural network
classifier has incorporated some relevant knowledge
about pronominal anaphora that the baseline ME clas-
sifier and earlier versions of our network have no ac-
cess to. This is particularly remarkable because no
data manually annotated for coreference was used for
training.
8 Related work
Even though it was recognised years ago that the
information contained in parallel corpora may pro-
vide valuable information for the improvement of
anaphora resolution systems, there have not been
many attempts to cash in on this insight. Mitkov and
Barbu (2003) exploit parallel data in English and
French to improve pronominal anaphora resolution
by combining anaphora resolvers for the individual
languages with handwritten rules to resolve conflicts
between the output of the language-specific resolvers.
Veselovsk? et al (2012) apply a similar strategy to
English-Czech data to resolve different uses of the
pronoun it. Other work has used word alignments to
project coreference annotations from one language
to another with a view to training anaphora resolvers
in the target language (Postolache et al, 2006; de
Souza and Ora?san, 2011). Rahman and Ng (2012)
instead use machine translation to translate their test
data into a language for which they have an anaphora
resolver and then project the annotations back to the
original language. Completely unsupervised mono-
lingual anaphora resolution has been approached us-
ing, e. g., Markov logic (Poon and Domingos, 2008)
and the Expectation-Maximisation algorithm (Cherry
and Bergsma, 2005; Charniak and Elsner, 2009). To
the best of our knowledge, the direct application of
machine learning techniques to parallel data in a task
related to anaphora resolution is novel in our work.
Neural networks and deep learning techniques
have recently gained some popularity in natural lan-
guage processing. They have been applied to tasks
such as language modelling (Bengio et al, 2003;
Schwenk, 2007), translation modelling in statistical
machine translation (Le et al, 2012), but also part-of-
speech tagging, chunking, named entity recognition
and semantic role labelling (Collobert et al, 2011).
In tasks related to anaphora resolution, standard feed-
forward neural networks have been tested as a clas-
sifier in an anaphora resolution system (Stuckardt,
2007), but the network design presented in our work
is novel.
9 Conclusion
In this paper, we have introduced cross-lingual pro-
noun prediction as an independent natural language
processing task. Even though it is not an end-to-end
task, pronoun prediction is interesting for several rea-
sons. It is related to the problem of pronoun transla-
tion in SMT, a currently unsolved problem that has
been addressed in a number of recent research publi-
cations (Le Nagard and Koehn, 2010; Hardmeier and
Federico, 2010; Guillou, 2012) without reaching a
major breakthrough. In this work, we have shown that
pronoun prediction can be effectively modelled in a
neural network architecture with relatively simple
features. More importantly, we have demonstrated
that the task can be exploited to train a classifier with
a latent representation of anaphoric links. With paral-
lel text as its only supervision this classifier achieves
a level of performance that is similar to, if not bet-
ter than, that of a classifier using a regular anaphora
resolution system trained with manually annotated
data.
389
References
Yoshua Bengio, R?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155.
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodriguez, Lorenza Romano,
Olga Uryupina, Yannick Versley, and Roberto Zanoli.
2010. BART: A multilingual anaphora resolution sys-
tem. In Proceedings of the 5th International Work-
shop on Semantic Evaluations (SemEval-2010), Upp-
sala, Sweden, 15?16 July 2010.
Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012. WIT3: Web inventory of transcribed and trans-
lated talks. In Proceedings of the 16th Conference
of the European Association for Machine Translation
(EAMT), pages 261?268, Trento, Italy.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), pages 148?156, Athens, Greece.
Colin Cherry and Shane Bergsma. 2005. An Expecta-
tion Maximization approach to pronoun resolution. In
Proceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005), pages 88?
95, Ann Arbor, Michigan.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
Ronan Collobert, Jason Weston, L?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2461?2505.
Jos? de Souza and Constantin Ora?san. 2011. Can pro-
jected chains in parallel corpora help coreference reso-
lution? In Iris Hendrickx, Sobha Lalitha Devi, Ant?nio
Branco, and Ruslan Mitkov, editors, Anaphora Process-
ing and Applications, volume 7099 of Lecture Notes in
Computer Science, pages 59?69. Springer, Berlin.
Liane Guillou. 2012. Improving pronoun translation for
statistical machine translation. In Proceedings of the
Student Research Workshop at the 13th Conference of
the European Chapter of the Association for Computa-
tional Linguistics, pages 1?10, Avignon, France.
Christian Hardmeier and Marcello Federico. 2010. Mod-
elling pronominal anaphora in statistical machine trans-
lation. In Proceedings of the seventh International
Workshop on Spoken Language Translation (IWSLT),
pages 283?289, Paris, France.
Christian Hardmeier. 2012. Discourse in statistical ma-
chine translation: A survey and a case study. Discours,
11.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan.
Hai-Son Le, Alexandre Allauzen, and Fran?ois Yvon.
2012. Continuous space translation models with neural
networks. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 39?48, Montr?al, Canada.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding pro-
noun translation with co-reference resolution. In Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 252?261,
Uppsala, Sweden.
Ruslan Mitkov and Catalina Barbu. 2003. Using bilingual
corpora to improve pronoun resolution. Languages in
Contrast, 4(2):201?211.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment models.
Computational linguistics, 29:19?51.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 650?
659, Honolulu, Hawaii.
Oana Postolache, Dan Cristea, and Constantin Ora?san.
2006. Transferring coreference chains through word
alignment. In Proceedings of the 5th Conference
on International Language Resources and Evaluation
(LREC-2006), pages 889?892, Genoa.
Altaf Rahman and Vincent Ng. 2012. Translation-based
projection for multilingual coreference resolution. In
Proceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 720?
730, Montr?al, Canada.
Beno?t Sagot, Lionel Cl?ment, ?ric Villemonte de La
Clergerie, and Pierre Boullier. 2006. The Lefff 2
syntactic lexicon for French: architecture, acquisition,
use. In Proceedings of the 5th Conference on Inter-
national Language Resources and Evaluation (LREC-
2006), pages 1348?1351, Genoa.
Holger Schwenk. 2007. Continuous space language mod-
els. Computer Speech and Language, 21(3):492?518.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational lin-
guistics, 27(4):521?544.
Roland Stuckardt. 2007. Applying backpropagation net-
works to anaphor resolution. In Ant?nio Branco, editor,
Anaphora: Analysis, Algorithms and Applications. 6th
390
Discourse Anaphora and Anaphor Resolution Collo-
quium, DAARC 2007, number 4410 in Lecture Notes
in Artificial Intelligence, pages 107?124, Berlin.
Kater?ina Veselovsk?, Ngu.y Giang Linh, and Michal
Nov?k. 2012. Using Czech-English parallel corpora in
automatic identification of it. In Proceedings of the 5th
Workshop on Building and Using Comparable Corpora,
pages 112?120, Istanbul, Turkey.
391
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 193?198,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Docent: A Document-Level Decoder for
Phrase-Based Statistical Machine Translation
Christian Hardmeier Sara Stymne J?rg Tiedemann Joakim Nivre
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
We describe Docent, an open-source de-
coder for statistical machine translation
that breaks with the usual sentence-by-
sentence paradigm and translates complete
documents as units. By taking transla-
tion to the document level, our decoder
can handle feature models with arbitrary
discourse-wide dependencies and consti-
tutes an essential infrastructure compon-
ent in the quest for discourse-aware SMT
models.
1 Motivation
Most of the research on statistical machine trans-
lation (SMT) that was conducted during the last
20 years treated every text as a ?bag of sentences?
and disregarded all relations between elements in
different sentences. Systematic research into ex-
plicitly discourse-related problems has only begun
very recently in the SMT community (Hardmeier,
2012) with work on topics such as pronominal
anaphora (Le Nagard and Koehn, 2010; Hard-
meier and Federico, 2010; Guillou, 2012), verb
tense (Gong et al, 2012) and discourse connect-
ives (Meyer et al, 2012).
One of the problems that hamper the develop-
ment of cross-sentence models for SMT is the fact
that the assumption of sentence independence is
at the heart of the dynamic programming (DP)
beam search algorithm most commonly used for
decoding in phrase-based SMT systems (Koehn et
al., 2003). For integrating cross-sentence features
into the decoding process, researchers had to adopt
strategies like two-pass decoding (Le Nagard and
Koehn, 2010). We have previously proposed an
algorithm for document-level phrase-based SMT
decoding (Hardmeier et al, 2012). Our decoding
algorithm is based on local search instead of dy-
namic programming and permits the integration of
document-level models with unrestricted depend-
encies, so that a model score can be conditioned on
arbitrary elements occurring anywhere in the input
document or in the translation that is being gen-
erated. In this paper, we present an open-source
implementation of this search algorithm. The de-
coder is written in C++ and follows an object-
oriented design that makes it easy to extend it with
new feature models, new search operations or dif-
ferent types of local search algorithms. The code
is released under the GNU General Public License
and published on Github1 to make it easy for other
researchers to use it in their own experiments.
2 Document-Level Decoding with Local
Search
Our decoder is based on the phrase-based SMT
model described by Koehn et al (2003) and im-
plemented, for example, in the popular Moses
decoder (Koehn et al, 2007). Translation is
performed by splitting the input sentence into
a number of contiguous word sequences, called
phrases, which are translated into the target lan-
guage through a phrase dictionary lookup and op-
tionally reordered. The choice between different
translations of an ambiguous source phrase and the
ordering of the target phrases are guided by a scor-
ing function that combines a set of scores taken
from the phrase table with scores from other mod-
els such as an n-gram language model. The actual
translation process is realised as a search for the
highest-scoring translation in the space of all the
possible translations that could be generated given
the models.
The decoding approach that is implemented in
Docent was first proposed by Hardmeier et al
(2012) and is based on local search. This means
that it has a state corresponding to a complete, if
possibly bad, translation of a document at every
1https://github.com/chardmeier/docent/wiki
193
stage of the search progress. Search proceeds by
making small changes to the current search state in
order to transform it gradually into a better trans-
lation. This differs from the DP algorithm used in
other decoders, which starts with an empty trans-
lation and expands it bit by bit. It is similar to
previous work on phrase-based SMT decoding by
Langlais et al (2007), but enables the creation of
document-level models, which was not addressed
by earlier approaches.
Docent currently implements two search al-
gorithms that are different generalisations of the
hill climbing local search algorithm by Hardmeier
et al (2012). The original hill climbing algorithm
starts with an initial state and generates possible
successor states by randomly applying simple ele-
mentary operations to the state. After each op-
eration, the new state is scored and accepted if
its score is better than that of the previous state,
else rejected. Search terminates when the decoder
cannot find an acceptable successor state after a
certain number of attempts, or when a maximum
number of steps is reached.
Simulated annealing is a stochastic variant of
hill climbing that always accepts moves towards
better states, but can also accept moves towards
lower-scoring states with a certain probability that
depends on a temperature parameter in order to
escape local maxima. Local beam search gener-
alises hill climbing in a different way by keeping
a beam of a fixed number of multiple states at any
time and randomly picking a state from the beam
to modify at each move. The original hill climb-
ing procedure can be recovered as a special case
of either one of these search algorithms, by call-
ing simulated annealing with a fixed temperature
of 0 or local beam search with a beam size of 1.
Initial states for the search process can be gen-
erated either by selecting a random segmentation
with random translations from the phrase table in
monotonic order, or by running DP beam search
with sentence-local models as a first pass. For
the second option, which generally yields better
search results, Docent is linked with the Moses
decoder and makes direct calls to the DP beam
search algorithm implemented by Moses. In addi-
tion to these state initialisation procedures, Docent
can save a search state to a disk file which can be
loaded again in a subsequent decoding pass. This
saves time especially when running repeated ex-
periments from the same starting point obtained
by DP search.
In order to explore the complete search space
of phrase-based SMT, the search operations in a
local search decoder must be able to change the
phrase translations, the order of the output phrases
and the segmentation of the source sentence into
phrases. The three operations used by Hardmeier
et al (2012), change-phrase-translation, reseg-
ment and swap-phrases, jointly meet this require-
ment and are all implemented in Docent. Addi-
tionally, Docent features three extra operations, all
of which affect the target word order: The move-
phrases operation moves a phrase to another loca-
tion in the sentence. Unlike swap-phrases, it does
not require that another phrase be moved in the
opposite direction at the same time. A pair of
operations called permute-phrases and linearise-
phrases can reorder a sequence of phrases into ran-
dom order and back into the order corresponding
to the source language.
Since the search algorithm in Docent is
stochastic, repeated runs of the decoder will gen-
erally produce different output. However, the vari-
ance of the output is usually small, especially
when initialising with a DP search pass, and it
tends to be lower than the variance introduced
by feature weight tuning (Hardmeier et al, 2012;
Stymne et al, 2013a).
3 Available Feature Models
In its current version, Docent implements a selec-
tion of sentence-local feature models that makes
it possible to build a baseline system with a con-
figuration comparable to that of a typical Moses
baseline system. The published source code
also includes prototype implementations of a few
document-level models. These models should be
considered work in progress and serve as a demon-
stration of the cross-sentence modelling capabilit-
ies of the decoder. They have not yet reached a
state of maturity that would make them suitable
for production use.
The sentence-level models provided by Docent
include the phrase table, n-gram language models
implemented with the KenLM toolkit (Heafield,
2011), an unlexicalised distortion cost model with
geometric decay (Koehn et al, 2003) and a word
penalty cost. All of these features are designed
to be compatible with the corresponding features
in Moses. From among the typical set of baseline
features in Moses, we have not implemented the
194
lexicalised distortion model, but this model could
easily be added if required. Docent uses the same
binary file format for phrase tables as Moses, so
the same training apparatus can be used.
DP-based SMT decoders have a parameter
called distortion limit that limits the difference in
word order between the input and the MT out-
put. In DP search, this is formally considered to
be a parameter of the search algorithm because it
affects the algorithmic complexity of the search
by controlling how many translation options must
be considered at each hypothesis expansion. The
stochastic search algorithm in Docent does not re-
quire this limitation, but it can still be useful be-
cause the standard models of SMT do not model
long-distance reordering well. Docent therefore
includes a separate indicator feature to indicate
a violated distortion limit. In conjunction with a
very large weight, this feature can effectively en-
sure that the distortion limit is enforced. In con-
trast with the distortion limit parameter of a DP de-
coder, the weight of our distortion limit feature can
potentially be tuned to permit occasional distor-
tion limit violations when they contribute to better
translations.
The document-level models included in Docent
include a length parity model, a semantic lan-
guage model as well as a collection of document-
level readability models. The length parity model
is a proof-of-concept model that ensures that all
sentences in a document have either consistently
odd or consistently even length. It serves mostly as
a template to demonstrate how a simple document-
level model can be implemented in the decoder.
The semantic language model was originally pro-
posed by Hardmeier et al (2012) to improve lex-
ical cohesion in a document. It is a cross-sentence
model over sequences of content words that are
scored based on their similarity in a word vector
space. The readability models serve to improve
the readability of the translation by encouraging
the selection of easier and more consistent target
words. They are described and demonstrated in
more detail in section 5.
Docent can read input files both in the NIST-
XML format commonly used to encode docu-
ments in MT shared tasks such as NIST or WMT
and in the more elaborate MMAX format (M?ller
and Strube, 2003). The MMAX format makes
it possible to include a wide range of discourse-
level corpus annotations such as coreference links.
These annotations can then be accessed by the
feature models. To allow for additional target-
language information such as morphological fea-
tures of target words, Docent can handle simple
word-level annotations that are encoded in the
phrase table in the same way as target language
factors in Moses.
In order to optimise feature weights we have
adapted the Moses tuning infrastructure to Do-
cent. In this way we can take advantage of all its
features, for instance using different optimisation
algorithms such as MERT (Och, 2003) or PRO
(Hopkins and May, 2011), and selective tuning of
a subset of features. Since document features only
give meaningful scores on the document level and
not on the sentence level, we naturally perform
optimisation on document level, which typically
means that we need more data than for the op-
timisation of sentence-based decoding. The res-
ults we obtain are relatively stable and competit-
ive with sentence-level optimisation of the same
models (Stymne et al, 2013a).
4 Implementing Feature Models
Efficiently
While translating a document, the local search de-
coder attempts to make a great number of moves.
For each move, a score must be computed and
tested against the acceptance criterion. An over-
whelming majority of the proposed moves will be
rejected. In order to achieve reasonably fast de-
coding times, efficient scoring is paramount. Re-
computing the scores of the whole document at
every step would be far too slow for the decoder
to be useful. Fortunately, score computation can
be sped up in two ways. Knowledge about how
the state to be scored was generated from its pre-
decessor helps to limit recomputations to a min-
imum, and by adopting a two-step scoring proced-
ure that just computes the scores that can be calcu-
lated with little effort at first, we need to compute
the complete score only if the new state has some
chance of being accepted.
The scores of SMT feature models can usu-
ally be decomposed in some way over parts of
the document. The traditional models borrowed
from sentence-based decoding are necessarily de-
composable at the sentence level, and in practice,
all common models are designed to meet the con-
straints of DP beam search, which ensures that
they can in fact be decomposed over even smal-
195
ler sequences of just a few words. For genuine
document-level features, this is not the case, but
even these models can often be decomposed in
some way, for instance over paragraphs, anaphoric
links or lexical chains. To take advantage of this
fact, feature models in Docent always have access
to the previous state and its score and to a list of
the state modifications that transform the previous
state into the next. The scores of the new state are
calculated by identifying the parts of a document
that are affected by the modifications, subtract-
ing the old scores of this part from the previous
score and adding the new scores. This approach
to scoring makes feature model implementation
a bit more complicated than in DP search, but it
gives the feature models full control over how they
decompose a document while still permitting effi-
cient decoding.
A feature model class in Docent implements
three methods. The initDocument method is called
once per document when decoding starts. It
straightforwardly computes the model score for
the entire document from scratch. When a state
is modified, the decoder first invokes the estim-
ateScoreUpdate method. Rather than calculating
the new score exactly, this method is only required
to return an upper bound that reflects the max-
imum score that could possibly be achieved by this
state. The search algorithm then checks this upper
bound against the acceptance criterion. Only if the
upper bound meets the criterion does it call the
updateScore method to calculate the exact score,
which is then checked against the acceptance cri-
terion again.
The motivation for this two-step procedure is
that some models can compute an upper bound ap-
proximation much more efficiently than an exact
score. For any model whose score is a log probab-
ility, a value of 0 is a loose upper bound that can
be returned instantly, but in many cases, we can do
much better. In the case of the n-gram language
model, for instance, a more accurate upper bound
can be computed cheaply by subtracting from the
old score all log-probabilities of n-grams that are
affected by the state modifications without adding
the scores of the n-grams replacing them in the
new state. This approximation can be calculated
without doing any language model lookups at all.
On the other hand, some models like the distor-
tion cost or the word penalty are very cheap to
compute, so that the estimateScoreUpdate method
can simply return the precise score as a tight up-
per bound. If a state gets rejected because of a
low score on one of the cheap models, this means
we will never have to compute the more expensive
feature scores at all.
5 Readability: A Case Study
As a case study we report initial results on how
document-wide features can be used in Docent in
order to improve the readability of texts by encour-
aging simple and consistent terminology (Stymne
et al, 2013b). This work is a first step towards
achieving joint SMT and text simplification, with
the final goal of adapting MT to user groups such
as people with reading disabilities.
Lexical consistency modelling for SMT has
been attempted before. The suggested approaches
have been limited by the use of sentence-level
decoders, however, and had to resort to proced-
ures like post processing (Carpuat, 2009), multiple
decoding runs with frozen counts from previous
runs (Ture et al, 2012), or cache-based models
(Tiedemann, 2010). In Docent, however, we al-
ways have access to a full document translation,
which makes it straightforward to include features
directly into the decoder.
We implemented four features on the document
level. The first two features are type token ra-
tio (TTR) and a reformulation of it, OVIX, which
is less sensitive to text length. These ratios have
been related to the ?idea density? of a text (M?h-
lenbock and Kokkinakis, 2009). We also wanted
to encourage consistent translations of words, for
which we used the Q-value (Del?ger et al, 2006),
which has been proposed to measure term qual-
ity. We applied it on word level (QW) and phrase
level (QP). These features need access to the full
target document, which we have in Docent. In ad-
dition, we included two sentence-level count fea-
tures for long words that have been used to meas-
ure the readability of Swedish texts (M?hlenbock
and Kokkinakis, 2009).
We tested our features on English?Swedish
translation using the Europarl corpus. For train-
ing we used 1,488,322 sentences. As test data, we
extracted 20 documents with a total of 690 sen-
tences. We used the standard set of baseline fea-
tures: 5-gram language model, translation model
with 5 weights, a word penalty and a distortion
penalty.
196
Baseline Readability features Comment
de ?rade ledam?terna (the honourable
Members)
ledam?terna (the members) / ni
(you)
+ Removal of non-essential words
p? ett s?dant s?tt att (in such a way
that)
s? att (so that) + Simplified expression
gemenskapslagstiftningen (the
community legislation)
gemenskapens lagstiftning (the
community?s legislation)
+ Shorter words by changing long
compound to genitive construction
V?rldshandelsorganisationen (World
Trade Organisation)
WTO (WTO) ? Changing long compound to
English-based abbreviation
handlingsplanen (the action plan) planen (the plan) ? Removal of important word
?gnat s?rskild uppm?rksamhet ?t (paid
particular attention to)
s?rskilt uppm?rksam p?
(particular attentive on)
? Bad grammar because of changed
part of speech and missing verb
Table 2: Example translation snippets with comments
Feature BLEU OVIX LIX
Baseline 0.243 56.88 51.17
TTR 0.243 55.25 51.04
OVIX 0.243 54.65 51.00
QW 0.242 57.16 51.16
QP 0.243 57.07 51.06
All 0.235 47.80 49.29
Table 1: Results for adding single lexical consist-
ency features to Docent
To evaluate our system we used the BLEU score
(Papineni et al, 2002) together with a set of read-
ability metrics, since readability is what we hoped
to improve by adding consistency features. Here
we used OVIX to confirm a direct impact on con-
sistency, and LIX (Bj?rnsson, 1968), which is a
common readability measure for Swedish. Unfor-
tunately we do not have access to simplified trans-
lated text, so we calculate the MT metrics against a
standard reference, which means that simple texts
will likely have worse scores than complicated
texts closer to the reference translation.
We tuned the standard features using Moses and
MERT, and then added each lexical consistency
feature with a small weight, using a grid search ap-
proach to find values with a small impact. The res-
ults are shown in Table 1. As can be seen, for in-
dividual features the translation quality was main-
tained, with small improvements in LIX, and in
OVIX for the TTR and OVIX features. For the
combination we lost a little bit on translation qual-
ity, but there was a larger effect on the readability
metrics. When we used larger weights, there was
a bigger impact on the readability metrics, with a
further decrease on MT quality.
We also investigated what types of changes the
readability features could lead to. Table 2 shows a
sample of translations where the baseline is com-
pared to systems with readability features. There
are both cases where the readability features help
and cases where they are problematic. Overall,
these examples show that our simple features can
help achieve some interesting simplifications.
There is still much work to do on how to take
best advantage of the possibilities in Docent in or-
der to achieve readable texts. This attempt shows
the feasibility of the approach. We plan to ex-
tend this work for instance by better feature op-
timisation, by integrating part-of-speech tags into
our features in order to focus on terms rather than
common words, and by using simplified texts for
evaluation and tuning.
6 Conclusions
In this paper, we have presented Docent, an open-
source document-level decoder for phrase-based
SMT released under the GNU General Public Li-
cense. Docent is the first decoder that permits the
inclusion of feature models with unrestricted de-
pendencies between arbitrary parts of the output,
even crossing sentence boundaries. A number of
research groups have recently started to investig-
ate the interplay between SMT and discourse-level
phenomena such as pronominal anaphora, verb
tense selection and the generation of discourse
connectives. We expect that the availability of a
document-level decoder will make it substantially
easier to leverage discourse information in SMT
and make SMT models explore new ground bey-
ond the next sentence boundary.
References
Carl-Hugo Bj?rnsson. 1968. L?sbarhet. Liber, Stock-
holm.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 19?27, Boulder, Colorado.
197
Louise Del?ger, Magnus Merkel, and Pierre Zweigen-
baum. 2006. Enriching medical terminologies: an
approach based on aligned corpora. In International
Congress of the European Federation for Medical
Informatics, pages 747?752, Maastricht, The Neth-
erlands.
Zhengxian Gong, Min Zhang, Chew Lim Tan, and
Guodong Zhou. 2012. N-gram-based tense models
for statistical machine translation. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 276?285,
Jeju Island, Korea.
Liane Guillou. 2012. Improving pronoun translation
for statistical machine translation. In Proceedings of
the Student Research Workshop at the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 1?10, Avignon,
France.
Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-
chine translation. In Proceedings of the seventh In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT), pages 283?289, Paris, France.
Christian Hardmeier, Joakim Nivre, and J?rg
Tiedemann. 2012. Document-wide decoding
for phrase-based statistical machine translation.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning,
pages 1179?1190, Jeju Island, Korea.
Christian Hardmeier. 2012. Discourse in statistical
machine translation: A survey and a case study. Dis-
cours, 11.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1352?1362, Edinburgh, Scotland.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 conference of the North Amer-
ican chapter of the Association for Computational
Linguistics on Human Language Technology, pages
48?54, Edmonton.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for Statistical Ma-
chine Translation. In Annual meeting of the Associ-
ation for Computational Linguistics: Demonstration
session, pages 177?180, Prague, Czech Republic.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2007. A greedy decoder for phrase-based statist-
ical machine translation. In TMI-2007: Proceedings
of the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 104?113, Sk?vde, Sweden.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statist-
ical Machine Translation and MetricsMATR, pages
252?261, Uppsala, Sweden.
Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,
and Andrea Gesmundo. 2012. Machine translation
of labeled discourse connectives. In Proceedings of
the Tenth Biennial Conference of the Association for
Machine Translation in the Americas (AMTA), San
Diego, California, USA.
Katarina M?hlenbock and Sofie Johansson Kokkinakis.
2009. LIX 68 revisited ? an extended readability. In
Proceedings of the Corpus Linguistics Conference,
Liverpool, UK.
Christoph M?ller and Michael Strube. 2003. Multi-
level annotation in MMAX. In Proceedings of the
Fourth SIGdial Workshop on Discourse and Dia-
logue, pages 198?207, Sapporo, Japan.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167, Sapporo, Ja-
pan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA.
Sara Stymne, Christian Hardmeier, J?rg Tiedemann,
and Joakim Nivre. 2013a. Feature weight optim-
ization for discourse-level SMT. In Proceedings of
the Workshop on Discourse in Machine Translation
(DiscoMT), Sofia, Bulgaria.
Sara Stymne, J?rg Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013b. Statistical machine trans-
lation with readability constraints. In Proceedings of
the 19th Nordic Conference of Computational Lin-
guistics (NODALIDA 2013), pages 375?386, Oslo,
Norway.
J?rg Tiedemann. 2010. Context adaptation in stat-
istical machine translation using models with ex-
ponentially decaying cache. In Proceedings of the
ACL 2010 Workshop on Domain Adaptation for Nat-
ural Language Processing (DANLP), pages 8?15,
Uppsala, Sweden.
Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 417?426, Montr?al, Canada.
198
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 88?92,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
FBK at WMT 2010: Word Lattices for
Morphological Reduction and Chunk-based Reordering
Christian Hardmeier, Arianna Bisazza and Marcello Federico
Fondazione Bruno Kessler
Human Language Technologies
Trento, Italy
{hardmeier,bisazza,federico}@fbk.eu
Abstract
FBK participated in the WMT 2010
Machine Translation shared task with
phrase-based Statistical Machine Transla-
tion systems based on the Moses decoder
for English-German and German-English
translation. Our work concentrates on ex-
ploiting the available language modelling
resources by using linear mixtures of large
6-gram language models and on address-
ing linguistic differences between English
and German with methods based on word
lattices. In particular, we use lattices to in-
tegrate a morphological analyser for Ger-
man into our system, and we present some
initial work on rule-based word reorder-
ing.
1 System overview
The Human Language Technologies group at Fon-
dazione Bruno Kessler (FBK) participated in the
WMT 2010 Machine Translation (MT) evaluation
with systems for English-German and German-
English translation. While the English-German
system we submitted was relatively simple, we
put some more effort into the inverse translation
direction to make better use of the abundance
of language modelling data available for English
and to address the richness of German morphol-
ogy, which makes it hard for a Statistical Machine
Translation (SMT) system to achieve good vocab-
ulary coverage. In the remainder of this section,
an overview of the common features of our sys-
tems will be given. The next two sections provide
a more detailed description of our approaches to
language modelling, morphological preprocessing
and word reordering.
Both of our systems were based on the Moses
decoder (Koehn et al, 2007). They were simi-
lar to the WMT 2010 Moses baseline system. In-
stead of lowercasing the training data and adding
a recasing step, we retained the data in document
case throughout our system, except for the mor-
phologically normalised word forms described in
section 3. Our phrase tables were trained with the
standard Moses training script, then filtered based
on statistical significance according to the method
described by Johnson et al (2007). Finally, we
used Minimum Bayes Risk decoding (Kumar and
Byrne, 2004) based on the BLEU score (Papineni
et al, 2002).
2 Language modelling
At the 2009 NIST MT evaluation, our system ob-
tained good results using a mixture of linearly in-
terpolated language models (LMs) combining data
from different sources. As the training data pro-
vided for the present evaluation campaign again
included a large set of language modelling corpora
from different sources, especially for English as
a target language, we decided to adopt the same
strategy. The partial corpora for English and their
sizes can be found in table 1. Our base mod-
els of the English Gigaword texts were trained
on version 3 of the corpus (LDC2007T07). We
trained separate language models for the new data
from the years 2007 and 2008 included in ver-
sion 4 (LDC2009T13). Apart from the mono-
lingual English data, we also included language
models trained on the English part of the addi-
tional parallel datasets supplied for the French-
English and Czech-English tasks. All the mod-
els were estimated as 6-gram models with Kneser-
Ney smoothing using the IRSTLM language mod-
elling toolkit (Federico et al, 2008).
For technical reasons, we were unable to use all
the language models during decoding. We there-
fore selected a subset of the models with the fol-
lowing data selection procedure:
1. For a linear mixture of the complete set of
24 language models, we estimated a set of
88
Corpus n-grams
Europarl v5 115,702,157
News 1,437,562,740
News commentary 10 10,381,511
Gigaword v3: 6 models 7,990,828,834
Gigaword 2007/08: 6 models 1,418,281,597
109 fr-en 1,190,593,051
UNDOC fr-en 333,120,732
CzEng: 7 models 153,355,518
Total: 24 models 12,649,826,140
Table 1: Language modelling corpora for English
LMs Perplexity
DEV EVAL
2 188.57 181.38
5 163.68 158.99
10 156.43 151.73
15 154.71 144.98
20 154.39 144.91
24 154.42 144.92
Table 2: Perplexities of LM mixtures
optimal interpolation weights to minimise
the perplexity of the mixture model on the
news-test2008 development set.
2. By sorting the mixture coefficients in de-
scending order, we obtained an ordering of
the language models by their importance with
respect to the development set. We created
partial mixtures by selecting the top n mod-
els according to this order and retraining the
mixture weights with the same algorithm.
Computing the perplexities of these partial
mixtures on the news-test2008 (DEV) and
newstest2009 (EVAL) corpora shows that signif-
icant improvements can be obtained up to a mix-
tures size of about 15 elements. As this size still
turned out to be too large to be managed by our
systems, we used a 5-element mixture in our final
submission (see table 3 for details about the mix-
ture and table 4 for the evaluation results of the
submitted systems).
For the English-German system, the only cor-
pora available for the target language were Eu-
roparl v5, News commentary v10 and the mono-
lingual News corpus. Similar experiments showed
that the News corpus was by far the most impor-
tant for the text genre to be translated and that
including language models trained on the other
Weight Language model
0.368023 News
0.188156 109 fr-en
0.174802 Gigaword v3: NYT
0.144465 Gigaword v3: AFP
0.124553 Gigaword v3: APW
Table 3: 5-element LM mixture used for decoding
BLEU-cased BLEU
en-de
primary 15.5 15.8
secondary 15.3 15.6
primary: only News language model
secondary: linear mixture of 3 LMs
de-en
primary 20.9 21.9
secondary 20.3 21.3
primary: morph. reduction, linear mixture of 5 LMs
secondary: reordering, only News LM
Table 4: Evaluation results of submitted systems
corpora could even degrade system performance.
We therefore decided not to use Europarl or News
commentary for language modelling in our pri-
mary submission. However, we submitted a sec-
ondary system using a mixture of language models
based on all three corpora.
3 Morphological reduction and
decompounding of German
Compounding is a highly productive part of Ger-
man noun morphology. Unlike in English, Ger-
man compound nouns are usually spelt as sin-
gle words, which greatly increases the vocabulary.
For a Machine Translation system, this property
of the language causes a high number of out-of-
vocabulary (OOV) words. It is likely that many
compounds in an input text have not been seen in
the training corpus. We addressed this problem by
splitting compounds in the German source text.
Compound splitting was done using the Gert-
wol morphological analyser (Koskenniemi and
Haapalainen, 1996), a linguistically informed sys-
tem based on two-level finite state morphology.
Since Gertwol outputs all possible analyses of a
word form without taking into account the context,
the output has to be disambiguated. For this pur-
pose, we used part-of-speech (POS) tags obtained
from the TreeTagger (Schmid, 1994) along with
a set of POS-based heuristic disambiguation rules
89
provided to us by the Institute of Computational
Linguistics of the University of Zurich.
As a side effect, Gertwol outputs the base forms
of all words that it processes: Nominative singu-
lar of nouns, infinitive of verbs etc. We decided to
combine the tokens analysed by Gertwol, whether
or not they had been decompounded and lower-
cased, in a further attempt to reduce data sparse-
ness, with their original form in a word lattice
(see fig. 1) and to let the decoder make the choice
between the two according to the translations the
phrase table can provide for each.
Our word lattices are similar to those used by
Dyer et al (2008) for handling word segmentation
in Chinese and Arabic. For each word that was
segmented by Gertwol, we provide exactly one al-
ternative edge labelled with the component words
and base forms as identified by Gertwol, after re-
moving linking morphemes. The edge transition
probabilities are used to identify the source of an
edge: their values are e?1 = 0.36788 for edges de-
riving from Gertwol analysis and e0 = 1 for edges
carrying unprocessed words. Tokens whose de-
compounded base form according to Gertwol is
identical to the surface form in the input are rep-
resented by a single edge with transition proba-
bility e?0.5 = 0.606531. These transition proba-
bilities translate into a binary feature with values
?1, ?0.5 and 0 after taking logarithms in the de-
coder. The feature weight is determined by Min-
imum Error-Rate Training (Och, 2003), together
with the weights of the other feature functions
used in the decoder. During system training, the
processed version of the training corpus was con-
catenated with the unprocessed text.
Experiments show that decompounding and
morphological analysis have a significant impact
on the performance of the MT system. After
these steps, the OOV rate of the newstest2009
test set decreases from 5.88% to 3.21%. Us-
ing only the News language model, the BLEU
score of our development system (measured on
the newstest2009 corpus) increases from 18.77
to 19.31. There is an interesting interaction with
the language models. While using a linear mixture
of 15 language models instead of just the News
LM does not improve the performance of the base-
line system (BLEU score 18.78 instead of 18.77),
the BLEU score of the 15-LM system increases to
20.08 when adding morphological reduction. In
the baseline system, the additional language mod-
els did not have a noticeable effect on translation
quality; however, their impact was realised in the
decompounding system.
4 Word reordering
Current SMT systems are based on the assump-
tion that the word order of the source and the tar-
get languages are fundamentally similar. While
the models permit some local reordering, system-
atic differences in word order involving move-
ments of more than a few words pose major prob-
lems. In particular, Statistical Machine Transla-
tion between German and English is notoriously
impacted by the different fundamental word order
in subordinate clauses, where German Subject?
Object?Verb (SOV) order contrasts with English
Subject?Verb?Object (SVO) order.
In our English-German system, we made the
observation that the verb in an SVO subordi-
nate clause following a punctuation mark fre-
quently gets moved before the preceding punctu-
ation. This movement is triggered by the Ger-
man language model, which prefers verbs pre-
ceding punctuation as consistent with SOV or-
der, and it is facilitated by the fact that the dis-
tance from the verb to the end of the preceding
clause is often smaller than the distance to the end
of the current phrase, so moving the verb back-
wards results in a better score from the distance-
based reordering model. This tendency can be
counteracted effectively by enabling the Moses
decoder?s monotone-at-punctuation feature,
which makes sure that words are not reordered
across punctuation marks. The result is a mod-
est gain from 14.28 to 14.38 BLEU points
(newstest2009).
In the German-English system, we applied a
chunk-based technique to produce lattices repre-
senting multiple permutations of the test sentences
in order to enable long-range reorderings of verb
phrases. This approach is similar to the reorder-
ing technique based on part-of-speech tags pre-
sented by Niehues and Kolss (2009), which re-
sults in the addition of a large number of reorder-
ing paths to the lattices. By contrast, we assume
that verb reorderings only occur between shallow
syntax chunks, and not within them. This makes it
possible to limit the number of long-range reorder-
ing options in an effective way.
We used the TreeTagger to perform shallow
syntax chunking of the German text. By man-
90
Figure 1: Word lattice for morphological reduction
Sonst [drohe]VC , dass auch [weitere L?nder]NC [vom Einbruch]PC [betroffen sein w?rden]VC .
Figure 2: Chunk reordering lattice
BLEU
test-09 test-10
Baseline 18.77 20.1
+ chunk-based reordering 18.94 20.3
Morphological reduction 19.31 20.6
+ chunk-based reordering 19.79 21.1
note: only News LM, case-sensitive evaluation
Table 5: Results with morphological reduction and
chunk reordering on newstest 2009/2010
ual inspection of a data sample, we then identi-
fied a few recurrent patterns of long reorderings
involving the verbs. In particular, we focused on
clause-final verbs in German SOV clauses, which
we move to the left in order to approximate the En-
glish SVO word order. For each sentence a chunk-
based lattice is created, which is then expanded
into a word lattice like the one shown in fig. 2. The
lattice representation provides the decoder with up
to three possible reorderings for a particular verb
chunk. It always retains the original word order as
an alternative input.
For technical reasons, we were unable to pre-
pare a system with reordering, morphological re-
duction and all language models in time for the
shared task. Our secondary submission with re-
ordering is therefore not comparable with our best
system, which includes more language models
and morphological reduction. In subsequent ex-
periments, we combined morphological reduction
with chunk-based reordering (table 5). When mor-
phological reduction is used, the reordering ap-
proach yields an improvement of about 0.5 BLEU
percentage points.
5 Conclusions
There are three important features specific to the
FBK systems at WMT 2010: mixtures of large
language models, German morphological reduc-
tion and decompounding and word reordering.
Our approach to using large language models
proved successful at the 2009 NIST MT evalua-
tion. In the present evaluation, its effectiveness
was reduced by a number of technical problems,
which were mostly due to the limitations of disk
access throughput in our parallel computing en-
vironment. We are working on methods to re-
duce and distribute disk accesses to large lan-
guage models, which will be implemented in the
IRSTLM language modelling toolkit (Federico et
al., 2008). By doing so, we hope to overcome the
current limitations and exploit the power of lan-
guage model mixtures more fully.
The Gertwol-based morphological reduction
and decompounding component we used is a
working solution that results in a significant im-
provement in translation quality. It is an alterna-
tive to the popular statistical compound splitting
methods, such as the one by Koehn and Knight
(2003), incorporating a greater amount of linguis-
tic knowledge and offering morphological reduc-
tion even of simplex words to their base form in
addition. It would be interesting to compare the
relative performance of the two approaches sys-
tematically.
Word reordering between German and English
is a complex problem. Encouraged by the success
of chunk-based verb reordering lattices on Arabic-
English (Bisazza and Federico, 2010), we tried to
adapt the same approach to the German-English
language pair. It turned out that there is a larger
variety of long reordering patterns in this case.
Nevertheless, some experiments performed after
91
the official evaluation showed promising results.
We plan to pursue this work in several directions:
Defining a lattice weighting scheme that distin-
guishes between original word order and reorder-
ing paths could help the decoder select the more
promising path through the lattice. Applying sim-
ilar reordering rules to the training corpus would
reduce the mismatch between the training data and
the reordered input sentences. Finally, it would be
useful to explore the impact of different distortion
limits on the decoding of reordering lattices in or-
der to find an optimal trade-off between decoder-
driven short-range and lattice-driven long-range
reordering.
Acknowledgements
This work was supported by the EuroMatrixPlus
project (IST-231720), which is funded by the Eu-
ropean Commission under the Seventh Framework
Programme for Research and Technological De-
velopment.
References
Arianna Bisazza andMarcello Federico. 2010. Chunk-
based verb reordering in VSO sentences for Arabic-
English statistical machine translation. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and Metrics MATR, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Inter-
speech 2008, pages 1618?1621. ISCA.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967?975, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
EACL, pages 187?193.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for statistical ma-
chine translation. In Annual meeting of the Associa-
tion for Computational Linguistics: Demonstration
session, pages 177?180, Prague.
Kimmo Koskenniemi and Mariikka Haapalainen.
1996. GERTWOL ? Lingsoft Oy. In Roland
Hausser, editor, Linguistische Verifikation. Doku-
mentation zur Ersten Morpholympics 1994, chap-
ter 11, pages 121?140. Niemeyer, T?bingen.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 169?176, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece,
March. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st annual meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo (Japan).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia.
ACL.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
92
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 372?378,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The Uppsala-FBK systems at WMT 2011
Christian Hardmeier
Jo?rg Tiedemann
Uppsala universitet
Inst. fo?r lingvistik och filologi
Uppsala, Sweden
first.last@lingfil.uu.se
Markus Saers
Human Language
Technology Center
Hong Kong Univ. of
Science & Technology
masaers@cs.ust.hk
Marcello Federico
Mathur Prashant
Fondazione Bruno Kessler
Human Language Technologies
Trento, Italy
lastname@fbk.eu
Abstract
This paper presents our submissions to the
shared translation task at WMT 2011. We
created two largely independent systems
for English-to-French and Haitian Creole-to-
English translation to evaluate different fea-
tures and components from our ongoing re-
search on these language pairs. Key features
of our systems include anaphora resolution,
hierarchical lexical reordering, data selection
for language modelling, linear transduction
grammars for word alignment and syntax-
based decoding with monolingual dependency
information.
1 English to French
Our submission to the English-French task was a
phrase-based Statistical Machine Translation based
on the Moses decoder (Koehn et al, 2007). Phrase
tables were separately trained on Europarl, news
commentary and UN data and then linearly inter-
polated with uniform weights. For language mod-
elling, we used 5-gram models trained with the
IRSTLM toolkit (Federico et al, 2008) on the mono-
lingual News corpus and parts of the English-French
109 corpus. More unusual features of our system
included a special component to handle pronomi-
nal anaphora and the hierarchical lexical reordering
model by Galley and Manning (2008). Selected fea-
tures of our system will be discussed in depth in the
following sections.
1.1 Handling pronominal anaphora
Pronominal anaphora is the use of pronominal ex-
pressions to refer to ?something previously men-
tioned in the discourse? (Strube, 2006). It is a very
common phenomenon found in almost all kinds of
texts. Anaphora can be local to a sentence, or it can
cross sentence boundaries. Standard SMT methods
do not handle this phenomenon in a satisfactory way
at present: For sentence-internal anaphora, they de-
pend on the n-gram language model with its lim-
ited history, while cross-sentence anaphora is left
to chance. We therefore added a word-dependency
model (Hardmeier and Federico, 2010) to our sys-
tem to handle anaphora explicitly.
Our processing of anaphoric pronouns follows
the procedure outlined by Hardmeier and Federico
(2010). We use the open-source coreference resolu-
tion system BART (Broscheit et al, 2010) to link
pronouns to their antecedents in the text. Coref-
erence links are handled differently depending on
whether or not they cross sentence boundaries. If
a coreference link points to a previous sentence, we
process the sentence containing the antecedent with
the SMT system and look up the translation of the
antecedent in the translated output. If the corefer-
ence link is sentence-internal, the translation lookup
is done dynamically by the decoder during search.
In either case, the word-dependency model adds a
feature function to the decoder score representing
the probability of a particular pronoun choice given
the translation of the antecedent.
In our English-French system, this model was
only applied to the inanimate pronouns it and they,
which seemed to be the most promising candidates
for improvement since their French equivalents re-
quire gender marking. It was trained on data au-
tomatically annotated for anaphora taken from the
news-commentary corpus, and the vocabulary of the
predicted pronouns was limited to words recognised
as pronouns by the POS tagger.
372
1.2 Hierarchical lexical reordering
The basic word order model of SMT penalises any
divergence between the order of the words in the in-
put sentence and the order of their translation equiv-
alents in the MT output. All reordering must thus be
driven by the language model when no other reorder-
ing model is present. Lexical reordering models
making certain word order choices in the MT out-
put conditional on the identity of the words involved
have been a standard component in SMT for some
years. The lexical reordering model usually em-
ployed in the Moses decoder was implemented by
Koehn et al (2005). Adopting the perspective of the
SMT decoder, which produces the target sentence
from left to right while covering source phrases in
free order, the model distinguishes between three or-
dering classes, monotone, swap and discontinuous,
depending on whether the source phrases giving rise
to the two last target phrases emitted were adjacent
in the same order, adjacent in swapped order or sep-
arated by other source words. Probabilities for each
ordering class given source and target phrase are
estimated from a word-aligned training corpus and
integrated into MT decoding as extra feature func-
tions.
In our submission, we used the hierarchical lexi-
cal reordering model proposed by Galley and Man-
ning (2008) and recently implemented in the Moses
decoder.1 This model uses the same approach of
classifying movements as monotone, swap or dis-
continuous, but unlike the phrase-based model, it
does not require the source language phrases to be
strictly adjacent in order to be counted as monotone
or swap. Instead, a phrase can be recognised as ad-
jacent to, or swapped with, a contiguous block of
source words that has been segmented into multi-
ple phrases. Contiguous phrase blocks are recog-
nised by the decoder with a shift-reduce parsing al-
gorithm. As a result, fewer jumps are labelled with
the uninformative discontinuous class.
1.3 Data selection from the WMT Giga corpus
One of the supplied language resources for this eval-
uation is the French-English WMT Giga corpus,
1The hierarchical lexical reordering model was imple-
mented in Moses during MT Marathon 2010 by Christian Hard-
meier, Gabriele Musillo, Nadi Tomeh, Ankit Srivastava, Sara
Stymne and Marcello Federico.
 60 80 100 120 140 160 180 200 220 240 260 280
 100  150  200  250  300  350  400 60 80 100 120 140 160 180 200 220 240 260 280LM Perplexity LM size (million 5-grams)Data Selection ThresholdThreshold vs PerplexityThreshold vs LM Size
Figure 1: Perplexity and size of language models trained
on data of the WMT Giga corpus that were selected using
different perplexity thresholds.
aka 109 corpus, a large collection of parallel sen-
tences crawled from Canadian and European Union
sources. While this corpus was too large to be used
for model training with the means at our disposal,
we exploited it as a source of parallel data for trans-
lation model training as well as monolingual French
data for the language model by filtering it down to a
manageable size. In order to extract sentences close
to the news translation task, we applied a simple
data selection procedure based on perplexity. Sen-
tence pairs were selected from the WMT Giga cor-
pus if the perplexity of their French part with respect
to a language model (LM) trained on French news
data was below a given threshold. The rationale is
that text sentences which are better predictable by
the LM should be closer to the news domain. The
threshold was set in a way to capture enough novel
n-grams, from one side, but also to avoid adding too
many irrelevant n-grams. It was tuned by training
a 5-gram LM on the selected data and checking its
size and its perplexity on a development set. In fig-
ure 1 we plot perplexity and size of the WMT Giga
LM for different values of the data-selection thresh-
old. Perplexities are computed on the newstest2009
set. As a good perplexity-size trade-off, the thresh-
old 250 was chosen to estimate an additional 5-gram
LM (WMT Giga 250) that was interpolated with
the original News LM. The resulting improvement
in perplexity is reported in table 1. For translation
model data, a perplexity threshold of 159 was ap-
plied.
373
LM Perplexity OOV rate
News 146.84 0.82
News + WMT Giga 250 130.23 0.71
Table 1: Perplexity reduction after interpolating the News
LM with data selected from the 109 corpus.
newstest
2009 2010 2011
Primary submission 0.246 0.286 0.284
w/o Anaphora handling 0.246 0.286 0.284
WMT Giga data
w/o LM 0.244 0.289 0.280
w/o TM 0.247 0.286 0.282
w/o LM and TM 0.247 0.289 0.278
Lexical reordering
phrase-based reo 0.239 0.281 0.275
no lexical reo 0.239 0.281 0.275
with LDC data 0.254 0.293 0.291
Table 2: Ablation test results (case-sensitive BLEU)
1.4 Results and Ablation tests
Owing to time constraints, we were not able to run
thorough tests on our system before submitting it to
the evaluation campaign. We therefore evaluated the
various components included in a post hoc fashion
by running ablation tests. In each test, we left out
one of the system components to identify its effect
on the overall performance. The results of these tests
are reported in table 2.
Performance-wise, the most important particular-
ity of our SMT system was the hierarchical lexical
reordering model, which led to a sizeable improve-
ment of 0.7, 0.5 and 0.9 BLEU points for the 2009,
2010 and 2011 test sets, respectively. We had previ-
ously seen negative results when trying to apply the
same model to English-German SMT, so its perfor-
mance seems to be strongly dependent on the lan-
guage pair it is used with.
Compared to the scores obtained using the full
system, the anaphora handling system did not have
any effect on the BLEU scores. This result is
similar to our result for English-German transla-
tion (Hardmeier and Federico, 2010). Unfortu-
nately, for English-French, the negative results ex-
tends to the pronoun translation scores (not reported
here), where slightly higher recall with the word-
dependency model was overcompensated by de-
graded precision, so the outcome of the experiments
clearly suggests that the anaphora handling proce-
dure is in need of improvement.
The effect of the WMT Giga language model dif-
fers among the test sets. For the 2009 and 2011
test sets, it results in an improvement of 0.2 and 0.4
BLEU points, respectively, while the 2010 test set
fares better without this additional language model.
However, it should be noted that there may be a
problem with the 2010 test set and the News lan-
guage model, which was used as a component in all
our systems. In particular, upgrading the News LM
data from last year?s to this year?s release led to an
improvement of 4 BLEU points on the 2010 test set
and an unrealistically low perplexity of 73 as com-
pared to 130 for the 2009 test set, which makes us
suspect that the latest News LM data may be tainted
with data from the 2010 test corpus. If this is the
case, the 2010 test set should be considered unreli-
able for LM evaluation. The benefit of adding WMT
Giga data to the translation model is less clear. For
the 2009 and 2010 test sets, this leads to a slight
degradation, but for the 2011 corpus, we obtained
a small improvement.
Our shared task submission did not use the French
Gigaword corpus from the Linguistic Data Consor-
tium (LDC2009T28), which is not freely available
to sites without LDC membership. After the sub-
mission, we ran a contrastive experiment including
a 5-gram model trained on this corpus, which led
to a sizeable improvement of 0.7?0.8 BLEU points
across all test sets.
2 Haitian Creole to English
Our experiments with the Haitian Creole-English
data are independent of the system presented for the
English to French task above. We experimented with
both phrase-based SMT and syntax-based SMT. The
main questions we investigated were i) whether we
can improve word alignment and phrase extraction
for phrase-based SMT and ii) whether we can in-
tegrate dependency parsing into a syntax-based ap-
proach. All our experiments were conducted on the
clean data set using Moses for training and decod-
ing. In the following we will first describe the exper-
iments with phrase-based models and linear trans-
374
duction grammars for word alignment and, there-
after, our findings from integrating English depen-
dency parses into a syntax-based approach.
2.1 Phrase-based SMT
The phrase-based system that we used in this series
of experiments uses a rather traditional setup. For
the translations into English we used the news data
provided for the other translations tasks in WMT
2011 to build a large scale-background language
model. The English data from the Haitian Creole
task were used as a separate domain-specific lan-
guage model. For the other translation direction we
only used the in-domain data provided. We used
standard 5-gram models with Witten-Bell discount-
ing and backoff interpolation for all language mod-
els. For the translation model we applied standard
techniques and settings for phrase extraction and
score estimations. However, we applied two differ-
ent systems for word alignment: One is the standard
GIZA++ toolbox implementing the IBM alignment
models (Och and Ney, 2003) and extensions and the
other is based on transduction grammars which will
briefly be introduced in the next section.
2.1.1 Alignment with PLITGs
By making the assumption that the parallel cor-
pus constitutes a linear transduction (Saers, 2011)2
we can induce a grammar that is the most likely to
have generated the observed corpus. The grammar
induced will generate a parse forest for each sen-
tence pair in the corpus, and each parse tree in that
forest will correspond to an alignment between the
two sentences. Following Saers et al (2010), the
alignment corresponding to the best parse can be ex-
tracted and used instead of other word alignment ap-
proaches such as GIZA++. There are several gram-
mar types that generate linear transductions, and in
this work, stochastic bracketing preterminalized lin-
ear inversion transduction grammars (PLITG) were
used (Saers and Wu, 2011). Since we were mainly
interested in the word alignments, we did not induce
phrasal grammars.
Although alignments from PLITGs may not reach
the same level of translation quality as GIZA++,
they make different mistakes, so both complement
2A transduction is a set of pairs of strings, and thus repre-
sents a relation between two languages.
each other. By duplicating the training corpus and
aligning each copy of the corpus with a different
alignment tool, the phrase extractor seems to be able
to pick the best of both worlds, producing a phrase
table that is superior to one produced with either of
the alignments tools used in isolation.
2.1.2 Results
In the following we present our results on the pro-
vided test set3 for translating into both languages
with phrase-based systems trained on different word
alignments. Table 3 summarises the BLEU scores
obtained.
English-Haitian BLEU phrase-table
GIZA++ 0.2567 3,060,486
PLITG 0.2407 5,007,254
GIZA++ & PLITG 0.2572 7,521,754
Haitian-English BLEU phrase-table
GIZA++ 0.3045 3,060,486
PLITG 0.2922 5,049,280
GIZA++ & PLITG 0.3105 7,561,043
Table 3: Phrase-based SMT (pbsmt) on the Haitian
Creole-English test set with different word alignments.
From the table we can see that phrase-based sys-
tems trained on PLITG alignments performs slightly
worse than the ones trained on GIZA++. However
combining both alignments with the simple data du-
plication technique mentioned earlier produces the
overall best scores in both translation directions.
The fact that both alignments lead to complemen-
tary information can be seen in the size of the phrase
tables extracted (see table 3).
2.2 Syntax-based SMT
We used Moses and its syntax-mode for our exper-
iments with hierarchical phrase-based and syntax-
augmented models. Our main interest was to in-
vestigate the influence of monolingual parsing on
the translation performance. In particular, we tried
to integrate English dependency parses created by
MaltParser (Nivre et al, 2007) trained on the Wall
Street Journal section of the Penn Treebank (Mar-
cus et al, 1993) extended with about 4000 questions
3We actually swapped the development set and the test set
by mistake. But, of course, we never mixed development and
test data in any result reported.
375
from the Question Bank (Judge et al, 2006). The
conversion to dependency trees was done using the
Stanford Parser (de Marneffe et al, 2006). Again,
we ran both translation directions to test our settings
in more than just one task. Interesting here is also
the question whether there are significant differences
when integrating monolingual parses on the source
or on the target side.
The motivation for applying dependency parsing
in our experiments is to use the specific information
carried by dependency relations. Dependency struc-
tures encode functional relations between words that
can be seen as an interface to the semantics of a
sentence. This information is usually not avail-
able in phrase-structure representations. We believe
that this type of information can be beneficial for
machine translation. For example, knowing that a
noun acts as the subject of a sentence is more in-
formative than just marking it as part of a noun
phrase. Whether or not this information can be ex-
plored by current syntax-based machine translation
approaches that are optimised for phrase-structure
representations is a question that we liked to inves-
tigate. For comparison we also trained hierarchical
phrase-based models without any additional annota-
tion.
2.2.1 Converting projective dependency trees
First we needed to convert dependency parses to
a tree representation in order to use our data in
the standard models of syntax-based models imple-
mented in Moses. In our experiments, we used
a parser model that creates projective dependency
graphs that can be converted into tree structures of
nested segments. We used the yield of each word
(referring to that word and its transitive dependents)
to define spans of phrases and their dependency rela-
tions are used as span labels. Furthermore, we also
defined pre-terminal nodes that encode the part-of-
speech information of each word. These tags were
obtained using the HunPos tagger (Hala?csy et al,
2007) trained on the Wall Street Journal section of
the Penn Treebank. Figure 2 illustrates the conver-
sion process. Tagging and parsing is done for all En-
glish data without any manual corrections or optimi-
sation of parameters. After the conversion, we were
able to use the standard training procedures imple-
mented in Moses.
-ROOT- andCC howWRB oldJJ isVBZ yourPRP$ nephewNN ?.
advmoddep possnsubjcc
punctnull
<tree label="null">
<tree label="cc">
<tree label="CC">and</tree>
</tree>
<tree label="dep">
<tree label="advmod">
<tree label="WRB">how</tree>
</tree>
<tree label="JJ">old</tree>
</tree>
<tree label="VBZ">is</tree>
<tree label="nsubj">
<tree label="poss">
<tree label="PRP$">your</tree>
</tree>
<tree label="NN">nephew</tree>
</tree>
<tree label="punct">
<tree label=".">?</tree>
</tree>
</tree>
Figure 2: A dependency graph from the training corpus
and its conversion to a nested tree structure. The yield of
each word in the sentence defines a span with the label
taken from the relation of that word to its head. Part-of-
speech tags are used as additional pre-terminal nodes.
2.2.2 Experimental Results
We ran several experiments with slightly differ-
ent settings. We used the same basic setup for
all of them including the same language models
and GIZA++ word alignments that we have used
for the phrase-based models already. Further, we
used Moses for extracting rules of the syntax-based
translation model. We use standard settings for
the baseline system (=hiero) that does not employ
any linguistic markup. For the models that include
dependency-based trees we changed the maximum
span threshold to a high value of 999 (default: 15)
in order to extract as many rules as possible. This
large degree of freedom is possible due to the oth-
erwise strong constraints on rule flexibility imposed
by the monolingual syntactic markup. Rule tables
are dramatically smaller than for the unrestricted hi-
erarchical models (see table 4).
However, rule restriction by linguistic constraints
usually hurts performance due to the decreased cov-
erage of the rule set. One common way of improving
376
reference Are you going to let us die on Ile a` Vaches which is located close the city of Les Cayes. I am ...
pbsmt Do you are letting us die in Ilavach island?s on in Les Cayes. I am ...
hiero do you will let us die in the island Ilavach on the in Les Cayes . I am ...
samt2 Are you going to let us die in the island Ilavach the which is on the Les. My name is ...
reference I?m begging you please help me my situation is very critical.
pbsmt Please help me please. Because my critical situation very much.
hiero please , please help me because my critical situation very much .
samt2 Please help me because my situation very critical.
reference I don?t have money to go and give blood in Port au Prince from La Gonave.
pbsmt I don?t have money, so that I go to give blood Port-au-Prince since lagonave.
hiero I don ?t have any money , for me to go to give blood Port-au-Prince since lagonave .
samt2 I don?t have any money, to be able to go to give blood Port-au-Prince since Gona?ve Island.
Figure 3: Example translations for various models.
English-Haitian BLEU number of rules
hiero 0.2549 34,118,622
malt (source) 0.2180 1,628,496
- binarised 0.2327 9,063,933
- samt1 0.2311 11,691,279
- samt2 0.2366 29,783,694
Haitian-English BLEU number of rules
hiero 0.3034 33,231,535
malt (target) 0.2739 1,922,688
- binarised 0.2857 8,922,343
- samt1 0.2952 11,073,764
- samt2 0.2954 24,554,317
Table 4: Syntax-based SMT on the Haitian Creole-
English test set with (=malt) or without (=hiero) English
parse trees and various parse relaxation strategies. The
final system submitted to WMT11 is malt(target)-samt2.
rule extraction is based on tree manipulation and re-
laxed extraction algorithms. Moses implements sev-
eral algorithms that have been proposed in the lit-
erature. Tree binarisation is one of them. This can
be done in a left-branching and in a right-branching
mode. We used a combination of both in the set-
tings denoted as binarised. The other relaxation al-
gorithms are based on methods proposed for syntax-
augmented machine translation (Zollmann et al,
2008). We used two of them: samt1 combines pairs
of neighbouring children nodes into combined com-
plex nodes and creates additional complex nodes of
all children nodes except the first child and similar
complex nodes for all but the last child. samt2 com-
bines any pair of neighbouring nodes even if they are
not children of the same parent. All of these relax-
ation algorithms lead to increased rule sets (table 4).
In terms of translation performance there seems to
be a strong correlation between rule table size and
translation quality as measured by BLEU. None of
the dependency-based models beats the unrestricted
hierarchical model. Both translation directions be-
have similar with slightly worse performances of
the dependency-based models (relative to the base-
line) when syntax is used on the source language
side. Note also that all syntax-based models (includ-
ing hiero) are below the corresponding phrase-based
SMT systems. Of course, automatic evaluation has
its limits and interesting qualitative differences may
be more visible in manual assessments. The use of
linguistic information certainly has an impact on the
translation hypotheses produced as we can see in the
examples in figure 3. In the future, we plan to inves-
tigate the effect of dependency information on gram-
maticality of translated sentences in more detail.
3 Conclusions
In our English-French and Haitian Creole-English
shared task submissions, we investigated the use
of anaphora resolution, hierarchical lexical reorder-
ing and data selection for language modelling
(English-French) as well as LTG word alignment
and syntax-based decoding with dependency infor-
mation (Haitian Creole-English). While the re-
sults for the systems with anaphora handling were
somewhat disappointing and the effect of data fil-
tering was inconsistent, hierarchical lexical reorder-
ing brought substantial improvements. We also ob-
tained consistent gains by combining information
from different word aligners, and we presented a
simple way of including dependency parses in stan-
dard tree-based decoding.
377
Acknowledgements
Most of the features used in our English-French sys-
tem were originally developed while Christian Hard-
meier was at FBK. Activities at FBK were supported
by the EuroMatrixPlus project (IST-231720) and the
T4ME network of excellence (IST-249119), both
funded by the DG INFSO of the European Commis-
sion through the Seventh Framework Programme.
References
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodriguez, Lorenza Romano,
Olga Uryupina, Yannick Versley, and Roberto Zanoli.
2010. BART: A multilingual anaphora resolution sys-
tem. In Proceedings of the 5th International Workshop
on Semantic Evaluations (SemEval-2010), Uppsala.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an open source toolkit for handling
large scale language models. In Interspeech 2008,
pages 1618?1621. ISCA.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 847?855, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz. 2007.
Hunpos: an open source trigram tagger. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 209?
212.
Christian Hardmeier and Marcello Federico. 2010. Mod-
elling Pronominal Anaphora in Statistical Machine
Translation. In Marcello Federico, Ian Lane, Michael
Paul, and Franc?ois Yvon, editors, Proceedings of the
seventh International Workshop on Spoken Language
Translation (IWSLT), pages 283?289.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: creating a corpus of parse-annotated
questions. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 497?504.
Philipp Koehn, Amittai Axelrod, Alexandra
Birch Mayne, et al 2005. Edinburgh system
description for the 2005 iwslt speech translation
evaluation. In International workshop on spoken
language translation, Pittsburgh.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for Statistical Ma-
chine Translation. In Annual meeting of the Associ-
ation for Computational Linguistics: Demonstration
session, pages 177?180, Prague.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: The Penn Treebank. Computational
Linguistics, 19:313?330, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational linguistics, 29:19?51.
Markus Saers and Dekai Wu. 2011. Principled induction
of phrasal bilexica. In Proceedings of the 15th Annual
Conference of the European Association for Machine
Translation, Leuven, Belgium, May.
Markus Saers, Joakim Nivre, and Dekai Wu. 2010. Word
alignment with stochastic bracketing linear inversion
transduction grammar. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 341?344, Los Angeles, California,
June.
Markus Saers. 2011. Translation as Linear Transduc-
tion: Models and Algorithms for Efficient Learning in
Statistical Machine Translation. Ph.D. thesis, Uppsala
University, Department of Linguistics and Philology.
M. Strube. 2006. Anaphora and coreference resolution,
Statistical. In Encyclopedia of language and linguis-
tics, pages 216?222. Elsevier.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
mt. In Proceedings of the 22nd International Confer-
ence on Computational Linguistics - Volume 1, pages
1145?1152.
378
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 109?113,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Tree Kernels for Machine Translation Quality Estimation
Christian Hardmeier and Joakim Nivre and Jo?rg Tiedemann
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
This paper describes Uppsala University?s
submissions to the Quality Estimation (QE)
shared task at WMT 2012. We present a QE
system based on Support Vector Machine re-
gression, using a number of explicitly defined
features extracted from the Machine Transla-
tion input, output and models in combination
with tree kernels over constituency and de-
pendency parse trees for the input and output
sentences. We confirm earlier results suggest-
ing that tree kernels can be a useful tool for
QE system construction especially in the early
stages of system design.
1 Introduction
The goal of the WMT 2012 Quality Estimation
(QE) shared task (Callison-Burch et al, 2012) was
to create automatic systems to judge the quality
of the translations produced by a Statistical Ma-
chine Translation (SMT) system given the input
text, the proposed translations and information about
the models used by the SMT system. The shared
task organisers provided a training set of 1832 sen-
tences drawn from earlier WMT Machine Transla-
tion test sets, translated from English to Spanish
with a phrase-based SMT system, along with the
models used and diagnostic output produced by the
SMT system as well as manual translation quality
annotations on a 1?5 scale for each sentence. Ad-
ditionally, a set of 17 baseline features was made
available to the participants. Systems were evalu-
ated on a test set of 422 sentences annotated in the
same way.
Uppsala University submitted two systems to this
shared task. Our systems were fairly successful and
achieved results that were outperformed by only one
competing group. They improve over the baseline
performance in two ways, building on and extend-
ing earlier work by Hardmeier (2011), on which
the system description in the following sections is
partly based: On the one hand, we enhance the set
of 17 baseline features provided by the organisers
with another 82 explicitly defined features. On the
other hand, we use syntactic tree kernels to extract
implicit features from constituency and dependency
parse trees over the input sentences and the Machine
Translation (MT) output. The experimental results
confirm the findings of our earlier work, showing
tree kernels to be a valuable tool for rapid prototyp-
ing of QE systems.
2 Features
Our QE systems used two types of features: On
the one hand, we used a set of explicit features that
were extracted from the data before running the Ma-
chine Learning (ML) component. On the other hand,
syntactic parse trees of the MT input and output
sentences provided implicit features that were com-
puted directly by the ML component using tree ker-
nels.
2.1 Explicit features
Both of the QE systems we submitted to the shared
task used the complete set of 17 baseline features
provided by the workshop organisers. Additionally,
the UU best system also contained all the features
presented by Hardmeier (2011) with the exception
109
of a few features specific to the film subtitle genre
and inapplicable to the text type of the shared task,
as well as a small number of features not included
in that work. Many of these features were modelled
on QE features described by Specia et al (2009). In
particular, the following features were included in
addition to the baseline feature set:
? number of words, length ratio (4 features)
? source and target type-token ratios (2 features)
? number of tokens matching particular patterns
(3 features each):
? numbers
? opening and closing parentheses
? strong punctuation signs
? weak punctuation signs
? ellipsis signs
? hyphens
? single and double quotes
? apostrophe-s tokens
? short alphabetic tokens (? 3 letters)
? long alphabetic tokens (? 4 letters)
? source and target language model (LM) and
log-LM scores (4 features)
? LM and log-LM scores normalised by sentence
length (4 features)
? number and percentage of out-of-vocabulary
words (2 features)
? percentage of source 1-, 2-, 3- and 4-grams oc-
curring in the source part of the training corpus
(4 features)
? percentage of source 1-, 2-, 3- and 4-grams in
each frequency quartile of the training corpus
(16 features)
? a binary feature indicating that the output con-
tains more than three times as many alphabetic
tokens as the input (1 feature)
? percentage of unaligned words and words with
1 : 1, 1 : n, n : 1 and m : n alignments (10 fea-
tures)
? average number of translations per word, un-
weighted and weighted by word frequency and
reciprocal word frequency (3 features)
? translation model entropy for the input words,
cumulatively per sentence and averaged per
word, computed based on the SMT lexical
weight model (2 features).
Whenever applicable, features were computed for
both the source and the target language, and addi-
tional features were added to represent the squared
difference of the source and target language feature
values. All feature values were scaled so that their
values ranged between 0 and 1 over the training set.
The total number of features of the UU best sys-
tem amounted to 99. It should be noted, however,
that there is considerable redundancy in the feature
set and that the 82 features of Hardmeier (2011)
overlap with the 17 baseline features to some extent.
We did not make any attempt to reduce feature over-
lap and relied on the learning algorithm for feature
selection.
2.2 Parse trees
Both the English input text and the Spanish Machine
Translations were annotated with syntactic parse
trees from which to derive implicit features. In En-
glish, we were able to produce both constituency and
dependency parses. In Spanish, we were limited to
dependency parses because of the better availability
of parsing models. English constituency parses were
produced with the Stanford parser (Klein and Man-
ning, 2003) using the model bundled with the parser.
For dependency parsing, we used MaltParser (Nivre
et al, 2006). POS tagging was done with HunPOS
(Hala?csy et al, 2007) for English and SVMTool
(Gime?nez and Ma?rquez, 2004) for Spanish, with the
models provided by the OPUS project (Tiedemann,
2009). As in previous work (Hardmeier, 2011), we
treated the parser as a black box and made no at-
tempt to handle the fact that parsing accuracy may
be decreased over malformed SMT output.
To be used with tree kernels, the output of the de-
pendency parser had to be transformed into a sin-
gle tree structure with a unique label per node and
unlabelled edges, similar to a constituency parse
tree. We followed Johansson and Moschitti (2010)
in using a tree representation which encodes part-
of-speech tags, dependency relations and words as
sequences of child nodes (see fig. 1).
110
Figure 1: Representation of the dependency tree fragment
for the words Nicole ?s dad
A tree and some of its Subset Tree Fragments
S 
N 
NP 
D N 
VP 
V Mary 
brought 
a    cat 
NP 
D N 
a    cat 
N 
   cat 
D 
a 
V 
brought 
N 
Mary 
NP 
D N 
VP 
V 
brought 
a    cat 
Fig. 1. A syntactic parse tree with its sub-
trees (STs).
NP 
D N 
a   cat 
NP 
D N 
NP 
D N 
a 
NP 
D N 
NP 
D N 
VP 
V 
brought 
a    cat 
  cat 
NP 
D N 
VP 
V 
a    cat 
NP 
D N 
VP 
V 
N 
   cat 
D 
a 
V 
brought 
N 
Mary 
? 
Fig. 2. A tree with some of its subset trees
(SSTs).
NP 
D N 
VP 
V 
brought 
a    cat 
NP 
D N 
VP 
V 
a    cat 
NP 
D N 
VP 
a    cat 
NP 
D N 
VP 
a 
NP 
D 
VP 
a 
NP 
D 
VP 
NP 
N 
VP 
NP 
N 
NP NP 
D N D 
NP 
? 
VP 
Fig. 3. A tree with some of its partial trees
(PTs).
 
 
 
 
 
is 
What offer 
an plan 
direct      stock   purchase 
Fig. 4. A dependency tree of a question.
constraint over the SSTs, we obtain a more general form of substructures that we
call partial trees (PTs). These can be generated by the application of partial
production rules of the grammar, consequently [VP [V]] and [VP [NP]] are
valid PTs. Figure 3 shows that the number of PTs derived from the same tree as
before is still higher (i.e. 30 PTs). These different substructure numbers provide
an intuitive quantification of the different information levels among the tree-
based representations.
3 Fast Tree Kernel Functions
The main idea of tree kernels is to compute the number of common substructures
between two trees T1 and T2 without explicitly considering the whole fragment
space. We have designed a general function to compute the ST, SST and PT
kernels. Our fast evaluation of the PT kernel is inspired by the efficient evaluation
of non-continuous subsequences (described in [13]). To increase the computation
speed of the above tree kernels, we also apply the pre-selection of node pairs
which have non-null kernel.
3.1 The Partial Tree Kernel
The evaluation of the common PTs rooted in nodes n1 and n2 requires the
selection of the shared child subsets of the two nodes, e.g. [S [DT JJ N]] and
[S [DT N N]] have [S [N]] (2 times) and [S [DT N]] in common. As the order
of the children is important, we can use subsequence kernels for their generation.
More in detail, let F = {f1, f2, .., f|F|} be a tree fragment space of type PTs and
let the indicator function Ii(n) be equal to 1 if the target fi is rooted at node n
and 0 otherwise, we define the PT kernel as:
A tree and some of its Partial Tree Fragments
Figure 2: Tree fragments extracted by the Subset Tree
Kernel and by the Partial Tree Kernel. Illustrations by
Moschitti (2006a).
3 M chine Learning compon nt
3.1 Overview
The QE shared task asked both for an estimate of
a 1?5 quality score for each segment in the test set
and for a ranking of t e sentences according to qual-
ity. We decided to treat score estimation as primary
and address the task as a regression problem. For
the ranking task, we simply submitted the ranking
induced by the regression output, breaking ties ran-
domly.
Our system was based on SVM regression as
implemented by the SVMlight software (Joachims,
1999) with tree kernel extensions (Moschitti,
2006b). Predicted scores less than 1 were set to 1
and predicted scores greater than 5 were set to 5
as this was known to be the range of valid scores.
Our learning algorithm had some free hyperparam-
eters. Three of them were optimised by joint grid
search with 5-fold cross-validation over the training
set: the SVM training error/margin trade-off (C pa-
rameter), one free parameter of the explicit feature
kernel and the ratio between explicit feature and tree
kernels (see below). All other parameters were left
at their default values. Before running it over the
test set, the system was retrained on the complete
training set using the parameters found with cross-
validation.
3.2 Kernels for explicit features
To select a good kernel for our explicit features,
we initially followed the advice given by Hsu et al
(2010), using a Gaussian RBF kernel and optimis-
ing the SVM C parameter and the ? parameter of the
RBF with grid search. While this gave reasonable
results, it turned out that slightly better prediction
could be achieved by using a polynomial kernel, so
we chose to use this kernel for our final submission
and used grid search to tune th degree of the poly-
nomial instead. The improvement over the Gaussian
kernel was, however, marginal.
3.3 Tree kernels
To exploit parse tree information in our Machine
Learning (ML) component, we used tree kernel
functions. Tree kernels (Collins and Duffy, 2001)
are kernel functions defined over pairs of tree struc-
tures. They measure the similarity between two trees
by counting the number of common substructures.
Implicitly, they define an infinite-dimensional fea-
ture space whose dimensions correspond to all pos-
sible tree fragments. Features are thus available to
cover different kinds of abstract node configurations
that can occ r in a tree. The important feature i-
mensions are effectively selected by the SVM train-
ing algorithm through the selection and weighting
of the support vectors. The intuition behind our
use of tree kernels is that they may help us iden-
tify constructions that are difficult to translate in the
source language, and doubtful syntactic structures in
the output language. Note that we do not currently
compare parse trees across languages; tree kernels
111
Cross-validation Test set
Features T C d ? ? MAE RMS ? ? MAE RMS
UU best 99 explicit + TK 0.05 4 2 0.506 0.566 0.550 0.692 0.56 0.62 0.64 0.79
(a) 99 explicit + TK 0.03 8 3 0.502 0.564 0.552 0.700 0.56 0.61 0.63 0.78
(b) 17 explicit + TK 0.05 4 2 0.462 0.530 0.568 0.714 0.57 0.61 0.65 0.79
UU bltk 17 explicit + TK 0.03 8 3 0.466 0.534 0.566 0.712 0.58 0.61 0.64 0.79
(c) 99 explicit 0 8 2 0.492 0.560 0.554 0.700 0.56 0.59 0.65 0.80
(d) 17 explicit 0 8 2 0.422 0.466 0.598 0.748 0.52 0.55 0.70 0.83
(e) TK only ? 4 ? 0.364 0.392 0.632 0.782 0.51 0.51 0.70 0.85
T : Tree kernel weight C: Training error/margin trade-off d: Degree of polynomial kernel
?: DeltaAvg score ?: Spearman rank correlation MAE: Mean Average Error
RMS: Root Mean Square Error TK: Tree kernels
Table 1: Experimental results
are applied to trees of the same type in the same lan-
guage only.
We used two different types of tree kernels for the
different types of parse trees (see fig. 2). The Sub-
set Tree Kernel (Collins and Duffy, 2001) consid-
ers tree fragments consisting of more than one node
with the restriction that if one child of a node is in-
cluded, then all its siblings must be included as well
so that the underlying production rule is completely
represented. This kind of kernel is well suited for
constituency parse trees and was used for the source
language constituency parses. For the dependency
trees, we used the Partial Tree Kernel (Moschitti,
2006a) instead. It extends the Subset Tree Kernel by
permitting also the extraction of tree fragments com-
prising only part of the children of any given node.
Lifting this restriction makes sense for dependency
trees since a node and its children do not correspond
to a grammatical production in a dependency tree in
the same way as they do in a constituency tree (Mos-
chitti, 2006a). It was used for the dependency trees
in the source and in the target language.
The explicit feature kernel and the three tree ker-
nels were combined additively, with a single weight
parameter to balance the sum of the tree kernels
against the explicit feature kernel. This coefficient
was optimised together with the other two hyperpa-
rameters mentioned above. It turned out that best re-
sults could be obtained with a fairly low weight for
the tree kernels, but in the cross-validation experi-
ments adding tree kernels did give an improvement
over not having them at all.
4 Experimental Results
Results for some of our experiments are shown in
table 1. The two systems we submitted to the shared
task are marked with their system identifiers. A few
other systems are included for comparison and are
numbered (a) to (e) for easier reference.
Our system using only the baseline features (d)
performs a bit worse than the reference system of
the shared task organisers. We use the same learn-
ing algorithm, so this seems to indicate that the ker-
nel and the hyperparameters they selected worked
slightly better than our choices. Using only tree
kernels with no explicit features at all (e) creates a
system that works considerably worse under cross-
validation, however we note that its performance on
the test set is very close to that of system (d).
Adding the 82 additional features of Hardmeier
(2011) to the system without tree kernels slightly im-
proves the performance both under cross-validation
and on the test set (c). Adding tree kernels has a
similar effect, which is a bit less pronounced for
the cross-validation setting, but quite comparable on
the test set (UU bltk, b). Finally, combining the
full feature set with tree kernels results in an addi-
tional gain under cross-validation, but unfortunately
the improvement does not carry over to the test set
(UU best, a).
5 Conclusions
In sum, the results confirm the findings made in our
earlier work (Hardmeier, 2011). They show that tree
kernels can be a valuable tool to boost the initial
112
performance of a Quality Estimation system without
spending much effort on feature engineering. Unfor-
tunately, it seems that the gains achieved by tree ker-
nels over simple parse trees and by the additional ex-
plicit features used in our systems do not necessarily
add up. Nevertheless, comparison with other partici-
pating systems shows that either of them is sufficient
for state-of-the-art performance.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Machine
Translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of NIPS
2001, pages 625?632.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vec-
tor Machines. In Proceedings of the 4th Conference
on International Language Resources and Evaluation
(LREC-2004), Lisbon.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics. Companion Volume: Pro-
ceedings of the Demo and Poster Sessions, pages 209?
212, Prague, Czech Republic, June. Association for
Computational Linguistics.
Christian Hardmeier. 2011. Improving machine transla-
tion quality prediction with syntactic tree kernels. In
Mikel L. Forcada, Heidi Depraetere, and Vincent Van-
deghinste, editors, Proceedings of the 15th conference
of the European Association for Machine Translation
(EAMT 2011), pages 233?240, Leuven, Belgium.
Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.
2010. A practical guide to support vector classifica-
tion. Technical report, Department of Computer Sci-
ence, National Taiwan University.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods ? Sup-
port Vector Learning. MIT Press.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67?76, Uppsala, Sweden, July. Association
for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
Alessandro Moschitti. 2006a. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of the 17th European Conference on Ma-
chine Learning, Berlin.
Alessandro Moschitti. 2006b. Making tree kernels prac-
tical for natural language learning. In Proceedings of
the Eleventh International Conference of the European
Association for Computational Linguistics, Trento.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
MaltParser: A language-independent system for data-
driven dependency parsing. In Proceedings of the 5th
Conference on International Language Resources and
Evaluation (LREC-2006), pages 2216?2219, Genoa.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving the
confidence of Machine Translation quality estimates.
In Proceedings of MT Summit XII, Ottawa.
Jo?rg Tiedemann. 2009. News from OPUS ? a collection
of multilingual parallel corpora with tools and inter-
face. In N. Nicolov, K. Bontcheva, G. Angelova, and
R. Mitkov, editors, Recent Advances in Natural Lan-
guage Processing, pages 237?248. John Benjamins,
Amsterdam.
113
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 225?231,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Tunable Distortion Limits and Corpus Cleaning for SMT
Sara Stymne Christian Hardmeier Jo?rg Tiedemann Joakim Nivre
Uppsala University
Department of Linguistics and Philology
firstname.lastname@lingfil.uu.se
Abstract
We describe the Uppsala University sys-
tem for WMT13, for English-to-German
translation. We use the Docent decoder,
a local search decoder that translates at
the document level. We add tunable dis-
tortion limits, that is, soft constraints on
the maximum distortion allowed, to Do-
cent. We also investigate cleaning of the
noisy Common Crawl corpus. We show
that we can use alignment-based filtering
for cleaning with good results. Finally we
investigate effects of corpus selection for
recasing.
1 Introduction
In this paper we present the Uppsala University
submission to WMT 2013. We have submitted one
system, for translation from English to German.
In our submission we use the document-level de-
coder Docent (Hardmeier et al, 2012; Hardmeier
et al, 2013). In the current setup, we take advan-
tage of Docent in that we introduce tunable dis-
tortion limits, that is, modeling distortion limits as
soft constraints instead of as hard constraints. In
addition we perform experiments on corpus clean-
ing. We investigate how the noisy Common Crawl
corpus can be cleaned, and suggest an alignment-
based cleaning method, which works well. We
also investigate corpus selection for recasing.
In Section 2 we introduce our decoder, Docent,
followed by a general system description in Sec-
tion 3. In Section 4 we describe our experiments
with corpus cleaning, and in Section 5 we describe
experiments with tunable distortion limits. In Sec-
tion 6 we investigate corpus selection for recasing.
In Section 7 we compare our results with Docent
to results using Moses (Koehn et al, 2007). We
conclude in Section 8.
2 The Docent Decoder
Docent (Hardmeier et al, 2013) is a decoder for
phrase-based SMT (Koehn et al, 2003). It differs
from other publicly available decoders by its use
of a different search algorithm that imposes fewer
restrictions on the feature models that can be im-
plemented.
The most popular decoding algorithm for
phrase-based SMT is the one described by Koehn
et al (2003), which has become known as stack
decoding. It constructs output sentences bit by
bit by appending phrase translations to an initially
empty hypothesis. Complexity is kept in check,
on the one hand, by a beam search approach that
only expands the most promising hypotheses. On
the other hand, a dynamic programming technique
called hypothesis recombination exploits the lo-
cality of the standard feature models, in particu-
lar the n-gram language model, to achieve a loss-
free reduction of the search space. While this de-
coding approach delivers excellent search perfor-
mance at a very reasonable speed, it limits the
information available to the feature models to an
n-gram window similar to a language model his-
tory. In stack decoding, it is difficult to implement
models with sentence-internal long-range depen-
dencies and cross-sentence dependencies, where
the model score of a given sentence depends on
the translations generated for another sentence.
In contrast to this very popular stack decod-
ing approach, our decoder Docent implements a
search procedure based on local search (Hard-
meier et al, 2012). At any stage of the search pro-
cess, its search state consists of a complete docu-
ment translation, making it easy for feature mod-
els to access the complete document with its cur-
rent translation at any point in time. The search
algorithm is a stochastic variant of standard hill
climbing. At each step, it generates a successor
of the current search state by randomly applying
225
one of a set of state changing operations to a ran-
dom location in the document. If the new state
has a better score than the previous one, it is ac-
cepted, else search continues from the previous
state. The operations are designed in such a way
that every state in the search space can be reached
from every other state through a sequence of state
operations. In the standard setup we use three op-
erations: change-phrase-translation replaces the
translation of a single phrase with another option
from the phrase table, resegment alters the phrase
segmentation of a sequence of phrases, and swap-
phrases alters the output word order by exchang-
ing two phrases.
In contrast to stack decoding, the search algo-
rithm in Docent leaves model developers much
greater freedom in the design of their feature func-
tions because it gives them access to the transla-
tion of the complete document. On the downside,
there is an increased risk of search errors because
the document-level hill-climbing decoder cannot
make as strong assumptions about the problem
structure as the stack decoder does. In prac-
tice, this drawback can be mitigated by initializing
the hill-climber with the output of a stack decod-
ing pass using the baseline set of models without
document-level features (Hardmeier et al, 2012).
Since its inception, Docent has been used to ex-
periment with document-level semantic language
models (Hardmeier et al, 2012) and models to
enhance text readability (Stymne et al, 2013b).
Work on other discourse phenomena is ongoing.
In the present paper, we focus on sentence-internal
reordering by exploiting the fact that Docent im-
plements distortion limits as soft constraints rather
than strictly enforced limitations. We do not in-
clude any of our document-level feature functions.
3 System Setup
In this section we will describe our basic system
setup. We used all corpora made available for
English?German by the WMT13 workshop. We
always concatenated the two bilingual corpora Eu-
roparl and News Commentary, which we will call
EP-NC. We pre-processed all corpora by using
the tools provided for tokenization and we also
lower-cased all corpora. For the bilingual corpora
we also filtered sentence pairs with a length ra-
tio larger than three, or where either sentence was
longer than 60 tokens. Recasing was performed as
a post-processing step, trained using the resources
in the Moses toolkit (Koehn et al, 2007).
For the language model we trained two sepa-
rate models, one on the German side of EP-NC,
and one on the monolingual News corpus. In
both cases we trained 5-gram models. For the
large News corpus we used entropy-based prun-
ing, with 10?8 as a threshold (Stolcke, 1998). The
language models were trained using the SRILM
toolkit (Stolcke, 2002) and during decoding we
used the KenLM toolkit (Heafield, 2011).
For the translation model we also trained two
models, one with EP-NC, and one with Common
Crawl. These two models were interpolated and
used as a single model at decoding time, based on
perplexity minimization interpolation (Sennrich,
2012), see details in Section 4. The transla-
tion models were trained using the Moses toolkit
(Koehn et al, 2007), with standard settings with
5 features, phrase probabilities and lexical weight-
ing in both directions and a phrase penalty. We ap-
plied significance-based filtering (Johnson et al,
2007) to the resulting phrase tables. For decod-
ing we used the Docent decoder with random ini-
tialization and standard parameter settings (Hard-
meier et al, 2012; Hardmeier et al, 2013), which
beside translation and language model features in-
clude a word penalty and a distortion penalty.
Parameter optimization was performed using
MERT (Och, 2003) at the document-level (Stymne
et al, 2013a). In this setup we calculate both
model and metric scores on the document-level
instead of on the sentence-level. We produce k-
best lists by sampling from the decoder. In each
optimization run we run 40,000 hill-climbing it-
erations of the decoder, and sample translations
with interval 100, from iteration 10,000. This
procedure has been shown to give competitive re-
sults to standard tuning with Moses (Koehn et
al., 2007) with relatively stable results (Stymne
et al, 2013a). For tuning data we concate-
nated the tuning sets news-test 2008?2010 and
newssyscomb2009, to get a higher number of doc-
uments. In this set there are 319 documents and
7434 sentences.
To evaluate our system we use newstest2012,
which has 99 documents and 3003 sentences. In
this article we give lower-case Bleu scores (Pap-
ineni et al, 2002), except in Section 6 where we
investigate the effect of different recasing models.
226
Cleaning Sentences Reduction
None 2,399,123
Basic 2,271,912 5.3%
Langid 2,072,294 8.8%
Alignment-based 1,512,401 27.0%
Table 1: Size of Common Crawl after the different
cleaning steps and reduction in size compared to
the previous step
4 Cleaning of Common Crawl
The Common Crawl (CC) corpus was collected
from web sources, and was made available for the
WMT13 workshop. It is noisy, with many sen-
tences with the wrong language and also many
non-corresponding sentence pairs. To make better
use of this resource we investigated two methods
for cleaning it, by making use of language identi-
fication and alignment-based filtering. Before any
other cleaning we performed basic filtering where
we only kept pairs where both sentences had at
most 60 words, and with a length ratio of maxi-
mum 3. This led to a 5.3% reduction of sentences,
as shown in Table 1.
Language Identification For language identifi-
cation we used the off-the-shelf tool langid.py (Lui
and Baldwin, 2012). It is a python library, cover-
ing 97 languages, including English and German,
trained on data drawn from five different domains.
It uses a naive Bayes classifier with a multino-
mial event model, over a mixture of byte n-grams.
As for many language identification packages it
works best for longer texts, but Lui and Bald-
win (2012) also showed that it has good perfor-
mance for short microblog texts, with an accuracy
of 0.89?0.94.
We applied langid.py for each sentence in the
CC corpus, and kept only those sentence pairs
where the correct language was identified for both
sentences with a confidence of at least 0.999. The
total number of sentences was reduced by a further
8.8% based on the langid filtering.
We performed an analysis on a set of 1000 sen-
tence pairs. Among the 907 sentences that were
kept in this set we did not find any cases with
the wrong language. Table 2 shows an analysis
of the 93 sentences that were removed from this
test set. The overall accuracy of langid.py is much
higher than indicated in the table, however, since
it does not include the correctly identified English
and German sentences. We grouped the removed
sentences into four categories, cases where both
languages were correctly identified, but under the
confidence threshold of 0.999, cases where both
languages were incorrectly identified, and cases
where one language was incorrectly identified.
Overall the language identification was accurate
on 54 of the 93 removed sentences. In 18 of the
cases where it was wrong, the sentences were not
translation correspondents, which means that we
only wrongly removed 21 out of 1000 sentences.
It was also often the case when the language was
wrongly identified, that large parts of the sentence
consisted of place names, such as ?Forums about
Conil de la Frontera - Ca?diz.? ? ?Foren u?ber Conil
de la Frontera - Ca?diz.?, which were identified as
es/ht instead of en/de. Even though such sentence
pairs do correspond, they do not contain much use-
ful translation material.
Alignment-Based Cleaning For the alignment-
based cleaning, we aligned the data from the pre-
vious step using GIZA++ (Och and Ney, 2003)
in both directions, and used the intersection of
the alignments. The intersection of alignments is
more sparse than the standard SMT symmetriza-
tion heuristics, like grow-diag-final-and (Koehn et
al., 2005). Our hypothesis was that sentence pairs
with very few alignment points in the intersection
would likely not be corresponding sentences.
We used two types of filtering thresholds based
on alignment points. The first threshold is for the
ratio of the number of alignment points and the
maximum sentence length. The second threshold
is the absolute number of alignment points in a
sentence pair. In addition we used a third thresh-
old based on the length ratio of the sentences.
To find good values for the filtering thresholds,
we created a small gold standard where we man-
ually annotated 100 sentence pairs as being cor-
responding or not. In this set the sentence pairs
did not match in 33 cases. Table 3 show results for
some different values for the threshold parameters.
Overall we are able to get a very high precision
on the task of removing non-corresponding sen-
tences, which means that most sentences that are
removed based on this cleaning are actually non-
corresponding sentences. The recall is a bit lower,
indicating that there are still non-corresponding
sentences left in our data. In our translation sys-
tem we used the bold values in Table 3, since it
gave high precision with reasonable recall for the
removal of non-corresponding sentences, meaning
227
Identification Total Wrong lang. Non-corr Corr Languages identified
English and German < 0.999 15 0 7 8
Both English and German wrong 6 2 2 2 2:na/es, 2:et/et, 1: es/an, 1:es/ht
English wrong 13 1 6 6 5: es 4: fr 1: br, it, de, eo
German wrong 59 51 3 5 51: en 3: es 2:nl 1: af, la, lb
Total 93 54 18 21
Table 2: Reasons and correctness for removing sentences based on language ID for 93 sentences out of
a 1000 sentence subset, divided into wrong lang(uage), non-corr(esponding) pairs, and corr(esponding)
pairs.
Ratio align Min align Ratio length Prec. Recall F Kept
0.1 4 2 0.70 0.77 0.73 70%
0.28 4 2 0.94 0.72 0.82 57%
0.42 4 2 1.00 0.56 0.72 41%
0.28 2 2 0.91 0.73 0.81 59%
0.28 6 2 0.94 0.63 0.76 51%
0.28 4 1.5 0.94 0.65 0.77 52%
0.28 4 3 0.91 0.75 0.82 60%
Table 3: Results of alignment-based cleaning for different values of the filtering parameters, with pre-
cision, recall and F-score for the identification of erroneous sentence pairs and the percentage of kept
sentence pairs
that we kept most correctly aligned sentence pairs.
This cleaning method is more aggressive than
the other cleaning methods we described. For the
gold standard only 57% of sentences were kept,
but in the full training set it was a bit higher, 73%,
as shown in Table 1.
Phrase Table Interpolation To use the CC cor-
pus in our system we first trained a separate phrase
table which we then interpolated with the phrase
table trained on EP-NC. In this way we could al-
ways run the system with a single phrase table. For
interpolation, we used the perplexity minimization
for weighted counts method by Sennrich (2012).
Each of the four weights in the phrase table, back-
ward and forward phrase translation probabilities
and lexical weights, are optimized separately. This
method minimizes the cross-entropy based on a
held-out corpus, for which we used the concate-
nation of all available News development sets.
The cross-entropy and the contribution of CC
relative to EP-NC, are shown for phrase transla-
tion probabilities in both directions in Table 4. The
numbers for lexical weights show similar trends.
For each cleaning step the cross-entropy is re-
duced and the contribution of CC is increased. The
difference between the basic cleaning and langid is
very small, however. The alignment-based clean-
ing shows a much larger effect. After that cleaning
step the CC corpus has a similar contribution to
EP-NC. This is an indicator that the final cleaned
CC corpus fits the development set well.
p(S|T ) p(T |S)
Cleaning CE IP CE IP
Basic 3.18 0.12 3.31 0.06
Langid 3.17 0.13 3.29 0.07
Alignment-based 3.02 0.47 3.17 0.61
Table 4: Cross-entropy (CE) and relative interpo-
lation weights (IP) compared to EP-NC for the
Common Crawl corpus, with different cleaning
Results In Table 5 we show the translation re-
sults with the different types of cleaning of CC,
and without it. We show results of different corpus
combinations both during tuning and testing. We
see that we get the overall best result by both tun-
ing and testing with the alignment-based cleaning
of CC, but it is not as useful to do the extra clean-
ing if we do not tune with it as well. Overall we
get the best results when tuning is performed in-
cluding a cleaned version of CC. This setup gives
a large improvement compared to not using CC at
all, or to use it with only basic cleaning. There is
little difference in Bleu scores when testing with
either basic cleaning, or cleaning based on lan-
guage ID, with a given tuning, which is not sur-
prising given their small and similar interpolation
weights. Tuning was, however, not successful
when using CC with basic cleaning.
Overall we think that alignment-based corpus
cleaning worked well. It reduced the size of the
corpus by over 25%, improved the cross-entropy
for interpolation with the EP-NC phrase-table, and
228
Testing
Tuning not used basic langid alignment
not used 14.0 13.9 13.9 14.0
basic 14.2 14.5 14.3 14.3
langid 15.2 15.3 15.3 15.3
alignment 12.7 15.3 15.3 15.7
Table 5: Bleu scores with different types of clean-
ing and without Common Crawl
gave an improvement on the translation task. We
still think that there is potential for further improv-
ing this filtering and to annotate larger test sets to
investigate the effects in more detail.
5 Tunable Distortion Limits
The Docent decoder uses a hill-climbing search
and can perform operations anywhere in the sen-
tence. Thus, it does not need to enforce a strict
distortion limit. In the Docent implementation, the
distortion limit is actually implemented as a fea-
ture, which is normally given a very large weight,
which effectively means that it works as a hard
constraint. This could easily be relaxed, however,
and in this work we investigate the effects of using
soft distortion limits, which can be optimized dur-
ing tuning, like other features. In this way long-
distance movements can be allowed when they are
useful, instead of prohibiting them completely. A
drawback of using no or soft distortion limits is
that it increases the search space.
In this work we mostly experiment with variants
of one or two standard distortion limits, but with a
tunable weight. We also tried to use separate soft
distortion limits for left- and right-movement. Ta-
ble 6 show the results with different types of dis-
tortion limits. The system with a standard fixed
distortion limits of 6 has a somewhat lower score
than most of the systems with no or soft distortion
limits. In most cases the scores are similar, and
we see no clear affects of allowing tunable lim-
its over allowing unlimited distortion. The system
that uses two mono-directional limits of 6 and 10
has slightly higher scores than the other systems,
and is used in our final submission.
One possible reason for the lack of effect of al-
lowing more distortion could be that it rarely hap-
pens that an operator is chosen that performs such
distortion, when we use the standard Docent set-
tings. To investigate this, we varied the settings of
the parameters that guide the swap-phrases opera-
tor, and used the move-phrases operator instead of
swap-phrases. None of these changes led to any
DL type Limit Bleu
No DL ? 15.5
Hard DL 6 15.0
One soft DL 6 15.5
8 14.2
10 15.5
Two soft DLs 4,8 15.5
6,10 15.7
Bidirectional soft DLs 6,10 15.5
Table 6: Bleu scores for different distortion limit
(DL) settings
improvements, however.
While we saw no clear effects when using tun-
able distortion limits, we plan to extend this work
in the future to model movement differently based
on parts of speech. For the English?German lan-
guage pair, for instance, it would be reasonable to
allow long distance moves of verb groups with no
or little cost, but use a hard limit or a high cost for
other parts of speech.
6 Corpus Selection for Recasing
In this section we investigate the effect of using
different corpus combinations for recasing. We
lower-cased our training corpus, which means that
we need a full recasing step as post-processing.
This is performed by training a SMT system on
lower-cased and true-cased target language. We
used the Moses toolkit to train the recasing system
and to decode during recasing. We investigate the
effect of using different combinations of the avail-
able training corpora to train the recasing model.
Table 7 show case sensitive Bleu scores, which
can be compared to the previous case-insensitive
scores of 15.7. We see that there is a larger effect
of including more data in the language model than
in the translation model. There is a performance
jump both when adding CC data and when adding
News data to the language model. The results
are best when we include the News data, which
is not included in the English?German translation
model, but which is much larger than the other cor-
pora. There is no further gain by using News in
combination with other corpora compared to using
only News. When adding more data to the trans-
lation model there is only a minor effect, with the
difference between only using EP-NC and using
all available corpora is at most 0.2 Bleu points.
In our submitted system we use the monolingual
News corpus both in the LM and the TM.
There are other options for how to treat recas-
229
Language model
TM EP-NC EP-NC-CC News EP-NC-News EP-NC-CC-News
EP-NC 13.8 14.4 14.8 14.8 14.8
EP-NC-CC 13.9 14.5 14.9 14.8 14.8
News 13.9 14.5 14.9 14.9 14.9
EP-NC-News 13.9 14.5 14.9 14.9 14.9
EP-NC-CC-News 13.9 14.5 14.9 14.9 15.0
Table 7: Case-sensitive Bleu scores with different corpus combinations for the language model and
translation model (TM) for recasing
ing. It is common to train the system on true-
cased data instead of lower-cased data, which has
been shown to lead to small gains for the English?
German language pair (Koehn et al, 2008). In this
framework there is still a need to find the correct
case for the first word of each sentence, for which
a similar corpus study might be useful.
7 Comparison to Moses
So far we have only shown results using the Do-
cent decoder on its own, with a random initializa-
tion, since we wanted to submit a Docent-only sys-
tem for the shared task. In this section we also
show contrastive results with Moses, and for Do-
cent initialized with stack decoding, using Moses,
and for different type of tuning.
Previous research have shown mixed results for
the effect of initializing Docent with and with-
out stack decoding, when using the same feature
sets. In Hardmeier et al (2012) there was a drop
of about 1 Bleu point for English?French trans-
lation based on WMT11 data when random ini-
tialization was used. In Stymne et al (2013a),
on the other hand, Docent gave very similar re-
sults with both types of initialization for German?
English WMT13 data. The latter setup is similar
to ours, except that no Common Crawl data was
used.
The results with our setup are shown in Ta-
ble 8. In this case we lose around a Bleu point
when using Docent on its own, without Moses ini-
tialization. We also see that the results are lower
when using Moses with the Docent tuning method,
or when combining Moses and Docent with Do-
cent tuning. This indicates that the document-
level tuning has not given satisfactory results in
this scenario, contrary to the results in Stymne et
al. (2013a), which we plan to explore further in
future work. Overall we think it is important to
develop stronger context-sensitive models for Do-
cent, which can take advantage of the document
context.
Test system Tuning system Bleu
Docent (random) Docent 15.7
Docent (stack) Docent 15.9
Moses Docent 15.9
Docent (random) Moses 15.9
Docent (stack) Moses 16.8
Moses Moses 16.8
Table 8: Bleu scores for Docent initialized ran-
domly or with stack decoding compared to Moses.
Tuning is performed with either Moses or Docent.
For the top line we used tunable distortion lim-
its 6,10 with Docent, in the other cases a standard
hard distortion limit of 6, since Moses does not al-
low soft distortion limits.
8 Conclusion
We have presented the Uppsala University system
for WMT 2013. Our submitted system uses Do-
cent with random initialization and two tunable
distortion limits of 6 and 10. It is trained with the
Common Crawl corpus, cleaned using language
identification and alignment-based filtering. For
recasing we used the monolingual News corpora.
For corpus-cleaning, we present a novel method
for cleaning noisy corpora based on the number
and ratio of word alignment links for sentence
pairs, which leads to a large reduction of corpus
size, and to small improvements on the transla-
tion task. We also experiment with tunable dis-
tortion limits, which do not lead to any consistent
improvements at this stage.
In the current setup the search algorithm of
Docent is not strong enough to compete with
the effective search in standard decoders like
Moses. We are, however, working on developing
discourse-aware models that can take advantage of
the document-level context, which is available in
Docent. We also need to further investigate tuning
methods for Docent.
230
References
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179?1190, Jeju Island, Korea.
Christian Hardmeier, Sara Stymne, Jo?rg Tiedemann,
and Joakim Nivre. 2013. Docent: A document-level
decoder for phrase-based statistical machine transla-
tion. In Proceedings of the 51st Annual Meeting of
the ACL, Demonstration session, Sofia, Bulgaria.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 967?
975, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the NAACL, pages 48?54, Edmonton,
Alberta, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL,
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better machine translation quality for
the German-English language pairs. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 139?142, Columbus, Ohio, USA.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the ACL,
System Demonstrations, pages 25?30, Jeju Island,
Korea.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 42nd Annual Meeting of the ACL, pages 160?
167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311?
318, Philadelphia, Pennsylvania, USA.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 16th
Annual Conference of the European Association
for Machine Translation, pages 539?549, Avignon,
France.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of the
DARPA Broadcast News Transcription and Under-
standing Workshop, pages 270?274, Landsdowne,
Virginia, USA.
Andreas Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proceedings of the
Seventh International Conference on Spoken Lan-
guage Processing, pages 901?904, Denver, Col-
orado, USA.
Sara Stymne, Christian Hardmeier, Jo?rg Tiedemann,
and Joakim Nivre. 2013a. Feature weight opti-
mization for discourse-level SMT. In Proceedings
of the ACL 2013 Workshop on Discourse in Machine
Translation (DiscoMT 2013), Sofia, Bulgaria.
Sara Stymne, Jo?rg Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013b. Statistical machine trans-
lation with readability constraints. In Proceedings
of the 19th Nordic Conference on Computational
Linguistics (NODALIDA?13), pages 375?386, Oslo,
Norway.
231
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60?69,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Feature Weight Optimization for Discourse-Level SMT
Sara Stymne, Christian Hardmeier, Jo?rg Tiedemann and Joakim Nivre
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
We present an approach to feature weight
optimization for document-level decoding.
This is an essential task for enabling future
development of discourse-level statistical
machine translation, as it allows easy inte-
gration of discourse features in the decod-
ing process. We extend the framework of
sentence-level feature weight optimization
to the document-level. We show experi-
mentally that we can get competitive and
relatively stable results when using a stan-
dard set of features, and that this frame-
work also allows us to optimize document-
level features, which can be used to model
discourse phenomena.
1 Introduction
Discourse has largely been ignored in traditional
machine translation (MT). Typically each sentence
has been translated in isolation, essentially yield-
ing translations that are bags of sentences. It is
well known from translation studies, however, that
discourse is important in order to achieve good
translations of documents (Hatim and Mason,
1990). Most attempts to address discourse-level
issues for statistical machine translation (SMT)
have had to resort to solutions such as post-
processing to address lexical cohesion (Carpuat,
2009) or two-step translation to address pronoun
anaphora (Le Nagard and Koehn, 2010). Recently,
however, we presented Docent (Hardmeier et al,
2012; Hardmeier et al, 2013), a decoder based
on local search that translates full documents. So
far this decoder has not included a feature weight
optimization framework. However, feature weight
optimization, or tuning, is important for any mod-
ern SMT decoder to achieve a good translation
performance.
In previous research with Docent, we used grid
search to find weights for document-level features
while base features were optimized using stan-
dard sentence-level techniques. This approach is
impractical since many values for the extra fea-
tures have to be tried, and, more importantly, it
might not give the same level of performance as
jointly optimizing all parameters. Principled fea-
ture weight optimization is thus essential for re-
searchers that want to use document-level features
to model discourse phenomena such as anaphora,
discourse connectives, and lexical consistency. In
this paper, we therefore propose an approach that
supports discourse-wide features in document-
level decoding by adapting existing frameworks
for sentence-level optimization. Furthermore, we
include a thorough empirical investigation of this
approach.
2 Discourse-Level SMT
Traditional SMT systems translate texts sentence
by sentence, assuming independence between sen-
tences. This assumption allows efficient algo-
rithms based on dynamic programming for explor-
ing a large search space (Och et al, 2001). Be-
cause of the dynamic programming assumptions it
is hard to directly include discourse-level features
into a traditional SMT decoder. Nevertheless,
there have been several attempts to integrate inter-
sentential and long distance models for discourse-
level phenomena into standard decoders, usually
as ad-hoc additions to standard models, address-
ing a single phenomenon.
Several studies have tried to improve pro-
noun anaphora by adding information about the
antecedent, either by using two-step decoding
(Le Nagard and Koehn, 2010; Guillou, 2012) or
by extracting information from previously trans-
lated sentences (Hardmeier and Federico, 2010),
unfortunately without any convincing results. To
address the translation of discourse connectives,
source-side pre-processing has been used to anno-
tate surface forms either in the corpus or in the
60
phrase-table (Meyer and Popescu-Belis, 2012) or
by using factored decoding (Meyer et al, 2012)
to disambiguate connectives, with small improve-
ments. Lexical consistency has been addressed
by the use of post-processing (Carpuat, 2009),
multi-pass decoding (Xiao et al, 2011; Ture et al,
2012), and cache models (Tiedemann, 2010; Gong
et al, 2011). Gong et al (2012) addressed the
issue of tense selection for translation from Chi-
nese, by the use of inter-sentential tense n-grams,
exploiting information from previously translated
sentences. Another way to use a larger context
is by integrating word sense disambiguation and
SMT. This has been done by re-initializing phrase
probabilities for each sentence (Carpuat and Wu,
2007), by introducing extra features in the phrase-
table (Chan et al, 2007), or as a k-best re-ranking
task (Specia et al, 2008). Another type of ap-
proach is to integrate topic modeling into phrase
tables (Zhao and Xing, 2010; Su et al, 2012). For
a more thorough overview of discourse in SMT,
see Hardmeier (2012).
Here we instead choose to work with the re-
cent document-level SMT decoder Docent (Hard-
meier et al, 2012). Unlike in traditional decod-
ing were documents are generated sentence by
sentence, feature models in Docent always have
access to the complete discourse context, even
before decoding is finished. It implements the
phrase-based SMT approach (Koehn et al, 2003)
and is based on local search, where a state con-
sists of a full translation of a document, which is
improved by applying a series of operations to im-
prove the translation. A hill-climbing strategy is
used to find a (local) maximum. The operations
allow changing the translation of a phrase, chang-
ing the word order by swapping the positions of
two phrases, and resegmenting phrases. The initial
state can either be initialized randomly in mono-
tonic order, or be based on an initial run from a
standard sentence-based decoder. The number of
iterations in the decoder is controlled by two pa-
rameters, the maximum number of iterations and
a rejection limit, which stops the decoder if no
change was made in a certain number of iterations.
This setup is not limited by dynamic programming
constraints, and enables the use of the translated
target document to extract features. It is thus easy
to directly integrate discourse-level features into
Docent. While we use this specific decoder in our
experiments, the method proposed for document-
level feature weight optimization is not limited to
it. It can be used with any decoder that outputs
feature values at the document level.
3 Sentence-Level Tuning
Traditionally, feature weight optimization, or tun-
ing, for SMT is performed by an iterative process
where a development set is translated to produce a
k-best list. The parameters are then optimized us-
ing some procedure, generally to favor translations
in the k-best list that have a high score on some
MT metric. The translation step is then repeated
using the new weights for decoding, and optimiza-
tion is continued on a new k-best list, or on a com-
bination of all k-best lists. This is repeated until
some end condition is satisfied, for instance for a
set number of iterations, until there is only very
small changes in parameter weights, or until there
are no new translations in the k-best lists.
SMT tuning is a hard problem in general, partly
because the correct output is unreachable and
also because the translation process includes la-
tent variables, which means that many efficient
standard optimization procedures cannot be used
(Gimpel and Smith, 2012). Nevertheless, there
are a number of techniques including MERT (Och,
2003), MIRA (Chiang et al, 2008; Cherry and
Foster, 2012), PRO (Hopkins and May, 2011),
and Rampion (Gimpel and Smith, 2012). All of
these optimization methods can be plugged into
the standard optimization loop. All of the meth-
ods work relatively well in practice, even though
there are limitations, for instance that many meth-
ods are non-deterministic meaning that their re-
sults are somewhat unstable. However, there are
some important differences. MERT is based on
scores for the full test set, whereas the other meth-
ods are based on sentence-level scores. MERT
also has the drawback that it only works well for
small sets of features. In this paper we are not
concerned with the actual optimization algorithm
and its properties, though, but instead we focus
on the integration of document-level decoding into
the existing optimization frameworks.
In order to adapt sentence-level frameworks to
our needs we need to address the granularity of
scoring and the process of extracting k-best lists.
For document-level features we do not have mean-
ingful scores on the sentence level which are re-
quired in standard optimization frameworks. Fur-
thermore, the extraction of k-best lists is not as
61
Input: inputDocs, refDocs, init weights ?0, max decoder iters max, sample start ss, sample interval si,
Output: learned weights ?
1: ? ? ?0
2: Initialize empty klist
3: run? 1
4: repeat
5: Initialize empty klistrun
6: for doc? 1, inputDocs.size do Initialize decoder state randomly for inputDocs[doc]
7: for iter? 1,max do
8: Perform one hill-climbing step for inputDocs[doc]
9: if iter >= ss & iter mod si == 0 then
10: Add translation for inputDocs[doc] to klistrun
11: end if
12: end for
13: end for
14: Merge klistrun with klist
15: modelScoresdoc ? ComputeModelScores(klist)
16: metricStatsdoc ? ComputeMetricStats(klist, refDocs)
17: ?run ? ?
18: ? ? Optimize(?run,modelScoresdoc,metricStatsdoc)
19: run? run + 1
20: until Done(run, ?, ?run)
Figure 1: Document-level feature weight optimization algorithm
straightforward in our hill-climbing decoder as in
standard sentence-level decoders such as Moses
(Koehn et al, 2007) where such a list can be ap-
proximated easily from the internal beam search
strategy. Working on output lattices is another op-
tion in standard approaches (Cherry and Foster,
2012) which is also not applicable in our case.
In the following section we describe how we
can address these issues in order to adapt sentence-
level frameworks for our purposes.
4 Document-Level Tuning
To allow document-level feature weight optimiza-
tion, we make some small changes to the sentence-
level framework. Figure 1 shows the algorithm we
use. It assumes access to an optimization algo-
rithm, Optimize, and an end criterion, Done.
The changes from standard sentence-level opti-
mization is that we compute scores on the docu-
ment level, and that we sample translations instead
of using standard k-best lists.
The main challenge is that we need meaning-
ful scores which we do not have at the sentence
level in document decoding. We handle this by
simply computing all scores (model scores and
metric scores) exclusively at the document level.
Remember that all standard MT metrics based on
sentence-level comparisons with reference trans-
lations can be aggregated for a complete test set.
Here we do the same for all sentences in a given
document. This can actually be an advantage com-
pared to optimization methods that use sentence-
level scores, which are known to be unreliable
(Callison-Burch et al, 2012). Document-level
scores should thus be more stable, since they are
based on more data. A potential drawback is that
we get fewer data points with a test set of the same
size, which might mean that we need more data to
achieve as good results as with sentence-level op-
timization. We will see the ability of our approach
to optimize weights with reasonable data sets in
our experiments further down.
The second problem, the extraction of k-best
lists can be addressed in several ways. It is pos-
sible to get a k-best list from Docent by extract-
ing the results from the last k iterations. However,
since Docent operates on the document-level and
does not accept updates in each iteration, there will
be many identical and/or very similar hypotheses
with such an approach. Another option would be
to extract the translations from the k last differ-
ent iterations, which would require some small
changes to the decoder. Instead, we opt to use k-
lists, lists of translations sampled with some inter-
val, which contains k translations, but not neces-
sarily all the k best translations that could be found
by the decoder. A k-best list is of course a k-list,
which we get with a sample interval of 1.
We also choose to restart Docent randomly in
each optimization iteration, since it allows us to
explore a larger part of the search space. We
empirically found that this strategy worked better
than restarting the decoder from the previous best
state.
62
German?English English?Swedish
Type Sentences Documents Type Sentences Documents
Training
Europarl 1.9M ? Europarl 1.5M ?
News Commentary 178K ? ? ? ?
Tuning
News2009 2525 111 Europarl (Moses) 2000 ?
News2008-2010 7567 345 Europarl (Docent) 1338 100
Test News2012 3003 99 Europarl 690 20
Table 1: Domain and number of sentences and documents for the corpora
As seen in Figure 1, there are some additional
parameters in our procedure: the sample start iter-
ation and the sample interval. We also need to set
the number of decoder iterations to run. In Sec-
tion 5 we empirically investigate the effect of these
parameters.
Compared to sentence-level optimization, we
also have a smaller number of units to get scores
from, since we use documents as units, and not
sentences. The importance of this depends on the
optimization algorithm. MERT calculates metric
scores over the full tuning set, not for individual
sentences, and should not be affected too much
by the change in granularity. Many other opti-
mization algorithms, like PRO, work on the sen-
tence level, and will likely be more affected by
the reduction of units. In this work we focus on
MERT, which is the most commonly used opti-
mization procedure in the SMT community, and
which tends to work quite well with relatively few
features. However, we also show contrastive re-
sults for PRO (Hopkins and May, 2011). A fur-
ther issue is that Docent is non-deterministic, i.e.,
it can give different results with the same param-
eter weights. Since the optimization process is al-
ready somewhat unstable this is a potential issue
that needs to be explored further, which we do in
Section 5.
Implementation-wise we adapted Docent to out-
put k-lists and adapted the infrastructure available
for tuning in the Moses decoder (Koehn et al,
2007) to work with document-level scores. This
setup allows us to use the variety of optimization
procedures implemented there.
5 Experiments
In this section we report experimental results
where we investigate several issues in connec-
tion with document-level feature weight optimiza-
tion for SMT. We first describe the experimental
setup, followed by baseline results using sentence-
level optimization. We then present validation ex-
periments with standard sentence-level features,
which can be compared to standard optimization.
Finally, we report results with a set of document-
level features that have been proposed for joint
translation and text simplification (Stymne et al,
2013).
5.1 Experimental Setup
Most of our experiments are for German-to-
English news translation using data from the
WMT13 workshop.1 We also show results with
document-level features for English-to-Swedish
Europarl (Koehn, 2005). The size of the training,
tuning, and test sets are shown in Table 1. First of
all, we need to extract documents for tuning and
testing with Docent. Fortunately, the news data al-
ready contain document markup, corresponding to
individual news articles. For Europarl we define a
document as a consecutive sequence of utterances
from a single speaker. To investigate the effect of
the size of the tuning set, we used different subsets
of the available tuning data.
All our document-level experiments are car-
ried out with Docent but we also contrast with
the Moses decoder (Koehn et al, 2007). For the
purpose of comparison, we use a standard set of
sentence-level features used in Moses in most of
our experiments: five translation model features,
one language model feature, a distance-based re-
ordering penalty, and a word count feature. For
feature weight optimization we also apply the
standard settings in the Moses toolkit. We opti-
mize towards the Bleu metric, and optimization
ends either when no weights are changed by more
than 0.00001, or after 25 iterations. MERT is used
unless otherwise noted.
Except for one of our baselines, we always run
Docent with random initialization. For test we run
the document decoder for a maximum of 227 iter-
ations with a rejection limit of 100,000. In our
experiments, the decoder always stopped when
reaching the rejection limit, usually between 1?5
1http://www.statmt.org/wmt13/
translation-task.html
63
million iterations.
We show results on the Bleu (Papineni et al,
2002) and NIST (Doddington, 2002) metrics. For
German?English we show the average result and
standard deviation of three optimization runs, to
control for optimizer instability as proposed by
Clark et al (2011). For English?Swedish we re-
port results on single optimization runs, due to
time constraints.
5.2 Baselines
Most importantly, we would like to show the effec-
tiveness of the document-level tuning procedure
described above. In order to do this, we created
a baseline using sentence-level optimization with
a tuning set of 2525 sentences and the News2009
corpus for evaluation. Increasing the tuning set is
known to give only modest improvements (Turchi
et al, 2012; Koehn and Haddow, 2012).
The feature weights optimized with the stan-
dard Moses decoder can then directly be used in
our document-level decoder as we only include
sentence-level features in our baseline model. As
expected, these optimized weights also lead to
a better performance in document-level decoding
compared to an untuned model as shown in Ta-
ble 2. Note, that Docent can be initialized in
two ways, by Moses and randomly. Not surpris-
ingly, the result for the runs initialized with Moses
are identical with the pure sentence-level decoder.
Initializing randomly gives a slightly lower Bleu
score but with a larger variation than with Moses
initialization, which is also expected. Docent is
non-deterministic, and can give somewhat varying
results with the same weights. However, this vari-
ation has been shown experimentally to be very
small (Hardmeier et al, 2012).
Our goal now is to show that document-level
tuning can perform equally well in order to verify
our approach. For this, we set up a series of ex-
periments looking at varying tuning sets and dif-
ferent parameters of the decoding and optimiza-
tion procedure. With this we like to demonstrate
the stability of the document-level feature weight
optimization approach presented above. Note that
the most important baselines for comparison with
the results in the next sections are the ones with
Docent and random initialization.
5.3 Sentence-Level Features
In this section we present validation results where
we investigate different aspects of document-
System Tuning Bleu NIST
Moses None 17.7 6.25
Docent-M None 17.7 6.25
Docent-R None 15.2 (0.05) 5.88 (0.00)
Moses Moses 18.3 (0.04) 6.22 (0.01)
Docent-M Moses 18.3 (0.04) 6.22 (0.01)
Docent-R Moses 18.1 (0.13) 6.23 (0.01)
Table 2: Baseline results, where Docent-M is ini-
tialized with Moses and Docent-R randomly
Docs Sent. Min Max Bleu NIST
111 2525 3 127 18.0 (0.11) 6.19 (0.04)
345 7567 3 127 18.1 (0.14) 6.25 (0.02)
100 1921 8 40 18.0 (0.05) 6.25 (0.10)
200 3990 8 40 17.9 (0.25) 6.20 (0.09)
100 2394 8 100 18.0 (0.12) 6.27 (0.07)
200 4600 8 100 18.1 (0.29) 6.26 (0.10)
300 6852 8 100 18.2 (0.13) 6.27 (0.03)
Table 3: Results for German?English with varying
sizes of tuning set, where the number of sentences
and documents are varied, as well as the minimum
and maximum number of sentences per document
level feature weight optimization with standard
sentence-level features. In this way we can com-
pare the results directly to standard sentence-level
optimization, and to the results of Moses.
Corpus size We investigate how tuning is af-
fected by corpus size. The corpus size was var-
ied in two ways, by changing the number of docu-
ments in the tuning set, and by changing the length
of documents in the tuning sets. In this exper-
iment we run 20000 decoder iterations per opti-
mization iteration, and use a k-list of size 101,
with sample interval 100. Table 3 shows the re-
sults with varying tuning set sizes for German?
English. There is very little variation between the
scores, and no clear tendencies. All results are of
similar quality to the baseline with random initial-
ization and sentence-level tuning, and better than
not using any tuning. The top line in Table 3 is
News2009, the same tuning set as for the base-
lines. The scores are somewhat more unstable than
the baseline scores, but stability is not related to
corpus size. In the following sections we will use
the tuning set with 200 documents, size 8-40.
Number of decoder iterations and k-list sam-
pling Two issues that are relevant for feature
weight optimization with the document-level de-
coder is the number of decoder hill-climbing iter-
ations in each optimization iteration, and the set-
tings for k-list sampling. These choices affect the
64
Iterations K-list UTK Bleu NIST
20000 101 55.6 17.9 (0.25) 6.20 (0.09)
30000 201 67.2 17.9 (0.06) 6.21 (0.01)
40000 301 79.9 18.2 (0.11) 6.28 (0.09)
50000 401 86.9 18.1 (0.20) 6.22 (0.05)
75000 651 99.2 17.8 (0.15) 6.13 (0.03)
100000 901 106.8 17.9 (0.17) 6.16 (0.03)
30000 101 21.6 18.0 (0.15) 6.21 (0.02)
40000 101 12.6 17.7 (0.53) 6.12 (0.15)
50000 101 8.2 17.9 (0.24) 6.18 (0.06)
Table 4: Results for German?English with a vary-
ing number of iterations and k-list size (UTK is
the average number of unique translations per doc-
ument in the k-lists)
quality of the translations in each optimization it-
eration, and the spread in the k-list. We will report
the average number of unique translations per doc-
ument in the k-lists, UTK, during feature weight
optimization, in this section.
The top half of Table 4 shows results with a
different number of iterations, when we sample
k-lists from iteration 10000 with interval 100 for
German?English, which means that the size of the
k-lists also changes. The differences on MT met-
rics are very small. The number of new unique
translations in the k-lists decrease with the number
of decoder iterations. With 20K iterations, 55%
of the k-lists entries are unique, which could be
compared to only 12% with 100K iterations. The
majority of the unique translations are thus found
in the beginning of the decoding, which is not sur-
prising.
The bottom half of Table 4 shows results with
a different number of decoder iterations, but a set
k-list size. In this setting the number of unique
hypotheses in the k-lists obviously decreases with
the number of decoder iterations. Despite this,
there are mostly small result differences, except
for 40K iterations, which has more unstable results
than the other settings. It does not seem useful to
increase the number of decoder iterations without
also increasing the size of the k-list. An even bet-
ter strategy might be to only include unique entries
in the k-lists. We will explore this in future work.
We also ran experiments where we did not
restart the decoder with a random state in each iter-
ation, but instead saved the previous state and con-
tinued decoding with the new weights from there.
This, however, was largely unsuccessful, and gave
very low scores. We believe that the reason for this
is mainly that a much smaller part of the search
space is explored when the decoder is not restarted
Interval Start UTK Bleu NIST
1 19900 1.4 18.2 (0.07) 6.25 (0.04)
10 19000 5.2 18.1 (0.08) 6.22 (0.03)
100 10000 55.6 17.9 (0.25) 6.20 (0.09)
200 0 82.2 17.9 (0.19) 6.15 (0.05)
Table 5: Results with different k-list-sample inter-
vals for k-lists size 101 (UTK is the average num-
ber of unique translations per document in the k-
lists)
with a new seed repeatedly. The fact that a higher
overall quality can be achieved with a higher num-
ber of iterations (see Figure 2) can apparently not
compensate for this drawback.
Finally, we investigate the effect of the sam-
ple interval for the k-lists. To get k-lists of equal
size, 101, we start the sampling at different itera-
tions. Table 5 shows the results, and we can see
that with a small sample interval, the number of
unique translations decreases drastically. Despite
this, there are no large result differences. There
is actually a slight trend that a smaller sample in-
terval is better. This does not confirm our intuition
that it is important with many different translations
in the k-list. Especially for interval 1 it is surpris-
ing, since there is often only 1 unique translation
for a single document. We believe that the fact that
k-lists from different iterations are joined, can be
part of the explanation for these results. We think
more work is needed in the future, to further ex-
plore these settings, and the interaction with the
total number of decoder iteration, and the k-list
sampling.
To further shed some light to these results, we
show learning curves from the optimization. Fig-
ure 2 shows Bleu scores for the system optimized
with 100K decoder iterations after different num-
bers of iterations, for the last three iterations in
each of the three optimization runs. As shown in
Hardmeier et al (2012), the translation quality in-
creases fast at first, but start to level out at around
40K iterations. Despite this, the optimization re-
sults are good even with 20K iterations, which is
somewhat surprising. Figures 3 and 4 show the
Bleu scores after each tuning iteration for the sys-
tems in Tables 4 and 5. As is normal for SMT tun-
ing, the convergence is slow, and there are some
oscillations even late in the optimization. Over-
all systems with many iterations seem somewhat
more stable.
Overall, the results are better than the untuned
65
 
14
 
14.5 15
 
15.5 16
 
16.5 17
 
17.5
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80
 
90 
100
Bleu
Dec
ode
r ite
ratio
ns (*
1000
)
1-23 1-24 1-25 2-23 2-24 2-25 3-23 3-24 3-25
Figure 2: Bleu scores during 100000 Docent iter-
ations during feature weight optimization
 
4
 
6
 
8
 
10
 
12
 
14
 
16
 
18  
0
 
5
 
10
 
15
 
20
 
25
Bleu
Tun
ing 
itera
tion
s20K 30K 40K 50K 75K 100
K
Figure 3: Bleu scores during feature weight opti-
mization for systems with different number of de-
coder iterations and k-list sizes.
baseline and on par with the sentence-level tuning
baselines in all settings, with a relatively modest
variation, even across settings. In fact, if we cal-
culate the total scores of all 36 systems in Tables 4
and 5, we get a Bleu score of 18.0 (0.23) and a
NIST score of 6.19 (0.07), with a variation that is
not higher than for many of the different settings.
Optimization method In this section we com-
pare the performance of the MERT optimiza-
tion algorithm with that of PRO, and a combi-
nation that starts MERT with weights initialized
with PRO (MERT+PRO), suggested by Koehn and
Haddow (2012). Here we run 30000 decoder it-
erations. Table 6 shows the results. Initializing
MERT with PRO did not affect the scores much.
The scores with only PRO, however, are slightly
lower than for MERT, and have a much larger
score variation. This could be because PRO is
 
4
 
6
 
8
 
10
 
12
 
14
 
16
 
18  
0
 
5
 
10
 
15
 
20
 
25
Bleu
Tun
ing 
itera
tion
s1 (20
K)
10 (2
0K)
100
 (20K
)
100
 (30K
)
100
 (40K
)
100
 (50K
)
200
 (20K
)
Figure 4: Bleu scores during feature weight opti-
mization for systems with different k-list sample
interval and number of decoder iterations.
Bleu NIST
MERT 17.9 (0.06) 6.21 (0.01)
PRO 17.5 (0.41) 6.15 (0.20)
MERT+PRO 18.0 (0.12) 6.18 (0.06)
Table 6: Results with different optimization algo-
rithms for German?English
likely to need more data, since it calculates met-
ric scores on individual units, sentences or docu-
ments, not across the full tuning set, like MERT.
This likely means that 200 documents are too few
for stable results with optimization methods that
depend on unit-level metric scores.
5.4 Document-Level Features
In this section we investigate the effect of opti-
mization with a number of document-level fea-
tures. We use a set of features proposed in Stymne
et al (2013), in order to promote the readability
of texts. In this scenario, however, we use these
features in a standard SMT setting, where they
can potentially improve the lexical consistency of
translations. The features are:
? Type token ratio (TTR) ? the ratio of types,
unique words, to tokens, total number of
words
? OVIX ? a reformulation of TTR that has tra-
ditionally been used for Swedish and that is
less sensible to text length than TTR, see
Eq. 1
? Q-value, phrase level (QP) - The Q-value was
developed as a measure for bilingual term
quality (Dele?ger et al, 2006), to promote
common and consistently translated terms.
See Eq. 2, where f(st) is the frequency of
66
German?English English?Swedish
System Optimization Bleu NIST Bleu NIST
Moses Sentence 18.3 (0.04) 6.22 (0.01) 24.3 6.12
Docent Sentence 18.1 (0.13) 6.23 (0.01) 24.1 6.06
Docent Document 17.9 (0.25) 6.20 (0.09) 23.4 6.01
TTR Document 18.3 (0.16) 6.33 (0.04) 23.6 6.15
OVIX Document 18.3 (0.13) 6.30 (0.03) 23.4 5.99
QW Document 18.1 (0.14) 6.22 (0.03) 24.2 6.11
QP Document 18.0 (0.10) 6.23 (0.05) 21.2 5.70
Table 7: Results when using document-level features
the phrase pair, n(s) is the number of unique
target phrases which the source phrase is
aligned to in the document, and n(t) is the
same for the target phrase. Here the Q-value
is applied on the phrase level.
? Q-value, word level (QW) - Same as above,
but here we apply the Q-value for source
words and their alignments on the target side.
OVIX =
log(count(tokens))
log
(
2?
log(count(types))
log(count(tokens))
) (1)
Q-value =
f(st)
n(s) + n(t)
(2)
We added these features one at a time to the
standard feature set. Optimization was performed
with 20000 decoder iterations, and a k-list of size
101. As shown in the previous sections, there
are slightly better settings, which could have been
used to boost the results somewhat.
The results are shown in Table 7. For German?
English, the results are generally on par with the
baselines for Bleu and slightly higher on NIST for
OVIX and TTR. For English?Swedish, we used a
smaller tuning set on the document level than on
the sentence level, see Table 1, due to time con-
straints. This is reflected in the scores, which are
generally lower than for sentence-level decoding.
Using the QW feature, however, we receive com-
petitive scores to the sentence-based baselines,
which indicates that it can be meaningful to use
document-level features with the suggested tuning
approach.
While the results do not improve much over
the baselines, these experiments still show that
we can optimize discourse-level features with
our approach. We need to identify more useful
document-level features in future work, however.
6 Conclusion
We have shown how the standard feature weight
optimization workflow for SMT can be adapted to
document-level decoding, which allows easy inte-
gration of discourse-level features into SMT. We
modified the standard framework by calculating
scores on the document-level instead of the sen-
tence level, and by using k-lists rather than k-best
lists.
Experimental results show that we can achieve
relatively stable results, on par with the results for
sentence-level optimization and better than with-
out tuning, with standard features. This is de-
spite the fact that we use the hill-climbing de-
coder without initialization by a standard decoder,
which means that it is somewhat unstable, and
is not guaranteed to find any global maximum,
even according to the model. We also show that
we can optimize document-level features success-
fully. We investigated the effect of a number of
parameters relating to tuning set size, the number
of decoder iterations, and k-list sampling. There
were generally small differences relating to these
parameters, however, indicating that the suggested
approach is robust. The interaction between pa-
rameters does need to be better explored in future
work, and we also want to explore better sampling,
without duplicate translations.
This is the first attempt of describing and exper-
imentally investigating feature weight optimiza-
tion for direct document-level decoding. While we
show the feasibility of extending sentence-level
optimization to the document level, there is still
much more work to be done. We would, for in-
stance, like to investigate other optimization pro-
cedures, especially for systems with a high num-
ber of features. Most importantly, there is a large
need for the development of useful discourse-level
features for SMT, which can now be optimized.
Acknowledgments
This work was supported by the Swedish strategic
research programme eSSENCE.
67
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 61?72, Prague, Czech Republic.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 19?27, Boulder, Colorado.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the ACL, pages 33?40, Prague,
Czech Republic.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427?436, Montre?al, Canada.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of Human
Language Technologies: The 2008 Annual Con-
ference of the NAACL, pages 224?233, Honolulu,
Hawaii.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the ACL: Human Language Tech-
nologies, pages 176?181, Portland, Oregon, USA.
Louise Dele?ger, Magnus Merkel, and Pierre Zweigen-
baum. 2006. Enriching medical terminologies:
an approach based on aligned corpora. In Inter-
national Congress of the European Federation for
Medical Informatics, pages 747?752, Maastricht,
The Netherlands.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology, pages 228?231, San Diego, California,
USA.
Kevin Gimpel and Noah A. Smith. 2012. Struc-
tured ramp loss minimization for machine transla-
tion. In Proceedings of the 2012 Conference of
the NAACL: Human Language Technologies, pages
221?231, Montre?al, Canada.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 909?919, Edinburgh, Scotland,
UK.
Zhengxian Gong, Min Zhang, Chew Lim Tan, and
Guodong Zhou. 2012. N-gram-based tense models
for statistical machine translation. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 276?285,
Jeju Island, Korea.
Liane Guillou. 2012. Improving pronoun translation
for statistical machine translation. In Proceedings of
the EACL 2012 Student Research Workshop, pages
1?10, Avignon, France.
Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-
chine translation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 283?289, Paris, France.
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179?1190, Jeju Island, Korea.
Christian Hardmeier, Sara Stymne, Jo?rg Tiedemann,
and Joakim Nivre. 2013. Docent: A document-level
decoder for phrase-based statistical machine transla-
tion. In Proceedings of the 51st Annual Meeting of
the ACL, Demonstration session, Sofia, Bulgaria.
Christian Hardmeier. 2012. Discourse in statistical
machine translation: A survey and a case study. Dis-
cours, 11.
Basil Hatim and Ian Mason. 1990. Discourse and the
Translator. Longman, London, UK.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 317?321,
Montre?al, Canada.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the NAACL, pages 48?54, Edmonton,
Alberta, Canada.
68
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL,
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit X, pages 79?86, Phuket, Thailand.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR, pages
252?261, Uppsala, Sweden.
Thomas Meyer and Andrei Popescu-Belis. 2012. Us-
ing sense-labeled discourse connectives for statisti-
cal machine translation. In Proceedings of the Joint
Workshop on Exploiting Synergies between Informa-
tion Retrieval and Machine Translation (ESIRMT)
and Hybrid Approaches to Machine Translation
(HyTra), pages 129?138, Avignon, France.
Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,
and Andrea Gesmundo. 2012. Machine translation
of labeled discourse connectives. In Proceedings of
the 10th Biennial Conference of the Association for
Machine Translation in the Americas, San Diego,
California, USA.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for Statisti-
cal Machine Translation. In Proceedings of the ACL
2001 Workshop on Data-Driven Machine Transla-
tion, pages 55?62, Toulouse, France.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 42nd Annual Meeting of the ACL, pages 160?
167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311?
318, Philadelphia, Pennsylvania, USA.
Lucia Specia, Baskaran Sankaran, and Maria das
Grac?as Volpe Nunes. 2008. N-best reranking for
the efficient integration of word sense disambigua-
tion and statistical machine translation. In Proceed-
ings of the 9th International Conference on Intelli-
gent Text Processing and Computational Linguistics
(CICLING), pages 399?410, Haifa, Israel.
Sara Stymne, Jo?rg Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013. Statistical machine trans-
lation with readability constraints. In Proceedings
of the 19th Nordic Conference on Computational
Linguistics (NODALIDA?13), pages 375?386, Oslo,
Norway.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the ACL,
pages 459?468, Jeju Island, Korea.
Jo?rg Tiedemann. 2010. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In Proceedings of the ACL
2010 Workshop on Domain Adaptation for Natural
Language Processing (DANLP), pages 8?15, Upp-
sala, Sweden.
Marco Turchi, Tijl De Bie, Cyril Goutte, and Nello
Cristianini. 2012. Learning to translate: A statis-
tical and computational analysis. Advances in Arti-
ficial Intelligence, 2012. Article ID 484580.
Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the
NAACL: Human Language Technologies, pages
417?426, Montre?al, Canada.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in
machine translation. In Proceedings of MT Summit
XIII, pages 131?138, Xiamen, China.
Bing Zhao and Eric P. Xing. 2010. HM-BiTAM: Bilin-
gual topic exploration, word alignment,and transla-
tion. In Advances in Neural Information Processing
Systems 20 (NIPS), pages 1689?1696, Cambridge,
Massachusetts, USA.
69
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 122?129,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Anaphora Models and Reordering for Phrase-Based SMT
Christian Hardmeier Sara Stymne J
?
org Tiedemann Aaron Smith Joakim Nivre
Uppsala University
Department of Linguistics and Philology
firstname.lastname@lingfil.uu.se
Abstract
We describe the Uppsala University sys-
tems for WMT14. We look at the integra-
tion of a model for translating pronomi-
nal anaphora and a syntactic dependency
projection model for English?French. Fur-
thermore, we investigate post-ordering and
tunable POS distortion models for English?
German.
1 Introduction
In this paper we describe the Uppsala University
systems for WMT14. We present three different
systems. Two of them are based on the document-
level decoder Docent (Hardmeier et al., 2012; Hard-
meier et al., 2013a). In our English?French sys-
tem we extend Docent to handle pronoun anaphora,
and in our English?German system we add part-
of-speech phrase-distortion models to Docent. For
German?English we also have a system based on
Moses (Koehn et al., 2007). Again the focus is
on word order, this time by using pre- and post-
reordering.
2 Document-Level Decoding
Traditional SMT decoders translate texts as bags
of sentences, assuming independence between sen-
tences. This assumption allows efficient algorithms
for exploring a large search space based on dy-
namic programming (Och et al., 2001). Because of
the dynamic programming assumptions it is hard to
directly include discourse-level and long-distance
features into a traditional SMT decoder.
In contrast to this very popular stack decoding
approach, our decoder Docent (Hardmeier et al.,
2012; Hardmeier et al., 2013a) implements a search
procedure based on local search. At any stage of
the search process, its search state consists of a
complete document translation, making it easy for
feature models to access the complete document
with its current translation at any point in time. The
search algorithm is a stochastic variant of standard
hill climbing. At each step, it generates a successor
of the current search state by randomly applying
one of a set of state changing operations to a ran-
dom location in the document, and accepts the new
state if it has a better score than the previous state.
The operations are to change the translation of a
phrase, to change the word order by swapping the
positions of two phrases or moving a sequence of
phrases, and to resegment phrases. The initial state
can either be initialized randomly, or be based on
an initial run from Moses. This setup is not limited
by dynamic programming constraints, and enables
the use of the full translated target document to
extract features.
3 English?French
Our English?French system is a phrase-based SMT
system with a combination of two decoders, Moses
(Koehn et al., 2007) and Docent (Hardmeier et al.,
2013a). The fundamental setup is loosely based
on the system submitted by Cho et al. (2013) to
the WMT 2013 shared task. Our phrase table is
trained on data taken from the News commentary,
Europarl, UN, Common crawl and 10
9
corpora.
The first three of these corpora were included in-
tegrally into the training set after filtering out sen-
tences of more than 80 words. The Common crawl
and 10
9
data sets were run through an additional
filtering step with an SVM classifier, closely fol-
lowing Mediani et al. (2011). The system includes
three language models, a regular 6-gram model
with modified Kneser-Ney smoothing (Chen and
Goodman, 1998) trained with KenLM (Heafield,
2011), a 4-gram bilingual language model (Niehues
et al., 2011) with Kneser-Ney smoothing trained
with KenLM and a 9-gram model over Brown clus-
ters (Brown et al., 1992) with Witten-Bell smooth-
ing (Witten and Bell, 1991) trained with SRILM
(Stolcke, 2002).
122
The latest version released in March is equipped with . . . It is sold at . . .
La derni`ere version lanc?ee en mars est dot?ee de . . . ? est vendue . . .
Figure 1: Pronominal Anaphora Model
Our baseline system achieved a cased BLEU
score of 33.2 points on the newstest2014 data set.
Since the anaphora model used in our submission
suffered from a serious bug, we do not discuss the
results of the primary submission in more detail.
3.1 Pronominal Anaphora Model
Our pronominal anaphora model is an adaptation
of the pronoun prediction model described by Hard-
meier et al. (2013b) to SMT. The model consists
of a neural network that discriminatively predicts
the translation of a source language pronoun from
a short list of possible target language pronouns us-
ing features from the context of the source language
pronouns and from the translations of possibly re-
mote antecedents. The objective of this model is to
handle situations like the one depicted in Figure 1,
where the correct choice of a target-language pro-
noun is subject to morphosyntactic agreement with
its antecedent. This problem consists of several
steps. To score a pronoun, the system must decide
if a pronoun is anaphoric and, if so, find potential
antecedents. Then, it can predict what pronouns
are likely to occur in the translation. Our pronoun
prediction model is trained on both tasks jointly,
including anaphora resolution as a set of latent vari-
ables. At test time, we split the network in two
parts. The anaphora resolution part is run sepa-
rately as a preprocessing step, whereas the pronoun
prediction part is integrated into the document-level
decoder with two additional feature models.
The features correspond to two copies of the neu-
ral network, one to handle the singular pronoun it
and one to handle the plural pronoun they. Each net-
work just predicts a binary distinction between two
cases, il and elle for the singular network and ils
and elles for the plural network. Unlike Hardmeier
et al. (2013b), we do not use an OTHER category to
capture cases that should not be translated with any
of these options. Instead, we treat all other cases in
the phrase table and activate the anaphora models
only if one of their target pronouns actually occurs
in the output.
To achieve this, we generate pronouns in two
steps. In the phrase table training corpus, we re-
place all pronouns that should be handled by the
classifier, i.e. instances of il and elle aligned to it
and instances of ils and elles aligned to they, with
special placeholders. At decoding time, if a place-
holder is encountered in a target language phrase,
the applicable pronouns are generated with equal
translation model probability, and the anaphora
model adds a score to discriminate between them.
To reduce the influence of the language model
on pronoun choice and give full control to the
anaphora model, our primary language model is
trained on text containing placeholders instead of
pronouns. Since all output pronouns can also be
generated without the interaction of the anaphora
model if they are not aligned to a source language
pronoun, we must make sure that the language
model sees training data for both placeholders and
actual pronouns. However, for the monolingual
training corpora we have no word alignments to
decide whether or not to replace a pronoun by a
placeholder. To get around this problem, we train a
6-gram placeholder language model on the target
language side of the Europarl and News commen-
tary corpora. Then, we use the Viterbi n-gram
model decoder of SRILM (Stolcke, 2002) to map
pronouns in the entire language model training set
to placeholders where appropriate. No substitu-
tions are made in the bilingual language model or
the Brown cluster language model.
3.2 Dependency Projection Model
Our English?French system also includes a depen-
dency projection model, which uses source-side
dependency structure to model target-side relations
between words. This model assigns a score to each
dependency arc in the source language by consider-
ing the target words aligned to the head and the de-
pendent. In Figure 2, for instance, there is an nsub-
jpass arc connecting dominated to production. The
head is aligned to the target word domin?ee, while
the dependent is aligned to the set {production,de}.
The score is computed by a neural network taking
as features the head and dependent words and their
part-of-speech tags in the source language, the tar-
get word sets aligned to the head and dependent,
the label of the dependency arc, the distance be-
tween the head and dependent word in the source
language as well as the shortest distance between
any pair of words in the aligned sets. The network
is a binary classifier trained to discriminate positive
examples extracted from human-made reference
123
Domestic meat production is dominated by chicken .
amod
nn
nsubjpass
auxpass prep pobj
punct
La production int?erieure de viande est domin?ee par le poulet .
Figure 2: Dependency projection model
translations from negative examples extracted from
n-best lists generated by a baseline SMT system.
4 English?German
For English?German we have two systems, one
based on Moses, and one based on Docent. In both
cases we have focused on word order, particularly
for verbs and particles.
Both our systems are trained on the same data
made available by WMT. The Common crawl data
was filtered using the method of Stymne et al.
(2013). We use factored models with POS tags
as a second output factor for German. The possi-
bility to use language models for different factors
has been added to our Docent decoder. Language
models include an in-domain news language model,
an out-of-domain model trained on the target side
of the parallel training data and a POS language
model trained on tagged news data. The LMs are
trained in the same way as for English?French.
All systems are tuned using MERT (Och, 2003).
Phrase-tables are filtered using entropy-based prun-
ing (Johnson et al., 2007) as implemented in Moses.
All BLEU scores are given for uncased data.
4.1 Pre-Ordered Alignment and
Post-Ordered Translation
The use of syntactic reordering as a separate pre-
processing step has already a long tradition in sta-
tistical MT. Handcrafted rules (Collins et al., 2005;
Popovi?c and Ney, 2006) or data-driven models (Xia
and McCord, 2004; Genzel, 2010; Rottmann and
Vogel, 2007; Niehues and Kolss, 2009) for pre-
ordering training data and system input have been
explored in numerous publications. For certain
language pairs, such as German and English, this
method can be very effective and often improves
the quality of standard SMT systems significantly.
Typically, the source language is reordered to better
match the syntax of the target language when trans-
lating between languages that exhibit consistent
word order differences, which are difficult to handle
by SMT systems with limited reordering capabil-
ities such as phrase-based models. Preordering is
often done on the entire training data as well to op-
timize translation models for the pre-ordered input.
Less common is the idea of post-ordering, which
refers to a separate step after translating source lan-
guage input to an intermediate target language with
corrupted (source-language like) word order (Na et
al., 2009; Sudoh et al., 2011).
In our experiments, we focus on the translation
from English to German. Post-ordering becomes
attractive for several reasons: One reason is the
common split of verb-particle constructions that
can lead to long distance dependencies in German
clauses. Phrase-based systems and n-gram lan-
guage models are not able to handle such relations
beyond a certain distance and it is desirable to keep
them as connected units in the phrase translation
tables. Another reason is the possible distance of
finite and infinitival verbs in German verb phrases
that can lead to the same problems described above
with verb-particle constructions. The auxiliary or
modal verb is placed at the second position but
the main verb appears at the end of the associated
verb phrase. The distances can be arbitrarily long
and long-range dependencies are quite frequent.
Similarly, negation particles and adverbials move
away from the inflected verb forms in certain con-
structions. For more details on specific phenomena
in German, we refer to (Collins et al., 2005; Go-
jun and Fraser, 2012). Pre-ordering, i.e. moving
English words into German word order does not
seem to be a good option as we loose the con-
nection between related items when moving par-
ticles and main verbs away from their associated
elements. Hence, we are interested in reordering
the target language German into English word or-
der which can be beneficial in two ways: (i) Re-
ordering the German part of the parallel training
data makes it possible to improve word alignment
(which tends to prefer monotonic mappings) and
subsequent phrase extraction which leads to better
translation models. (ii) We can explore a two-step
procedure in which we train a phrase-based SMT
model for translating English into German with
English word order first (which covers many long-
distance relations locally) and then apply a second
system that moves words into place according to
correct German syntax (which may involve long-
range distortion).
For simplicity, we base our experiments on hand-
124
crafted rules for some of the special cases discussed
above. For efficiency reasons, we define our rules
over POS tag patterns rather than on full syntac-
tic parse trees. We rely on TreeTagger and apply
rules to join verbs in discontinuous verb phrases
and to move verb-finals in subordinate clauses, to
move verb particles, adverbials and negation par-
ticles. Table 1 shows two examples of reordered
sentences together with the original sentences in
English and German. Our rules implement rough
heuristics to identify clause boundaries and word
positions. We do not properly evaluate these rules
but focus on the down-stream evaluation of the MT
system instead.
It is therefore dangerous to extrapolate from short-term trends.
Daher ist es gef?ahrlich, aus kurzfristigen Trends Prognosen abzuleiten.
Daher ist gef?ahrlich es, abzuleiten aus kurzfristigen Trends Prognosen.
The fall of Saddam ushers in the right circumstances.
Der Sturz von Saddam leitet solche richtigen Umst?ande ein.
Der Sturz von Saddam ein leitet solche richtigen Umst?ande.
Table 1: Two examples of pre-ordering outputs.
The first two lines are the original English and
German sentences and the third line shows the re-
ordered sentence.
We use three systems based on Moses to com-
pare the effect of reordering on alignment and trans-
lation. All systems are case-sensitive phrase-based
systems with lexicalized reordering trained on data
provided by WMT. Word alignment is performed
using fast align (Dyer et al., 2013). For tuning we
use newstest2011. Additionally, we also test paral-
lel data from OPUS (Tiedemann, 2012) filtered by
a method adopted from Mediani et al. (2011).
To contrast our baseline system, we trained a
phrase-based model on parallel data that has been
aligned on data pre-ordered using the reordering
rules for German, which has been restored to the
original word order after word alignment and be-
fore phrase extraction (similar to (Carpuat et al.,
2010; Stymne et al., 2010)). We expect that the
word alignment is improved by reducing crossings
and long-distance links. However, the translation
model as such has the same limitations as the base-
line system in terms of long-range distortions. The
final system is a two-step model in which we apply
translation and language models trained on pre-
ordered target language data to perform the first
step, which also includes a reordered POS language
model. The second step is also treated as a transla-
tion problem as in Sudoh et al. (2011), and in our
case we use a phrase-based model here with lexical-
ized reordering and a rather large distortion limit
of 12 words. Another possibility would be to apply
another rule set that reverts the misplaced words
to the grammatically correct positions. This, how-
ever, would require deeper syntactic information
about the target language to, for example, distin-
guish main from subordinate clauses. Instead, our
model is trained on parallel target language data
with the pre-ordered version as input and the orig-
inal version as output language. For this model,
both sides are tagged and a POS language model
is used again as one of the target language factors
in decoding. Table 2 shows the results in terms of
BLEU scores on the newstest sets from 2013 and
2014.
newstest2013 newstest2014
baseline 19.3 19.1
pre 19.4 19.3
post 18.6 18.7
baseline+OPUS 19.5 19.3
pre+OPUS 19.5 19.3
post+OPUS 19.7 18.8
Table 2: BLEU4 scores for English-German sys-
tems (w/o OPUS): Standard phrase-based (base-
line); phrase-based with pre-ordered parallel cor-
pus used for word alignment (pre); two-step phrase-
based with post-reordering (post)
The results show that pre-ordering has some ef-
fect on word alignment quality in terms of support-
ing better phrase extractions in subsequent steps.
Our experiments show a consistent but small im-
provement for models trained on data that have
been prepared in this way. In contrast, the two-step
procedure is more difficult to judge in terms of au-
tomatic metrics. On the 2013 newstest data we can
see another small improvement in the setup that
includes OPUS data but in most cases the BLEU
scores go down, even below the baseline. The
short-comings of the two-step procedure are ob-
vious. Separating translation and reordering in a
pipeline adds the risk of error propagation. Fur-
thermore, reducing the second step to single-best
translations is a strong limitation and using phrase-
based models for the final reordering procedure is
probably not the wisest decision. However, manual
inspections reveals that many interesting phenom-
ena can be handled even with this simplistic setup.
Table 3 illustrates this with a few selected out-
comes of our three systems. They show how verb-
particle constructions with long-range distortion
125
reference Schauspieler Orlando Bloom hat sich zur Trennung von seiner Frau , Topmodel Miranda Kerr , ge?au?ert .
baseline Schauspieler Orlando Bloom hat die Trennung von seiner Frau , Supermodel Miranda Kerr .
pre-ordering Schauspieler Orlando Bloom hat angek?undigt , die Trennung von seiner Frau , Supermodel Miranda Kerr .
post-ordering Schauspieler Orlando Bloom hat seine Trennung von seiner Frau angek?undigt , Supermodel Miranda Kerr .
reference Er gab bei einer fr?uheren Befragung den Kokainbesitz zu .
baseline Er gab den Besitz von Kokain in einer fr?uheren Anh?orung .
pre-ordering Er r?aumte den Besitz von Kokain in einer fr?uheren Anh?orung .
post-ordering Er r?aumte den Besitz von Kokain in einer fr?uheren Anh?orung ein .
reference Borussia Dortmund k?undigte daraufhin harte Konsequenzen an .
baseline Borussia Dortmund k?undigte an , es werde schwere Folgen .
pre-ordering Borussia Dortmund hat angek?undigt , dass es schwerwiegende Konsequenzen .
post-ordering Borussia Dortmund k?undigte an , dass es schwere Folgen geben werde .
Table 3: Selected translation examples from the newstest 2014 data; the human reference translation; the
baseline system, pre-ordering for word alignment and two-step translation with post-ordering.
such as ?r?aumte ... ein? can be created and how
discontinuous verb phrases can be handled (?hat ...
angek?undigt?) with the two-step procedure. The
model is also often better in producing verb finals
in subordinate clauses (see the final example with
?geben werde?). Note that many of these improve-
ments do not get any credit by metrics like BLEU.
For example the acceptable expression ?r?aumte ein?
which is synonymous to ?gab zu? obtains less credit
then the incomplete baseline translation. Interest-
ing is also to see the effect of pre-ordering when
used for alignment only in the second system. The
first example in Table 3, for example, includes a
correct main verb which is omitted in the baseline
translation, probably because it is not extracted as
a valid translation option.
4.2 Part-of-Speech Phrase-Distortion Models
Traditional SMT distortion models consist of two
parts. A distance-based distortion cost is based
on the position of the last word in a phrase, com-
pared to the first word in the next phrase, given the
source phrase order. A hard distortion limit blocks
translations where the distortion is too large. The
distortion limit serves to decrease the complexity
of the decoder, thus increasing its speed.
In the Docent decoder, the distortion limit is not
implemented as a hard limit, but as a feature, which
could be seen as a soft constraint. We showed in
previous work (Stymne et al., 2013) that it was
useful to relax the hard distortion limit by either
using a soft constraint, which could be tuned, or
removing the limit completely. In that work we
still used the standard parametrization of distortion,
based on the positions of the first and last words in
phrases.
Our Docent decoder, however, always provides
us with a full target translation that is step-wise im-
proved, which means that we can apply distortion
measures on the phrase-level without resorting to
heuristics, which, for instance, are needed in the
case of the lexicalized reordering models in Moses
(Koehn et al., 2005). Because of this it is possible
to use phrase-based distortion, where we calculate
distortion based on the order of phrases, not on the
order of some words. It is possible to parametrize
phrase-distortion in different ways. In this work we
use the phrase-distortion distance and a soft limit
on the distortion distance, to mimic the word-based
distortion. In our experiments we always set the
soft limit to a distance of four phrases. In addition
we use a measure based on how many crossings
a phrase order gives rise to. We thus have three
phrase-distortion features.
As captured by lexicalized reordering models,
different phrases have different tendencies to move.
To capture this to some extent, we also decided
to add part-of-speech (POS) classes to our mod-
els. POS has previously successfully been used
in pre-reordering approaches (Popovi?c and Ney,
2006; Niehues and Kolss, 2009). The word types
that are most likely to move long distances in
English?German translation are verbs and parti-
cles. Based on this observation we split phrases
into two classes, phrases that only contains verbs
and particles, and all other phrases. For these two
groups we use separate phrase-distortion features,
thus having a total of six part-of-speech phrase-
distortion features. All of these features are soft,
and are optimized during tuning.
In our system we initialize Docent by running
Moses with a standard distortion model and lexi-
calized reordering, and then continuing the search
with Docent including our part-of-speech phrase-
distortion features. Tuning was done separately for
the two components, first for the Moses component,
and then for the Docent component initialized by
126
reference Laut Dmitrij Kislow von der Organisation ?Pravo na oryzhie? kann man eine Pistole vom Typ Makarow f?ur 100 bis 300 Dollar kaufen.
baseline Laut Dmitry Kislov aus der Rechten zu Waffen, eine Makarov Gun-spiele erworben werden k?onnen f?ur 100-300 Dollar.
POS+phrase Laut Dmitry Kislov von die Rechte an Waffen, eine Pistole Makarov f?ur 100-300 Dollar erworben werden k?onnen.
reference Die Waffen gelangen ?uber mehrere Kan?ale auf den Schwarzmarkt.
baseline Der ?Schwarze? Markt der Waffen ist wieder aufgef ?ullt ?uber mehrere Kan?ale.
POS+phrase Der ?Schwarze? Markt der Waffen durch mehrere Kan?ale wieder aufgef ?ullt ist.
reference Mehr Kameras k?onnten m?oglicherweise das Problem l?osen...
baseline M?oglicherweise k?onnte das Problem l?osen, eine gro?e Anzahl von Kameras...
POS+phrase M?oglicherweise, eine gro?e Anzahl von Kameras k?onnte das Problem l?osen...
Table 4: Selected translation examples from the newstest2013 data; the human reference translation; the
baseline system (Moses with lexicalized reordering) and the system with a POS+phrase distortion model.
Moses with lexicalized reordering with its tuned
weights. We used newstest2009 for tuning. The
training data was lowercased for training and de-
coding, and recasing was performed using a sec-
ond Moses run trained on News data. As baselines
we present two Moses systems, without and with
lexicalized reordering, in addition to standard dis-
tortion features.
Table 5 shows results with our different distor-
tion models. Overall the differences are quite small.
The clearest difference is between the two Moses
baselines, where the lexicalized reordering model
leads to an improvement. With Docent, both the
word distortion and phrase distortion without POS
do not help to improve on Moses, with a small de-
crease in scores on one dataset. This is not very
surprising, since lexical distortion is currently not
supported by Docent, and the distortion models are
thus weaker than the ones implemented in Moses.
For our POS phrase distortion, however, we see a
small improvement compared to Moses, despite the
lack of lexicalized distortion. This shows that this
distortion model is actually useful, and can even
successfully replace lexicalized reordering. In fu-
ture work, we plan to combine this method with a
lexicalized reordering model, to see if the two mod-
els have complementary strengths. Our submitted
system uses the POS phrase-distortion model.
System Distortion newstest2013 newstest2014
Moses word 19.4 19.3
Moses word+LexReo 19.6 19.6
Docent word 19.5 19.6
Docent phrase 19.5 19.6
Docent POS+phrase 19.7 19.7
Table 5: BLEU4 scores for English?German sys-
tems with different distortion models.
If we inspect the translations, most of the differ-
ences between the Moses baseline and the system
with POS+phrase distortion are actually due to lex-
ical choice. Table 4 shows some examples where
there are word order differences. The result is quite
mixed with respect to the placement of verbs. In
the first example, both systems put the verbs to-
gether but in different positions, instead of splitting
them like the reference suggests. In the second
example, our system erroneously put the verbs at
the end, which would be fine if the sentence had
been a subordinate clause. In the third example,
the baseline system has the correct placement of
the auxiliary ?k?onnte?, while our system is bet-
ter at placing the main verb ?l?osen?. In general,
this indicates that our system is able to support
long-distance distortion as it is needed in certain
cases but sometimes overuses this flexibility. A
better model would certainly need to incorporate
syntactic information to distinguish main from sub-
ordinate clauses. However, this would add a lot of
complexity to the model.
5 Conclusion
We have described the three Uppsala University
systems for WMT14. In the English?French sys-
tem we extend our document-level decoder Do-
cent (Hardmeier et al., 2013a) to handle pronoun
anaphora and introduced a dependency projection
model. In our two English?German system we
explore different methods for handling reordering,
based on Docent and Moses. In particular, we look
at post-ordering as a separate step and tunable POS
phrase distortion.
Acknowledgements
This work forms part of the Swedish strategic re-
search programme eSSENCE. We also acknowl-
edge the use of the Abel cluster, owned by the
University of Oslo and the Norwegian metacenter
for High Performance Computing (NOTUR) and
operated by the Department for Research Comput-
ing at USIT, under project nn9106k. Finally, we
would also like to thank Eva Pettersson, Ali Basirat,
and Eva Martinez for help with human evaluation.
127
References
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving Arabic-to-English statistical ma-
chine translation by reordering post-verbal subjects
for alignment. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 178?183, Uppsala, Swe-
den.
Stanley F. Chen and Joshua Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical report, Computer Sci-
ence Group, Harvard University, Cambridge, Mas-
sachusetts, USA.
Eunah Cho, Thanh-Le Ha, Mohammed Mediani, Jan
Niehues, Teresa Herrmann, Isabel Slawik, and Alex
Waibel. 2013. The Karlsruhe Institute of Technol-
ogy translation systems for the WMT 2013. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 104?108, Sofia, Bulgaria.
Association for Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 531?540, Ann Arbor, Michi-
gan, USA.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of the 2013
Conference of the NAACL: Human Language Tech-
nologies, pages 644?648, Atlanta, Georgia, USA.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 376?
384, Beijing, China.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English?to?
German SMT. In Proceedings of the 13th Confer-
ence of the EACL, pages 726?735, Avignon, France.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1179?
1190, Jeju Island, Korea.
Christian Hardmeier, Sara Stymne, J?org Tiedemann,
and Joakim Nivre. 2013a. Docent: A document-
level decoder for phrase-based statistical machine
translation. In Proceedings of the 51st Annual Meet-
ing of the ACL, Demonstration session, pages 193?
198, Sofia, Bulgaria.
Christian Hardmeier, J?org Tiedemann, and Joakim
Nivre. 2013b. Latent anaphora resolution for cross-
lingual pronoun prediction. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 380?391, Seattle,
Washington, USA. Association for Computational
Linguistics.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland. Association
for Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 967?
975, Prague, Czech Republic.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL,
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The
KIT English?French translation systems for IWSLT
2011. In Proceedings of the International Workshop
on Spoken Language Translation, pages 73?78, San
Francisco, California, USA.
Hwidong Na, Jin-Ji Li, Jungi Kim, and Jong-Hyeok
Lee. 2009. Improving fluency by reordering tar-
get constituents using MST parser in English-to-
Japanese phrase-based SMT. In Proceedings of
MT Summit XII, pages 276?283, Ottawa, Ontario,
Canada.
Jan Niehues and Muntsin Kolss. 2009. A POS-based
model for long-range reorderings in SMT. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 206?214, Athens, Greece.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider context by using bilin-
gual language models in machine translation. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 198?206, Edinburgh, Scot-
land. Association for Computational Linguistics.
128
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for Statisti-
cal Machine Translation. In Proceedings of the ACL
2001 Workshop on Data-Driven Machine Transla-
tion, pages 55?62, Toulouse, France.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
42nd Annual Meeting of the ACL, pages 160?167,
Sapporo, Japan.
Maja Popovi?c and Hermann Ney. 2006. POS-based re-
orderings for statistical machine translation. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC?06), pages
1278?1283, Genoa, Italy.
Kay Rottmann and Stephan Vogel. 2007. Word re-
ordering in statistical machine translation with a
POS-based distortion model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 171?180, Sk?ovde, Sweden.
Andreas Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proceedings of the
Seventh International Conference on Spoken Lan-
guage Processing, pages 901?904, Denver, Col-
orado, USA.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and OOVs: Two problems for translation
between German and English. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 183?188, Uppsala,
Sweden.
Sara Stymne, Christian Hardmeier, J?org Tiedemann,
and Joakim Nivre. 2013. Tunable distortion limits
and corpus cleaning for SMT. In Proceedings of the
Eighth Workshop on Statistical Machine Translation,
pages 225?231, Sofia, Bulgaria.
Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Post-ordering
in statistical machine translation. In Proceedings of
MT Summit XIII, pages 316?323, Xiamen. China.
J?org Tiedemann. 2012. Parallel data, tools and in-
terfaces in OPUS. In Proceedings of the 8th In-
ternational Conference on Language Resources and
Evaluation (LREC?12), pages 2214?2218, Istanbul,
Turkey.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085?
1094.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
pages 508?514, Geneva, Switzerland.
129
