Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the SIGDIAL 2013 Conference, pages 233?241,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Evaluation of Speech Dialog Strategies
for Internet Applications in the Car
Hansjo?rg Hofmann
Ute Ehrlich
Andre? Berton
Daimler AG / Ulm, Germany,
hansjoerg.hofmann@daimler.com
Angela Mahr
Rafael Math
Christian Mu?ller
DFKI / Saarbru?cken, Germany
angela.mahr@dfki.de
Abstract
Due to the mobile Internet revolution, peo-
ple tend to browse the Web while driv-
ing their car which puts the driver?s safety
at risk. Therefore, an intuitive and non-
distractive in-car speech interface to the
Web needs to be developed. Before de-
veloping a new speech dialog system in a
new domain developers have to examine
what the user?s preferred interaction style
is in order to use such a system. This pa-
per reports from a very recent driving sim-
ulation study and its preliminary results
which are conducted in order to compare
different speech dialog strategies. The
use of command-based and conversational
SDS prototypes while driving is evaluated
on usability and driving performance. Dif-
ferent GUIs are designed in order to sup-
port the respective dialog strategy the most
and to evaluate the effect of the GUI on us-
ability and driver distraction. The prelim-
inary results show that the conversational
speech dialog performs more efficient than
the command-based dialog. However, the
conversational dialog distracts more from
driving than the command-based. Further-
more, the results indicate that an SDS sup-
ported by a GUI is more efficient and bet-
ter accepted by the user than without GUI.
1 Introduction
The pervasive use of smartphones in daily situ-
ations impacts the automotive environment. In
order to stay ?always connected? people tend to
use their smartphone?s Internet functions manually
while driving. However, using a smartphone man-
ually while driving, distracts the driver and endan-
gers the driver?s safety. According to Governors
Highway Safety Association (2011) 25% of U.S.
car crashes are related to drivers using their cell-
phones while driving. Therefore, the development
of an intuitive and non-distractive in-car speech in-
terface to the Web is essential in order to increase
driver safety (Peissner et al, 2011).
Before developing a new speech dialog system
(SDS) in a new domain developers have to ex-
amine how users would interact with such a sys-
tem. An Internet user study by Hofmann et al
(2012a) in which the subjects had to solve Internet
tasks orally, revealed that concerning communica-
tional (e.g. sending an Email) and transactional
tasks (e.g. booking a hotel) conversational and
command-based speaking styles were used with
equal frequency. Because of the equal frequency
of occurrence you have to examine which speech
dialog strategy - the command-based or the con-
versational - is the most suitable for these tasks.
First studies on the evaluation of dialog strate-
gies have been conducted by Devillers and
Bonneau-Maynard (1998) who compare two SDS
allowing the user to retrieve touristic information.
One dialog strategy guides the user via system
suggestions, the other does not. The evaluated di-
alog strategies comprise the fundamental ideas the
command-based and conversational dialog strat-
egy consist of. By applying qualitative and quan-
titative criteria they conclude that user guidance is
suitable for novices and appreciated by all kinds
of users. However, there was no GUI involved
and the speech interaction was performed as pri-
mary task. Considering the driving use case other
results may be achieved since the primary task is
driving. Furthermore, the use of these SDS among
advanced users needs to be investigated.
In the TALK project, Mutschler et al (2007)
compared a command-based speech dialog to a
conversational dialog where the driver had to con-
trol the in-car mp3-player by speech while driving.
The same graphical user interface (GUI) was used
for both dialog strategies. Although the conver-
233
sational dialog was more efficient the command-
based dialog was more appreciated by the sub-
jects. According to Mutschler et al the high error
rate of the conversational strategy was the reason
for the higher acceptance of the command-based
dialog. There were no significant differences in
the driving performance revealed when using the
different SDS.
The speech recognizer quality has improved
enormously within the last five years. There-
fore, the weak speech recognition performance of
Mutschler et al?s conversational dialog may be
nowadays less significant. Furthermore, the use of
the same GUI for different dialog strategies could
have additionally influenced the result. The GUI
should be adapted to the particular dialog strategy
in order to benefit from the advantages of the re-
spective strategy the most and to allow for a com-
parison of optimal systems.
This paper reports from a very recent driving
simulation study and its preliminary results which
are conducted in order to compare different speech
dialog strategies. The use of command-based and
conversational SDS prototypes while driving is
evaluated on usability and driving performance.
The systems have been developed for German and
allows users to perform a hotel booking by speech.
Different GUIs are designed in order to support the
respective dialog strategy the most and to evaluate
the effect of the GUI on usability and driver dis-
traction. The experiments have been conducted
at DFKI, Saarbru?cken using the OpenDS1 driv-
ing simulation. The research work is performed
within the scope of the EU FP7 funding project
GetHomeSafe2.
The remainder of the paper is structured as fol-
lows: In Section 2, the developed SDS prototypes
are briefly described. Section 3 presents the ex-
perimental setup and its results and finally, con-
clusions are drawn.
2 SDS Prototype Concepts
The chosen use case for the design of the SDS
concepts is booking a hotel by speech while driv-
ing since it covers many different subdialog types
(parameter input, list presentation and browsing,
etc.). For this purpose, the online hotel booking
service HRS3 has been used as data provider for
1http://www.opends.eu/
2http://www.gethomesafe-fp7.eu
3http://www.hrs.com
the SDS.
Each SDS prototype concept offers the same
functionality: First, the user has to input his search
parameter to retrieve a list of hotels. The user
can browse the list and ask for detailed informa-
tion about a certain hotel. If the hotel matches his
needs he is able to book the hotel. In addition, the
user can change the search parameters.
In the following, the different speech dialog
strategies and the corresponding GUI designs are
briefly decribed. A detailed description of the
human-machine interface (HMI) concepts can be
found in Hofmann et al (2012b).
2.1 Speech Dialog Strategy Design
SDS Prototypes for German language have been
developed including the following SDS features:
In order to speak to the system the driver has to
press a Push-To-Activate (PTA) button. Further-
more, the driver is able to interrupt the system
while prompting the user (?barge-in?). When de-
signing the different dialog strategies we particu-
larly focused our attention on the dialog initiative,
the possibility to enter multiple input parameters
and the acoustic feedback.
2.1.1 Command-based Speech Dialog
Strategy
The dialog behavior of the command-based dialog
strategy corresponds to the voice-control which
can be found in current state-of-the-art in-car SDS.
By calling explicit speech commands the speech
dialog is initiated and the requested information is
delivered or the demanded task is executed. There
are several synonyms available for each command.
By using implicit feedback in the voice prompts
the driver is informed about what the system has
understood. After the first command the user is
guided by the system and executes the steps which
are suggested and displayed by the system. The
GUI supports the speech dialog by showing the
?speakable? commands as widgets on the screen
(see Section 2.2). A sample dialog is illustrated in
the following:
Driver: Book a hotel.
System: Where would you like to book a hotel?
Driver: In Berlin.
System: When do you want to arrive in Berlin?
Driver: Tomorrow.
System: How long would you like to stay in Berlin?
Driver: Until the day after tomorrow.
234
2.1.2 Conversational Speech Dialog Strategy
In the conversational dialog strategy the dialog ini-
tiative switches during the speech interaction. The
driver is able to speak whole sentences where mul-
tiple parameters can be set within one single ut-
terance. Thereby, the dialog can be more natural,
flexible and efficient. The driver is informed about
what the system has understood by using implicit
feedback. The GUI does not present the ?speak-
able? commands on the screen. In order to indi-
cate the possible functions icons are displayed (see
Section 2.2). A sample dialog is presented in the
the following:
Driver: I would like to book a hotel in Berlin.
System: When do you arrive in Berlin?
Driver: I?ll arrive tomorrow and leave
the day after tomorrow.
As illustrated in the example the driver can al-
ready indicate some input parameters when ad-
dressing the system for the first time. The system
verifies which input parameters are missing in or-
der to send a hotel request. The system prompts
the user and collects the missing information. Al-
though the system asks for only one parameter, the
user is able to give more or other information than
requested.
2.2 GUI Design
The different GUIs have been designed in order to
support the speech dialog strategies and to eval-
uate the effect of the GUI on usability and driv-
ing performance. The different GUIs have been
customized corresponding to the dialog strate-
gies only as much as necessary since an objec-
tive comparison is targeted. When designing the
screens we followed the international standard-
ized AAM-Guidelines (Driver Focus-Telematics
Working Group, 2002).
2.2.1 Command-based GUI Design
In the command-based dialog strategy the driver
uses commands to speak to the system. In order
to give the driver an understanding of the ?speak-
able? commands, the speech dialog is supported
by the GUI. For that reason the currently possible
speech commands are displayed on the screen at
all times which may lead to a high visual distrac-
tion. Hence, in automotive terms the command-
based speech dialog strategy is also called ?speak-
what-you-see? strategy.
Figure 1(a) illustrates the main screen of the ho-
tel booking application at the beginning of the ho-
tel booking dialog. Here, the first input parameter
?destination? (?Ziel? in German) is requested by
the system. Afterwards the user is guided step-by-
step by the system. When the driver has given the
requested information, a new widget appears on
the screen and the system asks the driver for the
corresponding input.
2.2.2 Conversational GUI Design
In the conversational dialog strategy the driver can
speak freely and does not have to use certain com-
mands. There is no need to give the driver a vi-
sual feedback of the currently ?speakable? com-
mands whereby the visual distraction may be low-
ered. For that reason, the content on the head unit
screen does not have to indicate the possible op-
tions to proceed with the speech dialog. The sub-
function line which was used to indicate the avail-
able commands is replaced by only few symbols
which resemble the current GUI state. Figure 1(b)
shows the form filling main screen at the begin-
ning of the speech interaction where the user is
already able to input several parameters at once.
2.2.3 Without GUI
We also investigated the need for a visual feed-
back, why the two speech dialog strategies are
also evaluated ?without GUI?. In this case, with-
out GUI means that no content information is dis-
played on the screen. However, a visual feedback
which indicates if the user is allowed to talk is
presented in the top bar of the screen (see Figure
1(c)).
3 Evaluation
3.1 Method
3.1.1 Participants
The experiment was conducted at DFKI,
Saarbru?cken. In total, 24 German participants
(mainly students) participated in the experiment.
All participants received a monetary expense
allowance and possessed a valid driver?s license.
Due to missing data recordings during the exper-
iment data of 1 participant had to be excluded
from the analyses. The remaining participants
comprised 9 male and 14 female subjects and the
average age was 26 years (standard deviation (SD)
= 4,1). 56,5% of the participants were driving
their car at least once a day. 56,5% had little to no
experience with speech-controlled devices.
235
(a) Command-based GUI (b) Conversational GUI (c) ?without? GUI
Figure 1: Main Screens at the Beginning of the Interaction.
3.1.2 Experimental Design
Four different HMI concept variants were evalu-
ated in a 2x2 (speech dialog strategy: command-
based vs. conversational, GUI: with vs. without)
design. The Command-based and Conversational
GUI were only used with the corresponding dialog
strategy. The 4 HMI concepts were the following:
? Command-based speech dialog (?Comm?)
? with GUI (?CommGUI?) and
? without GUI (?CommNoGUI?)
? Conversational speech dialog (?Conv?)
? with GUI (?ConvGUI?) and
? without GUI (?ConvNoGUI?)
Each participant encountered all four conditions
(?within-design?). For each condition, two tasks
had to be accomplished. We investigated the
participants speech dialog performance and in-
fluences on driving performance while using the
SDS.
3.1.3 Materials
Speech Dialog Prototypes: In the experiment,
the speech dialog prototypes described in Section
2 have been used. In order to explain the func-
tionality and the control of the SDS prototypes to
the user, instruction videos for each speech dia-
log strategy were presented. By presenting tutorial
videos, we ensured that each participant was given
identical instructions.
During the experiment, participants had to solve
several tasks: They had to book a certain hotel
according to given search parameters. The tasks
were verbalized as little stories which contained
the necessary parameters in a memorable manner.
A sample task in English is presented below:
Imagine, you and your colleague are on the way
to Cologne for a two-day meeting right now. You
need two single rooms for these two nights which
you have not booked, yet. Your appointment
takes place in the city center of Cologne, where
you would like to spend your night. Please look
for a matching hotel for those nights.
In total, participants had to perform 16 tasks. Four
tasks were used as sample tasks to familiarize par-
ticipants with the respective speech dialog strategy
after showing the instruction video. The remain-
ing eight tasks were used for the data collection.
Questionnaires: During the experiment differ-
ent questionnaires were used:
? Preliminary Interview: In a preliminary ques-
tionnaire we collected demographical infor-
mation (age, gender, etc.) about the partic-
ipants. Furthermore, we surveyed driving
habits, experience with speech-controlled de-
vices, and hotel booking habits.
? SASSI questionnaire (Hone and Graham,
2001): The SASSI questionnaire covering 6
dimensions consists of 34 questions and is
widely used to measure subjective usability
evaluation of SDS.
? DALI questionnaire (Pauzie, 2008): The
DALI questionnaire covers 6 dimensions in
order to evaluate the user?s cognitive load.
The applied questionnaire consisted of 7
questions covering each dimension and an
additional question addressing the manual
demand.
? Final Interview: This questionnaire was de-
signed to allow for a direct comparison of the
respective SDS prototypes at the end of the
experiment. Each participant had to rate the
different SDS on a scale from 1 - 10 regard-
ing several subjective measures. For each of
the six SASSI dimensions, one question was
asked. Additionally, we asked questions to
directly compare cognitive load and to get
information about the participants? personal
preference of interaction style with the sys-
tem at different sub dialogs.
Driving Simulation Setup: The experiment
was conducted in the driving simulator at DFKI?s
?future lab? (see Figure 2). The participants were
236
sitting on the driver?s seat in a car which was
placed in front of a canvas onto which the driving
simulation was projected. The participants con-
trolled the driving simulation by the car steering
wheel and pedals. During the experiment the ex-
aminer was sitting on the passenger seat.
Figure 2: DFKI Driving Simulator Setup.
Previous driving simulation studies employ the
standard Lane Change Test (LCT) by Mattes
(2003). However, this driving task does not con-
tinuously mentally demand the user and thus, does
not reflect the real cognitive load while driving.
Furthermore, LCT is based on single tracks which
limits the recordings to a certain time. We em-
ployed the ConTRe (Continuous Tracking and Re-
action) task as part of the OpenDS1 driving sim-
ulation software which complements the de-facto
standard LCT including higher sensitivity and a
more flexible driving task without restart interrup-
tions. The steering task for lateral control resem-
bles a continuous follow drive which will help to
receive more detailed results about the two diverse
dialog strategies. Furthermore, mental demand is
addressed explicitly by employing an additional
reaction task implemented as longitudinal control.
A detailed description of the ConTRe task can be
found in Mahr et al (2012).
In the experiment, after giving the participant
the hotel booking task instructions, the experi-
menter started the driving simulation. When the
participant has crossed the start sign in the simula-
tion he had to begin the speech dialog. When the
hotel booking was completed, the experimenter
stopped the driving simulation. Thereby, driving
performance was only recorded during the speech
dialog.
3.1.4 Procedure
In the experiment, 4 conditions were evaluated:
The conversational speech dialog (with and with-
out GUI) and the command-based speech dialog
(with and without GUI). We did not randomize
all four conditions, because the participants might
have been confused if the speech dialog styles vary
too often. Therefore, we decided to employ dialog
styles blockwise (see Figure 3). In one block, only
one speech dialog variant with the two GUI condi-
tions was tested. The order of the two blocks was
counterbalanced between participants to control
for learning and order effects. Thereby, half of the
participants were first introduced to the command-
based dialog, whereas the other half of the partic-
ipants started with the conversational dialog. Fur-
thermore, the order of GUI conditions within one
block was balanced between participants. In each
of the four conditions, the participants had to per-
form two tasks. The order of the tasks was the
same for all participants regardless of the system
condition. Hence, all tasks were encountered in
all dialog and GUI combinations. When the sec-
ond task was finished, participants had to fill out
the SASSI and the DALI questionnaire for each
condition.
Task
 1
Task
 2
SASS
I + D
ALI
Task
 1
Task
 2
SASS
I + D
ALI
Task
 1
Task
 2
SASS
I + D
ALI
Task
 1
Task
 2
SASS
I + D
ALI
with
GUI
witho
utGU
I
witho
utGU
I
with
GUI
Data
 
Colle
ction
SDS 
Type
 
1
Data
 
Colle
ction
SDS 
Type
 
2

Figure 3: Experiment Structure.
The overall procedure of the experiment is il-
lustrated in Figure 4. At the beginning of the
experiment, participants had to fill out the pre-
liminary questionnaire. Afterwards they had the
possibility to get to know the driving simulation
in a test drive lasting at least 4 minutes. After
the test drive, the participants completed a 4 min-
utes baseline drive and had to fill out the DALI
questionnaire afterwards to assess driving perfor-
237
mance without secondary task. Next, the partic-
ipants were shown the video of their first speech
dialog variant and became familiar with the SDS
by performing the 4 explorative tasks. Subse-
quently, participants performed the first SDS con-
dition (SDS Type 1) both with and without GUI.
After testing SDS Type 1, SDS Type 2 was intro-
duced by presenting its instruction video and again
the explorative tasks were performed. Participants
performed the second SDS condition (SDS Type
2) also with and without GUI. Finally, participants
completed a second baseline drive and filled out
the final questionnaire.
Prel
imin
aryI
nter
view
Test
 
Driv
e
Bas
eline
 
Driv
e 1 +
DAL
I
Vide
o SD
S Ty
pe 1
Trial
 
Boo
king
 
(4 exp
lorat
iveT
asks
)
Data
 
Colle
ction
SDS
 
Type
 
1
Vide
o SD
S Ty
pe 2
Trial
 
Boo
king
(4 exp
lorat
iveT
asks
)
Data
 
Colle
ction
SDS
 
Type
 
2
Bas
eline
 
Driv
e 2
Fina
l Inte
rview
Figure 4: Overall Procedure of the Experiment.
3.1.5 Dependent Variables
In the experiment, we collected several types of
data to evaluate the speech dialog and the driv-
ing performance data. During speech interaction
the SDS produces log files, which contain the link
to the recorded audio file of the spoken user ut-
terance, the speech recognizer result, the inter-
pretation of the natural language understanding
(NLU) module, and the text-to-speech (TTS) out-
put. Based on the log file, the whole speech di-
alog can be reconstructed. The driving simula-
tion OpenDS also produces log files at runtime,
which contain the steering wheel deviation for lat-
eral control and the reaction times for longitudinal
control for each recorded time frame. During the
experiment, the examiner was observing the test
procedure in order to take notes on task success.
Based on the collected data, the measures illus-
trated in Table 1 were computed in order to evalu-
ate the speech dialog and the driving performance.
A detailed description and definition of the mea-
sures can be found in (Mo?ller, 2005).
In this preliminary analysis, due to time con-
straints, only the first block of each participant
could be transcribed and analyzed. In this report,
Measure Data Source
TS Observations
Speech Dialog NoT SDS logs
Performance DD SDS logs
Measures CER SDS logs
Subjective Usability SASSI,
Assessment Final Interview
Driving MDev OpenDS logs
Performance Subjective Assessment DALI,
Measures of Cognitive Load Final Interview
Table 1: Evaluation Measures of the Experiment.
we focus on the SDS performance. Based on the
observations the task success (TS) of each speech
dialog is assessed. The speech dialog logs are used
to compute the Number of Turns (NoT) and the
dialog duration (DD) of each dialog. We assess
the concept error rate (CER) of each user utter-
ance within a dialog instead of the word error rate
(WER) since this value is crucial to a successful
speech dialog. A subjective usability assessment is
achieved by employing the SASSI questionnaire.
Based on the OpenDS logs we compute the mean
deviation (MDev) of the steering wheel. In the
next step, the reaction time, the DALI question-
naire and the final interview are analyzed.
Overall, we expect better usability evaluation
for the conversational dialog conditions compared
with the command-based condition. The partic-
ipants will accept the conversational dialog bet-
ter than the command-based dialog because if re-
flects the human-human communication. Further-
more, we expect the conversational dialog to dis-
tract less than the command-based dialog because
it is easier to control. Generally, a visual feed-
back makes it more comfortable to interact with
an SDS. Therefore, we expect the participants to
accept the SDS with GUI better than without GUI.
However, concerning the influence of the GUI on
the driving performance, we expect the GUI to
cause more driver distraction due to the glances
onto the GUI screen.
3.2 Results
In the following, the preliminary results concern-
ing SDS quality and driving performance are pre-
sented. In total, 48 command-based dialogs and
44 conversational dialogs were transcribed and an-
alyzed. First, the results of the speech dialog eval-
uation are described, followed by the results of
the driving performance evaluation. When com-
paring the two speech dialog strategies (?Comm?
vs. ?Conv?) dependent t-tests for paired exam-
ples have been applied. Concerning the compar-
ison of the 4 GUI conditions (?CommGUI? vs.
?CommNoGUI?, ?ConvGUI? vs. ?ConvNoGUI?)
238
the repeated measures anova test was applied. For
each comparison, a significance level ? =0,05 was
assumed.
3.2.1 Speech Dialog
In this Section, first, the results of the speech dia-
log performance measures are presented, followed
by the results of the questionnaires.
Task Success: In the first block of each experi-
ment, each participant had to solve 4 tasks while
data was recorded. Each of the 92 dialogs were
finished with a hotel booking. If the participant
booked a hotel, which did not match the task re-
quirements the task was annotated as failed. Fig-
ure 5 shows the percentage of solved tasks for
both speech dialog strategies (left) and addition-
ally split according to the two GUI conditions
(right). Using the command-based SDS prototype,
participants were able to solve 95,8% of the tasks.
93,8% of the tasks could be solved when using
the conversational prototype. Participants solved
tasks more effective when using the command-
based prototype with GUI than without GUI. In
contrast, the participants solved more tasks suc-
cessfully when using the conversational prototype
without GUI than with GUI. However, none of the
differences was found to be significant.
95,8 93,1 
100 
91,7 90,9 95,5 
50
60
70
80
90
100
Comm Conv CommGUI CommNoGUI ConvGUI ConvNoGUI
Ta
sk 
Su
cce
ss 
[%
] 
Figure 5: Overall TS rates.
Number of Turns: Figure 6 presents the aver-
age NoT. The high number of turns is due to the
list browsing the user has to perform in order to
find the matching hotel. Using the conversational
SDS prototype, significantly fewer dialog turns
were needed than using the command-based SDS
prototype (p=0,047). The conditions without GUI
needed less turns than the conditions with GUI.
However, no significant differences were found
when comparing the conditions with GUI with the
conditions without GUI.
Dialog Duration: In Figure 7 the average DD
is illustrated. The dialogs of the conversational
32,2  
29,7  
31,5 
32,8  
27,9  
30,3 
25
26
27
28
29
30
31
32
33
34
Comm Conv CommGUI CommNoGUI ConvGUI ConvNoGUI
Numbe
r Of Tu
rns
 
Figure 6: Average NoT per speech dialog.
speech dialogs were significantly shorter than the
command-based speech dialogs (p=0,003). Com-
paring the GUI conditions within one speech di-
alog strategy, it seems that participants using the
conversational speech dialog needed less time to
accomplish a task when they could use the GUI.
However, there was no significant difference re-
vealed. Concerning the GUI conditions of the
command-based dialog, no significant differences
could be found, too.
104,9  
91,6 
104,4  105,3 
81,2  
102  
0
20
40
60
80
100
120
Comm Conv CommGUI CommNoGUI ConvGUI ConvNoGUI
Dialog Du
ration (s
ec)
 
Figure 7: Average DD per speech dialog.
Concept Error Rate: The average CER per
dialog is significantly smaller in the command-
based speech dialog compared to the conversa-
tional speech dialog strategy (p=0,02) (see Figure
8). When comparing the GUI conditions within
one speech dialog strategy, it seems that less con-
cept errors occurred when the participants used the
SDS prototypes supported by a GUI. However, no
significant differences were found.
5,2  
8,2  
4,9  5,5 
6,4  
10 
0
2
4
6
8
10
12
Comm Conv CommGUI CommNoGUI ConvGUI ConvNoGUI
Concept 
Error Rate
 [%
] 
 
Figure 8: Average CER per speech dialog.
239
SASSI: The overall result of the SASSI ques-
tionnaire is illustrated in Figure 9. All SDS
achieve a positive usability assessment. The con-
versational dialog is slightly better accepted by the
user. It seems that the users accept the SDS sup-
ported by a GUI better than without a GUI. How-
ever, for none of the comparisons significant dif-
ferences were found.


















	




























	



Figure 9: Overall SASSI result per speech dialog.
3.2.2 Driving Performance
In this Section a preliminary driving performance
result is presented.
Mean Deviation: Figure 10 shows the MDev of
the baseline drive (left), both speech dialog strate-
gies (middle) and additionally split according to
the two GUI conditions (right). The MDev of the
baseline drive is 0,1. The MDev was significantly
smaller when the participants used the command-
based speech dialog (p=0,01) while driving com-
pared to the conversational dialog. No significant
differences were found when comparing the con-
ditions with GUI with the conditions without GUI.
0,1 0,1 
0,12  
0,1 0,1 
0,12  0,12  
0
0,02
0,04
0,06
0,08
0,1
0,12
0,14
Mea
n Devia
tion
 
Figure 10: Average MDev per speech dialog.
3.3 Discussion
The preliminary results show that the participants
were able to successfully finish the tasks with
both SDS prototype variants. All SDS proto-
types achieved a positive subjective usability as-
sessment. Although the CER is higher when using
the conversational dialog, it performs more effi-
cient than the command-based dialog which is due
to the possibility to input multiple parameters at
once. The MDev of the baseline drive is as high
as when using the command-based speech dialog
while driving. Usually, one would expect a bet-
ter driving performance when performing no sec-
ondary task. However, the ConTRe task is a quite
difficult task since it continuously mentally de-
mands the user. Therefore, the MDev is relatively
high when only the driving task is performed. The
conversational speech dialog distracts more from
driving than the command-based dialog. Using the
command-based dialog, the user is guided by the
system step-by-step, which makes it easier to use.
The mental demand when using the command-
based SDS might be lower and therefore, this dia-
log strategy might be less distractive.
Concerning the comparison of the GUI condi-
tions the results indicate that the conditions with
GUI are more user-friendly than the conditions
without GUI. However, we did not find any sig-
nificant differences, yet, since the data set is too
small when comparing the GUI conditions. When
the whole data set of the experiment is analyzed
further significances might be revealed.
4 Conclusions
This paper reports from a very recent driving sim-
ulation study and its preliminary results which are
conducted in order to compare different speech di-
alog strategies. The use of command-based and
conversational SDS prototypes while driving is
evaluated on usability and driving performance.
Different GUIs are designed in order to support
the respective dialog strategy the most and to eval-
uate the effect of the GUI on usability and driver
distraction. The preliminary results show that the
conversational speech dialog performs more effi-
cient than the command-based dialog. However,
the conversational dialog distracts more from driv-
ing than the command-based. Furthermore, the re-
sults indicate that an SDS supported by a GUI is
more efficient and better accepted by the user than
without GUI.
In the next step, the data set will be analyzed on
all mentioned usability and driving performance
measures. The different subdialog types of each
dialog will be investigated in detail on dialog per-
formance and speaking styles. Furthermore, cross-
links between subdialogs and the driving perfor-
mance measures are analyzed.
240
References
L. Devillers and H. Bonneau-Maynard. 1998. Eval-
uation of dialog strategies for a tourist information
retrieval system. In Proc. ICSLP, pages 1187?1190.
Driver Focus-Telematics Working Group. 2002. State-
ment of principles, criteria and verification pro-
cedures on driver interactions with advanced in-
vehicle information and communication systems.
Alliance of Automotive Manufacturers.
Governors Highway Safety Association. 2011. Dis-
tracted driving: What research shows and what
states can do. Technical report, U.S. Department of
Transportation.
H. Hofmann, U. Ehrlich, A. Berton, and W. Minker.
2012a. Speech interaction with the internet - a user
study. In Proceedings of Intelligent Environments,
Guanajuato, Mexico, June.
H. Hofmann, Anna Silberstein, U. Ehrlich, A. Berton,
and A. Mahr. 2012b. Development of speech-based
in-car hmi concepts for information exchange inter-
net apps. In Proceedings of International Workshop
on Spoken Dialogue Systems, Paris, France, Decem-
ber.
K. S. Hone and R. Graham. 2001. Subjective assess-
ment of speech-system interface usability. In Pro-
ceedings of Eurospeech.
Angela Mahr, Michael Feld, Mohammad Mehdi
Moniri, and Rafael Math. 2012. The ConTRe (con-
tinuous tracking and reaction) task: A flexible ap-
proach for assessing driver cognitive workload with
high sensitivity. In Adjunct Proceedings of the 4th
International Conference on Automotive User Inter-
faces and Interactive Vehicular Applications, pages
88?91, Portsmouth, United States.
Stefan Mattes. 2003. The lane-change-task as a tool
for driver distraction evaluation. Proceedings of
IGfA, pages 1?30.
Sebastian Mo?ller. 2005. Parameters describing the in-
teraction with spoken dialogue systems. ITU-T Rec-
ommendation Supplement 24 to P-Series, Interna-
tional Telecommunication Union, Geneva, Switzer-
land, October. Based on ITU-T Contr. COM 12-17
(2009).
Hartmut Mutschler, Frank Steffens, and Andreas Ko-
rthauer. 2007. Final report on multimodal exper-
iments - part 1: Evaluation of the sammie system.
d6.4. talk public deliverables. Technical report.
Annie Pauzie. 2008. Evaluating driver mental work-
load using the driving activity load index (DALI).
In Proceedings of European Conference on Human
Interface Design for Intelligent Transport Systems,
pages 67?77.
Matthias Peissner, Vanessa Doebler, and Florian
Metze. 2011. Can voice interaction help reducing
the level of distraction and prevent accidents? meta-
study on driver distraction and voice interaction.
Technical report, Fraunhofer-Institute for Industrial
Engineering (IAO) and Carnegie Mellon University.
241
