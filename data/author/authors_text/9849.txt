Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 92?99, Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
The TermiNet Project: an Overview 
 
 
Ariani Di Felippo 
Interinstitutional Center for Research and Development in Computational Linguistics (NILC)/ 
Research Group of Terminology (GETerm), Federal University of S?o Carlos (UFSCar) 
Rodovia Washington Lu?s, km 235 - SP-310 
CP 676, 13565-905, S?o Carlos, SP, Brazil 
ariani@ufscar.br 
 
  
 
 
Abstract 
Linguistic resources with domain-specific 
coverage are crucial for the development of 
concrete Natural Language Processing (NLP) 
systems. In this paper we give a global intro-
duction to the ongoing (since 2009) TermiNet 
project, whose aims are to instantiate a gener-
ic NLP methodology for the development of 
terminological wordnets and to apply the in-
stantiated methodology for building a termi-
nological wordnet in Brazilian Portuguese. 
1 Introduction 
In knowledge-based Natural Language Processing 
(NLP) systems, the lexical knowledge database is 
responsible for providing, to the processing mod-
ules, the lexical units of the language and their 
morphological, syntactic, semantic-conceptual and 
even illocutionary properties (Hanks, 2004). 
In this scenario, there is an increasing need of 
accurate general lexical-conceptual resources for 
developing NLP applications. 
A revolutionary development of the 1990s was 
the Princeton WordNet (WN.Pr) (Fellbaum, 1998), 
an online reference lexical database built for 
North-American English that combines the design 
of a dictionary and a thesaurus with a rich ontolog-
ical potential. 
Specifically, WN.Pr is a semantic network, in 
which the meanings of nouns, verbs, adjectives, 
and adverbs are organized into ?sets of cognitive 
synonyms? (or synsets), each expressing a distinct 
concept. Synsets are interlinked through concep-
tual-semantic (i.e., hypernymy1/hyponymy2, holo-
nymy/meronymy, entailment3, and cause4) and 
lexical (i.e., antonymy) relations. Moreover, 
WN.Pr encodes a co-text sentence for each word-
form in a synset and a concept gloss for each syn-
set (i.e., an informal lexicographic definition of the 
concept evoked by the synset). 
The success of WN.Pr is largely due to its ac-
cessibility, linguistic adequacy and potential in 
terms of NLP. Given that, WN.Pr serves as a mod-
el for similarly conceived wordnets in several lan-
guages. In other words, the success of WN.Pr has 
determined the emergence of several projects that 
aim the construction of wordnets for other lan-
guages than English or to develop multilingual 
wordnets (the most important project in this line is 
EuroWordNet) (Vossen, 2002). 
Many recent projects with the objective of (i) in-
tegrating generic and specialized wordnets (e.g., 
Magnin and Speranza, 2001; Roventini and Mari-
nelli, 2004; Bentivogli et al, 2004), (ii) enriching 
generic wordnets with terminological units (e.g., 
Buitelaar and Sacaleanu, 2002) or (iii) constructing 
terminological wordnets (e.g.: Sagri et al, 2004; 
Smith and Fellbaum, 2004) have shown that con-
                                                        
1 The term Y is a hypernym of the term X if the entity denoted 
by X is a (kind of) entity denoted byY. 
2 If the term Y is a hypernym of the term X then the term X is 
a hyponym of Y. 
3 The action A1 denoted by the verb X entails the action A2 
denoted by the verb Y if A1 cannot be done unless A2 is, or 
has been, done 
4 The action A1 denoted by the verb X causes the action A2 
denoted by the verb Y. 
92
crete NLP application must be able to comprehend 
both expert and non-expert vocabulary. 
Despite the existence of a reasonable number of 
terminological wordnets, there is no a general me-
thodology for building this type of lexical data-
base. Thus, motivated by this gap and by the fact 
that Brazilian Potuguese (PB) is a resource-poor 
language, the two-years TermiNet project has been 
developed since September 2009. 
This paper gives an overview of the TermiNet 
project. Accordingly, in Section 2 we brief de-
scribe the original WN.Pr design that motivated the 
project. In Section 3 we present the aims of the 
TermiNet project and its methodological approach. 
In Section 4 we depict the current state of the 
project. In Section 5 we describe future work, and 
in Section 6 we outline potential points for collabo-
ration with researchers from the rest of the Ameri-
cas. 
2 Princeton WordNet and its Design 
WN.Pr contains information about nouns, verbs, 
adjectives and adverbs in North-American English 
and is organized around the notion of a synset. As 
mentioned, a synset is a set of words with the same 
part-of-speech that can be interchanged in a certain 
context. For example, {car; auto; automobile; ma-
chine; motorcar} form a synset because they can be 
used to refer to the same concept. A synset is often 
further described by a concept gloss5, e.g.: ?4-
wheeled; usually propelled by an internal combus-
tion engine?. 
 Finally, synsets can be related to each other by 
the conceptual-semantic relations of hyperonymy/ 
hyponymy, holonymy/meronymy, entailment and 
cause, and the lexical relation of antonymy. 
 In the example, taken from WN.Pr (2.1), the 
synset {car; auto; automobile; machine; motorcar} 
is related to: 
 
(i) more general concepts or the hyperonym syn-
set: {motor vehicle; automotive vehicle}; 
(ii) more specific concepts or hyponym synsets: 
e.g. {cruiser; squad car; patrol car; police car; 
prowl car} and {cab; taxi; hack; taxicab}; and 
(iii) parts it is composed of: e.g. {bumper}; {car 
door}, {car mirror} and {car window}. 
                                                        
5 An informal lexicographic definition of the concept evoked 
by the synset. 
WN.Pr also includes an English co-text sen-
tence for each word-form in a synset, and a seman-
tic type for each synset. 
Based on WN.Pr design, Brazilian Portuguese 
WordNet (WordNet.Br or WN.Br) project 
launched in 2003 departed from a previous lexical 
resource: the Brazilian Portuguese Thesaurus (Di-
as-da-Silva et al 2002). The original WN.Br data-
base is currently being refined, augmented, and 
upgraded. The improvements include the encoding 
of the following bits of information in to the data-
base: (a) the co-text sentence for each word-form 
in a synset; (b) the concept gloss for each synset; 
and (c) the relevant language-independent hierar-
chical conceptual-semantic relations. 
The current WN.Br database presents the fol-
lowing figures: 11,000 verb forms (4,000 synsets), 
17,000 noun forms (8,000 synsets), 15,000 adjec-
tive forms (6,000 synsets), and 1,000 adverb forms 
(500 synsets), amounting to 44,000 word forms 
and 18,500 synsets (Dias-da-Silva et al 2008). 
3 The TermiNet Project 
The TermiNet (?Terminological WordNet?) 
project started in September 2009 and shall be fi-
nished finish in August 2011. It has been devel-
oped in the laboratory of the Research Group of 
Terminology6 (GETerm) in Federal University of 
S?o Carlos (UFSCar) with the collaboration of the 
Interinstitutional Center for Research and Devel-
opment in Computational Linguistics7 
(NILC/University of S?o Paulo) researchers. 
The TermiNet project has two main objectives. 
The first is to instantiate the generic NLP metho-
dology, proposed by Dias-da-Silva (2006), for de-
veloping terminological databases according to the 
WN.Pr model. Such methodology distinguishes 
itself by conciliating the linguistic and computa-
tional facets of the NLP researches. The second is 
to apply the instantiated methodology to build a 
terminological wordnet or terminet8 in BP, since 
BP is a resource-poor language in NLP for which 
domain-specific databases in wordnet format have 
not been built yet. 
 It is important to emphasize that the main 
terminological resources in BP, which are availa-
                                                        
6 http://www.geterm.ufscar.br/ 
7 http://www.nilc.icmc.usp.br 
8 In the TermiNet project, a terminological wordnet database 
is called ?terminet?. 
93
ble through the OntoLP9 website, are in fact (for-
mal) ontologies or taxonomies. There is no 
nological WordNet-like database in BP. 
In order to achieve its objectives, TermiNet has, 
apart from the project leader (Prof. Ariani Di Fe-
lippo), an interdisciplinary team that includes six 
undergraduate students: five from Linguistics and 
one from Computer Science courses. The Linguis-
tics students are responsible for specific linguistic 
tasks in the project, such as: (i) corpus compila-
tion, (ii) candidate terms extraction, (iii) synonymy 
identification, and (iv) semantic-conceptual rela-
tions extraction (hypernymy/hyponymy). The res-
ponsability of the Computer Science student is to 
support the automatic processing related to the lin-
guistic (e.g., tagging, parsing, term extraction, 
etc.) and linguistic-computational domains during 
the initial stages of the project. 
Moreover, the project counts with the collabora-
tion of four PhD researchers from NILC. Specifi-
cally, TermiNet has the support of Prof. Gladis 
Maria de Barcellos Almeida, a specialist in termi-
nological research and the coordinator of GETerm; 
Prof. Maria da Gra?as Volpe Nunes, the coordina-
tor of NILC and one of the most important Brazili-
an NLP researchers; Prof. Sandra Aluisio, a 
specialist in corpus construction, and Prof. Thiago 
Pardo, who has interests in the development of lex-
ical resources for the automatic processing of BP. 
3.1 Instantiation of the NLP Tree-Domain 
Methodology 
Based on Expert Systems development, Dias-da-
Silva (2006) established a three-domain approach 
methodology to develop any research in NLP do-
main, assuming a compromise between Human 
Language Technology and Linguistics (Dias-da-
Silva, 1998). 
 The linguistic-related information to be compu-
tationally modeled is likened to a rare metal. So, it 
must be "mined", "molded", and "assembled" into 
a computer-tractable system (Durkin, 1994). Ac-
cordingly, the processes of designing and imple-
menting a terminet lexical database have to be 
developed in the following complementary do-
mains: the linguistic domain, the linguistic-
computational domain, and implementational or 
computational domain. 
 
                                                        
9 http://www.inf.pucrs.br/~ontolp/downloads.php 
(a) The Linguistic-related Domain 
In this domain, the lexical resources and the lexi-
cal-conceptual knowledge are mined. More specif-
ically, the research activities in the linguistic 
domain are divided in two processes: the selection 
of the lexical resources for building the terminet 
database, and the specification of the lexical-
conceptual knowledge that characterize a terminet. 
 The linguist starts off these procedures by deli-
mitating the specialized domain that will be en-
coded in wordnet format. 
 According to Almeida and Correia (2008), deal-
ing with an entire specialized domain is a very 
problematic task because the domains (e.g.: Mate-
rials Engineering) in general are composed of sub-
domains (e.g.: Ceramic Materials, Polymers and 
Metals) with different characteristics, generating a 
large universe of sources from which the lexical-
conceptual knowledge will have to be mined. 
 Consequently, the authors present some criteria 
that may lead to delimitate a specialized domain: 
(i) the interest of the domain experts by termino-
logical products (in this case, by a terminet); (ii) 
the relevance of the domain in the educational, so-
cial, political, economic, scientific and/or technol-
ogical scenarios, and (iii) the availability of 
specialized resources in digital format from which 
the lexical-conceptual knowledge will be extracted. 
After delimitating the domain, it is necessary to 
select the lexical resources describe in (iii). Ac-
cording to Rigau (1998), the two main sources of 
information for building wide-coverage lexicons 
for NLP systems are: structured resources (e.g.: 
conventional monolingual and bilingual dictiona-
ries, thesauri, taxonomies, vocabularies, etc.) and 
unstructured resources (i.e., corpora10). 
 Due to the unavailability of reusing structured 
resources, the corpora have become the main 
source of lexical knowledge (Nascimento, 2003; 
Agbago and Barri?re, 2005; Cabr? et al, 2005; 
Almeida, 2006). The increasing use of corpora in 
terminological researches is also due to the fact 
that ?el car?cter de t?rmino no se da per se, sino en 
funci?n del uso de una unidad l?xica en un contex-
to expresivo y situacional determinado? (Cabr?, 
1999: 124). Thus, in the TermiNet project, the cor-
                                                        
10 ?A corpus is a collection of pieces of language text in elec-
tronic form, selected according to external criteria to 
represent, as far as possible, a language or language variety as 
a source of data for linguistic research? (Sinclair, 2005). 
94
pus is considered the main lexical resource that can 
be used to construct a terminet. 
 Although there are available several specialized 
corpora, the development of a terminet of certain 
domains may require the compilation of a corpus. 
 Based on the assumptions of Corpus Linguistics 
(Aluisio and Alemida, 2007), the construction of a 
corpus must follow three steps: (i) the corpus pro-
jection, i.e., the specification of the corpus typolo-
gy according to the research purposes; (ii) the 
compilation of the texts that will compose the cor-
pus, and (iii) the pre-processing of the corpus (i.e., 
conversion, clean-up, manipulation, and annotation 
of the texts).  
 From the corpus, the specialized knowledge will 
be extracted, i.e., the terminological units (or 
terms), the lexical relations, and the conceptual-
semantic relations11. 
 As mentioned in previous sections, the lexical 
units are organized into four syntactic categories in 
WN.Pr: verbs, nouns, adjectives and adverbs. Giv-
en the relevance of nouns in the organization of 
any terminology (i.e., the set of all terms related to 
a given subject field or discipline), we decided to 
restrict the construction of a terminet to the catego-
ry of nouns. In other words, a terminet database, in 
principle, will only contain information about con-
cepts lexicalized by nouns. Additionally, it will 
only encode the hyperonymy/hyponymy relations, 
which are the most important conceptual-semantic 
relations between nouns. The co-text sentence for 
each word-form in a synset and the concept gloss 
for each synset will not be focused in building a 
terminet. 
 As the TermiNet a corpus-based project, we will 
apply approaches and strategies to automatically 
recognize and extract candidate terms and relations 
from corpus. 
 In order to better understand the automatic can-
didate terms and extraction, it can be useful to 
identify two mainstream approaches to the prob-
lem. In the first approach, statistical measures have 
been proposed to define the degree of termhood of 
candidate terms, i.e., to find appropriate measures 
that can help in selecting good terms from a list of 
candidates. In the second approach, computational 
terminologists have tried to define, identify and 
recognize terms looking at pure linguistic proper-
                                                        
11 The glosses and co-text sentences will not be specificied in 
the TermiNet projet. 
ties, using linguistic filtering techniques aiming to 
identify specific syntactic term patterns (Bernhard, 
2006; Pazienza et al, 2005; Cabr? et al, 2001). 
 Once extrated, the candidate terms have be vali-
dated. Two validation estrategies will be consi-
dered in the TermiNet project. The first strategy 
consists on manually validating by domain experts. 
The second consists on automatically comparing 
the list of candidate terms with a list of lexical un-
ities extracted from a general corpus in BP. 
 The automatic acquisition of hyper-
onym/hyponymy relation from corpus is common-
ly based on linguistic methods. These methods 
look for linguistic clues that indisputably indicate 
the relation of interest (Hearst, 1992). The linguis-
tic clues are basically lexico-syntactic patters such 
as: [NP such {NP,}*{(or|and)} NP] (e.g., ?works 
by such authors as Herrick, and Shakespeare?). 
The hierarchical relations extrated from corpus are 
commonly validated by domain experts. 
 
(b) The Linguistic-Computational Domain 
In this domain, the overall information selected 
and organized in the preceding domain is molded 
into a computer-tractable representation; in the 
case of a WordNet-like database, the computer-
tractable representation is based on the notions of: 
 word form ? a orthographic representation of an 
individual word or a string of individual words 
joined with underscore characters; 
 synset ? a set of words built on the basis of the 
notion of synonymy in context, i.e. word inter-
changeability in some context; 
 lexical matrix ? associations of sets of word 
forms and the concepts they lexicalize; 
 relational pointers ? formal representations of 
the relations between the word forms in a syn-
set and other synsets; synonymy of word forms 
is implicit by inclusion in the same synset; 
hyperonymy always relates one synset to 
another, and is an example of a semantic rela-
tion; hyperonymy, in particular, is represented 
by reflexive pointers (i.e., if a synset contains a 
pointer to another synset, the other synset 
should contain a corresponding reflexive poin-
ter back to the original synset). 
 
(c) The Computational Domain 
In this domain, the computer-tractable representa-
tions are assembled by utilities (i.e., a computa-
tional tool to create and edit lexical knowledge). In 
95
other words, it is generated, in this domain, the 
terminet database. The software tool that we will 
use to generate the terminet database is under in-
vestigation. 
4 TermiNet: Past and Current Stages of 
Development 
The project, which started in September 2009, is 
still in its early stages. Consequently, the research 
tasks that have been developed so far are those re-
lated to the linguistic domain. As described in Sec-
tion 3.1a, there are several linguistic tasks in the 
TermiNet project. Two of them ? the delimitation 
of the specialized domain and the corpus projec-
tion ? are completed. In subsections 4.1 and 4.2, 
we present these finished processes and in 4.3 we 
focus on the current activity. 
4.1 Delimitation of the specialized domain 
DE is conventionally defined as "any educational 
or learning process or system in which the teacher 
or instructor is separated geographically or in 
time from his or her students or educational re-
sources?. 
 According to the second Brazilian Yearbook of 
Statistics on Open and Distance Education 
(Anu?rio Brasileiro Estat?stico de Educa??o Aberta 
e a Dist?ncia12), in 2007 there were approximately 
2,5 millions of students enrolled in accredited DE 
courses, from basic to graduate education, in 257 
accredited institutions. The number of students in 
DE courses has grown 24.9% in relation to 2006. 
Thus, we can see the relevance of the DE modality 
in Brazil. Despite the relevance of the DE in the 
Brazilian educational (and political) scenario, there 
is no a lexical-conceptual representation of this 
domain, especially in a machine-readable format. 
 Consequently, the instantiated methodology will 
be validated by building DE.WordNet (DE.WN), a 
specialized wordnet of the Distance Education (or 
Distance Learning) domain in BP. The construc-
tion of such database has been supported by do-
main experts from the ?Open University of Brazil? 
(Universidade Aberta do Brasil ? UAB) project of 
the Federal University of S?o Carlos (UFSCar). 
 DE.WN can be integrated into the wordnet lexi-
cal database for BP, the WordNet.Br (Dias-da-
                                                        
12 http://www.abraead.com.br/anuario/anuario_2008.pdf 
Silva et al, 2008), enriching it with domain specif-
ic knowledge. 
4.2 Corpus projection 
Following the assumptions of Corpus Linguistics 
described in Section 3, the corpus of DE domain 
has been constructed according to the steps: (i) 
corpus projection, (ii) corpus compilation, and (iii) 
the pre-processing of the texts. 
 The corpus typology in the TermiNet project 
was specified based on: (i) the conception of ?cor-
pus?, (ii) the type of lexical resource to be built, 
and (iii) the project decisions (Di Felippo and Sou-
za, 2009). 
 The corpus definition or conception is common-
ly related to three criteria: representativeness, bal-
ance and authenticity. 
 According to the representativeness criterion, 
we have been compiled a representative corpus of 
the DE domain. There have been many attempts to 
set the size, or at least establish a minimum num-
ber of texts, from which a specialized corpus may 
be compiled. To satisfy the representativeness cri-
terion, we have been constructed a medium-large 
corpus, with at least 1 million of words. 
 In a specialized corpus, it is important to gather 
texts from different genres (i.e. technical-scientific, 
scientific divulgation, instructional, informative, 
and technical-administrative) and media (i.e, 
newswire, books, periodicals, etc.). Following the 
balance and authenticity criteria, we have been 
constructed a corpus with a balanced number of 
real texts per genre. 
 Besides, the format of the lexical database (i.e. a 
terminet) determined some characteristics of the 
corpus. Specifically, the corpus has to be syn-
chronic/ contemporary, since a wordnet (termino-
logical or not) encodes synchronic lexical-
conceptual knowledge. The corpus has only to 
store written texts, since wordnets are lingwares 
for written language processing. Finally, the cor-
pus in the TermiNet project has only to store texts 
from a specialized domain and in one language. 
Additionally, some project decisions deter-
mined other characteristics of the corpus. Two ini-
tial decisions in the project were: (i) to apply semi-
automatic methods of lexical-conceptual know-
ledge extraction, and (ii) to share the resources and 
results of the TermiNet project with Computational 
Linguistics community. As a consequency of the 
project decision described in (i), the corpus will be 
96
annotated with part-of-speech (PoS) information, 
since some automatic extraction methods require 
it. As a consequency of the decision presented in 
(ii), the corpus will be available and usable as 
widely as possible on the web. 
Finally, we also decided that once the corpus 
has been assembled, it will not be changed until the 
first version of DE.WN is ready. 
 Based on the typology proposed by Giouli and 
Peperidis (2002), the Table 1 summarizes the cha-
racteritics of the corpus previously described. 
 
Modality Written corpus 
Text Type Written corpus 
Medium Newspapers, books, jour-
nals, manuals and others 
Language coverage Specialized corpus 
Genre/register Technical-scientific, scien-
tific divulgation, instruc-
tional, informative and, 
technical-administrative 
Language variables Monolingual corpus 
Markup Annotated corpus (PoS 
annotation)  
Production Com-
munity 
Native speakers 
Open-endedness Closed corpus 
Historical variation Synchronic corpus 
Availability Online corpus 
Table 1. The corpus design. 
 
The specialized domain and corpus typology were 
specified by the undergraduate student responsible 
for the corpus compilation under the supervision of 
a PhD in Linguistics (leader of the project). 
4.3 Corpus compilation 
Currently, one undergraduate student from Lin-
guistics has been compiled the corpus. Specifical-
ly, the corpus compilation comprises two 
processes: (i) the selection of resources and (ii) the 
collect of texts from these resources. 
 In the TermiNet project, the web is the main 
source for collecting texts of DE. The choice of 
web reflects the fact that web has become an un-
precedented and virtually inexhaustible source of 
authentic natural language data for researchers in 
linguistics. 
 Although there are many computational tools 
that assist in gathering a considerable amount of 
texts on the web, the selection/collection of texts 
has been followed a manual process, which is 
composed of three steps: (i) to access a webpage 
whose content is important for compiling the cor-
pus, (ii) to search the texts on the webpage by 
search queries as ?distance education? and ?dis-
tance learning?, and (iii) to save the text files on 
the computer. 
In the pre-processing step, the text files in a non-
machine readable format (e.g. pdf) are manually 
converted to text format (txt), which is readable by 
machines. This process is important because the 
lexical-conceptual knowledge will be 
(semi)automatically extracted from the corpus, and 
the extraction tools require a corpus whose texts 
are in txt format. 
Data corrupted by the conversion or even unne-
cessary to the research (e.g. references, informa-
tion about filliation, etc.) are excluded during the 
cleaning process. After that, the metadata or exter-
nal information (e.g. authorship, publication de-
tails, genre and text type, etc.) on each text are 
being automatically annotated and encoded in a 
header. In the TermiNet project, we are using the 
header editor available at the ?Portal de Corpus? 
website13. 
5 Future Work 
According to the three-domain methodology, fu-
ture steps will involve the following tasks of the 
linguistic domain: candidate terms and relations 
extraction (and validation). 
 In the TermiNet project, two specific software 
tools constructed based on lingustic approaches 
will be used to extract candidate terms from the 
DE corpus: EXATOLP (Lopes et al, 2009) and On-
toLP (Ribeiro Jr., 2008). Additionally, we intend to 
extract the terms from corpus using the NSP 
(Ngram Statistics Package) tool (Bannerjee and 
Pedersen, 2003), i.e., a flexible and easy-to-use 
software tool that supports the identification and 
analysis of Ngrams. 
 To extract the hyperonymy and hyponymy rela-
tions, we will also use the OntoLP, which is a tool, 
actually a plug-in, for the ontologies editor 
Prot?g?14, a widely used editor in the scientific 
community and which gives support to the con-
struction of ontologies. The process of automatic 
                                                        
13 http://www.nilc.icmc.usp.br:8180/portal/ 
14 http://protege.stanford.edu/ 
97
ontology construction in the OntoLP tool also en-
globes the identification of hierarchical relation 
between the terms. 
 The synonymy relation will be also recognized 
and extracted automatically from the corpus. How-
ever, the automatic extraction method of such lexi-
cal relation is still under investigation. 
After the acquisition of all lexical-conceptual 
information, we will develop the tasks or processes 
of the linguistic-computational and computational 
domains. 
 Among the expected results of the TermiNet 
projet are: (i) a methodological framework for 
building a specific type of lingware, i.e. termino-
logical wordnets; (ii) a specialized corpus of the 
DE domain; (iii) a terminological lexical database 
based on the WN.Pr format of the DE domain. 
Moreover, there is the possibility of extending the 
WN.Br database through the inclusion of specia-
lized knowledge. 
Besides the benefits to NLP domain, the 
DE.WN may also contribute to the development of 
standard terminographic products (e.g., glossary, 
dictionary, vocabulary, etc.), of the DE domain 
since the organization of the lexical-conceptual 
knowledge is an essential step in building such 
products. 
6 Collaborative Opportunities 
We consider our experience in developing a termi-
net in BP as the major contribution that we can 
offer to other researchers in Latin America. Since 
the resources (i.e., corpus and lexical database) and 
tools (i.e., terms and relations extractors) that we 
have been used are language-dependent, they can-
not be used directly for Spanish and English. But, 
we are willing to share our expertise on (i) compil-
ing a terminological corpus, (ii) automatically ex-
tracting lexical-conceptual knowledge from 
corpus, and (iii) constructing a terminet database in 
order to develop similar projects for Spanish and 
English. 
 We are really interested in actively taking part in 
joint research projects that aim to construct termi-
nological lexical database for Spanish or English, 
especially in wordnet format. 
 Collaboration of researchers from the USA that 
were directly involved in the development of 
wordnet databases (terminological or not), willing 
to share their experience and tools, would be wel-
come. 
 We would appreciate collaboration from re-
searchers in the USA specifically in relation to 
computational programs or software tools used in 
building WordNet-like lexical database, which are 
responsible for the computer-tractable representa-
tion described in 3.1(b). The current WN.Br edit-
ing tool, which was originally designed to aid the 
linguist in carrying out the tasks of building syn-
sets, selecting co-text sentences from corpora, and 
writing synset concept glosses, has been modified 
to aid the linguistic in carrying out the task of en-
coding conceptual relations. However, this editor is 
just able to deal with the hypernymy/hyponymy 
relations when they are inherited from WN.Pr 
through a conceptual-semantic alignment strategy 
(Dias-da-Silva et al 2008). So, the WN.Br editor is 
not the most appropriate tool to TermiNer project 
tasks. Consequently, contributions to develop ?a 
kind of? Grinder15 for TermiNet would be wel-
come. We would also appreciate collaboration 
from re-searchers in the USA in relation to metho-
dological approaches to enriching generic word-
nets with terminological units. 
Acknowledgments 
We thank the Brazilian National Council for Scien-
tific and Technological Development (CNPq) 
(471871/2009-5), and the State of S?o Paulo Re-
search Foundation (FAPESP) (2009/06262-1) for 
supporting the TermiNet project. We also thank the 
NAACL HLT Young Investigators Workshop refe-
rees, who helped make this paper better. 
References 
Adriana Roventini and Rita Marinelli. 2004. Extending 
the Italian Wordnet with the specialized language of 
the maritime domain. In: Proceedings of the 2nd In-
ternational Global Wordnet Conference. Masaryk 
University, Brno, 193-198. 
Akakpo Agbago and Caroline Barri?re. 2005. Corpus 
construction for Terminology. In: Proceedings of the 
Corpus Lingustics Conference. Birmingham, 14-17. 
Bento Carlos Dias-da-Silva. 2006. Bridging the gap 
between linguistic theory and natural language 
processing. In: Proceedings of the 16th International 
                                                        
15 This is the most important program used in building WN.Pr. 
Lexicographers make their additions and changes in the lexi-
cal source files, and the Grinder takes those files and converts 
them into a lexical database (in wordnet format). 
98
Congress of Linguistics, 1997. Oxford: Elsevier 
Sciences, 1998, 1-10. 
Bento Carlos Dias-da-Silva. 2006. O estudo lingu?stico-
computacional da linguagem. Letras de Hoje, 41(2): 
103-138. 
Bento Carlos Dias-da-Silva, Ariani Di Felippo and Ma-
ria G. V. Nunes. 2008. The automatic mapping of 
Princeton Wordnet lexical-conceptual relations onto 
the Brazilian Portuguese Wordnet database. In: Pro-
ceedings of the 6th LREC. Marrakech, Morocco. 
Bento Carlos Dias-da-Silva, Mirna Fernanda de Olivei-
ra, H?lio Roberto de Moraes. 2002. Groundwork for 
the development of the Brazilian Portuguese Word-
net. In: Proceedings of the 3rd International Confe-
rence Portugal for Natural Language Processing 
(PorTal). Faro, Portugal. Berlin: Springer-Verlag, 
189-196. 
Bernardo Magnini and Manuela Speranza. 2001. Inte-
grating generic and specialized wordnets. In: Pro-
ceedings of the Conference on Recent Advances in 
Natural Language Processing. Bulgaria. 
Christiane Fellbaum (ed.). 1998. Wordnet: an electronic 
lexical database. The MIT Press, Ca, MA: 423p. 
Delphine Bernhard. 2006. Multilingual term extraction 
from domain-specific corpora using morphological 
structure. In: Proceedings of the 11th European 
Chapter Meeting of the ACL, Trento, Italy, 171-174. 
German Rigau Claramunt. 1998. Automatic acquisition 
of lexical knowledge from MRDs. PhD Thesis. De-
partament de Llenguatges i Sistemes Inform?tics, 
Barcelona. 
Gladis Maria Barcellos de Almeida. 2006. A Teoria 
Comunicativa da Terminologia e a sua pr?tica. Alfa, 
50:81-97 
Gladis Maria de Barcellos Almeida and Margarita 
Correia. 2008. Terminologia e corpus: rela??es, 
m?todos e recursos. In: Stella E. O. Tagnin and Oto 
Ara?jo Vale (orgs.). Avan?os da Ling??stica de Cor-
pus no Brasil. 1 ed. Humanitas/FFLCH/USP; S?o 
Paulo, volume 1, 63-93. 
Gladis Maria Barcellos de Almeida, Sandra Maria Alui-
sio and Leandro H. M. Oliveira. 2007. O m?todo em 
Terminologia: revendo alguns procedimentos. In: 
Aparecida N. Isquerdo and Ieda M. Alves. (orgs.). 
Ci?ncias do l?xico: lexicologia, lexicografia, termi-
nologia. 1 ed. Editora da UFMS/Humanitas: Campo 
Grande/S?o Paulo, volume 3, 409-420. 
John Durkin. 1994. Expert Systems: Design and Devel-
opment. Prentice Hall International, London, 800p. 
John Sinclair, J. 2005. Corpus and text: basic principles. 
In: Martin Wynne (ed.). Developing linguistic corpo-
ra: a guide to good practice. Oxbow Books: Oxford, 
1-16. Available at http://ahds.ac.uk/linguistic-
corpora/ 
Lucelene Lopes, Paulo Fernandes, Renata Vieira and 
Gustavo Fedrizzi. 2009. ExATOlp - an automatic 
tool for term extraction from Portuguese language 
corpora. In: Proceedings of the LTC?09, Poznam, 
Poland. 
Luisa Bentivogli, Andrea Bocco and Emanuele Pianta. 
2004. ArchiWordnet: integrating Wordnet with do-
main-specific knowledge. In: Proceedings of the 2nd 
International Global Wordnet Conference. Masaryk 
University, Brno, 39-47. 
Luiz Carlos Ribeiro Jr. 2008. OntoLP: constru??o semi-
autom?tica de ontologias a partir de textos da l?ngua 
portuguesa. MSc Thesis, UNISINOS, 131p. 
Maria Fernanda Bacelar do Nascimento. 2003. O papel 
dos corpora especializados na cria??o de bases termi-
nol?gicas. In: I. Castro and I. Duarte (orgs.). Raz?es 
e emo??es, miscel?nea de estudos em homenagem a 
Maria Helena Mateus. Imprensa Nacional-Casa da 
Moeda: Lisboa, volume II, 167-179. 
Maria Tereza Cabr?. 1999. La terminolog?a: represen-
taci?n y comunicaci?n: elementos para una teor?a de 
base comunicativa y otros art?culos. Institut Univer-
sitari de Lingu?stica Aplicada: Barcelona. 
Maria Tereza Cabr?, Anne Condamines and Fidelia 
Ibekwe-SanJuan. 2005. Application-driven terminol-
ogy engineering. Terminology, 11(2):1-19. 
Maria Tereza Cabr?, Rosa Estop? and Jordi Vivaldi 
Palatresi. 2001. Automatic term detection: a review 
of current systems, In: Didier Bourigault et al (eds.). 
Recent Advances in Computational Terminology. 
John Benjamins Publishing Co: Amsterdam & Phila-
delphia, 53-87. 
Maria Teresa Pazienza, Marco Pennacchiotti and Fabio 
Massimo Zanzotto. 2005. Terminology extraction: an 
analysis of linguistic and statistical approaches. Stu-
dies in Fuzziness and Soft Computing, 185:255-280. 
Maria Teresa Sagri, Daniela Tiscornia and Francesca 
Bertagna. 2004. Jur-Wordnet. In: Proceedings of the 
2nd International Global Wordnet Conference. Ma-
saryk University, Brno, 305-310. 
Marti A. Hearst, M. 1992. Automatic acquisition of 
hyponyms from large text corpora. In: Proceedings 
14th of the International Conference on Computa-
tional Linguistics. Nantes, 539-545. 
Paul Buitelaar and Bogdan Sacaleanu. 2002. Extending 
synsets with medical terms. In: Proceedings of the 
1st International Global Wordnet Conference. My-
sore, India, 2002. 
Piek Vossen (ed.). 2002. EuroWordnet general docu-
ment (Version 3?Final). Available at: 
http://www.vossen.info/docs/2002/EWNGeneral.pdf. 
Satanjeev Banerjee and Ted Pedersen. 2003. The De-
sign, Implementation, and Use of the Ngram Statis-
tics Package. In: Proceedings of the Fourth 
International Conference on Intelligent Text 
Processing and Computational Linguistics. Mexico 
City. 
99
