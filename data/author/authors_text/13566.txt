Proceedings of the ACL 2010 Conference Short Papers, pages 184?188,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning Common Grammar from Multilingual Corpus
Tomoharu Iwata Daichi Mochihashi
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
{iwata,daichi,sawada}@cslab.kecl.ntt.co.jp
Hiroshi Sawada
Abstract
We propose a corpus-based probabilis-
tic framework to extract hidden common
syntax across languages from non-parallel
multilingual corpora in an unsupervised
fashion. For this purpose, we assume a
generative model for multilingual corpora,
where each sentence is generated from a
language dependent probabilistic context-
free grammar (PCFG), and these PCFGs
are generated from a prior grammar that
is common across languages. We also de-
velop a variational method for efficient in-
ference. Experiments on a non-parallel
multilingual corpus of eleven languages
demonstrate the feasibility of the proposed
method.
1 Introduction
Languages share certain common proper-
ties (Pinker, 1994). For example, the word order
in most European languages is subject-verb-object
(SVO), and some words with similar forms are
used with similar meanings in different languages.
The reasons for these common properties can be
attributed to: 1) a common ancestor language,
2) borrowing from nearby languages, and 3) the
innate abilities of humans (Chomsky, 1965).
We assume hidden commonalities in syntax
across languages, and try to extract a common
grammar from non-parallel multilingual corpora.
For this purpose, we propose a generative model
for multilingual grammars that is learned in an
unsupervised fashion. There are some computa-
tional models for capturing commonalities at the
phoneme and word level (Oakes, 2000; Bouchard-
Co?te? et al, 2008), but, as far as we know, no at-
tempt has been made to extract commonalities in
syntax level from non-parallel and non-annotated
multilingual corpora.
In our scenario, we use probabilistic context-
free grammars (PCFGs) as our monolingual gram-
mar model. We assume that a PCFG for each
language is generated from a general model that
are common across languages, and each sentence
in multilingual corpora is generated from the lan-
guage dependent PCFG. The inference of the gen-
eral model as well as the multilingual PCFGs can
be performed by using a variational method for
efficiency. Our approach is based on a Bayesian
multitask learning framework (Yu et al, 2005;
Daume? III, 2009). Hierarchical Bayesian model-
ing provides a natural way of obtaining a joint reg-
ularization for individual models by assuming that
the model parameters are drawn from a common
prior distribution (Yu et al, 2005).
2 Related work
The unsupervised grammar induction task has
been extensively studied (Carroll and Charniak,
1992; Stolcke and Omohundro, 1994; Klein and
Manning, 2002; Klein and Manning, 2004; Liang
et al, 2007). Recently, models have been pro-
posed that outperform PCFG in the grammar in-
duction task (Klein and Manning, 2002; Klein and
Manning, 2004). We used PCFG as a first step
for capturing commonalities in syntax across lan-
guages because of its simplicity. The proposed
framework can be used for probabilistic grammar
models other than PCFG.
Grammar induction using bilingual parallel cor-
pora has been studied mainly in machine transla-
tion research (Wu, 1997; Melamed, 2003; Eisner,
2003; Chiang, 2005; Blunsom et al, 2009; Sny-
der et al, 2009). These methods require sentence-
aligned parallel data, which can be costly to obtain
and difficult to scale to many languages. On the
other hand, our model does not require sentences
to be aligned. Moreover, since the complexity of
our model increases linearly with the number of
languages, our model is easily applicable to cor-
184
pora of more than two languages, as we will show
in the experiments. To our knowledge, the only
grammar induction work on non-parallel corpora
is (Cohen and Smith, 2009), but their method does
not model a common grammar, and requires prior
information such as part-of-speech tags. In con-
trast, our method does not require any such prior
information.
3 Proposed Method
3.1 Model
Let X = {X l}l?L be a non-parallel and non-
annotated multilingual corpus, where X l is a set
of sentences in language l, and L is a set of lan-
guages. The task is to learn multilingual PCFGs
G = {Gl}l?L and a common grammar that gen-
erates these PCFGs. Here, Gl = (K,W l,?l)
represents a PCFG of language l, where K is a
set of nonterminals, W l is a set of terminals, and
?l is a set of rule probabilities. Note that a set of
nonterminals K is shared among languages, but
a set of terminals W l and rule probabilities ?l
are specific to the language. For simplicity, we
consider Chomsky normal form grammars, which
have two types of rules: emissions rewrite a non-
terminal as a terminal A ? w, and binary pro-
ductions rewrite a nonterminal as two nontermi-
nalsA? BC, whereA,B,C ?K and w ?W l.
The rule probabilities for each nonterminal
A of PCFG Gl in language l consist of: 1)
?Al = {?lAt}t?{0,1}, where ?lA0 and ?lA1 repre-
sent probabilities of choosing the emission rule
and the binary production rule, respectively, 2)
?lA = {?lABC}B,C?K , where ?lABC repre-
sents the probability of nonterminal production
A ? BC, and 3) ?lA = {?lAw}w?W l , where
?lAw represents the probability of terminal emis-
sion A? w. Note that ?lA0 + ?lA1 = 1, ?lAt ? 0,
?
B,C ?lABC = 1, ?lABC ? 0,
?
w ?lAw = 1,
and ?lAw ? 0. In the proposed model, multino-
mial parameters ?lA and ?lA are generated from
Dirichlet distributions that are common across lan-
guages: ?lA ? Dir(??A) and ?lA ? Dir(?
?
A),
since we assume that languages share a common
syntax structure. ??A and ?
?
A represent the param-
eters of a common grammar. We use the Dirichlet
prior because it is the conjugate prior for the multi-
nomial distribution. In summary, the proposed
model assumes the following generative process
for a multilingual corpus,
1. For each nonterminal A ?K :
?
??Aa,b
a,b
|L|
?A? lA
lA
|K|
? ?
?
? lA |L|
z 1
z 2 z 3
x2 x3
? 
? 
? ?
Figure 1: Graphical model.
(a) For each rule type t ? {0, 1}:
i. Draw common rule type parameters
??At ? Gam(a?, b?)
(b) For each nonterminal pair (B,C):
i. Draw common production parameters
??ABC ? Gam(a
?, b?)
2. For each language l ? L:
(a) For each nonterminal A ?K :
i. Draw rule type parameters
?lA ? Dir(??A)
ii. Draw binary production parameters
?lA ? Dir(?
?
A)
iii. Draw emission parameters
?lA ? Dir(??)
(b) For each node i in the parse tree:
i. Choose rule type
tli ? Mult(?lzi)
ii. If tli = 0:
A. Emit terminal
xli ? Mult(?lzi)
iii. Otherwise:
A. Generate children nonterminals
(zlL(i), zlR(i)) ? Mult(?lzi),
where L(i) and R(i) represent the left and right
children of node i. Figure 1 shows a graphi-
cal model representation of the proposed model,
where the shaded and unshaded nodes indicate ob-
served and latent variables, respectively.
3.2 Inference
The inference of the proposed model can be ef-
ficiently computed using a variational Bayesian
method. We extend the variational method to
the monolingual PCFG learning of Kurihara and
Sato (2004) for multilingual corpora. The goal
is to estimate posterior p(Z,?,?|X), where Z
is a set of parse trees, ? = {?l}l?L is a
set of language dependent parameters, ?l =
{?lA,?lA,?lA}A?K , and ? = {?
?
A,?
?
A}A?K
is a set of common parameters. In the variational
method, posterior p(Z,?,?|X) is approximated
by a tractable variational distribution q(Z,?,?).
185
We use the following variational distribution,
q(Z,?,?) =
?
A
q(??A)q(?
?
A)
?
l,d
q(zld)
?
?
l,A
q(?lA)q(?lA)q(?lA), (1)
where we assume that hyperparameters q(??A) and
q(??A) are degenerated, or q(?) = ???(?), and
infer them by point estimation instead of distribu-
tion estimation. We find an approximate posterior
distribution that minimizes the Kullback-Leibler
divergence from the true posterior. The variational
distribution of the parse tree of the dth sentence in
language l is obtained as follows,
q(zld) ?
?
A?BC
(
pi?lA1pi
?
lABC
)C(A?BC;zld,l,d)
?
?
A?w
(
pi?lA0pi
?
lAw
)C(A?w;zld,l,d)
, (2)
where C(r; z, l, d) is the count of rule r that oc-
curs in the dth sentence of language l with parse
tree z. The multinomial weights are calculated as
follows,
pi?lAt = exp
(
Eq(?lA)
[
log ?lAt
])
, (3)
pi?lABC = exp
(
Eq(?lA)
[
log ?lABC
])
, (4)
pi?lAw = exp
(
Eq(?lA)
[
log?lAw
])
. (5)
The variational Dirichlet parameters for q(?lA) =
Dir(??lA), q(?lA) = Dir(?
?
lA), and q(?lA) =
Dir(??lA), are obtained as follows,
??lAt = ??At +
?
d,zld
q(zld)C(A, t; zld, l, d), (6)
??lABC = ?
?
ABC+
?
d,zld
q(zld)C(A?BC; zld, l, d),
(7)
??lAw = ?
? +
?
d,zld
q(zld)C(A? w; zld, l, d),
(8)
where C(A, t; z, l, d) is the count of rule type t
that is selected in nonterminal A in the dth sen-
tence of language l with parse tree z.
The common rule type parameter ??At that min-
imizes the KL divergence between the true pos-
terior and the approximate posterior can be ob-
tained by using the fixed-point iteration method
described in (Minka, 2000). The update rule is as
follows,
??(new)At ?
a??1+??AtL
(
?(
?
t? ??At?)??(??At)
)
b? +
?
l
(
?(
?
t? ??lAt?)??(??lAt)
) ,
(9)
where L is the number of languages, and ?(x) =
? log ?(x)
?x is the digamma function. Similarly, the
common production parameter ??ABC can be up-
dated as follows,
??(new)ABC ?
a? ? 1 + ??ABCLJABC
b? +
?
l J ?lABC
, (10)
where JABC = ?(
?
B?,C? ?
?
AB?C?) ? ?(?
?
ABC),
and J ?lABC = ?(
?
B?,C? ?
?
lAB?C?)??(?
?
lABC).
Since factored variational distributions depend
on each other, an optimal approximated posterior
can be obtained by updating parameters by (2) -
(10) alternatively until convergence. The updat-
ing of language dependent distributions by (2) -
(8) is also described in (Kurihara and Sato, 2004;
Liang et al, 2007) while the updating of common
grammar parameters by (9) and (10) is new. The
inference can be carried out efficiently using the
inside-outside algorithm based on dynamic pro-
gramming (Lari and Young, 1990).
After the inference, the probability of a com-
mon grammar rule A ? BC is calculated by
??A?BC = ??1??ABC , where ??1 = ??1/(??0 + ??1)
and ??ABC = ??ABC/
?
B?,C? ?
?
AB?C? represent
the mean values of ?l0 and ?lABC , respectively.
4 Experimental results
We evaluated our method by employing the Eu-
roParl corpus (Koehn, 2005). The corpus con-
sists of the proceedings of the European Parlia-
ment in eleven western European languages: Dan-
ish (da), German (de), Greek (el), English (en),
Spanish (es), Finnish (fi), French (fr), Italian (it),
Dutch (nl), Portuguese (pt), and Swedish (sv), and
it contains roughly 1,500,000 sentences in each
language. We set the number of nonterminals at
|K| = 20, and omitted sentences with more than
ten words for tractability. We randomly sampled
100,000 sentences for each language, and ana-
lyzed them using our method. It should be noted
that our random samples are not sentence-aligned.
Figure 2 shows the most probable terminals of
emission for each language and nonterminal with
a high probability of selecting the emission rule.
186
2: verb and auxiliary verb (V)
5: noun (N)
7: subject (SBJ)
9: preposition (PR)
11: punctuation (.)
13: determiner (DT)
Figure 2: Probable terminals of emission for each
language and nonterminal.
0? 16 11 (R? S . ) 0.11
16? 7 6 (S? SBJ VP) 0.06
6? 2 12 (VP? V NP) 0.04
12? 13 5 (NP? DT N) 0.19
15? 17 19 (NP? NP N) 0.07
17? 5 9 (NP? N PR) 0.07
15? 13 5 (NP? DT N) 0.06
Figure 3: Examples of inferred common gram-
mar rules in eleven languages, and their proba-
bilities. Hand-provided annotations have the fol-
lowing meanings, R: root, S: sentence, NP: noun
phrase, VP: verb phrase, and others appear in Fig-
ure 2.
We named nonterminals by using grammatical cat-
egories after the inference. We can see that words
in the same grammatical category clustered across
languages as well as within a language. Fig-
ure 3 shows examples of inferred common gram-
mar rules with high probabilities. Grammar rules
that seem to be common to European languages
have been extracted.
5 Discussion
We have proposed a Bayesian hierarchical PCFG
model for capturing commonalities at the syntax
level for non-parallel multilingual corpora. Al-
though our results have been encouraging, a num-
ber of directions remain in which we must extend
our approach. First, we need to evaluate our model
quantitatively using corpora with a greater diver-
sity of languages. Measurement examples include
the perplexity, and machine translation score. Sec-
ond, we need to improve our model. For ex-
ample, we can infer the number of nonterminals
with a nonparametric Bayesian model (Liang et
al., 2007), infer the model more robustly based
on a Markov chain Monte Carlo inference (John-
son et al, 2007), and use probabilistic grammar
models other than PCFGs. In our model, all the
multilingual grammars are generated from a gen-
eral model. We can extend it hierarchically using
the coalescent (Kingman, 1982). That model may
help to infer an evolutionary tree of languages in
terms of grammatical structure without the etymo-
logical information that is generally used (Gray
and Atkinson, 2003). Finally, the proposed ap-
proach may help to indicate the presence of a uni-
versal grammar (Chomsky, 1965), or to find it.
187
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009.
Bayesian synchronous grammar induction. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-
vances in Neural Information Processing Systems 21,
pages 161?168.
Alexandre Bouchard-Co?te?, Percy Liang, Thomas Griffiths,
and Dan Klein. 2008. A probabilistic approach to lan-
guage change. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information Pro-
cessing Systems 20, pages 169?176, Cambridge, MA.
MIT Press.
Glenn Carroll and Eugene Charniak. 1992. Two experiments
on learning probabilistic dependency grammars from cor-
pora. In Working Notes of the Workshop Statistically-
Based NLP Techniques, pages 1?13. AAAI.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In ACL ?05: Proceedings
of the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263?270, Morristown, NJ, USA.
Association for Computational Linguistics.
Norm Chomsky. 1965. Aspects of the Theory of Syntax. MIT
Press.
Shay B. Cohen and Noah A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsuper-
vised grammar induction. In NAACL ?09: Proceedings of
Human Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 74?82, Morristown,
NJ, USA. Association for Computational Linguistics.
Hal Daume? III. 2009. Bayesian multitask learning with la-
tent hierarchies. In Proceedings of the Twenty-Fifth An-
nual Conference on Uncertainty in Artificial Intelligence
(UAI-09), pages 135?142, Corvallis, Oregon. AUAI Press.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In ACL ?03: Proceedings of the
41st Annual Meeting on Association for Computational
Linguistics, pages 205?208, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Russell D. Gray and Quentin D. Atkinson. 2003. Language-
tree divergence times support the Anatolian theory of
Indo-European origin. Nature, 426(6965):435?439,
November.
Mark Johnson, Thomas Griffiths, and Sharon Goldwater.
2007. Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Human Language Technologies 2007:
The Conference of the North American Chapter of the
Association for Computational Linguistics; Proceedings
of the Main Conference, pages 139?146, Rochester, New
York, April. Association for Computational Linguistics.
J. F. C. Kingman. 1982. The coalescent. Stochastic Pro-
cesses and their Applications, 13:235?248.
Dan Klein and Christopher D. Manning. 2002. A generative
constituent-context model for improved grammar induc-
tion. In ACL ?02: Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics, pages
128?135, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of depen-
dency and constituency. In ACL ?04: Proceedings of the
42nd Annual Meeting on Association for Computational
Linguistics, page 478, Morristown, NJ, USA. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of the 10th
Machine Translation Summit, pages 79?86.
Kenichi Kurihara and Taisuke Sato. 2004. An applica-
tion of the variational Bayesian approach to probabilistic
context-free grammars. In International Joint Conference
on Natural Language Processing Workshop Beyond Shal-
low Analysis.
K. Lari and S.J. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algorithm.
Computer Speech and Language, 4:35?56.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical dirichlet pro-
cesses. In EMNLP ?07: Proceedings of the Empirical
Methods on Natural Language Processing, pages 688?
697.
I. Dan Melamed. 2003. Multitext grammars and syn-
chronous parsers. In NAACL ?03: Proceedings of the 2003
Conference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Language
Technology, pages 79?86, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Thomas Minka. 2000. Estimating a Dirichlet distribution.
Technical report, M.I.T.
Michael P. Oakes. 2000. Computer estimation of vocabu-
lary in a protolanguage from word lists in four daughter
languages. Journal of Quantitative Linguistics, 7(3):233?
243.
Steven Pinker. 1994. The Language Instinct: How the Mind
Creates Language. HarperCollins, New York.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction. In
Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint Con-
ference on Natural Language Processing of the AFNLP,
pages 73?81, Suntec, Singapore, August. Association for
Computational Linguistics.
Andreas Stolcke and Stephen M. Omohundro. 1994. In-
ducing probabilistic grammars by Bayesian model merg-
ing. In ICGI ?94: Proceedings of the Second International
Colloquium on Grammatical Inference and Applications,
pages 106?118, London, UK. Springer-Verlag.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Comput.
Linguist., 23(3):377?403.
Kai Yu, Volker Tresp, and Anton Schwaighofer. 2005.
Learning gaussian processes from multiple tasks. In
ICML ?05: Proceedings of the 22nd International Confer-
ence on Machine Learning, pages 1012?1019, New York,
NY, USA. ACM.
188
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 212?216,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Latent Semantic Matching: Application to Cross-language Text
Categorization without Alignment Information
Tsutomu Hirao and Tomoharu Iwata and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{hirao.tsutomu,iwata.tomoharu,nagata.masaaki}@lab.ntt.co.jp
Abstract
Unsupervised object matching (UOM) is
a promising approach to cross-language
natural language processing such as bilin-
gual lexicon acquisition, parallel corpus
construction, and cross-language text cat-
egorization, because it does not require
labor-intensive linguistic resources. How-
ever, UOM only finds one-to-one corre-
spondences from data sets with the same
number of instances in source and target
domains, and this prevents us from ap-
plying UOM to real-world cross-language
natural language processing tasks. To al-
leviate these limitations, we proposes la-
tent semantic matching, which embeds
objects in both source and target lan-
guage domains into a shared latent topic
space. We demonstrate the effectiveness
of our method on cross-language text cat-
egorization. The results show that our
method outperforms conventional unsu-
pervised object matching methods.
1 Introduction
Unsupervised object matching is a method for
finding one-to-one correspondences between ob-
jects across different domains without knowledge
about the relation between the domains. Kernel-
ized sorting (Novi et al, 2010) and canonical cor-
relation analysis based methods (Haghighi et al,
2008; Tripathi et al, 2010) are two such exam-
ples of unsupervised object matching, which have
been shown to be quite useful for cross-language
natural language processing (NLP) tasks. One of
the most important properties of the unsupervised
object matching is that it does not require any lin-
guistic resources which connects between the lan-
guages. This distinguishes it from other cross-
language NLP methods such as machine transla-
tion based and projection based approaches (Du-
mais et al, 1996; Gliozzo and Strapparava, 2005;
Platt et al, 2010), which we need bilingual dictio-
naries or parallel sentences.
When we apply unsupervised object matching
methods to cross-language NLP tasks, there are
two critical problems. The first is that they only
find one-to-one matching. The second is they re-
quire the same size of source- and target-data. For
example, the correct translation of a word is not
always unique. French words ?maison?, ?appart-
ment? and ?domicile? can be regarded as transla-
tion of an English word ?home?. In addition, En-
glish vocabulary size is not equal to that of French.
These discussions motivate us to introduce a
shared space in which both source and target do-
main objects will reside. If we can obtain such
a shared space, we can match objects within the
space, because we can use standard distance met-
rics on this space. This will also enable us to use
various kinds of non-strict matching. For exam-
ple, k-nearest objects in the source domain will be
retrieved for a query object in the target domain.
In this paper, we propose a simple but effective
method to find the shared space by assuming that
two languages have common latent topics, which
we call latent semantic matching. With latent se-
mantic matching, we first find latent topics in two
domains independently. Then, the topics in two
domains are aligned by kernelized sorting, and ob-
jects are embedded in a shared latent topic space.
Latent topic representations are successfully used
in a wide range of NLP tasks, such as information
retrieval and text classification, because they rep-
resent intrinsic information of documents (Deer-
wester et al, 1990). By matching latent topics,
we can find relation between source and target do-
mains, and additionally we can handle different
numbers of objects in two domains.
We compared latent semantic matching with
conventional unsupervised object matching meth-
212
ods on the task of cross-language text categoriza-
tion, i.e. classifying target side unlabeled docu-
ments by label information obtained from source
side documents. The results show that, with more
source side documents, our method achieved the
highest classification accuracy.
2 Related work
Many cross-language text processing methods
have been proposed that require correspondences
between source and target languages. For exam-
ple, (Dumais et al, 1996) proposed cross-lingual
latent semantic indexing, and (Platt et al, 2010)
employed oriented principle component analysis
and canonical correlation analysis (CCA). They
concatenate the document pairs (source document
and its translation) obtained from a document-
level parallel corpus. They then apply multi-
variate analysis to acquire the translingual projec-
tion. There are extensions of latent Dirichlet alo-
cation (LDA) (Blei et al, 2003) for cross-language
analysis, such as multilingual topic models (Boyd-
Graber and Blei, 2009), joint LDA (Jagadeesh
and Daume III, 2010) and multilingual LDA (Xi-
aochuan et al, 2011). They require a bilingual dic-
tionary or document-level parallel corpora.
Unsupervised object matching methods have
been proposed recently (Novi et al, 2010;
Haghighi et al, 2008; Yamada and Sugiyama,
2011). These methods are promising in terms of
language portability because they do not require
external language resources. (Novi et al, 2010)
proposed kernelized sorting (KS); it finds one-to-
one correspondences between objects in different
domains by permuting a set to maximize the de-
pendence between two sets. Here, the Hilbert-
Schmidt independence criterion is used for mea-
suring dependence. (Djuric et al, 2012) proposed
convex kernelized sorting as an extension of KS.
(Yamada and Sugiyama, 2011) proposed least-
squares object matching which maximizes the
squared-loss mutual information between matched
pairs. (Haghighi et al, 2008) proposed another
framework, matching CCA (MCCA), based on a
probabilistic interpretation of CCA (Bach and Jor-
dan, 2005). MCCA simultaneously finds latent
variables that represent correspondences and la-
tent features so that the latent features of corre-
sponding examples exhibit the maximum correla-
tion. However, these unsupervised object match-
ing methods have limitations. They require that
the source and target domains have the same data
size, and they find one-to-one correspondences.
There are critical weaknesses of these methods
when we attempt to apply them to real world
cross-language NLP applications.
3 Latent Semantic Matching
We propose latent semantic matching to find a
shared latent space by assuming that two lan-
guages have common latent topics. Our method
consists of following four steps: (1) for both
source and target domains, we map the documents
to a K-dimensional latent topic space indepen-
dently, (2) we find the one-to-one correspondences
between topics across source and target domains
by unsupervised object matching, (3) we permute
topics of the target side according to the corre-
spondences, while fixing the topics of the source
side, and (4) finally, we map documents in the
source and target domains to a shared latent space
by using permuted and fixed topics.
3.1 Topic Extraction as Dimension Reduction
Suppose that we have N documents in the source
domain. sn=(sni)Ii=1 is the nth document rep-
resented as a multi-dimensional column vector in
the domain, i.e. each document is represented as
a bag-of-words vector. Here, each element of the
vectors indicates the TF?IDF score of the corre-
sponding word in the document. I is the size of the
feature set, i.e., the vocabulary size in the source
domain. Also, we have M documents in the tar-
get domain. tm=(tmj)Jj=1 is the mth document
represented as a multi-dimensional vector. J is
the vocabulary size in the target domain. Thus,
the data set in the source domain is represented by
an I ? N matrix, S=(s1, ? ? ? , sN ), the data set
in the target is represented by a J ? M matrix,
T=(t1, ? ? ? , tM ).
We factorize these matrices using nonnegative
matrix factorization (Lee and Seung, 2000) to find
topics as follows:
S ?WSHS , (1)
T ?WTHT . (2)
WS is an I?K matrix that represents a set of top-
ics, i.e. each column vector denotes word weights
for each topic. HS is a K ? N matrix that de-
notes a set of latent semantic representations of
documents in the source domain, i.e. each row
213
?????? WS HS? * = 0 0 1 00 1 0 00 0 0 11 0 0 0
ment. I is the size of feature set, i.e., the size of vocabulary inthe source domain. Also, we have M documents in a targetdomain. tm = (tmj)Jj=1 is the m-th document representedas a multi-dimensional vector. J is the size of vocabulary inthe target domain. Thus, the data set in the source domain isrepresented as the I ?N matrix, S, the data set in the targetis represented as the J ?M matrix, T .Here, we assume that these matrices are approximated asthe product of low rank matrices as follows:
S ? WSHS , (1)T ? WTHT (2)WS is I?K matrix, which represents a set of topic propor-tions in the source domain, i.e., each column vector denotestopic proportion. HS is K ? N matrix, which denotes a setof documents in the K-dimensional latent space which cor-responds to the source domain, i.e., each row vector denotesthe document in the latent space. The k(1 ? k ? K)-th basisin the latent space corresponds to the k-th topic proportion.WT is I ? K matrix, which represents a set of topic pro-portions in the target domain. HT is K ? N matrix, whichdenotes a set of documents in the latent topic space with di-mentionaly K. K is less than I , J . In this paper, we employNon-negative Matrix Factorization (NMF) [Lee and Seung,2000] to factorize the original matrices.According to the factorization of the original matrices, wecan map the documents in the source and target domain tolatent topic space with dimentionaly K, independently.3.2 Finding Optimal Topic Alignments byUnsupervised Object MatchingTo connect the different latent space, the basis of the spacehave to be aligned each other. That is, topic proportion ex-tracted from the source language must be aligned that fromthe target language. This is reasonable consideration becausewe can assume the same latent concept for both language.For example, a topic proportion obtained from English docu-ments can be aligned a topic proportion obtained from Frenchdocuments. For all k and k?, k-th column vector in WS arealigned k?-th column vector in WT .However, we can not measure similarity between the topicproportions because we do not have any language resourcessuch as dictionary. Therefore, we utilize unsupervised ob-ject matching method to find one-to-one correspondences be-tween topic proportions. In this paper, we employ KernelizedSorting (KS) [Novi et al, 2010]. Of cource, we can replaceKS to another unsupervised object matching sush as MCCA[Haghighi et al, 2008], LSOM [Yamada and Sugiyama,2011].KS finds the best one-to-one matching by followings:
pi? = argmaxpi??K tr(G?SpiTG?Tpi),s.t. pi1K = 1K and piT1K = 1K . (3)pi is K?K matrix which represents one-to-one correspon-dence between topic proportion, i.e., piij = 1 indicates i-thtopic proportion in the source language corresponds to j-th
one of the target language. ? indicates set of all possibleK ? K matrices which store one-to-one corresponrence. GdenotesK?K kernel matrix obtained from topic proportion,Gij = K(WTi,:,W:,j), and G? is the centerd matrix of G. K(, )is a kernel function. 1K is K-dimensional column vector ofall ones. pi? is obtained by iterative procedure. According topi?, we can permutate the basis of the latent space obtainedfrom source language. See fig hoge.
S ? WSHS . (4)On the other hand, we can directly fomulate objective func-tion of unsupervised mapping. If the topic proportions arealigned each other, the correlation matrix (or gram matrix)obtained from source language is proportional to one fromtarget language:
||GS ? ?GT ||2 = 0. (5)? denotes the hyperparameter for tuning the socore range be-tween two gram matrices.By minimize the error of the matrix factorization (equa-tion (1),(2)) and the difference between correlation matrices(equation (6)), the objective function is defined as follow:
E = ?S ?WSHS?2+ ?T ?WTHT ?2+ ?||GS ? ?GT ||2. (6)
? is cost parameter between first, second argu-ment and third argument. The optimal parameters(WS,WT ,HS,HT ) are obtained by minimizing theobjective function. To mimimize the objective, gradient de-scend can be used. but However that is not convex function,we only obtained local optimal. Thefore, we employed abovetwo step procedure??????This objective function is not convex. That meanswe can only obtain local optimal parameters. By min-imizing equation (6), we can obtain a set of parameter(WS,WT ,HS,HT ) for unsupervised mapping. we couldbe employed gradient based algorithm but, as the first step,we employ former two step optimization procedure.3.3 Cross-lingual Text Categorization viaUnsupervised Mappingm-th document in the target domain (tm) is mapped to thesource domain as follows,
s(tm) = HT$:,mWS . (7)Here, HT :,m denotes the m-th column vector of HT , s(tm)is I dimentional vector.When each document in the source domain has a classlabel yn, we can train a classifier on the training data set{sn, yn}Nn=1. Therefore, the class label of the mapped docu-ment in the target domain s(tm) is assigned by the classifier.In the later experiments, we employ k(= 10)-NN as a classi-fier.WT HTMI NJ KKKTST
Figure 1: Topic alignments.
vector denotes an embedding of a document in the
K-dimensional latent space. Similarly, WT is an
I ?K matrix that represents a set of topics in the
target domain, and HT is a K ? M matrix that
denotes a set of latent semantic representations of
target documents. K is less than I and J .
By factorizing the original matrices, we can in-
dependently map the documents in the source and
target domains to the latent topic spaces whose di-
mensionality is K.
3.2 Finding Optimal Topic Alignments by
Unsupervised Object Matching
To connect the different latent spaces, topics ex-
tracted from the source language must be aligned
to one from the target language. This is reasonable
because we can assume that both languages share
the same latent concept.
However, we cannot quantify the similarity be-
tween the topics because we do not have any ex-
ternal language resources such as a dictionary.
Therefore, we utilize unsupervised object match-
ing method to find one-to-one correspondences
between topics. In this paper, we employ kernel-
ized sorting (KS) (Novi et al, 2010). KS finds the
best one-to-one matching as follows:
pi? = argmax
pi??K
tr(GSpi?GTpi),
s.t. pi1K=1K and pi?1K=1K . (3)
Here, pi is a K?K matrix that represents the one-
to-one correspondence between topics, i.e. piij=1
indicates that the ith topic in the source language
corresponds to the jth one of the target language.
Overall Average
KS 0.252 ? 0.112
CKS 0.249 ? 0.033
LSOM 0.278 ? 0.086
LSM(300) 0.298 ? 0.077
LSM(600) 0.359 ? 0.062
Table 1: Average accuracy over all language pairs
?K indicates the set of all possible matrices stor-
ing one-to-one correspondences. G denotes the
K ? K kernel matrix obtained from topic pro-
portion, Gij=K(W?i,: ,W:,j), and G is the centered
matrix of G. K(, ) is a kernel function. 1K is a
K-dimensional column vector of all ones. pi? is
obtained by iterative procedure.
According to pi?, we obtain permuted matrices,
WT=WTpi? and HT=pi??HT , and the product
of permuted matrices is the same with that of un-
permuted matrices as follows:
T ?WTHT=WTHT . (4)
Fig. 1 shows the topic alignment procedure.
Since documents from both domains are repre-
sented in a shared latent space, we can directly cal-
culate the similarity between the nth document in
the source domain and the mth document in the
target domain based on HT :,m (mth column vec-
tor of HT ) and HS:,n (nth column vector of HS).
4 Cross-language Text Categorization
via Latent Semantic Matching
Cross-language text categorization is the task of
exploiting labeled documents in the source lan-
guage (e.g. English) to classify documents in
the target language (e.g. French). Suppose we
have training data set {sn, yn}Nn=1 in the source
language domain. yn ? Y is the class label
for the nth document. We can train a classifier
in the K-dimensional latent space with data set
{H?S:,n, yn}Nn=1. H?S:,n is the projected vector of
sn. Also, the mth document in the target language
domain tm is projected into the latent space as
H?T :,m. Here, the documents in both domains are
projected into the same size latent space and the
basis vectors of the spaces are aligned. Therefore,
we can classify a document in the target domain
tm by a classifier trained with {H?S:,n, yn}Nn=1.
214
Books
English Hack, Parent, tale, subversion, Interesting, centre, Paper, T., prejudice, Murphy
German Lydia, Sebastian, Seelenbrecher, Patient, Fitzek, Patrick, Fiktion, Patientenakte, Realitt, Klinik
Electronics
English SD800, Angle, Digital, Optical, Silver, understnad, camra, 7.1MP, P3N, 10MP
German *****, 550D, 600D, Objektiv, Canon, ablichten, Body, Werkzeug, Kamera, einliet
Kitchen
English Briel, Electra-Craft, Chamonix, machine, Due, crema, supervisor, technician, espresso, tamp
German ESGE, Prierkopf, Zauberstab, Gummikupplung, Suppe/Sauce, Braun , Bolognese, prieren, Testsieger, Topf
Music
English Amy, Poison, Doherty, Schottin, Mid, Prince, Song, ausdrucksstark , Tempo, knocking
German Norah, mini, ?Little, ?Rome, ?Come, Gardot, Lana, listenings , dreamlike, digipak
Watch
English watch, indicate, timex, HRM, month, icon, Timex, datum, troubleshooting, reasonable
German Orient, Diver, Lnette, Leuchtpunkt, Zahlenringes, Handgelenksdurchmesser, Stoppsekunde, Uhrforum,
Konsumbereiche, Schwingungen/Std
Table 2: Examples of aligned latent topics
5 Experimental Evaluation
5.1 Experimental Settings
We compared our method, latent semantic match-
ing (LSM), with three unsupervised object match-
ing methods: Kernelized Sorting (KS), Convex
Kernelized Sorting (CKS), Least-Squares Object
Matching (LSOM). We set the number of the la-
tent topics K to 100 and employed the k-nearest
neighbor method (k=10) as the classifier.
For, KS, CKS and LSOM, we find the one-
to-one correspondence between documents in the
source language and documents in the target lan-
guage. Then, we assign class labels of the target
documents according to the correspondence.
In order to build a corpus with various lan-
guage pairs for evaluation, we crawled product
reviews from Amazon U.S., German, France and
Japan with five categories: ?Books?, ?Electronics?,
?Music?, ?Kitchen?, ?Watch?. The corpus is nei-
ther sentence level parallel nor comparable. For
each category, we randomly select 60 documents
as the test data (M=300) for all methods and 60
documents as the training data (N=300) for KS,
CKS, LSOM and LSM(300). We also compared
latent semantic matching with 120 training docu-
ments for each category (N=600), and called this
method LSM(600). Note that since KS, CKS and
LSOM require that the data sizes are the same for
source and target domains, they cannot use train-
ing data more than test data. To avoid local opti-
mum solutions of NMF, we executed our methods
with 100 different initialization values and chose
the solution that achieved the best objective func-
tion of KS.
5.2 Results and Discussion
Table 1 shows average accuracies with standard
division over all language pairs. From the table,
classification accuracy of all methods significantly
outperformed random classifier (accuracy=0.2).
The results showed the effectiveness of both un-
supervised object matching and latent semantic
matching. When comparing LSM(300) with KS,
CKS and LSOM, LSM(300) obtained better re-
sults than these unsupervised object matching
methods. The result supports the effectiveness of
the latent topic matching. Moreover, LSM(600)
achieved the highest accuracy. There are large dif-
ferences between LSM(600) and the others. This
result implies not only the effectiveness of the la-
tent topic matching but also increasing the number
of source side documents (labeled training data)
contributes to improving classification accuracy.
This is natural in terms of supervised learning but
only our method can deal with source side docu-
ments that are larger in number.
Table 2 shows examples of latent topics in
English and German extracted and aligned by
LSM(600). We can see that some author names,
words related to camera, and cooking equipment
appear in ?Books?, ?Electronics? and ?Kitchen?
topics, respectively. Similarity, there are some
artists? names in ?Music? and watch brands in
?Watch?.
215
6 Conclusion
As an extension of unsupervised object matching,
this paper proposed latent semantic matching that
considers the shared latent space between two lan-
guage domains. To generate such a space, top-
ics of the target space are permuted by exploit-
ing unsupervised object matching. We can mea-
sure distances between objects by standard met-
rics, which enable us retrieving k-nearest objects
in the source domain for a query object in the tar-
get domain. This is a significant advantage over
conventional unsupervised object matching meth-
ods. We used Amazon review corpus to demon-
strate the effectiveness of our method on cross-
language text categorization. The results showed
that our method outperformed conventional object
matching methods with the same number of train-
ing samples. Moreover, our method achieved even
higher performance by utilizing more documents
in the source domain.
Acknowledgements
The authors would like to thank Nemanja Djuric
for providing code for Convex Kernelized Sorting
and the three anonymous reviewers for thoughtful
suggestions.
References
Francis Bach and Michael Jordan. 2005. A probabilis-
tic interpretation of canonical correlation analysis.
Technical report, Department of Statistics, Univer-
sity of California, Berkeley.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. JMLR, 3(Jan.):993?
1022.
Jordan Boyd-Graber and David Blei. 2009. Multilin-
gual topic model for unaligned text. In Proc. of the
25th UAI, pages 75?82.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Nemanja Djuric, Mihajlo Grbovic, and Slobodan
Vucetic. 2012. Convex kernelized sorting. In Proc.
of the 26th AAAI, pages 893?899.
Susan Dumais, Lanauer Thomas, and Michael Littman.
1996. Automatic cross-linguistic information re-
trieval using latent semantic indexing. In Proc.
of the Workshop on Cross-Linguistic Information
Retieval in SIGIR, pages 16?23.
Alfio Gliozzo and Carlo Strapparava. 2005. Cross lan-
guage text categorization by acquiring multilingual
domain models from comparable corpora. In Proc.
of the ACL Workshop on Building and Using Paral-
lel Texts, pages 9?16.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. of ACL-08:
HLT, pages 771?779.
Jagarlamudi Jagadeesh and Hal Daume III. 2010. Ex-
tracting multilingual topics from unaligned corpora.
In Proc of the 32nd ECIR, pages 444?456.
Daniel Lee and Sebastian Seung. 2000. Algorithm
for non-negative matrix factorization. In Advances
in Neural Information Processing Systems 13, pages
556?562.
Quadrianto Novi, Smola Alexander, Song Le, and
Tuytelaars Tinne. 2010. Kernelized sorting. IEEE
Trans. on Pattern Analysis and Machine Intelli-
gence, 32(10):1809?1821.
Jhon Platt, Kristina Toutanova, andWen-tau Yih. 2010.
Translingual document representation from discrim-
inative projections. In Proc. of the 2010 Conference
on EMNLP, pages 251?261.
Abhishek Tripathi, Arto Klami, and Sami Virpioja.
2010. Bilingual sentence matching using kernel
CCA. In Proc. of the 2010 IEEE International
Workshop on MLSP, pages 130?135.
Ni Xiaochuan, Sun Lian-Tao, Hu Jian, and Chen
Zheng. 2011. Cross lingual text classification by
mining multilingual topics from wikipedia. In Proc.
of the 4th WSDM, pages 375?384.
Makoto Yamada and Masashi Sugiyama. 2011. Cross-
domain object matching with model selection. In
Proc. of the 14th AISTATS, pages 807?815.
216
