Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2064?2074, Dublin, Ireland, August 23-29 2014.
Effective Incorporation of Source Syntax into
Hierarchical Phrase-based Translation
Tong Xiao??, Adri
`
a de Gispert?, Jingbo Zhu??, Bill Byrne?
? Northeastern University, Shenyang 110819, China
? Hangzhou YaTuo Company, Hangzhou 310012, China
? University of Cambridge, CB2 1PZ Cambridge, U.K.
{xiaotong,zhujingbo}@mail.neu.edu.cn
{ad465,wjb31}@eng.cam.ac.uk
Abstract
In this paper we explicitly consider source language syntactic information in both rule extraction
and decoding for hierarchical phrase-based translation. We obtain tree-to-string rules by the
GHKM method and use them to complement Hiero-style rules. All these rules are then employed
to decode new sentences with source language parse trees. We experiment with our approach in
a state-of-the-art Chinese-English system and demonstrate +1.2 and +0.8 BLEU improvements
on the NIST newswire and web evaluation data of MT08 and MT12.
1 Introduction
Synchronous context free grammars (SCFGs) are widely used in statistical machine translation (SMT),
with hierarchical phrase-based translation (Chiang, 2005) as the dominant approach. Hiero grammars
are easily extracted from word-aligned parallel corpora and can capture complex nested translation re-
lationships. Hiero grammars are formally syntactic, but rules are not constrained by source or target
language syntax. This lack of constraint can lead to intractable decoding and bad performance due to
the over-generation of derivations in translation. To avoid these problems, the extraction and application
of SCFG rules is typically constrained by a source language span limit; (non-glue) rules are lexicalised;
and rules are limited to two non-terminals which are not allowed to be adjacent in the source language.
These constraints can yield good performing translation systems, although at a sacrifice in the ability to
model long-distance movement and complex reordering of multiple constituents.
By contrast, the GHKM approach to translation (Galley et al., 2006) relies on a syntactic parse on
either the source or target language side to guide SCFG extraction and translation. The parse tree provides
linguistically-motivated constraints both in grammar extraction and in translation. This allows for looser
span constraints; rules need not be lexicalised; and rules can have more than two non-terminals to model
complex reordering multiple constituents. There are also modelling benefits as more meaningful features
can be used to encourage derivations with ?well-formed? syntactic tree structures. However, GHKM can
have robustness problems in that translation relies on the quality of the parse tree and the diversity of
rule types can lead to sparsity and limited coverage.
In this paper we describe a simple but effective approach to introducing source language syntax into
hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we
do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style
rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal,
2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the
baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not.
We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation
task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on
the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM
formalism and find, for example, that our approach works well with binarized trees.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2064
IP
NP
PN
?
VP
PP
P
?
NP
NN
??
VP
VV
??
NN
??
he
was
satisfied with the
answer
Hiero-style SCFG Rules
h
1
X? ??, he?
h
2
X? ??, with?
h
3
X? ???, the answer?
h
4
X? ?????, was satisfied?
h
5
X? ?X
1
????, was satisfied X
1
?
h
6
X? ?X
1
?? X
2
, was X
2
X
1
?
h
7
X? ?X
1
? X
2
????,
X
1
was satisfied with X
2
?
Tree-to-String Rules
r
1
NP(PN(?))? he
r
2
P(?)? with
r
3
NP(NN(??))? the answer
r
4
VP(VV(??) NN(??))? was satisfied
r
5
PP(x
1
:P x
2
:NP)? x
1
x
2
r
6
VP(x
1
:PP x
2
:VP)? x
2
x
1
r
7
IP(x
1
:NP x
2
:VP)? x
1
x
2
r
8
VP(PP(P(?) x
1
:NP) x
2
:VP)? x
2
with x
1
Figure 1: Hiero-syle and tree-to-string rules extracted from a pair of word-aligned Chinese-English
sentences with a source language (Chinese) parse tree.
2 Background
2.1 Hierarchical Phrase-based Translation
In the hierarchical phrase-based approach, translation is modelled using SCFGs. In general, probabilistic
SCFGs can be learned from word-aligned parallel data using heuristic methods (Chiang, 2007). We can
first extract initial phrase pairs and then obtain hierarchical phrase rules (i.e., rules with non-terminals
on the right hand side). Once the SCFG is obtained, new sentences can be decoded by finding the most
likely derivation of SCFG rules. See Figure 1 for example rules extracted from a sentence pair with word
alignments. A sequence of such rules covering the words of the source sentence is a SCFG derivation,
e.g., rules h
7
, h
1
and h
3
generate a derivation for the sentence pair.
The Hiero SCFG allows vast numbers of derivations which can make unconstrained decoding in-
tractable. In practice, several constraints are applied to control the model size and reduce ambiguity.
Typically these are: (a) a rule span limit to be applied in decoding and sometimes also in rule extraction,
set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set
to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the
glue rules).
2.2 Tree-to-String Translation
Instead of modelling the problem based on surface strings, tree-to-string systems model the translation
equivalency relations from source language syntactic trees to target language strings using derivations
of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A
tree-to-string rule is a tuple ?s
r
, t
r
,??, where s
r
is a source language tree-fragment with terminals and
non-terminals at leaves; t
r
is a string of target-language terminals and non-terminals; and ? is a 1-to-1
alignment between the non-terminals of s
r
and t
r
, for example, VP(VV(??) x
1
:NN)? increases x
1
is a tree-to-string rule, where the non-terminals labeled with the same index x
1
indicate the alignment.
To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al.,
2006) on the bilingual sentences with both word alignment and source (or target) language phrase-
structure tree annotations. In GHKM extraction, we first compute the set of the minimally-sized transla-
tion rules that can explain the mappings between source language tree and target-language string while
respecting the alignment and reordering between the two languages. More complex rules are then learned
by composing two or more minimal rules. See Figure 1 for rules extracted using GHKM.
One of the advantages of the above model is that non-terminals in tree-to-string rules are linguistically
2065
rule
match
decoding
input
string
Hiero
SCFG
ouput
string
(a) decoding with Hiero rules only
rule
match
decoding
input
string&tree
larger
SCFG
Hiero
SCFG
t-to-s
rules
ouput
string
(b) decoding with Hiero and tree-to-string rules
Figure 2: Overview of the Hiero baseline (a) and
our approach (b). ?means input or output of the
decoder. t-to-s is a short for tree-to-string.
VP
PP
P
?
x
1
:NP
x
2
:VP
x
2
with
x
1
X
?
?
?
X
1
X
2
, X
2
with
X
1
?
tree-to-string:
Hiero:
Figure 3: Converting the tree-to-string rule r
8
from Figure 1 to a Hiero-style rule.
motivated and can span word sequences with arbitrary length. Also, one can use rules with consecutive
(or more than two) source language non-terminals when the source language parse tree is available. For
example, r
8
in Figure 1 has a good Chinese syntactic structure indicating the reordered translations of NP
and VP. However, such a rule would not normally be included in a Hiero grammar, as it would require
consecutive source language non-terminals (see Figure 3).
3 The Proposed Approach
Both the tree-to-string model and the hierarchical phrase-based model have their own strengths and
weaknesses. For example, tree-to-string systems are good at modelling long distance reordering, while
hierarchical phrase-based systems are relatively more powerful in handling ill-formed sentences
1
and
free translations (Zhao and Al-Onaizan, 2008; Vilar et al., 2010). Here we present a method to enhance
hierarchical phrase-based systems with tree-to-string rules and benefit from both models. The idea is
simple: we obtain both the tree-to-string grammar and the Hiero-style SCFG from the training data, and
then use tree-to-string rules as additional rules in decoding with the SCFG.
Figure 2 shows an overview of our approach and the usual hierarchical phrase-based approach. Our
approach requires source language parse trees to be input in both rule extraction and decoding. In rule
extraction, we acquire tree-to-string rules using the GHKM method and Hiero-style rules using the Hiero-
style rule extraction method to form a larger SCFG. Then, we make use of both the input string and parse
tree to decode with the SCFG rules. We now describe our approach.
3.1 Transforming Tree-to-String Rules into SCFG Rules
As described in Section 2, tree-to-string rules have a different form from that of SCFG rules. We will use
tree-to-string rules in our hierarchical phrase-based systems by converting each tree-to-string rule into an
SCFG rule. The purpose of doing this is to make tree-to-string rules directly accessible to the Hiero-style
decoder which performs decoding with SCFG rules.
The rule mapping is straightforward: given a tree-to-string rule ?s
r
, t
r
,??, we take the frontier nodes
of s
r
as the source language part of the right hand side of the resulting SCFG rule, and keep t
r
and
? unchanged. Then we replace the non-terminal label with that used in the hierarchical phrase-based
system (e.g., X). See Figure 3 for rule mapping of rule r
8
of Figure 1.
In this way, every tree-to-string rule is associated with exactly one SCFG rule. Therefore we can
obtain a larger SCFG by combining the rules from the original Hiero-style SCFG and the transformed
tree-to-string rules. As explained next, to prevent computational problems we will apply these new rules
1
For example, the parser fails for 4% of the sentences in our training corpus, and 3% and 6% of the newswire and web
development/test sentences, indicating that the data is sometimes ill-formed.
2066
only on the spans that are consistent with the input parse trees. The main goal is to use the tree and the
adapted tree-to-string rules to provide the decoder with new linguistically-sensible translation hypotheses
that may be prevented by the usual Hiero constraints, and to do so without incurring a computational
explosion.
We categorize SCFG rules into two categories based on their availability in Hiero and GHKM extrac-
tion. If an SCFG rule is obtained from Hiero extraction, it is a type 1 rule; If not (i.e., this rule is only
available in GHKM extraction), it is a type 2 rule. E.g., the SCFG rule in Figure 3 is a type 2 rule because
it is not available in the original Hiero-style SCFG but can be generated from the tree-to-string rule.
Next we describe how each of these rule types are applied in decoding. We also describe which
features are used and how they are computed for each rule type.
3.2 Decoding
Both types of SCFG rules can be employed by usual Hiero decoders with a slight modification. Here
we follow the description of Hiero decoding by Iglesias et al. (2011). The source sentence is parsed
under the Hiero grammar using the CYK algorithm. Each cell in the CYK grid has associated with it a
list of rules that apply to its span; these rules are used to construct a recursive transition network (RTN)
which represents all translations of the source sentence under the grammar. The RTN is expanded to a
weighted finite state automaton for composition with n-gram language models (de Gispert et al., 2010).
Translations are produced via shortest path computation.
This procedure accommodates type 1 rules directly. For tree-to-string rules associated with type 2, we
attempt to match rules to the source syntactic tree. If a match is found: the source span of the matching
tree fragment is noted and the CYK cell for that span is selected; the tree-to-string rule is converted to
a Hiero-style rule; and that rule is added to the list of rules in the selected CYK cell. Once this process
is finished, RTN construction, expansion, and language model composition proceeds as usual. Similar
modifications could be made to incorporate these rules into cube pruning (Chiang, 2007), cube growing
(Huang and Chiang, 2007), and PDT intersection and expansion (Iglesias et al., 2011). We now elaborate
on the rule matching strategy.
Type 1 Rules The source sentence is parsed as is usual in Hiero-style translation, with the exception
that we impose no span limit on rule applications for source spans corresponding to constituents in the
Chinese syntactic tree. Rule matching, the procedure that determines if a rule applies to a source span, is
based on string matching (see Figure 4(a)). For example, the type 1 rule h
9
in Figure 4(c) can be applied
to spans (1,13) and (2,13) since both of them agree with tree constituents (see Figure 4(b)). But h
9
is
not applied to span (3,13) because that span is longer than 10 words and agrees with no syntactic tree
constituent.
Type 2 Rules If the source side of a tree-to-string rule matches an input tree fragment: 1) that rule
is converted to a Hiero-style SCFG rule (Section 3.1); and 2) the Hiero-style rule is added to the rules
linked with the CYK grid cell associated with the span of the source syntactic tree fragment. Here, rules
are applied via tree matching. For example, rule h
11
in Figure 4(b) matches the tree fragment spanning
positions (2,13).
It is worth noting that some type 1 rules may be found via both Hiero-style and tree-to-string grammar
extraction. In this case we monitor whether a rule can be applied as a tree-to-string rule using tree-
matching so that features (Section 3.3) and weights can be set appropriately. As an example, rule h
10
in
Figure 4 is available in both extraction methods. For span (2,11), this rule can be matched via both string
matching and tree matching. We then note that we can apply h
10
as a tree-to-string rule for span (2, 11)
and activate the corresponding features defined in Section 3.3. For other spans (e.g., spans (2,3)-(2,10)),
no tree fragments can be matched and the baseline features are used for h
10
.
3.3 Features
The baseline feature set used in this work consists of 12 features (Pino et al., 2013), including a 4-gram
language model, a strong 5-gram language model, bidirectional translation probabilities, bidirectional
lexical weights, a word count, a phrase count, a glue rule count, a frequency-1 rule count, a frequency-2
2067
h9
: X? ?
X
1
?? , satisfied with X1 ?
???
1
?
2
??
3
?
4
?
5
?
6
?
7
??
8
?
9
??
10
??
11
??
12
??
13
. . .
.
.
.
.
.
.
.
.
.
Chart Used in Decoding
span
(10,13)
matching
(a) matching a type 1 rule (h
9
) with the input string
IP
NP
NR
???
1
VP
PP
P
?
2
NP
??
3
?
4
?
5
?
6
?
7
??
8
?
9
??
10
??
11
VP
VV
??
12
NN
??
13
VP(PP(P(?) x
1
:NP) x
2
:VP)
? x
2
with x
1
h
11
: X? ?? X
1
X
2
,
X
2
with X
1
?
converting
. . .
.
.
.
.
.
.
.
.
.
Chart Used in Decoding
matching
span
(2,13)
(b) matching a type 2 rule (h
11
) with the input parse tree
ID Type Hiero-style Rule Tree-to-string Rule Applicable Spans
h
8
type 1 X? ?????, is satisfied ? N/A (12,13)
h
9
type 1 X? ? X
1
??, satisfied with X
1
? N/A (i,13), i = 1, 2 or 4 ? i ? 12
h
10
type 1 X? ?? X
1
, with X
1
? PP(P(?) x
1
NP)? with NPx
1
(2,j), 3 ? j ? 11 or j = 13
h
11
type 2 X? ?? X
1
X
2
, X
2
with X
1
? VP(PP(P(?) x
1
:NP) x
2
:VP) (2,13)
? x
2
with x
1
(c) example rules used in decoding
Figure 4: Decoding with both Hiero-style and tree-to-string grammars (span limit = 10). A span (i,j)
means spanning from position i to position j.
rule count, and a larger-than-frequency-2 rule count
2
. In addition, we introduce several features for
applying tree-to-string rules.
? Rule type indicators. We consider four indicator features, indicating tree-to-string rules, lexicalized
tree-to-string rules, rules with consecutive non-terminals, and non-lexicalized rules. Note that the tree-
to-string rule indicator feature is in principle a generalization of the soft syntactic features (Marton and
Resnik, 2008), in that a bonus (or penalty) is applied when a rule application is consistent with a source
tree constituent. The difference lies in that the tree-to-string rule indicator feature does not distinguish
between different syntactic labels, whereas soft syntactic features do.
? Features in syntactic MT. In general tree-to-string rules have their own features which are different
from those used in Hiero-style systems. For example, the features in syntactic MT systems can be
defined as the generation probabilities conditioned on the root symbol of the tree-fragment. Here we
choose five popular features used in syntactic MT systems, including the bi-directional phrase-based
conditional translation probabilities (Marcu et al., 2006) and three syntax-based conditional probabil-
ities (Mi and Huang, 2008). All these probabilities can be computed by relative-frequency estimates.
For example, the phrase-based features are the probabilities of translating between the frontier nodes
of s
r
and t
r
. The syntax-based features are the probabilities of generating r conditioned on its root,
2
We experimented with soft syntactic features (Marton and Resnik, 2008) but found no improvement over our baseline
system.
2068
source and target language sides, respectively. More formally, we use the following estimates for these
probabilities:
P
phr
(t
r
| s
r
) =
?
r
??
:?(s
r
??
)=?(s
r
)?t
r
??
=t
r
c(r
??
)
?
r
?
:?(s
r
?
)=?(s
r
)
c(r
?
)
P
phr
(s
r
| t
r
) =
?
r
??
:?(s
r
??
)=?(s
r
)?t
r
??
=t
r
c(r
??
)
?
r
?
:t
r
?
=t
r
c(r
?
)
P(r | root(r)) =
c(r)
?
r
?
:root(r
?
)=root(r)
c(r
?
)
P(r | s
r
) =
c(r)
?
r
?
:s
r
?
=s
r
c(r
?
)
P(r | t
r
) =
c(r)
?
r
?
:t
r
?
=t
r
c(r
?
)
where c(r) is the count of r, and root(?) and ?(?) are functions that return the source root symbol for
a tree-to-string rule and the sequence of leaf nodes for a tree-fragment respectively.
4 Evaluation
4.1 Experimental Setup
We report results in the NIST MT12 Chinese-English task, where our baseline system was among the top
academic systems. The parallel training corpus consists of 9.2 million sentence pairs which are provided
within the NIST Chinese-English MT12 track. Word alignments are obtained using MTTK (Deng and
Byrne, 2008) in both Chinese-to-English and English-to-Chinese directions, and then unioning the links.
The data from newswire and web genres was used for tuning and test. The development sets contain
1,755 sentences and 2160 sentences for the two genres respectively. The test sets (newswire: 1,779
sentences, web: 1768 sentences) contain all newswire and web evaluation data of MT08 (mt08), MT12
(mt12), and MT08 progress test (mt08.p). All Chinese sentences in the training, development and test
sets were parsed using the Berkeley parser (Petrov and Klein, 2007). A Kneser-Ney 4-gram language
model was trained on the AFP and Xinhua portions of the English Gigaword in addition to the English
side of the parallel corpus. A stronger 5-gram language model was trained on all English data of NIST
MT12 and the Google counts corpus using the ?stupid? backoff method (Brants et al., 2007).
For decoding we use HiFST, which is implemented with weighted finite state transducers (de Gispert
et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the
translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring
of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from
the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al.,
2009). The span limit was set to 10 in extracting basic phrases and decoding. All features weights were
optimized using lattice-based minimum error rate training (Macherey et al., 2008).
For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and
extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we
restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded
lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules
with a Chinese-to-English translation probability of < 0.10.
4.2 Results
We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments
are organized as follows:
? Baseline and Span Limits (exp01 and exp02)
First we study the effect of removing the span limit for tree constituents, that is, SCFG rules can be
2069
Entry System Newswire Web
tune mt08 mt12 mt08.p all test tune mt08 mt12 mt08.p all test
(1755) (691) (400) (688) (1779) (2160) (666) (420) (682) (1768)
exp01 baseline 35.84 35.85 35.47 35.50 35.63* 29.98 25.15 23.07 27.19 25.33*
exp02 += no span limit 36.05 36.08 35.70 35.54 35.79* 30.11 25.28 23.08 27.17 25.37*
exp03 += t-to-s rules 36.63 36.51 36.08 36.09 36.25* 30.80 26.00 23.08 27.80 25.83*
exp04 += t-to-s features 36.82 36.49 36.53 36.16 36.38* 30.91 26.03 23.27 27.85 25.98*
exp05 t-to-s baseline 34.63 34.44 34.87 33.66 34.25* 28.30 23.40 21.38 25.30 23.56*
exp06 exp04 on spans > 10 36.17 36.11 35.71 35.86 35.92* 30.18 25.30 23.12 27.36 25.45*
exp07 exp04 with null trans. 36.10 36.03 35.35 34.86 35.42* 29.96 25.32 22.58 23.33 24.12*
exp08 exp04 + left binariz. 37.11 37.46 37.03 36.30 36.91* 31.18 26.15 23.54 27.98 26.13*
exp09 exp04 + right binariz. 36.58 36.56 36.41 35.70 36.20* 31.06 25.94 23.47 27.48 25.88*
exp10 exp04 + forest binariz. 37.03 37.27 37.09 36.62 36.98* 31.20 25.99 23.59 28.09 26.15*
Table 1: Case-insensitive BLEU[%] scores of various systems. += means incrementally adding method-
s/features to the previous system. * means that a system is significantly different than the exp01 baseline
at p < 0.01.
applied to any spans when they respect the tree constituents of the input tree. It can be regarded as
the simplest way of using source syntax in Hiero-style systems. Seen from Table 1, removing the
span limit shows modest BLEU improvements. It agrees with the previous result that loosening the
constraints on spans is helpful to systems based on the hard syntactic constraints (Li et al., 2013).
? GHKM+Hiero (exp03 and exp04)
The results of our proposed approach (w/o new features) are reported in exp03 and exp04. We see that
incorporating tree-to-string rules yields +0.6 and +0.5 improvements on the collected newswire and
web test sets (exp03 vs exp01). The new features (Section 3.3) give a further improvement (exp04 vs
exp03). This result confirms that the system can learn a preference for certain types of rules using the
new features.
? Impact of Search Space (exp05)
We also study the impact of search space on system performance. To do this, we force the improved
system (exp04) to respect source tree constituents and to discard any hypotheses which violate the
tree constituent constraints. Seen from exp05, this system has a lower BLEU score than both the
Hiero baseline (exp01) and GHKM+Hiero system (exp04), strongly suggesting that restricting MT
systems to a smaller space of hypotheses is harmful.
? GHKM+Hiero, Spans > 10 Only (exp06)
Another interesting question is whether tree-to-string rules and features are more helpful to larger
spans. We restricted our approach to spans > 10 only and conducted another experiment. As is shown
in exp06, applying tree-to-string rules and features for large spans is beneficial (exp06 vs. exp01). But
it underperforms the system with the full use of tree-to-string rules (exp06 vs. exp04). This interesting
observation implies that applying tree-to-string rules on smaller spans introduces good hypotheses that
can be selected with our additional features.
? Impact of Failed Parses (exp07)
As noted in Section 3, the parser fails to parse some of the sentences in our experiments. In this case
our approach generates the baseline result using the Hiero model (i.e., type 1 rules only). To investigate
the effect of failed parse trees on system performance, we also report the BLEU score including null
translations for which the parser fails. As shown in exp07, there are significantly lower BLEU scores
when null translations are included. It indicates that our approach is more robust than standard tree-
to-string systems which would generate an empty translation if the source language parser fails.
? Results on Binarization (exp08-10)
Tree binarization is a widely used method to improve syntactic MT systems (Wang et al., 2010).
exp08-10 show the results of our improved system with left-heavy, right-heavy and forest-based bina-
2070
Reference: After North Korea demanded concessions from U.S. again before the start of a new round of six-nation talks , ...
Baseline: In the new round of six-nation talks on North Korea again demanded that U.S. in the former promise concessions , ...
GHKM+Hiero: After
North Korea again demanded that U.S. promised concessions before the new round of six-nation talks
, ...
a Hiero rule X? ?? X
1
?, after X
1
? is applied on span (1,15)
Input:
IP
PP
P
?
1
LCP
IP
??
2
??
3
??
4
??
5
?
6
?
7
??
8
?
9
?
10
??
11
?
12
??
13
??
14
LC
?
15
PU
,
VP
...
Reference: The Chinese star performance troupe presented a wonderful Peking opera as well as singing and dancing
Reference: performance to Hong Kong audience .
Baseline: Star troupe of China, highlights of Peking opera and dance show to the audience of Hong Kong .
GHKM+Hiero: Chinese star troupe presented a wonderful Peking opera singing and dancing
to
Hong Kong audience
.
Input:
A tree-to-string rule is applied:
(VP BA(?) x
1
:NP x
2
:VP PP(P(?) x
3
:NP))
? x
2
x
1
to x
3
IP
NP
??
1
??
2
???
3
VP
BA
?
4
NP
?
5
?
6
??
7
?
8
??
9
??
10
VP
VV
??
11
PP
P
?
12
NP
??
13
??
14
.
Figure 5: Comparison of translations generated by the baseline and improved systems.
rization
3
. We see that left-heavy binarization is very helpful and exp08 achieves overall improvements
of 1.2 and 0.8 BLEU points on the newsire and web data. In contrast, right-heavy binarization does
not yield promising performance. This agrees with the previous report (Wang et al., 2010) that MT
systems prefer to use certain ways of binarization in most cases. exp10 shows that the additional trees
introduced in our forest-based scheme are not sufficient to make a big impact on BLEU scores. Pos-
sibly larger gains can be obtained if taking a forest of parse trees from the source parser, but this is
outside the scope of this paper.
4.3 Analysis
We then analyse rule usage in the 1-best derivations for our improved system on the tuning set. We find
that type 2 rules represent 13.97% of the rules used in the 1-best derivations. Also, 44.45% of the applied
rules are available from the tree-to-string model (i.e., rules that use the features described in Section 3.3).
These numbers indicate that the tree-to-string rules are beneficial and our decoder likes to use them.
Finally, we discuss two real translation examples from our tuning set. See Figure 5 for translations
generated by different systems. In the first example, the Chinese input sentence contains? ...? which
is usually translated into after ... (i.e., a Hiero rule X? ?? X
1
?, after X
1
?). However, because the
?? ...?? pattern spans 15 words and that is beyond the span limit, our baseline is unable to apply this
desired rule and chooses a wrong translation in for the Chinese word ?. When the source parse tree
3
We found that the CTB-style parse trees usually have a very flat top-level IP (i.e., single clause) tree structure. As the IP
structure in Chinese is very complicated, the system might prefer a flexible binarization scheme. Thus we considered both left
and right-heavy binarization to form a binarization forest for IPs in Chinese parse trees, and binarized other tree constituents in
a left-heavy fashion.
2071
is available, our approach removes the span limit for spans that agree with the tree constituents. In this
case, the MT system successfully applies the rule on span (1, 15) and generates a much better translation.
In the second example, the translation of the input sentence requires complex reordering of adjacent
constituents. The baseline system cannot handle this case and generates a monotonic translation using
the glue rules. This results in a wrong order for the translation of Chinese verb?? (show). By contrast,
the improved system chooses a tree-to-string rule with three non-terminals (some of which are adjacent
in the source language) and perfectly performs a syntactic movement of the required tree constituents.
5 Related Work
Recently linguistically-motivated models have been intensively investigated in MT. In particular, source
tree-based models (Liu et al., 2006; Huang et al., 2006; Eisner, 2003; Zhang et al., 2008; Liu et al.,
2009a; Xie et al., 2011) have received growing interest due to their good abilities in modelling source
language syntax for better lexicon selection and reordering. Alternatively, the hierarchical phrase-based
approach (Chiang, 2005) considers the underlying hierarchical structures of sentences but does not re-
quire linguistically syntactic trees on either language side.
There are several lines of work for augmenting hierarchical phrase-based systems with the use of
source language phrase-structure trees. Liu et al. (2009b) describe novel approaches to translation under
multiple translation grammars. Their approach is very much motivated by system combination, and they
develop procedures for joint decoding and optimisation within a single system that give the benefit of
combining hypotheses from multiple systems. They demonstrate their approach by combining full tree-
to-string and Hiero systems. Our approach is much simpler and emphasises changes to the grammar
rather than the decoder or its parameter optimisation (MERT). Our aim is to augment the search space
of Hiero with linguistically-motivated hypotheses, and not to develop a new decoder that is capable of
translation under multiple grammars. Moreover, we consider Hiero as the backbone model and only
introduce tree-to-string rules where they can contribute; we show that extracting tree-to-string rules from
just 10% of the data suffices to get good gains. This results in a small number of tree-to-string rules and
does not slow down the decoder.
Another related line of work is to introduce syntactic constraints or annotations to hierarchical phrase-
based systems. Marton and Resnik (2008) and Li et al. (2013) proposed several soft or hard constraints to
model syntactic compatibility of Hiero derivations and input source language parse trees. We note that,
despite significant development effort, we were not able to improve our baseline through the use of these
soft syntactic constraints; it was this experience that led us to develop the hybrid approach described in
this paper.
Several research groups used syntactic labels as non-terminal symbols in their SCFG rules and develop
new features (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010; Hoang and
Koehn, 2010). However, all these methods still resort to rule extraction procedures similar to that of the
standard phrase/hierarchical rule extraction method. In contrast, we use the GHKM method which is a
mature technique to extract rules from tree-string pairs but does not impose those Hiero-style constraints
on rule extraction. More importantly, we consider the hierarchical syntactic tree structure to make use of
well-formed rules in decoding, while such information is not used in standard SCFG-based systems. We
also keep to the simpler non-terminals of Hiero, and do not ?decorate? any non-terminals with syntactic
or other information.
6 Conclusion
We have presented an approach to improving Hiero-style systems by augmenting the SCFG with tree-
to-string rules and syntax-based features. The input parse trees are used to introduce new linguistically-
sensible hypotheses into the translation search space while maintaining the Hiero robustness qualities
and avoiding computational explosion. We obtain significant improvements over a strong Hiero baseline
in Chinese-to-English. Further improvements are achieved when applying tree binarization.
2072
Acknowledgements
This work was done while the first author was visiting the speech group at University of Cambridge, and
was supported in part by the National Science Foundation of China (Grants 61272376 and 61300097),
and the China Postdoctoral Science Foundation (Grant 2013M530131). We would like to thank the
anonymous reviewers for their pertinent and insightful comments. We also would like to thank Juan
Pino, Rory Waite, Federico Flego and Gonzalo Iglesias for building parts of the baseline system.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large Language Models in
Machine Translation. In Proceedings of EMNLP-CoNLL, pages 858?867, Prague, Czech Republic.
David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, Michigan, USA.
David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33:45?60.
David Chiang. 2010. Learning to Translate with Source and Target Syntax. In Proceedings of ACL, pages 1443?
1452, Uppsala, Sweden.
Adri`a de Gispert, Gonzalo Iglesias, Graeme Blackwood, Eduardo R. Banga, and William Byrne. 2010. Hierarchi-
cal Phrase-Based Translation with Weighted Finite-State Transducers and Shallow-n Grammars. Computational
Linguistics, 36(3):505?533.
Yonggang Deng and William Byrne. 2008. HMM Word and Phrase Alignment for Statistical Machine Translation.
IEEE Transactions on Audio, Speech & Language Processing, 16(3):494?507.
Jason Eisner. 2003. Learning Non-Isomorphic Tree Mappings for Machine Translation. In Proceedings of ACL,
pages 205?208, Sapporo, Japan.
Yang Feng, Yang Liu, Qun Liu, and Trevor Cohn. 2012. Left-to-Right Tree-to-String Decoding with Prediction.
In Proceedings of EMNLP-CoNLL, pages 1191?1200, Jeju Island, Korea.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thay-
er. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proceedings of
COLING-ACL, pages 961?968, Sydney, Australia.
Hieu Hoang and Philipp Koehn. 2010. Improved translation with source syntax labels. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 409?417, Uppsala, Sweden.
Liang Huang and David Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models. In
Proceedings of ACL, pages 144?151, Prague, Czech Republic.
Liang Huang and Haitao Mi. 2010. Efficient Incremental Decoding for Tree-to-String Translation. In Proceedings
of EMNLP, pages 273?283, Cambridge, MA, USA.
Liang Huang, Knight Kevin, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain
of locality. In Proceedings of AMTA, pages 66?73, Cambridge, MA, USA.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne. 2009. Rule Filtering by Pattern for
Efficient Hierarchical Translation. In Proceedings of EACL, pages 380?388, Athens, Greece.
Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adri`a de Gispert, and Michael Riley. 2011. Hierarchical Phrase-
based Translation Representations. In Proceedings of EMNLP, pages 1373?1383, Edinburgh, Scotland, UK.
Junhui Li, Philip Resnik, and Hal Daum?e III. 2013. Modeling Syntactic and Semantic Structures in Hierarchical
Phrase-based Translation. In Proceedings of NAACL-HLT, pages 540?549, Atlanta, Georgia.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-String Alignment Template for Statistical Machine Transla-
tion. In Proceedings of COLING-ACL, pages 609?616, Sydney, Australia.
Yang Liu, Yajuan L?u, and Qun Liu. 2009a. Improving Tree-to-Tree Translation with Packed Forests. In Proceed-
ings of ACL-IJCNLP, pages 558?566, Suntec, Singapore.
2073
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009b. Joint decoding with multiple translation models. In
Proceedings of ACL-IJCNLP, pages 576?584, Suntec, Singapore.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based Minimum Error Rate
Training for Statistical Machine Translation. In Proceedings of EMNLP, pages 725?734, Honolulu, Hawaii.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical Machine Translation
with Syntactified Target Language Phrases. In Proceedings of EMNLP, pages 44?52, Sydney, Australia.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrased-Based Translation. In
Proceedings of ACL-HLT, pages 1003?1011, Columbus, Ohio.
Haitao Mi and Liang Huang. 2008. Forest-based Translation Rule Extraction. In Proceedings of EMNLP, pages
206?214, Honolulu, Hawaii, USA.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-Based Translation. In Proceedings of ACL-HLT, pages
192?199, Columbus, Ohio.
Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. Bleu: a Method for Automatic Evaluation
of Machine Translation. In Proceedings of ACL, pages 311?318, Philadelphia, PA, USA.
Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLT-NAACL,
pages 404?411, Rochester, New York, USA.
Juan Pino, Aurelien Waite, Tong Xiao, Adri`a de Gispert, Federico Flego, and William Byrne. 2013. The University
of Cambridge Russian-English system at WMT13. In Proceedings of WMT, pages 200?205, Sofia, Bulgaria.
David Vilar, Daniel Stein, Stephan Peitz, and Hermann Ney. 2010. If i only had a parser: poor man?s syntax for
hierarchical machine translation. In Proceedings of IWSLT, pages 345?352.
Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu. 2010. Re-structuring, Re-labeling, and Re-aligning
for Syntax-Based Machine Translation. Computational Linguistics, 36(2):247?277.
Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li. 2012. NiuTrans: An Open Source Toolkit for Phrase-based
and Syntax-based Machine Translation. In Proceedings of ACL: System Demonstrations, pages 19?24, Jeju
Island, Korea.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation.
In Proceedings of EMNLP, pages 216?226, Edinburgh, Scotland.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A Tree Sequence
Alignment-based Tree-to-Tree Translation Model. In Proceedings of ACL-HLT, pages 559?567, Columbus,
Ohio, USA.
Bing Zhao and Yaser Al-Onaizan. 2008. Generalizing Local and Non-Local Word-Reordering Patterns for Syntax-
Based Machine Translation. In Proceedings of EMNLP, pages 572?581, Honolulu, Hawaii.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing. In
Proceedings of WMT, pages 138?141, New York City.
2074
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239?248,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Source-side Preordering for Translation using Logistic Regression and
Depth-first Branch-and-Bound Search
?
Laura Jehl
?
Adri
`
a de Gispert
?
Mark Hopkins
?
William Byrne
?
?
Dept. of Computational Linguistics, Heidelberg University. 69120 Heidelberg, Germany
jehl@cl.uni-heidelberg.de
?
SDL Research. East Road, Cambridge CB1 1BH, U.K.
{agispert,mhopkins,bbyrne}@sdl.com
Abstract
We present a simple preordering approach
for machine translation based on a feature-
rich logistic regression model to predict
whether two children of the same node
in the source-side parse tree should be
swapped or not. Given the pair-wise chil-
dren regression scores we conduct an effi-
cient depth-first branch-and-bound search
through the space of possible children per-
mutations, avoiding using a cascade of
classifiers or limiting the list of possi-
ble ordering outcomes. We report exper-
iments in translating English to Japanese
and Korean, demonstrating superior per-
formance as (a) the number of crossing
links drops by more than 10% absolute
with respect to other state-of-the-art pre-
ordering approaches, (b) BLEU scores im-
prove on 2.2 points over the baseline with
lexicalised reordering model, and (c) de-
coding can be carried out 80 times faster.
1 Introduction
Source-side preordering for translation is the task
of rearranging the order of a given source sen-
tence so that it best resembles the order of the tar-
get sentence. It is a divide-and-conquer strategy
aiming to decouple long-range word movement
from the core translation task. The main advan-
tage is that translation becomes computationally
cheaper as less word movement needs to be con-
sidered, which results in faster and better transla-
tions, if preordering is done well and efficiently.
Preordering also can facilitate better estimation
of alignment and translation models as the paral-
lel data becomes more monotonically-aligned, and
?
This work was done during an internship of the first au-
thor at SDL Research, Cambridge.
translation gains can be obtained for various sys-
tem architectures, e.g. phrase-based, hierarchical
phrase-based, etc.
For these reasons, preordering has a clear re-
search and commercial interest, as reflected by the
extensive previous work on the subject (see Sec-
tion 2). From these approaches, we are particu-
larly interested in those that (i) involve little or no
human intervention, (ii) require limited computa-
tional resources at runtime, and (iii) make use of
available linguistic analysis tools.
In this paper we propose a novel preordering
approach based on a logistic regression model
trained to predict whether to swap nodes in
the source-side dependency tree. For each pair
of sibling nodes in the tree, the model uses a
feature-rich representation that includes lexical
cues to make relative reordering predictions be-
tween them. Given these predictions, we conduct
a depth-first branch-and-bound search through
the space of possible permutations of all sibling
nodes, using the regression scores to guide the
search. This approach has multiple advantages.
First, the search for permutations is efficient and
does not require specific heuristics or hard limits
for nodes with many children. Second, the inclu-
sion of the regression prediction directly into the
search allows for finer-grained global decisions as
the predictions that the model is more confident
about are preferred. Finally, the use of a single
regression model to handle any number of child
nodes avoids incurring sparsity issues, while al-
lowing the integration of a vast number of features
into the preordering model.
We empirically contrast our proposed method
against another preordering approach based on
automatically-extracted rules when translating En-
glish into Japanese and Korean. We demonstrate
a significant reduction in number of crossing links
of more than 10% absolute, as well as translation
gains of over 2.2 BLEU points over the baseline.
239
We also show it outperforms a multi-class classifi-
cation approach and analyse why this is the case.
2 Related work
One useful way to organize previous preordering
techniques is by how they incorporate linguistic
knowledge.
On one end of the spectrum we find those ap-
proaches that rely on syntactic parsers and hu-
man knowledge, typically encoded via a set of
hand-crafted rules for parse tree rewriting or trans-
formation. Examples of these can be found
for French-English (Xia and McCord, 2004),
German-English (Collins et al., 2005), Chinese-
English (Wang et al., 2007), English-Arabic (Badr
et al., 2009), English-Hindi (Ramanathan et al.,
2009), English-Korean (Hong et al., 2009), and
English-Japanese (Lee et al., 2010; Isozaki et
al., 2010). A generic set of rules for transform-
ing SVO to SOV languages has also been de-
scribed (Xu et al., 2009). The main advantage of
these approaches is that a relatively small set of
good rules can yield significant improvements in
translation. The common criticism they receive is
that they are language-specific.
On the other end of the spectrum, there are pre-
ordering models that rely neither on human knowl-
edge nor on syntactic analysis, but only on word
alignments. One such approach is to form a cas-
cade of two translation systems, where the first
one translates the source to its preordered ver-
sion (Costa-juss`a and Fonollosa, 2006). Alterna-
tively, one can define models that assign a cost to
the relative position of each pair of words in the
sentence, and search for the sequence that opti-
mizes the global score as a linear ordering prob-
lem (Tromble and Eisner, 2009) or as a travel-
ing salesman problem (Visweswariah et al., 2011).
Yet another line of work attempts to automatically
induce a parse tree and a preordering model from
word alignments (DeNero and Uszkoreit, 2011;
Neubig et al., 2012). These approaches are at-
tractive due to their minimal reliance on linguistic
knowledge. However, their findings reveal that the
best performance is obtained when using human-
aligned data which is expensive to create.
Somewhere in the middle of the spectrum are
works that rely on automatic source-language syn-
tactic parses, but no direct human intervention.
Preordering rules can be automatically extracted
from word alignments and constituent trees (Li
et al., 2007; Habash, 2007; Visweswariah et
al., 2010), dependency trees (Genzel, 2010) or
predicate-argument structures (Wu et al., 2011),
or simply part-of-speech sequences (Crego and
Mari?no, 2006; Rottmann and Vogel, 2007). Rules
are assigned a cost based on Maximum En-
tropy (Li et al., 2007) or Maximum Likelihood es-
timation (Visweswariah et al., 2010), or directly
on their ability to make the training corpus more
monotonic (Genzel, 2010). The latter performs
very well in practice but comes at the cost of a
brute-force extraction heuristic that cannot incor-
porate lexical information. Recently, other ap-
proaches treat ordering the children of a node as
a learning to rank (Yang et al., 2012) or discrimi-
native multi-classification task (Lerner and Petrov,
2013). These are appealing for their use of finer-
grained lexical information, but they struggle to
adequately handle nodes with multiple children.
Our approach is closely related to this latter
work, as we are interested in feature-rich discrim-
inative approaches that automatically learn pre-
ordering rules from source-side dependency trees.
Similarly to Yang et al. (2012) we train a large
discriminative linear model, but rather than model
each child?s position in an ordered list of children,
we model a more natural pair-wise swap / no-swap
preference (like Tromble and Eisner (2009) did at
the word level). We then incorporate this model
into a global, efficient branch-and-bound search
through the space of permutations. In this way, we
avoid an error-prone cascade of classifiers or any
limit on the possible ordering outcomes (Lerner
and Petrov, 2013).
3 Preordering using logistic regression
and branch-and-bound search
Like Genzel (2010), our method starts with depen-
dency parses of source sentences (which we con-
vert to shallow constituent trees; see Figure 1 for
an example), and reorders the source text by per-
muting sibling nodes in the parse tree. For each
non-terminal node, we first apply a logistic regres-
sion model which predicts, for each pair of child
nodes, the probability that they should be swapped
or kept in their original order. We then apply
a depth-first branch-and-bound search to find the
global optimal reordering of children.
240
VB
he
NN
1
could
MD
2
stand
VB
3
NN
4
the
DT
smell
NN
nsubj
aux
HEAD
dobj
det HEAD
Figure 1: Shallow constituent tree generated from
the dependency tree. Non-terminal nodes inherit
the tag from the head.
3.1 Logistic regression
We build a regression model that assigns a prob-
ability of swapping any two sibling nodes, a and
b, in the source-side dependency tree. The proba-
bility of swapping them is denoted p(a, b) and the
probability of keeping them in their original order
is 1 ? p(a, b). We use LIBLINEAR (Fan et al.,
2008) for training an L1-regularised logistic re-
gression model based on positively and negatively
labelled samples.
3.1.1 Training data
We generate training examples for the logistic re-
gression from word-aligned parallel data which is
annotated with source-side dependency trees. For
each non-terminal node, we extract all possible
pairs of child nodes. For each pair, we obtain a
binary label y ? {?1, 1} by calculating whether
swapping the two nodes would reduce the number
of crossing alignment links. The crossing score of
having two nodes a and b in the given order is
cs(a, b) := |{(i, j) ? A
a
?A
b
: i > j}|
where A
a
and A
b
are the target-side positions to
which the words spanned by a and b are aligned.
The label is then given as
y(a, b) =
{
1 , cs(a, b) > cs(b, a)
?1 , cs(b, a) > cs(a, b)
Instances for which cs(a, b) = cs(b, a) are not
included in the training data. This usually happens
if either A
a
or A
b
is empty, and in this case the
alignments provide no indication of which order
is better. We also discard any samples from nodes
that have more than 16 children, as these are rare
cases that often result from parsing errors.

1
2 3 4
2 3
2
2
1
. . .
. . .
Figure 2: Branch-and-bound search: Partial search
space of permutations for a dependency tree node
with four children. The gray node marks a goal
node. For the root node of the tree in Figure 1, the
permutation corresponding to this path (1,4,3,2)
would produce ?he the smell stand could?.
3.1.2 Features
Using a machine learning setup allows us to in-
corporate fine-grained information in the form of
features. We use the following features to charac-
terise pairs of nodes:
l The dependency labels of each node
t The part-of-speech tags of each node.
hw The head words and classes of each node.
lm, rm The left-most and right-most words and classes
of a node.
dst The distances between each node and the head.
gap If there is a gap between nodes, the left-most
and right-most words and classes in the gap.
In order to keep the size of our feature space
manageable, we only consider features which oc-
cur at least 5 times
1
. For the lexical features, we
use the top 100 vocabulary items from our training
data, and 51 clusters generated by mkcls (Och,
1999). Similarly to previous work (Genzel, 2010;
Yang et al., 2012), we also explore feature con-
junctions. For the tag and label classes, we gen-
erate all possible combinations up to a given size.
For the lexical and distance features, we explicitly
specify conjunctions with the tag and label fea-
tures. Results for various feature configurations
are discussed in Section 4.3.1.
3.2 Search
For each non-terminal node in the source-side de-
pendency tree, we search for the best possible
1
Additional feature selection is achieved through L1-
regularisation.
241
permutation of its children. We define the score
of a permutation pi as the product of the proba-
bilities of its node pair orientations (swapped or
unswapped):
score(pi) =
?
1?i<j?k|pi[i]>pi[j]
p(i, j)
?
?
1?i<j?k|pi[i]<pi[j]
1? p(i, j)
Here, we represent a permutation pi of k nodes
as a k-length sequence containing each integer in
{1, ..., k} exactly once. Define a partial permu-
tation of k nodes as a k
?
< k length sequence
containing each integer in {1, ..., k} at most once.
We can construct a search space over partial per-
mutations in the natural way (see Figure 2). The
root node represents the empty sequence  and has
score 1. Then, given a search node representing
a k
?
-length partial permutation pi
?
, its successor
nodes are obtained by extending it by one element:
score(pi
?
? ?i?) = score(pi
?
)
?
?
j?V |i>j
p(i, j)
?
?
j?V |i<j
1? p(i, j)
where V = {1, ..., k}\(pi
?
? ?i?) is the set of source
child positions that have not yet been visited. Ob-
serve that the nodes at search depth k correspond
exactly to the set of complete permutations. To
search this space, we employ depth-first branch-
and-bound (Balas and Toth, 1983) as our search
algorithm. The idea of branch-and-bound is to
remember the best scoring goal node found thus
far, abandoning any partial paths that cannot lead
to a better scoring goal node. Algorithm 1 gives
pseudocode for the algorithm
2
. If the initial bound
(bound
0
) is set to 0, the search is guaranteed to
find the optimal solution. By raising the bound,
which acts as an under-estimate of the best scor-
ing permutation, search can be faster but possibly
fail to find any solution. All our experiments were
done with bound
0
= 0, i.e. exact search, but we
discuss search time in detail and pruning alterna-
tives in Section 4.3.2.
Since we use a logistic regression model and in-
corporate its predictions directly as swap probabil-
ities, our search prefers those permutations with
swaps which the model is more confident about.
2
See (Poole and Mackworth, 2010) for more details and a
worked example.
Algorithm 1 Depth-first branch-and-bound
Require: k: maximum sequence length, : empty sequence,
bound
0
: initial bound
procedure BNBSEARCH(, bound
0
, k)
best path? ?
bound? bound
0
SEARCH(??)
return best path
end procedure
procedure SEARCH(pi
?
)
if score(pi
?
) > bound then
if |pi
?
| = k then
best path? ?pi
?
?
bound? score(pi
?
)
return
else
for each i ? {1, ..., k}\pi
?
do
SEARCH(pi
?
? ?i?)
end for
end if
end if
end procedure
4 Experiments
4.1 Setup
We report translation results in English-to-
Japanese/Korean. Our corpora are comprised of
generic parallel data extracted from the web, with
some documents extracted manually and some au-
tomatically crawled. Both have about 6M sentence
pairs and roughly 100M words per language.
The dev and test sets are also generic. Source
sentences were extracted from the web and one
target reference was produced by a bilingual
speaker. These sentences were chosen to evenly
represent 10 domains, including world news,
chat/SMS, health, sport, science, business, and
others. The dev/test sets contain 602/903 sen-
tences and 14K/20K words each. We do English
part-of-speech tagging using SVMTool (Gim?enez
and M`arquez, 2004) and dependency parsing us-
ing MaltParser (Nivre et al., 2007).
For translation experiments, we use a phrase-
based decoder that incorporates a set of standard
features and a hierarchical reordering model (Gal-
ley and Manning, 2008) with weights tuned us-
ing MERT to optimize the character-based BLEU
score on the dev set. The Japanese and Korean lan-
guage models are 5-grams estimated on > 350M
words of generic web text.
For training the logistic regression model, we
automatically align the parallel training data and
intersect the source-to-target and target-to-source
alignments. We reserve a random 5K-sentence
242
approach EJ cs (%) EK cs (%)
rule-based (Genzel, 2010) 61.9 64.2
multi-class 65.2 -
df-bnb 51.4 51.8
Table 1: Percentage of the original crossing score
on the heldout set, obtained after applying each
preordering approach in English-Japanese (EJ,
left) and Korean (EK, right). Lower is better.
subset for intrinsic evaluation of preordering, and
use the remainder for model parameter estimation.
We evaluate our preordering approach with lo-
gistic regression and depth-first branch-and-bound
search (in short, ?df-bnb?) both in terms of reorder-
ing via crossing score reduction on the heldout set,
and in terms of translation quality as measured by
character-based BLEU on the test set.
4.2 Preordering baselines
We contrast our work against two data-driven pre-
ordering approaches. First, we implemented the
rule-based approach of Genzel (2010) and opti-
mised its multiple parameters for our task. We
report only the best results achieved, which corre-
spond to using ?100K training sentences for rule
extraction, applying a sliding window width of 3
children, and creating rule sequences of?60 rules.
This approach cannot incorporate lexical features
as that would make the brute-force rule extraction
algorithm unmanageable.
We also implemented a multi-class classifica-
tion setup where we directly predict complete per-
mutations of children nodes using multi-class clas-
sification (Lerner and Petrov, 2013). While this
is straightforward for small numbers of children,
it leads to a very large number of possible per-
mutations for larger sets of children nodes, mak-
ing classification too difficult. While Lerner and
Petrov (2013) use a cascade of classifiers and im-
pose a hard limit on the possible reordering out-
comes to solve this, we follow Genzel?s heuristic:
rather than looking at the complete set of children,
we apply a sliding window of size 3 starting from
the left, and make classification/reordering deci-
sions for each window separately. Since the win-
dows overlap, decisions made for the first window
affect the order of nodes in the second window,
etc. We address this by soliciting decisions from
the classifier on the fly as we preorder. One lim-
Figure 3: Crossing scores and classification accu-
racy improve with training data size.
itation of this approach is that it is able to move
children only within the window. We try to rem-
edy this by applying the method iteratively, each
time re-training the classifier on the preordered
data from the previous run.
4.3 Crossing score
We now report contrastive results in the intrin-
sic preordering task, as measured by the num-
ber of crossing links (Genzel, 2010; Yang et al.,
2012) on the 5K held-out set. Without preorder-
ing, there is an average of 22.2 crossing links in
English-Japanese and 20.2 in English-Korean. Ta-
ble 1 shows what percentage of these links re-
main after applying each preordering approach to
the data. We find that the ?df-bnb? method out-
performs the other approaches in both language
pairs, achieving more than 10 additional percent-
age points reduction over the rule-based approach.
Interestingly, the multi-class approach is not able
to match the rule-based approach despite using ad-
ditional lexical cues. We hypothesise that this is
due to the sliding window heuristic, which causes
a mismatch in train-test conditions: while samples
are not independent of each other at test time due
to window overlaps, they are considered to be so
when training the classifier.
4.3.1 Impact of training size and feature
configuration
We now report the effects of feature configura-
tion and training data size for the English-Japanese
case. We assess our ?df-bnb? approach in terms of
the classification accuracy of the trained logistic
243
features used acc (%) cs (%)
l,t,hw,lm,rm,dst,gap 82.43 51.3
l,t,hw,lm,rm,dst 82.44 51.4
l,t,hw,lm,rm 82.32 53.1
l,t,hw 82.02 55
l,t 81.07 58.4
Table 2: Ablation tests showing crossing scores
and classification accuracy as features are re-
moved. All models were trained on 8M samples.
regression model (using it to predict ?1 labels in
the held-out set) and by the percentage of crossing
alignment links reduced by preordering.
Figure 3 shows the performance of the logistic
regression model over different training set sizes,
extracted from the training corpus as described in
Section 3. We observe a constant increase in pre-
diction accuracy, mirrored by a steady decrease in
crossing score. However, gains are less for more
than 8M training examples. Note that a small vari-
ation in accuracy can produce a large variation in
crossing score if two nodes are swapped which
have a large number of crossing alignments.
Table 2 shows an ablation test for various fea-
ture configurations. We start with all features, in-
cluding head word and class (hw), left-most and
right-most word in each node?s span (lm, rm), each
node?s distance to the head (dst), and left-most
and right-most word of the gap between nodes
(gap). We then proceed by removing features to
end with only label and tag features (l,t), as in
Genzel (2010). For each configuration, we gener-
ated all tag- and label- combinations of size 2. We
then specified combinations between tag and label
and all other features. For the lexical features we
always used conjunctions of the word itself, and its
class. Class information is included for all words,
not just those in the top 100 vocabulary. Table 2
shows that lexical and distance feature groups con-
tribute to prediction accuracy and crossing score,
except for the gap features, which we omit from
further experiments.
4.3.2 Run time
We now demonstrate the efficiency of branch-and-
bound search for the problem of finding the opti-
mum permutation of n children at runtime. Even
though in the worst case the search could ex-
plore all n! permutations, making it prohibitive for
Figure 4: Average number of nodes explored in
branch-and-bound search by number of children.
nodes with many children, in practice this does
not happen. Many low-scoring paths are discarded
early by branch-and-bound search so that the opti-
mal solution can be found quickly. The top curve
in Figure 4 shows the average number of nodes
explored in searches run on our validation set (5K
sentences) as a function of the number of children.
All instances are far from the worst case
3
.
In our experiments, the time needed to conduct
exact search (bound
0
= 0) was not a problem ex-
cept for a few bad cases (nodes with more than 16
children), which we simply chose not to preorder;
in our data, 90% of the nodes have less than 6 chil-
dren, while only 0.9% have 10 children or more, so
this omission does not affect performance notice-
ably. We verified this on our held-out set, by car-
rying out exhaustive searches. We found that not
preordering nodes with 16 children did not worsen
the crossing score. In fact, setting a harsher limit
of 10 nodes would still produce a crossing score
of 51.9%, compared to the best score of 51.4%.
There are various ways to speed up the search,
if needed. First, one could impose a hard limit
on the number of explored nodes
4
. As shown
in Figure 4, a limit of 4K would still allow ex-
act search on average for permutations of up to
11 children, while stopping search early for more
children. We tested this for limits of 1K/4K nodes
and obtained crossing scores of 51.9/51.5%. Al-
ternatively, one could define a higher initial bound;
since the score of a path is a product of proba-
bilities, one would select a threshold probability
3
Note that 12!?479M nodes, whereas our search finds the
optimal permutation path after exploring <10K nodes.
4
As long as the limit exceeds the permutation length, a
solution will always be found as search is depth-first.
244
d approach ?LRM ? +LRM ?
baseline 25.39 - 26.62 -
rule-based 25.93 +0.54 27.65 +1.03
10
multi-class 25.60 +0.21 26.10 ?0.52
df-bnb 26.73 +1.34 28.09 +1.47
baseline 25.07 - 25.92 -
rule-based 26.35 +1.28 27.54 +1.62
4
multi-class 25.37 +0.30 26.31 +0.39
df-bnb 26.98 +1.91 28.13 +2.21
Table 3: English-Japanese BLEU scores with var-
ious preordering approaches (and improvement
over baseline) under two distortion limits d. Re-
sults reported both excluding and including lexi-
calised reordering model features (LRM).
p and calculate a bound depending on the size n
of the permutation as bound
0
= p
n?(n?1)
2
. Exam-
ples of this would be the lower curves of Figure 4.
The curve labels show the crossing score produced
with each threshold, and in parenthesis the per-
centage of searches that fail to find a solution with
a better score than bound
0
, in which case children
are left in their original order. As shown, this strat-
egy proves less effective than simply limiting the
number of explored nodes, because the more fre-
quent cases with less children remain unaffected.
4.4 Translation performance
Table 3 reports English-Japanese translation re-
sults for two different values of the distortion limit
d, i.e. the maximum number of source words that
the decoder is allowed to jump during search. We
draw the following conclusions. Firstly, all the
preordering approaches outperform the baseline
and the BLEU score gain they provide increases as
the distortion limit decreases. This is further anal-
ysed in Figure 5, where we report BLEU as a func-
tion of the distortion limit in decoding for both
English-Japanese and English-Korean. This re-
veals the power of preordering as a targeted strat-
egy to obtain high performance at fast decoding
times, since d can be drastically reduced with-
out performance degradation which leads to huge
decoding speed-ups; this is consistent with the
observations in (Xu et al., 2009; Genzel, 2010;
Visweswariah et al., 2011). We also find that with
preordering it is possible to apply harsher pruning
conditions in decoding while still maintaining the
Figure 5: BLEU scores as a function of distor-
tion limit in decoder (+LRM case). Top: English-
Japanese. Bottom: English-Korean.
exact same performance, achieving further speed-
ups. With preordering, our system is able to de-
code 80 times faster while producing translation
output of the same quality.
Secondly, we observe that the preordering
gains, which are correlated with the crossing score
reductions of Table 1, are largely orthogonal to
the gains obtained when incorporating a lexi-
calised reordering model (LRM). In fact, preorder-
ing gains are slightly larger with LRM, suggest-
ing that this reordering model can be better esti-
mated with preordered text. This echoes the notion
that reordering models are particularly sensitive
to alignment noise (DeNero and Uszkoreit, 2011;
Neubig et al., 2012; Visweswariah et al., 2013),
and that a ?more monotonic? training corpus leads
to better translation models.
Finally, ?df-bnb? outperforms all other preorder-
ing approaches, and achieves an extra 0.5?0.8
BLEU over the rule-based one even at zero distor-
tion limit. This is consistent with the substantial
crossing score reductions reported in Section 4.3.
We argue that these improvements are due to
the usage of lexical features to facilitate finer-
grained ordering decisions, and to our better
search through the children permutation space
which is not restricted by sliding windows, does
245
E
x
a
m
p
l
e
1
reference [
1
?????]
Barlow
[
2
???]
the smell
[
3
??]
endure
[
4
??????]
could
[
5
???]
hoped
[
6
?]
source [
1
Barlow] [
5
hoped] he [
4
could] [
3
stand] [
2
the smell] [
6
.]
preordered [
1
Barlow] he [
2
the smell] [
3
stand] [
4
could] [
5
hoped] [
6
.]
E
x
a
m
p
l
e
2
reference [
1
????]
my own
[
2
??]
experience
[
3
????]
in
, [
4
???????]
Rosa Parks
[
5
???]
called
[
6
???]
black
[
7
???]
woman
, [
8
???]
one day
[
9
????????]
somehow
[
10
???]
bus of
[
11
?????]
back seat in
[
12
??]
sit
??? [
13
????]
told being
[
14
???]
of
[
15
?????]
was fed up with
?
source [
3
In] [
1
my own] [
2
experience] , a [
6
black] [
7
woman] [
5
named] [
4
Rosa Parks] [
14
was just tired] [
8
one day]
[
14
of] [
13
being told] [
12
to sit] [
11
in the back] [
10
of the bus] .
rule-based [
1
my own] [
2
experience] [
3
In] [
14
was just tired] [
13
being told] [
10
the bus of] [
11
the back in] [
12
sit to] [
14
of]
[
8
one day] , [
6
a black] [
7
woman] [
4
Rosa Parks] [
5
named] .
df-bnb [
1
my own] [
2
experience] [
3
In] , [
5
named] [
6
a black] [
7
woman] [
4
Rosa Parks] [
10
the bus of] [
11
the back in]
[
12
sit to] [
13
told being] [
14
of] [
8
one day] [
14
was just tired] .
E
x
a
m
p
l
e
3
reference [
1
????]
we
?[
2
????]
quite
[
3
???]
Xi?an
[
4
??]
like
[
5
?]
to
[
6
?????]
come have
?
source [
1
we] [
6
have come] [
5
to] [
2
quite] [
4
like] [
1
xi?an] .
rule-based [
1
we] [
2
quite] [
4
like] [
3
xi?an] [
5
to] [
6
come have] .
df-bnb [
1
we] have [
2
quite] [
3
xi?an] [
4
like] [
5
to] [
6
come] .
baseline ????????????????
rule-based ??????????????????
df-bnb ????????????????
Table 4: Examples from our test data illustrating the differences between the preordering approaches.
not depend heavily on getting the right decision
in a multi-class scenario, and which incorporates
regression to carry out a score-driven search.
4.5 Analysis
Table 4 gives three English-Japanese examples
to illustrate the different preordering approaches.
The first, very short, example is preordered cor-
rectly by the rule-based and the df-bnb approach,
as the order of the brackets matches the order of
the Japanese reference.
For longer sentences we see more differences
between approaches, as illustrated by Example 2.
In this case, both approaches succeed at moving
prepositions to the back of the phrase (?my expe-
rience in?, ?the bus of?). However, while the df-
bnb approach correctly moves the predicate of the
second clause (?was just tired?) to the back, the
rule-based approach incorrectly moves the subject
(?a black woman named Rosa Parks?) to this posi-
tion - possibly because of the verb ?named? which
occurs in the phrase. This could be an indication
that the df-bnb is better suited for more compli-
cated constructions. With the exception of phrases
4 and 8, all other phrases are in the correct order
in the df-bnb reordering. None of the approaches
manage to reorder ?a black woman named Rosa
Parks? to the correct order.
Example 3 shows that the translations into
Japanese also reflect preordering quality. The
original source results in ?like? being translated
as the main verb (which is incorrectly interpreted
as ?to be like, to be equal to?). The rule-based
version correctly moves ?have come? to the end,
but fails to swap ?xi?an? and ?like?, resulting in
?come? being interpreted as a full verb, rather than
an auxiliary. Only the df-bnb version achieves al-
most perfect reordering, resulting in the correct
word choice of ?? (to get to, to become) for
?have come to?.
5
5 Conclusion
We have presented a novel preordering approach
that estimates a preference for swapping or not
swapping pairs of children nodes in the source-
side dependency tree by training a feature-rich
logistic regression model. Given the pair-wise
scores, we efficiently search through the space
of possible children permutations using depth-first
branch-and-bound search. The approach is able
to incorporate large numbers of features includ-
ing lexical cues, is efficient at runtime even with
a large number of children, and proves superior to
other state-of-the-art preordering approaches both
in terms of crossing score and translation perfor-
mance.
5
This translation is still not perfect, since it uses the wrong
level of politeness, an important distinction in Japanese.
246
References
Ibrahim Badr, Rabih Zbib, and James Glass. 2009.
Syntactic Phrase Reordering for English-to-Arabic
Statistical Machine Translation. In Proceedings of
EACL, pages 86?93, Athens, Greece.
Egon Balas and Paolo Toth. 1983. Branch and
Bound Methods for the Traveling Salesman Prob-
lem. Carnegie-Mellon Univ. Pittsburgh PA Manage-
ment Sciences Research Group.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan.
Marta R. Costa-juss`a and Jos?e A. R. Fonollosa. 2006.
Statistical Machine Reordering. In Proceedings of
EMNLP, pages 70?76, Sydney, Australia.
Josep M. Crego and Jos?e B. Mari?no. 2006. Integra-
tion of POStag-based Source Reordering into SMT
Decoding by an Extended Search Graph. In Pro-
ceedings of AMTA, pages 29?36, Cambridge, Mas-
sachusetts.
John DeNero and Jakob Uszkoreit. 2011. Inducing
Sentence Structure from Parallel Corpora for Re-
ordering. In Proceedings of EMNLP, pages 193?
203, Edinburgh, Scotland, UK.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of EMNLP, pages 847?
855, Honolulu, Hawaii.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of COLING, pages 376?384,
Beijing, China.
Jes?us Gim?enez and Llu??s M`arquez. 2004. SVMTool:
A general POS tagger generator based on Support
Vector Machines. In Proceedings of LREC, Lisbon,
Portugal.
Nizar Habash. 2007. Syntactic Preprocessing for Sta-
tistical Machine Translation. In Proceedings of MT-
Summit, pages 215?222, Copenhagen, Denmark.
Gumwon Hong, Seung-Wook Lee, and Hae-Chang
Rim. 2009. Bridging Morpho-Syntactic Gap be-
tween Source and Target Sentences for English-
Korean Statistical Machine Translation. In Proceed-
ings of ACL-IJCNLP, pages 233?236, Suntec, Sin-
gapore.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A Simple Re-
ordering Rule for SOV Languages. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 244?251, Up-
psala, Sweden.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.
2010. Constituent Reordering and Syntax Models
for English-to-Japanese Statistical Machine Trans-
lation. In Proceedings of COLING, pages 626?634,
Beijing, China.
Uri Lerner and Slav Petrov. 2013. Source-Side Clas-
sifier Preordering for Machine Translation. In Pro-
ceedings of EMNLP, Seattle, USA.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A Probabilistic
Approach to Syntax-based Reordering for Statistical
Machine Translation. In Proceedings of ACL, pages
720?727, Prague, Czech Republic.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a Discriminative Parser to Optimize
Machine Translation Reordering. In Proceedings of
EMNLP-CoNLL, pages 843?853, Jeju Island, Korea.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
EACL, pages 71?76, Bergen, Norway.
David L. Poole and Alan K. Mackworth. 2010. Ar-
tificial Intelligence: Foundations of Computational
Agents. Cambridge University Press. Full text on-
line at http://artint.info.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and Morphology: Addressing the crux
of the fluency problem in English-Hindi SMT. In
Proceedings of ACL-IJCNLP, pages 800?808, Sun-
tec, Singapore.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
TMI, pages 171?180, Sk?ovde, Sweden.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of EMNLP, pages 1007?1016, Singapore.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with auto-
matically derived rules for improved statistical ma-
chine translation. In Proceedings of COLING, pages
1119?1127, Beijing, China.
247
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for
improved machine translation. In Proceedings of
EMNLP, pages 486?496, Edinburgh, United King-
dom.
Karthik Visweswariah, Mitesh M. Khapra, and Anan-
thakrishnan Ramanathan. 2013. Cut the noise: Mu-
tually reinforcing reordering and alignments for im-
proved machine translation. In Proceedings of ACL,
pages 1275?1284, Sofia, Bulgaria.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese Syntactic Reordering for Statistical
Machine Translation. In Proceedings of EMNLP-
CoNLL, pages 737?745, Prague, Czech Republic.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
Pre-ordering Rules from Predicate-Argument Struc-
tures. In Proceedings of IJCNLP, pages 29?37, Chi-
ang Mai, Thailand.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of COLING,
Geneva, Switzerland.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a Dependency Parser to Improve
SMT for Subject-Object-Verb Languages. In Pro-
ceedings of HTL-NAACL, pages 245?253, Boulder,
Colorado.
Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.
2012. A ranking-based approach to word reordering
for statistical machine translation. In Proceedings of
ACL, pages 912?920, Jeju Island, Korea.
248
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 259?268,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Word Ordering with Phrase-Based Grammars
Adri
`
a de Gispert, Marcus Tomalin, William Byrne
Department of Engineering, University of Cambridge, UK
ad465@cam.ac.uk, mt126@cam.ac.uk, wjb31@cam.ac.uk
Abstract
We describe an approach to word ordering
using modelling techniques from statisti-
cal machine translation. The system in-
corporates a phrase-based model of string
generation that aims to take unordered
bags of words and produce fluent, gram-
matical sentences. We describe the gen-
eration grammars and introduce parsing
procedures that address the computational
complexity of generation under permuta-
tion of phrases. Against the best previous
results reported on this task, obtained us-
ing syntax driven models, we report huge
quality improvements, with BLEU score
gains of 20+ which we confirm with hu-
man fluency judgements. Our system in-
corporates dependency language models,
large n-gram language models, and mini-
mum Bayes risk decoding.
1 Introduction
Word ordering is a fundamental problem in NLP
and has been shown to be NP-complete in dis-
course ordering (Althaus et al., 2004) and in SMT
with arbitrary word reordering (Knight, 1999).
Typical solutions involve constraints on the space
of permutations, as in multi-document summari-
sation (Barzilay and Elhadad, 2011) and preorder-
ing in SMT (Tromble and Eisner, 2009; Genzel,
2010).
Some recent work attempts to address the fun-
damental word ordering task directly, using syn-
tactic models and heuristic search. Wan et al.
(2009) use a dependency grammar to address word
ordering, while Zhang and Clark (2011; 2012)
use CCG and large-scale n-gram language models.
These techniques are applied to the unconstrained
problem of generating a sentence from a multi-set
of input words.
We describe GYRO (Get Your Order Right), a
phrase-based approach to word ordering. Given a
bag of words, the system first scans a large, trusted
text collection and extracts phrases consisting of
words from the bag. Strings are then generated
by concatenating these phrases in any order, sub-
ject to the constraint that every string is a valid
reordering of the words in the bag, and the re-
sults are scored under an n-gram language model
(LM). The motivation is that it is easier to make
fluent sentences from phrases (snippets of fluent
text) than from words in isolation.
GYRO builds on approaches developed for syn-
tactic SMT (Chiang, 2007; de Gispert et al., 2010;
Iglesias et al., 2011). The system generates strings
in the form of weighted automata which can be
rescored using higher-order n-gram LMs, depen-
dency LMs (Shen et al., 2010), and Minimum
Bayes Risk decoding, either using posterior prob-
abilities obtained from GYRO or SMT systems.
We report extensive experiments using BLEU
and conclude with human assessments. We
show that despite its relatively simple formulation,
GYRO gives BLEU scores over 20 points higher
than the best previously reported results, gener-
ated by a syntax-based ordering system. Human
fluency assessments confirm these substantial im-
provements.
2 Phrase-based Word Ordering
We take as input a bag of N words ? =
{w
1
, . . . , w
N
}. The words are sorted, e.g. alpha-
betically, so that it is possible to refer to the i
th
word in the bag, and repeated words are distinct
tokens. We also take a set of phrases, L(?) that
259
are extracted from large text collections, and con-
tain only words from ?. We refer to phrases as u,
i.e. u ? L(?). The goal is to generate all permu-
tations of ? that can be formed by concatenation
of phrases from L(?).
2.1 Word Order Generation Grammar
Consider a subset A ? ?. We can represent A by
an N-bit binary string I(A) = I
1
(A) . . . I
N
(A),
where I
i
(A) = 1 if w
i
? A, and I
i
(A) = 0 other-
wise. A Context-Free Grammar (CFG) for gener-
ation can then be defined by the following rules:
Phrase-based Rules: ?A ? ? and ?u ? L(A)
I(A)? u
Concatenation Rules: ?A ? ?, B ? A,C ? A
such that I(A) = I(B)+I(C) and I(B)?I(C) =
0
I(A)? I(B) I(C)
where ? is the bit-wise logical AND
Root: S ? I(?)
We use this grammar to ?parse? the list of the
words in the bag ?. The grammar has one non-
terminal per possible binary string, so potentially
2
N
distinct nonterminals might be needed to gen-
erate the language. Each nonterminal can produce
either a phrase u ? L(A), or the concatenation of
two binary strings that share no bits in common. A
derivation is sequence of rules that starts from the
bit string I(?). Rules are unweighted in this basic
formulation.
For example, assume the following bag
? = {a, b, c, d, e}, which we sort alphabet-
ically. Assume the phrases are L(?) =
{?a b?, ?b a?, ?d e c?}. The generation grammar
contains the following 6 rules:
R
1
: 11000? ab
R
2
: 11000? ba
R
3
: 00111? dec
R
4
: 11111? 11000 00111
R
5
: 11111? 00111 11000
R
6
: S? 11111
Figure 1 represents all the possible derivations
in a hypergraph, which generate four alternative
strings. For example, string ?d e c b a? is ob-
tained with derivation R
6
R
5
R
3
R
2
, whereas string
?a b d e c? is obtained via R
6
R
4
R
1
R
3
.
2.2 Parsing a Bag of Words
We now describe a general algorithm for parsing a
bag of words with phrase constraints. The search
a b c d e
11000 00111
3
1
2
1
2
2
1
11111
1
2
2
1
{"a b d e c", 
"b a d e c", 
"d e c a b", 
"d e c b a"}
{"d e c"}
{"a b", "b a"}
Figure 1: Hypergraph representing gen-
eration from {a, b, c, d, e} with phrases
{?a b?, ?b a?, ?d e c?}.
is organized along a two-dimensional gridM [x, y]
of 2
N
?1 cells, where each cell is associated with
a unique nonterminal in the grammar (a bit string
I with at least one bit set to 1). Each row x in
the grid has
(
N
x
)
cells, representing all the possible
ways of covering exactly x words from the bag.
There are N rows in total.
For a bit string I , X(I) is the length of I , i.e.
the number of 1?s in I . In this way X(I(A))
points to the row associated with set A. There
is no natural ordering of cells within a row, so
we introduce a second function Y (I) which indi-
cates which cell in row X(I) is associated with I .
Hence M [X(I), Y (I)] is the cell associated with
bit string I . In the inverse direction, we using the
notation I
x,y
to indicate a bit string associated with
the cell M [x, y].
The basic parsing algorithm is given in Figure 2.
We first initialize the grid by filling the cells linked
to phrase-based rules (lines 1-4 of Figure 2). Then
parsing proceeds as follows. For each row in in-
creasing order (line 5), and for each of the non-
empty cells in the row (line 6), try to combine its
bit string with any other bit strings (lines 7-8). If
combination is admitted, then form the resultant
bit string and add the concatenation rule to the as-
sociated cell in the grid (lines 9-10). The combi-
nation will always yield a bit string that resides in
a higher row of the grid, so search is exhaustive.
If a rule is found in cell M [N, 1], there is a parse
(line 11); otherwise none exists. The complexity
of the algorithm isO(2
N
?K). If back-pointers are
kept, traversing these from cell M [N, 1] yields all
the generated word sequences.
The number of cells will grow exponentially as
the bag grows in size. In practice, the number of
260
PARSE-BAG-OF-WORDS
Input: bag of words ? of size N
Input: list of phrases L(?)
Initialize - Add phrase-based rules:
1 M [x, y]? ?
2 for each subset A ? ?
3 for each phrase u ? L(A)
4 add rule I(A)? u to cell M [X(I(A)), Y (I(A))]
Parse:
5 for each row x = 1, . . . , N
6 for each y = 1, . . . ,
(
N
x
)
7 for each valid A ? ?
8 if I
x,y
? I(A) = 0, then
9 I
?
? I
x,y
+ I(A)
10 add rule I
?
? I
x,y
I(A) to cell M [X(I
?
), Y (I
?
)]
11 if |M [N, 1]| > 0, success.
Figure 2: Parsing algorithm for a bag of words.
cells actually used in parsing can be smaller than
2
N
? 1. This depends strongly on the number of
distinct phrase-based rules and the distinct subsets
of ? they cover. For example, if we consider 1-
word subsets of ?, then all cells are needed and
GYRO attempts all word permutation. However,
if only 10 distinct 5-word phrases and 20 distinct
4-word phrases are considered for a bag of N=9
words, then fewer than 431 cells will be used (20
+ 10 for the initial cells at rows 4 and 5; plus all
combinations of 4-word subsets into row 8, which
is less than 400; plus 1 for the last cell at row 9).
2.3 Generation from Exact Parsing
We are interested in producing the space of word
sequences generated by the grammar, and in scor-
ing each of the sequences according to a word-
based n-gram LM. Assuming that parsing the bag
of words suceeded, this is a very similar scenario
to that of syntax-based approaches to SMT: the
output is a large collection of word sequences,
which are built by putting together smaller units
and which can be found by a process of expansion,
i.e. by traversing the back-pointers from an initial
cell in a grid structure. A significant difference is
that in syntax-based approaches the parsing stage
tends to be computationally easier than the pars-
ing stage has only a quadratic dependency on the
length of the input sentence.
We borrow techniques from SMT to represent
and manipulate the space of generation hypothe-
ses. Here we follow the approach of expand-
ing this space onto a Finite-State Automata (FSA)
described in (de Gispert et al., 2010; Iglesias et
al., 2011). This means that in parsing, each cell
M [x, y] is associated with an FSA F
x,y
, which en-
codes all the sequences generated by the grammar
0
111000
2
00111 3
00111
11000
0
1a
2
b 3
b
a
0 1d 2e 3c
0
1a
2b
7
d
3
b
a
8e
4d 5e
6
c
9c
10a
11
b
b
a
11000
00111
11111
Expansion of RTN 11111
Figure 3: RTN representing generation from
{a, b, c, d, e} with phrases {?a b?, ?b a?, ?d e c?}
(top) and its expansion as an FSA (bottom).
when covering the words marked by the bit string
of that cell. When a rule is added to a cell, a new
path from the initial to the final state of F
x,y
is
created so that each FSA is the union of all paths
arising from the rules added to the cell. Impor-
tantly, when an instance of the concatenation rule
is added to a cell, the new path is built with only
two arcs. These point to other FSAs at lower rows
in the grid so that the result has the form of a
Recursive Transition Network with a finite depth
of recursion. Following the example from Sec-
tion 2.1, the top three FSAs in Figure 3 represent
the RTN for example from Figure 1.
The parsing algorithm is modified as follows:
4 add rule I(A)? u
as path to FSA F
X(I(A)),Y (I(A))
...
10 add rule I
?
? I
x,y
I(A)
as path to FSA F
X(I
?
),Y (I
?
)
11 if NumStates(F
N,1
) > 1, success.
At this point we specify two strategies:
Algorithm 1: Full expansion is described by the
pseudocode in Figure 4, excluding lines 2-3. A
recursive FSA replacement operation (Allauzen et
al., 2007) can be used to expand the FSA in the
top-most cell. In our running example, the result
261
is the FSA at the bottom of Figure 3. We then
apply a word-based LM to the resulting FSA via
standard FSA composition. This outputs the com-
plete (unpruned) language of interest, where each
word sequence generated from the bag according
to the phrasal constraints is scored by the LM.
Algorithm 2: Pruned expansion is described by
the pseudocode in Figure 4, now including lines
2-3. We introduce pruning because full, unpruned
expansion may not be feasible for large bags with
many phrasal rules. Once parsing is done, we in-
troduce the following bottom-up pruning strategy.
For each row starting at row r, we union all FSAs
of the row and expand the unioned FSA through
the recursive replacement operation. This yields
the space of all generation hypotheses of length
r. We then apply the language model to this lat-
tice and reduce it under likelihood-based pruning
at weight ?. We then update each cell in the row
with a new FSA obtained as the intersection of its
original FSA and the pruned FSA.
1
This intersec-
tion may yield an empty FSA for a particular cell
(meaning that all its hypotheses were pruned out
of the row), but it will always leave at least one
surviving FSA per row, guaranteeing that if pars-
ing succeeds, the top-most cell will expand into
a non-empty FSA. As we process higher rows,
the replacement operation will yield smaller FSAs
because some back-pointers will point to empty
FSAs. In this way memory usage can be con-
trolled through parameters r and ?. Of course,
when pruning in this way, the final output lattice
L will not contain the complete space of hypothe-
ses that could be generated by the grammar.
2.4 Algorithm 3: Pruned Parsing and
Generation
The two generation algorithms presented above
rely on a completed initial parsing step. However,
given that the complexity of the parsing stage is
O(2
N
? K), this may not be achievable in prac-
tice. Leaving aside time considerations, the mem-
ory required to store 2
N
FSAs will grow exponen-
tially in N , even if the FSAs contain only pointers
to other FSAs. Therefore we also describe an al-
gorithm to perform bottom-up pruning guided by
1
This step can be performed much more efficiently with
a single forward pass of the resultant lattice. This is possible
because the replace operation can yield a transducer where
the input symbols encode a pointer to the original FSA, so
in traversing the arcs of the pruned lattice, we know which
arcs will belong to which cell FSAs. However, for ease of
explanation we avoid this detail.
FULL-PARSE-EXPANSION
Input: bag of words ? of size N
Input: list phrases L(?)
Input: word-based LM G
Output: word lattice L of generated sequences
Generate:
1 PARSE-BAG-OF-WORDS(?)
2 for each row x = r, . . . , N ? 1
3 PRUNE-ROW(x)
4 F ? FSA-REPLACE(F
N,1
)
5 return L? F ?G
6 function PRUNE-ROW(x) :
7 F ?
?
y
F
x,y
8 F ? FSA-REPLACE(F )
9 F ? F ?G
10 F ? FSA-PRUNE(F, ?)
11 for each cell y = 1, . . . ,
(
N
x
)
12 F
x,y
? F
x,y
? F
13 return
Figure 4: Pseudocode for Algorithm 1 (excluding
lines 2-3) and Algorithm 2 (including all lines).
the LM during parsing. The pseudocode is identi-
cal to that of Algorithm 1 except for the following
changes: in parsing (Figure 2) we pass G as input
and we call the row pruning function of Figure 4
after line 5 if x ? r.
We note that there is a strong connection be-
tween GYRO and the IDL approach of Soricut
and Marcu (2005; 2006). Our bag of words parser
could be cast in the IDL-formalism, and the FSA
?Replace? operation would be expressed by an
IDL ?Unfold? operation. However, whereas their
work applies pruning in the creation of the IDL-
expression prior to LM application, GYRO uses
unweighted phrase constraints so the LM must be
considered for pruning while parsing.
3 Experimental Results
We now report various experiments evaluating the
performance of the generation approach described
above. The system is evaluated using the MT08-
nw, and MT09-nw testsets. These correspond to
the first English reference of the newswire por-
tion of the Arabic-to-English NIST MT evalua-
tion sets
2
. They contain 813 and 586 sentences
respectively (53,325 tokens in total; average sen-
tence length = 38.1 tokens after tokenization). In
order to reduce the computational complexity, all
sentences with more than 20 tokens were divided
into sub-sentences, with 20 tokens being the up-
per limit. Between 70-80% of the sentences in the
2
http://www.itl.nist.gov/iad/mig/tests/mt
262
6 8 10 12 14 16 18 201
10
100
1000
10000 2grams3grams4grams5grams
Size of the bag of words
Num
ber o
f n-g
rams
Figure 5: Average number of extracted phrases as
a function of the bag of word size.
testsets were divided in this way. For each of these
sentences we create a bag.
The GYRO system uses a n-gram LM estimated
over 1.3 billion words of English text, including
the AFP and Xinhua portions of the GigaWord
corpus version 4 (1.1 billion words) and the En-
glish side of various Arabic-English parallel cor-
pora typically used in MT evaluations (0.2 billion
words).
Phrases of up to length 5 are extracted for each
bag from a text collection containing 10.6 bil-
lion words of English news text. We use efficient
Hadoop-based look-up techniques to carry out this
extraction step and to retrieve rules for genera-
tion (Pino et al., 2012). The average number of
phrases extracted as a function of the size of the
bag is shown in Figure 5. These are the phrase-
based rules of our generation grammar.
3.1 Computational Analysis
We analyze here the computational requirements
of the three alternative GYRO algorithms pre-
sented in Sections 2.3 and 2.4. We carry out this
analysis on a subset of 200 random subsentences
from MT08-nw and MT09-nw chosen to have the
same sentence length distribution as the whole
data set. For a fixed generation grammar com-
prised of 3-gram, 4-gram and 5-gram rules only,
we run each algorithm with a memory limitation
of 20GB. If the process reaches this limit, then it
is killed. Figure 6 reports the worst-case memory
memory required by each algorithm as a function
of the size of the bag.
As shown, Full Expansion (Algorithm 1) is only
feasible for bags that contain at most 12 words.
By contrast, Pruned Expansion (Algorithm 2) with
? = 10 is feasible for bags of up to 18 words. For
4 6 8 10 12 14 16 18 20
0
2
4
6
8
10
12
14
16
18
20
bag of words size
me
mo
ry c
ons
ump
tion
 (in G
B)
 
 
Algorithm 1
Algorithm 2 ?=10
Algorithm 3 ?=10
Algorithm 3 ?=5
Figure 6: Worst-case memory required (GB) by
each GYRO algorithm relative to the size of the
bags.
bigger bags, the requirements of unpruned pars-
ing make generation intractable under the mem-
ory limit. Finally, Pruned Parsing and Generation
(Algorithm 3) is feasible at all bag sizes (up to 20
words), and its memory requirements can be con-
trolled via the beam-width pruning parameter ?.
Harsher pruning (i.e. lower ?) will incur more
coverage problems, so it is desirable to use the
highest feasible value of ?.
We emphasise that Algorithm 3, with suitable
pruning strategies, can scale up to larger problems
quite readily and generate output from much larger
input sets than reported here. We focus here on
generation quality for moderate sized problems.
3.2 Generation Performance
We now compare the GYRO system with the
Combinatory Categorial Grammar (CCG)-based
system described in (Zhang et al., 2012). By
means of extracted CCG rules, the CCG sys-
tem searches for an optimal parse guided by
large-margin training. Each partial hypothesis (or
?edge?) is scored using the syntax model and a 4-
gram LM trained similarly on one billion words of
English Gigaword data. Both systems are evalu-
ated using BLEU (Papineni et al., 2002; Espinosa
et al., 2010).
For GYRO, we use the pruned parsing algo-
rithm of Section 2.4 with r = 6 and ? = 10
and a memory usage limit of 20G. The phrase-
based rules of the grammar contain only 3-grams,
263
LM System MT08-nw MT09-nw
4g CCG 48.0 48.8
3g GYRO 59.0 58.4
GYRO +3g 63.0 64.1
4g GYRO +4g 65.5 65.9
100-best oracle 76.1 76.1
lattice oracle 80.4 80.2
Table 1: CCG and GYRO BLEU scores.
4-grams and 5-grams.
3
Under these conditions,
GYRO finds an output for 91.4% of the bags. For
the remainder, we obtain an output either by prun-
ing less or by adding bigram rules (in 7.2% of the
bags), or simply by adding all words as unigram
rules (1.4% of the bags).
Table 1 gives the results obtained by CCG and
GYRO under a 3-gram or a 4-gram LM. Because
GYRO outputs word lattices as opposed to a 1-
best hypothesis, we can reapply the same LM to
the concatenated lattices of any sentences longer
than 20 to take into account context in subsentence
boundaries. This is the result in the third row in
the Table, labeled ?GYRO +3g?. We can see that
GYRO benefits significantly from this rescoring,
beating the CCG system across both sets. This is
possibly explained by the CCG system?s depen-
dence upon in-domain data that have been explic-
itly marked-up using the CCG formalism. The fi-
nal row reports the positive impact of increasing
the LM order to 4.
Impact of generation grammar. To measure
the benefits of using high-order n-grams as con-
straints for generation, we also ran GYRO with
unigram rules only. This effectively does permu-
tation under the LM with the pruning mechanisms
described. The BLEU scores are 54.0 and 54.5 for
MT08-nw and MT09 respectively. This indicates
that a strong GYRO grammar is very much needed
for this type of parsing and generation.
Quality of generated lattices. We assess the
quality of the lattices output by GYRO under the
4-gram LM by computing the oracle BLEU score
of either the 100-best lists or the whole lattices
4
in the last two rows of Table 1. In order to com-
pute the latter, we use the linear approximation
to BLEU that allows an efficient FST-based im-
plementation of an Oracle search (Sokolov et al.,
2012). We draw two conclusions from these re-
sults: (a) that there is a significant potential for im-
3
Any word in the bag that does not occur in the large col-
lection of English material is added as a 1-gram rule.
4
Obtained by pruning at ? = 10 in generation.
provement from rescoring, in that even for small
100-best lists the improvement found by the Ora-
cle can exceed 10 BLEU points; and (b) that the
output lattices are not perfect in that the Oracle
score is not 100.
3.2.1 Rescoring GYRO output
We now report on rescoring procedures intended
to improve the first-pass lattices generated by
GYRO.
Higher-order language models. The first row
in Table 2 reports the result obtained when apply-
ing a 5-gram LM to the GYRO lattices generated
under a 4-gram. The 5-gram is estimated over the
complete 10.6 billion word collection using the
uniform backoff strategy of (Brants et al., 2007).
We find improvements of 3.0 and 1.9 BLEU with
respect to the 4-gram baseline.
Dependency language models. We now in-
vestigate the benefits of applying a dependency
LM (Shen et al., 2010) in a rescoring mode. We
run the MALT dependency parser
5
on the gener-
ation hypotheses and rescore them according to
log(p
LM
) + ?
d
log(p
depLM
), i.e. a weighted com-
bination of the word-based LM and the depen-
dency LM scores. Since it is not possible to run the
parser on the entire lattice, we carry out this exper-
iment using the 100-best lists generated from the
previous experiment (?+5g?). The dependency LM
is a 3-gram estimated on the entire GigaWord ver-
sion 5 collection (?5 billion words). Results are
shown in rows 2 and 3 in Table 2, where in each
row the performance over the set used to tune the
parameter ?
d
is marked with ?. In either case, we
observe modest but consistent gains across both
sets. We find this very promising considering that
the parser has been applied to noisy input sen-
tences.
Minimum Bayes Risk Decoding. We also use
Lattice-based Minimum Bayes Risk (LMBR) de-
coding (Tromble et al., 2008; Blackwood et al.,
2010a). Here, the posteriors over n-grams are
computed over the output lattices generated by the
GYRO system. The result is shown in row labeled
?+5g +LMBR?, where again we find modest but
consistent gains across the two sets with respect to
the 5-gram rescored lattices.
LMBR with MT posteriors. We investigate
LMBR decoding when applying to the generation
lattice a linear combination of the n-gram pos-
5
Available at www.maltparser.org
264
4g GYRO rescoring: MT08-nw MT09-nw
+5g 68.5 67.8
+5g +depLM ?
d
= 0.4 68.7
?
68.1
+5g +depLM ?
d
= 0.33 68.7 68.2
?
+5g +LMBR 68.6 68.3
+5g +LMBR-mt ? = 0.25 70.8
?
72.2
+5g +LMBR-mt ? = 0.25 70.8 72.2
?
Table 2: Results in BLEU when rescoring the lat-
tices generated by GYRO using various strategies.
Tuning conditions are marked by
?
.
terior probabilities extracted from (a) the same
generation lattice, and (b) from lattices produced
by an Arabic-to-English hierarchical-phrase based
MT system developed for the NIST 2012 OpenMT
Evaluation. As noted, LMBR relies on a posterior
distribution over n-grams as part of its computa-
tion or risk. Here, we use LMBR with a posterior
of the form ?p
GYRO
+ (1??) p
MT
. This is effec-
tively performing a system combination between
the GYRO generation system and the MT system
(de Gispert et al., 2009; DeNero et al., 2010) but
restricting the hypothesis space to be that of the
GYRO lattice (Blackwood et al., 2010b). Results
are reported in the last two rows of Table 2. Rel-
ative to 5-gram LM rescoring alone, we see gains
in BLEU of 2.3 and 4.4 in MT08-nw and MT09-
nw, suggesting that posterior distributions over n-
grams provided by SMT systems can give good
guidance in generation. These results also suggest
that if we knew what words to use, we could gen-
erate very good quality translation output.
3.3 Analysis and examples
Figure 7 gives GYRO generation examples. These
are often fairly fluent, and it is striking how the
output can be improved with guidance from the
SMT system. The examples also show the harsh-
ness of BLEU, e.g. ?german and turkish officials?
is penalised with respect to ? turkish and german
officials.? Metrics based on richer meaning rep-
resentations, such as HyTER, could be valuable
here (Dreyer and Marcu, 2012).
Figure 8 shows BLEU and Sentence Preci-
sion Rate (SPR), the percentage of exactly recon-
structed sentences. As expected, performance is
sensitive to length. For bags of up to 10, GYRO
reconstructs the reference perfectly in over 65%
of the cases. This is a harsh performance metric,
and performance falls to less than 10% for bags
of size 16-20. For bags of 6-10 words, we find
BLEU scores of greater than 85. Performance is
681 0862 66861 60842 46841 40g raams35?s12
11
02
01
?2
?1
?2
?1
?2
262
42?2
?212
02?2
?2?2Size of
 thbagwardbasNuagwamgn-?
Size
a??gn
b
 b?r
b??b
aonb?
t?tg?
afNrb
a? of
?
Figure 8: GYRO BLEU score and Sentence Pre-
cision Rate as a function of the bag of words size.
Computed on the concatenation of MT08-nw and
MT09-nw.
not as good for shorter segments, since these are
often headlines and bylines that can be ambiguous
in their ordering. The BLEU scores for bags of
size 21 and higher are an artefact of our sentence
splitting procedure. However, even for bag sizes
of 16-to-20 GYRO has BLEU scores above 55.
3.4 Human Assessments
Finally, the CCG and 4g-GYRO+5g systems were
compared using crowd-sourced fluency judge-
ments gathered on CrowdFlower. Judges were
asked ?Please read the reference sentence and
compare the fluency of items 1 & 2.? The test was
a selection of 75 fluent sentences of 20 words or
less taken from the MT dev sets. Each comparison
was made by at least 3 judges. With an average se-
lection confidence of 0.754, GYRO was preferred
in 45 cases, CCG was preferred in 14 cases, and
systems were tied 16 times. This is consistent with
the significant difference in BLEU between these
systems.
4 Related Work and Conclusion
Our work is related to surface realisation within
natural language generation (NLG). NLG typi-
cally assumes a relatively rich input representation
intended to provide syntactic, semantic, and other
relationships to guide generation. Example input
representations are Abstract Meaning Represen-
tations (Langkilde and Knight, 1998), attribute-
value pairs (Ratnaparkhi, 2000), lexical predicate-
argument structures (Bangalore and Rambow,
2000), Interleave-Disjunction-Lock (IDL) expres-
sions (Nederhof and Satta, 2004; Soricut and
Marcu, 2005; Soricut and Marcu, 2006), CCG-
bank derived grammars (White et al., 2007),
265
Hypothesis SBLEU
REF a third republican senator joins the list of critics of bush ?s policy in iraq .
(a) critics of bush ?s iraq policy in a third of republican senator joins the list . 47.2
(b) critics of bush ?s policy in iraq joins the list of a third republican senator . 69.8
(c) critics of bush ?s iraq policy in a list of republican senator joins the third . 39.1
(d) the list of critics of bush ?s policy in iraq a third republican senator joins . 82.9
REF it added that these messages were sent to president bashar al-asad through turkish and german officials .
(a-c) it added that president bashar al-asad through these messages were sent to german and turkish officials . 61.5
(d) it added that these messages were sent to president bashar al-asad through german and turkish officials . 80.8
REF a prominent republican senator has joined the ranks of critics of george bush ?s policy in iraq , calling
for a new strategy just days before a new confrontation in congress
(a) a prominent republican senator george has joined the ranks of critics of bush ?s policy in iraq , just days
before a new strategy in congress calling for a new confrontation
66.7
(b) a prominent republican senator has joined the ranks of critics of george bush ?s policy in iraq , just days
before congress calling for a new strategy in a new confrontation
77.8
(c) a prominent republican senator has joined the ranks of critics of george bush ?s policy in iraq , just days
before a new strategy in congress calling for a new confrontation
82.3
(d) a prominent republican senator has joined the ranks of critics of george bush ?s policy in iraq , calling
for a new strategy just days before a new confrontation in congress
100
Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b)
GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical
hypotheses.
meaning representation languages (Wong and
Mooney, 2007) and unordered syntactic depen-
dency trees (Guo et al., 2011; Bohnet et al., 2011;
Belz et al., 2011; Belz et al., 2012)
6
.
These input representations are suitable for ap-
plications such as dialog systems, where the sys-
tem maintains the information needed to gener-
ate the input representation for NLG (Lemon,
2011), or summarisation, where representations
can be automatically extracted from coherent,
well-formed text (Barzilay and Elhadad, 2011; Al-
thaus et al., 2004). However, there are other appli-
cations, such as automatic speech recognition and
SMT that could possibly benefit from NLG, but
which do not generate reliable linguistic annota-
tion in their output. For these problems it would
be useful to have systems, as described in this pa-
per, which do not require rich input representa-
tions. We plan to investigate these applications in
future work.
There is much opportunity for future develop-
ment. To improve coverage, the grammars of Sec-
tion 2.1 could perform generation with overlap-
ping, rather than concatenated, n-grams; and fea-
tures could be included to define tuneable log-
linear rule probabilities (Och and Ney, 2002; Chi-
ang, 2007). The GYRO grammar could be ex-
tended using techniques from string-to-tree SMT,
in particular by modifying the grammar so that
output derivations respect dependencies (Shen et
6
Surface Realisation Task, Generation Challenges 2011,
www.nltg.brighton.ac.uk/research/
genchal11
al., 2010); this will make it easier to integrate de-
pendency LMs into GYRO. Finally, it would be
interesting to couple the GYRO architecture with
automata-based models of poetry and rhythmic
text (Greene et al., 2010).
Acknowledgement
The research leading to these results has received
funding from the European Union Seventh
Framework Programme (FP7-ICT-2009-4)
under grant agreement number 247762, the
FAUST project faust-fp7.eu/faust/,
and the EPSRC (UK) Programme Grant
EP/I031022/1 (Natural Speech Technology)
natural-speech-technology.org .
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of CIAA, pages 11?23,
Prague, Czech Republic.
Ernst Althaus, Nikiforos Karamanis, and Alexander
Koller. 2004. Computing locally coherent dis-
courses. In Proceedings of the 42nd Annual Meeting
on Association for Computational Linguistics, page
399. Association for Computational Linguistics.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gen-
eration. In Proceedings of the 18th conference on
Computational linguistics - Volume 1, COLING ?00,
pages 42?48, Stroudsburg, PA, USA. Association
for Computational Linguistics.
266
Regina Barzilay and Noemie Elhadad. 2011. In-
ferring strategies for sentence ordering in multi-
document news summarization. arXiv preprint
arXiv:1106.1820.
Anja Belz, Mike White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and eval-
uation results. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 217?226,
Nancy, France.
Anja Belz, Bernd Bohnet, Simon Mille, Leo Wanner,
and Michael White. 2012. The surface realisation
task: Recent developments and future plans. In Pro-
ceedings of the 7th International Natural Language
Generation Conference, pages 136?140, Utica, IL,
USA.
Graeme Blackwood, Adri`a de Gispert, and William
Byrne. 2010a. Efficient path counting transducers
for minimum Bayes-risk decoding of statistical ma-
chine translation lattices. In Proceedings of ACL:
Short Papers, pages 27?32, Uppsala, Sweden.
Graeme Blackwood, Adri`a de Gispert, and William
Byrne. 2010b. Fluency constraints for minimum
Bayes-risk decoding of statistical machine transla-
tion lattices. In Proceedings of COLING, pages 71?
79, Beijing, China.
Bernd Bohnet, Simon Mille, Beno??t Favre, and Leo
Wanner. 2011. <StuMaBa>: From deep represen-
tation to surface. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 232?235,
Nancy, France.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-CoNLL, pages 858?867, Prague, Czech Re-
public.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Adri`a de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes risk com-
bination of translation hypotheses from alternative
morphological decompositions. In Proceedings of
HLT-NAACL: Short Papers, pages 73?76, Boulder,
CO, USA.
Adri`a de Gispert, Gonzalo Iglesias, Graeme Black-
wood, Eduardo R. Banga, and William Byrne. 2010.
Hierarchical phrase-based translation with weighted
finite-state transducers and shallow-n grammars.
Computational Linguistics, 36(3):505?533.
John DeNero, Shankar Kumar, Ciprian Chelba, and
Franz Och. 2010. Model combination for machine
translation. In Proceedings of HTL-NAACL, pages
975?983, Los Angeles, CA, USA.
Markus Dreyer and Daniel Marcu. 2012. Hyter:
Meaning-equivalent semantics for translation eval-
uation. In Proceedings of NAACL-HLT, pages 162?
171, Montr?eal, Canada.
Dominic Espinosa, Rajakrishnan Rajkumar, Michael
White, and Shoshana Berleant. 2010. Further
meta-evaluation of broad-coverage surface realiza-
tion. In Proceedings of EMNLP, pages 564?574,
Cambridge, MA, USA.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of COLING, pages 376?384,
Beijing, China.
Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010. Automatic analysis of rhythmic poetry with
applications to generation and translation. In Pro-
ceedings of EMNLP, pages 524?533, Cambridge,
MA, USA.
Yuqing Guo, Josef Van Genabith, and Haifeng Wang.
2011. Dependency-based n-gram models for gen-
eral purpose sentence realisation. Natural Language
Engineering, 17(04):455?483.
Gonzalo Iglesias, Cyril Allauzen, William Byrne,
Adri`a de Gispert, and Michael Riley. 2011. Hi-
erarchical phrase-based translation representations.
In Proceedings of EMNLP, pages 1373?1383, Edin-
burgh, Scotland, UK.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25(4):607?615.
Irene Langkilde and Kevin Knight. 1998. Gener-
ation that exploits corpus-based statistical knowl-
edge. In Proceedings of ACL/COLING, pages 704?
710, Montreal, Quebec, Canada.
Oliver Lemon. 2011. Learning what to say and how to
say it: Joint optimisation of spoken dialogue man-
agement and natural language generation. Com-
puter Speech & Language, 25(2):210?221.
Mark-Jan Nederhof and Giorgio Satta. 2004. IDL-
expressions: A formalism for representing and pars-
ing finite languages in natural language processing.
Journal of Artificial Intelligence Research, 21:287?
317.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL,
pages 295?302, Philadelphia, PA, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318, Philadelphia, PA, USA.
Juan Pino, Aurelien Waite, and William Byrne. 2012.
Simple and efficient model filtering in statistical ma-
chine translation. The Prague Bulletin of Mathemat-
ical Linguistics, 98:5?24.
267
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
NAACL, pages 194?201, Seattle, WA, USA.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4):649?671.
Artem Sokolov, Guillaume Wisniewski, and Francois
Yvon. 2012. Computing lattice bleu oracle scores
for machine translation. In Proceedings of EACL,
pages 120?129, Avignon, France.
Radu Soricut and Daniel Marcu. 2005. Towards devel-
oping generation algorithms for text-to-text applica-
tions. In Proceedings of ACL, pages 66?74, Ann
Arbor, MI, USA.
Radu Soricut and Daniel Marcu. 2006. Stochastic
Language Generation Using WIDL-Expressions and
its Application in Machine Translation and Summa-
rization. In Proceedings of ACL, pages 1105?1112,
Sydney, Australia.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of EMNLP, pages 1007?1016, Singapore.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
Proceedings of EMNLP, pages 620?629, Honolulu,
Hawaii, USA.
Stephen Wan, Mark Dras, Robert Dale, and C?ecile
Paris. 2009. Improving grammaticality in statisti-
cal sentence generation: Introducing a dependency
spanning tree algorithm with an argument satisfac-
tion model. In Proceedings of EACL, pages 852?
860, Athens, Greece.
Michael White, Rajakrishnan Rajkumar, and Scott
Martin. 2007. Towards broad coverage surface real-
ization with ccg. In Proc. of the Workshop on Using
Corpora for NLG: Language Generation and Ma-
chine Translation (UCNLG+ MT).
Yuk Wah Wong and Raymond J Mooney. 2007. Gen-
eration by inverting a semantic parser that uses sta-
tistical machine translation. Proceedings of Hu-
man Language Technologies: The Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT-07), pages
172?179.
Yue Zhang and Stephen Clark. 2011. Syntax-
based Grammaticality Improvement using CCG and
Guided Search. In Proceedings of EMNLP, pages
1147?1157, Edinburgh, Scotland, U.K.
Yue Zhang, Graeme Blackwood, and Stephen Clark.
2012. Syntax-based word ordering incorporating
a large-scale language model. In Proceedings of
EACL, pages 736?746, Avignon, France.
268
