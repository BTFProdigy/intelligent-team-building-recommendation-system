Proceedings of the Third Workshop on Statistical Machine Translation, pages 107?110,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
LIMSI?s statistical translation systems for WMT?08
Daniel D?chelotte, Gilles Adda, Alexandre Allauzen, H?l?ne Bonneau-Maynard,
Olivier Galibert, Jean-Luc Gauvain, Philippe Langlais? and Fran?ois Yvon
LIMSI/CNRS
firstname.lastname@limsi.fr
Abstract
This paper describes our statistical machine
translation systems based on the Moses toolkit
for the WMT08 shared task. We address the
Europarl and News conditions for the follow-
ing language pairs: English with French, Ger-
man and Spanish. For Europarl, n-best rescor-
ing is performed using an enhanced n-gram
or a neuronal language model; for the News
condition, language models incorporate extra
training data. We also report unconvincing re-
sults of experiments with factored models.
1 Introduction
This paper describes our statistical machine trans-
lation systems based on the Moses toolkit for the
WMT 08 shared task. We address the Europarl and
News conditions for the following language pairs:
English with French, German and Spanish. For Eu-
roparl, n-best rescoring is performed using an en-
hanced n-gram or a neuronal language model, and
for the News condition, language models are trained
with extra training data. We also report unconvinc-
ing results of experiments with factored models.
2 Base System architecture
LIMSI took part in the evaluations on Europarl data
and on News data, translating French, German and
Spanish from and to English, amounting a total
of twelve evaluation conditions. Figure 1 presents
the generic overall architecture of LIMSI?s transla-
tion systems. They are fairly standard phrase-based
?Univ. Montr?al, felipe@iro.umontreal.ca
Other
Targettext
Targettext
MosestextSource or
Translation model 4g language model 4g language model
and extractionRescoring
$n$?besttranslations
LM InterpolationPhrase pairextraction Neural network
or
or
+ News Co.EuroparlEuroparl EuroparlNews Co.sources
Figure 1: Generic architecture of LIMSI?s SMT systems.
Depending on the condition, the decoder generates ei-
ther the final output or n-best lists. In the latter case,
the rescoring incorporates the same translation features,
except for a better target language model (see text).
translation systems (Och and Ney, 2004; Koehn et
al., 2003) and use Moses (Koehn et al, 2007) to
search for the best target sentence. The search uses
the following models: a phrase table, providing 4
scores and a phrase penalty, a lexicalized reordering
model (7 scores), a language model score and a word
penalty. These fourteen scores are weighted and lin-
early combined (Och and Ney, 2002; Och, 2003);
their respective weights are learned on development
data so as to maximize the BLEU score. In the fol-
lowing, we detail several aspects of our systems.
2.1 Translation models
The translation models deployed in our systems for
the europarl condition were trained on the provided
Europarl parallel data only. For the news condition,
they were trained on the Europarl data merged with
107
the news-commentary parallel data, as depicted on
Figure 1. This setup was found to be more favor-
able than training on Europarl data only (for obvious
mismatching domain reasons) and than training on
news-commentary data only, most probably because
of a lack of coverage. Another, alternative way of
benefitting from the coverage of the Europarl corpus
and the relevance of the news-commentary corpus
is to use two phrase-tables in parallel, an interest-
ing feature of Moses. (Koehn and Schroeder, 2007)
found that this was the best way to ?adapt? a transla-
tion system to the news-commentary task. These re-
sults are corroborated in (D?chelotte, 2007)1 , which
adapts a ?European Parliament? system using a ?Eu-
ropean and Spanish Parliaments? development set.
However, we were not able to reproduce those find-
ings for this evaluation. This might be caused by the
increase of the number of feature functions, from 14
to 26, due to the duplication of the phrase table and
the lexicalized reordering model.
2.2 Language Models
2.2.1 Europarl language models
The training of Europarl language models (LMs)
was rather conventional: for all languages used in
our systems, we used a 4-gram LM based on the
entire Europarl vocabulary and trained only on the
available Europarl training data. For French, for
instance, this yielded a model with a 0.2 out-of-
vocabulary (OOV) rate on our LM development set,
and a perplexity of 44.9 on the development data.
For French also, a more accurate n-gram LM was
used to rescore the first pass translation; this larger
model includes both Europarl and giga word corpus
of newswire text, lowering the perplexity to 41.9 on
the development data.
2.2.2 News language models
For this condition, we took advantage of the a
priori information that the test text would be of
newspaper/newswire genre and from the November-
december 2007 period. We consequently built much
larger LMs for translating both to French and to En-
glish, and optimized their combination on appropri-
1(D?chelotte, 2007) further found that giving an increased
weight to the small in-domain data could out-perform the setup
with two phrase-tables in parallel. We haven?t evaluated this
idea for this evaluation.
ate source of data. For French, we interpolated five
different LMs trained on corpus containing respec-
tively newspapers, newswire, news commentary and
Europarl data, and tuned their combination with text
downloaded from the Internet. Our best LM had an
OOV rate of about 2.1% and a perplexity of 111.26
on the testset. English LMs were built in a similar
manner, our largest model combining 4 LMs from
various sources, which, altogether, represent about
850M words. Its perplexity on the 2008 test set was
approximately 160, with an OOV rate of 2.7%.
2.2.3 Neural network language models
Neural-Network (NN) based continuous space
LMs similar to the ones in (Schwenk, 2007) were
also trained on Europarl data. These networks com-
pute the probabilities of all the words in a 8192 word
output vocabulary given a context in a larger, 65000-
word vocabulary. Each word in the context is first
associated with a numerical vector of dimension 500
by the input layer. The activity of the 500 neurons in
the hidden layer is computed as the hyperbolic tan-
gent of the weighted sum of these vectors, projecting
the context into a [?1, 1] hypercube of dimension
500. Final projection on a set of 8192 output neurons
yields the final probabilities through a softmax-ed,
weighted sum of the coordinates in the hypercube.
The final NN-based model is interpolated with the
main LM model in a 0.4-0.6 ratio, and yields a per-
plexity reduction of 9% relative with respect to the
n-gram LM on development data.
2.3 Tuning procedure
We use MERT, distributed with the Moses decoder,
to tune the first pass of the system. The weights
were adjusted to maximize BLEU on the develop-
ment data. For the baseline system, a dozen Moses
runs are necessary for each MERT optimization, and
several optimization runs were started and compared
during the system?s development. Tuning was per-
formed using dev2006 for the Europarl task and on
News commentary dev2007 for the news task.
2.4 Rescoring and post processing
For the Europarl condition, distinct 100 best trans-
lations from Moses were rescored with improved
LMs: when translating to French, we used the
French model described in section 2.2.1; when
108
Es-En En-Es Fr-En En-Fr
baseline 32.21 31.62 32.41 29.31
Limsi 32.49 31.23 32.62 30.27
Table 1: Comparison of two tokenization policies
All results on Europarl test2007
CI system CS system
En?Fr 27.23 27.55
Fr?En 30.96 30.98
Table 2: Effect of training on true case texts, for English
to French (case INsensitive BLEU scores, untuned sys-
tems, results on test2006 dataset)
translating to English, we used the neuronal LM de-
scribed in section 2.2.3.
For all the ?lowcase? systems (see below), recase-
ing was finally performed using our own recaseing
tool. Case is restored by creating a word graph al-
lowing all possible forms of caseing for each word
and each component of a compound word. This
word graph is then decoded using a cased 4-gram
LM to obtain the most likely form. In a final step,
OOV words (with respect to the source language
word list) are recased to match their original form.
3 Experiments with the base system
3.1 Word tokenization and case
We developed our own tokenizer for English, French
and Spanish, and used the baseline tokenizer for
German. Experiments on the 2007 test dataset for
Europarl task show the impact of the tokenization
on the BLEU scores, with 3-gram LMs. Results are
always improved with our own tokenizer, except for
English to Spanish (Table 1).
Our systems were initially trained on lowercase
texts, similarly to the proposed baseline system.
However, training on true case texts proved bene-
ficial when translating from English to French, even
when scoring in a case insensitive manner. Table 2
shows an approximate gain of 0.3 BLEU for that di-
rection, and no impact on French to English perfor-
mance. Our English-French systems are therefore
case sensitive.
3.2 Language Models
For Europarl, we experimented with LMs of increas-
ing orders: we found that using a 5-gram LM only
yields an insignificant improvement over a 4-gram
LM. As a result, we used 4-gram LMs for all our
first pass decodings. For the second pass, the use
of the Neural Network LMs, if used with an appro-
priate (tuned) weight, yields a small, yet consistent
improvement of BLEU for all pairs.
Performance on the news task are harder to ana-
lyze, due to the lack of development data. Throwing
in large set of in-domain data was obviously helpful,
even though we are currently unable to adequately
measure this effect.
4 Experiments with factored models
Even though these models were not used in our sub-
missions, we feel it useful to comment here our (neg-
ative) experiments with factored models.
4.1 Overview
In this work, factored models (Koehn and Hoang,
2007) are experimented with three factors : the sur-
face form, the lemma and the part of speech (POS).
The translation process is composed of different
mapping steps, which either translate input factors
into output factors, or generate additional output fac-
tors from existing output factors. In this work, four
mapping steps are used with two decoding paths.
The first path corresponds to the standard and di-
rect mapping of surface forms. The second decod-
ing path consists in two translation steps for respec-
tively POS tag and the lemmas, followed by a gener-
ation step which produces the surface form given the
POS-lemma couple. The system also includes three
reordering models.
4.2 Training
Factored models have been built to translate from
English to French for the news task. To estimate the
phrase and generation tables, the training texts are
first processed in order to compute the lemmas and
POS information. The English texts are tagged and
lemmatized using the English version of the Tree-
tagger2. For French, POS-tagging is carried out
with a French version of the Brill?s tagger trained
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
109
on the MULTITAG corpus (Allauzen and Bonneau-
Maynard, 2008). Lemmatization is performed with
a French version of the Treetagger.
Three phrase tables are estimated with the Moses
utilities, one per factor. For the surface forms, the
parallel corpus is the concatenation of the official
training data for the tasks Europarl and News com-
mentary, whereas only the parallel data of news
commentary are used for lemmas and POS. For the
generation step, the table built on the parallel texts of
news commentary is augmented with a French dic-
tionary of 280 000 forms. The LM is the largest LM
available for French (see section 2.2.2).
4.3 Results and lessons learned
On the news test set of 2008, this system obtains a
BLEU score of 20.2, which is worse than our ?stan-
dard? system (20.9). A similar experiment on the
Europarl task proved equally unsuccessful.
Using only models which ignore the surface form
of input words yields a poor system. Therefore, in-
cluding a model based on surface forms, as sug-
gested (Koehn and Hoang, 2007), is also neces-
sary. This indeed improved (+1.6 BLEU for Eu-
roparl) over using one single decoding path, but not
enough to match our baseline system performance.
These results may be explained by the use of auto-
matic tools (POS tagger and lemmatizer) that are not
entirely error free, and also, to a lesser extend, by the
noise in the test data. We also think that more effort
has to be put into the generation step.
Tuning is also a major issue for factored trans-
lation models. Dealing with 38 weights is an op-
timization challenge, which took MERT 129 itera-
tions to converge. The necessary tradeoff between
the huge memory requirements of these techniques
and computation time is also detrimental to their use.
Although quantitative results were unsatisfactory,
it is finally worth mentioning that a manual exami-
nation of the output revealed that the explicit usage
of gender and number in our models (via POS tags)
may actually be helpful when translating to French.
5 Conclusion
In this paper, we presented our statistical MT sys-
tems developed for the WMT 08 shared task. As ex-
pected, regarding the Europarl condition, our BLEU
improvements over the best 2007 results are limited:
paying attention to tokenization and caseing issues
brought us a small pay-off; rescoring with better
language models gave also some reward. The news
condition was new, and more challenging: our satis-
factory results can be attributed to the use of large,
well tuned, language models. In comparison, our ex-
periments with factored models proved disappoint-
ing, for reasons that remain to be clarified. On a
more general note, we feel that the performance of
MT systems for these tasks are somewhat shadowed
by normalization issues (tokenization errors, incon-
sistent use of caseing, typos, etc), making it difficult
to clearly analyze our systems? performance.
References
A. Allauzen and H. Bonneau-Maynard. 2008. Training
and evaluation of POS taggers on the French multitag
corpus. In Proc. LREC?08, To appear.
D. D?chelotte. 2007. Traduction automatique de la pa-
role par m?thodes statistiques. Ph.D. thesis, Univ.
Paris XI, December.
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In Proc. EMNLP-CoNLL, pages 868?876.
P. Koehn and J. Schroeder. 2007. Experiments in domain
adaptation for statistical machine translation. In Proc.
of the Workshop on Statistical Machine Translation,
pages 224?227, Prague, Czech Republic.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. HLT-NAACL, pages
127?133, Edmonton, Canada, May.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL, demonstration
session, Prague, Czech Republic.
F.J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. ACL, pages 295?302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. ACL, Sapporo, Japan.
H. Schwenk. 2007. Continuous space language models.
Computer Speech and Language, 21:492?518.
110
Proceedings of the Fifth Law Workshop (LAW V), pages 92?100,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Proposal for an Extension of Traditional Named Entities:
From Guidelines to Evaluation, an Overview
Cyril Grouin?, Sophie Rosset?, Pierre Zweigenbaum?
Kar?n Fort?,? , Olivier Galibert?, Ludovic Quintard?
?LIMSI?CNRS, France ?INIST?CNRS, France ?LIPN, France ?LNE, France
{cyril.grouin,sophie.rosset,pierre.zweigenbaum}@limsi.fr
karen.fort@inist.fr, {olivier.galibert,ludovic.quintard}@lne.fr
Abstract
Within the framework of the construction of a
fact database, we defined guidelines to extract
named entities, using a taxonomy based on an
extension of the usual named entities defini-
tion. We thus defined new types of entities
with broader coverage including substantive-
based expressions. These extended named en-
tities are hierarchical (with types and compo-
nents) and compositional (with recursive type
inclusion and metonymy annotation). Human
annotators used these guidelines to annotate a
1.3M word broadcast news corpus in French.
This article presents the definition and novelty
of extended named entity annotation guide-
lines, the human annotation of a global corpus
and of a mini reference corpus, and the evalu-
ation of annotations through the computation
of inter-annotator agreements. Finally, we dis-
cuss our approach and the computed results,
and outline further work.
1 Introduction
Within the framework of the Quaero project?a mul-
timedia indexing project?we organized an evalu-
ation campaign on named entity extraction aiming
at building a fact database in the news domain, the
first step being to define what kind of entities are
needed. This campaign focused on broadcast news
corpora in French. While traditional named enti-
ties include three major classes (persons, locations
and organizations), we decided to extend the cov-
erage of our campaign to new types of entities and
to broaden their main parts-of-speech from proper
names to substantives, this extension being neces-
sary for ever-increasing knowledge extraction from
documents. We thus produced guidelines to specify
the way corpora had to be annotated, and launched
the annotation process.
In this paper, after covering related work (Sec-
tion 2), we describe the taxonomy we created (Sec-
tion 3) and the annotation process and results (Sec-
tion 4), including the corpora we gathered and the
tools we developed to facilitate annotation. We then
present inter-annotator agreement measures (Sec-
tion 5), outline limitations (Section 6) and conclude
on perspectives for further work (Section 7).
2 Related work
2.1 Named entity definitions
Named Entity recognition was first defined as recog-
nizing proper names (Coates-Stephens, 1992). Since
MUC-6 (Grishman and Sundheim, 1996; SAIC,
1998), named entities have been proper names
falling into three major classes: persons, locations
and organizations.
Proposals were made to sub-divide these entities
into finer-grained classes. The ?politicians? sub-
class was proposed for the ?person? class by (Fleis-
chman and Hovy, 2002) while the ?cities? subclass
was added to the ?location? class by (Fleischman,
2001; Lee and Lee, 2005).
The CONLL conference added a miscellaneous
type that includes proper names falling outside the
previous classes. Some classes have thus sometimes
been added, e.g. the ?product? class by (Bick, 2004;
Galliano et al, 2009).
92
Specific entities are proposed and handled in
some tasks: ?language? or ?shape? for question-
answering systems in specific domains (Rosset et
al., 2007), ?email address? or ?phone number? to
process electronic messages (Maynard et al, 2001).
Numeric types are also often described and used.
They include ?date?, ?time?, and ?amount? types
(?amount? generally covers money and percentage).
In specific domains, entities such as gene, protein,
are also handled (Ohta, 2002), and campaigns are or-
ganized for gene detection (Yeh et al, 2005). At the
same time, extensions of named entities have been
proposed: (Sekine, 2004) defined a complete hierar-
chy of named entities containing about 200 types.
2.2 Named Entities and Annotation
As for any other kind of annotation, some aspects are
known to lead to difficulties in obtaining coherence
in the manual annotation process (Ehrmann, 2008;
Fort et al, 2009). Three different classes of prob-
lems are distinguished: (1) selecting the correct cat-
egory in cases of ambiguity, where one entity can
fall into several classes, depending on the context
(?Paris? can be a town or a person name); (2) detect-
ing the boundaries (in a person designation, is only
the proper name to be annotated or the trigger ?Mr?
too?) and (3) annotating metonymies (?France? can
be a sports team, a country, etc.).
In the ACE Named Entity task (Doddington et al,
2004), a complex task, the obtained inter-annotator
agreement was 0.86 in 2002 and 0.88 in 2003. Some
tasks obtain better agreement. Desmet and Hoste
(2010) described the Named Entity annotation real-
ized within the Sonar project, where Named Entity
are clearly simpler. They follow the MUC Named
Entity definition with the subtypes as proposed
by ACE. The agreement computed over the Sonar
Dutch corpus ranges from 0.91 to 0.97 (kappa val-
ues) depending of the emphasized elements (span,
main type, subtype, etc.).
3 Taxonomy
3.1 Guidelines production
Having in mind the objective of building a fact
database through the extraction of named entities
from texts, we defined a richer taxonomy than those
used in other information extraction works.
Following (Bonneau-Maynard et al, 2005; Alex
et al, 2010), the annotation guidelines were first
written from December 2009 to May 2010 by three
researchers managing the manual annotation cam-
paign. During guidelines production, we evaluated
the feasibility of this specific annotation task and the
usefulness of the guidelines by annotating a small
part of the target corpus. Then, these guidelines
were delivered to the annotators. They consist of a
description of the objects to annotate, general anno-
tation rules and principles, and more than 250 pro-
totypical and real examples extracted from the cor-
pus (Rosset et al, 2010). Rules are important to set
the general way annotations must be produced. Ad-
ditionally, examples are essential for human annota-
tors to grasp the annotation rationale more easily.
Indeed, while producing the guidelines, we knew
that the given examples would never cover all possi-
ble cases because of the specificity of language and
of the ambiguity of formulations and situations de-
scribed in corpora, as shown in (Fort et al, 2009).
Nevertheless, guidelines examples must be consid-
ered as a way to understand the final objective of
the annotation work. Thanks to numerous meetings
from May to November 2010, we gathered feedback
from the annotators (four annotators plus one anno-
tation manager). This feedback allowed us to clarify
and extend the guidelines in several directions. The
guidelines are 72 pages long and consist of 3 major
parts: general description of the task and the prin-
ciples (25% of the overall document), presentation
of each type of named entity (57%), and a simpler
?cheat sheet? (18%).
3.2 Definition
We decided to use the three general types of
named entities: name (person, location, organi-
zation) as described in (Grishman and Sundheim,
1996; SAIC, 1998), time (date and duration), and
quantity (amount). We then included named entities
extensions proposed by (Sekine, 2004; Galliano et
al., 2009) (respectively products and functions) and
we extended the definition of named entities to ex-
pressions which are not composed of proper names
(e.g., phrases built around substantives). The ex-
tended named entities we defined are both hierar-
chical and compositional. For example, type pers
(person) is split into two subtypes, pers.ind (indi-
93
Person Function
pers.ind (individual
person)
pers.coll (group of
persons)
func.ind (individual
function)
func.coll (collectivity
of functions)
Location Product
administrative
(loc.adm.town,
loc.adm.reg,
loc.adm.nat,
loc.adm.sup)
physical
(loc.phys.geo,
loc.phys.hydro,
loc.phys.astro)
facilities
(loc.fac),
oronyms
(loc.oro),
address
(loc.add.phys,
loc.add.elec)
prod.object
(manufac-
tured object)
prod.serv
(transporta-
tion route)
prod.fin
(financial
products)
prod.doctr
(doctrine)
prod.rule
(law)
prod.soft
(software)
prod.art prod.media prod.award
Organization Time
org.adm (administra-
tion)
org.ent (services)
Amount
amount (with unit or general object), includ-
ing duration
time.date.abs
(absolute date),
time.date.rel (relative
date)
time.hour.abs
(absolute hour),
time.hour.rel (relative
hour)
Table 1: Types (in bold) and subtypes (in italic)
vidual person) and pers.coll (collective person), and
pers entities are composed of several components,
among which are name.first and name.last.
3.3 Hierarchy
We used two kinds of elements: types and compo-
nents. The types with their subtypes categorize a
named entity. While types and subtypes were used
before (ACE, 2000; Sekine, 2004; ACE, 2005; Gal-
liano et al, 2009), we consider that structuring the
contents of an entity (its components) is important
too. Components categorize the elements inside a
named entity.
Our taxonomy is composed of 7 main types
(person, function, location, product, organization,
amount and time) and 32 subtypes (Table 1). Types
and subtypes refer to the general category of a
named entity. They give general information about
the annotated expression. Almost each type is then
specified using subtypes that either mark an opposi-
tion between two major subtypes (individual person
vs. collective person), or add precisions (for exam-
ple for locations: administrative location, physical
location, etc.).
This two-level representation of named entities,
with types and components, constitutes a novel ap-
proach.
Types and subtypes To deal with the intrinsic am-
biguity of named entities, we defined two specific
transverse subtypes: 1. other for entities with a dif-
ferent subtype than those proposed in the taxon-
omy (for example, prod.other for games), and 2. un-
known when the annotator does not know which sub-
type to use.
Types and subtypes constitute the first level of an-
notation. They refer to a general segmentation of
the world into major categories. Within these cate-
gories, we defined a second level of annotation we
call components.
Components Components can be considered as
clues that help the annotator to produce an anno-
tation: either to determine the named entity type
(e.g. a first name is a clue for the pers.ind named
entity subtype), or to set the named entity bound-
aries (e.g. a given token is a clue for the named en-
tity, and is within its scope, while the next token is
not a clue and is outside its scope). Components are
second-level elements, and can never be used out-
side the scope of a type or subtype element. An en-
tity is thus composed of components that are of two
kinds: transverse components and specific compo-
nents (Table 2). Transverse components can be used
in several types of entities, whereas specific compo-
nents can only be used in one type of entity.
94
Transverse components
name (name of the entity), kind (hyperonym of the entity), qualifier (qualifying adjective), demonym
(inhabitant or ethnic group name), demonym.nickname (inhabitant or ethnic group nickname), val
(a number), unit (a unit), extractor (an element in a series), range-mark (range between two values),
time-modifier (a time modifier).
pers.ind loc.add.phys time.date.abs/rel amount
name.last, name.first,
name.middle, pseudonym,
name.nickname, title
address-number, po-box,
zip-code,
other-address-component
week, day, month, year,
century, millennium,
reference-era
object
prod.award
award-cat
Table 2: Transverse and specific components
3.4 Composition
Another original point in this work is the compo-
sitional nature of the annotations. Entities can be
compositional for three reasons: (i) a type contains a
component; (ii) a type includes another type, used as
a component; and (iii) in cases of metonymy. Dur-
ing the Ester II evaluation campaign, there was an
attempt to use compositionality in named entities for
two categories: persons and functions, where a per-
son entity could contain a function entity.
<pers.hum> <func.pol> pr?sident </func.pol>
<pers.hum> Chirac </pers.hum> </pers.hum>
Nevertheless, the Ester II evaluation did not take
this inclusion into account and only focused on
the encompassing annotation (<pers.hum> pr?sident
Chirac </pers.hum>). We drew our inspiration from
this experience, and allowed the annotators and the
systems to use compositionality in the annotations.
Cases of inclusion can be found in the function
type (Figure 1), where type func.ind, which spans
the whole expression, includes type org.adm, which
spans the single word ?budget?. In this case, we con-
sider that the designation of this function (?ministre
du budget?) includes both the kind (?ministre?) and
nouveau
qualifier
ministre
kind
du Budget
name
org.adm
func.ind
, Fran?ois
name.first
Baroin
name.last
pers.ind
Figure 1: Multi-level annotation of entity types (red tags)
and components (blue tags): new minister of budget ,
Fran?ois Baroin.
the name (?budget?) of the ministry, which itself is
typed as is relevant (org.adm). Recursive cases of
embedding can be found when a subtype includes
another named entity annotated with the same sub-
type (org.ent in Figure 2).
le collectif
kind
des associations
kind
des droits de l' Homme
name
prod.rule
au Sahara
name
loc.phys.geo
loc.adm.sup
org.ent
org.ent
Figure 2: Recursive embedding of the same subtype:
Collective of the Human Rights Organizations in Sahara.
Cases of metonymy include strict metonymy (a
term is substituted with another one in a relation
of contiguity) and antonomasia (a proper name is
used as a substantive or vice versa). In such cases,
the entity must be annotated with both types, first
(inside) with the intrinsic type of the entity, then
(outside) with the type that corresponds to the re-
sult of the metonymy. Basically, country names
correspond to ?national administrative? locations
(loc.adm.nat) but they can also designate the admin-
istration (org.adm) of the country (Figure 3).
depuis
time-modifier
plusieurs
val
mois
unit
amount
time.date.rel
, la Russie
name
loc.adm.nat
org.adm
Figure 3: Annotation with a metonymic use of country
?Russia? as its government: for several months , Russia...
95
3.5 Boundaries
Our definition of the scope of entities excludes rel-
ative clauses, subordinate clauses, and interpolated
clauses: the annotation of an entity must end before
these clauses. If an interpolated clause occurs inside
an entity, its annotation must be split. Moreover, two
distinct persons sharing the same last name must be
annotated as two separate entities (Figure 4); we in-
tend to use relations between entities to gather these
segments in the next step of the project.
depuis
utmi-oefrl
vifr-eua
il n.s,etui
utmi-oefrl
Rprveu
utmi-strl
vifr-eua
Figure 4: Separate (coordinated) named entities.
4 Annotation process
4.1 Corpus
We managed the annotation of a corpus of about one
hundred hours of transcribed speech from several
French-speaking radio stations in France and Mo-
rocco. Both news and entertainment shows were
transcribed, including dialogs, with speaker turns.1
Once annotated, the corpus was split into a de-
velopment corpus: one file from a French radio sta-
tion;2 a training corpus: 188 files from five French
stations3 and one Moroccan station;4 and a test cor-
pus: 18 files from two French stations already stud-
ied in the training corpus5 and from unseen sources,
both radio6 and television,7 in order to evaluate the
robustness of systems. These data have been used in
the 2011 Quaero named entity evaluation campaign.
1Potential named entities may be split across several seg-
ments or turns.
2News from France Culture.
3News from France Culture (refined language), France Info
(news with short news headlines), France Inter (generalist radio
station), Radio Classique (classical music and economic news),
RFI (international radio broadcast out of France).
4News from RTM (generalist French speaking radio).
5News from France Culture, news and entertainment from
France Inter.
6A popular entertainment show from Europe 1.
7News from Arte (public channel with art and culture),
France 2 (public generalist channel), and TF1 (private gener-
alist popular channel).
This corpus allows us to perform different evalua-
tions, depending of the knowledge the systems have
of the source (source seen in the training corpus vs.
unseen source), the kind of show (news vs. enter-
tainment), the language style (popular vs. refined),
and the type of media (radio vs. television).
4.2 Tools for annotators
To perform our test annotations (see Section 2.2),
we developed a very simple annotation tool as an in-
terface based on XEmacs. We provided the human
annotators with this tool and they decided to use it
for the campaign, despite the fact that it is very sim-
ple and that we told them about other, more generic,
annotation tools such as GATE8 or Glozz.9 This is
probably due to the fact that apart from being very
simple to install and use, it has interesting features.
The first feature is the insertion of annotations
using combinations of keyboard shortcuts based on
the initial of each type, subtype and component
name. For example, combination F2 key + initial
keys is used to annotate a subtype (pers.ind, etc.),
F3 + keys for a transverse component (name, kind,
etc.), F4 + keys for a specific component (name.first,
etc.), and F5 to delete the annotation selected with
the cursor (both opening and closing tags).
The second feature is boundary management: if
the annotator puts the cursor over the token to anno-
tate, the annotation tool will handle the boundaries
of this token; opening and closing tags will be in-
serted around the token.
However, it presents some limitations: tags are
inserted in the text (which makes visualization more
complex, especially for long sentences or in cases
of multiple annotations on the same entity), no per-
sonalization is offered (tags are of only one color),
and there is no function to express annotator uncer-
tainty (the user must choose among several possible
tags the one that fits the best;10 while producing the
guidelines, we did not consider it could be of inter-
est: as a consequence, no uncertainty management
was implemented). Therefore, this tool allows users
to insert tags rapidly into a text, but it offers no exter-
nal resources, as real annotation tools (e.g. GATE)
often do.
8http://gate.ac.uk/
9http://www.glozz.org/
10Uncertainty can be found in cases of lack of context.
96
These simplistic characteristics combined with a
fast learning curve allow the annotators to rapidly
annotate the corpora. Annotators were allowed not
to annotate the transverse component name (only if
it was the only component in the annotated phrase,
e.g. ?Russia? in Figure 3, blue tag) and to annotate
events, even though we do not focus on this type
of entity as of yet. We therefore also provided a
normalization tool which adds the transverse com-
ponent name in these instances, and which removes
event annotations.
4.3 Corpus annotation
Global annotation It took four human annotators
two months and a half to annotate the entire corpus
(10 man-month). These annotators were hired grad-
uate students (MS in linguistics). The overall corpus
was annotated in duplicate. Regular comparisons of
annotations were performed and allowed the anno-
tators to develop a methodology, which was subse-
quently used to annotate the remaining documents.
Mini reference corpus To evaluate the global an-
notation, we built a mini reference corpus by ran-
domly selecting 400 sentences from the training cor-
pus and distributing them into four files. These files
were annotated by four graduate human annotators
from two research institutes (Figure 5) with two hu-
mans per institute, in about 10 hours per annotator.
	








Figure 5: Creation of mini reference corpus and compu-
tation of inter-annotator agreement. Institute 1 = LIMSI?
CNRS, Institute 2 = INIST?CNRS
First, we merged the annotations of each file
within a given institute (1.5h per pair of annotators),
then merged the results across the two institutes
(2h). Finally, we merged the results with the anno-
tations of the hired annotators (8h). We thus spent
about 90 hours to annotate and merge annotations in
this mini reference corpus (0.75 man-month).
4.4 Annotation results
Our broadcast news corpus includes 1,291,225
tokens, among which there are 954,049 non-
punctuation tokens. Its annotation contains 113,885
named entities and 146,405 components (Table 3),
i.e. one entity per 8.4 non-punctuation tokens, and
one component per 6.5 non-punctuation tokens.
There is an average of 6 annotations per line.
PPPPPPPPInf.
Data
Training Test
# shows 188 18
# lines 43,289 5,637
# words 1,291,225 108,010
# entity types 113,885 5,523
# distinct types 41 32
# components 146,405 8,902
# distinct comp. 29 22
Table 3: Statistics on annotated corpora.
5 Inter-Annotator Agreement
5.1 Procedure
During the annotation campaign, we measured sev-
eral criteria on a regular basis: inter-annotator agree-
ment and disagreement. We used them to correct er-
roneous annotations, and mapped these corrections
to the original annotations. We also used these mea-
sures to give the annotators feedback on the en-
countered problems, discrepancies, and residual er-
rors. Whereas we performed these measurements all
along the annotation campaign, this paper focuses
on the final evaluation on the mini reference corpus.
5.2 Metrics
Because human annotation is an interpretation pro-
cess (Leech, 1997), there is no ?truth? to rely on. It
is therefore impossible to really evaluate the validity
of an annotation. All we can and should do is to eval-
uate its reliability, i.e. the consistency of the anno-
tation across annotators, which is achieved through
computation of the inter-annotator agreement (IAA).
97
The best way to compute it is to use one of
the Kappa family coefficients, namely Cohen?s
Kappa (Cohen, 1960) or Scott?s Pi (Scott, 1955),
also known as Carletta?s Kappa (Carletta, 1996),11
as they take chance into account (Artstein and Poe-
sio, 2008). However, these coefficients imply a
comparison with a ?random baseline? to establish
whether the correlation between annotations is sta-
tistically significant. This baseline depends on the
number of ?markables?, i.e. all the units that could
be annotated.
In the case of named entities, as in many others,
this ?random baseline? is known to be difficult?if
not impossible?to identify (Alex et al, 2010). We
wish to analyze this in more detail, to see how we
could actually compute these coefficients and what
information it would give us about the annotation.
Markables Annotators Both institutes
F = 0.84522 F = 0.91123
U1: n-grams
? = 0.84522 ? = 0.91123
pi = 0.81687 pi = 0.90258
U2: n-grams ? 6
? = 0.84519 ? = 0.91121
pi = 0.81685 pi = 0.90257
U3: NPs
? = 0.84458 ? = 0.91084
pi = 0.81628 pi = 0.90219
U4: Ester entities
? = 0.71300 ? = 0.82607
pi = 0.71210 pi = 0.82598
U5: Pooling
? = 0.71300 ? = 0.82607
pi = 0.71210 pi = 0.82598
Table 4: Inter-Annotator Agreements (? stands for Co-
hen?s Kappa, pi for Scott?s Pi, and F for F-measure). IAA
values were computed by taking as the reference the hired
annotators? annotation or that obtained by merging from
both institutes (see Figure 5).
In the present case, we could consider that, poten-
tially, all the noun phrases can be annotated (row U3
in Table 4, based on the PASSAGE campaign (Vil-
nat et al, 2010)). Of course, this is a wrong approx-
imation as named entities are not necessarily noun
phrases (e.g., ?? partir de l?automne prochain?, from
next autumn).
We could also consider all n-grams of tokens in
the corpus (row U1). However, it would be more
11For more details on terminology issues, we refer to the in-
troduction of (Artstein and Poesio, 2008).
relevant to limit their size. For a maximum size of
six, we get the results shown in row U2. All this, of
course, is artificial, as the named entity annotation
process is not random.
To obtain results that are closer to reality, we
could use numbers of named entities from previous
named entity annotation campaigns (row U4 based
on the Ester II campaign (Galliano et al, 2009)), but
as we consider here a largely extended version of
those, the results would again be far from reality.
Another solution is to consider as ?markables? all
the units annotated by at least one of the annotators
(row U5). In this particular case, units not annotated
by any of the annotators (i.e. silence) are overlooked.
The lowest IAA will be the one computed with
this last solution, while the highest IAA will be
equal to the F-measure (i.e. the measure computed
with all the markables as shown in row U1 in Ta-
ble 4). We notice that the first two solutions (U1
and U2 with n-grams) are not acceptable because
they are far from reality; even extended named en-
tities are sparse annotations, and just considering
all tokens as ?markables? is not suitable. The last
three ones seem to be more relevant because they
are based on an observed segmentation on similar
data. Still, the U3 solution (NPs) overrates the num-
ber of markables because not all noun phrases are
extended named entities. Although the U4 solution
(Ester entities) is based on the same corpus used for
a related task, it underrates the number of markables
because that task produced 16.3 times less annota-
tions. Finally the U5 solution (pooling) gives the
lower bound for the ? estimation which is an in-
teresting information but may easily undervalue the
quality of the annotation.
As (Hripcsak and Rothschild, 2005) showed, in
our case ? tends towards the F-measure when the
number of negative cases tends towards infinity. Our
results show that it is hard to build a justifiable hy-
pothesis on the number of markables which is larger
than the number of actually annotated entities while
keeping ? significantly under the F-measure. But
building no hypothesis leads to underestimating the
? value.
This reinforces the idea of using the F-measure
as the main inter-annotator agreement measure for
named entity annotation tasks.
98
6 Limitations
We used syntax to define some components (e.g. a
qualifier is an adjective) and to set the scope of en-
tities (e.g. stop at relative clauses). Nevertheless,
this syntactic definition cannot fit all named enti-
ties, which are mainly defined according to seman-
tics: the phrase ?dans les mois qui viennent? (?in
the coming months?) expresses an entity of type
time.date.rel where the relative clause ?qui vien-
nent? is part of the entity and contributes the time-
modifier component.
The distinction between some types of entities
may be fuzzy, especially for the organizations (is
the Social Security an administrative organization or
a company?) and for context-dependent annotations
(is lemonde.fr a URL, a media, or a company?). As a
consequence, some entity types might be converted
into specific components in a future revision, e.g. the
func type could become a component of the pers
type, where it would become a description of the
function itself instead of the person who performs
this function (Figure 6).
depuisptm
-its
oftrlits
vaienr
tn.pl,num
Rpeulits
depuisptm
oftr
vaienr
tn.pl,num
Rpeulits
Figure 6: Possible revision: current annotation (left),
transformation of func from entity to component (right).
7 Conclusion and perspectives
In this paper, we presented an extension of the tra-
ditional named entity categories to new types (func-
tions, civilizations) and new coverage (expressions
built over a substantive). We created guidelines
that were used by graduate annotators to annotate
a broadcast news corpus.
The organizers also annotated a small part of the
corpus to build a mini reference corpus. We evalu-
ated the human annotations with our mini-reference
corpus: the actual computed ? is between 0.71 et
0.85 which, given the complexity of the task, seems
to indicate a good annotation quality. Our results are
consistent with other studies (Dandapat et al, 2009)
in demonstrating that human annotators? training is
a key asset to produce quality annotations.
We also saw that guidelines are never fixed, but
evolve all along the annotation process due to feed-
back between annotators and organizers; the rela-
tionship between guidelines producers and human
annotators evolved from ?parent? to ?peer? (Akrich
and Boullier, 1991). This evolution was observed
during the annotation development, beyond our ex-
pectations. These data have been used for the 2011
Quaero Named Entity evaluation campaign.
Extensions and revisions are planned. Our first
goal is to add a new type of named entity for all
kinds of events; guidelines are being written and hu-
man annotation tests are ongoing. We noticed that
some subtypes are more difficult to disambiguate
than others, especially org.adm and org.ent (defi-
nition and examples in the guidelines are not clear
enough). We shall make decisions about this kind
of ambiguity, either by merging these subtypes or by
reorganizing the distinctions within the organization
type. We also plan to link the annotated entities us-
ing relations; further work is needed to define more
precisely the way we will perform these annotations.
Moreover, the taxonomy we defined was applied to
a broadcast news corpus, but we intend to use it in
other corpora. The annotation of an old press corpus
was performed according to the same process. Its
evaluation will start in the coming months.
Acknowledgments
We thank all the annotators who did such a great
work on this project, as well as Sabine Barreaux
(INIST?CNRS) for her work on the reference cor-
pus.
This work was partly realized as part of the
Quaero Programme, funded by Oseo, French State
agency for innovation and by the French ANR Etape
project.
References
ACE. 2000. Entity detection and tracking,
phase 1, ACE pilot study. Task definition.
http://www.nist.gov/speech/tests/ace/phase1/doc/summary-
v01.htm.
ACE. 2005. ACE (Automatic Con-
tent Extraction) English annotation guide-
lines for entities version 5.6.1 2005.05.23.
http://www.ldc.upenn.edu/Projects/ACE/docs/English-
Entities-Guidelines_v5.6.1.pdf.
99
Madeleine Akrich and Dominique Boullier. 1991. Le
mode d?emploi, gen?se, forme et usage. In Denis
Chevallier, editor, Savoir faire et pouvoir transmettre,
pages 113?131. ?d. de la MSH (collection Ethnologie
de la France, Cahier 6).
Beatrice Alex, Claire Grover, Rongzhou Shen, and Mijail
Kabadjov. 2010. Agile Corpus Annotation in Prac-
tice: An Overview of Manual and Automatic Annota-
tion of CVs. In Proc. of the Fourth Linguistic Annota-
tion Workshop, pages 29?37, Uppsala, Sweden. ACL.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Computa-
tional Linguistics, 34(4):555?596.
Eckhard Bick. 2004. A named entity recognizer for dan-
ish. In LREC?04.
H?l?ne Bonneau-Maynard, Sophie Rosset, Christelle Ay-
ache, Anne Kuhn, and Djamel Mostefa. 2005. Seman-
tic Annotation of the French Media Dialog Corpus. In
InterSpeech, Lisbon.
Jean Carletta. 1996. Assessing Agreement on Classifi-
cation Tasks: the Kappa Statistic. Computational Lin-
guistics, 22:249?254.
Sam Coates-Stephens. 1992. The analysis and acquisi-
tion of proper names for the understanding of free text.
Computers and the Humanities, 26:441?456.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological Mea-
surement, 20(1):37?46.
Sandipan Dandapat, Priyanka Biswas, Monojit Choud-
hury, and Kalika Bali. 2009. Complex Linguistic An-
notation - No Easy Way Out! A Case from Bangla
and Hindi POS Labeling Tasks. In Proc. of the Third
Linguistic Annotation Workshop, Singapour. ACL.
Bart Desmet and V?ronique Hoste. 2010. Towards a
balanced named entity corpus for dutch. In LREC.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ACE) program tasks, data, and evaluation. In Proc. of
LREC.
Maud Ehrmann. 2008. Les entit?s nomm?es, de la lin-
guistique au TAL : statut th?orique et m?thodes de
d?sambigu?sation. Ph.D. thesis, Univ. Paris 7 Diderot.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proc. of
COLING, volume 1, pages 1?7. ACL.
Michael Fleischman. 2001. Automated subcategoriza-
tion of named entities. In Proc. of the ACL 2001 Stu-
dent Research Workshop, pages 25?30.
Kar?n Fort, Maud Ehrmann, and Adeline Nazarenko.
2009. Towards a Methodology for Named Entities An-
notation. In Proceeding of the 3rd ACL Linguistic An-
notation Workshop (LAW III), Singapore.
Sylvain Galliano, Guillaume Gravier, and Laura
Chaubard. 2009. The ESTER 2 evaluation campaign
for the rich transcription of French radio broadcasts.
In Proc of Interspeech 2009.
Ralph Grishman and Beth Sundheim. 1996. Message
Understanding Conference - 6: A brief history. In
Proc. of COLING, pages 466?471.
George Hripcsak and Adam S. Rothschild. 2005. Tech-
nical brief: Agreement, the f-measure, and reliability
in information retrieval. JAMIA, 12(3):296?298.
Seungwoo Lee and Gary Geunbae Lee. 2005. Heuris-
tic methods for reducing errors of geographic named
entities learned by bootstrapping. In IJCNLP, pages
658?669.
Geoffrey Leech. 1997. Introducing corpus annotation.
In Geoffrey Leech Roger Garside and Tony McEnery,
editors, Corpus annotation: Linguistic information
from computer text corpora, pages 1?18. Longman,
London.
Diana Maynard, Valentin Tablan, Cristian Ursu, Hamish
Cunningham, and Yorick Wilks. 2001. Named en-
tity recognition from diverse text types. In Recent Ad-
vances in NLP 2001 Conference, Tzigov Chark.
Tomoko Ohta. 2002. The genia corpus: An annotated
research abstract corpus in molecular biology domain.
In Proc. of HLTC, pages 73?77.
Sophie Rosset, Olivier Galibert, Gilles Adda, and Eric
Bilinski. 2007. The LIMSI participation to the QAst
track. In Working Notes for the CLEF 2007 Workshop,
Budapest, Hungary.
Sophie Rosset, Cyril Grouin, and Pierre Zweigenbaum.
2010. Entit?s nomm?es : guide d?annotation Quaero,
November. T3.2, presse ?crite et orale.
SAIC. 1998. Proceedings of the seventh message under-
standing conference (MUC-7).
William A Scott. 1955. Reliability of Content Analysis:
The Case of Nominal Scale Coding. Public Opinion
Quaterly, 19(3):321?325.
Satoshi Sekine. 2004. Definition, dictionaries and tagger
of extended named entity hierarchy. In Proc. of LREC.
Anne Vilnat, Patrick Paroubek, Eric Villemonte de la
Clergerie, Gil Francopoulo, and Marie-Laure Gu?not.
2010. Passage syntactic representation: a minimal
common ground for evaluation. In Proc. of LREC.
Alex Yeh, Alex Morgan, Marc Colosimo, and Lynette
Hirschman. 2005. BioCreAtIvE task 1A: gene men-
tion finding evaluation. BMC Bioinformatics, 6(1).
100
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 168?177,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Automatic Named Entity Pre-Annotation
for Out-of-Domain Human Annotation
Sophie Rosset?, Cyril Grouin?, Thomas Lavergne?,? , Mohamed Ben Jannet?,?,?,?
Je?re?my Leixa, Olivier Galibert? , Pierre Zweigenbaum?.
?LIMSI?CNRS ?Universite? Paris-Sud ?LNE
?LPP, Universite? Sorbonne Nouvelle ELDA
{rosset,grouin,lavergne,ben-jannet,pz}@limsi.fr
leixa@elda.org, olivier.galibert@lne.fr
Abstract
Automatic pre-annotation is often used to
improve human annotation speed and ac-
curacy. We address here out-of-domain
named entity annotation, and examine
whether automatic pre-annotation is still
beneficial in this setting. Our study de-
sign includes two different corpora, three
pre-annotation schemes linked to two an-
notation levels, both expert and novice an-
notators, a questionnaire-based subjective
assessment and a corpus-based quantita-
tive assessment. We observe that pre-
annotation helps in all cases, both for
speed and for accuracy, and that the sub-
jective assessment of the annotators does
not always match the actual benefits mea-
sured in the annotation outcome.
1 Introduction
Human corpus annotation is a difficult, time-
consuming, and hence costly process. This mo-
tivates research into methods which reduce this
cost (Leech, 1997). One such method consists of
automatically pre-annotating the corpus (Marcus
et al, 1993; Dandapat et al, 2009) using an ex-
isting system, e.g., a POS tagger, syntactic parser,
named entity recognizer, according to the task for
which the annotations aim to provide a gold stan-
dard. The pre-annotations are then corrected by
the human annotators. The underlying hypothe-
sis is that this should reduce annotation time while
possibly at the same time increasing annotation
completeness and consistency.
We study here corpus pre-annotation in a spe-
cific setting, out-of-domain named entity annota-
tion, in which we examine specific questions that
we present below. We produced corpora and an-
notation guidelines for named entities which are
both hierarchical and compositional (Grouin et al,
2011),1 and which we used in contrastive stud-
ies of news texts in French (Rosset et al, 2012).
We want to rely on the same named entity def-
initions for studies on two types of data we did
not cover: parliament debates (Europarl corpus)
and regional, contemporary written news (L?Est
Re?publicain), both in French. To help the annota-
tion process we could reuse our system (Dinarelli
and Rosset, 2011), but needed first to examine
whether a system trained on one type of text (our
first Broadcast News data) could be used to pro-
duce a useful pre-annotation for different types of
text (our two corpora).
We therefore set up the present study in which
we aim to answer the following questions linked
to this point and to related annotation issues:
? can a system trained on data from one spe-
cific domain be useful on data from another
domain in a pre-annotation task?
? does this pre-annotation help human annota-
tors or bias them?
? what importance can we give to the annota-
tors? subjective assessment of the usefulness
of the pre-annotation?
? can we observe differences in the use of pre-
annotation depending on the level of exper-
tise of human annotators?
Moreover, as the aforementioned annotation
scheme is based on two annotation levels (entities
and components), we want to answer these ques-
tions taking into account these two levels.
We first examine related work on pre-annotation
(Section 2), then present our corpora and annota-
tion task (Section 3). We describe and discuss ex-
periments in Section 4, and make subjective and
1Corpora, guidelines and tools are available through
ELRA under references ELRA-S0349 and ELRA-W0073.
168
quantitative observations in Sections 5 and 6. Fi-
nally, we conclude and present some perspectives
in Section 7.
2 Related Work
Facilitating human annotations has been the topic
of a large amount of research. Two different
approaches can be distinguished: active learn-
ing (Ringger et al, 2007; Settles et al, 2008) and
pre-annotation (Marcus et al, 1993; Dandapat et
al., 2009). Our work falls into the latter type.
Pre-annotation can be used in several ways. The
first is to provide annotations to be corrected by
human annotators (Fort and Sagot, 2010). A vari-
ant consists of merging multiple automatic anno-
tations before having them corrected by human
curators to produce a gold-standard (Rebholz-
Schuhmann et al, 2011). The second type con-
sists of providing clues to help human annotators
perform the annotation task (Mihaila et al, 2013).
This work addresses the first type, a single-
system pre-annotation with human correction. An
objective is to examine whether a system trained
on one type of text can be useful to pre-annotate
texts of a different type. Most previous studies
have been performed on well-behaved tasks such
as part-of-speech tagging on in-domain data, i.e.,
the model used for pre-annotating the target data
had been trained on similar data. For instance, Fort
and Sagot (2010) provide a precise evaluation of
the usefulness of pre-annotation and compare the
impact of different quality levels in POS taggers
on the Penn TreeBank corpus. They first trained
different models on the training part of the cor-
pus and applied them to the test corpus. The pre-
annotated test corpus was then corrected by hu-
mans. They reported gains in accuracy and inter-
annotator agreement. The study focused on the
minimal quality (accuracy threshold) of automatic
annotation that would prove useful for human an-
notation. They reported a gain for human annota-
tion when accuracy ranged from 66.5% to 81.6%.
On the contrary, for a semantic-frame annotation
task, Rehbein et al (2009) observed no significant
gain in quality and speed of annotation even when
using a state-of-the-art system.
Generally speaking, annotators find the pre-
annotation stage useful (Rehbein et al, 2009;
South et al, 2011; Huang et al, 2011). Anno-
tation managers consider that a bias may occur
depending on how much human annotators trust
the pre-annotation (Rehbein et al, 2009; Fort and
Sagot, 2010; South et al, 2011). In their frame-
semantic argument structure annotation, Rehbein
et al (2009) addressed a specific question consid-
ering a two-level annotation scheme: is the pre-
annotation of frame assignment (low-level anno-
tation) useful for annotating semantic roles (high-
level annotation)? Although for the low-level an-
notation task they observed a significant difference
in quality of final annotation, for the high-level
task they found no difference.
Most of these studies used a pre-annotation sys-
tem trained on the same kind of data as those
which were to be annotated manually. Neverthe-
less some system-oriented studies have focused on
the results obtained by systems trained on one type
of corpus and applied to another type of corpus,
e.g., for a Latin POS tagger (Poudat and Longre?e,
2009; Skj?rholt, 2011) or for a CoNLL named en-
tity tagger for German (Faruqui and Pado?, 2010)
for which the authors noticed noticed a reduc-
tion of the F-measure when going from in-domain
(newswire data, F=0.782 for their best system) to
out-of-domain (Europarl data, F=0.656).
One of our objectives is then to examine
whether a system trained on one type of text can
be useful to pre-annotate texts of a different type.
We set up experiments to study precisely the pos-
sible induced bias and whether the level of experi-
ence of the annotators would make a difference in
such a context. In this study, we used two different
kinds of corpora, which were both different from
the corpus used to train the pre-annotation system.
3 Task and corpus description
3.1 Task
In this work, we used the structured named entity
definition we proposed in a previous study (Grouin
et al, 2011): entities are both hierarchical (types
have subtypes) and compositional (types and com-
ponents are included in entities) as in Figure 1.
func.coll
org.ent
name
BEIde la
kind
analystes financiersles
Figure 1: Multi-level annotation of entity sub-
types (red tags) and components (blue tags): the
financial analysts of the EBI
169
This taxonomy of entity types is composed of
7 types (person, location, organization, amount,
time, production and function) and 32 sub-types
(individual person pers.ind vs. group of persons
pers.coll; administrative organization org.adm vs.
services org.ent; etc.). Types and subtypes consti-
tute the first level of annotation.
Within these categories, components are
second-level elements (kind, name, first.name,
etc.), and can never be used outside the scope of a
type or subtype element.
3.2 Corpora
Two French corpora were sampled from larger
ones:
Europarl: Prepared speech (Parliament
Debates?Europarl): 15,306 word extract;
Press: Local, contemporary written news (L?Est
Re?publicain): 11,146 word extract.
These corpora were automatically annotated us-
ing the system described in (Dinarelli and Rosset,
2011). This system relies on a Conditional Ran-
dom Field (CRF) model for the detection of com-
ponents and on a probabilistic context-free gram-
mar (PCFG) model for types and sub-types. These
models have been trained on Broadcast News data.
This system achieved a Slot Error Rate (Makhoul
et al, 1999) of 37.0% on Broadcast conversation
and 29.7% on Broadcast news, and ranked first in
the Quaero evaluation campaign (Galibert et al,
2011).
4 Experiments
In this section we present the protocol we designed
to study the usefulness of pre-annotation under
different conditions, and its overall results.
4.1 Protocol
We defined the following protocol, similar to the
one used in Rehbein et al (2009).
Corpora. Four versions of our two corpora were
prepared: (i) raw text, (ii) pre-annotation of
types, (iii) pre-annotation of components, and
(iv) full pre-annotation of both types and compo-
nents. Each of these four versions was split into
four quarters.
Annotators. Eight human annotators were in-
volved in this task. Among them, four are con-
sidered as expert annotators (they annotated cor-
pora in the previous years) while the four re-
maining ones are novice annotators (this was the
first time they annotated such corpora; they were
given training sessions before starting actual anno-
tation). We defined four pairs of annotators, where
each pair was composed of an expert and a novice
annotator.
Quarter allocation. We allocated each corpus
quarter in such a way that each pair of annotators
processed, in each corpus, material from each one
of the four pre-annotated versions (see Table 3).
The same allocation was made in both corpora.
4.2 Results
For each corpus part, a reference was built based
on a majority vote by confronting all annotations.
The resulting reference corpus is presented in Ta-
ble 1.
Corpus # comp. # types # entities # words
P
re
ss
Q1 481 310 791 3047
Q2 367 246 673 2628
Q3 495 327 822 2971
Q4 413 282 695 2600
E
ur
op
ar
l Q1 362 259 621 3926
Q2 309 221 530 3809
Q3 378 247 625 3604
Q4 413 299 712 3967
Table 1: General description of the reference an-
notations: number of components, types, entities
(the sum of components and types), and words
Table 2 presents the performance of the au-
tomatic pre-annotation system against the refer-
ence corpus. We used the well known F-measure
and in addition the Slot Error Rate as it allows
to weight different error classes (deletions, inser-
tions, type or frontier errors). Fort and Sagot
(2010) reported a gain in human annotation when
pre-annotation accuracy ranged from 66.5% to
81.6%. Given their results we can hope for a gain
in both accuracy and annotation time when using
pre-annotation.
Table 3 presents all results obtained by each an-
notators given each pre-annotation condition (raw,
components, types and full) in terms of precision,
recall and F-measure.
170
Corpus #
Raw text Components Types Full
R P F R P F R P F R P F
Press
Q1
0.874 0.777 0.823 0.876 0.741 0.803 0.824 0.870 0.846 0.852 0.800 0.825
0.810 0.766 0.788 0.815 0.777 0.796 0.645 0.724 0.683 0.844 0.785 0.813
Q2
0.765 0.796 0.780 0.870 0.773 0.819 0.822 0.801 0.812 0.917 0.773 0.839
0.558 0.654 0.602 0.826 0.775 0.800 0.815 0.777 0.795 0.816 0.752 0.783
Q3
0.835 0.715 0.771 0.888 0.809 0.847 0.884 0.796 0.837 0.887 0.859 0.873
0.792 0.689 0.736 0.904 0.780 0.837 0.876 0.771 0.820 0.780 0.827 0.803
Q4
0.802 0.757 0.779 0.845 0.876 0.860 0.900 0.702 0.789 0.914 0.840 0.876
0.794 0.727 0.759 0.696 0.715 0.705 0.812 0.701 0.752 0.802 0.757 0.779
Europarl
Q1
0.809 0.728 0.766 0.800 0.568 0.665 0.776 0.862 0.817 0.754 0.720 0.736
0.754 0.720 0.736 0.720 0.609 0.660 0.687 0.607 0.644 0.736 0.638 0.683
Q2
0.776 0.792 0.784 0.782 0.617 0.690 0.797 0.645 0.713 0.821 0.526 0.641
0.563 0.498 0.529 0.802 0.619 0.699 0.698 0.553 0.617 0.769 0.566 0.652
Q3
0.747 0.459 0.569 0.749 0.624 0.681 0.805 0.800 0.803 0.735 0.744 0.739
0.732 0.598 0.658 0.736 0.717 0.726 0.822 0.738 0.777 0.808 0.734 0.769
Q4
0.742 0.624 0.678 0.874 0.760 0.813 0.732 0.480 0.580 0.743 0.608 0.669
0.721 0.566 0.634 0.695 0.652 0.672 0.707 0.600 0.649 0.738 0.603 0.664
Table 3: Overall recall, precision and F-measure for each pair of annotators (blue: pair #1, ocre: pair
#2, green: pair #3, white: pair #4) on each corpus quarter (Q1, Q2, Q3, Q4), depending on the kind of
pre-annotation (raw text, only components, only types, full pre-annotation). Expert annotator is on the
upper line of each quarter, novice annotator is on the lower line. Boldface indicates the best F-measure
for each novice and expert annotator among all pre-annotation tasks in a given corpus quarter
Corpus
Components Types Full
F SER F SER F SER
P
re
ss
Q1 72.4 37.9 63.5 46.3 68.9 41.0
Q2 77.2 32.2 66.8 43.5 73.1 36.6
Q3 76.1 34.1 68.3 41.7 73.1 36.9
Q4 76.1 33.3 63.3 45.7 71.0 38.2
E
ur
op
ar
l Q1 61.9 49.9 57.5 55.4 60.1 52.2
Q2 61.2 51.3 54.6 54.3 58.5 52.5
Q3 61.6 50.1 53.3 55.7 58.2 52.2
Q4 57.1 57.0 48.1 59.7 53.3 58.1
Broad. 88.3 29.1 73.1 39.1 73.2 33.1
Table 2: F-measure and Slot Error Rate achieved
by the automatic system on each kind of annota-
tion and on in-domain broadcast data
We also computed inter-annotator agreement
(IAA) for each corpus considering two groups of
annotators, experts and novices. We consider that
the inter-annotator agreement is somewhere be-
tween the F-measure and the standard IAA con-
sidering as markables all the units annotated by at
least one of the annotators (Grouin et al, 2011).
We computed Scott?s Pi (Scott, 1955), and Co-
hen?s Kappa (Cohen, 1960). The former considers
one model for all annotators while the latter con-
siders one model per annotator. In our case, these
two values are almost the same, which means that
the proportions and kinds of annotations are very
similar across experts and novices. Figure 2 shows
the IAA (Cohen?s Kappa and F-measure) obtained
on the two corpora given the four pre-annotation
conditions (no pre-annotation, components, types,
and full pre-annotation). As we can see, IAA is
systematically higher for the Press corpus than
for the Europarl corpus, which can be linked
to the higher performance of the automatic pre-
annotation system on this corpus. We also can see
that pre-annotation always improves agreement
and that full pre-annotation yields the best result.
We observe that, as expected, pre-annotation leads
human annotators to obtain higher consistency.
5 Subjective assessment
An important piece of information in any anno-
tation campaign is the feelings of the annotators
about the task. This can give interesting clues
about the expected quality of their work and on the
usefulness of the pre-annotation step. We asked
the annotators a few questions concerning sev-
eral features of this project, such as the annotation
171
 0.5
 0.6
 0.7
 0.8
 0.9
 1
raw comp types full
Press: Cohen's kappaPress: F-measureEuroparl: Cohen's kappaEuroparl: F-measure
Figure 2: Cohen?s Kappa (red and blue) and F-
measure (green and pink) measuring agreement of
experts and novices on Press and Europarl corpora
in four pre-annotation conditions. Each measure
compares the concatenated annotations of the four
experts with the four novices.
manual, or how they assessed the benefits of pre-
annotation in the different corpora (Section 5.1).
Another important point is the experience of the
annotators, which we also examine in the light of
theirs answers to the questionnaire (Section 5.2).
5.1 Questionnaire
The questionnaire submitted to the annotators con-
tained 4 questions, dealing with their feedback on
the annotation process:
1. According to you, which level of pre-
annotation has been the most helpful during
the annotation process? Types, components,
or both?
2. To what extent would you say that pre-
annotation helped you in terms of precision
and speed? Did it produce many errors you
had to correct?
3. If you had to choose between the Europarl
corpus and the Press corpus, could you say
that one has been easier to annotate than the
other?
4. Concerning the annotation manual, are there
topics that you would like to change, or cor-
rect? In the same way, which named entities
caused you the most difficulties to deal with?
All 8 annotators answered these questions. We
summarize below what we found in their answers.
5.1.1 Level of pre-annotation
Most of the annotators preferred the corpora that
were pre-annotated with types only. The reason,
for the most part, is that a pre-annotation of types
allows the annotator to work faster on their files,
because guessing the components from the types
is easier than guessing types from components.2
Indeed, the different types of entities defined in
the manual always imply the same components,
be they specific (to one entity type) or transverse
(common to several entity types). On the contrary,
a transverse component, such as <kind>, can be
part of any type of named entity. The other rea-
son for this choice of pre-annotation concerns the
readability brought to the corpora. An annotation
with types only is easier to read than an annotation
with components, and less exhausting after many
hours of work on the texts.
5.1.2 Gain in precision and speed
What motivated the answers to the second ques-
tion mainly concerns the accuracy of the different
pre-annotation methods. While all of them pre-
sented errors that needed to be corrected, the pre-
annotation of types was the one that they felt pre-
sented the smaller number of errors. Thus, annota-
tors spent less time reviewing the corpora in search
of errors, compared to the other pre-annotated cor-
pora (with components, and with both types and
components), where more errors had to be spot-
ted and corrected. This search for incorrect pre-
annotations impacted the time spent on each cor-
pus. Indeed, most annotators declared that pre-
annotation with types was quicker to deal with
than other pre-annotation schemes.
5.1.3 Corpus differences
About one half of the annotators agreed that the
Europarl corpus had been more difficult to anno-
tate. Despite obvious differences in register, sen-
tence structure and vocabulary between the two
corpora, Europarl seemed more redundant and
complex than the other corpus. For instance, one
of the annotators declared:
The Europarl corpus is more difficult
to annotate in the sense that the exist-
ing types and components do not al-
ways match the realities found in the
corpus, either because their definitions
2This feeling is supported by results about ambiguity pre-
sented in Fort et al (2012).
172
cannot apply exactly, or because the re-
quired types and components are miss-
ing (mainly for frequencies: ?five times
per year?).
The other half of the annotators did not feel any
specific difficulties in annotating one corpus or the
other. According to them, both corpora are the
same in terms of register and sentence structure.
5.1.4 Improvements in guidelines
All of the annotators were unanimous in think-
ing that two points need to be modified in the
manual. First of all, the distinction between the
<org.adm> and <org.ent> subtypes is too diffi-
cult to apprehend, above all in the Europarl corpus
where these entities are too ambiguous to be anno-
tated correctly. Secondly, the distinction between
the <pers> and <func> types has also been diffi-
cult to deal with. The other remarks about poten-
tial changes mainly concerned the introduction of
explicit rules for frequencies, which are recurrent
in the Europarl corpus.
5.2 Experience
As mentioned earlier in Section 4.1, we will now
see if the differences in experience between an-
notators impacted their difficulty in annotating the
corpora. First of all, when we look at the answers
given to question 3, we notice that both novice and
expert annotators consider the Europarl corpus the
most difficult to annotate. Most of their answers
deal with the redundancy and the formal register
of the data. Moreover, as everyone answered in
question 4, both <func> and <org> entities have
to be modified to be easier to understand and to
use. This unanimous opinion about what needs
to be reviewed in the manual allows us to think
that the annotators? level of experience has a low
impact on their apprehension of the corpora, both
Europarl and Press. To confirm this, we can look
at the answers given to questions 1 and 2, as indi-
cated in the previous paragraph. As has been ex-
plained, every annotator correctly pointed at the
many errors found in pre-annotation, regardless
of their experience. Besides, the assessment of
the benefits of pre-annotation is the same for al-
most everyone, regardless of their experience too:
both novice and expert annotators agree that pre-
annotation with type adds efficiency and speed to
annotation.
To conclude, according to our observations
based on the questionnaire, we cannot assert that
there has been a difference between novice and ex-
pert annotators. Both groups agreed on the same
difficulties, pointed at the same errors, and crit-
icized the same entities, saying that their defini-
tions needed to be clarified.
6 Quantitative observations
In this section we provide results of quantitative
observations in order to support, or not, the anno-
tators? subjective assessment.
6.1 Corpus statistics
The annotators reported different feelings depend-
ing on the corpora. Some of them reported that
the Europarl corpus was more difficult to annotate,
with more complex sentence structures, or usage
of fewer proper nouns.
To explore these differences, we computed
some statistics over the two original, un-annotated
corpora (which are much larger than the samples
annotated in this experiment) as well as over the
original broadcast news corpus used to train the
pre-annotation system. Each of these corpora con-
tains several million words.
Table 4 reports simple statistics about sentences
in the three corpora. Based on these statistics,
while the Europarl (Euro) corpus is very similar to
the original Broadcast News (BN), the Press cor-
pus shows differences: sentences are 20% shorter,
with fewer but larger chunks, confirming the im-
pression of simpler, less convoluted sentences.
BN Press Euro
Mean sentence length 30.2 23.9 29.7
Mean chunk count 10.9 6.7 10.4
Mean chunk length 2.7 3.6 2.8
Table 4: Sentence summary of the three corpora
Looking more closely at the contents of these
sentences, Figure 3 summarizes the proportions of
grammatical word classes. The sentiment of ex-
tensive naming of entities in the Press corpus is
confirmed by the four times higher rate of proper
nouns. On the other hand, entities are more often
referred to using nouns with an optional adjective
in the Europarl corpus, leading to a more frequent
usage of the latter.
173
Figure 3: Frequency of word classes in the three
corpora (BN = Broadcast News, Est = Press, Euro
= Europarl). TOOL = grammatical words, PCT/NB
= punctuation and numbers, ADJ/ADV = adjectives
and adverbs, NAM = proper name, NOM = noun,
VER = verb.
6.2 Influence of pre-annotation on the
behaviour of annotators
As already mentioned, it is often reported that a
bias may occur depending on human confidence
in the pre-annotation (Fort and Sagot, 2010; Re-
hbein et al, 2009; South et al, 2011). An im-
portant unknown is always the influence of pre-
annotation on the behaviour of annotators, and at
which point pre-annotation induces more errors
than it helps. This may obviously depend on pre-
annotation quality. Table 5 summarizes the er-
ror rates of the automatic annotator in the stud-
ied data (Press + Europarl) and in comparison to
in-domain data. Insertions (Ins) are extra anno-
tations, deletions (Del) missing annotations, and
substitutions (Subs) are annotations that are incor-
rect in type, boundaries, or both. We can see that
Domain Pre-annotation Ins Del Subs
Components 4.4% 33.6% 7.8%
Out Types 7.0% 36.2% 12.7%
Full 5.5% 34.6% 9.7%
In Full 3.7% 23.4% 10.6%
Table 5: Pre-annotation errors and comparison
with in-domain (Broadcast News) data
going out-of-domain increased deletions, proba-
bly through a lack of knowledge of domain vo-
cabulary. But it did not influence the other error
rates significantly. It is also noticeable that dele-
tion is the type of error most produced by the sys-
tem, with every third entity missed. Automatic,
full pre-annotation of Press + Europarl obtains a
precision of 0.79 and a recall of 0.56.
Human annotator performance can then be mea-
sured over the same three error types (Table 6). We
Pre-annotation Ins Del Subs
Raw 8.9% 18.9% 12.8%
Components 5.9% 16.7% 11.3%
Types 7.1% 16.5% 12.0%
Full 7.1% 16.5% 10.1%
Table 6: Mean human annotation error levels for
each pre-annotation scheme
can see that annotation quality was systematically
improved by pre-annotation, with the best global
result obtained by full pre-annotation. In addition
there was no increase in deletions (had the human
stopped looking at the unannotated text) or inser-
tions (had the human always trusted the system) as
might have been feared. This may be a side effect
of the high deletion rate, making it obvious to the
human that the system was missing things. In any
case, the annotation was clearly beneficial in our
experiment with no ill effects seen in error rates
compared to the gold standard.
6.3 Is pre-annotation useful and to whom?
All annotators asserted that pre-annotation is use-
ful, specifically with types. In this section, we pro-
vide observations concerning variations in annota-
tion both in terms of accuracy (F-measure is used)
and duration.
Raw Comp. Types Full
Experts 0.748 0.786 0.778 0.791
Novices 0.682 0.737 0.721 0.742
Table 7: Mean F-measure of experts and novices,
for each pre-annotation scheme
Raw Comp. Types Full
Experts 109.0 52.5 64.0 39.13
Novices 151.7 135.5 117.9 103.88
Table 8: Mean duration (in minutes) of annotation
for experts and novices, for each pre-annotation
scheme (two corpus quarters)
Tables 7 and 8 confirm the hypothesis that auto-
matic pre-annotation helps annotators to annotate
174
faster and to be more efficient. All pre-annotation
levels (components, types and both) seem to be
helpful for both experts and novices. Experts
reached a higher accuracy (F=0.791) and they
were more than twice faster with components or
full pre-annotation. Similarly, novices performed
better when working on a full pre-annotation
(F=0.742) and reached a faster working time
(48mn less than with no pre-annotation). This last
observation contradicts the annotators? reported
experience: the annotators felt more comfortable
and faster with a types-only pre-annotation than
with full pre-annotation (see Section 5.1.2). The
results show that full pre-annotation was the best
choice for both quality and speed.
These results confirm that pre-annotation is use-
ful, even with a moderate level of performance of
the system. Does it help to annotate components
and types equally? To answer this question, we
computed the F-measure of novices and experts
for both components and types separately (see Fig-
ure 4).
 60
 65
 70
 75
 80
 85
 90
raw comp types full
types/novicescomponents/novicestypes/expertscomponents/experts
Figure 4: Mean F-measure on each pre-annotation
level for expert and novice annotators
For experts we can see that all pre-annotation
levels allow them to improve their performance on
both types and components. However for novices,
pre-annotation with types does not improve their
performance in labeling components. We also no-
tice that pre-annotation in both types and compo-
nents allows experts and novices to reach their best
performance for both types and components.
7 Conclusion and Perspectives
Conclusion. In this paper, we studied the inter-
est of a pre-annotation process for a complex an-
notation task with only an out-of-domain annota-
tion system available. We also designed our exper-
iments to check whether the level of experience of
the annotators made a difference in such a context.
The experiment produced in the end a high-quality
gold standard (8-way merge including 2 versions
without pre-annotation) which enabled us to mea-
sure quantitatively the performance of every pre-
annotation scheme.
We noticed that the pre-annotation system
proved relatively precise for such a complex task,
with 79% correct pre-annotations, but with a poor
recall at 56%. This may be a good operating point
for a pre-annotation system to reduce bias though.
In our quantitative experiments we found that
the fullest pre-annotation helped most, both in
terms of quality and annotation speed, even though
the quality of the pre-annotation system varied de-
pending on the annotation layer. This contradicted
the feelings of the annotators who thought that a
type-only pre-annotation was the most efficient.
This shows that in such a setting self-evaluation
cannot be trusted. On the other hand their remarks
about the problems in the annotation guide itself
seemed rather pertinent.
When it comes to experts vs. novices, we noted
that their behaviour and remarks were essentially
identical. Experts were both better and faster
at annotating, but had similar reactions to pre-
annotation and essentially the same feelings.
In conclusion, even with an out-of-domain sys-
tem, a pre-annotation step proves extremely useful
in both annotation speed and annotation quality,
and at least in our setting, with a reasonably pre-
cise system (at the expense of recall) no bias was
detectable. In addition, no matter what the anno-
tators feel, as long as precision is good enough,
the more pre-annotations the better. Pre-filtering
either of our two levels did not help.
Perspectives. Based upon this conclusion, we
plan to use automatic pre-annotation in further an-
notation work, beginning with the present corpora.
As a first use, we plan to propose a few changes
to the annotation principles in the guidelines we
used. To annotate existing corpora with these
changes, automatic pre-annotation will be useful.
As a second piece of future work, we plan to
annotate new corpora with the existing annotation
framework. We also plan to add new types of
named entities (e.g., events) to extend the anno-
tation of existing annotated corpora, using the pre-
annotation process to reduce the overall workload.
175
Acknowledgments
This work has been partially funded by OSEO un-
der the Quaero program and by the French ANR
VERA project.
References
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Sandipan Dandapat, Priyanka Biswas, Monojit Choud-
hury, and Kalika Bali. 2009. Complex linguistic an-
notation ? no easy way out! A case from Bangla and
Hindi POS labeling tasks. In Proc of 3rd Linguistic
Annotation Workshop (LAW-III), pages 10?18, Sun-
tec, Singapore, August. ACL.
Marco Dinarelli and Sophie Rosset. 2011. Models
cascade for tree-structured named entity detection.
In Proc of IJCNLP, pages 1269?1278, Chiang Mai,
Thailand.
Manaal Faruqui and Sebastian Pado?. 2010. Training
and evaluating a German named entity recognizer
with semantic generalization. In Proc of Konvens,
Saarbru?cken, Germany.
Kare?n Fort and Beno??t Sagot. 2010. Influence of pre-
annotation on POS-tagged corpus development. In
Proc of 4th Linguistic Annotation Workshop (LAW-
IV), pages 56?63, Uppsala, Sweden. ACL.
Kare?n Fort, Adeline Nazarenko, and Sophie Rosset.
2012. Modeling the complexity of manual anno-
tation tasks: a grid of analysis. In Proceedings of
COLING 2012, pages 895?910, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Olivier Galibert, Sophie Rosset, Cyril Grouin, Pierre
Zweigenbaum, and Ludovic Quintard. 2011. Struc-
tured and extended named entity evaluation in au-
tomatic speech transcriptions. In Proc of IJCNLP,
Chiang Mai, Thailand.
Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,
Kare?n Fort, Olivier Galibert, and Ludovic Quin-
tard. 2011. Proposal for an extension of tradi-
tional named entities: From guidelines to evalua-
tion, an overview. In Proc of 5th Linguistic Anno-
tation Workshop (LAW-V), pages 92?100, Portland,
OR. ACL.
Minlie Huang, Aure?lie Ne?ve?ol, and Zhiyong Lu.
2011. Recommending MeSH terms for annotating
biomedical articles. Journal of the American Medi-
cal Informatics Association, 18(5):660?7.
Geoffrey Leech. 1997. Introducing corpus annota-
tion. In Roger Garside, Geoffrey Leech, and Tony
McEnery, editors, Corpus annotation: Linguistic in-
formation from computer text corpora, pages 1?18.
Longman, London.
John Makhoul, Francis Kubala, Richard Schwartz, and
Ralph Weischedel. 1999. Performance measures for
information extraction. In Proc. of DARPA Broad-
cast News Workshop, pages 249?252.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn TreeBank. Com-
putational Linguistics, 19(2):313?330.
Claudiu Mihaila, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. Biocause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14:2.
Ce?line Poudat and Doninique Longre?e. 2009. Vari-
ations langagie`res et annotation morphosyntaxique
du latin classique. Traitement Automatique des
Langues, 50(2):129?148.
Dietrich Rebholz-Schuhmann, Antonio Jimeno, Chen
Li, Senay Kafkas, Ian Lewin, Ning Kang, Peter Cor-
bett, David Milward, Ekaterina Buyko, Elena Beiss-
wanger, Kerstin Hornbostel, Alexandre Kouznetsov,
Rene? Witte, Jonas B Laurila, Christopher JO Baker,
Cheng-Ju Kuo, Simone Clematide, Fabio Rinaldi,
Richa?rd Farkas, Gyo?rgy Mo?ra, Kazuo Hara, Laura I
Furlong, Michael Rautschka, Mariana Lara Neves,
Alberto Pascual-Montano, Qi Wei, Nigel Collier,
Md Faisal Mahbub Chowdhury, Alberto Lavelli,
Rafael Berlanga, Roser Morante, Vincent Van Asch,
Walter Daelemans, Jose? L Marina, Erik van Mulli-
gen, Jan Kors, and Udo Hahn. 2011. Assessment of
NER solutions against the first and second CALBC
silver standard corpus. J Biomed Semantics, 2.
Ines Rehbein, Josef Ruppenhofer, and Caroline
Sporleder. 2009. Assessing the benefits of partial
automatic pre-labeling for frame-semantic annota-
tion. In Proc of 3rd Linguistic Annotation Workshop
(LAW-III), pages 19?26, Suntec, Singapore. ACL.
Eric Ringger, Peter McClanahan, Robbie Haertel,
George Busby, Marc Carmen, James Carroll, Kevin
Seppi, and Deryle Lonsdale. 2007. Active learning
for part-of-speech tagging: Accelerating corpus an-
notation. In Proc of Linguistic Annotation Workshop
(LAW), pages 101?108. ACL.
Sophie Rosset, Cyril Grouin, Kare?n Fort, Olivier Gal-
ibert, Juliette Kahn, and Pierre Zweigenbaum. 2012.
Structured named entities in two distinct press cor-
pora: Contemporary broadcast news and old news-
papers. In Proc of 6th Linguistic Annotation Work-
shop (LAW-VI), pages 40?48, Jeju, South Korea.
ACL.
William A Scott. 1955. Reliability of content analysis:
The case of nominal scale coding. Public Opinion
Quaterly, 19(3):321?325.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Proc
of the NIPS Workshop on Cost-Sensitive Learning.
176
Arne Skj?rholt. 2011. More, faster: Accelerated cor-
pus annotation with statistical taggers. Journal for
Language Technology and Computational Linguis-
tics, 26(2):151?163.
Brett R South, Shuying Shen, Robyn Barrus, Scott L
DuVall, O?zlem Uzuner, and Charlene Weir. 2011.
Qualitative analysis of workflow modifications used
to generate the reference standard for the 2010
i2b2/VA challenge. In Proc of AMIA.
177
