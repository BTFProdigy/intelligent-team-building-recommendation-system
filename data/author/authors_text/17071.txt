Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1357?1367, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Word Salad: Relating Food Prices and Descriptions
Victor Chahuneau Kevin Gimpel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{vchahune,kgimpel}@cs.cmu.edu
Bryan R. Routledge
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
routledge@cmu.edu
Lily Scherlis
Phillips Academy
Andover, MA 01810, USA
lily.scherlis@gmail.com
Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We investigate the use of language in food
writing, specifically on restaurant menus and
in customer reviews. Our approach is to build
predictive models of concrete external vari-
ables, such as restaurant menu prices. We
make use of a dataset of menus and customer
reviews for thousands of restaurants in several
U.S. cities. By focusing on prediction tasks
and doing our analysis at scale, our method-
ology allows quantitative, objective measure-
ments of the words and phrases used to de-
scribe food in restaurants. We also explore
interactions in language use between menu
prices and sentiment as expressed in user re-
views.
1 Introduction
What words might a menu writer use to justify the
high price of a steak? How does describing an item
as chargrilled vs. charbroiled affect its price? When
a customer writes an unfavorable review of a restau-
rant, how is her word choice affected by the restau-
rant?s prices? In this paper, we explore questions
like these that relate restaurant menus, prices, and
customer sentiment. Our goal is to understand how
language is used in the food domain, and we di-
rect our investigation using external variables such
as restaurant menu prices.
We build on a thread of NLP research that seeks
linguistic understanding by predicting real-world
quantities from text data. Recent examples include
prediction of stock volatility (Kogan et al 2009)
and movie revenues (Joshi et al 2010). There, pre-
diction tasks were used for quantitative evaluation
and objective model comparison, while analysis of
learned models gave insight about the social process
behind the data.
We echo this pattern here as we turn our atten-
tion to language use on restaurant menus and in user
restaurant reviews. We use data from a large cor-
pus of restaurant menus and reviews crawled from
the web and formulate several prediction tasks. In
addition to predicting menu prices, we also consider
predicting sentiment along with price.
The relationship between language and senti-
ment is an active area of investigation (Pang and
Lee, 2008). Much of this research has focused on
customer-written reviews of goods and services, and
perspectives have been gained on how sentiment is
expressed in this type of informal text. In addition
to sentiment, however, other variables are reflected
in a reviewer?s choice of words, such as the price of
the item under consideration. In this paper, we take
a step toward joint modeling of multiple variables
in review text, exploring connections between price
and sentiment in restaurant reviews.
Hence this paper contributes an exploratory data
1357
analysis of language used to describe food (by its
purveyors and by its consumers). While our primary
goal is to understand the language used in our cor-
pus, our findings bear relevance to economics and
hospitality research as well. This paper is a step on
the way to the eventual goal of using linguistic anal-
ysis to understand social phenomena like sales and
consumption.
2 Related Work
There are several areas of related work scattered
throughout linguistics, NLP, hospitality research,
and economics.
Freedman and Jurafsky (2011) studied the use of
language in food advertising, specifically the words
on potato chip bags. They argued that, due to
the ubiquity of food writing across cultures, eth-
nic groups, and social classes, studying the use of
language for describing food can provide perspec-
tive on how different socioeconomic groups self-
identify using language and how they are linguisti-
cally targeted. In particular, they showed that price
affects how ?authenticity? is realized in marketing
language, a point we return to in ?5. This is an ex-
ample of how price can affect how an underlying
variable is expressed in language. Among other ex-
plorations in this paper, we consider how price inter-
acts with expression of sentiment in user reviews of
restaurants.
As mentioned above, our work is related to re-
search in predicting real-world quantities using text
data (Koppel and Shtrimberg, 2006; Ghose et al
2007; Lerman et al 2008; Kogan et al 2009; Joshi
et al 2010; Eisenstein et al 2010; Eisenstein et
al., 2011; Yogatama et al 2011). Like much of
this prior work, we aim to learn how language is
used in a specific context while building models that
achieve competitive performance on a quantitative
prediction task.
Along these lines, there is recent interest in ex-
ploring the relationship between product sales and
user-generated text, particularly online product re-
views. For example, Ghose and Ipeirotis (2011)
studied the sales impact of particular properties of
review text, such as readability, the presence of
spelling errors, and the balance between subjective
and objective statements. Archak et al(2011) had a
similar goal but decomposed user reviews into parts
describing particular aspects of the product being
reviewed (Hu and Liu, 2004). Our paper differs
from price modeling based on product reviews in
several ways. We consider a large set of weakly-
related products instead of a homogeneous selection
of a few products, and the reviews in our dataset are
not product-centered but rather describe the overall
experience of visiting a restaurant. Consequently,
menu items are not always mentioned in reviews and
rarely appear with their exact names. This makes it
difficult to directly use review features in a pricing
model for individual menu items.
Menu planning and pricing has been studied for
many years by the culinary and hospitality research
community (Kasavana and Smith, 1982; Kelly et al
1994), often including recommendations for writing
menu item descriptions (Miller and Pavesic, 1996;
McVety et al 2008). Their guidelines frequently
include example menus from successful restaurants,
but typically do not use large corpora of menus or
automated analysis, as we do here. Other work
focused more specifically on particular aspects of
the language used on menus, such as the study by
Zwicky and Zwicky (1980), who made linguistic ob-
servations through manual analysis of a corpus of
200 menus.
Relatedly, Wansink et al(2001; 2005) showed
that the way that menu items are described af-
fects customers? perceptions and purchasing behav-
ior. When menu items are described evocatively,
customers choose them more often and report higher
satisfaction with quality and value, as compared to
when they are given the same items described with
conventional names. Wansink et aldid not use a
corpus, but rather conducted a small-scale experi-
ment in a working cafeteria with customers and col-
lected surveys to analyze consumer reaction. While
our goals are related, our experimental approach is
different, as we use automated analysis of thousands
of restaurant menus and rely on a set of one mil-
lion reviews as a surrogate for observing customer
behavior.
Finally, the connection between products and
prices is also a central issue in economics. How-
ever, the stunning heterogeneity in products makes
empirical work challenging. For example, there are
over 50,000 menu items in New York that include
1358
City # Restaurants # Menu Items # Reviews
train dev. test train dev. test train dev. test
Boston 930 107 113 63,422 8,426 8,409 80,309 10,976 11,511
Chicago 804 98 100 51,480 6,633 6,939 73,251 9,582 10,965
Los Angeles 624 80 68 17,980 2,938 1,592 75,455 13,227 5,716
New York 3,965 473 499 365,518 42,315 45,728 326,801 35,529 37,795
Philadelphia 1,015 129 117 83,818 11,777 9,295 52,275 7,347 5,790
San Francisco 1,908 255 234 103,954 12,871 12,510 499,984 59,378 67,010
Washington, D.C. 773 110 121 47,188 5,957 7,224 71,179 11,852 14,129
Total 10,019 1,252 1,252 733,360 90,917 91,697 1,179,254 147,891 152,916
Table 1: Dataset statistics.
the word chicken. What is the price of chicken? This
is an important practical and daunting matter when
measuring inflation (e.g., Consumer Price Index is
measured with a precisely-defined basket of goods).
Price dispersion across goods and the variation of
the goods is an important area of industrial organi-
zation economic theory. For example, economists
are interested in models of search, add-on pricing,
and obfuscation (Baye et al 2006; Ellison, 2005).
3 Data
We crawled Allmenus.com (www.allmenus.
com) to gather menus for restaurants in seven
U.S. cities: Boston, Chicago, Los Angeles, New
York, Philadelphia, San Francisco, and Washing-
ton, D.C. Each menu includes a list of item names
with optional text descriptions and prices. Most All-
menus restaurant pages contain a link to the cor-
responding page on Yelp (www.yelp.com) with
metadata and user reviews for the restaurant, which
we also collected.
The metadata consist of many fields for each
restaurant, which can be divided into three cate-
gories: location (city, neighborhood, transit stop),
services available (take-out, delivery, wifi, parking,
etc.), and ambience (good for groups, noise level,
attire, etc.). Also, the category of food and a price
range ($ to $$$$, indicating the price of a typical
meal at the restaurant) are indicated. The user re-
views include a star rating on a scale of 1 to 5.
The distribution of prices of individual menu
items is highly skewed, with a mean of $9.22 but
a median of $6.95. On average, a restaurant has
73 items on its menu with a median price of $8.69
and 119 Yelp reviews with a median rating of 3.55
????????
0
100k
200k
300k
400k
500k
  
 
 
 
 
 
 
 
 
star rating
??????? ????????
?????????????
????????????????????????????
???Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1677?1687,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Translating into Morphologically Rich Languages with Synthetic Phrases
Victor Chahuneau Eva Schlinger Noah A. Smith Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{vchahune,eschling,nasmith,cdyer}@cs.cmu.edu
Abstract
Translation into morphologically rich lan-
guages is an important but recalcitrant prob-
lem in MT. We present a simple and effec-
tive approach that deals with the problem in
two phases. First, a discriminative model is
learned to predict inflections of target words
from rich source-side annotations. Then, this
model is used to create additional sentence-
specific word- and phrase-level translations
that are added to a standard translation model
as ?synthetic? phrases. Our approach re-
lies on morphological analysis of the target
language, but we show that an unsupervised
Bayesian model of morphology can success-
fully be used in place of a supervised analyzer.
We report significant improvements in transla-
tion quality when translating from English to
Russian, Hebrew and Swahili.
1 Introduction
Machine translation into morphologically rich lan-
guages is challenging, due to lexical sparsity and the
large variety of grammatical features expressed with
morphology. In this paper, we introduce a method
that uses target language morphological grammars
(either hand-crafted or learned unsupervisedly) to
address this challenge and demonstrate its effective-
ness at improving translation from English into sev-
eral morphologically rich target languages.
Our approach decomposes the process of produc-
ing a translation for a word (or phrase) into two
steps. First, a meaning-bearing stem is chosen and
then an appropriate inflection is selected using a
feature-rich discriminative model that conditions on
the source context of the word being translated.
Rather than attempting to directly produce full-
sentence translations using such an elementary pro-
cess, we use our model to generate translations of
individual words and short phrases that augment?
on a sentence-by-sentence basis?the inventory of
translation rules obtained using standard translation
rule extraction techniques (Chiang, 2007). We call
these synthetic phrases.
The major advantages of our approach are: (i)
synthesized forms are targeted to a specific transla-
tion context; (ii) multiple, alternative phrases may
be generated with the final choice among rules left
to the global translation model; (iii) virtually no
language-specific engineering is necessary; (iv) any
phrase- or syntax-based decoder can be used with-
out modification; and (v) we can generate forms that
were not attested in the bilingual training data.
The paper is structured as follows. We first
present our ?translate-and-inflect? model for pre-
dicting lexical translations into morphologically rich
languages given a source word and its context (?2).
Our approach requires a morphological grammar to
relate surface forms to underlying ?stem, inflection?
pairs; we discuss how either a standard morpholog-
ical analyzer or a simple Bayesian unsupervised an-
alyzer can be used (?3). After describing an ef-
ficient parameter estimation procedure for the in-
flection model (?4), we employ the translate-and-
inflect model in an MT system. We describe
how we use our model to synthesize translation
options (?5) and then evaluate translation quality
on English?Russian, English?Hebrew, and English?
1677
Swahili translation tasks, finding significant im-
provements in all language pairs (?6). We finally
review related work (?7) and conclude (?8).
2 Translate-and-Inflect Model
The task of the translate-and-inflect model is illus-
trated in Fig. 1 for an English?Russian sentence pair.
The input will be a sentence e in the source language
(in this paper, always English) and any available lin-
guistic analysis of e. The output f will be composed
of (i) a sequence of stems, each denoted ? and (ii)
one morphological inflection pattern for each stem,
denoted ?. When the information is available, a
stem ? is composed of a lemma and an inflectional
class. Throughout, we use ?? to denote the set
of possible morphological inflection patterns for a
given stem ?. ?? might be defined by a grammar;
our models restrict ?? to be the set of inflections
observed anywhere in our monolingual or bilingual
training data as a realization of ?.1
We assume the availability of a deterministic
function that maps a stem ? and morphological in-
flection ? to a target language surface form f . In
some cases, such as our unsupervised approach in
?3.2, this will be a concatenation operation, though
finite-state transducers are traditionally used to de-
fine such relations (?3.1). We abstractly denote this
operation by ?: f = ? ? ?.
Our approach consists in defining a probabilistic
model over target words f . The model assumes in-
dependence between each target word f conditioned
on the source sentence e and its aligned position i in
this sentence.2 This assumption is further relaxed
in ?5 when the model is integrated in the translation
system.
We decompose the probability of generating each
target word f in the following way:
p(f | e, i) =
?
???=f
p(? | ei)
? ?? ?
gen. stem
? p(? | ?, e, i)
? ?? ?
gen. inflection
Here, each stem is generated independently from a
single aligned source word ei, but in practice we
1This prevents the model from generating words that would
be difficult for the language model to reliably score.
2This is the same assumption that Brown et al (1993) make
in, for example, IBM Model 1.
use a standard phrase-based model to generate se-
quences of stems and only the inflection model op-
erates word-by-word. We turn next to the inflection
model.
2.1 Modeling Inflection
In morphologically rich languages, each stem may
be combined with one or more inflectional mor-
phemes to express many different grammatical fea-
tures (e.g., case, definiteness, mood, tense, etc.).
Since the inflectional morphology of a word gen-
erally expresses multiple grammatical features, we
would like a model that naturally incorporates rich,
possibly overlapping features in its representation of
both the input (i.e., conditioning context) and out-
put (i.e., the inflection pattern). We therefore use
the following parametric form to model inflectional
probabilities:
u(?, e, i) = exp
[
?(e, i)>W?(?)+
?(?)>V?(?)
]
,
p(? | ?, e, i) =
u(?, e, i)
?
????? u(?
?, e, i)
. (1)
Here, ? is an m-dimensional source context fea-
ture vector function, ? is an n-dimensional mor-
phology feature vector function, W ? Rm?n and
V ? Rn?n are parameter matrices. As with the
more familiar log-linear parametrization that is writ-
ten with a single feature vector, single weight vec-
tor and single bias vector, this model is linear in its
parameters (it can be understood as working with
a feature space that is the outer product of the two
feature spaces). However, using two feature vectors
allows to define overlapping features of both the in-
put and the output, which is important for modeling
morphology in which output variables are naturally
expressed as bundles of features. The second term
in the sum in u enables correlations among output
features to be modeled independently of input, and
as such can be understood as a generalization of the
bias terms in multi-class logistic regression (on the
diagonal Vii) and interaction terms between output
variables in a conditional random field (off the diag-
onalVij).
1678
??? ???????? ???????? ???? ?? ?? ?????????
she had attempted to cross the road on her bike
PRP   VBD         VBN          TO    VB       DT     NN    IN  PRP$   NN
nsubj
aux
xcomp
?:????????_V,+,?:mis2sfm2e
C50   C473        C28          C8    C275   C37   C43  C82 C94   C331
root
-1 +1
??? ??? ???? ? ? ???? ???? ?? ?? ?????????
she had attempted to cross the road on her bike
PRP   VBD         VBN         TO   VB      DT     NN  IN  PRP$   NN
nsubj
aux
xcomp
?:????????_V,+,?:mis sfm2e
C50   C473        C28         C8    C275   C37   C43  C82 C94   C331
root
-1 +1
Figure 1: The inflection model predicts a form for the target verb lemma ? =???????? (pytat?sya) based on its
source attempted and the linear and syntactic source context. The correct inflection string for the observed Russian
form in this particular training instance is ? = mis-sfm-e (equivalent to the more traditional morphological string:
+MAIN+IND+PAST+SING+FEM+MEDIAL+PERF).
?
???
???
source aligned word ei
parent word epii with its dependency pii ? i
all children ej | pij = i with their dependency i? j
source words ei?1 and ei+1
?
???
???
?
?
?
token
part-of-speech tag
word cluster
?
?
?
? are ei, epii at the root of the dependency tree?
? number of children, siblings of ei
Figure 2: Source features ?(e, i) extracted from e and its linguistic analysis. pii denotes the parent of the token in
position i in the dependency tree and pii ? i the typed dependency link.
2.2 Source Context Features: ?(e, i)
In order to select the best inflection of a target-
language word, given the source word it translates
and the context of that source word, we seek to ex-
ploit as many features of the context as are avail-
able. Consider the example shown in Fig. 1, where
most of the inflection features of the Russian word
(past tense, singular number, and feminine gender)
can be inferred from the context of the English word
it is aligned to. Indeed, many grammatical functions
expressed morphologically in Russian are expressed
syntactically in English. Fortunately, high-quality
parsers and other linguistic analyzers are available
for English.
On the source side, we apply the following pro-
cessing steps:
? Part-of-speech tagging with a CRF tagger
trained on sections 02?21 of the Penn Tree-
bank.
? Dependency parsing with TurboParser (Mar-
tins et al, 2010), a non-projective dependency
parser trained on the Penn Treebank to produce
basic Stanford dependencies.
? Assignment of tokens to one of 600 Brown
clusters, trained on 8G words of English text.3
We then extract binary features from e using this
information, by considering the aligned source word
ei, its preceding and following words, and its syn-
tactic neighbors. These are detailed in Figure 2.
3 Morphological Grammars and Features
We now describe how to obtain morphological anal-
yses and convert them into feature vectors (?) for
our target languages, Russian, Hebrew, and Swahili,
using supervised and unsupervised methods.
3.1 Supervised Morphology
The state-of-the-art in morphological analysis uses
unweighted morphological transduction rules (usu-
3The entire monolingual data available for the translation
task of the 8th ACL Workshop on Statistical Machine Transla-
tion was used.
1679
ally in the form of an FST) to produce candidate
analyses for each word in a sentence and then sta-
tistical models to disambiguate among the analy-
ses in context (Hakkani-Tu?r et al, 2000; Hajic? et
al., 2001; Smith et al, 2005; Habash and Rambow,
2005, inter alia). While this technique is capable
of producing high quality linguistic analyses, it is
expensive to develop, requiring hand-crafted rule-
based analyzers and annotated corpora to train the
disambiguation models. As a result, such analyzers
are only available for a small number of languages,
and, as a practical matter, each analyzer (which re-
sulted from different development efforts) operates
differently from the others.
We therefore focus on using supervised analysis
for a single target language, Russian. We use the
analysis tool of Sharoff et al (2008) which produces
for each word in context a lemma and a fixed-length
morphological tag encoding the grammatical fea-
tures. We process the target side of the parallel data
with this tool to obtain the information necessary
to extract ?lemma, inflection? pairs, from which we
compute ? and morphological feature vectors ?(?).
Supervised morphology features: ?(?). Since
a positional tag set is used, it is straightforward to
convert each fixed-length tag ? into a feature vector
by defining a binary feature for each key-value pair
(e.g., Tense=past) composing the tag.
3.2 Unsupervised Morphology
Since many languages into which we might want to
translate do not have supervised morphological an-
alyzers, we now turn to the question of how to gen-
erate morphological analyses and features using an
unsupervised analyzer. We hypothesize that perfect
decomposition into rich linguistic structures may not
be required for accurate generation of new inflected
forms. We will test this hypothesis by experimenting
with a simple, unsupervised model of morphology
that segments words into sequences of morphemes,
assuming a (na??ve) concatenative generation process
and a single analysis per type.
Unsupervised morphological segmentation. We
assume that each word can be decomposed into any
number of prefixes, a stem, and any number of suf-
fixes. Formally, we let M represent the set of all
possible morphemes and define a regular grammar
M?MM? (i.e., zero or more prefixes, a stem, and
zero or more suffixes). To infer the decomposition
structure for the words in the target language, we as-
sume that the vocabulary was generated by the fol-
lowing process:
1. Sample morpheme distributions from symmet-
ric Dirichlet distributions: ?p ? Dir|M |(?p)
for prefixes, ?? ? Dir|M |(??) for stems, and
?s ? Dir|M |(?s) for suffixes.
2. Sample length distribution parameters
?p ? Beta(?p, ?p) for prefix sequences
and ?s ? Beta(?s, ?s) for suffix sequences.
3. Sample a vocabulary by creating each word
type w using the following steps:
(a) Sample affix sequence lengths:
lp ? Geometric(?p);
ls ? Geometric(?s).
(b) Sample lp prefixes p1, . . . , plp indepen-
dently from ?p; ls suffixes s1, . . . , sls in-
dependently from ?s; and a stem ? ? ??.
(c) Concatenate prefixes, the stem, and suf-
fixes: w = p1+? ? ?+plp+?+s1+? ? ?+sls .
We use blocked Gibbs sampling to sample seg-
mentations for each word in the training vocabulary.
Because of our particular choice of priors, it possible
to approximately decompose the posterior over the
arcs of a compact finite-state machine. Sampling a
segmentation or obtaining the most likely segmenta-
tion a posteriori then reduces to familiar FST opera-
tions. This model is reminiscent of work on learning
morphology using adaptor grammars (Johnson et al,
2006; Johnson, 2008).
The inferred morphological grammar is very sen-
sitive to the Dirichlet hyperparameters (?p, ?s, ??)
and these are, in turn, sensitive to the number of
types in the vocabulary. Using ?p, ?s  ??  1
tended to recover useful segmentations, but we have
not yet been able to find reliable generic priors for
these values. Therefore, we selected them empiri-
cally to obtain a stem vocabulary size on the parallel
data that is one-to-one with English.4 Future work
4Our default starting point was to use ?p = ?s =
10?6, ?? = 10?4 and then to adjust all parameters by factors
of 10.
1680
Table 1: Corpus statistics.
Parallel Parallel+Monolingual
Sentences EN-tokens TRG-tokens EN-types TRG-types Sentences TRG-tokens TRG-types
Russian 150k 3.5M 3.3M 131k 254k 20M 360M 1,971k
Hebrew 134k 2.7M 2.0M 48k 120k 806k 15M 316k
Swahili 15k 0.3M 0.3M 23k 35k 596k 13M 334k
will involve a more direct method for specifying or
inferring these values.
Unsupervised morphology features: ?(?). For
the unsupervised analyzer, we do not have a map-
ping from morphemes to structured morphological
attributes; however, we can create features from the
affix sequences obtained after morphological seg-
mentation. We produce binary features correspond-
ing to the content of each potential affixation posi-
tion relative to the stem:
prefix      suffix
...-3 -2 -1 STEM +1 +2 +3...
For example, the unsupervised analysis ? =
wa+ki+wa+STEM of the Swahili word wakiwapiga
will produce the following features:
?prefix[?3][wa](?) = 1,
?prefix[?2][ki](?) = 1,
?prefix[?1][wa](?) = 1.
4 Inflection Model Parameter Estimation
To set the parametersW andV of the inflection pre-
diction model (Eq. 1), we use stochastic gradient de-
scent to maximize the conditional log-likelihood of
a training set consisting of pairs of source (English)
sentence contextual features (?) and target word in-
flectional features (?). The training instances are
extracted from the word-aligned parallel corpus with
the English side preprocessed as discussed in ?2.2
and the target side disambiguated as discussed in ?3.
When morphological category information is avail-
able, we train an independent model for each open-
class category (in Russian, nouns, verbs, adjectives,
numerals, adverbs); otherwise a single model is used
for all words (excluding words less than four char-
acters long, which are ignored).
Statistics of the parallel corpora used to train the
inflection model are summarized in Table 1. It is
important to note here that our richly parameterized
model is trained on the full parallel training cor-
pus, not just on a handful of development sentences
(which are typically used to tune MT system param-
eters). Despite this scale, training is simple: the in-
flection model is trained to discriminate among dif-
ferent inflectional paradigms, not over all possible
target language sentences (Blunsom et al, 2008) or
learning from all observable rules (Subotin, 2011).
This makes the training problem relatively tractable:
all experiments in this paper were trained on a sin-
gle processor using a Cython implementation of the
SGD optimizer. For our largest model, trained on
3.3M Russian words, n = 231K ? m = 336 fea-
tures were produced, and 10 SGD iterations were
performed in less than 16 hours.
4.1 Intrinsic Evaluation
Before considering the broader problem of integrat-
ing the inflection model in a machine translation
system, we perform an artificial evaluation to ver-
ify that the model learns sensible source sentence-
target inflection patterns. To do so, we create an
inflection test set as follows. We preprocess the
source (English) sentences exactly as during train-
ing (?2.2), and using the target language morpholog-
ical analyzer, we convert each aligned target word to
?stem, inflection? pairs. We perform word alignment
on the held-out MT development data for each lan-
guage pair (cf. Table 1), exactly as if it were going to
produce training instances, but instead we use them
for testing.
Although the resulting dataset is noisy (e.g., due
to alignment errors), this becomes our intrinsic eval-
uation test set. Using this data, we measure inflec-
tion quality using two measurements:5
5Note that we are not evaluating the stem translation model,
1681
acc. ppl. |??|
S
up
er
vi
se
d
Russian
N 64.1% 3.46 9.16
V 63.7% 3.41 20.12
A 51.5% 6.24 19.56
M 73.0% 2.81 9.14
average 63.1% 3.98 14.49
U
ns
up
. Russian all 71.2% 2.15 4.73
Hebrew all 85.5% 1.49 2.55
Swahili all 78.2% 2.09 11.46
Table 2: Intrinsic evaluation of inflection model (N:
nouns, V: verbs, A: adjectives, M: numerals).
? the accuracy of predicting the inflection given
the source, source context and target stem, and
? the inflection model perplexity on the same set
of test instances.
Additionally, we report the average number of pos-
sible inflections for each stem, an upper bound to the
perplexity that indicates the inherent difficulty of the
task. The results of this evaluation are presented in
Table 2 for the three language pairs considered. We
remark on two patterns in these results. First, per-
plexity is substantially lower than the perplexity of a
uniform model, indicating our model is overall quite
effective at predicting inflections using source con-
text only. Second, in the supervised Russian results,
we see that predicting the inflections of adjectives
is relatively more difficult than for other parts-of-
speech. Since adjectives agree with the nouns they
modify in gender and case, and gender is an idiosyn-
cratic feature of Russian nouns (and therefore not
directly predictable from the English source), this
difficulty is unsurprising.
We can also inspect the weights learned by the
model to assess the effectiveness of the features
in relating source-context structure with target-side
morphology. Such an analysis is presented in Fig. 3.
4.2 Feature Ablation
Our inflection model makes use of numerous fea-
ture types. Table 3 explores the effect of removing
different kinds of (source) features from the model,
evaluated on predicting Russian inflections using
supervised morphological grammars.6 Rows 2?3
just the inflection prediction model.
6The models used in the feature ablation experiment were
trained on fewer examples, resulting in overall lower accuracies
show the effect of removing either linear or depen-
dency context. We see that both are necessary for
good performance; however removing dependency
context substantially degrades performance of the
model (we interpret this result as evidence that Rus-
sian morphological inflection captures grammatical
relationships that would be expressed structurally in
English). The bottom four rows explore the effect
of source language word representation. The results
indicate that lexical features are important for accu-
rate prediction of inflection, and that POS tags and
Brown clusters are likewise important, but they seem
to capture similar information (removing one has lit-
tle impact, but removing both substantially degrades
performance).
Table 3: Feature ablation experiments using supervised
Russian classification experiments.
Features (?(e, i)) acc.
all 54.7%
?linear context 52.7%
?dependency context 44.4%
?POS tags 54.5%
?Brown clusters 54.5%
?POS tags, ?Brown cl. 50.9%
?lexical items 51.2%
5 Synthetic Phrases
We turn now to translation; recall that our translate-
and-inflect model is used to augment the set of rules
available to a conventional statistical machine trans-
lation decoder. We refer to the phrases it produces
as synthetic phrases.
Our baseline system is a standard hierarchical
phrase-based translation model (Chiang, 2007). Fol-
lowing Lopez (2007), the training data is compiled
into an efficient binary representation which allows
extraction of sentence-specific grammars just before
decoding. In our case, this also allows the creation
of synthetic inflected phrases that are produced con-
ditioning on the sentence to translate.
To generate these synthetic phrases with new in-
flections possibly unseen in the parallel training
than seen in Table 2, but the pattern of results is the relevant
datapoint here.
1682
Russian supervised
Verb: 1st Person
child(nsubj)=I child(nsubj)=we
Verb: Future tense
child(aux)=MD child(aux)=will
Noun: Animate
source=animals/victims/...
Noun: Feminine gender
source=obama/economy/...
Noun: Dative case
parent(iobj)
Adjective: Genitive case
grandparent(poss)
Hebrew
Suffix ?? (masculine plural)
parent=NNS after=NNS
Prefix ? (first person sing. + future)
child(nsubj)=I child(aux)='ll
Prefix ? (preposition like/as)
child(prep)=IN parent=as
Suffix ? (possesive mark)
before=my child(poss)=my
Suffix ? (feminine mark)
child(nsubj)=she before=she
Prefix ?? (when)
before=when before=WRB
Swahili
Prefix li (past)
source=VBD source=VBN
Prefix nita (1st person sing. + future)
child(aux) child(nsubj)=I
Prefix ana (3rd person sing. + present)
source=VBZ
Prefix wa (3rd person plural)
before=they child(nsubj)=NNS
Suffix tu (1st person plural)
child(nsubj)=she before=she
Prefix ha (negative tense)
source=no after=not
Figure 3: Examples of highly weighted features learned by the inflection model. We selected a few frequent morpho-
logical features and show their top corresponding source context features.
data, we first construct an additional phrase-based
translation model on the parallel corpus prepro-
cessed to replace inflected surface words with their
stems. We then extract a set of non-gappy phrases
for each sentence (e.g., X ? <attempted,
???????? V>). The target side of each such phrase
is re-inflected, conditioned on the source sentence,
using the inflection model from ?2. Each stem is
given its most likely inflection.7
The original features extracted for the stemmed
phrase are conserved, and the following features
are added to help the decoder select good synthetic
phrases:
? a binary feature indicating that the phrase is
synthetic,
? the log-probability of the inflected forms ac-
cording to our model,
? the count of words that have been inflected,
with a separate feature for each morphological
category in the supervised case.
Finally, these synthetic phrases are combined with
the original translation rules obtained for the base-
line system to produce an extended sentence-specific
grammar which is used as input to the decoder. If a
7Several reviewers asked about what happens when k-best
inflections are added. The results for k ? {2, 4, 8} range from
no effect to an improvement over k = 1 of about 0.2 BLEU
(absolute). We hypothesize that larger values of k could have a
greater impact, perhaps in a more ?global? model of the target
string; however, exploration of this question is beyond the scope
of this paper.
phrase already existing in the standard phrase table
happens to be recreated, both phrases are kept and
will compete with each other with different features
in the decoder.
For example, for the large EN?RU system, 6%
of all the rules used for translation are synthetic
phrases, with 65% of these phrases being entirely
new rules.
6 Translation Experiments
We evaluate our approach in the standard discrim-
inative MT framework. We use cdec (Dyer et al,
2010) as our decoder and perform MIRA training
to learn feature weights of the sentence translation
model (Chiang, 2012). We compare the following
configurations:
? A baseline system, using a 4-gram language
model trained on the entire monolingual and
bilingual data available.
? An enriched system with a class-based n-gram
language model8 trained on the monolingual
data mapped to 600 Brown clusters. Class-
based language modeling is a strong baseline
for scenarios with high out-of-vocabulary rates
but in which large amounts of monolingual
target-language data are available.
? The enriched system further augmented with
our inflected synthetic phrases. We expect the
class-based language model to be especially
8For Swahili and Hebrew, n = 6; for Russian, n = 7.
1683
helpful here and capture some basic agreement
patterns that can be learned more easily on
dense clusters than from plain word sequences.
Detailed corpus statistics are given in Table 1:
? The Russian data consist of the News Com-
mentary parallel corpus and additional mono-
lingual data crawled from news websites.9
? The Hebrew parallel corpus is composed of
transcribed TED talks (Cettolo et al, 2012).
Additional monolingual news data is also used.
? The Swahili parallel corpus was obtained by
crawling the Global Voices project website10
for parallel articles. Additional monolingual
data was taken from the Helsinki Corpus of
Swahili.11
We evaluate translation quality by translating and
measuring the BLEU score of a 2000?3000 sentence-
long evaluation corpus, averaging the results over 3
MIRA runs to control for optimizer instability (Clark
et al, 2011). Table 4 reports the results. For all lan-
guages, using class language models improves over
the baseline. When synthetic phrases are added, sig-
nificant additional improvements are obtained. For
the English?Russian language pair, where both su-
pervised and unsupervised analyses can be obtained,
we notice that expert-crafted morphological analyz-
ers are more efficient at improving translation qual-
ity. Globally, the amount of improvement observed
varies depending on the language; this is most likely
indicative of the quality of unsupervised morpholog-
ical segmentations produced and the kinds of gram-
matical relations expressed morphologically.
Finally, to confirm the effectiveness of our ap-
proach as corpus size increases, we use our tech-
nique on top of a state-of-the art English?Russian
system trained on data from the 8th ACL Work-
shop on Machine Translation (30M words of bilin-
gual text and 410M words of monolingual text). The
setup is identical except for the addition of sparse
9http://www.statmt.org/wmt13/
translation-task.html
10http://sw.globalvoicesonline.org
11http://www.aakkl.helsinki.fi/cameel/
corpus/intro.htm
Table 4: Translation quality (measured by BLEU) aver-
aged over 3 MIRA runs.
EN?RU EN?HE EN?SW
Baseline 14.7?0.1 15.8?0.3 18.3?0.1
+Class LM 15.7?0.1 16.8?0.4 18.7?0.2
+Synthetic
unsupervised 16.2?0.1 17.6?0.1 19.0?0.1
supervised 16.7?0.1 ? ?
rule shape indicator features and bigram cluster fea-
tures. In these large scale conditions, the BLEU score
improves from 18.8 to 19.6 with the addition of word
clusters and reaches 20.0 with synthetic phrases.
Details regarding this system are reported in Ammar
et al (2013).
7 Related Work
Translation into morphologically rich languages is
a widely studied problem and there is a tremen-
dous amount of related work. Our technique of syn-
thesizing translation options to improve generation
of inflected forms is closely related to the factored
translation approach proposed by Koehn and Hoang
(2007); however, an important difference to that
work is that we use a discriminative model that con-
ditions on source context to make ?local? decisions
about what inflections may be used before combin-
ing the phrases into a complete sentence translation.
Combination pre-/post-processing solutions are
also frequently proposed. In these, the tar-
get language is generally transformed from multi-
morphemic surface words into smaller units more
amenable to direct translation, and then a post-
processing step is applied independent of the trans-
lation model. For example, Oflazer and El-Kahlout
(2007) experiment with partial morpheme groupings
to produce novel inflected forms when translating
into Turkish; Al-Haj and Lavie (2010) compare dif-
ferent processing schemes for Arabic. A related but
different approach is to enrich the source language
items with grammatical features (e.g., a source sen-
tence like John saw Mary is preprocessed into, e.g.,
John+subj saw+msubj+fobj Mary+obj) so as
to make the source and target lexicons have simi-
lar morphological contrasts (Avramidis and Koehn,
2008; Yeniterzi and Oflazer, 2010; Chang et al,
1684
2009). In general, this work suffers from the prob-
lem that it is extremely difficult to know a priori
what the right preprocessing is for a given language
pair, data size, and domain.
Several post-processing approaches have relied
on supervised classifiers to predict the optimal com-
plete inflection for an incomplete or lemmatized
translation. Minkov et al (2007) present a method
for predicting the inflection of Russian and Arabic
sentences aligned to English sentences. They train a
sequence model to predict target morphological fea-
tures from the lemmas and the syntactic structures
of both aligned sentences and demonstrate its ability
to recover accurately inflections on reference trans-
lations. Toutanova et al (2008) apply this method
to generate inflections after translation in two differ-
ent ways: by rescoring inflected n-best outputs or by
translating lemmas and re-inflecting them a posteri-
ori. El Kholy and Habash (2012) follow a similar
method and compare different approaches for gen-
erating rich morphology in Arabic after a transla-
tion step. Fraser et al (2012) observe improvements
for translation into German with a similar method.
As in that work, we model morphological features
rather than directly inflected forms. However, that
work may be criticized for providing no mechanism
to translate surface forms directly, even when evi-
dence for a direct translation is available in the par-
allel data.
Unsupervised morphology has begun to play a
role in translation between morphologically com-
plex languages. Stallard et al (2012) show that an
unsupervised approach to Arabic segmentation per-
forms as well as a supervised segmenter for source-
side preprocessing (in terms of English translation
quality). For translation into morphological rich lan-
guages, Clifton and Sarkar (2011) use an unsuper-
vised morphological analyzer to produce morpho-
logical affixes in Finnish, injecting some linguistic
knowledge in the generation process.
Several authors have proposed using conditional
models to predict the probability of phrase transla-
tion in context (Gimpel and Smith, 2008; Chan et
al., 2007; Carpuat and Wu, 2007; Jeong et al, 2010).
Of particular note is the work of Subotin (2011),
who use a conditional model to predict morpholog-
ical features conditioned on rich linguistic features;
however, this latter work also conditions on target
context, which substantially complicates decoding.
Finally, synthetic phrases have been used for
different purposes than generating morphology.
Callison-Burch et al (2006) expanded the cov-
erage of a phrase table by adding synthesized
phrases by paraphrasing source language phrases,
Chen et al (2011) produced ?fabricated? phrases
by paraphrasing both source and target phrases, and
Habash (2009) created new rules to handle out-of-
vocabulary words. In related work, Tsvetkov et al
(2013) used synthetic phrases to improve generation
of (in)definite articles when translating into English
from Russian and Czech, two languages which do
not lexically mark definiteness.
8 Conclusion
We have presented an efficient technique that ex-
ploits morphologically analyzed corpora to produce
new inflections possibly unseen in the bilingual
training data. Our method decomposes into two
simple independent steps involving well-understood
discriminative models.
By relying on source-side context to generate ad-
ditional local translation options and by leaving the
choice of the full sentence translation to the decoder,
we sidestep the difficulty of computing features on
target translations hypotheses. However, many mor-
phological processes (most notably, agreement) are
most best modeled using target language context. To
capture target context effects, we depend on strong
target language models. Therefore, an important
extension of our work is to explore the interaction
of our approach with more sophisticated language
models that more directly model morphology, e.g.,
the models of Bilmes and Kirchhoff (2003), or, alter-
natively, ways to incorporate target language context
in the inflection model.
We also achieve language independence by
exploiting unsupervised morphological segmen-
tations in the absence of linguistically informed
morphological analyses.
Code for replicating the experiments is available from
https://github.com/eschling/morphogen;
further details are available in (Schlinger et al, 2013).
1685
Acknowledgments
This work was supported by the U. S. Army Research
Laboratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533. We would
like to thank Kim Spasaro for curating the Swahili devel-
opment and test sets, Yulia Tsvetkov for assistance with
Russian, and the anonymous reviewers for their helpful
comments.
References
Hassan Al-Haj and Alon Lavie. 2010. The im-
pact of Arabic morphological segmentation on broad-
coverage English-to-Arabic statistical machine trans-
lation. In Proc. of AMTA.
Waleed Ammar, Victor Chahuneau, Michael Denkowski,
Greg Hanneman, Wang Ling, Austin Matthews, Ken-
ton Murray, Nicola Segall, Yulia Tsvetkov, Alon
Lavie, and Chris Dyer. 2013. The CMU machine
translation systems at WMT 2013: Syntax, synthetic
translation options, and pseudo-references. In Proc. of
WMT.
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proc. of ACL.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
Proc. of NAACL.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter es-
timation. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Improved statistical machine transla-
tion using paraphrases. In Proc. of NAACL.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proc. of EMNLP.
Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012. WIT3: Web inventory of transcribed and trans-
lated talks. In Proc. of EAMT.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL.
Pi-Chuan Chang, Dan Jurafsky, and Christopher D. Man-
ning. 2009. Disambiguating ?DE? for Chinese?
English machine translation. In Proc. of WMT.
Boxing Chen, Roland Kuhn, and George Foster. 2011.
Semantic smoothing and fabrication of phrase pairs for
SMT. In Proc. of IWSLT.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2012. Hope and fear for discrimina-
tive training of statistical translation models. JMLR,
13:1159?1187.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proc. of ACL.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proc. of ACL.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proc. of
ACL.
Ahmed El Kholy and Nizar Habash. 2012. Translate,
predict or generate: Modeling rich morphology in sta-
tistical machine translation. In Proc. of EAMT.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling inflection and word-
formation in SMT. In Proc. of EACL.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In
Proc. of WMT.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proc. of ACL.
Nizar Habash. 2009. REMOOV: A tool for online han-
dling of out-of-vocabulary words in machine transla-
tion. In Proceedings of the 2nd International Confer-
ence on Arabic Language Resources and Tools.
Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva, and
Vladim??r Petkevic?. 2001. Serial combination of rules
and statistics: A case study in Czech tagging. In Proc.
of ACL.
Dilek Z. Hakkani-Tu?r, Kemal Oflazer, and Go?khan Tu?r.
2000. Statistical morphological disambiguation for
agglutinative languages. In Proc. of COLING.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki, and
Chris Quirk. 2010. A discriminative lexicon model
for complex morphology. In Proc. of AMTA.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2006. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
NIPS, pages 641?648.
Mark Johnson. 2008. Unsupervised word segmentation
for Sesotho using adaptor grammars. In Proc. SIG-
MORPHON.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proc. of EMNLP.
1686
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In Proc. of EMNLP.
Andre? F.T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M.Q. Aguiar, and Ma?rio A.T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proc. of EMNLP.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proc. of ACL.
Kemal Oflazer and I?lknur Durgar El-Kahlout. 2007. Ex-
ploring different representational units in English-to-
Turkish statistical machine translation. In Proc. of
WMT.
Eva Schlinger, Victor Chahuneau, and Chris Dyer. 2013.
morphogen: Translation into morphologically rich lan-
guages with synthetic phrases. Prague Bulletin of
Mathematical Linguistics, (100).
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating a Russian tagset. In Proc. of LREC.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proc. of EMNLP.
David Stallard, Jacob Devlin, Michael Kayser,
Yoong Keok Lee, and Regina Barzilay. 2012.
Unsupervised morphology rivals supervised morphol-
ogy for Arabic MT. In Proc. of ACL.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Proc. ACL.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proc. of ACL.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna Bha-
tia. 2013. Generating English determiners in phrase-
based translation with synthetic translation options. In
Proc. of WMT.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based statis-
tical machine translation from English to Turkish. In
Proc. of ACL.
1687
Proceedings of NAACL-HLT 2013, pages 644?648,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Simple, Fast, and Effective Reparameterization of IBM Model 2
Chris Dyer Victor Chahuneau Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{cdyer,vchahune,nasmith}@cs.cmu.edu
Abstract
We present a simple log-linear reparame-
terization of IBM Model 2 that overcomes
problems arising from Model 1?s strong
assumptions and Model 2?s overparame-
terization. Efficient inference, likelihood
evaluation, and parameter estimation algo-
rithms are provided. Training the model is
consistently ten times faster than Model 4.
On three large-scale translation tasks, systems
built using our alignment model outperform
IBM Model 4.
An open-source implementation of the align-
ment model described in this paper is available
from http://github.com/clab/fast align .
1 Introduction
Word alignment is a fundamental problem in statis-
tical machine translation. While the search for more
sophisticated models that provide more nuanced ex-
planations of parallel corpora is a key research activ-
ity, simple and effective models that scale well are
also important. These play a crucial role in many
scenarios such as parallel data mining and rapid
large scale experimentation, and as subcomponents
of other models or training and inference algorithms.
For these reasons, IBM Models 1 and 2, which sup-
port exact inference in time ?(|f| ? |e|), continue to
be widely used.
This paper argues that both of these models are
suboptimal, even in the space of models that per-
mit such computationally cheap inference. Model
1 assumes all alignment structures are uniformly
likely (a problematic assumption, particularly for
frequent word types), and Model 2 is vastly overpa-
rameterized, making it prone to degenerate behav-
ior on account of overfitting.1 We present a simple
log-linear reparameterization of Model 2 that avoids
both problems (?2). While inference in log-linear
models is generally computationally more expen-
sive than in their multinomial counterparts, we show
how the quantities needed for alignment inference,
likelihood evaluation, and parameter estimation us-
ing EM and related methods can be computed using
two simple algebraic identities (?3), thereby defus-
ing this objection. We provide results showing our
model is an order of magnitude faster to train than
Model 4, that it requires no staged initialization, and
that it produces alignments that lead to significantly
better translation quality on downstream translation
tasks (?4).
2 Model
Our model is a variation of the lexical translation
models proposed by Brown et al (1993). Lexical
translation works as follows. Given a source sen-
tence f with length n, first generate the length of
the target sentence, m. Next, generate an alignment,
a = ?a1, a2, . . . , am?, that indicates which source
word (or null token) each target word will be a trans-
lation of. Last, generate the m output words, where
each ei depends only on fai .
The model of alignment configurations we pro-
pose is a log-linear reparameterization of Model 2.
1Model 2 has independent parameters for every alignment
position, conditioned on the source length, target length, and
current target index.
644
Given : f, n = |f|, m = |e|, p0, ?, ?
h(i, j,m, n) = ?
?
?
?
?
i
m ?
j
n
?
?
?
?
?(ai = j | i,m, n) =
?
??
??
p0 j = 0
(1? p0)? e
?h(i,j,m,n)
Z?(i,m,n)
0 < j ? n
0 otherwise
ai | i,m, n ? ?(? | i,m, n) 1 ? i ? m
ei | ai, fai ? ?(? | fai) 1 ? i ? m
null
j? = 1
j? = 2
j? = 3
j? = 4
j? = 5
i =
3
}
n
=
5
}
m = 6
i =
1
i =
2
i =
4
i =
5
i =
6
j?
j?
Figure 1: Our proposed generative process yielding a translation e and its alignment a to a source sentence f, given the
source sentence f, alignment parameters p0 and ?, and lexical translation probabilities ? (left); an example visualization
of the distribution of alignment probability mass under this model (right).
Our formulation, which we write as ?(ai = j |
i,m, n), is shown in Fig. 1.2 The distribution over
alignments is parameterized by a null alignment
probability p0 and a precision ? ? 0 which con-
trols how strongly the model favors alignment points
close to the diagonal. In the limiting case as ?? 0,
the distribution approaches that of Model 1, and, as
it gets larger, the model is less and less likely to de-
viate from a perfectly diagonal alignment. The right
side of Fig. 1 shows a graphical illustration of the
alignment distribution in which darker squares indi-
cate higher probability.
3 Inference
We now discuss two inference problems and give ef-
ficient techniques for solving them. First, given a
sentence pair and parameters, compute the marginal
likelihood and the marginal alignment probabilities.
Second, given a corpus of training data, estimate
likelihood maximizing model parameters using EM.
3.1 Marginals
Under our model, the marginal likelihood of a sen-
tence pair ?f, e? can be computed exactly in time
2Vogel et al (1996) hint at a similar reparameterization of
Model 2; however, its likelihood and its gradient are not effi-
cient to evaluate, making it impractical to train and use. Och
and Ney (2003) likewise remark on the overparameterization
issue, removing a single variable of the original conditioning
context, which only slightly improves matters.
?(|f| ? |e|). This can be seen as follows. For
each position in the sentence being generated, i ?
[1, 2, . . . ,m], the alignment to the source and its
translation is independent of all other translation and
alignment decisions. Thus, the probability that the
ith word of e is ei can be computed as:
p(ei, ai | f,m, n) = ?(ai | i,m, n)? ?(ei | fai)
p(ei | f,m, n) =
n?
j=0
p(ei, ai = j | f,m, n).
We can also compute the posterior probability over
alignments using the above probabilities,
p(ai | ei, f,m, n) =
p(ei, ai | f,m, n)
p(ei | f,m, n)
. (1)
Finally, since all words in e (and their alignments)
are conditionally independent,3
p(e | f) =
m?
i=1
p(ei | f,m, n)
=
m?
i=1
n?
j=0
?(ai | i,m, n)? ?(ei | fai).
3We note here that Brown et al (1993) derive their variant
of this expression by starting with the joint probability of an
alignment and translation, marginalizing, and then reorganizing
common terms. While identical in implication, we find the di-
rect probabilistic argument far more intuitive.
645
3.2 Efficient Partition Function Evaluation
Evaluating and maximizing the data likelihood un-
der log-linear models can be computationally ex-
pensive since this requires evaluation of normalizing
partition functions. In our case,
Z?(i,m, n) =
n?
j?=1
exp?h(i, j?,m, n).
While computing this sum is obviously possible
in ?(|f|) operations, our formulation permits exact
computation in ?(1), meaning our model can be ap-
plied even in applications where computational ef-
ficiency is paramount (e.g., MCMC simulations).
The key insight is that the partition function is the
(partial) sum of two geometric series of unnormal-
ized probabilities that extend up and down from the
probability-maximizing diagonal. The closest point
on or above the diagonal j?, and the next point down
j? (see the right side of Fig. 1 for an illustration), is
computed as follows:
j? =
? i? n
m
?
, j? = j? + 1.
Starting at j? and moving up the alignment col-
umn, as well as starting at j? and moving down, the
unnormalized probabilities decrease by a factor of
r = exp ??n per step.
To compute the value of the partition, we only
need to evaluate the unnormalized probabilities at
j? and j? and then use the following identity, which
gives the sum of the first ` terms of a geometric se-
ries (Courant and Robbins, 1996):
s`(g1, r) =
?`
k=1
g1rk?1 = g1
1? r`
1? r .
Using this identity, Z?(i,m, n) can be computed as
sj?(e?h(i,j?,m,n), r) + sn?j?(e?h(i,j?,m,n), r).
3.3 Parameter Optimization
To optimize the likelihood of a sample of parallel
data under our model, one can use EM. In the E-step,
the posterior probabilities over alignments are com-
puted using Eq. 1. In the M-step, the lexical trans-
lation probabilities are updated by aggregating these
as counts and normalizing (Brown et al, 1993). In
the experiments reported in this paper, we make the
further assumption that ?f ? Dirichlet(?) where
?i = 0.01 and approximate the posterior distribu-
tion over the ?f ?s using a mean-field approximation
(Riley and Gildea, 2012).4
During the M-step, the ? parameter must also
be updated to make the E-step posterior distribu-
tion over alignment points maximally probable un-
der ?(? | i,m, n). This maximizing value cannot
be computed analytically, but a gradient-based op-
timization can be used, where the first derivative
(here, for a single target word) is:
??L = Ep(ai|ei,f,m,n) [h(i, ai,m, n)]
? E?(j?|i,m,n)
[
h(i, j?,m, n)
]
(2)
The first term in this expression (the expected value
of h under the E-step posterior) is fixed for the du-
ration of each M-step, but the second term?s value
(the derivative of the log-partition function) changes
many times as ? is optimized.
3.4 Efficient Gradient Evaluation
Fortunately, like the partition function, the deriva-
tive of the log-partition function (i.e., the second
term in Eq. 2) can be computed in constant time us-
ing an algebraic identity. To derive this, we observe
that the values of h(i, j?,m, n) form an arithmetic
sequence about the diagonal, with common differ-
ence d = ?1/n. Thus, the quantity we seek is the
sum of a series whose elements are the products of
terms from an arithmetic sequence and those of the
geometric sequence above, divided by the partition
function value. This construction is referred to as
an arithmetico-geometric series, and its sum may be
computed as follows (Fernandez et al, 2006):
t`(g1,a1, r, d) =
?`
k=1
[a1 + d(k ? 1)] g1rk?1
= a`g`+1 ? a1g11? r +
d (g`+1 ? g1r)
(1? r)2 .
In this expression r, the g1?s and the `?s have the
same values as above, d = ?1/n and the a1?s are
4The ?i value was fixed at the beginning of experimentation
by minimizing the AER on the 10k sentence French-English cor-
pus discussed below.
646
equal to the value of h evaluated at the starting in-
dices, j? and j?; thus, the derivative we seek at each
optimization iteration inside the M-step is:
??L =Ep(ai|ei,f,m,n) [h(i, ai,m, n)]
? 1Z?
(tj?(e?h(i,j?,m,n), h(i, j?,m, n), r, d)
+ tn?j?(e?h(i,j?,m,n), h(i, j?,m, n), r, d)).
4 Experiments
In this section we evaluate the performance of
our proposed model empirically. Experiments are
conducted on three datasets representing different
language typologies and dataset sizes: the FBIS
Chinese-English corpus (LDC2003E14); a French-
English corpus consisting of version 7 of the Eu-
roparl and news-commentary corpora;5 and a large
Arabic-English corpus consisting of all parallel data
made available for the NIST 2012 Open MT evalua-
tion. Table 1 gives token counts.
We begin with several preliminary results. First,
we quantify the benefit of using the geometric series
trick (?3.2) for computing the partition function rel-
ative to na??ve summation. Our method requires only
0.62 seconds to compute all partition function values
for 0 < i,m, n < 150, whereas the na??ve algorithm
requires 6.49 seconds for the same.6
Second, using a 10k sample of the French-English
data set (only 0.5% of the corpus), we determined
1) whether p0 should be optimized; 2) what the op-
timal Dirichlet parameters ?i are; and 3) whether
the commonly used ?staged initialization? procedure
(in which Model 1 parameters are used to initialize
Model 2, etc.) is necessary for our model. First,
like Och and Ney (2003) who explored this issue for
training Model 3, we found that EM tended to find
poor values for p0, producing alignments that were
overly sparse. By fixing the value at p0 = 0.08,
we obtained minimal AER. Second, like Riley and
Gildea (2012), we found that small values of ? im-
proved the alignment error rate, although the im-
pact was not particularly strong over large ranges of
5http://www.statmt.org/wmt12
6While this computational effort is a small relative to the
total cost in EM training, in algorithms where ? changes more
rapidly, for example in Bayesian posterior inference with Monte
Carlo methods (Chahuneau et al, 2013), this savings can have
substantial value.
Table 1: CPU time (hours) required to train alignment
models in one direction.
Language Pair Tokens Model 4 Log-linear
Chinese-English 17.6M 2.7 0.2
French-English 117M 17.2 1.7
Arabic-English 368M 63.2 6.0
Table 2: Alignment quality (AER) on the WMT 2012
French-English and FBIS Chinese-English. Rows with
EM use expectation maximization to estimate the ?f , and
?Dir use variational Bayes.
Model Estimator FR-EN ZH-EN
Model 1 EM 29.0 56.2
Model 1 ?Dir 26.6 53.6
Model 2 EM 21.4 53.3
Log-linear EM 18.5 46.5
Log-linear ?Dir 16.6 44.1
Model 4 EM 10.4 45.8
Table 3: Translation quality (BLEU) as a function of
alignment type.
Language Pair Model 4 Log-linear
Chinese-English 34.1 34.7
French-English 27.4 27.7
Arabic-English 54.5 55.7
?. Finally, we (perhaps surprisingly) found that the
standard staged initialization procedure was less ef-
fective in terms of AER than simply initializing our
model with uniform translation probabilities and a
small value of ? and running EM. Based on these
observations, we fixed p0 = 0.08, ?i = 0.01, and
set the initial value of ? to 4 for the remaining ex-
periments.7
We next compare the alignments produced by our
model to the Giza++ implementation of the standard
IBM models using the default training procedure
and parameters reported in Och and Ney (2003).
Our model is trained for 5 iterations using the pro-
cedure described above (?3.3). The algorithms are
7As an anonymous reviewer pointed out, it is a near certainty
that tuning of these hyperparameters for each alignment task
would improve results; however, optimizing hyperparameters of
alignment models is quite expensive. Our intention is to show
that it is possible to obtain reasonable (if not optimal) results
without careful tuning.
647
compared in terms of (1) time required for training;
(2) alignment error rate (AER, lower is better);8 and
(3) translation quality (BLEU, higher is better) of hi-
erarchical phrase-based translation system that used
the alignments (Chiang, 2007). Table 1 shows the
CPU time in hours required for training (one direc-
tion, English is generated). Our model is at least
10? faster to train than Model 4. Table 3 reports
the differences in BLEU on a held-out test set. Our
model?s alignments lead to consistently better scores
than Model 4?s do.9
5 Conclusion
We have presented a fast and effective reparameteri-
zation of IBM Model 2 that is a compelling replace-
ment for for the standard Model 4. Although the
alignment quality results measured in terms of AER
are mixed, the alignments were shown to work ex-
ceptionally well in downstream translation systems
on a variety of language pairs.
Acknowledgments
This work was sponsored by the U. S. Army Research
Laboratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533.
References
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
V. Chahuneau, N. A. Smith, and C. Dyer. 2013.
Knowledge-rich morphological priors for Bayesian
language models. In Proc. NAACL.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
R. Courant and H. Robbins. 1996. The geometric pro-
gression. In What Is Mathematics?: An Elementary
Approach to Ideas and Methods, pages 13?14. Oxford
University Press.
8Our Arabic training data was preprocessed using a seg-
mentation scheme optimized for translation (Habash and Sadat,
2006). Unfortunately the existing Arabic manual alignments
are preprocessed quite differently, so we did not evaluate AER.
9The alignments produced by our model were generally
sparser than the corresponding Model 4 alignments; however,
the extracted grammar sizes were sometimes smaller and some-
times larger, depending on the language pair.
P. A. Fernandez, T. Foregger, and J. Pahikkala. 2006.
Arithmetic-geometric series. PlanetMath.org.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Proc. of
NAACL.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
D. Riley and D. Gildea. 2012. Improving the IBM align-
ment models using Variational Bayes. In Proc. of ACL.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. of
COLING.
648
Proceedings of NAACL-HLT 2013, pages 1206?1215,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Knowledge-Rich Morphological Priors for Bayesian Language Models
Victor Chahuneau Noah A. Smith Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{vchahune,nasmith,cdyer}@cs.cmu.edu
Abstract
We present a morphology-aware nonparamet-
ric Bayesian model of language whose prior
distribution uses manually constructed finite-
state transducers to capture the word forma-
tion processes of particular languages. This
relaxes the word independence assumption
and enables sharing of statistical strength
across, for example, stems or inflectional
paradigms in different contexts. Our model
can be used in virtually any scenario where
multinomial distributions over words would
be used. We obtain state-of-the-art results in
language modeling, word alignment, and un-
supervised morphological disambiguation for
a variety of morphologically rich languages.
1 Introduction
Despite morphological phenomena?s salience in
most human languages, many NLP systems treat
fully inflected forms as the atomic units of language.
By assuming independence of lexical stems? vari-
ous surface forms, this avoidance approach exacer-
bates the problem of data sparseness. If it is em-
ployed at all, morphological analysis of text tends
to be treated as a preprocessing step to other NLP
modules. While this latter disambiguation approach
helps address data sparsity concerns, it has substan-
tial drawbacks: it requires supervised learning from
expert-annotated corpora, and determining the op-
timal morphological granularity is labor-intensive
(Habash and Sadat, 2006).
Neither approach fully exploits the finite-state
transducer (FST) technology that has been so suc-
cessful for modeling the mapping between surface
forms and their morphological analyses (Karttunen
and Beesley, 2005), and the mature collections of
high quality transducers that already exist for many
languages (e.g., Turkish, Russian, Arabic). Much
linguistic knowledge is encoded in such FSTs.
In this paper, we develop morphology-aware non-
parametric Bayesian language models that bring to-
gether hand-written FSTs with statistical modeling
and require no token-level annotation. The sparsity
issue discussed above is addressed by hierarchical
priors that share statistical strength across different
inflections of the same stem by backing off to word
formation models that piece together morphemes us-
ing FSTs. Furthermore, because of the nonparamet-
ric formulation of our models, the regular morpho-
logical patterns found in the long tail of word types
will rely more heavily on deeper analysis, while fre-
quent and idiosyncratically behaved forms are mod-
eled opaquely.
Our prior can be used in virtually any generative
model of language as a replacement for multino-
mial distributions over words, bringing morphologi-
cal awareness to numerous applications. For various
morphologically rich languages, we show that:
? our model can provide rudimentary unsuper-
vised disambiguation for a highly ambiguous
analyzer;
? integrating morphology into n-gram language
models allows better generalization to unseen
words and can improve the performance of ap-
plications that are truly open vocabulary; and
? bilingual word alignment models also bene-
fit greatly from sharing translation information
1206
across stems.
We are particularly interested in low-resource sce-
narios, where one has to make the most of the
small quantity of available data, and overcoming
data sparseness is crucial. If analyzers exist in such
settings, they tend to be highly ambiguous, and an-
notated data for learning to disambiguate are also
likely to be scarce or non-existent. Therefore, in our
experiments with Russian, we compare two analyz-
ers: a rapidly-developed guesser, which models reg-
ular inflectional paradigms but contains no lexicon
or irregular forms, and a high-quality analyzer.
2 Word Models with Morphology
In this section, we describe a generative model of
word formation based on Pitman-Yor processes that
generates word types using a finite-state morpho-
logical generator. At a high level, the process first
produces lexicons of stems and inflectional patterns;
then it generates a lexicon of inflected forms us-
ing the finite-state generator. Finally, the inflected
forms are used to generate observed data. Different
independence assumptions can be made at each of
these levels to encode beliefs about where stems, in-
flections, and surface forms should share statistical
strength.
2.1 Pitman-Yor Processes
Our work relies extensively on Pitman-Yor pro-
cesses, which provide a flexible framework for ex-
pressing backoff and interpolation relationships and
extending standard models with richer word distri-
butions (Pitman and Yor, 1997). They have been
shown to match the performance of state-of-the-art
language models and to give estimates that follow
appropriate power laws (Teh, 2006).
A draw from a Pitman-Yor process (PYP), de-
noted G ? PY(d, ?,G0), is a discrete distribution
over a (possibly infinite) set of events, which we de-
note abstractly E . The process is parameterized by a
discount parameter 0 ? d < 1, a strength parameter
? > ?d, and a base distribution G0 over the event
space E .
In this work, our focus is on the base distribution
G0. We place vague priors on the hyperparameters
d ? U([0, 1]) and (? + d) ? Gamma(1, 1). Infer-
ence in PYPs is discussed below.
2.2 Unigram Morphology Model
The most basic expression of our model is a uni-
gram model of text. So far, we only assume that
each word can be analyzed into a stem and a se-
quence of morphemes forming an inflection pattern.
LetGs be a distribution over stems,Gp be a distribu-
tion over inflectional patterns, and let GENERATE be
a deterministic mapping from ?stem, pattern? pairs
to inflected word forms.1 An inflected word type is
generated with the following process, which we des-
ignate MP(Gs, Gd,GENERATE):
stem ? Gs
pattern ? Gp
word = GENERATE(stem, pattern)
For example, in Russian, we might sample stem
= ??????,2 pattern = STEM+Adj+Pl+Dat, and
obtain word = ??????.
This model could be used directly to generate ob-
served tokens. However, we have said nothing about
Gs and Gp, and the assumption that stems and pat-
terns are independent is clearly unsatisfying. We
therefore assume that both the stem and the pattern
distributions are generated from PY processes, and
that MP(Gs, Gp,GENERATE) is itself the base dis-
tribution of a PYP.
Gs ? PY(ds, ?s, G0s)
Gp ? PY(dp, ?p, G0p)
Gw ? PY(d, ?,MP(Gs, Gp,GENERATE))
A draw Gw from this PYP is a unigram distribu-
tion over tokens.
2.3 Base Stem Model G0s
In general there are an unbounded number of stems
possible in any language, so we set G0s to be charac-
ter trigram model, which we statically estimate, with
Kneser-Ney smoothing, from a large corpus of word
types in the language being modeled. While using
fixed parameters estimated to maximize likelihood is
1The assumption of determinism is only inappropriate in
cases of inflectional spelling variants (e.g., modeled vs. mod-
elled) or pronunciation variants (e.g., reduced forms in certain
environments).
2?????? (pronounced [pr5tCij]) = other
1207
questionable from the perspective of Bayesian learn-
ing, it is tremendously beneficial for computational
reasons. For some applications (e.g., word align-
ment), the set of possible stems for a corpus S can be
precomputed, so we will also experiment with using
a uniform stem distribution based on this set.
2.4 Base Pattern Model G0p
Several choices are possible for the base pattern dis-
tribution:
MP0 We can assume a uniformG0p when the num-
ber of patterns is small.
MP1 To be able to generalize to new patterns, we
can draw the length of the pattern from a Poisson
distribution and generate morphemes one by one
from a uniform distribution.
MP2 A more informative prior is a Markov chain
of morphemes, where each morpheme is generated
conditional on the preceding morpheme.
The choice of the base pattern distribution could
depend on the complexity of the inflectional patterns
produced by the morphological analyzer, reflecting
the type of morphological phenomena present in a
given language. For example, the number of possi-
ble patterns can practically be considered finite in
Russian, but this assumption is not valid for lan-
guages with more extensive derivational morphol-
ogy like Turkish.
2.5 Posterior Inference
For most applications, rather than directly gener-
ating from a model using the processes outlined
above, we seek to infer posterior distributions over
latent parameters and structures, given a sample of
data.
Although there is no known analytic form of
the PYP density, it is possible to marginalize the
draws from it and to work directly with observa-
tions. This marginalization produces the classi-
cal Chinese restaurant process representation (Teh,
2006). When working with the morphology mod-
els we are proposing, we also need to marginalize
the different latent forms (stems s and patterns p)
that may have given rise to a given word w. Thus,
we require that the inverse relation of GENERATE is
available to compute the marginal base word distri-
bution:
p(w | G0w) =
?
GENERATE(s,p)=w
p(s | Gs) p(p | Gp)
Since our approach encodes morphology using
FSTs, which are invertible, this poses no problem.
To illustrate, consider the Russian word ??????,
which may be analyzed in several ways:
?????? +Adj +Sg +Neut +Instr
?????? +Adj +Sg +Masc +Instr
?????? +Adj +Pl +Dat
??????? +Verb +Pl +1P
?????? +Pro +Sg +Ins
Because the set of possible analyses is in general
small, marginalization is fast and complex blocked
sampling is not necessary.
Finally, to infer hyperparameter values (d, ?, . . .),
a Metropolis-Hastings update is interleaved with
Gibbs sampling steps for the rest of the hidden vari-
ables.3
Having described a model for generating words,
we now show its usage in several contexts.
3 Unsupervised Morphological
Disambiguation
Given a rule-based morphological analyzer encoded
as an unweighted FST and a corpus on which the
analyzer has been run ? possibly generating multi-
ple analyses for each token ? we can use our un-
igram model to learn a probabilistic model of dis-
ambiguation in an unsupervised setting (i.e., with-
out annotated examples). The corpus is assumed to
be generated from the unigram distribution Gw, and
the base stem model is set to a fixed character tri-
gram model.4 After learning the parameters of the
model, we can find for each word in the vocabulary
its most likely analysis and use this as a crude dis-
ambiguation step.
3The proposal distribution for Metropolis-Hastings is a Beta
distribution (d) or a Gamma distribution (?+d) centered on the
previous parameter values.
4Experiments suggest that this is important to constrain the
model to realistic stems.
1208
3.1 Morphological Guessers
Finite-state morphological analyzers are usually
specified in three parts: a stem lexicon, which de-
fines the words in the language and classifies them
into several categories according to their grammat-
ical function and their morphological properties; a
set of prefixes and suffixes that can be applied to
each category to form surface words; and possibly
alternation rules that can encode exceptions and
spelling variations. The combination of these parts
provides a powerful framework for defining a gener-
ative model of words. Such models can be reversed
to obtain an analyzer. However, while the two latter
parts can be relatively easy to specify, enumerating
a comprehensive stem lexicon is a time consuming
and necessarily incomplete process, as some cate-
gories are truly open-class.
To allow unknown words to be analyzed, one
can use a guesser that attempts to analyze words
missing in the lexicon. Can we eliminate the stem
lexicon completely and use only the guesser? This
is what we try to do by designing a lexicon-free
analyzer for Russian. A guesser was developed
in three hours; it is prone to over-generation and
produces ambiguous analyses for most words
but covers a large number of morphological phe-
nomena (gender, case, tense, etc.). For example,
the word ??????5 can be correctly analyzed as
?????+Noun+Masc+Prep+Sg but also as the in-
correct forms: ??????+Verb+Pres+2P+Pl,
??????+Noun+Fem+Dat+Sg, ????-
??+Noun+Fem+Prep+Sg, and more.
3.2 Disambiguation Experiments
We train the unigram model on a 1.7M-word cor-
pus of TED talks transcriptions translated into Rus-
sian (Cettolo et al, 2012) and evaluate our ana-
lyzer against a test set consisting of 1,500 gold-
standard analyses obtained from the morphology
disambiguation task of the DIALOG 2010 confer-
ence (Lya?evskaya et al, 2010).6
Each analysis is composed of a lemma (?????),
a part of speech (Noun), and a sequence of ad-
ditional functional morphemes (Masc,Prep,Sg).
We consider only open-class categories: nouns, ad-
5?????? = Hebrew (masculine noun, prepositional case)
6http://ru-eval.ru
jectives, adverbs and verbs, and evaluate the output
of our model with three metrics: the lemma accu-
racy, the part-of-speech accuracy, and the morphol-
ogy F -measure.7
As a baseline, we consider picking a random anal-
ysis from output of the analyzer or choosing the
most frequent lemma and the most frequent morpho-
logical pattern.8 Then, we use our model with each
of the three versions of the pattern model described
in ?2.2. Finally, as an upper bound, we use the gold
standard to select one of the analyses produced by
the guesser.
Since our evaluation is not directly comparable
to the standard for this task, we use for reference
a high-quality analyzer from Xerox9 disambiguated
with the MP0 model (all of the models have very
close accuracy in this case).
Model Lemma POS Morph.
Random 29.8 70.9 50.2
Frequency 31.1 74.4 48.8
Guesser MP0 60.0 82.2 66.3
Guesser MP1 58.9 82.5 69.5
Guesser MP2 59.9 82.4 65.5
Guesser oracle 68.4 84.9 83.0
Xerox MP0 83.6 96.4 78.1
Table 1: Russian morphological disambiguation.
Considering the amount of effort put in develop-
ing the guesser, the baseline POS tagging accuracy
is relatively good. However, the disambiguation is
largely improved by using our unigram model with
respect to all the evaluation categories. We are still
far from the performance of a high-quality analyzer
but, in absence of such a resource, our technique
might be a sensible option. We also note that there is
no clear winner in terms of pattern model, and con-
clude that this choice is task-specific.
7F -measure computed for the set of additional morphemes
and averaged over the words in the corpus.
8We estimate these frequencies by assuming each analysis of
each token is uniformly likely, then summing fractional counts.
9http://open.xerox.com/Services/
fst-nlp-tools/Pages/morphology
1209
4 Open Vocabulary Language Models
We now integrate our unigram model in a hierar-
chical Pitman-Yor n-gram language model (Fig. 1).
The training corpus words are assumed to be
generated from a distribution Gnw drawn from
PY(dn, ?n, Gn?1w ), where Gn?1w is defined recur-
sively down to the base model G0w. Previous work
Teh (2006) simply used G0w = U(V ) where V is
the word vocabulary, but in our case G0w is the MP
defined in ?2.2.
G2wG
3
w G
1
w
d3, ?3 d2, ?2 d1, ?1
Gs
ds, ?s
Gp G0p
dp, ?p
G0sw
Figure 1: The trigram version of our language model rep-
resented as a graphical model. G1w is the unigram model
of ?2.2.
We are interested in evaluating our model in an
open vocabulary scenario where the ability to ex-
plain new unseen words matters. We expect our
model to be able to generalize better thanks to the
combination of a morphological analyzer and a stem
distribution which is less sparse than the word dis-
tribution (for example, for the 1.6M word Turkish
corpus, |V | ? 3.5|S| ? 140k).
To integrate out-of-vocabulary words in our eval-
uation, we use infinite base distributions: G0w (in the
baseline model) or G0s (in the MP) are character tri-
gram models. We define perplexity of a held-out test
corpus in the standard way:
ppl = exp
(
?
1
N
N?
i=1
log p (wi | wi?n+1 ? ? ?wi?1)
)
but compared to the common practice, we do not
need to discount OOVs from this sum since the
model vocabulary is infinite. Note that we also
marginalize by summing over all the possible analy-
ses for a given word when computing its base prob-
ability according to the MP.
4.1 Language Modeling Experiments
We train several trigram models on the Russian TED
talks corpus used in the previous section. Our base-
line is a hierarchical PY trigram model with a tri-
gram character model as the base word distribution.
We compare it with our model using the same char-
acter model for the base stem distribution. Both of
the morphological analyzers described in the previ-
ous section help obtaining perplexity reductions (Ta-
ble 2). We ran a similar experiment on the Turkish
version of this corpus (1.6M words) with a high-
quality analyzer (Oflazer, 1994) and obtain even
larger gains (Table 3).
Model ppl
PY-character LM 563
Guesser MP2 530
Xerox MP2 522
Table 2: Evaluation of the Russian n-gram model.
Model ppl
PY-character LM 1,640
Oflazer MP2 1,292
Table 3: Evaluation of the Turkish n-gram model.
These results can partly be attributed to the high
OOV rate in these conditions: 4% for the Russian
corpus and 6% for the Turkish corpus.
4.2 Predictive Text Input
It is difficult to know whether a decrease in perplex-
ity, as measured in the previous section, will result in
a performance improvement in downstream applica-
tions. As a confirmation that correctly modeling new
words matters, we consider a predictive task with a
truly open vocabulary and that requires only a lan-
guage model: predictive text input.
Given some text, we encode it using a lossy de-
terministic character mapping, and try to recover the
original content by computing the most likely word
sequence. This task is inspired by predictive text
input systems available on cellphones with a 9-key
keypad. For example, the string gave me a cup
is encoded as 4283 63 2 287, which could also
be decoded as: hate of a bus.
1210
Silfverberg et al (2012) describe a system de-
signed for this task in Finnish, which is composed
of a weighted finite-state morphological analyzer
trained on IRC logs. However, their system is re-
stricted to words that are encoded in the analyzer?s
lexicon and does not use context for disambiguation.
In our experiments, we use the same Turkish TED
talks corpus as the previous section. As a baseline,
we use a trigram character language model. We pro-
duce a character lattice which encodes all the pos-
sible interpretations for a word and compose it with
a finite-state representation of the character LM us-
ing OpenFST (Allauzen et al, 2007). Alternatively,
we can use a unigram word model to decode this lat-
tice, backing off to the character language model if
no solution is found. Finally, to be able to make use
of word context, we can extract the k most likely
paths according to the character LM and produce a
word lattice, which is in turn decoded with a lan-
guage model defined over the extracted vocabulary.
Model WER CER
Character LM 48.37 16.72
1-gram + character LM 8.50 3.28
1-gram MP2 6.46 2.37
3-gram + character LM 7.86 3.07
3-gram MP2 5.73 2.15
Table 4: Evaluation of Turkish predictive text input.
We measure word and character error rate (WER,
CER) on the predicted word sequence and observe
large improvements in both of these metrics by mod-
eling morphology, both at the unigram level and
when context is used (Table 4).
Preliminary experiments with a corpus of 1.6M
Turkish tweets, an arguably more appropriate do-
main this task, show smaller but consistent improv-
ing: the trigram word error rate is reduced from 26%
to 24% when our model is used.
4.3 Limitations
While our model is an important step forward in
practical modeling of OOVs using morphological
processes, we have made the linguistically naive as-
sumption that morphology applies inside the lan-
guage?s lexicon but has no effect on the process that
put inflected lexemes together into sentences. In this
regard, our model is a minor variant on traditional n-
gram models that work with ?opaque? word forms.
How to best relax this assumption in a computation-
ally tractable way is an important open question left
for future work.
5 Word Alignment Model
Monolingual models of language are not the only
models that can benefit from taking into account
morphology. In fact, alignment models are a good
candidate for using richer word distributions: they
assume a target word distribution conditioned on
each source word. When the target language is mor-
phologically rich, classic independence assumptions
produce very weak models unless some kind of pre-
processing is applied to one side of the corpus. An
alternative is to use our unigram model as a word
translation distribution for each source word in the
corpus.
Our alignment model is based on a simple variant
of IBM Model 2 where the alignment distribution is
only controlled by two parameters, ? and p0 (Dyer et
al., 2013). p0 is the probability of the null alignment.
For a source sentence f of length n, a target sentence
e of lengthm and a latent alignment a, we define the
following alignment link probabilities (j 6= 0):
p(ai = j | n,m) ? (1? p0) exp
(
??
?
?
?
?
i
m ?
j
n
?
?
?
?
)
? controls the flatness of this distribution: larger val-
ues make the probabilities more peaked around the
diagonal of the alignment matrix.
Each target word is then generated given a source
word and a latent alignment link from the word
translation distribution p(ei | fai , Gw). Note that
this is effectively a unigram distribution over tar-
get words, albeit conditioned on the source word
fj . Here is where our model differs from classic
alignment models: the unigram distribution Gw is
assumed be generated from a PY process. There are
two choices for the base word distribution:
? As a baseline, we use a uniform base distribu-
tion over the target vocabulary: G0w = U(V ).
? We define a stem distribution Gs[f ] for each
source word f , a shared pattern distributionGp,
and set G0w[f ] = MP(Gs[f ], Gp). In this case,
1211
we obtain the model depicted in Fig. 2. The
stem and the pattern models are also given PY
priors with uniform base distribution (G0s =
U(S)).
Finally, we put uninformative priors on the align-
ment distribution parameters: p0 ? Beta(?, ?) is
collapsed and ? ? Gamma(k, ?) is inferred using
Metropolis-Hastings.
f e
a
p0 
Gw
dw, ?w
Gp
G0sGs
G0p
dp, ?p
?, 
ds, ?s
Figure 2: Our alignment model, represented as a graphi-
cal model.
Experiments
We evaluate the alignment error rate of our models
for two language pairs with rich morphology on the
target side. We compare to alignments inferred us-
ing IBM Model 4 trained with EM (Brown et al,
1993),10 a version of our baseline model (described
above) without PY priors (learned using EM), and
the PY-based baseline. We consider two language
pairs.
English-Turkish We use a 2.8M word cleaned
version of the South-East European Times corpus
(Tyers and Alperen, 2010) and gold-standard align-
ments from ?akmak et al (2012). Our morphologi-
cal analyzer is identical to the one used in the previ-
ous sections.
English-Czech We use the 1.3M word News
Commentary corpus and gold-standard alignments
10We use the default GIZA++ stage training scheme:
Model 1 + HMM + Model 3 + Model 4.
from Bojar and Prokopov? (2006). The morpholog-
ical analyzer is provided by Xerox.
Results Results are shown in Table 5. Our lightly
parameterized model performs much better than
IBM Model 4 in these small-data conditions. With
an identical model, we find PY priors outperform
traditional multinomial distributions. Adding mor-
phology further reduced the alignment error rate, for
both languages.
AER
Model en-tr en-cs
Model 4 52.1 34.5
EM 43.8 28.9
PY-U(V ) 39.2 25.7
PY-U(S) 33.8 24.8
Table 5: Word alignment experiments on English-Turkish
(en-tr) and English-Czech (en-cs) data.
As an example of how our model generalizes bet-
ter, consider the sentence pair in Fig. 3, taken from
the evaluation data. The two words composing the
Turkish sentence are not found elsewhere in the cor-
pus, but several related inflections occur.11 It is
therefore trivial for the stem-base model to find the
correct alignment (marked in black), while all the
other models have no evidence for it and choose an
arbitrary alignment (gray points).
I w
a
s
n
o
t
a
b
l
e
t
o
f
i
n
i
s
h
m
y
h
o
m
e
w
o
r
k
?devimi
bitiremedim
Figure 3: A complex Turkish-English word alignment
(alignment points in gray: EM/PY-U(V ); black: PY-
U(S)).
6 Related Work
Computational morphology has received consider-
able attention in NLP since the early work on two-
level morphology (Koskenniemi, 1984; Kaplan and
11?devinin, ?devini, ?devleri; bitmez, bitirileceg?inden,
bitmesiyle, ...
1212
Kay, 1994). It is now widely accepted that finite-
state transducers have sufficient power to express
nearly all morphological phenomena, and the XFST
toolkit (Beesley and Karttunen, 2003) has con-
tributed to the practical adoption of this modeling
approach. Recently, open-source tools have been re-
leased: in this paper, we used Foma (Hulden, 2009)
to develop the Russian guesser.
Since some inflected forms have several possible
analyses, there has been a great deal of work on se-
lecting the intended one in context (Hakkani-T?r et
al., 2000; Hajic? et al, 2001; Habash and Rambow,
2005; Smith et al, 2005; Habash et al, 2009). Our
disambiguation model is closely related to genera-
tive models used for this purpose (Hakkani-T?r et
al., 2000).
Rule-based analysis is not the only approach
to modeling morphology, and many unsupervised
models have been proposed.12 Heuristic segmenta-
tion approaches based on the minimum description
length principle (Goldsmith, 2001; Creutz and La-
gus, 2007; de Marcken, 1996; Brent et al, 1995)
have been shown to be effective, and Bayesian
model-based versions have been proposed as well
(Goldwater et al, 2011; Snyder and Barzilay, 2008;
Snover and Brent, 2001). In ?3, we suggested a third
way between rule-based approaches and fully un-
supervised learning that combines the best of both
worlds.
Morphological analysis or segmentation is crucial
to the performance of several applications: machine
translation (Goldwater and McClosky, 2005; Al-
Haj and Lavie, 2010; Oflazer and El-Kahlout, 2007;
Minkov et al, 2007; Habash and Sadat, 2006, in-
ter alia), automatic speech recognition (Creutz et al,
2007), and syntactic parsing (Tsarfaty et al, 2010).
Several methods have been proposed to integrate
morphology into n-gram language models, includ-
ing factored language models (Bilmes and Kirch-
hoff, 2003), discriminative language modeling (Ar?-
soy et al, 2008), and more heuristic approaches
(Monz, 2011).
Despite the fundamentally open nature of the lex-
icon (Heaps, 1978), there has been distressingly lit-
12Developing a high-coverage analyzer can be a time-
consuming process even with the simplicity of modern toolkits,
and unsupervised morphology learning is an attractive problem
for computational cognitive science.
tle attention to the general problem of open vocabu-
lary language modeling problem (most applications
make a closed-vocabulary assumption). The classic
exploration of open vocabulary language modeling
is Brown et al (1992), which proposed the strategy
of interpolating between word- and character-based
models. Character-based language models are re-
viewed by Carpenter (2005). So-called hybrid mod-
els that model both words and sublexical units have
become popular in speech recognition (Shaik et al,
2012; Parada et al, 2011; Bazzi, 2002). Open-
vocabulary language language modeling has also re-
cently been explored in the context of assistive tech-
nologies (Roark, 2009).
Finally, Pitman-Yor processes (PYPs) have be-
come widespread in natural language processing
since they are natural power-law generators. It has
been shown that the widely used modified Kneser-
Ney estimator (Chen and Goodman, 1998) for n-
gram language models is an approximation of the
posterior predictive distribution of a language model
with hierarchical PYP priors (Goldwater et al, 2011;
Teh, 2006).
7 Conclusion
We described a generative model which makes use
of morphological analyzers to produce richer word
distributions through sharing of statistical strength
between stems. We have shown how it can be in-
tegrated into several models central to NLP appli-
cations and have empirically validated the effective-
ness of these changes. Although this paper mostly
focused on languages that are well studied and for
which high-quality analyzers are available, our mod-
els are especially relevant in low-resource scenarios
because they do not require disambiguated analyses.
In future work, we plan to apply these techniques to
languages such as Kinyarwanda, a resource-poor but
morphologically rich language spoken in Rwanda.
It is our belief that knowledge-rich models can help
bridge the gap between low- and high-resource lan-
guages.
Acknowledgments
We thank Kemal Oflazer for making his Turkish lan-
guage morphological analyzer available to us and Bren-
dan O?Connor for gathering the Turkish tweets used in
1213
the predictive text experiments. This work was spon-
sored by the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant number
W911NF-10-1-0533.
References
H. Al-Haj and A. Lavie. 2010. The impact
of Arabic morphological segmentation on broad-
coverage English-to-Arabic statistical machine trans-
lation. Proc. of AMTA.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Implementation and Application of Au-
tomata, pages 11?23.
Ebru Ar?soy, Brian Roark, Izhak Shafran, and Murat
Sara?lar. 2008. Discriminative n-gram language mod-
eling for Turkish. In Proc. of Interspeech.
Issam Bazzi. 2002. Modelling out-of-vocabulary words
for robust speech recognition. Ph.D. thesis, MIT.
K.R. Beesley and L. Karttunen. 2003. Finite-state mor-
phology: Xerox tools and techniques. CSLI, Stanford.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
Proc. of NAACL.
Ondr?ej Bojar and Magdalena Prokopov?. 2006. Czech-
English word alignment. In Proc. of LREC.
Michael R. Brent, Sreerama K. Murthy, and Andrew
Lundberg. 1995. Discovering morphemic suffixes: A
case study in MDL induction. In Proceedings of the
Fifth International Workshop on Artificial Intelligence
and Statistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, Robert L. Mercer, and Jennifer C. Lai.
1992. An estimate of an upper bound for the entropy
of English. Computational Linguistics, 18(1):31?40.
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
Bob Carpenter. 2005. Scaling high-order character lan-
guage models to gigabytes. In Proceedings of the ACL
Workshop on Software.
Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012. WIT3: Web inventory of transcribed and trans-
lated talks. In Proc. of EAMT.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Harvard University.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1).
M. Creutz, T. Hirsim?ki, M. Kurimo, A. Puurula,
J. Pylkk?nen, V. Siivola, M. Varjokallio, E. Arisoy,
M. Sara?lar, and A. Stolcke. 2007. Morph-based
speech recognition and modeling of out-of-vocabulary
words across languages. ACM Transactions on Speech
and Language Processing, 5(1):3.
Carl G. de Marcken. 1996. Unsupervised Language Ac-
quisition. Ph.D. thesis, MIT.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameterization
of IBM Model 2. In Proc. of NAACL.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2):153?198.
S. Goldwater and D. McClosky. 2005. Improving statis-
tical MT through morphological analysis. In Proc. of
EMNLP.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2011. Producing power-law distributions and
damping word frequencies with two-stage language
models. Journal of Machine Learning Research,
12:2335?2382.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging, and morphological
disambiguation in one fell swoop. In Proc. of ACL.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proc. of NAACL.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Proceedings
of the Second International Conference on Arabic Lan-
guage Resources and Tools.
Jan Hajic?, P. Krbec, P. Kve?ton?, K. Oliva, and V. Petrovic?.
2001. Serial combination of rules and statistics. In
Proc. of ACL.
D. Z. Hakkani-T?r, Kemal Oflazer, and G. T?r. 2000.
Statistical morphological disambiguation for aggluti-
native languages. In Proc. of COLING.
Harold Stanley Heaps. 1978. Information Retrieval:
Computational and Theoretical Aspects. Academic
Press.
M. Hulden. 2009. Foma: a finite-state compiler and li-
brary. In Proc. of EACL.
Ronald M. Kaplan and Martin Kay. 1994. Regular mod-
els of phonological rule systems. Computational Lin-
guistics, 20(3):331?378.
Lauri Karttunen and Kenneth R. Beesley. 2005. Twenty-
five years of finite-state morphology. In Inquiries into
Words, Constraints and Contexts, pages 71?83. CSLI.
Kimmo Koskenniemi. 1984. A general computational
model for word-form recognition and production. In
Proc. of ACL-COLING.
1214
O. Lya?evskaya, I. Astaf?yeva, A. Bonch-Osmolovskaya,
A. Garej?ina, Y. Gri?ina, V. D?yac?kov, M. Ionov,
A. Koroleva, M. Kudrinskij, A. Lityagina, Y. Luc?ina,
Y. Sidorova, S. Toldova, S. Savc?uk, and S. Ko-
val?. 2010. Ocenka metodov avtomatic?eskogo
analiza teksta: morfologic?eskie parseri russkogo
yazyka. Komp?juternaya lingvistika i intellektual?nye
texnologii (Computational linguistics and intellectual
technologies).
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proc. of ACL.
Christof Monz. 2011. Statistical machine translation
with local language models. In Proc. of EMNLP.
Kemal Oflazer and I?lknur Durgar El-Kahlout. 2007. Ex-
ploring different representational units in English-to-
Turkish statistical machine translation. In Proc. of
StatMT.
K. Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Computing,
9(2):137?148.
Carolina Parada, Mark Dredze, Abhinav Sethy, and Ariya
Rastrow. 2011. Learning sub-word units for open vo-
cabulary speech recognition. In Proc. of ACL.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25(2):855?90.
Brian Roark. 2009. Open vocabulary language model-
ing for binary response typing interfaces. Technical
Report CSLU-09-001, Oregon Health & Science Uni-
versity.
M. Ali Basha Shaik, David Rybach, Stefan Hahn, Ralf
Schlu?ter, and Hermann Ney. 2012. Hierarchical hy-
brid language models for open vocabulary continuous
speech recognition using wfst. In Proc. of SAPA.
M. Silfverberg, K. Lind?n, and M. Hyv?rinen. 2012.
Predictive text entry for agglutinative languages using
unsupervised morphological segmentation. In Proc.
of Computational Linguistics and Intelligent Text Pro-
cessing.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proc. of EMNLP.
Matt G. Snover and Michael R. Brent. 2001. A Bayesian
model for morpheme and paradigm identification. In
Proc. of ACL.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proc. of ACL.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL.
Reut Tsarfaty, Djam? Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing of morphologically rich languages: What, how
and whither. In Proc. of Workshop on Statistical Pars-
ing of Morphologically Rich Languages.
F. Tyers and M.S. Alperen. 2010. South-east european
times: A parallel corpus of Balkan languages. In
Proceedings of the LREC workshop on Exploitation
of multilingual resources and tools for Central and
(South) Eastern European Languages.
M. Talha ?akmak, S?leyman Acar, and G?ls?en Eryig?it.
2012. Word alignment for English-Turkish language
pair. In Proc. of LREC.
1215
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 279?287,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Identifying the L1 of non-native writers: the CMU-Haifa system
Yulia Tsvetkov? Naama Twitto? Nathan Schneider? Noam Ordan?
Manaal Faruqui? Victor Chahuneau? Shuly Wintner? Chris Dyer?
?Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA
cdyer@cs.cmu.edu
?Department of Computer Science
University of Haifa
Haifa, Israel
shuly@cs.haifa.ac.il
Abstract
We show that it is possible to learn to identify, with
high accuracy, the native language of English test
takers from the content of the essays they write.
Our method uses standard text classification tech-
niques based on multiclass logistic regression, com-
bining individually weak indicators to predict the
most probable native language from a set of 11 pos-
sibilities. We describe the various features used for
classification, as well as the settings of the classifier
that yielded the highest accuracy.
1 Introduction
The task we address in this work is identifying the
native language (L1) of non-native English (L2) au-
thors. More specifically, given a dataset of short
English essays (Blanchard et al, 2013), composed
as part of the Test of English as a Foreign Lan-
guage (TOEFL) by authors whose native language is
one out of 11 possible languages?Arabic, Chinese,
French, German, Hindi, Italian, Japanese, Korean,
Spanish, Telugu, or Turkish?our task is to identify
that language.
This task has a clear empirical motivation. Non-
native speakers make different errors when they
write English, depending on their native language
(Lado, 1957; Swan and Smith, 2001); understand-
ing the different types of errors is a prerequisite for
correcting them (Leacock et al, 2010), and systems
such as the one we describe here can shed interest-
ing light on such errors. Tutoring applications can
use our system to identify the native language of
students and offer better-targeted advice. Forensic
linguistic applications are sometimes required to de-
termine the L1 of authors (Estival et al, 2007b; Es-
tival et al, 2007a). Additionally, we believe that the
task is interesting in and of itself, providing a bet-
ter understanding of non-native language. We are
thus equally interested in defining meaningful fea-
tures whose contribution to the task can be linguis-
tically interpreted. Briefly, our features draw heav-
ily on prior work in general text classification and
authorship identification, those used in identifying
so-called translationese (Volansky et al, forthcom-
ing), and a class of features that involves determin-
ing what minimal changes would be necessary to
transform the essays into ?standard? English (as de-
termined by an n-gram language model).
We address the task as a multiway text-
classification task; we describe our data in ?3 and
classification model in ?4. As in other author attri-
bution tasks (Juola, 2006), the choice of features for
the classifier is crucial; we discuss the features we
define in ?5. We report our results in ?6 and con-
clude with suggestions for future research.
2 Related work
The task of L1 identification was introduced by Kop-
pel et al (2005a; 2005b), who work on the Inter-
national Corpus of Learner English (Granger et al,
2009), which includes texts written by students from
5 countries, Russia, the Czech Republic, Bulgaria,
France, and Spain. The texts range from 500 to
850 words in length. Their classification method
is a linear SVM, and features include 400 standard
function words, 200 letter n-grams, 185 error types
and 250 rare part-of-speech (POS) bigrams. Ten-
279
fold cross-validation results on this dataset are 80%
accuracy.
The same experimental setup is assumed by Tsur
and Rappoport (2007), who are mostly interested
in testing the hypothesis that an author?s choice of
words in a second language is influenced by the
phonology of his or her L1. They confirm this hy-
pothesis by carefully analyzing the features used by
Koppel et al, controlling for potential biases.
Wong and Dras (2009; 2011) are also motivated
by a linguistic hypothesis, namely that syntactic er-
rors in a text are influenced by the author?s L1.
Wong and Dras (2009) analyze three error types sta-
tistically, and then add them as features in the same
experimental setup as above (using LIBSVM with a
radial kernel for classification). The error types are
subject-verb disagreement, noun-number disagree-
ment and misuse of determiners. Addition of these
features does not improve on the results of Kop-
pel et al. Wong and Dras (2011) further extend
this work by adding as features horizontal slices of
parse trees, thereby capturing more syntactic struc-
ture. This improves the results significantly, yielding
78% accuracy compared with less than 65% using
only lexical features.
Kochmar (2011) uses a different corpus, the Cam-
bridge Learner Corpus, in which texts are 200-400
word long, and are authored by native speakers of
five Germanic languages (German, Swiss German,
Dutch, Swedish and Danish) and five Romance lan-
guages (French, Italian, Catalan, Spanish and Por-
tuguese). Again, SVMs are used as the classification
device. Features include POS n-grams, character n-
grams, phrase-structure rules (extracted from parse
trees), and two measures of error rate. The classi-
fier is evaluated on its ability to distinguish between
pairs of closely-related L1s, and the results are usu-
ally excellent.
A completely different approach is offered by
Brooke and Hirst (2011). Since training corpora for
this task are rare, they use mainly L1 (blog) cor-
pora. Given English word bigrams ?e1, e2?, they try
to assess, for each L1, how likely it is that an L1 bi-
gram was translated literally by the author, resulting
in ?e1, e2?. Working with four L1s (French, Span-
ish, Chinese, and Japanese), and evaluating on the
International Corpus of Learner English, accuracy is
below 50%.
3 Data
Our dataset in this work consists of TOEFL essays
written by speakers of eleven different L1s (Blan-
chard et al, 2013), distributed as part of the Na-
tive Language Identification Shared Task (Tetreault
et al, 2013). The training data consists of 1000
essays from each native language. The essays are
short, consisting of 10 to 20 sentences each. We
used the provided splits of 900 documents for train-
ing and 100 for development. Each document is an-
notated with the author?s English proficiency level
(low, medium, high) and an identification (1 to 8) of
the essay prompt. All essays are tokenized and split
into sentences. In table 1 we provide some statistics
on the training corpora, listed by the authors? profi-
ciency level. All essays were tagged with the Stan-
ford part-of-speech tagger (Toutanova et al, 2003).
We did not parse the dataset.
Low Medium High
# Documents 1,069 5,366 3,456
# Tokens 245,130 1,819,407 1,388,260
# Types 13,110 37,393 28,329
Table 1: Training set statistics.
4 Model
For our classification model we used the creg re-
gression modeling framework to train a 11-class lo-
gistic regression classifier.1 We parameterize the
classifier as a multiclass logistic regression:
p?(y | x) =
exp
?
j ? jh j(x, y)
Z?(x)
,
where x are documents, h j(?) are real-valued feature
functions of the document being classified, ? j are the
corresponding weights, and y is one of the eleven L1
class labels. To train the parameters of our model,
we minimized the following objective,
L = ?
`2 reg.
?????
j
?2j ?
?
{(xi,yi)}
|D|
i=1
(
log likelihood
?          ??          ?
log p?(yi | xi) +
?Ep?(y? |xi) log p?(y
? | xi)
?                      ??                      ?
?conditional entropy
)
,
1https://github.com/redpony/creg
280
which combines the negative log likelihood of the
training dataset D, an `2 (quadratic) penalty on the
magnitude of ? (weighted by ?), and the negative en-
tropy of the predictive model (weighted by ?). While
an `2 weight penalty is standard in regression prob-
lems like this, we found that the the additional en-
tropy term gave more reliable results. Intuitively,
the entropic regularizer encourages the model to re-
main maximally uncertain about its predictions. In
the metaphor of ?maximum entropy?, the entropic
prior finds a solution that has more entropy than the
?maximum? model that is compatible with the con-
straints.
The objective cannot be minimized in closed
form, but it does have a unique minimum and
is straightforwardly differentiable, so we used L-
BFGS to find the optimal weight settings (Liu et al,
1989).
5 Feature Overview
We define a large arsenal of features, our motivation
being both to improve the accuracy of classification
and to be able to interpret the characteristics of the
language produced by speakers of different L1s.
While some of the features were used in prior
work (?2), we focus on two broad novel categories
of features: those inspired by the features used
to identify translationese by Volansky et al (forth-
coming) and those extracted by automatic statisti-
cal ?correction? of the essays. Refer to figure 1 to
see the set of features and their values that were ex-
tracted from an example sentence.
POS n-grams Part-of-speech n-grams were used in
various text-classification tasks.
Prompt Since the prompt contributes information
on the domain, it is likely that some words (and,
hence, character sequences) will occur more fre-
quently with some prompts than with others. We
therefore use the prompt ID in conjunction with
other features.
Document length The number of tokens in the text
is highly correlated with the author?s level of flu-
ency, which in turn is correlated with the author?s
L1.
Pronouns The use of pronouns varies greatly
among different authors. We use the same list
of 25 English pronouns that Volansky et al (forth-
coming) use for identifying translationese.
Punctuation Similarly, different languages use
punctuation differently, and we expect this to taint
the use of punctuation in non-native texts. Of
course, character n-grams subsume this feature.
Passives English uses passive voice more fre-
quently than other languages. Again, the use of
passives in L2 can be correlated with the author?s
L1.
Positional token frequency The choice of the first
and last few words in a sentence is highly con-
strained, and may be significantly influenced by
the author?s L1.
Cohesive markers These are 40 function words
(and short phrases) that have a strong discourse
function in texts (however, because, in fact,
etc.). Translators tend to spell out implicit utter-
ances and render them explicitly in the target text
(Blum-Kulka, 1986). We use the list of Volansky
et al (forthcoming).
Cohesive verbs This is a list of manually compiled
verbs that are used, like cohesive markers, to spell
out implicit utterances (indicate, imply, contain,
etc.).
Function words Frequent tokens, which are mostly
function words, have been used successfully for
various text classification tasks. Koppel and Or-
dan (2011) define a list of 400 such words, of
which we only use 100 (using the entire list was
not significantly different). Note that pronouns
are included in this list.
Contextual function words To further capitalize
on the ability of function words to discriminate,
we define pairs consisting of a function word from
the list mentioned above, along with the POS tag
of its adjacent word. This feature captures pat-
terns such as verbs and the preposition or particle
immediately to their right, or nouns and the deter-
miner that precedes them. We also define 3-grams
consisting of one or two function words and the
POS tag of the third word in the 3-gram.
Lemmas The content of the text is not considered a
good indication of the author?s L1, but many text
categorization tasks use lemmas (more precisely,
the stems produced by the tagger) as features ap-
proximating the content.
Misspelling features Learning to perceive, pro-
duce, and encode non-native phonemic contrasts
281
Firstly the employers live more savely because they are going to have more money to spend for luxury .
Presence Considered alternatives/edits
Characters
"CHAR_l_y_ ": log 2 + 1
"CharPrompt_P5_g_o_i": log 1 + 1
"MFChar_e_ ": log 1 + 1
"Punc_period": log 1 + 1
"DeleteP_p_.": 1.0
"InsertP_p_,": 1.0
"MID:SUBST:v:f": log 1 + 1
"SUBST:v:f": log 1 + 1
Words
"DocLen_": log 19 + 1
"MeanWordRank": 422.6
"CohMarker_because": log 1 + 1
"MostFreq_have": log 1 + 1
"PosToken_last_luxury": log 1 + 1
"Pronouns_they": log 1 + 1
"MSP:safely": log 1 + 1
"Match_p_to": 0.5
"Delete_p_to": 0.5
"Delete_p_are": 1.0
"Delete_p_because": 1.0
"Delete_p_for": 1.0
POS "POS
_VBP_VBG_TO": log 1 + 1
"POS_p_VBP_VBG_TO": 0.059
Words + POS "VBP
_VBG_to": log 1 + 1
"FW__more RB": log 1 + 1
Figure 1: Some of the features extracted for an L1 German sentence.
is extremely difficult for L2 learners (Hayes-Harb
and Masuda, 2008). Since English?s orthogra-
phy is largely phonemic?even if it is irregular
in many places, we expect leaners whose na-
tive phoneme contrasts are different from those
of English to make characteristic spelling errors.
For example, since Japanese and Korean lack a
phonemic /l/-/r/ contrast, we expect native speak-
ers of those languages to be more likely to make
spelling errors that confuse l and r relative to
native speakers of languages such as Spanish in
which that pair is contrastive. To make this in-
formation available to our model, we use a noisy
channel spelling corrector (Kernighan, 1990) to
identify and correct misspelled words in the train-
ing and test data. From these corrections, we ex-
tract minimal edit features that show what inser-
tions, deletions, substitutions and joinings (where
two separate words are written merged into a sin-
gle orthographic token) were made by the author
of the essay.
Restored tags We focus on three important token
classes defined above: punctuation marks, func-
tion words and cohesive verbs. We first remove
words in these classes from the texts, and then
recover the most likely hidden tokens in a se-
quence of words, according to an n-gram lan-
guage model trained on all essays in the training
corpus corrected with a spell checker and con-
taining both words and hidden tokens. This fea-
ture should capture specific words or punctuation
marks that are consistently omitted (deletions),
or misused (insertions, substitutions). To restore
hidden tokens we use the hidden-ngram util-
ity provided in SRI?s language modeling toolkit
(Stolcke, 2002).
Brown clusters (Brown et al, 1992) describe an al-
gorithm that induces a hierarchical clustering of
a language?s vocabulary based on each vocabu-
lary item?s tendency to appear in similar left and
right contexts in a training corpus. While origi-
nally developed to reduce the number of parame-
ters required in n-gram language models, Brown
clusters have been found to be extremely effective
as lexical representations in a variety of regres-
sion problems that condition on text (Koo et al,
2008; Turian et al, 2010; Owoputi et al, 2013).
Using an open-source implementation of the al-
gorithm,2 we clustered 8 billion words of English
into 600 classes.3 We included log counts of all
4-grams of Brown clusters that occurred at least
100 times in the NLI training data.
5.1 Main Features
We use the following four feature types as the base-
line features in our model. For features that are sen-
sitive to frequency, we use the log of the (frequency-
plus-one) as the feature?s value. Table 2 reports the
accuracy of using each feature type in isolation (with
2https://github.com/percyliang/brown-cluster
3http://www.ark.cs.cmu.edu/cdyer/en-600/
cluster_viewer.html
282
Feature Accuracy (%)
POS 55.18
FreqChar 74.12
CharPrompt 65.09
Brown 72.26
DocLen 11.81
Punct 27.41
Pron 22.81
Position 53.03
PsvRatio 12.26
CxtFxn (bigram) 62.79
CxtFxn (trigram) 62.32
Misspell 37.29
Restore 47.67
CohMark 25.71
CohVerb 22.85
FxnWord 42.47
Table 2: Independent performance of feature types de-
tailed in ?5.1, ?5.2 and ?5.3. Accuracy is averaged over
10 folds of cross-validation on the training set.
10-fold cross-validation on the training set).
POS Part-of-speech n-grams. Features were ex-
tracted to count every POS 1-, 2-, 3- and 4-gram
in each document.
FreqChar Frequent character n-grams. We exper-
imented with character n-grams: To reduce the
number of parameters, we removed features only
those character n-grams that are observed more
than 5 times in the training corpus, and n ranges
from 1 to 4. High-weight features include:
TUR:<Turk>; ITA:<Ital>; JPN:<Japa>.
CharPrompt Conjunction of the character n-gram
features defined above with the prompt ID.
Brown Substitutions, deletions and insertions
counts of Brown cluster unigrams and bigrams in
each document.
The accuracy of the classifier on the development set
using these four feature types is reported in table 3.4
5.2 Additional Features
To the basic set of features we now add more spe-
cific, linguistically-motivated features, each adding
a small number of parameters to the model. As
above, we indicate the accuracy of each feature type
in isolation.
4For experiments in this paper combining multiple types of
features, we used Jonathan Clark?s workflow management tool,
ducttape (https://github.com/jhclark/ducttape).
Feature Group # Params Accuracy (%) `2
POS 540,947 55.18 1.0
+ FreqChar 1,036,871 79.55 1.0
+ CharPrompt 2,111,175 79.82 1.0
+ Brown 5,664,461 81.09 1.0
Table 3: Dev set accuracy with main feature groups,
added cumulatively. The number of parameters is always
a multiple of 11 (the number of classes). Only `2 regular-
ization was used for these experiments; the penalty was
tuned on the dev set as well.
DocLen Document length in tokens.
Punct Counts of each punctuation mark.
Pron Counts of each pronoun.
Position Positional token frequency. We use the
counts for the first two and last three words be-
fore the period in each sentence as features. High-
weight features for the second word include:
ARA:2<,>; CHI:2<is>; HIN:2<can>.
PsvRatio The proportion of passive verbs out of all
verbs.
CxtFxn Contextual function words. High-weight
features include: CHI:<some JJ>;
HIN:<as VBN>.
Misspell Spelling correction edits. Features
included substitutions, deletions, insertions,
doubling of letters and missing doublings of
letters, and splittings (alot?a lot), as well as the
word position where the error occurred.
High-weight features include: ARA:DEL<e>,
ARA:INS<e>, ARA:SUBST<e>/<i>;
GER:SUBST<z>/<y>; JPN:SUBST<l>/<r>,
JPN:SUBST<r>/<l>; SPA:DOUBLE<s>,
SPA:MID_INS<s>, SPA:INS<s>.
Restore Counts of substitutions, deletions and
insertions of predefined tokens that we restored
in the texts. High-weight features include:
CHI:DELWORD<do>; GER:DELWORD<on>;
ITA:DELWORD<be>
Table 4 reports the empirical improvement that each
of these brings independently when added to the
main features (?5.1).
5.3 Discarded Features
We also tried several other feature types that did not
improve the accuracy of the classifier on the devel-
opment set.
CohMark Counts of each cohesive marker.
283
Feature Group # Params Accuracy (%) `2
main + Position 6,153,015 81.00 1.0
main + PsvRatio 5,664,472 81.00 1.0
main 5,664,461 81.09 1.0
main + DocLen 5,664,472 81.09 1.0
main + Pron 5,664,736 81.09 1.0
main + Punct 5,664,604 81.09 1.0
main + Misspell 5,799,860 81.27 5.0
main + Restore 5,682,589 81.36 5.0
main + CxtFxn 7,669,684 81.73 1.0
Table 4: Dev set accuracy with main features plus addi-
tional feature groups, added independently. `2 regulariza-
tion was tuned as in table 3 (two values, 1.0 and 5.0, were
tried for each configuration; more careful tuning might
produce slightly better accuracy). Results are sorted by
accuracy; only three groups exhibited independent im-
provements over the main feature set.
CohVerb Counts of each cohesive verb.
FxnWord Counts of function words. These features
are subsumed by the highly discriminative CxtFxn
features.
6 Results
The full model that we used to classify the test set
combines all features listed in table 4. Using all
these features, the accuracy on the development set
is 84.55%, and on the test set it is 81.5%. The values
for ? and ? were tuned to optimize development set
performance, and found to be ? = 5, ? = 2.
Table 5 lists the confusion matrix on the test set,
as well as precision, recall and F1-score for each L1.
The largest error type involved predicting Telugu
when the true label was Hindi, which happened 18
times. This error is unsurprising since many Hindi
and Telugu speakers are arguably native speakers of
Indian English.
Production of L2 texts, not unlike translating from
L1 to L2, involves a tension between the impos-
ing models of L1 (and the source text), on the one
hand, and a set of cognitive constraints resulting
from the efforts to generate the target text, on the
other. The former is called interference in Trans-
lation Studies (Toury, 1995) and transfer in second
language acquisition (Selinker, 1972). Volansky et
al. (forthcoming) designed 32 classifiers to test the
validity of the forces acting on translated texts, and
found that features sensitive to interference consis-
tently yielded the best performing classifiers. And
indeed, in this work too, we find fingerprints of the
source language are dominant in the makeup of L2
texts. The main difference, however, between texts
translated by professionals and the texts we address
here, is that more often than not professional trans-
lators translate into their mother tongue, whereas L2
writers write out of their mother tongue by defini-
tion. So interference is ever more exaggerated in
this case, for example, also phonologically (Tsur and
Rappoport, 2007).
We explore the effects of interference by analyz-
ing several patterns we observe in the features. Our
classifier finds that the character sequence alot is
overrepresented in Arabic L2 texts. Arabic has no
indefinite article and we speculate that Arabic speak-
ers conceive a lot as a single word; the Arabic equiv-
alent for a lot is used adverbially like an -ly suffix
in English. For the same reason, another promi-
nent feature is a missing definite article before nouns
and adjectives. Additionally, Arabic, being an Ab-
jad language, rarely indicates vowels, and indeed we
find many missing e?s and i?s in the texts of Arabic
speakers. Phonologically, because Arabic conflates
/I/ and /@/ into /i/ (at least in Modern Standard Ara-
bic), we see that many e?s are indeed substituted for
i?s in these texts.
We find that essays that contain hyphens are more
likely to be from German authors. We again find
evidence of interference from the native language
here. First, relative clauses are widely used in Ger-
man, and we see this pattern in L2 English of L1
German speakers. For example, any given rational
being ? let us say Immanual Kant ? we find that.
Another source of extra hyphens stems from com-
pounding convention. So, for example, we find well-
known, community-help, spare-time, football-club,
etc. Many of these reflect an effort to both connect
and separate connected forms in the original (e.g.,
Fussballklub, which in English would be more natu-
rally rendered as football club). Another unexpected
feature of essays by native Germans is a frequent
substitution of the letter y for z and vice versa. We
suspect this owes to their switched positions on Ger-
man keyboards.
Lexical item frequency also provides clues to the
L1 of the essay writers. The word that occurs more
frequently in the texts of German L1 speakers. We
284
true? ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision (%) Recall (%) F1 (%)
ARA 80 0 2 1 3 4 1 0 4 2 3 80.8 80.0 80.4
CHI 3 80 0 1 1 0 6 7 1 0 1 88.9 80.0 84.2
FRE 2 2 81 5 1 2 1 0 3 0 3 86.2 81.0 83.5
GER 1 1 1 93 0 0 0 1 1 0 2 87.7 93.0 90.3
HIN 2 0 0 1 77 1 0 1 5 9 4 74.8 77.0 75.9
ITA 2 0 3 1 1 87 1 0 3 0 2 82.1 87.0 84.5
JPN 2 1 1 2 0 1 87 5 0 0 1 78.4 87.0 82.5
KOR 1 5 2 0 1 0 9 81 1 0 0 80.2 81.0 80.6
SPA 2 0 2 0 1 8 2 1 78 1 5 77.2 78.0 77.6
TEL 0 1 0 0 18 1 2 1 1 73 3 85.9 73.0 78.9
TUR 4 0 2 2 0 2 2 4 4 0 80 76.9 80.0 78.4
Table 5: Official test set confusion matrix with the full model. Accuracy is 81.5%.
hypothesize that in English it is optional in rela-
tive clauses whereas in German it is not, so Ger-
man speakers are less comfortable using the non-
obligatory form. Also, often is over represented. We
hypothesize that since it is cognate of German oft, it
is not cognitively expensive to retrieve it. We find
many times?a literal translation of muchas veces?
in Spanish essays.
Other informative features that reflect L1 features
include frequent misspellings involving confusions
of l and r in Japanese essays. More mysteriously,
the characters r and s are misused in Chinese and
Spanish, respectively. The word then is dominant
in the texts of Hindi speakers. Finally, it is clear
that authors refer to their native cultures (and, conse-
quently, native languages and countries); the strings
Turkish, Korea, and Ita were dominant in the texts of
Turkish, Korean and Italian native speakers, respec-
tively.
7 Discussion
We experimented with different classifiers and a
large set of features to solve an 11-way classifica-
tion problem. We hope that studying this problem
will improve to facilitate human assessment, grad-
ing, and teaching of English as a second language.
While the core features used are sparse and sensitive
to lexical and even orthographic features of the writ-
ing, many of them are linguistically informed and
provide insight into how L1 and L2 interact.
Our point of departure was the analogy between
translated texts as a genre in its own and L2 writ-
ers as pseudo translators, relying heavily on their
mother tongue and transferring their native models
to a second language. In formulating our features,
we assumed that like translators, L2 writers will
write in a simplified manner and overuse explicit
markers. Although this should be studied vis-?-vis
comparable outputs of mother tongue writers in En-
glish, we observe that the best features of our clas-
sifiers are of the ?interference? type, i.e. phonolog-
ical, morphological and syntactic in nature, mostly
in the form of misspelling features, restoration tags,
punctuation and lexical and syntactic modeling.
We would like to stress that certain features indi-
cating a particular L1 have no bearing on the quality
of the English produced. This has been discussed
extensively in Translation Studies (Toury, 1995),
where interference is observed by the overuse or un-
deruse of certain features reflecting the typological
differences between a specific pair of languages, but
which is still within grammatical limits. For exam-
ple, the fact that Italian native speakers favor the
syntactic sequence of determiner + adjective + noun
(e.g., a big risk or this new business) has little pre-
scriptive value for teachers.
A further example of how L2 quality and the
ability to predict L1 are uncorrelated, we noted
that certain L2 writers often repeat words appear-
ing in their essay prompts, and including informa-
tion about whether the writer was reusing prompt
words improved classification accuracy. We suggest
this reflects different educational backgrounds. This
feature says nothing about the quality of the text, just
as the tendency of Korean and Italian writers to men-
tion their home country more often does not.
285
Acknowledgments
This research was supported by a grant from the Is-
raeli Ministry of Science and Technology.
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
corpus of non-native English. Technical report, Edu-
cational Testing Service.
Shoshana Blum-Kulka. 1986. Shifts of cohesion and co-
herence in translation. In Juliane House and Shoshana
Blum-Kulka, editors, Interlingual and intercultural
communication Discourse and cognition in translation
and second language acquisition studies, volume 35,
pages 17?35. Gunter Narr Verlag.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Conference
of Learner Corpus Research (LCR2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4).
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007a. Author profil-
ing for English emails. In Proc. of PACLING, pages
263?272, Melbourne, Australia.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007b. TAT: An author
profiling tool with application to Arabic emails. In
Proc. of the Australasian Language Technology Work-
shop, pages 21?30, Melbourne, Australia, December.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English. Presses universitaires de Louvain,
Louvain-la-Neuve.
Rachel Hayes-Harb and Kyoko Masuda. 2008. Devel-
opment of the ability to lexically encode novel second
language phonemic contrasts. Second Language Re-
search, 24(1):5?33.
Patrick Juola. 2006. Authorship attribution. Founda-
tions and Trends in Information Retrieval, 1(3):233?
334.
Mark D. Kernighan. 1990. A spelling correction pro-
gram based on a noisy channel model. In Proc. of
COLING.
Ekaterina Kochmar. 2011. Identification of a writer?s na-
tive language by error analysis. Master?s thesis, Uni-
versity of Cambridge.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In Proc. of ACL-HLT, pages 1318?
1326, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005a.
Automatically determining an anonymous author?s na-
tive language. Intelligence and Security Informatics,
pages 41?76.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005b.
Determining an author?s native language by mining
a text for errors. In Proc. of KDD, pages 624?628,
Chicago, IL. ACM.
Robert Lado. 1957. Linguistics across cultures: applied
linguistics for language teachers. University of Michi-
gan Press, Ann Arbor, Michigan, June.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan and
Claypool.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge No-
cedal. 1989. On the limited memory BFGS method
for large scale optimization. Mathematical Program-
ming B, 45(3):503?528.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In Proc. of NAACL.
Larry Selinker. 1972. Interlanguage. International
Review of Applied Linguistics in Language Teaching,
10(1?4):209?232.
Andreas Stolcke. 2002. SRILM?an extensible lan-
guage modeling toolkit. In Procedings of Interna-
tional Conference on Spoken Language Processing,
pages 901?904.
Michael Swan and Bernard Smith. 2001. Learner En-
glish: A Teacher?s Guide to Interference and Other
Problems. Cambridge Handbooks for Language
Teachers. Cambridge University Press.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proc. of the Eighth Workshop on Inno-
vative Use of NLP for Building Educational Applica-
tions, Atlanta, GA, USA, June. Association for Com-
putational Linguistics.
Gideon Toury. 1995. Descriptive Translation Studies
and beyond. John Benjamins, Amsterdam / Philadel-
phia.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc.
of HLT-NAACL, pages 173?180, Edmonton, Canada,
June. Association for Computational Linguistics.
286
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proc. of
the Workshop on Cognitive Aspects of Computational
Language Acquisition, pages 9?16, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. of ACL.
Vered Volansky, Noam Ordan, and Shuly Wintner. forth-
coming. On the features of translationese. Literary
and Linguistic Computing.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. In Proc.
of the Australasian Language Technology Association
Workshop, pages 53?61, Sydney, Australia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proc. of EMNLP, pages 1600?1610, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
287
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70?77,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The CMU Machine Translation Systems at WMT 2013:
Syntax, Synthetic Translation Options, and Pseudo-References
Waleed Ammar Victor Chahuneau Michael Denkowski Greg Hanneman
Wang Ling Austin Matthews Kenton Murray Nicola Segall Yulia Tsvetkov
Alon Lavie Chris Dyer?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
?Corresponding author: cdyer@cs.cmu.edu
Abstract
We describe the CMU systems submit-
ted to the 2013 WMT shared task in ma-
chine translation. We participated in three
language pairs, French?English, Russian?
English, and English?Russian. Our
particular innovations include: a label-
coarsening scheme for syntactic tree-to-
tree translation and the use of specialized
modules to create ?synthetic translation
options? that can both generalize beyond
what is directly observed in the parallel
training data and use rich source language
context to decide how a phrase should
translate in context.
1 Introduction
The MT research group at Carnegie Mellon Uni-
versity?s Language Technologies Institute par-
ticipated in three language pairs for the 2013
Workshop on Machine Translation shared trans-
lation task: French?English, Russian?English,
and English?Russian. Our French?English sys-
tem (?3) showcased our group?s syntactic sys-
tem with coarsened nonterminal types (Hanne-
man and Lavie, 2011). Our Russian?English and
English?Russian system demonstrate a new multi-
phase approach to translation that our group is us-
ing, in which synthetic translation options (?4)
to supplement the default translation rule inven-
tory that is extracted from word-aligned training
data. In the Russian-English system (?5), we used
a CRF-based transliterator (Ammar et al, 2012)
to propose transliteration candidates for out-of-
vocabulary words, and used a language model
to insert or remove common function words in
phrases according to an n-gram English language
model probability. In the English?Russian system
(?6), we used a conditional logit model to predict
the most likely inflectional morphology of Rus-
sian lemmas, conditioning on rich source syntac-
tic features (?6.1). In addition to being able to
generate inflected forms that were otherwise unob-
served in the parallel training data, the translations
options generated in this matter had features re-
flecting their appropriateness given much broader
source language context than usually would have
been incorporated in current statistical MT sys-
tems.
For our Russian?English system, we addition-
ally used a secondary ?pseudo-reference? transla-
tion when tuning the parameters of our Russian?
English system. This was created by automatically
translating the Spanish translation of the provided
development data into English. While the output
of an MT system is not always perfectly gram-
matical, previous work has shown that secondary
machine-generated references improve translation
quality when only a single human reference is
available when BLEU is used as an optimization
criterion (Madnani, 2010; Dyer et al, 2011).
2 Common System Components
The decoder infrastructure we used was cdec
(Dyer et al, 2010). Only the constrained data
resources provided for the shared task were used
for training both the translation and language
models. Word alignments were generated us-
ing the Model 2 variant described in Dyer et al
(2013). Language models used modified Kneser-
Ney smoothing estimated using KenLM (Heafield,
2011). Translation model parameters were dis-
criminatively set to optimize BLEU on a held-out
development set using an online passive aggres-
sive algorithm (Eidelman, 2012) or, in the case of
70
the French?English system, using the hypergraph
MERT algorithm and optimizing towards BLEU
(Kumar et al, 2009). The remainder of the paper
will focus on our primary innovations in the vari-
ous system pairs.
3 French-English Syntax System
Our submission for French?English is a tree-to-
tree translation system that demonstrates several
innovations from group?s research on SCFG-based
translation.
3.1 Data Selection
We divided the French?English training data into
two categories: clean data (Europarl, News Com-
mentary, UN Documents) totaling 14.8 million
sentence pairs, and web data (Common Crawl,
Giga-FrEn) totaling 25.2 million sentence pairs.
To reduce the volume of data used, we filtered
non-parallel and other unhelpful segments accord-
ing to the technique described by Denkowski et al
(2012). This procedure uses a lexical translation
model learned from just the clean data, as well as
source and target n-gram language models to com-
pute the following feature scores:
? French and English 4-gram log likelihood (nor-
malized by length);
? French?English and English?French lexical
translation log likelihood (normalized by
length); and,
? Fractions of aligned words under the French?
English and English?French models.
We pooled previous years? WMT news test sets
to form a reference data set. We computed the
same features. To filter the web data, we retained
only sentence for which each feature score was
no lower than two standard deviations below the
mean on the reference data. This reduced the web
data from 25.2 million to 16.6 million sentence
pairs. Parallel segments from all parts of the data
that were blank on either side, were longer than 99
tokens, contained a token of more than 30 charac-
ters, or had particularly unbalanced length ratios
were also removed. After filtering, 30.9 million
sentence pairs remained for rule extraction: 14.4
million from the clean data, and 16.5 million from
the web data.
3.2 Preprocessing and Grammar Extraction
Our French?English system uses parse trees in
both the source and target languages, so tokeniza-
tion in this language pair was carried out to match
the tokenizations expected by the parsers we used
(English data was tokenized with the Stanford to-
kenizer for English and an in-house tokenizer for
French that targets the tokenization used by the
Berkeley French parser). Both sides of the par-
allel training data were parsed using the Berkeley
latent variable parser.
Synchronous context-free grammar rules were
extracted from the corpus following the method of
Hanneman et al (2011). This decomposes each
tree pair into a collection of SCFG rules by ex-
haustively identifying aligned subtrees to serve as
rule left-hand sides and smaller aligned subtrees
to be abstracted as right-hand-side nonterminals.
Basic subtree alignment heuristics are similar to
those by Galley et al (2006), and composed rules
are allowed. The computational complexity is held
in check by a limit on the number of RHS elements
(nodes and terminals), rather than a GHKM-style
maximum composition depth or Hiero-style max-
imum rule span. Our rule extractor also allows
?virtual nodes,? or the insertion of new nodes in
the parse tree to subdivide regions of flat struc-
ture. Virtual nodes are similar to the A+B ex-
tended categories of SAMT (Zollmann and Venu-
gopal, 2006), but with the added constraint that
they may not conflict with the surrounding tree
structure.
Because the SCFG rules are labeled with non-
terminals composed from both the source and tar-
get trees, the nonterminal inventory is quite large,
leading to estimation difficulties. To deal with
this, we automatically coarsening the nonterminal
labels (Hanneman and Lavie, 2011). Labels are
agglomeratively clustered based on a histogram-
based similarity function that looks at what tar-
get labels correspond to a particular source label
and vice versa. The number of clusters used is de-
termined based on spikes in the distance between
successive clustering iterations, or by the number
of source, target, or joint labels remaining. Start-
ing from a default grammar of 877 French, 2580
English, and 131,331 joint labels, we collapsed
the label space for our WMT system down to 50
French, 54 English, and 1814 joint categories.1
1Selecting the stopping point still requires a measure of
intuition. The label set size of 1814 chosen here roughly cor-
responds to the number of joint labels that would exist in the
grammar if virtual nodes were not included. This equivalence
has worked well in practice in both internal and published ex-
periments on other data sets (Hanneman and Lavie, 2013).
71
Extracted rules each have 10 features associated
with them. For an SCFG rule with source left-
hand side `s, target left-hand side `t, source right-
hand side rs, and target right-hand side rt, they
are:
? phrasal translation log relative frequencies
log f(rs | rt) and log f(rt | rs);
? labeling relative frequency log f(`s, `t | rs, rt)
and generation relative frequency
log f(rs, rt | `s, `t);
? lexical translation log probabilities log plex(rs |
rt) and log plex(rt | rs), defined similarly to
Moses?s definition;
? a rarity score exp( 1c )?1exp(1)?1 for a rule with frequency
c (this score is monotonically decreasing in the
rule frequency); and,
? three binary indicator features that mark
whether a rule is fully lexicalized, fully abstract,
or a glue rule.
Grammar filtering. Even after collapsing la-
bels, the extracted SCFGs contain an enormous
number of rules ? 660 million rule types from just
under 4 billion extracted instances. To reduce the
size of the grammar, we employ a combination of
lossless filtering and lossy pruning. We first prune
all rules to select no more than the 60 most fre-
quent target-side alternatives for any source RHS,
then do further filtering to produce grammars for
each test sentence:
? Lexical rules are filtered to the sentence level.
Only phrase pairs whose source sides match the
test sentence are retained.
? Abstract rules (whose RHS are all nontermi-
nals) are globally pruned. Only the 4000 most
frequently observed rules are retained.
? Mixed rules (whose RHS are a mix of terminals
and nonterminals) must match the test sentence,
and there is an additional frequency cutoff.
After this filtering, the number of completely lex-
ical rules that match a given sentence is typically
low, up to a few thousand rules. Each fully ab-
stract rule can potentially apply to every sentence;
the strict pruning cutoff in use for these rules is
meant to focus the grammar to the most important
general syntactic divergences between French and
English. Most of the latitude in grammar pruning
comes from adjusting the frequency cutoff on the
mixed rules since this category of rule is by far the
most common type. We conducted experiments
with three different frequency cutoffs: 100, 200,
and 500, with each increase decreasing the gram-
mar size by 70?80 percent.
3.3 French?English Experiments
We tuned our system to the newstest2008 set of
2051 segments. Aside from the official new-
stest2013 test set (3000 segments), we also col-
lected test-set scores from last year?s newstest2012
set (3003 segments). Automatic metric scores
are computed according to BLEU (Papineni et al,
2002), METEOR (Denkowski and Lavie, 2011),
and TER (Snover et al, 2006), all computed ac-
cording to MultEval v. 0.5 (Clark et al, 2011).
Each system variant is run with two independent
MERT steps in order to control for optimizer in-
stability.
Table 1 presents the results, with the metric
scores averaged over both MERT runs. Quite in-
terestingly, we find only minor differences in both
tune and test scores despite the large differences in
filtered/pruned grammar size as the cutoff for par-
tially abstract rules increases. No system is fully
statistically separable (at p < 0.05) from the oth-
ers according to MultEval?s approximate random-
ization algorithm. The closest is the variant with
cutoff 200, which is generally judged to be slightly
worse than the other two. METEOR claims full
distinction on the 2013 test set, ranking the sys-
tem with the strictest grammar cutoff (500) best.
This is the version that we ultimately submitted to
the shared translation task.
4 Synthetic Translation Options
Before discussing our Russian?English and
English?Russian systems, we introduce the
concept of synthetic translation options, which
we use in these systems. We provide a brief
overview here; for more detail, we refer the reader
to Tsvetkov et al (2013).
In language pairs that are typologically similar,
words and phrases map relatively directly from
source to target languages, and the standard ap-
proach to learning phrase pairs by extraction from
parallel data can be very effective. However, in
language pairs in which individual source lan-
guage words have many different possible transla-
tions (e.g., when the target language word could
have many different inflections or could be sur-
rounded by different function words that have no
72
Dev (2008) Test (2012) Test (2013)
System BLEU METR TER BLEU METR TER BLEU METR TER
Cutoff 100 22.52 31.44 59.22 27.73 33.30 53.25 28.34 * 33.19 53.07
Cutoff 200 22.34 31.40 59.21 * 27.33 33.26 53.23 * 28.05 * 33.07 53.16
Cutoff 500 22.80 31.64 59.10 27.88 * 33.58 53.09 28.27 * 33.31 53.13
Table 1: French?English automatic metric scores for three grammar pruning cutoffs, averaged over two
MERT runs each. Scores that are statistically separable (p < 0.05) from both others in the same column
are marked with an asterisk (*).
direct correspondence in the source language), we
can expect the standard phrasal inventory to be
incomplete, except when very large quantities of
parallel data are available or for very frequent
words. There simply will not be enough exam-
ples from which to learn the ideal set of transla-
tion options. Therefore, since phrase based trans-
lation can only generate input/output word pairs
that were directly observed in the training corpus,
the decoder?s only hope for producing a good out-
put is to find a fluent, meaning-preserving transla-
tion using incomplete translation lexicons. Syn-
thetic translation option generation seeks to fill
these gaps using secondary generation processes
that produce possible phrase translation alterna-
tives that are not directly extractable from the
training data. By filling in gaps in the transla-
tion options used to construct the sentential trans-
lation search space, global discriminative transla-
tion models and language models can be more ef-
fective than they would otherwise be.
From a practical perspective, synthetic transla-
tion options are attractive relative to trying to build
more powerful models of translation since they
enable focus on more targeted translation prob-
lems (for example, transliteration, or generating
proper inflectional morphology for a single word
or phrase). Since they are translation options and
not complete translations, many of them may be
generated.
In the following system pairs, we use syn-
thetic translation options to augment hiero gram-
mar rules learned in the usual way. The synthetic
phrases we include augment draw from several
sources:
? transliterations of OOV Russian words (?5.3);
? English target sides with varied function words
(for example, given a phrase that translates into
cat we procedure variants like the cat, a cat and
of the cat); and,
? when translating into Russian, we generate
phrases by first predicting the most likely Rus-
sian lemma for a source word or phrase, and
then, conditioned on the English source context
(including syntactic and lexical features), we
predict the most likely inflection of the lemma
(?6.1).
5 Russian?English System
5.1 Data
We used the same parallel data for both the
Russian?English and English Russian systems.
Except for filtering to remove sentence pairs
whose log length ratios were statistical outliers,
we only filtered the Common Crawl corpus to re-
move sentence pairs with less than 50% concentra-
tion of Cyrillic characters on the Russian side. The
remaining data was tokenized and lower-cased.
For language models, we trained 4-gram Markov
models using the target side of the bitext and any
available monolingual data (including Gigaword
for English). Additionally, we trained 7-gram lan-
guage models using 600-class Brown clusters with
Witten-Bell smoothing.2
5.2 Baseline System
Our baseline Russian?English system is a hierar-
chical phrase-based translation model as imple-
mented in cdec (Chiang, 2007; Dyer et al, 2010).
SCFG translation rules that plausibly match each
sentence in the development and deftest sets were
extracted from the aligned parallel data using the
suffix array indexing technique of Lopez (2008).
A Russian morphological analyzer was used to
lemmatize the training, development, and test
data, and the ?noisier channel? translation ap-
proach of Dyer (2007) was used in the Russian?
English system to let unusually inflected surface
forms back off to per-lemma translations.
2http://www.ark.cs.cmu.edu/cdyer/ru-600/.
73
5.3 Synthetic Translations: Transliteration
Analysis revealed that about one third of the un-
seen Russian tokens in the development set con-
sisted of named entities which should be translit-
erated. We used individual Russian-English word
pairs in Wikipedia parallel headlines 3 to train a
linear-chained CRF tagger which labels each char-
acter in the Russian token with a sequence of zero
or more English characters (Ammar et al, 2012).
Since Russian names in the training set were in
nominative case, we used a simple rule-based mor-
phological generator to produce possible inflec-
tions and filtered out the ones not present in the
Russian monolingual corpus. At decoding, un-
seen Russian tokens are fed to the transliterator
which produces the most probable 20 translitera-
tions. We add a synthetic translation option for
each of the transliterations with four features: an
indicator feature for transliterations, the CRF un-
normalized score, the trigram character-LM log-
probability, and the divergence from the average
length-ratio between an English name and its Rus-
sian transliteration.
5.4 Synthetic Translations: Function Words
Slavic languages like Russian have a large number
of different inflected forms for each lemma, repre-
senting different cases, tenses, and aspects. Since
our training data is rather limited relative to the
number of inflected forms that are possible, we use
an English language model to generate a variety
of common function word contexts for each con-
tent word phrase. These are added to the phrase
table with a feature indicating that they were not
actually observed in the training data, but rather
hallucinated using SRILM?s disambig tool.
5.5 Summary
Table 5.5 summarizes our Russian-English trans-
lation results. In the submitted system, we addi-
tionally used MBR reranking to combine the 500-
best outputs of our system, with the 500-best out-
puts of a syntactic system constructed similarly to
the French?English system.
6 English?Russian System
The bilingual training data was identical to the
filtered data used in the previous section. Word
alignments was performed after lemmatizing the
3We contributed the data set to the shared task participants
at http://www.statmt.org/wmt13/wiki-titles.ru-en.tar.gz
Table 2: Russian-English summary.
Condition BLEU
Baseline 30.8
Function words 30.9
Transliterations 31.1
Russian side of the training corpus. An unpruned,
modified Kneser-Ney smoothed 4-gram language
model (Chen and Goodman, 1996) was estimated
from all available Russian text (410 million words)
using the KenLM toolkit (Heafield et al, 2013).
A standard hierarchical phrase-based system
was trained with rule shape indicator features, ob-
tained by replacing terminals in translation rules
by a generic symbol. MIRA training was per-
formed to learn feature weights.
Additionally, word clusters (Brown et al, 1992)
were obtained for the complete monolingual Rus-
sian data. Then, an unsmoothed 7-gram language
model was trained on these clusters and added as
a feature to the translation system. Indicator fea-
tures were also added for each cluster and bigram
cluster occurence. These changes resulted in an
improvement of more than a BLEU point on our
held-out development set.
6.1 Predicting Target Morphology
We train a classifier to predict the inflection of
each Russian word independently given the cor-
responding English sentence and its word align-
ment. To do this, we first process the Russian
side of the parallel training data using a statisti-
cal morphological tagger (Sharoff et al, 2008) to
obtain lemmas and inflection tags for each word
in context. Then, we obtain part-of-speech tags
and dependency parses of the English side of the
parallel data (Martins et al, 2010), as well as
Brown clusters (Brown et al, 1992). We extract
features capturing lexical and syntactical relation-
ships in the source sentence and train structured
linear logistic regression models to predict the tag
of each English word independently given its part-
of-speech.4 In practice, due to the large size of
the corpora and of the feature space dimension,
we were only able to use about 10% of the avail-
able bilingual data, sampled randomly from the
Common Crawl corpus. We also restricted the
4We restrict ourselves to verbs, nouns, adjectives, adverbs
and cardinals since these open-class words carry most inflec-
tion in Russian.
74
??? ???????? ???????? ???? ?? ?? ?????????
she had attempted to cross the road on her bike
PRP   VBD         VBN          TO    VB       DT     NN    IN  PRP$   NN
nsubj
aux
xcomp
aux
????????_V*+*mis/sfm/e
C50   C473        C28          C8    C275   C37   C43  C82 C94   C331
Figure 1: The classifier is trained to predict the verbal inflection mis-sfm-e based on the linear and
syntactic context of the words aligned to the Russian word; given the stem ???????? (pytat?sya), this
inflection paradigm produces the observed surface form ???????? (pytalas?).
set of possible inflections for each word to the set
of tags that were observed with its lemma in the
full monolingual training data. This was neces-
sary because of our choice to use a tagger, which
is not able to synthesize surface forms for a given
lemma-tag pair.
We then augment the standard hierarchical
phrase-base grammars extracted for the baseline
systems with new rules containing inflections not
necessarily observed in the parallel training data.
We start by training a non-gappy phrase transla-
tion model on the bilingual data where the Russian
has been lemmatized.5 Then, before translating an
English sentence, we extract translation phrases
corresponding to this specific sentence and re-
inflect each word in the target side of these phrases
using the classifier with features extracted from
the source sentence words and annotations. We
keep the original phrase-based translation features
and add the inflection score predicted by the clas-
sifier as well as indicator features for the part-of-
speech categories of the re-inflected words.
On a held-out development set, these synthetic
phrases produce a 0.3 BLEU point improvement.
Interestingly, the feature weight learned for using
these phrases is positive, indicating that useful in-
flections might be produced by this process.
7 Conclusion
The CMU systems draws on a large number of
different research directions. Techniques such as
MBR reranking and synthetic phrases allow dif-
ferent contributors to focus on different transla-
5We keep intact words belonging to non-predicted cate-
gories.
tion problems that are ultimately recombined into
a single system. Our performance, in particular,
on English?Russian machine translation was quite
satisfying, we attribute our biggest gains in this
language pair to the following:
? Our inflection model that predicted how an En-
glish word ought best be translated, given its
context. This enabled us to generate forms that
were not observed in the parallel data or would
have been rare independent of context with pre-
cision.
? Brown cluster language models seem to be quite
effective at modeling long-range morphological
agreement patterns quite reliably.
Acknowledgments
We sincerely thank the organizers of the work-
shop for their hard work, year after year, and the
reviewers for their careful reading of the submit-
ted draft of this paper. This research work was
supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533,
by the National Science Foundation under grant
IIS-0915327, by a NPRP grant (NPRP 09-1140-
1-177) from the Qatar National Research Fund (a
member of the Qatar Foundation), and by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
The statements made herein are solely the respon-
sibility of the authors.
75
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In NEWS workshop at ACL.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computional Linguistics, 18(4):467?479.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310?318, Santa Cruz, California, USA,
June. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Crontrolling for
optimizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers, pages 176?181, Portland,
Oregon, USA, June.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, UK, July.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The cmu-avenue french-english translation
system. In Proceedings of the NAACL 2012 Work-
shop on Statistical Machine Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English translation system. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proc. of NAACL.
Chris Dyer. 2007. The ?noiser channel?: Translation
from morphologically complex languages. In Pro-
ceedings of WMT.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the ACL, pages 961?968, Sydney, Australia,
July.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Proceedings of SSST-5: Fifth Work-
shop on Syntax, Semantics, and Structure in Statis-
tical Translation, pages 98?106, Portland, Oregon,
USA, June.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL-HLT
2013, pages 288?297, Atlanta, Georgia, USA, June.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proceedings of SSST-
5: Fifth Workshop on Syntax, Semantics, and Struc-
ture in Statistical Translation, pages 135?144, Port-
land, Oregon, USA, June.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, Sofia, Bulgaria,
August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, Scotland, UK, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efficient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In Proc. of
ACL-IJCNLP.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proc. of COLING.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. the-
sis, Department of Computer Science, University of
Maryland College Park.
Andre? F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and Ma?rio A. T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
76
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating a russian tagset. In Proc. of LREC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Seventh Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Batia. 2013. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of the Eighth Workshop on
Statistical Machine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, New
York, USA, June.
77
