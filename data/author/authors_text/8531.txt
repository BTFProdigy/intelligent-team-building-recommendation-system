Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 369?379, Prague, June 2007. c?2007 Association for Computational Linguistics
Detecting Compositionality of Verb-Object Combinations using Selectional
Preferences
Diana McCarthy
University of Sussex
Falmer, East Sussex
BN1 9QH, UK
dianam@sussex.ac.uk
Sriram Venkatapathy
International Institute
of Information Technology
Hyderabad, India
sriram@research.iiit.ac.in
Aravind K. Joshi
University of Pennsylvania,
Philadelphia
PA, USA.
joshi@linc.cis.upenn.edu
Abstract
In this paper we explore the use of se-
lectional preferences for detecting non-
compositional verb-object combinations. To
characterise the arguments in a given gram-
matical relationship we experiment with
three models of selectional preference. Two
use WordNet and one uses the entries from
a distributional thesaurus as classes for rep-
resentation. In previous work on selectional
preference acquisition, the classes used for
representation are selected according to the
coverage of argument tokens rather than be-
ing selected according to the coverage of
argument types. In our distributional the-
saurus models and one of the methods us-
ing WordNet we select classes for represent-
ing the preferences by virtue of the number
of argument types that they cover, and then
only tokens under these classes which are
representative of the argument head data are
used to estimate the probability distribution
for the selectional preference model. We
demonstrate a highly significant correlation
between measures which use these ?type-
based? selectional preferences and composi-
tionality judgements from a data set used in
previous research. The type-based models
perform better than the models which use to-
kens for selecting the classes. Furthermore,
the models which use the automatically ac-
quired thesaurus entries produced the best
results. The correlation for the thesaurus
models is stronger than any of the individ-
ual features used in previous research on the
same dataset.
1 Introduction
Characterising the semantic behaviour of phrases in
terms of compositionality has particularly attracted
attention in recent years (Lin, 1999; Schone and Ju-
rafsky, 2001; Bannard, 2002; Bannard et al, 2003;
Baldwin et al, 2003; McCarthy et al, 2003; Ban-
nard, 2005; Venkatapathy and Joshi, 2005). Typi-
cally the phrases are putative multiwords and non-
compositionality is viewed as an important feature
of many such ?words with spaces? (Sag et al, 2002).
For applications such as paraphrasing, information
extraction and translation, it is essential to take the
words of non-compositional phrases together as a
unit because the meaning of a phrase cannot be ob-
tained straightforwardly from the constituent words.
In this work we are investigate methods of deter-
mining semantic compositionality of verb-object 1
combinations on a continuum following previous
research in this direction (McCarthy et al, 2003;
Venkatapathy and Joshi, 2005).
Much previous research has used a combination
of statistics and distributional approaches whereby
distributional similarity is used to compare the con-
stituents of the multiword with the multiword itself.
In this paper, we will investigate the use of selec-
tional preferences of verbs. We will use the pref-
erences to find atypical verb-object combinations as
we anticipate that such combinations are more likely
to be non-compositional.
1We use object to refer to direct objects.
369
Selectional preferences of predicates have been
modelled using the man-made thesaurus Word-
Net (Fellbaum, 1998), see for example (Resnik,
1993; Li and Abe, 1998; Abney and Light, 1999;
Clark and Weir, 2002). There are also distribu-
tional approaches which use co-occurrence data to
cluster distributionally similar words together. The
cluster output can then be used as classes for se-
lectional preferences (Pereira et al, 1993), or one
can directly use frequency information from distri-
butionally similar words for smoothing (Grishman
and Sterling, 1994).
We used three different types of probabilistic
models, which vary in the classes selected for rep-
resentation over which the probability distribution of
the argument heads 2 is estimated. Two use WordNet
and the other uses the entries in a thesaurus of distri-
butionally similar words acquired automatically fol-
lowing (Lin, 1998). The first method is due to Li and
Abe (1998). The classes over which the probabil-
ity distribution is calculated are selected according
to the minimum description length principle (MDL)
which uses the argument head tokens for finding the
best classes for representation. This method has pre-
viously been tried for modelling compositionality of
verb-particle constructions (Bannard, 2002).
The other two methods (we refer to them as ?type-
based?) also calculate a probability distribution us-
ing argument head tokens but they select the classes
over which the distribution is calculated using the
number of argument head types (of a verb in a cor-
pus) in a given class, rather than the number of ar-
gument head tokens in contrast to previous WordNet
models (Resnik, 1993; Li and Abe, 1998; Clark and
Weir, 2002). For example, if the object slot of the
verb park contains the argument heads { car, car,
car, car, van, jeep } then the type-based models use
the word type ?car? only once when determining the
classes over which the probability distribution is to
be estimated. Classes are selected which maximise
the number of types that they cover, rather than the
number of tokens. This is done to avoid the selec-
tional preferences being heavily influenced by noise
from highly frequent arguments which may be poly-
semous and some or all of their meanings may not be
2Argument heads are the nouns occurring in the object slot
of the target verb.
semantically related to the ?prototypical? arguments
of the verb. For example car has a gondola sense in
WordNet.
The third method uses entries in a distributional
thesaurus rather than classes from WordNet. The en-
tries used as classes for representation are selected
by virtue of the number of argument types they en-
compass. As with the WordNet models, the tokens
are used to estimate a probability distribution over
these entries.
In the next section, we discuss related work on
identifying compositionality. In section 3, we de-
scribe the methods we are using for acquiring our
models of selectional preference. In section 4, we
test our models on a dataset used in previous re-
search. We compare the three types of models in-
dividually and also investigate the best performing
model when used in combination with other features
used in previous research. We conclude in section 5.
2 Related Work
Most previous work using distributional approaches
to compositionality either contrasts distributional
information of candidate phrases with constituent
words (Schone and Jurafsky, 2001; Bannard et al,
2003; Baldwin et al, 2003; McCarthy et al, 2003)
or uses distributionally similar words to detect non-
productive phrases (Lin, 1999).
Lin (1999) used his method (Lin, 1998) for au-
tomatic thesaurus construction. He identified can-
didate phrases involving several open-class words
output from his parser and filtered these by the log-
likelihood statistic. Lin proposed that if there is a
phrase obtained by substitution of either the head
or modifier in the phrase with a ?nearest neighbour?
from the thesaurus then the mutual information of
this and the original phrase must be significantly dif-
ferent for the original phrase to be considered non-
compositional. He evaluated the output manually.
As well as distributional similarity, researchers
have used a variety of statistics as indicators of
non-compositionality (Blaheta and Johnson, 2001;
Krenn and Evert, 2001). Fazly and Stevenson (2006)
use statistical measures of syntactic behaviour to
gauge whether a verb and noun combination is likely
to be a idiom. Although they are not specifically
detecting compositionality, there is a strong corre-
370
lation between syntactic rigidity and semantic id-
iosyncrasy.
Venkatapathy and Joshi (2005) combine differ-
ent statistical and distributional methods using sup-
port vector machines (SVMs) for identifying non-
compositional verb-object combinations. They ex-
plored seven features as measures of compositional-
ity:
1. frequency
2. pointwise mutual information (Church and
Hanks, 1990),
3. least mutual information difference with simi-
lar collocations, based on (Lin, 1999) and us-
ing Lin?s thesaurus (Lin, 1998) for obtaining
the similar collocations.
4. The distributed frequency of an object, which
takes an average of the frequency of occurrence
with an object over all verbs occurring with the
object above a threshold.
5. distributed frequency of an object, using the
verb, which considers the similarity between
the target verb and the verbs occurring with the
target object above the specified threshold.
6. a latent semantic approach (LSA) based
on (Schu?tze, 1998; Baldwin et al, 2003) and
considering the dissimilarity of the verb-object
pair with its constituent verb
7. the same LSA approach, but considering the
similarity of the verb-object pair with the ver-
bal form of the object (to capture support verb
constructions e.g. give a smile
Venkatapathy and Joshi (2005) produced a dataset
of verb-object pairs with human judgements of com-
positionality. We say more about this dataset and
Venkatapathy and Joshi?s results in section 4 since
we use the dataset for our experiments.
In this paper, we investigate the use of selec-
tional preferences to detect compositionality. Ban-
nard (2002) did some pioneering work to try and
establish a link between the compositionality of
verb particle constructions and the selectional pref-
erences of the multiword and its constituent verb.
His results were hampered by models based on (Li
and Abe, 1998) which involved rather uninforma-
tive models at the roots of WordNet. There are
several reasons for this. The classes for the model
are selected using MDL by compromising between a
simple model with few classes and one which ex-
plains the data well. The models are particularly
affected by the quantity of data available (Wagner,
2002). Also noise from frequent but idiosyncratic or
polysemous arguments weakens the signal. There
is scope for experimenting with other approaches
such as (Clark and Weir, 2002), however, we feel
a type-based approach is worthwhile to avoid the
noise introduced from frequent but polysemous ar-
guments and bias from highly frequent arguments
which might be part of a multiword rather than a pro-
totypical argument of the predicate in question, for
example eat hat. In contrast to Bannard, our experi-
ments are with verb-object combinations rather than
verb particle constructions. We compare Li and Abe
models with WordNet models which use the num-
ber of argument types to obtain the classes for rep-
resentation of the selectional preferences. In addi-
tion to experiments with these WordNet models, we
propose models using entries in distributional the-
sauruses for representing preferences.
3 Three Methods for Acquiring Selectional
Preferences
All models were acquired from verb-object data ex-
tracted using the RASP parser (Briscoe and Carroll,
2002) from the 90 million words of written English
from the BNC (Leech, 1992). We extracted verb and
common noun tuples where the noun is the argu-
ment head of the object relation. The parser was also
used to extract the grammatical relation data used
for acquisition of the thesaurus described below in
section 3.3.
3.1 TCMs
This approach is a reimplementation of Li and Abe
(1998). Each selectional preference model (referred
to as a tree cut model, or TCM) comprises a set of
disjunctive noun classes selected from all the pos-
sibilities in the WordNet hyponym hierarchy 3 us-
ing MDL (Rissanen, 1978). The TCM covers all the
3We use WordNet version 2.1 for the work in this paper.
371
noun senses in the WordNet hierarchy and is associ-
ated with a probability distribution over these noun
senses in the hierarchy reflecting the argument head
data occurring in the given grammatical relationship
with the specified verb. MDL finds the classes in the
TCM by considering the cost measured in bits of de-
scribing both the model and the argument head data
encoded in the model. A compromise is made by
having as simple a model as possible using classes
further up the hierarchy whilst also providing a good
model for the set of argument head tokens (TK).
The classes are selected by recursing from the top
of the WordNet hierarchy comparing the cost (or de-
scription length) of using the mother class to the cost
of using the hyponym daughter classes. In any path,
the mother is preferred unless using the daughters
would reduce the cost. If using the daughters for the
model is less costly than the mother then the recur-
sion continues to compare the cost of the hyponyms
beneath.
The cost (or description length) for a set of classes
is calculated as the model description length (mdl)
and the data description length (ddl) 4 :-
mdl + ddl
k
2 ? log |TK| + ?
?
tk?TK log p(tk) (1)
k, is the number of WordNet classes being cur-
rently considered for the TCM minus one. The MDL
method uses the size of TK on the assumption that
a larger dataset warrants a more detailed model. The
cost of describing the argument head data is calcu-
lated using the log of the probability estimate from
the classes currently being considered for the model.
The probability estimate for a class being considered
for the model is calculated using the cumulative fre-
quency of all the hyponym nouns under that class
that occur in TK , divided by the number of noun
senses that these nouns have, to account for their
polysemy. This cumulative frequency is also divided
by the total number of noun hyponyms under that
class in WordNet to obtain a smoothed estimate for
all nouns under the class. The probability of the
class is obtained by dividing this frequency estimate
by the total frequency of the argument heads. The
algorithm is described fully by Li and Abe (1998).
4See (Li and Abe, 1998) for a full explanation.
0.17
distancestreet
vancar
mile
street car distancelane corner
0.18 0.10 0.17 0.03
entity
physical_
entityentity
abstract_
way gondola
Example nouns
hyponym classes
locationvehicle
tcm
self?propelled
Figure 1: portion of the TCM for the objects of park.
A small portion of the TCM for the object slot of
park is shown in figure 1. WordNet classes are dis-
played in boxes with a label which best reflects the
meaning of the class. The probability estimates are
shown for the classes on the TCM. Examples of the
argument head data are displayed below the Word-
Net classes with dotted lines indicating membership
at a hyponym class beneath these classes. We can-
not show the full TCM due to lack of space, but we
show some of the higher probability classes which
cover some typical nouns that occur as objects of
park. Note that probability under the classes ab-
stract entity, way and location arise because of a
systematic parsing error where adverbials such as
distance in park illegally some distance from the
railway station are identified by the parser as ob-
jects. Systematic noise from the parser has an im-
pact on all the selectional preference models de-
scribed in this paper.
3.2 WNPROTOs
We propose a method of acquiring selectional pref-
erences which instead of covering all the noun
senses in WordNet, just gives a probability distribu-
tion over a portion of prototypical classes, we refer
to these models as WNPROTOs. A WNPROTO con-
sists of classes within the noun hierarchy which have
the highest proportion of word types occurring in
the argument head data, rather than using the num-
ber of tokens, or frequency, as is used for the TCMs.
This allows less frequent, but potentially informa-
tive arguments to have some bearing on the models
acquired to reduce the impact of highly frequent but
polysemous arguments. We then used the frequency
data to populate these selected classes.
372
The classes (C) in the WNPROTO are selected
from those which include at least a threshold of 2
argument head types 5 occurring in the training data.
Each argument head in the training data is disam-
biguated according to whichever of the WordNet
classes it occurs at or under which has the highest
?type ratio?. Let TY be the set of argument head
types in the object slot of the verb for which we are
acquiring the preference model. The type ratio for a
class (c) is the ratio of noun types (ty ? TY ) occur-
ring in the training data also listed at or beneath that
class in WordNet to the total number of noun types
listed at or beneath that particular class in WordNet
(wnty ? c). The argument types attested in thetraining data are divided by the number of Word-
Net classes that the noun (classes(ty)) belongs to,
to account for polysemy in the training data.
type ratio(c) =
?
ty?TY ?c
1
|classes(ty)|
|wnty ? c|
(2)
If more than one class has the same type ratio then
the argument is not used for calculating the probabil-
ity of the preference model. In this way, only argu-
ments that can be disambiguated are used for calcu-
lating the probability distribution. The advantage of
using the type ratio to determine the classes used to
represent the model and to disambiguate the argu-
ments is that it prevents high frequency verb noun
combinations from masking the information from
prototypical but low frequency arguments. We wish
to use classes which are as representative of the ar-
gument head types as possible to help detect when
an argument head is not related to these classes and
is therefore more likely to be non-compositional.
For example, the class motor vehicle is selected
for the WNPROTO model of the object slot of park
even though there are 5 meanings of car in WordNet
including elevator car and gondola. There are 174
occurrences of car which overwhelms the frequency
of the other objects (e.g. van 11, vehicle 8) but by
looking for classes with a high proportion of types
(rather than word tokens) car is disambiguated ap-
propriately and the class motor vehicle is selected
for representation.
5We have experimented with a threshold of 3 and obtained
similar results.
0.03
0.04
tanker
boat
pram.61
car
0.05
van
caravan
entity
physical_
entity
Example nouns
hyponym classes
vehicle
self?propelled
transport
vehicle
wheeled
model
motor
vehicle caravan
  classes in
Figure 2: Part of WNPROTO for the object slot of
park
The relative frequency of each class is obtained
from the set of disambiguated argument head tokens
and used to provide the probability distribution over
this set of classes. Note that in WNPROTO, classes
can be subsumed by others in the hyponym hierar-
chy. The probability assigned to a class is appli-
cable to any descendants in the hyponym hierarchy,
except those within any hyponym classes within the
WNPROTO. The algorithm for selecting C and cal-
culating the probability distribution is shown as Al-
gorithm 1. Note that we use brackets for comments.
In figure 2 we show a small portion of the WN-
PROTO for park. Again, WordNet classes are dis-
played in boxes with a label which best reflects the
meaning of the class. The probability estimates are
shown in the boxes for all the classes included in
the WNPROTO. The classes in the WNPROTO model
are shown with dashed lines. Examples of the ar-
gument head data are displayed below the WordNet
classes with dotted lines indicating membership at
a hyponym class beneath these classes. We cannot
show the full WNPROTO due to lack of space, but
we show some of the classes with higher probability
which cover some typical nouns that occur as objects
of park.
373
Algorithm 1 WNPROTO algorithm
C = (){classes in WNPROTO}
D = () {disambiguated ty ? TY }
fD = 0 {frequency of disambiguated items}
TY = argument head types {nouns occurring as objects of verb, with associated frequencies}
C1 ? WordNet
where |ty ? TY occurring in c ? C1| > 1
for all ty ? TY do
find c ? classes(ty) ? C1 where c = argmaxc typeratio(c)
if c & c /? C then
add c to C
add ty ? c to D {Disambiguated ty with c}
end if
end for
for all c ? C do
if |ty ? c ? D| > 1 then
fD = fD + frequency(ty){sum frequencies of types under classes to be used in model}
else
remove c from C {classes with less than two disambiguated nouns are removed}
end if
end for
for all c ? C do
p(c) = frequency-of-all-tys-disambiguated-to-class(c,D)fD {calculating class probabilities}
end for
Algorithm 2 DSPROTO algorithm
C = (){classes in DSPROTO}
D = () {disambiguated ty ? TY }
fD = 0 {frequency of disambiguated items}
TY = argument head types {nouns occurring as objects of verb, with associated frequencies}
C1 = cty ? TY where num-types-in-thesaurus(cty, TY ) > 1
order C1 by num-types-in-thesaurus(cty, TY ) {classes ordered by coverage of argument head types}
for all cty ? ordered C1 do
Dcty = () {disambiguated for this class}
for all ty ? TY where in-thesaurus-entry(cty, ty) do
if ty /? D then
add ty to Dcty {types disambiguated to this class only if not disambiguated by a class used already}
end if
end for
if |Dcty| > 1 then
add cty to C
for all ty ? Dcty do
add ty ? cty to D {Disambiguated ty with cty}
fD = fD + frequency(ty)
end for
end if
end for
for all cty ? C do
p(cty) = frequency-of-all-tys-disambiguated-to-class(cty,D)fD {calculating class probabilities}
end for
374
3.3 DSPROTOs
We use a thesaurus acquired using the method
proposed by Lin (1998). For input we used the
grammatical relation data from automatic parses of
the BNC. For each noun we considered the co-
occurring verbs in the object and subject relation,
the modifying nouns in noun-noun relations and
the modifying adjectives in adjective-noun relations.
Each thesaurus entry consists of the target noun and
the 50 most similar nouns, according to Lin?s mea-
sure of distributional similarity, to the target.
The argument head noun types (TY ) are used
to find the entries in the thesaurus as the ?classes?
(C) of the selectional preference for a given verb.
As with WNPROTOs, we only cover argument types
which form coherent groups with other argument
types since we wish i) to remove noise and ii) to
be able to identify argument types which are not re-
lated with the other types and therefore may be non-
compositional. As our starting point we only con-
sider an argument type as a class for C if its entry in
the thesaurus covers at least a threshold of 2 types. 6
To select C we use a best first search. This method
processes each argument type in TY in order of the
number of the other argument types from TY that it
has in its thesaurus entry of 50 similar nouns. An ar-
gument head is selected as a class for C (cty ? C) 7
if it covers at least 2 of the argument heads that are
not in the thesaurus entries of any of the other classes
already selected for C . Each argument head is dis-
ambiguated by whichever class in C under which it
is listed in the thesaurus and which has the largest
number of the TY in its thesaurus entry. When the
algorithm finishes processing the ordered argument
heads to select C , all argument head types are dis-
ambiguated by C apart from those which after dis-
ambiguation occur in isolation in a class without
other argument types. Finally a probability distri-
bution over C is estimated using the frequency (to-
kens) of argument types that occur in the thesaurus
entries for any cty ? C . If an argument type oc-
curs in the entry of more than one cty then it is as-
signed to whichever of these has the largest number
6As with the WNPROTOs, we experimented with a value of
3 for this threshold and obtained similar results.
7We use cty for the classes of the DSPROTO. These classes
are simply groups of nouns which occur under the entry of a
noun (ty) in the thesaurus.
class (p(c)) disambiguated objects (freq)
van (0.86) car (174) van (11) vehicle (8) . . .
mile (0.05) street (5) distance (4) mile (1) . . .
yard (0.03) corner (4) lane (3) door (1)
backside (0.02) backside (2) bum (1) butt (1) . . .
Figure 3: First four classes of DSPROTO model for
park
of disambiguated argument head types and its token
frequency is attributed to that class. We show the
algorithm as Algorithm 2.
The algorithms for WNPROTO algorithm 1 and
DSPROTO (algorithm 2) differ because of the na-
ture of the inventories of candidate classes (Word-
Net and the distributional thesaurus). There are a
great many candidate classes in WordNet. The WN-
PROTO algorithm selects the classes from all those
that the argument heads belong to directly and indi-
rectly by looping over all argument types to find the
class that disambiguates each by having the largest
type ratio calculated using the undisambiguated ar-
gument heads. The DSPROTO only selects classes
from the fixed set of argument types. The algorithm
loops over the argument types with at least two ar-
gument heads in the thesaurus entry and ordered by
the number of undisambiguated argument heads in
the thesaurus entry. This is a best first search to min-
imise the number of argument heads used in C but
maximise the coverage of argument types.
In figure 3, we show part of a DSPROTO model for
the object of park. 8 Note again that the class mile
arises because of a systematic parsing error where
adverbials such as distance in park illegally some
distance from the railway station are identified by
the parser as objects.
4 Experiments
Venkatapathy and Joshi (2005) produced a dataset of
verb-object pairs with human judgements of com-
positionality. They obtained values of rs between0.111 and 0.300 by individually applying the 7 fea-
tures described above in section 2. The best corre-
lation was given by feature 7 and the second best
was feature 3. They combined all 7 features using
SVMs and splitting their data into test and training
data and achieve a rs of 0.448, which demonstrates
8We cannot show the full model due to lack of space.
375
significantly better correlation with the human gold-
standard than any of the features in isolation
We evaluated our selectional preference models
using the verb-object pairs produced by Venkatapa-
thy and Joshi (2005). 9 This dataset has 765 verb-
object collocations which have been given a rat-
ing between 1 and 6, by two annotators (both flu-
ent speakers of English). Kendall?s Tau (Siegel and
Castellan, 1988) was used to measure agreement,
and a score of 0.61 was obtained which was highly
significant. The ranks of the two annotators gave a
Spearman?s rank-correlation coefficient (rs) of 0.71.
The Verb-Object pairs included some adjectives
(e.g. happy, difficult, popular), pronouns and com-
plements e.g. become director. We used the sub-
set of 638 verb-object pairs that involved common
nouns in the object relationship since our preference
models focused on the object relation for common
nouns. For each verb-object pair we used the pref-
erence models acquired from the RASP parses of the
BNC to obtain the probability of the class that this
object occurs under. Where the object noun is a
member of several classes (classes(noun) ? C)
in the model, the class with the largest probability
is used. Note though that for WNPROTOs we have
the added constraint that a hyponym class from C is
selected in preference to a hypernym in C . Compo-
sitionality of an object noun and verb is computed
as:-
comp(noun, verb) = maxc?classes(noun)?C p(c|verb) (3)
We use the probability of the class, rather than an
estimate of the probability of the object, because we
want to determine how likely any word belonging
to this class might occur with the given verb, rather
than the probability of the specific noun which may
be infrequent, yet typical, of the objects that occur
with this verb. For example, convertible may be
an infrequent object of park, but it is quite likely
given its membership of the class motor vehicle.
We do not want to assume anything about the fre-
quency of non-compositional verb-object combina-
tions, just that they are unlikely to be members of
classes which represent prototypical objects. We
9This verb-object dataset is available from
http://www.cis.upenn.edu/?sriramv/mywork.html.
method rs p < (one tailed)
selectional preferences
TCM 0.090 0.0119
WNPROTO 0.223 0.00003
DSPROTO 0.398 0.00003
features from V&J
frequency (f1) 0.141 0.00023
MI (f2) 0.274 0.00003
Lin99 (f3) 0.139 0.00023
LSA2 (f7) 0.209 0.00003
combination with SVM
f2,3,7 0.413 0.00003
f1,2,3,7 0.419 0.00003
DSPROTO f1,2,3,7 0.454 0.00003
Table 1: Correlation scores for 638 verb object pairs
will contrast these models with a baseline frequency
feature used by Venkatapathy and Joshi.
We use our selectional preference models to pro-
vide the probability that a candidate is represen-
tative of the typical objects of the verb. That is,
if the object might typically occur in such a rela-
tionship then this should lessen the chance that this
verb-object combination is non-compositional. We
used the probability of the classes from our 3 selec-
tional preference models to rank the pairs and then
used Spearman?s rank-correlation coefficient (rs) tocompare these ranks with the ranks from the gold-
standard.
Our results for the three types of preference mod-
els are shown in the first section of table 1. 10 All the
correlation values are significant, but we note that
using the type based selectional preference mod-
els achieves a far greater correlation than using the
TCMs. The DSPROTO models achieve the best re-
sults which is very encouraging given that they only
require raw data and an automatic parser to obtain
the grammatical relations.
We applied 4 of the features used by Venkatapa-
thy and Joshi (2005) 11 and described in section 2
to our subset of 638 items. These features were ob-
10We show absolute values of correlation following (Venkat-
apathy and Joshi, 2005).
11The other 3 features performed less well on this dataset so
we do not report the details here. This seems to be because they
worked particularly well with the adjective and pronoun data in
the full dataset.
376
tained using the same BNC dataset used by Venkat-
apathy and Joshi which was obtained using Bikel?s
parser (Bikel, 2004). We obtained correlation val-
ues for these features as shown in table 1 under
V&J. These features are feature 1 frequency, feature
2 pointwise mutual information, feature 3 based on
(Lin, 1999) and feature 7 LSA feature which consid-
ers the similarity of the verb-object pair with the ver-
bal form of the object. Pointwise mutual informa-
tion did surprisingly well on this 84% subset of the
data, however the DSPROTO preferences still out-
performed this feature. We combined the DSPROTO
and V&J features with an SVM ranking function and
used 10 fold cross validation as Venkatapathy and
Joshi did. We contrast the result with the V&J fea-
tures without the preference models. The results in
the bottom section of table 1 demonstrate that the
preference models can be combined with other fea-
tures to produce optimal results.
5 Conclusions and Directions for Future
Work
We have demonstrated that the selectional prefer-
ences of a verbal predicate can be used to indi-
cate if a specific combination with an object is non-
compositional. We have shown that selectional pref-
erence models which represent prototypical argu-
ments and focus on argument types (rather than to-
kens) do well at the task. Models produced from
distributional thesauruses are the most promising
which is encouraging as the technique could be ap-
plied to a language without a man-made thesaurus.
We find that the probability estimates from our
models show a highly significant correlation, and
are very promising for detecting non-compositional
verb-object pairs, in comparison to individual fea-
tures used previously.
Further comparison of WNPROTOs and
DSPROTOs to other WordNet models are war-
ranted to contrast the effect of our proposal for
disambiguation using word types with iterative
approaches, particularly those of Clark and Weir
(2002). A benefit of the DSPROTOs is that they
do not require a hand-crafted inventory. It would
also be worthwhile comparing the use of raw data
directly, both from the BNC and from google?s
Web 1T corpus (Brants and Franz, 2006) since
web counts have been shown to outperform the
Clark and Weir models on a pseudo-disambiguation
task (Keller and Lapata, 2003).
We believe that preferences should NOT be used
in isolation. Whilst a low preference for a noun
may be indicative of peculiar semantics, this may
not always be the case, for example chew the fat.
Certainly it would be worth combining the prefer-
ences with other measures, such as syntactic fixed-
ness (Fazly and Stevenson, 2006). We also believe it
is worth targeting features to specific types of con-
structions, for example light verb constructions un-
doubtedly warrant special treatment (Stevenson et
al., 2003)
The selectional preference models we have pro-
posed here might also be applied to other tasks. We
hope to use these models in tasks such as diathesis
alternation detection (McCarthy, 2000; Tsang and
Stevenson, 2004) and contrast with WordNet mod-
els previously used for this purpose.
6 Acknowledgements
We acknowledge support from the Royal Society
UK for a Dorothy Hodgkin Fellowship to the first
author. We thank the anonymous reviewers for their
constructive comments on this work.
References
Steven Abney and Marc Light. 1999. Hiding a semantic
class hierarchy in a Markov model. In Proceedings of
the ACL Workshop on Unsupervised Learning in Nat-
ural Language Processing, pages 1?8.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model ofmultiword expression decomposability. In Proceed-
ings of the ACL Workshop on multiword expressions:
analysis, acquisition and treatment, pages 89?96.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.2003. A statistical approach to the semantics ofverb-particles. In Proceedings of the ACL Workshop
on multiword expressions: analysis, acquisition and
treatment, pages 65?72.
Colin. Bannard. 2002. Statistical techniquesfor automatically inferring the semantics of verb-particle constructions. Technical Report WP-2002-
06, University of Edinburgh, School of Informatics.http://lingo.stanford.edu/pubs/WP-2002-06.pdf.
377
Colin Bannard. 2005. Learning about the meaning of
verb-particle constructions from corpora. Computer
Speech and Language, 19(4):467?478.
Daniel M. Bikel. 2004. A distributional analysis of a lex-
icalized statistical parsing model. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), Barcelona, Spain,
July. Association for Computational Linguistics.
Don Blaheta and Mark Johnson. 2001. Unsuper-vised learning of multi-word verbs. In Proceedings
of the ACL Workshop on Collocations, pages 54?60,Toulouse, France.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
corpus version 1.1. Technical Report.
Edward Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the Third International Conference on Language
Resources and Evaluation (LREC), pages 1499?1504,
Las Palmas, Canary Islands, Spain.
Kenneth Church and Patrick Hanks. 1990. Word asso-ciation norms, mutual information and lexicography.
Computational Linguistics, 19(2):263?312.
Stephen Clark and David Weir. 2002. Class-based prob-ability estimation using a semantic hierarchy. Compu-
tational Linguistics, 28(2):187?206.
Afsaneh Fazly and Suzanne Stevenson. 2006. Automat-ically constructing a lexicon of verb phrase idiomatic
combinations. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL-2006), pages 337?344,
Trento, Italy, April.
Christiane Fellbaum, editor. 1998. WordNet, An Elec-
tronic Lexical Database. The MIT Press, Cambridge,
MA.
Ralph Grishman and John Sterling. 1994. Generalizing
automatically generated selectional patterns. In Pro-
ceedings of the 15th International Conference of Com-
putational Linguistics. COLING-94, volume I, pages742?747.
Frank Keller and Mirella Lapata. 2003. Using the web toobtain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459?484.
Brigitte Krenn and Stefan Evert. 2001. Can we do betterthan frequency? A case study on extracting PP-verb
collocations. In Proceedings of the ACL Workshop on
Collocations, pages 39?46, Toulouse, France.
Geoffrey Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,28(1):1?13.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217?244.
Dekang Lin. 1998. Automatic retrieval and clusteringof similar words. In Proceedings of COLING-ACL 98,
Montreal, Canada.
Dekang Lin. 1999. Automatic identification of non-compositional phrases. In Proceedings of ACL-99,
pages 317?324, Univeristy of Maryland, College Park,Maryland.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasalverbs. In Proceedings of the ACL 03 Workshop: Multi-
word expressions: analysis, acquisition and treatment,pages 73?80.
Diana McCarthy. 2000. Using semantic preferences toidentify verbal participation in role switching alter-nations. In Proceedings of the First Conference of
the North American Chapter of the Association for
Computational Linguistics. (NAACL), pages 256?263,Seattle,WA.
Fernando Pereira, Nattali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 31st Annual Meeting of the Association
for Computational Linguistics, pages 183?190.
Philip Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.Ph.D. thesis, University of Pennsylvania.
Jorma Rissanen. 1978. Modelling by shortest data de-scription. Automatica, 14:465?471.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. In Proceedings of
the Third International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing
2002), pages 1?15, Mexico City, Mexico.
Patrick Schone and Daniel Jurafsky. 2001. Isknowledge-free induction of multiword unit dictionaryheadwords a solved problem? In Proceedings of the
2001 Conference on Empirical Methods in Natural
Language Processing, pages 100?108, Hong Kong.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Sidney Siegel and N. John Castellan. 1988. Non-
Parametric Statistics for the Behavioral Sciences.
McGraw-Hill, New York.
Suzanne Stevenson, Afsaneh Fazly, and Ryan North.2003. Statistical measures of the semi-productivity oflight verb constructions. In Proceedings of the ACL
2004 Workshop on Multiword Expressions: Integrat-
ing Processing, Barcelona, Spain.
378
Vivian Tsang and Suzanne Stevenson. 2004. Using se-
lectional profile distance to detect verb alternations. In
Proceedings of NAACL Workshop on Computational
Lexical Semantics (CLS-04), pages 30?37, Boston,
MA.
Sriram Venkatapathy and Aravind K. Joshi. 2005. Mea-suring the relative compositionality of verb-noun (v-n)collocations by integrating features. In Proceedings of
the joint conference on Human Language Technology
and Empirical methods in Natural Language Process-
ing, pages 899?906, Vancouver, B.C., Canada.
Andreas Wagner. 2002. Learning thematic role relations
for wordnets. In Proceedings of ESSLLI-2002 Work-
shop on Machine Learning Approaches in Computa-
tional Linguistics, Trento.
379
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 899?906, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Measuring the relative compositionality of verb-noun (V-N) collocations by
integrating features
Sriram Venkatapathy
 
Language Technologies Research Centre,
International Institute of Information
Technology - Hyderabad,
Hyderabad, India.
sriram@research.iiit.ac.in
Aravind K. Joshi
Department of Computer and
Information Science and Institute for
Research in Cognitive Science,
University of Pennsylvania,
Philadelphia, PA, USA.
joshi@linc.cis.upenn.edu
Abstract
Measuring the relative compositionality
of Multi-word Expressions (MWEs) is
crucial to Natural Language Processing.
Various collocation based measures have
been proposed to compute the relative
compositionality of MWEs. In this paper,
we define novel measures (both colloca-
tion based and context based measures) to
measure the relative compositionality of
MWEs of V-N type. We show that the
correlation of these features with the hu-
man ranking is much superior to the cor-
relation of the traditional features with the
human ranking. We then integrate the pro-
posed features and the traditional features
using a SVM based ranking function to
rank the collocations of V-N type based
on their relative compositionality. We
then show that the correlation between the
ranks computed by the SVM based rank-
ing function and human ranking is signif-
icantly better than the correlation between
ranking of individual features and human
ranking.
1 Introduction
The main goal of the work presented in this paper
is to examine the relative compositionality of col-
1Part of the work was done at Institute for Research in Cog-
nitive Science (IRCS), University of Pennsylvania, Philadel-
phia, PA 19104, USA, when he was visiting IRCS as a Visiting
Scholar, February to December, 2004.
locations of V-N type using a SVM based ranking
function. Measuring the relative compositionality of
V-N collocations is extremely helpful in applications
such as machine translation where the collocations
that are highly non-compositional can be handled in
a special way (Schuler and Joshi, 2004) (Hwang
and Sasaki, 2005).
Multi-word expressions (MWEs) are those whose
structure and meaning cannot be derived from their
component words, as they occur independently.
Examples include conjunctions like ?as well as?
(meaning ?including?), idioms like ?kick the bucket?
(meaning ?die?), phrasal verbs like ?find out? (mean-
ing ?search?) and compounds like ?village commu-
nity?. A typical natural language system assumes
each word to be a lexical unit, but this assumption
does not hold in case of MWEs (Becker, 1975)
(Fillmore, 2003). They have idiosyncratic interpre-
tations which cross word boundaries and hence are
a ?pain in the neck? (Sag et al, 2002). They account
for a large portion of the language used in day-to-
day interactions (Schuler and Joshi, 2004) and so,
handling them becomes an important task.
A large number of MWEs have a standard syn-
tactic structure but are non-compositional semanti-
cally. An example of such a subset is the class of
non-compositional verb-noun collocations (V-N col-
locations). The class of non-compositional V-N col-
locations is important because they are used very
frequently. These include verbal idioms (Nunberg
et al, 1994), support-verb constructions (Abeille,
1988), (Akimoto, 1989), among others. The ex-
pression ?take place? is a MWE whereas ?take a gift?
is not a MWE.
899
It is well known that one cannot really make a
binary distinction between compositional and non-
compositional MWEs. They do not fall cleanly into
mutually exclusive classes, but populate the con-
tinuum between the two extremes (Bannard et al,
2003). So, we rate the MWEs (V-N collocations in
this paper) on a scale from 1 to 6 where 6 denotes
a completely compositional expression, while 1 de-
notes a completely opaque expression.
Various statistical measures have been suggested
for ranking expressions based on their composition-
ality. Some of these are Frequency, Mutual Infor-
mation (Church and Hanks, 1989) , distributed fre-
quency of object (Tapanainen et al, 1998) and LSA
model (Baldwin et al, 2003) (Schutze, 1998). In
this paper, we define novel measures (both collo-
cation based and context based measures) to mea-
sure the relative compositionality of MWEs of V-N
type (see section 6 for details). Integrating these sta-
tistical measures should provide better evidence for
ranking the expressions. We use a SVM based rank-
ing function to integrate the features and rank the
V-N collocations according to their compositional-
ity. We then compare these ranks with the ranks
provided by the human judge. A similar compari-
son between the ranks according to Latent-Semantic
Analysis (LSA) based features and the ranks of hu-
man judges has been made by McCarthy, Keller and
Caroll (McCarthy et al, 2003) for verb-particle con-
structions. (See Section 3 for more details). Some
preliminary work on recognition of V-N collocations
was presented in (Venkatapathy and Joshi, 2004).
We show that the measures which we have defined
contribute greatly to measuring the relative compo-
sitionality of V-N collocations when compared to the
traditional features. We also show that the ranks as-
signed by the SVM based ranking function corre-
lated much better with the human judgement that the
ranks assigned by individual statistical measures.
This paper is organized in the following sections
(1) Basic Architecture, (2) Related work, (3) Data
used for the experiments, (4) Agreement between
the Judges, (5) Features, (6) SVM based ranking
function, (7) Experiments & Results, and (8) Con-
clusion.
2 Basic Architecture
Every V-N collocation is represented as a vector of
features which are composed largely of various sta-
tistical measures. The values of these features for
the V-N collocations are extracted from the British
National Corpus. For example, the V-N collocation
?raise an eyebrow? can be represented as
 
Frequency = 271, Mutual Information = 8.43, Dis-
tributed frequency of object = 1456.29, etc.  . A
SVM based ranking function uses these features to
rank the V-N collocations based on their relative
compositionality. These ranks are then compared
with the human ranking.
3 Related Work
(Breidt, 1995) has evaluated the usefulness of the
Point-wise Mutual Information measure (as sug-
gested by (Church and Hanks, 1989)) for the ex-
traction of V-N collocations from German text cor-
pora. Several other measures like Log-Likelihood
(Dunning, 1993), Pearson?s  (Church et al,
1991), Z-Score (Church et al, 1991) , Cubic As-
sociation Ratio (MI3), etc., have been also pro-
posed. These measures try to quantify the associ-
ation of two words but do not talk about quantify-
ing the non-compositionality of MWEs. Dekang Lin
proposes a way to automatically identify the non-
compositionality of MWEs (Lin, 1999). He sug-
gests that a possible way to separate compositional
phrases from non-compositional ones is to check the
existence and mutual-information values of phrases
obtained by replacing one of the words with a sim-
ilar word. According to Lin, a phrase is proba-
bly non-compositional if such substitutions are not
found in the collocations database or their mutual
information values are significantly different from
that of the phrase. Another way of determining the
non-compositionality of V-N collocations is by us-
ing ?distributed frequency of object? (DFO) in V-N
collocations (Tapanainen et al, 1998). The basic
idea in there is that ?if an object appears only with
one verb (or few verbs) in a large corpus we expect
that it has an idiomatic nature? (Tapanainen et al,
1998).
Schone and Jurafsky (Schone and Jurafsky, 2001)
applied Latent-Semantic Analysis (LSA) to the anal-
ysis of MWEs in the task of MWE discovery, by way
900
of rescoring MWEs extracted from the corpus. An
interesting way of quantifying the relative composi-
tionality of a MWE is proposed by Baldwin, Ban-
nard, Tanaka and Widdows (Baldwin et al, 2003).
They use LSA to determine the similarity between
an MWE and its constituent words, and claim that
higher similarity indicates great decomposability. In
terms of compositionality, an expression is likely
to be relatively more compositional if it is decom-
posable. They evaluate their model on English NN
compounds and verb-particles, and showed that the
model correlated moderately well with the Word-
net based decomposability theory (Baldwin et al,
2003).
McCarthy, Keller and Caroll (McCarthy et al,
2003) judge compositionality according to the de-
gree of overlap in the set of most similar words to
the verb-particle and head verb. They showed that
the correlation between their measures and the hu-
man ranking was better than the correlation between
the statistical features and the human ranking. We
have done similar experiments in this paper where
we compare the correlation value of the ranks pro-
vided by the SVM based ranking function with the
ranks of the individual features for the V-N collo-
cations. We show that the ranks given by the SVM
based ranking function which integrates all the fea-
tures provides a significantly better correlation than
the individual features.
4 Data used for the experiments
The data used for the experiments is British Na-
tional Corpus of 81 million words. The corpus is
parsed using Bikel?s parser (Bikel, 2004) and the
Verb-Object Collocations are extracted. There are
4,775,697 V-N collocations of which 1.2 million are
unique. All the V-N collocations above the fre-
quency of 100 (n=4405) are taken to conduct the ex-
periments so that the evaluation of the system is fea-
sible. These 4405 V-N collocations were searched in
Wordnet, American Heritage Dictionary and SAID
dictionary (LDC,2003). Around 400 were found in
at least one of the dictionaries. Another 400 were
extracted from the rest so that the evaluation set has
roughly equal number of compositional and non-
compositional expressions. These 800 expressions
were annotated with a rating from 1 to 6 by us-
ing guidelines independently developed by the au-
thors. 1 denotes the expressions which are totally
non-compositional while 6 denotes the expressions
which are totally compositional. The brief expla-
nation of the various ratings is as follows: (1) No
word in the expression has any relation to the ac-
tual meaning of the expression. Example : ?leave a
mark?. (2) Can be replaced by a single verb. Ex-
ample : ?take a look?. (3) Although meanings of
both words are involved, at least one of the words
is not used in the usual sense. Example : ?break
news?. (4) Relatively more compositional than (3).
Example : ?prove a point?. (5) Relatively less com-
positional than (6). Example : ?feel safe?. (6) Com-
pletely compositional. Example : ?drink coffee?.
5 Agreement between the Judges
The data was annotated by two fluent speakers of
English. For 765 collocations out of 800, both the
annotators gave a rating. For the rest, at least one
of the annotators marked the collocations as ?don?t
know?. Table 1 illustrates the details of the annota-
tions provided by the two judges.
Ratings 6 5 4 3 2 1
Annotator1 141 122 127 119 161 95
Annotator2 303 88 79 101 118 76
Table 1: Details of the annotations of the two anno-
tators
From the table 1 we see that annotator1 dis-
tributed the rating more uniformly among all the
collocations while annotator2 observed that a sig-
nificant proportion of the collocations were com-
pletely compositional. To measure the agreement
between the two annotators, we used the Kendall?s
TAU (   ) (Siegel and Castellan, 1988).   is the cor-
relation between the rankings1 of collocations given
by the two annotators.   ranges between 0 (little
agreement) and 1 (full agreement).   is defined as,


	



	




	
Relative Compositionality of Multi-word
Expressions: A Study of Verb-Noun (V-N)
Collocations
Sriram Venkatapathy1, and Aravind K. Joshi2
1 Language Technologies Research Center,
International Institute of Information Technology - Hyderabad, Hyderabad, India
sriram@research.iiit.ac.in
2 Department of Computer and Information Science
and Institute of Research in Cognitive Science,
University of Pennsylvania, Philadelphia, PA, USA
joshi@linc.cis.upenn.edu
Abstract. Recognition of Multi-word Expressions (MWEs) and their
relative compositionality are crucial to Natural Language Processing.
Various statistical techniques have been proposed to recognize MWEs.
In this paper, we integrate all the existing statistical features and in-
vestigate a range of classifiers for their suitability for recognizing the
non-compositional Verb-Noun (V-N) collocations. In the task of ranking
the V-N collocations based on their relative compositionality, we show
that the correlation between the ranks computed by the classifier and hu-
man ranking is significantly better than the correlation between ranking
of individual features and human ranking. We also show that the prop-
erties ?Distributed frequency of object? (as defined in [27]) and ?Nearest
Mutual Information? (as adapted from [18]) contribute greatly to the
recognition of the non-compositional MWEs of the V-N type and to the
ranking of the V-N collocations based on their relative compositionality.
1 Introduction
The main goals of the work presented in this paper are (1) To investigate a range
of classifiers for their suitability in recognizing the non-compositional V-N collo-
cations, and (2) To examine the relative compositionality of collocations of V-N
type. Measuring the relative compositionality of V-N collocations is extremely
helpful in applications such as machine translation where the collocations that
are highly non-compositional can be handled in a special way.
Multi-word expressions (MWEs) are those whose structure and meaning can-
not be derived from their component words, as they occur independently. Ex-
amples include conjunctions like ?as well as? (meaning ?including?), idioms like
 Part of the work was done at Institute for Research in Cognitive Science, University
of Pennsylvania, Philadelphia, PA 19104, USA, when he was visiting IRCS as a
visiting Scholar, February to December, 2004.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 553?564, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
554 S. Venkatapathy and A.K. Joshi
?kick the bucket? (meaning ?die?), phrasal verbs like ?find out? (meaning ?search?)
and compounds like ?village community?. A typical natural language system as-
sumes each word to be a lexical unit, but this assumption does not hold in case
of MWEs [6] [12]. They have idiosyncratic interpretations which cross word
boundaries and hence are a ?pain in the neck? [23]. They account for a large
portion of the language used in day-to-day interactions [25] and so, handling
them becomes an important task.
A large number of MWEs have a standard syntactic structure but are non-
compositional semantically. An example of such a subset is the class of non-
compositional verb-noun collocations (V-N collocations). The class of V-N col-
locations which are non-compositional is important because they are used very
frequently. These include verbal idioms [22], support-verb constructions [1] [2]
etc. The expression ?take place? is a MWE whereas ?take a gift? is not a MWE.
It is well known that one cannot really make a binary distinction between
compositional and non-compositional MWEs. They do not fall cleanly into mu-
tually exclusive classes, but populate the continuum between the two extremes
[4]. So, we rate the MWEs (V-N collocations in this paper) on a scale from 1
to 6 where 6 denotes a completely compositional expression, while 1 denotes a
completely opaque expression. But, to address the problem of identification, we
still need to do an approximate binary distinction. We call the expressions with
a rating of 4 to 6 compositional and the expressions with rating of 1 to 3 as
non-compositional. (See Section 4 for further details).
Various statistical measures have been suggested for identification of MWEs
and ranking expressions based on their compositionality. Some of these are Fre-
quency, Mutual Information [9], Log-Likelihood [10] and Pearson?s ?2 [8].
Integrating all the statistical measures should provide better evidence for rec-
ognizing MWEs and ranking the expressions. We use various Machine Learning
Techniques (classifiers) to integrate these statistical features and classify the V-
N collocations as MWEs or Non-MWEs. We also use a classifier to rank the V-N
collocations according to their compositionality. We then compare these ranks
with the ranks provided by the human judge. A similar comparison between
the ranks according to Latent-Semantic Analysis (LSA) based features and the
ranks of human judges has been done by McCarthy, Keller and Caroll [19] for
verb-particle constructions. (See Section 3 for more details). Some preliminary
work on recognition of V-N collocations was presented in [28].
In the task of classification, we show that the technique of weighted features
in distance-weighted nearest-neighbour algorithm performs slightly better than
other machine learning techniques. We also find that the ?distributed frequency
of object (as defined by [27])? and ?nearest mutual information (as adapted
from [18])? are important indicators of the non-compositionality of MWEs. In
the task of ranking, we show that the ranks assigned by the classifier correlated
much better with the human judgement than the ranks assigned by individual
statistical measures.
This paper is organised in the following sections (2) Basic Architecture,
(3) Related work, (4) Data used for the experiments, (5) Agreement between
Relative Compositionality of Multi-word Expressions 555
the Judges, (6) Features, (7) Experiments - Classification, (8) Experiments -
Ranking and (9) Conclusion.
2 Basic Architecture
Recognition of MWEs can be regarded as a classification task where every V-N
collocation can be classified either as a MWE or as a Non-MWE. Every V-N
collocation is represented as a vector of features which are composed largely of
various statistical measures. The values of these features for the V-N collocations
are extracted from the British National Corpus. For example, the V-N collocation
?raise an eyebrow? can be represented as
[ Frequency = 271, Mutual Information = 8.43, Log-Likelihood = 1456.29, etc.].
Now, to recognise the MWEs, the classifier has to do a binary classification
of this vector. So, ideally, the classifier should take the above information and
classify ?raise an eyebrow? as an MWE. The classifier can also be used to rank
these vectors according to their relative compositionality.
3 Related Work
Church and Hanks (1989) proposed a measure of association called Mutual In-
formation [9]. Mutual Information (MI) is the logarithm of the ratio between
the probability of the two words occurring together and the product of the prob-
ability of each word occurring individually. The higher the MI, the more likely
are the words to be associated with each other. The usefulness of the statistical
approach suggested by Church and Hanks [9] is evaluated for the extraction
of V-N collocations from German text Corpora [7]. Several other measures like
Log-Likelihood [10], Pearson?s ?2 [8], Z-Score [8] , Cubic Association Ratio
(MI3), Log-Log [17], etc., have been proposed. These measures try to quan-
tify the association of the two words but do not talk about quantifying the
non-compositionality of MWEs. Dekang Lin proposes a way to automatically
identify the non-compositionality of MWEs [18]. He suggests that a possible
way to separate compositional phrases from non-compositional ones is to check
the existence and mutual-information values of phrases obtained by replacing
one of the words with a similar word. According to Lin, a phrase is proba-
bly non-compositional if such substitutions are not found in the collocations
database or their mutual information values are significantly different from that
of the phrase. Another way of determining the non-compositionality of V-N col-
locations is by using ?distributed frequency of object?(DFO) in V-N collocations
[27]. The basic idea in there is that ?if an object appears only with one verb (or
few verbs) in a large corpus we expect that it has an idiomatic nature? [27].
Schone and Jurafsky [24] applied Latent-Semantic Analysis (LSA) to the
analysis of MWEs in the task of MWE discovery, by way of rescoring MWEs
extracted from the corpus. An interesting way of quantifying the relative com-
positionality of a MWE is proposed by Baldwin, Bannard, Tanaka and Widdows
[3]. They use latent semantic analysis (LSA) to determine the similarity between
556 S. Venkatapathy and A.K. Joshi
an MWE and its constituent words, and claim that higher similarity indicates
great decomposability. In terms of compositionality, an expression is likely to be
relatively more compositional if it is decomposable. They evaluate their model
on English NN compounds and verb-particles, and showed that the model cor-
related moderately well with the Wordnet based decomposibility theory [3].
Evert and Krenn [11] compare some of the existing statistical features for
the recognition of MWEs of adjective-noun and preposition-noun-verb types.
Galiano, Valdivia, Santiago and Lopez [14] use five statistical measures to clas-
sify generic MWEs using the LVQ (Learning Vector Quantization) algorithm. In
contrast, we do a more detailed and focussed study of V-N collocations and the
ability of various classifiers in recognizing MWEs. We also compare the roles of
various features in this task.
McCarthy, Keller and Caroll [19] judge compositionality according to the
degree of overlap in the set of most similar words to the verb-particle and head
verb. They showed that the correlation between their measures and the human
ranking was better than the correlation between the statistical features and
the human ranking. We have done similar experiments in this paper where we
compare the correlation value of the ranks provided by the classifier with the
ranks of the individual features for the V-N collocations. We show that the ranks
given by the classifier which integrates all the features provides a significantly
better correlation than the individual features.
4 Data Used for the Experiments
The data used for the experiments is British National Corpus of 81 million words.
The corpus is parsed using Bikel?s parser [5] and the Verb-Object Collocations
are extracted. There are 4,775,697 V-N of which 1.2 million were unique. All
the V-N collocations above the frequency of 100 (n=4405) are taken to conduct
the experiments so that the evaluation of the system is feasible. These 4405
V-N collocations were searched in Wordnet, American Heritage Dictionary and
SAID dictionary (LDC,2003). Around 400 were found in at least one of the dic-
tionaries. Another 400 were extracted from the rest so that the evaluation set
has roughly equal number of compositional and non-compositional expressions.
These 800 expressions were annotated with a rating from 1 to 6 by using guide-
lines independently developed by the authors. 1 denotes the expressions which
are totally non-compositional while 6 denotes the expressions which are totally
compositional. The brief explanation of the various rating are (1) No word in
the expression has any relation to the actual meaning of the expression. Exam-
ple: ?leave a mark?. (2) Can be replaced by a single verb. Example : ?take
a look?. (3) Although meanings of both words are involved, at least one of the
words is not used in the usual sense. Example : ?break news?. (4) Relatively
more compositional than (3). Example : ?prove a point?. (5) Relatively less
compositional than (6). Example : ?feel safe?. (6) Completely compositional.
Example : ?drink coffee?. For the experiments on classification (Section 7), we
call the expressions with ratings of 4 to 6 as compositional and the expressions
Relative Compositionality of Multi-word Expressions 557
with rating of 1 to 3 as non-compositional. For the experiments on ranking the
expressions based on their relative compositionality, we use all the 6 ratings to
represent the relative compositionality of these expressions.
5 Agreement Between the Judges
The data was annotated by two fluent speakers of English. For 765 collocations
out of 800, both the annotators gave a rating. For the rest, atleast one of the an-
notators marked the collocations as ?don?t know?. Table 1 illustrates the details
of the annotations provided by the two judges.
Table 1. Details of the annotations of the two annotators
Ratings 6 5 4 3 2 1 Compositional Non-Compositional
(4 to 6) (1 to 3)
Annotator1 141 122 127 119 161 95 390 375
Annotator2 303 88 79 101 118 76 470 195
From the table we see that annotator1 distributed the rating more uniformly
among all the collocations while annotator2 observed that a significant propor-
tion of the collocations were completely compositional. To measure the agree-
ment between the two annotators, we used the Kendall?s TAU (?). ? is the
correlation between the rankings1 of collocations given by the two annotators.
W ranges between 0 (little agreement) and 1 (full agreement). W is calculated
as below,
? =
?
i<j sgn(xi ? xj)sgn(yi ? yj)
?
(T0 ? T1)(T0 ? T2)
where T0 = n(n ? 1)/2, T1 =
?
ti(ti ? 1)/2, T2 =
?
ui(ui ? 1)/2 and where,
n is the number of collocations, ti is the number of tied x values of ith group of
tied x values and ui is the number of tied y values of ith group of tied y values.
We obtained a ? score of 0.61 which is highly significant. This shows that the
annotators were in a good agreement with each other in deciding the rating to
be given to the collocations. We also compare the ranking of the two annotators
using Spearman?s Rank-Correlation coefficient (rs) (more details in section 8).
We obtained a rs score of 0.71 indicating a good agreement between the an-
notators. A couple of examples where the annotators differed are (1) ?perform
a task? was rated 3 by annotator1 while it was rated 6 by annotator2 and (2)
?pay tribute? was rated 1 by annotator1 while it was rated 4 by annotator2.
The 765 samples annotated by both the annotators were then divided into a
training set and a testing set in several possible ways to cross-validate the results
of classification and ranking.
1 Computed from the ratings.
558 S. Venkatapathy and A.K. Joshi
6 Features
Each collocation is represented by a vector whose dimensions are the statistical
features obtained from the British National Corpus. This list of features are given
in Table 2.2 While conducting the experiments, all features are scaled from 0 to
1 to ensure that all features are represented uniformly.
Table 2. List of features and their top-3 example collocations
Feature Top-3 Feature Top-3
take place Mutual Information shrug shoulder
Frequency have effect [9] bridge gap
have time plead guilty
Cubic Association take place Log-Log shake head
Measure shake head [17] commit suicide
(Oakes, 1998) play role fall asleep
Log-Likelihood take place Pearson?s ?2 shake head
[10] shake head [8] commit suicide
play role fall asleep
T-Score take place Z-Score shake head
[9] have effect [26] commit suicide
shake head fall asleep
?-coefficient bridge gap Distributed come true
shrug shoulder freq. of object become difficult
press button (DFO) make sure
[27]
Nearest MI Collocations Whether object (Binary feature)
(NMI) with no can occur
[18] neigh. MI as a verb
Whether object (Binary feature)
is a nomin.
of some verb
7 Experiments - Classification
The evaluation data (765 vectors) is divided randomly into training and testing
vectors in 10 ways for cross-validation. The training data consists of 90% of 786
vectors and the testing data consists of the remaining.
We used various Machine Learning techinques to classify the V-N colloca-
tions into MWEs and non-MWEs. For every classifier, we calculated the average
accuracy of all the test sets of each of the annotators. We then compare the aver-
age accuracies of all the classifiers. We found that the classifier that we used, the
technique of weighted features in distance-weighted nearest-algorithm, performs
somewhat better than other machine learning techniques.
The following are brief descriptions of the classifiers that we used in this
paper.
2 The formulas of features are not given due to lack of space.
Relative Compositionality of Multi-word Expressions 559
7.1 Nearest-Neighbour Algorithm
This is an instance-based learning technique where the test vector is classified
based on its nearest vectors in the training data. The simple distance between
two vectors xi and xj is defined as d(xi,xj), where
d(xi, xj) =
?
?
?
?
n
?
r=1
(ar(xi) ? ar(xj))2.
Here, x is an instance of a vector and ar(x) is the value of the rth feature.
One can use K neighbours to judge the class of the test vector. The test
vector is assigned the class of maximum number of neighbours. This can be
furthur modified by calculating the inverse weighted distance between the test
vector and the neighbouring training vectors in each of the classes. The test
vector is then assigned the class which has the higher inverse-weighted distance.
One can also use all the training vectors and the weighted-distance principle to
classify the test vector.
The average classification accuracy of each of the above methods on the test
sets of each of the annotators is shown in Table 3.
Table 3. Average accuracies of MWE recognition using simple nearest-neighbour
algorithms and weighted distance nearest neighbour algorithms
Simple K-Nearest neighbour Weighted-distance Nearest neighbour
Type K=1 K=2 K=3 K=1 K=2 K=3 K=All
Annot.1 62.35 61.31 62.48 62.35 62.35 62.61 66.66
Annot.2 57.64 54.10 60.89 57.64 57.64 60.37 63.52
7.2 SVM-Based Classifiers
SVMs [15] have been very successful in attaining high accuracy for various
machine-learning tasks. Unlike the error-driven algorithms (Perceptron etc.),
SVM searches for the two distinct classes and maximizes the margin between
two classes. Data of higher dimension can also be classified using the appropriate
Kernel. We used Linear and Polynomial Kernel (degree=2) to test the evaluation
data. We also used the radial-basis network in SVMs to compare the results
because of their proximity to the nearest-neigbour algorithms.
Table 4. Average accuracies of MWE recognition using SVMs (Linear, Polynomial
and Radial Basis Function Kernel)
Linear Ker. Polynomial Ker. Radial Basis networks
Parameters ? = 0.5 ? = 1.0 ? = 1.5 ? = 2.0
Annot.1 65.89 65.75 67.06 66.66 66.93 67.06
Annot.2 62.61 65.09 64.17 63.51 62.99 62.99
560 S. Venkatapathy and A.K. Joshi
The average classification accuracy of each of the above methods on the test
sets of each of the annotators is shown in Table 4.
7.3 Weighted Features in Distance-Weighted Nearest-Neighbour
Algorithm
Among all the features used, only a few might be very relevant to recognizing
the non-compositionality of the MWE. As a result, the distance metric used
by the nearest-neighbour algorithm which depends on all the features might
be misleading. The distance between the neighbour will be dominated by large
number of irrelevant features.
A way of overcoming this problem is to weight each feature differently when
calculating the distance between the two instances. This also gives us an insight
into which features are mainly responsible for recognizing the non-compositional-
ity of MWEs. The jth feature can be multiplied by the weight zj , where the values
of z1...zn are chosen to minimize the true classification error of the learning
algorithm [20]. The distance using these weights is represented as
d(xi, xj) =
?
?
?
?
n
?
r=1
(zr ? (ar(xi) ? ar(xj)))2,
where zr is the weight of the rth feature.
The values of z1...zn can be determined by cross-validation of the training
data. We use leave-one-out cross-validation [21], in which the set of m training
vectors are repeatedly divided into a training set of m-1 and a test set of 1,
in all possible ways. So, each vector in the training data is classified using the
remaining vectors. The classification accuracy is defined as
Clacc = 100 ? (
m
?
1
classify(i)/m)
where classify(i)=1, if the ith training example is classified correctly using the
distance-weighted nearest neighbour algorithm, otherwise classify(i)=0.
Now, we try to maximize the classification accuracy in the following way,
? In every iteration, vary the weights of the features one by one.
? Choose the feature and its weight which brings the maximum increase in the
value of Clacc. One can also choose the feature and its weight such that it
brings the minimum increase in the value of Clacc.
? Update the weight of this particular feature and go for the next iteration.
? If there is no increase in classification accuracy, stop.
When the weights are updated such that there is maximum increase in classi-
fication accuracy in every step, the average accuracies are 66.92% and 64.30%
on the test sets of the two annotators respectively. But when the weights are
updated such there is a minimum increase in classification accuracy at every
Relative Compositionality of Multi-word Expressions 561
Table 5. The top three features according to the average weight when there is maxi-
mum increase in Clacc at every step
Annotator1 Weight Annotator2 Weight
DFO 1.09 MI 1.17
T-Score 1.0 T-Score 1.1
Z-Score 1.0 ?-coefficient 1.0
Table 6. The top three features according to the average weight calculated when there
is minimum increase in Clacc at every step
Annot.1 Weight Annot.2 Weight
DFO 1.07 MI 2.06
NMI 1.02 T-Score 1.0
Log-Like. 0.97 ?-coefficient 1.0
step, the average accuracies are 66.13% and 64.04% on the test sets of the
two annotators respectively, which are slightly better than that obtained by the
other Machine Learning Techniques.
In the above two methods (Updating weights such that there is maximum or
minimum increase in classification accuracy), we add the weights of the features
of each of the evaluation sets. According to the average weights, the top three
features (having high average weight) are shown in Tables 5 and 6.
In both the above cases, we find that the properties ?Mutual-Information?
and the compositionality oriented feature ?Distributed Frequency of an Object?
performed significantly better than the other features.
8 Experiments - Ranking
All the statistical measures show that the expressions ranked higher according
to their decreasing values are more likely to be non-compositional. We compare
these ranks with the average of the ranks given by the annotator (obtained from
his rating). To compare, we use Spearman Rank-Order Correlation Coefficient
(rs), defined as
rs =
(Ri ? R?)(Si ? S?)
?
?
(Ri ? R?)2
?
(Si ? S?)2
where Ri is the rank of ith x value, Si is the rank of ith y value, R? is the mean
of the Ri values and S? is the mean of Si values.
We use an SVM-based ranking system [16] for our training. Here, we use
10% of the 765 vectors for training and the remaining for testing. The SVM-
based ranking system builds a preference matrix of the training vectors to learn.
It then ranks the test vectors. The ranking system takes a lot of time to train
itself, and hence, we decided to use only a small proportion of the evaluation set
for training.
562 S. Venkatapathy and A.K. Joshi
Table 7. The correlation values of the ranking of individual features and the ranking
of classifier with the ranking of human judgements
MI -0.125 Z-Score -0.059
MI3 0.001 ?-coeff -0.102
Log-Log -0.086 DFO -0.113
Log-Likelihood 0.005 NMI -0.167
?2 -0.056 Class. 0.388
T-Score 0.045
We also compare our ranks (the average of the ranks suggested by the clas-
sifier) with the gold standard using the Spearman Rank-Order Correlation Co-
efficient. The results are shown in Table 7.
In Table 7, we observe that the correlation between the ranks computed by
the classifier and human ranking is better than the correlation between ranking
of individual statistical features and human ranking.
We observe that among all the statistical features the ranks based on the
properties ?Mutual Information?, ?Distributed Frequency of an Object? [27] and
?Nearest mutual information? [18] correlated better with the ranks provided
by the annotator. This is in accordance with the observation we made while
describing the classification experiments, where we observed that the proper-
ties ?Distributed Frequency of an Object? and ?Mutual Information? contributed
much to the classification of the expressions. When we compare the correlation
values of MI, Log-likelihood and ?2, we see that the Mutual-Information values
correlated better. This result is similar to the observation made by McCarthy,
Keller and Caroll [19] for phrasal verbs.
9 Conclusion
In this paper, we integrated the statistical features using various classifiers and
investigated their suitability for recognising non-compositional MWEs of the V-
N type. We also used a classifier to rank the V-N collocations according to their
relative compositionality. This type of MWEs constitutes a very large percent-
age of all MWEs and are crucial for NLP applications, especially for Machine
Translation. Our main results are as follows.
? The technique of weighted features in distance-weighted nearest neighbour
algorithm performs better than other Machine Learning Techniques in the
task of recognition of MWEs of V-N type.
? We show that the correlation between the ranks computed by the classi-
fier and human ranking is significantly better than the correlation between
ranking of individual features and human ranking.
? The properties ?Distributed frequency of object? and ?Nearest MI? contribute
greatly to the recognition of the non-compositional MWEs of the V-N type
and to the ranking of the V-N collocations based on their relative composi-
tionality.
Relative Compositionality of Multi-word Expressions 563
Our future work will consist of the following tasks
? Evaluate the effectiveness of the techniques developed in this paper for ap-
plications like Machine Translation.
? Improve our annotation guidelines and create more annotated data.
? Extend our approach to other types of MWEs.
Acknowledgements
We want to thank Libin Shen and Nikhil Dinesh for their help in clarifying
various aspects of Machine Learning Techniques. We would like to thank Roder-
ick Saxey and Pranesh Bhargava for annotating the data and Mark Mandel for
considerable editorial help.
References
1. Abeille, Anne . Light verb constuctions and extraction out of NP in a tree adjoining
grammar. Papers of the 24th Regional Meeting of the Chicago Linguistics Society.
(1988)
2. Akimoto, Monoji . Papers of the 24th Regional Meeting of the Chicago Linguistics
Society. Shinozaki Shorin . (1989)
3. Baldwin, Timothy and Bannard, Colin and Tanaka, Takaaki and Widdows, Do-
minic . An Empirical Model of Multiword Expression . Proceedings of the ACL-
2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment.
(2003)
4. Bannard, Colin and Baldwin, Timothy and Lascarides, Alex . A Statistical Ap-
proach to the Semantics of Verb-Particles . Proceedings of the ACL-2003 Workshop
on Multiword Expressions: Analysis, Acquisition and Treatment. (2003)
5. Bikel, Daniel M. . A Distributional Analysis of a Lexicalized Statistical Parsing
Model . Proceedings of EMNLP . (2004)
6. Becker, Joseph D. . The Phrasal Lexicon . Theoritical Issues of NLP, Workshop in
CL, Linguistics, Psychology and AI, Cambridge, MA. (1975)
7. Breidt, Elisabeth . Extraction of V-N-Collocations from Text Corpora: A Feasibil-
ity Study for German . CoRR-1996 . (1995)
8. Church, K. and Gale, W. and Hanks, P. and Hindle, D. . Parsing, word associations
and typical predicate-argument relations . Current Issues in Parsing Technology.
Kluwer Academic, Dordrecht, Netherlands, 1991 . (1991)
9. Church, K. and Patrick Hanks . Word Association Norms, Mutual Information,
and Lexicography . Proceedings of the 27th. Annual Meeting of the Association
for Computational Linguistics, 1990 . (1989)
10. Dunning, Ted . Accurate Methods for the Statistics of Surprise and Coincidence .
Computational Linguistics - 1993 . (1993)
11. Stefan Evert and Brigitte Krenn . Methods for the Qualitative Evaluation of Lexical
Association Measures . Proceedings of the ACL - 2001 . (2001)
12. Charles Fillmore . An extremist approach to multi-word expressions . A talk given
at IRCS, University of Pennsylvania, 2003. (2003)
13. Fontenelle and Bruls, Th. W. and Thomas, L. and Vanallemeersch, T. and Jansen,
J. . Survey of collocation extraction tools . Deliverable D-1a, MLAP-Project 93-19
DECIDE, University of Liege, Belgium. (1994)
564 S. Venkatapathy and A.K. Joshi
14. Diaz-Galiano, M.C. and Martin-Valdivia, M.T. and Martinez-Santiago, F. and
Urena-Lopez, L. A. . Multi-word Expressions Recognition with the LVQ Algorithm.
Proceedings of Methodologies and Evaluation of Multiword Unit in Real-world Ap-
plications, LREC, 2004 . (2004)
15. Joachims, T. . Making large-Scale SVM Learning Practical . Advances in Kernel
Methods - Support Vector Learning . (1999)
16. Joachims, T. . Optimizing Search Engines Using Clickthrough Data. Advances
in Kernel Methods - Support Vector Learning edings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD), ACM, 2002. (2002)
17. Kilgariff, A. and Rosenzweig, J. . Framework and Results for English Senseval .
Computers and the Humanities, 2000 . (2000)
18. Dekang Lin . Automatic Identification of non-compositonal phrases. Proceedings
of ACL- 99, College Park, USA . (1999)
19. McCarthy, D. and Keller, B. and Carroll, J. . Detecting a Continuum of Composi-
tionality in Phrasal Verbs . Proceedings of the ACL-2003 Workshop on Multi-word
Expressions: Analysis, Acquisition and Treatment, 2003. (2003)
20. Mitchell, T. Instance-Based Learning . Machine Learning, McGraw-Hill Series in
Computer Science, 1997 . (1997)
21. Moore, A. W. and Lee, M.S. . Proceedings of the 11 International Conference on
Machine Learning, 1994. (1994)
22. Nunberg, G. and Sag, I. A. and Wasow, T. . Idioms . Language, 1994 . (1994)
23. Sag, I. A. and Baldwin, Timothy and Bond, Francis and Copestake, Ann and
Flickinger, Dan. . Multi-word expressions: a pain in the neck for nlp . Proceedings
of CICLing , 2002 . (2002)
24. Schone, Patrick and Jurafsky, Dan. Is Knowledge-Free Induction of Multiword Unit
Dictionary Headwords a Solved Problem? . Proceedings of EMNLP , 2001 . (2001)
25. Schuler, William and Joshi, Aravind K. Relevance of tree rewriting systems for
multi-word expressions. To be published. (2005)
26. Smadja, F. . Retrieving Collocations from Text : Xtract . Computational Linguis-
tics - 1993 . (1993)
27. Tapanainen, Pasi and Piitulaine, Jussi and Jarvinen, Timo Idiomatic object usage
and support verbs . 36th Annual Meeting of the Association for Computational
Linguistics . (1998)
28. Venkatapathy, Sriram and Joshi, Aravind K. Recognition of Multi-word Expres-
sions: A Study of Verb-Noun (V-N) Collocations. Proceedings of the International
Conference on Natural Language Processing, 2004. (2004)
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 19?24,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Sentence Realisation from Bag of Words with dependency constraints
Karthik Gali, Sriram Venkatapathy
Language Technologies Research Centre,
IIIT-Hyderabad, Hyderabad, India
{karthikg@students,sriram@research}.iiit.ac.in
Abstract
In this paper, we present five models for sentence
realisation from a bag-of-words containing mini-
mal syntactic information. It has a large variety
of applications ranging from Machine Translation
to Dialogue systems. Our models employ simple
and efficient techniques based on n-gram Language
modeling.
We evaluated the models by comparing the syn-
thesized sentences with reference sentences using
the standard BLEU metric(Papineni et al, 2001).
We obtained higher results (BLEU score of 0.8156)
when compared to the state-of-art results. In fu-
ture, we plan to incorporate our sentence realiser in
Machine Translation and observe its effect on the
translation accuracies.
1 Introduction
In applications such as Machine Translation (MT)
and Dialogue Systems, sentence realisation is a ma-
jor step. Sentence realisation involves generating a
well-formed sentence from a bag of lexical items.
These lexical items may be syntactically related to
each other. The level of syntactic information at-
tached to the lexical items might vary with applica-
tion. In order to appeal to the wide range of applica-
tions that use sentence realisation, our experiments
assume only basic syntactic information, such as un-
labeled dependency relationships between the lexi-
cal items.
In this paper, we present different models for sen-
tence realisation. These models consider a bag of
words with unlabelled dependency relations as input
and apply simple n-gram language modeling tech-
niques to get a well-formed sentence.
We now present the role of a sentence realiser
in the task of MT. In transfer-based approaches for
MT1 (Lavie et al, 2003), the source sentence is
first analyzed by a parser (a phrase-structure or a
dependency-based parser). Then the source lexical
items are transferred to the target language using a
bi-lingual dictionary. The target language sentence
is finally realised by applying transfer-rules that map
the grammar of both the languages. Generally, these
transfer rules make use of rich analysis on the source
side such as dependency labels etc. The accuracy of
having such rich analysis (dependency labeling ) is
low and hence, might affect the performance of the
sentence realiser. Also, the approach of manually
constructing transfer rules is costly, especially for
divergent language pairs such as English and Hindi
or English and Japanese. Our models can be used
in this scenario, providing a robust alternative to the
transfer rules.
A sentence realiser can also be used in the frame-
work of a two-step statistical machine translation.
In the two-step framework, the semantic transfer
and sentence realisation are decoupled into indepen-
dent modules. This provides an opporunity to de-
velop simple and efficient modules for each of the
steps. The model for Global Lexical Selection and
Sentence Re-construction (Bangalore et al, 2007)
is one such approach. In this approach, discrimi-
native techniques are used to first transfer semantic
information of the source sentence by looking at the
source sentence globally, this obtaining a accurate
bag-of-words in the target language. The words in
the bag might be attached with mild syntactic infor-
mation (ie., the words they modify) (Venkatapathy
and Bangalore, 2007). We propose models that take
1http://www.isi.edu/natural-language/mteval/html/412.html
19
this information as input and produce the target sen-
tence. We can also use our sentence realiser as an
ordering module in other approaches such as (Quirk
et al, 2005), where the goal is to order an unordered
bag (of treelets in this case) with dependency links.
In Natural Language Generation applications
such as Dialogue systems etc, the set of concepts
and the dependencies between the concepts is ob-
tained first which is known as text planning. These
concepts are then realized into words resulting in a
bag of words with syntactic relations (Bangalore and
Rambow, 2000). This is known as sentence plan-
ning. In the end, the surface string can be obtained
by our models.
In this paper, we do not test our models with any
of the applications mentioned above. However, we
plan to test our models with these applications, es-
pecially on the two-stage statistical MT approach
using the bag-of-words obtained by Global Lexi-
cal Selection (Bangalore et al, 2007),(Venkatapathy
and Bangalore, 2007). Here, we test our models in-
dependent of any application, by beginning with a
given bag-of-words (with dependency links).
The structure of the paper is as follows. We give
an overview of the related work in section 2. In sec-
tion 3, we talk about the effect of dependency con-
straints and gives details of the experimental setup in
section 4. In section 5, we describe about the exper-
iments that have been conducted. In section 6, our
experimental results are presented. In section 7, we
talk about the possible future work and we conclude
with section 8.
2 Related Work
There have been approaches for sentence realisation
such as FUF/SURGE (Elhadad, 1991), OpenCCG
(White, 2004) and XLE (Crouch et al, 2007)
that apply hand-crafted grammars based on partic-
ular linguistic theories. These approaches expect
rich syntactic information as input in order to re-
alise the sentence. There are other approaches in
which the generation grammars are extracted semi-
automatically (Belz, 2007) or automatically (such as
HPSG (Nakanishi and Miyao, 2005), LFG (Cahill
and van Genabith, 2006; Hogan et al, 2007) and
CCG (White et al, 2007)). The limitation of these
approaches is that these cannot be incorporated into
a wide range of applications as they rely on rich
syntactic information for generation. On the con-
trary, we use simple n-gram models to realise (or lin-
earize) a bag-of-words where the only information
available is the presence of various links between the
words.
Our work is similar to a recently published work
by Guo (Guo et al, 2008). They use n-gram models
to realise sentences from the f-structures of HPSG
(equivalent to labeled dependency structure). Their
models rely heavily on the dependency relation la-
bels (also called grammatical roles) available in
HPSG. However, the dependency role information
(of any dependency formalism) is either not read-
ily available in a variety of applications in NLP. We
propose to explore the realisation of a sentence us-
ing minimal syntactic information. Apart from de-
pendency links, we also make use of part-of-speech
tags which are easily available and hence, our sen-
tence realiser can be plugged much easily into var-
ious applications. Guo (Guo et al, 2008) conduct
their experiments by considering gold data as input.
Apart from using gold data as input, we also con-
duct experiments by assuming noisy input data to
test the robustness of our models. The search al-
gorithm used by both Guo and us is locally greedy
i.e., we compute the best string at every node. Guo
uses the Viterbi algorithm to get best string whereas
we consider and score all permutations to obtain the
best string.
There has been burgeoning interest in the prob-
abilistic models for sentence realisation, especially
for realisation ranking in a two stage sentence real-
isation architecture where in the first stage a set of
sentence realisations are generated and then a real-
isation ranker will choose the best of them (Banga-
lore and Rambow, 2000).
One major observation in our experiments was
that the POS tags held immensely in the task of sen-
tence realisation.
3 Effect of Dependency Constraints
There is a major advantage in using dependency
constraints for sentence realisation. The search
space reduces drastically when the constraints are
applied. These constraints state that the realised sen-
tences should be projective with respect to the de-
20
pendency structure (unordered) of the input bag-of-
words ie.., any word and its children in the depen-
dency tree should project as a contiguous unit in the
realised sentence. This is a safe assumption to make
as the non-projectivity in English is only used to
account for Long-Distance Dependencies and such
cases are low in number (Guo et al, 2008).
is going
Ram to school
Figure 1: Bag of words with dependency constraints
and head marked
We now present an example to show how the de-
pendency constraints reduce the search space. For
example, consider an unordered dependency tree in
Figure 1, which has five words. If we don?t use the
constraints provided by the dependency tree then the
search space is 5! (120). But, if we use the con-
straints provided by the dependency tree then the
search space is 2! + 4! = 28. There is a huge reduc-
tion in the search space if we use the constraints pro-
vided by the dependency tree. Further, it has been
shown in (Chang and Toutanova, 2007) that apply-
ing the constraints also aids for the synthesis of bet-
ter constructed sentences.
4 Experimental Set-up
For the experiments, we use the WSJ portion of the
Penn tree bank (Marcus et al, 1993), using the stan-
dard train/development/test splits, viz 39,832 sen-
tences from 2-21 sections, 2416 sentences from sec-
tion 23 for testing and 1,700 sentences from sec-
tion 22 for development. The input to our sen-
tence realiser are bag of words with dependency
constraints which are automatically extracted from
the Penn treebank using head percolation rules used
in (Magerman, 1995), which do not contain any or-
der information. We also use the provided part-of-
speech tags in some experiments.
In a typical application, the input to the sentence
realiser is noisy. To test the robustness of our models
in such scenarios, we also conduct experiments with
noisy input data. We parse the test data with an un-
labelled projective dependency parser (Nivre et al,
2006) and drop the order information to obtain the
input to our sentence realiser. However we still use
the correct bag of words. We propose to test this as-
pect in future by plugging our sentence realiser in
Machine Translation.
Table 1 shows the number of nodes having a par-
ticular number of children in the test data.
Children countNodes Children countNodes
0 30219 5 1017
1 13649 6 685
2 5887 7 269
3 3207 8 106
4 1526 > 8 119
Table 1: The number of nodes having a particular
number of children in the test data
From Table 1, we can see that more than 96% of
the internal nodes of the trees contain five or less
children. It means that for almost all the nodes, the
reordering complexity is minimal. This makes this
approach very feasible if the order of a sub-tree is
computed after the order of the sub-trees of its chil-
dren is fixed. Hence, the approaches that we present
in the next section use bottom-up traversal of the
tree. During the traversal, the appropriate order of
every sub-tree is fixed.
5 Experiments
The task here is to realise a well formed sentence
from a bag of words with dependency constraints
(unordered dependency tree) for which we propose
five models using n-gram based Language modeling
techinque. We train the language models of order 3
using Good-Turning smoothing on the training data
of Penn Treebank.
5.1 Model 1 : Sentential Language Model
We traverse the tree in bottom up manner and find
the best phrase at each subtree. The best phrase cor-
responding to the subtree is assigned to the root node
of the sub-tree during the traversal.
Let the node n have N children represented as ci
(1 < i < N ). During the bottom up traversal, the
21
children ci are assigned best phrases before process-
ing node n. Let the best phrases corresponding to the
children be p(ci). The best phrase corresponding to
the node n is computed by exploring the permuta-
tions of n and the best phrases p(ci) corresponding
to the children ci. The total number of permutations
that are explored are (N+1)!. A sentential language
model is applied on each of the candidate phrases to
select the best phrase.
p(n) = bestPhrase ( perm (n, ? i p(ci)) o LM )
(1)
In Sentential Language Model, we used a LM that
is trained on complete sentences of the training cor-
pus to score the permutations.
5.2 Model 2 : Subtree-type based Language
Models(STLM)
The major problem with model 1 is that we are us-
ing a common sentential language model (trained on
complete sentences) to score phrases corresponding
to various sub-tree types. In this model, we build
different LMs for phrases corresponding to different
subtree-types.
To build STLMs, the training data is parsed first.
Each subtree in the parse structure is represented
by the part-of-speech tag of its head. Different lan-
guage models are created for each of the POS tags.
We have 44 different language models each corre-
sponding to a particular POS tag. For example, a
IN language model contains phrases like in hour, of
chaos, after crash, in futures, etc and VBD language
model contains phrases like were criticized, never
resumed while training.
So, in this model we realise a sentence from a
unordered dependency tree by traversing the depen-
dency tree in bottom-up manner as we did in model
1; but while scoring the permuted phrases we use
different language models for subtrees headed by
words of various pos tags.
p(n) = bestPhrase ( perm (n, ? i p(ci)) o LMPOS(n) )
(2)
Here, LMPOS(n) represents the language model
associated with the part-of-speech of the node n.
5.3 Model 3 : Head-word STLM
In the models presented earlier, a node and its chil-
dren are ordered using the best phrases of the chil-
dren. For example, the best phrase assigned to the
node ?was? is computed by taking of the permutation
of ?was? and its children ?The equity market?, ?illiq-
uid? and ?.? and then applying the language model.
In model 3, instead of considering best phrases while
ordering, the heads of the the children ci are consid-
ered. For example, the best phrase assigned to the
node ?was? is computed by first permuting the nodes
?was?, ?market?, ?illiquid? and ?.? and then apply-
ing the language models trained on the treelets (head
and children) and not on entire sub-trees.
The major advantage of using this model is that
order at a node is independent of the best phrases of
its descendants and also any mistakes in computa-
tion of best phrases of descendants doesn?t effect the
choice of reordering decision at a particular node.
5.4 Model 4 : POS based STLM
We now experiment by using Part-Of-Speech (POS)
tags of words for ordering the nodes. In the previ-
ous approaches, the language models were trained
on the words which were then used to compute the
best strings associated with various nodes. Here,
we order the node and its children using a language
model trained on POS tag sequences. The motiva-
tion behind buliding such kind of Language models
is that it deals with unseen words effectively. Hence,
in this model, the best phrase corresponding to the
node ?was? is obtained by permuting the POS tags
of the words ?was?, ?market?, ?illiquid and ?.? which
are ?VBZ?, ?NN?, ?NN? and ?.? respectively. As the
best POS tag sequence might correspond to several
orderings of the treelet, a word based STLM is ap-
plied to choose the correct ordering.
The major advantages of this model is that it is
more general and it deals with unseen words effec-
tively. Also, it is much faster than earlier models as
this model is a POS tag based model.
5.5 Model 5: Head-marked POS based STLM
In POS based STLM, the head of a particular node
isn?t marked while applying the language model.
Hence, all the nodes of the treelet are treated equally
while applying the LM. For example, in Figure 2, the
structures of treelets is not taken into account while
applying the head-POS based language model. Both
are treated in the same manner while applying TLM.
In this model, we experiment by marking the head
22
information for the POS of the head word which
treats the treelets in Figure 2 in a different manner to
obtain the best phrase. As the best POS tag sequence
might correspond to several orderings of the treelet,
we test various word-based approaches to choose the
best ordering among the many possibilities. The best
approach was the one where head-word of the treelet
had the POS tag attached to it.
VB
VBP NN
VBP
VB NN
Figure 2: Two different treelets which would have
same best POS tag sequence
6 Results and Discussion
To evaluate our models, we compare the system gen-
erated sentences with reference sentences and get
the BLEU score. As mentioned in section 4, We
evaluate our models on two different types of in-
put. In the first input type, we have bag of words
with dependency constraints extracted from tree-
bank and in the second input type, the dependency
constraints among the bag of words are extracted
from the parser which are noisy. Table 2 shows the
results of model 1-5.
Model Treebank(gold) Parser(noisy)
Model 1 0.5472 0.5514
Model 2 0.6886 0.6870
Model 3 0.7284 0.7227
Model 4 0.7890 0.7783
Model 5 0.8156 0.8027
Table 2: The results of Model 1-5
We can observe that in model 1, BLEU score of
the parser input is high when compared to Treebank
input. This might be because, the parser input is pro-
jective (as we used projective parsing) whereas the
treebank input might contain some non-projective
cases. In general, for all the models, the results with
noisy dependency links are comparable to the cases
where gold dependency links are used which is en-
couraging.
We have taken the Table-3 from (Guo et al,
2008), which shows the BLEU scores of different
Paper BLEU score
Langkilde(2002) 0.757
Nakanishi(2005) 0.705
Cahill(2006) 0.6651
Hogan(2007) 0.6882
White(2007) 0.5768
Guo(2008) 0.7440
Our Model 0.8156
Table 3: Comparsion of results for English WSJ sec-
tion 23
systems on section 23 of PTB. Its really difficult to
compare sentence realisers as the information con-
tained in the input vaires greatly between systems.
But, we can clearly see that the our system performs
better than all the systems. The main observations
from the results are, (1) Searching the entire space of
O(n!) helps, (2) Treelet LM capture characteristics
of phrases headed by various POS tags, in contrast to
sentential LM which is a general LM, (3) POS tags
can play an important role in ordering nodes of a de-
pendency structure, (4) The head models performed
better than the models that used all the nodes of the
sub-tree, and (5) Marking the head of a treelet pro-
vides vital clues to the language model for reorder-
ing.
7 Future Experiments
Although the results of the proposed models are
much higher when compared to other methods, the
major constraint with our models is the computa-
tional complexity, which is O(n!). However, our ap-
proach is still tractable because of the low values of
n. We plan to reduce the search space complexity by
using Viterbi search (Guo et al, 2008), and examine
the drop in results because of that.
The models proposed in paper, consider only the
locally best phrases (local to the sub-tree) at every
step. In order to retain the globally best possibilities
at every step, we plan to use beam search, where we
retain K-best best phrases for every sub-tree.
Also, the goal is to test the approach for
morphologically-rich languages such as Hindi.
Also, it would require us to expand our features set.
We also plan to test the factored models.
The most important experiment that we plan to
23
perform is to test our system in the context of MT,
where the input is more real and noisy.
To train more robust language models, we plan to
use the much larger data on a web scale.
8 Conclusion
In this paper, we had experimented with five ngram
based models for sentence realisation from bag of
words with dependency constraints. We have evalu-
ated our models on two different types of input(gold
and noisy). From the results, we can conclude that
the model ?Marked Head-POS based LM? works
best in both cases.
Acknowledgments
The authors of this work were supported by ILMT
grant 11(10)/2006-HCC(TDIL) and EILMT grant
11(9)/2006HCC(TDIL). We would also like to thank
the four reviewers for their valuable reviews.
References
S. Bangalore and O. Rambow. 2000. Exploiting a proba-
bilistic hierarchical model for generation. Proceedings
of the 18th conference on Computational linguistics.
S. Bangalore, P. Haffner, and S. Kanthak. 2007. Statisti-
cal Machine Translation through Global Lexical Selec-
tion and Sentence Reconstruction. In Annual Meeting
- ACL, volume 45.
A. Belz. 2007. Probabilistic Generation of Weather
Forecast Texts. In Proceedings of NAACL HLT.
A. Cahill and J. van Genabith. 2006. Robust
PCFG-Based Generation Using Automatically Ac-
quired LFG Approximations. In ANNUAL MEETING-
ASSOCIATION FOR COMPUTATIONAL LINGUIS-
TICS, volume 44.
P.C. Chang and K. Toutanova. 2007. A Discriminative
Syntactic Word Order Model for Machine Translation.
Proceedings of the 45th Annual Meeting of the ACL.
D. Crouch, M. Dalrymple, R. Kaplan, T. King,
J. Maxwell, and P. Newman. 2007. XLE documen-
tation. Available on-line.
M. Elhadad. 1991. FUF: The universal unifier user man-
ual version 5.0. Department of Computer Science,
Columbia University. New York.
Y. Guo, J. van Genabith, and H. Wang. 2008.
Dependency-Based N-Gram Models for General Pur-
pose Sentence Realisation. Proceedings of the 22nd
conference on Computational linguistics.
D. Hogan, C. Cafferkey, A. Cahill, and J. van Gen-
abith. 2007. Exploiting Multi-Word Units in History-
Based Probabilistic Generation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
A. Lavie, S. Vogel, L. Levin, E. Peterson, K. Probst,
A.F. Llitjo?s, R. Reynolds, J. Carbonell, and R. Cohen.
2003. Experiments with a Hindi-to-English transfer-
based MT system under a miserly data scenario. ACM-
TALIP, 2(2).
D.M. Magerman. 1995. Statistical decision-tree models
for parsing. In Proceedings of the 33rd annual meeting
on ACL. ACL Morristown, NJ, USA.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the penn treebank. Computational Linguistics, 19(2).
H. Nakanishi and Y. Miyao. 2005. Probabilistic models
for disambiguation of an HPSG-based chart generator.
In Proceedings of the International Workshop on Pars-
ing Technology.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Proceedings of the
Tenth CoNLL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. Proceedings of the 40th Annual Meeting
on ACL.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: syntactically informed
phrasal SMT. Proceedings of the 43rd Annual Meet-
ing of ACL.
S. Venkatapathy and S. Bangalore. 2007. Three mod-
els for discriminative machine translation using Global
Lexical Selection and Sentence Reconstruction. In
Proceedings of SSST, NAACLHLT/AMTA Workshop on
Syntax and Structure in Statistical Translation, pages
152?159.
M. White, R. Rajkumar, and S. Martin. 2007. Towards
Broad Coverage Surface Realization with CCG. In
Proceedings of the Workshop on Using Corpora for
NLG: Language Generation and Machine Translation
(UCNLG+ MT).
M. White. 2004. Reining in CCG Chart Realization.
LECTURE NOTES IN COMPUTER SCIENCE.
24
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 165?168, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Inferring semantic roles using sub-categorization frames and
maximum entropy model
Akshar Bharati, Sriram Venkatapathy and Prashanth Reddy
Language Technologies Research Centre, IIIT - Hyderabad, India.
{sriram,prashanth}@research.iiit.ac.in
Abstract
In this paper, we propose an approach
for inferring semantic role using sub-
categorization frames and maximum
entropy model. Our approach aims to
use the sub-categorization information
of the verb to label the mandatory ar-
guments of the verb in various possi-
ble ways. The ambiguity between the
assignment of roles to mandatory argu-
ments is resolved using the maximum
entropy model. The unlabelled manda-
tory arguments and the optional argu-
ments are labelled directly using the
maximum entropy model such that their
labels are not one among the frame el-
ements of the sub-categorization frame
used. Maximum entropy model is pre-
ferred because of its novel approach
of smoothing. Using this approach,
we obtained an F-measure of 68.14%
on the development set of the data
provided for the CONLL-2005 shared
task. We show that this approach per-
forms well in comparison to an ap-
proach which uses only the maximum
entropy model.
1 Introduction
Semantic role labelling is the task of assigning
appropriate semantic roles to the arguments of
a verb. The semantic role information is impor-
tant for various applications in NLP such as Ma-
chine Translation, Question Answering, Informa-
tion Extraction etc. In general, semantic role in-
formation is useful for sentence understanding.
We submitted our system for closed challenge
at CONLL-2005 shared task. This task encour-
ages participants to use novel machine learning
techniques suited to the task of semantic role la-
belling. Previous approaches on semantic role
labelling can be classified into three categories
(1) Explicit Probabilistic methods (Gildea and
Jurafsky, 2002). (2) General machine learning
algorithms (Pradhan et al, 2003) (Lim et al,
2004) and (3) Generative model (Thompson et
al., 2003).
Our approach has two stages; first, identifica-
tion whether the argument is mandatory or op-
tional and second, the classification or labelling
of the arguments. In the first stage, the arguments
of a verb are put into three classes, (1) mandatory,
(2) optional or (3) null. Null stands for the fact
that the constituent of the verb in the sentence is
not an semantic argument of the verb. It is used to
rule out the false argument of the verb which were
obtained using the parser. The maximum entropy
based classifier is used to classify the arguments
into one of the above three labels.
After obtaining information about the nature of
the non-null arguments, we proceed in the second
stage to classify the mandatory and optional ar-
guments into their semantic roles. The propbank
sub-categorization frames are used to assign roles
to the mandatory arguments. For example, in the
sentence ?John saw a tree?, the sub-categorization
frame ?A0 v A1? would assign the roles A0 to
John and A1 to tree respectively. After using
all the sub-categorization frames of the verb irre-
165
spective of the verb sense, there could be ambigu-
ity in the assignment of semantic roles to manda-
tory arguments. The unlabelled mandatory argu-
ments and the optional arguments are assigned
the most probable semantic role which is not one
of the frame elements of the sub-categorization
frame using the maximum entropy model. Now,
among all the sequences of roles assigned to the
non-null arguments, the sequence which has the
maximum joint probability is chosen. We ob-
tained an accuracy of 68.14% using our approach.
We also show that our approach performs better
in comparision to an approach with uses a simple
maximum entropy model. In section 4, we will
talk about our approach in greater detail.
This paper is organised as follows, (2) Features,
(3) Maximum entropy model, (4) Description of
our system, (5) Results, (6) Comparison with our
other experiments, (7) Conclusion and (8) Future
work.
2 Features
The following are the features used to train the
maximum entropy classifier for both the argument
identification and argument classification. We
used only simple features for these experiments,
we are planning to use richer features in the near
future.
1. Verb/Predicate.
2. Voice of the verb.
3. Constituent head and Part of Speech tag.
4. Label of the constituent.
5. Relative position of the constituent with re-
spect to the verb.
6. The path of the constituent to the verb
phrase.
7. Preposition of the constituent, NULL if it
doesn?t exist.
3 Maximum entropy model
The maximum entropy approach became the pre-
ferred approach of probabilistic model builders
for its flexibility and its novel approach to
smoothing (Ratnaparakhi, 1999).
Many classification tasks are most naturally
handled by representing the instance to be classi-
fied as a vector of features. We represent features
as binary functions of two arguments, f(a,H),
where ?a? is the observation or the class and ?H? is
the history. For example, a feature fi(a, H) is true
if ?a? is Ram and ?H? is ?AGENT of a verb?. In a
log linear model, the probability function P (a|H)
with a set of features f1, f2, ....fj that connects ?a?
to the history ?H?, takes the following form.
P (a|H) = e
?
i ?i(a,H)?fi(a,H)
Z(H)
Here ?i?s are weights between negative and
positive infinity that indicate the relative impor-
tance of a feature: the more relevant the feature to
the value of the probability, the higher the abso-
lute value of the associated lambda. Z(H), called
the partition function, is the normalizing constant
(for a fixed H).
4 Description of our system
Our approach labels the semantic roles in two
stages, (1) argument identification and (2) ar-
gument classification. As input to our sys-
tem, we use full syntactic information (Collins,
1999), Named-entities, Verb senses and Propbank
frames. For our experiments, we use Zhang Le?s
Maxent Toolkit 1, and the L-BFGS parameter esti-
mation algorithm with Gaussian prior smoothing
(Chen and Rosenfield, 1999).
4.1 Argument identification
The first task in this stage is to find the candidate
arguments and their boundaries using a parser.
We use Collins parser to infer a list of candidate
arguments for every predicate. The following are
some of the sub-stages in this task.
? Convert the CFG tree given by Collins parser
to a dependency tree.
? Eliminate auxilliary verbs etc.
? Mark the head of relative clause as an argu-
ment of the verb.
1http://www.nlplab.cn/zhangle/maxent toolkit.html
166
? If a verb is modified by another verb, the
syntactic arguments of the superior verb
are considered as shared arguments between
both the verbs.
? If a prepositional phrase attached to a verb
contains more than one noun phrase, attach
the second noun phrase to the verb.
The second task is to filter out the constituents
which are not really the arguments of the pred-
icate. Given our approach towards argument
classification, we also need information about
whether an argument is mandatory or optional.
Hence, in this stage the constituents are marked
using three labels, (1) MANDATORY argument,
(2) OPTIONAL argument and (3) NULL, using a
maximum entropy classifier. For example, a sen-
tence ?John was playing football in the evening?,
?John? is marked MANDATORY, ?football? is
marked MANDATORY and ?in the evening? is
marked OPTIONAL.
For training, the Collins parser is run on the
training data and the syntactic arguments are
identified. Among these arguments, the ones
which do not exist in the propbank annotation of
the training data are marked as null. Among the
remaining arguments, the arguments are marked
as mandatory or optional according to the prop-
bank frame information. Mandatory roles are
those appearing in the propbank frames of the
verb and its sense, the rest are marked as optional.
A propbank frame contains information as illus-
trated by the following example:
If Verb = play, sense = 01,
then the roles A0, A1 are MANDATORY.
4.2 Argument classification
Argument classification is done in two steps. In
the first step, the propbank sub-categorization
frames are used to assign the semantic roles to the
mandatory arguments in the order specified by the
sub-categorization frames. Sometimes, the num-
ber of mandatory arguments of a verb in the sen-
tence may be less than the number of roles which
can be assigned by the sub-categorization frame.
For example, in the sentence
?MAN1 MAN2 V MAN3 OPT1?, roles could
be assigned in the following two possible ways by
the sub-categorization frame ?A0 v A1? of verb
V1.
? A0[MAN1] MAN2 V1 A1[MAN3] OPT1
? MAN1 A0[MAN2] V A1[MAN3] OPT1
In the second step, the task is to label the un-
labelled mandatory arguments and the arguments
which are marked as optional. This is done by
marking these arguments with the most probable
semantic role which is not one of the frame ele-
ments of the sub-categorization frame ?A0 v A1?.
In the above example, the unlabelled mandatory
arguments and the optional arguments cannot be
labelled as either A0 or A1. Hence, after this step,
the following might be the role-labelling for the
sentence ?MAN1 MAN2 V1 MAN3 OPT1?.
? A0[MAN1] AM-TMP[MAN2] V1
A1[MAN3] AM-LOC[OPT1]
? AM-MNC[MAN1] A0[MAN2] V1
A1[MAN3] AM-LOC[OPT1]
The best possible sequence of semantic roles
(R?) is decided by the taking the product of prob-
abilities of individual assignments. This also dis-
ambiguates the ambiguity in the assignment of
mandatory roles. The individual probabilities are
computed using the maximum entropy model.
For a sequence ~R, the product of the probabilities
is defined as
P (~R) = ?Ri?~RP (Ri|Argi)
The best sequence of semantic roles R? is de-
fined as
R? = argmax P (~R)
For training the maximum entropy model, the
outcomes are all the possible semantic roles. The
list of sub-categorization frames for a verb is ob-
tained from the training data using information
about mandatory roles from the propbank. The
propbank sub-categorization frames are also ap-
pended to this list.
We present our results in the next section.
167
Precision Recall F?=1
Development 71.88% 64.76% 68.14
Test WSJ 73.76% 65.52% 69.40
Test Brown 65.25% 55.72% 60.11
Test WSJ+Brown 72.66% 64.21% 68.17
Test WSJ Precision Recall F?=1
Overall 73.76% 65.52% 69.40
A0 85.17% 73.34% 78.81
A1 74.08% 66.08% 69.86
A2 54.51% 48.47% 51.31
A3 52.54% 35.84% 42.61
A4 71.13% 67.65% 69.35
A5 25.00% 20.00% 22.22
AM-ADV 52.18% 47.23% 49.59
AM-CAU 60.42% 39.73% 47.93
AM-DIR 45.65% 24.71% 32.06
AM-DIS 75.24% 73.12% 74.17
AM-EXT 73.68% 43.75% 54.90
AM-LOC 50.80% 43.53% 46.88
AM-MNR 47.24% 49.71% 48.44
AM-MOD 93.67% 91.29% 92.46
AM-NEG 94.67% 92.61% 93.63
AM-PNC 42.02% 43.48% 42.74
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 74.13% 66.97% 70.37
R-A0 82.27% 80.80% 81.53
R-A1 73.28% 61.54% 66.90
R-A2 75.00% 37.50% 50.00
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 100.00% 57.14% 72.73
R-AM-MNR 25.00% 16.67% 20.00
R-AM-TMP 70.00% 53.85% 60.87
V 97.28% 97.28% 97.28
Table 1: Overall results (top) and detailed results
on the WSJ test (bottom).
5 Results
The results of our approach are presented in table
1.
When we used an approach which uses a sim-
ple maximum entropy model, we obtained an F-
measure of 67.03%. Hence, we show that the
sub-categorization frames help in predicting the
semantic roles of the mandatory arguments, thus
improving the overall performance.
6 Conclusion
In this paper, we propose an approach for in-
ferring semantic role using sub-categorization
frames and maximum entropy model. Using this
approach, we obtained an F-measure of 68.14%
on the development set of the data provided for
the CONLL-2005 shared task.
7 Future work
We have observed that the main limitation of our
system was in argument identification. Currently,
the recall of the arguments inferred from the out-
put of the parser is 75.52% which makes it the up-
per bound of recall of our system. In near future,
we would focus on increasing the upper bound
of recall. In this direction, we would also use
the partial syntactic information. The accuracy
of the first stage of our approach would increase
if we include the mandatory/optional information
for training the parser (Yi and Palmer, 1999).
8 Acknowledgements
We would like to thank Prof. Rajeev Sangal, Dr.
Sushama Bendre and Dr. Dipti Misra Sharma for
guiding us in this project. We would like to thank
Szu-ting for giving some valuable advice.
References
S. Chen and R. Rosenfield. 1999. A gaussian prior for
smoothing maximum entropy models.
M. Collins. 1999. Head driven statistical models for
natural language processing.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles.
Hwang Young Sook Lim, Joon-H and, So-Young Park,
and Hae-Chang Rim. 2004. Semantic role labelling
using maximum entropy model.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James. H. Martin, and Daniel Juraf-
sky. 2003. Support Vector Learning for Semantic
Argument Classification.
Adwait Ratnaparakhi. 1999. Learning to parse natural
language with maximum entropy models.
Cynthia A. Thompson, Roger Levy, and Christo-
pher D. Manning. 2003. A generative model for
semantic role labelling.
Szu-ting Yi and M. Palmer. 1999. The integration of
syntactic parsing and semantic role labeling.
168
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 20?27,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Information about Multi-word Expressions
for the Word-Alignment Task
Sriram Venkatapathy1
Language Technologies Research Center,
Indian Institute of
Information Technology,
Hyderabad, India.
sriramv@linc.cis.upenn.edu
Aravind K. Joshi
Department of Computer and
Information Science and Institute for
Research in Cognitive Science,
University of Pennsylvania, PA, USA.
joshi@linc.cis.upenn.edu
Abstract
It is well known that multi-word expres-
sions are problematic in natural language
processing. In previous literature, it has
been suggested that information about
their degree of compositionality can be
helpful in various applications but it has
not been proven empirically. In this pa-
per, we propose a framework in which
information about the multi-word expres-
sions can be used in the word-alignment
task. We have shown that even simple
features like point-wise mutual informa-
tion are useful for word-alignment task in
English-Hindi parallel corpora. The align-
ment error rate which we achieve (AER =
0.5040) is significantly better (about 10%
decrease in AER) than the alignment error
rates of the state-of-art models (Och and
Ney, 2003) (Best AER = 0.5518) on the
English-Hindi dataset.
1 Introduction
In this paper, we show that measures representing
compositionality of multi-word expressions can
be useful for tasks such as Machine Translation,
word-alignment to be specific here. We use an on-
line learning framework called MIRA (McDon-
ald et al, 2005; Crammer and Singer, 2003) for
training a discriminative model for the word align-
ment task (Taskar et al, 2005; Moore, 2005). The
discriminative model makes use of features which
represent the compositionality of multi-word ex-
pressions.
1At present visiting Institute for Research in Cognitive
Science, University of Pennsylvania, PA, USA.
Multi-word expressions (MWEs) are those
whose structure and meaning cannot be derived
from their component words, as they occur inde-
pendently. Examples include conjunctions such
as ?as well as? (meaning ?including?), idioms like
?kick the bucket? (meaning ?die?) phrasal verbs
such as ?find out? (meaning ?search?) and com-
pounds like ?village community?. They can be de-
fined roughly as idiosyncratic interpretations that
cross word boundaries (Sag et al, 2002).
A large number of MWEs have standard
syntactic structure but are semantically non-
compositional. Here, we consider the class of verb
based expressions (verb is the head of the phrase),
which occur very frequently. This class of verb
based multi-word expressions include verbal id-
ioms, support-verb constructions, among others.
The example ?take place? is a MWE but ?take a
gift? is not.
In the past, various measures have been sug-
gested for measuring the compositionality of
multi-word expressions. Some of these are mu-
tual information (Church and Hanks, 1989), dis-
tributed frequency (Tapanainen et al, 1998) and
Latent Semantic Analysis (LSA) model (Baldwin
et al, 2003). Even though, these measures have
been shown to represent compositionality quite
well, compositionality itself has not been shown to
be useful in any application yet. In this paper, we
explore this possibility of using the information
about compositionality of MWEs (verb based) for
the word alignment task. In this preliminary work,
we use simple measures (such as point-wise mu-
tual information) to measure compositionality.
The paper is organized as follows. In section 2,
we discuss the word-alignment task with respect
to the class of multi-word expressions of interest
in this paper. In section 3, we show empirically,
20
the behavior of verb based expressions in a paral-
lel corpus (English-Hindi in our case). We then
discuss our alignment algorithm in section 4. In
section 5, we describe the features which we have
used in our training model. Section 6 discusses the
training algorithm and in section 7, the results of
our discriminative model for the word alignment
task. Related work and conclusion follow in sec-
tion 8 and 9 respectively.
2 Task: Word alignment of verbs and
their dependents
The task is to align the verbs and their dependents
(arguments and adjuncts) in the source language
sentence (English) with words in the target lan-
guage sentence (Hindi). The dependents of the
verbs in the source sentence are represented by
their head words. Figure 1. shows an example
of the type of multi-word expressions which we
consider for alignment.
subj
obj
 prep_in
event place
took
The cycling
Philadelphia
     (The cycling event took place in Philadelphia) 
Figure 1: Example of MWEs we consider
In the above example, the goal will the to align
the words ?took?, ?event?, ?place? and ?Philadel-
phia? with corresponding word(s) in the target lan-
guage sentence (which is not parsed) using a dis-
criminative approach. The advantage in using the
discriminative approach for alignment is that it lets
you use various compositionality based features
which are crucial towards aligning these expres-
sions. Figure 2. shows the appropriate alignment
of the expression in Figure 1. with the words in the
target language. The pair (take place), in English,
a verb and one of its dependents is aligned with a
single verbal unit in Hindi.
It is essential to obtain the syntactic roles for de-
pendents in the source language sentence as they
are required for computing the compositionality
value between the dependents and their verbs. The
Philadelphia   mein   saikling     kii    pratiyogitaa   hui
Philadelphia
The cycling
took
placeevent
 prep_in
objsubj
Figure 2: Alignment of Verb based expression
syntactic roles on the source side are obtained by
applying simple rules to the output of a depen-
dency parser. The dependency parser which we
used in our experiments is a stochastic TAG based
dependency parser (Shen, 2006). A sentence
could have one or more verbs. We would like
to align all the expressions represented by those
verbs with words in the target language.
3 Behavior of MWEs in parallel corpora
In this section, we will briefly discuss the com-
plexity of the alignment problem based on the
verb based MWE?s. From the word aligned sen-
tence pairs, we compute the fraction of times a
source sentence verb and its dependent are aligned
together with the same word in the target lan-
guage sentence. We count the number of times a
source sentence verb and its dependent are aligned
together with the same word in the target lan-
guage sentence, and divide it by the total num-
ber of dependents. The total size of our word
aligned corpus is 400 sentence pairs which in-
cludes both training and test sentences. The total
number of dependents present in these sentences
are 2209. Total number of verb dependent pairs
which aligned with same word in target language
are 193. Hence, the percentage of such occur-
rences is 9%, which is a significant number.
4 Alignment algorithm
In this section, we describe the algorithm for align-
ing verbs and their dependents in the source lan-
guage sentence with the words in the target lan-
guage. Let V be the number of verbs and A be the
number of dependents. Let the number of words in
21
the target language be N. If we explore all the ways
in which the V + A words in the source sentence
are aligned with words in the target language be-
fore choosing the best alignment, the total number
of possibilites are NV+A. This is computationally
very expensive. Hence, we use a Beam-search al-
gorithm to obtain the K-best alignments.
Our algorithm has three main steps.
1. Populate the Beam : Use the local features
(which largely capture the co-occurence in-
formation between the source word and the
target word) to determine the K-best align-
ments of verbs and their dependents with
words in the target language.
2. Re-order the Beam: Re-order the above
alignments using more complex features
(which include the global features and the
compositionality based feature(s)).
3. Post-processing : Extend the alignment(s) of
the verb(s) (on the source side) to include
words which can be part of the verbal unit
on the target side.
For a source sentence, let the verbs and depen-
dents be denoted by s
ij
. Here i is the index of
the verb (1 <= i <= V ). The variable j is
the index of the dependents (0 <= j <= A)
except when j = 0 which is used to represent
the verb itself. Let the source sentences be de-
noted as S = fs
ij
g and the target sentences by
T = ft
n
g. The alignment from a source sen-
tence S to target sentence T is defined as the map-
ping a = fa
ijn
j a
ijn
 (s
ij
! t
n
);8i; jg. A
beam is used to store a set of K-best alignments
between a source sentence and the target sentence.
It is represented using the symbol B where B
k
(0 <= k <= K) is used to refer to a particular
alignment configuration.
4.1 Populate the Beam
The task in this step is to obtain the K-best can-
didate alignments using local features. The local
features mainly contain the coccurence informa-
tion between a source and a target word and are in-
dependent of other alignment links or words in the
sentences. Let the local feature vector be denoted
as f
L
(s
ij
; t
k
). The score of a particular alignment
link is computed by taking the dot product of the
weight vector W with the local feature vector (of
words connected by the alignment link). Hence,
the local score will be
sore
L
(s
ij
; t
k
) = W:f
L
(s
ij
; t
k
)
The total score of an alignment configuration is
computed by adding the scores of individual links
in the alignment configuration. Hence, the align-
ment score will be
sore
La
(a; S; T ) =
X
sore
L
(s
ij
; t
k
)
8s
ij
2 S & s
ij
! t
k
2 a
We propose an algorithm of order O((V +
A)Nlog(N) + K) to compute the K-best align-
ment configurations. First, the local scores of each
verb and its dependents are computed for each
word in the target sentence and stored in a lo-
cal beam denoted by b
ij
. The local beams cor-
responding to all the verbs and dependents are
then sorted. This operation has the complexity
(V + A) N log(N).
The goal now is to pick the K-best configura-
tions of alignment links. A single slot in the local
beam corresponds to one alignment link. We de-
fine a boundary which partitions each local beam
into two sets of slots. The slots above the bound-
ary represent the slots which have been explored
by the algorithm while slots below the boundary
have still to be explored. The figure 3. shows the
boundary which cuts across the local beams.
Bb (i,j)
Beam
Alignment
Boundary
Local Beams
Figure 3: Boundary
We keep on modifying the boundary untill all
the K slots in the Alignment Beam are filled with
the K-best configurations. At the beginning of the
algorithm, the boundary is a straight line passing
through the top of all the local beams. The top slot
of the alignment beam at the beginning represents
22
the combination of alignment links with the best
local scores.
The next slot b
ij
[p? (from the set of unexplored
slots) to be included in the boundary is the slot
which has the least difference in score from the
score of the slot at the top of its local beam. That
is, we pick the slot b
ij
[p? such that sore(b
ij
[p?) 
sore(b
ij
[1?) is the least among all the unexplored
slots (or alignment links). Trivially, b
ij
[p  1? was
already a part of the boundary.
When the slot b
ij
[p? is included in the boundary,
various configurations, which now contain b
ij
[p?,
are added to the alignment beam. The new con-
figurations are the same as the ones which previ-
ously contained b
ij
[p   1? but with the replace-
ment of b
ij
[p  1? by b
ij
[p?. The above procedure
ensures that the the alignment configurations are
K-best and are sorted according to the scores ob-
tained using local features.
4.2 Re-order the beam
We now use global features to re-order the beam.
The global features look at the properties of the en-
tire alignment configuration instead of alignment
links locally.
The global score is defined as the dot product of
the weight vector and the global feature vector.
sore
G
(a) = W:f
G
(a)
The overall score is calculated by adding the local
score and the global score.
sore(a) = sore
La
(a) + sore
G
(a)
The beam is now sorted based on the overall
scores of each alignment. The alignment config-
uration at the top of the beam is the best possible
alignment between source sentence and the target
sentence.
4.3 Post-processing
The first two steps in our alignment algorithm
compute alignments such that one verb or depen-
dent in the source language side is aligned with
only one word in the target side. But, in the case
of compound verbs in Hindi, the verb in English is
aligned to all the words which represent the com-
pound verb in Hindi. For example, in Figure 3, the
verb ?lost? is aligned to both ?khoo? and ?dii?.
Our alignment algorithm would have aligned
?lost? only to ?khoo?. Hence, we look at the win-
dow of words after the word which is aligned to
mainee    Shyam   ki    kitaaba  khoo   dii
Shyam?s
bookI
lost
Figure 4: Case of compound verb in Hindi
the source verb and check if any of them is a verb
which has not been aligned with any word in the
source sentence. If this condition is satisfied, we
align the source verb to these words too.
5 Parameters
As the number of training examples (294 sen-
tences) is small, we choose to use very representa-
tive features. Some of the features which we used
in this experiment are as follows,
5.1 Local features (F
L
)
The local features which we consider are mainly
co-occurence features. These features estimate the
likelihood of a source word aligning to a target
word based on the co-occurence information ob-
tained from a large sentence aligned corpora1.
1. DiceWords: Dice Coefficient of the source
word and the target word
DCoe (s
ij
; t
k
) =
2  Count(s
ij
; t
k
)
Count(s
ij
) + Count(t
k
)
where Count(s
ij
; t
k
) is the number of times
the word t
k
was present in the translation of
sentences containing the word s
ij
in the par-
allel corpus.
2. DiceRoots: Dice Coefficient of the lemma-
tized forms of the source and target words.
It is important to consider this feature be-
cause the English-Hindi parallel corpus is not
large and co-occurence information can be
learnt effectively only after we lemmatize the
words.
3. Dict: Whether there exists a dictionary entry
from the source word s
ij
to the target word
150K sentence pairs originally collected as part of TIDES
MT project and later refined at IIIT-Hyderabad, India.
23
tk
. For English-Hindi, we used a dictionary
available at IIIT - Hyderabad, India.
4. Null: Whether the source word s
ij
is aligned
to nothing in the target language.
5.2 Global features
The following are the four global features which
we have considered,
 AvgDist: The average distance between the
words in the target language sentence which
are aligned to the verbs in the source lan-
guage sentence . AvgDist is then normalized
by dividing itself by the number of words in
the target language sentence. If the average
distance is small, it means that the verbs in
the source language sentence are aligned with
words in the target language sentence which
are located at relatively close distances, rela-
tive to the length of the target language sen-
tence.
This feature expresses the distribution of
predicates in the target language.
 Overlap: This feature stores the count of
pairs of verbs in the source language sentence
which align with the same word in the target
language sentence. Overlap is normalized by
dividing itself by the total pairs of verbs.
This feature is used to discourage overlaps
among the words which are alignments of
verbs in the source language sentence.
 MergePos: This feature can be considered as
a compositionality based feature. The part
of speech tag of a dependent is essential to
determine the likelihood of the dependent to
align with the same word in the target lan-
guage sentence as the word to which its verb
is aligned.
This binary feature is active when the align-
ment links of a dependent and its verb
merge. For example, in Figure 5., the feature
?merge RP? will be active (that is, merge RP
= 1).
 MergeMI: This is a compositionality based
feature which associates point-wise mutual
information (apart from the POS informa-
tion) with the cases where the dependents
which have the same alignment in the target
He/N away/RP
ran/V
    vaha     bhaaga     gayaa
Figure 5: Example of MergePos feature
language as their verbs. This features which
notes the the compositionality value (repre-
sented by point-wise mutual information in
our experiments) is active if the alignment
links of dependent and its verb merge.
The mutual information (MI) is classified
into three groups depending on its absolute
value. If the absolute value of mutual infor-
mation rounded to nearest integer is in the
range 0-2, it is considered LOW. If the value
is in the range 3-5, it is considered MEDIUM
and if it is above 5, it is considered HIGH.
The feature ?merge RP HIGH? is active in
the example shown in figure 6.
He/N away/RP
ran/V
    vaha     bhaaga     gayaa
MI = HIGH
Figure 6: Example of MergeMI feature
6 Online large margin training
For parameter optimization, we have used an on-
line large margin algorithm called MIRA (Mc-
Donald et al, 2005) (Crammer and Singer, 2003).
We describe the training algorithm that we used
very briefly. Our training set is a set of English-
Hindi word aligned parallel corpus. We get the
verb based expressions in English by running a de-
pendency parser (Shen, 2006). Let the number of
sentence pairs in the training data be m. We have
24
fS
q
; T
q
; a^
q
g for training where q <= m is the in-
dex number of the sentence pair fS
q
; T
q
g in the
training set and a^
q
is the gold alignment for the
pair fS
q
; T
q
g. Let W be the weight vector which
has to be learnt, W
i
be the weight vector after the
end of ith update. To avoid over-fitting, W is ob-
tained by averaging over all the weight vectors W
i
.
A generic large margin algorithm is defined fol-
lows for the training instances fS
q
; T
q
; a^
q
g,
1. Initialize W
0
, W , i
2. for p:1 to NIterations
3. for q:1 to m
4. Get K-Best predictions 
q
= fa
1
; a
2
:::a
k
g
for the training example (S
q
; T
q
; a^
q
) using
the current model W i and applying step
1 and 2 of section 4. Compute W i+1 by
updating W i based on (S
q
; T
q
; a^
q
; 
q
).
5. i = i + 1
6. W = W + W i+1
7. W = W
NIterationsm
The goal of MIRA is to minimize the change in
W
i such that the score of the gold alignment a^ ex-
ceeds the score of each of the predictions in  by a
margin which is equal to the number of mistakes in
the predictions when compared to gold alignment.
While computing the number of mistakes, the mis-
takes due to the mis-alignment of head verb could
be given greater weight, thus prompting the opti-
mization algorithm to give greater importance to
verb related mistakes and thereby improving over-
all performance.
Step 4 in the algorithm mentioned above can
be substituted by the following optimization
problem,
minimize k(W i+1  W i)k
s.t. 8k, sore(a^
q
; S
q
; T
q
)   sore(a
q;k
; S
q
; T
q
)
>= Mistakes(a
k
; a^
q
; S
q
; T
q
)
The above optimization problem is converted to
the Dual form using one Lagrangian multiplier for
each constraint. In the Dual form, the Lagrangian
multipliers are solved using Hildreth?s algorithm.
Here, prediction of  is similar to the prediction
of K   best classes in a multi-class classification
problem. Ideally, we need to consider all the possi-
ble classes and assign margin constraints based on
every class. But, here the number of such classes
is exponential and thus we restrict ourselves to the
K   best classes.
7 Results on word-alignment task
7.1 Dataset
We have divided the 400 word aligned sentence
pairs into a training set consisting of 294 sen-
tence pairs and a test set consisting of 106 sentence
pairs. The source sentences are all dependency
parsed (Shen, 2006) and only the verb and its de-
pendents are considered for both training and test-
ing our algorithm. Our training algorithm requires
that the each of the source words is aligned to only
one or zero target words. For this, we use simple
heuristics to convert the training data to the appro-
priate format. For the words aligned to a source
verb, the first verb is chosen as the gold alignment.
For the words aligned to any dependent which is
not a verb, the last content word is chosen as the
alignment link. For test data, we do not make any
modifications and the final output from our align-
ment algorithm is compared with the original test
data.
7.2 Experiments with Giza
We evaluated our discriminative approach by com-
paring it with the state-of-art Giza++ alignments
(Och and Ney, 2003). The metric that we have
used to do the comparison is the Alignment Error
Rate (AER). The results shown below also contain
Precision, Recall and F-measure.
Giza was trained using an English-Hindi
aligned corpus of 50000 sentence pairs. In Table
1., we report the results of the GIZA++ alignments
run from both the directions (English to Hindi and
Hindi to English). We also show the results of the
intersected model. See Table 1. for the results of
the GIZA++ alignments.
Prec. Recall F-meas. AER
Eng! Hin 0.45 0.38 0.41 0.5874
Hin! Eng 0.46 0.27 0.34 0.6584
Intersected 0.82 0.19 0.31 0.6892
Table 1: Results of GIZA++ - Original dataset
We then lemmatize the words in both the source
and target sides of the parallel corpora and then
run Giza++ again. As the English-Hindi dataset
25
of 50000 sentence pairs is relatively small, we ex-
pect lemmatizing to improve the results. Table 2.
shows the results. As we hoped, the results after
lemmatizing the word forms are better than those
without.
Prec. Recall F-meas. AER
Eng! Hin 0.52 0.40 0.45 0.5518
Hin! Eng 0.53 0.30 0.38 0.6185
Intersected 0.82 0.23 0.36 0.6446
Table 2: Results of GIZA++ - lemmatized set
7.3 Experiments with our model
We trained our model using the training set of 294
word aligned sentence pairs. For training the pa-
rameters, we used a beam size of 3 and number of
iterations equal to 3. Table 3. shows the results
when we used only the basic local features (Dice-
Words, DiceRoots, Dict and Null) to train and test
our model.
Prec. Recall F-meas. AER
Local Feats. 0.47 0.38 0.42 0.5798
Table 3: Results using the basic features
When we add the the global features (AvgDist,
Overlap), we obtain the AER shown in Table 4.
Prec. Recall F-meas. AER
+ AvgD., Ove. 0.49 0.39 0.43 0.5689
Table 4: Results using the features - AvgDist,
Overlap
Now, we add the transition probabilities ob-
tained from the experiments with Giza++ as fea-
tures in our model. Table 5. contains the results.
The compositionality related features are now
added to our discriminative model to see if there is
any improvement in performance. Table 6. shows
the results by adding one feature at a time.
We observe that there is an improvement in the
AER by using the compositionality based features,
thus showing that compositionality based features
aid in the word-alignment task in a significant way
(AER = 0.5045).
8 Related work
Various measures have been proposed in the past
to measure the compositionality of multi-word ex-
Prec. Recall F-meas. AER
+ Giza++ prob. 0.54 0.44 0.49 0.5155
Table 5: Results using the Giza++ probabilities
Prec. Recall F-meas. AER
+ MergePos 0.54 0.45 0.49 0.5101
+ MergeMI 0.55 0.45 0.50 0.5045
Table 6: Results using the compositionality based
features
pressions of various types. Some of them are Fre-
quency, Point-wise mutual information (Church
and Hanks, 1989), Distributed frequency of object
(Tapanainen et al, 1998), Distributed frequency
of object using verb information (Venkatapathy
and Joshi, 2005), Similarity of object in verb-
object pair using the LSA model (Baldwin et al,
2003), (Venkatapathy and Joshi, 2005) and Lex-
ical and Syntactic fixedness (Fazly and Steven-
son, 2006). These features have largely been eval-
uated by the correlation of the compositionality
value predicted by these measures with the gold
standard value suggested by human judges. It has
been shown that the correlation of these measures
is higher than simple baseline measures suggest-
ing that these measures represent compositionality
quite well. But, the compositionality as such has
not been used in any specific application yet.
In this paper, we have suggested a framework
for using the compositionality of multi-word ex-
pressions for the word alignment task. State-of-art
systems for doing word alignment use generative
models like GIZA++ (Och and Ney, 2003; Brown
et al, 1993). Discriminative models have been
tried recently for word-alignment (Taskar et al,
2005; Moore, 2005) as these models give the abil-
ity to harness variety of complex features which
cannot be provided in the generative models. In
our work, we have used the compositionality of
multi-word expressions to predict how they align
with the words in the target language sentence.
For parameter optimization for the word-
alignment task, Taskar, Simon and Klein (Taskar
et al, 2005) used a large margin approach by fac-
toring the structure level constraints to constraints
at the level of an alignment link. We cannot do
such a factorization because the scores of align-
ment links in our case are not computed in a com-
pletely isolated manner. We use an online large
margin approach called MIRA (McDonald et al,
26
2005; Crammer and Singer, 2003) which fits well
with our framework. MIRA has previously been
used by McDonald, Pereira, Ribarov and Hajic
(McDonald et al, 2005) for learning the param-
eter values in the task of dependency parsing.
It should be noted that previous word-alignment
experiments such as Taskar, Simon and Klein
(Taskar et al, 2005) have been done with very
large datasets and there is little word-order vari-
ation in the languages involved. Our dataset is
small at present and there is substantial word order
variation between the source and target languages.
9 Conclusion and future work
In this paper, we have proposed a discriminative
approach for using the compositionality informa-
tion about verb-based multi-word expressions for
the word-alignment task. For training our model,
use used an online large margin algorithm (Mc-
Donald et al, 2005). For predicting the alignment
given a model, we proposed a K-Best beam search
algorithm to make our prediction algorithm com-
putationally feasible.
We have investigated the usefulness of simple
features such as point-wise mutual information for
the word-alignment task in English-Hindi bilin-
gual corpus. We have show that by adding the
compositionality based features to our model, we
obtain an decrease in AER from 0.5155 to 0.5045.
Our overall results are better than those obtained
using the GIZA++ models (Och and Ney, 2003).
In future, we will experiment with more ad-
vanced compositionality based features. But, this
would require a larger dataset for training and we
are working towards buidling such a large dataset.
Also, we would like to conduct similar exper-
iments on other language pairs (e.g. English-
French) and compare the results with the state-of-
art results reported for those languages.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Di-
ana McCarthy Francis Bond, Anna Korhonen and
Aline Villavicencio, editors, Proceedings of the ACL
2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment, pages 89?96.
P. Brown, S. A. Pietra, V. J. Della, Pietra, and R. L.
Mercer. 1993. The mathmatics of stastistical ma-
chine translation. In Computational Linguistics.
Kenneth Church and Patrick Hanks. 1989. Word as-
sociation norms, mutual information, and lexicog-
raphy. In Proceedings of the 27th. Annual Meet-
ing of the Association for Computational Linguis-
tics, 1990.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. In
Journal of Machine Learning Research.
Afsaneh Fazly and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations. In Proceedings of European
Chapter of Association of Computational Linguis-
tics. Trento, Italy, April.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530, Vancouver,
British Columbia, Canada, October. Association of
Computational Linguistics.
Robert C. Moore. 2005. A discriminative frame-
work for bilingual word alignment. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 81?88, Vancouver, British
Columbia, Canada, October. Association of Compu-
tational Linguistics.
F. Och and H. Ney. 2003. A systematic comparisoin
of various statistical alignment models. In Compu-
tational Linguistics.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-word
expressions: a pain in the neck for nlp. In Proceed-
ings of CICLing , 2002.
Libin Shen. 2006. Statistical LTAG Parsing. Ph.D.
thesis.
Pasi Tapanainen, Jussi Piitulaine, and Timo Jarvinen.
1998. Idiomatic object usage and support verbs. In
36th Annual Meeting of the Association for Compu-
tational Linguistics.
Ben Taskar, Locoste-Julien Simon, and Klein Dan.
2005. A discriminative machine learning approach
to word alignment. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 73?80, Vancouver, British Columbia,
Canada, October. Association of Computational
Linguistics.
Sriram Venkatapathy and Aravind Joshi. 2005. Mea-
suring the relative compositionality of verb-noun (v-
n) collocations by integrating features. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 899?906. Association
of Computational Linguistics, Vancouver, British
Columbia, Canada, October.
27
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 49?56,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Discriminative word alignment by learning the alignment structure
and syntactic divergence between a language pair
Sriram Venkatapathy1
Language Technologies Research
Centre, IIIT -Hyderabad
Hyderabad - 500019, India.
sriram@research.iiit.ac.in
Aravind K. Joshi
Department of Computer and
Information Science and Institute for
Research in Cognitive Science,
University of Pennsylvania, PA, USA.
joshi@linc.cis.upenn.edu
Abstract
Discriminative approaches for word align-
ment have gained popularity in recent
years because of the flexibility that they
offer for using a large variety of features
and combining information from various
sources. But, the models proposed in the
past have not been able to make much use
of features that capture the likelihood of an
alignment structure (the set of alignment
links) and the syntactic divergence be-
tween sentences in the parallel text. This is
primarily because of the limitation of their
search techniques. In this paper, we pro-
pose a generic discriminative re-ranking
approach for word alignment which allows
us to make use of structural features effec-
tively. These features are particularly use-
ful for language pairs with high structural
divergence (like English-Hindi, English-
Japanese). We have shown that by us-
ing the structural features, we have ob-
tained a decrease of 2.3% in the absolute
value of alignment error rate (AER). When
we add the cooccurence probabilities ob-
tained from IBM model-4 to our features,
we achieved the best AER (50.50) for the
English-Hindi parallel corpus.
1 Introduction
In this paper, we propose a discriminative re-
ranking approach for word alignment which al-
lows us to make use of structural features effec-
tively. The alignment algorithm first generates
11Part of the work was done at Institute for Research
in Cognitive Science (IRCS), University of Pennsylvania,
Philadelphia, PA 19104, USA, when he was visiting IRCS
as a Visiting Scholar, February to December, 2006.
a list of k-best alignments using local features.
Then it re-ranks this list of k-best alignments us-
ing global features which consider the entire align-
ment structure (set of alignment links) and the syn-
tactic divergence that exists between the sentence
pair. Use of structural information associated with
the alignment can be particularly helpful for lan-
guage pairs for which a large amount of unsuper-
vised data is not available to measure accurately
the word cooccurence values but which do have a
small set of supervised data to learn the structure
and divergence across the language pair. We have
tested our model on the English-Hindi language
pair. Here is an example of an alignment between
English-Hindi which shows the complexity of the
alignment task for this language pair.
Figure 1: An example of an alignment between an
English and a Hindi sentence
To learn the weights associated with the param-
eters used in our model, we have used a learning
framework called MIRA (The Margin Infused Re-
laxed Algorithm) (McDonald et al, 2005; Cram-
mer and Singer, 2003). This is an online learning
algorithm which looks at one sentence pair at a
time and compares the k-best predictions of the
alignment algorithm with the gold alignment to
update the parameter weights appropriately.
In the past, popular approaches for doing word
alignment have largely been generative (Och and
Ney, 2003; Vogel et al, 1996). In the past cou-
ple of years, the discriminative models for doing
word alignment have gained popularity because of
49
the flexibility they offer in using a large variety of
features and in combining information from vari-
ous sources.
(Taskar et al, 2005) cast the problem of align-
ment as a maximum weight bipartite matching
problem, where nodes correspond to the words
in the two sentences. The link between a pair
of words, (ep,hq) is associated with a score
(score(ep,hq)) reflecting the desirability of the ex-
istence of the link. The matching problem is
solved by formulating it as a linear programming
problem. The parameter estimation is done within
the framework of large margin estimation by re-
ducing the problem to a quadratic program (QP).
The main limitation of this work is that the fea-
tures considered are local to the alignment links
joining pairs of words. The score of an align-
ment is the sum of scores of individual alignment
links measured independently i.e., it is assumed
that there is no dependence between the align-
ment links. (Lacoste-Julien et al, 2006) extend
the above approach to include features for fertil-
ity and first-order correlation between alignment
links of consecutive words in the source sentence.
They solve this by formulating the problem as a
quadratic assignment problem (QAP). But, even
this algorithm cannot include more general fea-
tures over the entire alignment. In contrast to the
above two approaches, our approach does not im-
pose any constraints on the feature space except
for fertility (?1) of words in the source language.
In our approach, we model the one-to-one and
many-to-one links between the source sentence
and target sentence. The many-to-many alignment
links are inferred in the post-processing stage us-
ing simple generic rules. Another positive aspect
of our approach is the application of MIRA. It, be-
ing an online approach, converges fast and still re-
tains the generalizing capability of the large mar-
gin approach.
(Moore, 2005) has proposed an approach which
does not impose any restrictions on the form of
model features. But, the search technique has cer-
tain heuristic procedures dependent on the types
of features used. For example, there is little vari-
ation in the alignment search between the LLR
(Log-likelihood ratio) based model and the CLP
(Conditional-Link Probability) based model. LLR
and CLP are the word association statistics used
in Moore?s work (Moore, 2005). In contrast to
the above approach, our search technique is more
general. It achieves this by breaking the search
into two steps, first by using local features to get
the k-best alignments and then by using struc-
tural features to re-rank the list. Also, by using
all the k-best alignments for updating the parame-
ters through MIRA, it is possible to model the en-
tire inference algorithm but in Moore?s work, only
the best alignment is used to update the weights
of parameters. (Fraser and Marcu, 2006) have
proposed an algorithm for doing word alignment
which applies a discriminative step at every iter-
ation of the traditional Expectation-Maximization
algorithm used in IBM models. This model still
relies on the generative story and achieves only a
limited freedom in choosing the features. (Blun-
som and Cohn, 2006) do word alignment by com-
bining features using conditional random fields.
Even though their approach allows one to include
overlapping features while training a discrimina-
tive model, it still does not allow us to use fea-
tures that capture information of the entire align-
ment structure.
In Section 2, we describe the alignment search
in detail. Section 3 describes the features that
we have considered in our paper. Section 4 talks
about the Parameter optimization. In Section 5,
we present the results of our experiments. Section
6 contains the conclusion and our proposed future
work.
2 Alignment Search
The goal of the word alignment algorithm is to link
words in the source language with words in the tar-
get language to get the alignments structure. The
best alignment structure between a source sen-
tence and a target sentence can be predicted by
considering three kinds of information, (1) Prop-
erties of alignment links taken independently, (2)
Properties of the entire alignment structure taken
as a unit, and (3) The syntactic divergence between
the source sentence and the target sentence, given
the alignment structure. Using the set of alignment
links, the syntactic structure of the source sentence
is first projected onto the target language to ob-
serve the divergence.
Let ep and hq denote the source and target
words respectively. Let n be the number of words
in source sentence and m be the number of words
in target sentence. Let S be the source sentence
and T be the target sentence.
50
2.1 Populate the Beam
The task in this step is to obtain the k-best candi-
date alignment structures using the local features.
The local features mainly contain the cooccurence
information between a source and a target word
and are independent of other alignment links in
the sentence pair. Let the local feature vector be
denoted as fL(ep, hq). The score of a particular
alignment link is computed by taking a dot prod-
uct of the weight vector W with the local feature
vector of the alignment link. More formally, the
local score of an alignment link is
scoreL(ep, hq) = W.fL(ep, hq)
The total score of an alignment structure is com-
puted by adding the scores of individual alignment
links present in the alignment. Hence, the score of
an alignment structure a? is,
scoreLa(a?, S, T ) =
?
(ep,hq)?a?
scoreL(ep, hq)
We have proposed a dynamic programming al-
gorithm of worst case complexity O(nm2 + nk2)
to compute the k-best alignments. First, the local
score of each source word with every target word
is computed and stored in local beams associated
with the source words. The local beams corre-
sponding to all the source words are sorted and the
top-k alignment links in each beam are retained.
This operation has the worst-case complexity of
O(nm2).
Now, the goal is to get the k-best alignments in
the global beam. The global beam initially con-
tains no alignments. The k best alignment links of
the first source word e0 are added to the global
beam. To add the alignment links of the next
source word to the global beam, the k2 (if k < m)
combinations of the alignments in the global beam
and alignments links in the local beam are taken
and the best k are retained in the global beam.
If k > m, then the total combinations taken are
mk. This is repeated till the entries in all the lo-
cal beams are considered, the overall worst case
complexity being O(nk2) (or O(nmk) if k > m).
2.2 Reorder the beam
We now have the k-best alignments using the local
features from the last step. We then use global fea-
tures to reorder the beam. The global features look
at the properties of the entire alignment structure
instead of the alignment links locally.
Let the global feature vector be represented as
fG(a?). The global score is defined as the dot prod-
uct of the weight vector and the global feature vec-
tor.
scoreG(a?) = W.fG(a?)
The overall score is calculated by adding the local
score and the global score.
score(a?) = scoreLa(a?) + scoreG(a?)
The beam is now sorted based on the overall scores
of each alignment. The alignment at the top of
the beam is the best possible alignment between
source sentence and the target sentence.
2.3 Post-processing
The previous two steps produce alignment struc-
tures which contain one-to-one and many-to-one
links. In this step, the goal is to extend the best
alignment structure obtained in the previous step
to include the other alignments links of one-to-
many and many-to-many types.
The majority of the links between the source
sentence and the target sentence are one-to-one.
Some of the cases where this is not true are the in-
stances of idioms, alignment of verb groups where
auxiliaries do not correspond to each other, the
alignment of case-markers etc. Except for the
cases of idioms in target language, most of the
many-to-many links between a source and target
sentences can be inferred from the instances of
one-to-one and many-to-one links using three lan-
guage language specific rules (Hindi in our case)
to handle the above cases. Figure 1, Figure 2 and
Figure 3 depict the three such cases where many-
to-many alignments can be inferred. The align-
ments present at the left are those which can be
predicted by our alignment model. The alignments
on the right side are those which can be inferred in
the post-processing stage.
.....  are  playing ......
....... khel rahe hain 
.....  are  playing ......
....... khel rahe hain 
    
   (play  cont  be)
Figure 2: Inferring the many-to-many alignments
of verb and auxiliaries
After applying the language specific rules, the
dependency structure of the source sentence is tra-
versed to ensure the consistency of the alignment
51
John  ne  ....
John ..........
John  ne  ....
John ..........
Figure 3: Inferring the one-to-many alignment to
case-markers in Hindi
... kicked the bucket 
..........  mara gaya
... kicked the bucket 
..........  mara gaya
 (die   go?light verb)
Figure 4: Inferring many-to-many alignment for
source idioms
structure. If there is a dependency link between
two source words eo and ep, where eo is the head
and ep is the modifier and if eo and ep are linked
to one or more common target word(s), it is log-
ical to imagine that the alignment should be ex-
tended such that both eo and ep are linked to the
same set of target words. For example, in Figure 4,
new alignment link is first formed between ?kick?
and ?gayA? using the language specific rule, and
as ?kick? and ?bucket? are both linked to ?mara?,
?bucket? is also now linked to ?gayA?. Similarity,
?the? is linked to both ?mara? and ?gayA?. Hence,
the rules are applied by traversing through the de-
pendency tree associated with the source sentence
words in depth-first order. The dependency parser
used by us was developed by (Shen, 2006). The
following summarizes this step,
? Let w be the next word considered in the dependency
tree, let pw be the parent of w.
? If w and pw are linked to one or more common
word(s) in target language, align w to all target
words which are aligned to pw.
? Else, Use the target-specific rules (if they match)
to extend the alignments of w.
? Recursively consider all the children of w
3 Parameters
As the number of training examples is small, we
chose to use features (both local and structural)
which are generic. Some of the features which we
used in this experiment are as follows:
3.1 Local features (FL)
The local features which we consider are mainly
co-occurrence features. These features estimate
the likelihood of a source word aligning to a tar-
get word based on the co-occurrence information
obtained from a large sentence aligned corpora1.
3.1.1 DiceWords
Dice Coefficient of the source word and the tar-
get word (Taskar et al, 2005).
DCoeff(ep, hq) = 2 ? Count(ep, hq)Count(ep) + Count(hq)
where Count(ep, hq) is the number of times the
word hq was present in the translation of sentences
containing the word ep in the parallel corpus.
3.1.2 DiceRoots
Dice Coefficient of the lemmatized forms of the
source and target words. It is important to consider
this feature for language pairs which do not have a
large unsupervised sentence aligned corpora. Co-
occurrence information can be learnt better after
we lemmatize the words.
3.1.3 Dict
This feature tests whether there exists a dictio-
nary entry from the source word ep to the target
word hq. For English-Hindi, we used a medium-
coverage dictionary (25000 words) available from
IIIT - Hyderabad, India 2.
3.1.4 Null POS
These parameters measures the likelihood of a
source word with a particular part of speech tag3 to
be aligned to no word (Null) on the target language
side. This feature was extremely useful because
it models the cooccurence information of words
with nulls which is not captured by the features
DiceWords and DiceRoots. Here are some of the
features of this type with extreme estimated pa-
rameter weights.
3.2 Lemmatized word pairs
The word pairs themselves are a good indicator
of whether an alignment link exists between the
word pair or not. Also, taking word-pairs as fea-
ture helps in the alignment of some of the most
common words in both the languages. A variation
of this feature was used by (Moore, 2005) in his
paper.
150K sentence pairs originally collected as part of TIDES
MT project and later refined at IIIT-Hyderabad, India.
2http://ltrc.iiit.ac.in/onlineServices/Dictionaries/Dict Frame.html
3We have limited the number of POS tags by considering
only the first alphabets of Penn Tags as our POS tag cate-
gories
52
Param. weight Param. weight
Null ? 0.2737 null C -0.7030
Null U 0.1969 null D -0.6914
Null L 0.1814 null V -0.6360
Null . 0.0383 null N -0.5600
Null : 0.0055 null I -0.4839
Table 1: Top Five Features each with Maximum
and Minimum weights
Other parameters like the relative distance be-
tween the source word ep and the target word hq,
RelDist(ep, hq) = abs(j/|e| ? k/|h|), which are
mentioned as important features in the previous
literature, did not perform well for the English-
Hindi language pair. This is because of the pre-
dominant word-order variation between the sen-
tences of English and Hindi (Refer Figure 1).
3.3 Structural Features (FG)
The global features are used to model the prop-
erties of the entire alignment structure taken as a
unit, between the source and the target sentence.
In doing so, we have attempted to exploit the syn-
tactic information available on both the source and
the target sides of the corpus. The syntactic infor-
mation on the target side is obtained by projecting
the syntactic information of the source using the
alignment links. Some of the features which we
have used in our work are in the following subsec-
tion.
3.3.1 Overlap
This feature considers the instances in a sen-
tence pair where a source word links to a target
word which is a participant in more than one align-
ment links (has a fertility greater than one). This
feature is used to encourage the source words to
be linked to different words in the target language.
For example, we would prefer the alignment in
Figure 6 when compared to the alignment in Fig-
ure 5 even before looking at the actual words. This
parameter captures such prior information about
the alignment structure.
Figure 5: Alignment where many source words are
linked to one target word
Figure 6: Alignment where the source words are
aligned to many different target words
Formally, it is defined as
Overlap(a?) =
?
hq?T,Fert(hq)>1 Fert
2(hq)
?
h?T Fert(h)
where T is the Hindi sentence. ? Fert2(hq) is
measured in the numerator so that a more uniform
distribution of target word fertilities be favored in
comparison to others. The weight of overlap as
estimated by our model is -6.1306 which indicates
the alignments having a low overlap value are pre-
ferred.
3.3.2 NullPercent
This feature measures the percentage of words
in target language sentence which are not aligned
to any word in the source language sentence. It is
defined as
NullPercent =
|hq|hq?T,Fertility(hq)==0
|h|h?T
3.3.3 Direction DepPair
The following feature attempts to capture the
first order interdependence between the alignment
links of pairs of source sentence words which are
connected by dependency relations. One way in
which such an interdependence can be measured
is by noting the order of the target sentence words
linked to the child and parent of a source sentence
dependency relation. Figures 7, 8 and 9 depict
the various possibilities. The words in the source
sentence are represented using their part-of-speech
tags. These part-of-speech tags are also projected
onto the target words. In the figures p is the parent
and c is the part-of-speech of the child.
p c
c p
Figure 7: Target word linked to a child precedes
the target word linked to a parent
53
p c
p c
Figure 8: Target word linked to a parent precedes
the target word linked to a child
p c
p c
Figure 9: Parent and the child are both linked to
same target word
The situation in Figure 9 is an indicator that the
parent and child dependency pair might be part or
whole of a multi-word expression on the source
side. This feature thus captures the divergence be-
tween the source sentence dependency structure
and the target language dependency structure (in-
duced by taking the alignment as a constraint).
Hence, in the test data, the alignments which do
not express this divergence between the depen-
dency trees are penalized. For example, the align-
ment in Figure 10 will be heavily penalized by
the model during re-ranking step primarily for two
reasons, 1) The word aligned to the preposition
?of? does not precede the word aligned to the noun
?king? and 2) The word aligned to the preposition
?to? does not succeed the word aligned to the noun
?king?.
......... to the king of Rajastan .......
......  Rajastan  ke   Raja  ko   ..........
( Rajastan   of    King   to  )
Figure 10: A simple example of an alignment
that would be penalized by the feature Direc-
tion DepPair
3.3.4 Direction Bigram
This feature is a variation of the previous fea-
ture. In the previous feature, the dependency pair
on the source side was projected to the target side
to observe the divergence of the dependency pair.
In this feature, we take a bigram instead of a de-
pendency pair and observe its order in the target
side. This feature is equivalent to the first-order
features used in the related work.
There are three possibilities here, (1) The words
of the bigram maintain their order when projected
onto the target words, (2) The words of the bigram
are reversed when projected, (3) Both the words
are linked to the same word of the target sentence.
4 Online large margin training
For parameter optimization, we have used an on-
line large margin algorithm called MIRA (Mc-
Donald et al, 2005) (Crammer and Singer, 2003).
We will briefly describe the training algorithm that
we have used. Our training set is a set of English-
Hindi word aligned parallel corpus. Let the num-
ber of sentence pairs in the training data be t. We
have {Sr, Tr, a?r} for training where r ? t is the
index number of the sentence pair {Sr, Tr} in the
training set and a?r is the gold alignment for the
pair {Sr, Tr}. Let W be the weight vector which
has to be learnt, Wi be the weight vector after the
end of ith update. To avoid over-fitting, W is ob-
tained by averaging over all the weight vectors Wi.
A generic large margin algorithm is defined
follows for the training instances {Sr, Tr, a?r},
Initialize W0, W , i
for p = 1 to Iterations do
for r = 1 to t do
Get K-Best predictions ?r = {a1, a2...ak}
for the training example (Sr, Tr, a?r)
using the current model W i and applying
step 1 and 2 of section 4. Compute W i+1
by updating W i based on
(Sr, Tr, a?r, ?r).
i = i + 1
W = W + W i+1
W = WIterations?m
end for
end for
The goal of MIRA is to minimize the change in
W i such that the score of the gold alignment a? ex-
ceeds the score of each of the predictions in ? by a
margin which is equal to the number of mistakes in
the predictions when compared to the gold align-
ment. One could choose a different loss function
which assigns greater penalty for certain kinds of
mistakes when compared to others.
Step 4 (Get K-Best predictions) in the algo-
54
rithm mentioned above can be substituted by the
following optimization problem,
minimize ?(W i+1 ? W i)?
s.t. ?k, score(a?r, Sr, Tr)? score(aq,k, Sr, Tr)
>= Mistakes(ak, a?r, Sr, Tr)
For optimization of the parameters, ideally, we
need to consider all the possible predictions and
assign margin constraints based on every predic-
tion. But, here the number of such classes is ex-
ponential and therefore we restrict ourselves to the
k ? best predictions.
We estimate the parameters in two steps. In the
first step, we estimate only the weights of the lo-
cal parameters. After that, we keep the weights
of local parameters constant and then estimate the
weights of global parameters. It is important to
decouple the parameter estimation to two steps.
We also experimented estimating the parameters
in one stage but as expected, it had an adverse
impact on the parameter weights of local features
which resulted in generation of poor k-best list af-
ter the first step while testing.
5 Experiments and Results
5.1 Data
We have used English-Hindi unsupervised data of
50000 sentence pairs4. This data was used to ob-
tain the cooccurence statistics such as DiceWords
and DiceRoots which we used in our model. This
data was also used to obtain the predictions of
GIZA++ (Implements the IBM models and the
HMM model). We take the alignments of GIZA++
as baseline and evaluate our model for the English-
Hindi language pair.
The supervised training data which is used to
estimate the parameters consists of 4252 sentence
pairs. The development data consists of 100 sen-
tence pairs and the test data consists of 100 sen-
tence pairs. This supervised data was obtained
from IRCS, University of Pennsylvania. For train-
ing our model, we need to convert the many-to-
many alignments in the corpus to one-to-one or
may-to-one alignments. This is done by applying
inverse operations of those performed during the
post-processing step (section 2.3).
4Originally collected as part of TIDES MT project and
later refined at IIIT-Hyderabad, India.
5.2 Experiments
We first obtain the predictions of GIZA++ to ob-
tain the baseline accuracies. GIZA++ was run in
four different modes 1) English to Hindi, 2) Hindi
to English, 3) English to Hindi where the words in
both the languages are lemmatized and 4) Hindi to
English where the words are lemmatized. We then
take the intersections of the predictions run from
both the directions (English to Hindi and Hindi to
English). Table 2 contains the results of experi-
ments with GIZA++. As the recall of the align-
ment links of the intersection is very low for this
dataset, further refinements of the alignments as
suggested by (Och and Ney, 2003) were not per-
formed.
Mode Prec. Rec. F-meas. AER
Normal: Eng-Hin 47.57 40.87 43.96 56.04
Normal: Hin-Eng 47.97 38.50 42.72 57.28
Normal: Inter. 88.71 27.52 42.01 57.99
Lemma.: Eng-Hin 53.60 44.58 48.67 51.33
Lemma.: Hin-Eng 53.83 42.68 47.61 52.39
Lemma.: Inter. 86.14 32.80 47.51 52.49
Table 2: GIZA++ Results
In Table 3, we observe that the best result
(51.33) is obtained when GIZA++ is run after lem-
matizing the words on the both sides of the unsu-
pervised corpus. The best results obtained without
lemmatizing is 56.04 when GIZA++ is run from
English to Hindi.
The table 4 summarizes the results when we
used only the local features in our model.
Features Prec. Rec. F-meas. AER
DiceRoots 41.49 38.71 40.05 59.95
+ DiceWords
+ Null POS 42.82 38.29 40.43 59.57
+ Dict. 43.94 39.30 41.49 58.51
+ Word pairs 46.27 41.07 43.52 56.48
Table 3: Results using local features
We now add the global features. While esti-
mating the parameter weights associated with the
global features, we keep the weights of local fea-
tures constant. We choose the appropriate beam
size as 50 after testing with several values on the
development set. We observed that the beam sizes
(between 10 and 100) did not affect the alignment
error rates very much.
55
Features Prec. Rec. F-meas. AER
Local feats. 46.27 41.07 43.52 56.48
Local feats. 48.17 42.76 45.30 54.70
+ Overlap
Local feats. 47.93 42.55 45.08 54.92
+ Direc. Deppair
Local feats. 48.31 42.89 45.44 54.56
+ Direc. Bigram
Local feats. 48.81 43.31 45.90 54.10
+ All Global feats.
Table 4: Results after adding global features
We see that by adding global features, we ob-
tained an absolute increase of about 2.3 AER sug-
gesting the usefulness of structural features which
we considered. Also, the new AER is much better
than that obtained by GIZA++ run without lem-
matizing the words.
We now add the IBM Model-4 parameters (co-
occurrence probabilities between source and tar-
get words) obtained using GIZA++ and our fea-
tures, and observe the results (Table 6). We can
see that structural features resulted in a significant
decrease in AER. Also, the AER that we obtained
is slightly better than the best AER obtained by the
GIZA++ models.
Features Prec. Rec. F-meas. AER
IBM Model-4 Pars. 48.85 43.98 46.29 52.71
+ LocalFeats
IBM Model-4 Pars. 48.95 50.06 49.50 50.50
+ All feats.
Table 5: Results after combining IBM model-4 pa-
rameters with our features
6 Conclusion and Future Work
In this paper, we have proposed a discriminative
re-ranking approach for word alignment which al-
lows us to make use of structural features effec-
tively. We have shown that by using the structural
features, we have obtained a decrease of 2.3% in
the absolute value of alignment error rate (AER).
When we combine the prediction of IBM model-4
with our features, we have achieved an AER which
is slightly better than the best AER of GIZA++
for the English-Hindi parallel corpus (a language
pair with significant structural divergences). We
expect to get large improvements when we add
more number of relevant local and structural fea-
tures. We also plan to design an appropriate de-
pendency based decoder for machine translation
to make good use of the parameters estimated by
our model.
References
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proceedings of the 21st COLING and 44th Annual
Meeting of the ACL, Sydney, Australia, July. ACL.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. In
Journal of Machine Learning Research.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
Proceedings of the 21st COLING and 44th Annual
Meeting of the ACL, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael I. Jordan. 2006. Word alignment via
quadratic assignment. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 112?119, New York City,
USA, June. Association for Computational Linguis-
tics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-project dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530, Vancouver,
British Columbia, Canada, October. Association of
Computational Linguistics.
Robert C. Moore. 2005. A discriminative frame-
work for bilingual word alignment. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 81?88, Vancouver, British
Columbia, Canada, October. Association of Compu-
tational Linguistics.
F. Och and H. Ney. 2003. A systematic comparisoin
of various statistical alignment models. In Compu-
tational Linguistics.
Libin Shen. 2006. Statistical LTAG Parsing. Ph.D.
thesis.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative machine approach to word
alignment. In Proceedings of HLT-EMNLP, pages
73?80, Vancouver, British Columbia, Canada, Octo-
ber. Association of Computational Linguistics.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th International
Conference on Computational Linguistics.
56
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 96?102,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Three models for discriminative machine translation using
Global Lexical Selection and Sentence Reconstruction
Sriram Venkatapathy
Language Technologies Research
Centre, IIIT-Hyderabad
Hyderabad - 500019, India.
sriram@research.iiit.ac.in
Srinivas Bangalore
AT&T Labs - Research
Florham Park, NJ 07932
USA
srini@research.att.com
Abstract
Machine translation of a source language
sentence involves selecting appropriate
target language words and ordering the se-
lected words to form a well-formed tar-
get language sentence. Most of the pre-
vious work on statistical machine transla-
tion relies on (local) associations of target
words/phrases with source words/phrases
for lexical selection. In contrast, in this
paper, we present a novel approach to lex-
ical selection where the target words are
associated with the entire source sentence
(global) without the need for local asso-
ciations. This technique is used by three
models (Bag?of?words model, sequential
model and hierarchical model) which pre-
dict the target language words given a
source sentence and then order the words
appropriately. We show that a hierarchi-
cal model performs best when compared
to the other two models.
1 Introduction
The problem of machine translation can be viewed
as consisting of two subproblems: (a) lexical se-
lection, where appropriate target language lexi-
cal items are chosen for each source language
lexical item and (b) lexical reordering, where
the chosen target language lexical items are rear-
ranged to produce a meaningful target language
string. Most of the previous work on statisti-
cal machine translation, as exemplified in (Brown
et al, 1993), employs word?alignment algorithm
(such as GIZA++ (Och et al, 1999)) that provides
local associations between source words and target
words. The source?to?target word?alignments are
sometimes augmented with target?to?source word
alignments in order to improve the precision of
these local associations. Further, the word?level
alignments are extended to phrase?level align-
ments in order to increase the extent of local asso-
ciations. The phrasal associations compile some
amount of (local) lexical reordering of the target
words?those permitted by the size of the phrase.
Most of the state?of?the?art machine translation
systems use these phrase?level associations in
conjunction with a target language model to pro-
duce the target sentence. There is relatively little
emphasis on (global) lexical reordering other than
the local re-orderings permitted within the phrasal
alignments. A few exceptions are the hierarchical
(possibly syntax?based) transduction models (Wu,
1997; Alshawi et al, 1998; Yamada and Knight,
2001; Chiang, 2005) and the string transduction
models (Kanthak et al, 2005).
In this paper, we present three models for doing
discriminative machine translation using global
lexical selection and lexical reordering.
1. Bag?of?Words model : Given a source sen-
tence, each of the target words are chosen by
looking at the entire source sentence. The
target language words are then permuted in
various ways and then, the best permutation
is chosen using the language model on the
target side. The size of the search space of
these permutations can be set by a parameter
called the permutation window. This model
does not allow long distance re-orderings of
target words unless a very large permutation
window chosen which is very expensive.
2. Sequential Lexical Choice model : Given
a source sentence, the target words are pre-
dicted in an order which is faithful to the or-
96
der of words in the source sentence. Now,
the number of permutations that need to be
examined to obtain the best target language
strings are much less when compared to the
Bag?of?Words model. This model is ex-
pected to give good results for language pairs
such as English?French for which only lo-
cal word order variations exist between sen-
tences.
3. Hierarchical lexical association and re-
ordering model : For language pairs such
as English?Hindi or English?Japanese where
there is a high degree of global reordering
(Figure 1), it is necessary to be able to handle
long distance movement of words/phrases.
In this approach, the target words predicted
through global lexical selection are associ-
ated with various nodes of the source depen-
dency tree and then, hierarchical reordering is
done to obtain the order of words in the tar-
get sentence. Hierarchical reordering allows
phrases to distort to longer distances than the
previous two models.
Figure 1: Sample distortion between En-
glish?Hindi
The outline of the paper is as follows. In Section
2, we talk about the global lexical selection. Sec-
tion 3 describes three models for global lexical se-
lection and reordering. In Section 4, we report the
results of the translation models on English?Hindi
language pair and contrast the strengths and limi-
tations of the models.
2 Global lexical selection
For global lexical selection, in contrast to the
local approaches of associating target words to
the source words, the target words are associated
to the entire source sentence. The intuition is
that there may be lexico?syntactic features of the
source sentence (not necessarily a single source
word) that might trigger the presence of a target
word in the target sentence. Furthermore, it might
be difficult to exactly associate a target word to
a source sentence in many situations - (a) when
translations are not exact but paraphrases (b) the
target language does not have one lexical item
to express the same concept that is expressed in
the source word. The extensions of word align-
ments to phrasal alignments attempt to address
some of these situations in additional to alleviat-
ing the noise in word?level alignments.
As a consequence of the global lexical selection
approach, we no longer have a tight association
between source language words/phrases and tar-
get language words/phrases. The result of lexical
selection is simply a bag of words(phrases) in the
target language and the target sentence has to be
reconstructed using this bag of words.
The target words in the bag, however, might
be enhanced with rich syntactic information that
could aid in the reconstruction of the target sen-
tence. This approach to lexical selection and
sentence reconstruction has the potential to cir-
cumvent the limitations of word?alignment based
methods for translation between significantly dif-
ferent word order languages. However, in this pa-
per, to handle large word order variations, we asso-
ciate the target words with source language depen-
dency structures to enable long distance reorder-
ing.
3 Training the discriminative models for
lexical selection and reordering
In this section, we present our approach for a
global lexical selection model which is based on
discriminatively trained classification techniques.
Discriminant modeling techniques have become
the dominant method for resolving ambiguity in
speech and natural language processing tasks, out-
performing generative models for the same task.
We expect the discriminatively trained global lex-
ical selection models to outperform generatively
trained local lexical selection models as well as
provide a framework for incorporating rich mor-
pho?syntactic information.
Statistical machine translation can be formu-
lated as a search for the best target sequence that
maximizes P (T | S), where S is the source sen-
tence and T is the target sentence. Ideally, P (T |
S) should be estimated directly to maximize the
conditional likelihood on the training data (dis-
criminant model). However, T corresponds to
a sequence with a exponentially large combina-
tion of possible labels, and traditional classifica-
tion approaches cannot be used directly. Although
97
Conditional Random Fields (CRF) (Lafferty et al,
2001) train an exponential model at the sequence
level, in translation tasks such as ours the compu-
tational requirements of training such models are
prohibitively expensive.
3.1 Bag-of-Words Lexical Choice Model
This model doesn?t require the sentences to be
word aligned in order to learn the local associa-
tions. Instead, we take the sentence aligned cor-
pus as before but we treat the target sentence as a
bag?of?words or BOW assigned to the source sen-
tence. The goal is, given a source sentence S, to
estimate the probability that we find a given word
(tj) in its translation ie.., we need to estimate the
probabilities P (true|tj , S) and P (false|tj, S).
To train such a model, we need to build binary
classifiers for all the words in the target lan-
guage vocabulary. The probability distributions
of these binary classifiers are learnt using maxi-
mum entropy model (Berger et al, 1996; Haffner,
2006). For the word tj , the training sentence
pairs are considered as positive examples where
the word appears in the target, and negative other-
wise. Thus, the number of training examples for
each binary classifier equals the number of train-
ing examples. In this model, classifiers are train-
ing using n?gram features (BOgrams(S)).
During decoding, instead of producing the tar-
get sentence directly, what we initially obtain is
the target bag of words. Each word in the target
vocabulary is detected independently, so we have
here a very simple use of binary static classifiers.
Given a sentence S, the bag of words (BOW (T )
contains those words whose distributions have the
positive probability greater than a threshold (? ).
BOW (T ) = {t | P (true | t, BOgrams(S)) > ?}
(1)
In order to reconstruct the proper order of words
in the target sentence, we consider various permu-
tations of words in BOW (T ) and weight them by
a target language model. Considering all possible
permutations of the words in the target sentence
is computationally not feasible. But, the number
of permutations examined can be reduced by us-
ing heuristic forward pruning or by constraining
the permutations to be within a local window of
adjustable size (also see (Kanthak et al, 2005)).
We have chosen to constrain permutations here.
Constraining the permutation using a local win-
dow can provide us some very useful local re-
orderings.
The bag?of?words approach can also be modi-
fied to allow for length adjustments of target sen-
tences, if we add optional deletions in the final
step of permutation decoding. The parameter ?
and an additional word deletion penalty ? can then
be used to adjust the length of translated outputs.
3.2 Sequential Lexical Choice Model
The previous approach gives us a predetermined
order of words initially which are then permuted to
obtain the best target string. Given that we would
not be able to search the entire space, it would be a
helpful if we could start searching various permu-
tations using a more definite string. One such def-
inite order in which the target words can be placed
is the order of source words itself. In this model,
during the lexical selection, we try to place the
target words in an order which is faithful to the
source sentence.
This model associates sets of target words with
every position in the source sentence and yet re-
tains the power of global lexical selection. For
every position (i) of the source sentence, a prefix
string is formed which consists of the sequence of
words from positions 1 to i. Each of these prefix
strings are used to predict bags of target words us-
ing the global lexical selection. Now, these bags
generated using the prefix strings are processed in
the order of source positions. Let Ti be the bag of
target words generated by prefix string i (Figure
2).
T (i+1)
T (i)
i i+1
Figure 2: The generation of target bags associated
with source sentence position
The goal is to associate a set of target words
with every source position. A target word t
is attached to the ith source position if it is
present in Ti but not in Ti?1 and the probability
P (true|t, Ti) > ? . The intuition behind this ap-
proach is that a word t is associated with a position
i if there was some information present at the ith
source position that triggered the probability of the
t to exceed the threshold ? .
98
Hence, the initial target string is the sequence
of target language words associated with the se-
quence of source language positions. This string
is now permuted in all possible ways (section 3.1)
and the best target string is chosen using the lan-
guage model.
3.3 Hierarchical lexical association and
reordering model
The Sequential Lexical Choice Model presented in
the last section is expected to work best for lan-
guage pairs for which there are mostly local word
order variations. For language pairs with signifi-
cant word order variation, the search for the target
string may still fail examine the best target lan-
guage string given the source sentence. The model
proposed in this section should be able to handle
such long distance movement of words/phrases.
In this model, the goal is to search for the best
target string T which maximizes the probability
P (T |S,D(S)), where S is the source sentence
and D(S) is the dependency structure associated
with the source sentence S. The probabilities of
the target words given the source sentence are
estimated in the same way as the bag?of?words
model. The only main difference during the esti-
mation stage is that we consider the dependency
tree based features apart from the n-gram features.
The decoding of the source sentence S takes
place in three steps,
1. Predict the bag?of?words : Given a source
sentence S, predict the bag of words BOW(T)
whose distributions have a positive probabil-
ities greater than a threshold (? ).
2. Attachment to Source nodes : These target
words are now attached to the nodes of source
dependency trees. For making the attach-
ments, the probability distributions of target
words conditioned on features local to the
source nodes are used.
3. Ordering the target language words : Tra-
verse the source dependency tree in a bottom-
up fashion to obtain the best target string.
3.3.1 Predict the bag?of?words
Given a source sentence S, all the target words
whose positive probability distributions are above
? are included in the bag.
BOW (T ) = {t | P (true|t, f(S))} (2)
In addition to the n?gram features, this model uses
cues provided by the dependency structure to pre-
dict the target bag?of?words.
S1
S2
S3 S4
S5
Figure 3: Dependency tree of a source sentence
with words s1, s2, s3, s4 and s5
Hence, the features that we have considered in
the model are (Figure 3),
1. N-grams. For example, in Figure 2, ?s1?, ?s2
s3 s4?, ?s4 s5? etc.
2. Dependency pair (The pair of nodes and its
parents). Example in Figure 2., ?s2 s1?, ?s4
s2? etc.
3. Dependency treelet (The triplet of a node, it?s
parent and sibling). For example, ?s3 s2 s4?,
?s2 s1 s5? etc.
3.3.2 Attachment to Source nodes
For every target word tj in the bag, the most
likely source nodes are determined by measuring
the positive distribution of the word tj given the
features of the particular node (Figure 4). Let
S(tj) denote the set of source nodes to which the
word tj can be attached to, then S(tj) is deter-
mined as,
S1
S2
S3 S4
S5
T1          T2        T3          T4
Figure 4: Dependency tree of a source sentence
with words S1, S2, S3, S4 and S5
S(tj) = argmaxs(P (true|tj , f(s)) (3)
where f(s) denotes the features of S in which
only those features are active which contain the
99
lexical item representing the node s. The target
words are in the global bag are processed in the
order of their global probabilities p(t|S). While
attaching the target words, it is ensured that no
source node had more than ? target words attached
to it. Also, a target word should not be attached
to more to more than ? number of times. There
is another constraint that can be applied to ensure
that the ratio of the total target words (which are
attached to source nodes) to the total number of
words in the source sentence does exceed a value
(?).
3.4 Ordering the target language words
In this step, the source sentence dependency tree is
traversed in a bottom?up fashion. At every node,
the best possible order of target words associated
with the sub-tree rooted at the node is determined.
This string is then used as a cohesive unit by the
superior nodes.
S1
S2
S3 S4
S5
t1 t2 t3 t4 t5
t6 t7t1 t2 t3 t7 t4 t5 t6
Figure 5: The target string associated with node
S1 is determined by permuting strings attached to
the children (in rectangular boxes, to signify that
they are frozen) and the lexical items attached to
S1
For example, in Figure 5, let ?t1 t2 t3?, ?t4 t5?
be the best strings associated with the children of
nodes s2 and s3 respectively. Let t6 and t7 be the
words that are attached to node s1. The best string
for the node s1 is determined by permuting the
strings ?t1 t2 t3?, ?t4 t5?, ?t6? ?t7? in all possible
ways and then choosing the best string using the
language model.
4 Dataset
The language pair that we considered for our ex-
periments are English?Hindi. The training set
consists of 37967 sentence pairs, the development
set contains 819 sentence pairs and the test set
has 699 sentence pairs. The dataset is from the
newspaper domain with topics ranging from pol-
itics to tourism. The sentence pairs have a maxi-
mum source sentence length of 30 words. The av-
erage length of English sentences is 18 while that
of Hindi sentences is 20.
The source language vocabulary is 41017 and
target sentence vocabulary is 48576. The to-
ken/type ratio of English in the dataset is 16.70
and that of Hindi is 15.64. This dataset is rela-
tively sparse. So, the translation accuracies on this
dataset would be relatively less when compared to
those on much larger datasets. In the target side
of the development corpus, the percentage of un-
seen tokens is 13.48%(3.87% types) while in the
source side, the percentage of unseen tokens is
10.77%(3.20% types). On furthur inspection of
a small portion of the dataset, we found that the
maximum percentage of the unseen words on the
target side are the named entities.
5 Results
5.1 Bag-of-Words model
The quality of the bag?of?words obtained is gov-
erned by the parameter ? (probability threshold).
To determine the best ? value, we experiment with
various values of ? and measure the lexical accu-
racies (F-score) of the bags generated on the de-
velopment set (See Figure 6). The total number
of features used for training this model are 53166
(with count-cutoff of 2).
Figure 6: Lexical Accuracies of the Bags-of-
words
Now, we order the bags of words obtained
through global selection to get the target lan-
guage strings. While reordering using the lan-
guage model, some of the noisy words from the
bag can be deleted by setting a deletion cost (?).
We experimented with various deletion costs, and
tuned it according to the best BLEU score that we
100
obtained on the development set. Figure 7 shows
the best BLEU scores obtained by reordering the
bags associated with various threshold values.
Figure 7: Lexical Accuracies of the Bags-of-
words
We can see that we obtained the best BLEU
when we choose a threshold of 0.17 to obtain the
bag?of?words, when the deletion cost is set to 19.
The reference target strings of the development
set has 15986 tokens. So, while tuning the param-
eters, we should ensure that the bags (obtained us-
ing the global lexical selection) that we consider
have more tokens than 15986 to allow some dele-
tions during reordering, and in effect obtain the
target strings whose total token count is approx-
imately equal to 15986. Figure 8 shows the varia-
tion in BLEU scores for various deletion costs by
fixing the threshold at 0.17.
Figure 8: BLEU scores for various deletion costs
when the threshold for global lexical selection is
set to 0.17
On the test set, we now fix the threshold at 0.17
(? ) and the deletion cost (?) at 19 to obtain the
target language strings. The BLEU score that we
obtained for this set is 0.0428.
5.2 Sequential Lexical Choice Model
The lexical accuracy values of the sequence of
words obtained by the sequential lexical choice
model are comparable to those obtained using the
bag?of?words model. The real difference comes
for the BLEU score. The best BLEU score ob-
tained on the development set was 0.0586 when ?
was set to 0.14 and deletion cost was 15. On the
test set, the BLEU score obtained was 0.0473.
5.3 Tree based model
The lexical accuracy values of the words obtained
in this model are comparable to the lexical accu-
racy values of the bag of words model. The total
number of features used for training this model are
118839 (with count-cutoff of 2). On the develop-
ment set, we obtained a BLEU score of 0.0650 for
? set at 0.17 and the deletion cost set at 20. On
the test set, we obtained a BLEU score of 0.0498.
We can see that the BLEU scores are now bet-
ter than the ones obtained using any of the other
models discussed before. This is because the Tree
based model has both the strengths of the global
lexical selection that ensures high quality lexical
items in the target sentences and that of an efficient
reconstruction model which takes care of long dis-
tance reordering. The table summarizes the BLEU
scores obtained by the three models on the devel-
opment and test sets.
Devel. Set Test. Set
Bag-of-Words 0.0545 0.0428
Sequential 0.0586 0.0473
Hierarchical 0.0650 0.0498
Table 1: Summary of the results
6 Conclusion
In this paper, we present a novel approach to lex-
ical selection where the target words are associ-
ated with the entire source sentence (global) with-
out the need for local associations. This technique
is used by three models (Bag?of?words model, se-
quential model and hierarchical model) which pre-
dict the target language words given a source sen-
tence and then order the words appropriately. We
show that a hierarchical model performs best when
compared to the other two models. The hierar-
chical model presented in this paper has both the
strengths of the global lexical selection and effi-
cient reconstruction model.
101
In the future, we are planning to improve the hi-
erarchical model by making two primary additions
? Handling cases of structural non-
isomorphism between source and target
sentences.
? Obtaining K-best target string per node of the
source dependency tree instead of just one
per node. This would allow us to explore
more possibilities without having to compro-
mise much on computational complexity.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou-
glas. 1998. Automatic acquisition of hierarchical
transduction models for machine translation. In Pro-
ceedings of the 36th Annual Meeting Association for
Computational Linguistics, Montreal, Canada.
A.L. Berger, Stephen A. D. Pietra, D. Pietra, and J. Vin-
cent. 1996. A Maximum Entropy Approach to Nat-
ural Language Processing. Computational Linguis-
tics, 22(1):39?71.
P. Brown, S.D. Pietra, V.D. Pietra, and R. Mercer.
1993. The Mathematics of Machine Translation:
Parameter Estimation. Computational Linguistics,
16(2):263?312.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
263?270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
P. Haffner. 2006. Scaling large margin classifiers for
spoken language understanding. Speech Communi-
cation, 48(iv):239?261.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proceedings of
the ACL Workshop on Building and Using Parallel
Texts, pages 167?174, Ann Arbor, Michigan.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML, San Francisco, CA.
Franz Och, Christoph Tillmann, and Herman Ney.
1999. Improved alignment models for statistical
machine translation. In In Proc. of the Joint Conf. of
Empirical Methods in Natural Language Processing
and Very Large Corpora, pages 20?28.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proceedings of 39th
ACL.
102
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1114?1123, Dublin, Ireland, August 23-29 2014.
Fast Domain Adaptation of SMT models without in-Domain Parallel Data
Prashant Mathur
?
Fondazione Bruno Keseler
Povo - 38100 (IT)
first@fbk.eu
Sriram Venkatapathy
Xerox Research Center Europe
Meylan (FR)
first.last@xrce.xerox.com
Nicola Cancedda
?
Microsoft, London (UK)
first.last@gmail.com
Abstract
We address a challenging problem frequently faced by MT service providers: creating a domain-
specific system based on a purely source-monolingual sample of text from the domain. We solve
this problem by introducing methods for domain adaptation requiring no in-domain parallel data.
Our approach yields results comparable to state-of-the-art systems optimized on an in-domain
parallel set with a drop of as little as 0.5 BLEU points across 4 domains.
1 Introduction
We consider the problem of creating the best possible statistical machine translation (SMT) system for
a specific domain when no parallel sample or training data from such domain is available. We assume
that we have access to a collection of phrase tables (PT) and other models independently created from
now unavailable corpora, and we receive a monolingual source language sample from a text source we
would like to optimize for.
For a MT provider to deliver a SMT system tailored to a customer?s domain, a sample dataset is
requested. In most cases, the customer is able to provide an in-domain mono-lingual sample from his
operations. However, it is generally not feasible for the customer to provide the translations as well
because the customer has to hire professional translators to do that. In such a scenario, the translations has
to be generated by MT service provider itself by hiring human translators thus requiring an investment
upfront. The methods proposed in this paper aim to avoid that by building a good quality pilot SMT
system leveraging only sample mono-lingual source corpus, and previously trained library of models.
This in turn postpones the task of generating in-domain parallel data to a later date when there is a
commitment by the customer.
Unavailability of the raw parallel data could derive from a trading model where data owners share
intermediate-level resources like PTs, Reordering Models (RM) and Language Models (LM), but can
not, or do not want to, share the textual data such resources were derived from. This particular scenario
has been explained in (Cancedda, 2012).
This scenario is similar to the multi-model framework studied in (Sennrich et al., 2013), with the
additional challenge that no parallel development set is available. We build on the linear mixture model
combination of the cited work, extending it to our more challenging environment:
1. We propose a new measure derived from the popular BLEU score (Papineni et al., 2002) to assess
the fitness of a PT to cope with a given monolingual sample S. This measure is computed from
n-gram statistics that can be easily extracted from a PT.
2. We propose a new method for tuning the parameters of a log linear model that does not require
an in-domain parallel development set, and yet achieves results very close to traditional tuning on
parallel in-domain data.
We present our proposed metric BLEU-PT and computation of multi-model in Section 2. The pa-
rameter estimation of log-linear parameters of the SMT system is described in Section 3. We present
experiments and results in Sections 4 and 6 respectively.
?
Major part of the work was performed when the authors were in Xerox Research Center Europe.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1114
2 Building Multi-Model
Given a library of phrase tables, the goal of this step is to generate a domain adapted multi-model. The
challenging aspect in our scenario is the lack of in-domain parallel data, as well as absence of original
parallel corpora corresponding to the library of models. This rules out the possibility of using metrics
such as cross-entropy (Sennrich, 2012b) or LM-perplexity for computing the mixing coefficients. We
present our proposed metric in section 2.1, and interpolation of the phrase tables in section 2.2.
2.1 BLEU-PT
Given a source corpus s, and a set of phrase tables {pt
1
, pt
2
,. . . ,pt
n
}, the goal is to measure the similarity
of each of these tables with s. For measuring the similarity, we use BLEU-PT which is an adaptation of
the popular BLEU score for measuring the similarity between a corpus and a phrase table. The metric
BLEU-PT is measured as described in Equation 1.
BLEU-PT(PT, S) =
(
4
?
n=1
match(n|pt, s)
total(n|s)
)
1/4
(1)
where match(n|pt, s) is the count of n-grams of order n in the source corpus s that exist in the source
side of the phrase table pt. total(n|s) is the number of n-grams of order n in the source corpus.
2.2 Interpolating Models
A state-of-the-art approach for building multi-models is through linear interpolation of component mod-
els, exemplified in Equation 2 for the case of the forward conditional phrase translation model.
h
phr
(s, t) = log
N
?
j=1
?
j
P
phr,j
(t|s) (2)
Various approaches have been suggested for computing the coefficients ? of the interpolated model, the
most recent being perplexity minimization described in (Sennrich, 2012b), where each translation model
feature is optimized separately on the parallel development set. Our work is set in a scenario where no
parallel development set is available for optimizing the interpolation coefficients. We have also observed
that perplexity minimization is computationally intensive, requires aligned parallel development set, and
the optimization time increases rapidly with increasing number of component models (for details, see
Section 4.2).
We propose a simple approach for computation of the mixing coefficients that relies on the similarity
of each model with respect to the test set. The mixing coefficients are obtained by normalizing similarity
values. The similarity between a model (phrase table) and a corpus is computed using the BLEU-PT
metric proposed in the previous section. Another similarity metric that can be used is LM Perplexity.
However, in the current scenario we do not have resources (training data) to build a source side LM for
computing the perplexity.
We empirically compare our method for computing mixing coefficients with the the perplexity min-
imization method. We also experiment with applying the mixing coefficients obtained by using our
method for mixing features of a reordering and language model.
3 Parameter Estimation
The overall quality of translation is strongly impacted by how optimized the weights of the log-linear
combination of various translation features are for a domain of interest. MERT (Och, 2003) and MIRA
(Watanabe et al., 2007) are popular solution to compute an optimal weight vector by minimizing the error
on a held-out parallel development set. BLEU and its approximations are commonly used error metrics.
In this paper we assume lack of a parallel development set, therefore the above methods cannot be used.
Pecina et. al. (2012) showed that the optimized log-linear weight vector
1
of a SMT system does not
depend as much on the actual domain of the development set (on which the system was optimized), as
1
Not to be confused with the mixing coefficients in a linear combination of model components.
1115
on how ?distant? the relevant domain is from the domain of the training corpus used to build the SMT
models. This is an important finding. It means that the weight vector can be modeled as a function of the
distance/similarity between the in-domain development set and the model built from the training set. In
this work, we learn this function from examples of previous parameter optimizations, using our BLEU-
PT as a similarity metric. Once we have retrieved the most relevant PTs (translation and reordering
models) from our library, and we have linearly interpolated them using normalized BLEU-PT, we use
the learned model to estimate the optimal value of the log-linear weights, instead of optimizing them.
In order to learn this mapping, we create a dataset of examples (pairs of the form <BLEU-PT, log-
linear weight vector>, where weight vectors are normalized to ensure comparability across models) by
performing repeated optimizations for out-domain models on a number of parallel development sets (see
section 4 for more details of this data) using a traditional optimization method (MIRA in this work).
Based on this dataset, the function of our interest can therefore be learnt using a supervised approach.
We explore two parametric methods and a non-parametric method. We present these in Section 3.1, and
3.2 respectively. For a mono-lingual source in a new domain, the BLEU-PT can be computed, and then
mapped to the appropriate weight vector using the methods presented below.
3.1 Parametric Methods
We considered two distinct parametric methods for estimating the mapping from model/corpus similarity
into weight vectors. The first one makes the assumption that parameters can be estimated independently
of one another, given the similarity, whereas the second tries to leverage known covariance between
distinct parameters in the vector.
3.1.1 Linear Regression
Motivated by initial experiments highlighting strong correlation between BLEU-PT and optimal feature
weights (see Section 5.1 below), we assumed here a simple linear relation of the form:
?
?
i
= W
i
X + b
i
(3)
where ?
?
i
is the optimal log-linear weight for feature i, X is the feature vector (BLEU-PT vector), W
i
and b
i
are coefficients to be estimated. While a drastic assumption, this has the advantage of limiting
the risk of overfitting in a situation like ours where there is only relatively few datapoints to learn from.
We estimate a
i
and b
i
by simple least squares regression. Once these are available for all features, we
can predict the log linear weights of any model given its BLEU-PT similarity to a monolingual source
sample using Eq. 3.
3.1.2 Multi-Task learning
Optimal log-linear parameters might not be fully independent given BLEU-PT, especially since it is
known that model features can be highly correlated. To account for correlation between parameter
weights, we explore the use of multi-task lasso
2
(Caruana, 1997) where several functions corresponding
to each parameter are learned jointly considering the correlation between their values observed in the
training data. Multi-task lasso consists of a least square loss model trained along with a regularizer and
the objective is to minimize the following:
argmin
w
1
2N
||X ?W ? ?||
2
2
+ ?||W ||
21
where; ||W ||
21
=
M
?
j
?
?
i
w
2
ij
(4)
Here, N is the number of training samples, X is the feature vector(BLEU-PT score vector) ? is the label
vector(log linear weights). ||W ||
21
is the l
21
regularizer (Yang et al., 2011). The problem of prediction
of log linear weights is reduced to prediction of i interlinked tasks where each task has M features
3
.
Coefficients are calculated using coordinate descent algorithm in Multi-Task lasso. Once the coefficients
are calculated we use Eq. 3 to predict the log linear weights.
2
http://scikit-learn.org/
3
In our case we only have 1 feature i.e. BLEU-PT score.
1116
3.2 Non Parametric: Nearest Neighbor
Finally, instead of building a parametric predictor for log linear weights, we experimented with a simple
nearest-neighbor approach:
?
?
i
= ?
i
(M
j
?
) (5)
whereM
j
ranges over the linearly interpolated phrase tables, and ?
i
(M) returns the stored optimal value
for the i
th
log-linear weight, and:
j
?
= argmin
j
min
s
?
(|BLEU-PT(M, s)? BLEU-PT
?
(M
j
, s
?
)|) (6)
where s is the monolingual sample on which we want to calculate the BLEU-PT and s
?
ranges over
the source sides of our available parallel development sets. In other words, a BLEU-PT of a model is
calculated on the source sample to be translated and the log-linear weight is chosen which corresponds
to BLEU-PT
?
, where BLEU-PT
?
is a training data point closest to BLEU-PT. This approach is close to
the cross-domain tuning of Pecina et. al. (2012).
4 Experimental Program
We conducted a number of experiments for English-French language pair, comparing the methods pro-
posed in the previous sections among one another and against state-of-the-art baselines and oracles.
4.1 Datasets
In this section, we present the datasets (EN-FR) that we have used for our experiments and the training
data that was created for the purpose of supervised learning. We collected a set of 12 publicly available
corpora and 1 proprietary corpus, statistics of datasets are provided in Table 1.
Corpus Train Development Test
Commoncrawl 78M 12.4K 12.6K
ECB 4.7M 13.9K 14K
EMEA 13.8M 14K 15.7K
EUconst 133K 8K 8.4K
Europarl 52.8M 13.5K 13.5K
P1 5M 35K 14.5K
KDE4 1.5M 12.8K 5.8K
News Comm. 4M 12.7K 65K
OpenOffice 400K 5.4K 5.6K
OpenSubs 156M 16K 15.7K
PHP 314K 3.5K 4K
TED 2.65M 21K 14.6K
UN 1.92M 21K 21K
Table 1: Statistics of parallel sets (# of source tokens)
500 1000 1500 2000 2500 3000 3500 4000 4500Phrase Table size (MB)0
500
1000
1500
2000
2500
Search
 Time 
(secon
ds)
Calculating time of Metrics v/s Size of Phrase Tablebleu-ptcross entropy
Figure 1: BLEU-PT v/s Cross-Entropy
Commoncrawl (CC) (Smith et al., 2013) and News Commentary (Bojar et al., 2013) corpora were
provided in the 2013 shared translation task organized with workshop on machine translation. TED talks
data was released as a part of IWSLT evaluation task (Cettolo et al., 2012). ECB, EMEA, EUconst,
OpenOffice, OpenSubs 2011, PHP and UN corpora are provided as a part of OPUS parallel corpora
(Tiedemann, 2012). The parallel corpora from OPUS were randomly split into training, development
and testsets. Commoncrawl, News Commentary and TED datasets were used as they were provided in
the evaluation task.
Out of 13 different domain datasets we selected 4 datasets randomly: Commoncrawl, KDE4, TED and
UN (in bold in Table 1), to test our methods.
4.2 BLEU-PT v/s Cross-Entropy
We compared the overheads of calculating BLEU-PT and Cross-Entropy
4
. We are interested in estimat-
ing whether with increasing number of phrase tables the computation of both measures becomes slow or
memory intensive.
4
We used tmcombine.py script that comes along with the moses package to calculate the mixing coefficients.
1117
Another advantage of using BLEU-PT apart from fast retrieval is that we can index the phrase tables
using wFSA based indexing (explanation of indexing the phrase tables is not in the scope of this paper)
and store the FSTs in binarised format on disk. When a source sample comes, we just load the indexed
binaries and calculate the BLEU-PT while this cannot be achieved when we want to calculate cross
entropy because we have to do one pass over all the phrase tables in question.
Experimental results depicted in Figure 1 shows that computation of BLEU-PT is fast (160 seconds)
while computation of cross-entropy is slow (42 minutes) when we combine 12 phrase tables with total
size of 4.2GB.
4.3 Training data for supervised learning and testing
As mentioned earlier, for estimating the parameters we require a training data containing the tuples of
<BLEU-PT, log-linear-weight>. We perform parameter estimation on four of our datasets: Common-
crawl, KDE4, TED and UN. So, for obtaining evaluation results on say, UN, the rest of the resources
are used for generating the training data. Our experimental setup can be explained well using the Venn
diagram shown in Figure 2.
We set one of four domains as the test domain (in this case, UN) whose parallel set is not available to us
and call it setup-UN. The training data tuples obtained from the rest of the 12 datasets are used to estimate
parameters for the UN domain. From these 12 datasets we perform a round-robin experiment where one
by one each dataset is considered as in-domain and the rest as out-domain. In-domain dataset provides the
development set and the rest 11 out-domain models are linearly combined to build translation models.
In figure 2, for example, the development set from the TED domain is taken as the development set
of the multi-model build using the rest (i.e. excluding TED and UN). This multi-model is built by a
weighted linear combination of the out-domain models (11 models). The parameters of this multi-model
are tuned on the in-domain development set using MIRA. Simultaneously, we also calculate the BLEU-
PT of the linear interpolated model on the source side of the in-domain development set (i.e. TED).
This provides us the tuples of BLEU-PT and the log linear weights, which is our training data. So, four
sets of experiments are conducted (one each for four datasets considered for testing), and for each set
of experiments, there are 12 training data points. The final evaluation is done by measuring the BLEU
score obtained on each test set using the predicted parameter estimates.
Reiterating, our optimizing method is fast, and hence, we are not not looking to learn the parameters
apriori for all the domains based on a source side of the development set. The goal is to do a fast
adaptation by predicting the parameters using statistical models for every new test in a particular domain
even in the absence of a parallel development set.
4.4 Prediction
For prediction of parameters for a new domain, the BLEU-PT of the sample source corpus (UN in our
example) is measured with the multi-model built on all the models (all the rest of 12 datasets including
the TED model) and then the supervised predictor is applied. In our experiments, we test both parametric
and non-parametric methods to estimate the parameters based on the training data obtained using the 12
domains.
TEST
In-domain
UN
...
...
EMEA
ECB
KDE
PHP
DEV
In-domain
TED
Figure 2: Cross domain tuning setup
Figure 3: Correlation of log linear weights with BLEU-PT
when indomain sets set to UN and TED
1118
Domain Linear Interpolation
System Train Dev Param. Est. TM(coeff.) RM(coeff.) LM(coeff.)
in-dom-train In In mira N.A N.A N.A
mira-bleupt-tm-rm Out In mira 3 3 7
mira-perp-tm-bleupt-rm Out In mira 3(Perp. Min) 3 7
mira-bleupt-tm-rm-perp-lm Out In mira 3 3 3(LM Perp. Min.)
mira-bleupt-all Out In mira 3 3 3
def-bleupt-all Out 7 def 3 3 3
gen-reg-bleupt-all Out 7 regression 3 3 3
gen-mtl-bleupt-all Out 7 multi-task 3 3 3
gen-nn-bleupt-all Out 7 Near.Neigh. 3 3 3
top5-reg-bleupt-all Out 7 regression 3 3 3
top5-mtl-bleupt-all Out 7 multi-task 3 3 3
top5-nn-bleupt-all Out 7 Near.Neigh. 3 3 3
Table 2: System Description: Each system?s training domain and development set domain along with the optimizer/predictor
is mentioned. def-bleupt-all uses default weights from Moses decoder. Near.Neigh. shows that we used Nearest Neighbor
predictor for optimizing weights. 7 represent log linear interpolation of models while 3 represents linear interpolation. The
mixing coefficients for linear interpolation are calculated by normalizing bleu-pt scores unless mentioned otherwise.
5 Experiments and Results
5.1 Correlation analysis
Before embarking in the actual regression task, we examined the correlation between the similarity values
(BLEU-PT) and the various weights in the training data. If there is good correlation between BLEU-PT
and a particular parameter, then the linear regressor is expected to fit well and then predict an accurate
parameter value for a new domain. For computing the correlation, we use Pearson correlation coefficient
(PCC). Figure 3 shows the PCC between the feature weights and the BLEU-PT scores. The tm?s are the
translation model features, and rm?s are the reordering model features.
We see that there is either a strong positive correlation or a strong negative correlation for most fea-
tures in both the experimental setups shown in the figure 3. This validates our hypothesis that optimal
parameters for a new test domain can indeed be estimated with good reliability. One can also observe
that the correlation level also varies based on the mixture of training models. For example, the correla-
tion is much higher in the training data that excluded UN (setup-UN) than the one that excluded TED
(setup-TED).
In figure 3, one can also see that tm0 (forward phrase conditional probability) and tm2 (backward
phrase conditional probability) which are shown in previous work to be the two most important features
amongst all SMT features (Lopez and Resnik, 2006) in terms of their impact on translation quality, have
a high correlation in setup-UN.
5.2 Systems
All SMT systems were built using the Moses toolkit (Koehn et al., 2007). To automatically align the
parallel corpora we used MGIZA (Gao and Vogel, 2008). Aligned training data in each domain was
then used to create the corresponding component translation models and lexical reordering models. We
created 5-gram language models for every domain using SRILM (Stolcke, 2002) with improved Kneser-
Ney smoothing (Chen and Goodman, 1999) on the target side of the training parallel corpora. Log linear
weights for the systems were optimized using MIRA (Watanabe et al., 2007; Hasler et al., 2011) which
is provided in the Moses toolkit. Performance of the systems are measured in terms of BLEU computed
using the MultEval script (mteval-v13.pl).
We built one in-dom-train system where only in-domain training data is taken into account. This
system shows the importance of in-domain training data in SMT (Haddow and Koehn, 2012). Three
oracle systems are trained on out-domain training corpus and tuned on in-domain development data (in
this case there are four domains we chose to test on: UN, TED, CommonCrawl and KDE4), thus 4
systems for each of the in-domain test sets.
We build another set of SMT systems in which language models are combined by linear interpolation
5
.
5
Linear interpolation of 12 LMs result in one single large LM, thus, one weight. So, a total of 14 weights have to be
optimized or predicted
1119
The systems using linear interpolated LM (mixing coefficients are normalized BLEU-PT scores) are def-
bleupt-all, mira-bleupt-all, gen-reg-bleupt-all, gen-mtl-bleupt-all and gen-nn-bleupt-all. We compare
mira-bleupt-all with mira-bleupt-tm-rm-perp-lm where mixing coefficients for LM interpolation are cal-
culated by standard LM perplexity minimization method over target side of development set.
As mentioned earlier, ideally only a subset of all the models closer to the source sample should be
taken into account for quick adaptation, so we select the top five domains related to the source sample
and interpolate the respective models and address them as top5-* systems. Adding more domains would
unnecesary increase the size of the model and add more noise. Table 2 shows the configuration of
different systems. In the next section we compare the performances of these systems and report the
findings.
6 Results and Discussion
Table 3 presents results of the systems that use an in-domain parallel data. As expected, when an in-
domain corpus is used both for training as well as for optimizing the log-linear parameters, the pefor-
mance is much higher than those systems that do not use in-domain parallel corpus for training (Koehn
and Schroeder, 2007). We also observe that the use of normalized BLEU-PT for computing mixing
coefficients gives comparable performance to using Cross-Entropy. The primary advantage in using
BLEU-PT is that it can be compute much faster than Cross-Entropy (as shown in Figure 1). Evidently,
normalized BLEU-PT scores as mixing coefficients performs at par with mixing coefficients retrieved by
standard perplexity minimization method (Bertoldi and Federico, 2009). One can also use BLEU-PT for
LM interpolation in cases where target side in-domain text is not available.
System UN TED CC KDE
in-dom-train 67.87 29.98 26.62 35.82
mira-bleupt-tm-rm 44.14 31.20 17.43 24.25
mira-perp-tm-bleupt-rm 43.56 31.36 17.54 24.72
mira-bleupt-tm-rm-perp-lm 43.96 31.85 18.45 23.39
mira-bleupt-all 43.66 32.04 18.44 23.09
Table 3: Comparison of In-Domain system versus the estab-
lished Oracles in different setups.
System UN TED CC KDE
gen-reg-bleupt-all 43.27 32.18 17.95 21.05
gen-mtl-bleupt-all 43.35 32.61 18.26 20.67
gen-nn-bleupt-all 42.73 31.04 18.24 21.85
Table 4: Performance of generic systems (gen-*) in all se-
tups.
Table 4 illustrates the impact of phrase table retrieval on the performance of multi-model. All the
systems presented in this table use BLEU-PT for computing mixing coefficients, while the weights
are computed using the three techniques that we explored in this paper. We see that in case of re-
gression, the phrase table retrieval also results in a better MT performance. In the other two cases,
the results are comparable. It shows that retrieval helps in building smaller sized multi-models while
being more accurate on an average. Phrase table retrieval, thus, becomes particularly useful when a
multi-model needs to be built from a library of dozens of pre-trained phrase tables of various domains.
 15.5
 16
 16.5
 17
 17.5
 18
 18.5
 2  4  6  8  10  12
BLEU
 sco
re
Number of Models
Nearest NeighborRegressionMulti-Task Learning
Figure 4: BLEU scores when top k models were
used to evaluate commoncrawl test set where
k ? 1..12.
System UN TED CC KDE
def-bleupt-all 42.03 30.82 17.97 19.66
mira-bleupt-all 43.66 32.04 18.44 23.09
top5-reg-bleupt-all 43.39
N
32.31
N
18.10 21.54
N
top5-mtl-bleupt-all 43.56
N
32.60
N
18.14 20.91
N
top5-nn-bleupt-all 42.96
N
30.89
M
17.79 22.24
N
Table 5: Comparing the baseline system (def-bleupt-all)
and Oracle (mira-bleupt-all) with domain specific multi-model
systems trained on top5 domains.
N
and
M
denotes significantly
better results in comparison with def-bleupt-all system with
p-value < 0.0001 and < 0.05 respectively.
Table 5 compares our approach of computing log-linear weights (in the absence of in-domain develop-
ment set) to the state-of-art weight optimization technique MIRA (which requires an in-domain devel-
opment set). As a baseline, we set default weights to all the parameters, which was shown to a strong
1120
baseline in (Pecina et al., 2012). We see that the methods proposed by us perform significantly bet-
ter than the default weights baseline (improvement of more than 1.5 BLEU score on an average across 4
domains). Among the three approaches for computing weights, the method that uses multi-task lasso per-
forms best (except in setup-KDE where the non-parametric method performs best), along the expected
lines as multi-task lasso considers the correlation between various features. In comparison to MIRA, our
methods result in an average drop of as little as 0.5 BLEU points across 4 domains (see Table 5).
Figure 4 shows BLEU score curve when we vary the k in top-k systems. BLEU score curve is almost
tangential zero when k is between 5 and 6 which essentially means that selection of k = 5 is a good
choice. For CommonCrawl test set, the top five domains used were Europarl, OpenSubs, NewsCom-
mentary, TED and ECB. This is a significant result which indicates that one can build a good system for
a domain even in the absence of the parallel data in the domain of interest.
7 Related Work
Domain adaptation in statistical machine translation has been widely studied and leveraged through
adding more training data (Koehn and Knight, 2001), filtering of out of domain training data (Axelrod
et al., 2011; Koehn and Haddow, 2012), fillup technique (Bisazza et al., 2011), language model adap-
tation by perplexity minimization over in-domain data (Bertoldi and Federico, 2009) and various other
approaches. However, all the above adaptation approaches require either parallel in-domain corpus or
monolingual in-domain target side corpus, thus, not applicable in our scenario.
In this paper we studied mixture modelling of heterogeneous translation models which was first pro-
posed in Foster et. al. (2007). They showed various ways of computing mixing coefficients for linear
interpolation using several distance based metrics borrowed from information theory. However, to cal-
culate any such metrics it was required that one has an access to the source/target training corpus and
source/target development corpus. Other noteable works in mixture modelling in SMT are (Civera and
Juan, 2007; Razmara et al., 2012; Duan et al., 2010).
More recently, Sennrich (2012b) designed an approach to calculate mixing coefficients by minimizing
the perplexity of translation models over an aligned development set for mixture modelling via linear
interpolation or by weighting the corpora. Sennrich et. al. (2012a) clustered of a large heterogeneous
development corpus and tuned a translation system on different clusters. In the decoding phase each
sentence was assigned to a cluster and the translation system tuned on that cluster was used to translate
that sentence.
(Banerjee et al., 2010) build several domain specific translation systems, and trained a classifier to
assign each incoming sentence to a domain and use the domain specific system to translate the corre-
sponding sentence. They assume that each sentence in test set belongs to one of the already existing
domains which means it would fail in the case where the sentence doesn?t belong to any of the existing
domains. In our case we do not make any such assumptions.
Academically, above approaches are well suited for solving the problem of domain adaptation, but
during the deployment of SMT systems in industrial scenario where the client is unable to deliver the
parallel in-domain data these approaches fail to provide a quick solution.
8 Conclusion
We present an approach to multi-model domain adaptation in a particularly challenging setting where
there is no parallel in-domain data. Parameter estimation without in-domain development set is a problem
that, to the best of our knowledge, has not been addressed before. We designed a method for tuning model
parameters without parallel development set and validated it through an experimental program for which
we compared performances against an array of Oracles and Baselines. The effectiveness of the proposed
method empirically supports the findings of (Pecina et al., 2012), who discovered that the log linear
weights largely depend on the distance of training domain from the domain on which the models are
being optimized on. As a side result, we designed in the process a novel similarity metric between a
phrase table and a source sample and implemented it effectively using wFSAs. We empirically showed
the excellent computation speed of BLEU-PT scores as compared to standard Cross-Entropy measure
using standard toolkits.
1121
Acknowledgement
The authors thank the three anonymous reviewers for their comments and suggestions.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages
355?362, Stroudsburg, PA, USA. Association for Computational Linguistics.
Pratyush Banerjee, Jinhua Du, Baoli Li, Sudip Kumar Naskar, Andy Way, and Josef Van Genabith. 2010. Com-
bining multi-domain statistical machine translation models using automatic classifiers. In Proceedings of 9th
Conference of the Association for Machine Translation in the Americas.
Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolin-
gual resources. In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ?09, pages
182?189, Stroudsburg, PA, USA. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus Interpolation Methods for Phrase-based
SMT Adaptation. In International Workshop on Spoken Language Translation (IWSLT), pages 136?143, San
Francisco, CA.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1?44, Sofia,
Bulgaria, August. Association for Computational Linguistics.
Nicola Cancedda. 2012. Private access to phrase tables for statistical machine translation. In ACL (2), pages
23?27.
Rich Caruana. 1997. Multitask learning. Mach. Learn., 28(1):41?75, July.
Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. Wit
3
: Web inventory of transcribed and translated
talks. In Proceedings of the 16
th
Conference of the European Association for Machine Translation (EAMT),
pages 261?268, Trento, Italy, May.
Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling.
Computer Speech and Language, 4(13):359?393.
Jorge Civera and Alfons Juan. 2007. Domain adaptation in statistical machine translation with mixture modelling.
In Proceedings of the Second Workshop on Statistical Machine Translation, pages 177?180, Prague, Czech
Republic, June. Association for Computational Linguistics.
Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou. 2010. Mixture model-based minimum bayes risk de-
coding using multiple machine translation systems. In Proceedings of the 23rd International Conference on
Computational Linguistics, pages 313?321. Association for Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-model adaptation for smt. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT ?07, pages 128?135, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Software Engineering,
Testing, and Quality Assurance for Natural Language Processing, SETQA-NLP ?08, pages 49?57, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Barry Haddow and Philipp Koehn. 2012. Analysing the effect of out-of-domain data on smt systems. In Pro-
ceedings of the Seventh Workshop on Statistical Machine Translation, pages 422?432, Montreal, Canada, June.
Association for Computational Linguistics.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2011. Margin Infused Relaxed Algorithm for Moses. The Prague
Bulletin of Mathematical Linguistics, 96:69?78.
Philipp Koehn and Barry Haddow. 2012. Towards effective use of training data in statistical machine transla-
tion. In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ?12, pages 317?321,
Stroudsburg, PA, USA. Association for Computational Linguistics.
1122
Philipp Koehn and Kevin Knight. 2001. Knowledge sources for word-level translation models. In In Proceedings
of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 27?35.
Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In
Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07, pages 224?227, Strouds-
burg, PA, USA. Association for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open Source Toolkit for Statisti-
cal Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177?180, Prague, Czech
Republic.
Adam Lopez and Philipp Resnik. 2006. Word-based alignment, phrase-based translation: What?s the link? In In
Proceedings of AMTA, pages 90?99.
Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Erhard Hinrichs and
Dan Roth, editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu. 2002. BLEU : a Method for Automatic Evalua-
tion of Machine Translation. In Computational Linguistics, volume pages, pages 311?318.
Pavel Pecina, Antonio Toral, and Josef van Genabith. 2012. Simple and effective parameter tuning for domain
adaptation of statistical machine translation. In COLING, pages 2209?2224.
Majid Razmara, George Foster, Baskaran Sankaran, and Anoop Sarkar. 2012. Mixing multiple translation models
in statistical machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers-Volume 1, pages 940?949. Association for Computational Linguistics.
Rico Sennrich, Holger Schwenk, and Walid Aransa. 2013. A multi-domain translation model framework for
statistical machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 832?840, Sofia, Bulgaria, August. Association for Computational
Linguistics.
Rico Sennrich. 2012a. Mixture-modeling with unsupervised clusters for domain adaptation in statistical machine
translation. In Proceedings of the 16th Annual Conference of the European Association of Machine Translation
(EAMT).
Rico Sennrich. 2012b. Perplexity minimization for translation model domain adaptation in statistical machine
translation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computa-
tional Linguistics, EACL ?12, pages 539?549, Stroudsburg, PA, USA. Association for Computational Linguis-
tics.
Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch, and Adam Lopez.
2013. Dirt cheap web-scale parallel text from the common crawl. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1374?1383, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of ICSLP, Denver,
Colorado.
J?org Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey, may. European Language Resources Association (ELRA).
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical
machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 764?773, Prague, Czech
Republic, June. Association for Computational Linguistics.
Yi Yang, Heng Tao Shen, Zhigang Ma, Zi Huang, and Xiaofang Zhou. 2011. l 2, 1-norm regularized discriminative
feature selection for unsupervised learning. In Proceedings of the Twenty-Second international joint conference
on Artificial Intelligence-Volume Volume Two, pages 1589?1594. AAAI Press.
1123
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 22?30,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Prediction of Learning Curves in Machine Translation
Prasanth Kolachina? Nicola Cancedda? Marc Dymetman? Sriram Venkatapathy?
? LTRC, IIIT-Hyderabad, Hyderabad, India
? Xerox Research Centre Europe, 6 chemin de Maupertuis, 38240 Meylan, France
Abstract
Parallel data in the domain of interest is the
key resource when training a statistical ma-
chine translation (SMT) system for a specific
purpose. Since ad-hoc manual translation can
represent a significant investment in time and
money, a prior assesment of the amount of
training data required to achieve a satisfac-
tory accuracy level can be very useful. In this
work, we show how to predict what the learn-
ing curve would look like if we were to manu-
ally translate increasing amounts of data.
We consider two scenarios, 1) Monolingual
samples in the source and target languages are
available and 2) An additional small amount
of parallel corpus is also available. We pro-
pose methods for predicting learning curves in
both these scenarios.
1 Introduction
Parallel data in the domain of interest is the key re-
source when training a statistical machine transla-
tion (SMT) system for a specific business purpose.
In many cases it is possible to allocate some budget
for manually translating a limited sample of relevant
documents, be it via professional translation services
or through increasingly fashionable crowdsourcing.
However, it is often difficult to predict how much
training data will be required to achieve satisfactory
translation accuracy, preventing sound provisional
budgetting. This prediction, or more generally the
prediction of the learning curve of an SMT system
as a function of available in-domain parallel data, is
the objective of this paper.
We consider two scenarios, representative of real-
istic situations.
1. In the first scenario (S1), the SMT developer is
given only monolingual source and target sam-
ples from the relevant domain, and a small test
parallel corpus.
?This research was carried out during an internship at Xerox
Research Centre Europe.
2. In the second scenario (S2), an additional small
seed parallel corpus is given that can be used
to train small in-domain models and measure
(with some variance) the evaluation score at a
few points on the initial portion of the learning
curve.
In both cases, the task consists in predicting an eval-
uation score (BLEU, throughout this work) on the
test corpus as a function of the size of a subset of
the source sample, assuming that we could have it
manually translated and use the resulting bilingual
corpus for training.
In this paper we provide the following contribu-
tions:
1. An extensive study across six parametric func-
tion families, empirically establishing that a
certain three-parameter power-law family is
well suited for modeling learning curves for the
Moses SMT system when the evaluation score
is BLEU. Our methodology can be easily gen-
eralized to other systems and evaluation scores
(Section 3);
2. A method for inferring learning curves based
on features computed from the resources avail-
able in scenario S1, suitable for both the sce-
narios described above (S1) and (S2) (Section
4);
3. A method for extrapolating the learning curve
from a few measurements, suitable for scenario
S2 (Section 5);
4. A method for combining the two approaches
above, achieving on S2 better prediction accu-
racy than either of the two in isolation (Section
6).
In this study we limit tuning to the mixing param-
eters of the Moses log-linear model through MERT,
keeping all meta-parameters (e.g. maximum phrase
length, maximum allowed distortion, etc.) at their
default values. One can expect further tweaking to
lead to performance improvements, but this was a
22
necessary simplification in order to execute the tests
on a sufficiently large scale.
Our experiments involve 30 distinct language pair
and domain combinations and 96 different learning
curves. They show that without any parallel data
we can predict the expected translation accuracy at
75K segments within an error of 6 BLEU points (Ta-
ble 4), while using a seed training corpus of 10K
segments narrows this error to within 1.5 points (Ta-
ble 6).
2 Related Work
Learning curves are routinely used to illustrate how
the performance of experimental methods depend
on the amount of training data used. In the SMT
area, Koehn et al (2003) used learning curves to
compare performance for various meta-parameter
settings such as maximum phrase length, while
Turchi et al (2008) extensively studied the be-
haviour of learning curves under a number of test
conditions on Spanish-English. In Birch et al
(2008), the authors examined corpus features that
contribute most to the machine translation perfor-
mance. Their results showed that the most predic-
tive features were the morphological complexity of
the languages, their linguistic relatedness and their
word-order divergence; in our work, we make use of
these features, among others, for predicting transla-
tion accuracy (Section 4).
In a Machine Learning context, Perlich et al
(2003) used learning curves for predicting maximum
performance bounds of learning algorithms and to
compare them. In Gu et al (2001), the learning
curves of two classification algorithms were mod-
elled for eight different large data sets. This work
uses similar a priori knowledge for restricting the
form of learning curves as ours (see Section 3), and
also similar empirical evaluation criteria for compar-
ing curve families with one another. While both ap-
plication and performance metric in our work are
different, we arrive at a similar conclusion that a
power law family of the form y = c ? a x?? is a
good model of the learning curves.
Learning curves are also frequently used for de-
termining empirically the number of iterations for
an incremental learning procedure.
The crucial difference in our work is that in the
previous cases, learning curves are plotted a poste-
riori i.e. once the labelled data has become avail-
able and the training has been performed, whereas
in our work the learning curve itself is the object of
the prediction. Our goal is to learn to predict what
the learning curve will be a priori without having to
label the data at all (S1), or through labelling only a
very small amount of it (S2).
In this respect, the academic field of Computa-
tional Learning Theory has a similar goal, since it
strives to identify bounds to performance measures1,
typically including a dependency on the training
sample size. We take a purely empirical approach
in this work, and obtain useful estimations for a case
like SMT, where the complexity of the mapping be-
tween the input and the output prevents tight theo-
retical analysis.
3 Selecting a parametric family of curves
The first step in our approach consists in selecting
a suitable family of shapes for the learning curves
that we want to produce in the two scenarios being
considered.
We formulate the problem as follows. For a cer-
tain bilingual test dataset d, we consider a set of
observations Od = {(x1, y1), (x2, y2)...(xn, yn)},
where yi is the performance on d (measured using
BLEU (Papineni et al, 2002)) of a translation model
trained on a parallel corpus of size xi. The corpus
size xi is measured in terms of the number of seg-
ments (sentences) present in the parallel corpus.
We consider such observations to be generated by
a regression model of the form:
yi = F (xi; ?) + i 1 ? i ? n (1)
where F is a function depending on a vector param-
eter ? which depends on d, and i is Gaussian noise
of constant variance.
Based on our prior knowledge of the problem,
we limit the search for a suitable F to families that
satisfies the following conditions- monotonically in-
creasing, concave and bounded. The first condition
just says that more training data is better. The sec-
ond condition expresses a notion of ?diminishing
returns?, namely that a given amount of additional
training data is more advantageous when added to
a small rather than to a big amount of initial data.
The last condition is related to our use of BLEU ?
which is bounded by 1 ? as a performance mea-
sure; It should be noted that some growth patterns
which are sometimes proposed, such as a logarith-
mic regime of the form y ' a + b log x, are not
1More often to a loss, which is equivalent.
23
compatible with this constraint.
We consider six possible families of functions sat-
isfying these conditions, which are listed in Table 1.
Preliminary experiments indicated that curves from
Model Formula
Exp3 y = c? e?ax+b
Exp4 y = c? e?ax
?+b
ExpP3 y = c? e(x?b)
?
Pow3 y = c? ax??
Pow4 y = c? (?ax+ b)??
ILog2 y = c? (a/ log x)
Table 1: Curve families.
the ?Power? and ?Exp? family with only two param-
eters underfitted, while those with five or more pa-
rameters led to overfitting and solution instability.
We decided to only select families with three or four
parameters.
Curve fitting technique Given a set of observa-
tions {(x1, y1), (x2, y2)...(xn, yn)} and a curve fam-
ily F (x; ?) from Table 1, we compute a best fit ??
where:
?? = argmin
?
n?
i=1
[yi ? F (xi; ?)]
2, (2)
through use of the Levenberg-Marquardt
method (More?, 1978) for non-linear regression.
For selecting a learning curve family, and for all
other experiments in this paper, we trained a large
number of systems on multiple configurations of
training sets and sample sizes, and tested each on
multiple test sets; these are listed in Table 2. All
experiments use Moses (Koehn et al, 2007). 2
Domain
Source Target # Test
Language Language sets
Europarl (Koehn, 2005)
Fr, De, Es En
4
En Fr, De, Es
KFTT (Neubig, 2011) Jp, En En, Jp 2
EMEA (Tiedemann, 2009) Da, De En 4
News (Callison-Burch et al, 2011) Cz,En,Fr,De,Es Cz,En,Fr,De,Es 3
Table 2: The translation systems used for the curve fit-
ting experiments, comprising 30 language-pair and do-
main combinations for a total of 96 learning curves.
Language codes: Cz=Czech, Da=Danish, En=English,
De=German, Fr=French, Jp=Japanese, Es=Spanish
The goodness of fit for each of the families is eval-
2The settings used in training the systems are those
described in http://www.statmt.org/wmt11/
baseline.html
uated based on their ability to i) fit over the entire set
of observations, ii) extrapolate to points beyond the
observed portion of the curve and iii) generalize well
over different datasets .
We use a recursive fitting procedure where the
curve obtained from fitting the first i points is used
to predict the observations at two points: xi+1, i.e.
the point to the immediate right of the currently ob-
served xi and xn, i.e. the largest point that has been
observed.
The following error measures quantify the good-
ness of fit of the curve families:
1. Average root mean-squared error (RMSE):
1
N
?
c?S
?
t?Tc
{
1
n
n?
i=1
[yi ? F (xi; ??)]
2
}1/2
ct
where S is the set of training datasets, Tc is the
set of test datasets for training configuration c,
?? is as defined in Eq. 2, N is the total number
of combinations of training configurations and
test datasets, and i ranges on a grid of training
subset sizes.The expressions n, xi, yi, ?? are all
local to the combination ct.
2. Average root mean squared residual at next
point X = xi+1 (NPR):
1
N
?
c?S
?
t?Tc
{
1
n? k ? 1
n?1?
i=k
[yi+1 ? F (xi+1; ??
i)]2
}1/2
ct
where ??i is obtained using only observations
up to xi in Eq. 2 and where k is the number of
parameters of the family.3
3. Average root mean squared residual at the last
point X = xn (LPR):
1
N
?
c?S
?
t?Tc
{
1
n? k ? 1
n?1?
i=k
[yn ? F (xn; ??
i)]2
}1/2
ct
Curve fitting evaluation The evaluation of the
goodness of fit for the curve families is presented
in Table 3. The average values of the root mean-
squared error and the average residuals across all the
learning curves used in our experiments are shown
in this table. The values are on the same scale as the
BLEU scores. Figure 1 shows the curve fits obtained
3We start the summation from i = k, because at least k
points are required for computing ??i.
24
Figure 1: Curve fits using different curve families on a
test dataset
for all the six families on a test dataset for English-
German language pair.
Curve Family RMSE NPR LPR
Exp3 0.0063 0.0094 0.0694
Exp4 0.0030 0.0036 0.0072
ExpP3 0.0040 0.0049 0.0145
Pow3 0.0029 0.0037 0.0091
Pow4 0.0026 0.0042 0.0102
ILog2 0.0050 0.0067 0.0146
Table 3: Evaluation of the goodness of fit for the six fam-
ilies.
Loooking at the values in Table 3, we decided to
use the Pow3 family as the best overall compromise.
While it is not systematically better than Exp4 and
Pow4, it is good overall and has the advantage of
requiring only 3 parameters.
4 Inferring a learning curve from mostly
monolingual data
In this section we address scenario S1: we have
access to a source-language monolingual collec-
tion (from which portions to be manually translated
could be sampled) and a target-language in-domain
monolingual corpus, to supplement the target side of
a parallel corpus while training a language model.
The only available parallel resource is a very small
test corpus. Our objective is to predict the evolution
of the BLEU score on the given test set as a function
of the size of a random subset of the training data
that we manually translate4. The intuition behind
this is that the source-side and target-side mono-
lingual data already convey significant information
about the difficulty of the translation task.
We proceed in the following way. We first train
models to predict the BLEU score at m anchor sizes
s1, . . . , sm, based on a set of features globally char-
acterizing the configuration of interest. We restrict
our attention to linear models:
?j = wj>?, j ? {1 . . .m}
where wj is a vector of feature weights specific to
predicting at anchor size j, and ? is a vector of size-
independent configuration features, detailed below.
We then perform inference using these models to
predict the BLEU score at each anchor, for the test
case of interest. We finally estimate the parameters
of the learning curve by weighted least squares re-
gression using the anchor predictions.
Anchor sizes can be chosen rather arbitrarily, but
must satisfy the following two constraints:
1. They must be three or more in number in order
to allow fitting the tri-parameter curve.
2. They should be spread as much as possible
along the range of sample size.
For our experiments, we take m = 3, with anchors
at 10K, 75K and 500K segments.
The feature vector? consists of the following fea-
tures:
1. General properties: number and average length
of sentences in the (source) test set.
2. Average length of tokens in the (source) test set
and in the monolingual source language corpus.
3. Lexical diversity features:
(a) type-token ratios for n-grams of order 1 to
5 in the monolingual corpus of both source
and target languages
(b) perplexity of language models of order 2
to 5 derived from the monolingual source
corpus computed on the source side of the
test corpus.
4We specify that it is a random sample as opposed to a subset
deliberately chosen to maximize learning effectiveness. While
there are clear ties between our present work and active learn-
ing, we prefer to keep these two aspects distinct at this stage,
and intend to explore this connection in future work.
25
4. Features capturing divergence between lan-
guages in the pair:
(a) average ratio of source/target sentence
lengths in the test set.
(b) ratio of type-token ratios of orders 1 to 5
in the monolingual corpus of both source
and target languages.
5. Word-order divergence: The divergence in the
word-order between the source and the target
languages can be captured using the part-of-
speech (pos) tag sequences across languages.
We use cross-entropy measure to capture sim-
ilarity between the n-gram distributions of the
pos tags in the monolingual corpora of the two
languages. The order of the n-grams ranges be-
tween n = 2, 4 . . . 12 in order to account for
long distance reordering between languages.
The pos tags for the languages are mapped to
a reduced set of twelve pos tags (Petrov et al,
2012) in order to account for differences in
tagsets used across languages.
These features capture our intuition that translation
is going to be harder if the language in the domain
is highly variable and if the source and target lan-
guages diverge more in terms of morphology and
word-order.
The weights wj are estimated from data. The
training data for fitting these linear models is ob-
tained in the following way. For each configuration
(combination of language pair and domain) c and
test set t in Table 2, a gold curve is fitted using the
selected tri-parameter power-law family using a fine
grid of corpus sizes. This is available as a byproduct
of the experiments for comparing different paramet-
ric families described in Section 3. We then compute
the value of the gold curves at the m anchor sizes:
we thus have m ?gold? vectors ?1, . . . ,?m with ac-
curate estimates of BLEU at the anchor sizes5. We
construct the design matrix ? with one column for
each feature vector ?ct corresponding to each com-
bination of training configuration c and test set t.
We then estimate weights wj using Ridge regres-
sion (L2 regularization):
wj = argmin
w
||?>w ? ?j ||2 + C||w||2 (3)
5Computing these values from the gold curve rather than di-
rectly from the observations has the advantage of smoothing the
observed values and also does not assume that observations at
the anchor sizes are always directly available.
where the regularization parameter C is chosen by
cross-validation. We also run experiments using
Lasso (L1) regularization (Tibshirani, 1994) instead
of Ridge. As baseline, we take a constant mean
model predicting, for each anchor size sj , the av-
erage of all the ?jct.
We do not assume the difficulty of predicting
BLEU at all anchor points to be the same. To allow
for this, we use (non-regularized) weighted least-
squares to fit a curve from our parametric family
through the m anchor points6. Following (Croarkin
and Tobias, 2006, Section 4.4.5.2), the anchor con-
fidence is set to be the inverse of the cross-validated
mean square residuals:
?j =
(
1
N
?
c?S
?
t?Tc
(?>ctw
\c
j ? ?jct)
2
)?1
(4)
where w\cj are the feature weights obtained by the
regression above on all training configurations ex-
cept c, ?jct is the gold value at anchor j for train-
ing/test combination c, t, and N is the total number
of such combinations7. In other words, we assign to
each anchor point a confidence inverse to the cross-
validated mean squared error of the model used to
predict it.
For a new unseen configuration with feature vec-
tor ?u, we determine the parameters ?u of the corre-
sponding learning curve as:
?u = argmin
?
?
j
?j
(
F (sj ; ?)? ?>uwj
)2
(5)
5 Extrapolating a learning curve fitted on
a small parallel corpus
Given a small ?seed? parallel corpus, the translation
system can be used to train small in-domain models
and the evaluation score can be measured at a few
initial sample sizes {(x1, y1), (x2, y2)...(xp, yp)}.
The performance of the system for these initial
points provides evidence for predicting its perfor-
mance for larger sample sizes.
In order to do so, a learning curve from the fam-
ily Pow3 is first fit through these initial points. We
6When the number of anchor points is the same as the num-
ber of parameters in the parametric family, the curve can be fit
exactly through all anchor points. However the general discus-
sion is relevant in case there are more anchor points than pa-
rameters, and also in view of the combination of inference and
extrapolation in Section 6.
7Curves on different test data for the same training configu-
ration are highly correlated and are therefore left out.
26
assume that p ? 3 for this operation to be well-
defined. The best fit ?? is computed using the same
curve fitting as in Eq. 2.
At each individual anchor size sj , the accuracy of
prediction is measured using the root mean-squared
error between the prediction of extrapolated curves
and the gold values:
(
1
N
?
c?S
?
t?Tc
[F (sj ; ??ct)? ?ctj ]
2
)1/2
(6)
where ??ct are the parameters of the curve fit using
the initial points for the combination ct.
In general, we observed that the extrapolated
curve tends to over-estimate BLEU for large sam-
ples.
6 Combining inference and extrapolation
In scenario S2, the models trained from the seed par-
allel corpus and the features used for inference (Sec-
tion 4) provide complementary information. In this
section we combine the two to see if this yields more
accurate learning curves.
For the inference method of Section 4, predictions
of models at anchor points are weighted by the in-
verse of the model empirical squared error (?j). We
extend this approach to the extrapolated curves. Let
u be a new configuration with seed parallel corpus of
size xu, and let xl be the largest point in our grid for
which xl ? xu. We first train translation models and
evaluate scores on samples of size x1, . . . , xl, fit pa-
rameters ??u through the scores, and then extrapolate
BLEU at the anchors sj : F (sj ; ??u), j ? {1, . . . ,m}.
Using the models trained for the experiments in Sec-
tion 3, we estimate the squared extrapolation error at
the anchors sj when using models trained on size up
to xl, and set the confidence in the extrapolations8
for u to its inverse:
?<lj =
(
1
N
?
c?S
?
t?Tc
(F (sj ; ?
<l
ct )? ?ctj)
2
)?1
(7)
where N , S, Tc and ?ctj have the same meaning as
in Eq. 4, and ?<lct are parameters fitted for config-
uration c and test t using only scores measured at
x1, . . . , xl. We finally estimate the parameters ?u of
8In some cases these can actually be interpolations.
the combined curve as:
?u = argmin
?
?
j
?j(F (sj ; ?)? ?
>
uwj)
2
+ ?<lj (F (sj ; ?)? F (sj ; ??u))
2
where ?u is the feature vector for u, and wj are the
weights we obtained from the regression in Eq. 3.
7 Experiments
In this section, we report the results of our experi-
ments on predicting the learning curves.
7.1 Inferred Learning Curves
Regression model 10K 75K 500K
Ridge 0.063 0.060 0.053
Lasso 0.054 0.060 0.062
Baseline 0.112 0.121 0.121
Table 4: Root mean squared error of the linear regression
models for each anchor size
In the case of inference from mostly monolingual
data, the accuracy of the predictions at each of the
anchor sizes is evaluated using root mean-squared
error over the predictions obtained in a leave-one-
out manner over the set of configurations from Ta-
ble 2. Table 4 shows these results for Ridge and
Lasso regression models at the three anchor sizes.
As an example, the model estimated using Lasso for
the 75K anchor size exhibits a root mean squared
error of 6 BLEU points. The errors we obtain are
lower than the error of the baseline consisting in tak-
ing, for each anchor size sj , the average of all the
?ctj . The Lasso regression model selected four fea-
tures from the entire feature set: i) Size of the test
set (sentences & tokens) ii) Perplexity of language
model (order 5) on the test set iii) Type-token ratio
of the target monolingual corpus . Feature correla-
tion measures such as Pearsons R showed that the
features corresponding to type-token ratios of both
source and target languages and size of test set have
a high correlation with the BLEU scores at the three
anchor sizes.
Figure 2 shows an instance of the inferred learn-
ing curves obtained using a weighted least squares
method on the predictions at the anchor sizes. Ta-
ble 7 presents the cumulative error of the inferred
learning curves with respect to the gold curves, mea-
sured as the average distance between the curves in
the range x ? [0.1K, 100K].
27
Figure 2: Inferred learning curve for English-Japanese
test set. The error-bars show the anchor confidence for
the predictions.
7.2 Extrapolated Learning Curves
As explained in Section 5, we evaluate the accuracy
of predictions from the extrapolated curve using the
root mean squared error (see Eq. 6) between the pre-
dictions of this curve and the gold values at the an-
chor points.
We conducted experiments for three sets of initial
points, 1) 1K-5K-10K, 2) 5K-10K-20K, and 3) 1K-
5K-10K-20K. For each of these sets, we show the
prediction accuracy at the anchor sizes, 10K9, 75K,
and 500K in Table 5.
Initial Points 10K 75K 500K
1K-5K-10K 0.005 0.017 0.042
5K-10K-20K 0.002 0.015 0.034
1K-5K-10K-20K 0.002 0.008 0.019
Table 5: Root mean squared error of the extrapolated
curves at the three anchor sizes
The root mean squared errors obtained by extrap-
olating the learning curve are much lower than those
obtained by prediction of translation accuracy using
the monolingual corpus only (see Table 4), which
is expected given that more direct evidence is avail-
able in the former case . In Table 5, one can also
see that the root mean squared error for the sets 1K-
5K-10K and 5K-10K-20K are quite close for anchor
9The 10K point is not an extrapolation point but lies within
the range of the set of initial points. However, it does give a
measure of the closeness of the curve fit using only the initial
points with the gold fit using all the points; the value of this gold
fit at 10K is not necessarily equal to the observation at 10K.
sizes 75K and 500K. However, when a configuration
of four initial points is used for the same amount of
?seed? parallel data, it outperforms both the config-
urations with three initial points.
7.3 Combined Learning Curves and Overall
Comparison
In Section 6, we presented a method for combin-
ing the predicted learning curves from inference and
extrapolation by using a weighted least squares ap-
proach. Table 6 reports the root mean squared error
at the three anchor sizes from the combined curves.
Initial Points Model 10K 75K 500K
1K-5K-10K
Ridge 0.005 0.015 0.038
Lasso 0.005 0.014 0.038
5K-10K-20K
Ridge 0.001 0.006 0.018
Lasso 0.001 0.006 0.018
1K-5K-10K-20K
Ridge 0.001 0.005 0.014
Lasso 0.001 0.005 0.014
Table 6: Root mean squared error of the combined curves
at the three anchor sizes
We also present an overall evaluation of all the
predicted learning curves. The evaluation metric is
the average distance between the predicted curves
and the gold curves, within the range of sample sizes
xmin=0.1K to xmax=500K segments; this metric is
defined as:
1
N
?
c?S
?
t?Tc
?xmax
x=xmin |F (x; ??ct)? F (x; ??ct)|
xmax ? xmin
where ??ct is the curve of interest, ??ct is the gold
curve, and x is in the range [xmin, xmax], with a step
size of 1. Table 7 presents the final evaluation.
Initial Points IR IL EC CR CL
1K-5K-10K 0.034 0.050 0.018 0.015 0.014
5K-10K-20K 0.036 0.048 0.011 0.010 0.009
1K-5K-10K-20K 0.032 0.049 0.008 0.007 0.007
Table 7: Average distance of different predicted
learning curves relative to the gold curve. Columns:
IR=?Inference using Ridge model?, IL=?Inference
using Lasso model?, EC=?Extrapolated curve?,
CR=?Combined curve using Ridge?, CL=?Combined
curve using Lasso?
We see that the combined curves (CR and CL)
perform slightly better than the inferred curves (IR
28
and IL) and the extrapolated curves (EC). The aver-
age distance is on the same scale as the BLEU score,
which suggests that our best curves can predict the
gold curve within 1.5 BLEU points on average (the
best result being 0.7 BLEU points when the initial
points are 1K-5K-10K-20K) which is a telling re-
sult. The distances between the predicted and the
gold curves for all the learning curves in our experi-
ments are shown in Figure 3.
Figure 3: Distances between the predicted and the gold
learning curves in our experiments across the range of
sample sizes. The dotted lines indicate the distance from
gold curve for each instance, while the bold line indi-
cates the 95th quantile of the distance between the curves.
IR=?Inference using Ridge model?, EC=?Extrapolated
curve?, CR=?Combined curve using Ridge?.
We also provide a comparison of the different pre-
dicted curves with respect to the gold curve as shown
in Figure 4.
Figure 4: Predicted curves in the three scenarios for
Czech-English test set using the Lasso model
8 Conclusion
The ability to predict the amount of parallel data
required to achieve a given level of quality is very
valuable in planning business deployments of statis-
tical machine translation; yet, we are not aware of
any rigorous proposal for addressing this need.
Here, we proposed methods that can be directly
applied to predicting learning curves in realistic sce-
narios. We identified a suitable parametric fam-
ily for modeling learning curves via an extensive
empirical comparison. We described an inference
method that requires a minimal initial investment in
the form of only a small parallel test dataset. For the
cases where a slightly larger in-domain ?seed? par-
allel corpus is available, we introduced an extrapola-
tion method and a combined method yielding high-
precision predictions: using models trained on up to
20K sentence pairs we can predict performance on a
given test set with a root mean squared error in the
order of 1 BLEU point at 75K sentence pairs, and
in the order of 2-4 BLEU points at 500K. Consider-
ing that variations in the order of 1 BLEU point on
a same test dataset can be observed simply due to
the instability of the standard MERT parameter tun-
ing algorithm (Foster and Kuhn, 2009; Clark et al,
2011), we believe our results to be close to what can
be achieved in principle. Note that by using gold
curves as labels instead of actual measures we im-
plicitly average across many rounds of MERT (14
for each curve), greatly attenuating the impact of the
instability in the optimization procedure due to ran-
domness.
For enabling this work we trained a multitude
of instances of the same phrase-based SMT sys-
tem on 30 distinct combinations of language-pair
and domain, each with fourteen distinct training
sets of increasing size and tested these instances on
multiple in-domain datasets, generating 96 learning
curves. BLEU measurements for all 96 learning
curves along with the gold curves and feature values
used for inferring the learning curves are available
as additional material to this submission.
We believe that it should be possible to use in-
sights from this paper in an active learning setting,
to select, from an available monolingual source, a
subset of a given size for manual translation, in such
a way at to yield the highest performance, and we
plan to extend our work in this direction.
29
References
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting Success in Machine Translation.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 745?
754, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 Work-
shop on Statistical Machine Translation. In Proceed-
ings of the Sixth Workshop on Statistical Machine
Translation, pages 22?64, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better Hypothesis Testing for Statis-
tical Machine Translation: Controlling for Optimizer
Instability. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 176?181, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Carroll Croarkin and Paul Tobias. 2006.
NIST/SEMATECH e-Handbook of Statistical Meth-
ods. NIST/SEMATECH, July. Available online:
http://www.itl.nist.gov/div898/handbook/.
George Foster and Roland Kuhn. 2009. Stabilizing
Minimum Error Rate Training. In Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 242?249, Athens, Greece, March. Association
for Computational Linguistics.
Baohua Gu, Feifang Hu, and Huan Liu. 2001. Mod-
elling Classification Performance for Large Data Sets.
In Proceedings of the Second International Conference
on Advances in Web-Age Information Management,
WAIM ?01, pages 317?328, London, UK. Springer-
Verlag.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of Human Language Technologies: The 2003 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 48?54,
Edmonton, Canada, May. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics Companion Vol-
ume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of the
10th Machine Translation Summit, Phuket, Thailand,
September.
Jorge J. More?. 1978. The Levenberg-Marquardt Algo-
rithm: Implementation and Theory. Numerical Anal-
ysis. Proceedings Biennial Conference Dundee 1977,
630:105?116.
Graham Neubig. 2011. The Kyoto Free Translation
Task. http://www.phontron.com/kftt.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Claudia Perlich, Foster J. Provost, and Jeffrey S. Si-
monoff. 2003. Tree Induction vs. Logistic Regres-
sion: A Learning-Curve Analysis. Journal of Machine
Learning Research, 4:211?255.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A Universal Part-of-Speech Tagset. In Proceedings
of the Eighth conference on International Language
Resources and Evaluation (LREC?12), Istanbul, May.
European Language Resources Association (ELRA).
Robert Tibshirani. 1994. Regression Shrinkage and Se-
lection Via the Lasso. Journal of the Royal Statistical
Society, Series B, 58:267?288.
Jo?rg Tiedemann. 2009. News from OPUS - A Collection
of Multilingual Parallel Corpora with Tools and Inter-
faces. In Recent Advances in Natural Language Pro-
cessing, volume V, pages 237?248. John Benjamins,
Amsterdam/Philadelphia, Borovets, Bulgaria.
Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008.
Learning Performance of a Machine Translation Sys-
tem: a Statistical and Computational Analysis. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 35?43, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
30
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 85?90,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
SORT: An Interactive Source-Rewriting Tool for Improved Translation
Shachar Mirkin, Sriram Venkatapathy, Marc Dymetman, Ioan Calapodescu
Xerox Research Centre Europe
6 Chemin de Maupertuis
38240 Meylan, France
firstname.lastname@xrce.xerox.com
Abstract
The quality of automatic translation is af-
fected by many factors. One is the diver-
gence between the specific source and tar-
get languages. Another lies in the source
text itself, as some texts are more com-
plex than others. One way to handle such
texts is to modify them prior to transla-
tion. Yet, an important factor that is of-
ten overlooked is the source translatabil-
ity with respect to the specific translation
system and the specific model that are be-
ing used. In this paper we present an in-
teractive system where source modifica-
tions are induced by confidence estimates
that are derived from the translation model
in use. Modifications are automatically
generated and proposed for the user?s ap-
proval. Such a system can reduce post-
editing effort, replacing it by cost-effective
pre-editing that can be done by monolin-
guals.
1 Introduction
While Machine Translation (MT) systems are con-
stantly improving, they are still facing many dif-
ficulties, such as out-of-vocabulary words (i.e.
words unseen at training time), lack of sufficient
in-domain data, ambiguities that the MT model
cannot resolve, and the like. An important source
of problems lies in the source text itself ? some
texts are more complex to translate than others.
Consider the following English-to-French
translation by a popular service, BING TRANS-
LATOR:1 Head of Mali defense seeks more arms
? D?fense de la t?te du Mali cherche bras plus.
There, apart from syntactic problems, both head
and arms have been translated as if they were
1http://www.bing.com/translator, accessed
on 4/4/2013.
body parts (t?te and bras). However, suppose
that we express the same English meaning in the
following way: Chief of Mali defense wants more
weapons. Then BING produces a much better
translation: Chef d??tat-major de la d?fense du
Mali veut plus d?armes.
The fact that the formulation of the source can
strongly influence the quality of the translation has
long been known, and there have been studies in-
dicating that adherence to so-called ?Controlled
Language? guidelines, such as Simplified Techni-
cal English2 can reduce the MT post-edition ef-
fort. However, as one such study (O?Brien, 2006)
notes, it is unfortunately not sufficient to just ?ap-
ply the rules [i.e. guidelines] and press Translate.
We need to analyze the effect that rules are hav-
ing on different language pairs and MT systems,
and we need to tune our rule sets and texts ac-
cordingly?.
In the software system presented here, SORT
(SOurce Rewriting Tool), we build on the basic in-
sight that formulation of the source needs to be
geared to the specific MT model being used, and
propose the following approach. First, we assume
that the original source text in English (say) is not
necessarily under the user?s control, but may be
given to her. While she is a fluent English speaker,
she does not know at all the target language, but
uses an MT system; crucially, this system is able
to provide estimates of the quality of its transla-
tions (Specia et al, 2009). SORT then automati-
cally produces a number of rewritings of each En-
glish sentence, translates them with the MT sys-
tem, and displays to the user those rewritings for
which the translation quality estimates are higher
than the estimate for the original source. The user
then interactively selects one such rewriting per
sentence, checking that it does not distort the orig-
inal meaning, and finally the translations of these
2http://www.asd-ste100.org
85
reformulations are made available.
One advantage of this framework is that the
proposed rewritings are implicitly ?aware? of the
underlying strengths and limitations of the spe-
cific MT model. A good quality estimation3
component, for instance, will feel more confident
about the translation of an unambiguous word like
weapon than about that of an ambiguous one such
as arm, or about the translation of a known term
in its domain than about a term not seen during
training.
Such a tool is especially relevant for business
situations where post-edition costs are very high,
for instance because of lack of people both ex-
pert in the domain and competent in the target lan-
guage. Post-edition must be reserved for the most
difficult cases, while pre-edition may be easier to
organize. While the setup cannot fully guarantee
the accuracy of all translations, it can reduce the
number of sentences that need to go through post-
edition and the overall cost of this task.
2 The rewriting tool
In this section we describe SORT, our implemen-
tation of the aforementioned rewriting approach.
While the entire process can in principle be fully
automated, we focus here on an interactive pro-
cess where the user views and approves suggested
rewritings. The details of the rewriting methods
and of the quality estimation used in the current
implementation are described in Sections 3 and 4.
Figure 1 presents the system?s interface, which
is accessed as a web application. With this in-
terface, the user uploads the document that needs
to be translated. The translation confidence of
each sentence is computed and displayed next to
it. The confidence scores are color-coded to en-
able quickly focusing on the sentences that require
more attention. Green denotes sentences for which
the translation confidence is high, and are thus ex-
pected to produce good translations. Red marks
sentences that are estimated to be poorly trans-
lated, and all those in between are marked with
an orange label.
We attempt to suggest rewritings only for sen-
tences that are estimated to be not so well trans-
lated. When we are able to propose rewriting(s)
with higher translation confidence than the origi-
nal, a magnifying glass icon is displayed next to the
sentence. Clicking it displays, on the right side of
3Also known as confidence estimation.
the screen, an ordered list of the more confident
rewritings, along with their corresponding confi-
dence estimations. The first sentence on the list
is always the original one, to let it be edited, and
to make it easier to view the difference between
the original and the rewritings. An example is
shown on the right side of Figure 1, where we see
a rewriting suggestion for the fourth sentence in
the document. Here, the suggestion is simply to
replace the word captured with the word caught, a
rewriting that is estimated to improve the transla-
tion of the sentence.
The user can select one of the suggestions or
choose to edit either the original or one of the
rewritings. The current sentence which is being
examined is marked with a different color and the
alternative under focus is marked with a small icon
(the bidirectional arrows). The differences between
the alternatives and the original are highlighted.
After the user?s confirmation (with the check mark
icon), the display of the document on the left-hand
side is updated based on her selection, including
the updated confidence estimation. At any time,
the user (if she speaks the target language) can
click on the cogwheel icon and view the transla-
tion of the source or of its rewritten version. When
done, the user can save the edited text or its trans-
lation. Moses Release 1.0 of an English-Spanish
Europarl-trained model4 was used in this work to
obtain English-Spanish translations.
2.1 System and software architecture
SORT is implemented as a web application, using
an MVC (Model View Controller) software archi-
tecture. The Model part is formed by Java classes
representing the application state (user input, se-
lected text lines, associated rewriting propositions
and scores). The Controller consists of several
servlet components handling each user interaction
with the backend server (file uploads, SMT tools
calls via XML-RPC or use of the embedded Java
library that handles the actual rewritings). Finally,
the View is built with standard web technologies:
HTML5, JavaScript (AJAX) and CSS style sheets.
The application was developed and deployed on
Linux (CentOS release 6.4), with a Java Runtime
6 (Java HotSpot 64-Bit Server VM), within a Tom-
cat 7.0 Application Server, and tested with Firefox
as the web client both on Linux and Windows 7.
Figure 2 shows the system architecture of SORT,
4http://www.statmt.org/moses/RELEASE-1.0/model/
86
Figure 1: SORT?s interface
Figure 2: SORT?s system architecture. For simplicity, only
partial input-output details are shown.
with some details of the current implementation.
The entire process is performed via a client-server
architecture in order to provide responsiveness, as
required in an interactive system. The user com-
municates with the system through the interface
shown in Figure 1. When a document is loaded,
its sentences are translated in parallel by an SMT
Moses server (Koehn et al, 2007). Then, the
source and the target are sent to the confidence es-
timator, and the translation model information is
also made available to it. The confidence estima-
tor extracts features from that input and returns a
confidence score. Specifically, the language model
features are computed with two SRILM servers
(Stolcke, 2002), one for the source language and
one for the target language. Rewritings are pro-
duced by the rewriting modules (see Section 3 for
the implemented rewriting methods). For each
rewriting, the same process of translation and con-
fidence estimation is performed. Translations are
cached during the session; thus, when the user
wishes to view a translation or download the trans-
lations of the entire document, the response is im-
mediate.
3 Source rewriting
Various methods can be used to rewrite a source
text. In what follows we describe two rewriting
methods, based on Text Simplification techniques,
which we implemented and integrated in the cur-
rent version of SORT. Simplification operations
include the replacement of words by simpler ones,
removal of complicated syntactic structures, short-
ening of sentences etc. (Feng, 2008). Our assump-
tion is that simpler sentences are more likely to
yield higher quality translations. Clearly, this is
not always the case; yet, we leave this decision to
the confidence estimation component.
Sentence-level simplification (Specia, 2010)
has proposed to model text simplification as a Sta-
tistical Machine Translation (SMT) task where the
goal is to translate sentences to their simplified
version in the same language. In this approach, a
simplification model is learnt from a parallel cor-
pus of texts and their simplified versions. Apply-
87
ing this method, we train an SMT model from En-
glish to Simple English, based on the PWKP par-
allel corpus generated from Wikipedia (Zhu et al,
2010);5 we use only alignments involving a single
sentence on each side. This results in a phrase ta-
ble containing many entries where source and tar-
get phrases are identical, but also phrase-pairs that
are mapping complex phrases to their simplified
counterparts, such as the following:
? due to its location on? because it was on
? primarily dry and secondarily cold ? both
cold and dry
? the high mountainous alps? the alps
Also, the language model is trained with Simple
English sentences to encourage the generation of
simpler texts. Given a source text, it is translated
to its simpler version, and its n-best translations
are assessed by the confidence estimation compo-
nent.
Lexical simplification One of the primary oper-
ations for text-simplification is lexical substitution
(Table 2 in (Specia, 2010)). Hence, in addition to
rewriting a full sentence using the previous tech-
nique, we implemented a second method, address-
ing lexical simplification directly, and only modi-
fying local aspects of the source sentence. The ap-
proach here is to extract relevant synonyms from
our trained SMT model of English to Simplified
English, and use them as substitutions to simplify
new sentences. We extract all single token map-
pings from the phrase table of the trained model,
removing punctuations, numbers and stop-words.
We check whether their lemmas were synonyms
in WordNet (Fellbaum, 1998) (with all possible
parts-of-speech as this information was not avail-
able in the SMT model). Only those are left as
valid substitution pairs. When a match of an En-
glish word is found in the source sentence it is re-
placed with its simpler synonym to generate an al-
ternative for the source. For example, using this
rewriting method for the source sentence ?Why the
Galileo research program superseded rival pro-
grams,? three rewritings of the sentence are gen-
erated when rival is substituted by competitor or
superseded by replaced, and when both substitu-
tions occur together.
5Downloaded from:
http://www.ukp.tu-darmstadt.de/data/
sentence-simplification
In the current version of SORT, both sentence-
level and lexical simplification methods are used
in conjunction to suggest rewritings for sentences
with low confidence scores.
4 Confidence estimation
Our confidence estimator is based on the system
and data provided for the 2012 Quality estima-
tion shared task (Callison-Burch et al, 2012). In
this task, participants were required to estimate the
quality of automated translations. Their estimates
were compared to human scores of the translation
which referred to the suitability of the translation
for post-editing. The scores ranged from 1 to 5,
where 1 corresponded to translation that practi-
cally needs to be done from scratch, and 5 to trans-
lations that requires little to no editing.
The task?s training set consisted of approxi-
mately 1800 source sentences in English, their
Moses translations to Spanish and the scores given
to the translations by the three judges. With this
data we trained an SVM regression model using
SVMlight (Joachims, 1999). Features were ex-
tracted with the task?s feature-extraction baseline
module. Two types of features are used in this
module (i) black-box features, which do not as-
sume access to the translation system, such as
the length of the source and the target, number
of punctuation marks and language model prob-
abilities, and (ii) glass-box features, which are ex-
tracted from the translation model, such as the
average number of translations per source word
(Specia et al, 2009).
5 Initial evaluation and analysis
We performed an initial evaluation of our ap-
proach in an English to Spanish translation setting,
using the 2008 News Commentary data.6 First,
two annotators who speak English but not Spanish
used SORT to rewrite an English text. They re-
viewed the proposed rewritings for 960 sentences
and were instructed to ?trust the judgment? of the
confidence estimator; that is, reviewing the sug-
gestions from the most to the least confident one,
they accepted the first rewriting that was fluent and
preserved the meaning of the source document as
a whole. 440 pairs of the original sentence and
the selected alternative were then both translated
to Spanish and were presented as competitors to
6Available at http://www.statmt.org
88
three native Spanish speakers. The sentences were
placed within their context in the original docu-
ment, taken from the Spanish side of the corpus.
The order of presentation of the two competitors
was random. In this evaluation, the translation of
the original was preferred 20.6% of the cases, the
rewriting 30.4% of them, and for 49% of the sen-
tences, no clear winner was chosen.7 Among the
two rewriting methods, the sentence-level method
more often resulted in preferred translations.
These results suggest that rewriting is esti-
mated to improve translation quality. However,
the amount of preferred original translations indi-
cates that the confidence estimator is not always
discriminative enough: by construction, for every
rewriting that is displayed, the confidence compo-
nent estimates the translation of the original to be
less accurate than that of the rewriting; yet, this is
not always reflected in the preferences of the eval-
uators. On a different dimension than translation
quality, the large number of cases with no clear
winner, and the analysis we conducted, indicate
that the user?s cognitive effort would be decreased
if we only displayed those rewritings associated
with a substantial improvement in confidence; due
to the nature of our methods, frequently, identi-
cal or near-identical translations were generated,
with only marginal differences in confidence, e.g.,
when two source synonyms were translated to the
same target word. Also, often a wrong synonym
was suggested as a replacement for a word (e.g.
Christmas air for Christmas atmosphere). This
was somewhat surprising as we had expected the
language model features of the confidence estima-
tor to help removing these cases. While they were
filtered by the English-speaking users, and thus
did not present a problem for translation, they cre-
ated unnecessary workload. Putting more empha-
sis on context features in the confidence estimation
or explicitly verifying context-suitability of a lex-
ical substitutions could help addressing this issue.
6 Related work
Some related approaches focus on the authoring
process and control a priori the range of possible
texts, either by interactively enforcing lexical and
syntactic constraints on the source that simplify
the operations of a rule-based translation system
(Carbonell et al, 1997), or by semantically guid-
7One should consider these figures with caution, as the
numbers may be too small to be statistically meaningful.
ing a monolingual author in the generation of mul-
tilingual texts (Power and Scott, 1998; Dymetman
et al, 2000). A recent approach (Venkatapathy
and Mirkin, 2012) proposes an authoring tool that
consults the MT system itself to propose phrases
that should be used during composition to obtain
better translations. All these methods address the
authoring of the source text from scratch. This
is inherently different from the objective of our
work where an existing text is modified to improve
its translatability. Moving away from authoring
approaches, (Choumane et al, 2005) propose an
interactive system where the author helps a rule-
based translation system disambiguate a source
text inside a structured document editor. The
techniques are generic and are not automatically
adapted to a specific MT system or model. Closer
to our approach of modifying the source text, one
approach is to paraphrase the source or to gener-
ate sentences entailed by it (Callison-Burch et al,
2006; Mirkin et al, 2009; Marton et al, 2009;
Aziz et al, 2010). These works, however, fo-
cus on handling out-of-vocabulary (OOV) words,
do not assess the translatability of the source sen-
tence and are not interactive.8 The MonoTrans2
project (Hu et al, 2011) proposes monolingual-
based editing for translation. Monolingual speak-
ers of the source and target language collaborate
to improve the translation. Unlike our approach,
here both the feedback for poorly translated sen-
tences and the actual modification of the source
is done by humans. This contrasts with the auto-
matic handling (albeit less accurate) of both these
tasks in our work.
7 Conclusions and future work
We introduced a system for rewriting texts for
translation under the control of a confidence esti-
mator. While we focused on an interactive mode,
where a monolingual user is asked to check the
quality of the source reformulations, in an exten-
sion of this approach, the quality of the reformu-
lations could also be assessed automatically, re-
moving the interactive aspects at the cost of an in-
creased risk of rewriting errors. For future work
we wish to add more powerful rewriting tech-
niques that are able to explore a larger space of
possible reformulations, but compensate this ex-
8Another way to use paraphrases for improved translation
has been proposed by (Max, 2010) who uses paraphrasing of
the source text to increase the number of training examples
for the SMT system.
89
panded space by robust filtering methods. Based
on an evaluation of the quality of the generated al-
ternatives as well as on user selection decisions,
we may be able to learn a quality estimator for
the rewriting operations themselves. Such meth-
ods could be useful both in an interactive mode,
to minimize the effort of the monolingual source
user, as well as in an automatic mode, to avoid
misinterpretation. In this work we used an avail-
able baseline feature extraction module for confi-
dence estimation. A better estimator could bene-
fit our system significantly, as we argued above.
Lastly, we wish to further improve the user inter-
face of the tool, based on feedback from actual
users.
References
[Aziz et al2010] Wilker Aziz, Marc Dymetman,
Shachar Mirkin, Lucia Specia, Nicola Cancedda,
and Ido Dagan. 2010. Learning an expert from
human annotations in statistical machine translation:
the case of out-of-vocabularywords. In Proceedings
of EAMT.
[Callison-Burch et al2006] Chris Callison-Burch,
Philipp Koehn, and Miles Osborne. 2006. Improved
statistical machine translation using paraphrases. In
Proceedings of HLT-NAACL.
[Callison-Burch et al2012] Chris Callison-Burch,
Philipp Koehn, Christof Monz, Matt Post, Radu
Soricut, and Lucia Specia. 2012. Findings of the
2012 workshop on statistical machine translation.
In Proceedings of WMT.
[Carbonell et al1997] Jaime G Carbonell, Sharlene L
Gallup, Timothy J Harris, James W Higdon, Den-
nis A Hill, David C Hudson, David Nasjleti,
Mervin L Rennich, Peggy M Andersen, Michael M
Bauer, et al 1997. Integrated authoring and transla-
tion system. US Patent 5,677,835.
[Choumane et al2005] Ali Choumane, Herv? Blan-
chon, and C?cile Roisin. 2005. Integrating transla-
tion services within a structured editor. In Proceed-
ings of the ACM symposium on Document engineer-
ing. ACM.
[Dymetman et al2000] Marc Dymetman, Veronika
Lux, and Aarne Ranta. 2000. Xml and multilin-
gual document authoring: Convergent trends. In
Proceedings of COLING.
[Fellbaum1998] Christiane Fellbaum, editor. 1998.
WordNet: An Electronic Lexical Database (Lan-
guage, Speech, and Communication). The MIT
Press.
[Feng2008] Lijun Feng. 2008. Text simplification: A
survey. Technical report, CUNY.
[Hu et al2011] Chang Hu, Philip Resnik, Yakov Kro-
nrod, Vladimir Eidelman, Olivia Buzek, and Ben-
jamin B. Bederson. 2011. The value of monolingual
crowdsourcing in a real-world translation scenario:
simulation using haitian creole emergency sms mes-
sages. In Proceedings of WMT.
[Joachims1999] T. Joachims. 1999. Making large-
scale SVM learning practical. In B. Sch?lkopf,
C. Burges, and A. Smola, editors, Advances in Ker-
nel Methods - Support Vector Learning, chapter 11,
pages 169?184. MIT Press.
[Koehn et al2007] Philipp Koehn, Hieu Hoang,
Alexandra Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan, Wade
Shen, Christine Moran, Richard Zens, Chris Dyer,
Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of
ACL, Demo and Poster Sessions.
[Marton et al2009] Yuval Marton, Chris Callison-
Burch, and Philip Resnik. 2009. Improved sta-
tistical machine translation using monolingually-
derived paraphrases. In Proceedings of EMNLP.
[Max2010] Aur?lien Max. 2010. Example-based para-
phrasing for improved phrase-based statistical ma-
chine translation. In Proceedings of EMNLP.
[Mirkin et al2009] Shachar Mirkin, Lucia Specia,
Nicola Cancedda, Ido Dagan, Marc Dymetman, and
Idan Szpektor. 2009. Source-language entailment
modeling for translating unknown terms. In Pro-
ceedings of ACL-IJCNLP.
[O?Brien2006] Sharon O?Brien. 2006. Controlled Lan-
guage and Post-Editing. Multilingual, 17(7):17?19.
[Power and Scott1998] Richard Power and Donia Scott.
1998. Multilingual authoring using feedback texts.
In Proceedings of ACL.
[Specia et al2009] Lucia Specia, Nicola Cancedda,
Marc Dymetman, Marco Turchi, and Nello Cristian-
ini. 2009. Estimating the sentence-level quality
of machine translation systems. In Proceedings of
EAMT.
[Specia2010] Lucia Specia. 2010. Translating from
complex to simplified sentences. In Proceedings of
PROPOR.
[Stolcke2002] Andreas Stolcke. 2002. SRILM - an
extensible language modeling toolkit. In INTER-
SPEECH.
[Venkatapathy and Mirkin2012] Sriram Venkatapathy
and Shachar Mirkin. 2012. An SMT-driven
authoring tool. In Proceedings of COLING 2012:
Demonstration Papers.
[Zhu et al2010] Zhemin Zhu, Delphine Bernhard, and
Iryna Gurevych. 2010. A monolingual tree-based
translation model for sentence simplification. In
Proceedings of COLING.
90
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 34?42,
COLING 2010, Beijing, August 2010.
Phrase Based Decoding using a Discriminative Model
Prasanth Kolachina
LTRC, IIIT-Hyderabad
{prasanth k}@research.iiit.ac.in
Sriram Venkatapathy
LTRC, IIIT-Hyderabad
{sriram}@research.iiit.ac.in
Srinivas Bangalore
AT&T Labs-Research, NY
{srini}@research.att.com
Sudheer Kolachina
LTRC, IIIT-Hyderabad
{sudheer.kpg08}@research.iiit.ac.in
Avinesh PVS
LTRC, IIIT-Hyderabad
{avinesh}@research.iiit.ac.in
Abstract
In this paper, we present an approach to
statistical machine translation that com-
bines the power of a discriminative model
(for training a model for Machine Transla-
tion), and the standard beam-search based
decoding technique (for the translation of
an input sentence). A discriminative ap-
proach for learning lexical selection and
reordering utilizes a large set of feature
functions (thereby providing the power to
incorporate greater contextual and linguis-
tic information), which leads to an effec-
tive training of these models. This model
is then used by the standard state-of-art
Moses decoder (Koehn et al, 2007) for the
translation of an input sentence.
We conducted our experiments on
Spanish-English language pair. We used
maximum entropy model in our exper-
iments. We show that the performance
of our approach (using simple lexical
features) is comparable to that of the
state-of-art statistical MT system (Koehn
et al, 2007). When additional syntactic
features (POS tags in this paper) are used,
there is a boost in the performance which
is likely to improve when richer syntactic
features are incorporated in the model.
1 Introduction
The popular approaches to machine translation
use the generative IBM models for training
(Brown et al, 1993; Och et al, 1999). The param-
eters for these models are learnt using the stan-
dard EM Algorithm. The parameters used in these
models are extremely restrictive, that is, a simple,
small and closed set of feature functions is used
to represent the translation process. Also, these
feature functions are local and are word based. In
spite of these limitations, these models perform
very well for the task of word-alignment because
of the restricted search space. However, they per-
form poorly during decoding (or translation) be-
cause of their limitations in the context of a much
larger search space.
To handle the contextual information, phrase-
based models were introduced (Koehn et al,
2003). The phrase-based models use the word
alignment information from the IBM models and
train source-target phrase pairs for lexical se-
lection (phrase-table) and distortions of source
phrases (reordering-table). These models are still
relatively local, as the target phrases are tightly as-
sociated with their corresponding source phrases.
In contrast to a phrase-based model, a discrim-
inative model has the power to integrate much
richer contextual information into the training
model. Contextual information is extremely use-
ful in making lexical selections of higher quality,
as illustrated by the models for Global Lexical Se-
lection (Bangalore et al, 2007; Venkatapathy and
34
Bangalore, 2009).
However, the limitation of global lexical se-
lection models has been sentence construction.
In global lexical selection models, lattice con-
struction and scoring (LCS) is used for the pur-
pose of sentence construction (Bangalore et al,
2007; Venkatapathy and Bangalore, 2009). In our
work, we address this limitation of global lexi-
cal selection models by using an existing state-of-
art decoder (Koehn et al, 2007) for the purpose
of sentence construction. The translation model
used by this decoder is derived from a discrimina-
tive model, instead of the usual phrase-table and
reordering-table construction algorithms. This al-
lows us to use the effectiveness of an existing
phrase-based decoder while retaining the advan-
tages of the discriminative model. In this paper,
we compare the sentence construction accuracies
of lattice construction and scoring approach (see
section 4.1 for LCS Decoding) and the phrase-
based decoding approach (see section 4.2).
Another advantage of using a discriminative ap-
proach to construct the phrase table and the re-
ordering table is the flexibility it provides to in-
corporate linguistic knowledge in the form of ad-
ditional feature functions. In the past, factored
phrase-based approaches for Machine Translation
have allowed the use of linguistic feature func-
tions. But, they are still bound by the local-
ity of context, and definition of a fixed struc-
ture of dependencies between the factors (Koehn
and Hoang, 2007). Furthermore, factored phrase-
based approaches place constraints both on the
type and number of factors that can be incorpo-
rated into the training. In this paper, though we do
not extensively test this aspect, we show that us-
ing syntactic feature functions does improve the
performance of our approach, which is likely to
improve when much richer syntactic feature func-
tions (such as information about the parse struc-
ture) are incorporated in the model.
As the training model in a standard phrase-
based system is relatively impoverished with re-
spect to contextual/linguistic information, integra-
tion of the discriminative model in the form of
phrase-table and reordering-table with the phrase-
based decoder is highly desirable. We propose to
do this by defining sentence specific tables. For
example, given a source sentence s, the phrase-
table contains all the possible phrase-pairs condi-
tioned on the context of the source sentence s.
In this paper, the key contributions are,
1. We combine a discriminative training model
with a phrase-based decoder. We ob-
tained comparable results with the state-of-
art phrase-based decoder.
2. We evaluate the performance of the lattice
construction and scoring (LCS) approach to
decoding. We observed that even though the
lexical accuracy obtained using LCS is high,
the performance in terms of sentence con-
struction is low when compared to phrase-
based decoder.
3. We show that the incorporation of syntactic
information (POS tags) in our discriminative
model boosts the performance of translation.
In future, we plan to use richer syntactic fea-
ture functions (which the discriminative ap-
proach allows us to incorporate) to evaluate
the approach.
The paper is organized in the following sec-
tions. Section 2 presents the related work. In
section 3, we describe the training of our model.
In section 4, we present the decoding approaches
(both LCS and phrase-based decoder). We de-
scribe the data used in our experiments in section
5. Section 6 consists of the experiments and re-
sults. Finally we conclude the paper in section 7.
2 Related Work
In this section, we present approaches that are di-
rectly related to our approach. In Direct Trans-
lation Model (DTM) proposed for statistical ma-
chine translation by (Papineni et al, 1998; Och
and Ney, 2002), the authors present a discrimi-
native set-up for natural language understanding
(and MT). They use a slightly modified equation
(in comparison to IBM models) as shown in equa-
tion 1. In equation 1, they consider the translation
model from f ? e (p(e|f)), instead of the the-
oretically sound (after the application of Bayes?
rule), e ? f (p(f |e)) and use grammatical fea-
tures such as the presence of equal number of
35
verbs forms etc.
e? = argmax
e
pTM (e|f) ? pLM (e) (1)
In their model, they use generic feature func-
tions such as language model, cooccurence fea-
tures such as presence of a lexical relationship in
the lexicon. Their search algorithm limited the use
of complex features.
Direct Translation Model 2 (DTM2) (Itty-
cheriah and Roukos, 2007) expresses the phrase-
based translation task in a unified log-linear prob-
abilistic framework consisting of three compo-
nents:
1. a prior conditional distribution P0
2. a number of feature functions ?i() that cap-
ture the effects of translation and language
model
3. the weights of the features ?i that are esti-
mated using MaxEnt training (Berger et al,
1996) as shown in equation 2.
Pr(e|f) = P0(e, j|f)Z exp
?
i
?i?i(e, j, f) (2)
In the above equation, j is the skip reordering
factor for the phrase pair captured by?i() and rep-
resents the jump from the previous source word.
Z represents the per source sentence normaliza-
tion term (Hassan et al, 2009). While a uni-
form prior on the set of futures results in a max-
imum entropy model, choosing other priors out-
put a minimum divergence models. Normalized
phrase count has been used as the prior P0 in the
DTM2 model.
The following decision rule is used to obtain opti-
mal translation.
e? = argmax
e
Pr(e|f)
= argmax
e
M?
m=1
?m?m(f, e)
(3)
The DTM2 model differs from other phrase-
based SMT models in that it avoids the redun-
dancy present in other systems by extracting from
a word aligned parallel corpora a set of minimal
phrases such that no two phrases overlap with
each other (Hassan et al, 2009).
The decoding strategy in DTM2 (Ittycheriah
and Roukos, 2007) is similar to a phrase-based de-
coder except that the score of a particular transla-
tion block is obtained from the maximum entropy
model using the set of feature functions. In our
approach, instead of providing the complete scor-
ing function ourselves, we compute the parame-
ters needed by a phrase based decoder, which in
turn uses these parameters appropriately. In com-
parison with the DTM2, we also use minimal non-
overlapping blocks as the entries in the phrase ta-
ble that we generate.
Xiong et al (2006) present a phrase reordering
model under the ITG constraint using a maximum
entropy framework. They model the reordering
problem as a two-class classification problem, the
classes being straight and inverted. The model is
used to merge the phrases obtained from trans-
lating the segments in a source sentence. The
decoder used is a hierarchical decoder motivated
from the CYK parsing algorithm employing a
beam search algorithm. The maximum entropy
model is presented with features extracted from
the blocks being merged and probabilities are es-
timated using the log-linear equation shown in
(4). The work in addition to lexical features and
collocational features, uses an additional metric
called the information gain ratio (IGR) as a fea-
ture. The authors report an improvement of 4%
BLEU score over the traditional distance based
distortion model upon using the lexical features
alone.
p?(y|x) =
1
Z?(x)
exp(
?
i
?i?i(x, y)) (4)
3 Training
The training process of our approach has two
steps:
1. training the discriminative models for trans-
lation and reordering.
2. integrating the models into a phrase based
decoder.
36
The input to our training step are the word-
alignments between source and target sentences
obtained using GIZA++ (implementation of IBM,
HMM models).
3.1 Training discriminative models
We train two models, one to model the transla-
tion of source blocks, and the other to model the
reordering of source blocks. We call the transla-
tion model a ?context dependent block translation
model? for two reasons.
1. It is concerned with the translation of mini-
mal phrasal units called blocks.
2. The context of the source block is used dur-
ing its translation.
The word alignments are used to obtain the set
of possible target blocks, and are added to the tar-
get vocabulary. A target block b is a sequence of n
words that are paired with a sequence ofm source
words (Ittycheriah and Roukos, 2007). In our ap-
proach, we restrict ourselves to target blocks that
are associated with only one source word. How-
ever, this constraint can be easily relaxed.
Similarly, we call the reordering model, a ?con-
text dependent block distortion model?. For train-
ing, we use the maximum entropy software library
Llama presented in (Haffner, 2006).
3.1.1 Context Dependent Block Translation
Model
In this model, the goal is to predict a target
block given the source word and contextual and
syntactic information. Given a source word and its
lexical context, the model estimates the probabil-
ities of the presence or absence of possible target
blocks (see Figure 1).
The probabilities of the candidate target blocks
are obtained from the maximum entropy model.
The probability pei of a candidate target block ei
is estimated as given in equation 5
pei = P (true|ei, fj , C) (5)
where fj is the source word corresponding to ei
and C is its context.
Using the maximum entropy model, binary
classifiers are trained for every target block in the
context window
source word
word syntactically dependent
SOURCE SENTENCE
target word 1 prob p1
............
target word 2 prob p2
prob pKtarget word K
on source word
Figure 1: Word prediction model
vocabulary. These classifiers predict if a particu-
lar target block should be present given the source
word and its context. This model is similar to the
global lexical selection (GLS) model described in
(Bangalore et al, 2007; Venkatapathy and Banga-
lore, 2009) except that in GLS, the predicted tar-
get blocks are not associated with any particular
source word unlike the case here.
For the set of experiments in this paper, we used
a context of size 6, containing three words to the
left and three words to the right. We also used
the POS tags of words in the context window as
features. In future, we plan to use the words syn-
tactically dependent on a source word as global
context(shown in Figure 1).
3.1.2 Context Dependent Block Distortion
Model
An IBM model 3 like distortion model is
trained to predict the relative position of a source
word in the target given its context. Given a
source word and its context, the model estimates
the probability of particular relative position be-
ing an appropriate position of the source word in
the target (see Figure 2).
context window
source wordSOURCE SENTENCE
0p0
1p1 2p2 wpw?1p?1?2p?2?wp?w ...
...
word syntactically dependent
on source word
Figure 2: Position prediction model
Using a maximum entropy model similar to
37
the one described in the context dependent block
translation model, binary classifiers are trained
for every possible relative position in the target.
These classifiers output a probability distribution
over various relative positions given a source word
and its context.
The word alignments in the training corpus are
used to train the distortion model. While comput-
ing the relative position, the difference in sentence
lengths is also taken into account. Hence, the rela-
tive position of the target block located at position
i corresponding to the source word located at po-
sition j is given in equation 6.
r = round(i ? mn ? j) (6)
where, m is the length of source sentence and n is
the number of target blocks. round is the function
to compute the nearest integer of the argument. If
the source word is not aligned to any target word,
a special symbol ?INF? is used to indicate such a
case. In our model, this symbol is also a part of
the target distribution.
The features used to train this model are the
same as those used for the block translation
model. In order to use further lexical information,
we also incorporated information about the target
word for predicting the distribution. The informa-
tion about possible target words is obtained from
the ?context dependent block translation model?.
The probabilities in this case are measured as
shown in equation 7
pr,ei = P (true|r, ei, fj , C) (7)
3.2 Integration with phrase-based decoder
The discriminative models trained are sentence
specific, i.e. the context of the sentence is used
to make predictions in these models. Hence,
the phrase-based decoder is required to use in-
formation specific to a source sentence. In order
to handle this issue, a different phrase-table and
reordering-table are constructed for every input
sentence. The phrase-table and reordering-table
are constructed using the discriminative models
trained earlier.
In Moses (Koehn et al, 2007), the phrase-
table contains the source phrase, the target phrase
and the various scores associated with the phrase
pair such as phrase translation probability, lexical
weighting, inverse phrase translation probability,
etc.1
In our approach, given a source sentence, the
following steps are followed to construct the
phrase table.
1. Extract source blocks (?words? in this work)
2. Use the ?context dependent block translation
model? to predict the possible target blocks.
The set of possible blocks can be predicted
using two criteria, (1) Probability threshold,
and (2) K-best. Here, we use a threshold
value to prune the set of possible candidates
in the target vocabulary.
3. Use the prediction probabilities to assign
scores to the phrase pairs.
A similar set of steps is used to construct the
reordering-table corresponding to an input sen-
tence in the source language.
4 Decoding
4.1 Decoding with LCS Decoder
The lattice construction and scoring algorithm, as
the name suggests, consists of two steps,
1. Lattice construction
In this step, a lattice representing various
possible target sequences is obtained. In the
approach for global lexical selection (Banga-
lore et al, 2007; Venkatapathy and Banga-
lore, 2009), the input to this step is a bag of
words. The bag of words is used to construct
an initial sequence (a single path lattice). To
this sequence, deletion arcs are added to in-
corporate additional paths (at a cost) that fa-
cilitate deletion of words in the initial se-
quence. This sequence is permuted using a
permutation window in order to construct a
lattice representing possible sequences. The
permutation window is used to control the
search space.
In our experiments, we used a similar process
for sentence construction. Using the con-
text dependent block translation algorithm,
1http://www.statmt.org/moses/?n=FactoredTraining.ScorePhrases
38
we obtain a number of translation blocks for
every source word. These blocks are inter-
connected in order to obtain the initial lattice
(see figure 3).
f_(i?1) f_(i) f_(i+1)
t_(i?1,1)
t_(i?1,2)
t_(i?1,3)
t_(i,2)
t_(i,1) t_(i+1,1)
t_(i+1,2)
t_(i+1,3)
.... ...............
SOURCE SENTENCE
INTIAL TARGET LATTICE
Figure 3: Lattice Construction
To control deletions at various source posi-
tions, deletion nodes may be added to the
initial lattice. This lattice is permuted us-
ing a permutation window to construct a lat-
tice representing possible sequences. Hence,
the parameters that dictate lattice construc-
tion are, (1) Threshold for lexical selection,
(2) Using deletion arcs or not, and (3) Per-
mutation window.
2. Scoring
In this step, each of the paths in the lattice
constructed in the earlier step is scored us-
ing a language model (Haffner, 2006), which
is same as the one used in the sentence con-
struction in global lexical selection models.
It is to be noted that we do not use the dis-
criminative reordering model in this decoder,
and only the language model is used to score
various target sequences.
The path with the lowest score is considered
the best possible target sentence for the given
source sentence. Using this decoder, we con-
ducted experiments on the development set by
varying threshold values and the size of the per-
mutation window. The best parameter values ob-
tained using the development set were used for de-
coding the test corpus.
4.2 Decoding with Moses Decoder
In this approach, the phrase-table and the
reordering-table are constructed using the dis-
criminative model for every source sentence (see
section 3.2). These tables are then used by the
state-of-art Moses decoder to obtain correspond-
ing translations.
The various training and decoding parameters
of the discriminative model are computed by ex-
haustively exploring the parameter space, and cor-
respondingly measuring the output quality on the
development set. The best set of parameters were
used for decoding the sentences in the test corpus.
We modified the weights assigned by MOSES to
the translation model, reordering model and lan-
guage model. Experiments were conducted by
performing pruning on the options in the phrase
table and by using the word penalty feature in
MOSES.
We trained a language model of order 5 built on
the entire EUROPARL corpus using the SRILM
package. The method uses improved Kneser-Ney
smoothing algorithm (Chen and Goodman, 1999)
to compute sequence probabilities.
5 Dataset
The experiments were conducted on the Spanish-
English language pair. The latest version of the
Europarl corpus(version-5) was used in this work.
A small set of 200K sentences was selected from
the training set to conduct the experiments. The
test and development sets containing 2525 sen-
tences and 2051 sentences respectively were used,
without making any changes.
Corpus No. of sentences Source Target
Training 200000 59591 36886
Testing 2525 10629 8905
Development 2051 8888 7750
Monolingual 200000 n.a 36886
English (LM)
Table 1: Corpus statistics for Spanish-English cor-
pus.
6 Experiments and Results
The output of our experiments was evaluated us-
ing two metrics, (1) BLEU (Papineni et al, 2002),
and (2) Lexical Accuracy (LexAcc). Lexical ac-
curacy measures the similarity between the un-
ordered bag of words in the reference sentence
39
against the unordered bag of words in the hypoth-
esized translation. Lexical accuracy is a measure
of the fidelity of lexical transfer from the source
to the target sentence, independent of the syntax
of the target language (Venkatapathy and Banga-
lore, 2009). We report lexical accuracies to show
the performance of LCS decoding in comparison
with the baseline system.
We first present the results of the state-of-art
phrase-based model (Moses) trained on a paral-
lel corpus. We treat this as our baseline. The re-
ordering feature used is msd-bidirectional, which
allows for all possible reorderings over a speci-
fied distortion limit. The baseline accuracies are
shown in table 2.
Corpus BLEU Lexical Accuracy
Development 0.1734 0.448
Testing 0.1823 0.492
Table 2: Baseline Accuracy
We conduct two types of experiments to test our
approach.
1. Experiments using lexical features (see sec-
tion 6.1), and
2. Experiments using syntactic features (see
section 6.2).
6.1 Experiments using Lexical Features
In this section, we present results of our exper-
iments that use only lexical features. First, we
measure the translation accuracy using LCS de-
coding. On the development set, we explored the
set of decoding parameters (as described in sec-
tion 4.1) to compute the optimal parameter val-
ues. The best lexical accuracy obtained on the de-
velopment set is 0.4321 and the best BLEU score
obtained is 0.0923 at a threshold of 0.17 and a per-
mutation window size of value 3. The accuracies
corresponding to a few other parameter values are
shown in Table 3.
On the test data, we obtained a lexical accu-
racy of 0.4721 and a BLEU score of 0.1023. As
we can observe, the BLEU score obtained using
the LCS decoding technique is low when com-
pared to the BLEU score of the state-of-art sys-
tem. However, the lexical accuracy is comparable
Threshold Perm. Window LexAcc BLEU
0.16 3 0.4274 0.0914
0.17 3 0.4321 0.0923
0.18 3 0.4317 0.0918
0.16 4 0.4297 0.0912
0.17 4 0.4315 0.0915
Table 3: Lexical Accuracies of Lattice-Output us-
ing lexical features alone for various parameter
values
to the lexical accuracy of Moses. This shows that
the discriminative model provides good lexical se-
lection, while the sentence construction technique
does not perform as expected.
Next, we present the results of the Moses based
decoder that uses the discriminative model (see
section 3.2). In our experiments, we did not use
MERT training for tuning the Moses parameters.
Rather, we explore a set of possible parameter val-
ues (i.e. weights of the translation model, reorder-
ing model and the language model) to check the
performance. We show the BLEU scores obtained
on the development set using Moses decoder in
Table 4.
Reordering LM Translation BLEU
weight(d) weight(l) weight(t)
0 0.6 0.3 0.1347
0 0.6 0.6 0.1354
0.3 0.6 0.3 0.1441
0.3 0.6 0.6 0.1468
Table 4: BLEU for different weight values using
lexical features only
On the test set, we obtained a BLEU score of
0.1771. We observe that both the lexical accuracy
and the BLEU scores obtained using the discrim-
inative training model combined with the Moses
decoder are comparable to the state-of-art results.
The summary of the results obtained using three
approaches and lexical feature functions is pre-
sented in Table 5.
6.2 Experiments using Syntactic Features
In this section, we present the effect of incorpo-
rating syntactic features using our model on the
40
Approach BLEU LexAcc
State-of-art(MOSES) 0.1823 0.492
LCS decoding 0.1023 0.4721
Moses decoder trained
using a discriminative 0.1771 0.4841
model
Table 5: Translation accuracies using lexical fea-
tures for different approaches
translation accuracies. Table 6 presents the results
of our approach that uses syntactic features at dif-
ferent parameter values. Here, we can observe
that the translation accuracies (both LexAcc and
BLEU) are better than the model that uses only
lexical features.
Reordering LM Translation BLEU
weight(d) weight(l) weight(t)
0 0.6 0.3 0.1661
0 0.6 0.6 0.1724
0.3 0.6 0.3 0.1780
0.3 0.6 0.6 0.1847
Table 6: BLEU for different weight values using
syntactic features
Table 7 shows the comparative performance of
the model using syntactic as well as lexical fea-
tures against the one with lexical features func-
tions only.
Model BLEU LexAcc
Lexical features 0.1771 0.4841
Lexical+Syntactic 0.201 0.5431
features
Table 7: Comparison between translation accura-
cies from models using syntactic and lexical fea-
tures
On the test set, we obtained a BLEU score of
0.20 which is an improvement of 2.3 points over
the model that uses lexical features alone. We also
obtained an increase of 6.1% in lexical accuracy
using this model with syntactic features as com-
pared to the model using lexical features only.
7 Conclusions and Future Work
In this paper, we presented an approach to statisti-
cal machine translation that combines the power
of a discriminative model (for training a model
for Machine Translation), and the standard beam-
search based decoding technique (for the transla-
tion of an input sentence). The key contributions
are:
1. We incorporated a discriminative model in
a phrase-based decoder. We obtained com-
parable results with the state-of-art phrase-
based decoder (see section 6.1). The ad-
vantage in using our approach is that it has
the flexibility to incorporate richer contextual
and linguistic feature functions.
2. We show that the incorporation of syntac-
tic information (POS tags) in our discrimina-
tive model boosted the performance of trans-
lation. The lexical accuracy using our ap-
proach improved by 6.1% when syntactic
features were used in addition to the lexi-
cal features. Similarly, the BLEU score im-
proved by 2.3 points when syntactic features
were used compared to the model that uses
lexical features alone. The accuracies are
likely to improve when richer linguistic fea-
ture functions (that use parse structure) are
incorporated in our approach.
In future, we plan to work on:
1. Experiment with rich syntactic and structural
features (parse tree-based features) using our
approach.
2. Experiment on other language pairs such as
Arabic-English and Hindi-English.
3. Improving LCS decoding algorithm using
syntactic cues in the target (Venkatapathy
and Bangalore, 2007) such as supertags.
References
Bangalore, S., P. Haffner, and S. Kanthak. 2007. Statistical machine transla-
tion through global lexical selection and sentence reconstruction. In An-
nual Meeting-Association for Computational Linguistics, volume 45, page
152.
Berger, A.L., V.J.D. Pietra, and S.A.D. Pietra. 1996. A maximum en-
tropy approach to natural language processing. Computational linguistics,
22(1):39?71.
41
Brown, P.F., V.J.D. Pietra, S.A.D. Pietra, and R.L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter estimation. Computa-
tional linguistics, 19(2):263?311.
Chen, S.F. and J. Goodman. 1999. An empirical study of smoothing
techniques for language modeling. Computer Speech and Language,
13(4):359?394.
Haffner, P. 2006. Scaling large margin classifiers for spoken language under-
standing. Speech Communication, 48(3-4):239?261.
Hassan, H., K. Sima?an, and A. Way. 2009. A syntactified direct translation
model with linear-time decoding. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Processing: Volume 3-Volume
3, pages 1182?1191. Association for Computational Linguistics.
Ittycheriah, A. and S. Roukos. 2007. Direct translation model 2. In Proceed-
ings of NAACL HLT, pages 57?64.
Koehn, P. and H. Hoang. 2007. Factored translation models. In Pro-
ceedings of the 2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 868?876.
Koehn, P., F.J. Och, and D. Marcu. 2003. Statistical phrase-based transla-
tion. In Proceedings of the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguistics on Human Language
Technology-Volume 1, pages 48?54. Association for Computational Lin-
guistics.
Koehn, P., H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi,
B. Cowan, W. Shen, C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In Annual meeting-association
for computational linguistics, volume 45, page 2.
Och, F.J. and H. Ney. 2002. Discriminative training and maximum entropy
models for statistical machine translation. In Proceedings of ACL, vol-
ume 2, pages 295?302.
Och, F.J., C. Tillmann, H. Ney, et al 1999. Improved alignment models
for statistical machine translation. In Proc. of the Joint SIGDAT Conf.
on Empirical Methods in Natural Language Processing and Very Large
Corpora, pages 20?28.
Papineni, KA, S. Roukos, and RT Ward. 1998. Maximum likelihood and
discriminative training of directtranslation models. In Acoustics, Speech
and Signal Processing, 1998. Proceedings of the 1998 IEEE International
Conference on, volume 1.
Papineni, K., S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a method for
automatic evaluation of machine translation. In Proceedings of the 40th
annual meeting on association for computational linguistics, pages 311?
318. Association for Computational Linguistics.
Venkatapathy, S. and S. Bangalore. 2007. Three models for discriminative
machine translation using Global Lexical Selection and Sentence Recon-
struction. In Proceedings of the NAACL-HLT 2007/AMTA Workshop on
Syntax and Structure in Statistical Translation, pages 96?102. Association
for Computational Linguistics.
Venkatapathy, Sriram and Srinivas Bangalore. 2009. Discriminative Machine
Translation Using Global Lexical Selection. ACM Transactions on Asian
Language Information Processing, 8(2).
Xiong, D., Q. Liu, and S. Lin. 2006. Maximum entropy based phrase reorder-
ing model for statistical machine translation. In Proceedings of the 21st
International Conference on Computational Linguistics and the 44th an-
nual meeting of the Association for Computational Linguistics, page 528.
Association for Computational Linguistics.
42
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 66?74,
COLING 2010, Beijing, August 2010.
A Discriminative Approach for Dependency Based
Statistical Machine Translation
Sriram Venkatapathy
LTRC, IIIT-Hyderabad
sriram@research.iiit.ac.in sangal@mail.iiit.ac.in
Rajeev Sangal
LTRC, IIIT-Hyderabad
Aravind Joshi
University of Pennsylvania
joshi@seas.upenn.edu karthik.gali@gmail.com
Karthik Gali1
Talentica
Abstract
In this paper, we propose a dependency
based statistical system that uses discrim-
inative techniques to train its parameters.
We conducted experiments on an English-
Hindi parallel corpora. The use of syntax
(dependency tree) allows us to address the
large word-reorderings between English
and Hindi. And, discriminative training
allows us to use rich feature sets, includ-
ing linguistic features that are useful in the
machine translation task. We present re-
sults of the experimental implementation
of the system in this paper.
1 Introduction
Syntax based approaches for Machine Translation
(MT) have gained popularity in recent times be-
cause of their ability to handle long distance re-
orderings (Wu, 1997; Yamada and Knight, 2002;
Quirk et al, 2005; Chiang, 2005), especially for
divergent language pairs such as English-Hindi
(or English-Urdu). Languages such as Hindi are
also known for their rich morphology and long
distance agreement of features of syntactically re-
lated units. The morphological richness can be
handled by employing techniques that factor the
lexical items into morphological factors. This
strategy is also useful in the context of English-
Hindi MT (Bharati et al, 1997; Bharati et al,
1This work was done at LTRC, IIIT-Hyderabad, when he
was a masters student, till July 2008
2002; Ananthakrishnan et al, 2008; Ramanathan
et al, 2009) where there is very limited paral-
lel corpora available, and breaking words into
smaller units helps in reducing sparsity. In or-
der to handle phenomenon such as long-distance
word agreement to achieve accurate generation of
target language words, the inter-dependence be-
tween the factors of syntactically related words
need to be modelled effectively.
Some of the limitations with the syntax based
approaches such as (Yamada and Knight, 2002;
Quirk et al, 2005; Chiang, 2005) are, (1) They
do not offer flexibility for adding linguistically
motivated features, and (2) It is not possible to
use morphological factors in the syntax based ap-
proaches. In a recent work (Shen et al, 2009), lin-
guistic and contextual information was effectively
used in the framework of a hierarchical machine
translation system. In their work, four linguistic
and contextual features are used for accurate se-
lection of translation rules. In our approach in
contrast, linguistically motivated features can be
defined that directly effect the prediction of var-
ious elements in the target during the translation
process. This features use syntactic labels and col-
location statistics in order to allow effective train-
ing of the model.
Some of the other approaches related to our
model are the Direct Translation Model 2 (DTM2)
(Ittycheriah and Roukos, 2007), End-to-End Dis-
criminative Approach to MT (Liang et al, 2006)
and Factored Translation Models (Koehn and
Hoang, 2007). In DTM2, a discriminative trans-
66
lation model is defined in the setting of a phrase
based translation system. In their approach, the
features are optimized globally. In contrast to
their approach, we define a discriminative model
for translation in the setting of a syntax based ma-
chine translation system. This allows us to use
both the power of a syntax based approach, as
well as, the power of a large feature space during
translation. In our approach, the weights are op-
timized in order to achieve an accurate prediction
of the individual target nodes, and their relative
positions.
We propose an approach for syntax based sta-
tistical machine translation which models the fol-
lowing aspects of language divergence effectively.
? Word-order variation including long-
distance reordering which is prevalent
between language pairs such as English-
Hindi and English-Japanese.
? Generation of word-forms in the target lan-
guage by predicting the word and its factors.
During prediction, the inter-dependence of
factors of the target word form with the fac-
tors of syntactically related words is consid-
ered.
To accomplish this goal, we visualize the prob-
lem of MT as transformation from a morpho-
logically analyzed source syntactic structure to a
target syntactic structure1 (See Figure 1). The
transformation is factorized into a series of mini-
transformations, which we address as features of
the transformation. The features denote the vari-
ous linguistic modifications in the source structure
to obtain the target syntactic structure. Some of
the examples of features are lexical translation of
a particular source node, the ordering at a particu-
lar source node etc. These features can be entirely
local to a particular node in the syntactic structure
or can span across syntactically related entities.
More about the features (or mini-transformations)
is explained in section 3. The transformation of
a source syntactic structure is scored by taking a
weighted sum of its features 2. Let ? represent
1Note that target structure contains only the target fac-
tors. An accurate and deterministic morphological generator
combines these factors to produce the target word form.
2The features can be either binary-values or real-valued
the transformation of source syntactic structure s,
the score of transformation is computed as repre-
sented in Equation 1.
score(? |s) =
?
i
wi ? fi(?, s) (1)
In Equation 1, f ?is are the various features of
transformation and w?is are the weights of the fea-
tures. The strength of our approach lies in the flex-
ibility it offers in incorporating linguistic features
that are useful in the task of machine translation.
These features are also known as prediction fea-
tures as they map from source language informa-
tion to information in the target language that is
being predicted.
During decoding a source sentence, the goal
is to choose a transformation that has the high-
est score. The source syntactic structure is tra-
versed in a bottom-up fashion and the target syn-
tactic structure is simultaneously built. We used
a bottom-up traversal while decoding because it
builds a contiguous sequence of nodes for the sub-
trees during traversal enabling the application of a
wide variety of language models.
In the training phase, the task is to learn the
weights of features. We use an online large-
margin training algorithm, MIRA (Crammer et
al., 2005), for learning the weights. The weights
are locally updated at every source node during
the bottom-up traversal of the source structure.
For training the translation model, automatically
obtained word-aligned parallel corpus is used. We
used GIZA++ (Och and Ney, 2003) along with the
growing heuristics to word-align the training cor-
pus.
The basic factors of the word used in our exper-
iments are root, part-of-speech, gender, number
and person. In Hindi, common nouns and verbs
have gender information whereas, English doesn?t
contain that information. Apart from the basic
factors, we also consider the role information pro-
vided by labelled dependency parsers. For com-
puting the dependency tree on the source side, We
used stanford parser (Klein and Manning, 2003)
in the experiments presented in this chapter3.
3Stanford parser gives both the phrase-structure tree as
well as dependency relations for a sentence.
67
root=mila,   tense=PAST
gnp=m3sg
root=se
gnp=x3sgroot=raamgnp=m1sg
root=shyaam
gnp=m1sg
root=pay,   tense=PAST
gnp=x3sg,  role=X 
paid/VBD
root=Ram,  gnp=x1sg
Ram/NNP
role=subj
visit/NN
root=visit,  gnp=x3sg
role=obj role=vmodroot=to,     gnp=x3sg
to/TO
root=Shyam,  gnp=x1sg
role=pmod
Shyam/NNP
root=a,    gnp=x3sg
role=nmod
a/DT
Figure 1: Transformation from source structure to target language
The function words such as prepositions and
auxiliary verbs largely express the grammatical
roles/functions of the content words in the sen-
tence. In fact, in many agglutinative languages,
these words are commonly attached to the con-
tent word to form one word form. In this pa-
per, we also conduct experiments where we begin
by grouping the function words with their corre-
sponding function words. These groups of words
are called local-word groups. In these cases, the
function words are considered as factors of the
content words. Section 2 explains more about the
local word groups in English and Hindi.
2 Local Word Groups
Local word groups (LWGs) (Bharati et al, 1998;
Vaidya et al, 2009) consist of a content word and
its associated function words. Local word group-
ing reduces a sentence to a sequence of content
words with the case-markers and tense-markers
acting as their factors. For example, consider
an English sentence ?People of these island have
adopted Hindi as a means of communication?.
?have adopted? is a LWG with root ?adopt? and
tense markers being ?have ed?. Another example
for the LWG will be ?of communication? where
?communication? is the root, and ?of? is the case-
marker. It is to be noted that Local word grouping
is different from chunking, where more than one
content word can be part of a chunk. We obtain lo-
cal word groups in English by processing the out-
put of the stanford parser. In Hindi, the function
words always appear immediately after the con-
tent word4, and it requires simple pattern
matching to obtain the LWGs. The rules ap-
plied are, (1) VM (RB|VAUX)+, and (2) N.* IN.
3 Features
There are three types of transformation features
explored by us, (1) Local Features, (2) Syntactic
Features and, (3) Contextual Features. In this sec-
tion, we describe each of these categories of fea-
tures representing different aspects of transforma-
tion with examples.
3.1 Local Features
The local features capture aspects of local trans-
formation of an atomic treelet in the source
structure to an atomic treelet in the target lan-
guage. Atomic treelet is a semantically non-
decomposible group of one or more nodes in the
syntactic structure. It usually contains only one
node, except for the case of multi-word expres-
sions (MWEs). Figure 2 presents the examples of
local transformation.
Some of the local features used by us in our ex-
periments are (1) dice coefficient, (2) dice coeffi-
cient of roots, (3) dice coefficient of null transla-
tions, (4) treelet translation probability, (5) gnp-
gnp pair, (5) preposition-postposition pair, (6)
tense-tense pair, (7) part-of-speech fertility etc.
Dice coefficients and treelet translation probabil-
ities are measures that express the statistical co-
occurrence of the atomic treelets.
4case-markers are called postpositions
68
root=Ram,  gnp=x1sg
Ram/NNP
role=subj
root=pay,   tense=PASTgnp=x3sg,  role=X 
paid/VBD
visit/NN
root=visit,  gnp=x3sg
role=obj
root=mila,   tense=PASTgnp=m3sgroot=raamgnp=m1sg
Figure 2: Local transformations
3.2 Syntactic Features
The syntactic features are used to model the differ-
ence in the word orders of the two languages. At
every node of the source syntactic structure, these
features define the changes in the relative order
of children during the process of transformation.
They heavily use source information such as part-
of-speech tags and syntactic roles of the source
nodes. One of the features used is reorderPostags.
This feature captures the change in relative po-
sitions of children with respect to their parents
during the tree transformation. An example fea-
ture for the transformation given in Figure 1 is
shown in Figure 3.
  IN   NNP
   
   VB    
  NNP
     VB
 TO
Figure 3: Syntactic feature - reorder postags
The feature reorderPostags is in the form of a
complete transfer rule. To handle cases, where the
left-hand side of ?reorderPostags? does not match
the syntactic structure of the source tree, the sim-
pler feature functions are used to qualify various
reorderings. Instead of using POS tags, feature
functions can be defined that use syntactic roles.
Apart from the above feature functions, we can
also have features that compute the score of a par-
ticular order of children using syntactic language
models (Gali and Venkatapathy, 2009; Guo et al,
2008). Different features can be defined that use
different levels of information pertaining to the
atomic treelet and its children.
3.3 Contextual Features
Contextual features model the inter-dependence
of factors of nodes connected by dependency arcs.
These features are used to enable access to global
information for prediction of target nodes (words
and its factors).
One of the features diceCoeffParent, relates the
parent of a source node to the corresponding target
node (see figure 4.
x1
x2
x3 x4
y
dice
Figure 4: Use of Contextual (parent) information
of x2 for generation of y
The use of this feature is expected to address of
the limitations of using ?atomic treelets? as the ba-
sic units in contrast to phrase based systems which
consider arbitrary sequences of words as units to
encode the local contextual information. In my
case, We relate the target treelet with the contex-
tual information of the source treelet using feature
functions rather than using larger units. Similar
features are used to connect the context of a source
node to the target node.
Various feature functions are defined to han-
dle interaction between the factors of syntacti-
cally related treelets. The gender-number-person
agreement is a factor that is dependent of gender-
number-person factors of the syntactically related
treelets in Hindi. The rules being learnt here
are simple. However, more complex interac-
tions can also be handled though features such as
prep Tense where, the case-marker in the target is
linked to the tense of parent verb.
4 Decoding
The goal is to compute the most probable target
sentence given a source sentence. First, the source
sentence is analyzed using a morphological ana-
lyzer5, local word grouper (see section 2) and a
dependency parser. Given the source structure,
the task of the decoding algorithm is to choose the
transformation that has the maximum score.
5http://www.cis.upenn.edu/?xtag/
69
The dependency tree of the source language
sentence is traversed in a bottom-up fashion for
building the target language structure. At every
source node during the traversal, the local trans-
formation is first computed. Then, the relative or-
der of its children is then computed using the syn-
tactic features. This results in a target structure
associated with the subtree rooted at the particular
node. The target structure associated with the root
node of the source structure is the result of the best
transformation of the entire source structure.
Hence, the task of computing the best transfor-
mation of the entire source structure is factorized
into the tasks of computing the best transforma-
tions of the source treelets. The equation for com-
puting the score of a transformation, Equation 1,
can be modified as Equation 2 given below.
score(? |s) =
?
r
|r| ?
?
i
wi ? fi(?r, r) (2)
where, ?j is the local transformation of the
source treelet r. The best transformation ?? of
source sentence s is,
?? = argmax? score(? |s) (3)
5 Training Algorithm
The goal of the training algorithm is to learn the
feature weights from the word aligned corpus. For
word-alignment, we used the IBM Model 5 imple-
mented in GIZA++ along with the growing heuris-
tics (Koehn et al, 2003). The gold atomic treelets
in the source and their transformation is obtained
by mapping the source node to the target using the
word-alignment information. This information is
stored in the form of transformation tables that is
used for the prediction of target atomic treelets,
prepositions and other factors. The transformation
tables are pruned in order to limit the search and
eliminate redundant information. For each source
element, only the top few entries are retained in
the table. This limit ranges from 3 to 20.
We used an online-large margin algorithm,
MIRA (McDonald and Pereira, 2006; Crammer
et al, 2005), for updating the weights. During
parameter optimization, it is sometimes impossi-
ble to achieve the gold transformation for a node
because the pruned transformation tables may not
lead to the target gold prediction for the source
node. In such cases where the gold transforma-
tion is unreachable, the weights are not updated
at all for the source node as it might cause erro-
neous weight updates. We conducted our exper-
iments by considering both the cases, (1) Identi-
fying source nodes with unreachable transforma-
tions, and (2) Updating weights for all the source
nodes (till a maximum iteration limit). The num-
ber of iterations on the entire corpus can also be
fixed. Typically, two iterations have been found to
be sufficient to train the model.
The dependency tree is traversed in a bottom-up
fashion and the weights are updated at each source
node.
6 Experiments and Results
The important aspects of the translation model
proposed in this paper have been implemented.
Some of the components that handle word in-
sertions and non-projective transformations have
not yet been implemented in the decoder, and
should be considered beyond the scope of this
paper. The focus of this work has been to
build a working syntax based statistical machine
translation system, which can act as a plat-
form for further experiments on similar lines.
The system would be available for download
at http://shakti.iiit.ac.in/?sriram/vaanee.html. To
evaluate this experimental system, a restricted
set of experiments are conducted. The experi-
ments are conducted on the English-Hindi lan-
guage pair using a corpus in tourism domain con-
taining 11300 sentence pairs6.
6.1 Training
6.1.1 Configuration
For training, we used DIT-TOURISM-ALIGN-
TRAIN dataset which is the word-aligned dataset
of 11300 sentence pairs. The word-alignment is
done using GIZA++ (Och and Ney, 2003) toolkit
and then growing heuristics are applied. For
our experiments, we use two growing heuristics,
GROW-DIAG-FINAL-AND and GROW-DIAG-
FINAL as they cover most number of words in
both the sides of the parallel corpora.
6DIT-TOURISM corpus
70
Number of Training Sentences 500
Iterations on Corpus 1-2
Parameter optimization algorithm MIRA
Beam Size 1-20
Maximum update attempts at source node 1-4
Unreachable updates False
Size of transformation tables 3
Table 1: Training Configuration
The training of the model can be performed un-
der different configurations. The configurations
that we used for the training experiments are given
in Table 6.1.1.
6.2 Results
For the complete training, the number of sen-
tences that should be used for the best perfor-
mance of the decoder should be the complete set.
In the paper, we have conducted experiments by
considering 500 training sentences to observe the
best training configuration.
At a source node, the weight vector is itera-
tively updated till the system predicts the gold
transformation. We conducted experiments by fix-
ing the maximum number of update attempts. A
source node, where the gold transformation is not
achieved even after the maximum updates limit,
the update at this source node is termed a update
failure. The source nodes, where the gold trans-
formation is achieved even without making any
updates is known as the correct prediction.
At some of the source nodes, it is not possible
to arrive at the gold target transformation because
of limited size of the training corpus. At such
nodes, we have avoided doing any weight update.
As the desired transformation is unachievable, any
attempt to update the weight vector would cause
noisy weight updates.
We observe various parameters to check the ef-
fectiveness of the training configuration. One of
the parameters (which we refer to as ?updateHits?)
computes the number of successful updates (S)
performed at the source nodes in contrast to num-
ber of failed updates (F ). Successful updates re-
sult in the prediction of the transformation that is
same as the reference transformation. A failed up-
date doesn?t result in the achievement of the cor-
rect prediction even after the maximum iteration
limit (see section 6.1.1) is reached. At some of the
source nodes, the reference transformations are
unreachable (U ). The goal is to choose the con-
figuration that has least number of average failed
updates (F ) because it implies that the model has
been learnt effectively.
UpdateHit
K m P S F U
1. 1 4 1680 2692 84 4081
2. 5 4 1595 2786 75 4081
3. 10 4 1608 2799 49 4081
4. 20 4 1610 2799 47 4081
Table 2: Training Statistics - Effect of Beam Size
From Table 2, we can see that the bigger beam
size leads to a better training of the model. The
beam size was varied between 1 and 20, and the
number of update failures (F ) was observed to be
least at K=20.
UpdateHit
K m P S F U
1. 20 1 1574 2724 158 4081
2. 20 2 1598 2767 91 4081
3. 20 4 1610 2799 47 4081
Table 3: Training Statistics - Effect of maximum
update attempts
In Table 3, we can see that an higher limit on
the maximum number of update attempts results
in less number of update attempts as expected. A
much higher value of m is not preferable because
the training updates makes noisy updates in case
of difficult nodes i.e., the nodes where target trans-
formation is reachable in theory, but is unreach-
able given the set of features.
UpdateHit
K i P S F U
1. 1 1 1680 2692 84 4081
2. 1 2 1679 2694 83 4081
Table 4: Training Statistics - Effect of number of
iterations
Now, we examine the effect of number of it-
71
erations on the quality of the model. In table 4,
we can observe that the number of iterations on
the data has no effect on the quality of the model.
This implies, that the model is adequately learnt
after one pass through the data. This is possible
because of the multiple number of update attempts
allowed at every node. Hence, the weights are up-
dated at a node till the model prediction is consis-
tent with the gold transformation.
Based on the above observations, we consider
the configuration 4 in Table 2 for the decoding ex-
periments.
Now, we present some of the top features
weights leant by the best configuration. The
weights convey that important properties of trans-
formation are being learnt well. Table 5 presents
the weights of the features ?diceRoot?, ?dice-
RootChildren? and ?diceRootParent?.
Feature Weight
dice 75.67
diceChildren 540.31
diceParent 595.94
treelet translation probability (ttp) 1 0.77
treelet translation probability (ttp) 2 389.62
Table 5: Weights of dice coefficient based features
We see that the dice coefficient based local and
contextual features have a positive impact on the
selection of correct transformations. A feature
that uses a syntactic language model to compute
the perplexity per word has a negative weight of
-1.115.
Table 6 presents the top-5 entries of contex-
tual features that describe the translation of source
argument ?nsubj? using contextual information
(?tense? of its parent).
Feature Weight
roleTenseVib:nsubj+NULL NULL 44.194196513246
roleTenseVib:nsubj+has VBN ne 14.4541356715382
roleTenseVib:nsubj+VBD ne 10.9241093097953
roleTenseVib:nsubj+VBP meM 6.14149937079584
roleTenseVib:nsubj+VBP NULL 4.76795730621754
Table 6: Top weights of a contextual feature :
preposition+Tense-postposition
Table 7 presents the top-10 ordering relative po-
sition feature where the head word is a verb. In
this feature, the relative position (left or right) of
the head and the child is captured. For example, a
feature ?relPos:amod-NN?, if active, conveys that
an argument with the role ?amod? is at the left of
a head word with POS tag ?NN?.
Feature Weight
relPos:amod-NN 6.70
relPos:NN-appos 1.62
relPos:lrb-NN 1.62
Table 7: Top weights of relPos feature
6.3 Decoding
We computed the translation accuracies using two
metrics, (1) BLEU score (Papineni et al, 2002),
and (2) Lexical Accuracy (or F-Score) on a test
set of 30 sentences. We compared the accuracy
of the experimental system (Vaanee) presented in
this paper, with Moses (state-of-the-art translation
system) and Shakti (rule-based translation system
7) under similar conditions (with using a develop-
ment set to tune the models). The rule-based sys-
tem considered is a general domain system tuned
to the tourism domain. The best BLEU score for
Moses on the test set is 0.118, and the best lexi-
cal accuracy is 0.512. The best BLEU score for
Shakti is 0.054, and the best lexical accuracy is
0.369.
In comparison, the best BLEU score of Vaanee
is 0.067, while the best lexical accuracy is 0.445.
As observed, the decoding results of the experi-
mental system mentioned here are not yet compa-
rable to the state-of-art. The main reasons for the
low translation accuracies are,
1. Poor Quality of the dataset
The dataset currently available for English-
Hindi language pair is noisy. This is an
extremely large limiting factor for a model
which uses rich linguistic information within
the statistical framework.
2. Low Parser accuracy
7http://shakti.iiit.ac.in/
72
The parser accuracy on the English-Hindi
dataset is low, the reasons being, (1) Noise,
(2) Length of sentences, and (3) Wide scope
of the tourism domain.
3. Word insertions not implemented yet
4. Non-projectivity not yet handled
5. BLEU is not an appropriate metric
BLEU is not an appropriate metric (Anan-
thakrishnan et al, ) for measuring the trans-
lation accuracy into Indian languages.
6. Model is context free as far as targets words
are concerned. Selection depends on chil-
dren but not parents and siblings
This point concerns the decoding algorithm.
The current algorithm is greedy while chos-
ing the best translation at every source node.
It first explores the K-best local transforma-
tions at a source node. It then makes a greedy
selection of the predicted subtree based on
it?s overall score after considering the predic-
tions at the child nodes, and the relative posi-
tion of the local transformation with respect
the predictions at the child nodes.
The problem in this approach is that, an er-
ror once made at a lower level of the tree
is propogated to the top, causing more mis-
takes. A computationally reasonable solution
to this problem is to maintain a K-best list
of predicted subtrees corresponding to every
source node. This allows rectification of a
mistake made at any stage.
The system, however, performs better than the
rule based system. As observed earlier, the right
type of information is being learnt by the model,
and the approach looks promising. The limitations
expressed here shall be addressed in the future.
7 Conclusion
In this work, we presented a syntax based de-
pendency model to effectively handle problems in
translation from English to Indian languages such
as, (1) Large word order variation, and (2) Ac-
curate generation of word-forms in the target lan-
guage by predicted the word and its factors. The
model that we have proposed, has the flexibility of
adding rich linguistic features.
An experimental version of the system has been
implemented, which is available for download at
http://shakti.iiit.ac.in/?sriram/vaanee.html. This
can facilitate as a platform for future research in
syntax based statistical machine translation from
English to Indian languages. We also plan to per-
form experiments using this system between Eu-
ropean languages in future.
The performance of the implemented transla-
tion system, is not yet comparable to the state-
of-art results primarily for two reasons, (1) Poor
quality of available data, because of which our
model which uses rich linguistic information
doesn?t perform as expected, and (2) Components
for word insertion and non-projectivity handling
are yet to be implemented in this version of the
system.
References
Ananthakrishnan, R, B Pushpak, M Sasikumar, and
Ritesh Shah. Some issues in automatic evaluation
of english-hindi mt: more blues for bleu. ICON-
2007.
Ananthakrishnan, R., Jayprasad Hegde, Pushpak Bhat-
tacharyya, and M. Sasikumar. 2008. Simple syntac-
tic and morphological processing can help english-
hindi statistical machine translation. In Proceedings
of IJCNLP-2008. IJCNLP.
Bharati, Akshar, Vineet Chaitanya, Amba P Kulkarni,
and Rajeev Sangal. 1997. Anusaaraka: Machine
translation in stages. A Quarterly in Artificial Intel-
ligence, NCST, Bombay (renamed as CDAC, Mum-
bai).
Bharati, Akshar, Medhavi Bhatia, Vineet Chaitanya,
and Rajeev Sangal. 1998. Paninian grammar
framework applied to english. South Asian Lan-
guage Review, (3).
Bharati, Akshar, Rajeev Sangal, Dipti M Sharma, and
Amba P Kulkarni. 2002. Machine translation activ-
ities in india: A survey. In Proceedings of workshop
on survey on Research and Development of Machine
Translation in Asian Countries.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
263?270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
73
Crammer, K., R. McDonald, and F. Pereira. 2005.
Scalable large-margin online learning for structured
classification. Technical report, University of Penn-
sylvania.
Gali, Karthik and Sriram Venkatapathy. 2009. Sen-
tence realisation from bag of words with depen-
dency constraints. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Student Research Workshop and Doctoral Consor-
tium, pages 19?24, Boulder, Colorado, June. Asso-
ciation for Computational Linguistics.
Guo, Yuqing, Josef van Genabith, and Haifeng Wang.
2008. Dependency-based n-gram models for gen-
eral purpose sentence realisation. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 297?304,
Manchester, UK, August. Coling 2008 Organizing
Committee.
Ittycheriah, Abraham and Salim Roukos. 2007. Di-
rect translation model 2. In Human Language Tech-
nologies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics; Proceedings of the Main Conference,
pages 57?64, Rochester, New York, April. Associa-
tion for Computational Linguistics.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
Koehn, Philipp and Hieu Hoang. 2007. Factored
translation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Koehn, P., F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the Hu-
man Language Technology Conference 2003 (HLT-
NAACL 2003), Edmonton, Canada, May.
Liang, P., A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to
machine translation. In International Conference
on Computational Linguistics and Association for
Computational Linguistics (COLING/ACL).
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
EACL.
Och, F.J. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Papineni, Kishore, Salim Roukos, Todd Ward, and W.J.
Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of
40th Annual Meeting of the Association of Compu-
tational Linguistics, pages 313?318, Philadelphia,
PA, July.
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 271?279, Ann
Arbor, Michigan, June. Association for Computa-
tional Linguistics.
Ramanathan, Ananthakrishnan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: Addressing the crux
of the fluency problem in english-hindi smt. In Pro-
ceedings of ACL-IJCNLP 2009. ACL-IJCNLP.
Shen, Libin, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of lin-
guistic and contextual information for statistical ma-
chine translation. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 72?80, Singapore, August. Asso-
ciation for Computational Linguistics.
Vaidya, Ashwini, Samar Husain, Prashanth Reddy, and
Dipti M Sharma. 2009. A karaka based annotation
scheme for english. In Proceedings of CICLing ,
2009.
Wu, Dekai. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377?404.
Yamada, Kenji and Kevin Knight. 2002. A decoder
for syntax-based statistical mt. In Proceedings of
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 303?310, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
74
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 472?483,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Investigations in Exact Inference for Hierarchical Translation
Wilker Aziz?, Marc Dymetman?, Sriram Venkatapathy?
?University of Wolverhampton, Wolverhampton, UK
?Xerox Research Centre Europe, Grenoble, France
?w.aziz@wlv.ac.uk, ?{first.last}@xrce.xerox.com
Abstract
We present a method for inference in hi-
erarchical phrase-based translation, where
both optimisation and sampling are per-
formed in a common exact inference
framework related to adaptive rejection
sampling. We also present a first imple-
mentation of that method along with ex-
perimental results shedding light on some
fundamental issues. In hierarchical transla-
tion, inference needs to be performed over
a high-complexity distribution defined by
the intersection of a translation hypergraph
and a target language model. We replace
this intractable distribution by a sequence
of tractable upper-bounds for which exact
optimisers and samplers are easy to obtain.
Our experiments show that exact inference
is then feasible using only a fraction of the
time and space that would be required by
the full intersection, without recourse to
pruning techniques that only provide ap-
proximate solutions. While the current im-
plementation is limited in the size of inputs
it can handle in reasonable time, our exper-
iments provide insights towards obtaining
future speedups, while staying in the same
general framework.
1 Introduction
In statistical machine translation (SMT), optimi-
sation ? the task of searching for an optimum
translation ? is performed over a high-complexity
distribution defined by the intersection between a
translation hypergraph and a target language model
(LM). This distribution is too complex to be repre-
sented exactly and one typically resorts to approx-
imation techniques such as beam-search (Koehn et
al., 2003) and cube-pruning (Chiang, 2007), where
maximisation is performed over a pruned represen-
tation of the full distribution.
Often, rather than finding a single optimum, one
is really interested in obtaining a set of proba-
bilistic samples from the distribution. This is the
case for minimum error rate training (Och, 2003;
Watanabe et al, 2007), minimum risk training
(Smith and Eisner, 2006) and minimum risk de-
coding (Kumar and Byrne, 2004). Due to the ad-
ditional computational challenges posed by sam-
pling, n-best lists, a by-product of optimisation, are
typically used as approximation to true probabilis-
tic samples. A known issue with n-best lists is that
they tend to be clustered around only one mode of
the distribution. A more direct procedure is to at-
tempt to directly draw samples from the underlying
distribution rather than rely on n-best list approxi-
mations (Arun et al, 2009; Blunsom and Osborne,
2008).
OS? (Dymetman et al, 2012a) is a recent ap-
proach that stresses a unified view between the two
types of inference, optimisation and sampling. In
this view, rather than resorting to pruning in or-
der to cope with the tractability issues, one upper-
bounds the complex goal distribution with a sim-
pler ?proposal? distribution for which dynamic
programming is feasible. This proposal is incre-
mentally refined to be closer to the goal until the
maximum is found, or until the sampling perfor-
mance exceeds a certain level.
This paper applies the OS? approach to the
problem of inference in hierarchical SMT (Chi-
ang, 2007). In a nutshell, the idea is to replace
the intractable problem of intersecting a context-
free grammar with a full language model by the
tractable problem of intersecting it with a simpli-
fied, optimistic version of this LM which ?forgets?
parts of n-gram contexts, and to incrementally add
more context based on evidence of the need to do
so. Evidence is gathered by optimising or sampling
from the tractable proxy distribution and focussing
on the most serious over-optimistic estimates rela-
tive to the goal distribution.
472
Our main contribution is to provide an exact op-
timiser/sampler for hierarchical SMT that is effi-
cient in exploring only a small fraction of the space
of n-grams involved in a full intersection. Al-
though at this stage our experiments are limited to
short sentences, they provide insights on the be-
havior of the technique and indicate directions to-
wards a more efficient implementation within the
same paradigm.
The paper is organized as follows: ?2 provides
background on OS? and hierarchical translation; ?3
describes our approach to exact inference in SMT;
in ?4 the experimental setup is presented and find-
ings are discussed; ?5 discusses related work, and
?6 concludes.
2 Background
2.1 OS?
The OS? approach (Dymetman et al, 2012a;
Dymetman et al, 2012b) proposes a unified view
of exact inference in sampling and optimisation,
where the two modalities are seen as extremes in a
continuum of inference tasks in Lp spaces (Rudin,
1987), with sampling associated with the L1 norm,
and optimisation with the L? norm.
The objective function p, over which inference
needs to be performed, is a complex non-negative
function over a discrete or continuous space X ,
which defines an unnormalised distribution over
X . The goal is to optimise or sample relative to
p ? where sampling is interpreted in terms of the
normalised distribution p?(.) = p(.)/ ?X p(x)dx.
Directly optimising or sampling from p is unfea-
sible; however, it is possible to define an (unnor-
malized) distribution q of lower complexity than
p, which upper-bounds p everywhere (ie. p(x) ?
q(x), ?x ? X), and from which it is feasible to
optimise or sample directly.
Sampling is performed through rejection sam-
pling: first a sample x is drawn from q, and then x
is accepted or rejected with probability given by the
ratio r = p(x)/q(x), which is less than 1 by con-
struction. Accepted x?s can be shown to produce
an exact sample from p (Robert and Casella, 2004).
When the sample x from q is rejected, it is used as
a basis for ?refining? q into a slightly more com-
plex q?, where p ? q? ? q is still an upper-bound to
p. This ?adaptive rejection sampling? technique in-
crementally improves the rate of acceptance, and is
pursued until some rate above a given threshold is
obtained, at which point one stops refining and uses
the current proposal to obtain further exact samples
from p.
In the case of optimisation, one finds the maxi-
mum x relative to q, and again computes the ratio
r = p(x)/q(x). If this ratio equals 1, then it is
easy to show that x is the actual maximum from
p.1 Otherwise we refine the proposal in a similar
way to the sampling case, continuing until we find
a ratio equal to 1 (or very close to 1 if we are will-
ing to accept an approximation to the maximum).
For finite spaces X , this optimisation technique is
argued to be a generalisation of A?.
An application of the OS? technique to sam-
pling/optimisation with High-Order HMM?s is de-
scribed in Carter et al (2012) and provides back-
ground for this paper. In that work, while the high-
order HMM corresponds to an intractable goal dis-
tribution, it can be upper-bounded by a sequence
of tractable distributions for which optimisers and
samplers can be obtained through standard dy-
namic programming techniques.
2.2 Hierarchical Translation
An abstract formulation of the decoding process
for hierarchical translation models such as that of
Chiang (2007) can be expressed as a sequence of
three steps. In a first step, a translation model
G, represented as a weighted synchronous context-
free grammar (SCFG) (Chiang, 2005), is applied to
(in other words, intersected with) the source sen-
tence f to produce a weighted context-free gram-
mar G(f) over the target language.2 In a second
step, G(f) is intersected with a weighted finite-
state automaton A representing the target language
model, resulting in a weighted context-free gram-
mar G?(f) = G(f) ? A. In a final step, a dynamic
programming procedure (see ?2.4) is applied to
find the maximum derivation x in G?(f), and the
sequence of leaves of yield(x) is the result transla-
tion.
While this formulation gives the general princi-
ple, already mentioned in Chiang (2007), most im-
plementations do not exactly follow these steps or
use this terminology. In practice, the closest ap-
proach to this abstract formulation is that of Dyer
(2010) and the related system cdec (Dyer et al,
2010); we follow a similar approach here.
1This is because if x? was such that p(x?) > p(x), then
q(x?) ? p(x?) > p(x) = q(x), and hence x would not be a
maximum for q, a contradiction.
2G(f) is thus a compact representation of a forest over
target sequences, and is equivalent to a hypergraph, using dif-
ferent terminology.
473
Whatever the actual implementation chosen, all
approaches face a common problem: the complex-
ity of the intersection G?(f) = G(f)?A increases
rapidly with the order of the language model, and
can become unwieldy for moderate-length input
sentences even with a bigram model. In order to
address this problem, most implementations em-
ploy variants of a technique called cube-pruning
(Chiang, 2007; Huang and Chiang, 2007), where
the cells constructed during the intersection pro-
cess retain only a k-best list of promising candi-
dates. This is an approximation technique, related
to beam-search, which performs well in practice,
but is not guaranteed to find the actual optimum.
In the approach presented here ? described in
detail in ?3 ? we do not prune the search space.
While we do construct the full initial grammar
G(f), we proceed by incrementally intersecting
it with simple automata associated with upper-
bounds ofA, for which the intersection is tractable.
2.3 Earley Intersection
In their classical paper Bar-Hillel et al (1961)
showed that the intersection of a CFG with a FSA is
a CFG, and Billot and Lang (1989) were possibly
the first to notice the connection of this construct
with chart-parsing. In general, parsing with a CFG
can be seen as a special case of intersection, with
the input sequence represented as a ?flat? (linear
chain) automaton, and this insight allows to gener-
alise various parsing algorithms to corresponding
intersection algorithms. One such algorithm, for
weighted context-free grammars and automata, in-
spired by the CKY parsing algorithm, is presented
in Nederhof and Satta (2008). The algorithm that
we are using is different; it is inspired by Earley
parsing, and was introduced in chapter 2 of Dyer
(2010). The advantage of Dyer?s ?Earley Intersec-
tion? algorithm is that it combines top-down pre-
dictions with bottom-up completions. The algo-
rithm thus avoids constructing many non-terminals
that may be justified from the bottom-up perspec-
tive, but can never be ?requested? by a top-down
derivation, and would need to be pruned in a sec-
ond pass. Our early experiments showed an impor-
tant gain in intermediary storage and in overall time
by using this Earley-based technique as opposed to
a CKY-based technique.
We do not describe the Earley Intersection algo-
rithm in detail here, but refer to Dyer (2010), which
we follow closely.
2.4 Optimisation and Sampling from a
WCFG
Optimisation in a weighted CFG (WCFG)3, that
is, finding the maximum derivation, is well stud-
ied and involves a dynamic programming proce-
dure that assigns in turn to each nonterminal, ac-
cording to a bottom-up traversal regime, a max-
imum derivation along with its weight, up to the
point where a maximum derivation is found for the
initial nonterminal in the grammar. This can be
seen as working in the max-times semiring, where
the weight of a derivation is obtained through the
product of the weights of its sub-derivations, and
where the weight associated with a nonterminal is
obtained by maximising over the different deriva-
tions rooted in that nonterminal.
The case of sampling can be handled in a very
similar way, by working in the sum-times instead
of the max-times semiring. Here, instead of max-
imising over the weights of the competing deriva-
tions rooted in the same nonterminal, one sums
over these weights. By proceeding in the same
bottom-up way, one ends with an accumulation of
all the weights on the initial nonterminal (this can
also be seen as the partition function associated
with the grammar). An efficient exact sampler is
then obtained by starting at the root nonterminal,
randomly selecting an expansion proportionally to
the weight of this expansion, and iterating in a top-
down way. This process is described in more detail
in section 4 of Johnson et al (2007), for instance.
3 Approach
The complexity of building the full intersection
G(f) ? A, when A represents a language model
of order n, is related to the fact that the number of
states of A grows exponentially with n, and that
each nonterminal N in G(f) tends to generate in
the grammar G?(f) many indexed nonterminals of
the form (i,N, j), where i, j are states of A and
the nonterminal (i,N, j) can be interpreted as an
N connecting an i state to a j state.
In our approach, instead of explicitly construct-
ing the full intersection G(f) ? A, which, using
the notation of ?2.1, is identified with the unnor-
malised goal distribution p(x), we incrementally
produce a sequence of ?proposal? grammars q(t),
which all upper-bound p, where q(0) = G(f) ?
A(0), ..., q(t+1) = q(t) ? A(t), etc. Here A(0) is
3Here the CFG is assumed to be acyclic, which is typically
the case in translation applications.
474
an optimistic, low complexity, ?unigram? version
of the automaton A, and each increment A(t) is a
small automaton that refines q(t) relative to some
specific k-gram context (i.e., sequence of k words)
not yet made explicit in the previous increments,
where k takes some value between 1 and n. This
process produces a sequence of grammars q(t) such
that q(0)(.) ? q(1)(.) ? q(2)(.) ? ... ? p(.).
In the limit ?Mt=0A(t) = A for some largeM , so
that we are in principle able to reconstruct the full
intersection p(.) = q(M) = G(f)?A(0)?...?A(M)
in finite time. In practice our actual process stops
much earlier: in optimisation, when the value of
the maximum derivation x?t relative to q(t) becomes
equal to its value according to the full language
model, in sampling when the acceptance rate of
samples from q(t) exceeds a certain threshold. The
process is detailed in what follows.
3.1 OS? for Hierarchical Translation
Our application of OS? to hierarchical translation is
illustrated in Algorithm 1, with the two modes, op-
timisation and sampling, made explicit and shown
side-by-side to stress the parallelism.
On line 1, we initialise the time step to 0, and
for sampling we also initialise the current accep-
tance rate (AR) to 0. On line 2, we initialise the
initial proposal grammar q(0), where A(0) is de-
tailed in ?3.2. On line 3, we start a loop: in op-
timisation we stop when we have found an x that
is accepted, meaning that the maximum has been
found; in sampling, we stop when the estimated
acceptance rate (AR) of the current proposal q(t)
exceeds a certain threshold (e.g. 20%) ? this AR
can be roughly estimated by observing how many
of the last (say) one hundred samples from the pro-
posal have been accepted, and tends to reflect the
actual acceptance rate obtained by using q(t) with-
out further refinements. On line 4, in optimisation,
we compute the argmax x from the proposal, and in
sampling we draw a sample x from the proposal.4
On line 5, we compute the ratio r = p(x)/q(t)(x);
by construction q(t) is an optimistic version of p,
thus r ? 1.
On line 6, in optimisation we accept x if the
ratio is equal to 1, in which case we have found
the maximum, and in sampling we accept x with
probability r, which is a form of adaptive rejec-
tion sampling and guarantees that accepted sam-
4Following the OS? approach, taking an argmax is actually
assimilated to an extreme form of sampling, with an L? space
taking the place of an L1 space.
ples form exact samples from p; see (Dymetman et
al., 2012a).
If x was rejected (line 7), we then (lines 8, 9)
refine q(t) into a q(t+1) such that p(.) ? q(t+1)(.) ?
q(t)(.) everywhere. This is done by defining the
incremental automatonA(t+1) on the basis of x and
q(t), as will be detailed below, and by intersecting
this automaton with q(t)
Finally, on line 11, in optimisation we return the
x which has been accepted, namely the maximum
of p, and in sampling we return the list of already
accepted x?s, which form an exact sample from p,
along with the current q(t), which can be used as a
sampler to produce further exact samples with an
acceptance rate performance above the predefined
threshold.
3.2 Incremental refinements
Initial automatonA(0) This deterministic au-
tomaton is an ?optimistic? version ofA which only
records unigram information. A(0) has only one
state q0, which is both initial and final. For each
word a of the target language it has a transition
(q0, a, q0) whose weight is denoted by w1(a). This
weight is called the ?max-backoff unigram weight?
(Carter et al, 2012) and it is defined as:
w1(a) ? maxh plm(a|h),
where plm(a|h) is the conditional language model
probability of a relative to the history h, and where
the maximum is taken over all possible histories,
that is, over all possible sequence of target words
that might precede a.
Max-backoffs Following Carter et al (2012),
for any language model of finite order, the unigram
max-backoff weights w1(a) can be precomputed in
a ?Max-ARPA? table, an extension of the ARPA
format (Jurafsky and Martin, 2000) for the target
language model, which can be precomputed on the
basis of the standard ARPA table.
From the Max-ARPA table one can also directly
compute the following ?max-backoff weights?:
w2(a|a?1), w3(a|a?2 a?1), ..., which are defined
by:
w2(a|a?1) ? maxh plm(a|h, a?1)
w3(a|a?2 a?1) ? maxh plm(a|h, a?2 a?1)
...
where the maximum is taken over the part of
the history which is not explicitely indicated.
475
Algorithm 1 OS? for Hierarchical Translation: Optimisation (left) and Sampling (right).
1: t? 0
2: q(0) ? G(f) ?A(0)
3: while not an x has been accepted do
4: Find maximum x in q(t)
5: r ? p(x)/q(t)(x)
6: Accept-or-Reject x according to r
7: if Rejected(x) then
8: define A(t+1) based on x and q(t)
9: q(t+1) ? q(t) ?A(t+1)
10: t? t + 1
11: return x
1: t? 0, AR? 0
2: q(0) ? G(f) ?A(0)
3: while not AR > threshold do
4: Sample x ? q(t)
5: r ? p(x)/q(t)(x)
6: Accept-or-Reject x according to r
7: if Rejected(x) then
8: define A(t+1) based on x and q(t)
9: q(t+1) ? q(t) ?A(t+1)
10: t? t + 1
11: return already accepted x?s along with q(t)
Note that: (i) if the underlying language model
is, say, a trigram model, then w3(a|a?2 a?1)
is simply plm(a|a?2 a?1), and similarly for an
underlying model of order k in general, and
(ii) w2(a|a?1) = maxa?2 w3(a|a?2 a?1) and
w1(a) = maxa?1 w2(a|a?1).
Incremental automata A(t) The weight
assigned to any target sentence by A(0) is larger or
equal to its weight according to A. Therefore, the
initial grammar q(0) = G(f) ? A(0) is optimistic
relative to the actual grammar p = G(f) ? A: for
any derivation x in p, we have p(x) ? q(0)(x).
We can then apply the OS? technique with q(0).
In the case of optimisation, this means that
we find the maximum derivation x from q(0).
By construction, with y = yield(x), we have
A(0)(y) ? A(y). If the two values are equal, we
have found the maximum,5 otherwise there must
be a word yi in the sequence ym1 = y for which
plm(yi|yi?11 ) is strictly smaller than w1(yi). Let us
take among such words the one for which the ratio
? = w2(yi|yi?1)/w1(yi) ? 1 is the smallest, and
for convenience let us rename b = yi?1, a = yi.
We then define the (deterministic) automaton A(1)
as illustrated in the following figure:
b:1 a:? 
else:1 
b:1 else:1 
0 1 
Here the state 0 is both initial and final, and the
state 1 is final; all edges carry a (multiplicative)
weight equal to 1, except edge (1, a, 0), which car-
ries the weight ?. We use the abbreviation ?else?
to refer to any label other than bwhen starting from
0, and other than b or a when starting from 1.
5This case is very unlikely with A(0), but helps introduce
the general case.
It is easy to check that this automaton assigns to
any word sequence y a weight equal to ?k, where k
is the number of occurrences of b a in y. In particu-
lar, if y is such that yi?1 = b, yi = a, then the tran-
sition in (the deterministic automaton) A(0) ?A(1)
that consumes yi carries the weight ? w1(a), in
other words, the weight w2(a|b). Thus the new
proposal grammar q(1) = q(0) ? A(1) has now
?incorporated? knowledge of the bigram a-in-the-
context-b, at the cost of some increase in its com-
plexity.6
The general procedure for choosing A(t+1) fol-
lows the same pattern. We find the max deriva-
tion x in q(t) along with its yield y; if p(x) =
q(t)(x), we stop and output x; otherwise we find
some subsequence yi?m?1, yi?m, ..., yi such that
the knowledge of the n-gram yi?m, ..., yi has al-
ready been registered in q(t), but not that of the
n-gram yi?m?1, yi?m, ..., yi, and we define an
automaton A(t+1) which assign to a sequence a
weight ?k, where
? = wm+1(yi|yi?m?1, yi?m, ..., yi?1)wm(yi|yi?m, ..., yi?1)
,
and where k is the number of occurrences of
yi?m?1, yi?m, ..., yi in the sequence.7
We note that we have p ? q(t+1) ? q(t) ev-
erywhere, and also that the number of possible re-
finement operations is bounded, because at some
point we would have expanded all contexts to their
maximum order, at which point we would have re-
produced p(.) on the whole space X of possible
6Note that without further increasing q(1)?s complexity one
can incorporate knowledge about all bigrams sharing the pre-
fix b. This is because A(1) does not need additional states
to account for different continuations of the context b, all we
need is to update the weights of the transitions leaving state 1
appropriately. More generally, it is not more costly to account
for all n-grams prefixed by the same context of n ? 1 words
than it is to account for only one of them.
7Building A(t+1) is a variant of the standard construction
for a ?substring-searching? automaton (Cormen et al, 2001)
and produces an automaton with n states (the order of the n-
gram). This construction is omitted for the sake of space.
476
derivations exactly. However, we typically stop
much earlier than that, without expanding contexts
in the regions of X which are not promising even
on optimistic assessments based on limited con-
texts.
Following the OS? methodology, the situation
with sampling is completely parallel to that of op-
timisation, the only difference being that, instead
of finding the maximum derivation x from q(t)(.),
we draw a sample x from the distribution asso-
ciated with q(t)(.), then accept it with probabil-
ity given by the ratio r = p(x)/q(t)(x) ? 1. In
the case of a reject, we identify a subsequence
yi?m?1, yi?m, ..., yi in yield(x) as in the optimi-
sation case, and similarly refine q(t) into q(t+1) =
q(t) ? A(t+1). The acceptance rate gradually in-
creases because q(t) comes closer and closer to p.
We stop the process at a point where the current ac-
ceptance rate, estimated on the basis of, say, the last
one hundred trials, exceeds a predefined threshold,
perhaps 20%.
3.3 Illustration
In this section, we present a small running example
of our approach. Consider the lowercased German
source sentence: eine letzte beobachtung .
Table 1 shows the translation associated with the
optimum derivation from each proposal q(i). The
n-gram whose cost, if extended by one word to the
left, would be increased by the largest factor is un-
derlined. The extended context selected for refine-
ment is highlighted in bold.
i Rules Optimum
0 311 <s> one last observation . </s>
1 454 <s> one last observation . </s>
2 628 <s> one last observation . </s>
3 839 <s> one final observation . </s>
4 1212 <s> one final observation . </s>
...
12 3000 <s> a final observation . </s>
13 3128 <s> one final observation . </s>
Table 1: Optimisation steps showing the iteration
(i), the number of rules in the grammar and the
translation associated to the optimum derivation.
Consider the very first iteration (i = 0), at which
point only unigram costs have been incorporated.
The sequence <s> one last observation . </s>
represents the translation associated to the best
derivation x in q(0). We proceed by choosing from
it one sequence to be the base for a refinement that
will lower q(0) bringing it closer to p. Amongst all
possible one-word (to the left) extensions, extend-
ing the unigram ?one? to the bigram ?<s> one? is
the operation that lowers q(0)(x) the most. It might
be helpful to understand it as the bigram ?<s> one?
being associated to the largest LM gap observed
in x. Therefore the context ?<s>? is selected for
refinement, which means that an automaton A(1)
is designed to down-weight derivations compatible
with bigrams prefixed by ?<s>?. The proposal q(0)
is intersected with A(1) producing q(1). We pro-
ceed like this iteratively, always selecting a con-
text not yet accounted for until q(i)(x) = p(x) for
the best derivation (13th iteration in our example),
when the true optimum is found with a certificate
of optimality.
Q Q Q Q Q Q Q Q Q Q Q Q Q Q
0 2 4 6 8 10 12
?
2
?
1
0
1
2
3
Iteration (i)
Scor
e (Q, 
P, B) ;
 Delta
 (C, M
) ; #st
ates (
R)
P P P
P P P P P P P P P P
PB B B B B B B B B B B B B B
C C C C C C C C C C C C C C
M M M M M M M M M M M M M M
R
R R R R
R
R
R R
R R
R
R
R
QPBCMR
QPBestCurrent gapMinimum gapRefinement
Figure 1: Certificate of optimality.
Figure 1 displays the progression of Q (score of
the best derivation) and P (that derivation?s true
score). As guaranteed by construction, Q is always
above P . B represents the score of the best deriva-
tion so far according to the true scoring function,
that is, B is a lower-bound on the true optimum8.
The optimal solution is achieved when P = Q.
Curve B in Figure 1 shows that the best scoring
solution was found quite early in the search (i = 3).
However, optimality could only be proven 10 itera-
tions later. Another way of stating the convergence
criterion Q = P is observing a zero gap (in the log
domain) between Q and P (see curve C ? current
gap), or a zero gap between Q and B (see curve M
? minimum gap). Observe how M drops quickly
from 1 to nearly 0, followed by a long tail whereM
8This observation allows for error-safe pruning in optimi-
sation: if x is a lower-bound on the true optimum, derivations
in q(i) that score lower than p(x) could be safely removed.
We have left that possibility for future work.
477
decreases much slower. Note that if we were will-
ing to accept an approximate solution, we could al-
ready stop the search if B remained unchanged for
a predetermined number of iterations or if changes
in B were smaller than some threshold, at the cost
of giving up on the optimality certificate.
Finally, curve R shows the number of states in
the automaton A(i) that refines the proposal at it-
eration i. Note how lower order n-grams (2-grams
in fact) are responsible for the largest drop in the
first iterations and higher-order n-grams (in fact 3-
grams) are refined later in the long tail.
Figure 2 illustrates the progression of the sam-
pler for the same German sentence. At each iter-
ation a batch of 500 samples is drawn from q(i).
The rejected samples in the batch are used to col-
lect statistics about overoptimistic n-grams and to
heuristically choose one context to be refined for
the next iteration, similar to the optimisation mode.
We start with a low acceptance rate which grows
up to 30% after 15 different contexts were incor-
porated. Note how the L1 norm of q (its partition
function) decreases after each refinement, that is,
q is gradually brought closer to p, resulting in the
increased number of exact samples and better ac-
ceptance rate.
Note that, starting from iteration one, all refine-
ments here correspond to 2-grams (i.e. one-word
contexts). This can be explained by the fact that,
in sampling, lower-order refinements are those that
mostly increase acceptance rate (rationale: high-
order n-grams are compatible with fewer grammar
rules).
Iteration (i)
1.0
1.5
2.0
0 5 10
l
l l l l l l l l l l l l lrefinement
0.1
0.2
0.3
l l
l l l l
l l l l l l l
laccrate
0
100
0
l l l l l
l l l
l l l
l l l
exact
9
10 l
l l l l l l l l l l l l l
L1
Figure 2: L1 norm of q, the number of exact sam-
ples drawn, the acceptance rate and the refinement
type at each iteration.
4 Experiments
We used the Moses toolkit (Koehn et al, 2007)
to extract a SCFG following Chiang (2005) from
the 6th version of the Europarl collection (Koehn,
2005) (German-English portion). We trained lan-
guage models using lmplz (Heafield et al, 2013)
and interpolated the models trained on the En-
glish monolingual data made available by the
WMT (Callison-Burch et al, 2012) (i.e. Eu-
roparl, newscommentaries, news-2012 and com-
moncrawl). Tuning was performed via MERT us-
ing newstest2010 as development set; test sen-
tences were extracted from newstest2011. Finally,
we restricted our SCFGs to having at most 10 tar-
get productions for a given source production.
Figure 3 shows some properties of the initial
grammar G(f) as a function of the input sentence
length (the quantities are averages over 20 sen-
tences for each class of input length). The number
of unigrams grows linearly with the input length,
while the number of unique bigrams compatible
with strings generated by G(f) appears to grow
quadratically9 and the size of the grammar in num-
ber of rules appears to be cubic ? a consequence
of having up to two nonterminals on the right-hand
side of a rule.
Figure 4 shows the number of refinement oper-
ations until convergence in optimisation and sam-
pling, as well as the total duration, as a function of
the input length.10 The plots will be discussed in
detail below.
4.1 Optimisation
In optimisation (Figures 4a and 4b), the number of
refinements up to convergence appears to be lin-
ear with the input length, while the total duration
grows much quicker. These findings are further
discussed in what follows.
Table 2 shows some important quantities regard-
ing optimisation with OS? using a 4-gram LM. The
first column shows how many sentences we are
considering, the second column shows the sentence
length, the third column m is the average num-
ber of refinements up to convergence. Column |A|
refers to the refinement type, which is the number
of states in the automaton A, that is, the order of
9The number of unique bigrams is an estimate obtained by
combining the terminals at the boundary of nonterminals that
may be adjacent in a derivation.
10The current implementation faces timeouts depending on
the length of the input sentence and the order of the language
model, explaining why certain curves are interrupted earlier
than others in Figure 4.
478
2 4 6 8 10
50
100
150
Input length
unigr
ams
l
l
l l
l
l
l l
l
l
(a) Unigrams in G(f)
2 4 6 8 10
0
2000
4000
6000
Input length
bigra
ms
l l
l l
l
l
l
l
l
l
(b) Bigrams compatible with G(f)
2 4 6 8 10
0
1000
2000
3000
4000
5000
Input length
R0
l l l
l
l
l
l
l
l
l
(c) Number of rules in G(f)
Figure 3: Properties of the initial grammar as function of input length
S Length m |A| count |Rf ||R0|9 4 45.0 2 20.3 74.6 ? 53.9
3 19.2
4 5.4
10 5 62.3 2 21.9 145.4 ? 162.6
3 32.9
4 7.5
9 6 102.8 2 34.7 535.8 ? 480.0
3 54.9
4 13.2
Table 2: Optimisation with a 4-gram LM.
the n-grams being re-weighted (e.g. |A| = 2 when
refining bigrams sharing a one-word context). Col-
umn count refers to the average number of refine-
ments that are due to each refinement type. Finally,
the last column compares the number of rules in the
final proposal to that of the initial one.
The first positive result concerns how much con-
text OS? needs to take into account for finding the
optimum derivation. Table 2 (column m) shows
that OS? explores a very reduced space of n-gram
contexts up to convergence. To illustrate that, con-
sider the last row in Table 2 (sentences with 6
words). On average, convergence requires incorpo-
rating only about 103 contexts of variable order, of
which 55 are bigram (2-word) contexts (remember
that |A| = 3 when accounting for a 2-word con-
text). According to Figure 3b, in sentences with
6 words, about 2,000 bigrams are compatible with
strings generated by G(f). This means that only
2.75% of these bigrams (55 out of 2,000) need to
be explicitly accounted for, illustrating how waste-
ful a full intersection would be.
A problem, however, is that the time until con-
vergence grows quickly with the length of the input
(Figure 4b). This can be explained as follows. At
each iteration the grammar is refined to account for
n-grams sharing a context of (n ? 1) words. That
S Input m |A| count |Rf ||R0|10 5 1.0 2 1.0 1.9 ? 1.0
10 6 6.6 2 6.3 17.6 ? 13.6
3 0.3
10 7 14.5 2 12.9 93.8 ? 68.9
3 1.5
4 0.1
Table 3: Sampling with a 4-gram LM and reaching
a 5% acceptance rate.
operation typically results in a larger grammar:
most rules are preserved, some rules are deleted,
but more importantly, some rules are added to ac-
count for the portion of the current grammar that
involves the selected n-grams. Enlarging the gram-
mar at each iteration means that successive refine-
ments become incrementally slower.
The histogram of refinement types of Table 2
highlights how efficient OS? is w.r.t. the space of
n-grams it needs to explore before convergence.
The problem is clearly not the number of refine-
ments, but rather the relation between the growth
of the grammar and the successive intersections.
Controlling for this growth and optimising the in-
tersection as to partially reuse previously computed
charts may be the key for a more generally tractable
solution.
4.2 Sampling
Figure 4c shows that sampling is more economi-
cal than optimisation in that it explicitly incorpo-
rates even fewer contexts. Note how OS? con-
verges to acceptance rates from 1% to 10% in much
fewer iterations than are necessary to find an opti-
mum11. Although the convergence in sampling is
11Currently we use MERT to train the model?s weight vec-
tor ? which is normalised by its L1 norm in the Moses im-
plementation. While optimisation is not sensitive to the scale
of the weights, in sampling the scale determines how flat or
479
2 2 2 2
2 2 2 2 2 2
2 4 6 8 10
0
20
40
60
80
100
Input length
Refi
nem
ents
3 3
3
3
3
3
3
4
4
4
4
4
4
234 2?gram LM3?gram LM4?gram LM
(a) Optimisation: number of refinements.
2 2 2 2 2 2 2 2
2 2
2 4 6 8 10
0
500
0
150
00
250
00
Input length
Tim
e (s)
3 3 3 3
3 3
3
4 4 4 4
4
4
234 2?gram LM3?gram LM4?gram LM
(b) Optimisation: time for convergence.
a a a a a a a a a a a a a a
2 4 6 8 10 12 14
0
5
10
15
20
25
Input length
Refin
emen
ts
b b b b b b b b b
b b b
b b
c c c c c c c
c
c c
c c
1 1 1 1 1 1 1
1
1
2 2 2 2 2 2
2
2
2
3 3 3 3 3
3 3
3
3
4 4 4 4
4
4
4
5 5 5
5
5
5
X X X X
X
X
Y Y Y
Y
Y
Y
abc12345XY
LM2 1%LM2 5%LM2 10%LM3 1%LM3 2%LM3 3%LM3 5%LM3 10%LM4 5%LM4 10%
(c) Sampling: number of refinements.
a a a a a a a a a a a a a
a
2 4 6 8 10 12 14
02
000
6000
1000
0
1400
0
Input length
Time
 (s)
b b b b b b b b b b b
b
b
b
c c c c c c c c c c
c
c
1 1 1 1 1 1 1 1
1
2 2 2 2 2 2 2 2
2
3 3 3 3 3 3 3
3
3
4 4 4 4 4 4
4
5 5 5 5 5 5X X X X X XY Y Y Y Y
Y
abc12345XY
LM2 1%LM2 5%LM2 10%LM3 1%LM3 2%LM3 3%LM3 5%LM3 10%LM4 5%LM4 10%
(d) Sampling: time for convergence.
Figure 4: Convergence for different LM order as function of the input length in optimisation (top) and
sampling (bottom). We show the number of refinements up to convergence on the left, and the convergence
time on the right. In optimisation we stop when the true optimum is found. In sampling we stop at different
acceptance rate levels: (a, b and c) use a 2-gram LM to reach 1, 5 and 10% AR; (1-4) use a 3-gram LM to
reach 2, 3, 5 and 10% AR; and (X, Y) use a 4-gram LM to reach 5 and 10% AR.
faster than in optimisation, the total duration is still
an issue (Figure 4b).
Table 3 shows the same quantities as Table 2, but
now for sampling. It is worth highlighting that even
though we are using an upper-bound over a 4-gram
LM (and aiming at a 5% acceptance rate), very few
contexts are selected for refinement, most of them
lower-order ones (one-word contexts ? rows with
|A| = 2).
Observe that an improved acceptance rate al-
ways leads to faster acquisition of exact samples
after we stop refining our proxy distribution. How-
ever, Figure 4d shows for example that moving
from 5% to 10% acceptance rate using a 4-gram
LM (curves X and Y) is time-consuming. Thus
there is a trade-off between how much time one
spends improving the acceptance rate and how
many exact samples one intends do draw. Figure
5 shows the average time to draw batches between
peaked the distribution is. Arun et al (2010) experiment with
scaling MERT-trained weights as to maximise BLEU on held-
out data, as well as with MBR training. A more adequate
training algorithm along similar lines is reserved for future
work.
1 1 1 1
1
1
1
1e+00 1e+02 1e+04 1e+06
200
500
1000
2000
5000
1000
0
Samples
Time
 (s)
2 2 2 2 2
2
212 5% AR10% AR
Figure 5: Average time to draw 1 to 1 million sam-
ples, for input sentences of length 6, using a 4-gram
LM at 5% (curve 1) and 10% (curve 2) acceptance
rate (including the time to produce the sampler).
one and one million samples from two exact sam-
plers that were refined up to 5% and 10% accep-
tance rate respectively. The sampler at 5% AR
(which is faster to obtain) turns out to be more effi-
cient if we aim at producing less than 10K samples.
Finally, note that samples are independently
480
drawn from the final proposal, making the ap-
proach an appealing candidate to parallelism in or-
der to increase the effective acceptance rate.
5 Related Work
Rush and Collins (2011) do not consider sampling,
but they address exact decoding for hierarchical
translation. They use a Dual Decomposition ap-
proach (a special case of Lagrangian Relaxation),
where the target CFG (hypergraph in their termi-
nology) component and the target language model
component ?trade-off? their weights so as to ensure
agreement on what each component believes to be
the maximum. In many cases, this technique is
able to detect the actual true maximum derivation.
When this is not the case, they use a finite-state-
based intersection mechanism to ?tighten? the first
component so that some constraints not satisfied by
the current solution are enforced, and iterate until
the true maximum is found or a time-out is met,
which results in a high proportion of finding the
true maximum.
Arun et al (2009, 2010) address the question
of sampling in a standard phrase-based transla-
tion model (Koehn et al, 2003). Contrarily to our
use of rejection sampling (a Monte-Carlo method),
they use a Gibbs sampler (a Markov-Chain Monte-
Carlo (MCMC) method). Samples are obtained
by iteratively re-sampling groups of well-designed
variables in such a way that (i) the sampler does not
tend to be trapped locally by high correlations be-
tween conditioning and conditioned variables, and
(ii) the combinatorial space of possibilities for the
next step is small enough so that conditional prob-
abilities can be computed explicitly. By contrast to
our exact approach, the samples obtained by Gibbs
sampling are not independent, but form a Markov
chain that only converges to the target distribution
in the limit, with convergence properties difficult
to assess. Also by contrast to us, these papers do
not address the question of finding the maximum
derivation directly, but only through finding a max-
imum among the derivations sampled so far, which
in principle can be quite different.
Blunsom and Osborne (2008) address proba-
bilistic inference, this time, as we do, in the context
of hierarchical translation, where sampling is used
both for the purposes of decoding and training the
model. When decoding in the presence of a lan-
guage model, an approximate sampling procedure
is performed in two stages. First, cube-pruning is
employed to construct a WCFG which generates
a subset of all the possible derivations that would
correspond to a full intersection with the target lan-
guage model. In a second step this grammar is
sampled through the same dynamic programming
procedure that we have described in ?2.4. By con-
trast to our approach, the paper does not attempt
to perform exact inference. However it does not
only address the question of decoding, but also that
of training the model, which requires, in addition
to sampling, an estimate of the model?s partition
function. In common with Arun et al (2010), the
authors stress the fact that a sampler of derivations
is also a sampler of translations as strings, while a
maximiser over derivations cannot be used to find
the maximum translation string.
6 Conclusions
The approach we have presented is, to our knowl-
edge, the first one to address the problem of ex-
act sampling for hierarchical translation and to do
that in a framework that also handles exact opti-
misation. Our experiments show that only a frac-
tion of the language model n-grams need to be in-
corporated in the target grammar in order to per-
form exact inference in this approach. However,
in the current implementation, we experience time-
outs for sentences of even moderate length. We are
working on improving this situation along three di-
mensions: (i) our implementation of the Earley In-
tersection rebuilds a grammar from scratch at each
intersection, while it could capitalise on the charts
built during the previous steps; (ii) the unigram-
level max-backoffs are not as tight as they could
be if one took into account more precisely the set
of contexts in which each word can appear rela-
tive to the grammar; (iii) most importantly, while
our refinements are ?local? in the sense of address-
ing one n-gram context at a time, they still affect
a large portion of the rules in the current grammar,
even rules that have very low probability of being
ever sampled by this grammar; by preventing re-
finement of such rules during the intersection pro-
cess, we may be able to make the intersection more
local and to produce much smaller grammars, with-
out losing the exactness properties of the approach.
Acknowledgements
The first author wishes to thank the PASCAL-2
Visit to Industry programme for partially funding
his visit to Xerox Research Centre Europe last Fall,
which initiated this collaboration.
481
References
Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blun-
som, Adam Lopez, and Philipp Koehn. 2009. Monte
carlo inference and maximization for phrase-based
translation. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning,
CoNLL ?09, pages 102?110, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Abhishek Arun, Barry Haddow, Philipp Koehn, Adam
Lopez, Chris Dyer, and Phil Blunsom. 2010. Monte
carlo techniques for phrase-based translation. Ma-
chine Translation, 24(2):103?121, June.
Yehoshua Bar-Hillel, Micha A. Perles, and Eli Shamir.
1961. On formal properties of simple phrase struc-
ture grammars. Zeitschrift fu?r Phonetik, Sprachwis-
senschaft und Kommunikationsforschung, (14):143?
172.
Sylvie Billot and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 27th Annual Meeting of the Association
for Computational Linguistics, pages 143?151, Van-
couver, British Columbia, Canada, June. Association
for Computational Linguistics.
Phil Blunsom and Miles Osborne. 2008. Probabilis-
tic inference for machine translation. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ?08, pages 215?
223, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10?
51, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Simon Carter, Marc Dymetman, and Guillaume
Bouchard. 2012. Exact Sampling and Decoding in
High-Order Hidden Markov Models. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1125?1134, Jeju
Island, Korea, July. Association for Computational
Linguistics.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ?05, pages 263?
270, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, 33:201?228.
Thomas H. Cormen, Clifford Stein, Ronald L. Rivest,
and Charles E. Leiserson. 2001. Introduction to Al-
gorithms. McGraw-Hill Higher Education, 2nd edi-
tion.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-
itkevitch, Phil Blunsom, and Philip Resnik. 2010.
cdec: a decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, ACLDemos ?10, pages 7?12, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Christopher Dyer. 2010. A Formal Model of Ambiguity
and its Applications in Machine Translation. Ph.D.
thesis, University of Maryland.
M. Dymetman, G. Bouchard, and S. Carter. 2012a. The
OS* Algorithm: a Joint Approach to Exact Optimiza-
tion and Sampling. ArXiv e-prints, July.
Marc Dymetman, Guillaume Bouchard, and Simon
Carter. 2012b. Optimization and sampling for nlp
from a unified viewpoint. In Proceedings of the
First International Workshop on Optimization Tech-
niques for Human Language Technology, pages 79?
94, Mumbai, India, December. The COLING 2012
Organizing Committee.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modified
Kneser-Ney language model estimation. In Proceed-
ings of the 51st Annual Meeting of the Association for
Computational Linguistics, Sofia, Bulgaria, August.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144?151, Prague, Czech Republic, June. Association
for Computational Linguistics.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
139?146, Rochester, New York, April. Association
for Computational Linguistics.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Natu-
ral Language Processing, Computational Linguistics
and Speech Recognition (Prentice Hall Series in Ar-
tificial Intelligence). Prentice Hall, 1 edition.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
482
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings of
Machine Translation Summit, pages 79?86.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes Risk Decoding for Statistical Machine
Translation. In Joint Conference of Human Lan-
guage Technologies and the North American chap-
ter of the Association for Computational Linguistics
(HLT-NAACL 2004).
Mark-Jan Nederhof and Giorgio Satta. 2008. Proba-
bilistic parsing. In M. Dolores Jimnez-Lpez G. Bel-
Enguix and C. Martn-Vide, editors, New Develop-
ments in Formal Languages and Applications, Stud-
ies in Computational Intelligence, volume 113, pages
229?258. Springer.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, volume 1 of ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods (Springer Texts in Statis-
tics). Springer-Verlag New York, Inc., Secaucus, NJ,
USA.
Walter Rudin. 1987. Real and Complex Analysis.
McGraw-Hill.
Alexander M. Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 72?82, Stroudsburg, PA,
USA. Association for Computational Linguistics.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, COLING-ACL ?06, pages 787?794,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 764?773, Prague, Czech Republic,
June. Association for Computational Linguistics.
483
