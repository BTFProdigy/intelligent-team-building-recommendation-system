Proceedings of the 12th Conference of the European Chapter of the ACL, pages 603?611,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Deterministic shift-reduce parsing for unification-based grammars by 
using default unification 
 
 
Takashi Ninomiya 
Information Technology Center 
University of Tokyo, Japan 
ninomi@r.dl.itc.u-tokyo.ac.jp 
Takuya Matsuzaki 
Department of Computer Science 
University of Tokyo, Japan 
matuzaki@is.s.u-tokyo.ac.jp 
  
Nobuyuki Shimizu 
Information Technology Center 
University of Tokyo, Japan 
shimizu@r.dl.itc.u-tokyo.ac.jp 
Hiroshi Nakagawa 
Information Technology Center 
University of Tokyo, Japan 
nakagawa@dl.itc.u-tokyo.ac.jp
 
 
Abstract 
Many parsing techniques including pa-
rameter estimation assume the use of a 
packed parse forest for efficient and ac-
curate parsing.  However, they have sev-
eral inherent problems deriving from the 
restriction of locality in the packed parse 
forest.  Deterministic parsing is one of 
solutions that can achieve simple and fast 
parsing without the mechanisms of the 
packed parse forest by accurately choos-
ing search paths.  We propose (i) deter-
ministic shift-reduce parsing for unifica-
tion-based grammars, and (ii) best-first 
shift-reduce parsing with beam threshold-
ing for unification-based grammars.  De-
terministic parsing cannot simply be ap-
plied to unification-based grammar pars-
ing, which often fails because of its hard 
constraints.  Therefore, it is developed by 
using default unification, which almost 
always succeeds in unification by over-
writing inconsistent constraints in gram-
mars. 
1 Introduction 
Over the last few decades, probabilistic unifica-
tion-based grammar parsing has been investi-
gated intensively.  Previous studies (Abney, 
1997; Johnson et al, 1999; Kaplan et al, 2004; 
Malouf and van Noord, 2004; Miyao and Tsujii, 
2005; Riezler et al, 2000) defined a probabilistic 
model of unification-based grammars, including 
head-driven phrase structure grammar (HPSG), 
lexical functional grammar (LFG) and combina-
tory categorial grammar (CCG), as a maximum 
entropy model (Berger et al, 1996).  Geman and 
Johnson (Geman and Johnson, 2002) and Miyao 
and Tsujii (Miyao and Tsujii, 2002) proposed a 
feature forest, which is a dynamic programming 
algorithm for estimating the probabilities of all 
possible parse candidates.  A feature forest can 
estimate the model parameters without unpack-
ing the parse forest, i.e., the chart and its edges.  
Feature forests have been used successfully 
for probabilistic HPSG and CCG (Clark and Cur-
ran, 2004b; Miyao and Tsujii, 2005), and its 
parsing is empirically known to be fast and accu-
rate, especially with supertagging (Clark and 
Curran, 2004a; Ninomiya et al, 2007; Ninomiya 
et al, 2006).  Both estimation and parsing with 
the packed parse forest, however, have several 
inherent problems deriving from the restriction 
of locality.  First, feature functions can be de-
fined only for local structures, which limit the 
parser?s performance.  This is because parsers 
segment parse trees into constituents and factor 
equivalent constituents into a single constituent 
(edge) in a chart to avoid the same calculation.  
This also means that the semantic structures must 
be segmented.  This is a crucial problem when 
we think of designing semantic structures other 
than predicate argument structures, e.g., syn-
chronous grammars for machine translation.  The 
size of the constituents will be exponential if the 
semantic structures are not segmented.  Lastly, 
we need delayed evaluation for evaluating fea-
ture functions.  The application of feature func-
tions must be delayed until all the values in the 
603
segmented constituents are instantiated.  This is 
because values in parse trees can propagate any-
where throughout the parse tree by unification.  
For example, values may propagate from the root 
node to terminal nodes, and the final form of the 
terminal nodes is unknown until the parser fi-
nishes constructing the whole parse tree.  Conse-
quently, the design of grammars, semantic struc-
tures, and feature functions becomes complex.  
To solve the problem of locality, several ap-
proaches, such as reranking (Charniak and John-
son, 2005), shift-reduce parsing (Yamada and 
Matsumoto, 2003), search optimization learning 
(Daum? and Marcu, 2005) and sampling me-
thods (Malouf and van Noord, 2004; Nakagawa, 
2007), were studied. 
In this paper, we investigate shift-reduce pars-
ing approach for unification-based grammars 
without the mechanisms of the packed parse for-
est.  Shift-reduce parsing for CFG and dependen-
cy parsing have recently been studied (Nivre and 
Scholz, 2004; Ratnaparkhi, 1997; Sagae and La-
vie, 2005, 2006; Yamada and Matsumoto, 2003), 
through approaches based essentially on deter-
ministic parsing.  These techniques, however, 
cannot simply be applied to unification-based 
grammar parsing because it can fail as a result of 
its hard constraints in the grammar.  Therefore, 
in this study, we propose deterministic parsing 
for unification-based grammars by using default 
unification, which almost always succeeds in 
unification by overwriting inconsistent con-
straints in the grammars.  We further pursue 
best-first shift-reduce parsing for unification-
based grammars. 
Sections 2 and 3 explain unification-based 
grammars and default unification, respectively.  
Shift-reduce parsing for unification-based gram-
mars is presented in Section 4.  Section 5 dis-
cusses our experiments, and Section 6 concludes 
the paper. 
2 Unification-based grammars 
A unification-based grammar is defined as a pair 
consisting of a set of lexical entries and a set of 
phrase-structure rules.  The lexical entries ex-
press word-specific characteristics, while the 
phrase-structure rules describe constructions of 
constituents in parse trees.  Both the phrase-
structure rules and the lexical entries are 
represented by feature structures (Carpenter, 
1992), and constraints in the grammar are forced 
by unification.  Among the phrase-structure rules, 
a binary rule is a partial function: ? ? ? ? ? , 
where ? is the set of all possible feature struc-
tures.  The binary rule takes two partial parse 
trees as daughters and returns a larger partial 
parse tree that consists of the daughters and their 
mother.  A unary rule is a partial function: 
? ? ?, which corresponds to a unary branch. 
In the experiments, we used an HPSG (Pollard 
and Sag, 1994), which is one of the sophisticated 
unification-based grammars in linguistics.  Gen-
erally, an HPSG has a small number of phrase-
structure rules and a large number of lexical en-
tries.  Figure 1 shows an example of HPSG pars-
ing of the sentence, ?Spring has come.?  The up-
per part of the figure shows a partial parse tree 
for ?has come,? which is obtained by unifying 
each of the lexical entries for ?has? and ?come? 
with a daughter feature structure of the head-
complement rule.  Larger partial parse trees are 
obtained by repeatedly applying phrase-structure 
rules to lexical/phrasal partial parse trees.  Final-
ly, the parse result is output as a parse tree that 
dominates the sentence. 
3 Default unification 
Default unification was originally investigated in 
a series of studies of lexical semantics, in order 
to deal with default inheritance in a lexicon.  It is 
also desirable, however, for robust processing, 
because (i) it almost always succeeds and (ii) a 
feature structure is relaxed such that the amount 
of information is maximized (Ninomiya et al, 
2002).  In our experiments, we tested a simpli-
fied version of Copestake?s default unification.  
Before explaining it, we first explain Carpenter?s 
 
Figure 1: Example of HPSG parsing. 
 
HEAD  noun
SUBJ  <>
COMPS <>
HEAD  verb
HEAD  noun
SUBJ  <      SUBJ  <>     >
COMPS <>
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <   >
HEAD  verb
SUBJ  <   >
COMPS <>
head-comp
Spring has come
1
1 12
2
HEAD  verb
SUBJ  <>
COMPS <>
HEAD  noun
SUBJ  <>
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <   >
HEAD  verb
SUBJ  <   >
COMPS <>
subject-head
head-comp
Spring has come
1
1 11 2
2
604
two definitions of default unification (Carpenter, 
1993). 
 
(Credulous Default Unification) 
? ?? ? ? =  ?? ? ????
? ? ? is maximal such
that ? ? ? ?is defined ? 
 
(Skeptical Default Unification) 
? ?? ? ? =  ?(? ?
?
? ?) 
 
?  is called a strict feature structure, whose in-
formation must not be lost, and ? is called a de-
fault feature structure, whose information can be 
lost but as little as possible so that ? and ? can 
be unified. 
Credulous default unification is greedy, in that 
it tries to maximize the amount of information 
from the default feature structure, but it results in 
a set of feature structures.  Skeptical default un-
ification simply generalizes the set of feature 
structures resulting from credulous default unifi-
cation.  Skeptical default unification thus leads to 
a unique result so that the default information 
that can be found in every result of credulous 
default unification remains.  The following is an 
example of skeptical default unification: 
 
[F: ?]  ?? ? ?
F: 1 ?
G: 1
H: ?
? =  ???
F: ?
G: ?
H: ?
? , ?
F: 1 ?
G: 1
H: ?
?? = ?
F: ?
G: ?
H: ?
?. 
 
Copestake mentioned that the problem with 
Carpenter?s default unification is its time com-
plexity (Copestake, 1993).  Carpenter?s default 
unification takes exponential time to find the op-
timal answer, because it requires checking the 
unifiability of the power set of constraints in a 
default feature structure.  Copestake thus pro-
posed another definition of default unification, as 
follows. Let ??(?) be a function that returns a 
set of path values in ?, and let ??(?) be a func-
tion that returns a set of path equations, i.e., in-
formation about structure sharing in ?. 
 
(Copestake?s default unification) 
? ?? ? ? =  ? ? ? ???
? ? ??(?)and there is no ?? ? ??(?)
such that ? ? ??is defined and
? ? ? ? ??is not defined
?, 
where ? = ? ? ???(?). 
 
Copestake?s default unification works effi-
ciently because all path equations in the default 
feature structure are unified with the strict fea-
ture structures, and because the unifiability of 
path values is checked one by one for each node 
in the result of unifying the path equations.  The 
implementation is almost the same as that of 
normal unification, but each node of a feature 
structure has a set of values marked as ?strict? or 
?default.?  When types are involved, however, it 
is not easy to find unifiable path values in the 
default feature structure.  Therefore, we imple-
mented a more simply typed version of Corpes-
take?s default unification. 
Figure 2 shows the algorithm by which we 
implemented the simply typed version.  First, 
each node is marked as ?strict? if it belongs to a 
strict feature structure and as ?default? otherwise. 
The marked strict and default feature structures 
procedure forced_unification(p, q) 
   queue := {?p, q?}; 
   while( queue is not empty ) 
      ?p, q? := shift(queue); 
      p := deref(p); q := deref(q); 
      if p ? q 
         ?(p) ?  ?(p) ? ?(q); 
         ?(q) ? ptr(p); 
         forall f ? feat(p)? feat(q) 
            if f ? feat(p) ? f ? feat(q) 
               queue := queue ? ??(f, p), ?(f, q)?; 
            if f ? feat(p) ? f ? feat(q) 
               ?(f, p) ?  ?(f, q); 
procedure mark(p, m) 
   p := deref(p); 
   if p has not been visited 
      ?(p) := {??(p),m?}; 
      forall f ? feat(p) 
         mark(?(f, p), m); 
procedure collapse_defaults(p) 
   p := deref(p); 
   if p has not been visited 
      ts := ?; td := ?; 
      forall ?t, ??????? ? ?(p) 
         ts := ts ? t; 
      forall ?t, ???????? ? ?(p) 
         td := td ? t; 
      if ts is not defined 
         return false; 
      if ts ? td is defined 
         ?(p) := ts ? td; 
      else 
         ?(p) := ts; 
      forall f ? feat(p) 
         collapse_defaults(?(f, p)); 
procedure default_unification(p, q) 
   mark(p, ??????); 
   mark(q, ???????); 
   forced_unification(p, q); 
   collapse_defaults(p); 
 
?(p) is (i) a single type, (ii) a pointer, or (iii) a set of pairs of 
types and markers in the feature structure node p. 
A marker indicates that the types in a feature structure node 
originally belong to the strict feature structures or the default 
feature structures. 
A pointer indicates that the node has been unified with other 
nodes and it points the unified node.  A function deref tra-
verses pointer nodes until it reaches to non-pointer node. 
?(f, p) returns a feature structure node which is reached by 
following a feature f from p. 
 
Figure 2: Algorithm for the simply typed ver-
sion of Corpestake?s default unification. 
605
are unified, whereas the types in the feature 
structure nodes are not unified but merged as a 
set of types.  Then, all types marked as ?strict? 
are unified into one type for each node.  If this 
fails, the default unification also returns unifica-
tion failure as its result.  Finally, each node is 
assigned a single type, which is the result of type 
unification for all types marked as both ?default? 
and ?strict? if it succeeds or all types marked 
only as ?strict? otherwise. 
4 Shift-reduce parsing for unification-
based grammars 
Non-deterministic shift-reduce parsing for unifi-
cation-based grammars has been studied by Bris-
coe and Carroll (Briscoe and Carroll, 1993).  
Their algorithm works non-deterministically with 
the mechanism of the packed parse forest, and 
hence it has the problem of locality in the packed 
parse forest.  This section explains our shift-
reduce parsing algorithms, which are based on 
deterministic shift-reduce CFG parsing (Sagae 
and Lavie, 2005) and best-first shift-reduce CFG 
parsing (Sagae and Lavie, 2006).  Sagae?s parser 
selects the most probable shift/reduce actions and 
non-terminal symbols without assuming explicit 
CFG rules.  Therefore, his parser can proceed 
deterministically without failure.  However, in 
the case of unification-based grammars, a deter-
ministic parser can fail as a result of its hard con-
straints in the grammar.  We propose two new 
shift-reduce parsing approaches for unification-
based grammars: deterministic shift-reduce pars-
ing and shift-reduce parsing by backtracking and 
beam search.  The major difference between our 
algorithm and Sagae?s algorithm is that we use 
default unification.  First, we explain the deter-
ministic shift-reduce parsing algorithm, and then 
we explain the shift-reduce parsing with back-
tracking and beam search. 
4.1 Deterministic shift-reduce parsing for 
unification-based grammars 
The deterministic shift-reduce parsing algorithm 
for unification-based grammars mainly compris-
es two data structures: a stack S, and a queue W.  
Items in S are partial parse trees, including a lex-
ical entry and a parse tree that dominates the 
whole input sentence.  Items in W are words and 
POSs in the input sentence.  The algorithm de-
fines two types of parser actions, shift and reduce, 
as follows. 
? Shift: A shift action removes the first item 
(a word and a POS) from W.  Then, one 
lexical entry is selected from among the 
candidate lexical entries for the item.  Fi-
nally, the selected lexical entry is put on 
the top of the stack. 
Common features: Sw(i), Sp(i), Shw(i), Shp(i), Snw(i), Snp(i), 
Ssy(i), Shsy(i), Snsy(i), wi-1, wi,wi+1, pi-2, pi-1, pi, pi+1, 
pi+2, pi+3 
Binary reduce features: d, c, spl, syl, hwl, hpl, hll, spr, syr, 
hwr, hpr, hlr 
Unary reduce features: sy, hw, hp, hl 
 
Sw(i) ? head word of i-th item from the top of the stack 
Sp(i) ? head POS of i-th item from the top of the stack 
Shw(i) ? head word of the head daughter of i-th item from the 
top of the stack 
Shp(i) ? head POS of the head daughter of i-th item from the 
top of the stack 
Snw(i) ? head word of the non-head daughter of i-th item 
from the top of the stack 
Snp(i) ? head POS of the non-head daughter of i-th item from 
the top of the stack 
Ssy(i) ? symbol of phrase category of the i-th item from the 
top of the stack 
Shsy(i) ? symbol of phrase category of the head daughter of 
the i-th item from the top of the stack 
Snsy(i) ? symbol of phrase category of the non-head daughter 
of the i-th item from the top of the stack 
d ? distance between head words of daughters 
c ? whether a comma exists between daughters and/or inside 
daughter phrases 
sp ? the number of words dominated by the phrase 
sy ? symbol of phrase category 
hw ? head word 
hp ? head POS 
hl ? head lexical entry 
 
Figure 3: Feature templates. 
Shift Features 
  [Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [wi-1, wi] [wi, wi+1] [pi-1, 
wi] [pi, wi] [pi+1, wi] [pi, pi+1, pi+2, pi+3] [pi-2, pi-1, pi] 
[pi-1, pi, pi+1] [pi, pi+1, pi+2] [pi-2, pi-1] [pi-1, pi] [pi, 
pi+1] [pi+1, pi+2] 
 
Binary Reduce Features 
[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [d,c,hw,hp,hl] [d,c,hw,hp] [d, 
c, hw, hl] [d, c, sy, hw] [c, sp, hw, hp, hl] [c, sp, hw, hp] [c, 
sp, hw,hl] [c, sp, sy, hw] [d, c, hp, hl] [d, c, hp] [d, c, hl] [d, 
c, sy] [c, sp, hp, hl] [c, sp, hp] [c, sp, hl] [c, sp, sy] 
 
Unary Reduce Features 
[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [hw, hp, hl] [hw, hp] [hw, hl] 
[sy, hw] [hp, hl] [hp] [hl] [sy]
 
Figure 4: Combinations of feature templates. 
606
? Binary Reduce: A binary reduce action 
removes two items from the top of the 
stack.  Then, partial parse trees are derived 
by applying binary rules to the first re-
moved item and the second removed item 
as a right daughter and left daughter, re-
spectively.  Among the candidate partial 
parse trees, one is selected and put on the 
top of the stack. 
? Unary Reduce: A unary reduce action re-
moves one item from the top of the stack.  
Then, partial parse trees are derived by 
applying unary rules to the removed item.  
Among the candidate partial parse trees, 
one is selected and put on the top of the 
stack. 
Parsing fails if there is no candidate for selec-
tion (i.e., a dead end).  Parsing is considered suc-
cessfully finished when W is empty and S has 
only one item which satisfies the sentential con-
dition: the category is verb and the subcategori-
zation frame is empty.  Parsing is considered a 
non-sentential success when W is empty and S 
has only one item but it does not satisfy the sen-
tential condition. 
In our experiments, we used a maximum en-
tropy classifier to choose the parser?s action.  
Figure 3 lists the feature templates for the clas-
sifier, and Figure 4 lists the combinations of fea-
ture templates.  Many of these features were tak-
en from those listed in (Ninomiya et al, 2007), 
(Miyao and Tsujii, 2005) and (Sagae and Lavie, 
2005), including global features defined over the 
information in the stack, which cannot be used in 
parsing with the packed parse forest.  The fea-
tures for selecting shift actions are the same as 
the features used in the supertagger (Ninomiya et 
al., 2007).  Our shift-reduce parsers can be re-
garded as an extension of the supertagger. 
The deterministic parsing can fail because of 
its grammar?s hard constraints.  So, we use de-
fault unification, which almost always succeeds 
in unification.  We assume that a head daughter 
(or, an important daughter) is determined for 
each binary rule in the unification-based gram-
mar.   Default unification is used in the binary 
rule application in the same way as used in Ni-
nomiya?s offline robust parsing (Ninomiya et al, 
2002), in which a binary rule unified with the 
head daughter is the strict feature structure and 
the non-head daughter is the default feature 
structure, i.e.,  (? ? ?) ?? ??, where R is a bi-
nary rule, H is a head daughter and NH is a non-
head daughter.  In the experiments, we used the 
simply typed version of Copestake?s default un-
ification in the binary rule application1.  Note 
that default unification was always used instead 
of normal unification in both training and evalua-
tion in the case of the parsers using default unifi-
cation.  Although Copestake?s default unification 
almost always succeeds, the binary rule applica-
tion can fail if the binary rule cannot be unified 
with the head daughter, or inconsistency is 
caused by path equations in the default feature 
structures.  If the rule application fails for all the 
binary rules, backtracking or beam search can be 
used for its recovery as explained in Section 4.2.  
In the experiments, we had no failure in the bi-
nary rule application with default unification. 
4.2 Shift-reduce parsing by backtracking 
and beam-search 
Another approach for recovering from the pars-
ing failure is backtracking.  When parsing fails 
or ends with non-sentential success, the parser?s 
state goes back to some old state (backtracking), 
and it chooses the second best action and tries 
parsing again.  The old state is selected so as to 
minimize the difference in the probabilities for 
selecting the best candidate and the second best 
candidate.  We define a maximum number of 
backtracking steps while parsing a sentence.  
Backtracking repeats until parsing finishes with 
sentential success or reaches the maximum num-
ber of backtracking steps.  If parsing fails to find 
a parse tree, the best continuous partial parse 
trees are output for evaluation. 
From the viewpoint of search algorithms, pars-
ing with backtracking is a sort of depth-first 
search algorithms.  Another possibility is to use 
the best-first search algorithm.  The best-first 
parser has a state priority queue, and each state 
consists of a tree stack and a word queue, which 
are the same stack and queue explained in the 
shift-reduce parsing algorithm.  Parsing proceeds 
by applying shift-reduce actions to the best state 
in the state queue.  First, the best state is re-
                                                 
1 We also implemented Ninomiya?s default unification, 
which can weaken path equation constraints.  In the prelim-
inary experiments, we tested binary rule application given 
as (? ? ?) ?? ?? with Copestake?s default unification, 
(? ? ?) ?? ?? with Ninomiya?s default unification, and 
(? ? ??) ?? ? with Ninomiya?s default unification.  How-
ever, there was no significant difference of F-score among 
these three methods.  So, in the main experiments, we only 
tested (? ? ?) ?? ?? with Copestake?s default unification 
because this method is simple and stable. 
607
moved from the state queue, and then shift-
reduce actions are applied to the state.  The new-
ly generated states as results of the shift-reduce 
actions are put on the queue.  This process re-
peats until it generates a state satisfying the sen-
tential condition.  We define the probability of a 
parsing state as the product of the probabilities of 
selecting actions that have been taken to reach 
the state.  We regard the state probability as the 
objective function in the best-first search algo-
rithm, i.e., the state with the highest probabilities 
is always chosen in the algorithm.  However, the 
best-first algorithm with this objective function 
searches like the breadth-first search, and hence, 
parsing is very slow or cannot be processed in a 
reasonable time.  So, we introduce beam thre-
sholding to the best-first algorithm.  The search 
space is pruned by only adding a new state to the 
state queue if its probability is greater than 1/b of 
the probability of the best state in the states that 
has had the same number of shift-reduce actions.  
In what follows, we call this algorithm beam 
search parsing. 
In the experiments, we tested both backtrack-
ing and beam search with/without default unifi-
cation.  Note that, the beam search parsing for 
unification-based grammars is very slow com-
pared to the shift-reduce CFG parsing with beam 
search.  This is because we have to copy parse 
trees, which consist of a large feature structures, 
in every step of searching to keep many states on 
the state queue.  In the case of backtracking, co-
pying is not necessary. 
5 Experiments 
We evaluated the speed and accuracy of parsing 
with Enju 2.3?, an HPSG for English (Miyao and 
Tsujii, 2005).  The lexicon for the grammar was 
extracted from Sections 02-21 of the Penn Tree-
bank (39,832 sentences).  The grammar consisted 
of 2,302 lexical entries for 11,187 words.  Two 
probabilistic classifiers for selecting shift-reduce 
actions were trained using the same portion of 
the treebank.  One is trained using normal unifi-
cation, and the other is trained using default un-
ification. 
We measured the accuracy of the predicate ar-
gument relation output of the parser.  A predi-
cate-argument relation is defined as a tuple 
??, ??, ?, ???, where ? is the predicate type (e.g., 
  Section 23 (Gold POS) 
  LP 
(%) 
LR 
(%) 
LF 
(%) 
Avg. 
Time 
(ms) 
# of 
backtrack
Avg. #
of 
states 
# of 
dead 
end 
# of non- 
sentential 
success 
# of 
sentential
success 
Previous 
studies 
(Miyao and Tsujii, 2005) 87.26 86.50 86.88 604 - - - - - 
(Ninomiya et al, 2007) 89.78 89.28 89.53 234 - - - - - 
Ours 
det 76.45 82.00 79.13 122 0 - 867 35 1514 
det+du 87.78 87.45 87.61 256 0 - 0 117 2299 
back40 81.93 85.31 83.59 519 18986 - 386 23 2007 
back10 + du 87.79 87.46 87.62 267 574 - 0 45 2371 
beam(7.4) 86.17 87.77 86.96 510 - 226 369 30 2017 
beam(20.1)+du 88.67 88.79 88.48 457 - 205 0 16 2400 
beam(403.4) 89.98 89.92 89.95 10246 - 2822 71 14 2331 
           
  Section 23 (Auto POS) 
  LP 
(%) 
LR 
(%) 
LF 
(%) 
Avg. 
Time 
(ms) 
# of 
backtrack
Avg. #
of 
states 
# of 
dead 
end 
# of non 
sentential 
success 
# of 
sentential
success 
Previous 
studies 
(Miyao and Tsujii, 2005) 84.96 84.25 84.60 674 - - - - - 
(Ninomiya et al, 2007) 87.28 87.05 87.17 260 - - - - - 
(Matsuzaki et al, 2007)  86.93 86.47 86.70 30 - - - - - 
(Sagae et al, 2007)  88.50 88.00 88.20 - - - - - - 
Ours 
det 74.13 80.02 76.96 127 0 - 909 31 1476 
det+du 85.93 85.72 85.82 252 0 - 0 124 2292 
back40 78.71 82.86 80.73 568 21068 - 438 27 1951 
back10 + du 85.96 85.75 85.85 270 589 - 0 46 2370 
beam(7.4) 83.84 85.82 84.82 544 - 234 421 33 1962 
beam(20.1)+du 86.59 86.36 86.48 550 - 222 0 21 2395 
beam(403.4) 87.70 87.86 87.78 16822 - 4553 89 16 2311 
 
Table 1: Experimental results for Section 23. 
608
adjective, intransitive verb), ?? is the head word 
of the predicate, ? is the argument label (MOD-
ARG, ARG1, ?, ARG4), and ??  is the head 
word of the argument.  The labeled precision 
(LP) / labeled recall (LR) is the ratio of tuples 
correctly identified by the parser, and the labeled 
F-score (LF) is the harmonic mean of the LP and 
LR. This evaluation scheme was the same one 
used in previous evaluations of lexicalized 
grammars (Clark and Curran, 2004b; Hocken-
maier, 2003; Miyao and Tsujii, 2005).  The expe-
riments were conducted on an Intel Xeon 5160 
server with 3.0-GHz CPUs. Section 22 of the 
Penn Treebank was used as the development set, 
and the performance was evaluated using sen-
tences of ? 100 words in Section 23.  The LP, 
LR, and LF were evaluated for Section 23. 
Table 1 lists the results of parsing for Section 
23.  In the table, ?Avg. time? is the average pars-
ing time for the tested sentences.  ?# of backtrack? 
is the total number of backtracking steps that oc-
curred during parsing.  ?Avg. # of states? is the 
average number of states for the tested sentences.  
?# of dead end? is the number of sentences for 
which parsing failed.  ?# of non-sentential suc-
cess? is the number of sentences for which pars-
ing succeeded but did not generate a parse tree 
satisfying the sentential condition.  ?det? means 
the deterministic shift-reduce parsing proposed 
in this paper.  ?back?? means shift-reduce pars-
ing with backtracking at most ? times for each 
sentence.  ?du? indicates that default unification 
was used.  ?beam?? means best-first shift-reduce 
parsing with beam threshold ?.  The upper half 
of the table gives the results obtained using gold 
POSs, while the lower half gives the results ob-
tained using an automatic POS tagger.  The max-
imum number of backtracking steps and the 
beam threshold were determined by observing 
the performance for the development set (Section 
22) such that the LF was maximized with a pars-
ing time of less than 500 ms/sentence (except 
?beam(403.4)?). The performance of 
?beam(403.4)? was evaluated to see the limit of 
the performance of the beam-search parsing. 
Deterministic parsing without default unifica-
tion achieved accuracy with an LF of around 
79.1% (Section 23, gold POS).  With backtrack-
ing, the LF increased to 83.6%.  Figure 5 shows 
the relation between LF and parsing time for the 
development set (Section 22, gold POS).  As 
seen in the figure, the LF increased as the parsing 
time increased.  The increase in LF for determi-
nistic parsing without default unification, how-
ever, seems to have saturated around 83.3%.  
Table 1 also shows that deterministic parsing 
with default unification achieved higher accuracy, 
with an LF of around 87.6% (Section 23, gold 
POS), without backtracking.  Default unification 
is effective: it ran faster and achieved higher ac-
curacy than deterministic parsing with normal 
unification.  The beam-search parsing without 
default unification achieved high accuracy, with 
an LF of around 87.0%, but is still worse than 
deterministic parsing with default unification.  
However, with default unification, it achieved 
the best performance, with an LF of around 
88.5%, in the settings of parsing time less than 
500ms/sentence for Section 22. 
For comparison with previous studies using 
the packed parse forest, the performances of 
Miyao?s parser, Ninomiya?s parser, Matsuzaki?s 
parser and Sagae?s parser are also listed in Table 
1.  Miyao?s parser is based on a probabilistic 
model estimated only by a feature forest.  Nino-
miya?s parser is a mixture of the feature forest 
 
Figure 5: The relation between LF and the average parsing time (Section 22, Gold POS). 
 
82.00%
83.00%
84.00%
85.00%
86.00%
87.00%
88.00%
89.00%
90.00%
0 1 2 3 4 5 6 7 8
LF
Avg. parsing time (s/sentence)
back
back+du
beam
beam+du
609
and an HPSG supertagger.  Matsuzaki?s parser 
uses an HPSG supertagger and CFG filtering.  
Sagae?s parser is a hybrid parser with a shallow 
dependency parser.  Though parsing without the 
packed parse forest is disadvantageous to the 
parsing with the packed parse forest in terms of 
search space complexity, our model achieved 
higher accuracy than Miyao?s parser. 
?beam(403.4)? in Table 1 and ?beam? in Fig-
ure 5 show possibilities of beam-search parsing.  
?beam(403.4)? was very slow, but the accuracy 
was higher than any other parsers except Sagae?s 
parser. 
Table 2 shows the behaviors of default unifi-
cation for ?det+du.?  The table shows the 20 
most frequent path values that were overwritten 
by default unification in Section 22.  In most of 
the cases, the overwritten path values were in the 
selection features, i.e., subcategorization frames 
(COMPS:, SUBJ:, SPR:, CONJ:) and modifiee 
specification (MOD:).  The column of ?Default 
type? indicates the default types which were 
overwritten by the strict types in the column of 
?Strict type,? and the last column is the frequency 
of overwriting.  ?cons? means a non-empty list, 
and ?nil? means an empty list.  In most of the 
cases, modifiee and subcategorization frames 
were changed from empty to non-empty and vice 
versa.  From the table, overwriting of head in-
formation was also observed, e.g., ?noun? was 
changed to ?verb.? 
6 Conclusion and Future Work 
We have presented shift-reduce parsing approach 
for unification-based grammars, based on deter-
ministic shift-reduce parsing.  First, we presented 
deterministic parsing for unification-based 
grammars.  Deterministic parsing was difficult in 
the framework of unification-based grammar 
parsing, which often fails because of its hard 
constraints.  We introduced default unification to 
avoid the parsing failure.  Our experimental re-
sults have demonstrated the effectiveness of de-
terministic parsing with default unification.  The 
experiments revealed that deterministic parsing 
with default unification achieved high accuracy, 
with a labeled F-score (LF) of 87.6% for Section 
23 of the Penn Treebank with gold POSs.  
Second, we also presented the best-first parsing 
with beam search for unification-based gram-
mars.  The best-first parsing with beam search 
achieved the best accuracy, with an LF of 87.0%, 
in the settings without default unification.  De-
fault unification further increased LF from 
87.0% to 88.5%.  By widening the beam width, 
the best-first parsing achieved an LF of 90.0%. 
References 
Abney, Steven P. 1997. Stochastic Attribute-Value 
Grammars. Computational Linguistics, 23(4), 597-
618. 
Path Strict 
type 
Default 
type 
Freq
SYNSEM:LOCAL:CAT:HEAD:MOD: cons nil 434
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD:MOD: cons nil 237
SYNSEM:LOCAL:CAT:VAL:SUBJ: nil cons 231
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SUBJ: nil cons 125
SYNSEM:LOCAL:CAT:HEAD: verb noun 110
SYNSEM:LOCAL:CAT:VAL:SPR:hd:LOCAL:CAT:VAL:SPEC:hd:LOCAL:CAT: 
HEAD:MOD: 
cons nil 101
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SPR:hd:LOCAL:CAT:VAL:SPEC:
hd:LOCAL:CAT:HEAD:MOD: 
cons nil 96
SYNSEM:LOCAL:CAT:HEAD:MOD: nil cons 92
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD: verb noun 91
SYNSEM:LOCAL:CAT:VAL:SUBJ: cons nil 79
SYNSEM:LOCAL:CAT:HEAD: noun verbal 77
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD: noun verbal 77
SYNSEM:LOCAL:CAT:HEAD: nominal verb 75
SYNSEM:LOCAL:CAT:VAL:CONJ:hd:LOCAL:CAT:HEAD:MOD: cons nil 74
SYNSEM:LOCAL:CAT:VAL:CONJ:tl:hd:LOCAL:CAT:HEAD:MOD: cons nil 69
SYNSEM:LOCAL:CAT:VAL:CONJ:tl:hd:LOCAL:CAT:VAL:SUBJ: nil cons 64
SYNSEM:LOCAL:CAT:VAL:CONJ:hd:LOCAL:CAT:VAL:SUBJ: nil cons 64
SYNSEM:LOCAL:CAT:VAL:COMPS:hd:LOCAL:CAT:HEAD: nominal verb 63
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SUBJ: cons nil 63
? ? ? ?
Total   10,598
 
Table 2: Path values overwritten by default unification in Section 22. 
610
Berger, Adam, Stephen Della Pietra, and Vincent Del-
la Pietra. 1996. A Maximum Entropy Approach to 
Natural Language Processing. Computational Lin-
guistics, 22(1), 39-71. 
Briscoe, Ted and John Carroll. 1993. Generalized 
probabilistic LR-Parsing of natural language (cor-
pora) with unification-based grammars. Computa-
tional Linguistics, 19(1), 25-59. 
Carpenter, Bob. 1992. The Logic of Typed Feature 
Structures: Cambridge University Press. 
Carpenter, Bob. 1993. Skeptical and Credulous De-
fault Unification with Applications to Templates 
and Inheritance. In Inheritance, Defaults, and the 
Lexicon. Cambridge: Cambridge University Press. 
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative 
Reranking. In proc. of ACL'05, pp. 173-180. 
Clark, Stephen and James R. Curran. 2004a. The im-
portance of supertagging for wide-coverage CCG 
parsing. In proc. of COLING-04, pp. 282-288. 
Clark, Stephen and James R. Curran. 2004b. Parsing 
the WSJ using CCG and log-linear models. In proc. 
of ACL'04, pp. 104-111. 
Copestake, Ann. 1993. Defaults in Lexical Represen-
tation. In Inheritance, Defaults, and the Lexicon. 
Cambridge: Cambridge University Press. 
Daum?, Hal III and Daniel Marcu. 2005. Learning as 
Search Optimization: Approximate Large Margin 
Methods for Structured Prediction. In proc. of 
ICML 2005. 
Geman, Stuart and Mark Johnson. 2002. Dynamic 
programming for parsing and estimation of sto-
chastic unification-based grammars. In proc. of 
ACL'02, pp. 279-286. 
Hockenmaier, Julia. 2003. Parsing with Generative 
Models of Predicate-Argument Structure. In proc. 
of ACL'03, pp. 359-366. 
Johnson, Mark, Stuart Geman, Stephen Canon, Zhiyi 
Chi, and Stefan Riezler. 1999. Estimators for Sto-
chastic ``Unification-Based'' Grammars. In proc. of 
ACL '99, pp. 535-541. 
Kaplan, R. M., S. Riezler, T. H. King, J. T. Maxwell 
III, and A. Vasserman. 2004. Speed and accuracy 
in shallow and deep stochastic parsing. In proc. of 
HLT/NAACL'04. 
Malouf, Robert and Gertjan van Noord. 2004. Wide 
Coverage Parsing with Stochastic Attribute Value 
Grammars. In proc. of IJCNLP-04 Workshop 
``Beyond Shallow Analyses''. 
Matsuzaki, Takuya, Yusuke Miyao, and Jun'ichi Tsu-
jii. 2007. Efficient HPSG Parsing with Supertag-
ging and CFG-filtering. In proc. of IJCAI 2007, pp. 
1671-1676. 
Miyao, Yusuke and Jun'ichi Tsujii. 2002. Maximum 
Entropy Estimation for Feature Forests. In proc. of 
HLT 2002, pp. 292-297. 
Miyao, Yusuke and Jun'ichi Tsujii. 2005. Probabilistic 
disambiguation models for wide-coverage HPSG 
parsing. In proc. of ACL'05, pp. 83-90. 
Nakagawa, Tetsuji. 2007. Multilingual dependency 
parsing using global features. In proc. of the 
CoNLL Shared Task Session of EMNLP-CoNLL 
2007, pp. 915-932. 
Ninomiya, Takashi, Takuya Matsuzaki, Yusuke 
Miyao, and Jun'ichi Tsujii. 2007. A log-linear 
model with an n-gram reference distribution for ac-
curate HPSG parsing. In proc. of IWPT 2007, pp. 
60-68. 
Ninomiya, Takashi, Takuya Matsuzaki, Yoshimasa 
Tsuruoka, Yusuke Miyao, and Jun'ichi Tsujii. 2006. 
Extremely Lexicalized Models for Accurate and 
Fast HPSG Parsing. In proc. of EMNLP 2006, pp. 
155-163. 
Ninomiya, Takashi, Yusuke Miyao, and Jun'ichi Tsu-
jii. 2002. Lenient Default Unification for Robust 
Processing within Unification Based Grammar 
Formalisms. In proc. of COLING 2002, pp. 744-
750. 
Nivre, Joakim and Mario Scholz. 2004. Deterministic 
dependency parsing of English text. In proc. of 
COLING 2004, pp. 64-70. 
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven 
Phrase Structure Grammar: University of Chicago 
Press. 
Ratnaparkhi, Adwait. 1997. A linear observed time 
statistical parser based on maximum entropy mod-
els. In proc. of EMNLP'97. 
Riezler, Stefan, Detlef Prescher, Jonas Kuhn, and 
Mark Johnson. 2000. Lexicalized Stochastic Mod-
eling of Constraint-Based Grammars using Log-
Linear Measures and EM Training. In proc. of 
ACL'00, pp. 480-487. 
Sagae, Kenji and Alon Lavie. 2005. A classifier-based 
parser with linear run-time complexity. In proc. of 
IWPT 2005. 
Sagae, Kenji and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In proc. of COL-
ING/ACL on Main conference poster sessions, pp. 
691-698. 
Sagae, Kenji, Yusuke Miyao, and Jun'ichi Tsujii. 
2007. HPSG parsing with shallow dependency 
constraints. In proc. of ACL 2007, pp. 624-631. 
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis with Support Vector 
Machines. In proc. of IWPT-2003. 
 
611
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1017?1024,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic Retrieval for the Accurate Identification of Relational Concepts
in Massive Textbases
Yusuke Miyao? Tomoko Ohta? Katsuya Masuda? Yoshimasa Tsuruoka?
Kazuhiro Yoshida? Takashi Ninomiya? Jun?ichi Tsujii??
?Department of Computer Science, University of Tokyo
?School of Informatics, University of Manchester
?Information Technology Center, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{yusuke,okap,kmasuda,tsuruoka,kyoshida,ninomi,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper introduces a novel framework
for the accurate retrieval of relational con-
cepts from huge texts. Prior to retrieval,
all sentences are annotated with predicate
argument structures and ontological iden-
tifiers by applying a deep parser and a term
recognizer. During the run time, user re-
quests are converted into queries of region
algebra on these annotations. Structural
matching with pre-computed semantic an-
notations establishes the accurate and effi-
cient retrieval of relational concepts. This
framework was applied to a text retrieval
system for MEDLINE. Experiments on
the retrieval of biomedical correlations re-
vealed that the cost is sufficiently small for
real-time applications and that the retrieval
precision is significantly improved.
1 Introduction
Rapid expansion of text information has motivated
the development of efficient methods of access-
ing information in huge texts. Furthermore, user
demand has shifted toward the retrieval of more
precise and complex information, including re-
lational concepts. For example, biomedical re-
searchers deal with a massive quantity of publica-
tions; MEDLINE contains approximately 15 mil-
lion references to journal articles in life sciences,
and its size is rapidly increasing, at a rate of more
than 10% yearly (National Library of Medicine,
2005). Researchers would like to be able to
search this huge textbase for biomedical correla-
tions such as protein-protein or gene-disease asso-
ciations (Blaschke and Valencia, 2002; Hao et al,
2005; Chun et al, 2006). However, the framework
of traditional information retrieval (IR) has diffi-
culty with the accurate retrieval of such relational
concepts because relational concepts are essen-
tially determined by semantic relations between
words, and keyword-based IR techniques are in-
sufficient to describe such relations precisely.
The present paper demonstrates a framework
for the accurate real-time retrieval of relational
concepts from huge texts. Prior to retrieval, we
prepare a semantically annotated textbase by ap-
plying NLP tools including deep parsers and term
recognizers. That is, all sentences are annotated
in advance with semantic structures and are stored
in a structured database. User requests are con-
verted on the fly into patterns of these semantic
annotations, and texts are retrieved by matching
these patterns with the pre-computed semantic an-
notations. The accurate retrieval of relational con-
cepts is attained because we can precisely describe
relational concepts using semantic annotations. In
addition, real-time retrieval is possible because se-
mantic annotations are computed in advance.
This framework has been implemented for a
text retrieval system for MEDLINE. We first ap-
ply a deep parser (Miyao and Tsujii, 2005) and
a dictionary-based term recognizer (Tsuruoka and
Tsujii, 2004) to MEDLINE and obtain annotations
of predicate argument structures and ontological
identifiers of genes, gene products, diseases, and
events. We then provide a search engine for these
annotated sentences. User requests are converted
into queries of region algebra (Clarke et al, 1995)
extended with variables (Masuda et al, 2006) on
these annotations. A search engine for the ex-
tended region algebra efficiently finds sentences
having semantic annotations that match the input
queries. In this paper, we evaluate this system with
respect to the retrieval of biomedical correlations
1017
Symbol CRP
Name C-reactive protein, pentraxin-related
Species Homo sapiens
Synonym MGC88244, PTX1
Product C-reactive protein precursor, C-reactive
protein, pentraxin-related protein
External links EntrezGene:1401, GDB:119071, . . .
Table 1: An example GENA entry
and examine the effects of using predicate argu-
ment structures and ontological identifiers.
The need for the discovery of relational con-
cepts has been investigated intensively in Infor-
mation Extraction (IE). However, little research
has targeted on-demand retrieval from huge texts.
One difficulty is that IE techniques such as pat-
tern matching and machine learning require heav-
ier processing in order to be applied on the fly.
Another difficulty is that target information must
be formalized beforehand and each system is de-
signed for a specific task. For instance, an IE
system for protein-protein interactions is not use-
ful for finding gene-disease associations. Apart
from IE research, enrichment of texts with vari-
ous annotations has been proposed and is becom-
ing a new research area for information manage-
ment (IBM, 2005; TEI, 2004). The present study
basically examines this new direction in research.
The significant contribution of the present paper,
however, is to provide the first empirical results of
this framework for a real task with a huge textbase.
2 Background: Resources and Tools for
Semantic Annotations
The proposed system for the retrieval of relational
concepts is a product of recent developments in
NLP resources and tools. In this section, ontology
databases, deep parsers, and search algorithms for
structured data are introduced.
2.1 Ontology databases
Ontology databases are collections of words and
phrases in specific domains. Such databases have
been constructed extensively for the systematic
management of domain knowledge by organizing
textual expressions of ontological entities that are
detached from actual sentences.
For example, GENA (Koike and Takagi, 2004)
is a database of genes and gene products that
is semi-automatically collected from well-known
databases, including HUGO, OMIM, Genatlas,
Locuslink, GDB, MGI, FlyBase, WormBase,
Figure 1: An output of HPSG parsing
Figure 2: A predicate argument structure
CYGD, and SGD. Table 1 shows an example of
a GENA entry. ?Symbol? and ?Name? denote
short forms and nomenclatures of genes, respec-
tively. ?Species? represents the organism species
in which this gene is observed. ?Synonym? is a
list of synonyms and name variations. ?Product?
gives a list of products of this gene, such as pro-
teins coded by this gene. ?External links? pro-
vides links to other databases, and helps to obtain
detailed information from these databases. For
biomedical terms other than genes/gene products,
the Unified Medical Language System (UMLS)
meta-thesaurus (Lindberg et al, 1993) is a large
database that contains various names of biomedi-
cal and health-related concepts.
Ontology databases provide mappings be-
tween textual expressions and entities in the real
world. For example, Table 1 indicates that CRP,
MGC88244, and PTX1 denote the same gene con-
ceptually. Hence, these resources enable us to
canonicalize variations of textual expressions of
ontological entities.
2.2 Parsing technologies
Recently, state-of-the-art CFG parsers (Charniak
and Johnson, 2005) can compute phrase structures
of natural sentences at fairly high accuracy. These
parsers have been used in various NLP tasks in-
cluding IE and text mining. In addition, parsers
that compute deeper analyses, such as predicate
argument structures, have become available for
1018
the processing of real-world sentences (Miyao and
Tsujii, 2005). Predicate argument structures are
canonicalized representations of sentence mean-
ings, and express the semantic relations of words
explicitly. Figure 1 shows an output of an HPSG
parser (Miyao and Tsujii, 2005) for the sentence
?A normal serum CRP measurement does not ex-
clude deep vein thrombosis.? The dotted lines ex-
press predicate argument relations. For example,
the ARG1 arrow coming from ?exclude? points
to the noun phrase ?A normal serum CRP mea-
surement?, which indicates that the subject of ?ex-
clude? is this noun phrase, while such relations are
not explicitly represented by phrase structures.
Predicate argument structures are beneficial for
our purpose because they can represent relational
concepts in an abstract manner. For example, the
relational concept of ?CRP excludes thrombosis?
can be represented as a predicate argument struc-
ture, as shown in Figure 2. This structure is univer-
sal in various syntactic expressions, such as pas-
sivization (e.g., ?thrombosis is excluded by CRP?)
and relativization (e.g., ?thrombosis that CRP ex-
cludes?). Hence, we can abstract surface varia-
tions of sentences and describe relational concepts
in a canonicalized form.
2.3 Structural search algorithms
Search algorithms for structured texts have been
studied extensively, and examples include XML
databases with XPath (Clark and DeRose, 1999)
and XQuery (Boag et al, 2005), and region alge-
bra (Clarke et al, 1995). The present study fo-
cuses on region algebra extended with variables
(Masuda et al, 2006) because it provides an ef-
ficient search algorithm for tags with cross bound-
aries. When we annotate texts with various levels
of syntactic/semantic structures, cross boundaries
are inherently nonnegligible. In fact, as described
in Section 3, our system exploits annotations of
predicate argument structures and ontological en-
tities, which include substantial cross boundaries.
Region algebra is defined as a set of operators
on regions, i.e., word sequences. Table 2 shows
operators of the extended region algebra, where
A and B denote regions, and results of operations
are also regions. For example, ?A & B? denotes a
region that includes both A and B. Four contain-
ment operators, >, >>, <, and <<, represent an-
cestor/descendant relations in XML. For example,
?A > B? indicates that A is an ancestor of B. In
[tag] Region covered with ?<tag>?
A > B A containing B
A >> B A containing B (A is not nested)
A < B A contained by B
A << B A contained by B (B is not nested)
A - B Starting with A and ending with B
A & B A and B
A | B A or B
Table 2: Operators of the extended region algebra
[sentence] >>
(([word arg1="$subject"] > exclude) &
([phrase id="$subject"] > CRP))
Figure 3: A query of the extended region algebra
Figure 4: Matching with the query in Figure 3
search algorithms for region algebra, the cost of
retrieving the first answer is constant, and that of
an exhaustive search is bounded by the lowest fre-
quency of a word in a query (Clarke et al, 1995).
Variables in the extended region algebra allow
us to express shared structures and are necessary
in order to describe predicate argument structures.
For example, Figure 3 shows a formula in the ex-
tended region algebra that represents the predicate
argument structure of ?CRP excludes something.?
This formula indicates that a sentence contains a
region in which the word ?exclude? exists, the
first argument (?arg1?) phrase of which includes
the word ?CRP.? A predicate argument relation is
expressed by the variable, ?$subject.? Figure 4
shows a situation in which this formula is satisfied.
Three horizontal bars describe regions covered by
<sentence>, <phrase>, and <word> tags,
respectively. The dotted line denotes the relation
expressed by this variable. Given this formula as a
query, a search engine can retrieve sentences hav-
ing semantic annotations that satisfy this formula.
3 A Text Retrieval System for MEDLINE
While the above resources and tools have been de-
veloped independently, their collaboration opens
up a new framework for the retrieval of relational
concepts, as described below (Figure 5).
Off-line processing: Prior to retrieval, a deep
parser is applied to compute predicate argument
1019
Figure 5: Framework of semantic retrieval
structures, and a term recognizer is applied to cre-
ate mappings from textual expressions into identi-
fiers in ontology databases. Semantic annotations
are stored and indexed in a structured database for
the extended region algebra.
On-line processing: User input is converted into
queries of the extended region algebra. A search
engine retrieves sentences having semantic anno-
tations that match the queries.
This framework is applied to a text retrieval en-
gine for MEDLINE. MEDLINE is an exhaustive
database covering nearly 4,500 journals in the life
sciences and includes the bibliographies of arti-
cles, about half of which have abstracts. Research
on IE and text mining in biomedical science has
focused mainly on MEDLINE. In the present pa-
per, we target al articles indexed in MEDLINE at
the end of 2004 (14,785,094 articles). The follow-
ing sections explain in detail off-/on-line process-
ing for the text retrieval system for MEDLINE.
3.1 Off-line processing: HPSG parsing and
term recognition
We first parsed all sentences using an HPSG parser
(Miyao and Tsujii, 2005) to obtain their predi-
cate argument structures. Because our target is
biomedical texts, we re-trained a parser (Hara et
al., 2005) with the GENIA treebank (Tateisi et
al., 2005), and also applied a bidirectional part-of-
speech tagger (Tsuruoka and Tsujii, 2005) trained
with the GENIA treebank as a preprocessor.
Because parsing speed is still unrealistic for
parsing the entire MEDLINE on a single ma-
chine, we used two geographically separated com-
puter clusters having 170 nodes (340 Xeon CPUs).
These clusters are separately administered and not
dedicated for use in the present study. In order to
effectively use such an environment, GXP (Taura,
2004) was used to connect these clusters and dis-
tribute the load among them. Our processes were
given the lowest priority so that our task would not
disturb other users. We finished parsing the entire
MEDLINE in nine days (Ninomiya et al, 2006).
# entries (genes) 517,773
# entries (gene products) 171,711
# entries (diseases) 148,602
# expanded entries 4,467,855
Table 3: Sizes of ontologies used for term recog-
nition
Event type Expressions
influence effect, affect, role, response, . . .
regulation mediate, regulate, regulation, . . .
activation induce, activate, activation, . . .
Table 4: Event expression ontology
Next, we annotated technical terms, such as
genes and diseases, to create mappings to onto-
logical identifiers. A dictionary-based term recog-
nition algorithm (Tsuruoka and Tsujii, 2004) was
applied for this task. First, an expanded term
list was created by generating name variations of
terms in GENA and the UMLS meta-thesaurus1.
Table 3 shows the size of the original database and
the number of entries expanded by name varia-
tions. Terms in MEDLINE were then identified
by the longest matching of entries in this expanded
list with words/phrases in MEDLINE.
The necessity of ontologies is not limited to
nominal expressions. Various verbs are used for
expressing events. For example, activation events
of proteins can be expressed by ?activate,? ?en-
hance,? and other event expressions. Although the
numbers of verbs and their event types are much
smaller than those of technical terms, verbal ex-
pressions are important for the description of rela-
tional concepts. Since ontologies of event expres-
sions in this domain have not yet been constructed,
we developed an ontology from scratch. We inves-
tigated 500 abstracts extracted from MEDLINE,
and classified 167 frequent expressions, including
verbs and their nominalized forms, into 18 event
types. Table 4 shows a part of this ontology. These
expressions in MEDLINE were automatically an-
notated with event types.
As a result, we obtained semantically annotated
MEDLINE. Table 5 shows the size of the orig-
inal MEDLINE and semantic annotations. Fig-
ure 6 shows semantic annotations for the sentence
in Figure 1, where ?-? indicates nodes of XML,2
1We collected disease names by specifying a query with
the semantic type as ?Disease or Syndrome.?
2Although this example is shown in XML, this textbase
contains tags with cross boundaries because tags for predicate
argument structures and technical terms may overlap.
1020
# papers 14,785,094
# abstracts 7,291,857
# sentences 70,935,630
# words 1,462,626,934
# successfully parsed sentences 69,243,788
# predicate argument relations 1,510,233,701
# phrase tags 3,094,105,383
# terms (genes) 84,998,621
# terms (gene products) 27,471,488
# terms (diseases) 19,150,984
# terms (event expressions) 51,810,047
Size of the original MEDLINE 9.3 GByte
Size of the semantic annotations 292 GByte
Size of the index file for region algebra 954 GByte
Table 5: Sizes of the original and semantically an-
notated MEDLINE textbases
- <sentence sentence_id="e6e525">
- <phrase id="0" cat="S" head="15" lex_head="18">
- <phrase id="1" cat="NP" head="4" lex_head="14">
- <phrase id="2" cat="DT" head="3" lex_head="3">
- <word id="3" pos="DT" cat="DT" base="a" arg1="4">
- A
- <phrase id="4" cat="NP" head="7" lex_head="14">
- <phrase id="5" cat="AJ" head="6" lex_head="6">
- <word id="6" pos="JJ" cat="AJ" base="normal" arg1="7">
- normal
- <phrase id="7" cat="NP" head="10" lex_head="14">
- <phrase id="8" cat="NP" head="9" lex_head="9">
- <word id="9" pos="NN" cat="NP" base="serum" mod="10">
- serum
- <phrase id="10" cat="NP" head="13" lex_head="14">
- <phrase id="11" cat="NP" head="12" lex_head="12">
- <entity_name id="entity-1" type="gene"
gene_id="GHS003134" gene_symbol="CRP"
gene_name="C-reactive protein, pentraxin-related"
species="Homo sapiens"
db_site="EntrezGene:1401|GDB:119071|GenAtlas:CRP">
- <word id="12" pos="NN" cat="NP" base="crp" mod="13">
- CRP
- <phrase id="13" cat="NP" head="14" lex_head="14">
- <word id="14" pos="NN" cat="NP" base="measurement">
- measurement
- <phrase id="15" cat="VP" head="16" lex_head="18">
- <phrase id="16" cat="VP" head="17" lex_head="18">
- <phrase id="17" cat="VP" head="18" lex_head="18">
- <word id="18" pos="VBZ" cat="VP" base="do"
arg1="1" arg2="21">
- does
- <phrase id="19" cat="AV" head="20" lex_head="20">
- <word id="20" pos="RB" cat="AV" base="not" arg1="21">
- not
- <phrase id="21" cat="VP" head="22" lex_head="23">
- <phrase id="22" cat="VP" head="23" lex_head="23">
- <word id="23" pos="VB" cat="VP" base="exclude"
arg1="1" arg2="24">
- exclude
...
Figure 6: A semantically annotated sentence
although the latter half of the sentence is omitted
because of space limitations. Sentences are an-
notated with four tags,3 ?phrase,? ?word,? ?sen-
tence,? and ?entity name,? and their attributes as
given in Table 6. Predicate argument structures are
annotated as attributes, ?mod? and ?argX ,? which
point to the IDs of the argument phrases. For ex-
ample, in Figure 6, the <word> tag for ?exclude?
has the attributes arg1="1" and arg2="24",
which denote the IDs of the subject and object
phrases, respectively.
3Additional tags exist for representing document struc-
tures such as ?title? (details omitted).
Tag Attributes
phrase id, cat, head, lex head
word id, cat, pos, base, mod, argX , rel type
sentence sentence id
entity name id, type, gene id/disease id, gene symbol,
gene name, species, db site
Attribute Description
id unique identifier
cat syntactic category
head head daughter?s ID
lex head lexical head?s ID
pos part-of-speech
base base form of the word
mod ID of modifying phrase
argX ID of the X-th argument of the word
rel type event type
sentence id sentence?s ID
type whether gene, gene prod, or disease
gene id ID in GENA
disease id ID in the UMLS meta-thesaurus
gene symbol short form of the gene
gene name nomenclature of the gene
species species that have this gene
db site links to external databases
Table 6: Tags (upper) and attributes (lower) for
semantic annotations
3.2 On-line processing
The off-line processing described above results in
much simpler on-line processing. User input is
converted into queries of the extended region al-
gebra, and the converted queries are entered into a
search engine for the extended region algebra. The
implementation of a search engine is described in
detail in Masuda et al (2006).
Basically, given subject x, object y, and verb v,
the system creates the following query:
[sentence] >>
([word arg1="$subject" arg2="$object"
base="v"] &
([phrase id="$subject"] > x) &
([phrase id="$object"] > y))
Ontological identifiers are substituted for x, y,
and v, if possible. Nominal keywords, i.e., x and
y, are replaced by [entity_name gene_id="n"]
or [entity_name disease_id="n"], where n is
the ontological identifier of x or y. For verbal key-
words, base="v" is replaced by rel_type="r",
where r is the event type of v.
4 Evaluation
Our system is evaluated with respect to speed and
accuracy. Speed is indispensable for real-time in-
teractive text retrieval systems, and accuracy is key
for the motivation of semantic retrieval. That is,
our motivation for employing semantic retrieval
1021
Query No. User input
1 something inhibit ERK2
2 something trigger diabetes
3 adiponectin increase something
4 TNF activate IL6
5 dystrophin cause disease
6 macrophage induce something
7 something suppress MAP phosphorylation
8 something enhance p53 (negative)
Table 7: Queries for experiments
[sentence] >>
([word rel_type="activation"] &
[entity_name type="gene" gene_id="GHS019685"] &
[entity_name type="gene" gene_id="GHS009426"])
[sentence] >>
([word arg1="$subject" arg2="$object"
rel_type="activation"] &
([phrase id="$subject"] >
[entity_name type="gene" gene_id="GHS019685"]) &
([phrase cat="np" id="$object"] >
[entity_name type="gene" gene_id="GHS009426"]))
Figure 7: Queries of the extended region algebra
for Query 4-3 (upper: keyword search, lower: se-
mantic search)
was to provide a device for the accurate identifica-
tion of relational concepts. In particular, high pre-
cision is desired in text retrieval from huge texts
because users want to extract relevant information,
rather than collect exhaustive information.
We have two parameters to vary: whether to
use predicate argument structures and whether to
use ontological identifiers. The effect of using
predicate argument structures is evaluated by com-
paring ?keyword search? with ?semantic search.?
The former is a traditional style of IR, in which
sentences are retrieved by matching words in a
query with words in sentences. The latter is a
new feature of the present system, in which sen-
tences are retrieved by matching predicate argu-
ment relations in a query with those in a semanti-
cally annotated textbase. The effect of using onto-
logical identifiers is assessed by changing queries
of the extended region algebra. When we use the
term ontology, nominal keywords in queries are
replaced with ontological identifiers in GENA and
the UMLS meta-thesaurus. When we use the event
expression ontology, verbal keywords in queries
are replaced with event types.
Table 7 is a list of queries used in the follow-
ing experiments. Words in italics indicate a class
of words: ?something? indicates that any word
can appear, and disease indicates that any dis-
ease expression can appear. These queries were
selected by a biologist, and express typical re-
lational concepts that a biologist may wish to
find. Queries 1, 3, and 4 represent relations of
genes/proteins, where ERK2, adiponectin, TNF,
and IL6 are genes/proteins. Queries 2 and 5 de-
scribe relations concerning diseases, and Query 6
is a query that is not relevant to genes or diseases.
Query 7 expresses a complex relation concern-
ing a specific phenomena, i.e., phosphorylation,
of MAP. Query 8 describes a relation concerning
a gene, i.e., p53, while ?(negative)? indicates that
the target of retrieval is negative mentions. This is
expressed by ?not? modifying a predicate.
For example, Query 4 attempts to retrieve sen-
tences that mention the protein-protein interaction
?TNF activates IL6.? This is converted into queries
of the extended region algebra given in Figure 7.
The upper query is for keyword search and only
specifies the appearances of the three words. Note
that the keywords are translated into the ontolog-
ical identifiers, ?activation,? ?GHS019685,? and
?GHS009426.? The lower query is for semantic
search. The variables in ?arg1? and ?arg2? indi-
cate that ?GHS019685? and ?GHS009426? are the
subject and object, respectively, of ?activation?.
Table 8 summarizes the results of the experi-
ments. The postfixes of query numbers denote
whether ontological identifiers are used. X-1 used
no ontologies, and X-2 used only the term ontol-
ogy. X-3 used both the term and event expression
ontologies4. Comparison of X-1 and X-2 clarifies
the effect of using the term ontology. Comparison
of X-2 and X-3 shows the effect of the event ex-
pression ontology. The results for X-3 indicate
the maximum performance of the current system.
This table shows that the time required for the se-
mantic search for the first answer, shown as ?time
(first)? in seconds, was reasonably short. Thus,
the present framework is acceptable for real-time
text retrieval. The numbers of answers increased
when we used the ontologies, and this result indi-
cates the efficacy of both ontologies for obtaining
relational concepts written in various expressions.
Accuracy was measured by judgment by a bi-
ologist. At most 100 sentences were retrieved for
each query, and the results of keyword search and
semantic search were merged and shuffled. A bi-
ologist judged the shuffled sentences (1,839 sen-
tences in total) without knowing whether the sen-
4Query 5-1 is not tested because ?disease? requires
the term ontology, and Query 6-2 is not tested because
?macrophage? is not assigned an ontological identifier.
1022
Query Keyword search Semantic search
No. # ans. time (first/all) precision n-precision # ans. time (first/all) precision relative recall
1-1 252 0.00/ 1.5 74/100 (74%) 74/100 (74%) 143 0.01/ 2.5 96/100 (96%) 51/74 (69%)
1-2 348 0.00/ 1.9 61/100 (61%) 61/100 (61%) 174 0.01/ 3.1 89/100 (89%) 42/61 (69%)
1-3 884 0.00/ 3.2 50/100 (50%) 50/100 (50%) 292 0.01/ 5.3 91/100 (91%) 21/50 (42%)
2-1 125 0.00/ 1.8 45/100 (45%) 9/ 27 (33%) 27 0.02/ 2.9 23/ 27 (85%) 17/45 (38%)
2-2 113 0.00/ 2.9 40/100 (40%) 10/ 26 (38%) 26 0.06/ 4.0 22/ 26 (85%) 19/40 (48%)
2-3 6529 0.00/ 12.1 42/100 (42%) 42/100 (42%) 662 0.01/1527.4 76/100 (76%) 8/42 (19%)
3-1 287 0.00/ 1.5 20/100 (20%) 4/ 30 (13%) 30 0.05/ 2.4 23/ 30 (80%) 6/20 (30%)
3-2 309 0.01/ 2.1 21/100 (21%) 4/ 32 (13%) 32 0.10/ 3.5 26/ 32 (81%) 6/21 (29%)
3-3 338 0.01/ 2.2 24/100 (24%) 8/ 39 (21%) 39 0.05/ 3.6 32/ 39 (82%) 8/24 (33%)
4-1 4 0.26/ 1.5 0/ 4 (0%) 0/ 0 (?) 0 2.44/ 2.4 0/ 0 (?) 0/ 0 (?)
4-2 195 0.01/ 2.5 9/100 (9%) 1/ 6 (17%) 6 0.09/ 4.1 5/ 6 (83%) 2/ 9 (22%)
4-3 2063 0.00/ 7.5 5/100 (5%) 5/ 94 (5%) 94 0.02/ 10.5 89/ 94 (95%) 2/ 5 (40%)
5-2 287 0.08/ 6.3 73/100 (73%) 73/100 (73%) 116 0.05/ 14.7 97/100 (97%) 37/73 (51%)
5-3 602 0.01/ 15.9 50/100 (50%) 50/100 (50%) 122 0.05/ 14.2 96/100 (96%) 23/50 (46%)
6-1 10698 0.00/ 42.8 14/100 (14%) 14/100 (14%) 1559 0.01/3014.5 65/100 (65%) 10/14 (71%)
6-3 42106 0.00/3379.5 11/100 (11%) 11/100 (11%) 2776 0.01/5100.1 61/100 (61%) 5/11 (45%)
7 87 0.04/ 2.7 34/ 87 (39%) 7/ 15 (47%) 15 0.05/ 4.2 10/ 15 (67%) 10/34 (29%)
8 1812 0.01/ 7.6 19/100 (19%) 17/ 84 (20%) 84 0.20/ 29.2 73/ 84 (87%) 7/19 (37%)
Table 8: Number of retrieved sentences, retrieval time, and accuracy
tence was retrieved by keyword search or semantic
search. Without considering which words actually
matched the query, a sentence is judged to be cor-
rect when any part of the sentence expresses all of
the relations described by the query. The modality
of sentences was not distinguished, except in the
case of Query 8. These evaluation criteria may be
disadvantageous for the semantic search because
its ability to exactly recognize the participants of
relational concepts is not evaluated. Table 8 shows
the precision attained by keyword/semantic search
and n-precision, which denotes the precision of
the keyword search, in which the same number,
n, of outputs is taken as the semantic search. The
table also gives the relative recall of the semantic
search, which represents the ratio of sentences that
are correctly output by the semantic search among
those correctly output by the keyword search. This
does not necessarily represent the true recall be-
cause sentences not output by keyword search are
excluded. However, this is sufficient for the com-
parison of keyword search and semantic search.
The results show that the semantic search exhib-
ited impressive improvements in precision. The
precision was over 80% for most queries and was
nearly 100% for Queries 4 and 5. This indicates
that predicate argument structures are effective for
representing relational concepts precisely, espe-
cially for relations in which two entities are in-
volved. Relative recall was approximately 30?
50%, except for Query 2. In the following, we
will investigate the reasons for the residual errors.
Table 9 shows the classifications of the errors of
Disregarding of noun phrase structures 45
Term recognition errors 33
Parsing errors 11
Other reasons 8
Incorrect human judgment 7
Nominal expressions 41
Phrasal verb expressions 26
Inference required 24
Coreference resolution required 19
Parsing errors 16
Other reasons 15
Incorrect human judgment 10
Table 9: Error analysis (upper: 104 false positives,
lower: 151 false negatives)
semantic retrieval. The major reason for false pos-
itives was that our queries ignore internal struc-
tures of noun phrases. The system therefore re-
trieved noun phrases that do not directly mention
target entities. For example, ?the increased mor-
tality in patients with diabetes was caused by . . . ?
does not indicate the trigger of diabetes. Another
reason was term recognition errors. For exam-
ple, the system falsely retrieved sentences con-
taining ?p40,? which is sometimes, but not nec-
essarily used as a synonym for ?ERK2.? Ma-
chine learning-based term disambiguation will al-
leviate these errors. False negatives were caused
mainly by nominal expressions such as ?the in-
hibition of ERK2.? This is because the present
system does not convert user input into queries
on nominal expressions. Another major reason,
phrasal verb expressions such as ?lead to,? is also
a shortage of our current strategy of query cre-
ation. Because semantic annotations already in-
1023
clude linguistic structures of these expressions, the
present system can be improved further by creat-
ing queries on such expressions.
5 Conclusion
We demonstrated a text retrieval system for MED-
LINE that exploits pre-computed semantic anno-
tations5. Experimental results revealed that the
proposed system is sufficiently efficient for real-
time text retrieval and that the precision of re-
trieval was remarkably high. Analysis of resid-
ual errors showed that the handling of noun phrase
structures and the improvement of term recogni-
tion will increase retrieval accuracy. Although
the present paper focused on MEDLINE, the NLP
tools used in this system are domain/task indepen-
dent. This framework will thus be applicable to
other domains such as patent documents.
The present framework does not conflict with
conventional IR/IE techniques, and integration
with these techniques is expected to improve the
accuracy and usability of the proposed system. For
example, query expansion and relevancy feedback
can be integrated in a straightforward way in order
to improve accuracy. Document ranking is useful
for the readability of retrieved results. IE systems
can be applied off-line, in the manner of the deep
parser in our system, for annotating sentences with
target information of IE. Such annotations will en-
able us to retrieve higher-level concepts, such as
relationships among relational concepts.
Acknowledgment
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Systems
Genomics? (MEXT, Japan), Genome Network
Project (NIG, Japan), and Solution-Oriented Re-
search for Science and Technology (JST, Japan).
References
C. Blaschke and A. Valencia. 2002. The frame-based
module of the SUISEKI information extraction sys-
tem. IEEE Intelligent Systems, 17(2):14?20.
S. Boag, D. Chamberlin, M. F. Ferna?ndez, D. Florescu,
J. Robie, and J. Sime?on. 2005. XQuery 1.0: An
XML query language.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proc. ACL 2005.
5A web-based demo of our system is available on-line at:
http://www-tsujii.is.s.u-tokyo.ac.jp/medie/
H.-W. Chun, Y. Tsuruoka, J.-D. Kim, R. Shiba, N. Na-
gata, T. Hishiki, and J. Tsujii. 2006. Extraction
of gene-disease relations from MedLine using do-
main dictionaries and machine learning. In Proc.
PSB 2006, pages 4?15.
J. Clark and S. DeRose. 1999. XML Path Language
(XPath) version 1.0.
C. L. A. Clarke, G. V. Cormack, and F. J. Burkowski.
1995. An algebra for structured text search and a
framework for its implementation. The Computer
Journal, 38(1):43?56.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Dis-
covering patterns to extract protein-protein interac-
tions from the literature: Part II. Bioinformatics,
21(15):3294?3300.
T. Hara, Y. Miyao, and J. Tsujii. 2005. Adapting
a probabilistic disambiguation model of an HPSG
parser to a new domain. In Proc. IJCNLP 2005.
IBM, 2005. Unstructed Information Management Ar-
chitecture (UIMA) SDK User?s Guide and Refer-
ence.
A. Koike and T. Takagi. 2004. Gene/protein/family
name recognition in biomedical literature. In Proc.
Biolink 2004, pages 9?16.
D. A. Lindberg, B. L. Humphreys, and A. T. Mc-
Cray. 1993. The Unified Medical Language Sys-
tem. Methods in Inf. Med., 32(4):281?291.
K. Masuda, T. Ninomiya, Y. Miyao, T. Ohta, and
J. Tsujii. 2006. Nested region algebra extended with
variables. In Preparation.
Y. Miyao and J. Tsujii. 2005. Probabilistic disam-
biguation models for wide-coverage HPSG parsing.
In Proc. 43rd ACL, pages 83?90.
National Library of Medicine. 2005. Fact Sheet MED-
LINE. Available at http://www.nlm.nih.
gov/pubs/factsheets/medline.html.
T. Ninomiya, Y. Tsuruoka, Y. Miyao, K. Taura, and
J. Tsujii. 2006. Fast and scalable HPSG parsing.
Traitement automatique des langues (TAL), 46(2).
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005.
Syntax annotation for the GENIA corpus. In Proc.
IJCNLP 2005, Companion volume, pages 222?227.
K. Taura. 2004. GXP : An interactive shell for the grid
environment. In Proc. IWIA2004, pages 59?67.
TEI Consortium, 2004. Text Encoding Initiative.
Y. Tsuruoka and J. Tsujii. 2004. Improving the per-
formance of dictionary-based approaches in protein
name recognition. Journal of Biomedical Informat-
ics, 37(6):461?470.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional infer-
ence with the easiest-first strategy for tagging se-
quence data. In Proc. HLT/EMNLP 2005, pages
467?474.
1024
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 850?857,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Trimming CFG Parse Trees for Sentence Compression Using Machine
Learning Approaches
Yuya Unno1 Takashi Ninomiya2 Yusuke Miyao1 Jun?ichi Tsujii134
1Department of Computer Science, University of Tokyo
2Information Technology Center, University of Tokyo
3School of Informatics, University of Manchester
4SORST, JST
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
{unno, yusuke, tsujii}@is.s.u-tokyo.ac.jp
ninomi@r.dl.itc.u-tokyo.ac.jp
Abstract
Sentence compression is a task of creating
a short grammatical sentence by removing
extraneous words or phrases from an origi-
nal sentence while preserving its meaning.
Existing methods learn statistics on trim-
ming context-free grammar (CFG) rules.
However, these methods sometimes elim-
inate the original meaning by incorrectly
removing important parts of sentences, be-
cause trimming probabilities only depend
on parents? and daughters? non-terminals
in applied CFG rules. We apply a maxi-
mum entropy model to the above method.
Our method can easily include various
features, for example, other parts of a
parse tree or words the sentences contain.
We evaluated the method using manually
compressed sentences and human judg-
ments. We found that our method pro-
duced more grammatical and informative
compressed sentences than other methods.
1 Introduction
In most automatic summarization approaches, text
is summarized by extracting sentences from a
given document without modifying the sentences
themselves. Although these methods have been
significantly improved to extract good sentences
as summaries, they are not intended to shorten sen-
tences; i.e., the output often has redundant words
or phrases. These methods cannot be used to
make a shorter sentence from an input sentence or
for other applications such as generating headline
news (Dorr et al, 2003) or messages for the small
screens of mobile devices. We need to compress
sentences to obtain short and useful summaries.
This task is called sentence compression.
While several methods have been proposed for
sentence compression (Witbrock and Mittal, 1999;
Jing and McKeown, 1999; Vandeghinste and Pan,
2004), this paper focuses on Knight and Marcu?s
noisy-channel model (Knight and Marcu, 2000)
and presents an extension of their method. They
developed a probabilistic model for trimming a
CFG parse tree of an input sentence. Their
method drops words of input sentences but does
not change their order or change the words. They
use a parallel corpus that contains pairs of origi-
nal and compressed sentences. The method makes
CFG parse trees of both original and compressed
sentences and learns trimming probabilities from
these pairs. Although their method is concise and
well-defined, its accuracy is still unsatisfactory.
Their method has two problems. One is that prob-
abilities are calculated only from the frequencies
of applied CFG rules, and other characteristics like
whether the phrase includes negative words cannot
be introduced. The other problem is that the parse
trees of original and compressed sentences some-
times do not correspond.
To solve the former problem, we apply a maxi-
mum entropy model to Knight and Marcu?s model
to introduce machine learning features that are de-
fined not only for CFG rules but also for other
characteristics in a parse tree, such as the depth
from the root node or words it contains. To solve
the latter problem, we introduce a novel matching
method, the bottom-up method, to learn compli-
cated relations of two unmatched trees.
We evaluated each algorithm using the Ziff-
Davis corpus, which has long and short sentence
pairs. We compared our method with Knight and
Marcu?s method in terms of F -measures, bigram
F -measures, BLEU scores and human judgments.
850
2 Background
2.1 The Noisy-Channel Model for Sentence
Compression
Knight and Marcu proposed a sentence compres-
sion method using a noisy-channel model (Knight
and Marcu, 2000). This model assumes that a long
sentence was originally a short one and that the
longer sentence was generated because some un-
necessary words were added. Given a long sen-
tence l, it finds a short sentence s that maximizes
P (s|l). This is equivalent to finding the s that
maximizes P (s) ? P (l|s) in Bayes? Rule.
The expression P (s) is the source model, which
gives the probability that s is the original short
string. When s is ungrammatical, P (s) becomes
small. The expression P (l|s) is the channel
model, which gives the probability that s is ex-
panded to l. When s does not include important
words of l, P (l|s) has a low value.
In the Knight and Marcu?s model, a proba-
bilistic context-free grammar (PCFG) score and a
word-bigram score are incorporated as the source
model. To estimate the channel model, Knight
and Marcu used the Ziff-Davis parallel corpus,
which contains long sentences and corresponding
short sentences compressed by humans. Note that
each compressed sentence is a subsequence of the
corresponding original sentence. They first parse
both the original and compressed sentences using
a CFG parser to create parse trees. When two
nodes of the original and compressed trees have
the same non-terminals, and the daughter nodes of
the compressed tree are a subsequence of the orig-
inal tree, they count the node pair as a joint event.
For example, in Figure 1, the original parse tree
contains a rule rl = (B ? D E F ), and the com-pressed parse tree contains rs = (B ? D F ).They assume that rs was expanded into rl, andcount the node pairs as joint events. The expan-
sion probability of two rules is given by:
Pexpand (rl|rs) =
count(joint(rl, rs))
count(rs)
.
Finally, new subtrees grow from new daugh-
ter nodes in each expanded node. In Figure 1,
(E (G g) (H h)) grows from E. The PCFG
scores, Pcfg , of these subtrees are calculated.Then, each probability is assumed to be indepen-
dent of the others, and the channel model, P (l|s),
is calculated as the product of all expansion prob-
abilities of joint events and PCFG scores of new
A
B C
E FD
d
g h
f
c
A
B C
FD
d f
c
G H
Figure 1: Examples of original and compressed
parse trees.
subtrees:
P (l|s) =
?
(rl,rs)?R
Pexpand (rl|rs) ?
?
r?R?
Pcfg(r),
where R is the set of rule pairs, and R? is the set
of generation rules in new subtrees.
To compress an input sentence, they create a
tree with the highest score of all possible trees.
They pack all possible trees in a shared-forest
structure (Langkilde, 2000). The forest structure
is represented by an AND-OR tree, and it con-
tains many tree structures. The forest represen-
tation saves memory and makes calculation faster
because the trees share sub structures, and this can
reduce the total number of calculations.
They normalize each log probability using the
length of the compressed sentence; that is, they di-
vide the log probability by the length of the com-
pressed sentence.
Turner and Charniak (Turner and Charniak,
2005) added some special rules and applied this
method to unsupervised learning to overcome the
lack of training data. However their model also
has the same problem. McDonald (McDonald,
2006) independently proposed a new machine
learning approach. He does not trim input parse
trees but uses rich features about syntactic trees
and improved performance.
2.2 Maximum Entropy Model
The maximum entropy model (Berger et al, 1996)
estimates a probability distribution from training
data. The model creates the most ?uniform? distri-
bution within the constraints given by users. The
distribution with the maximum entropy is consid-
ered the most uniform.
Given two finite sets of event variables, X and
Y , we estimate their joint probability distribution,
P (x, y). An output, y (? Y), is produced, and
851
contextual information, x (? X ), is observed. To
represent whether the event (x, y) satisfies a cer-
tain feature, we introduce a feature function. A
feature function fi returns 1 iff the event (x, y) sat-isfies the feature i and returns 0 otherwise.
Given training data {(x1, y1), ? ? ? , (xn, yn)},we assume that the expectation of fi on the dis-tribution of the model conforms to that on the em-
pirical probability distribution P? (x, y). We select
the probability distribution that satisfies these con-
straints of all feature functions and maximizes its
entropy, H(P ) = ??x,y P (x, y) log (P (x, y)).
3 Methods
3.1 Maximum Entropy Model for Sentence
Compression
We describe a maximum entropy method as a
natural extension of Knight and Marcu?s noisy-
channel model (Knight and Marcu, 2000). Knight
and Marcu?s method uses only mother and daugh-
ter local relations in CFG parse trees. Therefore,
it sometimes eliminates the meanings of the origi-
nal sentences. For example, their method cannot
distinguish ?never? and ?always? because these
two adverbs are assigned the same non-terminals
in parse trees. However, if ?never? is removed
from a sentence, the meaning of the sentence com-
pletely changes. Turner and Charniak (Turner and
Charniak, 2005) revised and improved Knight and
Marcu?s algorithm; however, their algorithm also
uses only mother and daughter relations and has
the same problem. We use other information as
feature functions of the maximum entropy model,
and this model can deal with many features more
appropriately than using simple frequency.
Suppose that we trim a node in the original full
parse tree. For example, suppose we have a mother
node A and daughter nodes (B C D) that are de-
rived using a CFG rule. We must leave at least one
non-terminal in the daughter nodes. The trim can-
didates of this rule are the members of the set of
subsequences, Y , of (B C D), or the seven non-
terminal sequences below:
Y = {B,C,D,BC,BD,CD,BCD}.
For each y (? Y), such as (B C), the trimming
probability, P (y|Y) = Ptrim(A ? B C|A ?
B C D), is calculated by using the maximum en-
tropy model. We assume that these joint events are
independent of each other and calculate the proba-
bility that an original sentence, l, is compressed to
Description
1 the mother node
2 the current node
3 the daughter node sequence in the original sentence
and which daughters are removed
4 the daughter node sequence in the compressed sen-
tence
5 the number of daughter nodes
6 the depth from the root
7 the daughter non-terminals that are removed
8 the daughter terminals that are removed
9 whether the daughters are ?negative adverbs?, and
removed
10 tri-gram of daughter nodes
11 only one daughter exists, and its non-terminal is the
same as that of the current node
12 only one daughter exists, and its non-terminal is the
same as that of the mother node
13 how many daughter nodes are removed
14 the number of terminals the current node contains
15 whether the head daughter is removed
16 the left-most and the right-most daughters
17 the left and the right siblings
Table 1: Features for maximum entropy model.
s as the product of all trimming probabilities, like
in Knight and Marcu?s method.
P (s|l) =
?
(rs,rl)?R
Ptrim(rs|rl),
where R is the set of compressed and original rule
pairs in joint events. Note that our model does not
use Bayes? Rule or any language models.
For example, in Figure 1, the trimming proba-
bility is calculated as below:
P (s|l) = Ptrim(A ? B C|A ? B C)
?Ptrim(B ? D F |B ? D E F ).
To represent all summary candidates, we cre-
ate a compression forest as Knight and Marcu did.
We select the tree assigned the highest probability
from the forest.
Features in the maximum entropy model are de-
fined for a tree node and its surroundings. When
we process one node, or one non-terminal x, we
call it the current node. We focus on not only x
and its daughter nodes, but its mother node, its
sibling nodes, terminals of its subtree and so on.
The features we used are listed in Table 1.
Knight and Marcu divided the log probabilities
by the length of the summary. We extend this idea
so that we can change the output length flexibly.
We introduce a length parameter, ?, and define a
score S? as S?(s) = length(s)? log P (s|l), where
l is an input sentence to be shortened, and s is a
852
summary candidate. Because log P (s|l) is nega-
tive, short sentences obtain a high score for large
?, and long ones get a low score. The parameter
? can be negative or positive, and we can use it to
control the average length of outputs.
3.2 Bottom-Up Method
As explained in Section 2.1, in Knight and
Marcu?s method, both original and compressed
sentences are parsed, and correspondences of CFG
rules are identified. However, when the daugh-
ter nodes of a compressed rule are not a subse-
quence of the daughter nodes in the original one,
the method cannot learn this joint event. A com-
plex sentence is a typical example. A complex
sentence is a sentence that includes another sen-
tence as a part. An example of a parse tree of a
complex sentence and its compressed version is
shown in Figure 2. When we extract joint events
from these two trees, we cannot match the two
root nodes because the sequence of the daughter
nodes of the root node of the compressed parse
tree, (NP ADVP VP .), is not a subsequence
of the daughter nodes of the original parse tree,
(S , NP VP .). Turner and Charniak (Turner and
Charniak, 2005) solve this problem by appending
special rules that are applied when a mother node
and its daughter node have the same label. How-
ever, there are several types of such problems like
Figure 2. We need to extract these structures from
a training corpus.
We propose a bottom-up method to solve the
problem explained above. In our method, only
original sentences are parsed, and the parse trees
of compressed sentences are extracted from the
original parse trees. An example of this method
is shown in Figure 3. The original sentence is ?d
g h f c?, and its compressed sentence is ?d g c?.
First, each terminal in the parse tree of the original
sentence is marked if it exists in the compressed
sentence. In the figure, the marked terminals are
represented by circles. Second, each non-terminal
in the original parse tree is marked if it has at least
one marked terminal in its sub-trees. These are
represented as bold boxes in the figure. If non-
terminals contain marked non-terminals in their
sub-trees, these non-terminals are also marked re-
cursively. These marked non-terminals and termi-
nals compose a tree structure like that on the right-
hand side in the figure. These non-terminals rep-
resent joint events at each node.
S
S ,
,
NP VP
I said
.
.
S
.
.
NP VPADVP
I never think soNP VPADVP
I never think so
top top
Figure 2: Example of parse tree pair that cannot
be matched.
A
B C
E FD
G H
h
f
A
B C
ED
d
g
c
d
g
c
G
Figure 3: Example of bottom-up method.
Note that this ?tree? is not guaranteed to be
a grammatical ?parse tree? by the CFG gram-
mar. For example, from the tree of Figure 2,
(S (S ? ? ? ) (, , ) (NP I) (VP said) (. .)), a new
tree, (S (S ? ? ? ) (. .)), is extracted. However, the
rule (S ? S .) is ungrammatical.
4 Experiment
4.1 Evaluation Method
We evaluated each sentence compression method
using word F -measures, bigram F -measures, and
BLEU scores (Papineni et al, 2002). BLEU scores
are usually used for evaluating machine transla-
tion quality. A BLEU score is defined as the
weighted geometric average of n-gram precisions
with length penalties. We used from unigram to
4-gram precisions and uniform weights for the
BLEU scores.
ROUGE (Lin, 2004) is a set of recall-based cri-
teria that is mainly used for evaluating summa-
rization tasks. ROUGE-N uses average N-gram re-
call, and ROUGE-1 is word recall. ROUGE-L uses
the length of the longest common subsequence
(LCS) of the original and summarized sentences.
In our model, the length of the LCS is equal to
the number of common words, and ROUGE-L is
equal to the unigram F -measure because words
are not rearranged. ROUGE-L and ROUGE-1 are
supposed to be appropriate for the headline gener-
853
ation task (Lin, 2004). This is not our task, but it
is the most similar task in his paper.
We also evaluated the methods using human
judgments. The evaluator is not the author but not
a native English speaker. The judgment used the
same criteria as those in Knight and Marcu?s meth-
ods. We performed two experiments. In the first
experiment, evaluators scored from 1 to 5 points
the grammaticality of the compressed sentence. In
the second one, they scored from 1 to 5 points
how well the compressed sentence contained the
important words of the original one.
We used the parallel corpus used in Ref. (Knight
and Marcu, 2000). This corpus consists of sen-
tence pairs extracted automatically from the Ziff-
Davis corpus, a set of newspaper articles about
computer products. This corpus has 1087 sentence
pairs. Thirty-two of these sentences were used for
the human judgments in Knight and Marcu?s ex-
periment, and the same sentences were used for
our human judgments. The rest of the sentences
were randomly shuffled, and 527 sentence pairs
were used as a training corpus, 263 pairs as a de-
velopment corpus, and 264 pairs as a test corpus.
To parse these corpora, we used Charniak and
Johnson?s parser (Charniak and Johnson, 2005).
4.2 Settings of Two Experiments
We experimented with/without goal sentence
length for summaries.
In the first experiment, the system was given
only a sentence and no sentence length informa-
tion. The sentence compression problem without
the length information is a general task, but evalu-
ating it is difficult because the correct length of a
summary is not generally defined even by humans.
The following example shows this.
Original:?A font, on the other hand, is a subcate-
gory of a typeface, such as Helvetica Bold or Hel-
vetica Medium.?
Human: ?A font is a subcategory of a typeface,
such as Helvetica Bold.?
System: ?A font is a subcategory of a typeface.?
The ?such as? phrase is removed in this sys-
tem output, but it is not removed in the human
summary. Neither result is wrong, but in such
situations, the evaluation score of the system de-
creases. This is because the compression rate of
each algorithm is different, and evaluation scores
are affected by the lengths of system outputs. For
this reason, results with different lengths cannot be
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
F-
m
ea
su
re
Compression ratio
Noisy-channel
ME
ME + bottom-up
Figure 4: F -measures and compression ratios.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
B
ig
ra
m
 F
-m
ea
su
re
Compression ratio
Noisy-channel
ME
ME + bottom-up
Figure 5: Bigram F -measures and compression
ratios.
compared easily. We therefore examined the rela-
tions between the average compression ratios and
evaluation scores for all methods by changing the
system summary length with the different length
parameter ? introduced in Section 3.1.
In the second experiment, the system was given
a sentence and the length for the compressed sen-
tence. We compressed each input sentence to the
length of the sentence in its goal summary. This
sentence compression problem is easier than that
in which the system can generate sentences of any
length. We selected the highest-scored sentence
from the sentences of length l. Note that the re-
calls, precisions and F-measures have the same
scores in this setting.
4.3 Results of Experiments
The results of the experiment without the sen-
tence length information are shown in Figure 4,
5 and 6. Noisy-channel indicates the results of the
noisy-channel model, ME indicates the results of
the maximum-entropy method, and ME + bottom-
up indicates the results of the maximum-entropy
854
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
B
L
E
U
 s
co
re
Compression ratio
Noisy-channel
ME
ME + bottom-up
Figure 6: BLEU scores and compression ratios.
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
Noisy-channel ME ME+bottom-up
F-measure
bigram-F-measure
BLEU
Figure 7: Results of experiments with length in-
formation.
method with the bottom-up method. We used the
length parameter, ?, introduced in Section 3.1, and
obtained a set of summaries with different aver-
age lengths. We plotted the compression ratios
and three scores in the figures. In these figures,
a compression ratio is the ratio of the total num-
ber of words in compressed sentences to the total
number of words in the original sentences.
In these figures, our maximum entropy meth-
ods obtained higher scores than the noisy-channel
model at all compression ratios. The maximum
entropy method with the bottom-up method obtain
the highest scores on these three measures.
The results of the experiment with the sentence
length information are shown in Figure 7. In this
experiment, the scores of the maximum entropy
methods were higher than the scores of the noisy-
channel model. The maximum entropy method
with the bottom-up method achieved the highest
scores on each measure.
The results of the human judgments are shown
in Table 2. In this experiment, each length of out-
put is same as the length of goal sentence. The
Method Grammar Importance
Human 4.94 4.31
Noisy-channel 3.81 3.38
ME 3.88 3.38
ME + bottom-up 4.22 4.06
Table 2: Results of human judgments.
maximum entropy with the bottom-up method ob-
tained the highest scores of the three methods. We
did t-tests (5% significance). Between the noisy-
channel model and the maximum entropy with the
bottom-up method, importance is significantly dif-
ferent but grammaticality is not. Between the hu-
man and the maximum entropy with the bottom-
up method, grammaticality is significantly differ-
ent but importance is not. There are no significant
differences between the noisy-channel model and
the maximum entropy model.
4.3.1 Problem of Negative Adverbs
One problem of the noisy-channel model is that
it cannot distinguish the meanings of removed
words. That is, it sometimes removes semantically
important words, such as ?not? and ?never?, be-
cause the expansion probability depends only on
non-terminals of parent and daughter nodes.
For example, our test corpus includes 15 sen-
tences that contain ?not?. The noisy-channel
model removed six ?not?s, and the meanings of
the sentences were reversed. However, the two
maximum entropy methods removed only one
?not? because they have ?negative adverb? as a
feature in their models. The first example in Ta-
ble 3 shows one of these sentences. In this exam-
ple, only Noisy-channel removed ?not?.
4.3.2 Effect of Bottom-Up Method
Our bottom-up method achieved the highest
accuracy, in terms of F -measures, bigram F -
measures, BLEU scores and human judgments.
The results were fairly good, especially when it
summarized complex sentences, which have sen-
tences as parts. The second example in Table 3 is
a typical complex sentence. In this example, only
ME + bottom-up correctly remove ?he said?.
Most of the complex sentences were correctly
compressed by the bottom-up method, but a few
sentences like the third example in Table 3 were
not. In this example, the original sentence was
parsed as shown in Figure 8 (left). If this sen-
tence is compressed to the human output, its parse
tree has to be like that in Figure 8 (middle) using
855
Original a file or application ?? alias ??
similar in effect to the ms-dos path
statement provides a visible icon in
folders where an aliased application
does not actually reside .
Human a file or application alias provides
a visible icon in folders where an
aliased application does not actually
reside .
Noisy-
channel
a similar in effect to ms-dos
statement provides a visible icon in
folders where an aliased application
does reside .
ME a or application alias statement
provides a visible icon in folders
where an aliased application does not
actually reside .
ME +
bottom-up
a file or application statement
provides a visible icon in folders
where an aliased application does not
actually reside .
Original the user can then abort the
transmission , he said .
Human the user can then abort the
transmission .
Noisy-
channel
the user can abort the transmission
said .
ME the user can abort the transmission
said .
ME +
bottom-up
the user can then abort the
transmission .
Original it is likely that both companies will
work on integrating multimedia with
database technologies .
Human both companies will work on
integrating multimedia with database
technologies .
Noisy-
channel
it is likely that both companies will
work on integrating .
ME it is likely that both companies will
work on integrating .
ME +
bottom-up
it is will work on integrating
multimedia with database technologies
.
Table 3: Examples of compressed sentences.
our method. When a parse tree is too long from
the root to the leaves like this, some nodes are
trimmed but others are not because we assume that
each trimming probability is independent. The
compressed sentence is ungrammatical, as in the
third example in Table 3.
We have to constrain such ungrammatical sen-
tences or introduce another rule that reconstructs
a short tree as in Figure 8 (right). That is, we in-
troduce a new transformation rule that compresses
(A1 (B (C (A2 ? ? ? )))) to (A2 ? ? ? ).
4.4 Comparison with Original Results
We compared our results with Knight and Marcu?s
original results. They implemented two methods:
one is the noisy-channel model and the other is
a decision-based model. Each model produced
32 compressed sentences, and we calculated F -
measures, bigram F -measures, and BLEU scores.
We used the length parameter ? = 0.5 for the
maximum-entropy method and ? = ?0.25 for
S
VP
is ADJP SBAR
likely that S
both companies 
will ...
S
It
both companies 
will ...
S
VP
SBAR
S
both companies 
will ...
(left) (middle) (right)
Figure 8: Parse trees of complicated complex sen-
tences.
Method Comp. F-measure bigram F-
measure
BLEU
Noisy-
channel
70.19% 68.80 55.96 44.54
Decision-
based
57.26% 71.25 61.93 58.21
ME 66.51% 73.10 62.86 53.51
ME +
bottom-up
58.14% 78.58 70.30 65.26
Human 53.59%
Table 4: Comparison with original results.
the maximum-entropy method with the bottom-up
method. These two values were determined using
experiments on the development set, which did not
contain the 32 test sentences.
The results are shown in Table 4. Noisy-channel
indicates the results of Knight and Marcu?s noisy-
channel model, and Decision-based indicates the
results of Knight and Marcu?s decision-based
model. Comp. indicates the compression ratio of
each result. Our two methods achieved higher ac-
curacy than the noisy-channel model. The results
of the decision-based model and our maximum-
entropy method were not significantly different.
Our maximum-entropy method with the bottom-
up method achieved the highest accuracy.
4.5 Corpus Size and Output Accuracy
In general, using more training data improves the
accuracy of outputs and using less data results in
low accuracy. Our experiment has the problem
that the training corpus was small. To study the re-
lation between training corpus size and accuracy,
we experimented using different training corpus
sizes and compared accuracy of the output.
Figure 9 shows the relations between training
corpus size and three scores, F -measures, bigram
F -measures and BLEU scores, when we used the
maximum entropy method with the bottom-up
method. This graph suggests that the accuracy in-
856
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0  100  200  300  400  500  600  700  800
Sc
or
e
Size of training corpus
BLEU score
F-measure
bigram F-measure
Figure 9: Relation between training corpus size
and evaluation score.
creases when the corpus size is increased. Over
about 600 sentences, the increase becomes slower.
The graph shows that the training corpus was
large enough for this study. However, if we intro-
duced other specific features, such as lexical fea-
tures, a larger corpus would be required.
5 Conclusion
We presented a maximum entropy model to ex-
tend the sentence compression methods described
by Knight and Marcu (Knight and Marcu, 2000).
Our proposals are two-fold. First, our maxi-
mum entropy model allows us to incorporate var-
ious characteristics, such as a mother node or the
depth from a root node, into a probabilistic model
for determining which part of an input sentence
is removed. Second, our bottom-up method of
matching original and compressed parse trees can
match tree structures that cannot be matched using
Knight and Marcu?s method.
The experimental results show that our maxi-
mum entropy method improved the accuracy of
sentence compression as determined by three eval-
uation criteria: F -measures, bigram F -measures
and BLEU scores. Using our bottom-up method
further improved accuracy and produced short
summaries that could not be produced by previ-
ous methods. However, we need to modify this
model to appropriately process more complicated
sentences because some sentences were not cor-
rectly summarized. Human judgments showed
that the maximum entropy model with the bottom-
up method provided more grammatical and more
informative summaries than other methods.
Though our training corpus was small, our ex-
periments demonstrated that the data was suffi-
cient. To improve our approaches, we can intro-
duce more feature functions, especially more se-
mantic or lexical features, and to deal with these
features, we need a larger corpus.
Acknowledgements
We would like to thank Prof. Kevin Knight and
Prof. Daniel Marcu for providing their parallel
corpus and the experimental results.
References
A. L. Berger, V. J. Della Pietra, and S. A. Della Pietra.1996. A Maximum Entropy Approach to NaturalLanguage Processing. Computational Linguistics,22(1):39?71.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking.
In Proc. of ACL?05, pages 173?180.
B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge Trim-
mer: A Parse-and-Trim Approach to Headline Gen-eration. In Proc. of DUC 2003, pages 1?8.
H. Jing and K. R. McKeown. 1999. The decomposi-
tion of human-written summary sentences. In Proc.of SIGIR?99, pages 129?136.
K. Knight and D. Marcu. 2000. Statistics-Based Sum-marization - Step One: Sentence Compression. InProc. of AAAI/IAAI?00, pages 703?710.
I. Langkilde. 2000. Forest-Based Statistical SentenceGeneration. In Proc. of NAACL?00, pages 170?177.
C. Lin. 2004. ROUGE: A Package for AutomaticEvaluation of Summaries. In Text SummarizationBranches Out: Proc. of ACL?04 Workshop, pages
74?81.
R. McDonald. 2006. Discriminative Sentence Com-
pression with Soft Syntactic Evidence. In Proc. ofEACL?06.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a Method for Automatic Evaluation of Ma-chine Translation. In Proc. of ACL?02, pages 311?318.
J. Turner and E. Charniak. 2005. Supervised and Un-supervised Learning for Sentence Compression. InProc. of ACL?05, pages 290?297.
V. Vandeghinste and Y. Pan. 2004. Sentence Com-pression for Automated Subtitling: A Hybrid Ap-
proach. In Text Summarization Branches Out: Proc.of ACL?04 Workshop, pages 89?95.
M. J. Witbrock and V. O. Mittal. 1999. Ultra-Summarization: A Statistical Approach to Generat-ing Highly Condensed Non-Extractive Summaries.
In Proc. of SIGIR?99, pages 315?316.
857
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 17?20,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An intelligent search engine and GUI-based efficient MEDLINE search
tool based on deep syntactic parsing
Tomoko Ohta
Yoshimasa Tsuruoka??
Jumpei Takeuchi
Jin-Dong Kim
Yusuke Miyao
Akane Yakushiji?
Kazuhiro Yoshida
Yuka Tateisi?
Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{okap, yusuke, ninomi, tsuruoka, akane, kmasuda, tj jug,
kyoshida, harasan, jdkim, yucca, tsujii}@is.s.u-tokyo.ac.jp
Takashi Ninomiya?
Katsuya Masuda
Tadayoshi Hara
Jun?ichi Tsujii
Abstract
We present a practical HPSG parser for
English, an intelligent search engine to re-
trieve MEDLINE abstracts that represent
biomedical events and an efficient MED-
LINE search tool helping users to find in-
formation about biomedical entities such
as genes, proteins, and the interactions be-
tween them.
1 Introduction
Recently, biomedical researchers have been fac-
ing the vast repository of research papers, e.g.
MEDLINE. These researchers are eager to search
biomedical correlations such as protein-protein or
gene-disease associations. The use of natural lan-
guage processing technology is expected to re-
duce their burden, and various attempts of infor-
mation extraction using NLP has been being made
(Blaschke and Valencia, 2002; Hao et al, 2005;
Chun et al, 2006). However, the framework of
traditional information retrieval (IR) has difficulty
with the accurate retrieval of such relational con-
cepts. This is because relational concepts are
essentially determined by semantic relations of
words, and keyword-based IR techniques are in-
sufficient to describe such relations precisely.
This paper proposes a practical HPSG parser
for English, Enju, an intelligent search engine for
the accurate retrieval of relational concepts from
?Current Affiliation:
?School of Informatics, University of Manchester
?Knowledge Research Center, Fujitsu Laboratories LTD.
?Faculty of Informatics, Kogakuin University
?Information Technology Center, University of Tokyo
F-Score
GENIA treebank Penn Treebank
HPSG-PTB 85.10% 87.16%
HPSG-GENIA 86.87% 86.81%
Table 1: Performance for Penn Treebank and the
GENIA corpus
MEDLINE, MEDIE, and a GUI-based efficient
MEDLINE search tool, Info-PubMed.
2 Enju: An English HPSG Parser
We developed an English HPSG parser, Enju 1
(Miyao and Tsujii, 2005; Hara et al, 2005; Ni-
nomiya et al, 2005). Table 1 shows the perfor-
mance. The F-score in the table was accuracy
of the predicate-argument relations output by the
parser. A predicate-argument relation is defined
as a tuple ??,wh, a, wa?, where ? is the predi-
cate type (e.g., adjective, intransitive verb), wh
is the head word of the predicate, a is the argu-
ment label (MOD, ARG1, ..., ARG4), and wa is
the head word of the argument. Precision/recall
is the ratio of tuples correctly identified by the
parser. The lexicon of the grammar was extracted
from Sections 02-21 of Penn Treebank (39,832
sentences). In the table, ?HPSG-PTB? means that
the statistical model was trained on Penn Tree-
bank. ?HPSG-GENIA? means that the statistical
model was trained on both Penn Treebank and GE-
NIA treebank as described in (Hara et al, 2005).
The GENIA treebank (Tateisi et al, 2005) consists
of 500 abstracts (4,446 sentences) extracted from
MEDLINE.
Figure 1 shows a part of the parse tree and fea-
1http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
17
Figure 1: Snapshot of Enju
ture structure for the sentence ?NASA officials
vowed to land Discovery early Tuesday at one
of three locations after weather conditions forced
them to scrub Monday?s scheduled return.?
3 MEDIE: a search engine for
MEDLINE
Figure 2 shows the top page of the MEDIE. ME-
DIE is an intelligent search engine for the accu-
rate retrieval of relational concepts from MED-
LINE 2 (Miyao et al, 2006). Prior to retrieval, all
sentences are annotated with predicate argument
structures and ontological identifiers by applying
Enju and a term recognizer.
3.1 Automatically Annotated Corpus
First, we applied a POS analyzer and then Enju.
The POS analyzer and HPSG parser are trained
by using the GENIA corpus (Tsuruoka et al,
2005; Hara et al, 2005), which comprises around
2,000 MEDLINE abstracts annotated with POS
and Penn Treebank style syntactic parse trees
(Tateisi et al, 2005). The HPSG parser generates
parse trees in a stand-off format that can be con-
verted to XML by combining it with the original
text.
We also annotated technical terms of genes and
diseases in our developed corpus. Technical terms
are annotated simply by exact matching of dictio-
2http://www-tsujii.is.s.u-tokyo.ac.jp/medie/
nary entries and the terms separated by space, tab,
period, comma, hat, colon, semi-colon, brackets,
square brackets and slash in MEDLINE.
The entire dictionary was generated by apply-
ing the automatic generation method of name vari-
ations (Tsuruoka and Tsujii, 2004) to the GENA
dictionary for the gene names (Koike and Takagi,
2004) and the UMLS (Unified Medical Language
System) meta-thesaurus for the disease names
(Lindberg et al, 1993). It was generated by ap-
plying the name-variation generation method, and
we obtained 4,467,855 entries of a gene and dis-
ease dictionary.
3.2 Functions of MEDIE
MEDIE provides three types of search, seman-
tic search, keyword search, GCL search. GCL
search provides us the most fundamental and pow-
erful functions in which users can specify the
boolean relations, linear order relation and struc-
tural relations with variables. Trained users can
enjoy all functions in MEDIE by the GCL search,
but it is not easy for general users to write ap-
propriate queries for the parsed corpus. The se-
mantic search enables us to specify an event verb
with its subject and object easily. MEDIE auto-
matically generates the GCL query from the se-
mantic query, and runs the GCL search. Figure 3
shows the output of semantic search for the query
?What disease does dystrophin cause??. This ex-
ample will give us the most intuitive understand-
ings of the proximal and structural retrieval with a
richly annotated parsed corpus. MEDIE retrieves
sentences which include event verbs of ?cause?
and noun ?dystrophin? such that ?dystrophin? is the
subject of the event verbs. The event verb and its
subject and object are highlighted with designated
colors. As seen in the figure, small sentences in
relative clauses, passive forms or coordination are
retrieved. As the objects of the event verbs are
highlighted, we can easily see what disease dys-
trophin caused. As the target corpus is already
annotated with diseases entities, MEDIE can ef-
ficiently retrieve the disease expressions.
4 Info-PubMed: a GUI-based
MEDLINE search tool
Info-PubMed is a MEDLINE search tool with
GUI, helping users to find information about
biomedical entities such as genes, proteins, and
18
Figure 2: Snapshot of MEDIE: top page?
Figure 3: Snapshot of MEDIE: ?What disease does
dystrophin cause??
the interactions between them 3.
Info-PubMed provides information from MED-
LINE on protein-protein interactions. Given the
name of a gene or protein, it shows a list of the
names of other genes/proteins which co-occur in
sentences from MEDLINE, along with the fre-
quency of co-occurrence.
Co-occurrence of two proteins/genes in the
same sentence does not always imply that they in-
teract. For more accurate extraction of sentences
that indicate interactions, it is necessary to iden-
tify relations between the two substances. We
adopted PASs derived by Enju and constructed ex-
traction patterns on specific verbs and their argu-
ments based on the derived PASs (Yakusiji, 2006).
Figure 4: Snapshot of Info-PubMed (1)
Figure 5: Snapshot of Info-PubMed (2)
Figure 6: Snapshot of Info-PubMed (3)
4.1 Functions of Info-PubMed
In the ?Gene Searcher? window, enter the name
of a gene or protein that you are interested in.
For example, if you are interested in Raf1, type
?raf1? in the ?Gene Searcher? (Figure 4). You
will see a list of genes whose description in our
dictionary contains ?raf1? (Figure 5). Then, drag
3http://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/
19
one of the GeneBoxes from the ?Gene Searcher?
to the ?Interaction Viewer.? You will see a list
of genes/proteins which co-occur in the same
sentences, along with co-occurrence frequency.
The GeneBox in the leftmost column is the one
you have moved to ?Interaction Viewer.? The
GeneBoxes in the second column correspond to
gene/proteins which co-occur in the same sen-
tences, followed by the boxes in the third column,
InteractionBoxes.
Drag an InteractionBox to ?ContentViewer? to
see the content of the box (Figure 6). An In-
teractionBox is a set of SentenceBoxes. A Sen-
tenceBox corresponds to a sentence in MEDLINE
in which the two gene/proteins co-occur. A Sen-
tenceBox indicates whether the co-occurrence in
the sentence is direct evidence of interaction or
not. If it is judged as direct evidence of interac-
tion, it is indicated as Interaction. Otherwise, it is
indicated as Co-occurrence.
5 Conclusion
We presented an English HPSG parser, Enju, a
search engine for relational concepts from MED-
LINE, MEDIE, and a GUI-based MEDLINE
search tool, Info-PubMed.
MEDIE and Info-PubMed demonstrate how the
results of deep parsing can be used for intelligent
text mining and semantic information retrieval in
the biomedical domain.
6 Acknowledgment
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Sys-
tems Genomics? (MEXT, Japan) and Solution-
Oriented Research for Science and Technology
(JST, Japan).
References
C. Blaschke and A. Valencia. 2002. The frame-based
module of the SUISEKI information extraction sys-
tem. IEEE Intelligent Systems, 17(2):14?20.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Dis-
covering patterns to extract protein-protein interac-
tions from the literature: Part II. Bioinformatics,
21(15):3294?3300.
H.-W. Chun, Y. Tsuruoka, J.-D. Kim, R. Shiba, N. Na-
gata, T. Hishiki, and J. Tsujii. 2006. Extraction
of gene-disease relations from MedLine using do-
main dictionaries and machine learning. In Proc.
PSB 2006, pages 4?15.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. of ACL?05, pages 83?90.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain. In Proc.
of IJCNLP 2005.
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid parsing
in probabilistic HPSG parsing. In Proc. of IWPT
2005, pages 103?114.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the
GENIA corpus. In Proc. of the IJCNLP 2005, Com-
panion volume, pp. 222?227.
Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-
masa Tsuruoka, Kazuhiro Yoshida, Takashi Ni-
nomiya and Jun?ichi Tsujii. 2006. Semantic Re-
trieval for the Accurate Identification of Relational
Concepts in Massive Textbases. In Proc. of ACL ?06,
to appear.
Yoshimasa Tsuruoka, Yuka Tateisi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Part-of-speech tagger for
biomedical text. In Proc. of the 10th Panhellenic
Conference on Informatics.
Y. Tsuruoka and J. Tsujii. 2004. Improving the per-
formance of dictionary-based approaches in protein
name recognition. Journal of Biomedical Informat-
ics, 37(6):461?470.
Asako Koike and Toshihisa Takagi. 2004.
Gene/protein/family name recognition in biomed-
ical literature. In Proc. of HLT-NAACL 2004
Workshop: Biolink 2004, pages 9?16.
D.A. Lindberg, B.L. Humphreys, and A.T. McCray.
1993. The unified medical language system. Meth-
ods in Inf. Med., 32(4):281?291.
Akane Yakushiji. 2006. Relation Information Extrac-
tion Using Deep Syntactic Analysis. Ph.D. Thesis,
University of Tokyo.
20
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 155?163,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extremely Lexicalized Models for Accurate and Fast HPSG Parsing
Takashi Ninomiya
Information Technology Center
University of Tokyo
Takuya Matsuzaki
Department of Computer Science
University of Tokyo
Yoshimasa Tsuruoka
School of Informatics
University of Manchester
Yusuke Miyao
Department of Computer Science
University of Tokyo
Jun?ichi Tsujii
Department of Computer Science, University of Tokyo
School of Informatics, University of Manchester
SORST, Japan Science and Technology Agency
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
{ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper describes an extremely lexi-
calized probabilistic model for fast and
accurate HPSG parsing. In this model,
the probabilities of parse trees are de-
fined with only the probabilities of select-
ing lexical entries. The proposed model
is very simple, and experiments revealed
that the implemented parser runs around
four times faster than the previous model
and that the proposed model has a high
accuracy comparable to that of the previ-
ous model for probabilistic HPSG, which
is defined over phrase structures. We
also developed a hybrid of our probabilis-
tic model and the conventional phrase-
structure-based model. The hybrid model
is not only significantly faster but also sig-
nificantly more accurate by two points of
precision and recall compared to the pre-
vious model.
1 Introduction
For the last decade, accurate and wide-coverage
parsing for real-world text has been intensively
and extensively pursued. In most of state-of-the-
art parsers, probabilistic events are defined over
phrase structures because phrase structures are
supposed to dominate syntactic configurations of
sentences. For example, probabilities were de-
fined over grammar rules in probabilistic CFG
(Collins, 1999; Klein and Manning, 2003; Char-
niak and Johnson, 2005) or over complex phrase
structures of head-driven phrase structure gram-
mar (HPSG) or combinatory categorial grammar
(CCG) (Clark and Curran, 2004b; Malouf and van
Noord, 2004; Miyao and Tsujii, 2005). Although
these studies vary in the design of the probabilistic
models, the fundamental conception of probabilis-
tic modeling is intended to capture characteristics
of phrase structures or grammar rules. Although
lexical information, such as head words, is known
to significantly improve the parsing accuracy, it
was also used to augment information on phrase
structures.
Another interesting approach to this problem
was using supertagging (Clark and Curran, 2004b;
Clark and Curran, 2004a; Wang and Harper, 2004;
Nasr and Rambow, 2004), which was originally
developed for lexicalized tree adjoining grammars
(LTAG) (Bangalore and Joshi, 1999). Supertag-
ging is a process where words in an input sen-
tence are tagged with ?supertags,? which are lex-
ical entries in lexicalized grammars, e.g., elemen-
tary trees in LTAG, lexical categories in CCG,
and lexical entries in HPSG. Supertagging was,
in the first place, a technique to reduce the cost
of parsing with lexicalized grammars; ambiguity
in assigning lexical entries to words is reduced
by the light-weight process of supertagging be-
fore the heavy process of parsing. Bangalore and
Joshi (1999) claimed that if words can be assigned
correct supertags, syntactic parsing is almost triv-
ial. What this means is that if supertags are cor-
rectly assigned, syntactic structures are almost de-
155
termined because supertags include rich syntac-
tic information such as subcategorization frames.
Nasr and Rambow (2004) showed that the accu-
racy of LTAG parsing reached about 97%, assum-
ing that the correct supertags were given. The
concept of supertagging is simple and interesting,
and the effects of this were recently demonstrated
in the case of a CCG parser (Clark and Curran,
2004a) with the result of a drastic improvement in
the parsing speed. Wang and Harper (2004) also
demonstrated the effects of supertagging with a
statistical constraint dependency grammar (CDG)
parser. They achieved accuracy as high as the
state-of-the-art parsers. However, a supertagger it-
self was used as an external tagger that enumerates
candidates of lexical entries or filters out unlikely
lexical entries just to help parsing, and the best
parse trees were selected mainly according to the
probabilistic model for phrase structures or depen-
dencies with/without the probabilistic model for
supertagging.
We investigate an extreme case of HPSG pars-
ing in which the probabilistic model is defined
with only the probabilities of lexical entry selec-
tion; i.e., the model is never sensitive to charac-
teristics of phrase structures. The model is simply
defined as the product of the supertagging proba-
bilities, which are provided by the discriminative
method with machine learning features of word
trigrams and part-of-speech (POS) 5-grams as de-
fined in the CCG supertagging (Clark and Curran,
2004a). The model is implemented in an HPSG
parser instead of the phrase-structure-based prob-
abilistic model; i.e., the parser returns the parse
tree assigned the highest probability of supertag-
ging among the parse trees licensed by an HPSG.
Though the model uses only the probabilities of
lexical entry selection, the experiments revealed
that it was as accurate as the previous phrase-
structure-based model. Interestingly, this means
that accurate parsing is possible using rather sim-
ple mechanisms.
We also tested a hybrid model of the su-
pertagging and the previous phrase-structure-
based probabilistic model. In the hybrid model,
the probabilities of the previous model are mul-
tiplied by the supertagging probabilities instead
of a preliminary probabilistic model, which is in-
troduced to help the process of estimation by fil-
tering unlikely lexical entries (Miyao and Tsujii,
2005). In the previous model, the preliminary
probabilistic model is defined as the probability
of unigram supertagging. So, the hybrid model
can be regarded as an extension of supertagging
from unigram to n-gram. The hybrid model can
also be regarded as a variant of the statistical CDG
parser (Wang, 2003; Wang and Harper, 2004), in
which the parse tree probabilities are defined as
the product of the supertagging probabilities and
the dependency probabilities. In the experiments,
we observed that the hybrid model significantly
improved the parsing speed, by around three to
four times speed-ups, and accuracy, by around two
points in both precision and recall, over the pre-
vious model. This implies that finer probabilistic
model of lexical entry selection can improve the
phrase-structure-based model.
2 HPSG and probabilistic models
HPSG (Pollard and Sag, 1994) is a syntactic the-
ory based on lexicalized grammar formalism. In
HPSG, a small number of schemata describe gen-
eral construction rules, and a large number of
lexical entries express word-specific characteris-
tics. The structures of sentences are explained us-
ing combinations of schemata and lexical entries.
Both schemata and lexical entries are represented
by typed feature structures, and constraints repre-
sented by feature structures are checked with uni-
fication.
An example of HPSG parsing of the sentence
?Spring has come? is shown in Figure 1. First,
each of the lexical entries for ?has? and ?come?
is unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly apply-
ing schemata to lexical/phrasal signs. Finally, the
parse result is output as a phrasal sign that domi-
nates the sentence.
Given a set W of words and a set F of feature
structures, an HPSG is formulated as a tuple, G =
?L,R?, where
L = {l = ?w,F ?|w ? W, F ? F} is a set of
lexical entries, and
R is a set of schemata; i.e., r ? R is a partial
function: F ? F ? F .
Given a sentence, an HPSG computes a set of
phrasal signs, i.e., feature structures, as a result of
parsing. Note that HPSG is one of the lexicalized
grammar formalisms, in which lexical entries de-
termine the dominant syntactic structures.
156
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
head-comp
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  nounSUBJ  < >COMPS  < >1
=?
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
HEAD  verbSUBJ  <    >COMPS  < >1
HEAD  verbSUBJ  < >COMPS  < >
1
subject-head
head-comp
Figure 1: HPSG parsing.
Previous studies (Abney, 1997; Johnson et al,
1999; Riezler et al, 2000; Malouf and van Noord,
2004; Kaplan et al, 2004; Miyao and Tsujii, 2005)
defined a probabilistic model of unification-based
grammars including HPSG as a log-linear model
or maximum entropy model (Berger et al, 1996).
The probability that a parse result T is assigned to
a given sentence w = ?w1, . . . , wn? is
phpsg(T |w) = 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
exp
(?
u
?ufu(T ?)
)
,
where ?u is a model parameter, fu is a feature
function that represents a characteristic of parse
tree T , and Zw is the sum over the set of all pos-
sible parse trees for the sentence. Intuitively, the
probability is defined as the normalized product
of the weights exp(?u) when a characteristic cor-
responding to fu appears in parse result T . The
model parameters, ?u, are estimated using numer-
ical optimization methods (Malouf, 2002) to max-
imize the log-likelihood of the training data.
However, the above model cannot be easily es-
timated because the estimation requires the com-
putation of p(T |w) for all parse candidates as-
signed to sentence w. Because the number of
parse candidates is exponentially related to the
length of the sentence, the estimation is intractable
for long sentences. To make the model estimation
tractable, Geman and Johnson (Geman and John-
son, 2002) and Miyao and Tsujii (Miyao and Tsu-
jii, 2002) proposed a dynamic programming algo-
rithm for estimating p(T |w). Miyao and Tsujii
HEAD  verbSUBJ  <>COMPS <>
HEAD  nounSUBJ  <>COMPS <>
HEAD  verbSUBJ  <   >COMPS <>
HEAD  verbSUBJ  <   >COMPS <   >
HEAD  verbSUBJ  <   >COMPS <>
subject-head
head-comp
Spring/NN has/VBZ come/VBN
1
1 11 22
froot= <S, has, VBZ,                  >HEAD  verbSUBJ  <NP>COMPS <VP>
fbinary=
head-comp, 1, 0,
1, VP, has, VBZ,                    ,
1, VP, come, VBN,
HEAD  verbSUBJ  <NP>COMPS <VP>
HEAD  verbSUBJ  <NP>COMPS <>
flex= <spring, NN,                    > HEAD  nounSUBJ  <>COMPS <>
Figure 2: Example of features.
(2005) also introduced a preliminary probabilistic
model p0(T |w) whose estimation does not require
the parsing of a treebank. This model is intro-
duced as a reference distribution of the probabilis-
tic HPSG model; i.e., the computation of parse
trees given low probabilities by the model is omit-
ted in the estimation stage. We have
(Previous probabilistic HPSG)
phpsg?(T |w) = p0(T |w) 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
p0(T ?|w) exp
(?
u
?ufu(T ?)
)
p0(T |w) =
n?
i=1
p(li|wi),
where li is a lexical entry assigned to word wi in T
and p(li|wi) is the probability of selecting lexical
entry li for wi.
In the experiments, we compared our model
with the probabilistic HPSG model of Miyao and
Tsujii (2005). The features used in their model are
combinations of the feature templates listed in Ta-
ble 1. The feature templates fbinary and funary
are defined for constituents at binary and unary
branches, froot is a feature template set for the
root nodes of parse trees, and flex is a feature tem-
plate set for calculating the preliminary probabilis-
tic model. An example of features applied to the
parse tree for the sentence ?Spring has come? is
shown in Figure 2.
157
fbinary =
? r, d, c,
spl, syl, hwl, hpl, hll,
spr, syr, hwr, hpr, hlr
?
funary = ?r, sy, hw, hp, hl?
froot = ?sy, hw, hp, hl?
flex = ?wi, pi, li?
combinations of feature templates for fbinary
?r, d, c, hw, hp, hl?, ?r, d, c, hw, hp?, ?r, d, c, hw, hl?,
?r, d, c, sy, hw?, ?r, c, sp, hw, hp, hl?, ?r, c, sp, hw, hp?,
?r, c, sp, hw, hl?, ?r, c, sp, sy, hw?, ?r, d, c, hp, hl?,
?r, d, c, hp?, ?r, d, c, hl?, ?r, d, c, sy?, ?r, c, sp, hp, hl?,
?r, c, sp, hp?, ?r, c, sp, hl?, ?r, c, sp, sy?
combinations of feature templates for funary
?r, hw, hp, hl?, ?r, hw, hp?, ?r, hw, hl?, ?r, sy, hw?,
?r, hp, hl?, ?r, hp?, ?r, hl?, ?r, sy?
combinations of feature templates for froot
?hw, hp, hl?, ?hw, hp?, ?hw, hl?,
?sy, hw?, ?hp, hl?, ?hp?, ?hl?, ?sy?
combinations of feature templates for flex
?wi, pi, li?, ?pi, li?
r name of the applied schema
d distance between the head words of the daughters
c whether a comma exists between daughters
and/or inside daughter phrases
sp number of words dominated by the phrase
sy symbol of the phrasal category
hw surface form of the head word
hp part-of-speech of the head word
hl lexical entry assigned to the head word
wi i-th word
pi part-of-speech for wi
li lexical entry for wi
Table 1: Features.
3 Extremely lexicalized probabilistic
models
In the experiments, we tested parsing with the pre-
vious model for the probabilistic HPSG explained
in Section 2 and other three types of probabilis-
tic models defined with the probabilities of lexi-
cal entry selection. The first one is the simplest
probabilistic model, which is defined with only
the probabilities of lexical entry selection. It is
defined simply as the product of the probabilities
of selecting all lexical entries in the sentence; i.e.,
the model does not use the probabilities of phrase
structures like the previous models.
Given a set of lexical entries, L, a sentence,
w = ?w1, . . . , wn?, and the probabilistic model
of lexical entry selection, p(li ? L|w, i), the first
model is formally defined as follows:
(Model 1)
pmodel1(T |w) =
n?
i=1
p(li|w, i),
where li is a lexical entry assigned to word wi
in T and p(li|w, i) is the probability of selecting
lexical entry li for wi.
The second model is defined as the product of
the probabilities of selecting all lexical entries in
the sentence and the root node probability of the
parse tree. That is, the second model is also de-
fined without the probabilities on phrase struc-
tures:
(Model 2)
pmodel2(T |w) =
1
Zmodel2 pmodel1(T |w) exp
?
??
?
u
(fu?froot)
?ufu(T )
?
??
Zmodel2 =
?
T ?
pmodel1(T ?|w) exp
?
??
?
u
(fu?froot)
?ufu(T ?)
?
?? ,
where Zmodel2 is the sum over the set of all pos-
sible parse trees for the sentence.
The third model is a hybrid of model 1 and the
previous model. The probabilities of the lexical
entries in the previous model are replaced with the
probabilities of lexical entry selection:
(Model 3)
pmodel3(T |w) =
1
Zmodel3 pmodel1(T |w) exp
(?
u
?ufu(T )
)
Zmodel3 =
?
T ?
pmodel1(T ?|w) exp
(?
u
?ufu(T ?)
)
.
In this study, the same model parameters used
in the previous model were used for phrase struc-
tures.
The probabilities of lexical entry selection,
p(li|w, i), are defined as follows:
(Probabilistic Model of Lexical Entry Selection)
p(li|w, i) = 1Zw exp
(?
u
?ufu(li,w, i)
)
158
fexlex =
?
wi?1, wi, wi+1,
pi?2, pi?1, pi, pi+1, pi+2
?
combinations of feature templates
?wi?1?, ?wi?, ?wi+1?,
?pi?2?, ?pi?1?, ?pi?, ?pi+1?, ?pi+2?, ?pi+3?,
?wi?1, wi?, ?wi, wi+1?,
?pi?1, wi?, ?pi, wi?, ?pi+1, wi?,
?pi, pi+1, pi+2, pi+3?, ?pi?2, pi?1, pi?,
?pi?1, pi, pi+1?, ?pi, pi+1, pi+2?
?pi?2, pi?1?, ?pi?1, pi?, ?pi, pi+1?, ?pi+1, pi+2?
Table 2: Features for the probabilities of lexical
entry selection.
procedure Parsing(?w1, . . . , wn?, ?L,R?, ?, ?, ?, ?, ?)
for i = 1 to n
foreach F ? ? {F |?wi, F ? ? L}
p =
?
u ?ufu(F
?)
pi[i? 1, i] ? pi[i? 1, i] ? {F ?}
if (p > ?[i? 1, i, F ?]) then
?[i? 1, i, F ?] ? p
LocalThresholding(i? 1, i,?, ?)
for d = 1 to n
for i = 0 to n? d
j = i+ d
for k = i+ 1 to j ? 1
foreach Fs ? ?[i, k], Ft ? ?[k, j], r ? R
if F = r(Fs, Ft) has succeeded
p = ?[i, k, Fs] + ?[k, j, Ft] +
?
u ?ufu(F )
pi[i, j] ? pi[i, j] ? {F}
if (p > ?[i, j, F ]) then
?[i, j, F ] ? p
LocalThresholding(i, j,?, ?)
GlobalThresholding(i, n, ?)
procedure IterativeParsing(w, G, ?0, ?0, ?0, ?0, ?0, ??, ??, ??,
??, ??, ?last, ?last, ?last, ?last, ?last)? ? ?0; ? ? ?0; ? ? ?0; ? ? ?0; ? ? ?0;
loop while ? ? ?last and ? ? ?last and ? ? ?last and ? ? ?last
and ? ? ?last
call Parsing(w, G, ?, ?, ?, ?, ?)
if pi[1, n] 6= ? then exit
? ? ?+??; ? ? ? +??;
? ? ?+??; ? ? ? +??; ? ? ? +??;
Figure 3: Pseudo-code of iterative parsing for
HPSG.
Zw =
?
l?
exp
(?
u
?ufu(l?,w, i)
)
,
where Zw is the sum over all possible lexical en-
tries for the word wi. The feature templates used
in our model are listed in Table 2 and are word
trigrams and POS 5-grams.
4 Experiments
4.1 Implementation
We implemented the iterative parsing algorithm
(Ninomiya et al, 2005) for the probabilistic HPSG
models. It first starts parsing with a narrow beam.
If the parsing fails, then the beam is widened, and
parsing continues until the parser outputs results
or the beam width reaches some limit. Though
the probabilities of lexical entry selection are in-
troduced, the algorithm for the presented proba-
bilistic models is almost the same as the original
iterative parsing algorithm.
The pseudo-code of the algorithm is shown in
Figure 3. In the figure, the pi[i, j] represents
the set of partial parse results that cover words
wi+1, . . . , wj , and ?[i, j, F ] stores the maximum
figure-of-merit (FOM) of partial parse result F
at cell (i, j). The probability of lexical entry
F is computed as ?u ?ufu(F ) for the previous
model, as shown in the figure. The probability
of a lexical entry for models 1, 2, and 3 is com-
puted as the probability of lexical entry selection,
p(F |w, i). The FOM of a newly created partial
parse, F , is computed by summing the values of
? of the daughters and an additional FOM of F if
the model is the previous model or model 3. The
FOM for models 1 and 2 is computed by only sum-
ming the values of ? of the daughters; i.e., weights
exp(?u) in the figure are assigned zero. The terms
? and ? are the thresholds of the number of phrasal
signs in the chart cell and the beam width for signs
in the chart cell. The terms ? and ? are the thresh-
olds of the number and the beam width of lexical
entries, and ? is the beam width for global thresh-
olding (Goodman, 1997).
4.2 Evaluation
We evaluated the speed and accuracy of parsing
with extremely lexicalized models by using Enju
2.1, the HPSG grammar for English (Miyao et al,
2005; Miyao and Tsujii, 2005). The lexicon of
the grammar was extracted from Sections 02-21 of
the Penn Treebank (Marcus et al, 1994) (39,832
sentences). The grammar consisted of 3,797 lex-
ical entries for 10,536 words1. The probabilis-
tic models were trained using the same portion of
the treebank. We used beam thresholding, global
thresholding (Goodman, 1997), preserved iterative
parsing (Ninomiya et al, 2005) and other tech-
1An HPSG treebank is automatically generated from the
Penn Treebank. Those lexical entries were generated by ap-
plying lexical rules to observed lexical entries in the HPSG
treebank (Nakanishi et al, 2004). The lexicon, however, in-
cluded many lexical entries that do not appear in the HPSG
treebank. The HPSG treebank is used for training the prob-
abilistic model for lexical entry selection, and hence, those
lexical entries that do not appear in the treebank are rarely
selected by the probabilistic model. The ?effective? tag set
size, therefore, is around 1,361, the number of lexical entries
without those never-seen lexical entries.
159
No. of tested sentences Total No. of Avg. length of tested sentences
? 40 words ? 100 words sentences ? 40 words ? 100 words
Section 23 2,162 (94.04%) 2,299 (100.00%) 2,299 20.7 22.2
Section 24 1,157 (92.78%) 1,245 (99.84%) 1,247 21.2 23.0
Table 3: Statistics of the Penn Treebank.
Section 23 (? 40 + Gold POSs) Section 23 (? 100 + Gold POSs)
LP LR UP UR Avg. time LP LR UP UR Avg. time
(%) (%) (%) (%) (ms) (%) (%) (%) (%) (ms)
previous model 87.65 86.97 91.13 90.42 468 87.26 86.50 90.73 89.93 604
model 1 87.54 86.85 90.38 89.66 111 87.23 86.47 90.05 89.27 129
model 2 87.71 87.02 90.51 89.80 109 87.38 86.62 90.17 89.39 130
model 3 89.79 88.97 92.66 91.81 132 89.48 88.58 92.33 91.40 152
Section 23 (? 40 + POS tagger) Section 23 (? 100 + POS tagger)
LP LR UP UR Avg. time LP LR UP UR Avg. time
(%) (%) (%) (%) (ms) (%) (%) (%) (%) (ms)
previous model 85.33 84.83 89.93 89.41 509 84.96 84.25 89.55 88.80 674
model 1 85.26 84.31 89.17 88.18 133 85.00 84.01 88.85 87.82 154
model 2 85.37 84.42 89.25 88.26 134 85.08 84.09 88.91 87.88 155
model 3 87.66 86.53 91.61 90.43 155 87.35 86.29 91.24 90.13 183
Table 4: Experimental results for Section 23.
niques for deep parsing2. The parameters for beam
searching were determined manually by trial and
error using Section 22: ?0 = 4,?? = 4, ?last =
20, ?0 = 1.0,?? = 2.5, ?last = 11.0, ?0 =
12,?? = 4, ?last = 28, ?0 = 6.0,?? =
2.25, ?last = 15.0, ?0 = 8.0,?? = 3.0, and
?last = 20.0. With these thresholding parame-
ters, the parser iterated at most five times for each
sentence.
We measured the accuracy of the predicate-
argument relations output of the parser. A
predicate-argument relation is defined as a tu-
ple ??,wh, a, wa?, where ? is the predicate type
(e.g., adjective, intransitive verb), wh is the head
word of the predicate, a is the argument label
(MODARG, ARG1, ..., ARG4), and wa is the
head word of the argument. Labeled precision
(LP)/labeled recall (LR) is the ratio of tuples cor-
rectly identified by the parser3. Unlabeled pre-
cision (UP)/unlabeled recall (UR) is the ratio of
tuples without the predicate type and the argu-
ment label. This evaluation scheme was the
same as used in previous evaluations of lexicalized
grammars (Hockenmaier, 2003; Clark and Cur-
2Deep parsing techniques include quick check (Malouf
et al, 2000) and large constituent inhibition (Kaplan et al,
2004) as described by Ninomiya et al (2005), but hybrid
parsing with a CFG chunk parser was not used. This is be-
cause we did not observe a significant improvement for the
development set by the hybrid parsing and observed only a
small improvement in the parsing speed by around 10 ms.
3When parsing fails, precision and recall are evaluated,
although nothing is output by the parser; i.e., recall decreases
greatly.
ran, 2004b; Miyao and Tsujii, 2005). The ex-
periments were conducted on an AMD Opteron
server with a 2.4-GHz CPU. Section 22 of the
Treebank was used as the development set, and
the performance was evaluated using sentences of
? 40 and 100 words in Section 23. The perfor-
mance of each parsing technique was analyzed us-
ing the sentences in Section 24 of ? 100 words.
Table 3 details the numbers and average lengths of
the tested sentences of ? 40 and 100 words in Sec-
tions 23 and 24, and the total numbers of sentences
in Sections 23 and 24.
The parsing performance for Section 23 is
shown in Table 4. The upper half of the table
shows the performance using the correct POSs in
the Penn Treebank, and the lower half shows the
performance using the POSs given by a POS tag-
ger (Tsuruoka and Tsujii, 2005). The left and
right sides of the table show the performances for
the sentences of ? 40 and ? 100 words. Our
models significantly increased not only the pars-
ing speed but also the parsing accuracy. Model
3 was around three to four times faster and had
around two points higher precision and recall than
the previous model. Surprisingly, model 1, which
used only lexical information, was very fast and
as accurate as the previous model. Model 2 also
improved the accuracy slightly without informa-
tion of phrase structures. When the automatic POS
tagger was introduced, both precision and recall
dropped by around 2 points, but the tendency to-
wards improved speed and accuracy was again ob-
160
76.00%
78.00%
80.00%
82.00%
84.00%
86.00%
88.00%
0 100 200 300 400 500 600 700 800 900
Parsing time (ms/sentence)
F
-
s
c
o
r
e
previous model
model 1
model 2
model 3
Figure 4: F-score versus average parsing time for sentences in Section 24 of ? 100 words.
served.
The unlabeled precisions and recalls of the pre-
vious model and models 1, 2, and 3 were signifi-
cantly different as measured using stratified shuf-
fling tests (Cohen, 1995) with p-values < 0.05.
The labeled precisions and recalls were signifi-
cantly different among models 1, 2, and 3 and
between the previous model and model 3, but
were not significantly different between the previ-
ous model and model 1 and between the previous
model and model 2.
The average parsing time and labeled F-score
curves of each probabilistic model for the sen-
tences in Section 24 of? 100 words are graphed in
Figure 4. The superiority of our models is clearly
observed in the figure. Model 3 performed sig-
nificantly better than the previous model. Models
1 and 2 were significantly faster with almost the
same accuracy as the previous model.
5 Discussion
5.1 Supertagging
Our probabilistic model of lexical entry selection
can be used as an independent classifier for select-
ing lexical entries, which is called the supertag-
ger (Bangalore and Joshi, 1999; Clark and Curran,
2004b). The CCG supertagger uses a maximum
entropy classifier and is similar to our model.
We evaluated the performance of our probabilis-
tic model as a supertagger. The accuracy of the re-
sulting supertagger on our development set (Sec-
tion 22) is given in Table 5 and Table 6. The test
sentences were automatically POS-tagged. Re-
sults of other supertaggers for automatically ex-
test data accuracy (%)
HPSG supertagger 22 87.51
(this paper)
CCG supertagger 00/23 91.70 / 91.45
(Curran and Clark, 2003)
LTAG supertagger 22/23 86.01 / 86.27
(Shen and Joshi, 2003)
Table 5: Accuracy of single-tag supertaggers. The
numbers under ?test data? are the PTB section
numbers of the test data.
? tags/word word acc. (%) sentence acc. (%)
1e-1 1.30 92.64 34.98
1e-2 2.11 95.08 46.11
1e-3 4.66 96.22 51.95
1e-4 10.72 96.83 55.66
1e-5 19.93 96.95 56.20
Table 6: Accuracy of multi-supertagging.
tracted lexicalized grammars are listed in Table 5.
Table 6 gives the average number of supertags as-
signed to a word, the per-word accuracy, and the
sentence accuracy for several values of ?, which is
a parameter to determine how many lexical entries
are assigned.
When compared with other supertag sets of au-
tomatically extracted lexicalized grammars, the
(effective) size of our supertag set, 1,361 lexical
entries, is between the CCG supertag set (398 cat-
egories) used by Curran and Clark (2003) and the
LTAG supertag set (2920 elementary trees) used
by Shen and Joshi (2003). The relative order based
on the sizes of the tag sets exactly matches the or-
der based on the accuracies of corresponding su-
pertaggers.
161
5.2 Efficacy of extremely lexicalized models
The implemented parsers of models 1 and 2 were
around four times faster than the previous model
without a loss of accuracy. However, what sur-
prised us is not the speed of the models, but
the fact that they were as accurate as the previ-
ous model, though they do not use any phrase-
structure-based probabilities. We think that the
correct parse is more likely to be selected if the
correct lexical entries are assigned high probabil-
ities because lexical entries include specific infor-
mation about subcategorization frames and syn-
tactic alternation, such as wh-movement and pas-
sivization, that likely determines the dominant
structures of parse trees. Another possible rea-
son for the accuracy is the constraints placed by
unification-based grammars. That is, incorrect
parse trees were suppressed by the constraints.
The best performer in terms of speed and ac-
curacy was model 3. The increased speed was,
of course, possible for the same reasons as the
speeds of models 1 and 2. An unexpected but
very impressive result was the significant improve-
ment of accuracy by two points in precision and
recall, which is hard to attain by tweaking param-
eters or hacking features. This may be because
the phrase structure information and lexical in-
formation complementarily improved the model.
The lexical information includes more specific in-
formation about the syntactic alternation, and the
phrase structure information includes information
about the syntactic structures, such as the dis-
tances of head words or the sizes of phrases.
Nasr and Rambow (2004) showed that the accu-
racy of LTAG parsing reached about 97%, assum-
ing that the correct supertags were given. We ex-
emplified the dominance of lexical information in
real syntactic parsing, i.e., syntactic parsing with-
out gold-supertags, by showing that the proba-
bilities of lexical entry selection dominantly con-
tributed to syntactic parsing.
The CCG supertagging demonstrated fast and
accurate parsing for the probabilistic CCG (Clark
and Curran, 2004a). They used the supertag-
ger for eliminating candidates of lexical entries,
and the probabilities of parse trees were calcu-
lated using the phrase-structure-based model with-
out the probabilities of lexical entry selection. Our
study is essentially different from theirs in that the
probabilities of lexical entry selection have been
demonstrated to dominantly contribute to the dis-
ambiguation of phrase structures.
We have not yet investigated whether our results
can be reproduced with other lexicalized gram-
mars. Our results might hold only for HPSG be-
cause HPSG has strict feature constraints and has
lexical entries with rich syntactic information such
as wh-movement.
6 Conclusion
We developed an extremely lexicalized probabilis-
tic model for fast and accurate HPSG parsing.
The model is very simple. The probabilities of
parse trees are defined with only the probabili-
ties of selecting lexical entries, which are trained
by the discriminative methods in the log-linear
model with features of word trigrams and POS 5-
grams as defined in the CCG supertagging. Ex-
periments revealed that the model achieved im-
pressive accuracy as high as that of the previous
model for the probabilistic HPSG and that the im-
plemented parser runs around four times faster.
This indicates that accurate and fast parsing is pos-
sible using rather simple mechanisms. In addi-
tion, we provided another probabilistic model, in
which the probabilities for the leaf nodes in a parse
tree are given by the probabilities of supertag-
ging, and the probabilities for the intermediate
nodes are given by the previous phrase-structure-
based model. The experiments demonstrated not
only speeds significantly increased by three to four
times but also impressive improvement in parsing
accuracy by around two points in precision and re-
call.
We hope that this research provides a novel ap-
proach to deterministic parsing in which only lex-
ical selection and little phrasal information with-
out packed representations dominates the parsing
strategy.
References
Steven P. Abney. 1997. Stochastic attribute-value
grammars. Computational Linguistics, 23(4):597?
618.
Srinivas Bangalore and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39?71.
162
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proc. of ACL?05, pages 173?180.
Stephen Clark and James R. Curran. 2004a. The im-
portance of supertagging for wide-coverage CCG
parsing. In Proc. of COLING-04.
Stephen Clark and James R. Curran. 2004b. Parsing
the WSJ using CCG and log-linear models. In Proc.
of ACL?04, pages 104?111.
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
Univ. of Pennsylvania.
James R. Curran and Stephen Clark. 2003. Investigat-
ing GIS and smoothing for maximum entropy tag-
gers. In Proc. of EACL?03, pages 91?98.
Stuart Geman and Mark Johnson. 2002. Dynamic pro-
gramming for parsing and estimation of stochastic
unification-based grammars. In Proc. of ACL?02,
pages 279?286.
Joshua Goodman. 1997. Global thresholding and mul-
tiple pass parsing. In Proc. of EMNLP-1997, pages
11?25.
Julia Hockenmaier. 2003. Parsing with generative
models of predicate-argument structure. In Proc. of
ACL?03, pages 359?366.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Proc.
of ACL ?99, pages 535?541.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In Proc. of
HLT/NAACL?04.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proc. of ACL?03,
pages 423?430.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value
grammars. In Proc. of IJCNLP-04 Workshop ?Be-
yond Shallow Analyses?.
Robert Malouf, John Carroll, and Ann Copestake.
2000. Efficient feature structure operations with-
out compilation. Journal of Natural Language En-
gineering, 6(1):29?46.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
CoNLL-2002, pages 49?55.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc. of
HLT 2002, pages 292?297.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. of ACL?05, pages 83?90.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii,
2005. Keh-Yih Su, Jun?ichi Tsujii, Jong-Hyeok Lee
and Oi Yee Kwong (Eds.), Natural Language Pro-
cessing - IJCNLP 2004 LNAI 3248, chapter Corpus-
oriented Grammar Development for Acquiring a
Head-driven Phrase Structure Grammar from the
Penn Treebank, pages 684?693. Springer-Verlag.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2004. An empirical investigation of the effect of lex-
ical rules on parsing with a treebank grammar. In
Proc. of TLT?04, pages 103?114.
Alexis Nasr and Owen Rambow. 2004. Supertagging
and full parsing. In Proc. of the 7th International
Workshop on Tree Adjoining Grammar and Related
Formalisms (TAG+7).
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid pars-
ing in probabilistic hpsg parsing. In Proc. of IWPT
2005, pages 103?114.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling
of constraint-based grammars using log-linear mea-
sures and EM training. In Proc. of ACL?00, pages
480?487.
Libin Shen and Aravind K. Joshi. 2003. A SNoW
based supertagger with application to NP chunking.
In Proc. of ACL?03, pages 505?512.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proc. of HLT/EMNLP
2005, pages 467?474.
Wen Wang and Mary P. Harper. 2004. A statisti-
cal constraint dependency grammar (CDG) parser.
In Proc. of ACL?04 Incremental Parsing work-
shop: Bringing Engineering and Cognition To-
gether, pages 42?49.
Wen Wang. 2003. Statistical Parsing and Language
Modeling based on Constraint Dependency Gram-
mar. Ph.D. thesis, Purdue University.
163
Proceedings of the 10th Conference on Parsing Technologies, pages 60?68,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A log-linear model with an n-gram reference distribution for accurate HPSG
parsing
Takashi Ninomiya
Information Technology Center
University of Tokyo
ninomi@r.dl.itc.u-tokyo.ac.jp
Takuya Matsuzaki
Department of Computer Science
University of Tokyo
matuzaki@is.s.u-tokyo.ac.jp
Yusuke Miyao
Department of Computer Science
University of Tokyo
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Department of Computer Science, University of Tokyo
School of Informatics, University of Manchester
NaCTeM (National Center for Text Mining)
tsujii@is.s.u-tokyo.ac.jp
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
Abstract
This paper describes a log-linear model with
an n-gram reference distribution for accurate
probabilistic HPSG parsing. In the model,
the n-gram reference distribution is simply
defined as the product of the probabilities
of selecting lexical entries, which are pro-
vided by the discriminative method with ma-
chine learning features of word and POS
n-gram as defined in the CCG/HPSG/CDG
supertagging. Recently, supertagging be-
comes well known to drastically improve
the parsing accuracy and speed, but su-
pertagging techniques were heuristically in-
troduced, and hence the probabilistic mod-
els for parse trees were not well defined.
We introduce the supertagging probabilities
as a reference distribution for the log-linear
model of the probabilistic HPSG. This is the
first model which properly incorporates the
supertagging probabilities into parse tree?s
probabilistic model.
1 Introduction
For the last decade, fast, accurate and wide-coverage
parsing for real-world text has been pursued in
sophisticated grammar formalisms, such as head-
driven phrase structure grammar (HPSG) (Pollard
and Sag, 1994), combinatory categorial grammar
(CCG) (Steedman, 2000) and lexical function gram-
mar (LFG) (Bresnan, 1982). They are preferred
because they give precise and in-depth analyses
for explaining linguistic phenomena, such as pas-
sivization, control verbs and relative clauses. The
main difficulty of developing parsers in these for-
malisms was how to model a well-defined proba-
bilistic model for graph structures such as feature
structures. This was overcome by a probabilistic
model which provides probabilities of discriminat-
ing a correct parse tree among candidates of parse
trees in a log-linear model or maximum entropy
model (Berger et al, 1996) with many features for
parse trees (Abney, 1997; Johnson et al, 1999; Rie-
zler et al, 2000; Malouf and van Noord, 2004; Ka-
plan et al, 2004; Miyao and Tsujii, 2005). Follow-
ing this discriminative approach, techniques for effi-
ciency were investigated for estimation (Geman and
Johnson, 2002; Miyao and Tsujii, 2002; Malouf and
van Noord, 2004) and parsing (Clark and Curran,
2004b; Clark and Curran, 2004a; Ninomiya et al,
2005).
An interesting approach to the problem of parsing
efficiency was using supertagging (Clark and Cur-
60
ran, 2004b; Clark and Curran, 2004a; Wang, 2003;
Wang and Harper, 2004; Nasr and Rambow, 2004;
Ninomiya et al, 2006; Foth et al, 2006; Foth and
Menzel, 2006), which was originally developed for
lexicalized tree adjoining grammars (LTAG) (Ban-
galore and Joshi, 1999). Supertagging is a process
where words in an input sentence are tagged with
?supertags,? which are lexical entries in lexicalized
grammars, e.g., elementary trees in LTAG, lexical
categories in CCG, and lexical entries in HPSG. The
concept of supertagging is simple and interesting,
and the effects of this were recently demonstrated in
the case of a CCG parser (Clark and Curran, 2004a)
with the result of a drastic improvement in the pars-
ing speed. Wang and Harper (2004) also demon-
strated the effects of supertagging with a statisti-
cal constraint dependency grammar (CDG) parser
by showing accuracy as high as the state-of-the-art
parsers, and Foth et al (2006) and Foth and Menzel
(2006) reported that accuracy was significantly im-
proved by incorporating the supertagging probabili-
ties into manually tuned Weighted CDG. Ninomiya
et al (2006) showed the parsing model using only
supertagging probabilities could achieve accuracy as
high as the probabilistic model for phrase structures.
This means that syntactic structures are almost de-
termined by supertags as is claimed by Bangalore
and Joshi (1999). However, supertaggers themselves
were heuristically used as an external tagger. They
filter out unlikely lexical entries just to help parsing
(Clark and Curran, 2004a), or the probabilistic mod-
els for phrase structures were trained independently
of the supertagger?s probabilistic models (Wang and
Harper, 2004; Ninomiya et al, 2006). In the case of
supertagging of Weighted CDG (Foth et al, 2006),
parameters for Weighted CDG are manually tuned,
i.e., their model is not a well-defined probabilistic
model.
We propose a log-linear model for probabilistic
HPSG parsing in which the supertagging probabil-
ities are introduced as a reference distribution for
the probabilistic HPSG. The reference distribution is
simply defined as the product of the probabilities of
selecting lexical entries, which are provided by the
discriminative method with machine learning fea-
tures of word and part-of-speech (POS) n-gram as
defined in the CCG/HPSG/CDG supertagging. This
is the first model which properly incorporates the su-
pertagging probabilities into parse tree?s probabilis-
tic model. We compared our model with the proba-
bilistic model for phrase structures (Miyao and Tsu-
jii, 2005). This model uses word and POS unigram
for its reference distribution, i.e., the probabilities of
unigram supertagging. Our model can be regarded
as an extension of a unigram reference distribution
to an n-gram reference distribution with features that
are used in supertagging. We also compared with a
probabilistic model in (Ninomiya et al, 2006). The
probabilities of their model are defined as the prod-
uct of probabilities of supertagging and probabilities
of the probabilistic model for phrase structures, but
their model was trained independently of supertag-
ging probabilities, i.e., the supertagging probabili-
ties are not used for reference distributions.
2 HPSG and probabilistic models
HPSG (Pollard and Sag, 1994) is a syntactic theory
based on lexicalized grammar formalism. In HPSG,
a small number of schemata describe general con-
struction rules, and a large number of lexical entries
express word-specific characteristics. The structures
of sentences are explained using combinations of
schemata and lexical entries. Both schemata and
lexical entries are represented by typed feature struc-
tures, and constraints represented by feature struc-
tures are checked with unification.
An example of HPSG parsing of the sentence
?Spring has come? is shown in Figure 1. First,
each of the lexical entries for ?has? and ?come?
is unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly applying
schemata to lexical/phrasal signs. Finally, the parse
result is output as a phrasal sign that dominates the
sentence.
Given a set W of words and a set F of feature
structures, an HPSG is formulated as a tuple, G =
?L,R?, where
L = {l = ?w,F ?|w ? W, F ? F} is a set of
lexical entries, and
R is a set of schemata; i.e., r ? R is a partial
function: F ? F ? F .
Given a sentence, an HPSG computes a set of
phrasal signs, i.e., feature structures, as a result of
61
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
head-comp
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  nounSUBJ  < >COMPS  < >1
=?
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
HEAD  verbSUBJ  <    >COMPS  < >1
HEAD  verbSUBJ  < >COMPS  < >
1
subject-head
head-comp
Figure 1: HPSG parsing.
parsing. Note that HPSG is one of the lexicalized
grammar formalisms, in which lexical entries deter-
mine the dominant syntactic structures.
Previous studies (Abney, 1997; Johnson et al,
1999; Riezler et al, 2000; Malouf and van Noord,
2004; Kaplan et al, 2004; Miyao and Tsujii, 2005)
defined a probabilistic model of unification-based
grammars including HPSG as a log-linear model or
maximum entropy model (Berger et al, 1996). The
probability that a parse result T is assigned to a
given sentence w = ?w1, . . . , wn? is
(Probabilistic HPSG)
phpsg(T |w) = 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
exp
(?
u
?ufu(T ?)
)
,
where ?u is a model parameter, fu is a feature func-
tion that represents a characteristic of parse tree T ,
and Zw is the sum over the set of all possible parse
trees for the sentence. Intuitively, the probability
is defined as the normalized product of the weights
exp(?u) when a characteristic corresponding to fu
appears in parse result T . The model parameters, ?u,
are estimated using numerical optimization methods
(Malouf, 2002) to maximize the log-likelihood of
the training data.
However, the above model cannot be easily esti-
mated because the estimation requires the compu-
tation of p(T |w) for all parse candidates assigned
to sentence w. Because the number of parse can-
didates is exponentially related to the length of the
sentence, the estimation is intractable for long sen-
tences. To make the model estimation tractable, Ge-
man and Johnson (Geman and Johnson, 2002) and
Miyao and Tsujii (Miyao and Tsujii, 2002) proposed
a dynamic programming algorithm for estimating
p(T |w). Miyao and Tsujii (2005) also introduced a
preliminary probabilistic model p0(T |w) whose es-
timation does not require the parsing of a treebank.
This model is introduced as a reference distribution
(Jelinek, 1998; Johnson and Riezler, 2000) of the
probabilistic HPSG model; i.e., the computation of
parse trees given low probabilities by the model is
omitted in the estimation stage (Miyao and Tsujii,
2005), or a probabilistic model can be augmented
by several distributions estimated from the larger
and simpler corpus (Johnson and Riezler, 2000). In
(Miyao and Tsujii, 2005), p0(T |w) is defined as the
product of probabilities of selecting lexical entries
with word and POS unigram features:
(Miyao and Tsujii (2005)?s model)
puniref (T |w) = p0(T |w) 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
p0(T ?|w) exp
(?
u
?ufu(T ?)
)
p0(T |w) =
n?
i=1
p(li|wi),
where li is a lexical entry assigned to word wi in
T and p(li|wi) is the probability of selecting lexical
entry li for wi.
In the experiments, we compared our model with
other two types of probabilistic models using a su-
pertagger (Ninomiya et al, 2006). The first one is
the simplest probabilistic model, which is defined
with only the probabilities of lexical entry selec-
tion. It is defined simply as the product of the prob-
abilities of selecting all lexical entries in the sen-
tence; i.e., the model does not use the probabilities
of phrase structures like the probabilistic models ex-
plained above. Given a set of lexical entries, L, a
sentence, w = ?w1, . . . , wn?, and the probabilistic
model of lexical entry selection, p(li ? L|w, i), the
first model is formally defined as follows:
62
HEAD  verbSUBJ  <>COMPS <>
HEAD  nounSUBJ  <>COMPS <>
HEAD  verbSUBJ  <   >COMPS <>
HEAD  verbSUBJ  <   >COMPS <   >
HEAD  verbSUBJ  <   >COMPS <>
subject-head
head-comp
Spring/NN has/VBZ come/VBN
1
1 11 22
froot= <S, has, VBZ,                  >HEAD  verbSUBJ  <NP>COMPS <VP>
fbinary=
head-comp, 1, 0,
1, VP, has, VBZ,                    ,
1, VP, come, VBN,
HEAD  verbSUBJ  <NP>COMPS <VP>
HEAD  verbSUBJ  <NP>COMPS <>
flex= <spring, NN,                    > HEAD  nounSUBJ  <>COMPS <>
Figure 2: Example of features.
(Ninomiya et al (2006)?s model 1)
pmodel1(T |w) =
n?
i=1
p(li|w, i),
where li is a lexical entry assigned to word wi in T
and p(li|w, i) is the probability of selecting lexical
entry li for wi.
The probabilities of lexical entry selection,
p(li|w, i), are defined as follows:
(Probabilistic model of lexical entry selection)
p(li|w, i) = 1Zw exp
(?
u
?ufu(li,w, i)
)
Zw =
?
l?
exp
(?
u
?ufu(l?,w, i)
)
,
where Zw is the sum over all possible lexical entries
for the word wi.
The second model is a hybrid model of supertag-
ging and the probabilistic HPSG. The probabilities
are given as the product of Ninomiya et al (2006)?s
model 1 and the probabilistic HPSG.
(Ninomiya et al (2006)?s model 3)
pmodel3(T |w) = pmodel1(T |w)phpsg(T |w)
In the experiments, we compared our model with
Miyao and Tsujii (2005)?s model and Ninomiya et
fbinary =
? r, d, c,
spl, syl, hwl, hpl, hll,
spr, syr, hwr, hpr, hlr
?
funary = ?r, sy, hw, hp, hl?
froot = ?sy, hw, hp, hl?
flex = ?wi, pi, li?
fsptag =
?
wi?1, wi, wi+1,
pi?2, pi?1, pi, pi+1, pi+2
?
r name of the applied schema
d distance between the head words of the daughters
c whether a comma exists between daughters
and/or inside daughter phrases
sp number of words dominated by the phrase
sy symbol of the phrasal category
hw surface form of the head word
hp part-of-speech of the head word
hl lexical entry assigned to the head word
wi i-th word
pi part-of-speech for wi
li lexical entry for wi
Table 1: Feature templates.
al. (2006)?s model 1 and 3. The features used in our
model and their model are combinations of the fea-
ture templates listed in Table 1 and Table 2. The
feature templates fbinary and funary are defined for
constituents at binary and unary branches, froot is a
feature template set for the root nodes of parse trees.
flex is a feature template set for calculating the uni-
gram reference distribution and is used in Miyao and
Tsujii (2005)?s model. fsptag is a feature template
set for calculating the probabilities of selecting lex-
ical entries in Ninomiya et al (2006)?s model 1 and
3. The feature templates in fsptag are word trigrams
and POS 5-grams. An example of features applied
to the parse tree for the sentence ?Spring has come?
is shown in Figure 2.
3 Probabilistic HPSG with an n-gram
reference distribution
In this section, we propose a probabilistic model
with an n-gram reference distribution for probabilis-
tic HPSG parsing. This is an extension of Miyao
and Tsujii (2005)?s model by replacing the unigram
reference distribution with an n-gram reference dis-
tribution. Our model is formally defined as follows:
63
combinations of feature templates for fbinary
?r, d, c, hw, hp, hl?, ?r, d, c, hw, hp?, ?r, d, c, hw, hl?,
?r, d, c, sy, hw?, ?r, c, sp, hw, hp, hl?, ?r, c, sp, hw, hp?,
?r, c, sp, hw, hl?, ?r, c, sp, sy, hw?, ?r, d, c, hp, hl?,
?r, d, c, hp?, ?r, d, c, hl?, ?r, d, c, sy?, ?r, c, sp, hp, hl?,
?r, c, sp, hp?, ?r, c, sp, hl?, ?r, c, sp, sy?
combinations of feature templates for funary
?r, hw, hp, hl?, ?r, hw, hp?, ?r, hw, hl?, ?r, sy, hw?,
?r, hp, hl?, ?r, hp?, ?r, hl?, ?r, sy?
combinations of feature templates for froot
?hw, hp, hl?, ?hw, hp?, ?hw, hl?,
?sy, hw?, ?hp, hl?, ?hp?, ?hl?, ?sy?
combinations of feature templates for flex
?wi, pi, li?, ?pi, li?
combinations of feature templates for fsptag
?wi?1?, ?wi?, ?wi+1?,
?pi?2?, ?pi?1?, ?pi?, ?pi+1?, ?pi+2?, ?pi+3?,
?wi?1, wi?, ?wi, wi+1?,
?pi?1, wi?, ?pi, wi?, ?pi+1, wi?,
?pi, pi+1, pi+2, pi+3?, ?pi?2, pi?1, pi?,
?pi?1, pi, pi+1?, ?pi, pi+1, pi+2?
?pi?2, pi?1?, ?pi?1, pi?, ?pi, pi+1?, ?pi+1, pi+2?
Table 2: Combinations of feature templates.
(Probabilistic HPSG with an n-gram reference distribution)
pnref (T |w) =
1
Znref pmodel1(T |w) exp
(?
u
?ufu(T )
)
Znref =
?
T ?
pmodel1(T ?|w) exp
(?
u
?ufu(T ?)
)
.
In our model, Ninomiya et al (2006)?s model 1
is used as a reference distribution. The probabilis-
tic model of lexical entry selection and its feature
templates are the same as defined in Ninomiya et al
(2006)?s model 1.
The formula of our model is the same as Ni-
nomiya et al (2006)?s model 3. But, their model
is not a probabilistic model with a reference distri-
bution. Both our model and their model consist of
the probabilities for lexical entries (= pmodel1(T |w))
and the probabilities for phrase structures (= the rest
of each formula). The only difference between our
model and their model is the way of how to train
model parameters for phrase structures. In both our
model and their model, the parameters for lexical en-
tries (= the parameters of pmodel1(T |w)) are first es-
timated from the word and POS sequences indepen-
dently of the parameters for phrase structures. That
is, the estimated parameters for lexical entries are
the same in both models, and hence the probabilities
of pmodel1(T |w) of both models are the same. Note
that the parameters for lexical entries will never be
updated after this estimation stage; i.e., the parame-
ters for lexical entries are not estimated in the same
time with the parameters for phrase structures. The
difference of our model and their model is the esti-
mation of parameters for phrase structures. In our
model, given the probabilities for lexical entries, the
parameters for phrase structures are estimated so as
to maximize the entire probabilistic model (= the
product of the probabilities for lexical entries and
the probabilities for phrase structures) in the train-
ing corpus. In their model, the parameters for phrase
structures are trained without using the probabili-
ties for lexical entries, i.e., the parameters for phrase
structures are estimated so as to maximize the prob-
abilities for phrase structures only. That is, the pa-
rameters for lexical entries and the parameters for
phrase structures are trained independently in their
model.
Miyao and Tsujii (2005)?s model also uses a ref-
erence distribution, but with word and POS unigram
features, as is explained in the previous section. The
only difference between our model and Miyao and
Tsujii (2005)?s model is that our model uses se-
quences of word and POS tags as n-gram features
for selecting lexical entries in the same way as su-
pertagging does.
4 Experiments
We evaluated the speed and accuracy of parsing
by using Enju 2.1, the HPSG grammar for English
(Miyao et al, 2005; Miyao and Tsujii, 2005). The
lexicon of the grammar was extracted from Sec-
tions 02-21 of the Penn Treebank (Marcus et al,
1994) (39,832 sentences). The grammar consisted
of 3,797 lexical entries for 10,536 words1. The prob-
1An HPSG treebank is automatically generated from the
Penn Treebank. Those lexical entries were generated by apply-
ing lexical rules to observed lexical entries in the HPSG tree-
bank (Nakanishi et al, 2004). The lexicon, however, included
many lexical entries that do not appear in the HPSG treebank.
64
No. of tested sentences Total No. of sentences Avg. length of tested sentences
Section 23 2,299 (100.00%) 2,299 22.2
Section 24 1,245 (99.84%) 1,247 23.0
Table 3: Statistics of the Penn Treebank.
Section 23 (Gold POSs)
LP LR LF UP UR UF Avg. time
(%) (%) (%) (%) (%) (%) (ms)
Miyao and Tsujii (2005)?s model 87.26 86.50 86.88 90.73 89.93 90.33 604
Ninomiya et al (2006)?s model 1 87.23 86.47 86.85 90.05 89.27 89.66 129
Ninomiya et al (2006)?s model 3 89.48 88.58 89.02 92.33 91.40 91.86 152
our model 1 89.78 89.28 89.53 92.58 92.07 92.32 234
our model 2 90.03 89.60 89.82 92.82 92.37 92.60 1379
Section 23 (POS tagger)
LP LR LF UP UR UF Avg. time
(%) (%) (%) (%) (%) (%) (ms)
Miyao and Tsujii (2005)?s model 84.96 84.25 84.60 89.55 88.80 89.17 674
Ninomiya et al (2006)?s model 1 85.00 84.01 84.50 88.85 87.82 88.33 154
Ninomiya et al (2006)?s model 3 87.35 86.29 86.82 91.24 90.13 90.68 183
Matsuzaki et al (2007)?s model 86.93 86.47 86.70 - - - 30
our model 1 87.28 87.05 87.17 91.62 91.38 91.50 260
our model 2 87.56 87.46 87.51 91.88 91.77 91.82 1821
Table 4: Experimental results for Section 23.
abilistic models were trained using the same portion
of the treebank. We used beam thresholding, global
thresholding (Goodman, 1997), preserved iterative
parsing (Ninomiya et al, 2005) and quick check
(Malouf et al, 2000).
We measured the accuracy of the predicate-
argument relations output of the parser. A
predicate-argument relation is defined as a tuple
??,wh, a, wa?, where ? is the predicate type (e.g.,
adjective, intransitive verb), wh is the head word of
the predicate, a is the argument label (MODARG,
ARG1, ..., ARG4), and wa is the head word of
the argument. Labeled precision (LP)/labeled re-
call (LR) is the ratio of tuples correctly identified
by the parser2. Unlabeled precision (UP)/unlabeled
recall (UR) is the ratio of tuples without the pred-
icate type and the argument label. This evaluation
scheme was the same as used in previous evaluations
of lexicalized grammars (Hockenmaier, 2003; Clark
The HPSG treebank is used for training the probabilistic model
for lexical entry selection, and hence, those lexical entries that
do not appear in the treebank are rarely selected by the proba-
bilistic model. The ?effective? tag set size, therefore, is around
1,361, the number of lexical entries without those never-seen
lexical entries.
2When parsing fails, precision and recall are evaluated, al-
though nothing is output by the parser; i.e., recall decreases
greatly.
and Curran, 2004b; Miyao and Tsujii, 2005). The
experiments were conducted on an AMD Opteron
server with a 2.4-GHz CPU. Section 22 of the Tree-
bank was used as the development set, and the per-
formance was evaluated using sentences of ? 100
words in Section 23. The performance of each
model was analyzed using the sentences in Section
24 of ? 100 words. Table 3 details the numbers
and average lengths of the tested sentences of ? 100
words in Sections 23 and 24, and the total numbers
of sentences in Sections 23 and 24.
The parsing performance for Section 23 is shown
in Table 4. The upper half of the table shows the per-
formance using the correct POSs in the Penn Tree-
bank, and the lower half shows the performance us-
ing the POSs given by a POS tagger (Tsuruoka and
Tsujii, 2005). LF and UF in the figure are labeled
F-score and unlabeled F-score. F-score is the har-
monic mean of precision and recall. We evaluated
our model in two settings. One is implemented with
a narrow beam width (?our model 1? in the figure),
and the other is implemented with a wider beam
width (?our model 2? in the figure)3. ?our model
3The beam thresholding parameters for ?our model 1? are
?0 = 10,?? = 5, ?last = 30, ?0 = 5.0,?? = 2.5, ?last =15.0, ?0 = 10,?? = 5, ?last = 30, ?0 = 5.0,?? =2.5, ?last = 15.0, ?0 = 6.0,?? = 3.5, and ?last = 20.0.
65
83.00%
83.50%
84.00%
84.50%
85.00%
85.50%
86.00%
86.50%
87.00%
87.50%
88.00%
0 100 200 300 400 500 600 700 800 900
Parsing time (ms/sentence)
F-sc
ore
Miyao and Tsujii(2005)'s modelNinomiya et al(2006)'s model 1Ninomiya et al(2006)'s model 3
our model
Figure 3: F-score versus average parsing time for sentences in Section 24 of ? 100 words.
1? was introduced to measure the performance with
balanced F-score and speed, which we think appro-
priate for practical use. ?our model 2? was intro-
duced to measure how high the precision and re-
call could reach by sacrificing speed. Our mod-
els increased the parsing accuracy. ?our model 1?
was around 2.6 times faster and had around 2.65
points higher F-score than Miyao and Tsujii (2005)?s
model. ?our model 2? was around 2.3 times slower
but had around 2.9 points higher F-score than Miyao
and Tsujii (2005)?s model. We must admit that the
difference between our models and Ninomiya et al
(2006)?s model 3 was not as great as the differ-
ence from Miyao and Tsujii (2005)?s model, but ?our
model 1? achieved 0.56 points higher F-score, and
?our model 2? achieved 0.8 points higher F-score.
When the automatic POS tagger was introduced, F-
score dropped by around 2.4 points for all models.
We also compared our model with Matsuzaki et
al. (2007)?s model. Matsuzaki et al (2007) pro-
The terms ? and ? are the thresholds of the number of phrasal
signs in the chart cell and the beam width for signs in the chart
cell. The terms ? and ? are the thresholds of the number and
the beam width of lexical entries, and ? is the beam width for
global thresholding (Goodman, 1997). The terms with suffixes
0 are the initial values. The parser iterates parsing until it suc-
ceeds to generate a parse tree. The parameters increase for each
iteration by the terms prefixed by ?, and parsing finishes when
the parameters reach the terms with suffixes last. Details of the
parameters are written in (Ninomiya et al, 2005). The beam
thresholding parameters for ?our model 2? are ?0 = 18,?? =
6, ?last = 42, ?0 = 9.0,?? = 3.0, ?last = 21.0, ?0 =18,?? = 6, ?last = 42, ?0 = 9.0,?? = 3.0, ?last = 21.0.
In ?our model 2?, the global thresholding was not used.
posed a technique for efficient HPSG parsing with
supertagging and CFG filtering. Their results with
the same grammar and servers are also listed in the
lower half of Table 4. They achieved drastic im-
provement in efficiency. Their parser ran around 6
times faster than Ninomiya et al (2006)?s model 3,
9 times faster than ?our model 1? and 60 times faster
than ?our model 2.? Instead, our models achieved
better accuracy. ?our model 1? had around 0.5 higher
F-score, and ?our model 2? had around 0.8 points
higher F-score. Their efficiency is mainly due to
elimination of ungrammatical lexical entries by the
CFG filtering. They first parse a sentence with a
CFG grammar compiled from an HPSG grammar,
and then eliminate lexical entries that are not in the
parsed CFG trees. Obviously, this technique can
also be applied to the HPSG parsing of our mod-
els. We think that efficiency of HPSG parsing with
our models will be drastically improved by applying
this technique.
The average parsing time and labeled F-score
curves of each probabilistic model for the sentences
in Section 24 of ? 100 words are graphed in Fig-
ure 3. The graph clearly shows the difference of
our model and other models. As seen in the graph,
our model achieved higher F-score than other model
when beam threshold was widen. This implies that
other models were probably difficult to reach the F-
score of ?our model 1? and ?our model 2? for Section
23 even if we changed the beam thresholding param-
eters. However, F-score of our model dropped eas-
66
ily when we narrow down the beam threshold, com-
pared to other models. We think that this is mainly
due to its bad implementation of parser interface.
The n-gram reference distribution is incorporated
into the kernel of the parser, but the n-gram fea-
tures and a maximum entropy estimator are defined
in other modules; n-gram features are defined in a
grammar module, and a maximum entropy estimator
for the n-gram reference distribution is implemented
with a general-purpose maximum entropy estimator
module. Consequently, strings that represent the n-
gram information are very frequently changed into
feature structures and vice versa when they go in and
out of the kernel of the parser. On the other hand, Ni-
nomiya et al (2006)?s model 3 uses the supertagger
as an external module. Once the parser acquires the
supertagger?s outputs, the n-gram information never
goes in and out of the kernel. This advantage of Ni-
nomiya et al (2006)?s model can apparently be im-
plemented in our model, but this requires many parts
of rewriting of the implemented parser. We estimate
that the overhead of the interface is around from 50
to 80 ms/sentence. We think that re-implementation
of the parser will improve the parsing speed as esti-
mated. In Figure 3, the line of our model crosses the
line of Ninomiya et al (2006)?s model. If the esti-
mation is correct, our model will be faster and more
accurate so that the lines in the figure do not cross.
Speed-up in our model is left as a future work.
5 Conclusion
We proposed a probabilistic model in which su-
pertagging is consistently integrated into the prob-
abilistic model for HPSG. In the model, the n-gram
reference distribution is simply defined as the prod-
uct of the probabilities of selecting lexical entries
with machine learning features of word and POS n-
gram as defined in the CCG/HPSG/CDG supertag-
ging. We conducted experiments on the Penn Tree-
bank with a wide-coverage HPSG parser. In the ex-
periments, we compared our model with the prob-
abilistic HPSG with a unigram reference distribu-
tion (Miyao and Tsujii, 2005) and the probabilistic
HPSG with supertagging (Ninomiya et al, 2006).
Though our model was not as fast as Ninomiya
et al (2006)?s models, it achieved the highest ac-
curacy among them. Our model had around 2.65
points higher F-score than Miyao and Tsujii (2005)?s
model and around 0.56 points higher F-score than
the Ninomiya et al (2006)?s model 3. When we sac-
rifice parsing speed, our model achieved around 2.9
points higher F-score than Miyao and Tsujii (2005)?s
model and around 0.8 points higher F-score than Ni-
nomiya et al (2006)?s model 3. Our model achieved
higher F-score because parameters for phrase struc-
tures in our model are trained with the supertagging
probabilities, which are not in other models.
References
Steven P. Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?618.
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39?71.
Joan Bresnan. 1982. The Mental Representation of
Grammatical Relations. MIT Press, Cambridge, MA.
Stephen Clark and James R. Curran. 2004a. The impor-
tance of supertagging for wide-coverage CCG parsing.
In Proc. of COLING-04.
Stephen Clark and James R. Curran. 2004b. Parsing the
WSJ using CCG and log-linear models. In Proc. of
ACL?04, pages 104?111.
Killian Foth and Wolfgang Menzel. 2006. Hybrid pars-
ing: Using probabilistic models as predictors for a
symbolic parser. In Proc. of COLING-ACL 2006.
Killian Foth, Tomas By, and Wolfgang Menzel. 2006.
Guiding a constraint dependency parser with su-
pertags. In Proc. of COLING-ACL 2006.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochas-
tic unification-based grammars. In Proc. of ACL?02,
pages 279?286.
Joshua Goodman. 1997. Global thresholding and mul-
tiple pass parsing. In Proc. of EMNLP-1997, pages
11?25.
Julia Hockenmaier. 2003. Parsing with generative
models of predicate-argument structure. In Proc. of
ACL?03, pages 359?366.
F. Jelinek. 1998. Statistical Methods for Speech Recog-
nition. The MIT Press.
67
Mark Johnson and Stefan Riezler. 2000. Exploiting
auxiliary distributions in stochastic unification-based
grammars. In Proc. of NAACL-2000, pages 154?161.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proc. of ACL ?99,
pages 535?541.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In Proc. of
HLT/NAACL?04.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value gram-
mars. In Proc. of IJCNLP-04 Workshop ?Beyond
Shallow Analyses?.
Robert Malouf, John Carroll, and Ann Copestake. 2000.
Efficient feature structure operations without compi-
lation. Journal of Natural Language Engineering,
6(1):29?46.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
CoNLL-2002, pages 49?55.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient HPSG parsing with supertagging and
CFG-filtering. In Proc. of IJCAI 2007, pages 1671?
1676.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In Proc. of HLT
2002, pages 292?297.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilistic
disambiguation models for wide-coverage HPSG pars-
ing. In Proc. of ACL?05, pages 83?90.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsu-
jii, 2005. Keh-Yih Su, Jun?ichi Tsujii, Jong-Hyeok
Lee and Oi Yee Kwong (Eds.), Natural Language
Processing - IJCNLP 2004 LNAI 3248, chapter
Corpus-oriented Grammar Development for Acquir-
ing a Head-driven Phrase Structure Grammar from the
Penn Treebank, pages 684?693. Springer-Verlag.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2004. An empirical investigation of the effect of lexi-
cal rules on parsing with a treebank grammar. In Proc.
of TLT?04, pages 103?114.
Alexis Nasr and Owen Rambow. 2004. Supertagging
and full parsing. In Proc. of the 7th International
Workshop on Tree Adjoining Grammar and Related
Formalisms (TAG+7).
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke Miyao,
and Jun?ichi Tsujii. 2005. Efficacy of beam threshold-
ing, unification filtering and hybrid parsing in proba-
bilistic HPSG parsing. In Proc. of IWPT 2005, pages
103?114.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006. Ex-
tremely lexicalized models for accurate and fast HPSG
parsing. In Proc. of EMNLP 2006, pages 155?163.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proc. of ACL?00, pages 480?487.
Mark Steedman. 2000. The Syntactic Process. The MIT
Press.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidirec-
tional inference with the easiest-first strategy for tag-
ging sequence data. In Proc. of HLT/EMNLP 2005,
pages 467?474.
Wen Wang and Mary P. Harper. 2004. A statistical con-
straint dependency grammar (CDG) parser. In Proc.
of ACL?04 Incremental Parsing workshop: Bringing
Engineering and Cognition Together, pages 42?49.
Wen Wang. 2003. Statistical Parsing and Language
Modeling based on Constraint Dependency Grammar.
Ph.D. thesis, Purdue University.
68
Lenient Default Unification for Robust Processing
within Unification Based Grammar Formalisms
Takashi NINOMIYA,?? Yusuke MIYAO,? and Jun?ichi TSUJII??
? Department of Computer Science, University of Tokyo
? CREST, Japan Science and Technology Corporation
e-mail: {ninomi, yusuke, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper describes new default unification, lenient
default unification. It works efficiently, and gives
more informative results because it maximizes the
amount of information in the result, while other de-
fault unification maximizes it in the default. We also
describe robust processing within the framework of
HPSG. We extract grammar rules from the results of
robust parsing using lenient default unification. The
results of a series of experiments show that parsing
with the extracted rules works robustly, and the cov-
erage of a manually-developed HPSG grammar for
Penn Treebank was greatly increased with a little
overgeneration.
1 Introduction
Parsing has often been considered to be crucial
for natural language processing, thus, efficient and
wide coverage parsing has been extensively pur-
sued in natural language literature. This study aims
at robust processing within the Head-driven Phrase
Structure Grammar (HPSG) to extend the cover-
age of manually-developed HPSG grammars. The
meaning of ?robust processing? is not limited to ro-
bust processing for ill-formed sentences found in
a spoken language, but includes robust processing
for sentences which are well-formed but beyond the
grammar writer?s expectation.
Studies of robust parsing within unification-based
grammars have been explored by many researchers
(Douglas and Dale, 1992; Imaichi and Matsumoto,
1995). They classified the errors found in analyzing
ill-formed sentences into several categories to make
them tractable, e.g., constraint violation, missing or
extra elements, etc. In this paper, we focus on re-
covery from the constraint violation errors, which is
a violation of feature values. All errors in agreement
fall into this category. Since many of the grammat-
ical components in HPSG are written as constraints
represented by feature structures, many of the er-
rors are expected to be recovered by the recovery of
constraint violation errors.
This paper proposes two new types of default
unification and describes their application to robust
processing. Default unification was originally stud-
ied to develop a system of lexical semantics to deal
with the default inheritance in a lexicon, but it is
also desirable for the recovery of such constraint vi-
olation errors due to the following merits: i) default
unification is always well-defined, and ii) a feature
structure is relaxed such that the amount of infor-
mation is maximized. From the viewpoint of robust
processing, an amount of lost information can be re-
garded as a cost (i.e., penalty) of robust processing.
In other words, default unification tries to minimize
the cost. Given a strict feature structure F and a
default feature structure G, default unification is de-
fined as unification that satisfies the following (writ-
ten as F <unionsq G): 1) It is always defined. 2) All strict
information is preserved. That is, F v (F <unionsq G). 3) It
reduces to standard unification in the case of F and
G being consistent. That is, (F <unionsq G) = (F unionsqG) if
F unionsqG is defined. With these definitions, Douglas?
relaxation technique can be regarded as a sort of de-
fault unification. They classify constraints into nec-
essary constraints and optional constraints, which
can be regarded as strict information and default in-
formation in the definition of default unification.
Carpenter (1993) gave concise and comprehen-
sive definitions of default unification. However, the
problem in Carpenter?s default unification is that it
tries to maximize the amount of information in a de-
fault feature structure, not the result of default uni-
fication. Consider the case where a grammar rule
is the default feature structure and the daughters are
the strict feature structure. The head feature prin-
ciple can be described as the structure-sharing be-
tween the values of the head feature in a mother
and in a head daughter. The set of constraints that
represent the head feature principle consists of only
one element. When we lose just one element in the
head feature principle, a large amount of informa-
tion in the daughter?s substructure is not propagated
to its mother. As Copestake (1993) mentioned, an-
other problem in Carpenter?s default unification is
that the time complexity for finding the optimal an-
swer of default unification is exponential because
we have to verify the unifiability of the power set of
constraints in a default feature structure.
Here, we propose ideal lenient default unifica-
tion, which tries to maximize the amount of infor-
mation of a result, not the amount of default infor-
mation. Thus, the problem of losing a large amount
of information in structure-sharing never arises. We
also propose lenient default unification whose algo-
rithm is much more efficient than the ideal one. Its
time complexity is linear to the size of the strict fea-
ture structure and the default feature structure. In-
stead, the amount of information of a result derived
by lenient default unification is equal to or less than
that of the ideal one.
We apply lenient default unification to robust pro-
cessing. Given an HPSG grammar, our approach
takes two steps; i) extraction of grammar rules from
the results of robust parsing using lenient default
unification for applying the HPSG grammar rules
(offline parsing), and ii) runtime parsing using the
HPSG grammar with the extracted rules. The ex-
tracted rules work robustly since they reflect the ef-
fects of recovery rules applied during offline robust
parsing and the conditions in which they are ap-
plied.
Sections 3 and 4 describe our default unification.
Our robust parsing is explained in Section 5. Sec-
tion 6 shows a series of experiments of robust pars-
ing with default unification.
2 Background
Default unification has been investigated by many
researchers (Bouma, 1990; Russell et al, 1991;
Copestake, 1993; Carpenter, 1993; Lascarides and
Copestake, 1999) in the context of developing lexi-
cal semantics. Here, we first explain the definition
given by Carpenter (1993) because his definition is
both concise and comprehensive.
2.1 Carpenter?s Default Unification
Carpenter proposed two types of default unification,
credulous default unification and skeptical default
unification.
(Credulous Default Unification)
F <unionsqc G =
{
F unionsqG?
??? G? v G is maximal such thatF unionsqG? is defined
}
(Skeptical Default Unification)
F <unionsqs G = ?(F <unionsqc G)
F is called a strict feature structure, whose in-
formation must not be lost, and G is called a de-
fault feature structure, whose information might be
lost but as little as possible so that F and G can be
unified. A credulous default unification operation
is greedy in that it tries to maximize the amount of
information it retains from the default feature struc-
ture. This definition returns a set of feature struc-
tures rather than a unique feature structure.
Skeptical default unification simply generalizes
the set of feature structures which results from cred-
ulous default unification. The definition of skeptical
default unification leads to a unique result. The de-
fault information which can be found in every result
of credulous default unification remains. Following
is an example of skeptical default unification.
[F: a] <unionsqs
[
F: 1 b
G: 1
H: c
]
= u
{ [F: a
G: b
H: c
]
,
[
F: 1 a
G: 1
H: c
]}
=
[F: a
G: ?
H: c
]
2.2 Forced Unification
Forced unification is another way to unify incon-
sistent feature structures. Forced unification always
succeeds by supposing the existence of the top type
(the most specific type) in a type hierarchy. Unifi-
cation of any pair of types is defined in the type hi-
erarchy, and therefore unification of any pair of fea-
ture structures is defined. One example is described
by Imaichi and Matsumoto (1995) (they call it cost-
based unification). Their unification always suc-
ceeds by supposing the top type, and it also keeps
the information about inconsistent types. Forced
unification can be regarded as one of the toughest
robust processing because it always succeeds and
never loses the information embedded in feature
structures. The drawback of forced unification is
the postprocessing of parsing, i.e., feature structures
with top types are not tractable. We write Funionsq f G for
the forced unification of F and G.
3 Ideal Lenient Default Unification
In this section, we explain our default unification,
ideal lenient default unification. Ideal lenient de-
fault unification tries to maximize the amount of
information of the result, subsuming the result of
forced unification. In other words, ideal lenient de-
fault unification tries to generate a result as similar
as possible to the result of forced unification such
that the result is defined in the type hierarchy with-
out the top type. Formally, we have:
Definition 3.1 Ideal Lenient Default Unification
F <unionsqi G = ?
{
F unionsqG?
?????
G? v f (Funionsq f G) is maximal
such that F unionsqG? is defined
without the top type
}
where v f is a subsumption relation where the top
type is defined.
From the definition of skeptical default unifica-
tion, ideal lenient default unification is equivalent
to F <unionsqs (Funionsq f G) assuming that skeptical default uni-
fication does not add the default information that in-
cludes the top type to the strict information.
Consider the following feature structures.
F =
?
???
F:
[F:a
G:b
H:c
]
G:
[F:a
G:a
H:c
]
?
???,G =
[
F: 1
G: 1
]
In the case of Carpenter?s default unification, the
results of skeptical and credulous default unification
become as follows: F <unionsqs G = F,F <unionsqc G = {F}. This
is because G is generalized to the bottom feature
structure, and hence the result is equivalent to the
strict feature structure.
With ideal lenient default unification, the result
becomes as follows.
F <unionsqi G =
?
?????
F:
[
F: 1 a
G:b
H: 2 c
]
G:
[
F: 1
G:a
H: 2
]
?
?????
v f
?
?F: 1
[F:a
G:>
H:c
]
G: 1
?
?
Note that the result of ideal lenient default unifica-
tion subsumes the result of forced unification.
As we can see in the example, ideal lenient de-
fault unification tries to keep as much information
of the structure-sharing as possible (ideal lenient
default unification succeeds in preserving the struc-
ture-sharing tagged as 1 and 2 though skeptical and
credulous default unification fail to capture it).
4 Lenient Default Unification
The optimal answer for ideal lenient default unifica-
tion can be found by calculating F <unionsqs (Funionsq f G). As
Copestake (1993) mentioned, the time complexity
of skeptical default unification is exponential, and
therefore the time complexity of ideal lenient de-
fault unification is also exponential.
As other researchers pursued efficient default uni-
fication (Bouma, 1990; Russell et al, 1991; Copes-
take, 1993), we also propose another definition of
default unification, which we call lenient default
unification. An algorithm derived for it finds its an-
swer efficiently.
Given a strict feature structure F and a default
feature structure G, let H be the result of forced uni-
fication, i.e., H = Funionsq f G. We define topnode(H)
as a function that returns the fail points (the nodes
that are assigned the top type in H), f pnode(H)
as a function that returns the fail path nodes (the
nodes from which a fail point can be reached), and
f pchild(H) as a a function that returns all the nodes
that are not fail path nodes but the immediate chil-
dren of fail path nodes.
Consider the following feature structures.
F =
?
????
F:F:
[F:F:a
G:G:b
H:H:c
]
G:
[
G:
[F:F:a
G:G:a
H:H:c
]
H:H:a
]
?
????,G =
[
F:F: 1?
G:G: 1
]
Figure 1 shows F , G and H in the graph notation.
This figure also shows the nodes that correspond to
topnode(H), f pnode(H) and f pchild(H).
 


 

 




F =

 
G =
	





H =












	




	


)(Htopnode? )(Hfpnode?
An Indexing Scheme for Typed Feature Structures
Takashi NINOMIYA,?? Takaki MAKINO,#? and Jun?ichi TSUJII??
?Department of Computer Science, University of Tokyo
?CREST, Japan Science and Technology Corporation
#Department of Complexity Science and Engineering, University of Tokyo?
?BSI, RIKEN
e-mail: {ninomi, mak, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper describes an indexing substrate for typed
feature structures (ISTFS), which is an efficient re-
trieval engine for typed feature structures. Given a
set of typed feature structures, the ISTFS efficiently
retrieves its subset whose elements are unifiable or
in a subsumption relation with a query feature struc-
ture. The efficiency of the ISTFS is achieved by
calculating a unifiability checking table prior to re-
trieval and finding the best index paths dynami-
cally.
1 Introduction
This paper describes an indexing substrate for typed
feature structures (ISTFS), which is an efficient re-
trieval engine for typed feature structures (TFSs)
(Carpenter, 1992). Given a set of TFSs, the ISTFS
can efficiently retrieve its subset whose elements are
unifiable or in a subsumption relation with a query
TFS.
The ultimate purpose of the substrate is aimed at
the construction of large-scale intelligent NLP sys-
tems such as IR or QA systems based on unification-
based grammar formalisms (Emele, 1994). Recent
studies on QA systems (Harabagiu et al, 2001) have
shown that systems using a wide-coverage noun tax-
onomy, quasi-logical form, and abductive inference
outperform other bag-of-words techniques in accu-
racy. Our ISTFS is an indexing substrate that en-
ables such knowledge-based systems to keep and
retrieve TFSs, which can represent symbolic struc-
tures such as quasi-logical forms or a taxonomy and
the output of parsing of unification-based grammars
for a very large set of documents.
The algorithm for our ISTFS is concise and effi-
cient. The basic idea used in our algorithm uses a
necessary condition for unification.
(Necessary condition for unification) Let PathF
be the set of all feature paths defined in
? This research is partially funded by JSPS Research Fellow-
ship for Young Scientists.
TFS F , and FollowedType(pi,F) be the
type assigned to the node reached by fol-
lowing path pi .1 If two TFSs F and G
are unifiable, then FollowedType(pi,F) and
FollowedType(pi,G) are defined and unifiable
for all pi ? (PathF ?PathG).
The Quick Check algorithm described in (Torisawa
and Tsujii, 1995; Malouf et al, 2000) also uses
this condition for the efficient checking of unifia-
bility between two TFSs. Given two TFSs and stat-
ically determined paths, the Quick Check algorithm
can efficiently determine whether these two TFSs
are non-unifiable or there is some uncertainty about
their unifiability by checking the path values. It is
worth noting that this algorithm is used in many
modern unification grammar-based systems, e.g.,
the LKB system (Copestake, 1999) and the PAGE
system (Kiefer et al, 1999).
Unlike the Quick Check algorithm, which checks
unifiability between two TFSs, our ISTFS checks
unifiability between one TFS and n TFSs. The
ISTFS checks unifiability by using dynamically de-
termined paths, not statically determined paths. In
our case, using only statically determined paths
might extremely degrades the system performance.
Suppose that any statically determined paths are not
defined in the query TFS. Because there is no path
to be used for checking unifiability, it is required to
unify a query with every element of the data set. It
should also be noted that using all paths defined in
a query TFS severely degrades the system perfor-
mance because a TFS is a huge data structure com-
prised of hundreds of nodes and paths, i.e., most of
the retrieval time will be consumed in filtering. The
1More precisely, FollowedType(pi,F) returns the type as-
signed to the node reached by following pi from the root node
of FSPAT H(pi,F), which is defined as follows.
FSPAT H(pi,F) = F unionsqPV (pi)
PV (pi) =
{
the least feature structure where
path pi is defined
That is, FollowedType(pi,F) might be defined even if pi does
not exist in F .
ISTFS dynamically finds the index paths in order of
highest filtering rate. In the experiments, most ?non-
unifiable? TFSs were filtered out by using only a few
index paths found by our optimization algorithm.
2 Algorithm
Briefly, the algorithm for the ISTFS proceeds ac-
cording to the following steps.
1. When a set of data TFSs is given, the ISTFS
prepares a path value table and a unifiability
checking table in advance.
2. When a query TFS is given, the ISTFS re-
trieves TFSs which are unifiable with the query
from the set of data TFSs by performing the
following steps.
(a) The ISTFS finds the index paths by using
the unifiability checking table. The index
paths are the most restrictive paths in the
query in the sense that the set of the data
TFSs can be limited to the smallest one.
(b) The ISTFS filters out TFSs that are non-
unifiable by referring to the values of the
index paths in the path value table.
(c) The ISTFS finds exactly unifiable TFSs
by unifying the query and the remains of
filtering one-by-one, in succession.
This algorithm can also find the TFSs that are
in the subsumption relation, i.e., more-specific or
more-general, by preparing subsumption checking
tables in the same way it prepared a unifiability
checking table.
2.1 Preparing Path Value Table and
Unifiability Checking Table
Let D(= {F1,F2, . . . ,Fn}) be the set of data TFSs.
When D is given, the ISTFS prepares two tables, a
path value table Dpi,? and a unifiability checking ta-
ble Upi,? , for all pi ? PathD and ? ? Type. 2 A
TFS might have a cycle in its graph structure. In
that case, a set of paths becomes infinite. Fortu-
nately, our algorithm works correctly even if the set
of paths is a subset of all existing paths. Therefore,
paths which might cause an infinite set can be re-
moved from the path set. We define the path value
table and the unifiability checking table as follows:
Dpi,? ? {F |F ?D ? FollowedType(pi,F) = ?}
Upi,? ? ?
?
(??Type ? ?unionsq? is defined)
|Dpi,? |
2Type is a finite set of types.
Assuming that ? is the type of the node reached by
following pi in a query TFS, we can limit D to a
smaller set by filtering out ?non-unifiable? TFSs. We
have the smaller set:
U ?pi,? ?
?
?
(??Type ? ?unionsq? is defined)
Dpi,?
Upi,? corresponds to the size of U ?pi,? . Note that the
ISTFS does not prepare a table of U ?pi,? statically, but
just prepares a table of Upi,? whose elements are in-
tegers. This is because the system?s memory would
easily be exhausted if we actually made a table of
U ?pi,? . Instead, the ISTFS finds the best paths by re-
ferring to Upi,? and calculates only U ?pi,? where pi is
the best index path.
Suppose the type hierarchy and D depicted in
Figure 1 are given. The tables in Figure 2 show Dpi,?
and Upi,? calculated from Figure 1.
2.2 Retrieval
In what follows, we suppose that D was given, and
we have already calculated Dpi,? and Upi,? .
Finding Index Paths
The best index path is the most restrictive path in the
query in the sense thatD can be limited to the small-
est set by referring to the type of the node reached
by following the index path in the query.
Suppose a query TFS X and a constant k, which is
the maximum number of index paths, are given. The
best index path in PathX is path pi such that Upi,? is
minimum where ? is the type of the node reached
by following pi from the root node of X . We can
also find the second best index path by finding the
path pi s.t. Upi,? is the second smallest. In the same
way, we can find the i-th best index path s.t. i ? k.
Filtering
Suppose k best index paths have already been cal-
culated. Given an index path pi , let ? be the type of
the node reached by following pi in the query. An
element of D that is unifiable with the query must
have a node that can be reached by following pi and
whose type is unifiable with ? . Such TFSs (=U ?pi,? )
can be collected by taking the union of Dpi,? , where
? is unifiable with ? . For each index path, U ?pi,?
can be calculated, and the D can be limited to the
smaller one by taking their intersection. After filter-
ing, the ISTFS can find exactly unifiable TFSs by
unifying the query with the remains of filtering one
by one.
Suppose the type hierarchy and D in Figure 1 are
?

  	

  

   ?????? ?
  
  
:CDR
:CAR
F1 =
?
????
cons
CAR: 1
CDR:
?
?
cons
CAR: 2
CDR:
[ cons
CAR: 3
CDR: nil
]
?
?
?
????
F2 =
[ cons
CAR: 4
CDR: nil
]
,
F3 =
?
?
cons
CAR: 5
CDR:
[ cons
CAR: 6
CDR: nil
]
?
?
D = {F1,F2,F3}
Figure 1: An example of a type hierarchy and TFSs
Dpi,?
?
pi ? integer 1 2 3 4 5 6 list cons nil
? ? ? ? ? ? ? ? ? ? {F1,F2 ,F3} ?
CAR: ? ? {F1} ? ? {F2} {F3} ? ? ?
CDR: ? ? ? ? ? ? ? ? ? {F1 ,F3} {F2}
CDR:CAR: ? ? ? {F1} ? ? ? {F3} ? ? ?
CDR:CDR: ? ? ? ? ? ? ? ? ? {F1} {F3}
CDR:CDR:CAR: ? ? ? ? {F1} ? ? ? ? ? ?
CDR:CDR:CDR: ? ? ? ? ? ? ? ? ? ? {F1}? is an empty set.
Upi,?
?
pi ? integer 1 2 3 4 5 6 list cons nil
? 3 0 0 0 0 0 0 0 3 *3 0
CAR: 3 *3 1 0 0 1 1 0 0 0 0
CDR: 3 0 0 0 0 0 0 0 3 *2 1
CDR:CAR: 2 2 0 1 0 0 0 *1 0 0 0
CDR:CDR: 2 0 0 0 0 0 0 0 *2 1 1
CDR:CDR:CAR: 1 1 0 0 1 0 0 0 0 0 0
CDR:CDR:CDR: 1 0 0 0 0 0 0 0 1 0 1
Figure 2: An example of Dpi,? and Upi,?
QuerySetA QuerySetB
# of the data TFSs 249,994 249,994
Avg. # of unifiables 68,331.58 1,310.70
Avg. # of more specifics 66,301.37 0.00
Avg. # of more generals 0.00 0.00
Table 1: The average number of data TFSs and an-
swers for QuerySetA and QuerySetB
given, and the following query X is given:
X =
?
?
cons
CAR: integer
CDR:
[ cons
CAR: 6
CDR: list
]
?
?
In Figure 2, Upi,? where the pi and ? pair exists in
the query is indicated with an asterisk. The best in-
dex paths are determined in ascending order of Upi,?
indicated with an asterisk in the figure. In this ex-
ample, the best index path is CDR:CAR: and its corre-
sponding type in the query is 6. Therefore the unifi-
able TFS can be found by referring to DCDR:CAR:,6,
and this is {F3}.
3 Performance Evaluation
We measured the performance of the ISTFS on a
IBM xSeries 330 with a 1.26-GHz PentiumIII pro-
cessor and a 4-GB memory. The data set consist-
ing of 249,994 TFSs was generated by parsing the
 
  
  
  
          	 
     
      	 
        
        
 

	


Figure 3: The size of Dpi,? for the size of the data
set
800 bracketed sentences in the Wall Street Journal
corpus (the first 800 sentences in Wall Street Jour-
nal 00) in the Penn Treebank (Marcus et al, 1993)
with the XHPSG grammar (Tateisi et al, 1998). The
size of the data set was 151 MB. We also generated
two sets of query TFSs by parsing five randomly
selected sentences in the Wall Street Journal cor-
pus (QuerySetA and QuerySetB). Each set had 100
query TFSs. Each element of QuerySetA was the
daughter part of the grammar rules. Each element of
QuerySetB was the right daughter part of the gram-
mar rules whose left daughter part is instantiated.
Table 1 shows the number of data TFSs and the av-
erage number of unifiable, more-specific and more-
general TFSs for QuerySetA and QuerySetB. The
total time for generating the index tables (i.e., a set
of paths, the path value table (Dpi,? ), the unifiabil-
ity checking table (Upi,? ), and the two subsumption
checking tables) was 102.59 seconds. The size of
the path value table was 972 MByte, and the size of
the unifiability checking table and the two subsump-
tion checking tables was 13 MByte. The size of the
unifiability and subsumption checking tables is neg-
ligible in comparison with that of the path value ta-
ble. Figure 3 shows the growth of the size of the
path value table for the size of the data set. As seen
in the figure, it grows proportionally.
Figures 4, 5 and 6 show the results of retrieval
time for finding unifiable TFSs, more-specific TFSs
and more-general TFSs respectively. In the figures,
the X-axis shows the number of index paths that
are used for limiting the data set. The ideal time
means the unification time when the filtering rate is
100%, i.e., our algorithm cannot achieve higher ef-
ficiency than this optimum. The overall time is the
sum of the filtering time and the unification time.
As illustrated in the figures, using one to ten index
paths achieves the best performance. The ISTFS
achieved 2.84 times speed-ups in finding unifiables
for QuerySetA, and 37.90 times speed-ups in find-
ing unifiables for QuerySetB.
Figure 7 plots the filtering rate. In finding unifi-
able TFSs in QuerySetA, more than 95% of non-
unifiable TFSs are filtered out by using only three
index paths. In the case of QuerySetB, more than
98% of non-unifiable TFSs are filtered out by using
only one index path.
4 Discussion
Our approach is said to be a variation of path in-
dexing. Path indexing has been extensively studied
in the field of automated reasoning, declarative pro-
gramming and deductive databases for term index-
ing (Sekar et al, 2001), and was also studied in the
field of XML databases (Yoshikawa et al, 2001). In
path indexing, all existing paths in the database are
first enumerated, and then an index for each path is
prepared. Other existing algorithms differed from
ours in i) data structures and ii) query optimization.
In terms of data structures, our algorithm deals with
typed feature structures while their algorithms deal
with PROLOG terms, i.e., variables and instanti-
ated terms. Since a type matches not only the same
type or variables but unifiable types, our problem is
much more complicated. Yet, in our system, hierar-
chical relations like a taxonomy can easily be repre-
sented by types. In terms of query optimization, our
algorithm dynamically selects index paths to mini-
mize the searching cost. Basically, their algorithms
take an intersection of candidates for all paths in a
query, or just limiting the length of paths (McCune,
2001). Because such a set of paths often contains
many paths ineffective for limiting answers, our ap-
proach should be more efficient than theirs.
5 Conclusion and Future Work
We developed an efficient retrieval engine for TFSs,
ISTFS. The efficiency of ISTFS is achieved by cal-
culating a unifiability checking table prior to re-
trieval and finding the best index paths dynamically.
In future work, we are going to 1) minimize the
size of the index tables, 2) develop a feature struc-
ture DBMS on a second storage, and 3) incorporate
structure-sharing information into the index tables.
References
B. Carpenter. 1992. The Logic of Typed Feature Struc-
tures. Cambridge University Press, Cambridge, U.K.
A. Copestake. 1999. The (new) LKB system. Technical
report, CSLI, Stanford University.
M. C. Emele. 1994. TFS ? the typed feature struc-
ture representation formalism. In Proc. of the Interna-
tional Workshop on Sharable Natural Language Re-
sources (SNLR-1994).
S. Harabagiu, D. Moldovan, M. Pas?ca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. G??rju, V. Rus, and
Mora?rescu. 2001. Falcon: Boosting knowledge for
answer engines. In Proc. of TREC 9.
B. Kiefer, H.-U. Krieger, J. Carroll, and R. Malouf.
1999. A bag of useful techniques for efficient and ro-
bust parsing. In Proc. of ACL-1999, pages 473?480,
June.
R. Malouf, J. Carroll, and A. Copestake. 2000. Effi-
cient feature structure operations without compilation.
Journal of Natural Language Engineering, 6(1):29?
46.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguistics,
19(2):313?330.
W. McCune. 2001. Experiments with discrimination-
tree indexing and path indexing for term retrieval. Au-
tomated Reasoning, 18(2):147?167.
R. Sekar, I. V. Ramakrishnan, and A. Voronkov. 2001.
Term indexing. In Handbook of Automated Reason-
ing, pages 1853?1964. Elsevier Science Publishers.
Y. Tateisi, K. Torisawa, Y. Miyao, and J. Tsujii. 1998.
Translating the XTAG English grammar to HPSG. In
Proc. of TAG+4, pages 172?175.
K. Torisawa and J. Tsujii. 1995. Compiling HPSG-
style grammar to object-oriented language. In Proc.
of NLPRS-1995, pages 568?573.
M. Yoshikawa, T. Amagasa, T. Shimura, and S. Uemura.
2001. XRel: A path-based approach to storage and re-
trieval of XML documents using relational databases.
ACM Transactions on Internet Technology, 1(1):110?
141.
 
   
   
   
 
               	 
     	  
   
    	  
     

	


     	     
    	     127
128
129
130
A Robust Retrieval Engine for Proximal and Structural Search
Katsuya Masuda? Takashi Ninomiya?? Yusuke Miyao? Tomoko Ohta?? Jun?ichi Tsujii??
? Department of Computer Science, Graduate School of Information Science and Technology,
University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
? CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan
{kmasuda,ninomi,yusuke,okap,tsujii}@is.s.u-tokyo.ac.jp
1 Introduction
In the text retrieval area including XML and Region Al-
gebra, many researchers pursued models for specifying
what kinds of information should appear in specified
structural positions and linear positions (Chinenyanga
and Kushmerick, 2001; Wolff et al, 1999; Theobald and
Weilkum, 2000; Clarke et al, 1995). The models at-
tracted many researchers because they are considered to
be basic frameworks for retrieving or extracting complex
information like events. However, unlike IR by keyword-
based search, their models are not robust, that is, they
support only exact matching of queries, while we would
like to know to what degree the contents in specified
structural positions are relevant to those in the query even
when the structure does not exactly match the query.
This paper describes a new ranked retrieval model
that enables proximal and structural search for structured
texts. We extend the model proposed in Region Alge-
bra to be robust by i) incorporating the idea of ranked-
ness in keyword-based search, and ii) expanding queries.
While in ordinary ranked retrieval models relevance mea-
sures are computed in terms of words, our model assumes
that they are defined in more general structural fragments,
i.e., extents (continuous fragments in a text) proposed in
Region Algebra. We decompose queries into subqueries
to allow the system not only to retrieve exactly matched
extents but also to retrieve partially matched ones. Our
model is robust like keyword-based search, and also en-
ables us to specify the structural and linear positions in
texts as done by Region Algebra.
The significance of this work is not in the development
of a new relevance measure nor in showing superiority
of structure-based search over keyword-based search, but
in the proposal of a framework for integrating proximal
and structural ranking models. Since the model treats all
types of structures in texts, not only ordinary text struc-
tures like ?title,? ?abstract,? ?authors,? etc., but also se-
mantic tags corresponding to recognized named entities
or events can also be used for indexing text fragments
and contribute to the relevance measure. Since extents
are treated similarly to keywords in traditional models,
our model will be integrated with any ranking and scala-
bility techniques used by keyword-based models.
We have implemented the ranking model in our re-
trieval engine, and had preliminary experiments to eval-
uate our model. Unfortunately, we used a rather small
corpus for the experiments. This is mainly because
there is no test collection of the structured query and
tag-annotated text. Instead, we used the GENIA cor-
pus (Ohta et al, 2002) as structured texts, which was
an XML document annotated with semantics tags in the
filed of biomedical science. The experiments show that
our model succeeded in retrieving the relevant answers
that an exact-matching model fails to retrieve because of
lack of robustness, and the relevant answers that a non-
structured model fails because of lack of structural spec-
ification.
2 A Ranking Model for Structured
Queries and Texts
This section describes the definition of the relevance be-
tween a document and a structured query represented by
the region algebra. The key idea is that a structured query
is decomposed into subqueries, and the relevance of the
whole query is represented as a vector of relevance mea-
sures of subqueries.
The region algebra (Clarke et al, 1995) is a set of op-
erators, which represent the relation between the extents
(i.e. regions in texts). In this paper, we suppose the re-
gion algebra has seven operators; four containment oper-
ators (?, ?, 6?, 6?) representing the containment relation
between the extents, two combination operators (4, 5)
corresponding to ?and? and ?or? operator of the boolean
model, and ordering operator (3) representing the order
of words or structures in the texts. For convenience of
explanation, we represent a query as a tree structure as
  
 
	
		
 
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 103?114,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Efficacy of Beam Thresholding, Unification Filtering and Hybrid
Parsing in Probabilistic HPSG Parsing
Takashi Ninomiya
CREST, JST
and
Department of Computer Science
The University of Tokyo
ninomi@is.s.u-tokyo.ac.jp
Yoshimasa Tsuruoka
CREST, JST
and
Department of Computer Science
The University of Tokyo
tsuruoka@is.s.u-tokyo.ac.jp
Yusuke Miyao
Department of Computer Science
The University of Tokyo
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Department of Computer Science
The University of Tokyo
and
School of Informatics
University of Manchester
and
CREST, JST
tsujii@is.s.u-tokyo.ac.jp
Abstract
We investigated the performance efficacy
of beam search parsing and deep parsing
techniques in probabilistic HPSG parsing
using the Penn treebank. We first tested
the beam thresholding and iterative pars-
ing developed for PCFG parsing with an
HPSG. Next, we tested three techniques
originally developed for deep parsing: quick
check, large constituent inhibition, and hy-
brid parsing with a CFG chunk parser. The
contributions of the large constituent inhi-
bition and global thresholding were not sig-
nificant, while the quick check and chunk
parser greatly contributed to total parsing
performance. The precision, recall and av-
erage parsing time for the Penn treebank
(Section 23) were 87.85%, 86.85%, and 360
ms, respectively.
1 Introduction
We investigated the performance efficacy of beam
search parsing and deep parsing techniques in
probabilistic head-driven phrase structure grammar
(HPSG) parsing for the Penn treebank. We first
applied beam thresholding techniques developed for
CFG parsing to HPSG parsing, including local
thresholding, global thresholding (Goodman, 1997),
and iterative parsing (Tsuruoka and Tsujii, 2005b).
Next, we applied parsing techniques developed for
deep parsing, including quick check (Malouf et al,
2000), large constituent inhibition (Kaplan et al,
2004) and hybrid parsing with a CFG chunk parser
(Daum et al, 2003; Frank et al, 2003; Frank, 2004).
The experiments showed how each technique con-
tributes to the final output of parsing in terms of
precision, recall, and speed for the Penn treebank.
Unification-based grammars have been extensively
studied in terms of linguistic formulation and com-
putation efficiency. Although they provide precise
linguistic structures of sentences, their processing is
considered expensive because of the detailed descrip-
tions. Since efficiency is of particular concern in prac-
tical applications, a number of studies have focused
on improving the parsing efficiency of unification-
based grammars (Oepen et al, 2002). Although sig-
nificant improvements in efficiency have been made,
parsing speed is still not high enough for practical
applications.
The recent introduction of probabilistic models of
wide-coverage unification-based grammars (Malouf
and van Noord, 2004; Kaplan et al, 2004; Miyao
and Tsujii, 2005) has opened up the novel possibil-
ity of increasing parsing speed by guiding the search
path using probabilities. That is, since we often re-
quire only the most probable parse result, we can
compute partial parse results that are likely to con-
tribute to the final parse result. This approach has
been extensively studied in the field of probabilistic
103
CFG (PCFG) parsing, such as Viterbi parsing and
beam thresholding.
While many methods of probabilistic parsing for
unification-based grammars have been developed,
their strategy is to first perform exhaustive pars-
ing without using probabilities and then select the
highest probability parse. The behavior of their al-
gorithms is like that of the Viterbi algorithm for
PCFG parsing, so the correct parse with the high-
est probability is guaranteed. The interesting point
of this approach is that, once the exhaustive pars-
ing is completed, the probabilities of non-local de-
pendencies, which cannot be computed during pars-
ing, are computed after making a packed parse for-
est. Probabilistic models where probabilities are as-
signed to the CFG backbone of the unification-based
grammar have been developed (Kasper et al, 1996;
Briscoe and Carroll, 1993; Kiefer et al, 2002), and
the most probable parse is found by PCFG parsing.
This model is based on PCFG and not probabilis-
tic unification-based grammar parsing. Geman and
Johnson (Geman and Johnson, 2002) proposed a dy-
namic programming algorithm for finding the most
probable parse in a packed parse forest generated by
unification-based grammars without expanding the
forest. However, the efficiency of this algorithm is
inherently limited by the inefficiency of exhaustive
parsing.
In this paper we describe the performance of beam
thresholding, including iterative parsing, in proba-
bilistic HPSG parsing for a large-scale corpora, the
Penn treebank. We show how techniques developed
for efficient deep parsing can improve the efficiency
of probabilistic parsing. These techniques were eval-
uated in experiments on the Penn Treebank (Marcus
et al, 1994) with the wide-coverage HPSG parser de-
veloped by Miyao et al (Miyao et al, 2005; Miyao
and Tsujii, 2005).
2 HPSG and probabilistic models
HPSG (Pollard and Sag, 1994) is a syntactic theory
based on lexicalized grammar formalism. In HPSG,
a small number of schemata describe general con-
struction rules, and a large number of lexical en-
tries express word-specific characteristics. The struc-
tures of sentences are explained using combinations
of schemata and lexical entries. Both schemata and
lexical entries are represented by typed feature struc-
tures, and constraints represented by feature struc-
tures are checked with unification.
Figure 1 shows an example of HPSG parsing of
the sentence ?Spring has come.? First, each of the
lexical entries for ?has? and ?come? is unified with a
daughter feature structure of the Head-Complement
Spring
HEAD  noun
SUBJ  < >
COMPS  < > 2
HEAD  verb
SUBJ  <    >
COMPS  <    >
1
has
HEAD  verb
SUBJ  <    >
COMPS  < >
1
come
2
head-comp
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  noun
SUBJ  < >
COMPS  < >
1
=?
Spring
HEAD  noun
SUBJ  < >
COMPS  < > 2
HEAD  verb
SUBJ  <    >
COMPS  <    >
1
has
HEAD  verb
SUBJ  <    >
COMPS  < >
1
come
2
HEAD  verb
SUBJ  <    >
COMPS  < >
1
HEAD  verb
SUBJ  < >
COMPS  < >
1
subject-head
head-comp
Figure 1: HPSG parsing
Schema. Unification provides the phrasal sign of
the mother. The sign of the larger constituent is
obtained by repeatedly applying schemata to lexi-
cal/phrasal signs. Finally, the parse result is output
as a phrasal sign that dominates the sentence.
Given set W of words and set F of feature struc-
tures, an HPSG is formulated as a tuple, G = ?L,R?,
where
L = {l = ?w,F ?|w ? W, F ? F} is a set of lexical
entries, and
R is a set of schemata, i.e., r ? R is a partial
function: F ? F ? F .
Given a sentence, an HPSG computes a set of phrasal
signs, i.e., feature structures, as a result of parsing.
Previous studies (Abney, 1997; Johnson et al,
1999; Riezler et al, 2000; Miyao et al, 2003; Mal-
ouf and van Noord, 2004; Kaplan et al, 2004; Miyao
and Tsujii, 2005) defined a probabilistic model of
unification-based grammars as a log-linear model or
maximum entropy model (Berger et al, 1996). The
probability of parse result T assigned to given sen-
tence w = ?w1, . . . , wn? is
p(T |w) = 1Zw
exp
(
?
i
?ifi(T )
)
Zw =
?
T ?
exp
(
?
i
?ifi(T ?)
)
,
where ?i is a model parameter, and fi is a feature
function that represents a characteristic of parse tree
T . Intuitively, the probability is defined as the nor-
malized product of the weights exp(?i) when a char-
acteristic corresponding to fi appears in parse result
T . Model parameters ?i are estimated using numer-
104
ical optimization methods (Malouf, 2002) so as to
maximize the log-likelihood of the training data.
However, the above model cannot be easily esti-
mated because the estimation requires the computa-
tion of p(T |w) for all parse candidates assigned to
sentence w. Because the number of parse candidates
is exponentially related to the length of the sentence,
the estimation is intractable for long sentences.
To make the model estimation tractable, Ge-
man and Johnson (Geman and Johnson, 2002) and
Miyao and Tsujii (Miyao and Tsujii, 2002) proposed
a dynamic programming algorithm for estimating
p(T |w). They assumed that features are functions
on nodes in a packed parse forest. That is, parse tree
T is represented by a set of nodes, i.e., T = {c}, and
the parse forest is represented by an and/or graph
of the nodes. From this assumption, we can redefine
the probability as
p(T |w) = 1Zw
exp
(
?
c?T
?
i
?ifi(c)
)
Zw =
?
T ?
exp
(
?
c?T ?
?
i
?ifi(c)
)
.
A packed parse forest has a structure similar to a
chart of CFG parsing, and c corresponds to an edge
in the chart. This assumption corresponds to the
independence assumption in PCFG; that is, only
a nonterminal symbol of a mother is considered in
further processing by ignoring the structure of its
daughters. With this assumption, we can compute
the figures of merit (FOMs) of partial parse results.
This assumption restricts the possibility of feature
functions that represent non-local dependencies ex-
pressed in a parse result. Since unification-based
grammars can express semantic relations, such as
predicate-argument relations, in their structure, the
assumption unjustifiably restricts the flexibility of
probabilistic modeling. However, previous research
(Miyao et al, 2003; Clark and Curran, 2004; Kaplan
et al, 2004) showed that predicate-argument rela-
tions can be represented under the assumption of
feature locality. We thus assumed the locality of fea-
ture functions and exploited it for the efficient search
of probable parse results.
3 Techniques for efficient deep
parsing
Many of the techniques for improving the parsing
efficiency of deep linguistic analysis have been de-
veloped in the framework of lexicalized grammars
such as lexical functional grammar (LFG) (Bresnan,
1982), lexicalized tree adjoining grammar (LTAG)
(Shabes et al, 1988), HPSG (Pollard and Sag, 1994)
or combinatory categorial grammar (CCG) (Steed-
man, 2000). Most of them were developed for ex-
haustive parsing, i.e., producing all parse results that
are given by the grammar (Matsumoto et al, 1983;
Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer
et al, 1999; Malouf et al, 2000; Torisawa et al, 2000;
Oepen et al, 2002; Penn and Munteanu, 2003). The
strategy of exhaustive parsing has been widely used
in grammar development and in parameter training
for probabilistic models.
We tested three of these techniques.
Quick check Quick check filters out non-unifiable
feature structures (Malouf et al, 2000). Sup-
pose we have two non-unifiable feature struc-
tures. They are destructively unified by travers-
ing and modifying them, and then finally they
are found to be not unifiable in the middle of the
unification process. Quick check quickly judges
their unifiability by peeping the values of the
given paths. If one of the path values is not
unifiable, the two feature structures cannot be
unified because of the necessary condition of uni-
fication. In our implementation of quick check,
each edge had two types of arrays. One con-
tained the path values of the edge?s sign; we
call this the sign array. The other contained the
path values of the right daughter of a schema
such that its left daughter is unified with the
edge?s sign; we call this a schema array. When
we apply a schema to two edges, e1 and e2, the
schema array of e1 and the sign array of e2 are
quickly checked. If it fails, then quick check re-
turns a unification failure. If it succeeds, the
signs are unified with the schemata, and the re-
sult of unification is returned.
Large constituent inhibition (Kaplan et al,
2004) It is unlikely for a large medial edge to
contribute to the final parsing result if it spans
more than 20 words and is not adjacent to the
beginning or ending of the sentence. Large
constituent inhibition prevents the parser from
generating medial edges that span more than
some word length.
HPSG parsing with a CFG chunk parser A
hybrid of deep parsing and shallow parsing
was recently found to improve the efficiency
of deep parsing (Daum et al, 2003; Frank et
al., 2003; Frank, 2004). As a preprocessor, the
shallow parsing must be very fast and achieve
high precision but not high recall so that the
105
procedure Viterbi(?w1, . . . , wn?, ?L?, R?, ?, ?, ?)
for i = 1 to n
foreach Fu ? {F |?wi, F ? ? L}
? =
?
i ?ifi(Fu)
pi[i? 1, i]? pi[i? 1, i] ? {Fu}
if (? > ?[i? 1, i, Fu]) then
?[i? 1, i, Fu]? ?
for d = 1 to n
for i = 0 to n? d
j = i + d
for k = i + 1 to j ? 1
foreach Fs ? pi[i, k], Ft ? pi[k, j], r ? R
if F = r(Fs, Ft) has succeeded
? = ?[i, k, Fs] + ?[k, j, Ft] +
?
i ?ifi(F )
pi[i, j]? pi[i, j] ? {F}
if (? > ?[i, j, F ]) then
?[i, j, F ]? ?
Figure 2: Pseudo-code of Viterbi algorithms for probabilistic HPSG parsing
total parsing performance in terms of precision,
recall and speed is not degraded. Because there
is trade-off between speed and accuracy in
this approach, the total parsing performance
for large-scale corpora like the Penn treebank
should be measured. We introduce a CFG
chunk parser (Tsuruoka and Tsujii, 2005a) as a
preprocessor of HPSG parsing. Chunk parsers
meet the requirements for preprocessors; they
are very fast and have high precision. The
grammar for the chunk parser is automatically
extracted from the CFG treebank translated
from the HPSG treebank, which is generated
during grammar extraction from the Penn
treebank. The principal idea of using the chunk
parser is to use the bracket information, i.e.,
parse trees without non-terminal symbols, and
prevent the HPSG parser from generating edges
that cross brackets.
4 Beam thresholding for HPSG
parsing
4.1 Simple beam thresholding
Many algorithms for improving the efficiency of
PCFG parsing have been extensively investigated.
They include grammar compilation (Tomita, 1986;
Nederhof, 2000), the Viterbi algorithm, controlling
search strategies without FOM such as left-corner
parsing (Rosenkrantz and Lewis II, 1970) or head-
corner parsing (Kay, 1989; van Noord, 1997), and
with FOM such as the beam search, the best-first
search or A* search (Chitrao and Grishman, 1990;
Caraballo and Charniak, 1998; Collins, 1999; Rat-
naparkhi, 1999; Charniak, 2000; Roark, 2001; Klein
and Manning, 2003). The beam search and best-
first search algorithms significantly reduce the time
required for finding the best parse at the cost of los-
ing the guarantee of finding the correct parse.
The CYK algorithm, which is essentially a bottom-
up parser, is a natural choice for non-probabilistic
HPSG parsers. Many of the constraints are ex-
pressed as lexical entries in HPSG, and bottom-up
parsers can use those constraints to reduce the search
space in the early stages of parsing.
For PCFG, extending the CYK algorithm to out-
put the Viterbi parse is straightforward (Ney, 1991;
Jurafsky and Martin, 2000). The parser can effi-
ciently calculate the Viterbi parse by taking the max-
imum of the probabilities of the same nonterminal
symbol in each cell. With the probabilistic model
defined in Section 2, we can also define the Viterbi
search for unification-based grammars (Geman and
Johnson, 2002). Figure 2 shows the pseudo-code of
Viterbi algorithm. The pi[i, j] represents the set of
partial parse results that cover words wi+1, . . . , wj ,
and ?[i, j, F ] stores the maximum FOM of partial
parse result F at cell (i, j). Feature functions are
defined over lexical entries and results of rule appli-
cations, which correspond to conjunctive nodes in a
feature forest. The FOM of a newly created partial
parse, F , is computed by summing the values of ? of
the daughters and an additional FOM of F .
The Viterbi algorithm enables various pruning
techniques to be used for efficient parsing. Beam
thresholding (Goodman, 1997) is a simple and effec-
tive technique for pruning edges during parsing. In
each cell of the chart, the method keeps only a por-
tion of the edges which have higher FOMs compared
to the other edges in the same cell.
106
procedure BeamThresholding(?w1, . . . , wn?, ?L?, R?, ?, ?, ?)
for i = 1 to n
foreach Fu ? {F |?wi, F ? ? L}
? =
?
i ?ifi(Fu)
pi[i? 1, i]? pi[i? 1, i] ? {Fu}
if (? > ?[i? 1, i, Fu]) then
?[i? 1, i, Fu]? ?
for d = 1 to n
for i = 0 to n? d
j = i + d
for k = i + 1 to j ? 1
foreach Fs ? pi[i, k], Ft ? pi[k, j], r ? R
if F = r(Fs, Ft) has succeeded
? = ?[i, k, Fs] + ?[k, j, Ft] +
?
i ?ifi(F )
pi[i, j]? pi[i, j] ? {F}
if (? > ?[i, j, F ]) then
?[i, j, F ]? ?
LocalThresholding(?, ?)
GlobalThresholding(n, ?)
procedure LocalThresholding(?, ?)
sort pi[i, j] according to ?[i, j, F ]
pi[i, j]? {pi[i, j]1, . . . , pi[i, j]?}
?max = maxF ?[i, j, F ]
foreach F ? pi[i, j]
if ?[i, j, F ] < ?max ? ?
pi[i, j]? pi[i, j]\{F}
procedure GlobalThresholding(n, ?)
f [0..n]? {0,?? ??, . . . ,??}
b[0..n]? {??,??, . . . ,??, 0}
#forward
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? pi[i, j]
f [j]? max(f [j], f [i] + ?[i, j, F ])
#backward
for i = n? 1 to 0
for j = i + 1 to n
foreach F ? pi[i, j]
b[i]? max(b[i], b[j] + ?[i, j, F ])
#global thresholding
?max = f [n]
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? pi[i, j]
if f [i] + ?[i, j, F ] + b[j] < ?max ? ? then
pi[i, j]? pi[i, j]\{F}
Figure 3: Pseudo-code of local beam search and global beam search algorithms for probabilistic HPSG
parsing
107
procedure IterativeBeamThresholding(w, G, ?0, ?0, ?0, ??, ??, ??, ?last, ?last, ?last)
?? ?0; ? ? ?0; ? ? ?0
loop while ? ? ?last and ? ? ?last and ? ? ?last
call BeamThresholding(w, G, ?, ?, ?)
if pi[1, n] 6= ? then exit
?? ? + ??; ? ? ? + ??; ? ? ? + ??
Figure 4: Pseudo-code of iterative beam thresholding
We tested three selection schemes for deciding
which edges to keep in each cell.
Local thresholding by number of edges Each
cell keeps the top ? edges based on their FOMs.
Local thresholding by beam width Each cell
keeps the edges whose FOM is greater than
?max ? ?, where ?max is the highest FOM
among the edges in the cell.
Global thresholding by beam width Each cell
keeps the edges whose global FOM is greater
than ?max??, where ?max is the highest global
FOM in the chart.
Figure 3 shows the pseudo-code of local beam
search, and global beam search algorithms for prob-
abilistic HPSG parsing. The code for local thresh-
olding is inserted at the end of the computation for
each cell. In Figure 3, pi[i, j]k denotes the k-th ele-
ment in sorted set pi[i, j]. We first take the first ?
elements that have higher FOMs and then remove
the elements with FOMs lower than ?max ? ?.
Global thresholding is also used for pruning edges,
and was originally proposed for CFG parsing (Good-
man, 1997). It prunes edges based on their global
FOM and the best global FOM in the chart. The
global FOM of an edge is defined as its FOM plus its
forward and backward FOMs, where the forward and
backward FOMs are rough estimations of the outside
FOM of the edge. The global thresholding is per-
formed immediately after each line of the CYK chart
is completed. The forward FOM is calculated first,
and then the backward FOM is calculated. Finally,
all edges with a global FOM lower than ?max ? ?
are pruned. Figure 3 gives further details of the al-
gorithm.
4.2 Iterative beam thresholding
We tested the iterative beam thresholding proposed
by Tsuruoka and Tsujii (2005b). We started the
parsing with a narrow beam. If the parser output
results, they were taken as the final parse results. If
the parser did not output any results, we widened the
Table 1: Abbreviations used in experimental results
num local beam thresholding by number
width local beam thresholding by width
global global beam thresholding by width
iterative iterative parsing with local beam
thresholding by number and width
chp parsing with CFG chunk parser
beam, and reran the parsing. We continued widen-
ing the beam until the parser output results or the
beam width reached some limit.
The pseudo-code is presented in Figure 4. It calls
the beam thresholding procedure shown in Figure 3
and increases parameters ? and ? until the parser
outputs results, i.e., pi[1, n] 6= ?.
Preserved iterative parsing Our implemented
CFG parser with iterative parsing cleared the
chart and edges at every iteration although the
parser regenerated the same edges using those
generated in the previous iteration. This is
because the computational cost of regenerating
edges is smaller than that of reusing edges to
which the rules have already been applied. For
HPSG parsing, the regenerating cost is even
greater than that for CFG parsing. In our
implementation of HPSG parsing, the chart
and edges were not cleared during the iterative
parsing. Instead, the pruned edges were marked
as thresholded ones. The parser counted the
number of iterations, and when edges were
generated, they were marked with the iteration
number, which we call the generation. If
edges were thresholded out, the generation was
replaced with the current iteration number plus
1. Suppose we have two edges, e1 and e2. The
grammar rules are applied iff both e1 and e2 are
not thresholded out, and the generation of e1
or e2 is equal to the current iteration number.
Figure 5 shows the pseudo-code of preserved
iterative parsing.
108
procedure BeamThresholding(?w1, . . . , wn?, ?L?, R?, ?, ?, ?, iternum)
for i = 1 to n
foreach Fu ? {F |?wi, F ? ? L}
? =
?
i ?ifi(Fu)
pi[i? 1, i]? pi[i? 1, i] ? {Fu}
if (? > ?[i? 1, i, Fu]) then
?[i? 1, i, Fu]? ?
for d = 1 to n
for i = 0 to n? d
j = i + d
for k = i + 1 to j ? 1
foreach Fs ? ?[i, k], Ft ? ?[k, j], r ? R
if gen[i, k, Fs] = iternum ? gen[k, j, Ft] = iternum
if F = r(Fs, Ft) has succeeded
gen[i, j, F ]? iternum
? = ?[i, k, Fs] + ?[k, j, Ft] +
?
i ?ifi(F )
pi[i, j]? pi[i, j] ? {F}
if (? > ?[i, j, F ]) then
?[i, j, F ]? ?
LocalThresholding(?, ?, iternum)
GlobalThresholding(n, ?, iternum)
procedure LocalThresholding(?, ?, iternum)
sort pi[i, j] according to ?[i, j, F ]
?[i, j]? {pi[i, j]1, . . . , pi[i, j]?}
?max = maxF ?[i, j, F ]
foreach F ? ?[i, j]
if ?[i, j, F ] < ?max ? ?
?[i, j]? ?[i, j]\{F}
foreach F ? (pi[i, j]? ?[i, j])
gen[i, j, F ]? iternum + 1
procedure GlobalThresholding(n, ?, iternum)
f [0..n]? {0,?? ??, . . . ,??}
b[0..n]? {??,??, . . . ,??, 0}
#forward
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? pi[i, j]
f [j]? max(f [j], f [i] + ?[i, j, F ])
#backward
for i = n? 1 to 0
for j = i + 1 to n
foreach F ? pi[i, j]
b[i]? max(b[i], b[j] + ?[i, j, F ])
#global thresholding
?max = f [n]
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? ?[i, j]
if f [i] + ?[i, j, F ] + b[j] < ?max ? ? then
?[i, j]? ?[i, j]\{F}
foreach F ? (pi[i, j]? ?[i, j])
gen[i, j, F ]? iternum + 1
procedure IterativeBeamThresholding(w, G, ?0, ?0, ?0, ??, ??, ??, ?last, ?last, ?last)
?? ?0; ? ? ?0; ? ? ?0; iternum = 0
loop while ? ? ?last and ? ? ?last and ? ? ?last
call BeamThresholding(w, G, ?, ?, ?, iternum)
if pi[1, n] 6= ? then exit
?? ? + ??; ? ? ? + ??; ? ? ? + ??; iternum? iternum + 1
Figure 5: Pseudo-code of preserved iterative parsing for HPSG
109
Table 2: Experimental results for development set (section 22) and test set (section 23)
Precision Recall F-score Avg. Time (ms) No. of failed sentences
development set 88.21% 87.32% 87.76% 360 12
test set 87.85% 86.85% 87.35% 360 15









        
  	 
  	      	  
         



	














        
  	 
  	      	  
         



	





Figure 7: Parsing time for the sentences in Section 24 of less than 15 words of Viterbi parsing (none) (Left)
and iterative parsing (iterative) (Right)

 
 
 


 
 
	 

 
 
               
                      