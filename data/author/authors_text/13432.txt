Proceedings of the ACL-HLT 2011 Student Session, pages 69?74,
Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational Linguistics
Effects of Noun Phrase Bracketing in Dependency Parsing and Machine
Translation
Nathan Green
Charles University in Prague
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics
green@ufal.mff.cuni.cz
Abstract
Flat noun phrase structure was, up until re-
cently, the standard in annotation for the Penn
Treebanks. With the recent addition of inter-
nal noun phrase annotation, dependency pars-
ing and applications down the NLP pipeline
are likely affected. Some machine translation
systems, such as TectoMT, use deep syntax
as a language transfer layer. It is proposed
that changes to the noun phrase dependency
parse will have a cascading effect down the
NLP pipeline and in the end, improve ma-
chine translation output, even with a reduc-
tion in parser accuracy that the noun phrase
structure might cause. This paper examines
this noun phrase structure?s effect on depen-
dency parsing, in English, with a maximum
spanning tree parser and shows a 2.43%, 0.23
Bleu score, improvement for English to Czech
machine translation.
1 Introduction
Noun phrase structure in the Penn Treebank has up
until recently been only considered, due to under-
specification, a flat structure. Due to the annota-
tion and work of Vadas and Curran (2007a; 2007b;
2008), we are now able to create Natural Language
Processing (NLP) systems that take advantage of the
internal structure of noun phrases in the Penn Tree-
bank. This extra internal structure introduces ad-
ditional complications in NLP applications such as
parsing.
Dependency parsing has been a prime focus of
NLP research of late due to its ability to help parse
languages with a free word order. Dependency pars-
ing has been shown to improve NLP systems in
certain languages and in many cases is considered
the state of the art in the field. Dependency pars-
ing made many improvements due to the CoNLL X
shared task (Buchholz and Marsi, 2006). However,
in most cases, these systems were trained with a flat
noun phrase structure in the Penn Treebank. Vadas?
internal noun phrase structure has been used in pre-
vious work on constituent parsing using Collin?s
parser (Vadas and Curran, 2007c), but has yet to be
analyzed for its effects on dependency parsing.
Parsing is very early in the NLP pipeline. There-
fore, improvements in parsing output could have an
improvement on other areas of NLP in many cases,
such as Machine Translation. At the same time, any
errors in parsing will tend to propagate down the
NLP pipeline. One would expect parsing accuracy
to be reduced when the complexity of the parse is in-
creased, such as adding noun phrase structure. But,
for a machine translation system that is reliant on
parsing, the new noun phrase structure, even with re-
duced parser accuracy, may yield improvements due
to a more detailed grammatical structure. This is
particularly of interest for dependency relations, as
it may aid in finding the correct head of a term in a
complex noun phrase.
This paper examines the results and errors in pars-
ing and machine translation of dependency parsers,
trained with annotated noun phrase structure, against
those with a flat noun phrase structure. These re-
sults are compared with two systems: a Baseline
Parser with no internally annotated noun phrases and
a Gold NP Parser trained with data which contains
69
gold standard internal noun phrase structure anno-
tation. Additionally, we analyze the effect of these
improvements and errors in parsing down the NLP
pipeline on the TectoMT machine translation sys-
tem (Z?abokrtsky? et al, 2008).
Section 2 contains background information
needed to understand the individual components of
the experiments. The methodology used to carry out
the experiments is described in Section 3. Results
are shown and discussed in Section 4. Section 5
concludes and discusses future work and implica-
tions of this research.
2 Related Work
2.1 Dependency Parsing
Dependence parsing is an alternative view to the
common phrase or constituent parsing techniques
used with the Penn Treebank. Dependency relations
can be used in many applications and have been
shown to be quite useful in languages with a free
word order. With the influx of many data-driven
techniques, the need for annotated dependency re-
lations is apparent. Since there are many data sets
with constituent relations annotated, this paper uses
free conversion software provided from the CoNLL
2008 shared task to create dependency relations (Jo-
hansson and Nugues, 2007; Surdeanu et al, 2008).
2.2 Dependency Parsers
Dependency parsing comes in two main forms:
Graph algorithms and Greedy algorithms. The
two most popular algorithms are McDonald?s MST-
Parser (McDonald et al, 2005) and Nivre?s Malt-
Parser (Nivre, 2003). Each parser has its advantages
and disadvantages, but the accuracy overall is ap-
proximately the same. The types of errors made
by each parser, however, are very different. MST-
Parser is globally trained for an optimal solution and
this has led it to get the best results on longer sen-
tences. MaltParser on the other hand, is a greedy al-
gorithm. This allows it to perform extremely well on
shorter sentences, as the errors tend to propagate and
cause more egregious errors in longer sentences with
longer dependencies (McDonald and Nivre, 2007).
We expect each parser to have different errors han-
dling internal noun phrase structure, but for this pa-
per we will only be examining the globally trained
MSTParser.
2.3 TectoMT
TectoMT is a machine translation framework based
on Praguian tectogrammatics (Sgall, 1967) which
represents four main layers: word layer, morpho-
logical layer, analytical layer, and tectogrammatical
layer (Popel et al, 2010). This framework is pri-
marily focused on the translation from English into
Czech. Since much of dependency parsing work
has been focused on Czech, this choice of machine
translation framework logically follows as TectoMT
makes direct use of the dependency relationships.
The work in this paper primarily addresses the noun
phrase structure in the analytical layer (SEnglishA
in Figure 1).
Figure 1: Translation Process in TectoMT in which
the tectogrammatical layer is transfered from English to
Czech.
TectoMT is a modular framework built in Perl.
This allows great ease in adding the two different
parsers into the framework since each experiment
can be run as a separate ?Scenario? comprised of dif-
ferent parsing ?Blocks?. This allows a simple com-
parison of two machine translation system in which
everything remains constant except the dependency
parser.
2.4 Noun Phrase Structure
The Penn Treebank is one of the most well known
English language treebanks (Marcus et al, 1993),
consisting of annotated portions of the Wall Street
Journal. Much of the annotation task is painstak-
ingly done by annotators in great detail. Some struc-
tures are not dealt with in detail, such as noun phrase
structure. Not having this information makes it dif-
ficult to tell the dependencies on phrases such as
70
?crude oil prices? (Vadas and Curran, 2007c). With-
out internal annotation it is ambiguous whether the
phrase is stating ?crude prices? (crude (oil prices))
or ?crude oil? ((crude oil) prices).
crude   oil   prices crude   oil   prices
Figure 2: Ambiguous dependency caused by internal
noun phrase structure.
Manual annotation of these phrases would be
quite time consuming and as seen in the example
above, sometimes ambiguous and therefore prone
to poor inter-annotator agreement. Vadas and Cur-
ran have constructed a Gold standard version Penn
treebank with these structures. They were also
able to train supervised learners to an F-score of
91.44% (Vadas and Curran, 2007a; Vadas and Cur-
ran, 2007b; Vadas and Curran, 2008). The addi-
tional complexity of noun phrase structure has been
shown to reduce parser accuracy in Collin?s parser
but no similar evaluation has been conducted for de-
pendency parsers. The internal noun phrase struc-
ture has been used in experiments prior but without
evaluation with respect to the noun phrases (Galley
and Manning, 2009).
3 Methodology
The Noun Phrase Bracketing experiments consist of
a comparison two systems.
1. The Baseline system is McDonald?s MST-
Parser trained on the Penn Treebank in English
without any extra noun phrase bracketing.
2. The Gold NP Parser is McDonald?s MSTParser
trained on the Penn Treebank in English with
gold standard noun phrase structure annota-
tions (Vadas and Curran, 2007a).
3.1 Data Sets
To maintain a consistent dataset to compare to pre-
vious work we use the Wall Street Journal (WSJ)
section of the Penn Treebank since it was used in
the CoNLL X shared task on dependency parsing
(Buchholz and Marsi, 2006). Using the same com-
mon breakdown of datasets, we use WST section
02-21 for training and section 22 for testing, which
allows us to have comparable results to previous
works. To test the effects of the noun phrase struc-
ture on machine translation, ACL 2008?s Workshop
on Statistical Machine translation?s (WMT) data are
used.
3.2 Process Flow
Figure 3: Experiment Process Flow. PTB (Penn Tree
Bank), NP (Noun Phrase Structure), LAS (Labeled Ac-
curacy Score), UAS (Unlabeled Accuracy Score), Wall
Street Journal (WSJ)
We begin the the experiments by constructing two
data sets:
1. The Penn Treebank with no internal noun
phrase structure (PTB w/o NP structure).
2. The Penn Treebank with gold standard noun
phrase annotations provided by Vadas and Cur-
ran (PTB w/ gold standard NP structure).
From these datasets we construct two separate
parsers. These parsers are trained using McDonald?s
Maximum Spanning Tree Algorithm (MSTParser)
(McDonald et al, 2005).
Both of the parsers are then tested on a subset of
the WSJ corpus, section 22, of the Penn Treebank
and the UAS and LAS scores are generated. Errors
generated by each of these systems are then com-
pared to discover where the internal noun phrase
structure affects the output. Parser accuracy is not
necessarily the most important aspect of this work.
71
The effect of this noun phrase structure down the
NLP pipeline is also crucial. For this, the parsers are
inserted into the TectoMT system.
3.3 Metrics
Labeled Accuracy Score (LAS) and Unlabeled
Accuracy Score (UAS) are the primary ways to eval-
uate dependency parsers. UAS is the percentage of
words that are correctly linked to their heads. LAS is
the percentage of words that are connected to their
correct heads and have the correct dependency la-
bel. UAS and LAS are used to compare one system
against another, as was done in CoNLL X (Buch-
holz and Marsi, 2006).
The Bleu (BiLingual Evaluation Understudy)
score is an automatic scoring mechanism for ma-
chine translation that is quick and can be reused as a
benchmark across machine translation tasks. Bleu is
calculated as the geometric mean of n-grams com-
paring a machine translation and a reference text
(Papineni et al, 2002). This experiment compares
the two parsing systems against each other using the
above metrics. In both cases the test set data is sam-
pled 1,000 times without replacement to calculate
statistical significance using a pairwise comparison.
4 Results and Discussion
When applied, the gold standard annotations
changed approximately 1.5% of the edges in the
training data. Once trained, both parsers were tested
against section 22 of their respective annotated cor-
pora. As Table 1 shows, the Baseline Parser obtained
near identical LAS and UAS scores. This was ex-
pected given the additional complexity of predicting
the noun phrase structure and the previous work on
noun phrase bracketing?s effect on Collin?s parser.
Systems LAS UAS
Baseline Parser 88.12% 91.11%
Gold NP Parser 88.10% 91.10%
Table 1: Parsing results for the Baseline and Gold NP
Parsers. Each is trained on Section 02-21 of the WSJ and
tested on Section 22
While possibly more error prone, the 1.5% change
in edges in the training data did appear to add more
useful syntactic structure to the resulting parses as
can be seen in Table 2. With the additional noun
phrase bracketing, the resulting Bleu score increased
0.23 points or 2.43%. The improvement is statis-
tically significant with 95% confidence using pair-
wise bootstrapping of 1,000 test sets randomly sam-
pled with replacement (Koehn, 2004; Zhang et al,
2004). In Figure 4 we can see that the difference be-
tween each of the 1,000 samples was above 0, mean-
ing the Gold NP Parser performed consistently bet-
ter given each sample.
Systems Bleu
Baseline Parser 9.47
Gold NP Parser 9.70
Table 2: TectoMT results of a complete system run with
both the Baseline Parser and Gold NP Parser. Both are
tested on WMT08 data. Results are an average of 1,000
bootstrapped test sets with replacement.
Figure 4: The Gold NP Parser shows statistically signif-
icant improvement with 95% confidence. The difference
in Bleu score is represented on the Y-axis and the boot-
strap iteration is displayed on the X-axis. The samples
were sorted by the difference in bleu score.
Visually, changes can be seen in the English side
parse that affect the overall translation quality. Sen-
tences that contained incorrect noun phrase structure
such as ?The second vice-president and Economy
minister, Pedro Solbes? as seen in Figure 5 and Fig-
ure 6 were more correctly parsed in the Gold NP
Parser. In Figure 5 ?and? is incorrectly assigned to
the bottom of a noun phrase and does not connect
any segments together in the output of the Baseline
Parser, while it connects two phrases in Figure 6
which is the output of the Gold NP Parser. This shift
in bracketing also allows the proper noun, which is
shaded, to be assigned to the correct head, the right-
most noun in the phrase.
72
Figure 5: The parse created with the data with flat struc-
tures does not appear to handle noun phrases with more
depth, in this case the ?and? does not properly connect the
two components.
Figure 6: With the addition of noun phrase structure in
parser, the complicated noun phrase appears to be better
structured. The ?and? connects two components instead
of improperly being a leaf node.
5 Conclusion
This paper has demonstrated the benefit of addi-
tional noun phrase bracketing in training data for use
in dependency parsing and machine translation. Us-
ing the additional structure, the dependency parser?s
accuracy was minimally reduced. Despite this re-
duction, machine translation, much further down
the NLP pipeline, obtained a 2.43% jump in Bleu
score and is statistically significant with 95% confi-
dence. Future work should examine similar experi-
ments with MaltParser and other machine translation
systems.
6 Acknowledgements
This research has received funding from the Euro-
pean Commissions 7th Framework Program (FP7)
under grant agreement n? 238405 (CLARA), and
from grant MSM 0021620838. I would like to thank
Zdene?k Z?abokrtsky? for his guidance in this research
and also the anonymous reviewers for their com-
ments.
References
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning, CoNLL-X ?06, pages 149?
164, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Michel Galley and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 773?781, Suntec, Singapore,
August. Association for Computational Linguistics.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007, pages 105?112,
Tartu, Estonia, May 25-26.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Comput. Linguist.,
19:313?330, June.
73
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency parsing
models. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 122?131.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Proceedings of the
conference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, HLT
?05, pages 523?530, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT,
pages 149?160.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Martin Popel, Zdene?k Z?abokrtsky?, and Jan Pta?c?ek. 2010.
Tectomt: Modular nlp framework. In IceTAL, pages
293?304.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska? dek-
linace. Academia, Prague, Czech Republic.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
conll-2008 shared task on joint parsing of syntactic
and semantic dependencies. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, CoNLL ?08, pages 159?177, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
David Vadas and James Curran. 2007a. Adding noun
phrase structure to the penn treebank. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 240?247, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
David Vadas and James R. Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In
Conference of the Pacific Association for Computa-
tional Linguistics (PACLING), pages 104?112, Mel-
bourne, Australia, September.
David Vadas and James R. Curran. 2007c. Parsing in-
ternal noun phrase structure with collins? models. In
Proceedings of the Australasian Language Technol-
ogy Workshop 2007, pages 109?116, Melbourne, Aus-
tralia, December.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proceedings of ACL-
08: HLT, pages 335?343, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
Tectomt: highly modular mt system with tectogram-
matics used as transfer layer. In Proceedings of the
Third Workshop on Statistical Machine Translation,
StatMT ?08, pages 167?170, Morristown, NJ, USA.
Association for Computational Linguistics.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting bleu/nist scores: How much improvement
do we need to have a better system. In In Proceedings
of Proceedings of Language Resources and Evaluation
(LREC-2004, pages 2051?2054.
74
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 227?234,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
PackPlay: Mining semantic data in collaborative games
Nathan Green
NC State University
890 Oval Drive
Raleigh, NC 27695
Paul Breimyer
NC State University
890 Oval Drive
Raleigh, NC 27695
Vinay Kumar
NC State University
890 Oval Drive
Raleigh, NC 27695
Nagiza F. Samatova
Oak Ridge National Lab
1 Bethel Valley Rd
Oak Ridge, TN 37831
Abstract
Building training data is labor-intensive
and presents a major obstacle to advanc-
ing machine learning technologies such as
machine translators, named entity recog-
nizers (NER), part-of-speech taggers, etc.
Training data are often specialized for a
particular language or Natural Language
Processing (NLP) task. Knowledge cap-
tured by a specific set of training data is
not easily transferable, even to the same
NLP task in another language. Emerging
technologies, such as social networks and
serious games, offer a unique opportunity
to change how we construct training data.
While collaborative games have been used
in information retrieval, it is an open is-
sue whether users can contribute accurate
annotations in a collaborative game con-
text for a problem that requires an exact
answer, such as games that would create
named entity recognition training data. We
present PackPlay, a collaborative game
framework that empirically shows players?
ability to mimic annotation accuracy and
thoroughness seen in gold standard anno-
tated corpora.
1 Introduction
Annotated corpora are sets of structured text
used in Natural Language Processing (NLP) that
contain supplemental knowledge, such as tagged
parts-of-speech, semantic concepts assigned to
phrases, or semantic relationships between these
concepts. Machine Learning (ML) is a subfield of
Artificial Intelligence that studies how computers
can obtain knowledge and create predictive mod-
els. These models require annotated corpora to
learn rules and patterns. However, these anno-
tated corpora must be manually curated for each
domain or task, which is labor intensive and te-
dious (Scannell, 2007), thereby creating a bot-
tleneck for advancing ML and NLP prediction
tools. Furthermore, knowledge captured by a spe-
cific annotated corpus is often not transferable to
another task, even to the same NLP task in an-
other language. Domain and language specific
corpora are useful for many language technol-
ogy applications, including named entity recogni-
tion (NER), machine translation, spelling correc-
tion, and machine-readable dictionaries. The An
Cru?bada?n Project, for example, has succeeded in
creating corpora for more than 400 of the world?s
6000+ languages by Web crawling. With a few ex-
ceptions, most of the 400+ corpora, however, lack
any linguistic annotations due to the limitations of
annotation tools (Rayson et al, 2006).
Despite the many documented advantages of
annotated data over raw data (Granger and
Rayson, 1998; Mair, 2005), there is a dearth of
annotated corpora in many domains. The ma-
jority of previous corpus annotation efforts re-
lied on manual annotation by domain experts,
automated prediction tagging systems, and hy-
brid semi-automatic systems that used both ap-
proaches. While yielding high quality and enor-
mously valuable corpora, manually annotating
corpora can be prohibitively costly and time con-
suming. For example, the GENIA corpus contains
9,372 sentences, curated by five part-time annota-
tors, one senior coordinator, and one junior coor-
dinator over 1.5 years (Kim et al, 2008). Semi-
automatic approaches decrease human effort but
often introduce significant error, while still requir-
ing human interaction.
The Web can help facilitate semi-automatic ap-
proaches by connecting distributed human users
at a previously unfathomable scale and presents
an opportunity to expand annotation efforts to
countless users using Human Computation, the
concept of outsourcing certain computational
227
processes to humans, generally to solve prob-
lems that are intractable or difficult for comput-
ers. This concept is demonstrated in our previ-
ous work, WebBANC (Green et al, 2009) and
BioDEAL (Breimyer et al, 2009), which allows
users to annotate Web documents through a Web
browser plugin for the purposes of creating lin-
guistically and biologically tagged annotated cor-
pora and with micro-tasking via Mechanical Turk,
which allows for a low cost option for manual la-
bor tasks (Snow et al, 2008; Kittur et al, 2008).
While the Web and Human Computation may
be a powerful tandem for generating data and
solving difficult problems, in order to succeed,
users must be motivated to participate. Humans
have been fascinated with games for centuries
and play them for many reasons, including for
entertainment, honing skills, and gaining knowl-
edge (FAS Summit, 2006). Every year, a large
amount of hours are spent playing online computer
games. The games range form simple card and
word games to more complex 3-D world games.
One such site for word, puzzle, and card games is
Pogo.com1. According to protrackr,2 Pogo has al-
most 6 million unique visitors a day. Alexa.com3
shows that the average user is on the site for 11
minutes at a time. When the average time spent on
the site is propagated to each user, the combined
time is equal to more than 45,000 days of human
time. Arguably if, the games on Pogo were used
to harvest useful data, various fields of Computer
Science research could be advanced.
There has been a recent trend to leverage hu-
man?s fascination in game playing to solve diffi-
cult problems through Human Computation. Two
such games include ESP and Google?s Image La-
beler (Ahn and Dabbish, 2004), in which play-
ers annotate images in a cooperative environment
to correctly match image tags with their partner.
Semantic annotation has also been addressed in
the game Phrase Detectives (Chamberlain et al,
2009), which has the goal of creating large scale
training data for anaphora resolution. These types
of games are part of a larger, serious games, initia-
tive (Annetta, 2008).
This paper introduces the Web-enabled collabo-
rative game framework, PackPlay, and investigates
1Pogo. http://www.pogo.com/
2Protrackr.com site information and statistis-
tics.http://www.protrackr.com/
3Alexa: The Web Information Company.
http://www.alexa.com/
how collaborative online gaming can affect anno-
tation throughput and annotation accuracy. There
are two main questions for such systems: first,
will overall throughput increase compared to tra-
ditional methods of annotating, such as the man-
ual construction of the Genia Corpus? Second,
how accurate are the collective annotations? A
successful human computation environment, such
as PackPlay, would represent a paradigm shift in
the way annotated corpora are created. However,
adoption of such a framework cannot be expected
until these questions are answered. We address
both of these questions in multiple games in our
PackPlay system through evaluation of the collec-
tive players? annotations with precision and recall
to judge accuracy of players? annotations and the
number of games played to judge throughput. We
show improvements in both areas over traditional
annotation methods and show accuracy compara-
ble to expert prediction systems that could be used
for semi-supervised annotation.
2 Methodology
We empirically show casual game players? abil-
ity to accurately and throughly annotate corpora
by conducting experiments following the process
described in Section 2.1 with 8 players using
the PackPlay System. The testers annotate the
datasets described in Section 2.2 and results are
analyzed using the equations in Section 2.3.
2.1 PackPlay Process Flow
Figure 1 shows the average PackPlay process flow
that a player will follow for a multi-player game.
Assuming the player is registered, the player will
always start by logging in and selecting the game
he or she wants to play. Once in the game screen,
the system will try to pair the player with another
player who is waiting. After a set time limit,
the game will automatically pair the user with a
PlayerBot. It is important to note that the player
will not know that his or her partner is a Player-
Bot.
Once paired, a game can start. In most games, a
question will be sampled from our database. How
this sampling takes place is up to the individual
game. Once sampled, the question will be dis-
played to one player or all players, depending on
whether the game is synchronous or asynchronous
(see definitions in Sections 3.1.2 and 3.2.2). Once
the question is displayed, two things can happen.
228
No
No
No
No
No
YesYes
Yes
Yes
Yes
Login
Select Game
Has Partner?
Wait for Partner for X seconds
Start Screen w/ leader board
Pair with Bot
Display Question
Annotate
Timer expired? Done?
More Questions? End Game
Has Partner?
Figure 1: User process flow for PackPlay games.
First, the timer can run out; this timer is set by each
game individually. Second, the player may answer
the question and move on to the next question. Af-
ter either one of those two options, a new question
will be sampled. This cycle continues until the
game session is over. This is usually determined
by the game, as each game can set the number of
questions in a session, or by a player quiting the
game.
2.2 Data Sources
To compare named entity results, PackPlay uses
sentences and annotations from CoNLL 2003, a
?gold? standard corpus (Tjong et al, 2003). We
use the CoNLL 2003 corpus since it has been cu-
rated by experts and the PackPlay system can com-
pare our players? annotations vs those of 16 sub-
mitted predictive models, also refered to as the
CoNLL average, in the 2003 conference on nat-
ural language learning. This paper will refer to the
training corpus as the CoNLL corpus, and we se-
lected it for our evaluation due to its widespread
adoption as a benchmark corpus.
2.3 Metrics
To measure how thoroughly and accurately our
players annotate the data, we calculate both recall
(Equation 1) and precision (Equation 2), in which
? is the set of words annotated in PackPlay and ?
is the set of words in the base CoNLL corpus.
Recall =
|? ? ? |
|?|
(1)
Precision =
|? ? ? |
|?|
(2)
Each game module in the PackPlay system has
its own scoring module, which is intended to im-
prove the players? precision. For this reason, scor-
ing is handled on a per game level. Each game has
its own leader board as well. The leader board is
used to motivate the players to continue playing
the PackPlay games. This is intended to improve
recall for annotations in the system.
3 Games
3.1 Entity Discovery
3.1.1 Game description
Named entities are a foundational part of many
NLP systems from information extraction sys-
tems to machine translation systems. The abil-
ity to detect an entity is an application area called
Named Entity Recognition (NER). The most com-
mon named entity categories are Person (Per), Lo-
cation (Loc), and Organization (Org). The ability
to extract these entities may be used in everyday
work, such as extracting defendants, cities, and
companies from court briefings, or it may be used
for critical systems in national defense, such as
monitoring communications for people and loca-
tions of interest.
To help with the creation of more NER systems,
Entity Discovery (see Figure 2), a game for an-
notating sentences with supplied entities was cre-
ated. The goal of the game is to pair players with
each other and allow them to annotate sentences
together. While this annotation task could be done
by one person, it is a very time consuming activ-
ity. By creating a game, we hope that players will
be more likely to annotate for fun and will anno-
tate correctly and completely in order to receive a
higher score in the PackPlay system.
3.1.2 Implementation
Entity Discovery is implemented as a synchronous
two-player game. A synchronous game is one in
which both players have the same task in the game,
in this case, to annotate a sentence. To have a base
comparison point, all players are asked to annotate
a random set of 60 sentences to start, for which we
have the correct answers. This way we will be able
to assess the trustworthiness score in future itera-
tions. After the pretest, the players will be shown
sentences randomly sampled with replacement.
229
Figure 2: Screenshot of a player annotating the Person entity Jimi Hendrix
In Entity Discovery, we made a design decision
to keep a player?s partner anonymous. This should
help reduce cheating, such as agreeing to select
the same word over and over, and it should reduce
the ability for a player to only play with his or
her friends, which might enhance their ability to
cheat by using other communication systems such
as instant messaging or a cell phone. Since Pack-
Play is still in the experimental stages, players may
not always be available. For this reason, we have
implemented a PlayerBot system. The PlayerBot
will mimic another player by selecting previously
annotated phrases for a given sentence from the
database. From the human players? point of view,
nothing seems different.
Players are asked to annotate, or tag, as many
entities as they can find in a sentence. Players are
also told at the beginning of the game that they are
paired with another user. Their goal is to annotate
the same things as their partner. Our assumption
is that if the game is a single player game then the
players may just annotate the most obvious enti-
ties for gaining more points. By having the player
to try to guess at what their partner may anno-
tate we hope to get better overall coverage of enti-
ties. We try to minimize the errors, which guess-
ing might produce, in a second game, Name That
Entity (Section 3.2).
To annotate a sentence, the player simply high-
lights a word or phrase and clicks on a relevant
entity. For instance in Entity Discovery, a player
can annotate the phrase ?Jimi Hendrix? as a Per-
son entity. From this point on, the player is free
to annotate more phrases in the sentence. When
the player completes annotating a sentence, the
player hits ?Next Problem.? The system then waits
for the player?s partner to hit ?Next Problem? as
well. When both players finish annotating, the
game points will be calculated and a new question
will be sampled for the players.
230
Figure 3: Screenshot of what the player sees at the
end of the Entity Discovery game
3.1.3 Scoring
Scoring can be done in a variety of ways, each hav-
ing an impact on players? performance and enjoy-
ment. For Entity Discovery, we decided to give
each user a flat score of 100 points for every an-
swer that matched their partner. At the end of
each game session, the player will see what an-
swers matched with their partner. For instance, if
both players tagged ?Jimi Hendrix? as a Person,
they will both receive 100 points. We do not show
the players their matched scores after each sen-
tence, since this might bias the user to tag more
or less depending on what their partner does. Fig-
ure 3 shows a typical scoring screen at the end
of a game; in Figure 3, the players matched 4
phrases, totaling 400 points. It is important to note
that at this stage we do not distinguish between
correct and incorrect annotations, just whether the
two players agree.
3.1.4 User Case Study Methodology
To examine Entity Discovery as a collaborative
game toward the creation of an annotated corpus,
we conducted a user experiment to collect sam-
ple data on a known data set. Over a short time,
8 players were asked to play both Entity Discov-
ery and Name That Entity. In PackPlay, through-
put can be estimated, since each game has a de-
fined time limit, defined as the average number
of entities annotated per question times the num-
ber of users times the average number of ques-
tions seen by a user. Unlike other systems such as
Mechanical Turk (Snow et al, 2008; Kittur et al,
2008), BioDeal (Breimyer et al, 2009), or Web-
BANC (Green et al, 2009), in PackPlay we define
the speed at which a user annotates.
Each game in Entity Discovery consists of 10
sentences from the CoNLL corpus. These sen-
tences are not guaranteed to have a named en-
tity within them. The users in the study were not
Table 1: Statistics returned from our user study for
the game Entity Discovery
Statistic Total Mean
# of games 29 3.62
# of annotations 291 40.85
informed of the entity content as to not bias the
experiment and falsely raise our precision scores.
With only 8 players, we obtained 291 annotations,
which averaged to about 40 annotations per user.
This study was not done over a long period of time,
so each user only played, on average, 3.6 games.
Two players were asked to intentionally anno-
tate poorly. The goal of using poor annotators
was to simulate real world players, who may just
click answers to ruin the game or who are clue-
less to what a named entity is. This information
can be used in later research to help automatically
detect ?bad? annotators using anomaly detection
techniques.
PackPlay also stores information not used in
this study, such as time stamps for each question
answered. This information will be incorporated
into future experiment analysis to see if we can
further improve our annotated corpora based on
the order and time spent forming an annotation.
For instance, the first annotation in a sentence may
have a higher probability than the last annotation.
It is possible that if a user answers too fast, the
answer is likely an error.
3.1.5 Output Quality
Every player completes part of a 60 sentence
pretest in which we know the answers. For each
game, the questions are sampled without replace-
ment but this does not carry over after a game.
For instance, if a player finishes game 1, he or she
will never see the same question twice. For game
two, no question within the game will be repeated,
however, the player might see a question he or she
answered in game 1. Because of this, each user
will not see all 60 questions, but we will have a
good sample to judge whether a user is accurate
or not. The ability to repeat a question in different
games allows us, in future research, to test play-
ers using intra-annotator agreement statistics. This
tests how well a player agrees with himself or her-
self. From this set of 60 questions we have calcu-
lated each player?s recall and precision scores.
As Table 2 shows, the recall scores for Entity
231
Table 2: Recall and precision for Entity Discovery
annotations of CoNLL data.
Per Loc Org Avg CoNLL
Avg
Recall
(All Data) 0.94 0.95 0.85 0.9 0.82
Precision
(All Data) 0.47 0.70 0.53 0.62 0.83
Discovery in this experiment were 0.94, 0.95, and
0.85 for Person, Location, and Organization, re-
spectively. The overall average was 0.9, which
beats out the CoNLL average, an average of 16
expert systems, for recall. Entity Discovery?s num-
bers are similar to the pattern seen in the CoNLL
predictive systems for Person, Location and Or-
ganization, in which Organization was the lowest
and Person was the highest. The precision num-
bers were quite lower, with an average of 0.62.
When examining the data, most of the precision
errors occurred because of word phrase boundary
issues with the annotation and also players often
are unsure whether to include titles such as Presi-
dent, Mr., or Dr. There were also quite a few errors
where players annotated concepts as People such
as ?The Judge? or ?The scorekeeper.? While this is
incorrect for named entity recognition, it might be
of interest to a co-reference resolution corpus. The
precision numbers are likely low because of our
untrained players and because some of the players
were told to intentionally annotate entities incor-
rectly. To improve on these numbers, we applied
a coverage requirement and majority voting. The
coverage requirement requires that more than one
player has annotated a given phrase for the an-
notation to be included in the corpus. Majority
voting indicates that the phrase is only included
if 50% or more of the playerss who annotated a
phrase, agreed on the specific entity assigned to
the phrase.
As Table 3 shows, both majority voting and
coverage requirements improve precision by more
than 10%. When combined, they improve the
overall precision to 0.88, a 26% improvement.
This is an improvement to the expert CoNLL sys-
tems score of 0.83. The majority voting likely
removed the annotations from our purposefully
?bad? annotators.
For future work, as the number of players in-
creases, we will have to increase our coverage re-
Table 3: Precision for Entity Discovery annota-
tions of CoNLL data with filtering
Per Loc Org Avg
Precision
(Majority Voting) 0.56 0.79 0.65 0.72
Precision
(Coverage Req.) 0.69 0.83 0.63 0.73
Precision
(Majority Voting +
Coverage Req.) 0.90 0.95 0.81 0.88
quirement to match. This ratio has not been deter-
mined and will need to be tested. A more success-
ful way to detect errors in our annotations may be
to create a separate game to verify given answers.
To initially test this concept we have made and set
up an experiment with a game, called Name That
Entity.
3.2 Name That Entity
3.2.1 Game Description
Name That Entity is another game with a focus
on named entities. Name That Entity was created
to show that game mechanics and the creation of
further games would enhance the value of an an-
notated corpus. In the case of Name That Entity,
we have created a multiple choice game in which
the player will select the entity that best represents
the highlighted word or phrase. Unlike Entity Dis-
covery, this allows us to focus the annotation ef-
fort on particular words or phrases. Once again,
this is modeled as a two-player game but the play-
ers are not playing simultaneously. The goal for
the player is to select the same entity type for the
highlighted word that their partner selects. In this
game, speed is of the essence since each question
will ask for one entity as opposed to Entity Discov-
ery, which was open ended to how many entities
might exist in a sentence.
3.2.2 Implementation
As described above, Name That Entity appears to
be a two-player synchronous game. The player
is under the assumption that he or she must once
again match his or her partner?s choice. What the
player does not know is that the multi-player is
simulated in this case. The player is replaced with
a PlayerBot which chooses annotations from the
Entity Discovery game. This, in essence, creates
232
an asynchronous game, in which one player has
the task of finding entities and the other player has
the task of verifying entities. This gives us a fur-
ther mechanism to check the validity of entities an-
notated by the Entity Discovery game.
As with Entity Discovery, the player?s partner is
anonymous. This anonymity allows us to keep the
asynchronous structure hidden, as well as judge a
new metric, intra-annotator agreement, not tested
in the previous game. Since it is possible that a
player in PackPlay may have a question sampled
that was previously annotated in the Entity Dis-
covery game by the same player, we can use intra-
annotator agreement. While well-known inter-
annotator statistics, such as Cohen?s Kappa, evalu-
ate one annotator versus the other annotator, intra-
annotator statistics allow us to judge an annota-
tor versus himself or herself to test for consis-
tency (Artstein and Poesio, 2008). In the Pack-
Play framework this allows us to detect playerss
who are randomly guessing and are therefore not
consistent with themselves.
3.2.3 Scoring
Since entity coverage of a sentence is not an is-
sue in the multiple choice game, we made use of
a different scoring system that would reward first
instincts. While the Entity Discovery game has a
set score for every answer, Name That Entity has a
sliding scale. For each question, the max score is
100 points, as the time ticks away the user receives
fewer points. The points remaining are indicated
to the user via a timing bar at the bottom of the
screen.
When the player completes a game, he or she
is allowed to view the results for that game. Un-
like the Entity Discovery game, we display to the
player what entity his or her partner chooses on
the question in which they both did not match.
This gives us a quick and simple form of annotator
training, since a player with no experience may not
be familiar with a particular entity. This was seen
with the players? ability to detect an Organization
entity. We expect that when a player sees what
his or her partner annotates a phrase as, the player,
is, in effect, being trained. However, displaying
this at the end should not have any affects toward
cheating since their partners are anonymous.
3.2.4 User Case Study Methodology
Of the 8 players who participated in the Entity Dis-
covery study, 7 also played Name That Entity dur-
ing their game sessions. We did not inform the
players, but the questions asked in Name That En-
tity were the same answers that the players gave in
the experiment in Section 3.1.4. The basic anno-
tation numbers from our small user study can be
seen in Table 4.
Table 4: Statistics returned from our user study for
the game Name That Entity
Statistic Total Mean
# of games 20 2.85
# of annotations 195 27.85
3.2.5 Output Quality
As Name That Entity is not intended to be a solo
mechanism to generate annotations, but instead
a way to verify existing annotations, we did not
assess the recall and precision of the game. In-
stead we are looking at the number of annota-
tions, unique annotations, and conflicting annota-
tions generated by our players in this game.
Table 5: Types of annotations generated by Name
That Entity
Error Count
Annotations 195
Unique Annotations 141
Conflicts 38
Unique Conflicts 35
In Table 5, unique annotations refer to annota-
tions verified by only one user. Of the 195 total
verified annotation, 38 had conflicting answers. In
the majority of the cases the players marked these
conflicts as ?None of the Above,? indicating that
the annotated phrase from Entity Discovery was
incorrect. For instance, many players made the
mistake in Entity Discovery of marking phrases
such as ?German,? ?English,? and ?French? as Lo-
cation entities when they are, in fact, just adjec-
tives. In Name That Entity, the majority of players
corrected each other and marked these as ?None
of the Above.?
The main use of this game will be to incorporate
it as an accuracy check for players based on these
conflicting annotation. This accuracy check will
be used in future work to deal with user confidence
scores and conflict resolution.
233
4 Conclusion
Annotated corpora generation presents a major ob-
stacle to advancing modern Natural Language Pro-
cessing technologies. In this paper we introduced
the PackPlay framework, which aims to leverage
a distributed web user community in a collabora-
tive game to build semantically-rich annotated cor-
pora from players annotations. PackPlay is shown
to have high precision and recall numbers when
compared to expert systems in the area of named
entity recognition. These annotated corpora were
generated from two collaborative games in Pack-
Play, Entity Discovery and Name That Entity. The
two games combined let us exploit the benefits of
both synchronous and asynchronous gameplay as
mechanisms to verify the quality of our annotated
corpora. Future work should combine the play-
ers output with a player confidence score based
on conflict resolution algorithms, using both inter-
and intra-annotator metrics.
References
Luis von Ahn and Laura Dabbish. 2004 Labeling im-
ages with a computer game. ACM, pages 319-326,
Vienna, Austria.
Leonard A. Annetta. 2008 Serious Educational
Games: From Theory to Practice. Sense Publishers.
Ron Artstein and Massimo Poesio. 2008 Inter-
coder agreement for computational linguistics.
Computational Linguistics, Vol. 34, Issue 4, pages
555-596.
Maged N. Kamel Boulos and Steve Wheeler. 2007.
The emerging web 2.0 social software: an enabling
suite of sociable technologies in health and health
care education. Health information and libraries
journal, Vol. 24, pages 223.
Paul Breimyer, Nathan Green, Vinay Kumar, and Na-
giza F. Samatova. 2009. BioDEAL: community
generation of biological annotations. BMC Medical
Informatics and Decision Making, Vol. 9, pages
Suppl+1.
Jon Chamberlain, Udo Kruschwitz, and Massimo Poe-
sio. 2009. Constructing an anaphorically anno-
tated corpus with non-experts: assessing the qual-
ity of collaborative annotations. People?s Web ?09:
Proceedings of the 2009 Workshop on The People?s
Web Meets NLP, pages 57-62.
FAS Summit on educational games: Harnessing the
power of video games for learning (report), 2006.
Sylviane Granger and Paul Rayson. 1998. Learner
English on Computer. Longman, London, and New
Yorks pp. 119-131.
Nathan Green, Paul Breimyer, Vinay Kumar, and Na-
giza F. Samatova. 2009. WebBANC: Build-
ing Semantically-Rich Annotated Corpora from
Web User Annotations of Minority Languages.
Proceedings of the 17th Nordic Conference of
Computational Linguistics (NODALIDA), Vol. 4,
pages 48-56, Odense, Denmark.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9:10.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with Mechanical Turk
CHI ?08: Proceeding of the twenty-sixth annual
SIGCHI conference on Human factors in computing
systems, pages 453-456, Florence, Italy.
Ravi Kumar, Jasmine Novak, and Andrew Tomkins.
2006 Structure and evolution of online social net-
works. KDD ?06: Proceedings of the 12th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 611-617, New
York, NY.
C. Mair. 2005. The corpus-based study of language
change in progress: The extra value of tagged cor-
pora. The AAACL/ICAME Conference, Ann Arbor,
MI.
Paul Rayson, James Walkerdine,William H. Fletcher,
and Adam Kilgarriff. 2006. Annotated web as cor-
pus The 2nd International Workshop on Web as
Corpus (EACL06), Trento, Italy.
Kevin P. Scannell. 2007. The Crbadn Project:
Corpus building for under-resourced languages.
Proceedings of the 3rd Web as Corpus Workshop
Louvain-la-Neuve, Belgium.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast ? but is it
good?: evaluating non-expert annotations for nat-
ural language tasks EMNLP ?08: Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 254?263, Honolulu,
Hawaii.
Erik F. Tjong, Kim Sang and Fien De Meul-
der 2003 Introduction to the conll-2003 shared
task: language-independent named entity recogni-
tion. Association for Computational Linguistics,
pages 142-147, Edmonton, Canada.
234
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 433?439,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Influence of Parser Choice on Dependency-Based MT
Martin Popel, David Marec?ek, Nathan Green and Zdene?k Z?abokrtsky?
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
{popel,marecek,green,zabokrtsky}@ufal.mff.cuni.cz
Abstract
Accuracy of dependency parsers is one of the
key factors limiting the quality of dependency-
based machine translation. This paper deals
with the influence of various dependency pars-
ing approaches (and also different training
data size) on the overall performance of an
English-to-Czech dependency-based statisti-
cal translation system implemented in the
Treex framework. We also study the relation-
ship between parsing accuracy in terms of un-
labeled attachment score and machine transla-
tion quality in terms of BLEU.
1 Introduction
In the last years, statistical n-gram models domi-
nated the field of Machine Translation (MT). How-
ever, their results are still far from perfect. Therefore
we believe it makes sense to investigate alternative
statistical approaches. This paper is focused on an
analysis-transfer-synthesis translation system called
TectoMT whose transfer representation has a shape
of a deep-syntactic dependency tree. The system has
been introduced by Z?abokrtsky? et al (2008). The
translation direction under consideration is English-
to-Czech.
It has been shown by Popel (2009) that the current
accuracy of the dependency parser employed in this
translation system is one of the limiting factors from
the viewpoint of its output quality. In other words,
the parsing phase is responsible for a large portion
of translation errors. The biggest source of trans-
lation errors in the referred study was (and prob-
ably still is) the transfer phase, however the pro-
portion has changed since and the relative impor-
tance of the parsing phase has grown, because the
tranfer phase errors have already been addressed by
improvements based on Hidden Markov Tree Mod-
els for lexical and syntactic choice as shown by
Z?abokrtsky? and Popel (2009), and by context sensi-
tive translation models based on maximum entropy
as described by Marec?ek et al (2010).
Our study proceeds along two directions. First,
we train two state-of-the-art dependency parsers on
training sets with varying size. Second, we use
five parsers based on different parsing techniques.
In both cases we document the relation between
parsing accuracy (in terms of Unlabeled Attachment
Score, UAS) and translation quality (estimated by
the well known BLEU metric).
The motivation behind the first set of experiments
is that we can extrapolate the learning curve and try
to predict how new advances in dependency parsing
can affect MT quality in the future.
The second experiment series is motivated by
the hypothesis that parsers based on different ap-
proaches are likely to have a different distribution
of errors, even if they can have competitive perfor-
mance in parsing accuracy. In dependency parsing
metrics, all types of incorrect edges typically have
the same weight,1 but some incorrect edges can be
more harmful than others from the MT viewpoint.
For instance, an incorrect attachment of an adverbial
node is usually harmless, while incorrect attachment
of a subject node might have several negative conse-
1This issue has been tackled already in the parsing literature;
for example, some authors disregard placement of punctuation
nodes within trees in the evaluation (Zeman, 2004).
433
quences such as:
? unrecognized finiteness of the governing verb,
which can lead to a wrong syntactization on the
target side (an infinitive verb phrase instead of
a finite clause),
? wrong choice of the target-side verb form (be-
cause of unrecognized subject-predicate agree-
ment),
? missing punctuation (because of wrongly rec-
ognized finite clause boundaries),
? wrong placement of clitics (because of wrongly
recognized finite clause boundaries),
? wrong form of pronouns (personal and posses-
sive pronouns referring to the clause?s subject
should have reflexive forms in Czech).
Thus it is obvious that the parser choice is im-
portant and that it might not be enough to choose a
parser, for machine translation, only according to its
UAS.
Due to growing popularity of dependency syntax
in the last years, there are a number of dependency
parsers available. The present paper deals with
five parsers evaluated within the translation frame-
work: three genuine dependency parsers, namely the
parsers described in (McDonald et al, 2005), (Nivre
et al, 2007), and (Zhang and Nivre, 2011), and two
constituency parsers (Charniak and Johnson, 2005)
and (Klein and Manning, 2003), whose outputs were
converted to dependency structures by Penn Con-
verter (Johansson and Nugues, 2007).
As for the related literature, there is no published
study measuring the influence of dependency parsers
on dependency-based MT to our knowledge.2
The remainder of this paper is structured as fol-
lows. The overall translation pipeline, within which
the parsers are tested, is described in Section 2. Sec-
tion 3 lists the parsers under consideration and their
main features. Section 4 summarizes the influence
of the selected parsers on the MT quality in terms of
BLEU. Section 5 concludes.
2However, the parser bottleneck of the dependency-based
MT approach was observed also by other researchers (Robert
Moore, personal communication).
2 Dependency-based Translation in Treex
We have implemented our experiments in the Treex
software framework (formerly TectoMT, introduced
by Z?abokrtsky? et al (2008)), which already offers
tool chains for analysis and synthesis of Czech and
English sentences.
We use the tectogrammatical (deep-syntactic)
layer of language representation as the transfer layer
in the presented MT experiments. Tectogrammat-
ics was introduced by Sgall (1967) and further
elaborated within the Prague Dependency Treebank
project (Hajic? et al, 2006). On this layer, each
sentence is represented as a tectogrammatical tree,
whose main properties (from the MT viewpoint) are
the following:
1. nodes represent autosemantic words,
2. edges represent semantic dependencies (a node
is an argument or a modifier of its parent),
3. there are no functional words (prepositions,
auxiliary words) in the tree, and the autose-
mantic words appear only in their base forms
(lemmas). Morphologically indispensable cat-
egories (such as number with nouns or tense
with verbs, but not number with verbs as it is
only imposed by agreement) are stored in sep-
arate node attributes (grammatemes).
The intuitions behind the decision to use tec-
togrammatics for MT are the following: we be-
lieve that (1) tectogrammatics largely abstracts from
language-specific means (inflection, agglutination,
functional words etc.) of expressing non-lexical
meanings and thus tectogrammatical trees are sup-
posed to be highly similar across languages, (2)
it enables a natural transfer factorization,3 (3) and
local tree contexts in tectogrammatical trees carry
more information (especially for lexical choice) than
local linear contexts in the original sentences.
The translation scenario is outlined in the rest of
this section.
3Morphological categories can be translated almost inde-
pendently from lemmas, which makes parallel training data
?denser?, especially when translating from/to a language with
rich inflection such as Czech.
434
2.1 Analysis
The input English text is segmented into sentences
and tokens. The tokens are lemmatized and tagged
with Penn Treebank tags using the Morce tagger
(Spoustova? et al, 2007). Then one of the studied
dependency parsers is applied and a surface-syntax
dependency tree (analytical tree in the PDT termi-
nology) is created for each sentence.
This tree is converted to a tectogrammatical tree.
Each autosemantic word with its associated func-
tional words is collapsed into a single tectogram-
matical node, labeled with a lemma, formeme,4 and
semantically indispensable morphologically cate-
gories; coreference is also resolved.
2.2 Transfer
The transfer phase follows, whose most difficult part
consists especially in labeling the tree with target-
side lemmas and formemes. There are also other
types of changes, such as node addition and dele-
tion. However, as shown by Popel (2009), changes
of tree topology are required relatively infrequently
due to the language abstractions on the tectogram-
matical layer.
Currently, translation models based on Maxi-
mum Entropy classifiers are used both for lemmas
and formemes (Marec?ek et al, 2010). Tree label-
ing is optimized using Hidden Tree Markov Mod-
els (Z?abokrtsky? and Popel, 2009), which makes
use of target-language dependency tree probabilistic
model.
All models used in the transfer phase are trained
using training sections of the Czech-English parallel
corpus CzEng 0.9 (Bojar and Z?abokrtsky?, 2009).
2.3 Synthesis
Finally, surface sentence shape is synthesized from
the tectogrammatical tree, which is basically the
reverse operation of the tectogrammatical analy-
sis. It consists of adding punctuation and functional
4Formeme captures the morphosyntactic means which are
used for expressing the tectogrammatical node in the surface
sentence shape. Examples of formeme values: v:that+fin ?
finite verb in a subordinated clause introduced with conjunction
that, n:sb ? semantic noun in a subject position, n:for+X ?
semantic noun in a prepositional group introduced with prepo-
sition for, adj:attr ? semantic adjective in an attributive po-
sition.
words, spreading morphological categories accord-
ing to grammatical agreement, performing inflection
(using Czech morphology database (Hajic?, 2004)),
arranging word order etc.
The difference from the analysis phase is that
there is not very much space for optimization in the
synthesis phase. In other words, final sentence shape
is determined almost uniquely by the tectogrammat-
ical tree (enriched with formemes) resulting from
the transfer phase. However, if there are not enough
constraints for a unique choice of a surface form of
a lemma, then a unigram language model is used for
the final decision. The model was trained using 500
million words from the Czech National Corpus.5
3 Involved Parsers
We performed experiments with parsers from
three families: graph-based parsers, transition-
based parsers, and phrase-structure parsers (with
constituency-to-dependency postprocessing).
3.1 Graph-based Parser
In graph-based parsing, we learn a model for scoring
graph edges, and we search for the highest-scoring
tree composed of the graph?s edges. We used Max-
imum Spanning Tree parser (Mcdonald and Pereira,
2006) which is capable of incorporating second or-
der features (MST for short).
3.2 Transition-based Parsers
Transition-based parsers utilize the shift-reduce al-
gorithm. Input words are put into a queue and
consumed by shift-reduce actions, while the out-
put parser is gradually built. Unlike graph-based
parsers, transition-based parsers have linear time
complexity and allow straightforward application of
non-local features.
We included two transition-based parsers into our
experiments:
? Malt ? Malt parser introduced by Nivre et al
(2007) 6
5http://ucnk.ff.cuni.cz
6We used stackeager algorithm, liblinear learner, and
the enriched feature set for English (the same configu-
ration as in pretrained English models downloadable at
http://maltparser.org.
435
? ZPar ? Zpar parser7 which is basically an al-
ternative implementation of the Malt parser,
employing a richer set of non-local features as
described by Zhang and Nivre (2011).
3.3 CFG-based Tree Parsers
Another option how to obtain dependency trees is
to apply a constituency parser, recognize heads in
the resulting phrase structures and apply a recur-
sive algorithm for converting phrase-structure trees
into constituency trees (the convertibility of the two
types of syntactic structures was studied already by
Gaifman (1965)).
We used two constituency parsers:
? Stanford ? The Stanford parser (Klein and
Manning, 2003),8
? CJ ? a MaxEnt-based parser combined with
discriminative reranking (Charniak and John-
son, 2005).9
Before applying the parsers on the text, the system
removes all spaces within tokens. For instance U. S.
becomes U.S. to restrict the parsers from creating
two new tokens. Tokenization built into both parsers
is bypassed and the default tokenization in Treex is
used.
After parsing, Penn Converter introduced by Jo-
hansson and Nugues (2007) is applied, with the
-conll2007 option, to change the constituent
structure output, of the two parsers, into CoNLL de-
pendency structure. This allows us to keep the for-
mats consistent with the output of both MST and
MaltParser within the Treex framework.
There is an implemented procedure for cre-
ating tectogrammatical trees from the English
phrase structure trees described by Kuc?erova? and
Z?abokrtsky? (2002). Using the procedure is more
straightforward, as it does not go through the
CoNLL-style trees; English CoNLL-style trees dif-
fer slightly from the PDT conventions (e.g. in at-
taching auxiliary verbs) and thus needs additional
7http://sourceforge.net/projects/zpar/ (version 0.4)
8Only the constituent, phrase based, parsed output is used in
these experiments.
9We are using the default settings from the August 2006 ver-
sion of the software.
postprocessing for our purposes. However, we de-
cided to stick to Penn Converter, so that the similar-
ity of the translation scenarios is maximized for all
parsers.
3.4 Common Preprocessing: Shallow Sentence
Chunking
According to our experience, many dependency
parsers have troubles with analyzing sentences that
contain parenthesed or quoted phrases, especially if
they are long.
We use the assumption that in most cases the con-
tent of parentheses or quotes should correspond to
a connected subgraph (subtree) of the syntactic tree.
We implemented a very shallow sentence chunker
(SentChunk) which recognizes parenthesed word
sequences. These sequences can be passed to a
parser first, and be parsed independently of the rest
of the sentence. This was shown to improve not only
parsing accuracy of the parenthesed word sequence
(which is forced to remain in one subtree), but also
the rest of the sentence.10
In our experiments, SentChunk is used only
in combination with the three genuine dependency
parsers.
4 Experiments and Evaluation
4.1 Data for Parsers? Training and Evaluation
The dependency trees needed for training the parsers
and evaluating their UAS were created from the
Penn Treebank data (enriched first with internal
noun phrase structure applied via scripts provided
by Vadas and Curran (2007)) by Penn Converter (Jo-
hansson and Nugues, 2007) with the -conll2007
option (PennConv for short).
All the parsers were evaluated on the same data ?
section 23.
All the parsers were trained on sections 02?21,
except for the Stanford parser which was trained
on sections 01?21. We were able to retrain the
parser models only for MST and Malt. For the
other parsers we used pretrained models available on
the Internet: CJ?s default model ec50spfinal,
Stanford?s wsjPCFG.ser.gz model, and
10Edge length is a common feature in dependency parsers, so
?deleting? parenthesed words may give higher scores to correct
dependency links that happened to span over the parentheses.
436
ZPar?s english.tar.gz. The model of ZPar
is trained on data converted to dependencies using
Penn2Malt tool,11 which selects the last member of
a coordination as the head. To be able to compare
ZPar?s output with the other parsers, we postpro-
cessed it by a simple ConjAsHead code that con-
verts this style of coordinations to the one used in
CoNLL2007, where the conjuction is the head.
4.2 Reference Translations Used for Evaluation
Translation experiments were evaluated using refer-
ence translations from the new-dev2009 data set,
provided by the organizors of shared translation task
with the Workshop on Statistical Machine Transla-
tion.
4.3 Influence of Parser Training Data Size
We trained a sequence of parser models for MST and
Malt, using a roughly exponentially growing se-
quence of Penn Treebank subsets. The subsets are
contiguous and start from the beginning of section
02. The results are collected in Tables 1 and 2.12
#tokens UAS BLEU NIST
100 0.362 0.0579 3.6375
300 0.509 0.0859 4.3853
1000 0.591 0.0995 4.6548
3000 0.623 0.1054 4.7972
10000 0.680 0.1130 4.9695
30000 0.719 0.1215 5.0705
100000 0.749 0.1232 5.1193
300000 0.776 0.1257 5.1571
990180 0.793 0.1280 5.1915
Table 1: The effect of training data size on parsing accu-
racy and on translation performance with MST.
The trend of the relation between the training data
size and BLEU is visible also in Figure 1. It is ob-
vious that increasing the training data has a positive
effect on the translation quality. However, the pace
of growth of BLEU is sublogarithmic, and becomes
unconvincing above 100,000 training tokens. It in-
dicates that given one of the two parsers integrated
11http://w3.msi.vxu.se/?nivre/research/
Penn2Malt.html
12To our knowledge, the best system participating in the
shared task reaches BLEU 17.8 for this translation direction.
#tokens UAS BLEU NIST
100 0.454 0.0763 4.0555
300 0.518 0.0932 4.4698
1000 0.591 0.1042 4.6769
3000 0.616 0.1068 4.7472
10000 0.665 0.1140 4.9100
30000 0.695 0.1176 4.9744
100000 0.723 0.1226 5.0504
300000 0.740 0.1238 5.1005
990180 0.759 0.1253 5.1296
Table 2: The effect of training data size on parsing accu-
racy and on translation performance with Malt.
 
0.05
 
0.06
 
0.07
 
0.08
 
0.09 0.1
 
0.11
 
0.12
 
0.13
 
100
 
100
0
 
100
00
 
100
000
 
1e+
06
BLEU
train
ing 
toke
ns
MS
T Mal
t
Figure 1: The effect of parser training data size of BLEU
with Malt and MST parsers.
into our translation framework, increasing the parser
training data alone would probably not lead to a sub-
stantial improvement of the translation performance.
4.4 Influence of Parser Choice
Table 3 summarizes our experiments with the five
parsers integrated into the tectogrammatical transla-
tion pipeline. Two configurations (with and without
SentChunk) are listed for the genuine dependency
parsers. The relationship between UAS and BLEU
for (the best configurations of) all five parsers is de-
picted also in Figure 2.
Additionally, we used paired bootstrap 95% con-
fidence interval testing (Zhang et al, 2004), to check
which BLEU differences are significant. For the
five compared parser (with SentChunk if appli-
cable), only four comparisons are not significant:
MST-CJ, MST-Stanford, Malt-Stanford,
and CJ-Stanford.
437
Parser Training data Preprocessing Postprocessing UAS BLEU NIST TER
MST PennTB + PennConv SentChunk ? 0.793 0.1280 5.192 0.735
MST PennTB + PennConv ? ? 0.794 0.1236 5.149 0.739
Malt PennTB + PennConv SentChunk ? 0.760 0.1253 5.130 0.740
Malt PennTB + PennConv ? ? 0.761 0.1214 5.088 0.744
Zpar PennTB + Penn2Malt SentChunk ConjAsHead 0.793 0.1176 5.039 0.749
Zpar PennTB + Penn2Malt ? ConjAsHead 0.792 0.1127 4.984 0.754
CJ PennTB ? PennConv 0.904 0.1284 5.189 0.737
Stanford PennTB ? PennConv 0.825 0.1277 5.137 0.740
Table 3: Dependency parsers tested in the translation pipeline.
 
0.1
 
0.10
5
 
0.11
 
0.11
5
 
0.12
 
0.12
5
 
0.13
 
0.13
5
 
0.14
 
0.14
5
 
0.15
 
0.74
 
0.76
 
0.78
 
0.8
 
0.82
 
0.84
 
0.86
 
0.88
 
0.9
 
0.92
BLEU
UAS
MS
T Mal
t
Zpa
r
Sta
nfor
d CJ
Figure 2: Unlabeled Attachment Score versus BLEU.
Even if BLEU grows relatively smoothly with
UAS for different parsing models of the same parser,
one can see that there is no obvious relation be-
tween UAS and BLEU accross all parsers. MST and
Zpar have the same UAS but quite different BLEU,
whereas MST and CJ have very similar BLEU but
distant UAS. It confirms the original hypothesis that
it is not only the overall UAS, but also the parser-
specific distribution of errors what matters.
4.5 Influence of Shallow Sentence Chunking
Table 3 confirms that parsing the contents paren-
theses separately from the rest of the sentence
(SentChunk) has a positive effect with all three
dependency parsers. Surprisingly, even if the effect
on UAS is negligible, the improvement is almost
half of BLEU point which is significant for all the
three parsers.
4.6 Discussion on Result Comparability
We tried to isolate the effects of the properties of
selected parsers, however, the separation from other
influencing factors is not perfect due to several tech-
nical issues:
? So far, we were not able to retrain the models
for all parsers ourselves and therefore their pre-
trained models (one of them based on slightly
different Penn Treebank division) must have
been used.
? Some parsers make their own choice of POS
tags within the parsed sentences, while other
parsers require the sentences to be tagged al-
ready on their input.
? The trees in the CzEng 0.9 parallel treebank
were created using MST. CzEng 0.9 was used
for training translation models used in the
transfer phase of the translation scenario; thus
these translation models might compensate for
some MST?s errors, which might handicap other
parsers. So far we were not able to reparse 8
million sentence pairs in CzEng 0.9 by all stud-
ied parsers.
5 Conclusions
This paper is a study of how the choice of a de-
pendency parsing technique influences the quality of
English-Czech dependency-based translation. Our
main observations are the following. First, BLEU
grows with the increasing amount of training depen-
dency trees, but only in a sublogarithmic pace. Sec-
ond, what seems to be quite effective for translation
438
is to facilitate the parsers? task by dividing the sen-
tences into smaller chunks using parenthesis bound-
aries. Third, if the parsers are based on different
approaches, their UAS does not correlate well with
their effect on the translation quality.
Acknowledgments
This research was supported by the
grants MSM0021620838, GAUK 116310,
GA201/09/H057, and by the European Com-
mission?s 7th Framework Program (FP7) under
grant agreements n? 238405 (CLARA), n? 247762
(FAUST), and n? 231720 (EuroMatrix Plus).
References
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng
0.9, Building a Large Czech-English Automatic Par-
allel Treebank. The Prague Bulletin of Mathematical
Linguistics, 92:63?83.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
Association for Computational Linguistics, ACL ?05,
pages 173?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Haim Gaifman. 1965. Dependency systems and phrase-
structure systems. Information and Control, pages
304?337.
Jan Hajic? et al 2006. Prague Dependency Treebank 2.0.
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T01, Philadelphia.
Jan Hajic?. 2004. Disambiguation of Rich Inflection ?
Computational Morphology of Czech. Charles Uni-
versity ? The Karolinum Press, Prague.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007, pages 105?112,
Tartu, Estonia, May 25-26.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of Association for Computational Lin-
guistics, pages 423?430.
Ivona Kuc?erova? and Zdene?k Z?abokrtsky?. 2002. Trans-
forming Penn Treebank Phrase Trees into (Praguian)
Tectogrammatical Dependency Trees. The Prague
Bulletin of Mathematical Linguistics, (78):77?94.
David Marec?ek, Martin Popel, and Zdene?k Z?abokrtsky?.
2010. Maximum entropy translation model in
dependency-based MT framework. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 201?201, Uppsala,
Sweden. Association for Computational Linguistics.
Ryan Mcdonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
HLT / EMNLP, pages 523?530, Vancouver, Canada.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Martin Popel. 2009. Ways to Improve the Quality of
English-Czech Machine Translation. Master?s thesis,
Institute of Formal and Applied Linguistics, Charles
University, Prague, Czech Republic.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska? dek-
linace. Academia, Prague.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The Best of Two
Worlds: Cooperation of Statistical and Rule-Based
Taggers for Czech. In Proceedings of the Workshop
on Balto-Slavonic Natural Language Processing, ACL
2007, pages 67?74, Praha.
David Vadas and James Curran. 2007. Adding Noun
Phrase Structure to the Penn Treebank. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 240?247, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Zdene?k Z?abokrtsky? and Martin Popel. 2009. Hidden
Markov Tree Model in Dependency-based Machine
Translation. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 145?148, Suntec, Sin-
gapore.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceedings
of the 3rd Workshop on Statistical Machine Transla-
tion, ACL, pages 167?170.
Daniel Zeman. 2004. Parsing with a Statistical Depen-
dency Model. Ph.D. thesis, Faculty of Mathematics
and Physics, Charles University in Prague.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In To
appear in the Proceedings of the 49th Annual Meeting
of the Association of Computational Linguistics.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting bleu/nist scores: How much improvement
do we need to have a better system. In Proceedings of
LREC, volume 4, pages 2051?2054.
439
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 19?26,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
Hybrid Combination of Constituency and Dependency Trees into an
Ensemble Dependency Parser
Nathan David Green and Zdene?k Z?abokrtsky?
Charles University in Prague
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics
Prague, Czech Republic
{green,zabokrtsky}@ufal.mff.cuni.cz
Abstract
Dependency parsing has made many ad-
vancements in recent years, in particu-
lar for English. There are a few de-
pendency parsers that achieve compara-
ble accuracy scores with each other but
with very different types of errors. This
paper examines creating a new depen-
dency structure through ensemble learn-
ing using a hybrid of the outputs of var-
ious parsers. We combine all tree out-
puts into a weighted edge graph, using 4
weighting mechanisms. The weighted edge
graph is the input into our ensemble sys-
tem and is a hybrid of very different parsing
techniques (constituent parsers, transition-
based dependency parsers, and a graph-
based parser). From this graph we take a
maximum spanning tree. We examine the
new dependency structure in terms of accu-
racy and errors on individual part-of-speech
values.
The results indicate that using a greater
number of more varied parsers will improve
accuracy results. The combined ensemble
system, using 5 parsers based on 3 different
parsing techniques, achieves an accuracy
score of 92.58%, beating all single parsers
on the Wall Street Journal section 23 test
set. Additionally, the ensemble system re-
duces the average relative error on selected
POS tags by 9.82%.
1 Introduction
Dependency parsing has made many advance-
ments in recent years. A prime reason for the
quick advancement has been the CoNLL shared
task competitions. These competitions gave the
community a common training/testing framework
along with many open source systems. These sys-
tems have, for certain languages, achieved fairly
high accuracy. Many of the top systems have
comparable accuracy but vary on the types of
errors they make. The approaches used in the
shared task vary from graph-based techniques to
transition-based techniques to the conversion of
constituent trees produced by state-of-the-art con-
stituent parsers. This varied error distribution
makes dependency parsing a prime area for the
application of new hybrid and ensemble algo-
rithms.
Increasing accuracy of dependency parsing of-
ten is in the realm of feature tweaking and opti-
mization. The idea behind ensemble learning is to
take the best of each parser as it currently is and
allow the ensemble system to combine the outputs
to form a better overall parse using prior knowl-
edge of each individual parser. This is often done
by different weighting or voting schemes.
2 Related Work
Ensemble learning (Dietterich, 2000) has been
used for a variety of machine learning tasks and
recently has been applied to dependency pars-
ing in various ways and with different levels of
success. (Surdeanu and Manning, 2010; Haf-
fari et al, 2011) showed a successful combina-
tion of parse trees through a linear combination
of trees with various weighting formulations. To
keep their tree constraint, they applied Eisner?s al-
gorithm for reparsing (Eisner, 1996).
Parser combination with dependency trees has
been examined in terms of accuracy (Sagae and
Lavie, 2006; Sagae and Tsujii, 2007; Zeman and
Z?abokrtsky?, 2005). However, the various tech-
niques have generally examined similar parsers
19
or parsers which have generated various different
models. To the best of our knowledge, our ex-
periments are the first to look at the accuracy and
part of speech error distribution when combining
together constituent and dependency parsers that
use many different techniques. However, POS
tags were used in parser combination in (Hall et
al., 2007) for combining a set of Malt Parser mod-
els with success.
Other methods of parser combinations have
shown to be successful such as using one parser
to generate features for another parser. This was
shown in (Nivre and McDonald, 2008), in which
Malt Parser was used as a feature to MST Parser.
The result was a successful combination of a
transition-based and graph-based parser, but did
not address adding other types of parsers into the
framework.
3 Methodology
The following sections describe the process flow,
choice of parsers, and datasets needed for oth-
ers to recreate the results listed in this paper.
Although we describe the specific parsers and
datasets used in this paper, this process flow
should work for any number of hybrid combina-
tions of parsers and datasets.
3.1 Process Flow
To generate a single ensemble parse tree, our sys-
tem takes N parse trees as input. The inputs are
from a variety of parsers as described in 3.2.
All edges in these parse trees are combined into
a graph structure. This graph structure accepts
weighted edges. So if more than one parse tree
contains the same tree edge, the graph is weighted
appropriately according to a chosen weighting al-
gorithm. The weighting algorithms used in our
experiments are described in 3.5.
Once the system has a weighted graph, it then
uses an algorithm to find a corresponding tree
structure so there are no cycles. In this set of ex-
periments, we constructed a tree by finding the
maximum spanning tree using ChuLiu/Edmonds?
algorithm, which is a standard choice for MST
tasks. Figure 1 graphically shows the decisions
one needs to make in this framework to create an
ensemble parse.
Figure 1: General flow to create an ensemble parse
tree.
3.2 Parsers
To get a complete representation of parsers in
our ensemble learning framework we use 5 of
the most commonly used parsers. They range
from graph-based approaches to transition-based
approaches to constituent parsers. Constituency
output is converted to dependency structures us-
ing a converter (Johansson and Nugues, 2007).
All parsers are integrated into the Treex frame-
work (Z?abokrtsky? et al, 2008; Popel et al, 2011)
using the publicly released parsers from the re-
spective authors but with Perl wrappers to allow
them to work on a common tree structure.
? Graph-Based: A dependency tree is a spe-
cial case of a weighted edge graph that
spawns from an artificial root and is acyclic.
Because of this we can look at a large history
of work in graph theory to address finding
the best spanning tree for each dependency
graph. In this paper we use MST Parser
(McDonald et al, 2005) as an input to our
ensemble parser.
? Transition-Based: Transition-based parsing
creates a dependency structure that is pa-
rameterized over the transitions used to cre-
ate a dependency tree. This is closely re-
lated to shift-reduce constituency parsing al-
gorithms. The benefit of transition-based
parsing is the use of greedy algorithms which
have a linear time complexity. However, due
to the greedy algorithms, longer arc parses
can cause error propagation across each tran-
sition (Ku?bler et al, 2009). We make use
20
of Malt Parser (Nivre et al, 2007b), which
in the shared tasks was often tied with the
best performing systems. Additionally we
use Zpar (Zhang and Clark, 2011) which is
based on Malt Parser but with a different set
of non-local features.
? Constituent Transformation While not a
true dependency parser, one technique of-
ten applied is to take a state-of-the-art con-
stituent parser and transform its phrase based
output into dependency relations. This has
been shown to also be state-of-the-art in ac-
curacy for dependency parsing in English. In
this paper we transformed the constituency
structure into dependencies using the Penn
Converter conversion tool (Johansson and
Nugues, 2007). A version of this converter
was used in the CoNLL shared task to create
dependency treebanks as well. For the fol-
lowing ensemble experiments we make use
of both (Charniak and Johnson, 2005) and
Stanford?s (Klein and Manning, 2003) con-
stituent parsers.
In addition to these 5 parsers, we also report
the accuracy of an Oracle Parser. This parser is
simply the best possible parse of all the edges of
the combined dependency trees. If the reference,
gold standard, tree has an edge that any of the 5
parsers contain, we include that edge in the Or-
acle parse. Initially all nodes of the tree are at-
tached to an artificial root in order to maintain
connectedness. Since only edges that exist in a
reference tree are added, the Oracle Parser main-
tains the acyclic constraint. This can be viewed
as the maximum accuracy that a hybrid approach
could achieve with this set of parsers and with the
given data sets.
3.3 Datasets
Much of the current progress in dependency pars-
ing has been a result of the availability of common
data sets in a variety of languages, made avail-
able through the CoNLL shared task (Nivre et al,
2007a). This data is in 13 languages and 7 lan-
guage families. Later shared tasks also released
data in other genres to allow for domain adap-
tation. The availability of standard competition,
gold level, data has been an important factor in
dependency based research.
For this study we use the English CoNLL data.
This data comes from the Wall Street Journal
(WSJ) section of the Penn treebank (Marcus et al,
1993). All parsers are trained on sections 02-21 of
the WSJ except for the Stanford parser which uses
sections 01-21. Charniak, Stanford and Zpar use
pre-trained models ec50spfinal, wsjPCFG.ser.gz,
english.tar.gz respectively. For testing we use sec-
tion 23 of the WSJ for comparability reasons with
other papers. This test data contains 56,684 to-
kens. For tuning we use section 22. This data is
used for determining some of the weighting fea-
tures.
3.4 Evaluation
As an artifact of the CoNLL shared tasks
competition, two standard metrics for com-
paring dependency parsing systems emerged.
Labeled attachment score (LAS) and unlabeled
attachment score (UAS). UAS studies the struc-
ture of a dependency tree and assesses whether the
output has the correct head and dependency arcs.
In addition to the structure score in UAS, LAS
also measures the accuracy of the dependency la-
bels on each arc. A third, but less common met-
ric, is used to judge the percentage of sentences
that are completely correct in regards to their LAS
score. For this paper since we are primarily con-
cerned with the merging of tree structures we only
evaluate UAS (Buchholz and Marsi, 2006).
3.5 Weighting
Currently we are applying four weighting algo-
rithms to the graph structure. First we give each
parser the same uniform weight. Second we ex-
amine weighting each parser output by the UAS
score of the individual parser taken from our tun-
ing data. Third we use plural voting weights
(De Pauw et al, 2006) based on parser ranks from
our tuning data. Due to the success of Plural vot-
ing, we try to exaggerate the differences in the
parsers by using UAS10 weighting. All four of
these are simple weighting techniques but even in
their simplicity we can see the benefit of this type
of combination in an ensemble parser.
? Uniform Weights: an edge in the graph gets
incremented +1 weight for each matching
edge in each parser. If an edge occurs in 4
parsers, the weight is 4.
? UAS Weighted: Each edge in the graph gets
21
incremented by the value of it?s parsers in-
dividual accuracy. So in the UAS results
in Table 1 an edge in Charniak?s tree gets
.92 added while MST gets .86 added to ev-
ery edge they share with the resulting graph.
This weighting should allow us to add poor
parsers with very little harm to the overall
score.
? Plural Voting Weights: In Plural Voting
the parsers are rated according to their rank
in our tuning data and each gets a ?vote?
based on their quality. With N parsers the
best parser gets N votes while the last place
parser gets 1 vote. In this paper, Charniak
received 5 votes, Stanford received 4 votes,
MST Parser received 3 votes, Malt Parser
received 2 votes, and Zpar received 1 vote.
Votes in this case are added to each edge as
a weight.
? UAS10: For this weighting scheme we took
each UAS value to the 10th power. This gave
us the desired affect of making the differ-
ences in accuracy more apparent and giving
more distance from the best to worse parser.
This exponent was empirically selected from
results with our tuning data set.
4 Results
Table 1 contains the results of different parser
combinations of the 5 parsers and Table 2 shows
the baseline scores of the respective individual
parsers. The results indicate that using two
parsers will result in an ?average? score, and no
combination of 2 parsers gave an improvement
over the individual parsers, these were left out
of the table. Ensemble learning seems to start to
have a benefit when using 3 or more parsers with a
few combinations having a better UAS score than
any of the baseline parsers, these cases are in bold
throughout the table. When we add a 4th parser
to the mix almost all configurations lead to an
improved score when the edges are not weighted
uniformly. The only case in which this does not
occur is when Stanford?s Parser is not used.
Uniform voting gives us an improved score in a
few of the model combinations but in most cases
does not produce an output that beats the best in-
dividual system. UAS weighting is not the best
overall but it does give improved performance in
the majority of model combinations. Problemati-
cally UAS weighted trees do not give an improved
accuracy when all 5 parsers are used. Given the
slight differences in UAS scores of the baseline
models in Table 2 this is not surprising as the
best graph edge can be outvoted as the number
of N parsers increases. The slight differences in
weight do not seem to change the MST parse dra-
matically when all 5 parsers are used over Uni-
form weighting. Based on the UAS scores learned
in our tuning data set, we next looked to amplify
the weight differences using Plural Voting. For
the majority of model combinations in Plural vot-
ing we achieve improved results over the individ-
ual systems. When all 5 parsers are used together
with Plural Voting, the ensemble parser improves
over the highest individual parser?s UAS score.
With the success of Plural voting we looked to
amplify the UAS score differences in a more sys-
tematic way. We looked at using UASx where
x was found experimentally in our tuning data.
UAS10 matched Plural voting in the amount of
system combinations that improved over their in-
dividual components. The top overall score is
when we use UAS10 weighting with all parsers.
For parser combinations that do not feature Char-
niak?s parser, we also find an increase in over-
all accuracy score compared to each individual
parser, although never beating Charniak?s individ-
ual score.
To see the maximum accuracy a hybrid combi-
nation can achieve we include an Oracle Ensem-
ble Parser in Table 1. The Oracle Parser takes
the edges from all dependency trees and only adds
each edge to the Oracle Tree if the corresponding
edge is in the reference tree. This gives us a ceil-
ing on what ensemble learning can achieve. As
we can see in Table 1, the ceiling of ensemble
learning is 97.41% accuracy. Because of this high
value with only 5 parsers, ensemble learning and
other hybrid approaches should be a very prosper-
ous area for dependency parsing research.
In (Ku?bler et al, 2009) the authors confirm that
two parsers, MST Parser and Malt Parser, give
similar accuracy results but with very different
errors. MST parser, a maximum spanning tree
graph-based algorithm, has evenly distributed er-
rors while Malt Parser, a transition based parser,
has errors on mainly longer sentences. This re-
22
System Uniform UAS Plural UAS10 Oracle
Weighting Weighted Voting Weighted UAS
Charniak-Stanford-Mst 91.86 92.27 92.28 92.25 96.48
Charniak-Stanford-Malt 91.77 92.28 92.3 92.08 96.49
Charniak-Stanford-Zpar 91.22 91.99 92.02 92.08 95.94
Charniak-Mst-Malt 88.80 89.55 90.77 92.08 96.3
Charniak-Mst-Zpar 90.44 91.59 92.08 92.08 96.16
Charniak-Malt-Zpar 88.61 91.3 92.08 92.08 96.21
Stanford-Mst-Malt 87.84 88.28 88.26 88.28 95.62
Stanford-Mst-Zpar 89.12 89.88 88.84 89.91 95.57
Stanford-Malt-Zpar 88.61 89.57 87.88 87.88 95.47
Mst-Malt-Zpar 86.99 87.34 86.82 86.49 93.79
Charniak-Stanford-Mst-Malt 90.45 92.09 92.34 92.56 97.09
Charniak-Stanford-Mst-Zpar 91.57 92.24 92.27 92.26 96.97
Charniak-Stanford-Malt-Zpar 91.31 92.14 92.4 92.42 97.03
Charniak-Mst-Malt-Zpar 89.60 89.48 91.71 92.08 96.79
Stanford-Mst-Malt-Zpar 88.76 88.45 88.95 88.44 96.36
All 91.43 91.77 92.44 92.58 97.41
Table 1: Results of the maximum spanning tree algorithm on a combined edge graph. Scores are in bold when
the ensemble system increased the UAS score over all individual systems.
Parser UAS
Charniak 92.08
Stanford 87.88
MST 86.49
Malt 84.51
Zpar 76.06
Table 2: Our baseline parsers and corresponding UAS
used in our ensemble experiments
sult comes from the approaches themselves. MST
parser is globally trained so the best mean solu-
tion should be found. This is why errors on the
longer sentences are about the same as the shorter
sentences. Malt Parser on the other hand uses a
greedy algorithm with a classifier that chooses a
particular transition at each vertex. This leads to
the possibility of the propagation of errors further
in a sentence. Along with this line of research,
we look at the error distribution for all 5 parsers
along with our best ensemble parser configura-
tion. Much like the previous work, we expect dif-
ferent types of errors, given that our parsers are
from 3 different parsing techniques. To examine
if the ensemble parser is substantially changing
the parse tree or is just taking the best parse tree
and substituting a few edges, we examine the part
of speech accuracies and relative error reduction
in Table 3.
As we can see the range of POS errors varies
dramatically depending on which parser we ex-
amine. For instance for CC, Charniak has 83.54%
accuracy while MST has only 71.16% accuracy.
The performance for certain POS tags is almost
universally low such as the left parenthesis (.
Given the large difference in POS errors, weight-
ing an ensemble system by POS would seem like
a logical choice in future work. As we can see
in Figure 2, the varying POS accuracies indicate
that the parsing techniques we have incorporated
into our ensemble parser, are significantly differ-
ent. In almost every case in Table 3, our ensemble
parser achieves the best accuracy for each POS,
while reducing the average relative error rate by
9.82%.
The current weighting systems do not simply
default to the best parser or to an average of all er-
rors. In the majority of cases our ensemble parser
obtains the top accuracy. The ability of the en-
semble system to use maximum spanning tree on
a graph allows the ensemble parser to connect
nodes which might have been unconnected in a
subset of the parsers for an overall gain, which
is preferable to techniques which only select the
best model for a particular tree. In all cases,
our ensemble parser is never the worst parser. In
23
POS Charniak Stanford MST Malt Zpar Best Relative Error
Ensemble Reduction
CC 83.54 74.73 71.16 65.84 20.39 84.63 6.62
NNP 94.59 92.16 88.04 87.17 73.67 95.02 7.95
VBN 91.72 89.81 90.35 89.17 88.26 93.81 25.24
CD 94.91 92.67 85.19 84.46 82.64 94.96 0.98
RP 96.15 95.05 97.25 95.60 94.51 97.80 42.86
JJ 95.41 92.99 94.47 93.90 89.45 95.85 9.59
PRP 97.82 96.21 96.68 95.64 95.45 98.39 26.15
TO 94.52 89.44 91.29 90.73 88.63 94.35 -3.10
WRB 63.91 60.90 68.42 73.68 4.51 63.91 0.00
RB 86.26 79.88 81.49 81.44 80.61 87.19 6.77
WDT 97.14 95.36 96.43 95.00 9.29 97.50 12.59
VBZ 91.97 87.35 83.86 80.78 57.91 92.46 6.10
( 73.61 75.00 54.17 58.33 15.28 73.61 0.00
POS 98.18 96.54 98.54 98.72 0.18 98.36 9.89
VB 93.04 88.48 91.33 90.95 84.37 94.24 17.24
MD 89.55 82.02 83.05 78.77 51.54 89.90 3.35
NNS 93.10 89.51 90.68 88.65 78.93 93.67 8.26
NN 93.62 90.29 88.45 86.98 83.84 94.00 5.96
VBD 93.25 87.20 86.27 82.73 64.32 93.52 4.00
DT 97.61 96.47 97.30 97.01 92.19 97.97 15.06
RBS 90.00 76.67 93.33 93.33 86.67 90.00 0.00
IN 87.80 78.66 83.45 80.78 73.08 87.48 -2.66
) 70.83 77.78 96.46 55.56 12.50 72.22 4.77
VBG 85.19 82.13 82.74 82.25 81.27 89.35 28.09
Average 9.82
Table 3: POS accuracies for each of our systems that are used in the ensemble system. We use these accuracies
to obtain the POS error distribution for our best ensemble system, which is the combination of all parsers using
UAS10 weighting. Relative error reduction is calculated between our best ensemble system against the Charniak
Parser which had the best individual scores.
24
Figure 2: POS errors of all 5 parsers and the best en-
semble system
cases where the POS is less frequent, our ensem-
ble parser appears to average out the error distri-
bution.
5 Conclusion
We have shown the benefits of using a maxi-
mum spanning tree algorithm in ensemble learn-
ing for dependency parsing, especially for the
hybrid combination of constituent parsers with
other dependency parsing techniques. This en-
semble method shows improvements over the cur-
rent state of the art for each individual parser. We
also show a theoretical maximum oracle parser
which indicates that much more work in this field
can take place to improve dependency parsing ac-
curacy toward the oracle score of 97.41%.
We demonstrated that using parsers of differ-
ent techniques, especially including transformed
constituent parsers, can lead to the best accuracy
within this ensemble framework. The improve-
ments in accuracy are not simply due to a few
edge changes but can be seen to improve the ac-
curacy of the majority of POS tags over all indi-
vidual systems.
While we have only shown this for English,
we expect the results to be similar for other lan-
guages since our methodology is language in-
dependent. Future work will contain different
weighting mechanisms as well as application to
other languages which are included in CoNLL
data sets.
6 Acknowledgments
This research has received funding from the
European Commission?s 7th Framework Pro-
gram (FP7) under grant agreement n? 238405
(CLARA)
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ?06,
pages 149?164, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Guy De Pauw, Gilles-Maurice de Schryver, and Peter
Wagacha. 2006. Data-driven part-of-speech tag-
ging of kiswahili. In Petr Sojka, Ivan Kopecek, and
Karel Pala, editors, Text, Speech and Dialogue, vol-
ume 4188 of Lecture Notes in Computer Science,
pages 197?204. Springer Berlin / Heidelberg.
Thomas G. Dietterich. 2000. Ensemble methods in
machine learning. In Proceedings of the First In-
ternational Workshop on Multiple Classifier Sys-
tems, MCS ?00, pages 1?15, London, UK. Springer-
Verlag.
Jason Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In
Proceedings of the 16th International Conference
on Computational Linguistics (COLING-96), pages
340?345, Copenhagen, August.
Gholamreza Haffari, Marzieh Razavi, and Anoop
Sarkar. 2011. An ensemble model that combines
syntactic and semantic clustering for discriminative
dependency parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
710?714, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen
Eryigit, Bea?ta Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single malt or blended?
a study in multilingual parser optimization. In
Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 933?939.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
25
English. In Proceedings of NODALIDA 2007,
pages 105?112, Tartu, Estonia, May 25-26.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis lectures on human lan-
guage technologies. Morgan & Claypool, US.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the Penn Treebank. Com-
put. Linguist., 19:313?330, June.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950?958, Columbus, Ohio, June. Association for
Computational Linguistics.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 shared task
on dependency parsing. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 915?932, Prague, Czech Republic,
June. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gulsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007b. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Martin Popel, David Marec?ek, Nathan Green, and
Zdene?k Z?abokrtsky?. 2011. Influence of parser
choice on dependency-based mt. In Proceedings of
the Sixth Workshop on Statistical Machine Trans-
lation, pages 433?439, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Kenji Sagae and Alon Lavie. 2006. Parser combi-
nation by reparsing. In Proceedings of the Human
Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 129?132,
New York City, USA, June. Association for Com-
putational Linguistics.
Kenji Sagae and Jun?ichi Tsujii. 2007. Depen-
dency parsing and domain adaptation with LR mod-
els and parser ensembles. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 1044?1050, Prague, Czech Republic,
June. Association for Computational Linguistics.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, HLT ?10, pages 649?652, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceed-
ings of the 3rd Workshop on Statistical Machine
Translation, ACL, pages 167?170.
Daniel Zeman and Zdene?k Z?abokrtsky?. 2005. Im-
proving parsing accuracy by combining diverse de-
pendency parsers. In In: Proceedings of the 9th In-
ternational Workshop on Parsing Technologies.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
26
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 72?77,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Using an SVM Ensemble System for Improved Tamil Dependency Parsing
Nathan Green, Loganathan Ramasamy and Zdene?k Z?abokrtsky?
Charles University in Prague
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics
Prague, Czech Republic
{green,ramasamy,zabokrtsky}@ufal.mff.cuni.cz
Abstract
Dependency parsing has been shown to im-
prove NLP systems in certain languages and
in many cases helps achieve state of the art re-
sults in NLP applications, in particular appli-
cations for free word order languages. Mor-
phologically rich languages are often short on
training data or require much higher amounts
of training data due to the increased size of
their lexicon. This paper examines a new
approach for addressing morphologically rich
languages with little training data to start.
Using Tamil as our test language, we cre-
ate 9 dependency parse models with a lim-
ited amount of training data. Using these
models we train an SVM classifier using only
the model agreements as features. We use
this SVM classifier on an edge by edge deci-
sion to form an ensemble parse tree. Using
only model agreements as features allows this
method to remain language independent and
applicable to a wide range of morphologically
rich languages.
We show a statistically significant 5.44%
improvement over the average dependency
model and a statistically significant 0.52% im-
provement over the best individual system.
1 Introduction
Dependency parsing has made many advancements
in recent years. A prime reason for the quick ad-
vancement has been the CoNLL shared task compe-
titions, which gave the community a common train-
ing/testing framework along with many open source
systems. These systems have, for certain languages,
achieved high accuracy ranging from on average
from approximately 60% to 80% (Buchholz and
Marsi, 2006). The range of scores are more of-
ten language dependent rather than system depen-
dent, as some languages contain more morpholog-
ical complexities. While some of these languages
are morphologically rich, we would like to addition-
ally address dependency parsing methods that may
help under-resourced languages as well, which often
overlaps with morphologically rich languages. For
this reason, we have chosen to do the experiments
in this paper using the Tamil Treebank (Ramasamy
and Z?abokrtsky?, 2012).
Tamil belongs to Dravidian family of languages
and is mainly spoken in southern India and also in
parts of Sri Lanka, Malaysia and Singapore. Tamil
is agglutinative and has a rich set of morphologi-
cal suffixes. Tamil has nouns and verbs as two ma-
jor word classes, and hundreds of word forms can
be produced by the application of concatenative and
derivational morphology. Tamil?s rich morphology
makes the language free word order except that it is
strictly head final.
When working with small datasets it is often very
difficult to determine which dependency model will
best represent your data. One can try to pick the
model through empirical means on a tuning set but
as the data grows in the future this model may no
longer be the best choice. The change in the best
model may be due to new vocabulary or through a
domain shift. If the wrong single model is chosen
early on when training is cheap, when the model is
applied in semi supervised or self training it could
lead to significantly reduced annotation accuracy.
72
For this reason, we believe ensemble combinations
are an appropriate direction for lesser resourced lan-
guages, often a large portion of morphologically
rich languages. Ensemble methods are robust as
data sizes grow, since the classifier can easily be re-
trained with additional data and the ensemble model
chooses the best model on an edge by edge basis.
This cost is substantially less than retraining multi-
ple dependency models.
2 Related Work
Ensemble learning (Dietterich, 2000) has been used
for a variety of machine learning tasks and recently
has been applied to dependency parsing in various
ways and with different levels of success. (Surdeanu
and Manning, 2010; Haffari et al, 2011) showed
a successful combination of parse trees through a
linear combination of trees with various weight-
ing formulations. Parser combination with depen-
dency trees have been examined in terms of accu-
racy (Sagae and Lavie, 2006; Sagae and Tsujii,
2007; Zeman and Z?abokrtsky?, 2005; S?gaard and
Rish?j, 2010). (Sagae and Lavie, 2006; Green and
Z?abokrtsky?, 2012) differ in part since their method
guarantees a tree while our system can, in some sit-
uations, produce a forest. POS tags were used in
parser combination in (Hall et al, 2007) for combin-
ing a set of Malt Parser models with an SVM clas-
sifier with success, however we believe our work is
novel in its use of an SVM classifier solely on model
agreements. Other methods of parse combinations
have shown to be successful such as using one parser
to generate features for another parser. This was
shown in (Nivre and McDonald, 2008; Martins et
al., 2008), in which Malt Parser was used as a fea-
ture to MST Parser.
Few attempts were reported in the literature on the
development of a treebank for Tamil. Our exper-
iments are based on the openly available treebank
(TamilTB) (Ramasamy and Z?abokrtsky?, 2012). De-
velopment of TamilTB is still in progress and the ini-
tial results for TamilTB appeared in (Ramasamy and
Z?abokrtsky?, 2011). Previous parsing experiments in
Tamil were done using a rule based approach which
utilized morphological tagging and identification of
clause boundaries to parse the sentences. The results
were also reported for Malt Parser and MST parser.
Figure 1: Process Flow for one run of our SVM Ensemble
system. This Process in its entirety was run 100 times for
each of the 8 data set splits.
When the morphological tags were available during
both training and testing, the rule based approach
performed better than Malt and MST parsers. For
other Indian languages, treebank development is ac-
tive mainly for Hindi and Telugu. Dependency pars-
ing results for them are reported in (Husain et al,
2010).
3 Methodology
3.1 Process Flow
When dealing with small data sizes it is often
not enough to show a simple accuracy increase.
This increase can be very reliant on the train-
ing/tuning/testing data splits as well as the sam-
pling of those sets. For this reason our experi-
ments are conducted over 7 training/tuning/testing
data split configurations. For each configuration
we randomly sample without replacement the train-
ing/tuning/testing data and rerun the experiment 100
times. These 700 runs, each on different samples,
allow us to better show the overall effect on the ac-
curacy metric as well as the statistically significant
changes as described in Section 3.5. Figure 1 shows
this process flow for one run of this experiment.
73
3.2 Parsers
A dependency tree is a special case of a depen-
dency graph that spawns from an artificial root, is
connected, follows a single-head constraint and is
acyclic. Because of this we can look at a large his-
tory of work in graph theory to address finding the
best spanning tree for each dependency graph. The
most common form of this type of dependency pars-
ing is Graph-Based parsing also called arc-factored
parsing and deals with the parameterization of the
edge weights. The main drawback of these meth-
ods is that for projective trees, the worst case sce-
nario for most methods is a complexity of O(n3)
(Eisner, 1996). However, for non-projective pars-
ing Chu-Liu-Edmond?s algorithm has a complexity
of O(n2) (McDonald et al, 2005). The most com-
mon tool for doing this is MST parser (McDonald et
al., 2005). For this parser we generate two models,
one projective and one non-projective to use in our
ensemble system.
Transition-based parsing creates a dependency
structure that is parameterized over the transitions.
This is closely related to shift-reduce constituency
parsing algorithms. The benefit of transition-based
parsing is the use greedy algorithms which have a
linear time complexity. However, due to the greedy
algorithms, longer arc parses can cause error propa-
gation across each transition (Ku?bler et al, 2009).
We make use of Malt Parser (Nivre et al, 2007),
which in the CoNLL shared tasks was often tied
with the best performing systems. For this parser
we generate 7 different models using different train-
ing parameters, seen in Table 1, and use them as
input into our ensemble system along with the two
Graph-based models described above. Each parser
has access to gold POS information as supplied by
the TamilTB described in 3.4.
Dependency parsing systems are often optimized
for English or other major languages. This opti-
mization, along with morphological complexities,
lead other languages toward lower accuracy scores
in many cases. The goal here is to show that
while the corpus is not the same in size or scope of
most CoNLL data, a successful dependency parser
can still be trained from the annotated data through
model combination for morphologically rich lan-
guages.
Training Parameter Model Description
nivreeager Nivre arc-eager
nivrestandard Nivre arc-standard
stackproj Stack projective
stackeager Stack eager
stacklazy Stack lazy
planar Planar eager
2planar 2-Planar eager
Table 1: Table of the Malt Parser Parameters used during
training. Each entry represents one of the parsing algo-
rithms used in our experiments. For more information see
http://www.maltparser.org/options.html
3.3 Ensemble SVM System
We train our SVM classifier using only model agree-
ment features. Using our tuning set, for each cor-
rectly predicted dependency edge, we create
(
N
2
)
features where N is the number of parsing models.
We do this for each model which predicted the cor-
rect edge in the tuning data. So for N = 3 the
first feature would be a 1 if model 1 and model 2
agreed, feature 2 would be a 1 if model 1 and model
3 agreed, and so on. This feature set is novel and
widely applicable to many languages since it does
not use any additional linguistic tools.
For each edge in the ensemble graph, we use our
classifier to predict which model should be correct,
by first creating the model agreement feature set
for the current edge of the unknown test data. The
SVM predicts which model should be correct and
this model then decides to which head the current
node is attached. At the end of all the tokens in a
sentence, the graph may not be connected and will
likely have cycles. Using a Perl implementation of
minimum spanning tree, in which each edge has a
uniform weight, we obtain a minimum spanning for-
est, where each subgraph is then connected and cy-
cles are eliminated in order to achieve a well formed
dependency structure. Figure 2 gives a graphical
representation of how the SVM decision and MST
algorithm create a final Ensemble parse tree which
is similar to the construction used in (Hall et al,
2007; Green and Z?abokrtsky?, 2012). Future itera-
tions of this process could use a multi-label SVM
or weighted edges based on the parser?s accuracy on
tuning data.
74
Figure 2: General flow to create an Ensemble parse tree
3.4 Data Sets
Table 2 shows the statistics of the TamilTB Tree-
bank. The last 2 rows indicate how many word types
have unique tags and how many have two tags. Also,
Table 2 illustrates that most of the word types can
be uniquely identified with single morphological tag
and only around 120 word types take more than one
morphological tag.
Description Value
#Sentences 600
#Words 9581
#Word types 3583
#Tagset size 234
#Types with unique tags 3461
#Types with 2 tags 112
Table 2: TamilTB: data statistics
Since this is a relatively small treebank and in or-
der to confirm that our experiments are not heavily
reliant on one particular sample of data we try a va-
riety of data splits. To test the effects of the train-
ing, tuning, and testing data we try 7 different data
splits. The tuning data in the Section 4 use the for-
mat training-tuning-testing. So 70-20-10 means we
used 70% of the TamilTB for training, 20% for tun-
ing the SVM classifier, and 10% for evaluation.
3.5 Evaluation
Made a standard in the CoNLL shared tasks com-
petition, two standard metrics for comparing depen-
dency parsing systems are typically used. Labeled
attachment score (LAS) and unlabeled attachment
score (UAS). UAS studies the structure of a depen-
dency tree and assesses whether the output has the
correct head and dependency arcs. In addition to the
structure score in UAS, LAS also measures the accu-
racy of the dependency labels on each arc (Buchholz
and Marsi, 2006). Since we are mainly concerned
with the structure of the ensemble parse, we report
only UAS scores in this paper.
To test statistical significance we use Wilcoxon
paired signed-rank test. For each data split we have
100 iterations each with different sampling. Each
model is compared against the same samples so a
paired test is appropriate in this case. We report sta-
tistical significance values for p < 0.01 and p <
0.05.
4 Results and Discussion
Data Average % Increase % Increase
Split SVM UAS over Avg over Best
70-20-10 76.50% 5.13% 0.52%
60-20-20 76.36% 5.68% 0.72%
60-30-10 75.42% 5.44% 0.52%
60-10-30 75.66% 4.83% 0.10%
85-5-10 75.33% 3.10% -1.21%
90-5-5 75.42% 3.19% -1.10%
80-10-10 76.44% 4.84% 0.48%
Table 3: Average increases and decreases in UAS score
for different Training-Tuning-Test samples. The average
was calculated over all 9 models while the best was se-
lected for each data split
For each of the data splits, Table 3 shows the per-
cent increase in our SVM system over both the av-
erage of the 9 individual models and over the best
individual model. As the Table 3 shows, our ap-
proach seems to decrease in value along with the de-
crease in tuning data. In both cases when we only
used 5% tuning data we did not get any improve-
ment in our average UAS scores. Examining Table
4, shows that the decrease in the 90-5-5 split is not
statistically significant however the decrease in 85-
5-10 is a statistically significant drop. However, the
increases in all data splits are statistically significant
except for the 60-20-20 data split. It appears that
75
Model 70-20-10 60-20-20 60-30-10 60-10-30 85-5-10 90-5-5 80-10-10
2planar * * * * * * **
mstnonproj * * * * * * **
mstproj * * * * * * **
nivreeager * * * * ** x *
nivrestandard * * ** x * * *
planar * * * * * * **
stackeager * * * x * ** *
stacklazy * * * x * ** *
stackproj ** * * x ** ** **
Table 4: Statistical Significance Table for different Training-Tuning-Test samples. Each experiment was sampled
100 times and Wilcoxon Statistical Significance was calculated for our SVM model?s increase/decrease over each
individual model. ? = p < 0.01 , ? ? p =< 0.05, x = p ? 0.05
the size of the tuning and training data matter more
than the size of the test data given the low variance
in Table 5. Since the TamilTB is relatively small
when compared to other CoNLL treebanks, we ex-
pect that this ratio may shift more when additional
data is supplied since the amount of out of vocab-
ulary, OOV, words will decrease as well. As OOV
words decrease, we expect the use of additional test
data to have less of an effect.
Data Splits SVM Variance
70-20-10 0.0011
60-20-20 0.0005
60-30-10 0.0010
60-10-30 0.0003
85-5-10 0.0010
90-5-5 0.0028
80-10-10 0.0010
Table 5: Variance of the UAS Scores of our Ensemble
SVM System over 100 data splits
The traditional approach of using as much data as
possible for training does not seem to be as effec-
tive as partitioning more data for tuning an SVM.
For instance the highest training percentage we use
is 90% applied to training with 5% for tuning and
testing each. In this case the best individual model
had a UAS of 76.25% and the SVM had a UAS of
75.42%. One might think using 90% of the data
would achieve a higher overall UAS than using less
training data. On the contrary, we achieve a better
UAS score on average using only 60%, 70%, 80%,
and 85% of the data towards training. This addi-
tional data spent for tuning appears to be worth the
cost.
5 Conclusion
We have shown a new SVM based ensemble parser
that uses only dependency model agreement fea-
tures. The ability to use only model agreements al-
lows us to keep this approach language independent
and applicable to a wide range of morphologically
rich languages. We show a statistically significant
5.44% improvement over the average dependency
model and a statistically significant 0.52% improve-
ment over the best individual system.
In the future we would like to examine how our
data splits? results change as more data is added.
This might be a prime use for self training. Since
the tuning data size for the SVM seems most impor-
tant, the UAS may be improved by only adding self
training data to our tuning sets. This would have the
additional benefit of eliminating the need to retrain
the individual parsers, thus saving computation time.
The tuning size may have a reduced effect for larger
treebanks but in our experiments it is critical to the
smaller treebank. Additionally, a full comparison of
various ensemble parsing error distributions will be
needed.
6 Acknowledgments
This research has received funding from the Euro-
pean Commission?s 7th Framework Program (FP7)
under grant agreement n? 238405 (CLARA)
76
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning, CoNLL-X ?06,
pages 149?164, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Thomas G. Dietterich. 2000. Ensemble methods in ma-
chine learning. In Proceedings of the First Interna-
tional Workshop on Multiple Classifier Systems, MCS
?00, pages 1?15, London, UK. Springer-Verlag.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics (COLING-96), pages 340?345,
Copenhagen, August.
Nathan Green and Zdene?k Z?abokrtsky?. 2012. Hybrid
Combination of Constituency and Dependency Trees
into an Ensemble Dependency Parser. In Proceedings
of the Workshop on Innovative Hybrid Approaches to
the Processing of Textual Data, pages 19?26, Avignon,
France, April. Association for Computational Linguis-
tics.
Gholamreza Haffari, Marzieh Razavi, and Anoop Sarkar.
2011. An ensemble model that combines syntactic
and semantic clustering for discriminative dependency
parsing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 710?714, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryigit,
Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single Malt or Blended? A Study in Mul-
tilingual Parser Optimization. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 933?939.
Samar Husain, Prashanth Mannem, Bharat Ram Ambati,
and Phani Gadde. 2010. The icon-2010 tools contest
on indian language dependency parsing. In Proceed-
ings of ICON-2010 Tools Contest on Indian Language
Dependency Parsing, pages 1?8.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Synthesis lectures on hu-
man language technologies. Morgan & Claypool, US.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 157?166, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, June. Association for Computational
Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Loganathan Ramasamy and Zdene?k Z?abokrtsky?. 2011.
Tamil dependency parsing: results using rule based
and corpus based approaches. In Proceedings of the
12th international conference on Computational lin-
guistics and intelligent text processing - Volume Part I,
CICLing?11, pages 82?95, Berlin, Heidelberg.
Loganathan Ramasamy and Zdene?k Z?abokrtsky?. 2012.
Prague dependency style treebank for Tamil. In Pro-
ceedings of LREC 2012, I?stanbul, Turkey.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, pages 129?132, New
York City, USA, June. Association for Computational
Linguistics.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044?1050,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Anders S?gaard and Christian Rish?j. 2010. Semi-
supervised dependency parsing using generalized tri-
training. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010), pages 1065?1073, Beijing, China, August.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble models for dependency parsing: cheap and
good? In HLT: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 649?652,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Daniel Zeman and Zdene?k Z?abokrtsky?. 2005. Improving
parsing accuracy by combining diverse dependency
parsers. In In: Proceedings of the 9th International
Workshop on Parsing Technologies.
77
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 19?24,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Improvements to Syntax-based Machine Translation using Ensemble
Dependency Parsers
Nathan David Green and Zdene?k Z?abokrtsky?
Charles University in Prague
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics
Prague, Czech Republic
{green,zabokrtsky}@ufal.mff.cuni.cz
Abstract
Dependency parsers are almost ubiqui-
tously evaluated on their accuracy scores,
these scores say nothing of the complex-
ity and usefulness of the resulting struc-
tures. The structures may have more com-
plexity due to their coordination structure
or attachment rules. As dependency parses
are basic structures in which other systems
are built upon, it would seem more reason-
able to judge these parsers down the NLP
pipeline.
We show results from 7 individual parsers,
including dependency and constituent
parsers, and 3 ensemble parsing tech-
niques with their overall effect on a Ma-
chine Translation system, Treex, for En-
glish to Czech translation. We show that
parsers? UAS scores are more correlated
to the NIST evaluation metric than to the
BLEU Metric, however we see increases
in both metrics.
1 Introduction
Ensemble learning (Dietterich, 2000) has been
used for a variety of machine learning tasks and
recently has been applied to dependency parsing
in various ways and with different levels of suc-
cess. (Surdeanu and Manning, 2010; Haffari
et al, 2011) showed a successful combination of
parse trees through a linear combination of trees
with various weighting formulations. To keep
their tree constraint, they applied Eisner?s algo-
rithm for reparsing (Eisner, 1996).
Parser combination with dependency trees has
been examined in terms of accuracy (Sagae and
Lavie, 2006; Sagae and Tsujii, 2007; Zeman
and Z?abokrtsky?, 2005; Holan and Z?abokrtsky?,
2006). Other methods of parser combinations
have shown to be successful such as using one
parser to generate features for another parser. This
was shown in (Nivre and McDonald, 2008), in
which Malt Parser was used as a feature to MST
Parser. The result was a successful combination of
a transition-based and graph-based parser, but did
not address adding other types of parsers into the
framework.
We will use three ensemble approaches. First a
fixed weight ensemble approach in which edges
are added together in a weighted graph. Sec-
ond, we added the edges using weights learned
through fuzzy clustering based on POS errors.
Third, we will use a meta-classifier that uses an
SVM to predict the correct model for edge using
only model agreements without any linguistic in-
formation added. Parsing accuracy and machine
translation has been examined in terms of BLEU
score (Quirk and Corston-Oliver, 2006). How-
ever, we believe our work is the first to examine
the NLP pipeline for ensemble parsing for both de-
pendency and constituent parsers as well as exam-
ining both BLEU and NIST scores? relationship to
their Unlabeled Accuracy Score(UAS).
2 Methodology
2.1 Annotation
To find the maximum effect that dependency pars-
ing can have on the NLP pipeline, we annotated
English dependency trees to form a gold standard.
Annotation was done with two annotators using
a tree editor, Tred (Pajas and Fabian, 2011), on
data that was preprocessed using MST parser. For
the annotation of our gold data, we used the stan-
dard developed by the Prague Dependency Tree-
bank (PDT) (Hajic?, 1998). PDT is annotated on
three levels, morphological, analytical, and tec-
togrammatical. For our gold data we do not touch
the morphological layer, we only correct the ana-
lytical layer (i.e. labeled dependency trees). For
machine translation experiments later in the paper
19
we allow the system to automatically generate a
new tectogrammatical layer based on our new an-
alytical layer annotation. Because the Treex ma-
chine translation system uses a tectogrammatical
layer, when in doubt, ambiguity was left to the tec-
togrammatical (t-layer in Figure 1) to handle.
2.1.1 Data Sets
For the annotation experiments we use data pro-
vided by the 2012 Workshop for Machine Trans-
lation (WMT2012). The data which consists
of 3,003 sentences was automatically tokenized,
tagged, and parsed. This data set was also chosen
since it is disjoint from the usual dependency train-
ing data, allowing researchers to use it as a out-of-
domain testing set. The parser used was an imple-
mentation of MST parser. We then hand corrected
the analytical trees to have a ?Gold? standard de-
pendency structure. Analytical trees were anno-
tated on the PDT standard. Most changes involved
coordination construction along with prepositional
phrase attachment. We plan to publicly release this
data and corresponding annotations in the near fu-
ture1.
Having only two annotators has limited us
to evaluating our annotation only through spot
checking and through comparison with other base-
lines. Annotation happened sequentially one after
another. Possible errors were additionally detected
through automatic means. As a comparison we
will evaluate our gold data set versus other parsers
in respect to their performance on previous data
sets, namely the Wall Street Journal (WSJ) section
23.
2.2 Translation
2.2.1 Data Sets
All the parsers were trained on sections 02-21 of
the WSJ, except the Stanford parser which also
uses section 01. We retrained MST and Malt
parsers and used pre-trained models for the other
parsers. Machine translation data was used from
WMT 2010, 2011, and 2012. Using our gold
standard we are able to evaluate the effective-
ness of different parser types from graph-base,
transition-based, constituent conversion to ensem-
ble approaches on the 2012 data while finding data
trends using previous years data.
1When available the data and description will be at
www.nathangreen.com/wmtdata
2.2.2 Translation Components
To examine the effects of dependency parsing
down the NLP pipeline, we now turn to syntax
based machine translation. Our dependency mod-
els will be evaluated using the Treex translation
system (Popel and Z?abokrtsky?, 2010). This sys-
tem, as opposed to other popular machine transla-
tion systems, makes direct use of the dependency
structure during the conversion from source to tar-
get languages via a tectogrammatical tree transla-
tion approach.
Figure 1: Treex syntax-based translation scenario
(Popel and Z?abokrtsky?, 2010)
We use the different parsers in separate trans-
lation runs each time in the same Treex parsing
block. So each translation scenario only differs in
the parser used and nothing else. As can be seen
in Figure 1, we are directly manipulating the An-
alytical portion of Treex. The parsers used are as
follows:
? MST: Implementation of Ryan McDonald?s
Minimum spanning tree parser (McDonald et
al., 2005)
? MST with chunking: Same implementation
as above but we parse the sentences based on
chunks and not full sentences. For instance
this could mean separating parentheticals or
separating appositions (Popel et al, 2011)
? Malt: Implementation of Nivre?s Malt Parser
trained on the Penn Treebank (Nivre, 2003)
? Malt with chunking: Same implementation
as above but with chunked parsing
? ZPar: Yue Zhang?s statistical parser. We
used the pretrained English model (en-
glish.tar.gz) available on the ZPar website for
all tests (Zhang and Clark, 2011)
? Charniak: A constituent based parser
(ec50spfinal model) in which we transform
20
the results using the Pennconverter (Johans-
son and Nugues, 2007)
? Stanford: Another constituent based
parser (Klein and Manning, 2003) whose
output is converted using Pennconverter as
well (wsjPCFG.ser.gz model)
? Fixed Weight Ensemble: A stacked en-
semble system combining five of the parsers
above (MST, Malt, ZPar, Charniak, Stan-
ford). The weights for each tree are as-
signed based on UAS score found in tun-
ing data, section 22 of the WSJ (Green and
Z?abokrtsky?, 2012)
? Fuzzy Cluster: A stacked ensemble system
as well but weights are determined by a clus-
ter analysis of POS errors found in the same
tuning data as above (Green and Z?abokrtsky?,
2012)
? SVM: An ensemble system in which each in-
dividual edge is picked by a meta classifier
from the same 5 parsers as the other ensemble
systems. The SVM meta classifier is trained
on results from the above tuning data (Green
et al, 2012a; Green et al, 2012b).
2.2.3 Evaluation
For Machine Translation we report two automatic
evaluation scores, BLEU and NIST. We examine
parser accuracy using UAS. This paper compares
a machine translation system integrating 10 differ-
ent parsing systems against each other, using the
below metrics.
The BLEU (BiLingual Evaluation Understudy)
and NIST(from the National Institute of Standards
and Technology), are automatic scoring mecha-
nisms for machine translation that are quick and
can be reused as benchmarks across machine
translation tasks. BLEU and NIST are calculated
as the geometric mean of n-grams multiplied by a
brevity penalty, comparing a machine translation
and a reference text (Papineni et al, 2002). NIST
is based upon the BLEU n-gram approach how-
ever it is also weighted towards discovering more
?informative? n-grams. The more rare an n-gram
is, the higher the weight for a correct translation of
it will be.
Made a standard in the CoNLL shared tasks
competition, UAS studies the structure of a depen-
dency tree and assesses how often the output has
the correct head and dependency arcs (Buchholz
and Marsi, 2006). We report UAS scores for each
parser on section 23 of the WSJ.
3 Results and Discussion
3.1 Type of Changes in WMT Annotation
Since our gold annotated data was preprocessed
with MST parser, our baseline system at the time,
we started with a decent baseline and only had
to change 9% of the dependency arcs in the data.
These 9% of changes roughly increase the BLEU
score by 7%.
3.2 Parser Accuracy
As seen in previous Ensemble papers (Farkas and
Bohnet, 2012; Green et al, 2012a; Green et al,
2012b; Green and Z?abokrtsky?, 2012; Zeman and
Z?abokrtsky?, 2005), parsing accuracy can be im-
proved by combining parsers? outputs for a variety
of languages. We apply a few of these systems, as
described in Section 2.2.2, to English using mod-
els trained for both dependencies and constituents.
3.2.1 Parsers vs our Gold Standard
On average our gold data differed in head agree-
ment from our base parser 14.77% of the time.
When our base parsers were tested on the WSJ
section 23 data they had an average error rate of
12.17% which is roughly comparable to the differ-
ence with our gold data set which indicates overall
our annotations are close to the accepted standard
from the community. The slight difference in per-
centage fits into what is expect in annotator error
and in the errors in the conversion process of the
WSJ by Pennconverter.
3.3 Parsing Errors Effect on MT
3.3.1 MT Results in WMT with Ensemble
Parsers
WMT 2010
As seen in Table 1, the highest resulting BLEU
score for the 2010 data set is from the fixed weight
ensemble system. The other two ensemble sys-
tems are beaten by one component system, Char-
niak. However, this changes when comparing
NIST scores. Two of the ensemble method have
higher NIST scores than Charniak, similar to their
UAS scores.
WMT 2011
The 2011 data corresponded the best with UAS
scores. While the BLEU score increases for all
21
Parser UAS NIST(10/11/12) BLEU(10/11/12)
MST 86.49 5.40/5.58/5.19 12.99/13.58/11.54
MST w chunking 86.57 5.43/5.63/5.23 13.43/14.00/11.96
Malt 84.51 5.37/5.57/5.14 12.90/13.48/11.27
Malt w chunking 87.01 5.41/5.60/5.19 13.39/13.80/11.73
ZPar 76.06 5.26/5.46/5.08 11.91/12.48/10.53
Charniak 92.08 5.47/5.65/5.28 13.49/13.95/12.26
Stanford 87.88 5.40/5.59/5.18 13.23/13.63/11.74
Fixed Weight 92.58 5.49/5.68/5.29 13.53/14.04/12.23
Fuzzy Cluster 92.54 5.47/5.68/5.26 13.47/14.06/12.06
SVM 92.60 5.48/5.68/5.28 13.45/14.11/12.22
Table 1: Scores for each machine translation run for each dataset (WMT 2010, 2011 and 2012)
the ensemble systems, the order of systems by
UAS scores corresponds exactly to the systems or-
dered by NIST score and corelates strongly (Table
2). Unlike the 2010 data, the MST parser was the
highest base parser in terms of the BLEU metric.
WMT 2012
The ensemble increases are statistically significant
for both the SVM and the Fixed Weight system
over the MST with chunking parser with 99% con-
fidence, our previous baseline and best scoring
base system from 2011 in terms of BLEU score.
We examine our data versus MST with chunking
instead of Charniak since we have preprocessed
our gold data set with MST, allowing us a direct
comparison in improvements. The fuzzy cluster
system achieves a higher BLEU evaluation score
than MST, but is not significant. In pairwise tests
it wins approximately 78% of the time. This is the
first dataset we have looked at where the BLEU
score is higher for a component parser and not an
ensemble system, although the NIST score is still
higher for the ensemble systems.
NIST BLEU
2010 0.98 0.93
2011 0.98 0.94
2012 0.95 0.97
Table 2: Pearson correlation coefficients for each
year and each metric when measured against UAS.
Statistics are taken from the WMT results in Table
1. Overall NIST has the stronger correlation to
UAS scores, however both NIST and BLEU show
a strong relationship.
3.3.2 Human Manual Evaluation: SVM vs
the Baseline System
We selected 200 sentences at random from our an-
notations and they were given to 7 native Czech
speakers. 77 times the reviewers preferred the
SVM system, 48 times they preferred the MST
system, and 57 times they said there was no differ-
ence between the sentences. On average each re-
viewer looked at 26 sentences with a median of 30
sentences. Reviewers were allowed three options:
sentence 1 is better, sentence 2 is better, both sen-
tences are of equal quality. Sentences were dis-
played in a random order and the systems were
randomly shuffled for each question and for each
user.
+ = -
+ 12 12 0
= 3 7
- 7
Table 3: Agreement for sentences with 2 or more
annotators for our baseline and SVM systems. (-,-)
all annotators agreed the baseline was better, (+,+)
all annotators agreed the SVM system was better,
(+,-) the annotators disagreed with each other
Table 3 indicates that the SVM system was pre-
ferred. When removing annotations marked as
equal, we see that the SVM system was preferred
24 times to the Baseline?s 14.
Although a small sample, this shows that using
the ensemble parser will at worse give you equal
results and at best a much improved result.
22
3.3.3 MT Results with Gold Data
In the perfect situation of having gold standard de-
pendency trees, we obtained a NIST of 5.30 and
a BLEU of 12.39. For our gold standard system
run, the parsing component was removed and re-
placed with our hand annotated data. These are
the highest NIST and BLEU scores we have ob-
tained including using all base parsers or any com-
binations of parsers. This indicates that while an
old problem which is a ?solved? problem for some
languages, Parsing is still worth researching and
improving for its cascading effects down the NLP
pipeline.
4 Conclusion
We have shown that ensemble parsing techniques
have an influence on syntax-based machine trans-
lation both in manual and automatic evaluation.
Furthermore we have shown a stronger correlation
between parser accuracy and the NIST rather than
the more commonly used BLEU metric. We have
also introduced a gold set of English dependency
trees based on the WMT 2012 machine translation
task data, which shows a larger increase in both
BLEU and NIST. While on some datasets it is in-
conclusive whether using an ensemble parser with
better accuracy has a large enough effect, we do
show that practically you will not do worse using
one and in many cases do much better.
5 Acknowledgments
This research has received funding from the
European Commission?s 7th Framework Pro-
gram (FP7) under grant agreement n? 238405
(CLARA). Additionally, this work has been us-
ing language resources developed and/or stored
and/or distributed by the LINDAT-Clarin project
of the Ministry of Education of the Czech Repub-
lic (project LM2010013).
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning, CoNLL-X
?06, pages 149?164, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Thomas G. Dietterich. 2000. Ensemble Methods
in Machine Learning. In Proceedings of the First
International Workshop on Multiple Classifier Sys-
tems, MCS ?00, pages 1?15, London, UK. Springer-
Verlag.
Jason Eisner. 1996. Three New Probabilistic Mod-
els for Dependency Parsing: An Exploration. In
Proceedings of the 16th International Conference
on Computational Linguistics (COLING-96), pages
340?345, Copenhagen, August.
Richa?rd Farkas and Bernd Bohnet. 2012. Stacking of
Dependency and Phrase Structure Parsers. In Pro-
ceedings of COLING 2012, pages 849?866, Mum-
bai, India, December. The COLING 2012 Organiz-
ing Committee.
Nathan Green and Zdene?k Z?abokrtsky?. 2012. Hy-
brid Combination of Constituency and Dependency
Trees into an Ensemble Dependency Parser. In Pro-
ceedings of the EACL 2012 Workshop on Innovative
hybrid approaches to the processing of textual data,
Avignon, France.
Nathan Green and Zdene?k Z?abokrtsky?. 2012. Ensem-
ble Parsing and its Effect on Machine Translation.
Technical Report 48.
Nathan Green, Septina Dian Larasati, and Zdene?k
Z?abokrtsky?. 2012a. Indonesian Dependency
Treebank: Annotation and Parsing. In Proceed-
ings of the 26th Pacific Asia Conference on Lan-
guage, Information, and Computation, pages 137?
145, Bali,Indonesia, November. Faculty of Com-
puter Science, Universitas Indonesia.
Nathan Green, Loganathan Ramasamy, and Zdene?k
Z?abokrtsky?. 2012b. Using an SVM Ensemble Sys-
tem for Improved Tamil Dependency Parsing. In
Proceedings of the ACL 2012 Joint Workshop on
Statistical Parsing and Semantic Processing of Mor-
phologically Rich Languages, pages 72?77, Jeju,
Republic of Korea, July 12. Association for Com-
putational Linguistics.
Gholamreza Haffari, Marzieh Razavi, and Anoop
Sarkar. 2011. An Ensemble Model that Combines
Syntactic and Semantic Clustering for Discrimina-
tive Dependency Parsing. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 710?714, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Jan Hajic?. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In Eva
Hajic?ova?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevova?, pages 12?19.
Prague Karolinum, Charles University Press.
Toma?s? Holan and Zdene?k Z?abokrtsky?. 2006. Com-
bining Czech Dependency Parsers. In Proceedings
of the 9th international conference on Text, Speech
and Dialogue, TSD?06, pages 95?102, Berlin, Hei-
delberg. Springer-Verlag.
23
Richard Johansson and Pierre Nugues. 2007. Ex-
tended Constituent-to-dependency Conversion for
English. In Proceedings of NODALIDA 2007, pages
105?112, Tartu, Estonia, May 25-26.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, HLT ?05, pages 523?530, Morristown, NJ,
USA. Association for Computational Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing Graph-Based and Transition-Based Dependency
Parsers. In Proceedings of ACL-08: HLT, pages
950?958, Columbus, Ohio, June. Association for
Computational Linguistics.
Joakim Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT, pages 149?160.
Petr Pajas and Peter Fabian. 2011. TrEd 2.0 - newly
refactored tree editor. http://ufal.mff.cuni.cz/tred/,
Institute of Formal and Applied Linguistics, MFF
UK.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, ACL ?02, pages 311?
318, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. Tec-
toMT: modular NLP framework. In Proceedings of
the 7th international conference on Advances in nat-
ural language processing, IceTAL?10, pages 293?
304, Berlin, Heidelberg. Springer-Verlag.
Martin Popel, David Marec?ek, Nathan Green, and
Zdenek Zabokrtsky. 2011. Influence of parser
choice on dependency-based mt. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 433?439, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed sta-
tistical machine translation. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?06, pages 62?69,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Kenji Sagae and Alon Lavie. 2006. Parser Combina-
tion by Reparsing. In Proceedings of the Human
Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 129?132,
New York City, USA, June. Association for Compu-
tational Linguistics.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
Parsing and Domain Adaptation with LR Models
and Parser Ensembles. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
1044?1050, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 649?652, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Daniel Zeman and Zdene?k Z?abokrtsky?. 2005. Improv-
ing Parsing Accuracy by Combining Diverse Depen-
dency Parsers. In In: Proceedings of the 9th Inter-
national Workshop on Parsing Technologies.
Yue Zhang and Stephen Clark. 2011. Syntactic Pro-
cessing Using the Generalized Perceptron and Beam
Search. Computational Linguistics, 37(1):105?151.
24
