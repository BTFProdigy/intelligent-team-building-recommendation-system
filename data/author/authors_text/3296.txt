The use of  error tags in ARTFL 's  Encyclopgdie: 
Does  good error ident i f i cat ion  lead to good  error cor rect ion?  
Derr ick  H igg ins  
Department ofLinguistics 
University of Chicago 
Abst ract  
Many corpora which are prime candidates 
for automatic error correction, such as the 
output of OCR software, and electronic texts 
incorporating markup tags, include informa- 
tion on which portions of the text are most 
likely to contain errors. 
This paper describes how the error 
markup tag <?> is being incorporated in 
the spell-checking of an electronic version 
of Diderot's Encyclopddie, and evaluates 
whether the presence of this tag has signif- 
icantly aided in correcting the errors which 
it marks. Although the usefulness of error 
tagging may vary from project to project, 
even as the precise way in which the tagging 
is done varies, error tagging does not nec- 
essarily confer any benefit in attempting to 
correct a given word. It may, of course, nev- 
ertheless be useful in marking errors to be 
fixed manually at a later stage of processing 
the text. 
1 The Encyclopddie 
1.1 P ro jec t  Overv iew 
The goal of this project is ultimately to 
detect and correct all errors in the elec- 
tronic version of the 18th century French 
encyclopedia of Diderot and d'Alembert, a
corpus of ca. 18 million words. This text is 
currently under development by the Project 
for American and French Research on the 
Treasury of the French Language (ARTFL); 
a project overview and limited sample of 
searchable text from the Encyclopddie are 
available at: 
http ://humanities. uchicago, edu/ARTFL/pro j ect s/encyc/. 
Andreev et al (1999) also provides a 
thorough summary of the goals and status 
of the project. 
The electronic text was largely transcribed 
from the original, although parts of it were 
produced by optical character recognition on 
scanned images. Unfortunately, whether a 
section of text was transcribed or produced 
by OCR was not recorded at the time of data 
capture, so that the error correction strategy 
cannot be made sensitive to this parameter. 
Judging from a small hand-checked section 
of the text, the error rate is fairly low; about 
one word in 40 contains an error. It should 
also be added that the version of the text 
with which I am working has already been 
subjected to some corrective measures after 
the initial data capture stage. For exam- 
ple, common and easily identifiable mistakes 
such as the word enfant showing up as en- 
sant were simply globally repaired through- 
out the text. (The original edition of the En- 
cyclop~die made use of the sharp 's', which 
was often confused with an 'f' during data 
entry--cf. Figure 1.) 
At present, my focus is on non-word er- 
ror detection and correction, although use of 
word n-grams eems to be a fairly straight- 
forward extension to allow for the kind of 
context-sensitivity in error correction which 
has been the focus of much recent work 
(cf. Golding and Roth (1999), Mays et al 
(1991), Yarowsky (1995)). 
The approach I am pursuing is an appli- 
cation of Bayesian statistics. We treat the 
process by which the electronic text was pro- 
duced as a noisy channel, and take as our 
goal the maximization of the probability of 
each input word, given a string which is the 
30 
Figure 1: Example text from the Encyclopddie. Note the similarity between the 'f' and the 
problematic sharp 's' in signifie 
;' gn e / 
ABSI~N'f ,adj .  cn Droit,  nifie n-g I qui- 
conq,== eft:.61oign~ de fon domicile, 
~B~r.t~r, Cn mat2er? de.p.ref'cri#tiO?l, fe  dit de cehd 
qttl eft: ?ta,~ une autre t,roi, it~ee qile cclle off eft: le 
pt,ff~fl'e.ur d= ftJn, lJt:rlrage. F .  P/~,r.scrt,t, rlo/,/o' Pru~- 
sr/?r. Les al,feat qu i  le font pour l'mt6r~t de l'?:a~ ? 
fon~ rt~ptl ~s pr~fe ns, quot ies de commodis ?orum ag~ tur. 
output of the noisy channel. If we repre- 
sent the correct form by we, and the ob- 
served form by wo, our goal can be described 
as finding the wc which maximizes p(wclwo), 
the probability that wc is the intended form, 
given that wo is the observed form. 
By Bayes' rule, this can be reduced to 
the problem of maximizing p(wolwc)p(wc) Of pCwo) 
course, the probability of the observed string 
will be constant across all candidate correc- 
tions, so the same w~ will also maximize 
p(wolwc)p(w~). 
The term p(w~) (the prior probability) can 
be estimated by doing frequency counts on a 
corpus. In this case, I am using an interpo- 
lated model of Good-Turing smoothed word 
and letter probabilities as the prior. 
The term p(WolW~) is called the error 
model. Intuitively, it quantifies the prob- 
ability of certain kinds of errors resulting 
from the noisy channel. It is implemented 
as a confusion matrix, which associates a
probability with each input/output charac- 
ter pair, representing the probability of the 
input character being replaced by the out- 
put character. These probabilities can be 
estimated from a large corpus tagged for er- 
rors, but since I do not have access to such 
a source for this project, I needed to train 
the matrix as described in Kernighan et al 
(1990). 
Cf. Jurafsky and Martin (1999) for an in- 
troduction to spelling correction using con- 
fusion matrices, and Kukich (1992) for a sur- 
vey of different strategies in spelling correc- 
tion. 
1.2 T reatment  of <?> 
A number of different SGML-style tags are 
currently used in the encoding of the En- 
cyclopddie, ranging from form-related tags 
such as <i> (for italic text), to semantic 
markup tags such as <art ic le>,  to the error 
tag <?>, the treatment of which is the focus 
of this article. The data entry specification 
for the project prescribes the use of <?> in 
all cases in which the keyboard operator has 
any doubt as to the identity of a printed 
character, and also when symbols appear 
in the text which cannot be represented in 
the Latin-1 codepage (except for Greek text, 
which is handled by other means). Other 
instances of the <?> tag were produced as 
indications of mistakes in OCR output. 
Some examples of the use of the error tag 
from the actual corpus include the following: 
<?> 
<?>darts 
J '<?>i  
ab<?>ci<?><?>es 
d 'aut re<?>alad ies  
for a Hebrew R 
for dans 
for J'ai 
for abscisses 
for d'autres maladies 
The first is a case where <?> was used to 
mark an untypeable character. In the sec- 
31 
ond case, it was somehow inserted superflu- 
ously (most likely by OCR). In the third row, 
<?> stands in for a single missing character, 
and in the fourth it does the same, but three 
times in a single word. Finally, in the last 
row the error tag indicates the omission of 
multiple characters, and even a word bound- 
ary. 
In fact, as Table 1 shows, words which in- 
clude the error tag generally have error types 
which are more difficult to correct than av- 
erage. Our confusion matrix-based approach 
is best at handling substitutions (e.g., onfin 
enfin), deletions (apeUent --~ appellent), 
and insertions (asselain ~ asselin), and can- 
not correct words with multiple errors at all. 1 
"Unrecoverable" rrors are those in which no 
"correction" is possible, for example, when 
non-ASCII symbols occur in the original. 
The fact that the error tag is used to code 
such a wide variety of irregularities in the 
corpus makes it difficult to incorporate into 
our general error correction strategy. Since 
<?> so often occurred as a replacement for a 
single, missing character, however, I treated 
it as a character in the language model, but 
one with an extremely low probability, so 
that any suggested correction would have to 
get rid of it in order to appreciably increase 
the probability of the word. 
In short, <?> is included in the confusion 
matrix as a character which may occur as the 
result of interference from the noisy chan- 
nel, but is highly unlikely to appear inde- 
pendently in the language. This approach 
ignores the many cases of multiple errors in- 
dicated by the error tag, but these probably 
pose too difficult a problem for this stage of 
the project anyhow. The funding available 
for the project does not currently allow us to 
pursue the possibility of computer-aided er- 
ror correction; rather, the program must cor- 
rect as many errors as it can without human 
intervention. Toward this end, we are will- 
ing to sacrifice the ability to cope with more 
1 Actually, it does have a mechanism for dealing 
with cases such as ab<?>ci<?><?>es, in which the 
error tag occurs multiple times, but stands for a sin- 
gle letter in each case. 
esoteric error types in order to improve the 
reliability of the system on other error types. 
The actual performance of the spelling 
correction algorithm on words which include 
the error tag, while comparable to the per- 
formance on other words, is perhaps not as 
high as we might initially have hoped, given 
that they were already tagged as errors. Of 
the corrections uggested for words without 
<?>, 47% were accurate, while of the cor- 
rections suggested for words with <?>, 29% 
were accurate. 2 Actually, if we include cases 
in which the program correctly identified an 
error as "unrecoverable", and opted to make 
no change, the percentage for <?> sugges- 
tions rises to 71%. 
It may seem that these numbers in fact 
undermine the thesis that  the error tagging 
in the Encyclopddie was not useful in error 
correction. I.e., if the correction algorithm 
exhibits the correct behavior on 47% of un- 
tagged errors, and on 71% of tagged errors, 
it seems that  the tags are helping out some- 
what. Actually, this is not the case. First, 
we should not give the same weight to cor- 
rect behavior on unrecoverable errors (which 
means giving up on correction) and correct 
behavior on other errors (which means actu- 
ally finding the correct form). Second, the 
tagged errors are often simply 'worse' than 
untagged errors, so that  even if the OCR or 
keyboard operator had made a guess at the 
correct form, they would have easily been 
identifiable as errors, and even errors of a 
certain type. For example, I maintain that 
the form ab<?>ci<?><?>es would have been 
no more difficult to correct had it occurred 
i ns tead  as ab fc i f fes .  
2 Conc lus ion  
In sum, the errors which are marked with 
the <?> tag in the electronic version of the 
2I admit that these numbers may seem low, but 
bear in mind that the percentage r flects the accu- 
racy of the first guess made by the system, since its 
operation is required to be entirely automatic. Fur- 
thermore, the correction task is made more difficult 
by the fact that the corpus is an encyclopedia, which 
contains more infrequent words and proper names 
than most corpora. 
32 
Substitution Deletion 
37.4% 0% 
Insertion Word- 
breaking 
2.2% 0% 
Multiple 
16.5% Contains <?> 
Does not 
contain <?> 58.5% 11.6% 6.8% 12.9% 10.2% 0% 
Unrecoverable 
44% 
Table 1: Breakdown of error types, according to whether the word contains <?> 
Encyclopddie ncompass so many distinct er- 
ror types, and errors of such difficulty, that 
it is hard to come up with corrections for 
many of them without human intervention. 
For this reason, experience with the Ency- 
clopddie project suggests that error tagging 
is not necessarily a great aid in performing 
automatic error correction. 
There is certainly a great deal of room for 
further investigation into the use of meta- 
data in spelling correction in general, how- 
ever. While the error tag is a somewhat 
unique member of the tagset, in that it typ- 
ically flags a subpart of a word, rather than 
a string of words, this should not be taken 
to mean that it is the only tag which could 
be employed in spelling correction. If noth- 
ing else, "wider-scope" markup tags can be 
helpful in determining when certain parts of 
the corpus should not be seen as represen- 
tative of the language model, or should be 
seen as representative of a distinct language 
model. (For example, the italic tag <?>. of- 
ten marks Latin text in the Encyclopddie.) 
Ultimately, I believe that what is needed 
in order for text tagging to be useful in er- 
ror correction is a recognition that the tagset 
will influence the correction process. Tags 
which are applied in such a way as to de- 
limit sections of text which are relevant o 
correction (such as names, equations, and 
foreign language text), will be of greater use 
than tags which represent a mixture of such 
classes. Error tagging in particular should 
be most useful if it does not conflate quite 
distinct things that may be "wrong" with 
a text, such as illegibility of the original, 
unrenderable symbols, and OCR inaccura- 
cies. Such considerations are certainly rele- 
vant in the evaluation of emerging text en- 
coding standards, such as the specification 
of the Text Encoding Initiative. 
Re ferences  
Leonid Andreev, Jack Iverson, and Mark 
Olsen. 1999. Re-engineering a war- 
machine: ARTFL 's Encyclopddie. Liter- 
ary and Linguistic Computing, 14(1):11- 
28. 
Denis Diderot and Jean Le Rond 
d'Alembert, editors. 1976 \[1751-1765\]. 
Encyclopddie, ou Dictionnaire raisonnd 
des sciences, des arts et des mdtiers. Re- 
search Publications, New Haven, Conn. 
Microfilm. 
Andrew R. Golding and Dan Roth. 1999. 
A winnow-based approach to context- 
sensitive spelling correction. Machine 
Learning, 34(1):107-130. 
Daniel Jurafsky and James Martin. 1999. 
Speech and Language Processing: An In- 
troduction to Speech Recognition, Natural 
Language Processing and Computational 
Linguistics. Prentice Hall. 
M. D. Kernighan, K. W. Church, and W. A. 
Gale. 1990. A spelling correction program 
based on a noisy channel model. In Pro- 
ceedings of the 13th International Confer- 
ence on Computational Linguistics (COL- 
ING '90), volume 2, pages 205-211. 
Karen Kukich. 1992. Techniques for auto- 
matically correcting words in text. A CM 
Computing Surveys, 24(4):377-439. 
Eric Mays, Fred J. Damerau, and Robert L. 
Mercer. 1991. Context based spelling cor- 
rection. Information Processing ~ Man- 
agement, 27(5):517-522. 
David Yarowsky. 1995. Unsupervised word 
sense disambiguation rivaling supervised 
methods. In Proceedings of the 33rd An- 
33 
nual Meeting of the Association for Com- 
putational Linguistics, volume 33, pages 
189-196. 
34 
c? 2003 Association for Computational Linguistics
A Machine Learning Approach to
Modeling Scope Preferences
Derrick Higgins? Jerrold M. Sadock?
University of Chicago University of Chicago
This article describes a corpus-based investigation of quantifier scope preferences. Following
recent work on multimodular grammar frameworks in theoretical linguistics and a long history
of combining multiple information sources in natural language processing, scope is treated as a
distinct module of grammar from syntax. This module incorporates multiple sources of evidence
regarding the most likely scope reading for a sentence and is entirely data-driven. The experiments
discussed in this article evaluate the performance of our models in predicting the most likely scope
reading for a particular sentence, using Penn Treebank data both with and without syntactic
annotation. We wish to focus attention on the issue of determining scope preferences, which has
largely been ignored in theoretical linguistics, and to explore different models of the interaction
between syntax and quantifier scope.
1. Overview
This article addresses the issue of determining the most accessible quantifier scope
reading for a sentence. Quantifiers are elements of natural and logical languages (such
as each, no, and some in English and ? and ? in predicate calculus) that have certain
semantic properties. Loosely speaking, they express that a proposition holds for some
proportion of a set of individuals. One peculiarity of these expressions is that there
can be semantic differences that depend on the order in which the quantifiers are
interpreted. These are known as scope differences.
(1) Everyone likes two songs on this album.
As an example of the sort of interpretive differences we are talking about, consider
the sentence in (1). There are two readings of this sentence; which reading is meant
depends on which of the two quantified expressions everyone and two songs on this
album takes wide scope. The first reading, in which everyone takes wide scope, simply
implies that every person has a certain preference, not necessarily related to anyone
else?s. This reading can be paraphrased as ?Pick any person, and that person will like
two songs on this album.? The second reading, in which everyone takes narrow scope,
implies that there are two specific songs on the album of which everyone is fond, say,
?Blue Moon? and ?My Way.?
In theoretical linguistics, attention has been primarily focused on the issue of scope
generation. Researchers applying the techniques of quantifier raising and Cooper stor-
age have been concerned mainly with enumerating all of the scope readings for a
? Department of Linguistics, University of Chicago, 1010 East 59th Street, Chicago, IL 60637. E-mail:
dchiggin@alumni.uchicago.edu.
? Department of Linguistics, University of Chicago, 1010 East 59th Street, Chicago, IL 60637. E-mail:
j-sadock@uchicago.edu.
74
Computational Linguistics Volume 29, Number 1
sentence that are possible, without regard to their relative likelihood or naturalness.
Recently, however, linguists such as Kuno, Takami, and Wu (1999) have begun to turn
their attention to scope prediction, or determining the relative accessibility of different
scope readings.
In computational linguistics, more attention has been paid to the factors that de-
termine scope preferences. Systems such as the SRI Core Language Engine (Moran
1988; Moran and Pereira 1992), LUNAR (Woods 1986), and TEAM (Martin, Appelt,
and Pereira 1986) have employed scope critics that use heuristics to decide between
alternative scopings. However, the rules that these systems use in making quantifier
scope decisions are motivated only by the researchers? intuitions, and no empirical
results have been published regarding their accuracy.
In this article, we use the tools of machine learning to construct a data-driven
model of quantifier scope preferences. For theoretical linguistics, this model serves as
an illustration that Kuno, Takami, and Wu?s approach can capture some of the clear-
est generalizations about quantifier scoping. For computational linguistics, this article
provides a baseline result on the task of scope prediction, with which other scope
critics can be compared. In addition, it is the most extensive empirical investigation
of which we are aware that collects data of any kind regarding the relative frequency
of different quantifier scope readings in English text.1
Section 2 briefly discusses treatments of scoping issues in theoretical linguistics,
and Section 3 reviews the computational work that has been done on natural language
quantifier scope. In Section 4 we introduce the models that we use to predict quantifier
scoping, as well as the data on which they are trained and tested. Section 5 combines
the scope model of the previous section with a probabilistic context-free grammar
(PCFG) model of syntax and addresses the issue of whether these two modules of
grammar ought to be combined in serial, with information from the syntax feeding the
quantifier scope module, or in parallel, with each module constraining the structures
provided by the other.
2. Approaches to Quantifier Scope in Theoretical Linguistics
Most, if not all, linguistic treatments of quantifier scope have closely integrated it with
the way in which the syntactic structure of a sentence is built up. Montague (1973) used
a syntactic rule to introduce a quantified expression into a derivation at the point where
it was to take scope, whereas generative semantic analyses such as McCawley (1998)
represented the scope of quantification at deep structure, transformationally lowering
quantifiers into their surface positions during the course of the derivation. More recent
work in the interpretive paradigm takes the opposite approach, extracting quantifiers
from their surface positions to their scope positions by means of a quantifier-raising
(QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular
technique is to percolate scope information up through the syntactic tree using Cooper
storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995;
Pollard and Yoo 1998).
The QR approach to dealing with scope in linguistics consists in the claim that
there is a covert transformation applying to syntactic structures that moves quantified
elements out of the position in which they are found on the surface and raises them to
a higher position that reflects their scope. The various incarnations of the strategy that
1 See Carden (1976), however, for a questionnaire-based approach to gathering data on the accessibility
of different quantifier scope readings.
75
Higgins and Sadock Modeling Scope Preferences
Figure 1
Simple illustration of the QR approach to quantifier scope generation.
follows from this claim differ in the precise characterization of this QR transforma-
tion, what conditions are placed upon it, and what tree-configurational relationship
is required for one operator to take scope over another. The general idea of QR is
represented in Figure 1, a schematic analysis of the reading of the sentence Someone
saw everyone in which someone takes wide scope (i.e., ?there is some person x such that
for all persons y, x saw y?).
In the Cooper storage approach, quantifiers are gathered into a store and passed
upward through a syntactic tree. At certain nodes along the way, quantifiers may be
retrieved from the store and take scope. The relative scope of quantifiers is determined
by where each quantifier is retrieved from the store, with quantifiers higher in the tree
taking wide scope over lower ones. As with QR, different authors implement this
scheme in slightly different ways, but the simplest case is represented in Figure 2, the
Cooper storage analog of Figure 1.
These structural approaches, QR and Cooper storage, have in common that they
allow syntactic factors to have an effect only on the scope readings that are available for
a given sentence. They are also similar in addressing only the issue of scope generation,
or identifying all and only the accessible readings for each sentence. That is to say,
they do not address the issue of the relative salience of these readings.
Kuno, Takami, and Wu (1999, 2001) propose to model the scope of quantified
elements with a set of interacting expert systems that basically consists of a weighted
vote taken of the various factors that may influence scope readings. This model is
meant to account not only for scope generation, but also for ?the relative strengths of
the potential scope interpretations of a given sentence? (1999, page 63). They illustrate
the plausibility of this approach in their paper by presenting a number of examples
that are accounted for fairly well by the approach even when an unweighted vote of
the factors is allowed to be taken.
So, for example, in Kuno, Takami and Wu?s (49b) (1999), repeated here as (2), the
correct prediction is made: that the sentence is unambiguous with the first quantified
noun phrase (NP) taking wide scope over the second (the reading in which we don?t
all have to hate the same people). Table 1 illustrates how the votes of each of Kuno,
Takami, and Wu?s ?experts? contribute to this outcome. Since the expression many of
us/you receives more votes, and the numbers for the two competing quantified expres-
sions are quite far apart, the first one is predicted to take wide scope unambiguously.
(2) Many of us/you hate some of them.
76
Computational Linguistics Volume 29, Number 1
Figure 2
Simple illustration of the Cooper storage approach to quantifier scope generation.
Table 1
Voting to determine optimal scope readings for quantifiers, according to Kuno, Takami, and
Wu (1999).
many of us/you some of them
Baseline:
? ?
Subject Q:
?
Lefthand Q:
?
Speaker/Hearer Q:
?
Total: 4 1
Some adherents of the structural approaches also seem to acknowledge the ne-
cessity of eventually coming to terms with the factors that play a role in determining
scope preferences in language. Aoun and Li (2000) claim that the lexical scope pref-
erences of quantifiers ?are not ruled out under a structural account? (page 140). It is
clear from the surrounding discussion, though, that they intend such lexical require-
ments to be taken care of in some nonsyntactic component of grammar. Although
Kuno, Takami, and Wu?s dialogue with Aoun and Li in Language has been portrayed
by both sides as a debate over the correct way of modeling quantifier scope, they are
not really modeling the same things. Whereas Aoun and Li (1993) provide an account
of scope generation, Kuno, Takami, and Wu (1999) intend to model both scope gen-
eration and scope prediction. The model of scope preferences provided in this article
is an empirically based refinement of the approach taken by Kuno, Takami, and Wu,
but in principle it is consistent with a structural account of scope generation.
77
Higgins and Sadock Modeling Scope Preferences
3. Approaches to Quantifier Scope in Computational Linguistics
Many studies, such as Pereira (1990) and Park (1995), have dealt with the issue of
scope generation from a computational perspective. Attempts have also been made
in computational work to extend a pure Cooper storage approach to handle scope
prediction. Hobbs and Shieber (1987) discuss the possibility of incorporating some sort
of ordering heuristics into the SRI scope generation system, in the hopes of producing
a ranked list of possible scope readings, but ultimately are forced to acknowledge that
?[t]he modifications turn out to be quite complicated if we wish to order quantifiers
according to lexical heuristics, such as having each out-scope some. Because of the
recursive nature of the algorithm, there are limits to the amount of ordering that can
be done in this manner? (page 55). The stepwise nature of these scope mechanisms
makes it hard to state the factors that influence the preference for one quantifier to
take scope over another.
Those natural language processing (NLP) systems that have managed to provide
some sort of account of quantifier scope preferences have done so by using a separate
system of heuristics (or scope critics) that apply postsyntactically to determine the most
likely scoping. LUNAR (Woods 1986), TEAM (Martin, Appelt, and Pereira 1986), and
the SRI Core Language Engine as described by Moran (1988; Moran and Pereira 1992)
all employ scope rules of this sort. By and large, these rules are of an ad hoc nature,
implementing a linguist?s intuitive idea of what factors determine scope possibilities,
and no results have been published regarding the accuracy of these methods. For
example, Moran (1988) incorporates rules from other NLP systems and from VanLehn
(1978), such as a preference for a logically weaker interpretation, the tendency for each
to take wide scope, and a ban on raising a quantifier across multiple major clause
boundaries. The testing of Moran?s system is ?limited to checking conformance to
the stated rules? (pages 40?41). In addition, these systems are generally incapable of
handling unrestricted text such as that found in the Wall Street Journal corpus in a
robust way, because they need to do a full semantic analysis of a sentence in order
to make scope predictions. The statistical basis of the model presented in this article
offers increased robustness and the possibility of more serious evaluation on the basis
of corpus data.
4. Modeling Quantifier Scope
In this section, we argue for an empirically driven machine learning approach to
the identification of factors relevant to quantifier scope and the modeling of scope
preferences. Following much recent work that applies the tools of machine learning to
linguistic problems (Brill 1995; Pedersen 2000; van Halteren, Zavrel, and Daelemans
2001; Soon, Ng, and Lim 2001), we will treat the prediction of quantifier scope as
an example of a classification task. Our aim is to provide a robust model of scope
prediction based on Kuno, Takami, and Wu?s theoretical foundation and to address
the serious lack of empirical results regarding quantifier scope in computational work.
We describe here the modeling tools borrowed from the field of artificial intelligence
for the scope prediction task and the data from which the generalizations are to be
learned. Finally, we present the results of training different incarnations of our scope
module on the data and assess the implications of this exercise for theoretical and
computational linguistics.
78
Computational Linguistics Volume 29, Number 1
4.1 Classification in Machine Learning
Determining which among multiple quantifiers in a sentence takes wide scope, given
a number of different sources of evidence, is an example of what is known in machine
learning as a classification task (Mitchell 1996). There are many types of classifiers
that may be applied to this task that both are more sophisticated than the approach
suggested by Kuno, Takami, and Wu and have a more solid probabilistic foundation.
These include the naive Bayes classifier (Manning and Schu?tze 1999; Jurafsky and
Martin 2000), maximum-entropy models (Berger, Della Pietra, and Della Pietra 1996;
Ratnaparkhi 1997), and the single-layer perceptron (Bishop 1995). We employ these
classifier models here primarily because of their straightforward probabilistic inter-
pretation and their similarity to the scope model of Kuno, Takami, and Wu (since they
each could be said to implement a kind of weighted voting of factors). In Section 4.3,
we describe how classifiers of these types can be constructed to serve as a grammatical
module responsible for quantifier scope determination.
All of these classifiers can be trained in a supervised manner. That is, given a sam-
ple of training data that provides all of the information that is deemed to be relevant
to quantifier scope and the actual scope reading assigned to a sentence, these classi-
fiers will attempt to extract generalizations that can be fruitfully applied in classifying
as-yet-unseen examples.
4.2 Data
The data on which the quantifier scope classifiers are trained and tested is an extract
from the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) that we have
tagged to indicate the most salient scope interpretation of each sentence in context.
Figure 3 shows an example of a training sentence with the scope reading indicated.
The quantifier lower in the tree bears the tag ?Q1,? and the higher quantifier bears the
tag ?Q2,? so this sentence is interpreted such that the lower quantifier has wide scope.
Reversing the tags would have meant that the higher quantifier takes wide scope, and
while if both quantifiers had been marked ?Q1,? this would have indicated that there
is no scope interaction between them (as when they are logically independent or take
scope in different conjuncts of a conjoined phrase).2
The sentences tagged were chosen from the Wall Street Journal (WSJ) section of
the Penn Treebank to have a certain set of attributes that simplify the task of design-
ing the quantifier scope module of the grammar. First, in order to simplify the coding
process, each sentence has exactly two scope-taking elements of the sort considered
for this project.3 These include most NPs that begin with a determiner, predeterminer,
or quantifier phrase (QP)4 but exclude NPs in which the determiner is a, an, or the. Ex-
2 This ?no interaction? class is a sort of ?elsewhere? category that results from phrasing the classification
question as ?Which quantifier takes wider scope in the preferred reading?? Where there is no scope
interaction, the answer is ?neither.? This includes cases in which the relative scope of operators does
not correspond to a difference in meaning, as in One woman bought one horse, or when they take scope
in different propositional domains, such as in Mary bought two horses and sold three sheep. The human
coders used in this study were instructed to choose class 0 whenever there was not a clear preference
for one of the two scope readings.
3 This restriction that each sentence contain only two quantified elements does not actually exclude
many sentences from consideration. We identified only 61 sentences with three quantifiers of the sort
we consider and 12 sentences with four. In addition, our review of these sentences revealed that many
of them simply involve lists in which the quantifiers do not interact in terms of scope (as in, for
example, ?We ask that you turn off all cell phones, extinguish all cigarettes, and open any candy before
the performance begins?). Thus, the class of sentences with more than two quantifiers is small and
seems to involve even simpler quantifier interactions than those found in our corpus.
4 These categories are intended to be understood as they are used in the tagging and parsing of the Penn
Treebank. See Santorini (1990) and Bies et al (1995) for details; the Appendix lists selected codes used
79
Higgins and Sadock Modeling Scope Preferences
( (S
(NP-SBJ
(NP (DT Those) )
(SBAR
(WHNP-1 (WP who) )
(S
(NP-SBJ-2 (-NONE- *T*-1) )
(ADVP (RB still) )
(VP (VBP want)
(S
(NP-SBJ (-NONE- *-2) )
(VP (TO to)
(VP (VB do)
(NP (PRP it) ))))))))
(?? ??)
(VP (MD will)
(ADVP (RB just) )
(VP (VB find)
(NP
(NP (DT-Q2 some) (NN way) )
(SBAR
(WHADVP-3 (-NONE- 0) )
(S
(NP-SBJ (-NONE- *) )
(VP (TO to)
(VP (VB get)
(PP (IN around) (?? ??)
(NP (DT-Q1 any) (NN attempt)
(S
(NP-SBJ (-NONE- *) )
(VP (TO to)
(VP (VB curb)
(NP (PRP it) ))))))
(ADVP-MNR (-NONE- *T*-3) ))))))))
(. .) ))
Figure 3
Tagged Wall Street Journal text from the Penn Treebank. The lower quantifier takes wide
scope, indicated by its tag ?Q1.?
cluding these determiners from consideration largely avoids the problem of generics
and the complexities of assigning scope readings to definite descriptions. In addi-
tion, only sentences that had the root node S were considered. This serves to exclude
sentence fragments and interrogative sentence types. Our data set therefore differs
systematically from the full WSJ corpus, but we believe it is sufficient to allow many
generalizations about English quantification to be induced. Given these restrictions on
the input data, the task of the scope classifier is a choice among three alternatives:5
(Class 0) There is no scopal interaction.
(Class 1) The first quantifier takes wide scope.
(Class 2) The second quantifier takes wide scope.
for annotating the Penn Treebank corpus. The category QP is particularly unintuitive in that it does not
correspond to a quantified noun phrase, but to a measure expression, such as more than half.
5 Some linguists may find it strange that we have chosen to treat the choice of preferred scoping for two
quantified elements as a tripartite decision, since the possibility of independence is seldom treated in
the linguistic literature. As we are dealing with corpus data in this experiment, we cannot afford to
ignore this possibility.
80
Computational Linguistics Volume 29, Number 1
The result is a set of 893 sentences,6 annotated with Penn Treebank II parse trees and
hand-tagged for the primary scope reading.
To assess the reliability of the hand-tagged data used in this project, the data were
coded a second time by an independent coder, in addition to the reference coding.
The independent codings agreed with the reference coding on 76.3% of sentences. The
kappa statistic (Cohen 1960) for agreement was .52, with a 95% confidence interval
between .40 and .64. Krippendorff (1980) has been widely cited as advocating the
view that kappa values greater than .8 should be taken as indicating good reliability,
with values between .67 and .8 indicating tentative reliability, but we are satisfied
with the level of intercoder agreement on this task. As Carletta (1996) notes, many
tasks in computational linguistics are simply more difficult than the content analysis
classifications addressed by Krippendorff, and according to Fleiss (1981), kappa values
between .4 and .75 indicate fair to good agreement anyhow.
Discussion between the coders revealed that there was no single cause for their dif-
ferences in judgments when such differences existed. Many cases of disagreement stem
from different assumptions regarding the lexical quantifiers involved. For example, the
coders sometimes differed on whether a given instance of the word any corresponds
to a narrow-scope existential, as we conventionally treat it when it is in the scope of
negation, or the ?free-choice? version of any. To take another example, two universal
quantifiers are independent in predicate calculus (?x?y[?] ?? ?y?x[?]), but in creat-
ing our scope-tagged corpus, it was often difficult to decide whether two universal-like
English quantifiers (such as each, any, every, and all) were actually independent in a
given sentence. Some differences in coding stemmed from coder disagreements about
whether a quantifier within a fixed expression (e.g., all the hoopla) truly interacts with
other operators in the sentence. Of course, another major factor contributing to inter-
coder variation is the fact that our data sentences, taken from Wall Street Journal text,
are sometimes quite long and complex in structure, involving multiple scope-taking
operators in addition to the quantified NPs. In such cases, the coders sometimes had
difficulty clearly distinguishing the readings in question.
Because of the relatively small amount of data we had, we used the technique of
tenfold cross-validation in evaluating our classifiers, in each case choosing 89 of the
893 total data sentences from the data as a test set and training on the remaining 804.
We preprocessed the data in order to extract the information from each sentence that
we would be treating as relevant to the prediction of quantifier scoping in this project.
(Although the initial coding of the preferred scope reading for each sentence was done
manually, this preprocessing of the data was done automatically.) At the end of this
preprocessing, each sentence was represented as a record containing the following
information (see the Appendix for a list of annotation codes for Penn Treebank):
? the syntactic category, according to Penn Treebank conventions, of the
first quantifier (e.g., DT for each, NN for everyone, or QP for more than half )
? the first quantifier as a lexical item (e.g., each or everyone). For a QP
consisting of multiple words, this field contains the head word, or ?CD?
in case the head is a cardinal number.
? the syntactic category of the second quantifier
? the second quantifier as a lexical item
6 These data have been made publicly available to all licensees of the Penn Treebank by means of a
patch file that may be retrieved from ?http://humanities.uchicago.edu/linguistics/students/dchiggin/
qscope-data.tgz?. This file also includes the coding guidelines used for this project.
81
Higgins and Sadock Modeling Scope Preferences
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
class: 2
first cat: DT
first head: some
second cat: DT
second head: any
join cat: NP
first c-commands: YES
second c-commands: NO
nodes intervening: 6
VP intervenes: YES
ADVP intervenes: NO
...
S intervenes: YES
conj intervenes: NO
, intervenes: NO
: intervenes: NO
...
? intervenes: YES
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 4
Example record corresponding to the sentence shown in Figure 3.
? the syntactic category of the lowest node dominating both quantified
NPs (the ?join? node)
? whether the first quantified NP c-commands the second
? whether the second quantified NP c-commands the first
? the number of nodes intervening7 between the two quantified NPs
? a list of the different categories of nodes that intervene between the
quantified NPs (actually, for each nonterminal category, there is a distinct
binary feature indicating whether a node of that category intervenes)
? whether a conjoined node intervenes between the quantified NPs
? a list of the punctuation types that are immediately dominated by nodes
intervening between the two NPs (again, for each punctuation tag in the
treebank there is a distinct binary feature indicating whether such
punctuation intervenes)
Figure 4 illustrates how these features would be used to encode the example in Fig-
ure 3.
The items of information included in the record, as listed above, are not the exact
factors that Kuno, Takami, and Wu (1999) suggest be taken into consideration in mak-
ing scope predictions, and they are certainly not sufficient to determine the proper
scope reading for all sentences completely. Surely pragmatic factors and real-world
knowledge influence our interpretations as well, although these are not represented
here. This list does, however, provide information that could potentially be useful in
predicting the best scope reading for a particular sentence. For example, information
7 We take a node ? to intervene between two other nodes ? and ? in a tree if and only if ? is the lowest
node dominating both ? and ?, ? dominates ? or ? = ?, and ? dominates either ? or ?.
82
Computational Linguistics Volume 29, Number 1
Table 2
Baseline performance, summed over all ten test sets.
Condition Correct Incorrect Percentage correct
First has wide scope 0 64 0/64 = 0.0%
Second has wide scope 0 281 0/281 = 0.0%
No scope interaction 545 0 545/545 = 100.0%
Total 545 345 545/890 = 61.2%
about whether one quantified NP in a given sentence c-commands the other corre-
sponds to Kuno, Takami, and Wu?s observation that subject quantifiers tend to take
wide scope over object quantifiers and topicalized quantifiers tend to outscope ev-
erything. The identity of each lexical quantifier clearly should allow our classifiers to
make the generalization that each tends to take wide scope, if this word is found in
the data, and perhaps even learn the regularity underlying Kuno, Takami, and Wu?s
observation that universal quantifiers tend to outscope existentials.
4.3 Classifier Design
In this section, we present the three types of model that we have trained to predict
the preferred quantifier scoping on Penn Treebank sentences: a naive Bayes classifier,
a maximum-entropy classifier, and a single-layer perceptron.8 In evaluating how well
these models do in assigning the proper scope reading to each test sentence, it is im-
portant to have a baseline for comparison. The baseline model for this task is one that
simply guesses the most frequent category of the data (?no scope interaction?) every
time. This simplistic strategy already classifies 61.2% of the test examples correctly, as
shown in Table 2.
It may surprise some linguists that this third class of sentences in which there is
no scopal interaction between the two quantifiers is the largest. In part, this may be
due to special features of the Wall Street Journal text of which the corpus consists. For
example, newspaper articles may contain more direct quotations than other genres. In
the process of tagging the data, however, it was also apparent that in a large proportion
of cases, the two quantifiers were taking scope in different conjuncts of a conjoined
phrase. This further tendency supports the idea that people may intentionally avoid
constructions in which there is even the possibility of quantifier scope interactions,
perhaps because of some hearer-oriented pragmatic principle. Linguists may also be
concerned that this additional category in which there is no scope interaction between
quantifiers makes it difficult to compare the results of the present work with theoretical
accounts of quantifier scope that ignore this case and concentrate on instances in which
one quantifier does take scope over another. In response to such concerns, however,
we point out first that we provide a model of scope prediction rather than scope
generation, and so it is in any case not directly comparable with work in theoretical
linguistics, which has largely ignored scope preferences. Second, we point out that
the empirical nature of this study requires that we take note of cases in which the
quantifiers simply do not interact.
8 The implementations of these classifiers are publicly available as Perl modules at ?http://humanities.
uchicago.edu/linguistics/students/dchiggin/classifiers.tgz?.
83
Higgins and Sadock Modeling Scope Preferences
Table 3
Performance of the naive Bayes classifier, summed over all 10 test runs.
Condition Correct Incorrect Percentage correct
First has wide scope 177 104 177/281 = 63.0%
Second has wide scope 41 23 41/64 = 64.1%
No scope interaction 428 117 428/545 = 78.5%
Total 646 244 646/890 = 72.6%
4.3.1 Naive Bayes Classifier. Our data D will consist of a vector of features (d0 ? ? ? dn)
that represent aspects of the sentence under examination, such as whether one quan-
tified expression c-commands the other, as described in Section 4.2. The fundamental
simplifying assumption that we make in designing a naive Bayes classifier is that
these features are independent of one another and therefore can be aggregated as in-
dependent sources of evidence about which class c? a given sentence belongs to. This
independence assumption is formalized in equations (1) and (2).
c? = arg max
c
P(c)P(d0 ? ? ? dn | c) (1)
? arg max
c
P(c)
n
?
k=0
P(dk | c) (2)
We constructed an empirical estimate of the prior probability P(c) by simply count-
ing the frequency with which each class occurs in the training data. We constructed
each P(dk | c) by counting how often each feature dk co-occurs with the class c to
construct the empirical estimate P?(dk | c) and interpolated this with the empirical
frequency P?(dk) of the feature dk, not conditioned on the class c. This interpolated
probability model was used in order to smooth the probability distribution, avoiding
the problems that can arise if certain feature-value pairs are assigned a probability of
zero.
The performance of the naive Bayes classifier is summarized in Table 3. For each
of the 10 test sets of 89 items taken from the corpus, the remaining 804 of the total
893 sentences were used to train the model. The naive Bayes classifier outperformed
the baseline by a considerable margin.
In addition to the raw counts of test examples correctly classified, though, we
would like to know something of the internal structure of the model (i.e., what sort of
features it has induced from the data). For this classifier, we can assume that a feature
f is a good predictor of a class c? when the value of P(f | c?) is significantly larger
than the (geometric) mean value of P(f | c) for all other values of c. Those features
with the greatest ratio P(f ) ? P(f |c
?)
geom.mean(?c =c?[P(f |c)]) are listed in Table 4.
9
The first-ranked feature in Table 4 shows that there is a tendency for quanti-
fied elements not to interact when they are found in conjoined constituents, and the
second-ranked feature indicates a preference for quantifiers not to interact when there
is an intervening comma (presumably an indicator of greater syntactic ?distance?).
Feature 3 indicates a preference for class 1 when there is an intervening S node,
9 We include the term P(f ) in the product in order to prevent sparsely instantiated features from
showing up as highly-ranked.
84
Computational Linguistics Volume 29, Number 1
Table 4
Most active features from naive Bayes classifier.
Rank Feature Predicted Ratio
class
1 There is an intervening conjunct node 0 1.63
2 There is an intervening comma 0 1.51
3 There is an intervening S node 1 1.33
4 The first quantified NP does not c-command the second 0 1.25
5 Second quantifier is tagged QP 1 1.16
6 There is an intervening S node 0 1.12
15 The second quantified NP c-commands the first 2 1.00
whereas feature 6 indicates a preference for class 0 under the same conditions. Pre-
sumably, this reflects a dispreference for the second quantifier to take wide scope
when there is a clause boundary intervening between it and the first quantifier. The
fourth-ranked feature in Table 4 indicates that, if the first quantified NP does not
c-command the second, it is less likely to take wide scope. This is not surprising,
given the importance that c-command relations have had in theoretical discussions
of quantifier scope. The fifth-ranked feature expresses a preference for quantified ex-
pressions of category QP to take narrow scope, if they are the second of the two
quantifiers under consideration. This may simply be reflective of the fact that class
1 is more common than class 2, and the measure expressions found in QP phrases
in the Penn Treebank (such as more than three or about half ) tend not to be logically
independent of other quantifiers. Finally, the feature 15 in Table 4 indicates a high
correlation between the second quantified expression?s c-commanding the first and
the second quantifier?s taking wide scope. We can easily see this as a translation into
our feature set of Kuno, Takami, and Wu?s claim that subjects tend to outscope ob-
jects and obliques and topicalized elements tend to take wide scope. Some of these
top-ranked features have to do with information found only in the written medium,
but on the whole, the features induced by the naive Bayes classifier seem consis-
tent with those suggested by Kuno, Takami, and Wu, although they are distinct by
necessity.
4.3.2 Maximum-Entropy Classifier. The maximum-entropy classifier is a sort of log-
linear model, defining the joint probability of a class and a data vector (d0 ? ? ? dn) as
the product of the prior probability of the class c with a set of features related to the
data:10
P(d0 ? ? ? dn, c) =
P(c)
Z
n
?
k=0
?k (3)
This classifier superficially resembles in form the naive Bayes classifier in equation (2),
but it differs from that classifier in that the way in which values for each ? are chosen
does not assume that the features in the data are independent. For each of the 10
training sets, we used the generalized iterative scaling algorithm to train this classifier
on 654 training examples, using 150 examples for validation to choose the best set of
10 Z in Equation 3 is simply a normalizing constant that ensures that we end up with a probability
distribution.
85
Higgins and Sadock Modeling Scope Preferences
Table 5
Performance of the maximum-entropy classifier, summed over all 10 test runs.
Condition Correct Incorrect Percentage correct
First has wide scope 148 133 148/281 = 52.7%
Second has wide scope 31 33 31/64 = 48.4%
No scope interaction 475 70 475/545 = 87.2%
Total 654 236 654/890 = 73.5%
Table 6
Most active features from maximum-entropy classifier.
Rank Feature Predicted ?c,.25
class
1 Second quantifier is each 2 1.13
2 There is an intervening comma 0 1.01
3 There is an intervening conjunct node 0 1.00
4 First quantified NP does not c-command the second 0 0.99
5 Second quantifier is every 2 0.98
6 There is an intervening quotation mark (?) 0 0.95
7 There is an intervening colon 0 0.95
12 First quantified NP c-commands the second 1 0.92
25 There is no intervening comma 1 0.90
values for the ?s.11 Test data could then be classified by choosing the class for the data
that maximizes the joint probability in equation (3).
The results of training with the maximum-entropy classifier are shown in Table 5.
The classifier showed slightly higher performance than the naive Bayes classifier, with
the lowest error rate on the class of sentences having no scope interaction.
To determine exactly which features of the data the maximum-entropy classifier
sees as relevant to the classification problem, we can simply look at the ? values (from
equation (3)) for each feature. Those features with higher values for ? are weighted
more heavily in determining the proper scoping. Some of the features with the highest
values for ? are listed in Table 6. Because of the way the classifier is built, predictor
features for class 2 need to have higher loadings to overcome the lower prior probabil-
ity of the class. Therefore, we actually rank the features in Table 6 according to ?P?(c)k
(which we denote as ?c,k). P?(c) represents the empirical prior probability of a class c,
and k is simply a constant (.25 in this case) chosen to try to get a mix of features for
different classes at the top of the list.
The features ranked first and fifth in Table 6 express lexical preferences for certain
quantifiers to take wide scope, even when they are the second of the two quantifiers
according to linear order in the string of words. The tendency for each to take wide
scope is stronger than for the other quantifier, which is in line with Kuno, Takami,
and Wu?s decision to list it as the only quantifier with a lexical preference for scoping.
Feature 2 makes the ?no scope interaction? class more likely if a comma intervenes, and
11 Overtraining is not a problem with the pure version of the generalized iterative scaling algorithm. For
efficiency reasons, however, we chose to take the training corpus as representative of the event space,
rather than enumerating the space exhaustively (see Jelinek [1998] for details). For this reason, it was
necessary to employ validation in training.
86
Computational Linguistics Volume 29, Number 1
Table 7
Performance of the single-layer perceptron, summed over all 10 test runs.
Condition Correct Incorrect Percentage correct
First has wide scope 182 99 182/281 = 64.8%
Second has wide scope 35 29 35/64 = 54.7%
No scope interaction 468 77 468/545 = 85.9%
Total 685 205 685/890 = 77.0%
feature 25 makes a wide-scope reading for the first quantifier more likely if there is no
intervening comma. The third-ranked feature expresses the tendency mentioned above
for quantifiers in conjoined clauses not to interact. Features 4 and 12 indicate that if the
first quantified expression c-commands the second, it is likely to take wide scope, and
that if this is not the case, there is likely to be no scope interaction. Finally, the sixth-
and seventh-ranked features in the table show that an intervening quotation mark or
colon will make the classifier tend toward class 0, ?no scope interaction,? which is easy
to understand. Quotations are often opaque to quantifier scope interactions. The top
features found by the maximum-entropy classifier largely coincide with those found
by the naive Bayes model, which indicates that these generalizations are robust and
objectively present in the data.
4.3.3 Single-Layer Perceptron. For our neural network classifier, we employed a feed-
forward single-layer perceptron, with the softmax function used to determine the acti-
vation of nodes at the output layer, because this is a one-of-n classification task (Bridle
1990). The data to be classified are presented as a vector of features at the input layer,
and the output layer has three nodes, representing the three possible classes for the
data: ?first has wide scope,? ?second has wide scope,? and ?no scope interaction.?
The output node with the highest activation is interpreted as the class of the datum
presented at the input layer.
For each of the 10 test sets of 89 examples, we trained the connection weights
of the network using error backpropagation on 654 training sentences, reserving 150
sentences for validation in order to choose the weights from the training epoch with the
highest classification performance. In Table 7 we present the results of the single-layer
neural network in classifying our test sentences. As the table shows, the single-layer
perceptron has much better classification performance than the naive Bayes classifier
and maximum-entropy model, possibly because the training of the network aims to
minimize error in the activation of the classification output nodes, which is directly
related to the classification task at hand, whereas the other models do not directly
make use of the notion of ?classification error.? The perceptron also uses a sort of
weighted voting and could be interpreted as an implementation of Kuno, Takami,
and Wu?s proposal for scope determination. This clearly illustrates that the tenability
of their proposal hinges on the exact details of its implementation, since all of our
classifier models are reasonable interpretations of their approach, but they have very
different performance results on our scope determination task.
To determine exactly which features of the data the network sees as relevant to
the classification problem, we can simply look at the connection weights for each
feature-class pair. Higher connection weights indicate a greater correlation between
input features and output classes. For one of the 10 networks we trained, some of
the features with the highest connection weights are listed in Table 8. Since class 0 is
87
Higgins and Sadock Modeling Scope Preferences
Table 8
Most active features from single-layer perceptron.
Rank Feature Predicted Weight
class
1 There is an intervening comma 0 4.31
2 Second quantifier is all 0 3.77
3 There is an intervening colon 0 2.98
4 There is an intervening conjunct node 0 2.72
17 The first quantified NP c-commands the second 1 1.69
18 Second quantifier is tagged RBS 2 1.69
19 There is an intervening S node 1 1.61
20 Second quantifier is each 2 1.50
simply more frequent in the training data than the other two classes, the weights for
this class tend to be higher. Therefore, we also list some of the best predictor features
for classes 1 and 2 in the table.
The first- and third-ranked features in Table 8 show that an intervening comma or
colon will make the classifier tend toward class 0, ?no scope interaction.? This finding
by the classifier is similar to the maximum-entropy classifier?s finding an intervening
quotation mark relevant and can be taken as an indication that quantifiers in distant
syntactic subdomains are unlikely to interact. Similarly, the fourth-ranked feature indi-
cates that quantifiers in separate conjuncts are unlikely to interact. The second-ranked
feature in the table expresses a tendency for there to be no scope interaction between
two quantifiers if the second of them is headed by all. This may be related to the
independence of universal quantifiers (?x?y[?] ?? ?y?x[?]). Feature 17 in Table 8
indicates a high correlation between the first quantified expression?s c-commanding
the second and the first quantifier?s taking wide scope, which again supports Kuno,
Takami, and Wu?s claim that scope preferences are related to syntactic superiority re-
lations. Feature 18 expresses a preference for a quantified expression headed by most
to take wide scope, even if it is the second of the two quantifiers (since most is the
only quantifier in the corpus that bears the tag RBS). Feature 19 indicates that the
first quantifier is more likely to take wide scope if there is a clause boundary in-
tervening between the two quantifiers, which supports the notion that the syntactic
distance between the quantifiers is relevant to scope preferences. Finally, feature 20
expresses the well-known tendency for quantified expressions headed by each to take
wide scope.
4.4 Summary of Results
Table 9 summarizes the performance of the quantifier scope models we have presented
here. All of the classifiers have test set accuracy above the baseline, which a paired
t-test reveals to be significant at the .001 level. The differences between the naive
Bayes, maximum-entropy, and single-layer perceptron classifiers are not statistically
significant.
The classifiers performed significantly better on those sentences annotated consis-
tently by both human coders at the beginning of the study, reinforcing the view that
this subset of the data is somehow simpler and more representative of the basic regu-
larities in scope preferences. For example, the single-layer perceptron classified 82.9%
of these sentences correctly. To further investigate the nature of the variation between
the two coders, we constructed a version of our single-layer network that was trained
88
Computational Linguistics Volume 29, Number 1
Table 9
Summary of classifier results.
Training data Validation data Test data
Baseline ? ? 61.2%
Na??ve Bayes 76.7% ? 72.6%
Maximum entropy 78.3% 75.5% 73.5%
Single-layer
perceptron 84.7% 76.8% 77.0%
on the data on which both coders agreed and tested on the remaining sentences. This
classifier agreed with the reference coding (the coding of the first coder) 51.4% of the
time and with the additional independent coder 35.8% of the time. The first coder con-
structed the annotation guidelines for this project and may have been more successful
in applying them consistently. Alternatively, it is possible that different individuals use
different strategies in determining scope preferences, and the strategy of the second
coder may simply have been less similar than the strategy of the first coder to that of
the single-layer network.
These three classifiers directly implement a sort of weighted voting, the method
of aggregating evidence proposed by Kuno, Takami, and Wu (although the classifiers?
implementation is slightly more sophisticated than the unweighted voting that is ac-
tually used in Kuno, Takami, and Wu?s paper). Of course, since we do not use exactly
the set of features suggested by Kuno, Takami, and Wu, our model should not be
seen as a straightforward implementation of the theory outlined in their 1999 paper.
Nevertheless, the results in Table 9 suggest that Kuno, Takami, and Wu?s suggested
design can be used with some success in modeling scope preferences. Moreover, the
project undertaken here provides an answer to some of the objections that Aoun and
Li (2000) raise to Kuno, Takami, and Wu. Aoun and Li claim that Kuno, Takami, and
Wu?s choice of experts is seemingly arbitrary and that it is unclear how the voting
weights of each expert are to be set, but the machine learning approach we employ
in this article is capable of addressing both of these potential problems. Supervised
training of our classifiers is a straightforward approach to setting the weights and
also constitutes our approach to selecting features (or ?experts? in Kuno, Takami, and
Wu?s terminology). In the training process, any feature that is irrelevant to scoping
preferences should receive weights that make its effect negligible.
5. Syntax and Scope
In this section, we show how the classifier models of quantifier scope determination
introduced in Section 4 may be integrated with a PCFG model of syntax. We com-
pare two different ways in which the two components may be combined, which may
loosely be termed serial and parallel, and argue for the latter on the basis of empirical
results.
5.1 Modular Design
Our use of a phrase structure syntactic component and a quantifier scope component
to define a combined language model is simplified by the fact that our classifiers are
probabilistic and define a conditional probability distribution over quantifier scopings.
The probability distributions that our classifiers define for quantifier scope structures
are conditional on syntactic phrase structure, because they are computed on the basis
89
Higgins and Sadock Modeling Scope Preferences
of syntactically provided features, such as the number of nodes of a certain type that
intervene between two quantifiers in a phrase structure tree.
Thus, the combined language model that we define in this article assigns probabil-
ities according to the pairs of structures that may be assigned to a sentence by the Q-
structure and phrase structure syntax modules. The probability of a word string w1?n
is therefore defined as in equation (4), where Q ranges over all possible Q-structures
in the set Q and S ranges over all possible syntactic structures in the set S.
P(w1?n) =
?
S?S,Q?Q
P(S, Q | w1?n) (4)
=
?
S?S,Q?Q
P(S | w1?n)P(Q | S, w1?n) (5)
Equation (5) shows how we can use the definition of conditional probability to
break our calculation of the language model probability into two parts. The first of
these parts, P(S | w1?n), which we may abbreviate as simply P(S), is the probability
of a particular syntactic tree structure?s being assigned to a particular word string. We
model this probability using a probabilistic phrase structure grammar (cf. Charniak
[1993, 1996]). The second distribution on the right side of equation (5) is the conditional
probability of a particular quantifier scope structure?s being assigned to a particular
word string, given the syntactic structure of that string. This probability is written as
P(Q | S, w1?n), or simply P(Q | S), and represents the quantity we estimated above
in constructing classifiers to predict the scopal representation of a sentence based on
aspects of its syntactic structure.
Thus, given a PCFG model of syntactic structure and a probabilistically defined
classifier of the sort introduced in Section 4, it is simple to determine the probability
of any pairing of two particular structures from each domain for a given sentence.
We simply multiply the values of P(S) and P(Q | S) to obtain the joint probability
P(Q, S). In the current section, we examine two different models of combination for
these components: one in which scope determination is applied to the optimal syn-
tactic structure (the Viterbi parse), and one in which optimization is performed in the
space of both modules to find the optimal pairing of syntactic and quantifier scope
structures.
5.2 The Syntactic Module
Before turning to the application of our multimodular approach to the problem of
scope determination in Section 5.3, we present here a short overview of the phrase
structure syntactic component used in these projects. As noted above, we model syn-
tax as a probabilistic phrase structure grammar (PCFG), and in particular, we use a
treebank grammar (Charniak 1996) trained on the Penn Treebank.
A PCFG defines the probability of a string of words as the sum of the probabilities
of all admissible phrase structure parses (trees) for that string. The probability of a
given tree is the product of the probability of all of the rule instances used in the
construction of that tree, where rules take the form N ? ?, with N a nonterminal
symbol and ? a finite sequence of one or more terminals or nonterminals.
To take an example, Figure 5 illustrates a phrase structure tree for the sentence Su-
san might not believe you, which is admissible according to the grammar in Table 10. (All
of the minimal subtrees in Figure 5 are instances of one of our rules.) The probability
90
Computational Linguistics Volume 29, Number 1
Figure 5
A simple phrase structure tree.
Table 10
A simple probabilistic phrase structure grammar.
Rule Probability
S ? NP VP .7
S ? VP .2
S ? V NP VP .1
VP ? V VP .3
VP ? ADV VP .1
VP ? V .1
VP ? V NP .3
VP ? V NP NP .2
NP ? Susan .3
NP ? you .4
NP ? Yves .3
V ? might .2
V ? believe .3
V ? show .3
V ? stay .2
ADV ? not .5
ADV ? always .5
91
Higgins and Sadock Modeling Scope Preferences
of this tree, which we can indicate as ? , can be calculated as in equation (6).
P(?) =
?
??Rules(?)
P(?) (6)
= P(S ? NP VP) ? P(VP ? V VP) ? P(VP ? ADV VP)
? P(VP ? V NP) ? P(NP ? Susan) ? P(V ? might)
? P(ADV ? not) ? P(V ? believe) ? P(NP ? you) (7)
= .7 ? .3 ? .1 ? .3 ? .3 ? .2 ? .5 ? .3 ? .4 = 2.268 ? 10?5 (8)
The actual grammar rules and associated probabilities that we use in defining our
syntactic module are derived from the WSJ corpus of the Penn Treebank by maximum-
likelihood estimation. That is, for each rule N ? ? used in the treebank, we add the
rule to the grammar and set its probability to C(N??)?
?
C(N??) , where C(?) denotes the
?count? or a rule (i.e., the number of times it is used in the corpus). A grammar
composed in this manner is referred to as a treebank grammar, because its rules are
directly derived from those in a treebank corpus.
We used sections 00?20 of the WSJ corpus of the Penn Treebank for collecting the
rules and associated probabilities of our PCFG, which is implemented as a bottom-up
chart parser. Before constructing the grammar, the treebank was preprocessed using
known procedures (cf. Krotov et al [1998]; Belz [2001]) to facilitate the construction of
a rule list. Functional and anaphoric annotations (basically anything following a ?-?
in a node label; cf. Santorini [1990]; Bies et al [1995]) were removed from nonterminal
labels. Nodes that dominate only ?empty categories? such as traces were removed.
In addition, unary-branching constructions were removed by replacing the mother
category in such a structure with the daughter node. (For example, given an instance
of the rule X ? YZ, if the daughter category Y were expanded by the unary rule
Y ? W, our algorithm would induce the single rule X ? WZ.) Finally, we discarded
all rules that had more than 10 symbols on the right-hand side (an arbitrary limit of
our parser implementation). This resulted in a total of 18,820 rules, of which 11,156
were discarded as hapax legomena, leaving 7,664 rules in our treebank grammar.
Table 11 shows some of the rules in our grammar with the highest and lowest corpus
counts.
5.3 Unlabeled Scope Determination
In this section, we describe an experiment designed to assess the performance of
parallel and serial approaches to combining grammatical modules, focusing on the
task of unlabeled scope determination. This task involves predicting the most likely
Q-structure representation for a sentence, basically the same task we addressed in Sec-
tion 4, in comparing the performance levels of each type of classifier. The experiment
of this section differs, however, from the task presented in Section 4 in that instead of
providing a syntactic tree from the Penn Treebank as input to the classifier, we provide
the model only with a string of words (a sentence). Our dual-component model will
search for the optimal syntactic and scopal structures for the sentence (the pairing
(??,??)) and will be evaluated based on its success in identifying the correct scope
reading ??.
Our concern in this section will be to determine whether it is necessary to search
the space of possible pairings (? ,?) of syntactic and scopal structures or whether
it is sufficient to use our PCFG first to fix the syntactic tree ? , and then to choose
92
Computational Linguistics Volume 29, Number 1
Table 11
Rules derived from sections 00?20 of the Penn Treebank WSJ corpus. ?TOP? is a special ?start?
symbol that may expand to any of the symbols found at the root of a tree in the corpus.
Rule Corpus count
PP ? IN NP 59,053
TOP ? S 34,614
NP ? DT NN 28,074
NP ? NP PP 25,192
S ? NP VP 14,032
S ? NP VP . 12,901
VP ? TO VP 11,598
...
S ? CC PP NNP NNP VP . 2
NP ? DT ? NN NN NN ?? 2
NP ? NP PP PP PP PP PP 2
INTJ ? UH UH 2
NP ? DT ? NN NNS 2
SBARQ ? ? WP VP . ?? 2
S ? PP NP VP . ?? 2
a scope reading to maximize the probability of the pairing. That is, are syntax and
quantifier scope mutually dependent components of grammar, or can scope relations
be ?read off of? syntax? The serial model suggests that the optimal syntactic structure
?? should be chosen on the basis of the syntactic module only, as in equation (9),
and the optimal quantifier scope structure ?? then chosen on the basis of ??, as in
equation (10). The parallel model, on the other hand, suggests that the most likely
pairing of structures must be chosen in the joint probability space of both components,
as in equation (11).
?? = arg max
??S
PS(? | w1?n) (9)
?? = arg max
??Q
PQ(? | ??, w1?n) (10)
?? = { ? | (? ,?) = arg max
??S,??Q
PS(? | w1?n)PQ(? | ? , w1?n) } (11)
5.3.1 Experimental Design. For this experiment, we implement the scoping compo-
nent as a single-layer feed-forward network, because the single-layer perceptron clas-
sifier had the best prediction rate among the three classifiers tested in Section 4. The
softmax activation function we use for the output nodes of the classifier guarantees
that the activations of all of the output nodes sum to one and can be interpreted as
class probabilities. The syntactic component, of course, is determined by the treebank
PCFG grammar described above.
Given these two models, which respectively define PQ(? | ? , w1?n) and PS(? | w1?n)
from equation (11), it remains only to specify how to search the space of pairings
(? ,?) in performing this optimization to find ??. Unfortunately, it is not feasible to
examine all values ? ? S, since our PCFG will generally admit a huge number of
93
Higgins and Sadock Modeling Scope Preferences
Table 12
Performance of models on the unlabeled scope prediction task, summed over all 10 test runs.
Condition Correct Incorrect Percentage correct
Parallel model
First has wide scope 168 113 167/281 = 59.4%
Second has wide scope 26 38 26/64 = 40.6%
No scope interaction 467 78 467/545 = 85.7%
Total 661 229 661/890 = 74.3%
Serial model
First has wide scope 163 118 163/281 = 58.0%
Second has wide scope 27 37 27/64 = 42.2%
No scope interaction 461 84 461/545 = 84.6%
Total 651 239 651/890 = 73.1%
trees for a sentence (especially given a mean sentence length of over 20 words in
the WSJ corpus).12 Our solution to this search problem is to make the simplifying as-
sumption that the syntactic tree that is used in the optimal set of structures (??,??)
will always be among the top few trees ? for which PS(? | w1?n) is the greatest.
That is, although we suppose that quantifier scope information is relevant to pars-
ing, we do not suppose that it is so strong a determinant as to completely over-
ride syntactic factors. In practice, this means that our parser will return the top 10
parses for each sentence, along with the probabilities assigned to them, and these
are the only parses that are considered in looking for the optimal set of linguistic
structures.
We again used 10-fold cross-validation in evaluating the competing models, di-
viding the scope-tagged corpus into 10 test sections of 89 sentences each, and we
used the same version of the treebank grammar for our PCFG. The first model re-
trieved the top 10 syntactic parses (?0 ? ? ? ?9) for each sentence and computed the
probability P(? ,?) for each ? ? ?0 ? ? ? ?9,? ? 0, 1, 2, choosing that scopal represen-
tation ? that was found in the maximum-probability pairing. We call this the par-
allel model, because the properties of each probabilistic model may influence the
optimal structure chosen by the other. The second model retrieved only the Viterbi
parse ?0 from the PCFG and chose the scopal representation ? for which the pair-
ing (?0,?) took on the highest probability. We call this the serial model, because it
represents syntactic phrase structure as independent of other components of gram-
mar (in this case, quantifier scope), though other components are dependent
upon it.
5.3.2 Results. There was an appreciable difference in performance between these two
models on the quantifier scope test sets. As shown in Table 12, the parallel model
narrowly outperformed the serial model, by 1.2%. A 10-fold paired t-test on the test
sections of the scope-tagged corpus shows that the parallel model is significantly better
(p < .05).
12 Since we are allowing ? to range only over the three scope readings (0, 1, 2), however, it is possible to
enumerate all values of ? to be paired with a given syntactic tree ? .
94
Computational Linguistics Volume 29, Number 1
This result suggests that, in determining the syntactic structure of a sentence, we
must take aspects of structure into account that are not purely syntactic (such as quanti-
fier scope). Searching both dimensions of the hypothesis space for our dual-component
model allowed the composite model to handle the interdependencies between differ-
ent aspects of grammatical structure, whereas fixing a phrase structure tree purely
on the basis of syntactic considerations led to suboptimal performance in using that
structure as a basis for determining quantifier scope.
6. Conclusion
In this article, we have taken a statistical, corpus-based approach to the modeling
of quantifier scope preferences, a subject that has previously been addressed only
with systems of ad hoc rules derived from linguists? intuitive judgments. Our model
takes its theoretical inspiration from Kuno, Takami, and Wu (1999), who suggest an
?expert system? approach to scope preferences, and follows many other projects in the
machine learning of natural language that combine information from multiple sources
in solving linguistic problems.
0ur results are generally supportive of the design that Kuno, Takami, and Wu pro-
pose for the quantifier scope component of grammar, and some of the features induced
by our models find clear parallels in the factors that Kuno, Takami, and Wu claim to
be relevant to scoping. In addition, our final experiment, in which we combine our
quantifier scope module with a PCFG model of syntactic phrase structure, provides
evidence of a grammatical architecture in which different aspects of structure mutu-
ally constrain one another. This result casts doubt on approaches in which syntactic
processing is completed prior to the determination of other grammatical properties of
a sentence, such as quantifier scope relations.
Appendix: Selected Codes Used to Annotate Syntactic Categories in the Penn Tree-
bank, from Marcus et al (1993) and Bies et al (1995)
Part-of-speech tags
Tag Meaning Tag Meaning
CC Conjunction RB Adverb
CD Cardinal number RBR Comparative adverb
DT Determiner RBS Superlative adverb
IN Preposition TO ?to?
JJ Adjective UH Interjection
JJR Comparative adjective VB Verb in base form
JJS Superlative adjective VBD Past-tense verb
NN Singular or mass noun VBG Gerundive verb
NNS Plural noun VBN Past participial verb
NNP Singular proper noun VBP Non-3sg, present-
NNPS Plural proper noun tense verb
PDT Predeterminer VBZ 3sg, present-tense
verb
PRP Personal pronoun WP WH pronoun
PRP$ Possessive pronoun WP$ Possessive WH pronoun
95
Higgins and Sadock Modeling Scope Preferences
Phrasal categories
Code Meaning Code Meaning
ADJP Adjective phrase SBAR Clause introduced by
ADVP Adverb phrase a subordinating
INTJ Interjection conjunction
NP Noun phrase SBARQ Clause introduced by
PP Prepositional phrase a WH phrase
QP Quantifier phrase (i.e., SINV Inverted declarative
measure/amount sentence
phrase) SQ Inverted yes/no
S Declarative clause question following the
WH phrase in SBARQ
VP Verb phrase
Acknowledgments
The authors are grateful for an Academic
Technology Innovation Grant from the
University of Chicago, which helped to
make this work possible, and to John
Goldsmith, Terry Regier, Anne Pycha, and
Bob Moore, whose advice and collaboration
have considerably aided the research
reported in this article. Any remaining
errors are, of course, our own.
References
Aoun, Joseph and Yen-hui Audrey Li. 1993.
The Syntax of Scope. MIT Press, Cambridge.
Aoun, Joseph and Yen-hui Audrey Li. 2000.
Scope, structure, and expert systems: A
reply to Kuno et al Language,
76(1):133?155.
Belz, Anja. 2001. Optimisation of
corpus-derived probabilistic grammars. In
Proceedings of Corpus Linguistics 2001,
pages 46?57.
Berger, Adam L., Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural
language processing. Computational
Linguistics, 22(1):39?71.
Bies, Ann, Mark Ferguson, Karen Katz, and
Robert MacIntyre. 1995. Bracketing
guidelines for Treebank II style. Technical
report, Penn Treebank Project, University
of Pennsylvania.
Bishop, Christopher M. 1995. Neural
Networks for Pattern Recognition. Oxford
University Press, Oxford.
Bridle, John S. 1990. Probabilistic
interpretation of feedforward
classification network outputs with
relationships to statistical pattern
recognition. In F. Fougelman-Soulie and
J. Herault, editors, Neurocomputing?
Algorithms, Architectures, and Applications.
Springer-Verlag, Berlin, pages 227?236.
Brill, Eric. 1995. Transformation-based
error-driven learning and natural
language processing: A case study in
part-of-speech tagging. Computational
Linguistics, 21(4):543?565.
Carden, Guy. 1976. English Quantifiers:
Logical Structure and Linguistic Variation.
Academic Press, New York.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249?254.
Charniak, Eugene. 1993. Statistical language
learning. MIT Press, Cambridge.
Charniak, Eugene. 1996. Tree-bank
grammars. In AAAI/IAAI, vol. 2,
pages 1031?1036.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20:37?46.
Cooper, Robin. 1983. Quantification and
Syntactic Theory. Reidel, Dordrecht.
Fleiss, Joseph L. 1981. Statistical Methods for
Rates and Proportions. John Wiley & Sons,
New York.
Hobbs, Jerry R. and Stuart M. Shieber. 1987.
An algorithm for generating quantifier
scopings. Computational Linguistics,
13:47?63.
Hornstein, Norbert. 1995. Logical Form: From
GB to Minimalism. Blackwell, Oxford and
Cambridge.
Jelinek, Frederick. 1998. Statistical Methods for
Speech Recognition. MIT Press, Cambridge.
Jurafsky, Daniel and James H. Martin. 2000.
Speech and Language Processing. Prentice
Hall, Upper Saddle River, New Jersey.
Krippendorff, Klaus. 1980. Content Analysis:
An Introduction to Its Methodology. Sage
96
Computational Linguistics Volume 29, Number 1
Publications, Beverly Hills, California.
Krotov, Alexander, Mark Hepple, Robert J.
Gaizauskas, and Yorick Wilks. 1998.
Compacting the Penn treebank grammar.
In COLING-ACL, pages 699?703.
Kuno, Susumu, Ken-Ichi Takami, and Yuru
Wu. 1999. Quantifier scope in English,
Chinese, and Japanese. Language,
75(1):63?111.
Kuno, Susumu, Ken-Ichi Takami, and Yuru
Wu. 2001. Response to Aoun and Li.
Language, 77(1):134?143.
Manning, Christopher D. and Hinrich
Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Martin, Paul, Douglas Appelt, and
Fernando Pereira. 1986. Transportability
and generality in a natural-language
interface system. In B. J. Grosz, K. Sparck
Jones, and B. L. Webber, editors, Natural
Language Processing. Kaufmann, Los Altos,
California, pages 585?593.
May, Robert. 1985. Logical Form: Its Structure
and Derivation. MIT Press, Cambridge.
McCawley, James D. 1998. The Syntactic
Phenomena of English. University of
Chicago Press, Chicago, second edition.
Mitchell, Tom M. 1996. Machine Learning.
McGraw Hill, New York.
Montague, Richard. 1973. The proper
treatment of quantification in ordinary
English. In J. Hintikka et al, editors,
Approaches to Natural Language. Reidel,
Dordrecht, pages 221?242.
Moran, Douglas B. 1988. Quantifier scoping
in the SRI core language engine. In
Proceedings of the 26th Annual Meeting of the
Association for Computational Linguistics
(ACL?88), pages 33?40.
Moran, Douglas B. and Fernando C. N.
Pereira. 1992. Quantifier scoping. In
Hiyan Alshawi, editor, The Core Language
Engine. MIT Press, Cambridge,
pages 149?172.
Nerbonne, John. 1993. A feature-based
syntax/semantics interface. In
A. Manaster-Ramer and W. Zadrozsny,
editors, Annals of Mathematics and Artificial
Intelligence (Special Issue on Mathematics of
Language), 8(1?2):107?132. Also published
as DFKI Research Report RR-92-42.
Park, Jong C. 1995. Quantifier scope and
constituency. In Proceedings of the 33rd
Annual Meeting of the Association for
Computational Linguistics (ACL?95),
pages 205?212.
Pedersen, Ted. 2000. A simple approach to
building ensembles of na??ve Bayesian
classifiers for word sense disambiguation.
In Proceedings of the First Meeting of the
North American Chapter of the Association for
Computational Linguistics (NAACL 2000),
pages 63?69.
Pereira, Fernando. 1990. Categorial
semantics and scoping. Computational
Linguistics, 16(1):1?10.
Pollard, Carl. 1989. The syntax-semantics
interface in a unification-based phrase
structure grammar. In S. Busemann,
C. Hauenschild, and C. Umbach, editors,
Views of the Syntax/Semantics Interface
KIT-FAST Report 74. Technical University
of Berlin, pages 167?185.
Pollard, Carl and Eun Jung Yoo. 1998. A
unified theory of scope for quantifiers
and WH-phrases. Journal of Linguistics,
34(2):415?446.
Ratnaparkhi, Adwait. 1997. A simple
introduction to maximum entropy models
for natural language processing. Technical
Report 97-08, Institute for Research in
Cognitive Science, University of
Pennsylvania.
Santorini, Beatrice. 1990. Part-of-speech
tagging guidelines for the Penn Treebank
project. Technical Report MS-CIS-90-47,
Department of Computer and Information
Science, University of Pennsylvania.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine
learning approach to coreference
resolution of noun phrases. Computational
Linguistics, 27(4):521?544.
van Halteren, Hans, Jakub Zavrel, and
Walter Daelemans. 2001. Improving
accuracy in word class tagging through
the combination of machine learning
systems. Computational Linguistics,
27(2):199?229.
VanLehn, Kurt A. 1978. Determining the
scope of English quantifiers. Technical
Report AITR-483, Massachusetts Institute
of Technology Artificial Intelligence
Laboratory, Cambridge.
Woods, William A. 1986. Semantics and
quantification in natural language
question answering. In B. J. Grosz,
K. Sparck Jones, and B. L. Webber, editors,
Natural Language Processing. Kaufmann,
Los Altos, California, pages 205?248.
99
100
101
102
Evaluating Multiple Aspects of Coherence in Student Essays
Derrick Higgins
Educational
Testing Service
Jill Burstein
Educational
Testing Service
Daniel Marcu
University of Southern
California
/ Information Sciences
Institute
Claudia Gentile
Educational
Testing Service
Abstract
CriterionSM Online Essay Evaluation Service
includes a capability that labels sentences in
student writing with essay-based discourse el-
ements (e.g., thesis statements). We describe
a new system that enhances Criterion?s capa-
bility, by evaluating multiple aspects of co-
herence in essays. This system identifies fea-
tures of sentences based on semantic similarity
measures and discourse structure. A support
vector machine uses these features to capture
breakdowns in coherence due to relatedness
to the essay question and relatedness between
discourse elements. Intra-sentential quality is
evaluated with rule-based heuristics. Results
indicate that the system yields higher perfor-
mance than a baseline on all three aspects.
1 Overview
This work is motivated by a need for advanced discourse
analysis capabilities for writing instruction applications.
CriterionSM Online Essay Evaluation Service is an appli-
cation for writing instruction which includes a capability
to annotate sentences in student essays with discourse el-
ement labels. These labels include the categories Thesis
Statement, Main Idea, Supporting Idea, and Conclusion
(Burstein et al, 2003b). Though it accurately annotates
sentences with essay-based discourse labels, Criterion
does not provide an evaluation of the expressive quality
of the sentences that comprise a discourse segment. The
system might accurately label a student?s essay as hav-
ing all of the typically expected discourse elements: the-
sis statement, 3 main ideas, supporting evidence linked
to each main idea, and a conclusion. As teachers have
pointed out, however, an essay may have all of these or-
ganizational elements, but the quality of individual ele-
ments may need improvement.
In this paper, we present a capability that captures ex-
pressive quality of sentences in the discourse segments
of an essay. For this work, we have defined expressive
quality in terms of four aspects related to global and lo-
cal essay coherence. The first two dimensions capture
global coherence, and the latter two relate to local coher-
ence: a) relatedness to the essay question (topic), b) re-
latedness between discourse elements, c) intra-sentential
quality, and d) sentence-relatedness within a discourse
segment. Each dimension represents a different aspect
of coherence.
Essentially, the goal of the system is to be able to pre-
dict whether a sentence in a discourse segment has high
or low expressive quality with regard to a particular co-
herence dimension. We have deliberately developed an
approach to essay coherence that is comprised of multi-
ple dimensions, so that an instructional application may
provide appropriate feedback to student writers, based on
the system?s prediction of high or low for each dimen-
sion. For instance, sentences in the student?s thesis state-
ment may have a strong relationship to the essay topic,
but may have a number of serious grammatical errors that
make it hard to follow. For this student, we may want to
point out that on the one hand, the sentences in the thesis
address the topic, but the thesis statement as a discourse
segment might be more clearly stated if the grammar er-
rors were fixed. By contrast, the sentences that comprise
the student?s thesis statement may be grammatically cor-
rect, but only loosely related to the essay topic. For this
student, we would also want the system to provide ap-
propriate feedback to, so that the student could revise the
thesis statement text appropriately.
In earlier work, Foltz, Kintsch & Landauer (1998),
and Wiemer-Hastings & Graesser (2000) have devel-
oped systems that also examine coherence in student
writing. Their systems measure lexical relatedness be-
tween text segments by using vector-based similarity
between adjacent sentences. This linear approach to
similarity scoring is in line with the TextTiling scheme
(Hearst and Plaunt, 1993; Hearst, 1997), which may
be used to identify the subtopic structure of a text.
Miltsakaki and Kukich (2000) have also addressed the is-
sue of establishing the coherence of student essays, using
the Rough Shift element of Centering Theory. Again, this
previous work looks at the relatedness of adjacent text
segments, and does not explore global aspects of text co-
herence.
Hierarchical models of discourse have been applied to
the question of coherence (Mann and Thompson, 1986),
but so far these have been more useful in language gen-
eration than in determining how coherent a given text is,
or in identifying the specific problem, such as the break-
down of coherence in a document.
Our approach differs in fundamental ways from this
earlier work that deals with student writing. First, Foltz
et al (1998), Wiemer-Hastings and Graesser (2000),
and Miltsakaki and Kukich (2000) assume that text co-
herence is linear. They calculate the similarity between
adjacent segments of text. By contrast, our approach
considers the discourse structure in the text, following
Burstein et al (2003b). Our method considers sentences
with regard to their discourse segments, and how the sen-
tences relate to other text segments both inside (such as
the essay thesis) and outside (such as the essay topic) of a
document. This allows us to identify cases in which there
may be a breakdown in coherence due to more global as-
pects of essay-based discourse structure. Second, previ-
ous work has used Latent Semantic Analysis as a seman-
tic similarity measure (Landauer and Dumais, 1997). We
have adapted another vector-based method of semantic
representation: Random Indexing (Kanerva et al, 2000;
Sahlgren, 2001). Another difference between our sys-
tem and earlier systems is that we use essays manually
annotated on the four coherence dimensions to train our
system.
The final system employs a hybrid approach to classify
the first two of the four coherence dimensions with a high
or low quality rank. For these dimensions, a support vec-
tor machine is used to model features derived from Ran-
dom Indexing and from essay-based discourse structure
information. A third local coherence dimension compo-
nent is driven by rule-based heuristics. A fourth dimen-
sion related to coherence within a discourse segment can-
not be classified due to a lack of data characterizing low
expressive quality. This is fully explained later in the pa-
per.
2 Protocol Development and Human
Annotation
2.1 Protocol Development
The development of this system required a corpus of hu-
man annotated essay data for modeling purposes. In the
end, the goal is to have the system make judgments sim-
ilar to those made by a human with regard to ranking the
coherence of an essay on four dimensions. Therefore, we
created a detailed protocol for annotating the expressive
quality of essay-based discourse elements in essays with
regard to four aspects related to global and local essay
coherence. This protocol was designed for the following
purposes:
1. To yield annotations that are useful for the purpose
of providing students with feedback about the ex-
pressive relatedness of discourse elements in their
essays, given four relatedness dimensions;
2. To permit human annotators to achieve high levels
of consistency during the annotation process;
3. To produce annotations that have the potential of be-
ing derivable by computer programs through train-
ing on corpora annotated by humans.
2.1.1 Expressive Quality of Discourse Segments:
Protocol Description
According to writing experts who collaborated in this
work, the expressive relatedness of a sentence discourse
element may be characterized in terms of four dimen-
sions: a) relationship to prompt (essay question topic),
b) relationship to other discourse elements, c) relevance
with discourse segment, and d) errors in grammar, us-
age, and mechanics. For the sake of brevity, we refer to
these four dimensions as DimP (relatedness to prompt),
DimT (typically, relatedness to thesis), DimS (related-
ness within a discourse segment), and DimERR.
The two annotators were required to label each sen-
tence of an essay for expressive quality on the four di-
mensions (above). For the 989 essays used in this study,
each sentence had already been manually annotated with
these discourse labels: background material, thesis, main
idea, supporting idea, and conclusion (Burstein et al,
2003b).1 An assignment of high (1) or low (0) was given
to each sentence, on the dimensions relevant to the dis-
course element. Not all dimensions apply to all discourse
elements. The protocol is extremely specific as to how
annotators should label the expressive quality for each
sentence in a discourse element with regard to the four
dimensions. In this paper, we provide a brief description
of the labeling protocol, so that the purpose of each di-
mension is clear.
Figure 1 shows a sample essay and prompt. A hu-
man judge has assigned a label to each sentence in the
essay, resulting in the illustrated division into discourse
segments. In addition, the figure indicates human annota-
tors? ratings for two of our coherence dimensions (DimP
and DimT , discussed below). By and large, the essay
consistently follows up on the ideas of the essay thesis,
and so most sentences get a high relatedness score on
DimT . However, much of the essay fails to directly ad-
dress the question posed in the essay prompt, and so many
sentences are assigned low relatedness on DimP .
Dimension 1: DimP (Relatedness to Prompt)
The text of the discourse element and the prompt (text
of the essay question) must be related. Specifically, the
thesis statement, main ideas, and conclusion statement
should all contain text that is strongly related to the essay
topic. If this relationship does not exist, this is perhaps
evidence that the student has written an off-topic essay.
For this dimension, a high rank is assigned to each sen-
tence from background material, thesis, main idea and
conclusion statement that is related to the prompt text;
otherwise a low rank is assigned.
1The annotated data from the Burstein et al (2003b) study
were used to develop a commercial application that automati-
cally assigns these discourse labels to student essays.
Discourse Sentence DimP DimT
Segment
Prompt Images of beauty?both male and female?are promoted in magazines, in movies, on
billboards, and on television. Explain the extent to which you think these images can
be beneficial or harmful.
Background
A lot of people really care about how they look or how other people look. Low High
A lot of people like reading magazines or watch t.v about how you can fix your looks if
you don?t like the way your looks are. High High
Thesis
People that care about how they look is because they have problems at home, their parents
don?t pay attention to them or even that they have a high self-steem which that is not good. Low N/A
A lot of people get to the extent of killing themselfs just because they?re not happy with
there looks. Low N/A
Support Many people go thru make-overs to experiment how they will look but, some people stilldon?t like themself. N/A High
Main Point
The people that don?t like themselfs need some helps and they probably feel like that be-
cause they have told them oh! your ugly , you look like Blank! or maybe a guy never ask a
her out.
Low Low
Support
In case of a guy probably the same comments but he won?t dare to ask a girl out because
he feels that the girl is going to say no because of the way he looks. N/A High
Things like this make people don?t like each other. N/A High
Conclusion I suggest that a those people out here that are not happy with their looks get some help. Low HighTheirs alot of programs that you can get help. Low Low
Figure 1: Student essay with discourse segments and two coherence dimensions as annotated by human judge
Dimension 2: DimT (Relatedness to Thesis)
The relationship between a discourse element and
other discourse elements in the text governs the global
coherence of the essay text. For a text to hold together,
certain discourse elements must be related or the text will
appear choppy and will be difficult to follow. Specifi-
cally, a high rank is assigned to each sentence in the back-
ground material, main ideas and conclusion that is related
to the thesis, and supporting idea sentences that relate to
the relevant main idea. A conclusion sentence may also
be given a high rank if it is related to a main idea or back-
ground information. Low ranks are assigned to sentences
that do not have these relationships.
Dimension 3: DimS (Relatedness within Segment)
This dimension indicates the cohesiveness of the mul-
tiple sentences in a discourse segment of a text. This
dimension distinguishes a text segment that may go off
task within a discourse segment. For this dimension, a
high rank was assigned to each sentence in a discourse
segment that related to at least one other sentence in the
segment; otherwise the sentence received a low rank. If
the discourse segment contained only one sentence, then
the DimT label was assigned as the default.
Dimension 4: DimERR (Technical Errors)
Dimension 4 measures a sentence?s relatedness of ex-
pression with regard to grammar, mechanics and word
usage. More specifically, a sentence is considered to be
low on this dimension if it contains frequent patterns of
error, defined as follows: (a) contains 2 errors in gram-
mar, word usage or mechanics (i.e., spelling, capitaliza-
tion or punctuation), (b) is an incomplete sentence, or (c)
is a run-on sentence (i.e., 4 or more independent clauses
within a sentence).
2.2 Topics, Human Annotation, and Human
Agreement
2.2.1 Topics & Writing Genre
Essays written to two genres were used: five of the top-
ics were persuasive, and one was expository. Persuasive
writing requires the reader to state an opinion on a par-
ticular topic, support the stated opinion, and convince the
reader that the perspective is valid and well-supported.
An expository topic requires the writer only to state an
opinion on a topic. This typically elicits more personal
and descriptive writing. Four of the five sets of persua-
sive essay responses were written by college freshman,
and the fifth by 12th graders. The set of expository re-
sponses were also written by 12th graders.
2.2.2 Human Annotation
Two human judges participated in this study. The
judges were instructed to assign relevant dimension la-
bels to each sentence. Pre-training of the judges was done
using a set of approximately 50 essays across the six top-
ics in the study. During this phase, the authors and the
judges discussed and labeled the essays together. During
the next training phase, the judges labeled a total of 292
essays across six topics. They labeled the identical set of
essays, and were allowed to discuss their decisions. In the
next annotation phase, the judges did not discuss their an-
notations. In this post-training phase (annotation phase),
each judge labeled an average of about 278 unique es-
says for each of four prompts (556 essays together). Each
judge also labeled an additional set of 141 essays that was
overlapping. So, about 20 percent of the data annotated
by each judge in the annotation phase was overlapping,
Agreement ?
DimP (N=779) 99% .99
DimT (N=1890) 100% .99
DimS (N=2119) 100% .99
DimERR (N=2170) 99% .98
Table 1: Annotator agreement across coherence
dimensions?data from annotation phase
and 80 percent was unique. The 20 percent is used to ob-
tain human agreement.2 During both the training and an-
notation phases, Kappa statistics were run on their judg-
ments regularly, and if the Kappa for any particular cate-
gory fell below 0.8, then the judges were asked to review
the protocol until their agreement was acceptable. At the
end of the annotation phase, we had a total of 989 labeled
essays: 292 (training phase) + 278 ? 2 (unique essays
from annotator 1 + annotator 2, annotation phase) + 141
(overlapping set, annotation phase).
Human Judge Agreement
It is critical that the annotation process yields agree-
ment that is high enough between human judges, such
that it suggests that people can agree on how to categorize
the discourse elements. As is stated in the above section,
during the training of the judges for this study, Kappa
statistics were computed on a regular basis. Kappa be-
tween the judges for each category had to be maintained
at least 0.8, since this is believed to represent strong
agreement (Krippendorff, 1980). In Table 1 we report
human agreement for overlapping data from the four top-
ics on all four dimensions. Clearly, the level of human
agreement is quite high across all four coherence dimen-
sions. In addition, if we look at kappas of sentences based
on discourse category, no kappa falls below 0.9.
3 Method
Our final system uses a hybrid approach to label three of
the four coherence dimensions. For DimP and DimT ,
assigning coherence judgments to sentences in an essay
proceeds in three stages 1) identifying the discourse label
associated with each sentence in an essay, 2) computing
features that quantify the semantic similarity between dif-
ferent discourse segments of the essay, and 3) applying a
classifier to make a coherence judgment on a dimension.
Consistent with the human annotated data, a coherence
judgment on any dimension is either ?high? or ?low.? The
method for DimERR is rule-based, and is discussed later.
3.1 Discourse element feature identification
As noted earlier, the two human judges in this study anno-
tated the four coherence dimensions according to the hu-
2For the annotation phase, we were unable to collect data
for two essay prompts because of our annotators? availability.
This means that we only have inter-annotator agreement statis-
tics on 4 prompts, although some data from all six prompts was
available for training and testing our models (with the extra two
prompts being represented in the training phase of annotation).
man discourse label assignments. Accordingly, we also
used the human assigned discourse labels as features for
predicting coherence judgments. In a deployed system,
however, we would use discourse element labels gener-
ated from Criterion?s discourse analysis system (Burstein
et al, 2003b). Further evaluation is, of course, necessary
in order to determine the effect of using these automat-
ically assigned labels in place of the gold standard dis-
course labels.
3.2 Semantic similarity features
Given the partition of an essay into discourse segments,
we then derive a set of features from the essay in order
to predict how closely related each sentence is to various
important text segments, such as the essay topic, and dis-
course elements, such as thesis statement. As described
in Section 4, the features that are most useful for clas-
sifying sentences according to coherence are semantic
similarity features derived from Random Indexing (Kan-
erva et al, 2000; Sahlgren, 2001). Random Indexing is
a vector-based semantic representation system similar to
Latent Semantic Analysis. Our Random Indexing (RI)
semantic space is trained on about 30 million words of
newswire text.
When we extract a feature such as ?RI similarity to
prompt? for a sentence, this essentially measures to what
extent the sentence contains terms in the same semantic
domain as compared to those found in the prompt. Within
any discourse segment, any semantic information that is
word-order dependent is lost.
3.3 Support vector classification
Finally, for each sentence in the essay we use the fea-
tures derived from the essay to make a determination as
to whether it meets our criteria for coherence in these
dimensions (DimP and DimT ). To make this determi-
nation, we use a support vector machine (SVM) classi-
fier (Vapnik, 1995; Christianini and Shawe-Taylor, 2000).
Specifically, we use an SVM with a radial basis function
kernel, which exhibited good performance on a subset of
about 30 essays from the pre-training data.
4 Results
In each of the experiments below, the results are re-
ported for the entire set of 989 essays annotated for this
project. We performed ten-fold cross-validation, training
our SVM classifier on 910 of the data at a time, and testing
on the remaining 110 . We report the results on the cross-
validation set for all runs combined.
For each dimension, we also report the performance
of a simple baseline measure, which assumes that all of
our essay coherence criteria are satisfied. That is, our
baseline assigns category 1 (high relevance) to every
sentence, on every dimension.
These essays were written in response to six different
prompts, and had an average (human-assigned) score of
Score DimP DimT DimS DimERR
1?2 64.1% 71.2% 94.8% 61.1%
5?6 72.0% 70.9% 97.2% 92.9%
Table 2: Baseline performance on each coherence dimen-
sion, broken down by essay score point
4.0 on a six-point scale. Therefore, a priori, it seems pos-
sible that we could build a better baseline model by con-
ditioning its predictions on the overall score of the essay
(assigning 1?s to sentences from better-scoring essays,
and 0?s to sentences from lower-scoring essays). How-
ever, the coherence requirements of each of our dimen-
sions are usually met even in the lowest-scoring essays,
as shown in Table 2, which lists the percentage of sen-
tences in different essay score ranges which our human
annotators assigned category 1. Looking at the highest
and lowest score points on our six-point scale, it is clear
that higher-scoring essays do tend to have fewer problems
with coherence, but this effect is not overwhelming. (The
largest gap between the highest- and lowest-scoring es-
says is on DimERR, which deals with errors in grammar,
usage, and mechanics.)
4.1 DimP
According to the protocol, there are four discourse ele-
ments for which DimP , the degree of relatedness to the
essay prompt, is relevant: Background, Conclusion, Main
Point, and Thesis. The Supporting Idea category of sen-
tence is not required to be related to the prompt, because
it may express an elaboration of one of the main points of
the essay, and has a more tenuous and mediated logical
connection to the essay prompt text.
The features which we provide to the SVM for predict-
ing a sentence?s relatedness to the prompt are:
1. The RI similarity score of the target sentence with
the entire essay prompt,
2. The maximum RI similarity score of the target sen-
tence with any sentence in the essay prompt,
3. The RI similarity score of the target sentence with
the required task sentence (a designated portion of
the prompt text which contains an explicit directive
to the student to write about a specific topic),
4. The RI similarity score of the target sentence with
the entire thesis of the essay,
5. The maximum RI similarity score of the target sen-
tence with any sentence in the thesis,
6. The maximum RI similarity score of the target sen-
tence with any sentence in the preceding discourse
chunk,
7. The number of sentences in the current chunk,
8. The offset of the target sentence (sentence number)
from the beginning of the current discourse chunk,
9. The number of sentences in the current chunk whose
similarity with the prompt is greater than .2,
10. The number of sentences in the current chunk whose
similarity with the required task sentence is greater
than .2,
11. The number of sentences in the current chunk whose
similarity with the essay thesis is greater than .2,
12. The number of sentences in the current chunk whose
similarity with the prompt is greater than .4,
13. The number of sentences in the current chunk whose
similarity with the required task sentence is greater
than .4,
14. The number of sentences in the current chunk whose
similarity with the essay thesis is greater than .4,
15. The length of the target sentence in words,
16. A Boolean feature indicating whether the target sen-
tence contains a transition word, such as ?however?,
or ?although?,
17. A Boolean feature indicating whether the target sen-
tence contains an anaphoric element, and
18. The category of the current chunk. (This is encoded
as five Boolean features: one bit for each of ?Back-
ground?, ?Conclusion?, ?Main Point?, ?Supporting
Idea?, and ?Thesis?.)
In calculating features 2, 5, and 6, we use the maximum
similarity score of the sentence with any other sentence in
the relevant discourse segment, rather than simply using
the similarity score of the sentence with the entire text
chunk. We add this feature based on the intuition that for
a sentence to be relevant to another discourse segment, it
need only be connected to some part of that segment.
It is perhaps surprising that we include features which
measure the degree of similarity between the sentence
and the thesis, since we are trying to predict its related-
ness to the prompt, rather than the thesis. However, there
are two reasons we believe this is fruitful. First, since we
are dealing with a relatively small amount of text, com-
paring a single sentence to a short essay prompt, looking
at the thesis as well helps to overcome data sparsity is-
sues. Second, it may be that the relevance of the current
sentence to the prompt is mediated by the student?s thesis
statement. For example, the prompt may ask the student
to take a position on some topic. They may state this po-
sition in the thesis, and provide an example to support it
as one of their Main Points. In such a case, the example
would be more clearly linked to the Thesis, but this would
suffice for it to be related to the prompt.
Considering the similarity scores of sentences in the
current discourse segment is also, in part, an attempt to
overcome data sparsity issues, but is also motivated by
the idea that it may be an entire discourse segment which
can properly be said to be (ir)relevant to the essay prompt.
The sentence length and transition word features do
not directly reflect the relatedness of a sentence to the
prompt, but they are likely to be useful correlates.
Finally, the feature (#17) indicating the presence of
a pronoun is to help the system deal with cases in
which a sentence contains very few content words, but
is still linked to other material in the essay by means of
anaphoric elements, such as ?This is shown by my argu-
ment.? In such as case, the sentence would normally get
a low similarity score with the prompt (and other parts of
the essay), but the information that it contains a pronoun
might still allow the system to classify it correctly.
Table 3 shows results using the baseline algorithm to
classify sentences according to their relatedness to the
prompt. Table 4 presents the results using the SVM clas-
sifier. We provide precision, recall, and f-measure for the
assignment of the labels 1 and 0, and an overall accuracy
measure in the far right column. (The accuracy measure
is the value for precision and recall when 1 and 0 ranks
are collapsed. Precision and recall will be the same, since
the number of labels assigned by the model is equal to the
number of labels in the target assignment.)
The SVM model outperforms the baseline on every
subcategory, with the largest gains on Background sen-
tences, most of which are, in fact, unrelated to the prompt
according to our human judges. This low baseline result
on Background sentences could indicate that many stu-
dents have a problem with providing unnecessary and ir-
relevant prefaces to the important points in their essays.
Note that the trained SVM has around .9 recall on the
class of sentences which according to our human annota-
tors have high relevance to the prompt. This means that
our system is less likely to incorrectly assign a low rank
to a sentence that is high. So, the system will tend to err
on the side of the student, which is a preferable trade-off.
In part, this is due to the nature of the semantic similarity
measure we are using, which does not take word order
into account. While RI does allow us to capture a richer
meaning component than simply matching words which
co-occur in the target sentence and prompt, it still does
not encompass all that goes into determining whether a
sentence ?relates? to another chunk of text. Students of-
ten write something which bears a loose topical connec-
tion with the essay prompt, but does not directly address
the question. This sort of problem is hard to address with
a tool such as LSA or RI; the vocabulary of the sentence
on its own will not provide a clue to the sentence?s failure
to address the task.
4.2 DimT
The annotation protocol states that these four discourse
elements come into play for DimT : Background, Con-
clusion, Main Point, and Supporting Idea. Because this
dimension indicates the degree of relatedness to the the-
sis of the essay (and also other discourse segments in the
case of Supporting Idea and Conclusion sentences; see
Section 2.1.1 above), we do not consider thesis sentences
with regard to this aspect of coherence.
The features which we provide to the SVM for pre-
dicting whether or not a given sentence is related to the
thesis are almost the same ones used for DimP . The only
difference is that we omit features #12 and #13 in our
model of DimT . These are the features which evaluate
how many sentences in the current chunk have a simi-
larity score with the prompt and required task sentence
greater than 0.4. While DimP is to some degree sensitive
to the similarity of a sentence to the thesis, and DimT can
likewise benefit from the information about a sentence?s
similarity to the prompt, it seems that the latter link is less
important, so a single cutoff suffices for this model.
Tables 5?6 present the results for our SVM model and
for a baseline which assigns all sentences ?high? rele-
vance. The improvements on DimT are smaller than the
ones reported for DimP , but we still record an overall
gain of four percentage points in accuracy. Only on con-
clusion sentences were we unable to produce an improve-
ment over the baseline; we need to investigate this further.
Again, the system achieves high recall on sentences
with high relatedness. It outperforms the baseline by cor-
rectly identifying a modest percentage of the sentences
labeled as having low relatedness with the thesis.
4.3 DimS
DimS , which concerns whether the target sentence re-
lates to another sentence within the same discourse seg-
ment, seems another good candidate for applying our se-
mantic similarity score to the task of establishing coher-
ence. At present, however we have not made substan-
tial progress on this task. The baselines for DimS are
substantially higher than those for dimensions DimP and
DimT ? 98.1% of all sentences in our data were anno-
tated as ?highly related? with respect to this dimension.
This indicates that it is relatively rare to find a sentence
which is not related to anything in the same discourse
segment. This makes our task, to characterize those sen-
tences which are not related to the discourse segment,
much more difficult, since there are so few examples of
sentences with low-ranking coherence.
4.4 DimERR
DimERR is clearly a different kind of problem. Here, we
are looking for clarity of expression, or coherence within
a sentence. We base this solely on technical correctness.
We are able to automatically assign high and low ranks to
essay sentences using a set of rules based on the number
of grammar, usage and mechanics errors. The rules used
for DimERR are as follows: a) assign a low label if the
sentence is a fragment, if the sentence contains 2 or more
grammar, usage, and mechanics errors, or if the sentence
is a run-on, b) assign a high label if no criteria in (a) apply.
Criterion?s discourse analysis system also provides
an essay score with e-rater?, and qualitative feedback
about grammar, usage, mechanics, and style (Leacock
High Low Total
Precision Recall F-measure Precision Recall F-measure Accuracy
Background (N = 1077) 0.486 1.000 0.654 0.000 0.000 0.000 0.486
Conclusion (N = 1830) 0.757 1.000 0.862 0.000 0.000 0.000 0.757
Main Point (N = 1566) 0.663 1.000 0.797 0.000 0.000 0.000 0.663
Thesis (N = 1899) 0.712 1.000 0.832 0.000 0.000 0.000 0.712
All sentence types (N = 6372) 0.675 1.000 0.806 0.000 0.000 0.000 0.675
Table 3: Baseline performance on DimP
High Low Total
Precision Recall F-measure Precision Recall F-measure Accuracy
Background (N = 1077) 0.714 0.702 0.708 0.723 0.735 0.729 0.719
Conclusion (N = 1830) 0.784 0.959 0.863 0.578 0.175 0.269 0.768
Main Point (N = 1566) 0.729 0.888 0.801 0.616 0.352 0.448 0.708
Thesis (N = 1899) 0.771 0.929 0.843 0.644 0.318 0.426 0.753
All sentence types (N = 6372) 0.759 0.901 0.824 0.665 0.407 0.505 0.740
Table 4: SVM performance on DimP
High Low Total
Precision Recall F-measure Precision Recall F-measure Accuracy
Background (N = 1060) 0.793 1.000 0.885 0.000 0.000 0.000 0.793
Conclusion (N = 1829) 0.834 1.000 0.909 0.000 0.000 0.000 0.834
Main Point (N = 1556) 0.742 1.000 0.852 0.000 0.000 0.000 0.742
Support (N = 10332) 0.664 1.000 0.798 0.000 0.000 0.000 0.664
All sentence types (N = 14777) 0.702 1.000 0.825 0.000 0.000 0.000 0.702
Table 5: Baseline performance on DimT
High Low Total
Precision Recall F-measure Precision Recall F-measure Accuracy
Background (N = 1060) 0.856 0.980 0.914 0.827 0.368 0.509 0.853
Conclusion (N = 1829) 0.834 1.000 0.910 0.000 0.000 0.000 0.834
Main Point (N = 1556) 0.776 0.997 0.873 0.958 0.172 0.292 0.785
Support (N = 10332) 0.709 0.945 0.810 0.684 0.237 0.352 0.706
All sentence types (N = 14777) 0.744 0.962 0.839 0.709 0.221 0.337 0.741
Table 6: SVM performance on DimT
and Chodorow, 2000; Burstein et al, 2003a). We can
easily use Criterion?s outputs about grammar, usage, and
mechanics errors to assign high and low ranks to essay
sentences, using the rules described in the previous sec-
tion.
The performance of the module that does the DimERR
assignments is in Table 7. We used half of the 292 essays
from the training phase of annotation for development,
and the remaining data from the training and post-training
phases of annotation for cross-validation. Results are re-
ported for the cross-validation set. Text labeled as titles,
or opening or closing salutations, are not included in the
results. The baselines were computed by assigning all
sentences a high rank label. The baseline is high; how-
ever, the algorithm outperforms the baseline.
5 Discussion and Conclusions
There were multiple goals in this work. We wanted to in-
troduce a concept of essay coherence comprising multi-
ple aspects, and investigate what linguistic features drive
each aspect in student essay writing. Further, we wanted
Sentence N Precision Recall F-measure
Baseline
High 11789 0.83 1.00 0.91
Low 2351 0.00 0.00 0.00
Overall 14140 0.83 0.83 0.83
Algorithm
High 11789 0.88 0.96 0.92
Low 2351 0.63 0.34 0.44
Overall 14140 0.86 0.86 0.86
Table 7: Performance on DimERR
to build a system to automatically evaluate these multiple
aspects of coherence, so that appropriate feedback can be
provided through a writing instruction application.
To accomplish these goals, we have worked with writ-
ing experts to develop a comprehensive protocol that de-
tails how coherence in writing can be evaluated, either
manually or automatically. Using this protocol, human
annotators labeled a corpus of student essays, using the
coherence dimensions. These annotations built on a pre-
vious set of annotations for these data, whereby discourse
element labels were assigned. The result is a richly anno-
tated data set with information about discourse elements,
as well as their coherence in the context of the discourse
structure. Using this data set, we were able to learn what
linguistic features can be used to evaluate various aspects
of coherence in student writing. We then developed a
prototype system that ranks global and local aspects of
coherence in an essay. This capability shows promise in
ranking three aspects of coherence in essays: a) relation-
ship to essay topic, b) relationship between discourse ele-
ments, and c) intra-sentential technical quality. More low
ranking data on a fourth dimension, coherence within a
discourse segment, needs to be identified and annotated
before this dimension can be modeled.
The approach used is innovative, since it moves beyond
earlier methods of evaluating coherence in student writ-
ing that capture only local information between adjacent
sentences. Two methods are used to model the aspects
of coherence handled by the system. For the two global
coherence dimensions, DimP and DimT , a support vec-
tor machine provides a coherence ranking of sentences
based on features related to essay-based discourse infor-
mation, and semantic similarity values derived from the
RI algorithm. Using this classification method, we are
able to rank the expressive quality of sentences in essay-
based discourse segments, with regard to relatedness to
the text of the prompt, and also as they relate to the thesis
statement. With regard to the local coherence dimension,
DimERR, we use a rule-based heuristic to rank intra-
sentential quality. This addresses the issue of sentences in
essays that have serious grammatical problems that may
interfere with a reader?s comprehension. We take advan-
tage of Criterion?s identification of grammar, usage, and
mechanics errors to design the rules for ranking this local
coherence dimension.
We hope that in further investigation of this richly an-
notated data set, we will be able to build on the current
prototype and develop a full-scale writing instruction ca-
pability that provides feedback on the coherence dimen-
sions described in this paper.
Acknowledgements
We would like to thank Irma Lorenz and Shauna Cooper
for advice on protocol development and for the annota-
tion work, and Martin Chodorow for discussions about
Random Indexing. We thank the anonymous reviewers
for their helpful feedback.
Any opinions expressed here are those of the authors
and not necessarily of the Educational Testing Service.
References
Jill Burstein, Martin Chodorow, and Claudia Leacock.
2003a. CriterionSM: Online essay evaluation: An ap-
plication for automated evaluation of student essays.
In Proceedings of the Fifteenth Annual Conference on
Innovative Applications of Artificial Intelligence, Aca-
pulco, Mexico.
Jill Burstein, Daniel Marcu, and Kevin Knight. 2003b.
Finding the WRITE stuff: Automatic identification of
discourse structure in student essays. IEEE Trans-
actions on Intelligent Systems: Special Issue on Ad-
vances in Natural Language Processing, 181:32?39.
Nello Christianini and John Shawe-Taylor. 2000. Sup-
port Vector Machines and other Kernel-based Learn-
ing Methods. Cambridge University Press, Cam-
bridge, UK.
Peter Foltz, Walter Kintsch, and Thomas K. Landauer.
1998. The measurement of textual coherence with
Latent Semantic Analysis. Discourse Processes,
25(2&3):285?307.
Marti A. Hearst and Christian Plaunt. 1993. Subtopic
structuring for full-length document access. In Pro-
ceedings of ACM SIGIR, pages 59?68.
Marti A. Hearst. 1997. TextTiling: Segmenting text
into multi-paragraph subtopic passages. Computa-
tional Linguistics, 23(1):33?64.
P. Kanerva, J. Kristoferson, and A. Holst. 2000. Random
indexing of text samples for Latent Semantic Analysis.
In L. R. Gleitman and A. K. Josh, editors, Proc. 22nd
Annual Conference of the Cognitive Science Society.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology. Sage Publications.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato?s problem: The Latent Semantic Analy-
sis theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104:211?240.
Claudia Leacock and Martin Chodorow. 2000. An unsu-
pervised method for detecting grammatical errors. In
Proceedings of NAACL 2000, pages 140?147.
William Mann and Sandra Thompson. 1986. Relational
processes in discourse. Discourse Processes, 9:57?90.
Eleni Miltsakaki and Karen Kukich. 2000. Automated
evaluation of coherence in student essays. In Proceed-
ings of LREC 2000, Athens, Greece.
Magnus Sahlgren. 2001. Vector based semantic analy-
sis: Representing word meanings based on random la-
bels. In Proceedings of the ESSLLI 2001 Workshop on
Semantic Knowledge Acquisition and Categorisation.
Helsinki, Finland.
Vladimir Vapnik. 1995. The Nature of Statistical Learn-
ing Theory. Springer Verlag, New York.
Peter Wiemer-Hastings and Arthur Graesser. 2000.
Select-a-Kibitzer: A computer tool that gives mean-
ingful feedback on student compositions. Interactive
Learning Environments, 8(2):149?169.
Unsupervised Learning of Bulgarian POS Tags
Derrick Higgins
Educational Testing Service
dchiggin@alumni.uchicago.edu
Abstract
This paper presents an approach to the
unsupervised learning of parts of speech
which uses both morphological and syn-
tactic information.
While the model is more complex than
those which have been employed for un-
supervised learning of POS tags in En-
glish, which use only syntactic infor-
mation, the variety of languages in the
world requires that we consider mor-
phology as well. In many languages,
morphology provides better clues to a
word?s category than word order.
We present the computational model for
POS learning, and present results for ap-
plying it to Bulgarian, a Slavic language
with relatively free word order and rich
morphology.
1 Preliminaries
In designing a model to induce parts of speech
(POS categories) from a corpus, the first question
which arises is exactly what sort of entities the
target categories are. Depending on exactly how
these categories are defined, and which words are
taken to be members of each, different sorts of lin-
guistic information will clearly be relevant to their
identification.
For concreteness, we will be concerned with
the part-of-speech categories used in tagging
electronic texts such as the Bulgarian Treebank
(Simov et al, 2002). Since the goal of this paper
is to devise a model which will induce POS cate-
gories automatically from an untagged text, with
no prior knowledge of the structure of the lan-
guage, we will be using these tagged corpora as a
gold standard to evaluate the performance of com-
peting models.
2 Previous approaches
While this study is unique in attempting to in-
corporate both syntactic and morphological fac-
tors, previous work by other researchers has ex-
plored unsupervised methods of deriving clusters
of words based on their linguistic behavior.
(Brown et al, 1992) is one of the first works
to use statistical methods of distributional analysis
to induce clusters of words. These authors define
an initial, very fine categorization of the vocabu-
lary of a corpus, in which each word is the sole
member of its own category, and then iteratively
merge these word classes until the desired level
of granularity is achieved. The objective func-
tion which they use to determine the optimal set
of word classes
 
for a corpus is the inter-class
mutual information between adjacent words in the
corpus. Since there is no practical way of deter-
mining the classification
 
which maximizes this
quantity for a given corpus, (Brown et al, 1992)
use a greedy algorithm which proceeds from the
initial classification, performing the merge which
results in the least loss in mutual information at
each stage. (Lee, 1997) pursues a similar ap-
proach in clustering nouns which occur as direct
objects to verbs, but uses a soft clustering algo-
rithm in place of the agglomerative clustering al-
gorithm used by Brown et al, and Lee uses the KL
divergence between the nouns? distributions as a
measure of closeness, rather than the loss in inter-
class mutual information. (McMahon and Smith,
1996) employ a similar algorithm to that of Brown
et al, but use a top-down search in determining
word clusters, rather than a bottom-up one.
A number of other studies have attempted to use
distributional analysis to derive POS categories.
(Brill et al, 1990) use an ad-hoc similarity met-
ric to cluster words into POS-like classes, but the
problem is significantly simplified by their pre-
processing of the data to replace infrequent open-
class words with their correct POS tags. Finch
& Chater (1994) describe a model based on clus-
tering words in a vector space derived from their
corpus contexts, but perform this analysis only
for 1,000?2,000 common words in their USENET
corpus. Hinrich Schu?tze (1995) presents perhaps
the most sophisticated model of word clustering
for POS identification. Schu?tze first constructs
a context vector to represent each word?s co-
occurrence properties, and then trains a recurrent
neural network to predict the word?s location in the
space based on the context vectors for surrounding
words. The output vectors of the network are then
clustered to produce POS-like classes. This model
architecture, which classifies a word in context, al-
lows the same word to be tagged differently, de-
pending on how it is used.
3 Model components
The approach to the identification of POS cate-
gories which we pursue in this paper attempts to
incrementally home in on an optimal set of cat-
egories through the incorporation of morpholog-
ical information and local syntactic information.
The procedure is uses gradient descent training of
a hidden neural network model, including an em-
bedded morphological model based on informa-
tion from the Linguistica (Goldsmith, 2001) en-
gine for unsupervised morphological analysis. Be-
cause this morphological information is the output
of a completely unsupervised process of induction,
our model of POS category induction is also an un-
supervised one.
In the following subsections, we provide a short
summary of each of these components of our
model of part-of-speech learning, and in Section
4, we present the results of testing our model on a
tagged Bulgarian corpus.
3.1 Hidden neural networks
The model which we use for inducing clusters of
word tokens in a corpus (which ought to corre-
spond to parts of speech) is actually a generaliza-
tion of a hidden Markov model, called a hidden
neural network (HNN) (Baldi and Chauvin, 1996;
Riis, 1998). Each state in the HNN corresponds to
a single word cluster, and the category to which a
word belongs is determined by Viterbi decoding.
In a hidden neural network, either the transition
probabilities, or the observation probabilities, or
both, may be replaced with a feed-forward neural
network which computes these values on the fly,
possibly as a function of information from else-
where in the observation sequence. This gives an
HNN considerably more expressive power than an
HMM, because it is not tied strictly to the inde-
pendence assumptions which are inherent in the
architecture of a hidden Markov model.
Gradient descent training of the embedded net-
works and HNN parameters is entirely straight-
forward, and is described by (Baldi and Chauvin,
1996).
3.2 Linguistica
While the syntactic information in our model is
derived from the state transition and observa-
tion parameters through HNN training, the mor-
phological information available to our model of
POS category induction is provided by the Lin-
guistica (Goldsmith, 2001) system for unsuper-
vised morphological learning. Linguistica apples
a minimum-description length criterion to induce
the morphological categories of a language, such
as stems and suffixes, from an unlabeled corpus
of text from that language. It is important that
the morphological analysis which Linguistica pro-
vides is not informed by prior knowledge of the
language, since this allows our method to remain
an unsupervised approach to POS learning.
In Goldsmith?s framework, a morphological
analysis consists of three primary components: a
list of stems, a list of suffixes, and a list of signa-
tures. A signature, in Goldsmith?s conception, is
similar to the notion of a morphological paradigm,
but more general, and automatically determined
from an analysis of a corpus. A stem?s signature
consists of a list of all of the suffixes which may
occur with that stem. Table 1 illustrates some of
the highest-ranked signatures found in Linguistica
analyses of text from Bulgarian. The occurrence
of the string ?NULL? in a signature indicates that
the stem may also occur with no suffix.
Notable in these signatures is the identification
of the gender-markings         .
Table 1: Top-ranked signatures found by Linguis-
tica, for Bulgarian
Signature Exemplars
 
.  . 
 

 	
 


 



, A transformation-based approach to argument labeling
Derrick Higgins
Educational Testing Service
Mail Stop 12-R
Rosedale Road
Princeton, NJ 08541
dhiggins@ets.org
Abstract
This paper presents the results of applying
transformation-based learning (TBL) to the
problem of semantic role labeling. The great
advantage of the TBL paradigm is that it pro-
vides a simple learning framework in which the
parallel tasks of argument identification and ar-
gument labeling can mutually influence one an-
other. Semantic role labeling nevertheless dif-
fers from other tasks in which TBL has been
successfully applied, such as part-of-speech
tagging and named-entity recognition, because
of the large span of some arguments, the de-
pendence of argument labels on global infor-
mation, and the fact that core argument labels
are largely arbitrary. Consequently, some care
is needed in posing the task in a TBL frame-
work.
1 Overview
In the closed challenge of the CoNLL shared task, the
system is charged with both identifying argument bound-
aries, and correctly labeling the arguments with the cor-
rect semantic role, without using a parser to suggest
candidate phrases. Transformation-based learning (Brill,
1995) is well-suited to simultaneously addressing this
dual task of identifying and labeling semantic arguments
of a predicate, because it allows intermediate hypothe-
ses to influence the ultimate decisions made. More con-
cretely, the category of an argument may decisively in-
fluence how the system places its boundaries, and con-
versely, the shape of an argument is an important factor
in predicting its category.
We treat the task as a word-by-word tagging problem,
using a variant of the IOB2 labeling scheme.
2 Transformation-based learning
TBL is a general machine learning tool for assigning
classes to a sequence of observations. TBL induces a
set of transformational rules, which apply in sequence to
change the class assigned to observations which meet the
rules? conditions.
We use the software package fnTBL to design
the model described here. This package, and the
TBL framework itself, are described in detail by
Ngai and Florian (2001).
3 Task Definition
Defining the task of semantic role labeling in TBL terms
requires four basic steps. First, the problem has to be re-
duced to that of assigning an appropriate tag to each word
in a sentence. Second, we must define the features asso-
ciated with each word in the sentence, on which the trans-
formational rules will operate. Third, we must decide on
the exact forms the transformational rules will be allowed
to take (the rule templates). Finally, we must determine
a mapping from our word-by-word tag assignment to the
labeled bracketing used to identify semantic arguments in
the test data. Each of these steps is addressed below.
3.1 Tagging scheme
The simplest way of representing the chunks of text
which correspond to semantic arguments is to use
some variant of the IOB tagging scheme (Sang and
Veenstra, 1999). This is the approach taken by
Hacioglu et al (2003), who apply the IOB2 tagging
scheme in their word-by-word models, as shown in the
second row of Figure 1.
However, two aspects of the problem at hand make this
tag assignment difficult to use for TBL. First, semantic
argument chunks can be very large in size. An argu-
ment which contains a relative clause, for example, can
easily be longer than 20 words. Second, the label an ar-
gument is assigned is largely arbitrary, in the sense that
core argument labels (A0, A1, etc.) generally cannot be
assigned without some information external to the con-
stituent, such as the class of the predicate, or the identity
of other arguments which have already been assigned. So
using the IOB2 format, it might take a complicated se-
quence of TBL rules to completely re-tag, say, an A0 ar-
gument as A1. If this re-tagging is imperfectly achieved,
we are left with the difficult decision of how to interpret
the stranded I-A0 elements, and the problem that they
may incorrectly serve as an environment for other trans-
formational rules.
For this reason, we adopt a modified version of the
IOB2 scheme which is a compromise between addressing
the tasks of argument identification and argument label-
ing. The left boundary (B) tags indicate the label of the
argument, but the internal (I) tags are non-specific as to
argument label, as in the last row of Figure 1. This al-
lows a a single TBL rule to re-label an argument, while
still allowing for interleaving of TBL rules which affect
argument identification and labeling.
3.2 Feature Coding
With each word in a sentence, we associate the following
features:
Word The word itself, normalized to lower-case.
Tag The word?s part-of-speech tag, as predicted by the
system of Gime?nez and Ma`rquez (2003).
Chunk The chunk label of the word, as predicted by the
system of Carreras and Ma`rquez (2003).
Entity The named-entity label of the word, as predicted
by the system of Chieu and Ng (2003).
L/R A feature indicating whether the word is to the left
(L) or right (R) of the target verb.
Indent This feature indicates the clause level of the cur-
rent word with respect to the target predicate. Us-
ing the clause boundaries predicted by the system
of Carreras and Ma`rquez (2003), we compute a fea-
ture based on the linguistic notion of c-command.1
If both the predicate and the current word are in
the same basic clause, Indent=0. If the predicate c-
commands the current word, and the current word is
one clause level lower, Indent=1. If it is two clause
levels lower, Indent=2, and so on. If the c-command
relations are reversed, the indent levels are negative,
and if neither c-commands the other, Indent=?NA?.
(Figure 2 illustrates how this feature is defined.) The
absolute value of the Indent feature is not permitted
to exceed 5.
is-PP A boolean feature indicating whether the word is
included within a base prepositional phrase. This is
1A node ? (reflexively) c-commands a node ? iff there is a
node ? such that ? directly dominates ?, and ? dominates ?.
Note that only clauses (S nodes) are considered in our applica-
tion described above.
true if its chunk tag is B-PP or I-PP, or if it is within
an NP chunk directly following a PP chunk.
PP-head If is-PP is true, this is the head of the preposi-
tional phrase; otherwise it is zero.
N-head The final nominal element of the next NP chunk
at the same indent level as the current word, if it
exists. For purposes of this feature, a possessive NP
chunk is combined with the following NP chunk.
Verb The target predicate under consideration.
V-Tag The POS tag of the target predicate.
V-Passive A boolean feature indicating whether the tar-
get verb is in the passive voice. This is determined
using a simple regular expression over the sentence.
Path As in (Pradhan et al, 2003), this feature is an or-
dered list of the chunk types intervening between the
target verb and the current word, with consecutive
NP chunks treated as one.
3.3 Rule Templates
In order to define the space of rules searched by the TBL
algorithm, we must specify a set of rule templates, which
determine the form transformational rules may take. The
rule templates used in our system are 130 in number, and
fall into a small number of classes, as described below.
These rules all take the form f 1 . . . fn ? labelw,
where f1 through fn are features of the current word w or
words in its environment, and usually include the current
(semantic argument) label assigned to w. The categoriza-
tion of rule templates below, then, basically amounts to a
list of the different feature sets which are used to predict
the argument label of each word.
The initial assignment of tags which is given to the
TBL algorithm is a very simple chunk-based assignment.
Every word is given the tag O (outside all semantic argu-
ments), except if it is within an NP chunk at Indent level
zero. In that case, the word is assigned the tag I if its
chunk label is I-NP, B-A0 if its chunk label is B-NP and
it is to the left of the verb, and B-A1 if its chunk label is
B-NP and it is to the right of the verb.
3.3.1 Basic rules (10 total)
The simplest class of rules simply change the current
word?s argument label based on its own local features,
including the current label, and the features L/R, Indent,
and Chunk.
3.3.2 Basic rules using local context (29)
An expanded set of rules using all features of the cur-
rent word, as well as the argument labels of the current
and previous words. For example, the following rule will
change the label O to I within an NP chunk, if the initial
Argument boundaries [A1 The deal] [V collapsed] [AM-TMP on Friday] .
IOB2 [B-A1 The] [I-A1 deal] [B-V collapsed] [B-AM-TMP on] [I-AM-TMP Friday] [O .]
Modified scheme [B-A1 The] [I deal] [B-V collapsed] [B-AM-TMP on] [I Friday] [O .]
Figure 1: Tag assignments for word-by-word semantic role assignment
W
V W
V W V
indent = NAindent = ?1indent = 1
Figure 2: Sample values of Indent feature for different clause embeddings of a word W and target verb V
portion of the chunk has already been marked as within a
semantic argument:
labelw0 = O
indentw0 = 0
chunkw0 = I-NP
L/Rw0 = R
labelw?1 = I
? labelw0 = I.
3.3.3 Lexically conditioned rules (14)
These rules change the argument label of the current
word based on the Word feature of the current or sur-
rounding words, in combination with argument labels and
chunk labels from the surrounding context. For example,
this rule marks the adverb back as a directional modifier
when it follows the target verb:
labelw0 = O
chunkw0 = B-ADVP
wordw0 = back
labelw?1 = B-V
chunkw?1 = B-VP
? labelw0 = B-AM-DIR.
3.3.4 Entity (24)
These rules further add the named-entity tag of the cur-
rent, preceding, or following word to the basic and local-
context rules above.
3.3.5 Verb tag (15)
These rules add the POS tag of the predicate to the
basic and simpler local-context rules above.
3.3.6 Verb-Noun dependency (9)
These rules allow the argument label of the current
word to be changed, based on its Verb and N-head fea-
tures,as well as other local features.
3.3.7 Word-Noun dependency (3)
These rules allow the argument label of the current
word to be changed, based on its Word, N-head, Indent,
L/R, and Chunk features, as well as the argument labels
of adjacent words.
3.3.8 Long-distance rules (6)
Because many of the dependencies involved in the se-
mantic role labeling task hold over the domain of the en-
tire sentence, we include a number of long-distance rules.
These rules allow the argument label to be changed de-
pending on the word?s current label, the features L/R, In-
dent, Verb, and the argument label of a word within 50 or
100 words of the current word. These rules are intended
to support generalizations like ?if the current word is la-
beled A0, but there is already an A0 further to the left,
change it to I?.
3.3.9 ?Smoothing? rules (15)
Finally, there are a number of ?smoothing? rules,
which are designed primarily to prevent I tags from
becoming stranded, so that arguments which contain a
large number of words can successfully be identified.
These rules allow the argument label of a word to be
changed based on the argument labels of the previous two
words, the next two words, and the chunk tags of these
words. This sample rule marks a word as being argument-
internal, if both its neighbors are already so marked:
labelw?1 = I
labelw0 = O
labelw1 = I
? labelw0 = I.
3.3.10 Path rules (5)
Finally, we include a number of rule templates using
the highly-specific Path feature. These rules allow the ar-
gument label of a word to be changed based on its current
value, as well as the value of the feature Path in combi-
nation with L/R, Indent, V-Tag, Verb, and Word.
3.4 Tag interpretation
The final step in our transformation-based approach to
semantic role labeling is to map the word-by word IOB
tags predicted by the TBL model back to the format of the
original data set, which marks only argument boundaries,
so that we can calculate precision and recall statistics for
each argument type. The simplest method of performing
this mapping is to consider an argument as consisting of
an initial labeled boundary tag (such as B-A0, followed
by zero or more argument-internal (I) tags, ignoring any-
thing which does not conform to this structure (in partic-
ular, strings of Is with no initial boundary marker).
In fact, this method works quite well, and it is used for
the results reported below.
Finally, there is a post-processing step in which adjucts
may be re-labeled if the same sequence of words is found
as an adjunct in the training data, and always bears the
same role. This affected fewer than twenty labels on the
development data, and added only about 0.1 to the overall
f-measure.
4 Results
The results on the test section of the CoNLL 2004 data
are presented in Table 1 below. The overall result, an f-
score of 60.66, is considerably below results reported for
systems using a parser on a comparable data set. How-
ever, it is a reasonable result given the simplicity of our
system, which does not make use of the additional infor-
mation found in the PropBank frames themselves.
It is an interesting question to what extent our re-
sults depend on the use of the Path feature (which
Pradhan et al (2003) found to be essential to their mod-
els? performance). Since this Path feature is also likely
to be one of the model?s most brittle features, depend-
ing heavily on the accuracy of the syntactic analysis, we
might hope that the system does not depend too heav-
ily on it. In fact, the overall f-score on the development
set drops from 62.75 to 61.33 when the Path feature is
removed, suggestig that it is not essential to our model,
though it does help performance to some extent.
References
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case study
in part-of-speech tagging. Computational Linguistics,
21(4):543?565.
Xavier Carreras and Llu??s Ma`rquez. 2003. Phrase recog-
nition by filtering and ranking with perceptrons. In
Proceedings of RANLP 2003.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach. In
Proceedings of CoNLL 2003.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2003. Fast and accu-
rate part-of-speech tagging: the SVM approach revis-
ited. In Proceedings of RANLP 2003.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H.
Martin, and Dan Jurafsky. 2003. Shallow semantic
Precision Recall F?=1
Overall 64.17% 57.52% 60.66
A0 72.48% 68.94% 70.67
A1 63.57% 61.88% 62.72
A2 51.32% 40.90% 45.52
A3 51.58% 32.67% 40.00
A4 36.07% 44.00% 39.64
A5 0.00% 0.00% 0.00
AM-ADV 41.08% 32.25% 36.13
AM-CAU 63.33% 38.78% 48.10
AM-DIR 31.58% 24.00% 27.27
AM-DIS 56.93% 53.99% 55.42
AM-EXT 70.00% 50.00% 58.33
AM-LOC 26.34% 21.49% 23.67
AM-MNR 46.90% 26.67% 34.00
AM-MOD 96.24% 91.10% 93.60
AM-NEG 90.98% 95.28% 93.08
AM-PNC 37.93% 12.94% 19.30
AM-PRD 0.00% 0.00% 0.00
AM-TMP 51.81% 38.42% 44.12
R-A0 82.00% 77.36% 79.61
R-A1 78.26% 51.43% 62.07
R-A2 100.00% 22.22% 36.36
R-A3 0.00% 0.00% 0.00
R-AM-LOC 50.00% 25.00% 33.33
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 100.00% 7.14% 13.33
V 98.15% 98.15% 98.15
Table 1: Results on test set: closed challenge
parsing using support vector machines. Technical Re-
port CSLR-2003-01, Center for Spoken Language Re-
search, University of Colorado at Boulder.
Grace Ngai and Radu Florian. 2001. Transformation-
based learning in the fast lane. In Proceedings of
NAACL 2001, pages 40?47, June.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky.
2003. Support vector learning for semantic argument
classification. Technical Report CSLR-2003-03, Cen-
ter for Spoken Language Research, University of Col-
orado at Boulder.
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Rep-
resenting text chunks. In Proceedings of EACL 1999,
pages 173?179.
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 53?56,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Amazon Mechanical Turk for Transcription of Non-Native Speech
Keelan Evanini, Derrick Higgins, and Klaus Zechner
Educational Testing Service
{KEvanini, DHiggins, KZechner}@ets.org
Abstract
This study investigates the use of Amazon
Mechanical Turk for the transcription of non-
native speech. Multiple transcriptions were
obtained from several distinct MTurk workers
and were combined to produce merged tran-
scriptions that had higher levels of agreement
with a gold standard transcription than the in-
dividual transcriptions. Three different meth-
ods for merging transcriptions were compared
across two types of responses (spontaneous
and read-aloud). The results show that the
merged MTurk transcriptions are as accurate
as an individual expert transcriber for the read-
aloud responses, and are only slightly less ac-
curate for the spontaneous responses.
1 Introduction
Orthographic transcription of large amounts of
speech is necessary for improving speech recogni-
tion results. Transcription, however, is a time con-
suming and costly procedure. Typical transcription
speeds for spontaneous, conversational speech are
around 7 to 10 times real-time (Glenn and Strassel,
2008). The transcription of non-native speech is an
even more difficult task?one study reports an aver-
age transcription time of 12 times real-time for spon-
taneous non-native speech (Zechner, 2009).
In addition to being more costly and time consum-
ing, transcription of non-native speech results in a
higher level of disagreement among transcribers in
comparison to native speech. This is especially true
when the speaker?s proficiency is low and the speech
contains large numbers of grammatical errors, in-
correct collocations, and disfluencies. For exam-
ple, one study involving highly predictable speech
shows a decline in transcriber agreement (measured
using Word Error Rate, WER) from 3.6% for na-
tive speech to 6.4% for non-native speech (Marge et
al., to appear). Another study involving spontaneous
non-native speech showed a range of WER between
15% and 20% (Zechner, 2009).
This study uses the Amazon Mechanical Turk
(MTurk) resource to obtain multiple transcriptions
for non-native speech. We then investigate several
methods for combining these multiple sources of in-
formation from individual MTurk workers (turkers)
in an attempt to obtain a final merged transcription
that is more accurate than the individual transcrip-
tions. This methodology results in transcriptions
that approach the level of expert transcribers on this
difficult task. Furthermore, a substantial savings in
cost can be achieved.
2 Previous Work
Due to its ability to provide multiple sources of
information for a given task in a cost-effective
way, several recent studies have combined multi-
ple MTurk outputs for NLP annotation tasks. For
example, one study involving annotation of emo-
tions in text used average scores from up to 10 turk-
ers to show the minimum number of MTurk anno-
tations required to achieve performance compara-
ble to experts (Snow et al, 2008). Another study
used preference voting to combine up to 5 MTurk
rankings of machine translation quality and showed
that the resulting judgments approached expert inter-
annotator agreement (Callison-Burch, 2009). These
53
tasks, however, are much simpler than transcription.
MTurk has been used extensively as a transcrip-
tion provider, as is apparent from the success of a
middleman site that act as an interface to MTurk
for transcription tasks.1 However, to our knowledge,
only one previous study has systematically evaluated
the quality of MTurk transcriptions (Marge et al,
to appear). This recent study also combined multi-
ple MTurk transcriptions using the ROVER method
(Fiscus, 1997) to produce merged transcriptions that
approached the accuracy of expert transcribers. Our
study is similar to that study, except that the speech
data used in our study is much more difficult to
transcribe?the utterances used in that study were rel-
atively predictable (providing route instructions for
robots), and contained speech from native speak-
ers and high-proficiency non-native speakers. Fur-
thermore, we investigate two additional merging al-
gorithms in an attempt to improve over the perfor-
mance of ROVER.
3 Experimental Design
3.1 Audio
The audio files used in this experiment consist of
responses to an assessment of English proficiency
for non-native speakers. Two different types of re-
sponses are examined: spontaneous and read-aloud.
In the spontaneous task, the speakers were asked to
respond with their opinion about a topic described
in the prompt. The speech in these responses is thus
highly unpredictable. In the read-aloud task, on the
other hand, the speakers were asked to read a para-
graph out loud. For these responses, the speech is
highly predictable; any deviations from the target
script are due to reading errors or disfluencies.
For this experiment, one set of 10 spontaneous
(SP) responses (30 seconds in duration) and two sets
of 10 read-aloud (RA) responses (60 seconds in du-
ration) were used. Table 1 displays the characteris-
tics of the responses in the three batches.
3.2 Transcription Procedure
The tasks were submitted to the MTurk interface in
batches of 10, and a turker was required to complete
the entire batch in order to receive payment. Turkers
1http://castingwords.com/
Batch Duration # of Words
(Mean)
# of Words
(Std. Dev.)
SP 30 sec. 33 14
RA1 60 sec. 97 4
RA2 60 sec. 93 10
Table 1: Characteristics of the responses used in the study
received $3 for a complete batch of transcriptions
($0.30 per transcription).
Different interfaces were used for transcribing the
two types of responses. For the spontaneous re-
sponses, the task was a standard transcription task:
the turkers were instructed to enter the words that
they heard in the audio file into a text box. For the
read-aloud responses, on the other hand, they were
provided with the target text of the prompt, one word
per line. They were instructed to make annotations
next to words in cases where the speaker deviated
from the target text (indicating substitutions, dele-
tions, and insertions). For both types of transcription
task, the turkers were required to successfully com-
plete a short training task before proceeding onto the
batch of 10 responses.
4 Methods for Merging Transcriptions
4.1 ROVER
The ROVER method was originally developed for
combining the results from multiple ASR systems to
produce a more accurate hypothesis (Fiscus, 1997).
This method iteratively aligns pairs of transcriptions
to produce a word transition network. A voting pro-
cedure is then used to produce the merged transcrip-
tion by selecting the most frequent word (including
NULL) in each correspondence set; ties are broken
by a random choice.
4.2 Longest Common Subsequence
In this method, the Longest Common Subsequence
(LCS) among the set of transcriptions is found by
first finding the LCS between two transcriptions,
comparing this output with the next transcription to
find their LCS, and iterating over all transcriptions in
this manner. Then, each transcription is compared to
the LCS, and any portions of the transcription that
are missing between words of the LCS are tallied.
Finally, words are interpolated into the LCS by se-
54
lecting the most frequent missing sequence from the
set of transcriptions (including the empty sequence);
as with the ROVER method, ties are broken by a ran-
dom choice among the most frequent candidates.
4.3 Lattice
In this method, a word lattice is formed from the
individual transcriptions by iteratively adding tran-
scriptions into the lattice to optimize the match be-
tween the transcription and the lattice. New nodes
are only added to the graph when necessary. Then,
to produce the merged transcription, the optimal
path through the lattice is determined. Three dif-
ferent configurations for computing the optimal path
through the lattice method were compared. In the
first configuration, ?Lattice (TW),? the weight of
a path through the lattice is determined simply by
adding up the total of the weights of each edge
in the path. Note that this method tends to fa-
vor longer paths over shorter ones, assuming equal
edge weights. In the next configuration, ?Lattice
(AEW),? a cost for each node based on the aver-
age edge weight is subtracted as each edge of the
lattice is traversed, in order to ameliorate the prefer-
ence for longer paths. Finally, in the third configura-
tion, ?Lattice (TWPN),? the weight of a path through
the lattice is defined as the total path weight in the
?Lattice (TW)? method, normalized by the number
of nodes in the path (again, to offset the preference
for longer paths).
4.4 WER calculation
All three of the methods for merging transcriptions
are sensitive to the order in which the individual
transcriptions are considered. Thus, in order to accu-
rately evaluate the methods, for each number of tran-
scriptions used to create the merged transcription,
N ? {3, 4, 5}, all possible permutations of all pos-
sible combinations were considered. This resulted
in a total of 5!(5?N)! merged transcriptions to be eval-
uated. For each N, the overall WER was computed
from this set of merged transcriptions.
5 Results
Tables 2 - 4 present the WER results for differ-
ent merging algorithms for the two batches of read-
aloud responses and the batch of spontaneous re-
sponses. In each table, the merging methods are or-
Method N=3 N=4 N=5
Individual Turkers 7.0%
Lattice (TWPN) 6.4% 6.4% 6.4%
Lattice (TW) 6.4% 6.4% 6.4%
LCS 6.0% 5.6% 5.6%
Lattice (AEW) 6.1% 6.0% 5.5%
ROVER 5.5% 5.2% 5.1%
Expert 4.7%
Table 2: WER results 10 read-aloud responses (RA1)
Method N=3 N=4 N=5
Individual Turkers 9.7%
Lattice (TW) 9.5% 9.5% 9.4%
Lattice (TWPN) 8.3% 8.0% 8.0%
Lattice (AEW) 8.2% 7.4% 7.8%
ROVER 7.9% 7.9% 7.6%
LCS 8.3% 8.0% 7.5%
Expert 8.1%
Table 3: WER results for 10 read-aloud responses (RA2)
dered according to their performance when all tran-
scriptions were used (N=5). In addition, the overall
WER results for the individual turkers and an expert
transcriber are provided for each set of responses.
In each case, the WER is computed by comparison
with a gold standard transcription that was created
by having an expert transcriber edit the transcription
of a different expert transcriber.
In all cases, the merged transcriptions have a
lower WER than the overall WER for the individual
turkers. Furthermore, for all methods, the merged
output using all 5 transcriptions has a lower (or
equal) WER to the output using 3 transcriptions. For
the first batch of read-aloud responses, the ROVER
method performed best, and reduced the WER in
the set of individual transcriptions by 27.1% (rela-
tive) to 5.1%. For the second batch of read-aloud
responses, the LCS method performed best, and re-
duced the WER by 22.6% to 7.5%. Finally, for the
batch of spontaneous responses, the Lattice (TW)
method performed best, and reduced the WER by
25.6% to 22.1%.
55
Method N=3 N=4 N=5
Individual Turkers 29.7%
Lattice (TWPN) 29.1% 28.9% 28.3%
LCS 29.2% 28.4% 27.0%
Lattice (AEW) 28.1% 25.8% 25.1%
ROVER 25.4% 24.5% 24.9%
Lattice (TW) 25.5% 23.5% 22.1%
Expert 18.3%
Table 4: WER results for 10 spontaneous responses
6 Conclusions
As is clear from the levels of disagreement be-
tween the expert transcriber and the gold standard
transcription for all three tasks, these responses are
much more difficult to transcribe accurately than
native spontaneous speech. For native speech, ex-
pert transcribers can usually reach agreement lev-
els over 95% (Deshmukh et al, 1996). For these
responses, however, the WER for the expert tran-
scriber was worse than this even for the read-aloud
speech. These low levels of agreement can be at-
tributed to the fact that the speech is drawn from a
wide range of English proficiency levels among test-
takers. Most of the responses contain disfluencies,
grammatical errors, and mispronunciations, leading
to increased transcriber uncertainty.
The results of merging multiple MTurk transcrip-
tions of this non-native speech showed an improve-
ment over the performance of the individual tran-
scribers for all methods considered. For the read-
aloud speech, the agreement level of the merged
transcriptions approached that of the expert tran-
scription when only three MTurk transcriptions were
used. For the spontaneous responses, the perfor-
mance of the best methods still lagged behind the ex-
pert transcription, even when five MTurk transcrip-
tions were used. Due to the consistent increase in
performance, and the low cost of adding additional
transcribers (in this study the cost was $0.30 per au-
dio minute for read-aloud speech and $0.60 per au-
dio minute for spontaneous speech), the approach of
combining multiple transcriptions should always be
considered when MTurk is used for transcription. It
is also possible that lower payments per task could
be provided without a decrease in transcription qual-
ity, as demonstrated by Marge et al (to appear). Ad-
ditional experiments will address the practicality of
producing more accurate merged transcriptions for
an ASR system?simply collecting larger amounts
of non-expert transcriptions may be a better invest-
ment than producing higher quality data (Novotney
and Callison-Burch, 2010).
It is interesting that the Lattice (TW) method
of merging transcriptions clearly outperformed all
other methods for the spontaneous responses, but
was less beneficial than the LCS and ROVER meth-
ods for read-aloud speech. It is likely that this is
caused by the preference of the Lattice (TW) method
for longer paths through the word lattice, since indi-
vidual transcribers of spontaneous speech may mark
different words as unitelligible, even though these
words exist in the gold standard transcription. Fur-
ther studies with a larger number of responses will
be needed to test this hypothesis.
References
Chris Callison-Burch. 2009. Fast, cheap and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proc. EMNLP.
Neeraj Deshmukh, Richard Jennings Duncan, Aravind
Ganapathiraju, and Joseph Picone. 1996. Benchmark-
ing human performance for continuous speech recog-
nition. In Proc. ICSLP.
Jonathan G. Fiscus. 1997. A post-processing system to
yield word error rates: Recognizer Ooutput Voting Er-
ror Reduction (ROVER). In Proc. ASRU.
Meghan Lammie Glenn and Stephanie Strassel. 2008.
Shared linguistic resources for the meeting domain.
In Lecture Notes in Computer Science, volume 4625,
pages 401?413. Springer.
Matthew Marge, Satanjeev Banerjee, and Alexander I.
Rudnicky. to appear. Using the Amazon Mechanical
Turk for transcription of spoken language. In Proc.
ICASSP.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
fast, and good enough: Automatic speech recognition
with non-expert transcription. In Proc. NAACL.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast ? But is it
good? Evaluating non-expert annotations for natural
language tasks. In Proc. EMNLP.
Klaus Zechner. 2009. What did they actually say?
Agreement and disagreement among transcribers of
non-native spontaneous speech responses in an En-
glish proficiency test. In Proc. ISCA-SLaTE.
56
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 92?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Off-topic essay detection using short prompt texts
Annie Louis
University of Pennsylvania
Philadelphia, PA 19104, USA
lannie@seas.upenn.edu
Derrick Higgins
Educational Testing Service
Princeton, NJ 08541, USA
dhiggins@ets.org
Abstract
Our work addresses the problem of predict-
ing whether an essay is off-topic to a given
prompt or question without any previously-
seen essays as training data. Prior work has
used similarity between essay vocabulary and
prompt words to estimate the degree of on-
topic content. In our corpus of opinion es-
says, prompts are very short, and using sim-
ilarity with such prompts to detect off-topic
essays yields error rates of about 10%. We
propose two methods to enable better compar-
ison of prompt and essay text. We automat-
ically expand short prompts before compari-
son, with words likely to appear in an essay
to that prompt. We also apply spelling correc-
tion to the essay texts. Both methods reduce
the error rates during off-topic essay detection
and turn out to be complementary, leading to
even better performance when used in unison.
1 Introduction
It is important to limit the opportunity to sub-
mit uncooperative responses to educational software
(Baker et al, 2009). We address the task of detect-
ing essays that are irrelevant to a given prompt (es-
say question) when training data is not available and
the prompt text is very short.
When example essays for a prompt are available,
they can be used to learn word patterns to distin-
guish on-topic from off-topic essays. Alternatively,
prior work (Higgins et al, 2006) has motivated us-
ing similarity between essay and prompt vocabular-
ies to detect off-topic essays. In Section 2, we exam-
ine the performance of prompt-essay comparison for
four different essay types. We show that in the case
of prompts with 9 or 13 content words on average,
the error rates are higher compared to those with 60
or more content words. In addition, more errors are
observed when the method is used on essays written
by English language learners compared to more ad-
vanced test takers. An example short prompt from
our opinion essays? corpus is shown below. Test-
takers provided arguments for/or against the opinion
expressed by the prompt.
[1] ?In the past, people were more friendly than
they are today.?
To address this problem, we propose two en-
hancements. We use unsupervised methods to ex-
pand the prompt text with words likely to appear
in essays to that prompt. Our approach is based
on the intuition that regularities exist in the words
which appear in essays, beyond the prevalence of
actual prompt words. In a similar vein, misspellings
in the essays, particulary of the prompt words, are
also problematic for prompt-based methods. There-
fore we apply spelling correction to the essay text
before comparison. Our results show that both meth-
ods lower the error rates. The relative performance
of the two methods varies depending on the essay
type; however, their combination gives the overall
best results regardless of essay type.
2 Effect of prompt and essay properties
In this section, we analyze the off-topic essay pre-
diction accuracies resulting from direct comparison
of original prompt and essay texts. We use four dif-
ferent corpora of essays collected and scored during
high stakes tests with an English writing component.
They differ in task type and average prompt length,
as well as the skill level expected from the test taker.
92
In one of the tasks, the test taker reads a passage
and listens to a lecture and then writes a summary
of the main points. For such essays, the prompt
text (reading passage plus lecture transcript) avail-
able for comparison is quite long (about 276 con-
tent words). In the other 3 tasks, the test taker has
to provide an argument for or against some opin-
ion expressed in the prompt. One of these has long
prompts (60 content words). The other two involve
only single sentence prompts as in example [1] and
have 13 and 9 content words on average. Two of
these tasks focused on English language learners and
the other two involved advanced users (applicants to
graduate study programs in the U.S.). See Table 1
for a summary of the essay types.1
2.1 Data
For each of the task types described above, our cor-
pus contains essays written to 10 different prompts.
We used essays to 3 prompts as development data.
To build an evaluation test set, we randomly sam-
pled 350 essays for each of the 7 remaining prompts
to use as positive examples. It is difficult to as-
semble a sufficient number of naturally-occurring
off-topic essays for testing. However, an essay on-
topic to a particular prompt can be considered as
pseudo off-topic to a different prompt. Hence, to
complement the positive examples for each prompt,
an equal number of negative examples were chosen
at random from essays to the remaining 6 prompts.
2.2 Experimental setup
We use the approach for off-topic essay detection
suggested in prior work by Higgins et al (2006).
The method uses cosine overlap between tf*idf vec-
tors of prompt and essay content words to measure
the similarity between a prompt-essay pair.
sim(prompt, essay) = vessay .vprompt?vessay??vprompt?
(1)
An essay is compared with the target prompt
(prompt with which topicality must be checked) to-
gether with a set of reference prompts, different from
the target. The reference prompts are also chosen
to be different from the actual prompts of the neg-
ative examples in our dataset. If the target prompt
1Essay sources: Type 1-TOEFL integrated writing task,
Type 4-TOEFL independent writing task, Types 2 & 3-
argument and issue tasks in Analytical Writing section of GRE
Type Skill Prompt len. Avg FP Avg FN
1 Learners 276 0.73 11.79
2 Advanced 60 0.20 6.20
3 Advanced 13 2.94 8.90
4 Learners 9 9.73 11.07
Table 1: Effect of essay types: average prompt length,
false positive and false negative rates
is ranked as most similar2 in the list of compared
prompts, the essay is classified as on-topic. 9 refer-
ence prompts were used in our experiments.
We compute two error rates.
FALSE POSITIVE - percentage of on-topic essays in-
correctly flagged as off-topic.
FALSE NEGATIVE - percentage of off-topic essays
which the system failed to flag.
In this task, it is of utmost importance to maintain
very low false positive rates, as incorrect labeling of
an on-topic essay as off-topic is undesirable.
2.3 Observations
In Table 1, we report the average false positive and
false negative rates for the 7 prompts in the test set
for each essay type. For long prompts, both Types 1
and 2, the false positive rates are very low. The clas-
sification of Type 2 essays which were also written
by advanced test takers is the most accurate.
However, for essays with shorter prompts (Types
3 and 4), the false positive rates are higher. In fact,
in the case of Type 4 essays written by English lan-
guage learners, the false positive rates are as high as
10%. Therefore we focus on improving the results
in these two cases which involve short prompts.
Both prompt length and the English proficiency
of the test taker seem to influence the prediction ac-
curacies for off-topic essay detection. In our work,
we address these two challenges by: a) automatic
expansion of short prompts (Section 3) and b) cor-
rection of spelling errors in essay texts (Section 4).
3 Prompt expansion
We designed four automatic methods to add relevant
words to the prompt text.
2Less strict cutoffs may be used, for example, on-topic if
target prompt is within rank 3 or 5, etc. However even a cutoff
of 2 incorrectly classifies 25% of off-topic essays as on-topic.
93
3.1 Unsupervised methods
Inflected forms: Given a prompt word, ?friendly?,
its morphological variants??friend?, ?friendlier?,
?friendliness??are also likely to be used in essays
to that prompt. Inflected forms are the simplest and
most restrictive class in our set of expansions. They
were obtained by a rule-based approach (Leacock
and Chodorow, 2003) which adds/modifies prefixes
and suffixes of words to obtain inflected forms.
These rules were adapted from WordNet rules de-
signed to get the base forms of inflected words.
Synonyms: Words with the same meaning as
prompt words might also be mentioned over the
course of an essay. For example, ?favorable?
and ?well-disposed? are synonyms for the word
?friendly? and likely to be good expansions. We
used an in-house tool to obtain synonyms from
WordNet for each of the prompt words. The lookup
involves a word sense disambiguation step to choose
the most relevant sense for polysemous words. All
the synonyms for the chosen sense of the prompt
word are added as expansions.
Distributionally similar words: We also consider
as expansions words that appear in similar contexts
as the prompt words. For example, ?cordial?, ?po-
lite?, ?cheerful?, ?hostile?, ?calm?, ?lively? and
?affable? often appear in the same contexts as the
word ?friendly?. Such related words form part of
a concept like ?behavioral characteristics of people?
and are likely to appear in a discussion of any one
aspect. These expansions could comprise antonyms
and other related words too. This idea of word simi-
larity was implemented in work by Lin (1998). Sim-
ilarity between two words is estimated by examin-
ing the degree of overlap of their contexts in a large
corpus. We access Lin?s similarity estimates using
a tool from Leacock and Chodorow (2003) that re-
turns words with similarity values above a cutoff.
Word association norms: Word associations have
been of great interest in psycholinguistic research.
Participants are given a target word and asked to
mention words that readily come to mind. The most
frequent among these are recorded as free associa-
tions for that target. They form another interesting
category of expansions for our purpose because they
are known to be frequently recalled by human sub-
jects for a particular stimulus word. We added the
associations for prompt words from a collection of
5000 target words with their associations produced
by about 6000 participants (Nelson et al, 1998).
Sample associations for the word ?friendly? include
?smile?, ?amiable?, ?greet? and ?mean?.
3.2 Weighting of prompt words and expansions
After expansion, the prompt lengths vary between
87 (word associations) and 229 (distributionally
similar words) content words, considerably higher
than the original average length of 9 and 13 content
words. We use a simple weighting scheme3 to mit-
igate the influence of noisy expansions. We assign
a weight of 20 to original prompt words and 1 to all
the expansions. While computing similarity, we use
these weight values as the assumed frequency of the
word in the prompt. In this case, the term frequency
of original words is set as 20 and all expansion terms
are considered to appear once in the new prompt.
4 Spelling correction of essay text
Essays written by learners of a language are prone to
spelling errors. When such errors occur in the use of
the prompt words, prompt-based techniques will fail
to identify the essay as on-topic even if it actually is.
The usefulness of expansion could also be limited
if there are several spelling errors in the essay text.
Hence we explored the correction of spelling errors
in the essay before off-topic detection.
We use a tool from Leacock and Chodorow
(2003) to perform directed spelling correction, ie.,
focusing on correcting the spellings of words most
likely to match a given target list. We use the prompt
words as the targets. We also explore the simultane-
ous use of spelling correction and expansion. We
first obtain expansion words from one of our unsu-
pervised methods. We then use these along with
the prompt words for spelling correction followed
by matching of the expanded prompt and essay text.
5 Results and discussion
We used our proposed methods on the two essay col-
lections with very short prompts, Type 3 written by
3Without any weighting there was an increase in error rates
during development tests. We also experimented with a graph-
based approach to term weighting which gave similar results.
94
advanced test takers and Type 4 written by learn-
ers of English. Table 2 compares the suggested en-
hancements with the previously proposed method by
Higgins et al (2006). As discussed in Section 2.3,
using only the original prompt words, error rates are
around 10% for both essay types. For advanced test
takers, the false positive rates are lower, around 3%.
Usefulness of expanded prompts All the expansion
methods lower the false positive error rates on es-
says written by learners with almost no increase in
the rate of false negatives. On average, the false
positive errors are reduced by about 3%. Inflected
forms constitute the best individual expansion cat-
egory. The overall best performance on this type
of essays is obtained by combining inflected forms
with word associations.
In contrast, for essays written by advanced test
takers, inflected forms is the worst expansion cate-
gory. Here word associations give the best results
reducing both false positive and false negative er-
rors; the reduction in false positives is almost 50%.
These results suggest that advanced users of English
use more diverse vocabulary in their essays which
are best matched by word associations.
Effect of spelling correction For essays written by
learners, spell-correcting the essay text before com-
parison (Spell) leads to huge reductions in error
rates. Using only the original prompt, the false pos-
itive rate is 4% lower with spelling correction than
without. Note that this result is even better than the
best expansion technique?inflected forms. However,
for essays written by advanced users, spelling cor-
rection does not provide any benefits. This result
is expected since these test-takers are less likely to
produce many spelling errors.
Combination of methods The benefits of the two
methods appear to be population dependent. For
learners of English, a spelling correction module
is necessary while for advanced users, the benefits
are minimal. On the other hand, prompt expansion
works extremely well for essays written by advanced
users. The expansions are also useful for essays
written by learners but the benefits are lower com-
pared to spelling correction. However, for both es-
say types, the combination of spelling correction and
best prompt expansion method (Spell + best expn.)
is better compared to either of them individually.
Learners Advanced
Method FP FN FP FN
Prompt only 9.73 11.07 2.94 9.06
Synonyms 7.03 12.01 1.39 9.76
Dist. 6.45 11.77 1.63 8.98
WAN 6.33 11.97 1.59 8.74
Infl. forms 6.25 11.65 2.53 9.06
Infl. forms + WAN 6.04 11.48 - -
Spell 5.43 12.71 2.53 9.27
Spell + best expn. 4.66 11.97 1.47 9.02
Table 2: Average error rates after prompt expansion and
spelling correction
Therefore the best policy would be to use both en-
hancements together for prompt-based methods.
6 Conclusion
We have described methods for improving the accu-
racy of off-topic essay detection for short prompts.
We showed that it is possible to predict words that
are likely to be used in an essay based on words that
appear in its prompt. By adding such words to the
prompt automatically, we built a better representa-
tion of prompt content to compare with the essay
text. The best combination included inflected forms
and word associations, reducing the false positives
by almost 4%. We also showed that spelling correc-
tion is a very useful preprocessing step before off-
topic essay detection.
References
R.S.J.d. Baker, A.M.J.B. de Carvalho, J. Raspat,
V. Aleven, A.T. Corbett, and K.R. Koedinger. 2009.
Educational software features that encourage and dis-
courage ?gaming the system?. In Proceedings of the
International Conference on Artificial Intelligence in
Education.
D. Higgins, J. Burstein, and Y. Attali. 2006. Identifying
off-topic student essays without topic-specific training
data. Natural Language Engineering, 12(2):145?159.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated scoring of short-answer questions. Computers
and the Humanities, 37(4):389?405.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In COLING-ACL, pages 768?774.
D. L Nelson, C. L. McEvoy, and T. A. Schreiber.
1998. The University of South Florida word
association, rhyme, and word fragment norms,
http://www.usf.edu/FreeAssociation/.
95
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 161?169,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Non-English Response Detection Method for Automated Proficiency Scoring
System
Su-Youn Yoon and Derrick Higgins
Educational Testing Service
660 Rosedale Road, Princeton, NJ, USA
{syoon,dhiggins}@ets.org
Abstract
This paper presents a method for identifying
non-English speech, with the aim of support-
ing an automated speech proficiency scoring
system for non-native speakers.
The method uses a popular technique from the
language identification domain, a single phone
recognizer followed by multiple language-
dependent language models. This method
determines the language of a speech sample
based on the phonotactic differences among
languages.
The method is intended for use with non-
native English speakers. Therefore, the
method must be able to distinguish non-
English responses from non-native speakers?
English responses. This makes the task more
challenging, as the frequent pronunciation er-
rors of non-native speakers may weaken the
phonetic and phonotactic distinction between
English responses and non-English responses.
In order to address this issue, the speaking
rate measure was used to complement the
language identification based features in the
model.
The accuracy of the method was 98%, and
there was 45% relative error reduction over
a system based on the conventional language
identification technique. The model using
both feature sets furthermore demonstrated an
improvement in accuracy for speakers at all
English proficiency levels.
1 Introduction
We developed a non-English response identifica-
tion method as a supplementary module for the au-
tomated speech proficiency scoring of non-native
speakers. The method can identify speech samples
of test takers who try to game the system by speak-
ing in their native languages. For the items that
elicited spontaneous speech, fluency features such
as speaking rate have been one of the most impor-
tant features in the automated scoring. By speak-
ing in their native languages, speakers can generate
fluent speech, and the automated proficiency scor-
ing system may assign a high score. This problem
has been rarely recognized, and none of research has
focused on it as to the authors? knowledge. In or-
der to address this issue, the automated proficiency
scoring system in this study first filters out the re-
sponses in non-English languages, and for the re-
maining responses, it predicts the proficiency score
using a scoring model.
Non-English detection is strongly related to lan-
guage identification(Lamel and Gauvain, 1993;
Zissman, 1996; Li et al, 2007); language identifi-
cation is the process of determining which language
a spoken response is in, while non-English detec-
tion makes a binary decision whether the spoken re-
sponse is in English or not. Due to the strong simi-
larity between the two tasks, the language identifica-
tion method was used here for non-English response
detection.
In contrast to previous research, the method de-
scribed here was intended for use with non-native
speakers, and the English responses for model train-
ing and evaluation were accordingly collected from
non-native speakers. Among other differences,
non-native speakers? speech tends to display non-
standard pronunciation characteristics which can
161
make the task of language identification more chal-
lenging. For instance, when native Korean speak-
ers speak English, they may replace some English
phonemes not in their language with their native
phones, and epenthesize vowels within consonant
clusters. Such processes tend to reduce the pho-
netic and phonotactic distinction between English
and other languages. The frequency of these pro-
nunciation errors is influenced by speakers? na-
tive language and proficiency level, with lower-
proficiency speakers likely to exhibit the greatest
degree of divergence from standard pronunciation.
Language identification method may not effectively
distinguish non-fluent speakers? English responses
from non-English responses. In order to address
these non-native speech characteristics, the model
described here includes the speaking rate feature,
which has been found to be an indicator of speak-
ing proficiency in previous research(Strik and Cuc-
chiarini, 1999; Zechner et al, 2009). Non-fluent
speakers? English responses can be distinguished
from non-English responses by slow speaking rate.
This paper will proceed as follows: we first re-
view previous studies in section 2, then describe the
data in section 3, and present the experiment in sec-
tion 4. The results and discussion are presented in
section 5, and the conclusions are presented in sec-
tion 6.
2 Previous Work
Many previous studies in language identification
focused on phonetic and phonotactic differences
among languages. The frequencies of phones and
phone sequences differ according to languages and
some phone sequences occur only in certain lan-
guages. The literature in language identification
captured this characteristic using the likelihood
score of speech recognizers, which signals the de-
gree of a match between the test sentences and
speech recognizer models. Both the language model
(hereafter, LM) and acoustic model (hereafter, AM)
of a phone recognizer are optimized for the acoustic
characteristics and the phoneme distribution of the
training data. If a spoken response is recognized us-
ing a recognizer trained on a different language, it
may result in a low likelihood score due to a mis-
match between the test sentences and the models.
Lamel and Gauvain (1993) trained multiple
language-dependent-phone-recognizers and se-
lected the language with the highest matching score
as the input language (hereafter, parallel PRLM).
For instance, if the test data contained English and
Hindi speech data, the English-phone-recognizer
and the Hindi-phone-recognizer were trained in-
dependently. In the test, the given speech samples
were recognized using two phone recognizers,
and the language that had a higher matching
score was selected. However, training multiple
phone recognizers was time-consuming and labor
intensive; therefore, Zissman (1996) proposed a
system using single-language phone recognition
followed by multiple language-dependent language
modeling (hereafter, PRLM). PRLM was able to
achieve comparable performance to parallel PRLM
for long speech (longer than 30 seconds), and in a
two-language situation, the error rate was between
5 and 7%.
Instead of language-dependent LM, Li et al
(2007) used vector space modeling (VSM). They
applied metrics frequently used in information re-
trieval. As with the PRLM method, the speech was
converted into phone sequences using the phone rec-
ognizer, and cooccurrence statistics such as term fre-
quency (TF) and inverse document frequency (IDF)
were calculated. The method outperformed the
PRLM approach for long speech.
These methods can be challenging and time-
consuming to implement, as they require implemen-
tation of methods beyond those typically available
in a standard word-based recognition system. In
particular, the application of the phone recognizer
increases the processing time substantially. Be-
cause of this problem, Lim et al (2004) presented a
method based on the features that were readily avail-
able for speech recognizers: a confidence score and
the cross-entropy of the LM. The confidence scoring
method measured the acoustic match between the
word hypotheses and the real sound, while the cross-
entropy measured how well a sentence matched a
given language model. If the test sentence was rec-
ognized by the speech recognizer in a different lan-
guage, the phonetic and lexical mismatches between
two languages resulted in a low confidence score and
a high cross-entropy. Using this methodology, Lim
et al (2004) achieved 99.8% accuracy in their three-
162
way classification task.
The current study can be distinguished from the
previous studies in the following points. First of all,
special features were implemented to model non-
native speech since the method was developed for
non-native speech. In our study, the data contained
non-native speakers? English speech, characterized
by inaccurate pronunciation. It resulted in a mis-
match between the speech-recognizer models and
test sentences, even for utterances in English. In par-
ticular, the mismatch was more salient in non-fluent
speakers? speech, which comprised a high propor-
tion of our data. In order to address this issue, speak-
ing rate, which has achieved good performance in
the estimation of non-native speakers? speaking pro-
ficiency (Strik and Cucchiarini, 1999; Zechner et
al., 2009), was implemented as an additional feature.
Secondly, in contrast to previous studies that deter-
mined which language the speech was in, we made
a binary decision whether the speech was in English
or not. Finally, the method was developed as part of
a language assessment system.
3 Data
The OGI Multi-language corpus (Muthusamy et al,
1992), a standard language identification develop-
ment data set, was used in the training and evalua-
tion of the system. It contains a total of 1,957 calls
from speakers of 10 different languages (English,
Farsi, French, German, Japanese, Korean, Mandarin
Chinese, Spanish, Tamil, and Vietnamese). The cor-
pus was composed of short speech and long speech;
the short files contained approximately 10 seconds
speech, while the long files contained speech ranged
from 30 seconds to 50 seconds.
The method described here was implemented to
distinguish non-English responses from non-native
speakers? English responses. Therefore, the English
data used to train and evaluate the model for non-
English response detection was collected from non-
native speakers. In particular, responses to the En-
glish Practice Test (EPT) were used. The EPT is
an online practice test which allows students to gain
familiarity with the format of a high-stakes test of
English proficiency and receive immediate feedback
on their test responses based on automated scor-
ing methods. The speaking section of the EPT as-
sessment consists of 6 items in which speakers are
prompted to provide open-ended responses of 45-60
seconds in length. The scoring scale of each item is
discrete from 1 to 4, where 4 indicates high speaking
proficiency and 1 low proficiency.
The non-English detection task is composed of
two major components: training of PRLM, and
training of the classifier which makes a binary de-
cision about whether a speech sample is in the En-
glish language, given PRLM-based features and the
speaking rate.
The OGI corpus was used in training of both
PRLM and the classifier; a total of 9,033 short files
from the OGI corpus were used in PRLM training,
and 158 long files were used in classifier training.
(The small number of long files in the OGI corpus
limited the number of samples comparable in length
to our English-language data described below, so
that only these 158 OGI samples could be used in
classifier training and evaluation.) For English, only
short samples were selected for use in this experi-
ment.
In addition, a total of 3,021 EPT responses were
used in classifier training. As the English profi-
ciency levels of speakers may have an influence
on the accuracy of non-English response detection,
the EPT responses were selected to include simi-
lar numbers of responses for each score level. Re-
sponses were classified into four groups according
to their proficiency scores and 1000 responses were
randomly selected from each group. For score 1
and 4, where the number of available responses was
smaller than 1000, all available responses were se-
lected. Ultimately, 156 responses for score 1, 1000
responses for score 2 and score 3, and 865 responses
for score 4 were selected.
The resultant training and evaluation data sets are
summarized in Table 1.
Due to the lack of non-Engilsh responses in EPT
data, 158 non-English utterances in OGI data were
used in both training and evaluation of non-English
detection. EPT responses were collected from many
different countries, and speakers with 75 different
native languages were participated in the data collec-
tion. Due to the large variations, many of their native
languages were not covered by OGI data. However,
all 9 languages in OGI data were in top 15 L1 lan-
guages and covered approximately 60% of speakers?
163
Partition name Purpose Number of
English files
Number of non-
English files
PRLM-train Training of Language-
dependent LM
1,716 (OGI) 7,317 (OGI)
EN-detection Training and evaluation of non-
English detection classifier
3,021 (EPT) 158 (OGI)
Table 1: Data partition
native languages.
4 Experiment
4.1 Overview
Due to the efficiency in processing time and im-
plementation, a PRLM was implemented instead of
a parallel PRLM. However, the difference between
PRLM and parallel PRLM in this study may not be
significant since PRLM has been shown to be com-
parable to parallel PRLM for test samples longer
than 30 seconds, and the duration of test instances in
this study was longer than 30 seconds. In addition
to PRLM, speaking rate was calculated as a feature.
4.2 PRLM based features
The PRLM based method in this study is composed
of three parts: training of a phone recognizer, train-
ing of language-dependent LMs, and generation of
PRLM-based features. In contrast to the conven-
tional language identification approach that only fo-
cused on identifying the language with the highest
matching score, 6 additional features were imple-
mented to capture the difference between English
model and other models.
Phone recognizer: An English triphone acoustic
model was trained on 30 hours of non-native English
speech (EPT data) using the HTK toolkit (Young et
al., 2002). The model contained 43 monophones
and 4,887 triphones. Due to the difference in the
sampling rate of EPT (11,025 Hz) and the OGI cor-
pus (8,000 Hz), the EPT data was down-sampled to
8,000 Hz and the acoustic model was trained using
the down-sampled data. In order to avoid the in-
fluence of English in phone hypothesis generation,
a triphone bigram language model with a uniform
probability distribution was used as the LM. (All
possible combinations of two triphones were gener-
ated and a uniform probability was assigned to each
combination.) The phone recognition accuracy rate
was 42.7% on the 94 held-out EPT test samples.
This phone recognizer was used in phone hypoth-
esis generation for all data; the same recognizer was
used for all languages.
Language-dependent LMs: For responses in the
PRLM-train partition, phone hypothesis was gener-
ated using the English recognizer. Instead of the
manual transcription, a language-dependent phone
bigram LM was trained using the phone hypothe-
sis. In order to avoid a data sparseness problem, tri-
phones were converted into monophones by remov-
ing left and right context phones, and a bigram LM
with closed vocabulary was trained. 10 language-
dependent bigram LMs, including one for English,
were trained.
PRLM based feature generation: For each re-
sponse in the EN-detection partition, phone hypoth-
esis was generated, and triphones were converted
into monophones. Given monophone hypothesis, an
LM score was calculated for each language using a
language-dependent LM. A total of 10 LM scores
were calculated.
Since the LM score increases as the number of
phones increases, the LM score was normalized by
the number of phones in each response, in order
to avoid the influence of hypothesis length. 7 fea-
tures were generated based on these normalized LM
scores:
? MaxLanguage: The language with the maxi-
mum LM score
? SecondLanguage: The language with the
second-largest LM score.
? MaxScore: Normalized LM score of the
MaxLanguage.
164
? MaxDifference: Difference between normal-
ized English LM score and MaxScore
? MaxRatio: Ratio between normalized English
LM score and MaxScore
? AverageDifference: Difference between nor-
malized English LM score and the average of
normalized LM scores for languages other than
English
? AverageRatio: Ratio between normalized En-
glish LM score and the average of normalized
LM scores for languages other than English
Among above 6 features, 4 features (MaxDiffer-
ence, MaxRatio, AverageDifference, and AverageR-
atio) were designed to measure the difference be-
tween matching of a test responses with English
model and it with the other models. These features
may be particularly effective when MaxLanguage of
the English response is not English; these values will
be close to 0 when the divergence due to non-native
characteristics result in only slightly better match
with other language than that with English.
4.3 Speaking rate calculation
The speaking rate was calculated as a feature rele-
vant to establishing speakers? proficiency level, as
established in previous research. Speaking rate was
calculated from the phone hypothesis as the number
of phones divided by the duration of responses (cf.
Strik and Cucchiarini (1999)).
4.4 Model building
For each response, both PRLM-based features and
speaking rate were calculated, and a decision tree
model was trained to predict binary values (0 for En-
glish and 1 for non-English) using the J48 algorithm
(WEKA implementation of C4.5) of the WEKA ma-
chine learning toolkit (Hall et al, 2009).
Due to the limited number of non-English re-
sponses in the EN-detection partition, three-fold
cross validation was performed during classifier
training and evaluation. The 3,179 responses were
partitioned into three sets to include approximately
same numbers of non-English responses and English
responses for each proficiency score group. Each
partition contained 52 ? 53 non-English responses
and 1007 English responses. In each fold, the de-
cision tree was trained using two of these partitions
and tested on the remaining one.
5 Evaluation
First, the accuracy of the PRLM method was eval-
uated based on multiple forced-choice experiments
with two alternatives using OGI data; in addition
to non-English responses in EN-detection partition,
English responses from the OGI data were used in
this experiment. For each response (in English and
one other language), phone hypothesis was gener-
ated and two normalized LM scores were calculated
using the English LM and the LM for the other lan-
guage. The MaxLanguage was hypothesized as the
source language of the speech. The same experiment
was performed for 9 combinations of English and
other languages. Each experiment was comprised
of 17 English utterances and 17 non-English utter-
ances1. The majority class baseline was thus 0.5.
The mean accuracy of the 9 experiments in this study
was 0.943, which is comparable to (1996)?s perfor-
mance: in his study, the best performing PRLM
exhibited an average accuracy of 0.950. This ini-
tial evaluation used the same data and feature as
Zissman (1996). (Of the seven PRLM-based fea-
tures listed above, only MaxLanguage was used in
(1996)?s study.)
Table 2 summarizes the evaluation results of the
non-English response detection experiments using
three-fold cross-validation within the EN-detection
partition. In order to investigate the impact of dif-
ferent types of features, the features were classi-
fied into four sets?MaxLanguage only, PRLM
(encompassing all PRLM features), SpeakingRate,
and all?and models were trained using each set.
The baseline using majority voting demonstrated an
accuracy of 0.95 by classifying all responses as En-
glish responses.
All models achieved improvements over baseline.
In particular, the model using all features achieved
a 66% relative error reduction over the baseline of
0.95. Furthermore, the all-features model outper-
formed the model based only on PRLM or speaking
1Due to the languages where the available responses were
only 17, the same 17 English responses were used in the all
experiment although 18 responses were available
165
Features Acc. Pre. Rec. F-
score
Base-
line
0.950 0.000 0.000 0.000
Max-
Language
0.969 0.943 0.411 0.572
PRLM 0.966 0.675 0.633 0.649
Speaking-
Rate
0.962 0.886 0.278 0.415
All 0.983 0.909 0.746 0.816
Table 2: Performance of non-English response detection
rate; the accuracy of the all-features model was ap-
proximately 1-2% higher than other models in abso-
lute value and represented approximately a 45-50%
relative error reduction over these models.
The PRLM-based model had higher overall accu-
racy than the speaking rate-based model, and the dif-
ference was even more salient by the F-score mea-
sure: the PRLM-based model achieved an F-score
approximately 24% higher than the speaking rate-
based model.
The model based on all PRLM features did not
achieve a higher accuracy than the model based on
only MaxLanguage. However, there was a clear im-
provement in F-score by using the additional fea-
tures. The PRLM-based model achieved an F-score
approximately 8% higher than the model based only
on MaxLanguage.
In order to investigate the influence of speakers?
proficiency on the accuracy of non-English detec-
tion, the responses in EN-detection were divided
into 4 groups according to proficiency score, and the
performance was calculated for each score group;
the performance of each score group was calcu-
lated using subset comprised of all non-English re-
sponses and English responses with the correspond-
ing scores.
A majority class baseline (classifying all re-
sponses as English) was again used. Table 3 sum-
marizes the results observed, by score level, for the
baseline model and for four different models used in
Table 2. Note that the baseline is lower in Table 3
than in Table 2, because the ratio of English to non-
English responses is lower for each of the subsets of
the EN-detection partitions used for the evaluations
Figure 1: Relationship between proficiency score and
MaxDifference
at a given score level.
For all score groups, the model using all features
achieved high accuracy. The model?s accuracy on all
data sets except for score group 1 was approximately
0.96 and the F-score approximately 0.85. The accu-
racy on score group 1 was 0.87, relatively lower than
other score groups. This is largely due to the smaller
number of English responses available at score level
1, and the consequent lower baseline on this data
set. However, the relative error reduction was much
larger; it was 74% for score group 1.
For all score groups, the PRLM-based mod-
els outperformed MaxLanguage based models and
speaking rate based models. Additional PRLM
features improved the performance over the mod-
els only based on MaxLanguage (conventional lan-
guage identification method). In addition, the com-
bination of both types of features resulted in further
improvement.
The consistent improvement of the model using
both PRLM and speaking rate features suggests a
compensatory relationship between these features.
In order to investigate this relationship in further de-
tail, two representative features, MaxDifference and
AverageDifference were selected, and boxplots were
created. Figure 1 and Figure 2 show the relationship
between proficiency score and PRLM features. In
these figures, the label ?NE? is used to indicate the
non-English group, while the labels 1, 2, 3, and 4
correspond to each score group.
Figure 1 shows that MaxDifference decreases as
166
Score Features Acc. Pre. Rec. F-score
1 Baseline 0.497 0.000 0.000 0.000
MaxLanguage 0.696 0.970 0.411 0.577
PRLM 0.792 0.936 0.633 0.752
SpeakingRate 0.636 1.000 0.278 0.432
All 0.869 0.992 0.746 0.851
2 Baseline 0.865 0.000 0.000 0.000
MaxLanguage 0.919 0.983 0.411 0.579
PRLM 0.930 0.811 0.633 0.709
SpeakingRate 0.901 1.000 0.278 0.432
All 0.962 0.971 0.746 0.843
3 Baseline 0.865 0.000 0.000 0.000
MaxLanguage 0.920 1.000 0.411 0.582
PRLM 0.939 0.903 0.633 0.738
SpeakingRate 0.901 0.983 0.278 0.430
All 0.963 0.976 0.746 0.845
4 Baseline 0.846 0.000 0.000 0.000
MaxLanguage 0.908 0.987 0.411 0.579
PRLM 0.936 0.934 0.633 0.752
SpeakingRate 0.882 0.896 0.278 0.417
All 0.955 0.956 0.746 0.837
Table 3: Performance of non-English detection according to speakers? proficiency level
Figure 2: Relationship between proficiency score and Av-
erageDifference
the speaker?s proficiency decreases, although the
feature displays a large variance. The feature mean
for non-English responses is lower than for score
groups 2, 3, and 4, but the distinction between
non-English and English becomes smaller as the
proficiency score decreases. The feature mean for
score group 1 is even lower than for non-English re-
sponses. This obscures the distinction between En-
glish responses and non-English responses at lower
score levels.
As Figure 2 shows, AverageDifference is rela-
tively stable across score levels, compared to MaxD-
ifference. Although the mean feature value de-
creases as the proficiency score decreases, the de-
crease is smaller than for MaxDifference. In addi-
tion, the mean feature values of the English groups
are consistently higher than those for non-English
responses.
Figure 3 shows the relationship between profi-
ciency score and speaking rate.
For the speaking rate feature, the distinction be-
tween non-English and English responses increases
as speakers? proficiency level decreases, as shown
167
Figure 3: Relationship between proficiency score and
SpeakingRate
in Figure 3. The speaking rate of non-English re-
sponses is the highest among all groups compared,
and the speaking rate decreases for English re-
sponses as the speaker?s proficiency score decreases.
Thus, the PRLM features tend to display better dis-
crimination between English and non-English re-
sponses at the higher end of the proficiency scale,
while the SpeakingRate feature provides better dis-
crimination at the lower end of the scale. By com-
bining both feature classes, we are able to produce
a model which outperforms both a PRLM-based
model and a model using speaking rate alone.
6 Conclusion
In this study, we presented a non-English response
detection method for non-native speakers? speech. A
decision tree model was trained using PRLM-based
features and speaking rate.
The method was intended for use as a supple-
mentary module of an automated speech proficiency
scoring system. The characteristics of non-native
English speech (frequent pronunciation errors) re-
duced the phonetic distinction between English re-
sponses and non-English responses, and correspond-
ingly, the differences between the feature values for
non-English and English speech decreased as well.
In order to address this issue, a speaking rate fea-
ture was added to the model. This feature was spe-
cialized for second language (L2) learners? speech,
as speaking rate has previously proved useful in es-
timating non-native speakers? speaking proficiency.
In contrast to PRLM-based features, the speaking
rate feature showed increasing discrimination be-
tween non-English and English speech samples as
speakers? proficiency level decreased. The com-
plementary relationship between PRLM-based fea-
tures and speaking rate led to an improvement in
the model when these features were combined. Im-
provements resulting from the combined feature set
extended across speakers at all proficiency levels
studied in the context of this paper.
The speaking rate becomes less effective if test
takers speak slowly in their native languages. How-
ever, the test takers are unlikely to use this strategy,
since it will result in a low score although they can
game the system.
Due to lack of non-English responses in EPT data,
non-English utterances were extracted from OGI
data. Since the features in this study were not di-
rectly related to acoustic scores, the acoustic dif-
ferences between EPT and OGI data may not give
significant impact on the results. However, in order
to avoid any influence by differences between cor-
pora, the non-English responses will be collected us-
ing EPT setup and the evaluation will be performed
using new data in future.
References
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
In SIGKDD Explorations, volume 11.
Lori F. Lamel and Jean-Luc Gauvain. 1993. Cross-
lingual experiments with phone recognition. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing, volume 2,
pages 507?510.
Haizhou Li, Bin Ma, and Chin-Hui Lee. 2007. A vec-
tor space modeling approach to spoken language iden-
tification. Audio, Speech and Language Processing,
15:271 ? 284.
Boon Pang Lim, Haizhou Li, and Yu Chen. 2004. Lan-
guage identification through large vocabulary continu-
ous speech recognition. In Proceedings of the 2004 In-
ternational Symposium on Chinese Spoken Language
Processing, pages 49 ? 52.
Yeshwant K. Muthusamy, Ronald A. Cole, and Beat-
rice T. Oshika. 1992. The OGI multi-language tele-
phone speech corpus. In Proceedings of the Inter-
168
national Conference on Spoken Language Processing,
pages 895?898.
Helmer Strik and Catia Cucchiarini. 1999. Automatic as-
sessment of second language learners? fluency. In Pro-
ceedings of the 14th International Congress of Pho-
netic Sciences, pages 759?762, San Francisco, USA.
Steve Young, Gunnar Evermann, Dan Kershaw, Gareth
Moore, Julian Odell, Dave Ollason, Dan Povey,
Valtcho Valtchev, and Phil Woodland. 2002. The HTK
Book (for HTK Version3.2). Microsoft Corporation
and Cambridge University Engineering Department.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring of
non-native spontaneous speech in tests of spoken en-
glish. Speech Communication, 51(10):883 ? 895.
Marc A. Zissman. 1996. Comparison of four ap-
proaches to automatic language identification of tele-
phone speech. Speech and Audio Processing, 4:31 ?
44.
169
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 63?72,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Measuring the Use of Factual Information in Test-Taker Essays
Beata Beigman Klebanov
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
bbeigmanklebanov@ets.org
Derrick Higgins
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
dhiggins@ets.org
Abstract
We describe a study aimed at measuring the
use of factual information in test-taker essays
and assessing its effectiveness for predicting
essay scores. We found medium correlations
with the proposed measures, that remained
significant after the effect of essay length was
factored out. The correlations did not dif-
fer substantionally between a simple, rela-
tively robust measure vs a more sophisticated
measure with better construct validity. Impli-
cations for development of automated essay
scoring systems are discussed.
1 Introduction
Automated scoring of essays deals with various as-
pects of writing, such as grammar, usage, mecha-
nics, as well as organization and content (Attali
and Burstein, 2006). For assessment of content,
the focus is traditionally on topical appropriateness
of the vocabulary (Attali and Burstein, 2006; Lan-
dauer et al, 2003; Louis and Higgins, 2010; Chen
et al, 2010; De and Kopparapu, 2011; Higgins et
al., 2006; Ishioka and Kameda, 2006; Kakkonen et
al., 2005; Kakkonen and Sutinen, 2004; Lemaire
and Dessus, 2001; Rose? et al, 2003; Larkey, 1998),
although recently other aspects, such as detection
of sentiment or figurative language, have started to
attract attention (Beigman Klebanov et al, 2012;
Chang et al, 2006).
The nature of factual information used in an es-
say has not so far been addressed, to our knowledge;
yet a misleading premise, insufficient factual basis,
or an example that flies in the face of the reader?s
knowledge clearly detract from an essay?s quality.
This paper presents a study on assessing the use
of factual knowledge in argumentative essays on ge-
neral topics written for a graduate school entrance
exam. We propose a definition of fact, and an opera-
tionalization thereof. We find that the proposed mea-
sure has positive medium-strength correlation with
essay grade, which remains significant after the im-
pact of essay length is factored out. In order to
quantify which aspects of the measure drive the ob-
served correlations, we gradually relax the measure-
ment procedure, down to a simple and robust proxy
measure. Surprisingly, we find that the correlations
do not change throughout the relaxation process. We
discuss the findings in the context of validity vs re-
liability of measurement, and point out implications
for automated essay scoring.
2 What is a Fact?
To help articulate the notion of fact, we use the fol-
lowing definition from a seminal text in argumenta-
tion theory: ?... in the context of argumentation, the
notion of fact is uniquely characterized by the idea
that is held of agreements of a certain type relating
to certain data, those which refer to an objective rea-
lity, and, in Poincare?s words, designate essentially
?what is common to several thinking beings, and
could be common to all? (Perelman and Olbrechts-
Tyteca, 1969, 67). Factuality is thus a matter of se-
lecting certain kinds of data and securing a certain
type of agreement over those data.
Of the different statements that refer to objec-
tive reality, the term facts is used to ?designate ob-
63
jects of precise, limited agreement? (Perelman and
Olbrechts-Tyteca, 1969, 69). These are contrasted
with presumptions ? statements connected to what
is normal and likely (ibid.). We suggest that the dis-
tinctions in the scope of the required agreement can
be related to the referential device used in a state-
ment: If the reference is more rigid (Kripke, 1980),
that is, less prone to change in time and to inde-
terminacy of the boundaries, the scope of the ne-
cessary agreement is likely to be more precise and
limited. With proper names prototypically being the
most rigid designators, we will focus our efforts on
statements about named entities.1
Perhaps the simplest model of the universal au-
dience is an encyclopedia ? a body of knowledge
that is verified by experts, and is, therefore, ?com-
mon to several thinking beings, and could be com-
mon to all? by virtue of the authority of the experts
and the wide availability of the resource. However,
many facts known to various groups of people that
could be known to all are absent from any encyclo-
pedia. The knowledge contained in the WWW at
large, reaching not only statements explicitly con-
tributed to an encyclopedia but also those made by
people on their blogs ? is perhaps as close as it gets
to a working model of the universal audience.
Recent developments in Open Information Ex-
traction make it possible to tap into this vast know-
ledge resource. Indeed, fact-checking is one of the
applications the developers of OpenIE have in mind
for their emergent technology (Etzioni et al, 2008).
3 Open Information Extraction
Traditionally, the goal of an information extrac-
tion system is automated population of structured
databases of events or concepts of interest and their
properties by analyzing large corpora of text (Chin-
chor et al, 1993; Onyshkevych, 1993; Grishman and
Sundheim, 1995; Ravichandran and Hovy, 2002;
Agichtein and Gravano, 2000; Davidov and Rap-
poport, 2009).
1For example, Barack Obama picks out precisely one per-
son, and the same one in 2010 as it did in 1990. In contrast, the
current US president picks out different people every 4-8 years.
For indeteminacy of boundaries, consider a statement like US
officials are wealthy. To determine its truth, one must first se-
cure agreement on acceptable referents of US officials.
In contrast, the recently proposed Open Informa-
tion Extraction paradigm aims to detect related pairs
of entities without knowing in advance what kinds of
relations exist between entities in the source data and
without any seeding (Banko and Etzioni, 2008). The
possibility of such extraction in English is attributed
by the authors to a small number of syntactic pat-
terns that realize binary relations between entities.
In particular, they found that almost 40% of such re-
lations are realized by the argument-verb-argument
pattern (henceforth, AVA) (see Table 1 in Banko and
Etzioni (2008)).
The TextRunner system (Banko and Etzioni,
2008) is trained using a CRF classifier on S-V-O
tuples from a parsed corpus as positive examples,
and tuples that violate phrasal structure as negative
ones. The examples are described using features
that do not require parsing or semantic role labe-
ling. Features include part-of-speech tags, regular
expressions (detecting capitalization, punctuation,
etc.), context words belonging to closed classes, and
conjunctions of features occurring in adjacent posi-
tions within six words of the current word.
TextRunner achieves P=0.94, R=0.65, and F-
Score=0.77 on the AVA pattern (Banko and Etzioni,
2008). We note that all relations in the test sen-
tences involve a predicate connecting two named en-
tities, or a named entity and a date.2 The authors
kindly made available to us for research purposes a
database of about 2 bln AVA extractions produced
by TextRunner; this database was used in the expe-
riments reported below.
4 Data
We randomly sampled essays written on 10 diffe-
rent prompts, 200 essays per prompt. Essays are
graded on the scale of 1-6; the distribution of grades
is shown in table 1.
Grade 1 2 3 4 5 6
% 0.6 4.9 23.5 42.6 23.8 4.7
Table 1: The distribution of grades for 2,000 essays.
2http://www.cs.washington.edu/research/knowitall/hlt-
naacl08-data.txt
64
5 Building Queries from Essays
We define a query as a 3-tuple <NE,?,NP>,3 where
NE is a named entity and NP is a noun phrase from
the same or neighboring sentence in a test-taker es-
say (the selection process is described in section
5.2). We use the pattern of predicate matches against
the TextRunner database to assess the degree and the
equivocality of the connection between NE and NP.
5.1 Named Entities in Test-Taker Essay
We use the Stanford Named Entity Recognizer
(Finkel et al, 2005) that tags named entities as peo-
ple, locations, organizations, and miscellaneous. We
annotated a sample of 90 essays for named entities;
the sample yielded 442 tokens, which we classified
as shown in Table 2. The Enamex classes (people,
locations, organizations) account for 58% of all the
entities in the sample. The recognizer?s recall of
people and locations is excellent (though they are
not always classified correctly ? see caption of Ta-
ble 2), although test-taker essays feature additional
entity types that are not detected as well.
Category Recall Examples
Location 0.98 Iraq, USA
Person 0.96 George W. Bush, Freud
Org. 0.87 Guggenheim Foundation
Gov. 0.79 No Child Left Behind
Awards 0.79 Nobel Prize
Events 0.68 Civil War, World War I
Sci & Tech 0.59 GPS, Windows 3.11
Art 0.44 Beowulf, Little Women
Table 2: Recall of the Stanford NER by category. Note
that an entity is counted as recalled as long as it is iden-
tified as belonging to any NE category, even if it is mis-
classified. For example, Freud is tagged as location, but
we count it towards the recall of people.
In terms of precision, we observed that the tagger
made few clear mistakes, such as tagging sentence-
initial adverbs and their mis-spelled versions as
named entities (Eventhough, Afterall). The bulk of
3We do not attempt matching the predicate, as (1) in many
cases there is no clearly lexicalized predicate (see the discussion
of single step patterns in section 5.2) and (2) adding a predicate
field would make matches against the database sparser (see sec-
tion 6.1).
the 96 items over-generated by the tagger are in the
?grey area? ? while we haven?t marked them, they
are not clearly mistakes. A common case are names
of national and religious groups, such as Muslim
or Turkish, or capitalizations of otherwise common
nouns for emphasis and elevation, such as Arts or
Masters. Given our objective to ground the queries
in items with specific referents, these are less sui-
table. If all such cases are counted as mistakes, the
tagger?s precision is 82%.
5.2 Selection of NPs
We employ a grammar-based approach for selecting
NPs. We use the Stanford dependency parser (de
Marneffe et al, 2006; Klein and Manning, 2003) to
determine dependency relations.
In order to find out which dependency paths con-
nect between named entities and clearly related NPs
in essays, we manually marked concepts related to
95 NEs in 10 randomly sampled essays. We marked
210 query-able concepts in total. The resulting 210
dependency paths were classified according to the
direction of the movement.
Out of the 210 paths, 51 (24%) contain a single
upward or downard step, that is, are cases where
the NE is the head of the constituent in which the
NP is embedded, or the other way around. Some
examples are shown in Figure 1. Note that the pre-
dicate connecting NE and NP is not lexicalized, but
the existence of connection is signaled by the close-
knit grammatical pattern.
The most prolific family of paths starts with an
upward step, followed by a sequences of 1-4 down-
wards steps; 71 (34%) of all paths are of this type.
Most typically, the first upward move connects the
NE to the predicate of which it is an argument, and,
down from there, to either the head of another argu-
ment (??) or to an argument?s head?s modifier (???).
These are explicit relations, where the relation is
typically lexicalized by the predicate.
We expand the context of extraction beyond a sin-
gle sentence only for NEs classified as PERSON. We
apply a gazetteer of private names by gender from
US Census 2010 to expand a NE of a given gen-
der with the appropriate personal pronouns; a word
that is a part of the original name (only surname, for
4NE=Kroemer; NP=Heterojunction Bipolar Transitor
65
? a Nobel Prize in a science field
? Chaucer, in the 14 century, ...
? the prestige of the Nobel Prize
? Kidman?s talent
?? Kroemer received the Nobel Prize
??? Kroemer received the Nobel Prize for his work
on the Heterojunction Bipolar Transitor4
Figure 1: Examples of dependency paths used for query
construction.
example), is also considered an anaphor and a can-
didate for expansion. We expand the context of the
PERSON entity as long as the subsequent sentence
uses any of the anaphors for the name. This way, we
hope to capture an extended discussion of a named
entity and construct queries around its anaphoric
mentions just as we do around the regular, NE men-
tion. A name that is not predominantly male or fe-
male is not expanded with personal pronouns. Ta-
ble 3 shows the distribution of queries automatically
generated from the sample of 2,000 essays.
? 2,817 15.9%
? 798 4.5%
?? 813 4.6%
?? 372 2.1%
?? 4,940 27.8%
??? 2,691 15.1%
???? 1,568 8.8%
??? 3,772 21.2%
total 17,771 100%
Table 3: Distribution of queries by path type.
6 Matching and Filtering Queries
6.1 Relaxation for improved matching
To estimate the coverage of the fact repository with
respect to the queries extracted from essays, we sub-
mit each query to the TextRunner repository in the
<NE,?,NP> format and record the number of times
the repository returned any matches at all. The per-
centage of matched queries is 21%. To increase the
chances of finding a match, we process the NP to re-
move determiners and pre-modifiers of the head that
are very frequent words, such as removing a very
from a very beautiful photograph.
Additionally, we produce three variants of the NP.
The first, NP1, contains only the sequence of nouns
ending with the head noun; in the example, NP1
would be photograph. The second variant, NP2,
contains only the word that is rarest in the whole
of NP. All capitalized words are given the lowest
frequency of 1. Thus, if any of the NP words are
capitalized, the NP2 would either contain an out of
vocabulary word to the left of the first capitalized
word, or the leftmost capitalized word. This means
that names would typically be split such that only the
first name is taken. For example, the NP the author
Orhan Phamuk would generate NP2 Orhan. When
no capitalized words exist, we take the rarest one,
thus a NP category 3 hurricane would yield NP2
hurricane. The third variant only applies to NPs
with capitalized parts, and takes the rightmost capi-
talized word in the query. Thus, the NP the actress
Nicole Kidman would yield NP3 Kidman.
Applying these procedures to every NP inflates
the number of actual queries posed to the TextRun-
ner repository by almost two-fold (31,211 instead of
17,771), while yielding a 50% increase in the num-
ber of cases where at least one variant of the original
query had at least one match against the repository
(from 21% to 35%).
6.2 Match-specific filters
In order to zero in on matches that correpond to fac-
tual statements and indeed pertain to the queried ar-
guments, we implement a number of filters.
Predicate filters
We filter out modal and hedged predicates, using
lists of relevant markers. We remove predicates like
might turn out to be or possibly attended, as well as
future tense predicates (marked with will).
Argument filters
For matches that passed the predicate filters, we
check the arguments. Let mARG be the actual
string that matched ARG (ARG ?{NE,NP}). Let
EC (Essay Context) refer to source sentence(s) in
66
the essay.5 We filter out the following matches:
? Capitalized words follow ARG in mARG that
are not in EC;
? >1 capitalized or rare words precede ARG in
mARG that are not in EC and not honorifics;
? mARG is longer than 8 words;
? More than 3 words follow ARG in mARG.
The filters target cases where mARG is more spe-
cific than ARG, and so the connection to ARG might
be tenuous, such as ARG=Harriet Beecher Stowe,
mARG = Harriet Beecher Stowe Center.
6.3 Filters based on overall pattern of matches
6.3.1 Negation filter
For all matches for a given query that passed the
filters in section 6.2, we tally positive vs negative
predicates.6 If the ratio of negative to positive is
above a threshold (we use 0.1), we consider the
query an unsuitable candidate for being ?potentially
common to all,? and therefore do not credit the au-
thor with having mentioned a fact.
This criterion of potential acceptance by a uni-
versal audience fails a query such as <Barack
Obama,?,US citizen>, based on the following pat-
tern of matches:
Count Predicate
10 is not
4 is
2 was always
1 is really
1 isn?t
1 was not
In a similar fashion, an essay writer?s statement
that ?The beating of Rodney King in Los Angeles
... made for tense race relations? is not quite in ac-
cord with the 16 hits garnered by the statement ?The
Los Angeles riots were not caused by the Rodney
King verdict,? against other hits with predicates like
erupted after, occurred after, resulted from, were
sparked by, followed.
5A single sentence, unless anaphor-based expansion was
carried out; see section 5.2.
6We use a list of negation markers to detect those.
Somewhat more subtly, the connection between
Albert Einstein and atomic bomb, articulated as ?For
example, Albert Einstein?s accidental development
of the atomic bomb has created a belligerent tech-
nological front? by a test-taker, is opposed by 6 hits
with the predicate did not build against matches with
predicates such as paved the way to, led indirectly
to, helped in, created the theory of. The conflicting
accounts seem to reflect a lack of consensus on the
degree of Einstein?s responsibility.
The cases above clearly demonstrate the implica-
tions of the argumentative notion of facts used in
our project. Facts are statements that the audience is
prepared to accept without further justification, dif-
ferently from arguments, and even from presump-
tions (statements about what is normal and likely),
for which, as Perelman and Olbrechts-Tyteca (1969)
observe, ?additional justification is beneficial for
strengthening the audience?s adherence.? Certainly
in the Obama case and possibly in others, a different
notion of factuality, for example, a notion that em-
phasizes availability of legally acceptable suppor-
ting evidence, would have led to a different result.
Yet, in an ongoing instance of argumentation, the
mere need to resort to such a proof is already a sign
that the audience is not prepared to accept a state-
ment as a fact.
6.4 Additional filters
We also implemented a number of filters aimed at
detecting excessive diversity in the matches, which
could suggest that there is no clear and systema-
tic relation between the NE and the NP. The filters
are conjunctions of thresholds operating over mea-
sures such as purity of matches (percentage of exact
matches in NE or NP), degree of overlap of non-pure
matches with the context of the query in the essay,
clustering of the predicates (recurrence of the same
predicates across matches), general frequencies of
NE and NP.
7 Evaluation
7.1 Manual check of queries
A manual check of a small subset of queries was ini-
tially intended as an interim evaluation of the query
construction process, to see how often the produced
queries are deficient candidates for later verification.
67
However, we also decided to include a human fact-
check of the queries that were found to be verifiable,
to see the kinds of factual mistakes made in essays.
A research assistant was asked to classify 500
queries into Wrong (the NE and NP are not
related in the essay), Trivial (almost any NE
could be substituted, as in <WWI,?, Historians>),
Subjective (<T.S.Eliot,?,the most frightening poet
of all time>), VC ? verifiable and correct, VI ? veri-
fiable and incorrect. Table 4 shows the distribution.
W T S VC VI
18% 13% 13% 54% 2%
Table 4: The distribution of query types for 500 queries.
Queries classified as Wrong (18%) mostly cor-
respond to parser mistakes. Trivial and Subjective
queries, while not attributing to the author connec-
tions that she has not made, are of questionable value
as far as fact-checking goes. Perhaps the most sur-
prising figure is the meager amount of verifiable and
incorrect queries. Examples of relevant statements
from essays include (NE and NP are boldfaced):
? For example, Paul Gaugin who was a sucess-
ful business man, with a respectable wife and
family, suddenly gave in to the calling of the
arts and left his life. (He was a failing busi-
nessman immediately before leaving family.)
? For example, in Jane Austin?s Little Women,
she portrays the image of a lovely family and
the wonders of womenhood. (The book is by
Louisa May Alcott.)
? This occurrence can be seen with the Rod-
ney King problem in California during the late
1980?s. (The Rodney King incident occurred
on March 3, 1991).
? We see the philosophers Aristotle, Plato,
Socrates and their practical writings of the
political problems and issues of the day.
(Socrates is not known to have left writings.)
First, we observe that factual mistakes are rare.
Furthermore, they seem to pertain to one in a series
of related facts, most of which are correct and testify
to the author?s substantial knowledge about the mat-
ter ? consider Paul Gaugin?s biography or the con-
tents of ?Little Women? in the examples above. It
is therefore unclear how detrimental the occasional
factual ?glitches? are to the quality of the essay.
8 Application to Essay Scoring
We show Pearson correlations between human
scores given to essays and a number of characte-
ristics derived from the work described here, as well
as the partial correlations when the effect of essay
length is factored out. We calculated both the cor-
relations using raw numbers and on a logarithmic
scale, with the latter generally producing higher cor-
realtions. Therefore, we are reporting the correla-
tions between grade and the logarithm of the rele-
vant characteristic. The characteristics are:
#NE The number of NE tokens in an essay.
#Queries The number of queries generated by the
system from the given essay (as described in
section 5.2).
#Matched Queries The number of queries for
which a match was found in the TextRunner
database. If the original query or any of its ex-
pansion variants (see section 6.1) had matches,
the query contributes a count of 1.
#Filtered Matches The number of queries that
passed the filters introduced in section 6. If the
original query or any of its expansion variants
passed the filters, the query contributes a count
of 1.
Table 5 shows the results. First, we find that all
correlations are significant at p=0.05, as well as the
partial correlations exluding the effect of length for 7
out of 10 prompts. All correlations are positive, that
is, the more factual information a writer employs in
an essay, the higher the grade ? beyond the oft re-
ported correlations between the grade and the length
of an essay (Powers, 2005).
Second, we notice that all characteristics ? from
the number of named entities to the number of fil-
tered matches ? produce similar correlation figures.
Third, there are large differences between average
numbers of named entities per essay across prompts.
68
Prompt NE Pearson Corr. with Grade Partial Corr. Removing Length
#NE #Q #Mat. # Filt. #NE #Q #Mat. # Filt.
P1 280 0.144 0.154 0.182 0.185 0.006 0.019 0.058 0.076
P2 406 0.265 0.259 0.274 0.225 0.039 0.053 0.072 0.069
P3 452 0.245 0.225 0.188 0.203 0.049 0.033 0.009 0.051
P4 658 0.327 0.302 0.335 0.327 0.165 0.159 0.177 0.160
P5 704 0.470 0.477 0.473 0.471 0.287 0.294 0.304 0.305
P6 750 0.429 0.415 0.388 0.373 0.271 0.242 0.244 0.257
P7 785 0.470 0.463 0.479 0.469 0.302 0.302 0.341 0.326
P8 838 0.423 0.390 0.406 0.363 0.264 0.228 0.266 0.225
P9 919 0.398 0.445 0.426 0.393 0.158 0.209 0.233 0.219
P10 986 0.455 0.438 0.375 0.336 0.261 0.257 0.170 0.175
AV. 678 0.363 0.357 0.353 0.335 0.180 0.180 0.187 0.186
Table 5: Pearson correlation and partial correlation removing the effect of length between a number of characteristics
(all on a log scale) and the grade. The second column shows the total number of identified named entities in the
200-essay sample from the given prompt. The prompts are sorted by the second column.
Generally, the higher the number, the better the num-
ber of named entities in the essay predicts its grade
(the more NEs the higher the grade). This suggests
that the use of named entities might be relatively
irrelevant for some prompts, and much more rele-
vant for others. For example, prompt P10 reads
?The arts (painting, music, literature, etc.) reveal
the otherwise hidden ideas and impulses of a soci-
ety,? thus practically inviting exemplification using
specific works of art or art movements, while suc-
cess with prompt P1 ? ?The human mind will al-
ways be superior to machines because machines are
only tools of human minds? ? is apparently not as
dependent on named entity based exemplification.
Excluding prompts with smaller than average total
number of named entities (<678), the correlations
average 0.40-0.44 across the various characteristics,
with partial correlations averaging 0.25-0.26.
9 Discussion and Conclusion
9.1 Summary of the main result
In this article, we proposed a way to measure the
use of factual information in text-taker essays. We
demonstrated that the use of factual information is
indicative of essay quality, observing positive corre-
lations between the count of instances of fact-use in
essays and the grade of the essay, beyond what can
be attributed to a correlation between the total num-
ber of words in an essay and the grade.
9.2 What is driving the correlations?
We also investigated which of the components of
the fact-use measure were responsible for the ob-
served correlations. Specifically, we considered (a)
the number instances of fact-use that were verified
against a database of human-produced assertions,
filtered for controversy and excessive diversity; (b)
the number of instances of fact-use that were verified
against the database, without subsequent filtering;
(c) the number of instances of fact-use identified in
an essay (without checking against the database); (d)
the number of named entities used in an essay (with-
out constructing queries around the entity). These
steps correspond to a gradual relaxation of the full
fact-checking procedure all the way to a proxy mea-
sure that counts the number of named entities.
We observed similar correlations throughout the
relaxation procedure. We therefore conclude that the
number of named entities is the driving force behind
the correlations, with no observed effect of the query
construction and verification procedures.7 This re-
sult could be explained by two factors.
First, a manual check of 500 queries showed that
factual mistakes are rare ? only 2% of the queries
corresponded to factually incorrect statements. Fur-
thermore, mistakes were often accompanied by the
7While the trend is in the direction of an increase in Pearson
correlations from (a) to (d), the differences are not statistically
significant.
69
test-taker?s use of additional facts about the same en-
tity which were correct; this might alleviate the im-
pact of a mistake in the eyes of a grader.
Second, the query verification procedure applied
to only about 35% of the queries ? those for which
at least one match was found in the database, that
is, 65% of the queries could not be assessed using
the database of 2 bln extractions. The verification
procedure is thus much less robust than the proce-
dure for detecting named entities, which performs at
above >80% recall and precision.
9.3 Implications for automated scoring
Our results suggest that essays on a general topic
written by adults for a high-stakes exam contain
few incorrect facts, so the potential for a full fact-
checking system to improve correlations with grades
beyond merely detecting the potential for a factual
statement using a named entity recognizer is not
large. While a measure based on the number of
?verified? facts found in an essay demonstrated a
significant correlation with human scores beyond
the contribution of essay length, a simpler measure
based only on the number of named entities in the
essay demonstrated a similar relationship with hu-
man scores.
Given the similarity in the two features? empiri-
cal usefulness, it would seem that the feature that
counts the number of named entities in an essay is a
better candidate, due to its simplicity and robustness.
However, there is another perspective from which a
feature based only on the number of named entities
in an essay may be less suitable for use in scoring:
the perspective of construct validity, the degree to
which a test (or, in this case, a scoring system) ac-
tually measures what it purports to. As mentioned
above, the number of named entities in an essay is,
at best, a proxy measure,8 roughly indicative of the
referencing of factual statements in support of an ar-
gument within an essay. Because the measure itself
is not directly sensitive to how named entities are
used in the essay, though, even entities with no con-
nection to the essay topic would tend to contribute
to the score, and the measure is therefore vulnerable
to manipulation by test-takers.
8For a discussion of proxes vs trins in essay grading, see
(Page and Petersen, 1995).
An obvious strategy to exploit this scoring mecha-
nism would be to simply include more named enti-
ties in an essay, either interspersing them randomly
throughout the text, or including them in long lists of
examples to illustrate a single point. Such a blatant
approach could potentially be detected by the use of
a filter or advisory (Higgins et al, 2006; Landauer
et al, 2003) designed to identify anomalous writing
strategies. However, there could be more subtle ap-
proaches to exploiting such a feature. For example,
it is possible that test-takers might be inclined to in-
crease their use of named entities by adducing more
facts in support of an argument, and would go be-
yond the comfort zone of their actual factual know-
ledge, thus making more factual mistakes. Test gam-
ing strategies have been recognized as a threat to au-
tomated scoring systems for some time (Powers et
al., 2001), and there is evidence based on test tak-
ers? own self-reported behavior that this threat is real
(Powers, 2011). This is one major reason why large-
scale operational testing programs (such as GRE or
TOEFL) use automated essay scoring only in com-
bination with human ratings. In sum, the degree to
which a linguistic feature is predictive of human es-
say scores is not the only criterion for evaluation; the
washback effects of using the feature (on writing be-
havior and on instruction) must also be considered.
The second finding of this study is that the ef-
fectiveness of fact-checking for essay assessment is
compromised by the limited coverage of the wealth
of factual statements made by essay writers, with
only 35% of queries garnering any hits at all in a
large general-purpose database of assertions. It is
possible, however, that OpenIE technology can be
used to collect more focused repositories on specific
topics, such as the history of the American Civil
War, which could be used to assess responses to
tasks related to that particular subject matter. This
is one of the directions of our future research.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the 5th ACM conference on Digital
Libraries, pages 85?94. ACM.
Yigal Attali and Jill Burstein. 2006. Automated Es-
70
say Scoring With e-rater R?V.2. Journal of Technology,
Learning, and Assessment, 4(3).
Michele Banko and Oren Etzioni. 2008. The Tradeoffs
Between Open and Traditional Relation Extraction. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 28?36,
Columbus, OH, June. Association for Computational
Linguistics.
Beata Beigman Klebanov, Jill Burstein, Nitin Madnani,
Adam Faulkner, and Joel Tetreault. 2012. Building
Subjectivity Lexicon(s) From Scratch For Essay Data.
In Proceedings of CICLING, New Delhi, India.
Tao-Hsing Chang, Chia-Hoang Lee, and Yu-Ming
Chang. 2006. Enhancing Automatic Chinese Es-
say Scoring System from Figures-of-Speech. In Pro-
ceedings of the 20th Pacific Asia Conference on Lan-
guage, Information and Computation, pages 28?34.
Yen-Yu Chen, Chien-Liang Liu, Chia-Hoang Lee, and
Tao-Hsing Chang. 2010. An Unsupervised Auto-
mated Essay Scoring System. IEEE Transactions on
Intelligent Systems, 25(5):61?67.
Nancy Chinchor, Lynette Hirschman, and David Lewis.
1993. Evaluating Message Understanding Systems:
An Analysis of the Third Message Understanding
Conference (MUC-3). Computational Linguistics,
19(3):409?449.
Dmitry Davidov and Ari Rappoport. 2009. Geo-mining:
Discovery of Road and Transport Networks Using Di-
rectional Patterns. In Proceedings of EMNLP, pages
267?275.
Arijit De and Sunil Kopparapu. 2011. An unsupervised
approach to automated selection of good essays. In
Recent Advances in Intelligent Computational Systems
(RAICS), 2011 IEEE, pages 662 ?666.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating Typed De-
pendency Parses from Phrase Structure Parses. In Pro-
ceedings of LREC, pages 449?454, Genoa, Italy, May.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel Weld. 2008. Open information extraction from
the web. Commun. ACM, 51(12):68?74.
Jenny Finkel, Trond Grenager, and Christopher Manning.
2005. Incorporating Non-local Information into In-
formation Extraction Systems by Gibbs Sampling. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 363?370,
Ann Arbor, MI, June. Association for Computational
Linguistics.
Ralph Grishman and Beth Sundheim. 1995. Design of
the MUC-6 evaluation. In Proceedings of MUC, pages
1?11.
Derrick Higgins, Jill Burstein, and Yigal Attali. 2006.
Identifying off-topic student essays without topic-
specific training data. Natural Language Engineering,
12(2):145?159.
Tsunenori Ishioka and Masayuki Kameda. 2006. Auto-
mated Japanese Essay Scoring System based on Arti-
cles Written by Experts. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 233?240, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Tuomo Kakkonen and Erkki Sutinen. 2004. Automatic
assessment of the content of essays based on course
materials. In Proceedings of the International Confer-
ence on Information Technology: Research and Edu-
cation, pages 126?130, London, UK.
Tuomo Kakkonen, Niko Myller, Jari Timonen, and Erkki
Sutinen. 2005. Automatic Essay Grading with Prob-
abilistic Latent Semantic Analysis. In Proceedings of
the Second Workshop on Building Educational Appli-
cations Using NLP, pages 29?36, Ann Arbor, Michi-
gan, June. Association for Computational Linguistics.
Dan Klein and Christopher Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
Saul Kripke. 1980. Naming and Necessity. Harvard Uni-
versity Press.
Thomas Landauer, Darrell Laham, and Peter Foltz. 2003.
Automated scoring and annotation of essays with the
Intelligent Essay Assessor. In Mark Shermis and Jill
Burstein, editors, Automated essay scoring: A cross-
disciplinary perspective, pages 87?112. Lawrence Erl-
baum Associates, Mahwah, New Jersey.
Leah Larkey. 1998. Automatic essay grading using text
categorization techniques. In Proceedings of SIGIR,
pages 90?95, Melbourne, AU.
Beno??t Lemaire and Philippe Dessus. 2001. A System to
Assess the Semantic Content of Student Essays. Jour-
nal of Educational Computing Research, 24:305?320.
Annie Louis and Derrick Higgins. 2010. Off-topic essay
detection using short prompt texts. In Proceedings of
the NAACL HLT 2010 Fifth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 92?95, Los Angeles, California, June. Associ-
ation for Computational Linguistics.
Boyan Onyshkevych. 1993. Template design for infor-
mation extraction. In Proceedings of MUC, pages 19?
23.
Ellis Page and Nancy Petersen. 1995. The computer
moves into essay grading: Updating the ancient test.
Phi Delta Kappan, 76:561?565.
Cha??m Perelman and Lucie Olbrechts-Tyteca. 1969. The
New Rhetoric: A Treatise on Argumentation. Notre
71
Dame, Indiana: University of Notre Dame Press.
Translated by John Wilkinson and Purcell Weaver
from French original published in 1958.
Donald Powers, Jill Burstein, Martin Chodorow, Mary
Fowles, and Karen Kukich. 2001. Stumping
E-Rater: Challenging the Validity of Automated
Essay Scoring. ETS research report RR-01-03,
http://www.ets.org/research/policy research reports/rr-
01-03.
Donald Powers. 2005. ?Wordiness?: A selective review
of its influence, and suggestions for investigating
its relevance in tests requiring extended written
responses. ETS research memorandum RM-04-08,
http://www.ets.org/research/policy research reports/rm-
04-08.
Donald Powers. 2011. Scoring the TOEFL
Independent Essay Automatically: Re-
actions of Test Takers and Test Score
Users. ETS research manuscript RM-11-34,
http://www.ets.org/research/policy research reports/rm-
11-34.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a Question Answering System.
In Proceedings of ACL, pages 41?47.
Carolyn Rose?, Antonio Roqueand, Dumisizwe Bhembe,
and Kurt VanLehn. 2003. A hybrid text classifica-
tion approach for analysis of student essays. In Pro-
ceedings of the Second Workshop on Building Educa-
tional Applications Using NLP, pages 29?36.
72
