Dynamic Lexical Acquisition in Chinese Sentence Analysis 
 
Andi Wu 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
andiwu@microsoft.com 
Joseph Pentheroudakis 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
josephp@microsoft.com 
Zixin Jiang 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
      jiangz@microsoft.com 
                         Abstract  
Dynamic lexical acquisition is a procedure 
where the lexicon of an NLP system is 
updated automatically during sentence 
analysis.  In our system, new words and new 
attributes are proposed online according to 
the context of each sentence, and then get 
accepted or rejected during syntactic analysis.  
The accepted lexical information is stored in 
an auxiliary lexicon which can be used in 
conjunction with the existing dictionary in 
subsequent processing.  In this way, we are 
able to process sentences with an incomplete 
lexicon and fill in the missing info without 
the need of human editing.  As the auxiliary 
lexicons are corpus-based, domain-specific 
dictionaries can be created automatically by 
combining the existing dictionary with 
different auxiliary lexicons.  Evaluation 
shows that this mechanism significantly 
improves the coverage of our parser.    
Introduction 
The quality of many NLP systems depends 
heavily on the completeness of the dictionary 
they use.  However, no dictionary can ever be 
complete since new words are being coined 
constantly and the properties of existing words 
can change over time.  In addition, a dictionary 
can be relatively complete for a given domain but 
massively incomplete for a different domain.  
The traditional way to make a dictionary more 
complete is to edit the dictionary itself, either by 
hand or through batch updates using data 
obtained from other sources.  This approach is 
undesirable because 
(1) it can be very expensive due to the 
amount of hand work required; 
(2) the job will never be complete since new 
words and new usages of words will 
continue to appear. 
(3) certain words and usages of words decay 
after a while or  only exist in a certain 
domain, and it is inappropriate to make 
them a permanent part of the dictionary. 
This paper discusses an alternative approach 
where, instead of editing a static dictionary, we 
acquire lexical information dynamically during 
sentence analysis.  This approach is currently 
implemented in our Chinese system and Chinese 
examples will be used to illustrate the process.  In 
Section 1, we will discuss how the new lexical 
information is discovered.  Section 2 discusses 
how such information is filtered, lexicalized, and 
used in future processing.  Section 3 is devoted to 
evaluation.  
1 Proposing words and attributes 
Two major types of lexical information are being 
acquired dynamically in our current Chinese 
system: new words and new grammatical 
attributes such as parts of speech (POS) and 
sub-categorization frames.   The acquisition 
assumes the availability of an existing dictionary 
which is relatively mature though incomplete in 
many ways.  In our case, we have a lexicon of 
88,000 entries with grammatical attributes in 
most of them.  Our assumption is that, once a 
dictionary has reached this scale, we should have 
enough information to predict the missing 
information in the context of sentence analysis.  
We can then stop hand-editing the static 
dictionary and let dynamic lexical acquisition 
take over. 
 
In most cases, the grammatical properties of a 
word define the syntactic context in which this 
word may appear.  Therefore, it is often possible 
to detect the grammatical properties of a word by 
looking at the surrounding context of this word in 
a sentence.  In fact, this is one of the main criteria 
used by lexicographers, who often apply a 
conscious or subconscious contextual ?template? 
for each grammatical property they assign.  We 
have coded those templates in our system so that 
a computer can make similar judgments.1  When 
a word is found to fit into a template for a given 
property but we do not have that property in the 
dictionary yet, we can make a guess and propose 
to add that property.  Our current Chinese system 
has 29 such templates, 14 for detecting new 
words and 15 for detecting new grammatical 
attributes for new or existing words.   
1.1 Proposing new words 
Two types of unlisted words exist in Chinese: 
(1) single-character bound morphemes used 
as words; 
(2) new combinations of characters as words. 
An example of Type (1) is ?.  This is a bound 
morpheme in our dictionary, appearing only as a 
part in words like ??? (have a good chat).   
However, like many other bound morphemes in 
Chinese, it can occasionally be used as an 
independent word, as in the following sentence: 
 
?  ?  ? ?       ?  ?   ?   ?  ?? 
 he  at  I  home  chat LE  two CL  hour 
He chatted for two hours at my house. 
 
The usual response to this problem is to treat it as 
a lexical gap and edit the entry of ? to make it a 
verb in the dictionary.  This is undesirable for at 
least two reasons.  First of all, many bound 
morphemes in Chinese can be occasionally used 
as words and making all of them independent 
words will introduce a lot of noise in sentence 
analysis.  Secondly, it will be a difficult task for 
lexicographers, not just because it takes time, but 
because the lexicographers will often be unable 
to make the decision unless they see sentences 
where a given bound morpheme is used as a 
word.   
 
In our system, we leave the existing dictionary 
untouched.  Instead, we ?promote? a bound 
                                                     
1
 Currently these templates are hand-coded heuristics 
based on linguists? intuition.  We are planning to use 
machine learning techniques to acquire those 
templates automatically. 
morpheme to be a word dynamically when it 
appears in certain contextual templates.  The 
template that promotes ?  to be a verb may 
include conditions such as: 
? not subsumed by a longer word, such as   
????; 
? being part of an existing multiple-character 
verb, such as ?  in ????; 
? followed by an aspect marker, such as ; 
? etc. 
Currently we have 4 such templates, promoting 
morphemes to nouns, verbs, adjectives and 
adverbs respectively. 
 
Examples of Type (2) are found all the time and 
adding them all to the existing dictionary will be 
a never-ending job.  Here is an example: 
 
?    ?   ?? ?? ?  ?? ?? ?  ?? 
not need again start then can  dock  or  undock  
     ??            ?? 
easy-to-carry  computer  
You can dock and undock your laptop without 
restarting. 
 
? ? (dock),  ? ? (undock) and ? ?
(easy-to-carry) are not entries in our dictionary.  
Instead of adding them to the dictionary, we use 
templates to recognize them online.  The 
template that combines two individual characters 
to form a verb may include conditions such as: 
? none of the characters is subsumed by a 
longer word; 
? the joint probability of the characters being 
independent words in text is low; 
? the internal structure of the new word 
conforms to the word formation rules of 
Chinese 
? the component characters have similar 
behavior in existing words 
? etc. 
The details can be found in Wu & Jiang (2000).  
Currently we have 10 such templates, which are 
capable of identifying nouns, verbs, adjectives 
and adverbs of various lengths. 
 
1.2. Proposing grammatical attributes 
POS and sub-categorization information is 
crucial for the success of sentence analysis.  
However, there is no guarantee that every word in 
the existing dictionary will have the correct POS 
and sub-categorization information.  Besides, 
words can behave differently in different 
domains or develop new properties over time.  
Take the Chinese word (synchronize) for 
example.  It is an intransitive verb in our 
dictionary, but it is now often used as a transitive 
verb, especially in the computer domain.  For 
instance: 
 
MADC ?  ??  ?        ?? Exchange ?? 
            can easily DE synchronize           account 
MADC (Microsoft Active Directory Connector) 
can easily synchronize Exchange accounts. 
 
We may want to change the existing dictionary to 
make words like??transitive verbs, but that 
may not be appropriate lexicographically, at least 
in the general domain, not to mention the human 
labor involved in such an undertaking.  However, 
the sentence above cannot get a spanning parse 
unless??is a transitive verb.  To overcome this 
difficulty, our system can dynamically create a 
transitive verb in certain contexts.  An obvious 
context would be ?followed by an NP?, for 
example.  This way we are able to parse the 
sentence without changing the dictionary. 
 
A similar approach is taken in cases where a word 
is used in a part of speech other than the one(s) 
specified in the dictionary.  In the following 
sentence, for example, the noun ?? (cluster) is 
used as a verb instead: 
 
? ?? ??  32 ? ??? 
you can cluster 32 CL server      
You can cluster 32 servers. 
 
Rather than edit the dictionary to permanently 
add the verb POS to nouns like ??, we turn 
them into verbs dynamically during sentence 
analysis if they fit into the verb template.  The 
conditions in the verb template may include: 
? preceded by an modal or auxiliary verb 
? followed by aspectual markers such as ?, ? 
and ? 
? preceded by adverbials 
? etc. 
Such templates are in effect very similar to POS 
taggers, though we use them exclusively to create 
new POS instead of choosing from existing POS. 
2 Harvesting new words and attributes 
Proposing of new words and attributes as 
described in the previous section is only intended 
to be intelligent guesses, which can be wrong 
sometimes.  For example, although transitive 
verbs tend to be followed by NPs, not all verbs 
that precede NPs are transitive verbs.  To make 
sure that (1) the wrong guesses do not introduce 
too much noise into the analysis and (2) only the 
correct guesses are accepted as true lexical 
information, we take the following steps to filter 
out the errors that result from over-guessing. 
 
2.1 Set up the competition 
The proposed words and attributes are assigned 
lower probability in our system.  This is 
straightforward for new words.  We simply 
assign them low scores when we add them (as 
new terminal nodes) to the parsing chart2.  For 
new attributes on existing words, we make a new 
node which is a copy of the original node and 
assign the new attributes and a lower probability 
to this node.  As a result, the chart will contain 
two nodes for the same word, one with the new 
attributes and one without.  The overall effect is 
that the newly proposed nodes will compete with 
other nodes to get into a parse, though with a 
disadvantage. The sub-trees built with the new 
nodes will have lower scores and will not be in 
the preferred analysis unless there is no other way 
to get a spanning parse.  Therefore, if the guesses 
are wrong and the sentence can be successfully 
parsed without the additional nodes, the best 
parse (the parse with the highest score) will not 
contain those nodes and the guesses are 
practically ignored.  On the other hand, if the 
guesses are right and we cannot get any 
successful parse unless we use them, then they 
will end up in the top parse3 in spite of their low 
                                                     
2
 See Jensen et al(1993) and Heidorn (2000) for a 
general description of how chart parsing works in our 
system.  A Chinese-specific description of the system 
can be found in Wu & Jiang (1998). 
3
 Our system can produce more than one parse for a 
given sentence and the top parse is the one with the 
probability.   
 
 2.2 Keep the winners 
For each sentence, we pick the top parse and 
check it to see if  there are any terminal nodes that 
are  new words or nodes containing new 
attributes.  If so, we know that these nodes are 
necessary at least to make the current sentence 
analyzable.  The fact that they are able to beat 
their competitors despite their disadvantage 
suggests that they probably represent lexical 
information that is missing in the existing 
dictionary.  We therefore collect such 
information and store it away in a separate 
lexicon.   This auxiliary lexicon contains entries 
for the new words and the new attributes of 
existing words.  Each entry in this lexicon carries 
a frequency count which records the number of 
times a given new word or new attribute has 
appeared in good parses during the processing of 
certain texts.  The content of this lexicon depends 
on the corpora, of course, and different lexicons 
can be built for different domains.  When 
processing future sentences, the entries in those 
lexicons can be dynamically merged with the 
entries in the main lexicon, so that we do not have 
to make the same guesses again. 
 
2.3 Use the fittest 
The information lexicalized in those auxiliary 
lexicons, though good in general, is not 
guaranteed to be correct.  While being necessary 
for a successful parse is strong evidence for its 
validity, that is not a sufficient condition for the 
correctness of such information.  Consequently, 
there can be some noise in those lexicons.  
However, a real linguistic property is likely to be 
found consistently whereas mistakes tend to be 
random.   To prevent the use of wrongly 
lexicalized entries, we may require a frequency 
threshold during the merging process: only those 
entries that have been encountered more than n 
times in the corpora are allowed to be merged 
with the main lexicon and used in future analysis.  
If a given new word or linguistic property is 
found to occur repeatedly across different 
domains, we may even consider physically 
                                                                               
highest score. 
merging it into the main dictionary, as it may be a 
piece of information that is worth adding 
permanently. 
3 Evaluation 
The system described above has been evaluated 
in terms of the contribution it makes in parsing.  
The corpus parsed in the evaluation consists of 
121,863 sentences from Microsoft technical 
manuals.  The choice is based on the 
consideration that this is a typical 
domain-specific text where there are many 
unlisted words and many novel usages of words.4  
To tease apart the effects of online guessing and 
lexicalization, we did two separate tests, one with 
online guessing only and one with lexicalization 
as well.  When lexicalization is switched on, the 
new words and attributes that are stored in the 
auxiliary lexicon are used in subsequent 
processing.  Once a new word or attribute has 
been recognized in n sentences, it will act as if it 
were an entry in the main dictionary and can be 
used in the analysis of any other sentence with 
normal probability. 
 
3.1 Online guessing only 
In this test, we parsed the corpus twice, once with 
guessing and once without.  Then we picked out 
all the sentences that had different analyses in the 
two passes and compared their parses to see if 
they became better when lexical guessing is on.  
Since comparing the parses requires human 
inspection and is therefore very time consuming, 
we randomly selected 10,000 sentences out of the 
121,863 and used only those sentences in the test.   
 
It turns out that 1,459 of those 10,000 sentences 
got different parses when lexical guessing is 
switched on.  Human comparison of those 
differences shows that, of the 1,459, the guessing 
made 1,153 better, 82 worse, and 224 stay the 
same (different parses but equally good or bad).  
The net gain is 1,071.  In other words, 10.71% of 
the sentences became better when lexical 
guessing is used.   
 
                                                     
4
 The novel usages are mainly due to the fact that the 
text is translated from English. 
More detailed analysis shows that 48% of the 
improvements are due to the recognition of new 
words and 52% to the addition of new 
grammatical attributes.  Of the 82 sentences that 
became worse, 6 failed because of the lack of 
storage during processing caused by the 
additional resources required by the guessing 
algorithm.  The rest are due to over-guessing, or 
more precisely, the failure to rule out the 
over-guesses in sentence analysis.  The guessing 
component is designed to over-guess, since the 
goal there is recall rather than precision. The 
latter is achieved by the filtering effect of the 
parser.     
 
3.2 Additional gain with lexicalization 
In this second test, we evaluated the effect of 
lexicalization on new word recognition5.  We 
parsed all the 121,863 sentences twice, once with 
lexicalization and once without.  The number of 
unique new words recognized in this corpus is 
9226.  Notice that this number does not change 
between the two processes.  Using the lexicon 
created by dynamic lexicalization will increase 
the instances of those words being recognized, 
but will not change the number of unique words, 
since the entries in the auxiliary lexicon can also 
be recognized online.  However, the numbers of 
instances are different in the two cases.  When 
lexicalization is turned off, we are able to get 
5963 instances of those 922 new words in 5239 
sentences.   When lexicalization is on, however, 
we are able to get 6464 instances in 5608 
sentences.  In other words, we can increase the 
recognition rate by 8.4% and potentially save 369 
additional sentences in parsing.  The reason for 
this improvement is that, without lexicalization, 
we may fail to identify the new words in certain 
sentences because there were not enough good 
contexts in those sentences for the identification.  
Once those words are lexicalized, we no longer 
have to depend on context-based guessing and 
                                                     
5
 We would like to look at the effect on grammatical 
attributes as well, but the evaluation is not as 
straightforward there and much more 
time-consuming.   
6
 The total number of unique words used in this corpus 
is 17,110.  So at least 5% of the words are missing in 
the original dictionary. 
those sentences can benefit from what we have 
learned from other sentences.  Here is a concrete 
example for illustration: 
 
?  ??    ?   ?? ???? ?    ??? 
He master LE undock  laptop   DE technology 
He mastered the technology of undocking a 
laptop. 
 
In this sentence, we do not have enough context 
to identify the new word ?? because ?? is a 
word in Chinese (Remember there are no spaces 
between words in Chinese!).  This destroys the 
condition that none of the characters in the new 
word should be subsumed by a longer word.  
However, if ?? has been recognized in some 
other sentences, such as the one we saw in 
Section 1.1, and has been lexicalized, we can 
simply look up this word in the dictionary and 
use it right away.  In short, lexicalization enables 
what is learned locally to be available globally. 
Conclusion 
In this paper, we have demonstrated a mechanism 
for dynamic dictionary update.  This method 
reduces human effort in dictionary maintenance 
and facilitates domain-switching in sentence 
analysis.  Evaluation shows that this mechanism 
makes a significant contribution to parsing, 
especially the parsing of large, domain-specific 
corpora.  
References  
Heidorn G. E. (2000) Intelligent writing assistance,. 
In "A Handbook of Natural Language Processing: 
Techniques and Applications for the Processing of 
Language as Text ", Dale R., Moisl H., and Somers 
H. eds., Marcel Dekker, New York, pp. 181-207.  
Jenson K., Heidorn G. and Richardson S.  (1993)  
Natural Language Processing: the PLNLP 
Approach? Boston, Kluwer  
Wu A. and Jiang Z. (1998)  Word Segmentation in 
Sentence Analysis. In "Proceedings of the 1998 
International Conference on Chinese Information 
Processing", Beijing, China. 
Wu A. and Jiang Z. (2000)  Statistically-Enhanced 
New Word Identification in a Rule-based Chinese 
System. In "Proceedings of the Second ACL 
Chinese Processing Workshop", HKUST, Hong 
Kong. 
Statistically-Enhanced New Word Identification 
in a Rule-Based Chinese System 
Andi Wu 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
Andiwu @microsoft.com 
Zixin Jiang 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
jiangz@ microsoft.tom 
Abstract 
This paper presents a mechanism of new 
word identification i  Chinese text where 
probabilities are used to filter candidate 
character strings and to assign POS to the 
selected strings in a ruled-based system. This 
mechanism avoids the sparse data problem of 
pure statistical approaches and the 
over-generation problem of rule-based 
approaches. It improves parser coverage and 
provides a tool for the lexical acquisition of 
new words. 
1 Introduction 
In this paper, new words refer to newly coined 
words, occasional words and other rarely used 
words that are neither found in the dictionary of a 
natural language processing system nor 
recognized by the derivational rules or proper 
name identification rules of the system. Typical 
examples of such words are shown in the 
following sentences, with the new words 
underlined inbold. 
~ ~ , ~ ~ " ~ " ,  
~ ~ ~ .  
~- -~E~f f~, ,~R~"  
*\[\]~.2/..~W~m~@~o 
~ ~ . ~ ~ o  
The automatic dentification fsuch words by a 
machine is a trivial task in languages where 
words are separated by spaces in written texts. In 
languages like Chinese, where no word boundary 
exists in written texts, this is by no means an easy 
job. In many cases the machine will not even 
realize that there is an unfound word in the 
sentence since most single Chinese characters 
can be words by themselves. 
Purely statistical methods of word 
segmentation (e.g. de Marcken 1996, Sproat et al
1996, Tung and Lee 1994, Lin et al(1993), 
Chiang et al(1992), Lua, Huang et al etc.) often 
fail to identify those words because of the sparse 
data problem, as the likelihood for those words to 
appear in the training texts is extremely ow. 
There are also hybrid approaches such as (Nie 
dt al 1995) where statistical approaches and 
heuristic rules are combined to identify new 
words. They generally perform better than 
purely statistical segmenters, but the new words 
they are able to recognize are usually proper 
names and other elatively frequent words. They 
require a reasonably big training corpus and the 
performance is often domain-specific depending 
on the training corpus used. 
Many word segmenters ignore low-frequency 
new words and treat heir component characters 
as independent words, since they are often of 
46 
little significance in applications where the 
structure of sentences is not taken into 
consideration. For in-depth natural language 
understanding where full parsing is required, 
however, the identification of those words is 
critical, because a single unidentified word can 
cause a whole sentence to fail. 
The new word identification mechanism to be 
presented here is used in a wide coverage 
Chinese parser that does full sentence analysis. It 
assumes the word segmentation process 
described in Wu and Jiang (1998). In this model, 
word segmentation, including unfound word 
identification, is not a stand-alone process, but an 
integral part of sentence analysis. The 
segmentation component provides a word lattice 
of the sentence that contains all the possible 
words, and the final disambiguation is achieved 
in the parsing process. 
In what follows, we will discuss two 
hypotheses and their implementation. The first 
one concerns the selection of candidate strings 
and the second one concerns the assignment of 
parts of speech (POS) to those strings. 
2 Selection of candidate strings 
2.1 Hypothesis 
Chinese used to be a monosyllabic language, 
with one-to-one correspondences between 
syllables, characters and words, but most words 
in modem Chinese, especially new words, 
consist of two or more characters. Of the 85,135 
words in our system's dictionary, 9217 of them 
are monosyllabic, 47778 are disyllabic, 17094 
are m-syllabic, and the rest has four or more 
characters. Since hardly any new character is 
being added to the language, the unfound words 
we are trying to identify are almost always 
multiple character words. Therefore, if we find a 
sequence of single characters (not subsumed by 
any words) after the completion of basic word 
segmentation, derivational morphology and 
proper name identification, this sequence is very 
likely to be a new word. This basic intuition has 
been discussed in many papers, such as Tung and 
Lee (1994). Consider the following sentence. 
(1) ~.~rj~ IIA~,~t~l~.J~)-~l~-~-.~t:a--. 
This sentence contains two new words (not 
including the name "~t~l~ which is recognized 
by the proper name identification mechanism) 
that are unknown to our system: 
~f~:~rj (probably the abbreviated name of a 
junior high school) 
~:~j (a word used in sports only but not in our 
dictionary) 
Initial lexical processing based on dictionary 
lookup and proper name identification produces 
the following segmentation: 
where ~-~rJ and ~a~.~\]- are segmented into single 
characters. In this case, both single 
character-strings are the new words we want to 
find. 
However, not every character sequence is a 
word in Chinese. Many such sequences are 
simply sequences of.single-character words. 
Here is an example: 
After dictionary look up, we get 
which is a sequence of 10 single characters. 
However, every character here is an independent 
word and there is no new word in the sentence. 
From this we see that, while most new words 
show up as a sequence of single characters, not 
every sequence of single characters forms a new 
word. The existence of a single-character st ing 
is the necessary but not sufficient condition for a 
new word. Only those sequences of single 
characters where the characters are unlikely to 
be a sequence of independent words are good 
candidates for new words. 
2.2 Implementation 
The hypothesis n the previous ection can be 
implemented with the use of the Independent 
Word Probability (IWP), which can be a property 
of a single character or a string of characters. 
47 
2.1.1 Def'ming IWP 
Most Chinese characters can be used either as 
independent words or component parts of 
multiple character words. The IWP of a single 
character is the likelihood for this character to 
appear as an independent word in texts: 
N(Word(c)) IWP(c) = 
N(c) 
where N(Word(c)) is the number of occurrences 
of a character as an independent word in the 
sentences of a given text corpus and N(c) is the 
total number of occurrence of this character in the 
same corpus. In our implementation, we 
computed the probability from a parsed corpus 
where we went through all the leaves of the trees, 
counting the occurrences of each character and 
the occurrences of each character as an 
independent word. 
The parsed corpus we used contains about 
5,000 sentences and was of course not big enough 
to contain every character in the Chinese 
language. This did not turn out to be a major 
problem, though. We find that, as long as all the 
frequently used single-character words are in the 
corpus, we can get good results, for what really 
matters is the IWP of this small set of frequent 
characters/words. These characters/words are 
bound to appear in any reasonably large 
collection of texts. 
Once we have the IWP of individual characters 
(IWP(c)), we can compute the IWP of a character 
string (IWP(s)). IWP(s) is the probability of a 
sequence of two or more characters being a 
sequence of independent words. This is simply 
the joint probability of the IWP(c) of the 
component characters. 
2.1.2 Using lWP 
With IWP(c) and IWP(s) defined , we then 
define a threshold T for IWP. A sequence S of 
two or more characters i considered a candidate 
for a new word only if its IWP(s) < T.  When 
IWP(s) reaches T, the likelihood for the 
characters to be a sequence of independent words 
is too high and the string will notbe considered to 
be a possible new word. In our implementation, 
the value of Tis empirically determined. A lower 
T results in higher precision and lower recall 
while a higher T improves recall at the expense of 
precision. We tried different values and weighed 
recall against precision until we got the best 
performance. ~-~)J and ~ '~ in Sentence (1) are 
identified as candidate dates because 
1WP(s ) (~)  = 8% and lWP(s)(~'~\]~) = 10% 
while the threshold is 15%. In our system, 
precision is not a big concern at this stage 
because the final filtering is done in the parsing 
process. We put recall first to ensure that the 
parser will have every word it needs. We also 
tried to increase precision, but not at the expense 
of recall. 
3 POS Assignment 
Once a character string is identified to be a 
candidate for new word, we must decide what 
syntactic category or POS to assign to this 
? possible new word. This is required for sentence 
analysis where every word in the sentence must 
have at least one POS. 
3.1. Hypothes is  
Most multiple character words in Chinese have 
word-internal syntactic structures, which is 
roughly the POS sequence of the component 
characters (assuming each character has a POS or 
potential POS). A two-character verb, for 
example, can have a V-V, V-N, V-N or A(dv)-V 
internal structure. For a two-character string to 
be assigned the POS of verb, the POS/potential 
POS of its component characters must match one 
of those patterns. However, this matching alone 
is not the sufficient condition for POS assignment. 
Considering the fact that a single character can 
have more than one POS and a single POS 
sequence can correspond to the internal word 
structures of different parts of speech (V-N can 
be verb or a noun, for instance), simply assigning 
POS on the basis of word internal structurewill 
result in massive over-generation a d introduce 
too much noise into the parsing process. To 
prune away the unwanted guesses, we need more 
help from statistics. 
When we examine the word formation process 
in Chinese, we find that new words are often 
modeled on existing words. Take the newly 
coined verb ~?~J" as an example. Scanning our 
dictionary, we find that ~" appears many times as 
the first character of a two-character verb, such as 
F~'5~, ~,  ~ '~,  ~ '~,  ~\ [ , ,  ~'~'~J~, e tc .  
Meanwhile, ~J" appears many times as the second 
48 
character of a two-character verb, such as ~\ ]~,  
~,.~\]~j-, z\]z~, ~\]~\]., ~l-~J, ~\]r~, etc. This leads us 
to the following hypothesis: 
A candidate character string for a new word is 
likely to have a given POS if the component 
characters of this string have appeared in the 
corresponding positions of many existing words 
with this POS. 
3.2. Implementation 
To represent the likelihood for a character to 
appear in a given position of a word with a given 
POS and a given length, we assign probabilities 
of the following form to each character: 
P( Cat, Pos, Len )
where Cat is the category/POS of a word, Pos is 
the position of the character in the word, and Len 
is the length (number of characters) of the word. 
The probability of a character appearing as the 
second character in a four-character verb, for 
instance, is represented asP(Verb,2,4). 
3.1.1. Computing P(Cat, Pos, Len) 
There are many instantiations of 
P(Cat, Pos, Len), depending on the values of the 
three variables. In our implementation, we 
limited the values of Cat to Noun, Verb and 
Adjective, since they are the main open class 
categories and therefore the POSes of most new 
words. We also assume that most new words will 
have between 2 to 4 characters, thereby limiting 
the values of Pos to 1--4 and the values of Len to 
2--4. Consequently each character will have 27 
different kinds of probability values associated 
with it. We assign to each of them a 4-character 
name where the first character is always "P", the 
second the value of Cat, the third the value of Pos, 
and the fourth the value of Len. Here are some 
examples: 
Pnl2 (the probability of appearing as the first 
character of a two-character noun) 
Pv22 (the probability of appearing as the 
second character of a two-character verb) 
Pa34 (the probability of appearing as the third 
character of a four-character adjective) 
The values of those 27 kinds of probabilities are 
obtained by processing the 85,135 headwords in 
our dictionary. For each character inChinese, we 
count he number of occurrences of this character 
in a given position of words with a given length 
and given category and then divide it by the total 
number of occurrences of this character in the 
headwords of the dictionary. For example, 
N(vl2(c)) 
Pv12( c ) = 
N(c) 
where N(v12(c)) is the number of occurrences of
a character in the first position of a two-character 
verb while N(c) is the total number of 
occurrences of this character in the dictionary 
headwords. Here are some of the values we get 
for the character~: 
Pnl2(~b~) = 7% 
Pv12(~) = 3% 
Pv23(~\]) = 39% 
en22(~)  = 0% 
Pv22(~) =24% 
ea22(~)  =1% 
It is clear from those numbers that the character 
tend to occur in the second position of 
two-character and three-character v rbs. 
3.1.2. Using P(Cat, Pos, Len) 
Once a character string is identified as a new 
word candidate, we will calculate the POS 
probabilities for the string. For each string, we 
will get P(noun), P(verb) and P(adj) which are 
respectively the probabilities of this string being 
a noun, a verb or an adjective. They are the joint 
probabilities of the P(Cat, Pos, Len)o f  the 
component characters of this string. We then 
measure the outcome against a threshold. For a 
new word string to be assigned the syntactic 
category Cat, its P(Cat) must reach the threshold. 
The threshold for each P(Cat ) is independently 
determined so that we do not favor a certain POS 
(e.g. Noun) simply because there are more nouns 
in the dictionary. 
If a character string reaches the threshold of 
more than one P(Cat), it will be assigned more 
than one syntactic ategory. A string that has 
both P(noun) and P(verb) reaching the threshold, 
for example, will have both a noun and a verb 
added to the word lattice. The ambiguity is then 
resolved in the parsing process. If a string passes 
the IWP test but falls the P(Cat) test, it will 
49 
receive noun as its syntactic ategory. In other 
words, the default POS for a new word candidate 
is noun. This is what happened to ~f~ in the 
Sentence (l). ~-~D passed tlhe IWP test, but 
failed each of the P(Cat) tests. As a result, it is 
made a noun by default. As we can see, this 
assignment is the correct one (at least in this 
particular sentence). 
4. Results and Discussion 
4.1. Increase in Parser Coverage 
The new word identification mechanism 
discussed above has been part of our system for 
about 10 months. To find out how much 
contribution it makes to our parser coverage, we 
took 176,863 sentences that had been parsed 
successfully with the new word mechanism 
turned on and parsed them again with the new 
word mechanism turned off. When we did this 
test at the beginning of these 10 months, 37640 of 
those sentences failed to get a parse when the 
mechanism was turned off. In other words, 
21.3% of the sentences were "saved" by this 
mechanism. At the end of the 10 months, 
however, only 7749 of those sentences failed 
because of the removal of the mechanism. At 
first sight, this seems to indicate that the new 
word mechanism is doing a much less 
satisfactory job than before. What actually 
happened is that many of the words that were 
identified by the mechanism 10 months ago, 
especially those that occur frequently, have been 
added to our dictionary. In the past 10 months, 
we have been using this mechanism both as a 
component of robust parsing and as a method of 
lexical acquisition whereby new enwies are 
discovered from text corpora. This discovery 
procedure has helped us find many words that are 
found in none of the existing word lists we have 
access to. 
4.2. Precision of Identification 
Apart from its contribution to parser coverage, 
we can also evaluate the new word identification 
mechanism by looking at its precision. In our 
evaluation, we measured precision in two 
different ways. 
In the first measurement, we compared the 
number of new words that are proposed by the 
guessing mechanism and the number of words 
that end up in successful parses. If  we use NWA 
to stand for the number of new words that are 
added to the word lattice and NWU for the 
number of new words that appear in a parse tree, 
the precision rate will be NWU / NWA. Actual 
testing shows that this rate is about 56%. This 
means that the word guessing mechanism has 
over-guessed and added about twice as many 
words as we need. This is not a real problem in 
our system, however, because the final decision 
is made in the parsing process. The lexical 
component is only responsible for providing a 
word lattice of which one of the paths is correct. 
In the second measurement, we had a native 
speaker of Chinese go over all the new words that 
end up in successful parses and see how many of 
them sound like real words to her. This is a fairly 
subjective test but nonetheless meaningful one. 
It turns out that about 85% of the new words that 
"survived" the parsing process are real words. 
We would also like to run a large-scale recall 
test on the mechanism, but found it to be 
impossible. To run such a test, we have to know 
how many unlisted new words actually exist in a 
corpus of texts. Since there is no automatic way 
of knowing it, we would have to let a human 
manually check the texts. This is too expensive 
to be feasible. 
4.3. Contributions of Other Components 
While the results shown above do give us some 
idea about how much contribution the new word 
identification mechanism akes to our system, it 
is actually very difficult to say precisely how 
much credit goes to this mechanism and how 
much to other components of the system. As we 
can see, the performance of this mechanism also 
depends on the following two factors: 
(1) The word segmentation processes prior to 
the application of this mechanism. They 
include dictionary lookup, derivational 
morphology, proper name identification 
and the assembly of other items such as 
time, dates, monetary units, address, phone 
numbers, etc. These processes also group 
characters into words. Any improvement in 
those components will also improve the 
performance of the new word mechanism. 
If every word that "should" be found by 
50 
those processes has already been identified, 
the single-character sequences that remain 
after those processes will have a better 
chance of being real words. 
(2) The parsing process that follows. As 
mentioned earlier, the lexical component of 
our system does not make a final decision 
on "wordhood". It provides a word lattice 
from which the syntactic parser is supposed 
to pick the correct path. In the case of new 
word identification, the word lattice will 
contain both the new words that are 
identified and the all the words/characters 
that are subsumed by the new words. A 
new word proposed in the word lattice will 
receive its official wordhood only when it 
becomes part of a successful parse. To 
recognize a new word correctly, the parser 
has to be smart enough to accept he good 
guesses and reject the bad guesses. This 
ability of the parser will imporve as the 
parser improves in general and a better 
parser will yield better final results in new 
word identification. 
Generally speaking, the mechanisms using IWP 
and P(Cat, Pos, Len) provide the internal criteria 
for wordhood while word segmentation and 
parsing provide the external criteria. The internal 
criteria are statistically based whereas the 
external criteria are rule-based. Neither can do a 
good job on its own without the other. The 
approach we take here is not to be considered 
staff stical natural language processing, but it does 
show that a rule-based system can be enhanced 
by some statistics. The statistics we need can be 
extracted from a very small corpus and a 
dictionary and they are not domain dependent. 
We have benefited from the mechanism in the 
analysis of many different kinds of texts. 
References 
Chang, Jyun-Sheng, Shun-Der Chen, Sue-Jin Ker, 
Ying Chen and John S. Liu (1994) A 
multiple-corpus approach to recognition of proper 
names in Chinese texts, Computer Processing of 
Chinese and Oriental Languages, Vol. 8, No. 1 pp. 
75-85. 
Chen, Keh-Jiann and Shing-Huan Liu (1992). Word 
identification for Mandarin Chinese sentences, 
Proceedings of COLING-92, pp. 23-28. 
Chiang, T. H., Y. C. Lin and K.Y. Su (1992). 
Statisitical models for word segmentation and 
unknown word resolution, Proceedings of the 1992 
R. O. C. Computational Linguistics Conference, 
121-146, Taiwan. 
De Marcken, Carl (1996). Unsupervised Language 
Acquisition, Ph.D dissertation, MIT. 
Lin, M. Y. , T. H. Chiang and K. Y. Su (1993) A 
prelimnary study on unknown word problem in 
Chinese word segmentation, Proceedings of the 
1993 R. O. C. Computational Linguistics 
Conference, 119-137, Taiwan. 
Lua, K T. Experiments on the use of bigram mutual 
information i Chinese natural language processing. 
Nie, Jian Yun, et al (1995) Unknown Word Detection 
and Segmentation f Chinese using Statistical and 
Heuristic Knowledge, Communications of COUPS, 
vol 5, No. 1 &2, pp.47, Singapore. 
Sproat, Richard, Chilin Shih, William Gale and Nancy 
Chang (1996). A stochastic finite-state 
word-segmentation algorithm for Chinese. 
Computational Linguistics, Volume 22, Number 3. 
Tung, Cheng-Huang and Lee His-Jian (1994). 
Identification of unknown words from a corpus. 
Computer Processing of Chinese and Oriental 
Languages, Vol. 8 Supplement, pp. 131-145. 
Wu, Andi and Zixin Jiang (1998) Word segmentation 
in sentence analysis, Proceedings of the 1998 
International Conference on Chinese Information 
Processing, pp. 169-180. 
Yeh, Ching-Long and His-Jian Lee (1991). 
Rule-based word identification for Mandarin 
Chinese sentences - a unification approach, 
Computer Processing of Chinese and Oriental 
Languages, Vol 5, No 2, Page 97-118. 
51 
Multilingual Sentence Generation
Takako Aikawa
Maite Melero
Lee Schwartz
Andi Wu
Microsoft Research
One Microsoft Way
Redmond, WA 98008, USA
takakoa@microsoft.com
maitem@microsoft.com
leesc@micorosft.com
andiwu@microsoft.com
Abstract
This paper presents an overview of a
robust, broad-coverage, and
application-independent natural
language generation system. It
demonstrates how the different
language generation components
function within a multilingual
Machine Translation (MT) system,
using the languages that we are
currently working on (English,
Spanish, Japanese, and Chinese).
Section 1 provides a system
description. Section 2 focuses on the
generation components and their core
set of rules. Section 3 describes an
additional layer of generation rules
included to address application-
specific issues. Section 4 provides a
brief description of the evaluation
method and results for the MT system
of which our generation components
are a part.
1 System Description
We present a natural language generation
method in the context of a multi-lingual MT
system. The system that we have been
developing is a hybrid system with rule-based,
example-based, and statistical components.
Analysis and generation are performed with
linguistic parsers and syntactic realization
modules, the rules of which are coded by hand.
Transfer is accomplished using transfer
rules/mappings automatically extracted from
aligned corpora.
The MT process starts with a source sentence
being analyzed by the source-language parser,
which produces as output a syntactic tree. This
tree is input to the Logical Form module, which
produces a deep syntactic representation of the
input sentence, called the LF (Heidorn, G. E.,
2000). The LF uses the same basic set of
relation types for all languages. Figure 1 gives
the syntactic tree and LF for the simple English
sentence, ?I gave the pencils to John?.
Tree
LF
Figure 1
The LF is the final output of the analysis phase
and the input to the transfer phase.
Transfer extracts a set of mappings from the
source-target language MindNet (Richardson,
2000), a translation knowledge database, and
applies these mappings to the LF of the source
sentence to produce a target LF. The translation
MindNet for a language pair is a repository of
aligned LFs and portions of LFs (produced by
analyzing sentence-aligned corpora). An
alignment of two LFs is a set of mappings
between a node or set of nodes (and the relations
between them) in the source LF and a node or
set of nodes (and the relations between them) in
the target LF (Menezes & Richardson, 2001).
In the translation process, the transfer
component searches the alignments in the
MindNet for those that match portions of the LF
of the sentence being translated. Mappings with
larger context are preferred to mappings with
smaller context and higher frequency mappings
are preferred to lower frequency mappings. The
lemmas in any portion of the LF of the input
sentence that do not participate in a mapping are
mapped to a target lemma using a bilingual
dictionary. The target LF fragments from the
transfer mappings and dictionary mappings are
stitched together to produce the target LF
(Menezes & Richardson, 2001). For our
example in Figure 1, the transfer component
produces the following target LFs for Spanish,
Japanese, and Chinese (Figure 2).1
Source sentence: I gave the pencils to John.
Transferred Spanish LF:
Transferred Japanese LF:
Transferred Chinese LF:
Figure 2
The transferred LF is the input to the generation
component, which we will discuss in detail
below.
2 Syntactic Generation Component
The different language generation modules in
our system are syntactic realization components
that take as input an LF characteristic of the
language to be generated and produce a
syntactic tree and surface string for that
language. In this sense, they are functionally
similar to the REALPRO system (Lavoie and
Rambow, 1997).
1 English gloss is provided in Figure 2 for readability
purposes only.
The generation modules are not designed
specifically for MT, but rather are application-
independent. They can take as input an LF
produced by a dialog application, a critiquing
application, a database query application, an MT
application, etc. They only require a
monolingual dictionary for the language being
generated and an input LF that is characteristic
of that language. For each language there is
only one generation component that is used for
all applications, and for MT, it is used for
translation from all languages to that language.
At the beginning of generation, the input LF
is converted into a basic syntactic tree that
conforms to the tree geometry of the NLP
system. The nodes in LF become subtrees of this
tree and the LF relations become
complement/adjunct relationships between the
subtrees. This basic tree can be set up in
different ways. For English, Spanish, and
Chinese, we set it up as strictly head-initial with
all the complements/adjuncts following the
head, resembling the tree of a VSO language.
For Japanese, we set it up as strictly head-final,
with all the complements/adjuncts preceding the
head. Figure 3 gives the basic Spanish
generation tree produced from the Spanish
transferred LF in Figure 2.
Figure 3
The generation rules apply to the basic tree,
transforming it into a target language tree. In the
application of the rules, we traverse the tree in a
top-down, left-to-right, depth-first fashion,
visiting each node and applying the relevant
rules. Each rule can perform one or more of the
following operations:
(1) Assign a syntactic label to the node. For
example, the ?DECL? label will be assigned
to the root node of a declarative sentence.
(2) Modify a node by changing some
information within the node. For example, a
pronoun might be marked as reflexive if it is
found to be co-referential with the subject of
the clause it is in.
(3) Expand a node by introducing new node(s)
into the tree. For example, the ?Definite?
(+Def) feature on a node may become a
determiner phrase attached to the syntactic
subtree for that node.
(4) Delete a node. For example, for a pro-drop
language, a pronominal subject may be
removed from the tree.
(5) Move a node by deleting it from Position A
and inserting it in Position B. For example,
for an SVO language, the subject NP of a
sentence may be moved from a post-verbal
position to a pre-verbal position.
(6) Ensure grammatical agreement between
nodes. For example, if the subject of a
sentence is first person singular, those
number and person features will be assigned
to the main verb.
(7) Insert punctuation and capitalization.
The nodes in the generated tree are linked to
each other by relations such as ?head?, ?parent?
and ?sibling?. The entire tree is thus visible
from any given node via these relations. When
a rule is applied to a node, the decisions made in
that rule can be based not just on features of that
node, but also on features of any other node in
the tree. This basically eliminates the need for
backtracking, which would be necessary only if
there were local ambiguities resulting from the
absence of global information. In this sense, our
approach is similar to that of other large-scale
generators (Tomita and Nyberg, 1988).
The generation rules operate on a single tree.
Rule application is deterministic and thus very
efficient. If necessary, the tree can be traversed
more than once, as is the case in the generation
modules for the languages we are currently
working on. There is a ?feeding? relationship
among the rules. The rules that assign
punctuation and capitalization, for example, do
not apply until all the movement rules have
applied, and movement rules do not apply until
nodetypes and functional roles are assigned.
To improve efficiency and to prevent a rule
from applying at the wrong time or to the wrong
structure, the rules are classified into different
groups according to the passes in which they are
applied. Each traversal of the tree activates a
given group of rules. The order in which the
different groups of rules are applied depends on
the feeding relations.
For the simple example in Figure 2 above,
the Spanish, Chinese, and Japanese generation
components all have an initial pass that assigns
nodetypes and functional roles and a final pass
that inserts punctuation marks.
In addition, the Spanish component, in a first
pass that identifies syntactic functions, deletes
the pronominal subject and inserts a dative clitic
pronoun. It also inserts the definite article and
the personal marker ?a?. In a second pass, it
checks agreement between indirect object and
doubled clitic as well as between subject and
verb, assigning the appropriate person, number,
and gender agreement information to the
terminal nodes.
Reordering operations, such as moving the
clitic in front of the verb, if the verb is finite, or
after, if it is non-finite, come later. The last pass
takes care of euphonic issues, such as
contractions or apocopated adjectives. Figure 4a
shows the resulting tree.
Figure 4a
The Chinese component has a node-
modification pass, which adds the FUNCW
node headed by (le) to indicate past tense. In
this pass the direct object is also turned into a
prepositional phrase introduced by (ba) to
show the definiteness of the NP. Following this
pass, a movement pass moves the subject in
front of the verb.
Figure 4b
The Japanese component has a pass in which
case-markers or modifiers are inserted. In
Figure 4c, the nominative, the accusative, and
the dative case markers are inserted in the
subject, direct object, and indirect object NPs,
respectively. Also, the demonstrative
corresponding to English "that" is inserted at the
beginning of the definite NP (pencil).
Figure 4c
After the grammatical rules apply, the
morphological rules apply to the leaf nodes of
the tree. Since each node in the tree is a feature
matrix and agreement information has already
been assigned by the generation rules,
morphological processing simply turns the
feature matrices into inflected forms. For
instance, in our Spanish example, the verb ?dar?
with the ?past?, ?singular? and ?1st person?
features is spelled out as ?di?. Once all the
words are inflected, the inflected form of each
leaf node is displayed to produce the surface
string. This completes the generation process,
as exemplified for Spanish in Figure 5.
Figure 5
3 Application-Driven Generation
The example used in the previous sections is
quite simple, and not representative of the actual
problems that arise in MT. Applications, such
as MT, that automatically create input for the
generation component for a language will not
always produce ideal LFs for that language, i.e.,
LFs that could have been produced by the
analysis modules for that language.
We have designed the generation
components, therefore, to add a degree of
robustness to our applications. To some extent,
and based only on information about the
language being generated, the generation
components will fix incomplete or inconsistent
LFs and will verify that the structures they
generate comply with the constraints imposed
by the target language.
The core generation rules are designed to be
application-independent and source-language-
independent. Expanding the rule base to cover
all the idiosyncrasies of the input would
contaminate the core rules and result in loss of
generality. In order to maintain the integrity of
the core rules while accommodating imperfect
input, we have opted to add a pre-generation
layer to our generation components.
Pre-generation rules apply before the basic
syntactic tree is built. They can modify the
input LF by adding or removing features,
changing lemmas, or even changing structural
relations. Below we give examples of problems
solved in the pre-generation layers of our
different language generation modules. These
illustrate not just the source-language
independence, but also the application-
independence of the generation modules.
We start with the English generation
component, which was used in experimental
question-answering applications before being
used in MT. Among the pre-generation rules in
this component is one that removes the marker
indicating non-restrictive modification (Nonrest)
from LF nodes that are not in a modification
relationship to another LF node. So, for
example, when the question-answering
application is presented with the query ?When
did Hitler come to power,? the NLP system
analyzes the question, produces an LF for it,
searches its Encarta Mindnet (which contains
the LFs for the sentences in the Encarta
encyclopedia), retrieves the LF fragment in
Figure 6, and sends it to the English generation
component.
Figure 6
The LF that is the input to generation in this
example is a portion of the LF representation of
a complete sentence that includes the phrase
?Hitler, who came to power in 1933.? The part
of that sentence that answers the question is the
nonrestrictive relative clause ?who came to
power in 1933.? Yet, we do not want to
generate the answer as a non-restrictive relative
clause (as indicated by Nonrest in the LF), but
as a declarative sentence. So, rather than pollute
the core generation rules by including checks for
implausible contexts in the rule for generating
nonrestrictive modifiers, a pre-generation rule
simply cleans up the input. The rule is
application-independent (though motivated by a
particular application) and can only serve to
clean up bad input, whatever its source.
An example of a rule motivated by MT, but
useful for other applications, is the pre-
generation rule that changes the quantifier ?less?
to ?fewer?, and vice versa, in the appropriate
situations. When the LF input to the English
generation component specifies ?less? as a
quantifier of a plural count noun such as ?car,?
this rule changes the quantifier to ?fewer?.
Conversely, when an input LF has ?fewer?
specified as a quantifier of a mass noun such as
?luck?, the rule changes it to ?less.? This rule
makes no reference to the source of the input to
generation. This has the advantage that it will
apply in a grammar-checking application as well
as in an MT application (or any other
application). If the input to English generation
were the LF produced for the ungrammatical
sentence ?He has less cars,? the generation
component would produce the correct ?He has
fewer cars,? thereby effectively grammar
checking the sentence. And, if the ultimate
source of the same input LF were the Spanish
sentence ?Juan tiene menos coches, ? the result
would be the same, even if ?menos? which
corresponds to both ?less? and ?fewer? in
English, were not transferred correctly. Another
type of problem that a generation component
might encounter is the absence of necessary
information. The Spanish generation
component, for instance, may receive as input
underspecified nominal relations, such as the
one exemplified in Figure 7, in which a noun
(registro) is modified by another noun
(programa). The relationship between the two
nouns needs to be made explicit, in Spanish, by
means of a preposition when the modifying
noun is not a proper noun. Absent the necessary
information in the incoming LF, a pre-
generation rule introduces the default
preposition ?de? to specify this relationship.
Figure 7
Another example of a pre-generation rule, this
time from Japanese, deals with the unspecified
1st/2nd person pronominal subject for particular
types of predicates. The 1st/2nd person pronoun
( ) is not used as the subject in
sentences that express the speaker?s/the
listener?s desire (unless there is some
focus/contrast on the subject). So, one of the
Japanese pre-generation rules deletes the subject
in the input LF that involves such a predicate.
For instance, below is the input LF, the modified
LF, and the string produced from the English
sentence ?I want to read the book.?
Figure 8
From Chinese, we give an example of a rule that
actually changes the structure of an LF. In our
system, it is possible for the source and target
languages to have different LF representations
for similar structures. In English and other
European languages, for example, the verb ?BE?
is required in sentences like ?He is smart?. In
Chinese, however, no copula is used. Instead,
an adjectival predicate is used. While we might
attempt at the LF level to unify these
representations, we have not yet done so.
Moreover, the LF in our system is not intended
to be an interlingua representation. Differences
between languages and their LFs are tolerated.
Therefore, Chinese uses a pre-generation rule to
transform the be-predicate adjective LF into its
Chinese equivalent as shown in Figure 9, though
we soon expect transfer to automatically do this.
Figure 9
4 Evaluation
The generation components described in the
previous sections are part of an MT system that
has been run on actual Microsoft technical
documentation. The system is frequently
evaluated to provide a measure of progress and
to yield feedback on its design and development.
In evaluating our progress over time and
comparing our system with others, we have
performed several periodic, blind human
evaluations. We focus here on the evaluation of
our Spanish-English and English-Spanish
systems.
For each evaluation, several human raters
judge the same set of 200-250 sentences
randomly extracted from our technical corpora
(150K sentences).2 The raters are not shown the
source language sentence; instead, they are
presented with a human translation, along with
two machine-generated translations. Their task
is to choose between the alternatives, using the
human translation as a reference.
Table 1 summarizes a comparison of the
output of our Spanish-English system with that
of Babelfish (http://world.altavista.com/).
Table 2 does the same for our English-Spanish
system and Lernout & Hauspie?s English-
Spanish system (http://officeupdate.lhsl.com/).
In these tables, a rating of 1 means that raters
uniformly preferred the translation produced by
our system; a rating of 0 means that they did not
uniformly prefer either translation; a rating of -1
means that they uniformly preferred the
translation produced by the alternative system.3
Beside each rating is a confidence measure for
the mean preference at the .99 level (Richardson,
S., et al(2001)).
Spanish-English
Systems
Mean preference
score (7 raters)
Sample
size
Our 4/01 (2001)
MT vs. Babelfish
0.32 ? 0.11
(at .99)
250
sentences
Table 1. Our Spanish-English MT vs. Babelfish
English-Spanish
Systems
Mean preference
score (5 raters)
Sample
size
Our 4/01 (2001)
MT vs. L&H
0.19 ? 0.14
(at 0.99)
250
sentences
Table 2. Our English-Spanish MT vs. Lernout &
Hauspie
2 The human raters used for these evaluations work for an
independent agency and played no development role
building the systems they test.
3 In interpreting our results, it is important to keep in mind
that our MT system has been customized to the test domain,
while the Babelfish and Lernout & Hauspie systems have
not.
5 Conclusion
In this paper we have presented an overview of
the natural language generation component
developed at Microsoft Research and have
demonstrated how this component functions
within a multilingual Machine Translation
system. We have provided motivation for the
generation architecture, which consists of a set
of core rules and a set of application-driven pre-
generation rules, within a wide-coverage, robust,
application-independent, multilingual natural
language processing system. In addition we
have presented evaluation figures for Spanish-
English and English-Spanish, two of the
language pairs of the MT system in which our
generation components are used.
6 References
Heidorn, G. E. (2000): Intelligence Writing
Assistance. In Dale R., Moisl H., and Somers
H. (eds.), A Handbook of Natural Language
Processing: Techniques and Applications for
the Processing of Language as Text. Marcel
Dekker, New York, 1998 (published in
August 2000), pages 181-207.
Jensen, K., Heidorn G., and Richardson S.
(eds.) (1993): Natural Language Processing:
The PLNLP Approach, Boston, Kluwer.
Lavoie, Benoit and Owen Rambow. (1997): A
fast and portable realizer for text generation.
In Proceedings of the Fifth Conference on
Applied Natural-Language Processing
(ANLP-1997), pages 265-268.
Melero, M. and Font-Llitjos, A. (2001):
Construction of a Spanish Generation module
in the framework of a General-Purpose,
Multilingual Natural Language Processing
System. In Proceedings of the VII
International Symposium on Social
Communication, Santiago de Cuba.
Reiter, E. and Dale, R. (2000): Building Natural
Language Generation Systems, Cambridge
University Press.
Richardson, S., et al(2001): Overcoming the
customization bottleneck using example-
based MT, Paper submitted for Data-driven
MT Workshop at ACL 2001, Toulouse,
France.
Richardson, S. (2000): The evolution of an NLP
System. NLP Group Microsoft Research,
Presentation at the LREC?2000 Athens,
Greece.
Tomita, M. and Nyberg E. (1988): The GenKit
and Transformation Kit User?s Guide.
Technical Report CMU-CMT-88-MEMO,
Centre for Machine Translation, Carnegie
Mellon University.
Learning Verb-Noun Relations to Improve Parsing 
Andi Wu 
Microsoft Research 
One Microsoft Way, Redmond, WA 98052 
andiwu@microsoft.com 
 
Abstract 
The verb-noun sequence in Chinese often 
creates ambiguities in parsing.  These ambi-
guities can usually be resolved if we know 
in advance whether the verb and the noun 
tend to be in the verb-object relation or the 
modifier-head relation.  In this paper, we de-
scribe a learning procedure whereby such 
knowledge can be automatically acquired.  
Using an existing (imperfect) parser with a 
chart filter and a tree filter, a large corpus, 
and the log-likelihood-ratio (LLR) algo-
rithm, we were able to acquire verb-noun 
pairs which typically occur either in verb-
object relations or modifier-head relations.  
The learned pairs are then used in the pars-
ing process for disambiguation.  Evaluation 
shows that the accuracy of the original 
parser improves significantly with the use of 
the automatically acquired knowledge. 
 
1 Introduction 
Computer analysis of natural language sentences is 
a challenging task largely because of   the ambigui-
ties in natural language syntax.  In Chinese, the 
lack of inflectional morphology often makes the 
resolution of those ambiguities even more difficult.  
One type of ambiguity is found in the verb-noun 
sequence which can appear in at least two different 
relations, the verb-object relation and the modifier-
head relation, as illustrated in the following 
phrases. 
 
(1)   ??        ??      ?    ?? 
      dengji     shouxu     de   feiyong 
     register  procedure  DE  expense 
     ?the expense of the registration procedure? 
 
(2) ??        ??     ?    ?? 
banli      shouxu    de   feiyong 
handle procedure DE  expense 
?the expense of going through the procedure? 
 
In (1), the verb-noun sequence ??? ??? is an 
example of the modifier-head relation while ???
??? in (2) is an example of the verb-object rela-
tion.  The correct analyses of these two phrases are 
given in Figure 1 and Figure 2, where ?RELCL? 
stands for ?relative clause?: 
 
      
Figure 1. Correct analysis of (1) 
 
 
Figure 2. Correct analysis of (2) 
 
    However, with the set of grammar rules that 
cover the above phrases and without any semantic 
or collocational knowledge of the words involved, 
there is nothing to prevent us from the wrong 
analyses in Figure 3 and Figure 4. 
 
 
Figure 3. Wrong analysis of (1) 
 
 
 
Figure 4. Wrong analysis of (2) 
 
To rule out these wrong parses, we need to 
know that ?? is a typical modifier of?? while
?? typically takes ?? as an object.  The ques-
tion is how to acquire such knowledge automati-
cally.  In the rest of this paper, we will present a 
learning procedure that learns those relations by 
processing a large corpus with a chart-filter, a tree-
filter and an LLR filter.  The approach is in the 
spirit of Smadja (1993) on retrieving collocations 
from text corpora, but is more integrated with pars-
ing.  We will show in the evaluation section how 
much the learned knowledge can help improve 
sentence analysis. 
2 The Learning Procedure 
The syntactic ambiguity associated with the verb-
noun sequence can be either local or global.  The 
kind of ambiguity we have observed in (1) and (2) 
is global in nature, which exists even if this noun 
phrase is plugged into a larger structure or com-
plete sentence.  There are also local ambiguities 
where the ambiguity disappears once the verb-
noun sequence is put into a broader context.  In the 
following examples, the sentences in (3) and (4) 
can only receive the analyses in Figure 5 and Fig-
ure 6 respectively. 
 
(3)  ?  ?  ?   ?     ??   ??? 
      zhe shi xin  de    dengji  shouxu 
      this be new DE register procedure 
     ?This is a new registration procedure.?  
 
(4)   ?   ?   ?    ??    ??? 
        ni    bu   bi     banli   shouxu 
      you  not must handle procedure 
?You don?t have to go through the procedure.? 
 
 
Figure 5. Parse tree of (3) 
 
 
Figure 6. Parse tree of (4) 
 
    In the processing of a large corpus, sentences 
with global ambiguities only have a random 
chance of being analyzed correctly, but sentences 
with local ambiguities can often receive correct 
analyses.  Although local ambiguities will create 
some confusion in the parsing process, increase the 
size of the parsing chart, and slow down process-
ing, they can be resolved in the end unless we run 
out of resources (in terms of time and space) be-
fore the analysis is complete.  Therefore, there 
should be sufficient number of cases in the corpus 
where the relationship between the verb and the 
noun is clear.  An obvious strategy we can adopt 
here is to learn from the clear cases and use the 
learned knowledge to help resolve the unclear 
cases.  If a verb-noun pair appears predominantly 
in the verb-object relationship or the modifier head 
relationship throughout the corpus, we should pre-
fer this relationship everywhere else.  
 
    A simple way to learn such knowledge is by us-
ing a tree-filter to collect all instances of each 
verb-noun pair in the parse trees of a corpus, 
counting the number of times they appear in each 
relationship, and then comparing their frequencies 
to decide which relationship is the predominant 
one for a given pair.  Once we have the informa-
tion that ???? is typically a modifier of ???? 
and ???? typically takes ???? as an object, for 
instance, the sentence in (1) will only receive the 
analysis in Figure 1 and (2) only the analysis in 
Figure 2.  However, this only works in idealized 
situations where the parser is doing an almost per-
fect job, in which case no learning would be neces-
sary.  In reality, the parse trees are not always 
reliable and the relations extracted from the parses 
can contain a fair amount of noise.  It is not hard to 
imagine that a certain verb-noun pair may occur 
only a couple of times in the corpus and they are 
misanalyzed in every instance.  If such noise is not 
filtered out, the knowledge we acquire will mislead 
us and minimize the benefit we get from this ap-
proach. 
 
    An obvious solution to this problem is to ignore 
all the low frequency pairs and keep the high fre-
quency ones only, as wrong analyses tend to be 
random.  But the cut-off point is difficult to set if 
we are only looking at the raw frequencies, whose 
range is hard to predict.   The cut-off point will be 
too low for some pairs and too high for others.  We 
need a normalizing factor to turn the raw frequen-
cies into relative frequencies.  Instead of asking 
?which relation is more frequent for a given pair??, 
the question should be ?of all the instances of a 
given verb-noun pair in the corpus, which relation 
has a higher percentage of occurrence??.  The 
normalizing factor should then be the total count of 
a verb-noun pair in the corpus regardless of the 
syntactic relations between them.  The normalized 
frequency of a relation for a given pair is thus the 
number of times this pair is assigned this relation 
in the parses divided by this normalizing factor.  
For example, if ?? ?? occurs 10 times in the 
corpus and is analyzed as verb-object 3 times and 
modifier-head 7 times, the normalized frequencies 
for these two relations will be 30% and 70% re-
spectively.  What we have now is actually the 
probability of a given pair occurring in a given re-
lationship.  This probability may not be very accu-
rate, given the fact that the parse trees are not 
always correct, but it should a good approximation, 
assuming that the corpus is large enough and most 
of the potential ambiguities in the corpus are local 
rather than global in nature. 
 
    But how do we count the number of verb-noun 
pairs in a corpus?  A simple bigram count will un-
justly favor the modifier-head relation.  While the 
verb and the noun are usually adjacent when the 
verb modifies the noun, they can be far apart when 
the noun is the object of the verb, as illustrated in 
(5). 
 
 
 
(5) ??   ??  ?? ?    ??       ??   
tamen zhengzai  banli  qu   taiwan     canjia  
they       PROG  handle go Taiwan participate 
      ????        ??     ??   ??? 
dishijiujie         guoji          jisuan    yuyanxue 
 19th            international compute linguistics 
   ??   ?   ??? 
      huiyi      de     shouxu 
conference DE  procedure 
?They are going through the procedures for 
going to Taipei for  the 19th International Con-
ference on Computational Linguistics.? 
 
    To get a true normalizing factor, we must count 
all the potential dependencies, both local and long-
distance.  This is required also because the tree-
filter we use to collect pair relations consider both 
local and long-distance dependencies as well.  
Since simple string matching is not able to get the 
potential long-distance pairs, we resorted to the use 
of a chart-filter.  As the parser we use is a chart 
parser, all the potential constituents are stored in 
the chart, though only a small subset of those will 
end up in the parse tree.  Among the constituents 
created in the chart for the sentence in (5), for in-
stance, we are supposed to find [??] and [??
???????????????????] 
which are adjacent to each other.  The fact that ?
? is the head of the second phrase then makes?
? adjacent to ??.  We will therefore be able to 
get one count of ?? followed by ?? from (5) 
despite the long span of intervening words between 
them.  The use of the chart-filter thus enables us to 
make our normalizing factor more accurate.  The 
probability of a given verb-noun pair occurring in a 
given relation is now the total count of this relation 
in the parse trees throughout the corpus divided by 
the total count of all the potential relations found in 
all the charts created during the processing of this 
corpus.   
 
    The cut-off point we finally used is 50%, i.e. a 
pair+relation will be kept in our knowledge base if 
the probability obtained this way is more than 
50%.  This may seem low, but it is higher than we 
think considering the fact that verb-object and 
modifier-head are not the only relations that can 
hold between a verb and a noun.  In (6), for exam-
ple, ?? is not related to ?? in either way in 
spite of their adjacency.   
 
(6)   ?? ?  ??   ??    ??  ?? 
  tamen qu shanghai   banli     shouxu   suoxu  
   they   go Shanghai handle procedure need  
 ?   ??      ??? 
 de gongzheng      cailiao 
 DE   notarize        material  
 ?They went to Shanghai to handle the nota-
rized material needed for the procedure.? 
 
    We will still find the ??  ?? pair in the 
chart, but it is not expected to appear in either the 
verb-object relation or modifier-head relation in 
the parse tree.  Therefore, the baseline probability 
for any pair+relation might be far below 50% and 
more than 50% is a good indicator that a given pair 
does typically occur in a given relation.   We can 
also choose to keep all the pairs with their prob-
abilities in the knowledge base and let the prob-
abilities be integrated into the probability of the 
complete parse tree at the time of parse ranking.   
 
    The results we obtained from the above proce-
dure are quite clean, in the sense that most of the 
pairs that are classified into the two types of rela-
tions with a probability greater than 50% are cor-
rect.  Here are some sample pairs that we learned. 
 
Verb-Object: 
 
?? ?? test - truth 
?? ?? allocate - recourses 
?? ?? manage - business 
?? ?? offer - love 
?? ?? cheat - pedestrians 
 
Modifier-Head: 
 
?? ?? testing - standard 
?? ?? allocation - plan 
?? ?? management - mode 
?? ?? offering - spirit 
?? ?? cheating - behavior 
 
However, there are pairs that are correct but not 
?typical? enough, especially in the verb-object re-
lations.  Here are some examples: 
 
?? ?? have - meaning 
?? ?? have - impact 
?? ?? have - color 
?? ?? have - function 
?? ?? have - effect 
? 
 
These are truly verb-object relations, but we may 
not want to keep them in our knowledge base for 
the following reasons.  First of all, the verbs in 
such cases usually can take a wide range of objects 
and the strength of association between the verb 
and the object is weak.  In other words, the objects 
are not ?typical?.  Secondly, those verbs tend not 
to occur in the modifier-head relation with a fol-
lowing noun and we gain very little in terms of 
disambiguation by storing those pairs in the 
knowledge base.  To prune away those pairs, we 
used the log-likelihood-ratio algorithm (Dunning, 
1993) to compute the degree of association be-
tween the verb and the noun in each pair. Pairs 
where there is high ?mutual information? between 
the verb and noun would receive higher scores 
while pairs where the verb can co-occur with many 
different nouns would receive lower scores.  Pairs 
with association scores below a certain threshold 
were then thrown out.  This not only makes the 
remaining pairs more ?typical? but helps to clean 
out more garbage.  The resulting knowledge base 
therefore has higher quality. 
3 Evaluation 
The knowledge acquired by the method described 
in the previous section is used in subsequent sen-
tence analysis to prefer those parses where the 
verb-noun sequence is analyzed in the same way as 
specified in the knowledge base.  When processing 
a large corpus, what we typically do is analyzing 
the corpus twice. The first pass is the learning 
phase where we acquire additional knowledge by 
parsing the corpus.  The knowledge acquired is 
used in the second pass to get better parses.  This is 
one example of the general approach of ?improv-
ing parsing by parsing?, as described in (Wu et al
2002).   
  
    To find out how much the learned knowledge 
contributes to the improvement of parsing, we per-
formed a human evaluation.  In the evaluation, we 
used our existing sentence analyzer (Heidorn 2000, 
Jensen et al1993, Wu and Jiang 1998) to process a 
corpus of 271,690 sentences to learn the verb-noun 
relations.  We then parsed the same sentences first 
without the additional knowledge and then with the 
acquired knowledge.  Comparing the outputs, we 
found that 16,445 (6%) of the sentences had differ-
ent analyses in the two passes.  We then randomly 
selected 500 sentences from those ?diff? sentences 
and presented them to a linguist from an independ-
ent agency who, given two different parses of the 
same sentence, was asked to pick the parse she 
judged to be more accurate.  The order in which 
the parses were presented was randomized so that 
the evaluator had no idea as to which tree was from 
the first pass and which one from the second pass. 
 
The linguist?s judgment showed that, with the 
additional knowledge that we acquired, 350 (70%) 
of those sentences parsed better with the additional 
knowledge, 85 (17%) parsed worse, and 65 (13%) 
had parses that were equally good or bad.  In other 
words, the accuracy of sentence analysis improved 
significantly with the learning procedure discussed 
in this paper.   
 
Here is an example where the parse became bet-
ter when the automatically acquired knowledge is 
used.  Due to space limitation, only the parses of a 
fraction of the sentence is given here: 
 
(7)  ?      ??      ??    ??     ?? 
     yao  zunzhao  guojia   ceshi   biaozhun 
     want follow    nation  testing  standard 
 ?(You) must follow the national testing  
standards.? 
 
Because of the fact that?? is ambiguous be-
tween a verb (?follow?) and a preposition (?in ac-
cordance with?), this sentence fragment got the 
parse tree in Figure 7 before the learned knowledge 
was used, where?? was misanalyzed as the ob-
ject of??: 
 
 
Figure 7: old parse of (7) 
 
During the learning process, we acquired ???-
??? as a typical pair where the two words are in 
the modifier-head relationship.  Once this pair was 
added to our knowledge base, we got the correct 
parse, where ?? is analyzed as a verb and?? 
as a modifier of ??: 
 
 
Figure 8: New tree of (7) 
 
We later inspected the sentences where the 
parses became worse and found two sources for the 
regressions.  The main source was of course errors 
in the learned results, since they had not been 
manually checked.  The second source was an en-
gineering problem: the use of the acquired knowl-
edge required the use of additional memory and 
consequently exceeded some system limitations 
when the sentences were very long.   
 
4 Future work 
The approach described in this paper can be ap-
plied to the learning of many other typical syntac-
tic relations between words.  We have already used 
it to learn noun-noun pairs where the first noun is a 
typical modifier of the second noun.  This has 
helped us to rule out incorrect parses where the 
two nouns were not put into the same constituent.  
Other relations we have been trying to learn in-
clude:  
? Noun-noun pairs where the two nouns are in 
conjunction (e.g. ?? ?? ?bride and bride-
groom?); 
? Verb-verb pairs where the two verbs are in 
conjunction (e.g. ?? ?? ?investigate and 
study?); 
? Adjective-adjective pairs where two adjectives 
are in conjunction (e.g. ?? ??  ?young and 
beautiful?); 
? Noun-verb pairs where the noun is a typical 
subject of the verb. 
 
Knowledge of this kind, once acquired, will benefit 
not only parsing, but other NLP applications as 
well, such as machine translation and information 
retrieval. 
 
    In terms of parsing, the benefit we get there is 
similar to what we get in lexicalized statistical 
parsing where parsing decisions can be based on 
specific lexical items.  However, the training of a 
statistical parser requires a tree bank which is ex-
pensive to create while our approach does not.  Our 
approach does require an existing parser, but this 
parser does not have to be perfect and can be im-
proved as the learning goes on.  Once the parser is 
reasonably good, what we need is just raw text, 
which is available in large quantities. 
5 Conclusion 
We have shown in this paper that parsing quality 
can be improved by using the parser as an auto-
matic learner which acquires new knowledge in the 
first pass to help analysis in the second pass.  We 
demonstrated this through the learning of typical 
verb-object and modifier-head relations.  With the 
use of a chart-filter, a tree-filter and the LLR algo-
rithm, we are able to acquire such knowledge with 
high accuracy.  Evaluation shows that the quality 
of sentence analysis can improve significantly with 
the help of the automatically acquired knowledge. 
References 
Dunning, T. 1993.  Accurate methods for the statistics 
of surprise and coincidence.  Computational Linguis-
tics, 19(1): 61-74. 
Heidorn, G. E. 2000. Intelligent writing assistance, in A 
Handbook of Natural Language Processing: Tech-
niques and Applications for the Processing of Lan-
guage as Text, Dale R., Moisl H., and Somers H. eds., 
Marcel Dekker, New York, pp. 181-207.  
Jensen, K., G. Heidorn and S. Richardson. 1993.  Natu-
ral Language Processing: the PLNLP Approach?. 
Kluwer Academic Publishers, Boston. 
Smadja, F. 1993.  Retrieving collocations from text: 
Xtract.  Computational Linguistics, 19(1): 143-177. 
Wu, Andi, J. Pentheroudakis and Z. Jiang, 2002.  Dy-
namic lexical acquisition in Chinese sentence analy-
sis.  In Proceedings of the 19th International 
Conference on Computational Linguistics, pp. 1308-
1312, Taipei, Taiwan. 
Wu, Andi, J. and Z. Jiang, 1998. Word segmentation in 
sentence analysis, in Proceedings of 1998 Interna-
tional Conference on Chinese Information Process-
ing, pp. 46-51.169-180, Beijing, China. 
 
Chinese Word Segmentation in MSR-NLP 
Andi Wu 
Microsoft Research 
One Microsoft Way, Redmond, WA 98052 
andiwu@microsoft.com 
Abstract 
Word segmentation in MSR-NLP is an in-
tegral part of a sentence analyzer which 
includes basic segmentation, derivational 
morphology, named entity recognition, 
new word identification, word lattice 
pruning and parsing.  The final segmenta-
tion is produced from the leaves of parse 
trees.  The output can be customized to 
meet different segmentation standards 
through the value combinations of a set of 
parameters.  The system participated in 
four tracks of the segmentation bakeoff -- 
PK-open, PK-close, CTB-open and CTB-
closed ? and ranked #1, #2, #2 and #3 re-
spectively in those tracks.  Analysis of the 
results shows that each component of the 
system contributed to the scores. 
1 System Description 
The MSR-NLP Chinese system that participated in 
the current segmentation bakeoff is not a stand-
alone word segmenter.  It is a Chinese sentence 
analyzer where the leaves of parse trees are dis-
played as the output of word segmentation.  The 
components of this system are described below. 
1.1 Basic segmentation 
Each input sentence is first segmented into indi-
vidual characters. 1   These characters and their 
combinations are then looked up in a dictionary2 
and a word lattice containing lexicalized words 
only is formed.  Each node in the lattice is a feature 
                                                          
1 If an input line contains more than one sentence, a sentence 
separator is applied to break the line into individual sentences, 
which are then processed one by one and the results are con-
catenated to form a single output.   
2 The lookup is optimized so that not all possible combinations 
are tried.   
matrix that contains the part of speech and other 
grammatical attributes.   Multiple-character words 
may also have information for resolving segmenta-
tion ambiguities.  In general, multiple-character 
words are assigned higher scores than the words 
they subsume, but words like ???? are excep-
tions and such exceptional cases are usually 
marked in the dictionary.  For some of the words 
that tend to overlap with other words, there is also 
information as to what the preferred segmentation 
is.  For instance, the preferred segmentation for 
????? is ??+??? rather than ???+??. 
Such information was collected from segmented 
corpora and stored in the dictionary.   The scores 
are later used in word lattice pruning and parse 
ranking (Wu and Jiang 1998).   
1.2 Derivational morphology and named en-
tity recognition 
After basic segmentation, a set of augmented 
phrase structure rules are applied to the word lat-
tice to form larger word units which include: 
? Words derived from morphological proc-
esses such as reduplication, affixation, 
compounding, merging, splitting, etc. 
? Named entities such as person names, 
place names, company names, product 
names, numbers, dates, monetary units, etc. 
Each of these units is a tree that reflects the history 
of rule application. They are added to the existing 
word lattice as single nodes and treated as single 
words by the parser.  The internal structures are 
useful for various purposes, one of which is the 
customization of word segmentation:  words with 
such structures can all be displayed as single words 
or multiple words depending on where the ?cuts? 
are made in the word tree (Wu 2003).  
1.3 New word identification 
The expanded word lattice built in 1.2 is inspected 
to detect spots of possible OOV new words.  Typi-
cal spots of this kind are sequences of single char-
acters that are not subsumed by longer words.  We 
then use the following information to propose new 
words (Wu and Jiang, 2000). 
? The probability of the character string be-
ing a sequence of independent words; 
? The morphological and syntactic proper-
ties of the characters; 
? Word formation rules; 
? Behavior of each character in existing 
words (e.g. how likely is this character to 
be used as the second character of a two-
character verb).  
? The context in which the characters appear. 
The proposed new words are added to the word 
lattice and they will get used if no successful parse 
can be obtained without them.  When a new word 
proposed this way has been verified by the parser 
(i.e. used in a successful parse) more than n times, 
it will automatically become an entry in the dic-
tionary.  From then on, this word can be looked up 
directly from the dictionary instead of being pro-
posed online. This kind of dynamic lexical acquisi-
tion has been presented in Wu et al(2002).  
1.4 Word lattice pruning 
Now that all the possible words are in the word 
lattice, both statistical and linguistic methods are 
applied to eliminate certain paths.  For instance, 
those paths that contain one or more bound mor-
phemes are pruned away.  Single characters that 
are subsumed by longer words are also thrown out 
if their independent word probabilities are very 
low.  The result is a much smaller lattice that re-
sembles the n-best paths produced by a statistical 
word segmenter.  Because the final resolution of 
ambiguities is expected to be done during parsing, 
the lattice pruning is non-greedy so that no plausi-
ble path will be excluded prematurely.  Many of 
the ambiguities that are eliminated here can also be 
resolved by the parser, but the pruning greatly re-
duces the complexity of the parsing process, mak-
ing the parser much faster and more accurate. 
1.5 Parsing 
The cleaned-up word lattice is then submitted to 
the parser as the initial entries in the parsing chart.  
With the assumption that a successful parse of the 
sentence requires a correct segmentation of the 
sentence, many segmentation ambiguities are ex-
pected to be resolved here.  This assumption does 
not always hold, of course.  A sentence can often 
be parsed in multiple ways and the top-ranking 
parse is not always the correct one.  There are also 
sentences that are not covered by the grammar and 
therefore cannot be parsed at all.   In this latter case, 
we back off to partial parsing and use dynamic 
programming to assemble a tree that consists of the 
largest sub-trees in the chart.   
 
     In most cases, the use of the parser results in 
better segmentation, but the parser can also mis-
lead us.  One of the problems is that the parser 
treats every input as a sentence and tries to con-
struct an S out of it.  As a result, even a name like 
????? can be analyzed as a sentence with ? as 
the subject, ? as the verb and ? as the object, if it 
appears in the wrong context (or no context).   
1.6 Segmentation parameters 
Due to the differences in segmentation standards, 
the leaves of a parse tree do not always correspond 
to the words in a particular standard.  For instance, 
a Chinese full name is a single leaf in our trees, but 
it is supposed to be two words (family name + 
given name) according to the PK standard.  Fortu-
nately, most of the words whose segmentation is 
controversial are built dynamically in our system 
with their internal structures preserved.  A Chinese 
full name, for example, is a word tree where the 
top node dominates two nodes: the family name 
and the given name.  Each non-terminal node in a 
word tree as described in 1.2 is associated with a 
parameter whose value determines whether the 
daughters of this node are to be displayed as a 
singe word or multiple words.  Since all the dy-
namic words are built by phrase structure rules and 
their word trees reflect the derivational history of 
rule application, there is a one-to-one correspon-
dence between the types of words and the word-
internal structures of those words.  A segmentation 
parameter is associated with each type of words3 
and the value of this parameter determines how the 
given type of words should be segmented.   This 
makes it possible for the system to quickly adapt to 
different standards (Wu 2003). 
                                                          
3 There are about 50 parameters in our system. 
1.7 Speed 
Our system is not optimized for word segmentation 
in terms of speed.  As we have seen, the system is 
a sentence analyzer and word segmentation is just 
the by-product of a parser.  The speed we report 
here is in fact the speed of parsing. 
 
On a single 997 MHz Pentium III machine, the 
system is able to process 28,740 characters per 
minute.  The speed may vary according to sentence 
lengths: given texts of the same size, those contain-
ing longer sentences will take more time.  The 
number reported here is an average of the time 
taken to process the test sets of the four tracks we 
participated in.   
 
We have the option of turning off the parser 
during word segmentation. When the parser is 
turned off, segmentation is produced directly from 
the word lattice with dynamic programming which 
selects the shortest path.  The speed in this case is 
about 60,000 characters per minute. 
2 Evaluation  
We participated in the four GB tracks in the first 
international Chinese word segmentation bakeoff -
PK-open, PK-closed, CTB-open and CTB-closed ? 
and ranked #1, #2, #2, and #3 respectively in those 
tracks.  In what follows, we discuss how we got the 
results: what dictionaries we used, how we used 
the training data, how much each component con-
tributed to the scores, and the problems that af-
fected our performance. 
2.1 Dictionaries 
In the open tracks, we used our proprietary dic-
tionary of 89,845 entries, which includes the en-
tries of 7,017 single characters.  In the closed 
tracks, we removed from the dictionary all the 
words that did not appear in the training data, but 
kept all the single characters. This resulted in a 
dictionary of 34,681 entries in the PK track and 
18,207 entries in the CTB track.  It should be noted 
that not all the words in the training data are in our 
dictionary.  This explains why the total numbers of 
entries in those reduced dictionaries are smaller 
than the vocabulary sizes of the respective training 
sets even with all the single-character entries in-
cluded in them.      
 
The dictionary we use in each case is not a sim-
ple word list.  Every word has one or more parts-
of-speech and a number of other grammatical fea-
tures. No word can be used by the parser unless it 
has those features.  This made it very difficult for 
us to add all the words in the training data to the 
dictionary.   We did use a semi-automatic process 
to add as many words as possible, but both the ac-
curacy and coverage of the added grammatical fea-
tures are questionable due to the lack of manual 
verification.    
2.2 Use of the training data 
We used the training data mainly to tune the seg-
mentation parameters of our system.  As has been 
mentioned in 1.6, there are about 50 types of mor-
phologically derived words that are built online in 
our system and each type has a parameter to de-
termine whether a given unit should be displayed 
as a single word or separate words.  Since our de-
fault segmentation is very different from PK or 
CTB, and PK and CTB also follow different guide-
lines, we had to try different value combinations of 
the parameters in each case until we got the opti-
mal settings.   
 
The main problem in the tuning is that many 
morphologically derived words have been lexical-
ized in our dictionary and therefore do not have the 
word-internal structures that they would have if 
they had been constructed dynamically.  As a re-
sult, their segmentation is beyond the control of 
those parameters.  To solve this problem, we used 
the training data to automatically identify all such 
cases, create a word-internal structure for each of 
them, and store the word tree in their lexical en-
tries.4  This made it possible for the parameter val-
ues to apply to both the lexicalized and non-
lexicalized words.  This process can be fairly 
automatic if the annotation of the training data is 
completely consistent.  However, as we have dis-
covered, the training data is not as consistent as 
expected, which made total automation impossible. 
2.3 Contribution of each component 
After we received our individual scores and the 
reference testing data, we did some ablation ex-
                                                          
4 The work is incomplete, since the trees were created only for 
those words that are in the training data provided. 
periments to find out the contribution of each sys-
tem component in this competition.  We turned off 
the components one at a time (except basic seg-
mentation) and recorded the scores of each ablated 
system.  The results are summarized in the follow-
ing table, where ?DM-NER? stands for ?deriva-
tional morphology and named entity recognition?, 
?NW-ID? for ?new word identification and lexical-
ization?, ?pruning? for ?lattice pruning? and ?tun-
ing? for ?tuning of parameter values?. Each cell in 
the table has two percentages.  The top one is the 
F-measure and the bottom one is the OOV word 
recall rate. 
 
 PK 
Open 
PK 
closed 
CTB 
open 
CTB 
closed 
Complete 
System 
95.9 % 
79.9 % 
94.7 % 
68.0 % 
90.1 % 
73.8 % 
83.1 % 
43.1 % 
Without 
DM-NER 
90.2 % 
44.4 % 
88.9 % 
33.9 % 
86.6 % 
66.6 % 
79.2 % 
33.5 % 
Without 
NW-ID 
95.8 % 
77.3 % 
94.0 % 
61.2 % 
88.7 % 
69.0 % 
79.2 % 
28.2 % 
Without 
Pruning 
92.0 % 
77.5 % 
90.9 % 
65.9 % 
85.5 % 
69.0 % 
78.8 % 
39.5 % 
Without 
Parsing 
95.5 % 
79.9 % 
94.4 % 
68.5 % 
89.8 % 
75.0 % 
84.0 % 
48.1 % 
Without 
Tuning 
84.8 % 
43.4 % 
83.9 % 
33.3 % 
84.8 % 
72.3 % 
78.4 % 
43.3 % 
 
Several interesting facts are revealed in this 
break-down: 
? The tuning of parameter values has the big-
gest impact on the scores across the board. 
? Derivational morphology and NE recogni-
tion is also a main contributor, especially in 
the PK sets, which presumably contains 
more named entities. 
? The impact of new word identification is 
minimal when the OOV word rate is low, 
such as in the PK-open case, but becomes 
more and more significant as the OOV rate 
increases. 
? Lattice pruning makes a big difference as 
well.  Apparently it cannot be replaced by 
the parser in terms of the disambiguating 
function it performs.  Another fact, which is 
not represented in the table, is that parsing 
is three times slower when lattice pruning is 
turned off. 
? The parser has very limited impact on the 
scores.  Looking at the data, we find that 
parsing did help to resolve some of the 
most difficult cases of ambiguities and we 
would not be able to get the last few points 
without it.  But it seems that most of the 
common problems can be solved without 
the parser.  In one case (CTB closed), the 
score is higher when the parser is turned off.  
This is because the parser may prefer a 
structure where those dynamically recog-
nized OOV words are broken up into 
smaller units.  For practical purposes, there-
fore, we may choose to leave out the parser. 
2.4 Problems that affected our performance 
The main problem is the definition of new words.  
While our system is fairly aggressive in recogniz-
ing new words, both PK and CTB are quite con-
servative in this respect.  Expressions such as ??
??, ????, ?????, ?????? are 
considered single words in our system but not so in 
PK or CTB.  This made our new word recognition 
do more harm than good in many cases, though the 
overall impact is positive.  Consistency in the an-
notated corpora is another problem, but this affects 
every participant.  We also had a technical problem 
where some sentences remained unsegmented sim-
ply because some characters are not in our diction-
ary.   
 
References 
Wu, Andi. 2003. Customizable segmentation of mor-
phologically derived Words in Chinese, to appear in 
Computational Linguistics and Chinese Language 
Processing., 8(2). 
Wu, Andi, J. Pentheroudakis and Z. Jiang, 2002.  Dy-
namic lexical acquisition in Chinese sentence analy-
sis.  In Proceedings of the 19th International 
Conference on Computational Linguistics, pp. 1308-
1312, Taipei, Taiwan. 
Wu, Andi, J. and Z. Jiang, 2000. Statistically-enhanced 
new word identification in a rule-based Chinese sys-
tem, in Proceedings of the 2nd Chinese Language 
Processing Workshop, pp. 46-51, HKUST, Hong 
Kong. 
Wu, Andi, J. and Z. Jiang, 1998. Word segmentation in 
sentence analysis, in Proceedings of 1998 Interna-
tional Conference on Chinese Information Process-
ing, pp. 46-51.169-180, Beijing, China. 
Adaptive Chinese Word Segmentation1  
Jianfeng Gao*, Andi Wu*, Mu Li*, Chang-Ning Huang*, Hongqiao Li**, Xinsong Xia$, Haowei Qin&  
*
 Microsoft Research. {jfgao, andiwu, muli, cnhuang}@microsoft.com 
**
 Beijing Institute of Technology, Beijing. lhqtxm@bit.edu.cn 
$
 Peking University, Beijing. xia_xinsong@founder.com 
&
 Shanghai Jiaotong university, Shanghai. haoweiqin@sjtu.edu.cn 
                                                   
1
 This work was done while Hongqiao Li, Xinsong Xia and Haowei Qin were visiting Microsoft Research (MSR) Asia. We thank 
Xiaodan Zhu for his early contribution, and the three reviewers, one of whom alerted us the related work of (Uchimoto et al, 2001). 
Abstract 
This paper presents a Chinese word segmen-
tation system which can adapt to different 
domains and standards. We first present a sta-
tistical framework where domain-specific 
words are identified in a unified approach to 
word segmentation based on linear models. 
We explore several features and describe how 
to create training data by sampling. We then 
describe a transformation-based learning 
method used to adapt our system to different 
word segmentation standards. Evaluation of 
the proposed system on five test sets with dif-
ferent standards shows that the system 
achieves state- of-the-art performance on all of 
them. 
1 Introduction 
Chinese word segmentation has been a long- 
standing research topic in Chinese language proc-
essing. Recent development in this field shows that, 
in addition to ambiguity resolution and unknown 
word detection, the usefulness of a Chinese word 
segmenter also depends crucially on its ability to 
adapt to different domains of texts and different 
segmentation standards.  
The need of adaptation involves two research 
issues that we will address in this paper. The first is 
new word detection. Different domains/applications 
may have different vocabularies which contain new 
words/terms that are not available in a general 
dictionary. In this paper, new words refer to OOV 
words other than named entities, factoids and mor-
phologically derived words. These words are 
mostly domain specific terms (e.g. ??? ?cellular?) 
and time-sensitive political, social or cultural terms 
(e.g. ???Three Links?, ?? ?SARS?).  
The second issue concerns the customizable 
display of word segmentation. Different Chinese 
NLP-enabled applications may have different re-
quirements that call for different granularities of 
word segmentation. For example, speech recogni-
tion systems prefer ?longer words? to achieve 
higher accuracy whereas information retrieval 
systems prefer ?shorter words? to obtain higher 
recall rates, etc. (Wu, 2003). Given a word seg-
mentation specification (or standard) and/or some 
application data used as training data, a segmenter 
with customizable display should be able to provide 
alternative segmentation units according to the 
specification which is either pre-defined or implied 
in the data. 
In this paper, we first present a statistical 
framework for Chinese word segmentation, where 
various problems of word segmentation are solved 
simultaneously in a unified approach.  Our ap-
proach is based on linear models where component 
models are inspired by the source-channel models 
of Chinese sentence generation. We then describe in 
detail how the new word identification (NWI) 
problem is handled in this framework. We explore 
several features and describe how to create training 
data by sampling. We evaluate the performance of 
our segmentation system using an annotated test set, 
where new words are simulated by sampling. We 
then describe a transformation-based learning (TBL, 
Brill, 1995) method that is used to adapt our system 
to different segmentation standards. We compare 
the adaptive system to other state-of-the-art systems 
using four test sets in the SIGHAN?s First Interna-
tional Chinese Word Segmentation Bakeoff, each of 
which is constructed according to a different seg-
mentation standard. The performance of our system 
is comparable to the best systems reported on all 
four test sets. It demonstrates the possibility of 
having a single adaptive Chinese word segmenter 
that is capable of supporting multiple user applica-
tions. 
Word Class2 Model Feature Functions, f(S,W) 
Context Model Word class based trigram, P(W). -log(P(W)) 
Lexical Word (LW) --- 1 if S forms a word lexicon entry, 0 otherwise. 
Morphological Word (MW) --- 1 if S forms a morph lexicon entry, 0 otherwise. 
Named Entity (NE) Character/word bigram, P(S|NE). -log(P(S|NE)) 
Factoid (FT) --- 1 if S can be parsed using a factoid grammar, 0 otherwise 
New Word (NW) --- Score of SVM classifier 
Figure 1: Context model, word classes, and class models, and feature functions. 
                                                   
2
 In our system, we define three types of named entity: person name (PN), location name (LN), organization (ON) and translit-
eration name (TN); ten types of factoid: date, time (TIME), percentage, money, number (NUM), measure, e-mail, phone number, 
and WWW; and five types of morphologically derived words (MDW): affixation, reduplication, merging, head particle and split. 
2 Chinese Word Segmentation with 
Linear Models 
Let S be a Chinese sentence which is a character 
string. For all possible word segmentations W, we 
will choose the most likely one W* which achieves 
the highest conditional probability P(W|S): W* = 
argmaxw P(W|S). According to Bayes? decision rule 
and dropping the constant denominator, we can 
equivalently perform the following maximization: 
)|()(maxarg* WSPWPW
W
=
. 
(1) 
Equation (1) represents a source-channel approach 
to Chinese word segmentation. This approach 
models the generation process of a Chinese sen-
tence: first, the speaker selects a sequence of con-
cepts W to output, according to the probability 
distribution P(W); then he attempts to express each 
concept by choosing a sequence of characters, 
according to the probability distribution P(S|W).  
We define word class as a group of words that 
are supposed to be generated according to the same 
distribution (or in the same manner). For instance, 
all Chinese person names form a word class. We 
then have multiple channel models, each for one 
word class. Since a channel model estimates the 
likelihood that a character string is generated given 
a word class, it is also referred to as class model. 
Similarly, source model is referred to as context 
model because it indicates the likelihood that a word 
class occurs in a context. We have only one context 
model which is a word-class-based trigram model. 
Figure 1 shows word classes and class models that 
we used in our system. We notice that different 
class models are constructed in different ways (e.g. 
name entity models are n-gram models trained on 
corpora whereas factoid models use derivation rules 
and have binary values). The dynamic value ranges 
of different class models can be so different that it is 
improper to combine all models through simple 
multiplication as Equation (1). 
In this study we use linear models. The method 
is derived from linear discriminant functions widely 
used for pattern classification (Duda et al, 2001), 
and has been recently introduced into NLP tasks by 
Collins and Duffy (2001). It is also related to log- 
linear models for machine translation (Och, 2003).  
In this framework, we have a set of M+1 feature 
functions fi(S,W), i = 0,?,M. They are derived from 
the context model (i.e. f0(W)) and M class models, 
each for one word class, as shown in Figure 1: For 
probabilistic models such as the context model or 
person name model, the feature functions are de-
fined as the negative logarithm of the corresponding 
probabilistic models. For each feature function, 
there is a model parameter ?i. The best word seg-
mentation W* is determined by the decision rule as 
?
=
==
M
i
ii
W
M
W
WSfWSScoreW
0
0
* ),(maxarg),,(maxarg ??  (2) 
Below we describe how to optimize ?s. Our 
method is a discriminative approach inspired by the 
Minimum Error Rate Training method proposed in 
Och (2003). Assume that we can measure the 
number of segmentation errors in W by comparing it 
with a reference segmentation R using a function 
Er(R,W). The training criterion is to minimize the 
count of errors over the training data as 
?=
RWS
M
M
SWREr
M
,,
11
^
)),(,(minarg
1
??
?
, (3) 
where W is detected by Equation (2). However, we 
cannot apply standard gradient descent to optimize  
Initialization: ?0=?, ?i=1, i = 1,?,M. 
For t = 1 ? T,  j = 1 ? N 
 Wj = argmax ? ?i fi(Sj,W) 
 For i = 1? M 
  ?i = ?i + ?(Score(?,S,W)-Score(?,S,R))(fi(R) - fi(W)),  
where ?={?0, ?1,?,?M} and ? =0.001. 
Figure 2: The training algorithm for model parameters 
model parameters according to Equation (3) be-
cause the gradient cannot be computed explicitly 
(i.e., Er is not differentiable), and there are many 
local minima in the error surface. We then use a 
variation called stochastic gradient descent (or 
unthresholded perceptron, Mitchell, 1997). As 
shown in Figure 2, the algorithm takes T passes over 
the training set (i.e. N sentences). All parameters are 
initially set to be 1, except for the context model 
parameter ?0 which is set to be a constant ? during 
training, and is estimated separately on held-out 
data. Class model parameters are updated in a sim-
ple additive fashion. Notice that Score(?,S,W) is not 
less than Score(?,S,R). Intuitively the updated rule 
increases the parameter values for word classes 
whose models were ?underestimated? (i.e. expected 
feature value f(W) is less than observed feature 
value f(R)), and decreases the parameter values 
whose models were ?overestimated? (i.e. f(W) is 
larger than f(R)).  Although the method cannot 
guarantee a global optimal solution, it is chosen for 
our modeling because of its efficiency and the best 
results achieved in our experiments. 
Given the linear models, the procedure of word 
segmentation in our system is as follows: First, all 
word candidates (lexical words and OOV words of 
certain types) are generated, each with its word 
class tag and class model score. Second, Viterbi 
search is used to select the best W according to 
Equation (2). Since the resulting W* is a sequence of 
segmented words that are either lexical words or 
OOV words with certain types (e.g. person name, 
morphological words, new words) we then have a 
system that can perform word segmentation and 
OOV word detection simultaneously in a unified 
approach. Most previous works treat OOV word 
detection as a separate step after word segmentation. 
Compared to these approaches, our method avoids 
the error propagation problem and can incorporate a 
variety of knowledge to achieve a globally optimal 
solution. The superiority of the unified approach 
has been demonstrated empirically in Gao et al 
(2003), and will also be discussed in Section 5. 
3 New Word Identification 
New words in this section refer to OOV words that 
are neither recognized as named entities or factoids 
nor derived by morphological rules. These words 
are mostly domain specific and/or time-sensitive. 
The identification of such new words has not been 
studied extensively before. It is an important issue 
that would have substantial impact on the per-
formance of word segmentation. For example, 
approximately 30% of OOV words in the 
SIGHAN?s PK corpus (see Table 1) are new words 
of this type. There has been previous work on de-
tecting Chinese new words from a large corpus in 
an off-line manner and updating the dictionary 
before word segmentation. However, our approach 
is able to detect new words on-line, i.e. to spot new 
words in a sentence on the fly during the process of 
word segmentation where widely-used statistical 
features such as mutual information or term fre-
quency are not available. 
For brevity of discussion, we will focus on the 
identification of 2-character new words, denoted as 
NW_11. Other types of new words such as NW_21 
(a 2-character word followed with a character) and 
NW_12 can be detected similarly (e.g. by viewing 
the 2-character word as an inseparable unit, like a 
character). Below, we shall describe the class model 
and context model for NWI, and the creation of 
training data by sampling. 
3.1 Class Model 
We use a classifier (SVM in our experiments) to 
estimate the likelihood of two adjacent characters to 
form a new word. Of the great number of features 
we experimented, three linguistically-motivated 
features are chosen due to their effectiveness and 
availability for on-line detection. They are Inde-
pendent Word Probability (IWP), Anti-Word Pair 
(AWP), and Word Formation Analogy (WFA). 
Below we describe each feature in turn. In Section 
3.2, we shall describe the way the training data (new 
word list) for the classifier is created by sampling. 
IWP is a real valued feature. Most Chinese 
characters can be used either as independent words 
or component parts of multi-character words, or 
both. The IWP of a single character is the likelihood 
for this character to appear as an independent word 
in texts (Wu and Jiang, 2000): 
)(
) ,()(
xC
WxC
xIWP = . (4) 
where C(x, W) is the number of occurrences of the 
character x as an independent word in training data, 
and C(x) is the total number of x in training data. We 
assume that the IWP of a character string is the 
product of the IWPs of the component characters. 
Intuitively, the lower the IWP value, the more likely 
the character string forms a new word. In our im-
plementation, the training data is word-segmented. 
AWP is a binary feature derived from IWP. For 
example, the value of AWP of an NW_11 candidate 
ab is defined as: AWP(ab)=1 if IWP(a)>? or IWP(b) 
>?, 0 otherwise. ? ? [0, 1] is a pre-set threshold. 
Intuitively, if one of the component characters is 
very likely to be an independent word, it is unlikely 
to be able to form a word with any other characters. 
While IWP considers all component characters in a 
new word candidate, AWP only considers the one 
with the maximal IWP value. 
WFA is a binary feature. Given a character pair 
(x, y), a character (or a multi-character string) z is 
called the common stem of (x, y) if at least one of the 
following two conditions hold: (1) character strings 
xz and yz are lexical words (i.e. x and y as prefixes); 
and (2) character strings zx and zy are lexical words 
(i.e. x and y as suffixes). We then collect a list of 
such character pairs, called affix pairs, of which the 
number of common stems is larger than a pre-set 
threshold. The value of WFA for a given NW_11 
candidate ab is defined as: WFA(ab) = 1 if there 
exist an affix pair (a, x) (or (b, x)) and the string xb 
(or ax) is a lexical word, 0 otherwise. For example, 
given an NW_11 candidate ?? (xia4-gang3, ?out of 
work?), we have WFA(??) = 1 because (?, ?) is 
an affix pair (they have 32 common stems such as _
?,  ?,  ?,  ?,  ?,  ?,  ?) and ?? 
(shang4-gang3, ?take over a shift?) is a lexical word. 
3.2 Context Model 
The motivations of using context model for NWI 
are two-fold. The first is to capture useful contex-
tual information. For example, new words are more 
likely to be nouns than pronouns, and the POS 
tagging is context-sensitive. The second is more 
important. As described in Section 2, with a context 
model, NWI can be performed simultaneously with 
other word segmentation tasks (e.g.: word break, 
named entity recognition and morphological analy-
sis) in a unified approach. 
However, it is difficult to develop a training 
corpus where new words are annotated because ?we 
usually do not know what we don?t know?. Our 
solution is Monte Carlo simulation. We sample a set 
of new words from our dictionary according to the 
distribution ? the probability that any lexical word 
w would be a new word P(NW|w). We then generate 
a new-word-annotated corpus from a word-seg-
mented text corpus.  
Now we describe the way P(NW|w) is estimated. 
It is reasonable to assume that new words are those 
words whose probability to appear in a new docu-
ment is lower than general lexical words. Let Pi(k) 
be the probability of word wi that occurs k times in a 
document. In our experiments, we assume that 
P(NW|wi) can be approximated by the probability of 
wi occurring less than K times in a new document:  
??
=
?
1
0
)()|(
K
k
ii kPwNWP , (5) 
where the constant K is dependent on the size of the 
document: The larger the document, the larger the 
value. Pi(k) can be estimated using several term 
distribution models (see Chapter 15.3 in Manning 
and Sch?tze, 1999). Following the empirical study 
in (Gao and Lee, 2000), we use K-Mixture (Katz, 
1996) which estimate Pi(k) as 
k
ki kP )1(1)1()( 0, +++?= ?
?
?
??? , (6) 
where ?k,0=1 if  k=0, 0 otherwise. ? and ? are pa-
rameters that can be fit using the observed mean ? 
and the observed inverse document frequency IDF 
as follow: 
N
cf
=? , 
df
NIDF log= , 
df
dfcfIDF ?
=??= 12?? , and ?
?
? = , 
where cf is the total number of occurrence of word 
wi in training data, df is the number of documents in 
training data that wi occurs in, and N is the total 
number of documents. In our implementation, the 
training data contain approximately 40 thousand 
documents that have been balanced among domain, 
style and time. 
4 Adaptation to Different Standards 
The word segmentation standard (or standard for 
brevity) varies from system to system because there 
is no commonly accepted definition of Chinese
   
Condition: ?Affixation? Condition: ?Date? Condition: ?PersonName? 
Actions: Insert a boundary 
between ?Prefix? and ?Stem?? 
Actions: Insert a boundary between 
?Year? and ?Mon? ? 
Actions: Insert a boundary be-
tween ?FamilyName? and ?Given-
Name?? 
Figure 3: Word internal structure and class-type transformation templates. 
words and different applications may have different 
requirements that call for different granularities of 
word segmentation. 
It is ideal to develop a single word segmentation 
system that is able to adapt to different standards. 
We consider the following standard adaptation 
paradigm. Suppose we have a ?general? standard 
pre-defined by ourselves. We have also created a 
large amount of training data which are segmented 
according to this general standard. We then develop 
a generic word segmenter, i.e. the system described 
in Sections 2 and 3. Whenever we deploy the seg-
menter for any application, we need to customize 
the output of the segmenter according to an appli-
cation-specific standard, which is not always ex-
plicitly defined. However, it is often implicitly 
defined in a given amount of application data 
(called adaptation data) from which the specific 
standard can be partially learned. 
In our system, the standard adaptation is con-
ducted by a postprocessor which performs an or-
dered list of transformations on the output of the 
generic segmenter ? removing extraneous word 
boundaries, and inserting new boundaries ? to 
obtain a word segmentation that meets a different 
standard. 
The method we use is transformation-based 
learning (Brill, 1995), which requires an initial 
segmentation, a goal segmentation into which we 
wish to transform the initial segmentation and a 
space of allowable transformations (i.e. transfor-
mation templates). Under the abovementioned 
adaptation paradigm, the initial segmentation is the 
output of the generic segmenter. The goal segmen-
tation is adaptation data. The transformation tem-
plates can make reference to words (i.e. lexicalized 
templates) as well as some pre-defined types (i.e. 
class-type based templates), as described below. 
We notice that most variability in word seg-
mentation across different standards comes from 
those words that are not typically stored in the 
dictionary. Those words are dynamic in nature and 
are usually formed through productive morpho-
logical processes. In this study, we focus on three 
categories: morphologically derived words (MDW), 
named entities (NE) and factoids. 
For each word class that belongs to these cate-
gories2, we define an internal structure similar to 
(Wu, 2003). The structure is a tree with ?word class? 
as the root, and ?component types? as the other 
nodes. There are 30 component types. As shown in 
Figure 3, the word class Affixation has three 
component types: Prefix, Stem and Suffix. 
Similarly, PersonName has two component types 
and Date has nine ? 3 as non-terminals and 6 as 
terminals. These internal structures are assigned to 
words by the generic segmenter at run time. 
The transformation templates for words of the 
above three categories are of the form: 
Condition: word class 
Actions:  
z Insert ? place a new boundary 
between two component types. 
z Delete ? remove an existing 
boundary between two component 
types. 
Since the application of the transformations de-
rived from the above templates are conditioned on 
word class and make reference to component types, 
we call the templates class-type transformation 
templates. Some examples are shown in Figure 3. 
In addition, we also use lexicalized transforma-
tion templates as: 
z Insert ? place a new boundary 
between two lemmas. 
Mon Day
Pre_Y Pre_MDig_M Dig_D
Year 
Date
PersonName 
FamilyName GivenName
Affixation 
Prefix Stem Suffix
Pre_DDig_Y 
z Delete ? remove an existing 
boundary between two lemmas. 
Here, lemmas refer to those basic lexical words 
that cannot be formed by any productive morpho-
logical process. They are mostly single characters, 
bi-character words, and 4-character idioms. 
In short, our adaptive Chinese word segmenter 
consists of two components: (1) a generic seg-
menter that is capable of adapting to the vocabu-
laries of different domains and (2) a set of output 
adaptors, learned from application data, for adapt-
ing to different ?application-specific? standards  
5 Evaluation 
We evaluated the proposed adaptive word seg-
mentation system (henceforth AWS) using five 
different standards. The training and test corpora of 
these standards are detailed in Table 1, where MSR 
is defined by ourselves, and the other four are stan-
dards used in SIGHAN?s First International Chi-
nese Word Segmentation Bakeoff (Bakeoff test sets 
for brevity, see Sproat and Emperson (2003) for 
details). 
Corpus Abbrev. # Tr. Word # Te. Word 
?General? standard  MSR 20M 226K 
Beijing University PK 1.1M 17K 
U. Penn Chinese 
Treebank 
CTB 250K 40K 
Hong Kong City U. HK 240K 35K 
Academia Sinica AS 5.8M 12K 
Table 1: standards and corpora. 
MSR is used as the general standard in our ex-
periments, on the basis of which the generic seg-
menter has been developed. The training and test 
corpora were annotated manually, where there is 
only one allowable word segmentation for each 
sentence. The training corpus contains approxi-
mately 35 million Chinese characters from various 
domains of text such as newspapers, novels, maga-
zines etc. 90% of the training corpus are used for 
context model training, and 10% are held-out data 
for model parameter training as shown in Figure 2. 
The NE class models, as shown in Figure 1, were 
trained on the corresponding NE lists that were 
collected separately. The test set contains a total of 
225,734 tokens, including 205,162 lexi-
con/morph-lexicon words, 3,703 PNs, 5,287 LNs, 
3,822 ONs, and 4,152 factoids. In Section 5.1, we 
will describe some simulated test sets that are de-
rived from the MSR test set by sampling NWs from 
a 98,686-entry dictionary. 
The four Bakeoff standards are used as ?specific? 
standards into which we wish to adapt the general 
standard. We notice in Table 1 that the sizes of 
adaptation data sets (i.e. training corpora of the four 
Bakeoff standards) are much smaller than that of the 
MSR training set. The experimental setting turns 
out to be a good simulation of the adaptation para-
digm described in Section 4. 
The performance of word segmentation is 
measured through test precision (P), test recall (R), 
F score (which is defined as 2PR/(P+R)), the OOV 
rate for the test corpus (on Bakeoff corpora, OOV is 
defined as the set of words in the test corpus not 
occurring in the training corpus.), the recall on 
OOV words (Roov), and the recall on in-vocabulary 
(Riv) words. We also tested the statistical signifi-
cance of results, using the criterion proposed by 
Sproat and Emperson (2003), and all results re-
ported in this section are significantly different 
from each other. 
5.1 NWI Results 
This section discusses two factors that we believe 
have the most impact on the performance of NWI. 
First, we compare methods where we use the NWI 
component (i.e. an SVM classifier) as a post- 
processor versus as a feature function in the linear 
models of Equation (2). Second, we compare dif-
ferent sampling methods of creating simulated 
training data for context model. Which sampling 
method is best depends on the nature of P(NW|w). 
As described in Section 3.2, P(NW|w) is unknown 
and has to be approximated by Pi(k) in our study, so 
it is expected that the closer P(NW|w) and Pi(k) are, 
the better the resulting context model. We compare 
three estimates of Pi(k) in Equation (5) using term 
models based on Uniform, Possion, and K- Mixture 
distributions, respectively. 
Table 2 shows the results of the generic seg-
menter on three test sets that are derived from the 
MSR test set using the above three different sam-
pling methods, respectively. For all three distribu-
tions, unified approaches (i.e. using NWI compo-
nent as a feature function) outperform consecutive 
approaches (i.e. using NWI component as a post- 
processor). This demonstrates empirically the 
benefits of using context model for NWI and the 
unified approach to Chinese word segmentation, as 
described in 3.2. We also perform NWI on Bakeoff 
AWS w/o NW AWS w/ NW (post-processor) AWS w/ NW (unified approach) 
word segmentation word segmentation NW word segmentation NW  # of NW 
P% R% P% R% P% R% P% R% P% R% 
Uniform 5,682 92.6 94.5 94.7 95.2 64.1 66.8 95.1 95.5 68.1 78.4 
Poisson 3,862 93.4 95.6 94.5 95.9 61.4 45.6 95.0 95.7 57.2 60.6 
K-Mixture 2,915 94.7 96.4 95.1 96.2 44.1 41.5 95.6 96.2 46.2 60.4 
Table 2: NWI results on MSR test set, NWI as post-processor versus unified approach 
PK CTB 
 P R F OOV Roov Riv P R F OOV Roov Riv 
1. AWS w/o adaptation .824 .854 .839 .069 .320 .861 .799 .818 .809 .181 .624 .861 
2. AWS .952 .959 .955 .069 .781 .972 .895 .914 .904 .181 .746 .950 
3. AWS w/o NWI .949 .963 .956 .069 .741 .980 .875 .910 .892 .181 .690 .959 
4. FMM w/ adaptation .913 .946 .929 .069 .524 .977 .805 .874 .838 .181 .521 .952 
5. Rank 1 in Bakeoff .956 .963 .959 .069 .799 .975 .907 .916 .912 .181 .766 .949 
6. Rank 2 in Bakeoff .943 .963 .953 .069 .743 .980 .891 .911 .901 .181 .736 .949 
Table 3: Comparison scores for PK open and CTB open. 
HK AS 
 
P R F OOV Roov Riv P R F OOV Roov Riv 
1. AWS w/o adaptation .819 .822 .820 .071 .593 .840 .832 .838 .835 .021 .405 .847 
2. AWS .948 .960 .954 .071 .746 .977 .955 .961 .958 .021 .584 .969 
3. AWS w/o NWI .937 .958 .947 .071 .694 .978 .958 .943 .951 .021 .436 .969 
4. FMM w/ adaptation .818 .823 .821 .071 .591 .841 .930 .947 .939 .021 .160 .964 
5. Rank 1 in Bakeoff .954 .958 .956 .071 .788 .971 .894 .915 .904 .021 .426 .926 
6. Rank 2 in Bakeoff .863 .909 .886 .071 .579 .935 .853 .892 .872 .021 .236 .906 
Table 4: Comparison scores for HK open and AS open. 
test sets. As shown in Tables 3 and 4 (Rows 2 and 3), 
the use of NW functions (via the unified approach) 
substantially improves the word segmentation per-
formance. 
We find in our experiments that NWs sampled 
by Possion and K-Mixture are mostly specific and 
time-sensitive terms, in agreement with our intui-
tion, while NWs sampled by Uniform include more 
common words and lemmas that are easier to detect. 
Consequently, by Uniform sampling, the P/R of 
NWI is the highest but the P/R of the overall word 
segmentation is the lowest, as shown in Table 2. 
Notice that the three sampling methods are not 
comparable in terms of P/R of NWI in Table 2 
because of different sampling result in different sets 
of new words in the test set. We then perform NWI 
on Bakeoff test sets where the sets of new words are 
less dependent on specific sampling methods. The 
results however do not give a clear indication which 
sampling method is the best because the test sets are 
too small to show the difference. We then leave it to 
future work a thorough empirical comparison 
among different sampling methods. 
5.2 Standard Adaptation Results 
The results of standard adaptation on four Bakeoff 
test sets are shown in Tables 3 and 4. A set of 
transformations for each standard is learnt using 
TBL from the corresponding Bakeoff training set. 
For each test set, we report results using our system 
with and without standard adaptation (Rows 1 and 
2). It turns out that performance improves dra-
matically across the board in all four test sets. 
For comparison, we also include in each table 
the results of using the forward maximum matching 
(FMM) greedy segmenter as a generic segmenter 
(Row 4), and the top 2 scores (sorted by F) that are 
reported in SIGHAN?s First International Chinese 
Word Segmentation Bakeoff (Rows 5 and 6). We 
can see that with adaptation, our generic segmenter 
can achieve state-of-the-art performance on dif-
ferent standards, showing its superiority over other 
systems. For example, there is no single segmenter 
in SIGHAN?s Bakeoff, which achieved top-2 ranks 
in all four test sets (Sproat and Emperson, 2003). 
We notice in Table 3 and 4 that the quality of 
adaptation seems to depend largely upon the size of 
adaptation data: we outperformed the best bakeoff 
systems in the AS set because the size of the adap-
tation data is big while we are worse in the CTB set 
because of the small size of the adaptation data. To 
verify our speculation, we evaluated the adaptation 
results using subsets of the AS training set of dif-
ferent sizes, and observed the same trend. However, 
even with a much smaller adaptation data set (e.g. 
250K), we still outperform the best bakeoff results. 
6 Related Work 
Many methods of Chinese word segmentation have 
been proposed (See Wu and Tseng, 1993; Sproat 
and Shih, 2001 for reviews). However, it is difficult 
to compare systems due to the fact that there is no 
widely accepted standard. There has been less work 
on dealing with NWI and standard adaptation. 
All feature functions in Figure 1, except the NW 
function, are derived from models presented in 
(Gao et al, 2003). The linear models are similar to 
what was presented in Collins and Duffy (2001). An 
alternative to linear models is the log-linear models 
suggested by Och (2003). See Collins (2002) for a 
comparison of these approaches. 
The features for NWI were studied in Wu & 
Jiang (2000) and Li et al (2004). The use of sam-
pling was proposed in Della Pietra et al (1997) and 
Rosenfeld et al (2001). There is also a related work 
on this line in Japanese (Uchimoto et al, 2001). 
A detailed discussion on differences among the 
four Bakeoff standards is presented in Wu (2003), 
which also proposes an adaptive system where the 
display of the output can be customized by users. 
The method described in Section 4 can be viewed as 
an improved version in that the transformations are 
learnt automatically from adaptation data. The use 
of TBL for Chinese word segmentation was first 
suggested in Palmer (1997). 
7 Conclusion 
This paper presents a statistical approach to adap-
tive Chinese word segmentation based on linear 
models and TBL. The system has two components: 
A generic segmenter that can adapt to the vocabu-
laries of different domains, and a set of output 
adaptors, learned from application data, for adapt-
ing to different ?application-specific? standards. 
We evaluate our system on five test sets, each cor-
responding to a different standard. We achieve 
state-of-the-art performance on all test sets. 
References 
Brill, Eric. 1995. Transformation-based error-driven 
learning and natural language processing: a case study 
in Part-of-Speech tagging. In: Computational Linguis-
tics, 21(4). 
Collins, Michael and Nigel Duffy. 2001. Convolution 
kernels for natural language. In: Advances in Neural 
Information Processing Systems (NLPS 14). 
Collins, Michael. 2002. Parameter estimation for statis-
tical parsing models: theory and practice of distribu-
tion-free methods. To appear. 
Della Pietra, S., Della Pietra, V., and Lafferty, J. 1997. 
Inducing features of random fields. In: IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 19, 
380-393. 
Duda, Richard O, Hart, Peter E. and Stork, David G. 
2001. Pattern classification. John Wiley & Sons, Inc. 
Gao, Jianfeng and Kai-Fu Lee. 2000. Distribution based 
pruning of backoff language models. In: ACL2000. 
Gao, Jianfeng, Mu Li and Chang-Ning Huang. 2003. 
Improved source-channel model for Chinese word 
segmentation. In: ACL2003.  
Katz, S. M. 1996. Distribution of content words and 
phrases in text and language modeling, In: Natural 
Language Engineering, 1996(2): 15-59 
Li, Hongqiao, Chang-Ning Huang, Jianfeng Gao and 
Xiaozhong Fan. 2004. The use of SVM for Chinese 
new word identification. In: IJCNLP2004. 
Manning, C. D. and H. Sch?tze, 1999. Foundations of 
Statistical Natural Language Processing. The MIT 
Press.  
Mitchell, Tom M. 1997. Machine learning. The 
McGraw-Hill Companies, Inc. 
Och, Franz. 2003. Minimum error rate training in statis-
tical machine translation. In: ACL2003. 
Palmer, D. 1997. A trainable rule-based algorithm for 
word segmentation. In: ACL '97. 
Rosenfeld, R., S. F. Chen and X. Zhu. 2001. Whole 
sentence exponential language models: a vehicle for 
linguistic statistical integration. In: Computer Speech 
and Language, 15 (1). 
Sproat, Richard and Chilin Shih. 2002. Corpus-based 
methods in Chinese morphology and phonology. In: 
COLING 2002.  
Sproat, Richard and Tom Emerson. 2003. The first 
international Chinese word segmentation bakeoff. In: 
SIGHAN 2003. 
Uchimoto, K., S. Sekine and H. Isahara. 2001. The 
unknown word problem: a morphological analysis of 
Japanese using maximum entropy aided by a diction-
ary. In: EMNLP2001. 
Wu, Andi and Zixin Jiang. 2000. Statistically-enhanced 
new word identification in a rule-based Chinese system. 
In: Proc of the 2rd ACL Chinese Processing Workshop. 
Wu, Andi. 2003. Customizable segmentation of mor-
phologically derived words in Chinese. In: Interna-
tional Journal of Computational Linguistics and Chi-
nese Language Processing, 8(1): 1-27. 
Wu, Zimin and Gwyneth Tseng. 1993. Chinese text 
segmentation for text retrieval achievements and prob-
lems. In: JASIS, 44(9): 532-542. 
Chinese Word Segmentation and Named
Entity Recognition: A Pragmatic Approach
Jianfeng Gao?
Microsoft Research Asia
Mu Li?
Microsoft Research Asia
Andi Wu+
GrapeCity Inc.
Chang-Ning Huang?
Microsoft Research Asia
This article presents a pragmatic approach to Chinese word segmentation. It differs from most
previous approaches mainly in three respects. First, while theoretical linguists have defined
Chinese words using various linguistic criteria, Chinese words in this study are defined prag-
matically as segmentation units whose definition depends on how they are used and processed
in realistic computer applications. Second, we propose a pragmatic mathematical framework in
which segmenting known words and detecting unknown words of different types (i.e., morpho-
logically derived words, factoids, named entities, and other unlisted words) can be performed
simultaneously in a unified way. These tasks are usually conducted separately in other systems.
Finally, we do not assume the existence of a universal word segmentation standard that is
application-independent. Instead, we argue for the necessity of multiple segmentation standards
due to the pragmatic fact that different natural language processing applications might require
different granularities of Chinese words.
These pragmatic approaches have been implemented in an adaptive Chinese word segmenter,
called MSRSeg, which will be described in detail. It consists of two components: (1) a generic
segmenter that is based on the framework of linear mixture models and provides a unified
approach to the five fundamental features of word-level Chinese language processing: lexicon
word processing, morphological analysis, factoid detection, named entity recognition, and new
word identification; and (2) a set of output adaptors for adapting the output of (1) to different
application-specific standards. Evaluation on five test sets with different standards shows that
the adaptive system achieves state-of-the-art performance on all the test sets.
1. Introduction
This article is intended to address, with a unified and pragmatic approach, two funda-
mental questions in Chinese natural language processing (NLP): What is a ?word? in
Chinese?, and How does a computer identify Chinese words automatically? Our ap-
proach is distinguished from most previous approaches by the following three unique
? Natural Language Computing Group, Microsoft Research Asia, 5F, Sigma Center, No. 49, Zhichun Road,
Beijing, 100080, China. E-mail: jfgao@microsoft.com, muli@microsoft.com, cnhuang@msrchina.research.
microsoft.com.
+ The work reported in this article was done while the author was at Microsoft Research. His current e-mail
address is andi.wu@grapecity.com.
Submission received: 22 November 2004; revised submission received: 20 April 2005; accepted for
publication: 17 June 2005.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 4
components that are integrated into a single model: a taxonomy of Chinese words, a
unified approach to word breaking and unknown word detection, and a customizable
display of word segmentation.1 We will describe each of these in turn.
Chinese word segmentation is challenging because it is often difficult to define what
constitutes a word in Chinese. Theoretical linguists have tried to define Chinese words
using various linguistic criteria (e.g., Packard 2000). While each of those criteria pro-
vides valuable insights into ?word-hood? in Chinese, they do not consistently lead us to
the same conclusions. Fortunately, this may not be a serious issue in computational lin-
guistics, where the definition of words can vary and can depend to a large degree upon
how one uses and processes these words in computer applications (Sproat and Shih
2002).
In this article, we define the concept of Chinese words from the viewpoint of
computational linguistics. We develop a taxonomy in which Chinese words can be
categorized into one of the following five types: lexicon words, morphologically derived
words, factoids, named entities, and new words.2 These five types of words have
different computational properties and are processed in different ways in our system,
as will be described in detail in Section 3. Two of these five types, factoids and named
entities, are not important to theoretical linguists but are significant in NLP.
Chinese word segmentation involves mainly two research issues: word boundary
disambiguation and unknown word identification. In most of the current systems, these
are considered to be two separate tasks and are dealt with using different components
in a cascaded or consecutive manner.
However, we believe that these two issues are not separate in nature and are
better approached simultaneously. In this article, we present a unified approach to
the five fundamental features of word-level Chinese NLP (corresponding to the five
types of words described earlier): (1) word breaking, (2) morphological analysis, (3)
factoid detection, (4) named entity recognition (NER), and (5) new word identifica-
tion (NWI). This approach is based on a mathematical framework of linear mixture
models in which component models are inspired by the source?channel models of
Chinese sentence generation. There are basically two types of component models: a
source model and a set of channel models. The source model is used to estimate the
generative probability of a word sequence in which each word belongs to one word
type. For each of the word types, a channel model is used to estimate the likelihood
of a character string, given the word type. We shall show that this framework is
flexible enough to incorporate a wide variety of linguistic knowledge and statistical
models in a unified way.
In computer applications, we are more concerned with segmentation units than
words. While words are supposed to be unambiguous and static linguistic entities,
segmentation units are expected to vary from application to application. In fact, dif-
ferent Chinese NLP-enabled applications may have different requirements that request
different granularities of word segmentation. For example, automatic speech recog-
nition (ASR) systems prefer longer ?words? to achieve higher accuracy, whereas in-
1 In this article, we differentiate the terms word breaking and word segmentation. Word breaking refers
to the process of segmenting known words that are predefined in a lexicon. Word segmentation refers
to the process of both lexicon word segmentation and unknown word detection.
2 New words in this article refer to out-of-vocabulary words that are neither recognized as named entities
or factoids nor derived by morphological rules. These words are mostly domain-specific and/or
time-sensitive (see Section 5.5 for details).
532
Gao et al Chinese Word Segmentation: A Pragmatic Approach
formation retrieval (IR) systems prefer shorter ?words? to obtain higher recall rates
(Wu 2003).
Therefore, we do not assume that an application-independent universal word seg-
mentation standard exists. We argue instead for the existence of multiple segmenta-
tion standards, each for a specific application. It is undesirable to develop a set of
application-specific segmenters. A better solution would be to develop a generic seg-
menter with customizable output that is able to provide alternative segmentation units
according to the specification that is either predefined or implied in the application data.
To achieve this, we present a transformation-based learning (TBL; Brill 1995) method,
to be described in Section 6.
We implement the pragmatic approach to Chinese word segmentation in an adapt-
ive Chinese word segmenter called MSRSeg. It consists of two components: (1) a
generic segmenter that is based on the linear mixture model framework of word
breaking and unknown word detection and that can adapt to domain-specific vocab-
ularies, and (2) a set of output adaptors for adapting the output of (1) to different
application-specific standards. Evaluation on five test sets with different standards
shows that the adaptive system achieves state-of-the-art performance on all the test
sets. It thus demonstrates the possibility of a single adaptive Chinese word segmenter
that is capable of supporting multiple applications.
The remainder of this article is organized as follows. Section 2 presents previ-
ous work in this field. Section 3 introduces the taxonomy of Chinese words and de-
scribes the corpora we used in our study. Section 4 presents some of the theoretical
background on which our unified approach is based. Section 5 outlines the general
architecture of the Chinese word segmenter, MSRSeg, and describes each of the com-
ponents in detail, presenting a separate evaluation of each component where appro-
priate. Section 6 presents the TBL method of standards adaptation. While in Section 5
we presume the existence of an annotated training corpus, we focus in Section 7 on
the methods of creating training data in a (semi-)automatic manner, with minimal or
no human annotation. We thus demonstrate the possibilities of unsupervised learning
of Chinese words. Section 8 presents several evaluations of the system on the different
corpora, each corresponding to a different segmentation standard, in comparison with
other state-of-the-art systems. Finally, we conclude the article in Section 9.
2. Previous Work
2.1 Approaches to Word Segmentation
Many methods of Chinese word segmentation have been proposed: reviews include Wu
and Tseng (1993); Sproat and Shih (2002); and Sun and Tsou (2001). These methods can
be roughly classified as either dictionary-based or statistically-based methods, while
many state-of-the-art systems use hybrid approaches.
In dictionary-based methods, given an input character string, only words that are
stored in the dictionary can be identified. One of the most popular methods is maxi-
mum matching (MM), usually augmented with heuristics to deal with ambiguities in
segmentation. Studies that use this method or minor variants include Chen et al (1999)
and Nie, Jin, and Hannan (1994). The performance of these methods thus depends to
a large degree upon the coverage of the dictionary, which unfortunately may never be
complete because new words appear constantly. Therefore, in addition to the dictio-
nary, many systems also contain special components for unknown word identification.
533
Computational Linguistics Volume 31, Number 4
In particular, statistical methods have been widely applied because they use a proba-
bilistic or cost-based scoring mechanism rather than a dictionary to segment the text.
These methods have three drawbacks. First, some of these methods (e.g., Lin et al 1993;
Chang and Su 1997) identify OOV (out-of-vocabulary) words without identifying their
types. For instance, one might identify a string as a unit but fail to identify that it is
a person name. Second, many current statistical methods do not incorporate linguistic
knowledge effectively into segmentation. For example, Teahan et al (2000) and Dai et al
(1999) do not use any linguistic knowledge. Thus, the identified OOV words are likely to
be linguistically implausible, and consequently, additional manual checking is needed
for some subsequent tasks such as parsing. Third, in many current segmenters, OOV
identification is considered a separate process from segmentation (e.g., Chen 2003; Wu
and Jiang 2000; Chen and Bai 1998). For instance, Chen (2003) assumes that OOV words
are usually two or more characters long and are often segmented into single characters.
He then uses different components to detect OOV words of different types in a cascaded
manner after the basic word segmentation.
We believe that the identification of OOV words should not be treated as a problem
separate from word segmentation. We propose a unified approach that solves both
problems simultaneously. A previous work along this line is Sproat et al (1996), which is
based on weighted finite-state transducers (FSTs). Our approach is similarly motivated
but is based on a different mechanism: linear mixture models. As we shall see, the
models provide a more flexible framework for incorporating various kinds of lexical
and statistical information. Many types of OOV words that are not covered in Sproat?s
system can be dealt with in our system. The linear models we used are originally
derived from linear discriminant functions widely used for pattern classification (Duda,
Hart, and Stork 2001) and have been recently introduced into NLP tasks by Collins and
Duffy (2001). Other frameworks of Chinese word segmentation, which are similar to the
linear models, include maximum entropy models (Xue 2003) and conditional random
fields (Peng, Feng, and McCallum 2004). They also use a unified approach to word
breaking and OOV identification.
2.2 More on New Word Identification
In this article, we use the term ?new words? to refer to OOV words other than named en-
tities, factoids, and morphologically derived words. ?New words? are mostly domain-
specific terms (e.g., ? ?cellular?) and time-sensitive political, social, or cultural
terms (e.g., 	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 898?904,
Sydney, July 2006. c?2006 Association for Computational Linguistics
From Prosodic Trees to Syntactic Trees 
 
Andi Wu 
GrapeCity Inc. 
andi.wu@grapecity.com 
 
Kirk Lowery 
Westminster Hebrew Institute 
 klowery@whi.wts.edu 
 
Abstract  
This paper describes an ongoing effort to 
parse the Hebrew Bible. The parser consults 
the bracketing information extracted from the 
cantillation marks of the Masoetic text. We 
first constructed a cantillation treebank 
which encodes the prosodic structures of the 
text.  It was found that many of the prosodic 
boundaries in the cantillation trees 
correspond, directly or indirectly, to the 
phrase boundaries of the syntactic trees we 
are trying to build.   All the useful boundary 
information was then extracted to help the 
parser make syntactic decisions, either 
serving as hard constraints in rule application 
or used probabilistically in tree ranking.   
This has greatly improved the accuracy and 
efficiency of the parser and reduced the 
amount of manual work in building a Hebrew 
treebank.  
Introduction 
The text of the Hebrew Bible (HB) has been 
carefully studied throughout the centuries, with  
detailed lexical, phonological and morphological 
analysis available for every verse of HB.  
However, very few attempts have been made at a 
verse-by-verse syntactic analysis. The only 
known effort in this direction is the Hebrew 
parser built by George Yaeger (Yaeger 1998, 
2002), but the analysis is still incomplete in the 
sense that not all syntactic units are recognized 
and the accuracy of the trees are yet to be 
checked.   
 
Since a detailed syntactic analysis of HB is of 
interest to both linguistic and biblical studies,   
we launched a project to build a treebank of the 
Hebrew Bible.  In this project, the trees are 
automatically generated by a parser and then 
manually checked in a tree editor.  Once a tree 
has been edited or approved, its phrase 
boundaries are recorded in a database.  When the 
same verse is parsed again, the existing brackets 
will force the parser to produce trees whose 
brackets are exactly the same as those of the 
manually approved trees.  Compared with 
traditional approaches to treebanking where the 
correct structure is preserved in a set of tree files, 
our approach has much more agility.  In the event 
of design/format changes, we can automatically 
regenerate the trees according to the new 
specifications without manually touching the 
trees.  The bracketing information will persist 
through the updates and the basic structure of the 
trees will remain correct regardless of the 
changes in the details of trees.  We call this a 
?dynamic treebank? where, instead of 
maintaining a set of trees, we maintain a 
parser/grammar, a dictionary, a set of sentences, 
and a database of bracketing information.  The 
trees can be generated at any time. 
 
Since our parser/grammar can consult known 
phrase boundaries to build trees, its performance 
can be greatly improved if large amounts of 
bracketing information are available.    Human 
inspection and correction can provide those 
boundaries, but the amount of manual work can 
be reduced significantly if there is an existing 
source of bracketing information for us to use. 
Fortunately, a great deal of such information can 
be obtained from the cantillation marks of the 
Hebrew text.    
1 The cantillation treebank 
1.1 Cantillation marks 
The text of HB has been systematically annotated 
for more than a thousand years.  By the end of the 
9th century, a group of Jewish scholars known as 
the Masoretes had developed a system for 
898
marking the structures of the Bible verses.  The 
system contains a set of cantillation marks 1 
which indicate the division and subdivision of 
each verse, very much like the punctuation marks 
or the brackets we use to mark constituent 
structures.  At that time, those cantillation marks 
were intended to record the correct way of 
reading or chanting the Hebrew text: how to 
group words into phrases and where to put pauses 
between intonational units.  In the eyes of modern 
linguists, the hierarchical structures thus marked 
can be best understood as a prosodic 
representation of the verses (Dresher 1994).   
 
There are two types of cantillation marks: the 
conjunctive marks which group multiple words 
into single units and the disjunctive marks which 
divide and subdivide a verse in a binary fashion.  
The marking of Genesis 1:1, for example, is 
equivalent to the bracketing shown below.  
(English words are used here in place of Hebrew 
to make it easier for non-Hebrew-speakers to 
understand.  OM stands for object marker.)  
 
( ( ( In beginning )  
  ( created God )  
  )   
  ( ( OM  
      ( the heavens ) 
    )   
    (  ( and OM )   
       ( the earth )  
    ) 
  ) 
) 
 
This analysis resembles the prosodic structure in 
Selkirk (1984) and the performance structure in 
Gee and Grosjean (1983).  
1.2. Parsing the prosodic structure 
The cantillation system in the Mesoretic text is a 
very complex one with dozens of diacritic 
symbols and complicated annotation rules.  As a 
result, only a few trained scholars can decipher 
them and their practical use has been very limited. 
In order to make the information encoded by this 
system more accessible to both humans and 
                                                     
1 The cantillation marks show how a text is to be sung.  
See http://en.wikipedia.org/wiki/Cantillation.   
machines, we built a treebank where the prosodic 
structures of HB verses are explicitly represented 
as trees in XML format (Wu & Lowery, 2006).  
 
There have been quite a few studies of the 
Masoretic cantillation system.  After reviewing 
the existing analyses, such as Wickes (1881), 
Price (1990), Richter (2004) and Jacobson (2002), 
we adopted the binary analysis of British and 
Foreign Bible Society (BFBS 2002) which is 
based on the principle of dichotomy of Wicks 
(1881).  The binary trees thus generated are best 
for extracting brackets that are syntactically 
significant.    
 
We found all the binary rules that underlie the 
annotation and coded them in a context-free 
grammar.  This CFG was then used by the parser 
to automatically generate the prosodic trees.  The 
input text to the parser was the MORPH database 
developed by Groves & Lowery (2006) where the 
the cantillation marks are represented as numbers 
in its Michigan Code text.   
 
The following is the prosodic tree generated for 
Genesis 1:1, displayed in English glosses in the 
tree editor (going right-to-left according to the 
writing convention of Hebrew):  
 
 
 
Figure 1 
 
The node labels ?athnach?, ?tiphcha?, ?mereka? 
and ?munach? in this tree are names of the 
cantillation marks that indicate the types of 
boundaries between the two chunks they 
dominate.  Different types of boundaries have 
different (relative) boundary strengths. The ?m? 
nodes are morphemes and the ?w? nodes are 
words.  
899
1.3. A complete prosodic treebank 
Since the Mesoretic annotation is supposed to 
mark the structure of every verse unambiguously, 
we expect to parse every verse successfully with 
exactly one tree assigned to it, given that (1) the 
annotation is perfectly correct and (2) the CFG 
grammars correctly encoded the annotation rules.  
The actual results were close to our expectation: 
all the 23213 verses were successfully parsed, of 
which 23099 received exactly one complete tree.  
The success rate is 99.5 percent.  The 174 verses 
that received multiple parse trees all have words 
that carry more than one cantillation mark. This 
can of course create boundary ambiguities and 
result in multiple parse trees.  We have good 
reasons to believe that the grammars we used are 
correct.  We would have failed to parse some 
verses if the grammars had been incomplete and 
we would have gotten multiple trees for a much 
greater number of verses if the grammars had 
been ambiguous. 
2 Phrase boundary extraction 
Now that a cantillation treebank is available, we 
can get brackets from those trees and use them in 
syntactic parsing.  Although prosodic structures 
are not syntactic structures, they do correspond to 
each other in some systematic ways.  Just as there 
are ways to transform syntactic structures to 
prosodic structures (e.g. Abney 1992), prosodic 
structures can also provide clues to syntactic 
structures.  As we have discovered, some of the 
brackets in the cantillation trees can be directly 
mapped to syntactic boundaries, some can be 
mapped after some adjustment, and some have no 
syntactic significance at all. 
 
2.1 Direct correspondences 
Direct correspondences are most likely to be 
found at the clausal level.  Almost all the clause 
boundaries can be found in the cantillation trees. 
Take Genesis 1:3 as an example: 
 
 
 
Figure 2 
 
Here, the verse is first divided into two clauses: 
?God said let there be light? and ?there was light?.  
The first clause is further divided into ?God said? 
and ?let there be light?.  Such bracketing will 
prevent the wrong analysis where ?let there be 
light? and ?there was light? are conjoined to 
serve as the object of ?God said?.  Given the fact 
that there are no punctuation marks in HB, it is 
very difficult for the parser to rule out the wrong 
parse without the help of the cantillatioin 
information. 
 
Coordination is another area where the 
cantillation brackets are of great help.  The 
syntactic ambiguity associated with coordination 
is well-known, but the ambiguity can often be 
resolved with help of prosodic cues.  This is 
indeed what we find in the cantillation treebank.  
In Genesis 24:35, for example, we find the 
following sequence of words: ?male servants and 
maid servants and camels and donkeys?.  
Common sense tells us that there are only two 
possible analyses for this sequence: (1) a flat 
structure where the four NPs are sisters, or (2) 
?male servant? conjoins with ?maid servant, 
?camels? conjoins with ?donkeys?, and then the 
two conjoined NPs are further conjoined as 
sisters.  However, the computer is faced with 14 
different choices.  Fortunately, the cantillation 
tree can help us pick the correct structure: 
 
900
 
 
Figure 3 
 
The brackets extracted from this tree will force 
the parser to produce only the second analysis 
above.  
 
Good correspondences are also found for most 
base NPs and PPs.  Here is an example from 
Genesis 1:4, which means ?God separated the 
light from the darkness?: 
 
 
 
Figure 4 
 
As we can see, the noun phrases and 
prepositional phrases all have corresponding 
brackets in this tree. 
 2.2 Indirect correspondences 
Now we turn to prosodic structures that can be 
adjusted to correspond to syntactic structures.  
They usually involve the use of function words 
such as conjunctions, prepositions and 
determiners.  Syntactically, these words are 
supposed to be attached to complete NPs, often 
resulting in trees where those single words are 
sisters to large NP chunks.  Such ?unbalanced? 
trees are rarely found in prosodic structures, 
however, where a sentence tends to be divided 
into chunks of similar length for better rhythm 
and flow of speech.   
 
This is certainly the case in the HB cantillation 
treebank.  It must have already been noticed in 
the example trees we have seen so far that the 
conjunction ?and? is always attached to the word 
that immediately follows it.  As a matter of fact, 
the conjunction and the following word are often 
treated as a single word for phonological reasons. 
 
Prepositions are also traditionally treated as part 
of the following word.  It is therefore not a 
surprise to find trees of the following kind: 
 
 
 
Figure 5 
 
In this tree, the preposition ?over? is attached to 
?surface of? instead of ?surface of the waters?.  
We also see the conjunction ?and? is attached to 
?spirit of? rather than to the whole clause.   
 
A similar situation is found for determiners, as 
can be seen in this sub-tree where ?every of? is 
attached to ?crawler of? instead of ?crawler of the 
ground?.   
 
 
 
Figure 6 
 
901
In all these cases, the differences between 
prosodic structures and syntactic structures are 
systematic and predictable.   All of them can be 
adjusted to correspond better to syntactic 
structures by raising the function words out of 
their current positions and re-attach them to some 
higher nodes.   
 
2.3 Extracting the boundaries 
In the bracket extraction phase, we go through all 
the sub-trees and get their beginning and ending 
positions in the form of (begin, end).  Given the 
tree in Figure 6, for example, we can extract the 
following brackets: (n, n+3), (n, n+1), (n+2, 
n+3), where n is the position of the first word in 
the sub-tree. 
 
For cases of indirect correspondence discussed in 
2.2, we automatically adjust the brackets by 
removing the ones around the function word and 
its following word and adding a pair of brackets 
that start from the word following the function 
word and end in the last word of the phrase.  After 
this adjustment, the brackets extracted from 
Figure 6 will become (n, n+3), (n+1, n+3) and  
(n+2, n+3).  This in effect transforms this tree to 
the one in Figure 7 which corresponds better to its 
syntactic structure: 
 
 
 
                      Figure 7 
 
For trees that start with ?and?, we detach ?and? 
and re-attach it to the highest node that covers the 
phrase starting with ?and?. After this and other 
adjustments, the brackets we extract from Figure 
5 will be: 
 
 (n, n+7) 
 (n+1, n+7) 
 (n+1, n+2) 
 (n+3, n+7) 
 (n+4, n+7) 
 (n+5, n+7) 
 (n+6, n+7) 
  
These brackets transform the tree into the one in 
Figure 8: 
 
 
 
Figure 8 
 
The cantillation trees also contain brackets that 
are not related to syntactic structures at all.  Since 
it is difficult to identify those useless brackets 
automatically, we just leave them alone and let 
them be extracted anyway.  Fortunately, as we 
will see in the next section, the parser does not 
depend completely on the extracted bracketing 
information.  The useless brackets can simply be 
ignored in the parsing process. 
3 Building a syntactic treebank 
As we mentioned earlier, we use a parser to 
generate the treebank.  This parser uses an 
augmented context-free grammar that encodes 
the grammatical knowledge of Biblical Hebrew.  
Each rule in this grammar has a number of 
grammatical conditions which must be satisfied 
in order for the rule to apply.   In addition, it may 
have a bracketing condition which can either 
block the application of a rule or force a rule to 
apply.   
 
Besides serving as conditions in rule application, 
902
the bracketing information is also used to rank 
trees in cases where more than one tree is 
generated.   
3.1 Brackets as rule conditions 
Bracketing information is used in some grammar 
rules to guide the parser in making syntactic 
decisions.  In those rules, we have conditions that 
look at the beginning position and ending 
position of the sub-tree to be produced by the rule 
and check to see if those bracket positions are 
found in our phrase boundary database.  The 
sub-tree will be built only if the bracketing 
conditions are satisfied. 
 
There are two types of bracketing conditions.  
One type serves as the necessary and sufficient 
condition for rule application. These conditions 
work in disjunction with grammatical conditions.  
A rule will apply when either the grammatical 
conditions or the bracketing conditions are 
satisfied.  This is where the bracketing condition 
can force a rule to apply regardless of the 
grammatical conditions.  The brackets consulted 
by this kind of conditions must be the manually 
approved ones or the automatically extracted 
ones that are highly reliable.  Such conditions 
make it possible for us to override grammatical 
conditions that are too strict and build the 
structures that are known to be correct.  
 
The other type of bracketing conditions serves as 
the necessary conditions only.  They work in 
conjunction with grammatical conditions to 
determine the applicability of a rule.  The main 
function of those bracketing conditions is to 
block structures that the grammatical conditions 
fail to block because of lack of information.  
However, they cannot force a rule to apply.  The 
sub-tree to be produced the rule will be built only 
if both the grammatical conditions and the 
bracketing conditions are met. 
 
 The overall design of the rules and conditions are 
meant to build a linguistically motivated Hebrew 
grammar that is independent of the cantillation 
treebank while making use of its prosodic 
information. 
 
3.2 Brackets for tree ranking 
 
The use of bracketing conditions greatly reduces 
the number of trees the parser generates.  In fact, 
many verses yield a single parse only.  However, 
there are still cases where multiple trees are 
generated.  In those cases, we use the bracketing 
information to help rank the trees. 
 
During tree ranking, the brackets of each tree are 
compared with the brackets in the cantillation 
trees to find the number of mismatches.  Trees 
that have fewer mismatches are ranked higher 
than trees that have more mismatches.  In most 
cases, the top-ranking tree is the correct parse. 
 
Theoretically, it should be possible to remove all 
the bracketing conditions from the rules, let the 
parser produce all possible trees, and use the 
bracketing information solely at the tree-ranking 
stage to select the correct trees.  We can even use 
machine learning techniques to build a statistical 
parser.  However, a Treebank of the Bible 
requires 100% accuracy but none of the statistical 
models are capable of that standard yet.  As long 
as 100% accuracy is not guaranteed, manual 
checking will be required to fix all the individual 
errors.  Such case-by-case fixes are easy to do in 
our current approach but are very difficult in 
statistical models. 
 
3.3  Evaluation 
Since only a very small fraction of the trees 
generated by our parser have been manually 
verified, there is not yet a complete golden 
standard to objectively evaluate the accuracy of 
the parser.  However, some observations are 
obvious: 
 
(1) The parsing process can become intractable 
without the bracketing conditions.  We tried 
parsing with those conditions removed from 
the rules to see how many more trees we will 
get.  It turned out that parsing became so slow 
that we had to terminate it before it was 
finished.  This shows that the bracketing 
conditions are playing an indispensable role 
in making syntactic decisions. 
 
903
(2) The number of edits needed to correct the 
trees in manual checking is minimal.  Most 
trees generated by the machine are basically 
correct and only a few touches are necessary 
to make them perfect. 
(3) The boundary information extracted from the 
cantillation tree could take a long time to 
create if done by hand, and a great deal of 
manual work is saved by using the brackets 
from the cantillation treebank. 
Conclusion 
In this paper, we have demonstrated the use of 
prosodic information in syntactic parsing in a 
treebanking project.  There are correlations 
between prosodic structures and syntactic 
structures.  By using a parser that consults the 
prosodic phrase boundaries, the cost of building 
the treebank can be minimized.   
References  
Abney, S (1992) Prosodic Structure, Performance 
Structure and Phrase Structure.   In Proceedings, 
Speech and Natural. Language Workshop, pp. 
425-428. 
BFBS (2002)  The Masoretes and the Punctuation of 
Biblical Hebrew. British & Foreign Bible Society, 
Machine Assisted Translation Team.  
Dresher, B.E. (1994) The Prosodic Basis of the 
Tiberian Hebrew System of Accents.  In Language, 
Vol. 70,  No 1, pp. 1-52. 
Gee J. P. & F. Grosjean  (2002)  The Masoretes and 
the Punctuation of Biblical Hebrew. British & 
Foreign Bible Society, Machine Assisted 
Translation Team.  
Groves, A & K. Lowery, eds. (2006). The Westminster 
Hebrew Bible Morphology Database. Philadelphia: 
Westminster Hebrew Institute. 
Jacobson, J.R. (2002)  Chanting the Hebrew Bible.  
The Jewish Publication Society, Philadelphia..  
Price, J. (1990)  The Syntax of Masoretic Accents in 
the Hebrew Bible.  The Edwin Mellen Press, 
Lewiston/Queenston/Lampeter. 
Richter, H (2004)  Hebrew Cantillation Marks and 
Their Encoding.  Published at   
http://www.lrz-muenchen.de/~hr/teamim/ 
Selkirk, E. (1984) Phonology and Syntax: The 
Relation between Sound and Structure. Cambridge, 
MA: MIT Press. 
Wickes, W. (1881) Two Treatises on the Accentuation 
of the Old Testament.   Reprint by KTAV, New 
York, 1970.. 
Wu, A & K. Lowery (2006) A Hebrew Tree Bank 
Based on Cantillation Marks.  In Proceedings of 
LREC 2006. 
Yaeger, G (1998) Layered Parsing: a Principled 
Bottom-up Parsing Formalism for Classical Biblical 
Hebrew, a working paper, ASTER Institute, Point 
Pleasant, NJ. 
Yaeger, G (2002) A Layered Parser Implementation of 
a Schema of Clause Types in Classical Biblical 
Hebrew, SBL Conference, Toronto, Ontario, 
Canada. 
904
Treebank of Chinese Bible Translations 
Andi Wu 
GrapeCity Inc. 
andi.wu@grapecity.com 
 
Abstract 
This paper reports on a treebanking 
project where eight different modern 
Chinese translations of the Bible are 
syntactically analyzed.  The trees are 
created through dynamic treebanking 
which uses a parser to produce the 
trees.   The trees have been going 
through manual checking, but correc-
tions are made not by editing the tree 
files but by re-generating the trees with 
an updated grammar and dictionary.  
The accuracy of the treebank is high 
due to the fact that the grammar and 
dictionary are optimized for this specif-
ic domain.  The tree structures essen-
tially follow the guidelines of the Penn 
Chinese Treebank.  The total number 
of characters covered by the treebank is 
7,872,420 characters. The data has 
been used in Bible translation and Bi-
ble search.  It should also prove useful 
in the computational study of the Chi-
nese language in general. 
1 Introduction 
Since the publication of the Chinese Union 
Version (CUV???) in 1919, the Bible 
has been re-translated into Chinese again 
and again in the last 91 years.  The transla-
tions were done in different time periods 
and thus reflect the changes in the Chinese 
language in the last century.  They also 
represent different styles of Chinese writ-
ing, ranging over narration, exposition and 
poetry. Due to the diversity of the transla-
tors? backgrounds, some versions follow 
the language standards of mainland China, 
while other have more Taiwan or Hong 
Kong flavor.  But they have one thing in 
common: they were all done very profes-
sionally, with great care put into every sen-
tence.  Therefore the sentences are usually 
well-formed.  All this makes the Chinese 
translations of the Bible a high-quality and 
well-balanced corpus of the Chinese lan-
guage. 
 
To study the linguistic features of this text cor-
pus, we have been analyzing its syntactic 
structures with a Chinese parser in the last few 
years.  The result is a grammar that covers all 
the syntactic structures in this domain and a 
dictionary that contains all the words in this 
text corpus.  A lot of effort has also been put 
into tree-pruning and tree selection so that the 
bad trees can be filtered out. Therefore we are 
able to parse most of the sentences in this cor-
pus correctly and produce a complete treebank 
of all the Chinese translations.   
 
The value of such a treebank in the study and 
search of the Bible is obvious.  But it should 
also be a valuable resource for computational 
linguistic research outside the Bible domain.  
After all, it is a good representation of the syn-
tactic structures of Chinese. 
2 The Data Set 
The text corpus for the treebank includes eight 
different versions of Chinese translations of 
the Bible, both the Old Testament and the New 
Testament.  They are listed below in 
chronological order with their Chinese names, 
abbreviations, and years of publication: 
 
 Chinese Union Version 
 (??? CUV 1919)  
 Lv Zhenzhong Version  
(????? LZZ 1946) 
 Sigao Bible  
(???? SGB 1968 ) 
 Today?s Chinese Version  
(?????? TCV 1979) 
 Recovery Version  
(??? RCV 1987) 
 New Chinese Version  
(??? NCV 1992) 
 Easy-to-Read Version  
(????? ERV 2005) 
 Chinese Standard Bible  
(?????? CSB 2008)   
 
All these versions are in vernacular Chinese 
(???) rather than classical Chinese (??
?), with CUV representing ?early vernacular? 
( ? ? ? ? ? ) and the later versions 
representing contemporary Chinese.  The texts 
are all in simplified Chinese. Those 
translations which were published in 
traditional Chinese were converted to 
simplified Chinese.  For a linguistic 
comparison of those different versions, see Wu  
et al(2009). 
 
In terms of literary genre, more than 50% of 
the Bible is narration, about 15% poetry, 10% 
exposition, and the rest a mixture of narrative, 
prosaic and poetic writing.  The average 
number of characters in a single version is 
close to one million and the total number of 
characters of these eight versions is 7,672,420. 
 
Each book in the Bible consists of a number of 
chapters which in turn consist of a number of 
verses.  A verse often corresponds to a sen-
tence, but it may be composed of more than 
one sentence.  On the other hand, some sen-
tences may span multiple verses.  To avoid the 
controversy in sentence segmentation, we pre-
served the verse structure, with one tree for 
each verse.  The issues involved in this deci-
sion will be discussed later. 
3 Linguistic Issues 
In designing the tree structures, we essentially 
followed the Penn Chinese Treebank (PCTB) 
Guidelines (Xia 2000, Xue & Xia 2000) in 
segmentation, part-of-speech tagging and 
bracketing.  The tag set conforms to this stan-
dard completely while the segmentation and 
bracketing have some exceptions. 
 
In segmentation, we provide word-internal 
structures in cases where there can be varia-
tions in the granularity of segmentation.   For 
example, a verb-complement structure such as 
?? is represented as  
 
 
so that it can be treated either as a single word 
or as two individual words according to the 
user?s needs.  A noun-suffix structure such as 
???? is represented as 
 
to accommodate the need of segmenting it into 
either a single word or two separate words.  
Likewise, a compound word like ??  is 
represented as  
 
to account for the fact that it can also be ana-
lyzed as two words.  This practice is applied to 
all the morphologically derived words dis-
cussed in Wu (2003), which include all lin-
guistic units that function as single words syn-
tactically but lexically contains two or more 
words.  The nodes for such units all have (1) 
an attribute that specifies the word type (e.g. 
Noun-Suffix, Verb-Result, etc.) and (2) the 
sub-units that make up the whole word.  The 
user can use the word type and the layered 
structures to take different cuts of the trees to 
get the segmentation they desire. 
 
In bracketing, we follow the guidelines of 
Penn Chinese treebank, but we simplified the 
sentence structure by omitting the CP and IP 
nodes.  Instead, we use VP for any verbal unit, 
whether it is a complete sentence or not.  Here 
is an example where the tree is basically a pro-
jection of the verb, with all other elements be-
ing the arguments or adjuncts of the VP: 
 
 
There are two reasons for doing this.  First of 
all, we choose not to represent movement rela-
tionships with traces and empty categories 
which are not theory-neutral.  They add com-
plexities to automatic parsing, making it slow-
er and more prone to errors.  Secondly, as we 
mentioned earlier, the linguistic units we parse 
are verses which are not always a sentence 
with an IP and a CP.  Therefore we have to 
remain flexible and be able to handle multiple 
sentences, partial sentences, or any fragments 
of a sentence.  The use of VP as the maximal 
project enables us to be consistent across dif-
ferent chunks. Here is a verse with two sen-
tences: 
 
 
Notice that both sentences are analyzed as VPs 
and the punctuation marks are left out on their 
own.  Here is a verse with a partial sentence: 
 
This verse contains a PP and a VP.  Since it is 
not always possible to get a complete sentence 
within a verse, we aim at ?maximal parses? 
instead of complete parses, doing as much as 
we can be sure of and leaving the rest for fu-
ture processing.  To avoid the clause-level am-
biguities as to how a clause is related to anoth-
er, we also choose to leave the clausal relations 
unspecified.  Therefore, we can say that the 
biggest linguistic units in our trees are clauses 
rather than sentences.  In cases where verses 
consist of noun phrases or prepositional phras-
es, the top units we get can be NPs or PPs.  In 
short, the structures are very flexible and par-
tial analysis is accepted where complete analy-
sis is not available. 
 
While the syntactic structure in this treebank is 
underspecified compared to the Penn Chinese 
Treebank, the lexical information contained in 
the trees are considerably richer.  The trees are 
coded in XML where each node is a complex 
attribute-value matrix.  The trees we have seen 
above are visualizations of the XML in a tree 
viewer where we can also view the attributes 
of each node in a tooltip, as shown below: 
 
Here, the attributes tell us among other things 
that (1) this node is formed by the rule ?DNP-
NP?, (2) the head of this phrase is its second 
child (position is 0-based), (3) there is no 
coordination in this phrase, (4) this is not a 
location phrase, (5) this is not a time phrase, (6) 
the NP is a human being, and (7) the head 
noun can take any of those measure words (?
?): ??????????? and ?. There 
are many other attributes and a filter is applied 
to determine which attributes will show up 
when the XML is generated. 
4 Computational Issues 
As we have mentioned above, the trees are 
generated automatically by a Chinese parser.  
It is well-known that the state-of?the-art natu-
ral language parsers are not yet able to produce 
syntactic analysis that is 100% correct.  As a 
result, the automatically generated trees con-
tain errors and manual checking is necessary.  
The question is what we should do when errors 
are found. 
 
The approach adopted by most treebanking 
projects is manual correction which involves 
editing the tree files.  Once the trees have been 
modified by hand, the treebank becomes static.  
Any improvement or update on the treebank 
will require manual work from then on and 
automatic parsing is out of the picture.  This 
has several disadvantages.  First of all, it is 
very labor-intensive and not everyone can af-
ford to do it.  Secondly, the corrections are 
usually token-based rather than type-based, 
which requires repetitions of the same correc-
tion and opens doors to inconsistency.  Finally, 
this approach is not feasible with trees with 
complex feature structures where manual edit-
ing is difficult if not impossible. 
 
To avoid these problems, we adopted the ap-
proach of dynamic treebanking (Oepen et al
2002) where corrections/updates are not made 
in the tree files but in the grammar and dictio-
nary that is used to generate the trees.  Instead 
of fixing the trees themselves, we improve the 
tree-generator and make it produce the correct 
trees.  Every error found the trees can be traced 
back to some problem in the grammar rules, 
dictionary entries, or the tree selection process.  
Once a ?bug? is resolved, all problems of the 
same kind will be resolved throughout the 
whole treebank.  In this approach, we never 
have to maintain a static set of trees.  We can 
generate the trees at any time with any kind of 
customization based on users? requirement.    
 
Dynamic treebanking requires a high-accuracy 
syntactic parser which is not easy to build.  A 
Chinese parser has the additional challenge of 
word segmentation and name entity recogni-
tion.  These problems become more managea-
ble once the texts to be parsed are narrowed 
down to a specific domain, in our case the do-
main of Biblical texts.   
 
The dictionary used by our parser is based on 
the Grammatical Knowledge Base of Contem-
porary Chinese (GKBCC) licensed from Bei-
jing University.  It is a wide-coverage, feature-
rich dictionary containing more than 80,000 
words. On top of that, we added all the words 
in the eight translations, including all the prop-
er names, which are not in the GKBCC.  The 
total vocabulary is about 110,000 words.  
Since we follow the PCTB guidelines in our 
syntactic analysis, the grammatical categories 
of GKBCC were converted to the PCTB POS 
tags.   
 
With all the words in the dictionary, which 
eliminates the OOV problem, the only problem 
left in word segmentation is the resolution of 
combinational ambiguities and overlapping 
ambiguities. We resolve these ambiguities in 
the parsing process rather than use a separate 
word segmenter, because most wrong segmen-
tations can be ruled out in the course of syntac-
tic analysis (Wu and Jiang 1998).     
 
Our grammar is in the HPSG framework. In 
addition to feature-rich lexical projections, it 
also bases its grammatical decisions on the 
words in the preceding and following contexts.  
Multiple trees are generated and sorted accord-
ing to structural properties.  The treebank con-
tains the best parse of each verse by default, 
but it can also provide the top N trees.  The 
grammar is not intended to be domain-specific.  
Almost all the rules there apply to other do-
mains as well.  But the grammar is ?domain-
complete? in the sense that all the grammatical 
phenomena that occur in this domain are cov-
ered.   
 
The developers of the treebank only look at the 
top tree of each verse.  If it is found to be in-
correct, they can fix it by (1) refining the con-
ditions of the grammar rules, (2) correcting or 
adding attribute values in the lexicon, or (3) 
fine-tuning tree ranking and tree selection.  For 
phrases which occur frequently in the text or 
phrases which are hard to analyze, we store 
their correct analysis in a database so that they 
can be looked up just like a dictionary entry.  
These ?pre-generated? chunks are guaranteed 
to have the correct analysis and they greatly 
reduce the complexity of sentence analysis. 
 
The same grammar and dictionary are used to 
parse the eight different versions.  The devel-
opment work is mainly based on CSB.  There-
fore the trees of the CSB text have higher ac-
curacy than those of other versions.  However,  
due to the fact that all the eight versions are 
translations of the same source text, they share 
a large number of common phrases.  As our 
daily regression tests show, most fixes made in 
CSB also benefit the analysis of other versions. 
5 Evaluation 
Due to the optimization of the grammar and 
dictionary for the Bible domain, the accuracy 
of this Chinese parser is much higher than any 
other general-purpose Chinese parsers when 
the texts to be parsed are Chinese Bible texts. 
Therefore the accuracy of the trees is higher 
than any other automatically generated trees.  
Unfortunately, there is not an existing treebank 
of Chinese Bible translations that can be used 
as a gold standard for automatic evaluation.  
We can only examine the quality through ma-
nual inspection.  However, there does exist a 
segmented text of the CUV translation.1  Using 
this text as the gold standard is ideal because 
the development data for our system is CSB 
rather than CUV or other versions.  
 
As we have mentioned above, the segmenta-
tion from the trees can be customized by tak-
ing different cuts in cases where word-internal 
structures are available.  In order to make our 
segmentation match the existing CUV segmen-
tation as closely as possible, we studied the 
CUV segments and made a decision for each 
type of words.  For example, in a verb-
complement construction where both the verb 
and the directional/resultative complement are 
single characters, the construction will be 
treated as a single word.  
 
We evaluated the segmentation of our CUV 
trees with the scoring script used in the first 
                                                
1 The segmented CUV text was provided by Asia 
Bible Society. 
international Chinese segmentation bakeoff 
(Sproat & Emerson 2003).  Here are the results: 
 
Recall:  99.844% 
Precision: 99.826% 
F-Score: 99.845% 
 
We don't show the OOV numbers as they are 
not relevant here, because all the words have 
been exhaustively listed in our dictionary. 
 
Of a total of 31151 verses in the Bible, 30568 
verses (98.13%) do not contain a single error 
(whole verses segmented correctly).   
Of course, segmentation accuracy does not 
imply parsing accuracy, though wrong seg-
mentation necessarily implies a wrong parse.  
Since we do not have a separate word segmen-
ter and segmentation is an output of the pars-
ing process, the high segmentation accuracy 
does serve as a reflection of the quality of the 
trees.  There would be many more segmenta-
tion errors if the trees had many errors. 
6 Use of the Treebank 
The treebank has been used in the area of Bible 
translation and Bible search.  In Bible transla-
tion, the trees are aligned to the trees of the 
original Hebrew and Greek texts2.  By examin-
ing the correspondences between the Chinese 
trees and the Hebrew/Greek trees, one is able 
to measure how faithful each translation is to 
the original.  In Bible search, the trees makes it 
possible to use more intelligent queries based 
not only on words but on syntactic relations 
between words as well. 
 
An obvious use of the treebank is to train a 
statistical parser.  Though the domain speci-
                                                
2 The Hebrew and Greek trees were also provided 
by Asia Bible Society. 
ficity of the treebank makes it less likely to 
build from it a good lexicalized statistical pars-
er that can be used in the general domain, we 
can still extract a lot of non-lexical syntactic 
information from it.  It can fill many of the 
gaps in the parsers that are built from other 
treebanks which consist mainly of news ar-
ticles.   
 
A special feature of this treebank is that it is 
built from a number of parallel texts -- differ-
ent Chinese translations of the same verses.  
By aligning the parallel trees (ideally through 
the original Hebrew and Greek trees as pivots), 
we can acquire a knowledge base of Chinese 
synonyms and paraphrases.  Presumably, the 
different Chinese subtrees corresponding to the 
same Hebrew/Greek subtree are supposed to 
convey the same meaning.  The words and 
phrases covered by those subtrees therefore 
represent Chinese expressions that are syn-
onymous.  A knowledge base of this kind can 
be a valuable addition to the lexical study of 
Chinese. 
7 Summary 
We presented a Chinese treebank of parallel 
Bible translations.  The treebank is built 
through dynamic treebanking where the trees 
are automatically generated by a Chinese pars-
er optimized for parsing Biblical texts.  The 
trees can serve as a useful resource for differ-
ent language projects. 
 
References 
Sproat, Richard and Thomas Emerson. 2003. The 
First International Chinese Segmentation Ba-
keoff. In Proceedings of the Second SIGHAN 
Workshop on Chinese Language Processing, Ju-
ly 11-12, Sapporo, Japan.  
Wu, Andi, J. and Z. Jiang, 1998. Word segmenta-
tion in sentence analysis, in Proceedings of 1998 
International Conference on Chinese Information 
Processing, pp. 46--51. 169--180, Beijing, China.  
 Wu, Andi. 2003. Customizable Segmentation of 
Morphological Derived Words in Chinese. In In-
ternational Journal of Computational Linguistics 
and Chinese Language Processing, 8(1):1-27. 
Wu, And, Arron Ma, Dong Wang. 2009.  Fidelity 
and Readablity ? a quanatative comparison of 
Chinese translations of the New Testament.  
Proceedings of the Conference on ?Interpretation 
of Biblical Texts in Chinese Contexts?, Sichuan 
Univeristy, December 2009. 
Xia, Fei. 2000. Segmentation Guidelines for the 
Penn Chinese Treebank (3.0). Technical Report, 
University of Pennsylvania.  
Xia, Fei. 2000. The Part-Of-Speech Tagging Guide-
lines for the Penn Chinese Treebank (3.0). 
Technical Report, University of Pennsylvania. 
Xue, Nianwen and Fei Xia. 2000. The Bracketing 
Guidelines for the Penn Chinese Treebank (3.0). 
Technical Report, University of Pennsylvania. 
Oepen, Stephan, Dan Flickinger, Kristina Toutano-
va, Christoper D. Manning. 2002. LinGO Red-
woods: A Rich and Dynamic Treebank for 
HPSG In Proceedings of The First Workshop on 
Treebanks and Linguistic Theories (TLT2002), 
Sozopol, Bulgaria. 
  
