Language Model Based Arabic Word Segmentation 
 
Young-Suk Lee     Kishore Papineni      Salim Roukos 
IBM T. J. Watson Research Center 
Yorktown Heights, NY 10598 
 
Ossama Emam    Hany Hassan 
IBM Cairo Technology Development Center 
P.O.Box 166, El-Ahram, Giza, Egypt
  
Abstract 
 
We approximate Arabic?s rich 
morphology by a model that a word 
consists of a sequence of morphemes in 
the pattern prefix*-stem-suffix* (* 
denotes zero or more occurrences of a 
morpheme). Our method is seeded by a 
small manually segmented Arabic corpus 
and uses it to bootstrap an unsupervised 
algorithm to build the Arabic word 
segmenter from a large unsegmented 
Arabic corpus. The algorithm uses a 
trigram language model to determine the 
most probable morpheme sequence for a 
given input. The language model is 
initially estimated from a small manually 
segmented corpus of about 110,000 
words. To improve the segmentation 
accuracy, we use an unsupervised 
algorithm for automatically acquiring 
new stems from a 155 million word 
unsegmented corpus, and re-estimate the 
model parameters with the expanded 
vocabulary and training corpus. The 
resulting Arabic word segmentation 
system achieves around 97% exact match 
accuracy on a test corpus containing 
28,449 word tokens. We believe this is a 
state-of-the-art performance and the 
algorithm can be used for many highly 
inflected languages provided that one can 
create a small manually segmented 
corpus of the language of interest.  
 
 
 
1   Introduction 
 
Morphologically rich languages like       
Arabic present significant challenges to many 
natural language processing applications 
because a word often conveys complex 
meanings decomposable into several 
morphemes (i.e. prefix, stem, suffix).   By 
segmenting words into morphemes, we can 
improve the performance of natural language 
systems including machine translation (Brown 
et al 1993) and information retrieval (Franz, 
M. and McCarley, S. 2002). In this paper, we 
present a general word segmentation algorithm 
for handling inflectional morphology capable 
of segmenting a word into a prefix*-stem-
suffix* sequence, using a small manually 
segmented corpus and a table of 
prefixes/suffixes of the language. We do not 
address Arabic infix morphology where many 
stems correspond to the same root with various 
infix variations; we treat all the stems of a 
common root as separate atomic units. The use 
of a stem as a morpheme (unit of meaning) is 
better suited than the use of a root for the 
applications we are considering in information 
retrieval and machine translation (e.g. different 
stems of the same root translate into different 
English words.) Examples of Arabic words and 
their segmentation into prefix*-stem-suffix* are 
given in Table 1, where '#' indicates a 
morpheme being a prefix, and '+' a suffix.1 As  
                                                          
1 Arabic is presented in both native and Buckwalter 
transliterated Arabic whenever possible. All native 
Arabic is to be read from right-to-left, and transliterated 
Arabic is to be read from left-to-right. The convention of 
shown in Table 1, a word may include multiple 
prefixes, as in   ???? (l: for, Al: the),  or multiple 
suffixes, as in   ????? (t: feminine singular, h: his).  
A word may also consist only of a stem, as in 
 ?????  (AlY, to/towards). 
  The algorithm implementation involves (i) 
language model training on a morpheme-
segmented corpus, (ii) segmentation of input 
text into a sequence of morphemes using the 
language model parameters, and (iii) 
unsupervised acquisition of new stems from a 
large unsegmented corpus. The only linguistic 
resources required include  a small manually 
segmented corpus ranging from 20,000 words 
to 100,000 words, a table of prefixes and 
suffixes of the language and  a large 
unsegmented corpus.   
  In Section 2, we discuss related work. In 
Section 3, we describe the segmentation 
algorithm.  In Section 4, we discuss the  
unsupervised algorithm for new stem 
acquisition. In Section 5, we present 
experimental results. In Section 6, we 
summarize the paper. 
 
2   Related Work 
 
Our work adopts major components of the 
algorithm from (Luo & Roukos 1996): 
language model (LM) parameter estimation 
from a segmented corpus and input 
segmentation on the basis of LM probabilities.  
However, our work diverges from their work 
in two crucial respects: (i) new technique of 
computing all possible segmentations of a 
word into prefix*-stem-suffix* for decoding, 
and  (ii) unsupervised algorithm for new stem 
acquisition based on a stem candidate's 
similarity to stems occurring in the training 
corpus. 
  (Darwish 2002) presents a  supervised 
technique which identifies the root of an 
Arabic word by stripping away the prefix and 
the suffix of the word on the basis of manually 
acquired dictionary of word-root pairs and the 
likelihood that a prefix and a suffix would 
occur with the template from which the root is 
derived. He reports 92.7% segmentation 
accuracy on a 9,606 word evaluation corpus.  
His technique pre-supposes at most one prefix 
and one suffix per stem regardless of the actual 
number and meanings of prefixes/suffixes 
associated with the stem.  (Beesley 1996)  
presents a finite-state morphological analyzer 
for Arabic, which displays the root, pattern, 
and prefixes/suffixes. The analyses are based 
on manually acquired lexicons and rules.  
Although his analyzer is comprehensive in the 
types of knowledge it presents, it has been 
criticized for their extensive development time 
and lack of robustness, cf. (Darwish 2002). 
                                                                                    
marking a prefix with '#" and a suffix with '+' will be 
adopted throughout the paper. 
  (Yarowsky and Wicentowsky 2000) 
presents a minimally supervised morphological 
analysis with a  performance of over 99.2% 
accuracy for the 3,888 past-tense test cases in 
English. The core algorithm lies in the 
estimation of a probabilistic alignment 
between inflected forms and root forms. The 
probability estimation is based on the lemma 
alignment by frequency ratio similarity among 
different inflectional forms derived from the 
same lemma, given a table of inflectional 
parts-of-speech, a list of the canonical suffixes 
for each part of speech, and a list of the 
candidate noun, verb and adjective roots of the 
language.  Their algorithm does not handle 
multiple affixes per word. 
  (Goldsmith 2000) presents an unsupervised 
technique based on the expectation-
maximization algorithm and minimum 
description length to segment exactly one 
suffix per word, resulting in an F-score of 81.8 
for suffix identification in English according to 
(Schone and Jurafsky 2001). (Schone and 
Jurafsky 2001) proposes an unsupervised 
algorithm capable of automatically inducing 
the morphology of inflectional languages using 
only text corpora. Their algorithm combines 
cues from orthography, semantics, and 
contextual information to induce 
morphological relationships in German, Dutch, 
and English, among others. They report F-
scores between 85 and 93 for suffix analyses 
and between 78 and 85 for circumfix analyses 
in these languages. Although their algorithm 
captures prefix-suffix combinations or 
circumfixes, it does not handle the multiple 
affixes per word we observe in Arabic.
 2
                Words            Prefixes                 Stems             Suffixes 
    Arabic    Translit.   Arabic  Translit.    Arabic    Translit.   Arabic   Translit. 
 ????????????? ?   AlwlAyAt  #??    Al#       ????       wlAy      ?? +    +At 
      ???????????    HyAth           ??????     HyA  ?  +? +    +t +h 
 ?????????????    llHSwl  #?#  ??     l# Al#    ??????     HSwl   
         ?????        AlY           ?????      AlY   
 Table 1  Segmentation of Arabic Words into Prefix*-Stem-Suffix* 
 
3  Morpheme Segmentation 
 
3.1 Trigram Language Model 
 
Given an Arabic sentence, we use a trigram 
language model on morphemes to segment it 
into a sequence of morphemes {m1, m2, ?,mn}. 
The input to the morpheme segmenter is a 
sequence of Arabic tokens ? we use a 
tokenizer that looks only at white space and 
other punctuation, e.g. quotation marks, 
parentheses, period, comma, etc.  A sample of 
a manually segmented corpus is given below2. 
Here multiple occurrences of prefixes and 
suffixes per word are marked with an 
underline. 
 
???? # ??? ??????? ???? ?? ?? ??# ?
??? # ???? ?? # ? ??+??? ?? ???? # ??
? ?????? ??? ?+???? ??? ???? # ?? #
 ??? ???+? +? ???? +???? ?? ???  #
??? #?# ??? # ????? ?# ?????? ?? ?? 
?? ??+???? # ? ??????# ??? ???? ? #
 ? ??? ?? ???? ???? ?????? +????? .
????? ?? ?????? #  ?? ???? ??#?# ?# ?
??????? ??????? ????? ???? # ??
??? # ???? ??? ??# ??????? ?? ??
 ?? ?+?? + ??? ???? ??? #? # ????? 
 ?? ?????????+???? ???? 
 
w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# 
Awl fy jA}z +p Al# nmsA Al# EAm Al# 
mADy Ely syAr +p fyrAry $Er b# AlAm fy 
bTn +h ADTr +t +h Aly Al# AnsHAb mn Al#  
tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al# 
fHwS +At Al# Drwry +p Hsb mA A$Ar fryq  
 
                                                          
2 A manually segmented Arabic corpus containing about 
140K word tokens has been provided by LDC 
(http://www.ldc.upenn.edu). We divided this corpus into 
training and the development test sets as described in 
Section 5. 
 
 
jAgwAr. w# s# y# Hl sA}q Al# tjArb fy 
jAgwAr Al# brAzyly lwsyAnw bwrty mkAn 
AyrfAyn fy Al# sbAq gdA Al# AHd Al*y s# 
y# kwn Awly xTw +At +h fy EAlm sbAq +At 
AlfwrmwlA 
 
Many instances of prefixes and suffixes in 
Arabic are meaning bearing and correspond to 
a word in English such as pronouns and 
prepositions.  Therefore, we choose a 
segmentation into multiple prefixes and 
suffixes. Segmentation into one prefix  and one 
suffix per word, cf. (Darwish 2002), is not very 
useful for applications like statistical machine 
translation, (Brown et al 1993), for which an 
accurate word-to-word alignment between the 
source and the target languages is critical for 
high quality translations. 
  The trigram language model probabilities 
of morpheme sequences, p(mi|mi-1, mi-2), are 
estimated from the morpheme-segmented 
corpus. At token boundaries, the morphemes 
from previous tokens constitute the histories of 
the current morpheme in the trigram language 
model.  The trigram model is smoothed using 
deleted interpolation with the bigram and 
unigram models, (Jelinek 1997), as in (1): 
 
(1) p(m3 | m1 ,m2) =  ?3 p(m3 |m1 ,m2) + ?2 
p(m3 |m2) + ?3 p(m3), where ?1+?2 +?3 = 1. 
 
  A small morpheme-segmented corpus 
results in a relatively high out of vocabulary 
rate for the stems. We describe below an 
unsupervised acquisition of new stems from a 
large unsegmented Arabic corpus.  However, 
we first describe the segmentation algorithm.   
 
3.2  Decoder for Morpheme Segmentation 
 
 3
We take the unit of decoding to be a sentence 
that has been tokenized using white space and 
punctuation.  The task of a decoder is to find 
the morpheme sequence which maximizes the 
trigram probability of the input sentence, as in 
(2): 
 
(2)  SEGMENTATIONbest = Argmax IIi=1, N 
p(mi|mi-1mi-2), N = number of morphemes in 
the input. 
 
Search algorithm for (2) is informally 
described for each word token as follows: 
 
Step 1: Compute all possible segmentations of 
the token  (to be elaborated in 3.2.1). 
Step 2: Compute the trigram language model 
score of each segmentation.  For some 
segmentations of a token, the stem may be an 
out of vocabulary item. In that case, we use an 
?UNKNOWN? class in the trigram language 
model with the model probability given by 
p(UNKNOWN|mi-1, mi-2) * UNK_Fraction, where 
UNK_Fraction is 1e-9 determined on empirical 
grounds. This allows us to segment new words 
with a high accuracy even with a relatively 
high number of unknown stems in the 
language model vocabulary, cf. experimental 
results in Tables 5 & 6. 
Step 3: Keep the top N highest scored 
segmentations. 
 
3.2.1  Possible Segmentations of  a Word 
 
Possible segmentations of a word token are 
restricted to those derivable from a table of 
prefixes and suffixes of the language for 
decoder speed-up and improved accuracy.   
  Table 2 shows examples of atomic (e.g. ??, 
??) and multi-component (e.g.  ??????     ,???????) 
prefixes and suffixes, along with their 
component morphemes in native Arabic.3 
 
                                                          
3 We have acquired the prefix/suffix table from a 110K 
word manually segmented LDC corpus (51 prefixes & 72 
suffixes) and from IBM-Egypt (additional 14 prefixes & 
122 suffixes). The performance improvement by the 
additional prefix/suffix list ranges from 0.07% to 0.54% 
according to the manually segmented training corpus 
size. The smaller the manually segmented corpus size is, 
the bigger the performance improvement by adding 
additional prefix/suffix list is. 
         Prefixes          Suffixes 
      ??          ??       ??# ??+ 
    ??????        ?#  ??# ?????+   ???     ??+ 
 ???????     ?#  ?#   ??# ?????+?? + ??  
          Table 2  Prefix/Suffix Table 
 
Each token is assumed to have the structure 
prefix*-stem-suffix*, and is compared against 
the prefix/suffix table for segmentation. Given 
a word token, (i) identify all of the matching 
prefixes and suffixes from the table, (ii) further 
segment each matching prefix/suffix at each 
character position, and (iii) enumerate all 
prefix*-stem-suffix* sequences derivable from 
(i) and (ii).  
  Table 3 shows all of its possible 
segmentations of the token ???????  
(wAkrrhA; 'and I repeat it'),4 where ? indicates 
the null prefix/suffix and the Seg Score is the 
language model probabilities of each 
segmentation S1 ... S12. For this token, there 
are two matching prefixes #?(w#) and 
#??(wA#) from the prefix table, and two 
matching suffixes ?+(+A) and ??+(+hA)  
from the suffix table. S1, S2, & S3 are the 
segmentations given the null prefix ? and 
suffixes ?, +A, +hA. S4, S5, & S6 are the 
segmentations given the prefix w# and suffixes 
?, +A, +hA. S7, S8, & S9 are the 
segmentations given the prefix wA# and 
suffixes ?, +A, +hA. S10, S11, & S12 are the 
segmentations given the prefix sequence w# 
A# derived from the prefix wA# and  suffixes 
?, +A, +hA. As illustrated by S12, derivation 
of sub-segmentations of the matching 
prefixes/suffixes enables the system to identify 
possible segmentations which would have been 
missed otherwise. In this case, segmentation 
including the derived prefix sequence               
??+??? # ?# ? (w# A# krr +hA) happens to 
be the correct one.  
 
3.2.2. Prefix-Suffix Filter 
 
While the number of possible segmentations is 
maximized by sub-segmenting matching 
                                                          
4 A sentence in which the token occurs is as follows:  ?????
??????? ???????? ???? ?? ????? ????? ????? ?? ???????? ??????? 
(qlthA wAkrrhA fAlm$klp lyst fy AlfnT AlxAm wAnmA fy 
Alm$tqAt AlnfTyp.) 
 4
prefixes and suffixes, some of illegitimate sub-
segmentations are filtered out on the basis of 
the knowledge specific to the manually 
segmented corpus. For instance, sub-
segmentation of the suffix hA into +h +A is 
ruled out because there is no suffix sequence 
+h +A in the training corpus. Likewise, sub-
segmentation of the prefix Al into A# l# is 
filtered out. Filtering out improbable 
prefix/suffix sequences improves the 
segmentation accuracy, as shown in Table 5. 
 
 Prefix Stem Suffix Seg Scores 
S1 ? wAkrrhA ? 2.6071e-05 
S2 ? wAkrrh +A 1.36561e-06 
S3 ? wAkrr +hA 9.45933e-07 
S4 w# AkrrhA ? 2.72648e-06 
S5 w# Akrrh +A 5.64843e-07 
S6 w# Akrr +hA 4.52229e-05 
S7 wA# krrhA ? 7.58256e-10 
S8 wA# krrh +A 5.09988e-11 
S9 wA# krr +hA 1.91774e-08 
S10 w# A# krrhA ? 7.69038e-07 
S11 w# A# krrh +A 1.82663e-07 
S12 w# A# krr +hA 0.000944511 
Table 3 Possible Segmentations of  
??????? (wAkrrhA) 
 
4  Unsupervised Acquisition  of  New  
Stems 
 
Once the seed segmenter is developed on the 
basis of a manually segmented corpus,  the 
performance may be improved by iteratively 
expanding the stem vocabulary  and retraining 
the language model on a large automatically 
segmented Arabic corpus.  
  Given a small manually segmented corpus 
and a large unsegmented corpus, segmenter 
development proceeds as follows. 
 
Initialization: Develop the seed segmenter 
Segmenter0 trained on the manually segmented 
corpus Corpus0, using the language model 
vocabulary, Vocab0, acquired from Corpus0.  
Iteration: For i = 1 to N, N = the number of 
partitions of the unsegmented corpus 
 i. Use Segmenteri-1 to segment Corpusi. 
 ii.  Acquire new stems from the newly 
segmented Corpusi. Add the new stems to 
Vocabi-1, creating an expanded vocabulary 
Vocabi.  
 iii. Develop Segmenteri trained on Corpus0 
through Corpusi with Vocabi.   
Optimal Performance Identification:  
Identify the Corpusi and Vocabi, which result 
in the best performance, i.e. system training 
with Corpusi+1 and Vocabi+1 does not improve 
the performance any more. 
  Unsupervised acquisition of new stems 
from an automatically segmented new corpus 
is a three-step process: (i)  select new stem 
candidates on the basis of a frequency 
threshold, (ii) filter out new stem candidates  
containing a sub-string with a high likelihood 
of being a prefix, suffix, or prefix-suffix. The 
likelihood of a sub-string being a prefix, suffix, 
and prefix-suffix of a token is computed as in  
(5) to (7), (iii) further filter out new stem 
candidates on the basis of contextual 
information, as in (8). 
 
(5)  Pscore = number of tokens with prefix P / 
number of tokens starting with sub-string P 
(6)  Sscore = number of tokens with suffix S / 
number of tokens ending with sub-string S 
(7)  PSscore = number of tokens with prefix P 
and suffix S / number of tokens starting with 
sub-string P and ending with  sub-string S 
 
Stem candidates containing a sub-string with a 
high prefix, suffix, or prefix-suffix likelihood 
are filtered out. Example sub-strings with the 
prefix, suffix, prefix-suffix likelihood 0.85 or 
higher in a 110K word manually segmented 
corpus are given in Table 4. If a token starts 
with the sub-string ???  (sn), and end with  ???  
(hA), the sub-string's likelihood of being the 
prefix-suffix of the token is 1.  If a token starts 
with the sub-string  ????  (ll), the sub-string's 
likelihood of being the prefix of the token is 
0.945, etc. 
 
        Arabic Transliteration      Score 
 ??? +  stem # ???     sn# stem+hA      1.0 
     ?+ stem # ?????  Al# stem+p      0.984        
         stem # ????   ll# stem      0.945 
  ??+  stem         stem+At      0.889 
    Table 4 Prefix/Suffix Likelihood Score 
 
 5
(8) Contextual Filter: (i) Filter out stems co-
occurring with prefixes/suffixes not present in 
the training corpus. (ii) Filter out stems whose 
prefix/suffix distributions are highly 
disproportionate to those seen in the training 
corpus.  
   According to (8), if a stem is followed by 
a potential suffix +m, not present in the 
training corpus, then it is filtered out as an 
illegitimate stem. In addition, if a stem is 
preceded by a prefix and/or followed by a 
suffix with a significantly higher proportion 
than that observed in the training corpus, it is 
filtered out. For instance, the probability for 
the suffix +A to follow a stem is less than 50% 
in the training corpus regardless of the stem 
properties, and therefore, if a candidate stem is 
followed by +A with the probability of over 
70%, e.g. mAnyl +A, then it is filtered out as 
an illegitimate stem. 
 
5  Performance Evaluations 
 
We present experimental results illustrating the 
impact of three factors on segmentation error 
rate: (i) the base algorithm, i.e. language model 
training and decoding, (ii) language model 
vocabulary and training corpus size, and (iii) 
manually segmented training corpus size.  
Segmentation error rate is defined in (9). 
 
(9)  (number of incorrectly segmented tokens /  
       total number of tokens)  x  100 
 
  Evaluations have been performed on a 
development test corpus containing 28,449 
word tokens.  The test set is extracted from 
20001115_AFP_ARB.0060.xml.txt through 
20001115_AFP_ARB.0236.xml.txt of the 
LDC Arabic Treebank: Part 1 v 2.0 Corpus. 
Impact of the core algorithm and the 
unsupervised stem acquisition has been 
measured on segmenters developed from 4 
different sizes of manually segmented seed 
corpora: 10K, 20K, 40K, and 110K words.    
  The experimental results are shown in 
Table 5. The baseline performances are 
obtained by assigning each token the most 
frequently occurring segmentation in the 
manually segmented training corpus. The 
column headed by '3-gram LM' indicates the 
impact of the segmenter using only trigram 
language model probabilities for decoding. 
Regardless of the manually segmented training 
corpus size, use of  trigram language model 
probabilities reduces the word error rate of the 
corresponding baseline by approximately 50%. 
The column headed by '3-gram LM + PS 
Filter' indicates the impact of the core 
algorithm plus Prefix-Suffix Filter discussed in 
Section 3.2.2. Prefix-Suffix Filter reduces the 
word error rate ranging from 7.4% for the 
smallest (10K word) manually segmented 
corpus to 21.8% for the largest (110K word) 
manually segmented corpus ?- around 1% 
absolute reduction for all segmenters. The 
column headed by '3-gram LM + PS Filter + 
New Stems' shows the impact of unsupervised 
stem acquisition from a 155 million word 
Arabic corpus.  Word error rate reduction due 
to the unsupervised stem acquisition is 38% for 
the segmenter developed from the 10K word 
manually segmented corpus and 32% for the 
segmenter developed from 110K word 
manually segmented corpus. 
  Language model vocabulary size (LM VOC 
Size) and the unknown stem ratio (OOV ratio) 
of various segmenters is given in Table 6. For 
unsupervised stem acquisition, we have set the 
frequency threshold at 10 for every 10-15 
million word corpus, i.e. any new morphemes 
occurring more than 10 times in a 10-15 
million word corpus are considered to be new 
stem candidates. Prefix, suffix, prefix-suffix 
likelihood score to further filter out illegitimate 
stem candidates was set at 0.5 for the 
segmenters developed from 10K, 20K, and 
40K manually segmented corpora, whereas it 
was set at 0.85 for the segmenters developed 
from a 110K manually segmented corpus.  
Both the frequency threshold and the optimal 
prefix, suffix, prefix-suffix likelihood scores 
were determined on empirical grounds. 
Contextual Filter stated in (8) has been applied 
only to the segmenter developed from 110K 
manually segmented training corpus.5 
Comparison of Tables 5 and 6 indicates a high 
correlation between the segmentation error rate 
and the unknown stem ratio.  
                                                          
5 Without the Contextual Filter, the  error rate of the 
same segmenter is 3.1%. 
 6
   
 
Manually Segmented 
Training Corpus Size 
      Baseline  3-gram LM  3-gram LM +  
PS Filter 
3-gram LM + PS 
Filter + New Stems 
        10K Words    26.0%        14.7%            13.6%          8.5% 
        20K Words       19.7%        9.1%            8.0%          5.9% 
        40K Words        14.3%        7.6%            6.5%          5.1% 
      110K Words        11.0%        5.5%            4.3%           2.9% 
Table 5 Impact of Core Algorithm and LM Vocabulary Size on Segmentation Error Rate 
 
                       3-gram LM  3-gram LM + PS Filter + New Stems Manually Segmented 
Training Corpus Size     LM VOC Size      OOV Ratio    LM VOC Size      OOV Ratio 
         10K Words           2,496          20.4%          22,964           7.8% 
         20K Words           4,111          11.4%          25,237           5.3% 
         40K Words           5,531            9.0%          21,156           4.7% 
       110K Words           8,196            5.8%          25,306           1.9% 
             Table 6 Language Model Vocabulary Size and Out of Vocabulary Ratio 
  
                                  3-gram LM + PS Filter + New Stems Manually Segmented 
Training Corpus Size   Unknown Stem          Alywm     Other Errors  Total # of Errors 
         10 K Words    1,844  (76.9%)        98 (4.1%)     455 (19.0%)          2,397 
         20 K Words    1,174  (71.1%)        82 (5.0%)     395 (23.9%)          1,651 
         40 K Words    1,005  (69.9%)        81 (5.6%)     351 (24.4%)          1,437 
       110 K Words       333  (39.6%)        82 (9.8%)     426 (50.7%)             841 
Table 7 Segmentation Error Analyses
  
Table 7 gives the error analyses of four 
segmenters according to three factors: (i) 
errors due to unknown stems, (ii) errors 
involving  ?????????? (Alywm), and (iii) errors due to 
other factors. Interestingly, the segmenter 
developed from a 110K manually segmented 
corpus has the lowest percentage of ?unknown 
stem? errors at 39.6% indicating that our 
unsupervised acquisition of new stems is 
working well, as well as suggesting to use a 
larger unsegmented corpus for unsupervised 
stem acquisition.  
    ?????????? (Alywm) should be segmented 
differently depending on its part-of-speech to 
capture the semantic ambiguities. If it is an 
adverb or a proper noun, it is segmented as 
 ?????????? 'today/Al-Youm', whereas if it is a noun, 
it is segmented as ?? # ??????   'the day.'  Proper 
segmentation of   ?????????? primarily requires its 
part-of-speech information, and cannot be 
easily handled by morpheme trigram models 
alone. 
  Other errors include over-segmentation of  
foreign words such as  ???????????????  (bwtyn) as  ?# 
 ??????????  and  ?????????????  (lytr)  'litre' as ? # ?# ????? .  
These errors are attributed to the segmentation 
ambiguities of these tokens:  ??????????????? is 
ambiguous between ' ??????????????? (Putin)' and '?# 
 ?????????? (by aorta)'.   ?????????????  is ambiguous 
between ' ????????????? (litre)' and ' ? # ?# ?????  (for him 
to harm)'. These errors may also be corrected 
by incorporating part-of-speech information 
for disambiguation. 
  To address the segmentation ambiguity 
problem, as illustrated by ' ??????????????? (Putin)' vs. 
' ? # ??????????  (by aorta)', we have developed a 
joint model for segmentation and part-of-
speech tagging for which the best 
segmentation of an input sentence is obtained 
according to the formula (10), where ti is the 
part-of-speech of morpheme mi, and N is the 
number of morphemes in the input sentence. 
 
(10) SEGMENTATIONbest = Argmax ?i=1,N  
p(mi|mi-1 mi-2) p(ti|ti-1 ti-2) p(mi|ti) 
 
By using the joint model, the segmentation 
word error rate of the best performing 
segmenter has been reduced by about 10% 
 7
from 2.9% (cf. the last column of Table 5) to 
2.6%. 
   
5  Summary and Future Work 
 
We have presented a robust word segmentation 
algorithm which segments a word into a 
prefix*-stem-suffix* sequence, along with 
experimental results. Our Arabic word 
segmentation system implementing the 
algorithm achieves around 97% segmentation 
accuracy on a development test corpus 
containing 28,449 word tokens. Since the 
algorithm can identify any number of prefixes 
and suffixes of a given token, it is generally 
applicable to various language families 
including agglutinative languages (Korean, 
Turkish, Finnish), highly inflected languages 
(Russian, Czech) as well as semitic languages 
(Arabic, Hebrew). 
   Our future work includes (i) application 
of the current technique to other highly 
inflected languages, (ii) application of the 
unsupervised stem acquisition technique on 
about 1 billion word unsegmented Arabic 
corpus, and (iii) adoption of a novel 
morphological analysis technique to handle 
irregular morphology, as realized in Arabic 
broken plurals  ????????? (ktAb) 'book' vs.  ???????? 
(ktb) 'books'. 
 
Acknowledgment 
 
This work was partially supported by the 
Defense Advanced Research Projects Agency 
and monitored by SPAWAR under contract No. 
N66001-99-2-8916. The views and findings 
contained in this material are those of the 
authors and do not necessarily reflect the 
position of policy of the Government and no 
official endorsement should be inferred. We 
would like to thank Martin Franz for discussions 
on language model building, and his help with 
the use of ViaVoice language model toolkit. 
 
References 
 
Beesley, K. 1996. Arabic Finite-State 
 Morphological Analysis and Generation. 
 Proceedings of COLING-96, pages 89?  94. 
Brown, P., Della Pietra, S., Della Pietra, V., 
 and Mercer, R. 1993. The mathematics  of 
 statistical machine translation:  Parameter 
 Estimation. Computational  Linguistics, 
 19(2): 263?311. 
Darwish, K. 2002. Building a Shallow  Arabic 
 Morphological Analyzer in  One  Day. 
 Proceedings of the  Workshop on 
 Computational  Approaches to Semitic 
 Languages,  pages 47?54.  
Franz, M. and McCarley, S. 2002. Arabic 
 Information Retrieval at IBM.  Proceedings 
 of TREC 2002, pages 402? 405. 
Goldsmith, J. 2000. Unsupervised  learning 
 of  the morphology of a natural  language.   
 Computational Linguistics, 27(1). 
Jelinek, F. 1997. Statistical Methods for 
 Speech Recognition. The MIT Press. 
Luo, X. and Roukos, S. 1996. An Iterative 
 Algorithm to Build Chinese Language 
 Models. Proceedings of ACL-96, pages 
 139?143. 
Schone, P. and Jurafsky, D. 2001. 
 Knowledge-Free Induction of  Inflectional 
 Morphologies. Proceedings  of  North 
 American Chapter of  Association for 
 Computational  Linguistics. 
Yarowsky, D. and Wicentowski, R. 2000. 
 Minimally supervised morphological 
 analysis by multimodal alignment. 
 Proceedings of ACL-2000, pages 207? 216. 
Yarowsky, D, Ngai G. and Wicentowski, R. 
 2001. Inducting Multilingual Text  Analysis 
 Tools via Robust Projection  across Aligned 
 Corpora. Proceedings of  HLT 2001, pages 
 161?168. 
 
 
 
 8
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 25?30,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Examining the Effect of Improved Context Sensitive Morphology on 
Arabic Information Retrieval 
 
 
 
Kareem Darwish Hany Hassan and Ossama Emam 
Dept. of Information Engineering & Technology IBM Technology Development Center 
German University in Cairo P.O. Box 166 
5th District, New Cairo, Cairo, Egypt El-Ahram, Giza, Egypt 
 and  
IBM Technology Development Center 
{hanyh,emam}@eg.ibm.com 
P.O. Box 166, El-Ahram, Giza, Egypt  
kareem@darwish.org  
 
 
 
 
Abstract 
This paper explores the effect of 
improved morphological analysis, 
particularly context sensitive morphology, 
on monolingual Arabic Information 
Retrieval (IR).  It also compares the effect 
of context sensitive morphology to non-
context sensitive morphology.  The results 
show that better coverage and improved 
correctness have a dramatic effect on IR 
effectiveness and that context sensitive 
morphology further improves retrieval 
effectiveness, but the improvement is not 
statistically significant. Furthermore, the 
improvement obtained by the use of 
context sensitive morphology over the use 
of light stemming was not significantly 
significant. 
1 Introduction 
Due to the morphological complexity of the Arabic 
language, much research has focused on the effect 
of morphology on Arabic Information Retrieval 
(IR).  The goal of morphology in IR is to conflate 
words of similar or related meanings.  Several 
early studies suggested that indexing Arabic text 
using roots significantly increases retrieval 
effectiveness over the use of words or stems [1, 3, 
11].  However, all the studies used small test 
collections of only hundreds of documents and the 
morphology in many of the studies was done 
manually.   
Performing morphological analysis for Arabic IR 
using existing Arabic morphological analyzers, 
most of which use finite state transducers [4, 12, 
13], is problematic for two reasons.  First, they 
were designed to produce as many analyses as 
possible without indicating which analysis is most 
likely.  This property of the analyzers complicates 
retrieval, because it introduces ambiguity in the 
indexing phase as well as the search phase of 
retrieval.  Second, the use of finite state 
transducers inherently limits coverage, which the 
number of words that the analyzer can analyze, to 
the cases programmed into the transducers.  
Darwish attempted to solve this problem by 
developing a statistical morphological analyzer for 
Arabic called Sebawai that attempts to rank 
possible analyses to pick the most likely one [7].  
He concluded that even with ranked analysis, 
morphological analysis did not yield statistically 
significant improvement over words in IR.  A later 
study by Aljlayl et al on a large Arabic collection 
of 383,872 documents suggested that lightly 
stemmed words, where only common prefixes and 
suffixes are stripped from them, were perhaps 
better index term for Arabic [2].  Similar studies by 
Darwish [8] and Larkey [14] also suggested that 
light stemming is indeed superior to morphological 
analysis in the context of IR.   
25
However, the shortcomings of morphology might 
be attributed to issues of coverage and correctness.  
Concerning coverage, analyzers typically fail to 
analyze Arabized or transliterated words, which 
may have prefixes and suffixes attached to them 
and are typically valuable in IR.  As for 
correctness, the presence (or absence) of a prefix 
or suffix may significantly alter the analysis of a 
word.  For example, for the word ?Alksyr? is 
unambiguously analyzed to the root ?ksr? and stem 
?ksyr.?  However, removing the prefix ?Al? 
introduces an additional analysis, namely to the 
root ?syr? and the stem ?syr.?  Perhaps such 
ambiguity can be reduced by using the context in 
which the word is mentioned.  For example, for the 
word ?ksyr? in the sentence ?sAr ksyr? (and he 
walked like), the letter ?k? is likely to be a prefix. 
The problem of coverage is practically eliminated 
by light stemming.  However, light stemming 
yields greater consistency without regard to 
correctness.  Although consistency is more 
important for IR applications than linguistic 
correctness, perhaps improved correctness would 
naturally yield great consistency.  Lee et al [15] 
adopted a trigram language model (LM) trained on 
a portion of the manually segmented LDC Arabic 
Treebank in developing an Arabic morphology 
system, which attempts to improve the coverage 
and linguistic correctness over existing statistical 
analyzers such as Sebawai [15].  The analyzer of 
Lee et al will be henceforth referred to as the 
IBM-LM analyzer.  IBM-LM's analyzer combined 
the trigram LM (to analyze a word within its 
context in the sentence) with a prefix-suffix filter 
(to eliminate illegal prefix suffix combinations, 
hence improving correctness) and unsupervised 
stem acquisition (to improve coverage).  Lee et al 
report a 2.9% error rate in analysis compared to 
7.3% error reported by Darwish for Sebawai [7]. 
This paper evaluates the IBM-LM analyzer in the 
context of a monolingual Arabic IR application to 
determine if in-context morphology leads to 
improved retrieval effectiveness compared to out-
of-context analysis.  To determine the effect of 
improved analysis, particularly the use of in-
context morphology, the analyzer is used to 
produce analyses of words in isolation (with no 
context) and in-context.  Since IBM-LM only 
produces stems, Sebawai was used to produce the 
roots corresponding to the stems produced by 
IBM-LM.  Both are compared to Sebawai and light 
stemming. 
The paper will be organized as follows:  Section 2 
surveys related work; Section 3 describes the IR 
experimental setup for testing the IBM-LM 
analyzer; Section 4 presents experimental results; 
and Section 5 concludes the paper.  
2 Related Work 
Most early studies of character-coded Arabic text 
retrieval relied on relatively small test collections 
[1, 3, 9, 11].  The early studies suggested that 
roots, followed by stems, were the best index terms 
for Arabic text.  More recent studies are based on a 
single large collection (from TREC-2001/2002) [9, 
10]. The studies examined indexing using words, 
word clusters [14], terms obtained through 
morphological analysis (e.g., stems and roots [9]), 
light stemming [2, 8, 14], and character n-grams of 
various lengths [9, 16].  The effects of normalizing 
alternative characters, removal of diacritics and 
stop-word removal have also been explored [6, 
19].  These studies suggest that perhaps light 
stemming and character n-grams are the better 
index terms.   
Concerning morphology, some attempts were 
made to use statistics in conjunction with rule-
based morphology to pick the most likely analysis 
for a particular word or context.  In most of these 
approaches an Arabic word is assumed to be of the 
form prefix-stem-suffix and the stem part may or 
may not be derived from a linguistic root.  Since 
Arabic morphology is ambiguous, possible 
segmentations (i.e. possible prefix-stem-suffix 
tuples) are generated and ranked based on the 
probability of occurrence of prefixes, suffixes, 
stems, and stem template.  Such systems that use 
this methodology include RDI?s MORPHO3 [5] 
and Sebawai [7].  The number of manually crafted 
rules differs from system to system.  Further 
MORPHO3 uses a word trigram model to improve 
in-context morphology, but uses an extensive set of 
manually crafted rules.  The IBM-LM analyzer 
uses a trigram language model with a minimal set 
of manually crafted rules [15].  Like other 
statistical morphology systems, the IBM-LM 
analyzer assumes that a word is constructed as 
prefix-stem-suffix.  Given a word, the analyzer 
generates all possible segmentations by identifying 
all matching prefixes and suffixes from a table of 
26
prefixes and suffixes.  Then given the possible 
segmentations, the trigram language model score is 
computed and the most likely segmentation is 
chosen.  The analyzer was trained on a manually 
segmented Arabic corpus from LDC.  
3 Experimental Design  
IR experiments were done on the LDC 
LDC2001T55 collection, which was used in the 
Text REtrieval Conference (TREC) 2002 cross-
language track.  For brevity, the collection is 
referred to as the TREC collection.  The collection 
contains 383,872 articles from the Agence France 
Press (AFP) Arabic newswire.  Fifty topics were 
developed cooperatively by the LDC and the 
National Institute of Standards and Technology  
(NIST), and relevance judgments were developed 
at the LDC by manually judging a pool of 
documents obtained from combining the top 100 
documents from all the runs submitted by the 
participating teams to TREC?s cross-language 
track in 2002.  The number of known relevant 
documents ranges from 10 to 523, with an average 
of 118 relevant documents per topic [17].  This is 
presently the best available large Arabic 
information retrieval test collection.  The TREC 
topic descriptions include a title field that briefly 
names the topic, a description field that usually 
consists of a single sentence description, and a 
narrative field that is intended to contain any 
information that would be needed by a human 
judge to accurately assess the relevance of a 
document [10].  Queries were formed from the 
TREC topics by combining the title and 
description fields.  This is intended to model the 
sort of statement that a searcher might initially 
make when asking an intermediary, such as a 
librarian, for help with a search. 
Experiments were performed for the queries with 
the following index terms:   
? w:  words.   
? ls:  lightly stemmed words, obtained using Al-
Stem [17]1. 
? SEB-s:  stems obtained using Sebawai. 
? SEB-r:  roots obtained using Sebawai. 
                                                        
1 A slightly modified version of Leah Larkey?s Light-10 light 
stemmer [8] was also tried, but the stemmer produced very 
similar results to Al-Stem. 
? cIBM-LMS:  stems obtained using the IBM-
LM analyzer in context.  Basically, the entire 
TREC collection was processed by the 
analyzer and the prefixes and suffixes in the 
segmented output were removed. 
? cIBM-SEB-r:  roots obtained by analyzing the 
in-context stems produced by IBM-LM using 
Sebawai. 
? IBM-LMS:  stems obtained using the IBM-LM 
analyzer without any contextual information.  
Basically, all the unique words in the 
collection were analyzed one by one and the 
prefixes and suffixes in the segmented output 
were removed. 
? IBM-SEB-r:  roots obtained by analyzing the 
out-of-context stems produced by IBM-LM 
using Sebawai. 
All retrieval experiments were performed using the 
Lemur language modeling toolkit, which was 
configured to use Okapi BM-25 term weighting 
with default parameters and with and without blind 
relevance feedback (the top 20 terms from the top 
5 retrieved documents were used for blind 
relevance feedback).   To observe the effect of 
alternate indexing terms mean uninterpolated 
average precision was used as the measure of 
retrieval effectiveness.  To determine if the 
difference between results was statistically 
significant, a Wilcoxon signed-rank test, which is a 
nonparametric significance test for correlated 
samples, was used with p values less than 0.05 to 
claim significance.   
4 Results and Discussion 
Figure 1 shows a summary of the results for 
different index terms.  Tables 1 and 2 show 
statistical significance between different index 
terms using the p value of the Wilcoxon test.  
When comparing index terms obtained using IBM-
LM and Sebawai, the results clearly show that 
using better morphological analysis produces 
better retrieval effectiveness.  The dramatic 
difference in retrieval effectiveness between 
Sebawai and IBM-LM highlight the effect of errors 
in morphology that lead to inconsistency in 
analysis.  When using contextual information in 
analysis (compared to analyzing words in isolation 
? out of context) resulted in only a 3% increase in 
mean average precision when using stems (IBM-
LMS), which is a small difference compared to the 
27
effect of blind relevance feedback (about 6% 
increase) and produced mixed results when using 
roots (IBM-SEB-r).  Nonetheless, the improvement 
for stems was almost statistically significant with p 
values of 0.063 and 0.054 for the cases with and 
without blind relevance feedback.  Also 
considering that improvement in retrieval 
effectiveness resulted from changing the analysis 
for only 0.12% of the words in the collection (from 
analyzing them out of context to analyzing them in 
context)2 and that the authors of IBM-LM report 
about 2.9% error rate in morphology, perhaps 
further improvement in morphology may lead to 
further improvement in retrieval effectiveness.  
However, further improvements in morphology 
and retrieval effectiveness are likely to be difficult.  
One of difficulties associated with developing 
better morphology is the disagreement on what 
constitutes ?better? morphology.  For example, 
should ?mktb? and ?ktb? be conflated?   ?mktb? 
translates to office, while ktb translates to books.  
Both words share the common root ?ktb,? but they 
are not interchangeable in meaning or usage.  One 
                                                        
2 Approximately 7% of unique tokens had two or more differ-
ent analysis in the collection when doing in-context morphol-
ogy.  In tokens with more than one analysis, one of the 
analyses was typically used more than 98% of the time.  
would expect that increasing conflation would 
improve recall at the expense of precision and 
decreasing conflation would have the exact 
opposite effect.  It is known that IR is more 
tolerant of over-conflation than under-conflation 
[18].  This fact is apparent in the results when 
comparing roots and stems.  Even though roots 
result in greater conflation than stems, the results 
for stems and roots are almost the same.  Another 
property of IR is that IR is sensitive to consistency 
of analysis.  In the case of light stemming, 
stemming often mistakenly removes prefixes and 
suffixes leading to over conflation, for which IR is 
tolerant, but the mistakes are done in a consistent 
manner.  It is noteworthy that sense 
disambiguation has been reported to decrease 
retrieval effectiveness [18]. However, since 
improving the correctness of morphological 
analysis using contextual information is akin to 
sense disambiguation, the fact that retrieval results 
improved, though slightly, using context sensitive 
morphology is a significant result. 
In comparing the IBM-LM analyzer (in context or 
out of context) to light stemming (using Al-Stem), 
although the difference in retrieval effectiveness is 
small and not statistically significant, using the 
IBM-LM analyzer, unlike using Al-Stem, leads to 
Figure 1.  Comparing index term with and without blind relevance feedback using mean average 
precision 
0.
30
8
0.3
030.3
20
0.2
56
0.
31
8
0.3
27
0.
29
7
0.
30
0
0.
21
7
0.
21
7
0.
28
7
0.
26
3
0.3
19
0.
33
2
0.
26
7
0.
27
8
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
w ls SEB-s SEB-r IBM-
LMS
IBM-
SEB-r
cIBM-
LMS
cIBM-
SEB-r
Index Terms
no feedback
with feedback
28
statistically significant improvement over using 
words.  Therefore there is some advantage, though 
only a small one, to using statistical analysis over 
using light stemming.  The major drawback to 
morphological analysis (specially in-context 
analysis) is that it requires considerably more 
computing time than light stemming3. 
5 Conclusion 
The paper investigated the effect of improved 
morphological analysis, especially context 
sensitive morphology, in Arabic IR applications 
compared to other statistical morphological 
analyzers and light stemming.  The results show 
that improving morphology has a dramatic effect 
on IR effectiveness and that context sensitive 
morphology slightly improved Arabic IR over non-
context sensitive morphology, increasing IR 
                                                        
3 The processing of the TREC collection using the in-context 
IBM-LM required 16 hours on a 2.4 GHz Pentium 4 machine 
with 1 Gigabyte of RAM compared to 10 minutes to perform 
light stemming.  
effectiveness by approximately 3%.  The 
improvement is almost statistically significant.  
Developing better morphology could lead to 
greater retrieval effectiveness, but improving 
analyzers is likely to be difficult and would require 
careful determination of the proper level of 
conflation.  In overcoming some of the difficulties 
associated with obtaining ?better? morphology (or 
more fundamentally the proper level of word 
conflation), adaptive morphology done on a per 
query term basis or user feedback might prove 
valuable.  Also, the scores that were used to rank 
the possible analyses in a statistical morphological 
analyzer may prove useful in further improving 
retrieval.  Other IR techniques, such as improved 
blind relevance feedback or combination of 
evidence approaches, can also improve 
monolingual Arabic retrieval. 
Perhaps improved morphology is particularly 
beneficial for other IR applications such as cross-
language IR, in which ascertaining proper 
translation of words is particularly important, and 
ls SEB-s SEB-r 
IBM-
LMS 
IBM-
SEB-r 
cIBM-
LMS 
cIBM-
SEB-r 
 
0.055 0.475 0.671 0.038 0.027 0.019 0.049 w 
 0.004 0.023 0.560 0.359 0.946 0.505 ls 
  0.633 0.005 0.001 0.001 0.012 SEB-s 
   0.039 0.007 0.020 0.064 SEB-r 
    0.0968 0.063 0.758 
IBM-
LMS 
     0.396 0.090 
IBM-
SEB-r 
      0.001 
cIBM-
LMS 
Table 1. Wilcoxon p values (shaded=significant) , with blind  relevance feedback. 
ls SEB-s SEB-r 
IBM-
LMS 
IBM-
SEB-r 
cIBM-
LMS 
cIBM-
SEB-r 
 
0.261 0.035 0.065 0.047 0.135 0.011 0.016 w 
 0.000 0.000 0.968 0.757 0.515 0.728 ls 
  0.269 0.000 0.000 0.000 0.000 SEB-s 
   0.000 0.000 0.000 0.000 SEB-r 
    0.732 0.054 0.584 
IBM-
LMS 
     0.284 0.512 
IBM-
SEB-r 
      0.005 
cIBM-
LMS 
Table 2. Wilcoxon p values (shaded=significant) , without blind relevanc e feedback 
 
29
in-document search term highlighting for display 
to a user.  
References  
1.  Abu-Salem, H., M. Al-Omari, and M. Evens.  
Stemming Methodologies Over Individual Query 
Words for Arabic Information Retrieval. JASIS, 
1999. 50(6): p.  524-529. 
2.  Aljlayl, M., S. Beitzel, E. Jensen, A. Chowdhury, D. 
Holmes, M. Lee, D.  Grossman, and O. Frieder. IIT 
at TREC-10. In TREC. 2001. Gaithersburg, MD. 
3.  Al-Kharashi, I. and M Evens.  Comparing Words, 
Stems, and Roots as Index Terms in an Arabic 
Information Retrieval System. JASIS, 1994. 45(8): p. 
548 - 560. 
4.  Antworth, E. PC-KIMMO: a two-level processor for 
morphological analysis. In Occasional Publications 
in Academic Computing. 1990. Dallas, TX: Summer 
Institute of Linguistics. 
5.  Ahmed, Mohamed Attia.  A Large-Scale 
Computational Processor of the Arabic Morphology, 
and Applications.  A Master?s Thesis, Faculty of 
Engineering, Cairo University, Cairo, Egypt, 2000. 
6. Chen, A. and F. Gey. Translation Term Weighting 
and Combining Translation Resources in Cross-
Language Retrieval. In TREC, 2001. Gaithersburg, 
MD. 
7. Darwish, K. Building a Shallow Morphological 
Analyzer in One Day.  ACL Workshop on 
Computational Approaches to Semitic Languages. 
2002. 
8.  Darwish, K. and D. Oard.  CLIR Experiments at 
Maryland for  TREC 2002:   Evidence Combination 
for Arabic-English Retrieval.  In TREC. 2002.  
Gaithersburg, MD. 
9. Darwish, K. and D. Oard. Term Selection for 
Searching Printed Arabic. SIGIR, 2002. Tampere, 
Finland. p. 261 - 268. 
10.  Gey, F. and D. Oard. The TREC-2001 Cross-
Language Information Retrieval Track: Searching 
Arabic Using English, French or Arabic Queries.  
TREC, 2001. Gaithersburg, MD. p. 16-23. 
11.  Hmeidi, I., G. Kanaan, and M. Evens.  Design and 
Implementation of Automatic Indexing for 
Information Retrieval with Arabic Documents. 
JASIS, 1997. 48(10):  p. 867 - 881. 
12.  Kiraz, G. Arabic Computation Morphology in the 
West.  In The 6th International  Conference and 
Exhibition on Multi-lingual Computing. 1998. 
Cambridge.   
13.  Koskenniemi, K., Two Level Morphology:  A 
General Computational Model for Word-form 
Recognition and Production. 1983, Department of 
General Linguistics, University of Helsinki. 
14. Larkey, L., L. Ballesteros, and M. Connell. 
Improving Stemming for Arabic Information 
Retrieval:  Light Stemming and Co-occurrence 
Analysis.  SIGIR 2002.  p. 275-282, 2002.  
15.  Lee, Y., K. Papineni, S. Roukos, O. Emam, and H. 
Hassan. Language Model Based Arabic Word 
Segmentation. In the Proceedings of the 41st Annual 
Meeting of the Association for Computational 
Linguistics, July 2003, Sapporo, Japan.  p. 399 - 406.  
16. Mayfield, J., P. McNamee, C. Costello, C. Piatko, 
and A. Banerjee. JHU/APL at TREC 2001:  
Experiments in Filtering and in Arabic, Video, and 
Web Retrieval. In TREC 2001. Gaithersburg, MD. p. 
322-329. 
17.  Oard, D. and F. Gey. The TREC 2002 
Arabic/English CLIR Track. In TREC 2002. 
Gaithersburg, MD. 
18.  Sanderson, M.  Word sense disambiguation and 
information  retrieval. In Proceedings  of the 17th 
ACM SIGIR Conference, p. 142-151, 1994 
19.  Xu, J., A. Fraser, and R. Weischedel. 2001 Cross-
Lingual Retrieval at BBN.  In TREC, 2001. 
Gaithersburg, MD. p. 68 - 75. 
 
30
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 501?508,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Information Extraction Approach Using Graph Mutual 
Reinforcement  
 
 
Hany Hassan Ahmed Hassan Ossama Emam 
 
IBM Cairo Technology Development Center 
Giza, Egypt 
P.O. Box 166 Al-Ahram 
 
hanyh@eg.ibm.com hasanah@eg.ibm.com emam@eg.ibm.com 
 
  
 
Abstract 
Information Extraction (IE) is the task of 
extracting knowledge from unstructured 
text. We present a novel unsupervised 
approach for information extraction 
based on graph mutual reinforcement. 
The proposed approach does not require 
any seed patterns or examples. Instead, it 
depends on redundancy in large data sets 
and graph based mutual reinforcement to 
induce generalized ?extraction patterns?. 
The proposed approach has been used to 
acquire extraction patterns for the ACE 
(Automatic Content Extraction) Relation 
Detection and Characterization (RDC) 
task. ACE RDC is considered a hard task 
in information extraction due to the ab-
sence of large amounts of training data 
and inconsistencies in the available data. 
The proposed approach achieves superior 
performance which could be compared to 
supervised techniques with reasonable 
training data.  
1 Introduction 
In this paper we propose a novel, and completely 
unsupervised approach for information extrac-
tion. We present a general technique; however 
we focus on relation extraction as an important 
task of Information Extraction. The approach 
depends on constructing generalized extraction 
patterns, which could match many instances, and 
deploys graph based mutual reinforcement to 
weight the importance of these patterns. The mu-
tual reinforcement is used to automatically iden-
tify the most informative patterns, where patterns 
that match many instances tend to be correct. 
Similarly, instances matched by many patterns 
tend to be correct. The intuition is that large un-
supervised data is redundant, i.e. different in-
stances of information could be found many 
times in different contexts and by different repre-
sentation. The problem can therefore be seen as 
hubs (instances) and authorities (patterns) prob-
lem which can be solved using the Hypertext 
Induced Topic Selection (HITS) algorithm 
(Kleinberg, 1998). 
HITS is an algorithmic formulation of the no-
tion of authority in web pages link analysis, 
based on a relationship between a set of relevant 
?authoritative pages? and a set of ?hub pages?. 
The HITS algorithm benefits from the following 
observation:  when a page (hub) links to another 
page (authority), the former confers authority 
over the latter.  
By analogy to the authoritative web pages 
problem, we could represent the patterns as au-
thorities and instances as hubs, and use mutual 
reinforcement between patterns and instances to 
weight the most authoritative patterns. Highly 
weighted patterns are then used in extracting in-
formation.  
The proposed approach does not need any 
seeds or examples. Human involvement is only 
needed in determining the entities of interest; the 
entities among which we are seeking relations. 
The paper proceeds as follows: in Section 2 
we discuss previous work followed by a brief 
definition of our general notation in Section 3. A 
detailed description of the proposed approach 
then follows in Section 4. Section 5 discusses the 
application of the proposed approach to the prob-
501
lem of detecting semantic relations from text. 
Section 6 discusses experimental results while 
the conclusion is presented in Section 7. 
2 Previous Work 
Most of the previous work on Information Ex-
traction (IE) focused on supervised learning. Re-
lation Detection and Characterization (RDC) was 
introduced in the Automatic Content Extraction 
Program (ACE) (ACE, 2004). The approaches 
proposed to the ACE RDC task such as kernel 
methods (Zelenko et al, 2002) and Maximum 
Entropy methods (Kambhatla, 2004) required the 
availability of large set of human annotated cor-
pora which are tagged with relation instances. 
However human annotated instances are limited, 
expensive, and time consuming to obtain, due to 
the lack of experienced human annotators and the 
low inter-annotator agreements. 
Some previous work adopted weakly super-
vised or unsupervised learning approaches. 
These approaches have the advantage of not 
needing large tagged corpora but need seed ex-
amples or seed extraction patterns. The major 
drawback of these approaches is their depend-
ency on seed examples or seed patterns which 
may lead to limited generalization due to de-
pendency on handcrafted examples. Some of 
these approaches are briefed here: 
 (Brin,98) presented an approach for extracting 
the authorship information as found in books de-
scription on the World Wide Web. This tech-
nique is based on dual iterative pattern relation 
extraction wherein a relation and pattern set is 
iteratively constructed. This approach has two 
major drawbacks: the use of handcrafted seed 
examples to extract more examples similar to 
these handcrafted seed examples and the use of a 
lexicon as the main source for extracting infor-
mation. 
(Blum and Mitchell, 1998) proposed an ap-
proach based on co-training that uses unlabeled 
data in a particular setting. They exploit the fact 
that, for some problems, each example can be 
described by multiple representations. 
(Riloff & Jones, 1999) presented the Meta-
Bootstrapping algorithm that uses an un-
annotated training data set and a set of seeds to 
learn a dictionary of extraction patterns and a 
domain specific semantic lexicon. Other works 
tried to exploit the duality of patterns and their 
extractions for the purpose of inferring the se-
mantic class of words like (Thelen & Riloff, 
2002) and (Lin et al 2003). 
(Muslea et al, 1999) introduced an inductive 
algorithm to generate extraction rules based on 
user labeled training examples. This approach 
suffers from the labeled data bottleneck. 
(Agichtein et. al, 2000) presented an approach 
using seed examples to generate initial patterns 
and to iteratively obtain further patterns. Then 
ad-hoc measures were deployed to estimate the 
relevancy of the patterns that have been newly 
obtained. The major drawbacks of this approach 
are:  its dependency on seed examples leads to 
limited capability of generalization, and the esti-
mation of patterns relevancy requires the de-
ployment of ad-hoc measures. 
(Hasegawa et. al. 2004) introduced unsuper-
vised approach for relation extraction depending 
on clustering context words between named enti-
ties; this approach depends on ad-hoc context 
similarity between phrases in the context and 
focused on certain types of relations. 
(Etzioni et al 2005) proposed a system for 
building lists of named entities found on the web. 
Their system uses a set of eight domain-
independent extraction patterns to generate can-
didate facts. 
All approaches, proposed so far, suffer from 
either requiring large amount of labeled data or 
the dependency on seed patterns (or examples) 
that result in limited generalization. 
3 General Notation 
In graph theory, a graph is a set of objects called 
vertices joined by links called edges. A bipartite 
graph, also called a bigraph, is a special graph 
where the set of vertices can be divided into two 
disjoint sets with no two vertices of the same set 
sharing an edge.  
The Hypertext Induced Topic Selection 
(HITS) algorithm is an algorithm for rating, and 
therefore ranking, web pages. The HITS algo-
rithm makes use of the following observation: 
when a page (hub) links to another page (author-
ity), the former confers authority over the latter. 
HITS uses two values for each page, the "author-
ity value" and the "hub value". "Authority value" 
and "hub value" are defined in terms of one an-
other in a mutual recursion. An authority value is 
computed as the sum of the scaled hub values 
that point to that authority. A hub value is the 
sum of the scaled authority values of the authori-
ties it points to. 
A template, as we define for this work, is a se-
quence of generic forms that could generalize 
502
over the given instances. An example template 
is:  
GPE POS  (PERSON)+ 
 
GPE: Geographical Political En-
tity 
POS: possessive ending 
PERSON: PERSON Entity 
 
This template could match the sentence: 
?France?s President Jacque Chirac...?.  This tem-
plate is derived from the representation of the 
Named Entity tags, Part-of-Speech (POS) tags 
and semantic tags. The choice of the template 
representation here is for illustration purpose 
only; any combination of tags, representations 
and tagging styles might be used.  
A pattern is more specific than a template. A 
pattern specifies the role played by the tags (first 
entity, second entity, or relation). An example of 
a pattern is:  
    
GPE(E2)  POS   (PERSON)+(E1) 
 
This pattern indicates that the word(s) with the 
tag GPE in the sentence represents the second 
en-tity (Entity 2) in the relation, while the 
word(s) tagged PERSON represents the first en-
tity (Entity 1) in this relation, the ?+? symbol 
means that the (PERSON) entity is repetitive (i.e. 
may consist of several tokens).  
A tuple, in our notation during this paper, is 
the result of the application of a pattern to un-
structured text. In the above example, one result 
of applying the pattern to some raw text is the 
following tuple: 
 
Entity 1: Jacque Chirac 
Entity 2: France 
Relation: EMP-Executive 
4 The Approach 
The unsupervised graph-based mutual rein-
forcement approach, we propose, depends on the 
construction of generalized ?extraction patterns? 
that could match many instances. The patterns 
are then weighted according to their importance 
by deploying graph based mutual reinforcement 
techniques. This duality in patterns and extracted 
information (tuples) could be stated that patterns 
could match different tuples, and tuples in turn 
could be matched by different patterns. The pro-
posed approach is composed of two main steps 
namely, initial patterns construction and pattern 
weighting or induction. Both steps are detailed in 
the next sub-sections. 
4.1 Initial Patterns Construction 
As shown in Figure 1, several syntactic, lexical, 
and semantic analyzers could be applied to the 
unstructured text. The resulting analyses could be 
employed in the construction of extraction pat-
terns. It is worth mentioning that the proposed 
approach is general enough to accommodate any 
pattern design; the introduced pattern design is 
for illustration purposes only. 
 
 
 
 
Initially, we need to start with some templates 
and patterns to proceed with the induction proc-
ess. Relatively large amount of text data is 
tagged with different taggers to produce the pre-
viously mentioned patterns styles. An n-gram 
language model is built on this data and used to 
construct weighted finite state machines.  
Paths with low cost (high language model 
probabilities) are chosen to construct the initial 
set of templates; the intuition is that paths with 
low cost (high probability) are frequent and 
could represent potential candidate patterns. 
The resulting initial set of templates is applied 
to a very large text data to produce all possible 
patterns. The number of candidate initial patterns 
could be reduced significantly by specifying the 
candidate types of entities; for example we might 
specify that the first entity could be PEROSN or 
PEOPLE while the second entity could be OR-
GANIZATION, LOCATION, COUNTRY and 
etc...  
The candidate patterns are then applied to the 
tagged stream and the unstructured text to collect 
a set of patterns and matched tuples pairs.  
The following procedure briefs the Initial Pat-
tern Construction Step: 
? Select a random set of text data. 
American vice President   Al Gore said today... 
PEOPLE    O         O       PERSON   O    O... 
ADJ     NOUN_PHRASE   NNP  VBD CD... 
PEOPLE NOUN_PHRASE  PERSON  VBD CD... 
Entities 
POS 
Tagged 
Stream 
Figure 1:  An example of the output of analys-
ers applied to the unstructured text  
 
503
? Apply various taggers on text data and con-
struct templates style. 
? Build n-gram language model on template 
style data. 
? Construct weighted finite state machines 
from the n-gram language model. 
? Choose n-best paths in the finite state ma-
chines. 
? Use best paths as initial templates. 
? Apply initial templates on large text data. 
? Construct initial patterns and associated tu-
ples sets. 
4.2 Pattern Induction 
The inherent duality in the patterns and tuples 
relation suggests that the problem could be inter-
preted as a hub authority problem. This problem 
could be solved by applying the HITS algorithm 
to iteratively assign authority and hub scores to 
patterns and tuples respectively. 
 
 
Patterns and tuples are represented by a bipar-
tite graph as illustrated in figure 2. Each pattern 
or tuple is represented by a node in the graph. 
Edges represent matching between patterns and 
tuples. The pattern induction problem can be 
formulated as follows: Given a very large set of 
data D containing a large set of patterns P which 
match a large set of tuples T, the problem is to 
identify P
~
, the set of patterns that match the set 
of the most correct tuples  T
~
. The intuition is 
that the tuples matched by many different pat-
terns tend to be correct and the patterns matching 
many different tuples tend to be good patterns. In 
other words; we want to choose, among the large 
space of patterns in the data, the most informa-
tive, highest confidence patterns that could iden-
tify correct tuples; i.e. choosing the most ?au-
thoritative? patterns in analogy with the hub au-
thority problem. However, both P
~
and T
~
are un-
known. The induction process proceeds as fol-
lows:  each pattern p in P is associated with a 
numerical authority weight av which expresses 
how many tuples match that pattern. Similarly, 
each tuple t in T has a numerical hub weight ht 
which expresses how many patterns were 
matched by this tuple. The weights are calculated 
iteratively as follows: 
( ) ( )( )
=
+
=
pT
u i
i
i
H
uhpa
1 )(
)(
)1(
 (1) 
( ) ( )( )
=
+
=
tP
u i
i
i
A
ua
th
1 )(
)(
)1(
 (2) 
where T(p) is the set of tuples matched by p, P(t) 
is the set of patterns matching t, ( )pa i )1( +  is the 
authoritative weight of pattern p  at iteration  
)1( +i , and ( )th i )1( +  is the hub weight of tuple t  
at iteration  )1( +i  . H(i) and A(i) are normaliza-
tion factors defined as: 
 
( )( ) 
= =
=
||
1 1
)()( P
p
pT
u
ii uhH  (3) 
( )( ) 
= =
=
||
1 1
)()( T
v
tP
u
ii uaA
 (4) 
 
Highly weighted patterns are identified and used 
for extracting relations. 
4.3 Tuple Clustering 
The tuple space should be reduced to allow more 
matching between pattern-tuple pairs. This space 
reduction could be accomplished by seeking a 
tuple similarity measure, and constructing a 
weighted undirected graph of tuples. Two tuples 
are linked with an edge if their similarity meas-
ure exceeds a certain threshold. Graph clustering 
algorithms could be deployed to partition the 
graph into a set of homogeneous communities or 
clusters. To reduce the space of tuples, we seek a 
matching criterion that group similar tuples to-
gether. Using WordNet, we can measure the se-
mantic similarity or relatedness between a pair of 
concepts (or word senses), and by extension, be-
tween a pair of sentences. We use the similarity 
P
P
P
P
P
T
T
T
T
T
P
P
T
T
Patterns Tuples
Figure 2: A bipartite graph represent-
ing patterns and tuples 
504
measure described in (Wu and Palmer, 1994) 
which finds the path length to the root  node 
from the least common subsumer (LCS) of the 
two word senses which is the most specific word 
sense they share as an ancestor. The similarity 
score of two tuples, ST, is calculated as follows: 
 
2
2
2
1 EET SSS +=    (5) 
 
where SE1, and SE2 are the similarity scores of the 
first entities in the two tuples, and their second 
entitles respectively. 
The tuple matching procedure assigns a simi-
larity measure to each pair of tuples in the data-
set. Using this measure we can construct an undi-
rected graph G. The vertices of G are the tuples. 
Two vertices are connected with an edge if the 
similarity measure between their underlying tu-
ples exceeds a certain threshold. It was noticed 
that the constructed graph consists of a set of 
semi isolated groups as shown in figure 3. Those 
groups have a very large number of inter-group 
edges and meanwhile a rather small number of 
intra-group edges. This implies that using a 
graph clustering algorithm would eliminate those 
weak intra-group edges and produce separate 
groups or clusters representing similar tuples. We 
used Markov Cluster Algorithm (MCL) for graph 
clustering (Dongen, 2000). MCL is a fast and 
scalable unsupervised clustering algorithm for 
graphs based on simulation of stochastic flow. 
 
 
 
Figure 3: Applying Clustering Algorithms to Tu-
ple graph  
 
An example of a couple of tuples that could be 
matched by this technique is: 
United Stated(E2) presi-
dent(E1) 
US(E2) leader(E1) 
  
A bipartite graph of patterns and tuple clusters 
is constructed. Weights are assigned to patterns 
and tuple clusters by iteratively applying the 
HITS algorithm and the highly ranked patterns 
are then used for relation extraction.  
5 Experimental Setup 
5.1 ACE Relation Detection and Charac-
terization 
In this section, we describe Automatic Content 
Extraction (ACE). ACE is an evaluation con-
ducted by NIST to measure Entity Detection and 
Tracking (EDT) and Relation Detection and 
Characterization (RDC). The EDT task is con-
cerned with the detection of mentions of entities, 
and grouping them together by identifying their 
coreference. The RDC task detects relations be-
tween entities identified by the EDT task. We 
choose the RDC task to show the performance of 
the graph based unsupervised approach we pro-
pose. To this end we need to introduce the notion 
of mentions and entities. Mentions are any in-
stances of textual references to objects like peo-
ple, organizations, geopolitical entities (countries, 
cities ?etc), locations, or facilities. On the other 
hand, entities are objects containing all mentions 
to the same object. Here, we present some exam-
ples of ACE entities and relations: 
Spain?s Interior Minister 
announced this evening the 
arrest of separatist organi-
zation Eta?s presumed leader 
Ignacio Garcia Arregui. Ar-
regui, who is considered to 
be the Eta organization?s 
top man, was arrested at 
17h45 Greenwich. The Spanish 
judiciary suspects Arregui 
of ordering a failed attack 
on King Juan Carlos in 1995. 
 
In this fragment, all the underlined phrases are 
mentions to ?Eta? organization, or to ?Garcia 
Arregui?. There is a management relation be-
tween ?leader? which references to ?Gar-
cia Arregui? and ?Eta?. 
5.2 Patterns Construction and Induction 
We used the LDC English Gigaword Corpus, 
AFE source from January to August 1996 as a 
source for unstructured text. This provides a total 
of 99475 documents containing 36 M words.  In 
the performed experiments, we focus on two 
types of relations EMP-ORG relations and GPE-
AFF relations which represent almost 50% of all 
relations in RDC ? ACE task. 
T
T T
T
T
T
T
T
T
T
TT
T T
T
T T
T
T
T
T
T
T
T
T
T
T T
Before Clustering After Clustering
505
POS (part of speech) tagger and mention tagger 
were applied to the data, the used pattern design 
consists of a mix between the part of speech 
(POS) tags and the mention tags for the words in 
the unsupervised data. We use the mention tag, if 
it exists; otherwise we use the part of speech tag. 
An example of the analyzed text and the pre-
sumed associated pattern is shown: 
 
Text: Eta?s presumed leader 
Arregui ? 
Pos: NNP POS JJ NN NNP 
Mention: ORG 0 0 0 PERSON 
Pattern: ORG(E2) POS JJ 
NN(R) PERSON(E1) 
 
An n-gram language model, 5-gram model and 
back off to lower order n-grams, was built on the 
data tagged with the described patterns? style. 
Weighted finite states machines were constructed 
with the language model probabilities. The n-best 
paths, 20 k paths, were identified and deployed 
as the initial template set. Sequences that do not 
contain the entities of interest, and hence cannot 
represent relations, were automatically filtered 
out. This resulted in an initial templates set of 
around 3000 element. This initial templates set 
was applied on the text data to establish initial 
patterns and tuples pairs. Graph based mutual 
reinforcement technique was deployed with 10 
iterations on the patterns and tuples pairs to 
weight the patterns. 
We conducted two groups of experiments, the 
first with simple syntactic tuple matching, and 
the second with semantic tuple clustering as de-
scribed in section 4.3 
6 Results and Discussion 
We compare our results to a state-of-the-art su-
pervised system similar to the system described 
in (Kambhatla, 2004). Although it is unfair to 
make a comparison between a supervised system 
and a completely unsupervised system, we chose 
to make this comparison to test the performance 
of the proposed unsupervised approach on a real 
task with defined test set and state-of-the-art per-
formance. The supervised system was trained on 
145 K words which contain 2368 instances of the 
two relation types we are considering. 
The system performance is measured using 
precision, recall and F-Measure with various 
amounts of induced patterns. Table 1 presents the 
precision, recall and F-measure for the two rela-
tions using the presented approach with the utili-
zation of different amount of highly weighted 
patterns. Table 2 presents the same results using 
semantic tuple matching and clustering, as de-
scribed in section 4.3.  
 
No. of  
Patterns Precision Recall F-Measure 
1500 35.9 66.3 46.58 
1000 41.2 59.7 48.75 
700 43.1 58.1 49.49 
500 46 56.5 50.71 
400 46.9 52.9 49.72 
200 50.1 44.9 47.36 
 
Table 1:  The effect of varying the number of 
induced patterns on the system performance 
(syntactic tuple matching) 
 
No. of  
Patterns Precision Recall F-Measure 
1500 36.1 67.2 46.97 
1000 43.7 59.6 50.43 
700 44.1 59.3 50.58 
500 46.3 57.2 51.18 
400 47.3 57.6 51.94 
200 48.1 45.9 46.97 
 
Table 2:  The effect of varying the number of 
induced patterns on the system performance (se-
mantic tuple matching) 
0
10
20
30
40
50
60
70
80
Sup 67.1 54.2 59.96
Unsup-Syn 46 56.5 50.71
Unsup-Sem 47.3 57.6 51.94
Precision Recall F Measure
 
 
Figure 4:  A comparison between the supervised 
system (Sup), the unsupervised system with syn-
tactic tuple matching (Unsup-Syn), and with se-
mantic tuple matching (Unsup-Sem) 
 
Best F-Measure is achieved using relatively 
small number of induced patterns (400 and  500 
patterns) while using more patterns increases the 
recall but degrades the precision. 
Table 2 indicates that the semantic clustering 
of tuples did not provide significant improve-
506
ment; although better performance was achieved 
with less number of patterns (400 patterns). We 
think that the deployed similarity measure and it 
needs further investigation to figure out the rea-
son for that. 
Figure 4 presents the comparison between the 
proposed unsupervised systems and the reference 
supervised system. The unsupervised systems 
achieves good results even in comparison to  a 
state-of-the-art supervised system. 
Sample patterns and corresponding matching 
text are introduced in Table 3 and Table 4. Table 
3 shows some highly ranked patterns while Table 
4 shows examples of low ranked patterns. 
 
Pattern Matches 
GPE (PERSON)+ Peruvian President Alberto Fu-jimori 
GPE (PERSON)+ Zimbabwean President Robert Mugabe 
GPE (PERSON)+ PLO leader Yasser Arafat 
GPE POS (PERSON)+ Zimbabwe 's President Robert Mugabe 
GPE JJ PERSON    American clinical neuropsy-
chologist 
GPE JJ PERSON    American diplomatic personnel 
PERSON IN JJ GPE candidates for local government 
ORGANIZATION PER-
SON Airways spokesman 
ORGANIZATION PER-
SON      Ajax players 
PERSON IN DT (OR-
GANIZATION)+  
chairman of the opposition par-
ties 
(ORGANIZATION)+ 
PERSON    opposition parties chairmans 
 
Table3: Examples of patterns with high weights 
 
Pattern Matches 
GPE CC (PERSON)+ Barcelona and Johan 
Cruyff 
GPE , CC PERSON Paris , but Riccardi 
GPE VBZ VBN PERSON Pyongyang has accepted 
Gallucci 
GPE VBZ VBN PERSON Russia has abandoned us 
GPE VBZ VBN P PER-
SON 
Rwanda 's defeated Hutu 
GPE VBZ VBN PERSON state has pressed Arafat 
GPE VBZ VBN TO VB 
PERSON 
Taiwan has tried to keep 
Lee 
(PERSON)+ VBD GPE 
ORGANIZATION 
Alfred Streim told Ger-
man radio 
(PERSON)+ VBD GPE 
ORGANIZATION 
Dennis Ross met Syrian 
army 
(PERSON)+ VBD GPE 
ORGANIZATION 
Van Miert told EU indus-
try 
 
Table4: Examples of patterns with low weights 
7 Conclusion and Future Work 
In this work, a general framework for unsuper-
vised information extraction based on mutual 
reinforcement in graphs has been introduced. We 
construct generalized extraction patterns and de-
ploy graph based mutual reinforcement to auto-
matically identify the most informative patterns. 
We provide motivation for our approach from a 
graph theory and graph link analysis perspective. 
Experimental results have been presented sup-
porting the applicability of the proposed ap-
proach to ACE Relation Detection and Charac-
terization (RDC) task, demonstrating its applica-
bility to hard information extraction problems. 
The proposed approach achieves remarkable re-
sults comparable to a state-of-the-art supervised 
system, achieving 51.94 F-measure compared to 
59.96 F-measure of the state-of-the-art super-
vised system which requires huge amount of hu-
man annotated data. The proposed approach 
represents a powerful unsupervised technique for 
information extraction in general and particularly 
for relations extraction that requires no seed pat-
terns or examples and achieves significant per-
formance. 
In our future work, we plan to focus on general-
izing the approach for targeting more NLP prob-
lems. 
8 Acknowledgements 
We would like to thank Salim Roukos for his 
invaluable suggestions and support. We would 
also like to thank Hala Mostafa for helping with 
the early investigation of this work. Finally we 
would like to thank the anonymous reviewers for 
their constructive criticism and helpful com-
ments. 
References 
ACE. 2004. The NIST ACE evaluation website. 
http://www.nist.gov/speech/tests/ace/ 
Eugene Agichtein and Luis Gravano. 2000.  Snow-
ball: Extracting Relations from Large Plain-Text 
Collections. Proceedings of the 5th ACM Confer-
ence on Digital Libraries (DL 2000). 
   Sergy Brin. 1998. Extracting Patterns and Relations 
from the World Wide Web. Proceedings of the 1998 
International Workshop on the Web and Data-
bases? 
Stijn van Dongen. 2000. A Cluster Algorithm for 
Graphs. Technical Report INS-R0010, National 
Research Institute for Mathematics and Computer 
Science in the Netherlands. 
507
Stijn van Dongen. 2000. Graph Clustering by Flow 
Simulation. PhD thesis, University of Utrecht 
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland, 
Daniel S. Weld, and Alexander Yates. 2004. Web-
scale information extraction in KnowItAll (prelimi-
nary results). In Proceedings of the 13th World 
Wide Web Conference, pages 100-109. 
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland, 
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised Named-Entity Extraction from the Web: 
An Experimental Study. Artificial Intelligence, 
2005. 
Radu Florian, Hany Hassan, Hongyan Jing, Nanda 
Kambhatla, Xiaqiang Luo, Nicolas Nicolov, and 
Salim Roukos. 2004. A Statistical Model for multi-
lingual entity detection and tracking. Proceedings 
of the Human Language Technologies Conference 
(HLT-NAACL 2004). 
Dayne Freitag, and Nicholas Kushmerick. 2000. 
Boosted wrapper induction. The 14th European 
Conference on Artificial Intelligence Workshop on 
Machine Learning for Information Extraction 
Rayid Ghani and Rosie Jones. 2002. A Comparison of 
Efficacy and Assumptions of Bootstrapping Algo-
rithms for Training Information Extraction Sys-
tems. Workshop on Linguistic Knowledge Acquisi-
tion and Representation: Bootstrapping Annotated 
Data at the Linguistic Resources and Evaluation 
Conference (LREC 2002). 
Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman. 
2004. Discovering Relations among Named Enti-
ties from Large Corpora. Proceedings of The 42nd 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2004). 
Taher Haveliwala. 2002. Topic-sensitive PageRank. 
Proceedings of the 11th International World Wide 
Web Conference 
Thorsten Joachims. 2003. Transductive Learning via 
Spectral Graph Partitioning. Proceedings of the In-
ternational Conference on Machine Learning 
(ICML 2003). 
Nanda Kambhatla. 2004. Combining Lexical, Syntac-
tic, and Semantic Features with Maximum Entropy 
Models for Information Extraction. Proceedings of 
The 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL 2004). 
John Kleinberg. 1998. Authoritative Sources in a Hy-
perlinked Environment. Proceedings of the 9th 
ACM-SIAM Symposium on Discrete Algorithms. 
N. Kushmerick, D.S. Weld, R.B. Doorenbos. 1997. 
Wrapper Induction for Information Extraction. 
Proceedings of the International Joint Conference 
on Artificial Intelligence.  
Winston Lin, Roman Yangarber, Ralph Grishman. 
2003. Bootstrapped Learning of Semantic Classes 
from Positive and Negative Examples. Proceedings 
of the 20th International Conference on Machine 
Learning (ICML 2003) Workshop on The Contin-
uum from Labeled to Unlabeled Data in Machine 
Learning and Data Mining. 
Ion Muslea, Steven Minton, and Craig 
Knoblock.1999.  A hierarchical approach to wrap-
per induction. Proceedings of the Third Interna-
tional Conference on Autonomous Agents. 
Ted Pedersen, Siddharth Patwardhan, and Jason 
Michelizzi. 2004, WordNet::Similarity - Measuring 
the Relatedness of Concepts. Proceedings of Fifth 
Annual Meeting of the North American Chapter of 
the Association for Computational Linguistics 
(NAACL 2004) 
Ellen Riloff and Rosie Jones. 2003. Learning diction-
aries for information extraction by multilevel boot-
strapping. Proceedings of the Sixteenth national 
Conference on Artificial Intelligence (AAAI 1999). 
Michael Thelen and Ellen Riloff. 2002. A Bootstrap-
ping Method for Learning Semantic Lexicons using 
Extraction Pattern Contexts. Proceedings of the 
2002 Conference on Empirical Methods in Natural 
Language Processing (EMNLP 2002). 
Scott White, and Padhraic Smyth. 2003. Algorithms 
for Discoveing Relative Importance in Graphs. 
Proceedings of Ninth ACM SIGKDD International 
Conference on Knowledge Discovery and Data 
Mining. 
Zhibiao Wu, and Martha Palmer. 1994. Verb seman-
tics and lexical selection. Proceedings of the 32nd 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL 1994). 
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 
2003. Semi-supervised Learning using Gaussian 
Fields and Harmonic Functions. Proceedings of 
the 20th International Conference on Machine 
Learning (ICML 2003). 
 
 
508
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 25?32,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Arabic Cross-Document Person Name Normalization 
Walid Magdy, Kareem Darwish, Ossama Emam, and Hany Hassan 
Human Language Technologies Group 
IBM Cairo Technology Development Center 
P.O. Box 166 El-Ahram, Giza, Egypt 
{wmagdy, darwishk, emam, hanyh}@eg.ibm.com 
 
Abstract 
This paper presents a machine learning 
approach based on an SVM classifier 
coupled with preprocessing rules for cross-
document named entity normalization.  The 
classifier uses lexical, orthographic, 
phonetic, and morphological features.  The 
process involves disambiguating different 
entities with shared name mentions and 
normalizing identical entities with different 
name mentions.  In evaluating the quality of 
the clusters, the reported approach achieves 
a cluster F-measure of 0.93.  The approach 
is significantly better than the two baseline 
approaches in which none of the entities are 
normalized or entities with exact name 
mentions are normalized.  The two baseline 
approaches achieve cluster F-measures of 
0.62 and 0.74 respectively.  The classifier 
properly normalizes the vast majority of 
entities that are misnormalized by the 
baseline system. 
1. Introduction: 
Much recent attention has focused on the 
extraction of salient information from unstructured 
text.  One of the enabling technologies for 
information extraction is Named Entity 
Recognition (NER), which is concerned with 
identifying the names of persons, organizations, 
locations, expressions of times, quantities, ... etc. 
(Chinchor, 1999; Maynard et al, 2001;  Sekine, 
2004; Joachims, 2002).  The NER task is 
challenging due to the ambiguity of natural 
language and to the lack of uniformity in writing 
styles and vocabulary used across documents 
(Solorio, 2004). 
Beyond NER, considerable work has focused 
on the tracking and normalization of entities that 
could be mentioned using different names (e.g. 
George Bush, Bush) or nominals (e.g. the 
president, Mr., the son) (Florian et al, 2004).  
Most of the named entity tracking work has 
focused on intra-document normalization with 
very limited work on cross-documents 
normalization. 
Recognizing and tracking entities of type 
?Person Name? are particularly important for 
information extraction.  Yet they pose interesting 
challenges that require special attention.  The 
problems can result from: 
1. A Person?s name having many variant spellings 
(especially when it is transliterated into a 
foreign language).  These variations are 
typically limited in the same document, but are 
very common across different documents from 
different sources (e.g. Mahmoud Abbas = 
Mahmod Abas, Mohamed El-Baradei = 
Muhammad AlBaradey ? etc). 
2. A person having more than one name (e.g. 
Mahmoud Abbas = Abu Mazen). 
3. Some names having very similar or identical 
names but refer to completely different persons 
(George H. W. Bush ? George W. Bush). 
4. Single token names (e.g. Bill Clinton = Clinton 
? Hillary Clinton). 
This paper will focus on Arabic cross-document 
normalization of named entities of type ?person 
name,? which would involve resolving the 
aforementioned problems.  As illustrated in Figure 
1, the task involves normalizing a set of person 
entities into a set of classes each of which is 
25
formed of at least one entity.  For N input entities, 
the output of normalization process will be M 
classes, where M ? N.  Each class would refer to 
only one person and each class would contain all 
entities referring to that person. 
For this work, intra-document normalization is 
assumed and an entity refers to a normalized set of 
name mentions and nominals referring to a single 
person in a single document.  Florian et al (2004) 
were kind enough to provide the authors access to 
an updated version of their state-of-the-art Named 
Entity Recognition and Tracking (NERT) system, 
which achieves an F-measure of 0.77 for NER, 
and an F-measure of 0.88 for intra-document 
normalization assuming perfect NER.  Although 
the NERT systems is efficient for relatively short 
documents, it is computational impractical for 
large documents, which precludes using the NERT 
system for cross-document normalization through 
combining the documents into one large 
document.  The main challenges of this work stem 
from large variations in the spelling of 
transliterated foreign names and the presence of 
many common Arabic names (such as 
Muhammad, Abdullah, Ahmed ?etc.), which 
increases the ambiguity in identifying the person 
referred to by the mentioned name.  Further, the 
NERT system output system contains many NER 
errors and intra-document normalization errors. 
In this paper, cross-document normalization 
system employs a two-step approach.  In the first 
step, preprocessing rules are used to remove errant 
named entities.  In the second step, a support 
vector machine (SVM) classifier is used to 
determine if two entities from two different 
documents need to be normalized.  The classifier 
is trained on lexical, orthographic, phonetic, and 
morphological features. 
The paper is organized as follows: Section 2 
provides a background on cross-document NE 
normalization; Section 3 describes the 
preprocessing steps and data used for training and 
testing;  Section 4 describes the normalization 
methodology; Section 5 describes the 
experimental setup;  Section 6 reports and 
discusses experimental results;  and Section 7 
concludes the paper and provides possible future 
directions. 
2. Background 
While considerable work has focused on named 
entity normalization within a single document, 
little work has focused on the challenges 
associated with resolving person name references 
across multiple documents.  Most of the work 
done in cross-document normalization focused on 
the problem of determining if two instances with 
the same name from different documents referring 
to the same person (Fleischman and Hovy, 2004).  
Fleischman and Hovy (2004) focused on 
distinguishing between individuals having 
identical names, but they did not extend 
normalization to different names referring to the 
same individual.  Their task is a subtask of what is 
examined in this paper.  They used a large number 
of features to accomplish their work, depending 
mostly on language specific dictionaries and 
wordnet.  Some these resources are not available 
for Arabic and many other languages.  Mann and 
Yarowsky (Mann and Yarowsky, 2003) examined 
the same problem but they treated it as a clustering 
task.  They focused on information extraction to 
build biographical profiles (date of birth, place of 
birth, etc.), and they wanted to disambiguate 
biographies belonging to different authors with 
identical names. 
Dozier and Zielund (Dozier and Zielund, 2004) 
reported on cross-document person name 
normalization in the legal domain.  They used a 
 
Figure 1 Normalization Model 
E1 
E3 
E7 E5 
E2 
E4 E6 
E8 
 
Normalization 
E1 
E4 E8 
E2 
E3 E7 
E5 
E6 
26
finite state machine that identifies paragraphs in a 
document containing the names of attorneys, 
judges, or experts and a semantic parser that 
extracts from the paragraphs template information 
about each named individual.  They relied on 
reliable biographies for each individual.  A 
biography would typically contain a person?s first 
name, middle name, last name, firm, city, state, 
court, and other information.  They used a 
Bayesian network to match the name mentions to 
the biographical records. 
Bhattacharya and Getoor (Bhattacharya and 
Getoor, 2006) introduced a collective decision 
algorithm for author name entity resolution, where 
decisions are not considered on an independent 
pairwise basis.  They focused on using relational 
links among the references and co-author 
relationships to infer collaboration groups, which 
would disambiguate entity names.  Such explicit 
links between co-authors can be extracted directly.  
However, implicit links can be useful when 
looking at completely unstructured text.  Other 
work has extended beyond entities of type ?person 
name? to include the normalization of location 
names (Li et al, 2002) and organizations (Ji and 
Grishman. 2004). 
3.  Preprocessing  and the Data Set 
For this work, a set of 7,184 person name entities 
was constructed.  Building new training and test 
sets is warranted, because the task at hand is 
sufficiently different from previously reported 
tasks in the literature.  The entities were 
recognized from 2,931 topically related documents 
(relating to the situation in the Gaza and Lebanon 
during July of 2006) from different Arabic news 
sources (obtained from searching the Arabic 
version of news.google.com).  The entities were 
recognized and normalized (within document) 
using the NERT system of Florian et al(2004).  
As shown in Figure 2, each entity is composed of 
a set of name mentions (one or more) and a set of 
nominal mentions (zero or more).  
The NERT system achieves an F-score of 0.77 
with precision of 0.82 and recall of 0.73 for person 
name mention and nominal recognition and an F-
score of 0.88 for tracking (assuming 100% 
recognition accuracy).  The produced entities may 
suffer from the following: 
1. Errant name mentions: Two name mentions 
referring to two different entities are 
concatenated into an errant name mention (e.g. 
?Bush Blair?, ?Ahmadinejad Bush?).  These 
types of errors stem from phrases such as ?The 
meeting of Bush Blair? and generally due to 
lack of sufficient punctuation marks. 
2. NE misrecognitions: Regular words are 
recognized as person name mentions and are 
embedded into person entities (e.g. Bush = 
George Bush = said). 
3. Errant entity tracking: name mentions of 
different entities are recognized as different 
mentions of the same entity (e.g. Bush = 
Clinton = Ahmadinejad). 
4. Lack of nominal mentions: Many entities do 
not contain any nominal mentions, which 
increases the entity ambiguity (especially 
when there is only one name mention 
composed of a single token).  
To overcome these problems, entities were 
preprocessed as follows: 
1. Errant name mentions such as ?Bush Blair? 
were automatically removed.  In this step, a 
dictionary of person name mentions was built 
from the 2,931 documents collection from 
which the entities were recognized and 
normalized along with the frequency of 
appearance in the collection.  For each entity, 
all its name mentions are checked in the 
dictionary and their frequencies are compared 
to each other.  Any name mention with a 
frequency less than 1/30 of the frequency of 
the name mention with the highest frequency 
is automatically removed (1/30 was picked 
based on manual examination of the training 
set).   Figure 2 Entity Description  
27
2. Name mentions formed of a single token 
consisting of less than 3 characters are 
removed.  Such names are almost always 
misrecognized name entities. 
3. Name entities with 10 or more different name 
mentions are automatically removed.  The 
NERT system often produces entities that 
include many different name mentions 
referring to different persons as one.  Such 
entities are errant because they over normalize 
name mentions.  Persons are referred to using 
a limited number of name mentions. 
4. Nominal mentions are stemmed using a 
context sensitive Arabic stemmer (Lee et al 
2003) to overcome the morphological 
complexity of Arabic.  For example, ?JKL?? = 
?president?, ? O?JKLQ ? = ?the president?, 
? O??JKLQ ? = ?and the president?, ? SKL?TU ? = ?its 
presidents? ? etc are stemmed to ?JKL?? = 
?president?.  
 
Cross-document entities are compared in a 
pairwise manner and binary decision is taken on 
whether they are the same.  Therefore, the 
available 7,184 entities lead to nearly 26 million 
pairwise comparisons (For N entities, the number 
of pair wise comparisons = 2
)1( ?NN ). 
Entity pairs were chosen to be included in the 
training set if they match any of the following 
criteria: 
1. Both entities have one shared name mention. 
2. Both entities have shared nominal mentions. 
3. A name mention in one of the entities is a 
substring of a name mention in the other 
entity. 
4. Both entities have nearly identical name 
mentions (small edit distance between both 
mentions). 
The resulting set was composed of 19,825 
pairs, which were manually judged to determine if 
they should be normalized or not.  These criteria 
skew the selection of pairs towards more 
ambiguous cases, which would be better 
candidates to train the intended SVM classifier, 
where the items near the boundary dividing the 
hyperplane are the most important.  For the 
training set, 18,503 pairs were normalized, and 
1,322 pairs were judged as different.  
Unfortunately, the training set selection criteria 
skewed the distribution of training examples 
heavily in favor of positive examples.  It would 
interesting to examine other training sets where 
the distribution of positives and negatives is 
balanced or skewed in favor of negatives. 
The test set was composed of 470 entities that 
were manually normalized into 253 classes, of 
which 304 entities were normalized to 87 classes 
and 166 entities remained unnormalized (forming 
single-entity classes).  Using 470 entities leads to 
110,215 pairwise comparisons.  The test set, which 
was distinct from the training set, was chosen 
using the same criteria as the training set.  Further, 
all duplicate (identical) entities were removed 
from the test set.  The selection criteria insure that 
the test set is skewed more towards ambiguous 
cases.  Randomly choosing entities would have 
made the normalization too easy.   
4. Normalization Methodology 
SVMLight, an SVM classifier (Joachims, 2002), 
was used for classification with a linear kernel and 
default parameters.  The following training 
features were employed: 
1. The percentage of shared name mentions 
between two entities calculated as: 
Name Commonality = 
? ??>< ???
?
???
?
namescommon j
i
j
i
f
f
f
f
2
2
1
1 ,min  
 where f1i is the frequency of the shared name 
mention in first entity, and f2i is the frequency 
of the shared name mention in the second 
entity.  ? f1i is the number of name mentions 
appearing in the entity. 
2. The maximum number of tokens in the shared 
name mentions, i.e. if there exists more than 
one shared name mention then this feature is 
the number of tokens in the longest shared 
name mention. 
3. The percentage of shared nominal mentions 
between two entities, and it is calculated as the 
name commonality but for nominal mentions.  
4. The smallest minimum edit distance 
(Levenshtein distance with uniform weights) 
between any two name mentions in both 
entities (Cohen et al, 2003) and this feature is 
only enabled when name commonality 
between both entities equals to zero. 
28
5. Phonetic edit distance, which is similar to edit 
distance except that phonetically similar 
characters, namely {(? ? t, ? ? T), (? ? k, ? ? 
q),(? ? d, ? ? D),(? ? v, ? ? s, ? ? S), (? ? *, 
? ? z, ? ? Z),(? ? j, ? ? g),(i?  ? p, k? ? h),(? ? <, 
n ? |, ? ,< ? ? ? A)1}, are normalized, vowels are 
removed, and spaces between tokens are 
removed. 
6. The number of tokens in the pair of name 
mentions that lead to the minimum edit 
distance. 
Some of the features might seem duplicative.  
However, the edit distance and phonetic edit 
distance are often necessary when names are 
transliterated into Arabic and hence may have 
different spellings and consequently no shared 
name mentions.  Conversely, given a shared name 
mention between a pair of entities will lead to zero 
edit distance, but the name commonality may also 
be very low indicating two different persons may 
have a shared name mention.  For example 
?Abdullah the second? and ?Abdullah bin 
Hussein? have the shared name mention 
?Abdullah? that leads to zero edit distance, but 
they are in fact two different persons.  In this case, 
the name commonality feature can be indicative of 
the difference.  Further, nominals are important in 
differentiating between identical name mentions 
that in fact refer to different persons (Fleischman 
and Hovy, 2004).  The number of tokens feature 
indicates the importance of the presence of 
similarity between two name mentions, as the 
similarity between name mentions formed of one 
token cannot be indicative for similarity when the 
number of tokens is more than one. 
Further, it is assumed that entities are transitive 
and are not available all at once, but rather the 
system has to normalize entities incrementally as 
they appear.  Therefore, for a given set of entity 
pairs, if the classifier deems that Entityi = Entityj 
and Entityj = Entityk, then Entityi is set to equal 
Entityk even if the classifier indicates that Entityi ? 
Entityk, and all entities (i, j, and k) are merged into 
one class.   
                                                 
1 Buckwalter transliteration scheme is used throughout 
the paper 
5. Experimental Setup 
Two baselines were established for the 
normalization process.  In the first, no entities are 
normalized, which produces single entity classes 
(?no normalization? condition).  In the second, any 
two entities having two identical name mentions in 
common are normalized (?surface normalization? 
condition).  For the rest of the experiments, focus 
was given to two main issues: 
1. Determining the effect of the different features 
used for classification. 
2. Determining the effect of varying the number 
of training examples.  
To determine the effect of different features, 
multiple classifiers were trained using different 
features, namely: 
? All features: all the features mentioned above 
are used,  
? Edit distance removed:  edit distance features 
(features 4, 5, and 6) are removed,  
? Number of tokens per name mention removed:  
the number of shared tokens and the number 
of tokens leading to the least edit distance 
(features 2 and 6) are removed.   
To determine the effect of training examples, 
the classifier was trained using all features but 
with a varying number of training example pairs, 
namely all 19,825 pairs, a set of randomly picked 
5,000 pairs, and a set of randomly picked 2,000 
pairs.   
For evaluation, 470 entities in test set were 
normalized into set of classes with different 
thresholds for the SVM classifier.  The quality of 
the clusters was evaluated using purity, entropy, 
and Cluster F-measure (CF-measure) in the 
manner suggested by Rosell et al (2004).  For the 
cluster quality measures, given cluster i (formed 
using automatic normalization) and each cluster j 
(reference normalization formed manually), cluster 
precision (p) and recall (r) are computed as 
follows: 
i
ij
ij n
n
p = , and 
j
ij
ij n
n
r = , where ni number of 
entities in cluster i, nj number of entities in cluster 
j, and nij number of shared entities between cluster 
i and j.  
The CF-measure for an automatic cluster i 
against a manually formed reference cluster j is:  
29
ijij
ijij
ij pr
pr
CF +
??= 2 , and the CF-measure for a 
reference cluster j is: 
}{max ijij CFCF = .  
The final CF-measure is computed over all the 
reference clusters as follows: ?= j jij CFn
n
CF . 
Purity of (?i) of an automatically produced 
cluster i is the maximum cluster precision obtained 
when comparing it with all the reference clusters 
as follows: }{max ijji p=? , and the weighted 
average purity over all clusters is: 
?= i i
i
ij
n
n ?? , where n is the total number of 
entities in the set to be normalized (470 in this 
case). 
As for entropy of a cluster, it is calculated as:  
??= j ijiji ppE log , and the average entropy 
as: 
?= i i
i
i E
n
nE . 
The CF-measure captures both precision and 
recall while purity and entropy are precision 
oriented measures (Rosell et al, 2004). 
6. Results and Discussion 
Figure 3 shows the purity and CF-measure for the 
two baseline conditions (no normalization, and 
surface normalization) and for the normalization 
system with different SVM thresholds.  Since 
purity is a precision measure, purity is 100% when 
no normalization is done.  The CF-measure is 62% 
and 74% for baseline runs with no normalization 
and surface normalization respectively.  As can be 
seen from the results, the baseline run based on 
exact matching of name mentions in entities 
achieves low CF-measure and low purity.  Low 
CF-measure values stem from the inability to 
match identical entities with different name 
mentions, and the low purity value stems from not 
disambiguating different entities with shared name 
mentions.  Some notable examples where the 
surface normalization baseline failed include:  
1. The normalization of the different entities 
referring to the Israeli soldier who is 
imprisoned in Gaza with different Arabic 
spellings for his name, namely ?tKuv ?Twux? 
(jlEAd $lyT), ?tKOTv ?Twux? (jlEAd $AlyT), 
?zKuv ?|}~O?? (the soldier $lyt), and so forth.  
2. The separation between ??T?O?? ? |?? ?u?O?? 
(King Abdullah the Second) and ? ?? ?? |?? ?u?O?
???wO? |??? (King Abdullah ibn Abdul-Aziz) 
that have a shared name mention ???|?? ?u?O?? 
(King Abdullah). 
3. The normalization of the different entities 
representing the president of Palestinian 
Authority with different name mentions, 
namely ???T? ???? (Abu Mazen) and ? ?????
?T??? (Mahmoud Abbas).   
 
The proposed normalization technique 
properly normalized the aforementioned examples.  
Given different SVM thresholds, Figure 3 shows 
that the purity of resultant classes increases as the 
SVM threshold increases since the number of 
normalized entities decreases as the threshold 
increases.  The best CF-measure of 93.1% is 
obtained at a threshold of 1.4 and as show in Table 
1 the corresponding purity and entropy are 97.2% 
and 0.056 respectively.  The results confirm the 
success of the approach. 
Table 1 highlights the effect of removing 
different training feature and the highest CF-
measures (at different SVM thresholds) as a result.  
The table shows that using all 6 features produced 
the best results and the removal of the shared 
names and tokens (features 2 and 6) had the most 
adverse effect on normalization effectiveness.  The 
adverse effect is reasonable especially given that 
some single token names such as ?Muhammad? 
and ?Abdullah? are very common and matching 
one of these names across entities is an insufficient 
indicator that they are the same.  Meanwhile, the 
exclusion of edit distance features (features 4, 5, 
and 6) had a lesser but significant adverse impact 
on normalization effectiveness.  Table 1 reports 
the best results obtained using different thresholds.  
Perhaps, a separate development set should be 
used for ascertaining the best threshold. 
Table 2 shows that decreasing the number of 
training examples (all six features are used) has a 
noticeable but less pronounced effect on 
normalization effectiveness compared to removing 
training features. 
30
 
Table 1 Quality of clusters as measured by purity (higher values are better), entropy (lower values are 
better), and CF-measure (higher values are better) for different feature sets.  Values are shown for max 
CF-measure.  Thresholds were tuned for max CF-measure for each feature configuration separately 
Training Data Purity Maximum CF-Measure Entropy Threshold 
No Normalization 100.0% 62.6% 0.000 - 
Baseline 83.4% 74.7% 0.151 - 
All Features 97.2% 93.1% 0.056 1.4 
Edit Distance removed 99.4% 85.5% 0.010 1.0 
# of tokens/name removed 96.6% 77.8% 0.071 1.5 
 
Normalization Evaluation
60%
65%
70%
75%
80%
85%
90%
95%
100%
No
No
rm
al
iza
tio
n
Ba
se
lin
e
1.
0
1.
1
1.
2
1.
3
1.
4
1.
5
1.
6
1.
7
1.
8
1.
9
2.
0
SVM Threshold
Purity
CF-Measure
 
Figure 3 Purity and cluster F-measure versus SVM Threshold 
 
Table 2 Effect of number of training examples on normalization effectiveness 
Training Data Purity Maximum CF-Measure Entropy Threshold 
20k training pairs 97.2% 93.1% 0.056 1.4 
5k training pairs 97.4% 90.5% 0.053 1.5 
2k training pairs 98.5% 90.3% 0.031 1.6 
 
7. Conclusion: 
This paper presented a two-step approach to cross-
document named entity normalization.  In the first 
step, preprocessing rules are used to remove errant 
named entities.  In the second step, a machine 
learning approach based on an SVM classifier to 
disambiguate different entities with matching 
name mentions and to normalize identical entities 
with different name mentions.  The classifier was 
trained on features that capture name mentions and 
nominals overlap between entities, edit distance, 
and phonetic similarity.  In evaluating the quality 
of the clusters, the reported approach achieved a 
cluster F-measure of 0.93.  The approach 
outperformed that two baseline approaches in 
which no normalization was done or normalization 
was done when two entities had matching name 
31
mentions.  The two approaches achieved cluster F-
measures of 0.62 and 0.74 respectively. 
For future work, implicit links between entities 
in the text can serve as the relational links that 
would enable the use of entity attributes in 
conjunction with relationships between entities.  
An important problem that has not been 
sufficiently explored is cross-lingual cross-
document normalization.  This problem would 
pose unique and interesting challenges.  The 
described approach could be generalized to 
perform normalization of entities of different types 
across multilingual documents.  Also, the 
normalization problem was treated as a 
classification problem.  Examining the problem as 
a clustering (or alternatively an incremental 
clustering) problem might prove useful.  Lastly, 
the effect of cross-document normalization should 
be examined on applications such as information 
extraction, information retrieval, and relationship 
and social network visualization.   
References: 
Bhattacharya I. and Getoor L. ?A Latent Dirichlet 
Allocation Model for Entity Resolution.? 6th SIAM 
Conference on Data Mining (SDM), Bethesda, USA, 
April 2006. 
Chinchor N., Brown E., Ferro L., and Robinson P.  
?Named Entity Recognition Task Definition.? 
MITRE, 1999. 
Cohen W., Ravikumar P., and Fienberg S. E. ?A 
Comparison of String Distance Metrics for Name-
Matching Tasks.?  In Proceedings of the 
International Joint Conference on Artificial 
Intelligence, 2003. 
Dozier C. and Zielund T. ?Cross-document Co-
Reference Resolution Applications for People in the 
Legal Domain.? In 42nd Annual Meeting of the 
Association for Computational Linguistics, 
Reference Resolution Workshop, Barcelona, Spain. 
July 2004. 
Fleischman M. B. and Hovy E. ?Multi-Document 
Person Name Resolution.?  In 42nd Annual Meeting 
of the Association for Computational Linguistics, 
Reference Resolution Workshop, Barcelona, Spain. 
July 2004. 
Ji H. and Grishman R. ?Applying Coreference to 
Improve Name Recognition?. In 42nd Annual 
Meeting of the Association for Computational 
Linguistics, Reference Resolution Workshop, 
Barcelona, Spain. July (2004). 
Ji H. and Grishman R. "Improving Name Tagging by 
Reference Resolution and Relation Detection." ACL 
2005 
Joachims T. ?Learning to Classify Text Using Support 
Vector Machines.? Ph.D. Dissertation, Kluwer, 
(2002). 
Joachims T. ?Optimizing Search Engines Using Click-
through Data.?  Proceedings of the ACM Conference 
on Knowledge Discovery and Data Mining (KDD), 
(2002).  
Lee Y. S., Papineni K., Roukos S., Emam O., Hassan 
H. ?Language Model Based Arabic Word 
Segmentation.?  In ACL 2003, pp. 399-406, (2003). 
Li H., Srihari R. K., Niu C., and Li W. ?Location 
Normalization for Information Extraction.?  
Proceedings of the 19th international conference on 
Computational linguistics, pp. 1-7, 2002 
Li H., Srihari R. K., Niu C., and Li W. ?Location 
Normalization for Information Extraction.?  
Proceedings of the sixth conference on applied 
natural language processing, 2000. pp. 247 ? 254. 
Mann G. S. and Yarowsky D. ?Unsupervised Personal 
Name Disambiguation.? Proceedings of the seventh 
conference on Natural language learning at HLT-
NAACL 2003.  pp. 33-40. 
Maynard D., Tablan V., Ursu C., Cunningham H., and 
Wilks Y. ?Named Entity Recognition from Diverse 
Text Types.? Recent Advances in Natural Language 
Processing Conference, (2001). 
Palmer D. D. and Day D. S. ?A statistical Profile of the 
Named Entity Task?.  Proceedings of the fifth 
conference on Applied natural language processing, 
pp. 190-193, (1997). 
R. Florian R., Hassan H., Ittycheriah A., Jing H., 
Kambhatla N., Luo X., Nicolov N., and Roukos S. 
?A Statistical Model for Multilingual Entity 
Detection and Tracking.?  In HLT-NAACL, 2004. 
Rosell M., Kann V., and Litton J. E.  ?Comparing 
Comparisons: Document Clustering Evaluation 
Using Two Manual Classifications.?  In ICON 2004 
Sekine S. ?Named Entity: History and Future?.  Project 
notes, New York University, (2004). 
Solorio T. ?Improvement of Named Entity Tagging by 
Machine Learning.?  Ph.D. thesis, National Institute 
of Astrophysics, Optics and Electronics, Puebla, 
Mexico, September 2005. 
 
32
BioNLP 2007: Biological, translational, and clinical language processing, pages 89?96,
Prague, June 2007. c?2007 Association for Computational Linguistics
BioNoculars: Extracting Protein-Protein Interactions from Biomedical Text
Amgad Madkour, *Kareem Darwish, Hany Hassan, Ahmed Hassan, Ossama Emam
Human Language Technologies Group
IBM Cairo Technology Development Center
P.O.Box 166 El-Ahram, Giza, Egypt
{amadkour,hanyh,hasanah,emam}@eg.ibm.com,*kareem@darwish.org
Abstract
The vast number of published medical doc-
uments is considered a vital source for rela-
tionship discovery. This paper presents a sta-
tistical unsupervised system, called BioNoc-
ulars, for extracting protein-protein interac-
tions from biomedical text. BioNoculars
uses graph-based mutual reinforcement to
make use of redundancy in data to construct
extraction patterns in a domain independent
fashion. The system was tested using MED-
LINE abstract for which the protein-protein
interactions that they contain are listed in the
database of interacting proteins and protein-
protein interactions (DIPPPI). The system
reports an F-Measure of 0.55 on test MED-
LINE abstracts.
1 Introduction
With the ever-increasing number of published
biomedical research articles and the dependency
of new research and previously published research,
medical researchers and practitioners are faced with
the daunting prospect of reading through hundreds
or possibly thousands of research articles to sur-
vey advances in areas of interest. Much work has
been done to ease access and discovery of articles
that match the interest of researchers via the use
of search engines such as PubMed, which provides
search capabilities over MEDLINE, a collection of
more than 15 million journal paper abstracts main-
tained by the National Library of Medicine (NLM).
However, with the addition of abstracts from more
than 5,000 medical journals to MEDLINE every
year, the number of articles containing information
that is pertinent to users needs has grown consider-
ably. These 5,000 journals constitute only a subset
of the published biomedical research. Further, med-
ical articles often contain redundant information and
only subsections of articles are typically of direct in-
terest to researchers. More advanced information
extraction tools have been developed to effectively
distill medical articles to produce key pieces of in-
formation from articles while attempting to elimi-
nate redundancy. These tools have focused on areas
such as protein-protein interaction, gene-disease re-
lationship, and chemical-protein interaction (Chun
et al, 2006). Many of these tools have been used
to extract key pieces of information from MED-
LINE. Most of the reported information extraction
approaches use sets of handcrafted rules in conjunc-
tion with manually curated dictionaries and ontolo-
gies.
This paper presents a fully unsupervised statisti-
cal technique to discover protein-protein interaction
based on automatically discoverable repeating pat-
terns in text that describe relationships. The paper
is organized as follows: section 2 surveys related
work; section 3 describes BioNoculars; Section 4
describes the employed experimental setup; section
5 reports and comments on experimental results; and
section 6 concludes the paper.
2 Background
The background will focus primarily on the tagging
of Biomedical Named Entities (BNE), such genes,
gene-products, proteins, and chemicals and the Ex-
89
traction of protein-protein interactions from text.
2.1 BNE Tagging
Concerning BNE tagging, the most common ap-
proaches are based on hand-crafted rules, statisti-
cal classifiers, or a hybrid of both (usually in con-
junction with dictionaries of BNE). Rule-based sys-
tems (Fukuda et al, 1998; Hanisch et al, 2003; Ya-
mamoto et al, 2003) that use dictionaries tend to
exhibit high precision in tagging named entities but
generally with lower tagging recall. They tend to
lag the latest published research and are sensitive
to the expression of the named entities. Dictionar-
ies of BNE are typically laborious and expensive to
build, and they are dependant on nomenclatures and
specific species. Statistical approaches (Collier et
al., 2000; Kazama et al, 2002; Settles, 2004) typ-
ically improve recall at the expense of precision,
but are more readily retargetable for new nomen-
clatures and organisms. Hybrid systems (Tanabe
and Wilbur, 2002; Mika and Rost, 2004) attempt to
take advantage of both approaches. Although these
approaches tend to generate acceptable recognition,
they are heavily dependent on the type of data on
which they are trained.
(Fukuda et al, 1998) proposed a rule-based pro-
tein name extraction system called PROPER (PRO-
tein Proper-noun phrase Extracting Rules) system,
which utilizes a set of rules based on the surface
form of text in conjunction with a Part-Of-Speech
(POS) tagging to identify what looks like a protein
without referring to any specific BNE dictionary.
They reported a 94.7% precision and a 98.84% re-
call for the identification of BNEs. The results that
they achieved seem to be too specific to their train-
ing and test sets.
(Hanisch et al, 2003) proposed a rule-based
protein and gene name extraction system called
ProMiner, which is based on the construction of a
general-purpose dictionary along with different dic-
tionaries of synonyms and an automatic curation
procedure based on a simple token model of protein
names. Results showed that their system achieved a
0.80 F-measure score in the name extraction task on
the BioCreative test set (BioCreative).
(Yamamoto et al, 2003) proposed the use of mor-
phological analysis to improve protein name tag-
ging. Their approach tags proteins based on mor-
pheme chunking to properly determine protein name
boundary. They used the GENIA corpus for training
and testing and obtained an F-measure score of 0.70
for protein name tagging.
(Collier et al, 2000) used a machine learning ap-
proach to protein name extraction based on a linear
interpolation Hidden Markov Model (HMM) trained
using bi-grams. They focused on finding the most
likely protein sequence classes (C) for a given se-
quence of words (W), by maximizing the probabil-
ity of C given W, P(C?W). Unlike traditional dic-
tionary based methods, the approach uses no manu-
ally crafted patterns. However, their approach may
misidentify term boundaries for phrases containing
potentially ambiguous local structures such as co-
ordination and parenthesis. They reported an F-
measure score of 0.73 for different mixtures of mod-
els tested on 20 abstracts.
(Kazama et al, 2002) proposed a machine learn-
ing approach to BNE tagging based on support vec-
tor machines (SVM), which was trained on the GE-
NIA corpus. Their preliminary results of the system
showed that the SVM with the polynomial kernel
function outperforms techniques of Maximum En-
tropy based systems.
Yet another BNE tagging system is ABNER (Set-
tles, 2005), which utilizes machine learning, namely
conditional random fields, with a variation of or-
thographic and contextual features and no seman-
tic or syntactic features. ABNER achieves an F-
measure score of 0.71 on the NLPA 2004 shared
task dataset corpus and 0.70 on the BioCreative cor-
pus.and scored an F1-measure of 51.8set.
(Tanabe and Wilbur, 2002) used a combination
of statistical and knowledge-based strategies, which
utilized automatically generated rules from transfor-
mation based POS tagging and other generated rules
from morphological clues, low frequency trigrams,
and indicator terms. A key step in their method is
the extraction of multi-word gene and protein names
that are dominant in the corpus but inaccessible to
the POS tagger. The advantage of such an approach
is that it is independent of any biomedical domain.
However, it can miss single word gene names that
do not occur in contextual gene theme terms. It
can also incorrectly tag compound gene names, plas-
mids, and phages.
(Mika and Rost, 2004) developed NLProt, which
90
combines the use of dictionaries, rules-based filter-
ing, and machine learning based on an SVM classi-
fier to tag protein names in MEDLINE. The NLProt
system used rules for pre-filtering and the SVM for
classification, and it achieved a precision of 75% and
recall 76%.
2.2 Relationship Extraction
As for the extraction of interactions, most efforts in
extraction of biomedical interactions between enti-
ties from text have focused on using rule-based ap-
proaches due to the familiarity of medical terms that
tend to describe interactions. These approaches have
proven to be successful with notably good results. In
these approaches, most researchers attempted to de-
fine an accurate set of rules to describe relationship
types and patterns and to build ontologies and dic-
tionaries to be consulted in the extraction process.
These rules, ontologies, and dictionaries are typi-
cally domain specific and are often not generalizable
to other problems.
(Blaschke et al, 1999) reported a domain spe-
cific approach for extracting protein-protein interac-
tions from biomedical text based on a set of pre-
defined patterns and words describing interactions.
Later work attempted to automatically extract inter-
actions, which are referenced in the database of in-
teracting proteins (Xenarios et al, 2000), from the
text mentioning the interactions (Blaschke and Va-
lencia, 2001). They achieved surprisingly low recall
(25%), which they attributed to problems in properly
identifying protein names in the text.
(Koike et al, 2005) developed a system called
PRIME, which was used to extract biological func-
tions of genes, proteins, and their families. Their
system used a shallow parser and sentence struc-
ture analyzer. They extracted so-called ACTOR-
OBJECT relationships from the shallow parsed sen-
tences using rule based sentence structure analysis.
The identification of BNEs was done by consulting
the GENA gene name dictionary and family name
dictionary. In extracting the biological functions of
genes and proteins, their system reported a recall of
64% and a precision of 94%.
Saric et al developed a system to extract gene
expression regulatory information in yeast as well
as other regulatory mechanisms such phosphoryla-
tion (Saric et al, 2004; Saric et al, 2006). They
used a rule based named entity recognition module,
which recognizes named entities via cascading finite
state automata. They reported a precision of 83-90%
and 86-95% for the extraction of gene expression
and phosphorylation regulatory information respec-
tively.
(Leroy and Chen, 2005) used linguistic parsers
and Concept Spaces, which use a generic co-
occurrence based technique that extracts relevant
medical phrases using a noun chunker. Their system
employed UMLS (Humphreys and Lindberg, 1993),
GO (Ashburner et al, 2000), and GENA (Koike and
Takagi, 2004) to further improve extraction. Their
main purpose was entity identification and cross ref-
erence to other databases to obtain more knowledge
about entities involved in the system.
Other extraction approaches such as the one re-
ported on by (Cooper and Kershenbaum, 2005) uti-
lized a large manually curated dictionary of many
possible combinations of gene/protein names and
aliases from different databases and ontologies.
They annotated their corpus using a dictionary-
based longest matching technique. In addition, they
used filtering with a maximum entropy based named
entity recognizer in order to remove the false posi-
tives that were generated from merging databases.
The problem with this approach is the resulting in-
consistencies from merging databases, which could
hurt the effectiveness of the system. They reported
a recall of 87.1 % and a precision of 78.5% in the
relationship extraction task.
Work by (Mack et al, 2004) used the Munich In-
formation Center for Protein Sequences (MIPS) for
entity identification. Their system was integrated in
the IBM Unstructured Information Management Ar-
chitecture (UIMA) framework (Ferrucci and Lally,
2004) for tokenization, identification of entities, and
extraction of relations. Their approach was based on
a combination of computational linguistics, statis-
tics, and domain specific rules to detect protein in-
teractions. They reported a recall of 61% and a pre-
cision of 97%.
(Hao et al, 2005) developed an unsupervised ap-
proach, which also uses patterns that were deduced
using minimum description lengths. They used pat-
tern optimization techniques to enhance the patterns
by introducing most common keywords that tend to
describe interactions.
91
(Jo?rg et. al., 2005) developed Ali Baba which
uses sequence alignments applied to sentences an-
notated with interactions and part of speech tags.It
also uses finite state automata optimized with a ge-
netic algorithm in its approach. It then matches the
generated patterns against arbitrary text to extract in-
teractions and their respective partners. The system
scored an F1-measure of 51.8% on the LLL?05 eval-
uation set.
The aforementioned systems used either rule-
based approaches, which require manual interven-
tion from domain experts, or statistical approaches,
either supervised or semi-supervised, which also re-
quire manually curated training data.
3 BioNoculars
BioNoculars is a relationship extraction system that
based on a fully unsupervised technique suggested
by (Hassan et al, 2006) to automatically extract
protein-protein interaction from medical articles. It
can be retargeted to different domains such as pro-
tein interactions in diseases. The only requirement
is to compile domain specific taggers and dictionar-
ies, which would aid the system in performing the
required task.
The approach uses an unsupervised graph-based
mutual reinforcement, which depends on the con-
struction of generalized extraction patterns that
could match instances of relationships (Hassan et
al., 2006). Graph-based mutual reinforcement is
similar to the idea of hubs and authorities in web
pages depicted by the HITS algorithm (Kleinberg,
1998). The basic idea behind the algorithm is that
the importance of a page increases when more and
more good pages link to it. The duality between pat-
terns and extracted information (tuples) leads to the
fact that patterns could express different tuples, and
tuples in turn could be expressed by different pat-
terns. Tuple in this context contains three elements,
namely two proteins and the type of interaction be-
tween them. The proposed approach is composed of
two main steps, namely initial pattern construction
and then pattern induction.
For pattern construction, the text is POS tagged
and BNE tagged. The tags of Noun Phrases or se-
quences of nouns that constitute a BNE are removed
and replaced with a BNE tag. Then, an n-gram lan-
guage model is built on the tagged text (using tags
only) and is used to construct weighted finite state
machines. Paths with low cost (high language model
probabilities) are chosen to construct the initial set
of patterns; the intuition is that paths with low cost
(high probability) are frequent and could represent
potential candidate patterns. The number of candi-
date initial patterns could be reduced significantly
by specifying the candidate types of entities of in-
terest. In the case of BioNoculars, the focus was
on relationships between BNEs of type PROTEIN.
The candidate patterns are then applied to the tagged
stream to produce in-sentence relationship tuples.
As for pattern induction, due to the duality in the
patterns and tuples relation, patterns and tuples are
represented by a bipartite graph as illustrated in Fig-
ure 1.
Figure 1: A bipartite graph representing patterns and
tuples
Each pattern or tuple is represented by a node in
the graph. Edges represent matching between pat-
terns and tuples. The pattern induction problem can
be formulated as follows: Given a very large set of
data D containing a large set of patterns P, which
match a large set of tuples T, the problem is to iden-
tify , which is the set of patterns that match the set
of the most correct tuples T. The intuition is that
the tuples matched by many different patterns tend
to be correct and the patterns matching many differ-
ent tuples tend to be good patterns. In other words,
BioNoculars attempts to choose from the large space
of patterns in the data the most informative, high-
est confidence patterns that could identify correct tu-
ples; i.e. choosing the most authoritative patterns in
analogy with the hub-authority problem. The most
authoritative patterns can then be used for extracting
relations from free text. The following pattern-tuple
pairs show how patterns can match tuples in the cor-
pus:
(protein) (verb) (noun) (prep.) (protein)
92
Cla4 induces phosphorylation of Cdc24
(protein) (I-protein) (Verb) (prep.) (protein)
NS5A interacts with Cdk1
The proposed approach represents an unsuper-
vised technique for information extraction in general
and particularly for relations extraction that requires
no seed patterns or examples and achieves signifi-
cant performance. Given enough domain text, the
extracted patterns can support many types of sen-
tences with different styles (such passive and active
voice) and orderings (the interaction of X and Y vs.
X interacts with Y).
One of the critical prerequisites of the above-
mentioned approach is the use of a POS tagger,
which is tuned for biomedical text, and a BNE tag-
ger to properly identify BNEs. Both are critical for
determining the types of relationships that are of in-
terest. For POS tagging, a decision tree based tagger
developed by (Schmid, 1994) was used in combi-
nation with a model, which was trained on a cor-
rected/revised GENIA corpus provided by (Saric et
al., 2004) and was reported to achieve 96.4% tagging
accuracy (Saric et al, 2006). This POS tagger will
be referred to as the Schmid tagger. For BNE tag-
ging, ABNER was used. The accuracy of ABNER
is approximately state of the art with precision and
recall of 74.5% and 65.9% respectively with training
done using the BioCreative corpora (BioCreative).
Nonetheless we still face entity identification prob-
lems such as missed identifications in the text which
in turn affects our results considerably. We do be-
lieve if we use a better identification method , we
would yield better results.
4 Experimental Setup
Experiments aimed at extracting protein-protein
interactions for Bakers yeast (Sacharomyces
Cerevesiae) to assess BioNoculars (Cherry et al,
1998). The experiments were performed using
109,440 MEDLINE abstracts that contained the
varying names of the yeast, namely Sacharomyces
cerevisiae, S. Cerevisiae, Bakers yeast, Brewers
yeast and Budding yeast. MEDLINE abstracts
typically summarize the important aspects of papers
possibly including protein-protein interactions if
they are of relevance to the article. The goal was
to deduce the most appropriate extraction patterns
that can be later used to extract relations from any
document. All the MEDLINE abstracts were used
for pattern extraction except for 70 that were set
aside for testing. There were no test documents in
the training set. To build ground-truth, the test set
was semi-manually POS and BNE tagged. They
were also annotated with the interactions that are
contained in the text. There was a condition that
all the abstracts that are used for testing must have
entries in the Database of Interacting Proteins and
Protein-Protein Interactions (DIPPPI), which is
a subset of the Database of Interacting Proteins
(DIP) (Xenarios et al, 2000) restricted to proteins
from yeast. DIPPPI lists the known protein-protein
interactions in the MEDLINE abstracts. There were
297 protein-protein interactions in the test set of 70
abstracts. One of the disadvantages of DIPPPI is
that the presence of interactions is indicated without
mentioning their types or from which sentences
they were extracted. Although BioNoculars is able
to guess the sentence from which an interaction was
extracted and the type of interaction, this informa-
tion was ignored when evaluating against DIPPPI.
Unfortunately, there is no standard test set for the
proposed task, and most of the evaluation sets are
proprietary. The authors hope that others can benefit
from their test set, which is freely available.
The abstracts used for pattern extraction were
POS tagged using the Schmid tagger and BNE tag-
ging was done using ABNER. The patterns were re-
stricted to only those with protein names. For extrac-
tion of interaction tuples, the test set was POS and
BNE tagged using the Schmid tagger and ABNER
respectively. A varying number of final patterns
were then used to extract tuples from the test set and
the average recall and precision were computed. An-
other setup was used in which the relationships were
filtered using preset keywords for relationships such
as inhibits, interacts, and activates to properly com-
pare BioNoculars to systems in the literature that use
such keywords. The keywords were obtained from
the (Hakenberg et al, 2005) and (Temkin and Gilder,
2003). One of the generated pattern-tuple pairs was
as follows:
(PROTEIN) (Verb) (Conjunction) (PROTEIN)
NS5A interacts with Cdk1
One consequence of tuple extraction is generation
of redundant tuples, which contain the same enti-
93
Pattern Count 30 59 78 103 147 192 205 217
Recall 0.51 0.70 0.76 0.81 0.84 0.89 0.89 0.93
Precision 0.47 0.42 0.43 0.35 0.30 0.26 0.26 0.16
FMeasure 0.49 0.53 0.55 0.49 0.44 0.40 0.40 0.27
Table 1: Recall, Precision, and F-measure for extrac-
tion of tuples using a varying number of top rated
patterns
ties and relations. Consequently, all protein aliases
and full text names were resolved to a unified nam-
ing scheme and the unified scheme was used to re-
place all variations of protein names in patterns. All
potential protein-protein interactions that BioNocu-
lars extracted were compared to those in the DIPPPI
databases.
5 Results and Discussion
For the first set of experiments, the experimental
setup described above was used without modifica-
tion. Table 1 and Figure 2 report on the resulting
recall and precision when taking different number
of highest rated patterns. The highest rated 217 pat-
terns were divided on a linear scale into 8 clusters
based on their relative weights.
Figure 2: Recall, Precision, and F-measure for tuple
extraction using a varying number of top patterns
As expected, Figure 2 clearly shows an inverse
relationship between precision and recall. This is
because using more extraction patterns yields more
tuples thus increasing recall at the expense of pre-
cision. The F-measure (with ? = 1) peeks at 78
patterns, which seems to provide the best score
given that precision and recall are equally important.
However, the technique seems to favor recall, reach-
ing a recall of 93% when using all 217 patterns. The
Pattern Count 30 59 78 103 147 192 205 217
Recall 0.31 0.44 0.46 0.48 0.64 0.73 0.74 0.78
Precision 0.31 0.36 0.35 0.34 0.39 0.35 0.35 0.37
FMeasure 0.31 0.40 0.40 0.40 0.48 0.47 0.48 0.50
Table 2: Recall, Precision, and Recall for extraction
of tuples using a varying number of top rated patters
keyword filtering
low precision levels warrant thorough investigation.
In the second set of experiments, extracted tuples
were filtered using preset keywords indicating inter-
actions. Table 2 and Figure 3 show the results of the
experiments.
Figure 3: Recall, Precision, and F-measure for tu-
ple extraction using a varying number of top patterns
with keyword filtering
The results show that filtering with keywords led
to lower recall, but precision remained fairly steady
as the number of patterns changed. Nonetheless, the
best precision in Figure 3 is lower than the best pre-
cision in Figure 2 and the maximum F-measure for
this set of experiments is lower than the maximum
F-measure when no filtering was used. The BioNoc-
ulars system with no filtering can be advantageous
for recall oriented applications. The use of no filter-
ing suggests that some interaction may be expressed
in more generic forms or patterns. An intermediate
solution would be to increase the size of the list of
most commonly occurring keywords to filter the ex-
tracted tuples further.
Currently, ABNER, which is used by the system,
has a precision of 75.4% and a recall of 65.9%. Per-
haps improved tagging may improve the extraction
effectiveness.
The effectiveness of BioNoculars needs to be
94
thoroughly compared to existing systems via the use
of standard test sets, which are not readily available.
Most of previously reported work has been tested
on proprietary test sets or sets that are not publicly
available. The creation of standard publicly avail-
able test set can prompt research in this area.
6 Conclusion and Future Work
This paper presented a system for extracting
protein-protein interaction from biomedical text call
BioNoculars. BioNoculars uses a statistical un-
supervised learning algorithm, which is based on
graph mutual reinforcement and data redundancy
to extract extraction patterns. The system is re-
call oriented and is able to properly extract 93% of
the interaction mentions from test MEDLINE ab-
stracts. Nonetheless, the systems precision remains
low. Precision can be enhanced by using keywords
that describe interactions to filter to the resulting in-
teraction, but this would be at the expense of recall.
As for future work, more attention should be fo-
cused on improving extraction patterns. Currently,
the system focuses on extracting interactions be-
tween exactly two proteins. Some of the issues that
need to be handled include complex relationship (X
and Y interact with A and B), linguistic variabil-
ity (passive vs. active voice; presence of superflu-
ous words such as modifiers, adjectives, and prepo-
sitional phrases), protein lists (W interacts with X,
Y, and Z), nested interactions (W, which interacts
with X, also interacts with Y). Resolving these is-
sues would require an investigation of how patterns
can be generalized in automatic or semi-automatic
ways. Further, the identification of proteins in the
text requires greater attention. Also, the BioNocu-
lars approach can be combined with other rule-based
approaches to produce better results.
References
Ashburner, M., C. A. Ball, J. A. Blake, D. Botstein, H.
Butler, J. M. Cherry, A. P. Davis, K. Dolinski, S. S.
Dwight, J. T. Eppig, M. A. Harris, D. P. Hill, L. Issel-
Tarver, A. Kasarskis, S. Lewis, J. C. Matese, J. E.
Richardson, M. Ringwald, G. M. Rubin, and G. Sher-
lock. 2000. Gene ontology: tool for the unification of
biology. Nature Genetics,volume 25 pp.25-29.
BioCreative. 2004. [Online].
Blaschke C., M. A. Andrade, C. Ouzounis, and A. Valen-
cia. 1999. Automatic Extraction of Biological Infor-
mation from Scientific Text: Protein-Protein Interac-
tions. ISMB99, pp. 60-67.
Blaschke, C. and A. Valencia. 2001. Can Bibliographic
Pointers for Known Biological Protein Interactions
as a Case Study. Comparative and Functional Ge-
nomics,vol. 2: 196-206.
Cherry, J. M., C. Adler, C. Ball, S. A. Chervitz, S. S.
Dwight, E. T. Hester, Y. Jia, G. Juvik, T. Roe, M.
Schroeder, S. Weng, and D. Botstein. 1998. SGD:
Saccharomyces Genome Database. Nucleic Acids Re-
search, 26, 73-9.
Chun, H. W., Y. Tsuruka, J. D. Kim, R. Shiba, N. Nagata,
T. Hishiki, and J. Tsujii. 2006. Extraction of Gene-
Disease Relations from MEDLINE Using Domain Dic-
tionaries and Machine Learning. Pacific Symposium
on Biocomputing 11:4-15.
Collier, N., C. Nobata, and J. Tsujii. 2000. Extracting
the Names of Genes and Gene Products with a Hidden
Markov Model. COLING, 2000, pp. 201207.
Cooper, J. and A. Kershenbaum. 2005. Discovery of
protein-protein interactions using a combination of
linguistic, statistical and graphical information. BMC
Bioinformatics.
DIPPPI http://www2.informatik.hu-berlin.de/ haken-
ber/corpora. 2006.
Ferrucci, D. and A. Lally. 2004. UIMA: an architec-
tural approach to unstructured information processing
in the corporate research environment. Natural Lan-
guage Engineering 10, No. 3-4, 327-348.
Fukuda, K., T. Tsunoda, A. Tamura, and T. Takagi. 1998.
Toward information extraction: identifying protein
names from biological papers. PSB, pages 705716.
Hakenberg, J., C. Plake, U. Leser, H. Kirsch, and D.
Rebholz-Schuhmann. 2005. LLL?05 Challenge:
Genic Interaction Extraction with Alignments and Fi-
nite State Automata. Proc Learning Language in Logic
Workshop (LLL?05) at ICML 2005, pp. 38-45. Bonn,
Germany.
Hanisch, D., J. Fluck, HT. Mevissen, and R. Zimmer.
2003. Playing biologys name game: identifying pro-
tein names in scientific text. PSB, pages 403414.
Hao, Y., X. Zhu, M. Huang, and M. Li. 2005. Discov-
ering patterns to extract protein-protein interactions
from the literature: Part II. Bioinformatics, Vol. 00
no. 0 2005 pages 1-7.
95
Hassan, H., A. Hassan, and O. Emam. 2006. Un-
supervised Information Extraction Approach Using
Graph Mutual Reinforcement. Proceedings of Em-
pirical Methods for Natural Language Processing (
EMNLP ).
Humphreys B. L. and D. A. B. Lindberg. 1993. The
UMLS project: making the conceptual connection be-
tween users and the information they need. Bulletin of
the Medical Library Association, 1993; 81(2): 170.
Jo?rg Hakenberg, Conrad Plake, Ulf Leser. 2005. Genic
Interaction Extraction with Alignments and Finite
State Automata. Proc Learning Language in Logic
Workshop (LLL?05) at ICML 2005, pp. 38-45. Bonn,
Germany (August 2005)
Kazama, J., T. Makino, Y. Ohta, and J. Tsujii. 2002. Tun-
ing Support Vector Machines for Biomedical Named
Entity Recognition. ACL Workshop on NLP in
Biomedical Domain, pages 18.
Kleinberg, J. 1998. Authoritative sources in a hy-
perlinked environment. In Proc. Ninth Ann. ACM-
SIAM Symp. Discrete Algorithms, pages 668-677,
ACM Press, New York.
Koike A. and T. Takagi. 2004. Gene/protein/family
name recognition in biomedical literature. BioLINK
2004: Linking Biological Literature, Ontologies, and
Database, pp. 9-16.
Koike, A., Y. Niwa, and T. Takagi 2005. Automatic
extraction of gene/protein biological functions from
biomedical text. Bioinformatics, Vol. 21, No. 7.
Leroy, G. and H. Chen. 2005. Genescene: An Ontology-
enhanced Integration of Linguistic and Co-Occurance
based Relations in Biomedical Text. JASIST Special
Issue on Bioinformatics.
Mack, R. L., S. Mukherjea, A. Soffer, N. Uramoto, E. W.
Brown, A. Coden, J. W. Cooper, A. Inokuchi, B. Iyer,
Y. Mass, H. Matsuzawa, L. V. Subramaniam. 2004.
Text analytics for life science using the Unstructured
Information Management Architecture. IBM Systems
Journal 43(3): 490-515.
Mika, S. and B. Rost. 2004. NLProt: extracting pro-
tein names and sequences from papers. Nucleic Acids
Research, 32 (Web Server issue): W634W637.
Saric, J., L. J. Jensen, R. Ouzounova, I. Rojas, and P.
Bork. 2004. Extracting regulatory gene expression
networks from PUBMED. Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, Barcelona, Spain, pp.191-198.
Saric, J., L. J. Jensen, R. Ouzounova, I. Rojas, and P.
Bork. 2006. Extraction of regulatory gene/protein
networks from Medline. Bioinformatics Vol.22 no
6,pp. 645-650.
Schmid, H. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In the International Conference
on New Methods in Language Processing, Manch-
ester, UK.
Settles, B. 2004. Biomedical Named Entity Recognition
Using Conditional Random Fields and Rich Feature
Sets. In Proceedings of the International Joint Work-
shop on Natural Language Processing in Biomedicine
and its Applications (NLPBA), Geneva, Switzerland,
pages 104-107.
Settles, B. 2005. ABNER: an open source tool for au-
tomatically tagging genes, proteins, and other entity
names in text. Bioinformatics, 21(14): 3191-3192.
Tanabe L., and W. J. Wilbur. 2002. Tagging gene
and protein names in biomedical text. Bioinformatics,
18(8):11241132.
Temkin, J. M. and M. R. Gilder. 2003. Extraction
of protein interaction information from unstructured
text using a context-free grammar. Bioinformatics
19(16):2046-2053.
Xenarios I, Rice DW, Salwinski L, Baron MK, Marcotte
EM, Eisenberg D. 2000. DIP: the Database of Inter-
acting Proteins. Nucleic Acids Res 28: 289291.
Yamamoto, K., T. Kudo, A. Konagaya, Y. Matsumoto.
2003. Protein Name Tagging for Biomedical Annota-
tion in Text. Proceedings of the ACL 2003 Workshop
on Natural Language Processing in Biomedicine, pp.
65-72.
96
