Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 107?115,
Beijing, August 2010
Towards an optimal weighting of context words based on distance
Bernard Brosseau-Villeneuve*#, Jian-Yun Nie*, Noriko Kando#
* Universit? de Montr?al, Email: {brosseab, nie}@iro.umontreal.ca
# National Institute of Informatics, Email: {bbrosseau, kando}@nii.ac.jp
Abstract
Word Sense Disambiguation (WSD) of-
ten relies on a context model or vector
constructed from the words that co-occur
with the target word within the same text
windows. In most cases, a fixed-sized
window is used, which is determined by
trial and error. In addition, words within
the same window are weighted uniformly
regardless to their distance to the target
word. Intuitively, it seems more reason-
able to assign a stronger weight to con-
text words closer to the target word. How-
ever, it is difficult to manually define the
optimal weighting function based on dis-
tance. In this paper, we propose a unsu-
pervised method for determining the op-
timal weights for context words accord-
ing to their distance. The general idea is
that the optimal weights should maximize
the similarity of two context models of the
target word generated from two random
samples. This principle is applied to both
English and Japanese. The context mod-
els using the resulting weights are used
in WSD tasks on Semeval data. Our ex-
perimental results showed that substantial
improvements in WSD accuracy can be
obtained using the automatically defined
weighting schema.
1 Introduction
The meaning of a word can be defined by the
words that accompany it in the text. This is the
principle often used in previous studies on Word
Sense Disambiguation (WSD) (Ide and V?ronis,
1998; Navigli, 2009). In general, the accompa-
nying words form a context vector of the target
word, or a probability distribution of the context
words. For example, under the unigram bag-of-
words assumption, this means building p(x|t) =
count(x,t)?
x? count(x?,t)
, where count(x, t) is the count of
co-occurrences of word x with the target word t
under a certain criterion. In most studies, x and
t should co-occur within a window of up to k
words or sentences. The bounds are usually se-
lected in an ad-hoc fashion to maximize system
performance. Occurrences inside the window of-
ten weight the same without regard to their po-
sition. This is counterintuitive. Indeed, a word
closer to the target word generally has a greater
semantic constraint on the target word than a more
distant word. It is however difficult to define
the optimal weighting function manually. To get
around this, some systems add positional features
for very close words. In information retrieval, to
model the strength of word relations, some studies
have proposed non-uniform weighting methods of
context words, which decrease the importance of
more distant words in the context vector. How-
ever, the weighting functions are defined manu-
ally. It is unclear that these functions can best cap-
ture the impact of the context words on the target
word.
In this paper, we propose an unsupervised
method to automatically learn the optimal weight
of a word according to its distance to the target
word. The general principle used to determine
such weight is that, if we randomly determine
two sets of windows containing the target word
from the same corpus, the meaning ? or mixture
of meanings for polysemic words ? of the target
word in the two sets should be similar. As the con-
text model ? a probability distribution for the con-
text words ? determines the meaning of the target
word, the context models generated from the two
sets should also be similar. The weights of con-
text words at different distance are therefore de-
107
termined so as to maximize the similarity of con-
text models generated from the two sets of sam-
ples. In this paper, we propose a gradient descent
method to find the optimal weights. We will see
that the optimal weighting functions are different
from those used in previous studies. Experimenta-
tion on Semeval-2007 English and Semeval-2010
Japanese lexical sample task data shows that im-
provements can be attained using the resulting
weighting functions on simple Na?ve Bayes (NB)
systems in comparison to manually selected func-
tions. This result validates the general principle
we propose in this paper.
The remainder of this paper is organized as fol-
lows: typical uses of text windows and related
work are presented in Section 2. Our method
is presented in Section 3. In Section 4 to 6,
we show experimental results on English and
Japanese WSD. We conclude in Section 7 with
discussion and further possible extensions.
2 Uses of text windows
Modeling the distribution of words around one
target word, which we call context model, has
many uses. For instance, one can use it to define
a co-occurrence-based stemmer (Xu and Croft,
1998), which uses window co-occurrence statis-
tics to calculate the best equivalence classes for a
group of word forms. In the study of Xu and Croft,
they suggest using windows of up to 100 words.
Context models are also widely used in WSD.
For example, top performing systems on English
WSD tasks in Semeval-2007, such as NUS-ML
(Cai et al, 2007), all made use of bag-of-words
features around the target word. In this case, they
found that the best results can be achieved using a
window size of 3.
Both systems limit the size of their windows for
different purposes. The former uses a large size in
order to model the topic of the documents contain-
ing the word rather than the word?s meaning. The
latter would limit the size because bag-of-words
features further from the target word would not be
sufficiently related to its meaning (Ide and V?ro-
nis, 1998). We see that there is a compromise be-
tween taking fewer, highly related words, or tak-
ing more, lower quality words. However, there is
no principled way to determine the optimal size
of windows. The size is determined by trial and
error.
A more questionable aspect in the above sys-
tems is that for bag-of-words features, all words
in a window are given equal weights. This is
counterintuitive. One can easily understand that
a context word closer to the target word gener-
ally imposes a stronger constraint on the meaning
of the latter, than a more distant context word. It
is then reasonable to define a weighting function
that decreases along with distance. Several studies
in information retrieval (IR) have proposed such
functions to model the strength of dependency be-
tween words. For instance, Gao et al (2002)
proposed an exponential decay function to capture
the strength of dependency between words. This
function turns out to work better than the uniform
weighting in the IR experiments.
Song and Bruza (2003) used a fixed-size slid-
ing window to determine word co-occurrences.
This is equivalent to define a linear decay func-
tion for context words. The context vectors de-
fined this way are used to estimate similarity be-
tween words. A use of the resulting similarity in
query expansion in IR turned out to be successful
(Bai et al, 2005).
In a more recent study, Lv and Zhai (2009) eval-
uated several kernel functions to determine the
weights of context words according to distance,
including Gaussian kernel, cosine kernel, and so
on. As for the exponential and linear decaying
functions, all these kernel functions have fixed
shapes, which are determined manually.
Notice that the above functions have only been
tested in IR experiments. It is not clear how
these functions perform in WSD. More impor-
tantly, all the previous studies have investigated
only a limited number of weighting functions for
context words. Although some improvements us-
ing these functions have been observed in IR, it
is not clear whether the functions can best capture
the true impact of the context words on the mean-
ing of the target word. Although the proposed
functions comply with the general principle that
closer words are more important than more dis-
tant words, no principled way has been proposed
to determine the particular shape of the function
for different languages and collections.
108
In this paper, we argue that there is indeed a hid-
den weighting function that best capture the im-
pact of context words, but the function cannot be
defined manually. Rather, the best function should
be the one that emerges naturally from the data.
Therefore, we propose an unsupervised method to
discover such a function based on the following
principle: the context models for a target word
generated from two random samples should be
similar. In the next section, we will define in detail
how this principle is used.
3 Computing weights for distances
In this section, we present our method for choos-
ing how much a word occurrence should count in
the context model according to its distance to the
target word. In this study, for simplicity, we as-
sume that all word occurrences at a given distance
count equally in the context model. That is, we
ignore other features such as POS-tags, which are
used in other studies on WSD.
Let C be a corpus, W a set of text windows for
the target word w, cW,i,x the count of occurrences
of word x at distance i in W , cW,i the sum of these
counts, and ?i the weight put on one word occur-
rence at distance i. Then,
PML,W (x) =
?
i ?icW,i,x?
i ?icW,i
(1)
is the maximum likelihood estimator for x in the
context model of w. To counter the zero probabil-
ity problem, we apply Dirichlet smoothing with
the collection language model as a prior:
PDir,W (x) =
?
i ?icW,i,x + ?WP (x|C)?
i ?icW,i + ?W
(2)
The pseudo-count ?W can be a constant, or can be
found by using Newton?s method, maximizing the
log likelihood via leave-one-out estimation:
L?1(?|W, C) =?
i
?
x?V ?icW,i,x log
?icW,i,x??i+?P (x|C)?
j ?jcW,j??i+?
The general process, which we call automatic
Dirichlet smoothing, is similar to that described
in (Zhai and Lafferty, 2002).
To find the best weights for our model we pro-
pose the following process:
? Let T be the set of all windows containing
the target word. We randomly split this set
into two sets A and B.
? We want to find ?? that maximizes the sim-
ilarity of the models obtained from the two
sets, by minimizing their mutual cross en-
tropy:
l(?) = H(PML,A, PDir,B) + (3)
H(PML,B , PDir,A)
In other words, we want ?i to represent how much
an occurrence at distance i models the context
better than the collection language model, whose
counts are weighted by the Dirichlet parameter.
We hypothesize that target words occur in limited
contexts, and as we get farther from them, the pos-
sibilities become greater, resulting in sparse and
less related counts. Since two different sets of the
same word are essentially noisy samples of the
same distribution, the weights maximizing their
mutual generation probabilities should model this
phenomenon.
One may wonder why we do not use a distri-
bution similarity metric such as Kullback?Leibler
(KL) divergence or Information Radius (IRad).
The reason is that with enough word occurrences
(big windows or enough samples), the most sim-
ilar distributions are found with uniform weights,
when all word counts are used. KL divergence
is especially problematic as, since it requires
smoothing, the weights will converge to the de-
generate weights ? = 0, where only the identical
smoothing counts remain. Entropy minimization
is therefore needed in the objective function.
To determine the optimal weight of ?i, we pro-
pose a simple gradient descent minimizing (3)
over ?. The following are the necessary deriva-
tives:
?l
??i
= ?H(PML,A, PDir,B)??i
+
?H(PML,B , PDir,A)
??i
?H
(
PML,W , PDir,(T?W )
)
??i
=
109
?
?
x?V
[?PML,W (x)
??i
log PDir,(T?W )(x)+
?PDir,(T?W )(x)
??i
? PML,W (x)PDir,(T?W )(x)
]
?PML,W (x)
??i
= cW,i,x ? PML,W (x)cW,i?
j ?jcW,j
?PDir,W (x)
??i
= cW,i,x ? PDir,W (x)cW,i?
j ?jcW,j + ?W
We use stochastic gradient descent: one word is
selected randomly, it?s gradient is computed, a
small gradient step is done and the process is re-
peated. A pseudo-code of the process can be
found in Algorithm 1.
Algorithm 1 LearnWeight(C, ?, )
? ? 1k
repeat
T ?{Get windows for next word}
(A,B) ?RandomPartition(T )
for W in A,B do
PML,W ?MakeML(W ,?)
?W ?ComputePseudoCount(W ,C)
PDir,W ?MakeDir( PML,W , ?W , C)
end for
grad ? ?H(PML,A, PDir,B) +
?H(PML,B, PDir,A)
? ? ?? ? grad?grad?
until ??i < 
return ?/max{?i}
Now, as the objective function would eventu-
ally go towards putting nearly all weight on ?1,
we hypothesize that the farthest distances should
have a near-zero contribution, and determine the
stop criterion as having one weight go under a
small threshold. Alternatively, a control set of
held out words can be used to observe the progress
of the objective function or the gradient length.
When more and more weight is put on the few
closest positions, the objective function and gra-
dient depends on less counts and will become less
stable. This can be used as a stop criterion.
The above weight learning process is applied
on an English collection and a Japanese collection
with ? =  = 0.001, and ? = 1000. In the next
sections, we will describe both resulting weight-
ing functions in the context of WSD experiments.
4 Classifiers for supervised WSD tasks
Since we use the same systems for both English
and Japanese experiments, we will briefly discuss
the used classifiers in this section. In both tasks,
the objective is to maximize WSD accuracy on
held-out data, given that we have a set of training
text passages containing a sense-annotated target
word.
The first of our baselines, the Most Frequent
Sense (MFS) system always selects the most fre-
quent sense in the training set. It gives us a lower
bound on system accuracies.
Na?ve Bayes (NB) classifiers score classes us-
ing the Bayes formula under a feature indepen-
dence assumption. Let w be the target word in a
given window sample to be classified, the scoring
formula for sense class S is:
Score(w,S) = P (S)PTar(w|S)?Tar??
xi?context(w) PCon(xi|S)
?Con?dist(xi)
where dist(xi) is the distance between the context
word xi and the target word w. The target word
being an informative feature present in all sam-
ples, we use it in a target word language model
PTar . The surrounding words are summed in the
context model PCon as shown in equation (1). As
we can see with the presence of ? in the equation,
the scoring follows the same weighting scheme as
we do when accumulating counts, since the sam-
ples to classify follow the same distribution as the
training ones. Also, when a language model uses
automatic Dirichlet smoothing, the impact of the
features against the prior is controlled with the
manual parameters ?Tar or ?Con. When a man-
ual smoothing parameter is used, it also handles
impact control. Our systems use the following
weight functions:
Uniform: ?i = 11?i??, where ? is a window size
and 1 the indicator function.
Linear: ?i = max{0, 1 ? (i ? 1)?}, where ? is
the decay rate.
110
Exponential: ?i = e?(i?1)? , where ? is the ex-
ponential parameter.
Learned: ?i is the weight learned as shown pre-
viously.
The parameters for NB systems are identical for
all words of a task and were selected by exhaustive
search, maximizing leave-one-out accuracy on the
training set. For each language model, we tried
Laplace, manual Dirichlet and automatic Dirichlet
smoothing.
For the sake of comparison, also we provide a
Support Vector Machine (SVM) classifier, which
produces the best results in Semeval 2007. We
used libSVM with a linear kernel, and regular-
ization parameters were selected via grid search
maximizing leave-one-out accuracy on the train-
ing set. We tested the following windows limits:
all words in sample, current sentence, and various
fixed window sizes. We used the same features
as the NB systems, testing Boolean, raw count,
log-of-counts and counts from weight functions
representations. Although non-Boolean features
had good leave-one-out precision on the training
data, since SVM does not employ smoothing, only
Boolean features kept good results on test data, so
our SVM baseline uses Boolean features.
5 WSD experiments on Semeval-2007
English Lexical Sample
The Semeval workshop holds WSD tasks such as
the English Lexical Sample (ELS) (Pradhan et al,
2007). The task is to maximize WSD accuracy on
a selected set of polysemous words, 65 verbs and
35 nouns, for which passages were taken from the
WSJ Tree corpus. Passages contain a couple of
sentences around the target word, which is manu-
ally annotated with a sense taken from OntoNotes
(Hovy et al, 2006). The sense inventory is quite
coarse, with an average of 3.6 senses per word.
Instances count are listed in Table 1.
Train Test Total
Verb 8988 2292 11280
Noun 13293 2559 15852
Total 22281 4851
Table 1: Number of instances in the ELS data
Figure 1: Weight curve for AP88-90
Since there are only 100 target words and in-
stances are limited in the Semeval collection, we
do not have sufficient samples to estimate the op-
timal weights for context words. Therefore, we
used the AP88-90 corpus of the TREC collection
(CD 1 & 2) in our training process. The AP col-
lection contains 242,918 documents. Since our
classifiers use word stems, the collection was also
stemmed with the Porter stemmer and sets of win-
dows were built for all word stems. To get near-
uniform counts in all distances, only full win-
dows with a size of 100, which was considered
big enough without any doubt, were kept. In order
to get more samples, windows to the right and to
the left were separated. For each target word, we
used 1000 windows. A stoplist of the top 10 fre-
quent words was used, but place holders were left
in the windows to preserve the distances. Mul-
tiple consecutive stop words (ex: ?of the?) were
merged, and the target word stem, being the same
for all samples of a set, was ignored in the con-
struction of context models. The AP collection re-
sults in 32,650 target words containing 5,870,604
windows. The training process described in Sec-
tion 3 is used to determine the best weights of con-
text words. Figure 1 shows the first 40 elements
of the resulting weighting function curve.
As we can see, the curve is neither exponen-
tial, linear, or any of the forms used by Lv and
Zhai. Its form is rather similar to x??, or rather
log?1(? + x) minus some constant. The decrease
111
System Cross-Val (%) Test set (%)
MFS 78.66 77.76
Uniform NB 86.04 84.52
SVM 85.53 85.03
Linear NB 86.89 85.71
Exp. NB 87.80 86.23
Learned NB 88.46 86.70
Table 2: WSD accuracy on Semeval-2007 ELC
rate is initially very high and then reduces as it
becomes closer to zero. This long tail is not
present in any of the previously suggested func-
tions. The large difference between the above op-
timal weighting function and the functions used
in previous studies would indicate that the latter
are suboptimal. Also, as we can see, the rela-
tion between context words and the target word
is mostly gone after a few words. This would
motivate the commonly used very small windows
when using a uniform weights, since using a big-
ger window would further widen the gap between
the used weight and the optimal ones.
Now for the system settings, the context words
were processed the same way as the external cor-
pus. The target word was used without stemming
but had the case stripped. The NB systems used
the concatenation of the AP collection and the
Semeval data for the collection language model.
This is motivated by the fact that the Semeval data
is not balanced: it contains only a small number of
passages containing the target words. This makes
words related to them unusually frequent. The
class priors used an absolute discounting of 0.5 on
class counts. Uniform NB uses a window of size 4,
a Laplace smoothing of 0.65 on PTar and an au-
tomatic Dirichlet with ?Con = 0.7 on PCon. Lin-
ear NB has ? = 0.135, uses a Laplace smoothing
of 0.85 on PTar and an automatic Dirichlet with
?Con = 0.985 on PCon. Exp NB has ? = 0.27,
uses a Laplace smoothing of 2.8 on PTar and an
automatic Dirichlet with ?Con = 1.01 on PCon.
The SVM system uses a window of size 3. Our
system, Learned NB uses a Laplace smoothing of
1.075 on PTar , and an automatic Dirichlet with
?Con = 1.025 on PCon. The results on WSD are
listed in Table 2. WSD accuracy is measured by
the proportion of correctly disambiguated words
among all the word samples. The cross-validation
is performed on the training data with leave-one-
out and is shown as a hint of the capacity of the
models. A randomization test comparing Expo-
nential NB and Learned NB gives a p-value of
0.0508, which is quite good considering the exten-
sive trials used to select the exponential parameter
in comparison to a single curve computed from a
different corpus. This performance is comparable
to the current state of the art. It outperforms most
of the systems participating in the task (Pradhan et
al., 2007). Out of 14 systems, the best results had
accuracies of 89.1*, 89.1*, 88.7, 86.9 and 86.4 (*
indicates post-competition submissions). Notice
that most previous systems used SVM with ad-
ditional features such as local collocations, posi-
tional word features and POS tags. Our approach
only uses bag-of-words in a Na?ve Bayes classi-
fier. Therefore, the performance of our method is
sub-optimal. With additional features and better
classification methods, we can expect that better
performance can be obtained. In future work, we
will investigate the applications of SVM with our
new term weighting scheme, together with addi-
tional types of features.
6 WSD experiments on Semeval-2010
Japanese Lexical Sample
The Semeval-2010 Japanese WSD task (Okumura
et al, 2010) consists of 50 polysemous words
for which examples were taken from the BCCWJ
corpus (Maekawa, 2008). It was manually seg-
mented, POS-tagged, and annotated with senses
taken from the Iwanami Kokugo dictionary. The
selected words have 50 samples for both the train-
ing and test set. The task is identical to the ELS
of the previous experiment.
Since the data was again insufficient to com-
pute the optimal weighting curve, we used the
Mainichi-2005 corpus of NTCIR-8. We tried to
reproduce the same kind of segmentation as the
training data by using the Chasen parser with Uni-
Dic, which nevertheless results in different word
segments as the training data. For the corpus and
Semeval data, conjugations (setsuzoku-to, jod?-
shi, etc.), particles (all jo-shi), symbols (blanks,
kig?, etc.), and numbers were stripped. When a
112
Figure 2: Weight curve for Mainichi 2005
base-form reading was present (for verbs and ad-
jectives), the token was replaced by the Kanjis
(Chinese characters) in the word writing concate-
nated with the base-form reading. This treatment
is somewhat equivalent to the stemming+stop list
of the ELS tasks. The resulting curve can be seen
in Figure 2.
As we can see, the general form of the curve
is similar to that of the English collection, but
is steeper. This suggests that the meaning of
Japanese words can be determined using only
the closest context words. Words further than a
few positions away have very small impact on
the target word. This can be explained by the
grammatical structure of the Japanese language.
While English can be considered a Subject-Verb-
Complement language, Japanese is considered
Subject-Complement-Verb. Verbs, mostly found
at the end of a sentence, can be far apart from their
subject, and vice versa. The window distance is
therefore less useful to capture the relatedness in
Japanese than in English since Japanese has more
non-local dependencies.
The Semeval Japanese test data being part of a
balanced corpus, untagged occurrences of the tar-
get words are plenty, so we can benefit from using
the collection-level counts for smoothing. Uni-
form NB uses a window of size 1, manual Dirich-
let smoothing of 4 for PTar and 90 for the PCon.
Linear NB has ? = 0.955, uses a manual Dirichlet
smoothing of 6.25 on PTar and manual Dirichlet
System Cross-Val (%) Test set (%)
MFS 75.23 68.96
SVM 82.55 74.92
Uniform NB 82.47 76.16
Linear NB 82.63 76.48
Exp. NB 82.68 76.44
Learned NB 82.67 76.52
Table 3: WSD accuracy on Semeval-2010 JWSD
smoothing with ?Con = 65 on PCon. Exp NB
has ? = 2.675, uses a manual Dirichlet smooth-
ing of 6.5 on PTar and a manual Dirichlet of 70
on PCon. The SVM system uses a window size of
1 and Boolean features. Learned NB used a man-
ual Dirichlet smoothing of 4 for PTar and auto-
matic Dirichlet smoothing with ?Con = 0.6 for
PCon. We believe this smoothing is beneficial
only on this system because it uses more words
(the long tail), that makes the estimation of the
pseudo-count more accurate. Results on WSD are
listed in Table 3. As we can see, the difference be-
tween the NB models is less substantial than for
English. This may be due to differences in the
segmentation parameters of our external corpus:
we used the human-checked segmentation found
in the Semeval data for classification, but used a
parser to segment our external corpus for weight
learning. We are positive that the Chasen parser
with the UniDic dictionary was used to create the
initial segmentation in the Semeval data, but there
may be differences in versions and the initial seg-
mentation results were further modified manually.
Another reason for the results could be that the
systems use almost the same weights: Uniform
NB and SVM both used windows of size 1, and
the Japanese curve is steeper than the English one,
making the context model account to almost only
immediately adjacent words. So, even if our con-
text model contains more context words at larger
distances, their weights are very low. This makes
all context model quite similar. Nevertheless, we
still observe some gain in WSD accuracy. These
results show that the curves work as expected even
in different languages. However, the weighting
curve is strongly language-dependent. It could
also be collection-dependent ? we will investigate
113
this aspect in the future, using different collec-
tions.
7 Conclusions
The definition of context vector and context model
is critical in WSD. In previous studies in IR, de-
caying weight along with distance within a text
window have been proposed. However, the de-
caying functions are defined manually. Although
some of the functions produced better results than
the uniform weighting, there is no evidence show-
ing that these functions best capture the impact
of the context words on the meaning of the tar-
get word. This paper proposed an unsupervised
method for finding optimal weights for context
words according to their distance to the target
word. The general idea was to find the weights
that best fit the data, in such a way that the context
models for the same target word generated from
two random windows samples become similar. It
is the first time that this general principle is used
for this purpose. Our experiments on WSD in En-
glish and Japanese suggest the validity of the prin-
ciple.
In this paper, we limited context models to bag-
of-words features, excluding additional features
such as POS-tags. Despite this simple type of fea-
ture and the use of a simple Na?ve Bayes classifier,
the WSD accuracy we obtained can rival the other
state-of-the-art systems with more sophisticated
features and classification algorithms. This result
indicates that a crucial aspect in WSD is the def-
inition of an appropriate context model, and our
weighting method can generate more reasonable
weights of context words than using a predefined
decaying function.
Our experiments also showed that the optimal
weighting function is language-dependent. We
obtained two different functions for English and
Japanese, although their general shapes are simi-
lar. In fact, the optimal weighting function reflects
the linguistic properties: as dependent words in
Japanese can be further away from the target word
due to its linguistic structure, the optimal weight-
ing quickly decays, meaning that we can rely less
on distant context words. This also shows a lim-
itation of this study: distance is not the sole cri-
terion to determine the impact of a context word.
Other factors, such as POS-tag and syntactic de-
pendency, can play an important role in the con-
text model. These additional factors are comple-
mentary to the distance criterion and our approach
can be extended to include such additional fea-
tures. This extension is part of our future work.
Another limitation of straight window distance
is that all words introduce the same distance, re-
gardless of their nature. In our experiments, to
make the distance a more sensible metric, we
merged consecutive stop words in one placeholder
token. The idea behind this it that some words,
such as stop words, should introduce less distance
than others. On the opposite, we can easily un-
derstand that tokens such as commas, full stops,
parentheses and paragraph should introduce a big-
ger distance than regular words. We could there-
fore use a congruence score for a word, an indi-
cator showing on average how much what comes
before is similar to what comes after the word.
Also, we have combined our weighting schema
with NB classifier. Other classifiers such as SVM
could lead to better results. The utilization of our
new weighting schema with SVM is another fu-
ture work.
Finally, the weights computed with our method
has been used in WSD tasks. The weights could
be seen as the expected strength of relation be-
tween two words in a document according to their
distance. The consideration of word relationships
in documents and queries is one of the endeav-
ors in current research in IR. The new weighting
schema could be easily integrated with a depen-
dency model in IR. We plan to perform such inte-
gration in the future.
Acknowledgments
The authors would like to thank Florian Boudin
and Satoko Fujisawa for helpful comments on
this work. This work is partially supported
by Japanese MEXT Grant-in-Aid for Scientific
Research on Info-plosion (#21013046) and the
Japanese MEXT Research Student Scholarship
program.
114
References
Bai, Jing, Dawei Song, Peter Bruza, Jian-Yun Nie, and
Guihong Cao. 2005. Query expansion using term
relationships in language models for information re-
trieval. In CIKM ?05 Proceedings, pages 688?695,
New York, NY, USA. ACM.
Cai, Jun Fu, Wee Sun Lee, and Yee Whye Teh. 2007.
Nus-ml: improving word sense disambiguation us-
ing topic features. In SemEval ?07 Proceedings,
pages 249?252, Morristown, NJ, USA. Association
for Computational Linguistics.
Cheung, Percy and Pascale Fung. 2004. Translation
disambiguation in mixed language queries. Ma-
chine Translation, 18(4):251?273.
Gao, Jianfeng, Ming Zhou, Jian-Yun Nie, Hongzhao
He, and Weijun Chen. 2002. Resolving query trans-
lation ambiguity using a decaying co-occurrence
model and syntactic dependence relations. In SI-
GIR ?02 Proceedings, pages 183?190, New York,
NY, USA. ACM.
Ide, Nancy and Jean V?ronis. 1998. Introduction to
the special issue on word sense disambiguation: the
state of the art. Comput. Linguist., 24(1):2?40.
Lv, Yuanhua and ChengXiang Zhai. 2009. Positional
language models for information retrieval. In SIGIR
?09 Proceedings, pages 299?306, New York, NY,
USA. ACM.
Maekawa, Kikuo. 2008. Compilation of the bal-
anced corpus of contemporary written japanese in
the kotonoha initiative (invited paper). In ISUC
?08 Proceedings, pages 169?172, Washington, DC,
USA. IEEE Computer Society.
Navigli, Roberto. 2009. Word sense disambiguation:
A survey. ACM Comput. Surv., 41(2):1?69.
Okumura, Manabu, Kiyoaki Shirai, Kanako Komiya,
and Hikaru Yokono. 2010. Semeval-2010 task:
Japanese wsd. In SemEval ?10 Proceedings. Asso-
ciation for Computational Linguistics.
Pradhan, Sameer S., Edward Loper, Dmitriy Dligach,
and Martha Palmer. 2007. Semeval-2007 task 17:
English lexical sample, srl and all words. In Se-
mEval ?07 Proceedings, pages 87?92, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Song, D. and P. D. Bruza. 2003. Towards context sen-
sitive information inference. Journal of the Amer-
ican Society for Information Science and Technol-
ogy, 54(4):321?334.
Xu, Jinxi and W. Bruce Croft. 1998. Corpus-
based stemming using cooccurrence of word vari-
ants. ACM Trans. Inf. Syst., 16(1):61?81.
Zhai, ChengXiang and John Lafferty. 2002. Two-
stage language models for information retrieval. In
SIGIR ?02 Proceedings, pages 49?56, New York,
NY, USA. ACM.
115
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 375?378,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
RALI: Automatic weighting of text window distances
Bernard Brosseau-Villeneuve*#, Noriko Kando#, Jian-Yun Nie*
* Universit? de Montr?al, Email: {brosseab, nie}@iro.umontreal.ca
# National Institute of Informatics, Email: {bbrosseau, kando}@nii.ac.jp
Abstract
Systems using text windows to model
word contexts have mostly been using
fixed-sized windows and uniform weights.
The window size is often selected by trial
and error to maximize task results. We
propose a non-supervised method for se-
lecting weights for each window distance,
effectively removing the need to limit win-
dow sizes, by maximizing the mutual gen-
eration of two sets of samples of the same
word. Experiments on Semeval Word
Sense Disambiguation tasks showed con-
siderable improvements.
1 Introduction
The meaning of a word can be defined by the
words that accompany it in the text. This is the
principle often used in previous studies on Word
Sense Disambiguation (WSD) (Ide and V?ronis,
1998; Navigli, 2009). In general, the accompa-
nying words form a context vector of the target
word, or a probability distribution of the context
words. For example, under the unigram bag-of-
word assumption, this means building p(x|t) =
count(x,t)
?
x
?
count(x
?
,t)
, where count(x, t) is the count of
co-occurrences of word x with the target word t
under a certain criterion. In most studies, x and t
should co-occur within a window of up to k words
or sentences. The bounds are usually selected as
to maximize system performance. Occurrences in-
side the window usually weight the same with-
out regard to their position. This is counterintu-
itive. Indeed, a word closer to the target word usu-
ally has a greater semantic constraint on the tar-
get word than a more distant word. Some studies
have also proposed decaying factors to decrease
the importance of more distant words in the con-
text vector. However, the decaying functions are
defined manually. It is unclear that the functions
defined can capture the true impact of the con-
text words on the target word. In this paper, we
propose an unsupervised method to automatically
learn the optimal weight of a word according to its
distance to the target word. The general idea used
to determine such weight is that, if we randomly
determine two sets of texts containing the target
word, the resulting probability distributions for its
context words in the two sets should be similar.
Therefore, the weights of context words at differ-
ent distance are determined so as to maximize the
mutual generation probabilities of two sets of sam-
ples. Experimentation on Semeval-2007 English
and Semeval-2010 Japanese lexical sample task
data shows that improvements can automatically
be attained on simple Naive Bayes (NB) systems
in comparison to the best manually selected fixed
window system.
The remainder of this paper is organized as fol-
lows: example uses of text windows and related
work are presented in Section 2. Our method
is presented in Section 3. In Section 4 and
5, we show experimental results on English and
Japanese WSD. We conclude in Section 6 with
discussion and further possible extensions.
2 Uses of text windows
Modeling the distribution of words around one
target word has many uses. For instance, the
Xu&Croft co-occurrence-based stemmer (Xu and
Croft, 1998) uses window co-occurrence statis-
tics to calculate the best equivalence classes for
a group of word forms. They suggest using win-
dows of up to 100 words. Another example can be
found in WSD systems, where a shorter window is
preferred. In Semeval-2007, top performing sys-
tems on WSD tasks, such as NUS-ML (Cai et al,
2007), made use of bag-of-word features around
the target word. In this case, they found that the
best results can be achieved using a window size
of 3.
375
Both these systems limit the size of their win-
dows for different purposes. The former aims to
model the topic of the documents containing the
word rather than the word?s meaning. The latter
limits the size because bag-of-word features fur-
ther from the target word would not be sufficiently
related to its meaning (Ide and V?ronis, 1998). We
see that because of sparsity issues, there is a com-
promise between taking few, highly related words,
or taking several, lower quality words.
In most current systems, all words in a window
are given equal weight, but we can easily under-
stand that the occurrences of words should gener-
ally count less as they become farther; they form
a long tail that we should use. Previous work pro-
posed using non-linear functions of the distance
to model the relation between two words. For in-
stance, improvements can be obtained by using an
exponential function (Gao et al, 2002). Yet, there
is no evidence that the exponential ? with its man-
ually selected parameter ? is the best function.
3 Computing weights for distances
In this section, we present our method for choos-
ing howmuch a word should count according to its
distance to the target word. First, for some defini-
tions, let C be a corpus, W a set of text windows,
c
W,i,x
the count of occurrences of word x at dis-
tance i in W , c
W,i
the sum of these counts, and ?
i
the weight put on one word at distance i. Then,
P
ML,W
(x) =
?
i
?
i
c
W,i,x
?
i
?
i
c
W,i
(1)
is the maximum likelihood estimator for x. To
counter the zero-probability problem, we apply
Dirichlet smoothing with the collection language
model as a prior:
P
Dir,W
(x) =
?
i
?
i
c
W,i,x
+ ?
W
P (x|C)
?
i
?
i
c
W,i
+ ?
W
(2)
The pseudo-count ?
W
is found by using Newton?s
method via leave-one-out estimation. We follow
the procedure shown in (Zhai and Lafferty, 2002),
but since occurrences have different weights, the
log-likelihood is changed to
L
?1
(?|W, C) = (3)
?
i
?
x?V
?
i
c
W,i,x
log
?
i
c
W,i,x
??
i
+?P (x|C)
?
j
?
j
c
W,j
??
i
+?
To find the best weights for our model we pro-
pose the following:
? Let T be the set of all windows containing
the target word. We randomly split this set
into two sets A and B.
? We want to find ?
?
that maximizes the mu-
tual generation of the two sets, by minimizing
their cross-entropy:
l(?) = H(P
ML,A
, P
Dir,B
) + H(P
ML,B
, P
Dir,A
)
(4)
In other words, we want ?
i
to represent how
much an occurrence at distance i models the con-
text better than the collection language model,
whose counts are controlled by the Dirichlet
pseudo-count. We hypothesize that target words
occurs in limited contexts, and as we get farther
from them, the possibilities become greater, re-
sulting in sparse and less related counts.
3.1 Gradient descent
We propose a simple gradient descent minimiz-
ing (4) over ?. For the following experiments,
we used one single curve for all words in a task.
We used the mini-batch type of gradient descent:
the gradients of a fixed amount of target words are
summed, a gradient step is done, and the proces
is repeated while cycling the data. The starting
state was with all ?
i
to one, the batch size of 50
and a learning rate of 1. We notice that as the al-
gorithm progress, weights on close distances in-
crease and the farthest decrease. As further dis-
tances contribute less and less, middle distances
start to decay more and more, until at some point,
all distances but the closest start to decrease, head-
ing towards a degenerate solution. We therefore
suggest using the observation of several consecu-
tive decreases of all except ?
1
as an end criterion.
We used 10 consecutive steps for our experiments.
4 Experiments on Semeval-2007 English
Lexical Sample
The Semeval workshop holds WSD tasks such as
the English Lexical Sample (ELS) (Pradhan et al,
2007). It consists of a selected set of polysemous
words, contained within passages where a sense
taken from a sense inventory is manually anno-
tated. The task is to create supervised classifiers
maximizing accuracy on test data.
Since there are only 50 words and instances are
few, we judged there was not enough data to com-
pute weights. Instead, we used the AP Newswire
corpus of the TREC collection (CD 1 & 2). Words
376
were stemmed with the Porter stemmer and text
windows were grouped for all words. For sim-
plicity and efficiency, windows to the right and to
the left were considered independent, and we only
kept words with between 30 and 1000 windows.
Also, only windows with a size of 100, which was
considered big enough without any doubt, were
kept. A stop list of the top 10 frequent words was
used, but place holders were left in the windows to
preserve the distances. Multiple consecutive stop
words (ex: ?of the?) were merged, and the tar-
get word, being the same for all samples of a set,
was ignored. This results in 32,650 sets contain-
ing 5,870,604 windows. In Figure 1, we can see
the resulting weight curve.
0 20 40 60 80 100
distance
0.0
0.2
0.4
0.6
0.8
1.0
w
e
i
g
h
t
Figure 1: Weight curve for AP Newswire
Since the curve converges, words over the 100th
distance were assigned the minimumweight found
in the curve. From this we constructed NB models
whose class priors used an absolute discounting of
0.5. The collection language model used the con-
catenation of the AP collection and the Semeval
data. As the unstemmed target word is an impor-
tant feature it was added to the models. It?s weight
was chosen to be 0.7 by maximizing accuracy on
one-held-out cross-validation of the training data.
The results are listed in Table 1.
System Cross-Val (%) Test set (%)
Prior only 78.66 77.76
Best uniform 85.48 83.28
RALI-2 88.23 86.45
Table 1: WSD accuracy on Semeval-2007 ELC
We used two baselines: most frequent sense
(prior only), and the best uniform (except target
word) fixed size window found from extensive
search on the training data. The best settings were
a window of size 4, with a weight of 4.4 on the
target word and a Laplace smoothing of 2.9. The
improvements seen using our system are substan-
tial, beating most of the systems originally pro-
posed for the task (Pradhan et al, 2007). Out
of 15 systems, the best results had accuracies of
89.1*, 89.1*, 88.7, 86.9 and 86.4 (* indicates post-
competition submissions). Notice that most were
using Support Vector Machine (SVM) with bag-
of-word features in a very small window, local col-
locations and POS tags. In our future work, we
will investigate the applications of SVM with our
new term weighting scheme.
5 Experiments on Semeval-2010
Japanese WSD
The Semeval-2010 Japanese WSD task (Okumura
et al, 2010) consists of 50 polysemous words
for which examples were taken from the BC-
CWJ tagged corpus. It was manually segmented,
tagged, and annotated with senses taken from the
Iwanami Kokugo dictionary. The task is identical
to the ELS of the previous experiment.
Since the data was again insufficient to com-
pute curves, we used the Mainichi-2005 corpus of
NTCIR-8. We tried to reproduce the same kind
of segmentation as the training data by using the
Chasen parser with UniDic. For the corpus and
Semeval data, conjugations (setsuzoku-to, jod?-
shi, etc.), particles (all jo-shi), symbols (blanks,
kig?, etc.), and numbers were stripped. When a
base-form reading was present (for verbs and ad-
jectives), the token was replaced by the Kanjis
(chinese characters) in the word writing concate-
nated with the base-form reading. This treatment
is somewhat equivalent to the stemming+stop list
of the ELS tasks. The resulting curve can be seen
in Figure 2.
The NB models are the same as in the previous
experiments. Target words were again added the
same way as in the ELS task. The best fixed win-
dow model was found to have a window size of 1
with a target word weight of 0.6 and used manual
Dirichlet smoothing with a pseudo-count of 110.
We submited two systems with the following set-
tings: RALI-1 used manual Dirichlet smoothing
and 0.9 for the target word. RALI-2 used auto-
377
0 20 40 60 80 100
distance
0.0
0.2
0.4
0.6
0.8
1.0
w
e
i
g
h
t
Figure 2: Weight curve for Mainichi Shinbun 2005
matic Dirichlet smoothing and 1.7 for the target
word weight. Results are listed in Table 2.
System Cross-Val (%) Test set (%)
prior only 75.23 68.96
Best uniform 82.29 76.12
RALI-1 82.77 75.92
RALI-2 83.05 76.36
Table 2: WSD accuracy on Semeval-2010 JWSD
As we can see, the results are not significantly
different from the best uniform model. This may
be due to differences in the segmentation parame-
ters of our external corpus. Another reason could
be that the systems use almost the same weights:
the best fixed window had size 1, and the Japanese
curve is steeper than the English one.
This steeper curve can be explained by the
grammatical structure of the Japanese language.
While English can be considered a Subject-
Verb-Complement language, Japanese is consid-
ered Subject-Complement-Verb. Verbs are mostly
found at the end of the sentence, far from their sub-
ject, and vice versa. The window distance is there-
fore less useful in Japanese than in English since
it has more non-local dependencies. These results
show that the curves work as expected even in dif-
ferent languages.
6 Conclusions
This paper proposed an unsupervised method for
finding weights for counts in text windows ac-
cording to their distance to the target word. Re-
sults from the Semeval-2007 English lexical sam-
ple showed a substantial improvement in preci-
sion. Yet, as we have seen with the Japanese task,
window distance is not always a good indicator of
word relatedness. Fortunately, we can easily imag-
ine extensions to the current scheme that bins word
counts by factors other than word distance. For in-
stance, we could also bin counts by parsing tree
distance, sentence distance or POS-tags.
Acknowledgments
The authors would like to thank Florian Boudin
and Satoko Fujisawa for helpful comments on
this work. This work is partially supported
by Japanese MEXT Grant-in-Aid for Scientific
Research on Info-plosion (#21013046) and the
Japanese MEXT Research Student Scholarship
program.
References
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007.
Nus-ml: improving word sense disambiguation us-
ing topic features. In SemEval ?07 Proceedings,
pages 249?252, Morristown, NJ, USA. Association
for Computational Linguistics.
Jianfeng Gao, Ming Zhou, Jian-Yun Nie, Hongzhao
He, and Weijun Chen. 2002. Resolving query trans-
lation ambiguity using a decaying co-occurrence
model and syntactic dependence relations. In SI-
GIR ?02 Proceedings, pages 183?190, New York,
NY, USA. ACM.
Nancy Ide and Jean V?ronis. 1998. Introduction to
the special issue on word sense disambiguation: the
state of the art. Comput. Linguist., 24(1):2?40.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Comput. Surv., 41(2):1?69.
Manabu Okumura, Kiyoaki Shirai, Kanako Komiya,
and Hikaru Yokono. 2010. Semeval-2010 task:
Japanese wsd. In SemEval ?10 Proceedings. Associ-
ation for Computational Linguistics.
Sameer S. Pradhan, Edward Loper, Dmitriy Dligach,
and Martha Palmer. 2007. Semeval-2007 task 17:
English lexical sample, srl and all words. In Se-
mEval ?07 Proceedings, pages 87?92, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Jinxi Xu and W. Bruce Croft. 1998. Corpus-
based stemming using cooccurrence of word vari-
ants. ACM Trans. Inf. Syst., 16(1):61?81.
ChengXiang Zhai and John Lafferty. 2002. Two-stage
language models for information retrieval. In SIGIR
?02 Proceedings, pages 49?56, NewYork, NY, USA.
ACM.
378
