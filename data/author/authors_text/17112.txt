Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 405?413,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Predicting and Characterising User Impact on Twitter
Vasileios Lampos
1
, Nikolaos Aletras
2
, Daniel Preot?iuc-Pietro
2
and Trevor Cohn
3
1
Department of Computer Science, University College London
2
Department of Computer Science, University of Sheffield
3
Computing and Information Systems, The University of Melbourne
v.lampos@ucl.ac.uk, {n.aletras,d.preotiuc}@dcs.shef.ac.uk, trevor.cohn@gmail.com
Abstract
The open structure of online social net-
works and their uncurated nature give rise
to problems of user credibility and influ-
ence. In this paper, we address the task of
predicting the impact of Twitter users based
only on features under their direct control,
such as usage statistics and the text posted
in their tweets. We approach the problem as
regression and apply linear as well as non-
linear learning methods to predict a user
impact score, estimated by combining the
numbers of the user?s followers, followees
and listings. The experimental results point
out that a strong prediction performance is
achieved, especially for models based on
the Gaussian Processes framework. Hence,
we can interpret various modelling com-
ponents, transforming them into indirect
?suggestions? for impact boosting.
1 Introduction
Online social networks have become a wide spread
medium for information dissemination and inter-
action between millions of users (Huberman et al.,
2009; Kwak et al., 2010), turning, at the same
time, into a popular subject for interdisciplinary
research, involving domains such as Computer Sci-
ence (Sakaki et al., 2010), Health (Lampos and
Cristianini, 2012) and Psychology (Boyd et al.,
2010). Open access along with the property of struc-
tured content retrieval for publicly posted data have
brought the microblogging platform of Twitter into
the spotlight.
Vast quantities of human-generated text from
a range of themes, including opinions, news and
everyday activities, spread over a social network.
Naturally, issues arise, like user credibility (Castillo
et al., 2011) and content attractiveness (Suh et al.,
2010), and quite often trustful or appealing informa-
tion transmitters are identified by an impact assess-
ment.
1
Intuitively, it is expected that user impact
cannot be defined by a single attribute, but depends
on multiple user actions, such as posting frequency
and quality, interaction strategies, and the text or
topics of the written communications.
In this paper, we start by predicting user impact
as a statistical learning task (regression). For that
purpose, we firstly define an impact score function
for Twitter users driven by basic account proper-
ties. Afterwards, from a set of accounts, we mea-
sure several publicly available attributes, such as
the quantity of posts or interaction figures. Textual
attributes are also modelled either by word frequen-
cies or, more generally, by clusters of related words
which quantify a topic-oriented participation. The
main hypothesis being tested is whether textual
and non textual attributes encapsulate patterns that
affect the impact of an account.
To model this data, we present a method based
on nonlinear regression using Gaussian Processes,
a Bayesian non-parametric class of methods (Ras-
mussen and Williams, 2006), proven more effec-
tive in capturing the multimodal user features. The
modelling choice of excluding components that
are not under an account?s direct control (e.g. re-
ceived retweets) combined with a significant user
impact prediction performance (r = .78) enabled
us to investigate further how specific aspects of a
user?s behaviour relate to impact, by examining the
parameters of the inferred model.
Among our findings, we identify relevant fea-
tures for this task and confirm that consistent ac-
tivity and broad interaction are deciding impact
factors. Informativeness, estimated by computing
a joint user-topic entropy, contributes well to the
separation between low and high impact accounts.
Use case scenarios based on combinations of fea-
tures are also explored, leading to findings such as
that engaging about ?serious? or more ?light? topics
may not register a differentiation in impact.
1
For example, the influence assessment metric of Klout ?
http://www.klout.com.
405
2 Data
For the experimental process of this paper, we
formed a Twitter data set (D1) of more than 48 mil-
lion tweets produced by |U | = 38, 020 users geolo-
cated in the UK in the period between 14/04/2011
and 12/04/2012 (both dates included, ?t = 365
days). D1 is a temporal subset of the data set used
for modelling UK voting intentions in (Lampos et
al., 2013). Geolocation of users was carried out
by matching the location field in their profile with
UK city names on DBpedia as well as by check-
ing that the user?s timezone is set to G.M.T. (Rout
et al., 2013). The use of a common greater geo-
graphical area (UK) was essential in order to derive
a data set with language and topic homogeneity.
A distinct Twitter data set (D2) consisting of ap-
prox. 400 million tweets was formed for learning
term clusters (Section 4.2). D2 was retrieved from
Twitter?s Gardenhose stream (a 10% sample of the
entire stream) from 02/01 to 28/02/2011. D1 and
D2 were processed using TrendMiner?s pipeline
(Preot?iuc-Pietro et al., 2012).
3 User Impact Definition
On the microblogging platform of Twitter, user ?
or, in general, account ? popularity is usually quan-
tified by the raw number of followers (?
in
? 0),
i.e. other users interested in this account. Likewise,
a user can follow others, which we denote as his set
of followees (?
out
? 0). It is expected that users
with high numbers of followers are also popular
in the real world, being well-known artists, politi-
cians, brands and so on. However, non popular
entities, the majority in the social network, can also
gain a great number of followers, by exploiting,
for example, a follow-back strategy.
2
Therefore,
using solely the number of followers to quantify
impact may lead to inaccurate outcomes (Cha et al.,
2010). A natural alternative, the ratio of ?
in
/?
out
is not a reliable metric, as it is invariant to scal-
ing, i.e. it cannot differentiate accounts of the type
{?
in
, ?
out
} = {m,n} and {? ? m, ? ? n}. We
resolve this problem by squaring the number of
followers
(
?
2
in
/?
out
)
; note that the previous expres-
sion is equal to (?
in
? ?
out
)? (?
in
/?
out
) +?
in
and
thus, it incorporates the ratio as well as the differ-
ence between followers and followees.
An additional impact indicator is the number of
times an account has been listed by others (?
?
? 0).
Lists provide a way to curate content on Twitter;
thus, users included in many lists are attractors of
2
An account follows other accounts randomly expecting
that they will follow back.
?5 0 5 10 15 20 25 300
0.05
0.1
0.15
Impact Score (S)
Prob
abilit
y De
nsity
@guardian
@David_Cameron
@PaulMasonNews
@lampos
@nikaletras
@spam?
Figure 1: Histogram of the user impact scores in
our data set. The solid black line represents a gen-
eralised extreme value probability distribution fit-
ted in our data, and the dashed line denotes the
mean impact score (= 6.776). User @spam? is a
sample account with ?
in
= 10, ?
out
= 1000 and
?
?
= 0; @lampos is a very active account, whereas
@nikaletras is a regular user.
interest. Indeed, Pearson?s correlation between ?
in
and ?
?
for all the accounts in our data set is equal
to .765 (p < .001); the two metrics are correlated,
but not entirely and on those grounds, it would be
reasonable to use both for quantifying impact.
Consequently, we have chosen to represent user
impact (S) as a log function of the number of fol-
lowers, followees and listings, given by
S(?
in
, ?
out
, ?
?
) = ln
(
(?
?
+ ?) (?
in
+ ?)
2
?
out
+ ?
)
,
(1)
where ? is a smoothing constant set equal to 1 so
that the natural logarithm is always applied on a
real positive number. Figure 1 shows the impact
score distribution for all the users in our sample,
including some pointers to less or more popular
Twitter accounts. The depicted user impact scores
form the response variable in the regression models
presented in the following sections.
4 User Account Features
This section presents the features used in the user
impact prediction task. They are divided into two
categories: non-textual and text-based. All features
have the joint characteristic of being under the
user?s direct control, something essential for char-
acterising impact based on the actions of a user.
Attributes such as the number of received retweets
or @-mentions (of a user in the tweets of others)
were not considered as they are not controlled by
the account itself.
406
a1
# of tweets
a
2
proportion of retweets
a
3
proportion of non-duplicate tweets
a
4
proportion of tweets with hashtags
a
5
hashtag-tokens ratio in tweets
a
6
proportion of tweets with @-mentions
a
7
# of unique @-mentions in tweets
a
8
proportion of tweets with @-replies
a
9
links ratio in tweets
a
10
# of favourites the account made
a
11
total # of tweets (entire history)
a
12
using default profile background (binary)
a
13
using default profile image (binary)
a
14
enabled geolocation (binary)
a
15
population of account?s location
a
16
account?s location latitude
a
17
account?s location longitude
a
18
proportion of days with nonzero tweets
Table 1: Non textual attributes for a Twitter account
used in the modelling process. All attributes refer
to a set of 365 days (?t) with the exception of a
11
,
the total number of tweets in the entire history of an
account. Attributes a
i
, i ? {2? 6, 8, 9} are ratios
of a
1
, whereas attribute a
18
is a proportion of ?t.
4.1 Non textual attributes
The non-textual attributes (a) are derived either
from general user behaviour statistics or directly
from the account?s profile. Table 1 presents the 18
attributes we extracted and used in our models.
4.2 Text features
We process the text in the tweets of D1 and com-
pute daily unigram frequencies. By discarding
terms that appear less than 100 times, we form
a vocabulary of size |V | = 71, 555. We then form
a user term-frequency matrix of size |U |?|V | with
the mean term frequencies per user during the time
interval ?t. All term frequencies are normalised
with the total number of tweets posted by the user.
Apart from single word frequencies, we are also
interested in deriving a more abstract representa-
tion for each user. To achieve this, we learn word
clusters from a distinct reference corpus (D2) that
could potentially represent specific domains of
discussion (or topics). From a multitude of pro-
posed techniques, we have chosen to apply spec-
tral clustering (Shi and Malik, 2000; Ng et al.,
2002), a hard-clustering method appropriate for
high-dimensional data and non-convex clusters
(von Luxburg, 2007). Spectral clustering performs
graph partitioning on the word-by-word similar-
ity matrix. In our case, tweet-term similarity is
reflected by the Normalised Pointwise Mutual In-
formation (NPMI), an information theoretic mea-
sure indicating which words co-occur in the same
context (Bouma, 2009). We use the random walk
graph Laplacian and only keep the largest compo-
nent of the resulting graph, eliminating most stop
words in the process. The number of clusters needs
to be specified in advance and each cluster?s most
representative words are identified by the following
metric of centrality:
C
w
(c) =
?
v?c
NPMI(w, v)
|c| ? 1
, (2)
where w is the target word and c the cluster it be-
longs (|c| denotes the cluster?s size). Examples of
extracted word clusters are illustrated in Table 4.
Other techniques were also applied, such as online
LDA (Hoffman et al., 2010), but we found that
the results were not satisfactory, perhaps due to
the short message length and the foreign terms co-
occuring within a tweet. After forming the clusters
using D2, we compute a topic score (? ) for each
user-topic pair in D1, representing a normalised
user-word frequency sum per topic.
5 Methods
This section presents the various modelling ap-
proaches for the underlying inference task, the im-
pact score (S) prediction of Twitter users based on
a set of their actions.
5.1 Learning functions for regression
We formulate this problem as a regression task,
i.e. we infer a real numbered value based on a set
of observed features. As a simple baseline, we ap-
ply Ridge Regression (RR) (Hoerl and Kennard,
1970), a reguralised version of the ordinary least
squares. Most importantly, we focus on nonlinear
methods for the impact score prediction task given
the multimodality of the feature space. Recently, it
was shown by Cohn and Specia (2013) that Sup-
port Vector Machines for Regression (SVR) (Vap-
nik, 1998; Smola and Sch?olkopf, 2004), commonly
considered the state-of-the-art for NLP regression
tasks, can be outperformed by Gaussian Processes
(GPs), a kernelised, probabilistic approach to learn-
ing (Rasmussen and Williams, 2006). Their setting
is close to ours, in that they had few (17) features
and were also aiming to predict a complex con-
tinuous phenomenon (human post-editing time).
The initial stages of our experimental process con-
firmed that GPs performed better than SVR; thus,
407
we based our modelling around them, including
RR for comparison.
In GP regression, for the inputs x ? R
d
we want
to learn a function f : R
d
? R that is drawn from
a GP prior
f(x) ? GP
(
m(x), k(x,x
?
)
)
, (3)
where m(x) and k(x,x
?
) denote the mean (set to
0 in our experiments) and covariance (or kernel)
functions respectively. The GP kernel function rep-
resents the covariance between pairs of input. We
wish to limit f to smooth functions over the inputs,
with different smoothness in each input dimension,
assuming that some features are more useful than
others. This can be accommodated by a squared ex-
ponential covariance function with Automatic Rele-
vance Determination (ARD) (Neal, 1996; Williams
and Rasmussen, 1996):
k
ard
(x,x
?
) = ?
2
exp
[
d
?
i
?
(x
i
? x
?
i
)
2
2`
2
i
]
, (4)
where ?
2
denotes the overall variance and `
i
is
the length-scale parameter for feature x
i
; all hy-
perparameters are learned from data during model
inference. Parameter `
i
is inversely proportional to
the feature?s relevancy in the model, i.e. high val-
ues of `
i
indicate a low degree of relevance for the
corresponding x
i
. By setting `
i
= ` in Eq. 4, we
learn a common length-scale for all the dimensions
? this is known as the isotropic squared exponen-
tial function (k
iso
) since it is based purely on the
difference |x ? x
?
|. k
iso
is a preferred choice when
the dimensionality of the input space is high. Hav-
ing set our covariance functions, predictions are
conducted using Bayesian integration
P(y
?
|x
?
,O) =
?
f
P(y
?
|x
?
, f)P(f |O), (5)
where y
?
is the response variable,O a labelled train-
ing set and x
?
the current observation. We learn the
hyperparameters of the model by maximising the
log marginal likelihood P(y|O) using gradient as-
cent. However, inference becomes intractable when
many training instances (n) are present as the num-
ber of computations needed is O(n
3
) (Qui?nonero-
Candela and Rasmussen, 2005). Since our training
samples are tens of thousands, we apply a sparse
approximation method (FITC), which bases param-
eter learning on a few inducing points in the train-
ing set (Qui?nonero-Candela and Rasmussen, 2005;
Snelson and Ghahramani, 2006).
5.2 Models
For predicting user impact on Twitter, we develop
three regression models that build on each other.
The first and simplest one (A) uses only the non-
textual attributes as features; the performance of A
is tested using RR,
3
SVR as well as a GP model.
For SVR we used an RBF kernel (equivalent to
k
iso
), whereas for the GP we applied the following
covariance function
k(a,a
?
) = k
ard
(a,a
?
) + k
noise
(a,a
?
) + ?, (6)
where k
noise
(a,a
?
) = ?
2
? ?(a,a
?
), ? is a Kro-
necker delta function and ? is the regression bias;
this function consists of (|a| + 3) hyperparame-
ters. Note that the sum of covariance functions is
also a valid covariance function (Rasmussen and
Williams, 2006).
The second model (AW) extends model A by
adding word-frequencies as features. The 500 most
frequent terms in D1 are discarded as stop words
and we use the following 2, 000 ones (denoted by
w). Setting x = {a,w}, the covariance function
becomes
k(x,x
?
) = k
ard
(a,a
?
) + k
iso
(w,w
?
)
+ k
noise
(x,x
?
) + ?,
(7)
where we apply k
iso
on the term-frequencies due to
their high dimensionality; the number of hyperpa-
rameters is (|a|+ 5). This is an intermediate model
aiming to evaluate whether the incorporation of
text improves prediction performance.
Finally, in the third model (AC) instead of rely-
ing on the high dimensional space of single words,
we use topic-oriented collections of terms extracted
by applying spectral clustering (see Section 4.2).
By denoting the set of different clusters or topics
as ? and the entire feature space as x = {a,? }, the
covariance function now becomes
k(x,x
?
) = k
ard
(x,x
?
) + k
noise
(x,x
?
) + ?. (8)
The number of hyperparameters is equal to (|a|+
|? |+ 3) and this model is applied for |? | = 50 and
100.
6 Experiments
Here we present the experimental results for the
user impact prediction task and then investigate the
factors that can affect it.
6.1 Predictive Accuracy
We evaluated the performance of the proposed
models via 10-fold cross-validation. Results are
presented in Table 2; Root Mean Squared Error
3
Given that the representation of attributes a
16
and a
17
(latitude, longitude) is ambiguous in a linear model, they were
not included in the RR-based models.
408
Linear (RR) Nonlinear (GP)
Model r RMSE r RMSE
A .667 2.642 .759 2.298
AW .712 2.529 .768 2.263
AC, |? | = 50 .703 2.518 .774 2.234
AC, |? | = 100 .714 2.480 .780 2.210
Table 2: Average performance (RMSE and Pear-
son?s r) derived from 10-fold cross-validation for
the task of user impact score prediction.
Model Top relevant features
A a

13
, a
11
, a
7
, a
1
, a
9
, a
8
, a
18
, a
4
, a
6
, a
3
AW a
7
, a
1
, a
11
, a

13
, a
9
, a
8
, a
18
, a
4
, a
6
, a
15
AC, ? = 50 a

13
, a
11
, a
7
, ?
?
1
, a
1
, a
9
, a
8
, ?
?
2
, a
6
, ?
?
3
AC, ? = 100 a

13
, a
11
, a
7
, a
1
, a
9
, ?
1
, ?
2
, ?
3
, a
18
, a
8
Table 3: The 10 most relevant features in descend-
ing relevance order for all GP models. ?
?
i
and ?
i
denote word clusters (may vary in each model).
6
(RMSE) and Pearson?s correlation (r) between pre-
dictions and responses were used as the perfor-
mance metrics. Overall, the best performance in
terms of both RMSE (2.21 impact points) and lin-
ear correlation (r = .78, p < .001) is achieved
by the GP model (AC) that combines non-textual
attributes with a 100 topic clusters; the difference
in performance with all other models is statistically
significant.
4
The linear baseline (RR) follows the
same pattern of improvement through the differ-
ent models, but never manages to reach the perfor-
mance of the nonlinear alternative. As mentioned
previously, we have also tried SVR with an RBF
kernel for model A (parameters were optimised on
a held-out development set) and the performance
(RMSE: 2.33, r = .75, p < .001) was significantly
worse than the one achieved by the GP model.
4
Notice that when word-based features are intro-
duced in model AW, performance improves. This
was one of the motivations for including text in the
modelling, apart from the notion that the posted
content should also affect general impact. Lastly,
turning this problem from regression to classifi-
cation by creating 3 impact score pseudo-classes
based on the .25 and the .9 quantiles of the re-
sponse variable (4.3 and 11.4 impact score points
respectively) and by using the outputs of model
AC (? = 100) in each phase of the 10-fold cross-
validation, we achieve a 75.86% classification ac-
curacy.
5
4
Indicated by performing a t-test (5% significance level).
5
Similar performance scores can be estimated for different
class threshold settings.
0
100
0
100
0
100
0
100
0 10 20 300
100
0 10 20 30
L H
L H
L H
L H
L H
Tweetszinzentirezhistoryz(?11)
Uniquez@-mentionsz(?7)
Linksz(?9)
@-repliesz(?8)
Dayszwithznonzeroztweetsz(?18)
Figure 2: User impact distribution (x-axis: impact
points, y-axis: # of user accounts) for users with a
low (L) or a high (H) participation in a selection
of relevant non-textual attributes. Dot-dashed lines
denote the respective mean impact score; the red
line is the mean of the entire sample (= 6.776).
6.2 Qualitative Analysis
Given the model?s strong performance, we now
conduct a more thorough analysis to identify and
characterise the properties that affect aspects of
the user impact. GP?s length-scale parameters (`
i
)
? which are inversely proportional to feature rele-
vancy ? are used for ranking feature importance.
Note that since our data set consists of UK users,
some results may be biased towards specific cul-
tural properties.
Non-textual attributes. Table 3 lists the 10 most
relevant attributes (or topics, where applicable) as
extracted in each GP model. Ranking is determined
by the mean value of the length-scale parameter for
each feature in the 10-fold cross-validation process.
We do not show feature ranking derived from the
RR models as we focus on the models with the best
performance. Despite this, it is worth mentioning
6
Length-scales are comparable for features of the same
variance (z-scored). Binary features (denoted by ) are not
z-scored, but for comparison purposes we have rescaled their
length-scale using the feature?s variance.
409
Label ?(`)? ?(`) Cluster?s words ranked by centrality |c|
?
1
: Weather 3.73? 1.80 mph, humidity, barometer, gust, winds, hpa, temperature, kt, #weather [...] 309
?
2
: Healthcare
Finance
Housing
5.44? 1.55 nursing, nurse, rn, registered, bedroom, clinical, #news, estate, #hospital,
rent, healthcare, therapist, condo, investment, furnished, medical, #nyc,
occupational, investors, #ny, litigation, tutors, spacious, foreclosure [...]
1281
?
3
: Politics 6.07? 2.86 senate, republican, gop, police, arrested, voters, robbery, democrats, presi-
dential, elections, charged, election, charges, #religion, arrest, repeal, dems,
#christian, reform, democratic, pleads, #jesus, #atheism [...]
950
?
4
: Showbiz
Movies
TV
7.36? 2.25 damon, potter, #tvd, harry, elena, kate, portman, pattinson, hermione, jen-
nifer, kristen, stefan, robert, catholic, stewart, katherine, lois, jackson, vam-
pire, natalie, #vampirediaries, tempah, tinie, weasley, turner, rowland [...]
1943
?
5
: Commerce 7.83? 2.77 chevrolet, inventory, coupon, toyota, mileage, sedan, nissan, adde, jeep, 4x4,
2002, #coupon, enhanced, #deal, dodge, gmc, 20%, suv, 15%, 2005, 2003,
2006, coupons, discount, hatchback, purchase, #ebay, 10% [...]
608
?
6
: Twitter
Hashtags
8.22? 2.98 #teamfollowback, #500aday, #tfb, #instantfollowback, #ifollowback, #in-
stantfollow, #followback, #teamautofollow, #autofollow, #mustfollow [...]
194
?
7
: Social
Unrest
8.37? 5.52 #egypt, #tunisia, #iran, #israel, #palestine, tunisia, arab, #jan25, iran, israel,
protests, egypt, #yemen, #iranelection, israeli, #jordan, regime, yemen,
#gaza, protesters, #lebanon, #syria, egyptian, #protest, #iraq [...]
321
?
8
: Non English 8.45? 3.80 yg, nak, gw, gue, kalo, itu, aku, aja, ini, gak, klo, sih, tak, mau, buat [...] 469
?
9
: Horoscope
Gambling
9.11? 3.07 horoscope, astrology, zodiac, aries, libra, aquarius, pisces, taurus, virgo,
capricorn, horoscopes, sagitarius, comprehensive, lottery, jackpot [...]
1354
?
10
: Religion
Sports
10.29? 6.27 #jesustweeters, psalm, christ, #nhl, proverbs, unto, salvation, psalms, lord,
kjv, righteousness, niv, bible, pastor, #mlb, romans, awards, nhl [...]
1610
Table 4: The 10 most relevant topics (for model AC, |? | = 100) in the prediction of a user?s impact score
together with their most central words. The topics are ranked by their mean length-scale, ?(`), in the
10-fold cross-validation process (?(`) is the respective standard deviation).
that RR?s outputs also followed similar ranking pat-
terns, e.g. the top 5 features in model A were a
18
,
a
7
, a
3
, a
11
and a
9
. Notice that across all models,
among the strongest features are the total number
of posts either in the entire account?s history (a
11
)
or within the 365-day interval of our experiment
(a
1
) and the number of unique @-mentions (a
7
),
good indicators of user activity and user interaction
respectively. Feature a
13
is also a very good predic-
tor, but is of limited utility for modelling our data
set because very few accounts maintain the default
profile photo (0.4%). Less relevant attributes (not
shown) are the ones related to the location of a
user (a
16
, a
17
) signalling that the whereabouts of a
user may not necessarily relate to impact. Another
low relevance attribute is the number of favourites
that an account did (a
10
), something reasonable, as
those weak endorsements are not affecting the main
stream of content updates in the social network.
In Figure 2, we present the distribution of user
impact for accounts with low (left-side) and high
(right-side) participation in a selection of non-
textual attributes. Low (L) and high (H) participa-
tions are defined by selecting the 500 accounts with
lowest and highest scores for this specific attribute.
The means of (L) and (H) are compared with the
mean impact score in our sample. As anticipated,
accounts with low activity (a
11
) are likely to be
assigned impact scores far below the mean, while
very active accounts may follow a quite opposite
pattern. Avoiding mentioning (a
7
) or replying (a
8
)
to others may not affect (on average) an impact
score positively or negatively; however, accounts
that do many unique @-mentions are distributed
around a clearly higher impact score. On the other
hand, users that overdo @-replies are distributed be-
low the mean impact score. Furthermore, accounts
that post irregularly with gaps longer than a day
(a
18
) or avoid using links in their tweets (a
9
) will
probably appear in the low impact score range.
Topics. Regarding prediction accuracy (Table 2),
performance improves when topics are included.
In turn, some of the topics replace non-textual at-
tributes in the relevancy ranking (Table 3). Table 4
presents the 10 most relevant topic word-clusters
based on their mean length-scale ?(`) in the 10-
fold cross-validation process for the best perform-
ing GP model (AC, |? | = 100). We see that clusters
with their most central words representing topics
such as ?Weather?, ?Healthcare/Finance?, ?Politics?
and ?Showbiz? come up on top.
Contrary to the non-textual attributes, accounts
with low participation in a topic (for the vast major-
ity of topics) were distributed along impact score
values lower than the mean. Based on the fact that
word clusters are not small in size, this is a rational
outcome indicating that accounts with small word-
frequency sums (i.e. the ones that do not tweet
much) will more likely be users with small impact
410
0100
0 10 20 300
100
0 10 20 30 0 10 20 30 0 10 20 30 0 10 20 30
?1 ?2 ?3 ?4 ?5
?6 ?7 ?8 ?9 ?10
Figure 3: User impact distribution (x-axis: impact points, y-axis: # of user accounts) for accounts with a
high participation in the 10 most relevant topics. Dot-dashed lines denote mean impact scores; the red line
is the mean of the entire sample (= 6.776).
Nu
mb
er o
f A
cco
unt
s
Impact Score (S)
0 10 20 300
50
100
All
Low Entropy
High Entropy
Figure 4: User impact distribution for accounts with
high (blue) and low (dark grey) topic entropy. Lines
denote the respective mean impact scores.
scores. Hence, in Figure 3 we only show the user
impact distribution for the 500 accounts with the
top participation in each topic. Informally, this is a
way to quantify the contribution of each domain or
topic of discussion in the impact score. Notice that
the topics which ?push? users towards the highest
impact scores fall into the domains of ?Politics? (?
3
)
and ?Showbiz? (?
4
). An equally interesting observa-
tion is that engaging a lot about a specific topic will
more likely result to a higher than average impact;
the only exception is ?
8
which does not deviate
from the mean, but ?
8
rather represents the use of a
non-English language (Indonesian) and therefore,
does not form an actual topic of discussion.
To further understand how participation in the
10 most relevant topics relates to impact, we also
computed the joint user-topic entropy defined by
H(u
i
, ?) = ?
M
?
j=1
P(u
i
, ?
j
)? log
2
P(u
i
, ?
j
), (9)
where u
i
is a user and M = 10 (Shannon, 2001).
This is a measure of user pseudo-informativeness,
meaning that users with high entropy are consid-
ered as more informative (without assessing the
quality of the information). Figure 4 shows the im-
pact score distributions for the 500 accounts with
the lowest and highest entropy. Low and high en-
tropies are separated, with the former being placed
clearly below the mean user impact score and the
latter above. This pictorial assessment suggests that
a connection between informativeness and impact
may exist, at least in their extremes (their correla-
tion in the entire sample is r = .35, p < .001).
Use case scenarios. Most of the previous analysis
focused on the properties of single features. How-
ever, the user impact prediction models we learn
depend on feature combinations. For that reason,
it is of interest to investigate use case scenarios
that bring various attributes together. To reduce
notation in this paragraph, we use x
+
i
(x is ei-
ther a non-textual attribute a or a topic ? ) to ex-
press x
i
> ?(x
i
), the set of users for which the
value of feature x
i
is above the mean; equivalently
x
?
i
: x
i
< ?(x
i
). We also use ?
?
A
to express the
more complex set {?
+
A
? ?
?
j
? ... ? ?
?
z
}, an inter-
section of users that are active in one topic (?
A
),
but not very active in the rest. Figure 5 depicts the
user impact distributions for five use case scenarios.
Scenario A compares interactive to non interac-
tive users, represented by P(a
+
1
, a
+
6
, a
+
7
, a
+
8
) and
P(a
+
1
, a
?
6
, a
?
7
, a
?
8
) respectively; interactivity, de-
fined by an intersection of accounts that tweet regu-
larly, do many @-mentions and @-replies, but also
411
0 10 20 300
150
300
450
600
750
900 IA
NIA
0 10 20 300
100
200
300
400 IAIAC
0 10 20 300
100
200
300
400
500 L
NL
0 10 20 300
100
200
300
400
500 TO
TF
0 10 20 300
50
100
150
200 LT
ST
A B C D E
Figure 5: User impact distribution (x-axis: impact points, y-axis: # of user accounts) for five Twitter
use scenarios based on subsets of the most relevant attributes and topics ? IA: Interactive, IAC: Clique
Interactive, L: Using many links, TO: Topic-Overall, TF: Topic-Focused, LT: ?Light? topics, ST: ?Serious?
topics. (N) denotes negation and lines the respective mean impact scores.
mention many different users, seems to be rewarded
on average with higher impact scores. Interactive
users gain more impact than clique-interactive ac-
counts represented by P(a
+
1
, a
+
6
, a
?
7
, a
+
8
), i.e. users
who interact, but do not mention many differ-
ent accounts, possibly because they are conduct-
ing discussions with a specific circle only (sce-
nario B). The use of links when writing about
the most prevalent topics (?Politics? and ?Show-
biz?) appears to be an important impact-wise fac-
tor (scenario C); the compared probability distri-
butions in that case were P
(
a
+
1
, (?
+
3
? ?
+
4
), a
+
9
)
against P
(
a
+
1
, (?
+
3
? ?
+
4
), a
?
9
)
. Surprisingly, when
links were replaced by hashtags in the previous
distributions, a clear class separation was not
achieved. In scenario D, topic-focused accounts,
i.e. users that write about one topic consistently,
represented by P
(
a
+
1
, (?
?
2
? ?
?
3
? ?
?
4
? ?
?
7
? ?
?
10
)
)
,
have on average slightly worse impact scores when
compared to accounts tweeting about many top-
ics, P(a
+
1
, ?
+
2
, ?
+
3
, ?
+
4
, ?
+
7
, ?
+
10
). Finally, scenario
E shows thats users engaging about more ?seri-
ous? topics, P
(
a
+
1
, ?
?
4
, ?
?
5
, ?
?
9
, (?
+
3
? ?
+
7
)
)
, were
not differentiated from the ones posting about more
?light? topics, P
(
a
+
1
, (?
+
4
? ?
+
5
? ?
+
9
), ?
?
3
, ?
?
7
)
.
7 Related Work
The task of user-impact prediction based on a ma-
chine learning approach that incorporates text fea-
tures is novel, to the best of our knowledge. De-
spite this fact, our work is partly related to research
approaches for quantifying and analysing user in-
fluence in online social networks. For example,
Cha et al. (2010) compared followers, retweets
and @-mentions received as measures of influ-
ence. Bakshy et al. (2011) aggregated all posts by
each user, computed an individual-level influence
and then tried to predict it by modelling user at-
tributes (# of followers, followees, tweets and date
of joining) together with past user influence. Their
method, based on classification and regression trees
(Breiman, 1984), achieved a modest performance
(r = .34). Furthermore, Romero et al. (2011) pro-
posed an algorithm for determining user influence
and passivity based on information-forwarding ac-
tivity, and Luo et al. (2013) exploited user attributes
to predict retweet occurrences. The primary differ-
ence with all the works described above is that we
aim to predict user impact by exploiting features
under the user?s direct control. Hence, our findings
can be used as indirect insights for strategies that in-
dividual users may follow to increase their impact
score. In addition, we incorporate the actual text
posted by the users in the entire modelling process.
8 Conclusions and Future Work
We have introduced the task of user impact pre-
diction on the microblogging platform of Twitter
based on user-controlled textual and non-textual
attributes. Nonlinear methods, in particular Gaus-
sian Processes, were more suitable than linear ap-
proaches for this problem, providing a strong per-
formance (r = .78). That result motivated the anal-
ysis of specific characteristics in the inferred model
to further define and understand the elements that
affect impact. In a nutshell, activity, non clique-
oriented interactivity and engagement on a diverse
set of topics are among the most decisive impact
factors. In future work, we plan to improve various
modelling components and gain a deeper under-
standing of the derived outcomes in collaboration
with domain experts. For more general conclusions,
the consideration of different cultures and media
sources is essential.
Acknowledgments
This research was supported by EU-FP7-ICT
project n.287863 (?TrendMiner?). Lampos also ac-
knowledges the support from EPSRC IRC project
EP/K031953/1.
412
References
Eytan Bakshy, Jake M. Hofman, Winter A. Mason, and Dun-
can J. Watts. 2011. Everyone?s an influencer: quantifying
influence on Twitter. In 4th International Conference on
Web Search and Data Mining, WSDM?11, pages 65?74.
Gerlof Bouma. 2009. Normalized (pointwise) mutual in-
formation in collocation extraction. In Biennial GSCL
Conference, pages 31?40.
Danah Boyd, Scott Golder, and Gilad Lotan. 2010. Tweet,
Tweet, Retweet: Conversational Aspects of Retweeting on
Twitter. In System Sciences, HICSS?10, pages 1?10.
Leo Breiman. 1984. Classification and regression trees.
Chapman & Hall.
Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011.
Information credibility on Twitter. In 20th International
Conference on World Wide Web, WWW?11, pages 675?
684.
Meeyoung Cha, Hamed Haddadi, Fabricio Benevenuto, and
Krishna P. Gummadi. 2010. Measuring User Influence in
Twitter: The Million Follower Fallacy. In 4th International
Conference on Weblogs and Social Media, ICWSM?10,
pages 10?17.
Trevor Cohn and Lucia Specia. 2013. Modelling Annotator
Bias with Multi-task Gaussian Processes: An Application
to Machine Translation Quality Estimation. In 51st Annual
Meeting of the Association for Computational Linguistics,
ACL?13, pages 32?42.
Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge Re-
gression: Biased Estimation for Nonorthogonal Problems.
Technometrics, 12(1):55?67.
Matthew Hoffman, David Blei, and Francis Bach. 2010. On-
line Learning for Latent Dirichlet Allocation. In Advances
in Neural Information Processing Systems, NIPS?10, pages
856?864.
Bernardo A. Huberman, Daniel M. Romero, and Fang Wu.
2009. Social Networks that Matter: Twitter Under the
Microscope. First Monday, 14(1).
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is Twitter, a social network or a news
media? In 19th International Conference on World Wide
Web, WWW?10, pages 591?600.
Vasileios Lampos and Nello Cristianini. 2012. Nowcast-
ing Events from the Social Web with Statistical Learning.
ACM Transactions on Intelligent Systems and Technology,
3(4):72:1?72:22.
Vasileios Lampos, Daniel Preot?iuc-Pietro, and Trevor Cohn.
2013. A user-centric model of voting intention from Social
Media. In 51st Annual Meeting of the Association for
Computational Linguistics, ACL?13, pages 993?1003.
Zhunchen Luo, Miles Osborne, Jintao Tang, and Ting Wang.
2013. Who will retweet me?: finding retweeters in Twit-
ter. In 36th International Conference on Research and
Development in Information Retrieval, SIGIR?13, pages
869?872.
Radford M. Neal. 1996. Bayesian Learning for Neural Net-
works. Springer.
Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. 2002. On
spectral clustering: Analysis and an algorithm. In Advances
in Neural Information Processing Systems, NIPS?02, pages
849?856.
Daniel Preot?iuc-Pietro, Sina Samangooei, Trevor Cohn,
Nicholas Gibbins, and Mahesan Niranjan. 2012. Trend-
miner: An Architecture for Real Time Analysis of Social
Media Text. In 6th International Conference on Weblogs
and Social Media, ICWSM?12, pages 38?42.
Joaquin Qui?nonero-Candela and Carl E. Rasmussen. 2005.
A unifying view of sparse approximate Gaussian Process
regression. Journal of Machine Learning Research, 6:1939?
1959.
Carl E. Rasmussen and Christopher K. I. Williams. 2006.
Gaussian Processes for Machine Learning. MIT Press.
Daniel M. Romero, Wojciech Galuba, Sitaram Asur, and
Bernardo A. Huberman. 2011. Influence and Passivity
in Social Media. In Machine Learning and Knowledge
Discovery in Databases, volume 6913, pages 18?33.
Dominic Rout, Daniel Preot?iuc-Pietro, Bontcheva Kalina, and
Trevor Cohn. 2013. Where?s @wally: A classification
approach to geolocating users based on their social ties. In
24th Conference on Hypertext and Social Media, HT?13,
pages 11?20.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010.
Earthquake shakes Twitter users: real-time event detection
by social sensors. In 19th International Conference on
World Wide Web, WWW?10, pages 851?860.
Claude E. Shannon. 2001. A mathematical theory of com-
munication. SIGMOBILE Mob. Comput. Commun. Rev.,
5(1):3?55 (reprint with corrections).
Jianbo Shi and Jitendra Malik. 2000. Normalized cuts and
image segmentation. Transactions on Pattern Analysis and
Machine Intelligence, 22(8):888?905.
Alex J. Smola and Bernhard Sch?olkopf. 2004. A tutorial
on support vector regression. Statistics and Computing,
14(3):199?222.
Edward Snelson and Zoubin Ghahramani. 2006. Sparse
Gaussian Processes using Pseudo-inputs. In Advances in
Neural Information Processing Systems, NIPS?06, pages
1257?1264.
Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H. Chi.
2010. Want to be Retweeted? Large Scale Analytics on
Factors Impacting Retweet in Twitter Network. In Social
Computing, SocialCom?10, pages 177?184.
Vladimir N. Vapnik. 1998. Statistical learning theory. Wiley.
Ulrike von Luxburg. 2007. A tutorial on spectral clustering.
Statistics and computing, 17(4):395?416.
Christopher K. I. Williams and Carl E. Rasmussen. 1996.
Gaussian Processes for Regression. In Advances in Neural
Information Processing Systems, NIPS?96, pages 514?520.
413
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 22?27,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Measuring the Similarity between Automatically Generated Topics
Nikolaos Aletras and Mark Stevenson
Department of Computer Science,
University of Sheffield,
Regent Court, 211 Portobello,
Sheffield,
United Kingdom S1 4DP
{n.aletras, m.stevenson}@dcs.shef.ac.uk
Abstract
Previous approaches to the problem of
measuring similarity between automati-
cally generated topics have been based on
comparison of the topics? word probability
distributions. This paper presents alterna-
tive approaches, including ones based on
distributional semantics and knowledge-
based measures, evaluated by compari-
son with human judgements. The best
performing methods provide reliable esti-
mates of topic similarity comparable with
human performance and should be used in
preference to the word probability distri-
bution measures used previously.
1 Introduction
Topic models (Blei et al., 2010) have proved to be
useful for interpreting and organising the contents
of large document collections. It seems intuitively
plausible that some automatically generated topics
will be similar while others are dis-similar. For ex-
ample, a topic about basketball (team game james
season player nba play knicks coach league) is
more similar to a topic about football (world cup
team soccer africa player south game match goal)
than one about the global finance (fed financial
banks federal reserve bank bernanke rule crisis
credit). Methods for automatically determining
the similarity between topics have several poten-
tial applications, such as analysis of corpora to de-
termine topics being discussed (Hall et al., 2008)
or within topic browsers to decide which topics
should be shown together (Chaney and Blei, 2012;
Gretarsson et al., 2012; Hinneburg et al., 2012).
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) is a popular type of topic model but can-
not capture such correlations unless the seman-
tic similarity between topics is measured. Other
topic models, such as the Correlated Topic Model
(CTM) (Blei and Lafferty, 2006), overcome this
limitation and identify correlations between top-
ics.
Approaches to identifying similar topics for a
range of tasks have been described in the litera-
ture but they have been restricted to using informa-
tion from the word probability distribution to com-
pare topics and have not been directly evaluated.
Word distributions have been compared using a
variety of measures such as KL-divergence (Li and
McCallum, 2006; Wang et al., 2009; Newman et
al., 2009), cosine measure (He et al., 2009; Ram-
age et al., 2009) and the average Log Odds Ratio
(Chaney and Blei, 2012). Kim and Oh (2011) also
applied the cosine measure and KL-Divergence
which were compared with four other measures:
Jaccard?s Coefficient, Kendall?s ? coefficient, Dis-
count Cumulative Gain and Jensen Shannon Di-
vergence (JSD).
This paper compares a wider range of ap-
proaches to measuring topic similarity than pre-
vious work. In addition these measures are eval-
uated directly by comparing them against human
judgements.
2 Measuring Topic Similarity
We compare measures based on word probability
distributions (Section 2.1), distributional semantic
methods (Sections 2.2-2.4), knowledge-based ap-
proaches (Section 2.5) and their combination (Sec-
tion 2.6).
2.1 Topic Word Probability Distribution
We first experimented with measures based on
comparison of the topics? word distributions (see
Section 1), by applying the JSD, KL-divergence
and Cosine approaches and the Log Odds Ratio
(Chaney and Blei, 2012).
22
2.2 Topic Model Semantic Space
The semantic space generated by the topic model
can be used to represent the topics and the topic
words. By definition each topic is a probability
distribution over the words in the training corpus.
For a corpus with D documents and V words, a
topic model learns a relation between words and
topics, T , as a T ?V matrix,W, that indicates the
probability of each word in each topic. W is the
topic model semantic space and each topic word
can be represented as a vector, V
i
, with topics as
features weighted by the probability of the word
in each topic. The similarity between two topics
is computed as the average pairwise cosine sim-
ilarity between their top-10 most probable words
(TS-Cos).
2.3 Reference Corpus Semantic Space
Topic words can also be represented as vectors
in a semantic space constructed from an external
source. We adapt the method proposed by Aletras
and Stevenson (2013) for measuring topic coher-
ence using distributional semantics
1
.
Top-N Features A semantic space is con-
structed considering only the top n most frequent
words in Wikipedia (excluding stop words) as con-
text features. Each topic word is represented as a
vector of n features weighted by computing the
Pointwise Mutual Information (PMI) (Church and
Hanks, 1989) between the topic word and each
context feature, PMI(w
i
, w
j
)
?
. ? is a variable for
assigning more importance to higher PMI values.
In our experiments, we set ? = 3 and found that
the best performance is obtained for n = 5000.
Similarity between two topics is defined as the av-
erage cosine similarity of the topic word vectors
(RCS-Cos-N).
Topic Word Space Alternatively, we consider
only the top-10 topic words from the two topics
as context features to generate topic word vectors.
Then, topic similarity is computed as the pairwise
cosine similarity of the topic word vectors (RCS-
Cos-TWS).
Word Association Topic similarity can also be
computed by applying word association measures
directly. Newman et al. (2010) measure topic
coherence as the average PMI between the topic
words. This approach can be adapted to measure
1
Wikipedia is used as a reference corpus to count word
co-occurrences and frequencies using a context window of
?10 words centred on a topic word.
topic similarity by computing the average pairwise
PMI between the topic words in two topics (PMI).
2.4 Training Corpus Semantic Space
Term-Document Space A matrixX can be cre-
ated using the training corpus. Each term (row)
represents a topic word vector. Element x
ij
in X
is the tf.idf of the term i in document j. Topic
similarity is computed as the pairwise cosine sim-
ilarity of the topic word vectors (TCS-Cos-TD).
Word Co-occurrence in Training Documents
Alternatively, we generate a matrix Z of co-
document frequencies. The matrix Z consists of
V rows and columns representing the V vocab-
ulary words. Element z
ij
is the log of the num-
ber of documents that contains the words i and
j normalised by the document frequency, DF, of
the word j. Mimno et al. (2011) introduced that
metric to measure topic coherence. We adapted
it to estimate topic similarity by aggregating the
co-document frequency of the words between two
topics (Doc-Co-occ).
2.5 Knowledge-based Methods
UKB (Agirre et al., 2009) is used to generate a
probability distribution over WordNet synsets for
each word in the vocabulary V of the topic model
using the Personalized PageRank algorithm. The
similarity between two topic words is calculated
by transforming these distributions into vectors
and computing the cosine metric. The similar-
ity between two topics is computed by measur-
ing pairwise similarity between their top-10 topic
words and selecting the highest score.
Explicit Semantic Analysis (ESA) proposed by
Gabrilovich and Markovitch (2007) transforms
the topic keywords into vectors that consist of
Wikipedia article titles weighted by their relevance
to the keyword. For each topic, the centroid is
computed from the keyword vectors. Similarity
between topics is computed as the cosine similar-
ity of the ESA centroid vectors.
2.6 Feature Combination Using SVR
We also evaluate the performance of a support
vector regression system (SVR) (Vapnik, 1998)
with a linear kernel using a combination of ap-
proaches described above as features
2
. The system
is trained and tested using 10-fold cross validation.
2
With the exception of JSD, features based on the topics?
word probability distributions were not used by SVR since it
was found that including them reduced performance.
23
3 Evaluation
Data We created a data set consisting of pairs of
topics generated by two topic models (LDA and
CTM) over two document collections using differ-
ent numbers of topics. The first consists of 47,229
news articles from New York Times (NYT) in the
GigaWord corpus and the second contains 50,000
articles from ukWAC (Baroni et al., 2009). Each
article is tokenised then stop words and words ap-
pearing fewer than five times in the corpora re-
moved. This results in a total of 57,651 unique to-
kens for the NYT corpus and 72,672 for ukWAC.
LDA Topics are learned by training LDA mod-
els over the two corpora using gensim
3
. The num-
ber of topics is set to T = 50, 100, 200 and hy-
perparameters, ? and ?, are set to
1
T
. Randomly
selecting pairs of topics will result to a data set
in which the majority of pairs would not be simi-
lar. We overcome that problem by assuming that
the JSD between likely relevant pairs will be low
while it will be higher for less relevant pairs of
topics. We selected 800 pairs of topics. 600 pairs
represent topics with similar word distributions (in
the top 6 most relevant topics ranked by JSD). The
remaining 200 pairs were selected randomly.
CTM is trained using the EM algorithm
4
. The
number of topics to learn is set to T =
50, 100, 200 and the rest of the settings are set to
their default values. The topic graph generated by
CTM was used to create all the possible pairs be-
tween topics that are connected. This results in a
total of 70, 468 and 695 pairs in NYT, and a total
of 80, 246 and 258 pairs in ukWAC for the 50, 100
and 200 topics respectively.
Incoherent topics are removed using an ap-
proach based on distributional semantics (Aletras
and Stevenson, 2013). Each topic is represented
using the top 10 words with the highest marginal
probability.
Human Judgements of Topic Similarity were
obtained using an online crowdsourcing platform,
Crowdflower. Annotators were provided with
pairs of topics and were asked to judge how simi-
lar the topics are by providing a rating on a scale of
0 (completely unrelated) to 5 (identical). The av-
erage response for each pair was calculated in or-
der to create the final similarity judgement for use
as a gold-standard. The average Inter-Annotator
3
http://radimrehurek.com/gensim
4
http://www.cs.princeton.edu/
?
blei/
ctm-c/index.html
agreement (IAA) across all pairs for all of the col-
lections is in the range of 0.53-0.68. The data set
together with gold-standard annotations is freely
available
5
.
4 Results
Table 1 shows the correlation (Spearman) between
the topic similarity metrics described in Section 2
and average human judgements for the LDA and
CTM topic pairs. It also shows the performance
of a Word Overlap baseline which measures the
number of terms that two topics have in common
normalised by the total number of topic terms.
The correlations obtained using the topics?
word probability distributions (Section 2.1), i.e.
JSD, KL-divergence and Cos, are comparable with
the baseline for all of the topic collections and
topic models. The metric proposed by Chaney
and Blei (2012) also compares probability distri-
butions and fails to perform well on either data
set. These results suggest that these metrics may
be sensitive to the high dimensionality of the vo-
cabulary. They also assign high similarity to top-
ics that contain ambiguous words, resulting in low
correlations with human judgements.
Performance of the cosine of the word vec-
tor (TS-Cos) in the Topic Model Semantic Space
(Section 2.2) varies implying that the quality of the
latent space generated by LDA and CTM is sensi-
tive to the number of topics.
The similarity metrics that use the reference
corpus (Section 2.3) consistently produce good
correlations for topic pairs generated using both
LDA and CTM. The best overall correlation for a
single feature in most cases is obtained using av-
erage PMI (in a range of 0.43-0.74). The perfor-
mance of the distributional semantic metric using
the Topic Word Space (RCS-Cos-TWS) is com-
parable and slightly lower for the top-N features
(RCS-Cos-N). This indicates that the reference
corpus covers a broader range of semantic subjects
than the latent space produced by the topic model.
When the term-document matrix from the train-
ing corpus is used as a vector space (Section 2.4)
performance is worse than when the reference
corpus is used. In addition, using co-document
frequency derived from the training corpus does
not correlate particularly well with human judge-
ments. These methods are sensitive to the size
of the corpus, which may be too small to gener-
5
http://staffwww.dcs.shef.ac.uk/
people/N.Aletras/resources/topicSim.
tar.gz
24
Spearman?s r
LDA CTM
NYT ukWAC NYT ukWAC
Method 50 100 200 50 100 200 50 100 200 50 100 200
Baseline
Word Overlap 0.32 0.40 0.51 0.22 0.32 0.41 0.56 0.45 0.49 0.35 0.33 0.53
Topic Word Probability Distribution
JSD 0.37 0.44 0.53 0.29 0.30 0.34 0.59 0.43 0.49 0.38 0.34 0.60
KL-Divergence 0.29 0.29 0.41 0.20 0.24 0.33 0.54 0.39 0.56 0.31 0.29 0.47
Cos 0.31 0.37 0.59 0.30 0.30 0.36 0.58 0.45 0.52 0.50 0.40 0.58
Chaney and Blei (2012) 0.16 0.26 0.18 0.29 0.21 0.25 0.29 0.40 0.31 -0.23 0.12 0.61
Topic Model Semantic Space
TS-Cos 0.35 0.41 0.67 0.29 0.35 0.42 0.67 0.51 0.49 0.51 0.42 0.42
Reference Corpus Semantic Space
RCS-Cos-N 0.37 0.46 0.61 0.35 0.32 0.39 0.60 0.47 0.61 0.57 0.42 0.41
RCS-Cos-TWS 0.40 0.54 0.70 0.38 0.43 0.51 0.63 0.59 0.62 0.60 0.55 0.54
PMI 0.43 0.63 0.74 0.43 0.53 0.64 0.68 0.70 0.64 0.58 0.62 0.64
Training Corpus Semantic Space
TCS-Cos-TD 0.36 0.42 0.67 0.29 0.31 0.40 0.64 0.54 0.58 0.49 0.43 0.43
Doc-Co-occ 0.28 0.29 0.45 0.28 0.22 0.30 0.65 0.36 0.57 0.31 0.26 0.34
Knowledge-based
UKB 0.25 0.38 0.56 0.22 0.35 0.41 0.52 0.41 0.40 0.41 0.43 0.42
ESA 0.43 0.58 0.71 0.46 0.55 0.61 0.69 0.67 0.64 0.70 0.62 0.61
Feature Combination
SVR 0.46 0.64 0.75 0.46 0.58 0.66 0.72 0.71 0.62 0.60 0.65 0.66
IAA 0.54 0.58 0.61 0.53 0.56 0.60 0.68 0.68 0.64 0.67 0.63 0.64
Table 1: Results for various approaches to topic similarity. All correlations are significant p < 0.001.
Underlined scores denote best performance of a single feature. Bold denotes best overall performance.
ate reliable estimates of tf.idf or co-document fre-
quency.
ESA, one of the knowledge-based methods
(Section 2.5), performs well and is comparable to
(or in some cases better than) PMI. UKB does
not perform particularly well because the topics
often contain named entities that do not exist in
WordNet. ESA is based on Wikipedia and does
not suffer from this problem. Overall, metrics for
computing topic similarity based on rich semantic
resources (e.g. Wikipedia) are more appropriate
than metrics based on the topic model itself be-
cause of the limited size of the training corpus.
Combining the features using SVR gives the
best overall result for LDA (in the range 0.46-
0.75) and CTM (0.60-0.72). However, the fea-
ture combination performs slightly lower than the
best single feature in two cases when CTM is
used (T=200, NYT and T=50, ukWAC). Analy-
sis of the coefficients produced by the SVR in
each fold demonstrated that including JSD and
the Word Overlap reduce SVR performance. We
repeated the experiments by removing these fea-
tures
6
which resulted in higher correlations (0.64
and 0.65 respectively).
Another interesting observation is that using
LDA the correlations of the various similarity met-
6
These features are useful for the other experiments since
performance drops when they are removed.
rics with human judgements increase with the
number of topics for both corpora. This result
is consistent with the findings of Stevens et al.
(2012) that topic model coherence increases with
the number of topics. Fewer topics makes the task
of identifying similar topics more difficult because
it is likely that they will contain some terms that do
not relate to the topic?s main subject. Correlations
in CTM are more stable for different number of
topics because of the nature of the model, the pairs
have been generated using the topic graph which
by definition contains correlated topics.
5 Conclusions
We explored the task of determining the similar-
ity between pairs of automatically generated top-
ics and described a range of approaches to the
problem. We constructed a data set of pairs of
topics generated by two topic models, LDA and
CTM, together with human judgements of simi-
larity. The data set was used to evaluate a wide
range of approaches. The most interesting finding
is the poor performance of the metrics based on
word probability distributions previously used for
this task. Our results demonstrate that word asso-
ciation measures, such as PMI, and state-of-the-art
textual similarity metrics, such as ESA, are more
appropriate.
25
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics (NAACL-HLT ?09), pages 19?27, Boulder, Col-
orado.
Nikolaos Aletras and Mark Stevenson. 2013. Evaluat-
ing topic coherence using distributional semantics.
In Proceedings of the 10th International Conference
on Computational Semantics (IWCS 2013) ? Long
Papers, pages 13?22, Potsdam, Germany.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide
web: a collection of very large linguistically pro-
cessed web-crawled corpora. Language resources
and evaluation, 43(3):209?226.
David Blei and John Lafferty. 2006. Correlated topic
models. In Y. Weiss, B. Sch?olkopf, and J. Platt,
editors, Advances in Neural Information Processing
Systems 18, pages 147?154. MIT Press, Cambridge,
MA.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
David Blei, Lawrence Carin, and David Dunson. 2010.
Probabilistic topic models. Signal Processing Mag-
azine, IEEE, 27(6):55?65.
Allison June-Barlow Chaney and David M. Blei. 2012.
Visualizing topic models. In Proceedings of the
Sixth International AAAI Conference on Weblogs
and Social Media, Dublin, Ireland.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicog-
raphy. In Proceedings of the 27th Annual Meeting
of the Association for Computational Linguistics,
pages 76?83, Vancouver, British Columbia, Canada.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the International Joint Conference on Artificial In-
telligence (IJCAI ?07), pages 1606?1611.
Brynjar Gretarsson, John O?Donovan, Svetlin Bostand-
jiev, Tobias H?ollerer, Arthur Asuncion, David New-
man, and Padhraic Smyth. 2012. TopicNets: Visual
analysis of large text corpora with topic modeling.
ACM Trans. Intell. Syst. Technol., 3(2):23:1?23:26.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 363?371, Honolulu, Hawaii.
Qi He, Bi Chen, Jian Pei, Baojun Qiu, Prasenjit Mi-
tra, and Lee Giles. 2009. Detecting topic evolution
in scientific literature: how can citations help? In
Proceedings of the 18th ACM Conference on Infor-
mation and Knowledge Management (CIKM ?09),
pages 957?966, Hong Kong, China.
Alexander Hinneburg, Rico Preiss, and Ren?e Schr?oder.
2012. TopicExplorer: Exploring document collec-
tions with topic models. In Peter A. Flach, Tijl
Bie, and Nello Cristianini, editors, Machine Learn-
ing and Knowledge Discovery in Databases, volume
7524 of Lecture Notes in Computer Science, pages
838?841. Springer Berlin Heidelberg.
Dongwoo Kim and Alice Oh. 2011. Topic chains
for understanding a news corpus. In Computational
Linguistics and Intelligent Text Processing, pages
163?176. Springer.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In Proceedings of the 23rd International
Conference on Machine Learning (ICML ?06), pages
577?584.
David Mimno, Hanna Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
262?272, Edinburgh, Scotland, UK.
David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms for
topic models. J. Mach. Learn. Res., 10:1801?1828.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (NAACL-HLT ?10), pages 100?108, Los
Angeles, California.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA:
A supervised topic model for credit attribution in
multi-labeled corpora. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ?09), pages 248?256,
Singapore.
Keith Stevens, Philip Kegelmeyer, David Andrzejew-
ski, and David Buttler. 2012. Exploring topic co-
herence over many models and many topics. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP
?12), pages 952?961, Jeju Island, Korea.
Vladimir N Vapnik. 1998. Statistical learning theory.
Wiley, New York.
26
Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen.
2009. Mining common topics from multiple asyn-
chronous text streams. In Proceedings of the Sec-
ond ACM International Conference on Web Search
and Data Mining (WSDM ?09), pages 192?201,
Barcelona, Spain.
27
Proceedings of NAACL-HLT 2013, pages 158?167,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Representing Topics Using Images
Nikolaos Aletras and Mark Stevenson
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP, UK
{n.aletras, m.stevenson}@dcs.shef.ac.uk
Abstract
Topics generated automatically, e.g. using
LDA, are now widely used in Computational
Linguistics. Topics are normally represented
as a set of keywords, often the n terms in a
topic with the highest marginal probabilities.
We introduce an alternative approach in which
topics are represented using images. Candi-
date images for each topic are retrieved from
the web by querying a search engine using the
top n terms. The most suitable image is se-
lected from this set using a graph-based al-
gorithm which makes use of textual informa-
tion from the metadata associated with each
image and features extracted from the images
themselves. We show that the proposed ap-
proach significantly outperforms several base-
lines and can provide images that are useful to
represent a topic.
1 Introduction
Topic models are statistical methods for summaris-
ing the content of a document collection using latent
variables known as topics (Hofmann, 1999; Blei et
al., 2003). Within a model, each topic is a multino-
mial distribution over words in the collection while
documents are represented as distributions over top-
ics. Topic modelling is now widely used in Natural
Language Processing (NLP) and has been applied to
a range of tasks including word sense disambigua-
tion (Boyd-Graber et al, 2007), multi-document
summarisation (Haghighi and Vanderwende, 2009),
information retrieval (Wei and Croft, 2006), image
labelling (Feng and Lapata, 2010a) and visualisation
of document collections (Chaney and Blei, 2012).
Topics are often represented by using the n terms
with the highest marginal probabilities in the topic to
generate a set of keywords. For example, wine, bot-
tle, grape, flavour, dry. Interpreting such lists may
not be straightforward, particularly since there may
be no access to the source collection used to train the
model. Therefore, researchers have recently begun
developing automatic methods to generate meaning-
ful and representative labels for topics. These tech-
niques have focussed on the creation of textual la-
bels (Mei et al, 2007; Lau et al, 2010; Lau et al,
2011).
An alternative approach is to represent a topic us-
ing an illustrative image (or set of images). Im-
ages have the advantage that they can be under-
stood quickly and are language independent. This is
particularly important for applications in which the
topics are used to provide an overview of a collec-
tion with many topics being shown simultaneously
(Chaney and Blei, 2012; Gretarsson et al, 2012;
Hinneburg et al, 2012).
This paper explores the problem of selecting im-
ages to illustrate automatically generated topics.
Our approach generates a set of candidate images for
each topic by querying an image search engine with
the top n topic terms. The most suitable image is
selected using a graph-based method that makes use
of both textual and visual information. Textual in-
formation is obtained from the metadata associated
with each image while visual features are extracted
from the images themselves. Our approach is evalu-
ated using a data set created for this study that was
annotated by crowdsourcing. Results of the evalu-
ation show that the proposed method significantly
158
outperforms three baselines.
The contributions of this paper are as follows: (1)
introduces the problem of labelling topics using im-
ages; (2) describes an approach to this problem that
makes use of multimodal information to select im-
ages from a set of candidates; (3) introduces a data
set to evaluate image labelling; and (4) evaluates the
proposed approach using this data set.
2 Related work
In early research on topic modelling, labels were
manually assigned to topics for convenient presen-
tation of research results (Mei and Zhai, 2005; Teh
et al, 2006).
The first attempt at automatically assigning la-
bels to topics is described by Mei et al (2007).
In their approach, a set of candidate labels are ex-
tracted from a reference collection using chunking
and statistically important bigrams. Then, a rele-
vance scoring function is defined which minimises
the Kullback-Leibler divergence between word dis-
tribution in a topic and word distribution in candi-
date labels. Candidate labels are ranked according
to their relevance and the top ranked label chosen to
represent the topic.
Magatti et al (2009) introduced an approach
for labelling topics that relied on two hierarchi-
cal knowledge resources labelled by humans, the
Google Directory and the OpenOffice English The-
saurus. A topics tree is a pre-existing hierarchi-
cal structure of labelled topics. The Automatic La-
belling Of Topics algorithm computes the similarity
between LDA inferred topics and topics in a topics
tree by computing scores using six standard similar-
ity measures. The label for the most similar topic in
the topic tree is assigned to the LDA topic.
Lau et al (2010) proposed selecting the most rep-
resentative word from a topic as its label. A la-
bel is selected by computing the similarity between
each word and all the others in the topic. Sev-
eral sources of information are used to identify the
best label including Pointwise Mutual Information
scores, WordNet hypernymy relations and distribu-
tional similarity. These features are combined in a
reranking model to achieve results above a baseline
(the most probable word in the topic).
In more recent work, Lau et al (2011) proposed
a method for automatically labelling topics by mak-
ing use of Wikipedia article titles as candidate la-
bels. The candidate labels are ranked using infor-
mation from word association measures, lexical fea-
tures and an Information Retrieval technique. Re-
sults showed that this ranking method achieves bet-
ter performance than a previous approach (Mei et al,
2007).
Mao et al (2012) introduced a method for la-
belling hierarchical topics which makes use of sib-
ling and parent-child relations of topics. Candidate
labels are generated using a similar approach to the
one used by Mei et al (2007). Each candidate la-
bel is then assigned a score by creating a distribu-
tion based on the words it contains and measuring
the Jensen-Shannon divergence between this and a
reference corpus.
Hulpus et al (2013) make use of the structured
data in DBpedia1 to label topics. Their approach
maps topic words to DBpedia concepts. The best
concepts are identified by applying graph central-
ity measures which assume that words that co-
occurring in text are likely to refer to concepts that
are close in the DBpedia graph.
Our own work differs from the approaches de-
scribed above since, to our knowledge, it is the first
to propose labelling topics with images rather than
text.
Recent advances in computer vision has lead to
the development of reliable techniques for exploit-
ing information available in images (Datta et al,
2008; Szeliski, 2010) and these have been combined
with NLP (Feng and Lapata, 2010a; Feng and Lap-
ata, 2010b; Agrawal et al, 2011; Bruni et al, 2011).
The closest work to our own is the text illustration
techniques which have been proposed for story pic-
turing (Joshi et al, 2006) and news articles illustra-
tion (Feng and Lapata, 2010b). The input to text il-
lustration models is a textual document and a set of
image candidates. The goal of the models is to as-
sociate the document with the correct image. More-
over, the problem of ranking images returned from
a text query is related to, but different from, the
one explored in our paper. Those approaches used
queries that were much smaller (e.g. between one
and three words) and more focussed than the ones
1http://dbpedia.org
159
we use (Jing and Baluja, 2008). In our work, the in-
put is a topic and the aim is to associate it with an
image, or images, denoting the main thematic sub-
ject.
3 Labelling Topics
In this section we propose an approach to identify-
ing images to illustrate automatically generated top-
ics. It is assumed that there are no candidate images
available so the first step (Section 3.1) is to generate
a set of candidate images. However, when a candi-
date set is available the first step can be skipped.
3.1 Selecting Candidate Images
For the experiments presented here we restrict our-
selves to using images from Wikipedia available un-
der the Creative Commons licence, since this allows
us to make the data available. The top-5 terms from
a topic are used to query Google using its Custom
Search API2. The search is restricted to the English
Wikipedia3 with image search enabled. The top-20
images retrieved for each search are used as candi-
dates for the topic.
3.2 Feature Extraction
Candidate images are represented by two modalities
(textual and visual) and features extracted for each.
3.2.1 Textual Information
Each image?s textual information consists of the
metadata retrieved by the search. The assumption
here is that image?s metadata is indicative of the im-
age?s content and (at least to some extent) related to
the topic. The textual information is formed by con-
catenating the title and the link fields of the search
result. These represent, respectively, the web page
title containing the image and the image file name.
The textual information is preprocessed by tokeniz-
ing and removing stop words.
3.2.2 Visual Information
Visual information is extracted using low-level
image keypoint descriptors, i.e. SIFT features
2https://developers.google.com/
apis-explorer/#s/customsearch/v1
3http://en.wikipedia.org
(Lowe, 1999; Lowe, 2004) sensitive to colour in-
formation. SIFT features denote ?interesting? ar-
eas in an image. Image features are extracted us-
ing dense sampling and described using Opponent
colour SIFT descriptors provided by the colorde-
scriptor4 software. Opponent colour SIFT descrip-
tors have been found to give the best performance
in object scene and face recognition (Sande et al,
2008). The SIFT features are clustered to form a vi-
sual codebook of 1,000 visual words using K-Means
such that each feature is mapped to a visual word.
Each image is represented as a bag-of-visual words
(BOVW).
3.3 Ranking Candidate Images
We rank images in the candidates set using graph-
based algorithms. The graph is created by treating
images as nodes and using similarity scores (textual
or visual) between images to weight the edges.
3.3.1 PageRank
PageRank (Page et al, 1999) is a graph-based al-
gorithm for identifying important nodes in a graph
that was originally developed for assigning impor-
tance to web pages. It has been used for a range
of NLP tasks including word sense disambiguation
(Agirre and Soroa, 2009) and keyword extraction
(Mihalcea and Tarau, 2004).
Let G = (V,E) be a graph with a set of ver-
tices, V , denoting image candidates and a set of
edges, E, denoting similarity scores between two
images. For example, sim(Vi, Vj) indicates the sim-
ilarity between images Vi and Vj . The PageRank
score (Pr) over G for an image (Vi) can be com-
puted by the following equation:
Pr(Vi) = d ?
?
Vj?C(Vi)
sim(Vi, Vj)
?
Vk?C(Vj)
sim(Vj , Vk)
Pr(Vj) + (1 ? d)v
(1)
where C(Vi) denotes the set of vertices which are
connected to the vertex Vi. d is the damping factor
which is set to the default value of d = 0.85 (Page et
al., 1999). In standard PageRank all elements of the
vector v are the same, 1N where N is the number of
nodes in the graph.
4http://koen.me/research/
colordescriptors
160
3.3.2 Personalised PageRank
Personalised PageRank (PPR) (Haveliwala et al,
2003) is a variant of the PageRank algorithm in
which extra importance is assigned to certain ver-
tices in the graph. This is achieved by adjusting the
values of the vector v in equation 1 to prefer certain
nodes. Nodes that are assigned high values in v are
more likely to also be assigned a high PPR score.
We make use of PPR to prefer images with textual
information that is similar to the terms in the topic.
3.3.3 Weighting Graph Edges
Three approaches were compared for computing
the values of sim(Vi, Vj) in equation 1 used to
weight the edges of the graph. Two of these make
use of the textual information associated with each
image while the final one relies on visual features.
The first approach is Pointwise Mutual Infor-
mation (PMI). The similarity between a pair of
images (vertices in the graph) is computed as the
average PMI between the terms in their metadata.
PMI is computed using word co-occurrence counts
over Wikipedia identified using a sliding window of
length 20. We also experimented with other word
association measures but these did not perform as
well. The PageRank over the graph weighted using
PMI is denoted as PRPMI.
The second approach, Explicit Semantic Anal-
ysis (ESA) (Gabrilovich and Markovitch, 2007), is
a knowledge-based similarity measure. ESA trans-
forms the text from the image metadata into vectors
that consist of Wikipedia article titles weighted by
their relevance. The similarity score between these
vectors is computed as the cosine of the angle be-
tween them. This similarity measure is used to cre-
ate the graph and its PageRank is denoted as PRESA.
The final approach uses the visual features ex-
tracted from the images themselves. The visual
words extracted from the images are used to form
feature vectors and the similarity between a pair of
images computed as the cosine of the angle between
them. The PageRank of the graph created using this
approach is PRvis and it is similar to the approach
proposed by Jing and Baluja (2008) for associating
images to text queries.
3.3.4 Initialising the Personalisation Vector
The personalisation vector (see above) is
weighted using the similarity scores computed be-
tween the topic and its image candidates. Similarity
is computed using PMI and ESA (see above). When
PMI and ESA are used to weight the personalisation
vector they compute the similarity between the
top 10 terms for a topic and the textual metadata
associated with each image in the set of candidates.
We refer to the personalisation vectors created
using PMI and ESA as Per(PMI) and Per(ESA)
respectively.
Using PPR allows information about the simi-
larity between the images? metadata and the topics
themselves to be considered when identifying a suit-
able image label. The situation is different when
PageRank is used since this only considers the sim-
ilarity between the images in the candidate set.
The personalisation vector used by PPR is em-
ployed in combination with a graph created us-
ing one of the approaches described above. For
example, the graph may be weighted using vi-
sual features and the personalisation vector created
using PMI scores. This approach is denoted as
PRvis+Per(PMI).
4 Evaluation
This section discusses the experimental design for
evaluating the proposed approaches to labelling top-
ics with images. To our knowledge no data set for
evaluating these approaches is currently available
and consequently we developed one for this study5.
Human judgements about the suitability of images
are obtained through crowdsourcing.
4.1 Data
We created a data set of topics from two collections
which cover a broad thematic range:
? NYT 47,229 New York Times news articles
(included in the GigaWord corpus) that were
published between May and December 2010.
? WIKI A set of Wikipedia categories randomly
selected by browsing its hierarchy in a breadth-
first-search manner starting from a few seed
5Data set can be downloaded from http://staffwww.
dcs.shef.ac.uk/people/N.Aletras/resources.
html.
161
police, officer, crime, street, man, city, gang, suspect, arrested, violence
game, season, team, patriot, bowl, nfl, quarterback, week, play, jet
military, afghanistan, force, official, afghan, defense, pentagon, american, war, gates
Figure 1: A sample of topics and their top-3 image candidates (i.e. with the highest average human annota-
tions).
categories (e.g. SPORTS, POLITICS, COMPUT-
ING). Categories that have more that 80 articles
associated with them are considered. These
articles are collected to produce a corpus of
approximately 60,000 articles generated from
1,461 categories.
Documents in the two collections are tokenised
and stop words removed. LDA was applied to learn
200 topics from NYT and 400 topics from WIKI.
The gensim package6 was used to implement and
compute LDA. The hyperparameters (?, ?) were set
to 1num of topics . Incoherent topics are filtered out
by applying the method proposed by Aletras and
Stevenson (2013).
We randomly selected 100 topics from NYT and
200 topics from WIKI resulting in a data set of 300
topics. Candidate images for these topics were gen-
erated using the approach described in Section 3.1,
producing a total of 6,000 candidate images (20 for
6http://pypi.python.org/pypi/gensim
each topic).
4.2 Human Judgements of Image Relevance
Human judgements of the suitability of each im-
age were obtained using an online crowdsourcing
platform, Crowdflower7. Annotators were provided
with a topic (represented as a set of 10 keywords)
and a candidate image. They were asked to judge
how appropriate the image was as a representation
of the main subject of the topic and provide a rating
on a scale of 0 (completely unsuitable) to 3 (very
suitable).
Quality control is important in crowdscourcing
experiments to ensure reliability (Kazai, 2011). To
avoid random answers, control questions with obvi-
ous answer were included in the survey. Annotations
by participants that failed to answer these questions
correctly or participants that gave the same rating for
all pairs were removed.
7http://crowdflower.com
162
The total number of filtered responses obtained
was 62, 221 from 273 participants. Each topic-
image pair was rated at least by 10 subjects. The
average response for each pair was calculated in or-
der to create the final similarity judgement for use as
a gold-standard. The average variance across judges
(excluding control questions) is 0.88.
Inter-Annotator agreement (IAA) is computed as
the average Spearman?s ? between the ratings given
by an annotator and the average ratings given by all
other annotators. The average IAA across all topics
was 0.50 which indicates the difficulty of the task,
even for humans.
Figure 1 shows three example topics from the data
set together with the images that received the highest
average score from the annotators.
4.3 Evaluation Metrics
Evaluation of the topic labelling methods is carried
out using a similar approach to the framework pro-
posed by Lau et al (2011) for labelling topics using
textual labels.
Top-1 average rating is the average human rating
assigned to the top-ranked label proposed by the sys-
tem. This provides an indication of the overall qual-
ity of the image the system judges as the best one.
The highest possible score averaged across all top-
ics is 2.68, since for many topics the average score
obtained from the human judgements is lower than
3.
The second evaluation measure is the normalized
discounted cumulative gain (nDCG) (Ja?rvelin and
Keka?la?inen, 2002; Croft et al, 2009) which com-
pares the label ranking proposed by the system to
the optimal ranking provided by humans. The dis-
counted cumulative gain at position p (DCGp) is
computed using the following equation:
DCGp = rel1 +
p?
i=2
reli
log2(i)
(2)
where reli is the relevance of the label to the topic
in position i. Then nDCG is computed as:
nDCGp =
DCGp
IDCGp
(3)
where IDCGp is the optimal ranking of the image
labels, in our experiments this is the ranking pro-
vided by the scores in the human annotated data set.
We follow Lau et al (2011) in computing nDCG-1,
nDCG-3 and nDCG-5 for the top 1, 3 and 5 ranked
system image labels respectively.
4.4 Baselines
Since there are no previous methods for labelling
topics using images, we compare our proposed mod-
els against three baselines.
The Random baseline randomly selects a label
for the topic from the 20 image candidates. The pro-
cess is repeated 10,000 times and the average score
of the selected labels is computed for each topic.
The more informed Word Overlap baseline se-
lects the image that is most similar to the topic terms
by applying a Lesk-style algorithm (Lesk, 1986) to
compare metadata for each image against the topic
terms. It is defined as the number of common terms
between a topic and image candidate normalised by
the total number of terms in the topic and image?s
metadata.
We also compared our approach with the ranking
returned by the Google Image Search for the top-20
images for a specific topic.
4.5 User Study
A user study was conducted to estimate human per-
formance on the image selection task. Three annota-
tors were recruited and asked to select the best image
for each of the 300 topics in the data set. The anno-
tators were provided with the topic (in the form of a
set of keywords) and shown all candidate images for
that topic before being asked to select exactly one.
The Average Top-1 Rating was computed for each
annotator and the mean of these values was 2.24.
5 Results
Table 1 presents the results obtained for each of the
methods on the collection of 300 topics. Results are
shown for both Top-1 Average rating and nDCG.
We begin by discussing the results obtained us-
ing the standard PageRank algorithm applied to
graphs weighted using PMI, ESA and visual features
(PRPMI, PRESA and PRvis respectively). Results us-
ing PMI consistently outperform all baselines and
those obtained using ESA. This suggests that distri-
butional word association measures are more suit-
able for identifying useful images than knowledge-
based similarity measures. The best results using
163
Model Top-1 Av. Rating nDCG-1 nDCG-3 nDCG-5
Baselines
Random 1.79 - - -
Word Overlap 1.85 0.69 0.72 0.74
Google Image Search 1.89 0.73 0.75 0.77
PageRank
PRPMI 1.87 0.70 0.73 0.75
PRESA 1.81 0.67 0.68 0.70
PRvis 1.96 0.73 0.75 0.76
Personalised PageRank
PRPMI+Per(PMI) 1.98 0.74 0.76 0.77
PRPMI+Per(ESA) 1.92 0.70 0.72 0.74
PRESA+Per(PMI) 1.91 0.70 0.72 0.73
PRESA+Per(ESA) 1.88 0.69 0.72 0.74
PRvis+Per(PMI) 2.00 0.74 0.75 0.76
PRvis+Per(ESA) 1.94 0.72 0.75 0.76
User Study 2.24 ? ? ?
Table 1: Results for various approaches to topic labelling.
standard PageRank are obtained when the visual
similarity measures are used to weight the graph,
with performance that significantly outperforms the
word overlap baseline (paired t-test, p < 0.05). This
demonstrates that visual features are a useful source
of information for deciding which images are suit-
able topic labels.
The Personalised version of PageRank produces
consistently higher results compared to standard
PageRank, demonstrating that the additional infor-
mation provided by comparing the image metadata
with the topics is useful for this task. The best
results are obtained when the personalisation vec-
tor is weighted using PMI (i.e. Per(PMI)). The
best overall result for the top-1 average rating (2.00)
is obtained when the graph is weighted using vi-
sual features and the personalisation vector using the
PMI scores (PRvis+Per(PMI)) while the best results
for the various DCG metrics are produced when
both the graph and the personalisation vector are
weighted using PMI scores (PRPMI+Per(PMI)). In
addition, these two methods, PRvis+Per(PMI) and
PRPMI+Per(PMI), perform significantly better than
the word overlap and the Google Image Search base-
lines (p < 0.01 and p < 0.05 respectively). Weight-
ing the personalisation vector using ESA consis-
tently produces lower performance compared to
PMI. These results indicate that graph-based meth-
ods for ranking images are useful for illustrating top-
ics.
6 Discussion
Figure 2 shows a sample of three topics together
with the top-3 candidates (left-to-right) selected by
applying the PRvis+Per(PMI) approach. Reasonable
labels have been selected for the first two topics. On
the other hand, the images selected for the third topic
do not seem to be as appropriate.
We observed that inappropriate labels can be gen-
erated for two reasons. Firstly, the topic may be ab-
stract and difficult to illustrate. For example, one of
the topics in our data set refers to the subject AL-
GEBRAIC NUMBER THEORY and contains the terms
number, ideal, group, field, theory, algebraic, class,
ring, prime, theorem. It is difficult to find a represen-
tative image for topics such as this one. Secondly,
there are topics for which none of the candidate im-
ages returned by the search engine is relevant. An
example of a topic like this in our data set is one
that refers to PLANTS and contains the terms family,
sources, plants, familia, order, plant, species, taxon-
omy, classification, genera. The images returned by
the search engine include pictures of the Sagrada Fa-
milia cathedral in Barcelona, a car called ?Familia?
164
dance, ballet, dancer, swan, company, dancing, nutcracker, balanchine, ballerina, choreographer
2.3 2.7 2.5 2.8 2.8 2.73
wine, bottle, grape, flavor, dry, vineyard, curtis, winery, sweet, champagne
2.1 2.6 2.7 2.83 2.8 2.8
haiti, haitian, earthquake, paterson, jean, prince, governor, au, cholera, country
1.0 1.2 0.2 1.91 1.7 1.6
Figure 2: A sample of topics and their top-3 images selected by applying the the PRvis+Per(PMI) approach
(left side) and the ones with the highest average human annotations (right side). The number under each
image represents its average human annotations score.
and pictures of families but no pictures of plants.
7 Conclusions
This paper explores the use of images to represent
automatically generated topics. An approach to se-
lecting appropriate images was described. This be-
gins by identifying a set of candidate images us-
ing a search engine and then attempts to select the
most suitable. Images are ranked using a graph-
based method that makes use of both textual and
visual information. Evaluation is carried out on a
data set created for this study. The results show that
the visual features are a useful source of information
for this task while the proposed graph-based method
significantly outperforms several baselines.
This paper demonstrates that it is possible to iden-
tify images to illustrate topics. A possible applica-
tion for this technique is to represent the contents
of large document collections in a way that supports
rapid interpretation and can be used to enable nav-
igation (Chaney and Blei, 2012; Gretarsson et al,
2012; Hinneburg et al, 2012). We plan to explore
this possibility in future work. Other possible exten-
sions to this work include exploring alternative ap-
proaches to generating candidate images and devel-
oping techniques to automatically identify abstract
topics for which suitable images are unlikely to be
found, thereby avoiding the problem cases described
in Section 6.
Acknowledgments
The research leading to these results was
carried out as part of the PATHS project
(http://paths-project.eu) funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 270082.
165
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL ?09), pages 33?41, Athens, Greece.
Rakesh Agrawal, Sreenivas Gollapudi, Anitha Kannan,
and Krishnaram Kenthapadi. 2011. Enriching text-
books with images. In Proceedings of the 20th ACM
International Conference on Information and Knowl-
edge Management (CIKM ?11), pages 1847?1856,
Glasgow, Scotland, UK.
Nikolaos Aletras and Mark Stevenson. 2013. Evaluat-
ing topic coherence using distributional semantics. In
Proceedings of the 10th International Conference on
Computational Semantics (IWCS ?13) ? Long Papers,
pages 13?22, Potsdam, Germany.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007.
A topic model for word sense disambiguation. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
?07), pages 1024?1033, Prague, Czech Republic.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In Pro-
ceedings of the Workshop on GEometrical Models of
Natural Language Semantics (GEMS ?11), pages 22?
32, Edinburgh, UK.
Allison June-Barlow Chaney and David M. Blei. 2012.
Visualizing topic models. In Proceedings of the Sixth
International AAAI Conference on Weblogs and Social
Media, Dublin, Ireland.
Bruce W. Croft, Donald Metzler, and Trevor Strohman.
2009. Search engines: Information retrieval in prac-
tice. Addison-Wesley.
Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z. Wang.
2008. Image Retrieval: Ideas, Influences, and Trends
of the New Age. ACM Computing Surveys, 40(2):1?
60.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? Automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1239?1249, Uppsala, Sweden.
Yansong Feng and Mirella Lapata. 2010b. Topic Models
for Image Annotation and Text Illustration. In Pro-
ceedings of Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
831?839, Los Angeles, California.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based
explicit semantic analysis. In Proceedings of the In-
ternational Joint Conference on Artificial Intelligence
(IJCAI ?07), pages 1606?1611, Hyberabad, India.
Brynjar Gretarsson, John O?Donovan, Svetlin Bostand-
jiev, Tobias Ho?llerer, Arthur Asuncion, David New-
man, and Padhraic Smyth. 2012. TopicNets: Vi-
sual analysis of large text corpora with topic modeling.
ACM Trans. Intell. Syst. Technol., 3(2):23:1?23:26.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362?370, Boulder, Colorado.
Taher Haveliwala, Sepandar Kamvar, and Glen Jeh.
2003. An analytical comparison of approaches to
personalizing PageRank. Technical Report 2003-35,
Stanford InfoLab.
Alexander Hinneburg, Rico Preiss, and Rene? Schro?der.
2012. TopicExplorer: Exploring document collec-
tions with topic models. In Peter A. Flach, Tijl Bie,
and Nello Cristianini, editors, Machine Learning and
Knowledge Discovery in Databases, volume 7524 of
Lecture Notes in Computer Science, pages 838?841.
Springer Berlin Heidelberg.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR ?99), pages
50?57, Berkeley, California, United States.
Ioana Hulpus, Conor Hayes, Marcel Karnstedt, and
Derek Greene. 2013. Unsupervised graph-based topic
labelling using DBpedia. In Proceedings of the 6th
ACM International Conference on Web Search and
Data Mining (WSDM ?13), pages 465?474, Rome,
Italy.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Trans. Inf. Syst., 20(4):422?446.
Yushi Jing and Shumeet Baluja. 2008. PageRank for
product image search. In Proceedings of the 17th In-
ternational Conference on World Wide Web (WWW
?08), pages 307?316, Beijing, China.
Dhiraj Joshi, James Z. Wang, and Jia Li. 2006. The Story
Picturing Engine?A system for automatic text illus-
tration. ACM Trans. Multimedia Comput. Commun.
Appl., 2(1):68?89.
Gabriella Kazai. 2011. In search of quality in crowd-
sourcing for search engine evaluation. Advances in In-
formation Retrieval, pages 165?176.
Jey Han Lau, David Newman, Sarvnaz Karimi, and Tim-
othy Baldwin. 2010. Best topic word selection for
166
topic labelling. In The 23rd International Conference
on Computational Linguistics (COLING ?10), pages
605?613, Beijing, China.
Jey Han Lau, Karl Grieser, David Newman, and Timothy
Baldwin. 2011. Automatic labelling of topic models.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 1536?1545, Portland, Ore-
gon, USA.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
5th Annual International Conference on Systems Doc-
umentation (SIGDOC ?86), pages 24?26, Toronto, On-
tario, Canada.
David G. Lowe. 1999. Object Recognition from Local
Scale-invariant Features. In Proceedings of the Sev-
enth IEEE International Conference on Computer Vi-
sion, pages 1150?1157, Kerkyra, Greece.
David G. Lowe. 2004. Distinctive Image Features from
Scale-Invariant Keypoints. International Journal of
Computer Vision, 60(2):91?110.
Davide Magatti, Silvia Calegari, Davide Ciucci, and
Fabio Stella. 2009. Automatic Labeling of Topics.
In Proceedings of the 9th International Conference on
Intelligent Systems Design and Applications (ICSDA
?09), pages 1227?1232, Pisa, Italy.
Xian-Li Mao, Zhao-Yan Ming, Zheng-Jun Zha, Tat-Seng
Chua, Hongfei Yan, and Xiaoming Li. 2012. Auto-
matic labeling hierarchical topics. In Proceedings of
the 21st ACM International Conference on Informa-
tion and Knowledge Management (CIKM ?12), Shera-
ton, Maui Hawai.
Qiaozhu Mei and ChengXiang Zhai. 2005. Discovering
evolutionary theme patterns from text: an exploration
of temporal text mining. In Proceedings of the 11th
ACM International Conference on Knowledge Discov-
ery in Data Mining (SIGKDD ?05), pages 198?207,
Chicago, Illinois, USA.
Qiaozhu Mei, Xuehua Shen, and Cheng Xiang Zhai.
2007. Automatic Labeling of Multinomial Topic Mod-
els. In Proceedings of the 13th ACM International
Conference on Knowledge Discovery and Data Mining
(SIGKDD ?07), pages 490?499, San Jose, California.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of Interna-
tional Conference on Empirical Methods in Natural
Language Processing (EMNLP ?04), pages 404?411,
Barcelona, Spain.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The PageRank citation ranking:
Bringing order to the web. Technical Report 1999-66,
Stanford InfoLab.
Koen E.A. Sande, Theo Gevers, and Cees G. M. Snoek.
2008. Evaluation of Color Descriptors for Object and
Scene Recognition. In Proceedings of the IEEE Com-
puter Society Conference on Computer Vision and Pat-
tern Recognition (CVPR ?08), pages 1?8, Anchorage,
Alaska, USA.
Richard Szeliski. 2010. Computer Vision: Algorithms
and Applications. Springer-Verlag Inc.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Xing Wei and W. Bruce Croft. 2006. LDA-based Doc-
ument Models for Ad-hoc Retrieval. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and Development in Information Re-
trieval (SIGIR ?06), pages 178?185, Seattle, Washing-
ton, USA.
167
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 151?156,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PATHS: A System for Accessing Cultural Heritage Collections
Eneko Agirre?, Nikolaos Aletras?, Paul Clough?, Samuel Fernando?,
Paula Goodale?, Mark Hall?, Aitor Soroa? and Mark Stevenson?
(?) IXA NLP Group, University of the Basque Country
Manuel Lardizabal, 1, 20.018 Donostia, Basque Country
(?) Department of Computer Science, Sheffield University
211 Portobello, Sheffield S1 4DP, United Kingdom
Abstract
This paper describes a system for navigat-
ing large collections of information about
cultural heritage which is applied to Eu-
ropeana, the European Library. Euro-
peana contains over 20 million artefacts
with meta-data in a wide range of Euro-
pean languages. The system currently pro-
vides access to Europeana content with
meta-data in English and Spanish. The pa-
per describes how Natural Language Pro-
cessing is used to enrich and organise this
meta-data to assist navigation through Eu-
ropeana and shows how this information is
used within the system.
1 Introduction
Significant amounts of information about cultural
heritage has been digitised in recent years and is
now easily available through online portals. How-
ever, this vast amount of material can also be over-
whelming for many users since they are provided
with little or no guidance on how to find and inter-
pret this information. Potentially useful and rel-
evant content is hidden from the users who are
typically offered simple keyword-based searching
functionality as the entry point into a cultural her-
itage collection. The situation is very different
within traditional mechanisms for viewing cultural
heritage (e.g. museums) where artefacts are or-
ganised thematically and users guided through the
collection.
This paper describes a system that allows users
to explore large cultural heritage collections. Nav-
igation is based around the metaphor of pathways
(or trails) through the collection, an approach that
has been widely explored as an alternative to stan-
dard keyword-based search (Furuta et al, 1997;
Reich et al, 1999; Shipman et al, 2000; White and
Huang, 2010). Pathways are sets of artefacts or-
ganised around a theme which form access points
to the collection.
Pathways are a useful way to access informa-
tion about cultural heritage. Users accessing these
collections are often unfamiliar with their content,
making keyword-based search unsuitable since
they are unable to formulate appropriate queries
(Wilson et al, 2010). Non-keyword-based search
interfaces have been shown to be suitable for ex-
ploratory search (Marchionini, 2006). Pathways
support this exploration by echoing the organised
galleries and guided tours found in museums.
2 Related Work
Heitzman et al (1997) describe the ILEX system
which acts as a guide through the jewellery col-
lection of the National Museum of Scotland. The
user explores the collection through a set of web
pages which provide descriptions of each artefact
that are personalised for each user. The system
makes use of information about the artefacts the
user has viewed to build up a model of their in-
terests and uses this to customise the descriptions
of each artefact and provide recommendations for
further artefacts in which they may be interested.
Grieser et al (2007) also explore providing rec-
ommendations based on the artefacts a user has
viewed so far. They make use of a range of tech-
niques including language modelling, geospatial
modelling and analysis of previous visitors? be-
haviour to provide recommendations to visitors to
the Melbourne Museum.
Grieser et al (2011) explore methods for de-
termining the similarity between museum arte-
facts, commenting that this is useful for navigation
through these collections and important for per-
sonalisation (Bowen and Filippini-Fantoni, 2004;
O?Donnell et al, 2001), recommendation (Bohn-
ert et al, 2009; Trant, 2009) and automatic tour
generation (Finkelstein et al, 2002; Roes et al,
2009). They also use exhibits from Melbourne
151
Museum and apply a range of approaches to deter-
mine the similarity between them, including com-
paring descriptions and measuring physical dis-
tance between them in the museum.
These approaches, like many of the systems
that have been developed for online access to cul-
tural heritage (e.g. (Hage et al, 2010)), are based
around virtual access to a concrete physical space
(i.e. a museum). They often provide tours which
are constrained by the physical layout of the mu-
seum, such as virtual museum visits. However,
these approaches are less suitable for unstructured
collections such as databases of cultural heritage
artefacts collected from multiple institutions or
artefacts not connected with existing physical pre-
sentation (e.g. in a museum). The PATHS sys-
tem is designed for these types of collections and
makes use of natural language analysis to sup-
port navigation. In particular, similarity between
artefacts is computed automatically (see Section
4.1), background information automatically added
to artefact descriptions (see Section 4.2) and a hi-
erarchy of artefacts generated (see Section 4.3).
3 Cultural Heritage Data
The PATHS system has been applied to data from
Europeana1. This is a web-portal to collections
of cultural heritage artefacts provided by a wide
range of European institutions. Europeana cur-
rently provides access to over 20 million artefacts
including paintings, films, books, archival records
and museum objects. The artefacts are provided
by around 1,500 institutions which range from
major institutions, including the Rijksmuseum in
Amsterdam, the British Library and the Louvre,
to smaller organisations such as local museums.
It therefore contains an aggregation of digital con-
tent from several sources and is not connected with
any one physical museum.
The PATHS system makes use of three collec-
tions from Europeana. The first of these con-
tains artefacts from content providers in the United
Kingdom which has meta-data in English. The
artefacts in the remaining two collections come
from institutions in Spain and have meta-data in
Spanish.
CultureGrid Culture Grid2 is a digital content
provider service from the Collection Trust3.
1http://www.europeana.eu
2http://www.culturegrid.org.uk
3http://www.collectionstrust.org.uk
It contains information about over one mil-
lion artefacts from 40 different UK content
providers such as national and regional mu-
seums and libraries.
Cervantes Biblioteca Virtual Miguel De Cer-
vantes4 contains digitalised Spanish text in
various formats. In total, the online library
contains about 75,000 works from a range of
periods in Spanish history.
Hispana The Biblioteca Nacional de Espan?a5
contains information about a diverse set of
content including text and drawings. The ma-
terial is collected from different providers in
Spain including museums and libraries.
Europeana stores metadata for each artefact in
an XML-based format which includes information
such as its title, the digital format, the collection,
the year of creation and also a short description of
each artefact. However, this meta-data is created
by the content providers and varies significantly
across artefacts. Many of the artefacts have only
limited information associated with them, for ex-
ample a single word title. In addition, the content
providers that contribute to Europeana use differ-
ent hierarchical structures to organise their collec-
tions (e.g. Library of Congress Subject Headings6
and the Art and Architecture Thesaurus7), or do
not organise their content into any structure. Con-
sequently the various hierarchies that are used in
Europeana only cover some of the artefacts and
are not compatible with each other.
3.1 Filtering Data
Analysis of the artefacts in these three collections
revealed that many have short and uninformative
titles or lack a description. This forms a challenge
to language processing techniques since the arte-
fact?s meta-data does not contain enough informa-
tion to model it accurately.
The collections were filtered by removing any
artefacts that have no description and have either
fewer than four words in their title or have a title
that is repeated more than 100 times in the col-
lection. Table 1 shows the number of artefacts
in each of the Europeana collections before and
4http://www.cervantesvirtual.com
5http://www.bne.es
6http://authorities.loc.gov/
7http://www.getty.edu/research/tools/
vocabularies/aat/
152
after this filter has been applied. Applying the
heuristic leads to the removal of around 31% of the
artefacts, although the number varies significantly
across the collections with 61% of the artefacts in
CultureGrid being removed and only 1% of those
in Hispana.
Collection Lang. Total Filtered
CultureGrid Eng. 1,207,781 466,958
Hispana Sp. 1,235,133 1,219,731
Cervantes Sp. 19,278 14,983
2,462,192 1,701,672
Table 1: Number of artefacts in Europeana collec-
tions before and after filtering
4 Data Processing
A range of pre-preprocessing steps were carried
out on these collections to provide additional in-
formation to support navigation in the PATHS sys-
tem.
4.1 Artefact Similarity
We begin by computing the similarity between
the various artefacts in the Europeana collections.
This information is useful for navigation and rec-
ommendation but is not available in the Europeana
collections since they are drawn from a diverse
range of sources.
Similarity is computed using an approach de-
scribed by Aletras et al (2012). in which the top-
ics generated from each artefact?s metadata using
a topic model are compared. Latent Dirichlet Al-
location (LDA) (Blei et al, 2003) is a widely used
type of topic model in which documents can be
viewed as probability distributions over topics, ?.
The similarity between a pair of documents can be
estimated by comparing their topic distributions.
This is achieved by viewing each distribution as
a vector of probabilities and then computing the
cosine of the angle between them:
sim(a, b) =
~?a.~?b
|~?a| ? | ~?b|
(1)
where ~?a is the vector created from the probability
distribution generated by LDA for document a.
This approach is evaluated using a set of 295
pairs of artefacts for which human judgements
of similarity were obtained using crowdsourcing
(Aletras et al, 2012). Pearson correlation between
the similarity scores and human judgements was
0.53.
The similarity between all the artefacts in the
collection is computed in a pairwise fashion. The
25 artefacts with the highest score are retained for
each artefact.
4.2 Background Links
The metadata associated with Europeana artefacts
is often very limited. Consequently links to rele-
vant articles in Wikipedia were added to each the
meta-data of each artefact using Wikipedia Miner
(Milne and Witten, 2008) to provide background
information. In addition to the link, Wikipedia
Miner returns a confidence value between 0 and
1 for each link based on the context of the item.
The accuracy of the links added by Wikipedia
Miner were evaluated using the meta-data associ-
ated with 21 randomly selected artefacts. Three
annotators analysed the links added and found that
a confidence value of 0.5 represented a good bal-
ance between accuracy and coverage. See Fer-
nando and Stevenson (2012) for further details.
4.3 Hierarchies
The range of hierarchies used by the various col-
lections that comprise the Europeana collection
make navigation difficult (see Section 3). Con-
sequently, the Wikipedia links added to the arte-
fact meta-data were used to automatically gener-
ate hierarchies that the cover the entire collection.
These hierarchies are used by the PATHS system
to assist browsing and exploration.
Two approaches are used to generate hierarchies
of Europeana artefacts (WikiFreq and WikiTax).
These are combined to generate the WikiMerge hi-
erarchy which is used in the PATHS system.
WikiFreq uses link frequencies across the en-
tire collection to organise the artefacts. The first
stage in the hierarchy generation process is to
compute the frequency with which each linked
Wikipedia article appears in the collection. The
links in each artefact are these analysed to con-
struct a hierarchy consisting of Wikipedia articles.
The links in the meta-data associated with each
artefact are ordered based on their frequency in the
entire collection and that set of links then inserted
into the hierarchy. For example, if the set of or-
dered links for an artefact is a1, a2, a3 ? ? ? an then
the artefact is then inserted into the hierarchy un-
der the branch a1 ? a2 ? a3 ? ? ? ? an, with
a1 at the top level in the tree and the artefact ap-
pearing under the node an. If this branch does not
already exist in the tree then it is created.
153
The hierarchy is pruned to removing nodes with
fewer than 20 artefacts in them. In addition, if a
node has more than 20 child nodes, only the 20
most frequent are used.
WikiTax uses the Wikipedia Taxonomy
(Ponzetto and Strube, 2011), a taxonomy derived
from Wikipedia categories. Europeana artefacts
are inserted into this taxonomy using the links
added by Wikipedia Miner with each artefact
being added to the taxonomy for all categories
listed in the links. This leads to a taxonomy in
which artefacts can occur in multiple locations.
Each approach was used to generate hierarchies
from the Europeana collections. The resulting hi-
erarchies were evaluated via online surveys, see
Fernando et al (2012) for further details. It was
found that WikiFreq performed well at placing
items into the correct location in the taxonomy and
grouping together similar items under the same
node. However, the overall structure of WikiTax
was judged to be more coherent and comprehensi-
ble.
WikiMerge combines combines WikiFreq and
WikiTax. WikiFreq is used to link each artefact
to Wikipedia articles a1 . . . an, but only the link
to the most specific article, an, is retained. The
an articles are linked to their parent WikiTax top-
ics based on the Wikipedia categories the articles
belong to. The resulting hierarchy is pruned re-
moving all WikiTax topics that do not have a Wik-
iFreq child or have only one child topic. Finally
top-level topics in the combined hierarchy are then
linked to their respective Wikipedia root node.
The resulting WikiMerge hierarchy has Wik-
iFreq topics as its leaves and WikiTax topics as
its interior and root nodes. Experiments showed
that this approach was successful in combining
the strengths of the two methods (Fernando et al,
2012).
5 The PATHS System
The PATHS system provides access to the Euro-
peana collections described in Section 3 by mak-
ing use of the additional information generated us-
ing the approaches described in Section 4. The in-
terface of the PATHS system has three main areas:
Paths enables users to navigate via pathways (see
Section 5.1).
Search supports discovery of both collection arte-
facts and pathways through keyword search
(see Section 5.2).
Explore enables users to explore the collections
using a variety of types of overview (see Sec-
tion 5.3).
5.1 Paths Area
This area provides users with access to Europeana
through pathways or trails. These are manually
generated sets of artefacts organised into a tree
structure which are designed to showcase the con-
tent available to the user in an organised way.
These can be created by users and can be pub-
lished for others to follow. An example path-
way on the topic ?railways? is shown in Figure
1. A short description of the pathway?s content is
shown towards the top of the figure and a graphical
overview of its contents at the bottom.
Figure 1: Example pathway on the topic ?rail-
ways?
Figure 2 shows as example artefact as displayed
in the system. The example artefact is a portrait
of Catherine the Great. The left side of the figure
shows information extracted directly from the Eu-
ropeana meta-data for this artefact. The title and
textual description are shown towards the top left
together with a thumbnail image of the artefact.
Other information from the meta-data is shown be-
neath the ?About this item? heading. The right
side of the figure shows additional information
Figure 2: Example artefact displayed in system in-
terface. Related artefacts and background links are
displayed on right hand side
154
Figure 3: Example visualisations of hierarchy: thesaurus view (top left), tag cloud (top right), map views
(bottom)
about the artefact generated using the approaches
described in Sections 4.1 and 4.2. Related arte-
facts are shown to the user one at a time, click-
ing on the thumbnail image leads to the equivalent
page for the related artefact. Below this are links
to the Wikipedia articles that are identified in the
text of the article?s title and description.
5.2 Search Area
This area allows users to search for artefacts and
pathways using standard keyword search imple-
mented using Lucene (McCandless et al, 2010).
5.3 Explore Area
The system provides a variety of ways to view
the hierarchies generated using the approach de-
scribed in Section 4.3. Figure 3 shows how these
are displayed for a section of the hierarchy with
the label ?Society?. The simplest view (shown in
the top left of Figure 3) is a thesaurus type view
in which levels of the hierarchy are represented by
indentation. The system also allows levels of the
hierarchy to be viewed as a tag cloud (top right of
Figure 3). The final representation of the hierar-
chy is as a map, shown in the bottom of Figure 3.
In this visualisation categories in the hierarchy are
represented as ?islands? on the map. Zooming in
on the map provides more detail about that area of
the hierarchy.
6 Summary and Future Developments
This paper describes a system for navigating Eu-
ropeana, an aggregation of collections of cultural
heritage artefacts. NLP analysis is used to organ-
ise the collection and provide additional informa-
tion. The results of this analysis are provided to
the user through an online interface which pro-
vides access to English and Spanish content in Eu-
ropeana.
Planned future development of this system in-
cludes providing recommendations and more per-
sonalised access. Similarity information (Sec-
tion 4.1) can be used to provide information from
which the recommendations can be made. Person-
alised access will make use of information about
individual users (e.g. from their browsing be-
haviour or information they provide about their
preferences) to generate individual views of Eu-
ropeana.
155
Online Demo
The PATHS system is available at
http://explorer.paths-project.eu/
Acknowledgments
The research leading to these results was
carried out as part of the PATHS project
(http://paths-project.eu) funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 270082
References
N. Aletras, M. Stevenson, and P. Clough. 2012. Com-
puting similarity between items in a digital library
of cultural heritage. Journal of Computing and Cul-
tural Heritage, 5(4):no. 16.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022.
F. Bohnert, D. Schmidt, and I. Zuckerman. 2009. Spa-
tial Process for Recommender Systems. In Proc. of
IJCAI 2009, pages 2022?2027, Pasadena, CA.
J. Bowen and S. Filippini-Fantoni. 2004. Personaliza-
tion and the Web from a Museum Perspective. In
Proc. of Museums and the Web 2004, pages 63?78.
Samuel Fernando and Mark Stevenson. 2012. Adapt-
ing Wikification to Cultural Heritage. In Proceed-
ings of the 6th Workshop on Language Technology
for Cultural Heritage, Social Sciences, and Human-
ities, pages 101?106, Avignon, France.
Samuel Fernando, Mark Hall, Eneko Agirre, Aitor
Soroa, Paul Clough, and Mark Stevenson. 2012.
Comparing taxonomies for organising collections of
documents. In Proc. of COLING 2012, pages 879?
894, Mumbai, India.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing Search in Context: The Concept Revisited. ACM
Trans. on Information Systems, 20(1):116?131.
R. Furuta, F.. Shipman, C. Marshall, D. Brenner, and
H. Hsieh. 1997. Hypertext paths and the World-
Wide Web: experiences with Walden?s Paths. In
Proc. of the Eighth ACM conference on Hypertext,
pages 167?176, New York, NY.
K. Grieser, T. Baldwin, and S. Bird. 2007. Dynamic
Path Prediction and Recommendation in a Museum
Environment. In Proc. of the Workshop on Lan-
guage Technology for Cultural Heritage Data (LaT-
eCH 2007), pages 49?56, Prague, Czech Republic.
K. Grieser, T. Baldwin, F. Bohnert, and L. Sonenberg.
2011. Using Ontological and Document Similarity
to Estimate Museum Exhibit Relatedness. Journal
of Computing and Cultural Heritage, 3(3):1?20.
W.R. van Hage, N. Stash, Y. Wang, and L.M. Aroyo.
2010. Finding your way through the Rijksmuseum
with an adaptive mobile museum guide. In Proc. of
ESWC 2010, pages 46?59.
J. Heitzman, C. Mellish, and J. Oberlander. 1997. Dy-
namic Generation of Museum Web Pages: The In-
telligent Labelling Explorer. Archives and Museum
Informatics, 11(2):117?125.
G. Marchionini. 2006. Exploratory Search: from Find-
ing to Understanding. Comm. ACM, 49(1):41?46.
M. McCandless, E. Hatcher, and O. Gospodnetic.
2010. Lucene in Action. Manning Publications.
D. Milne and I. Witten. 2008. Learning to Link with
Wikipedia. In Proc. of CIKM 2008, Napa Valley,
California.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: An architecture for a dynamic hy-
pertext generation system. Natural Language En-
gineering, 7:225?250.
S.P. Ponzetto and M. Strube. 2011. Taxonomy in-
duction based on a collaboratively built knowledge
repository. Artificial Intelligence, 175(9-10):1737?
1756.
S. Reich, L. Carr, D. De Roure, and W. Hall. 1999.
Where have you been from here? Trails in hypertext
systems. ACM Computing Surveys, 31.
I. Roes, N. Stash, Y. Wang, and L. Aroyo. 2009. A
personalized walk through the museum: the CHIP
interactive tour guide. In Proc. of the 27th Interna-
tional Conference on Human Factors in Computing
Systems, pages 3317?3322, Boston, MA.
F. Shipman, R. Furuta, D. Brenner, C. Chung, and
H. Hsieh. 2000. Guided paths through web-based
collections: Design, experiences, and adaptations.
Journal of the American Society for Information Sci-
ence, 51(3):260?272.
J. Trant. 2009. Tagging, folksonomies and art mu-
seums: Early experiments and ongoing research.
Journal of Digital Information, 10(1).
R. White and J. Huang. 2010. Assessing the scenic
route: measuring the value of search trails in web
logs. In Proc. of SIGIR 2010, pages 587?594.
M. Wilson, Kulesm B., M. Schraefel, and B. Schnei-
derman. 2010. From keyword search to explo-
ration: Designing future search interfaces for the
web. Foundations and Trends in Web Science,
2(1):1?97.
156
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 631?636,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Labelling Topics using Unsupervised Graph-based Methods
Nikolaos Aletras and Mark Stevenson
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP
United Kingdom
{n.aletras, m.stevenson}@dcs.shef.ac.uk
Abstract
This paper introduces an unsupervised
graph-based method that selects textual
labels for automatically generated topics.
Our approach uses the topic keywords to
query a search engine and generate a graph
from the words contained in the results.
PageRank is then used to weigh the words
in the graph and score the candidate labels.
The state-of-the-art method for this task is
supervised (Lau et al, 2011). Evaluation
on a standard data set shows that the per-
formance of our approach is consistently
superior to previously reported methods.
1 Introduction
Topic models (Hofmann, 1999; Blei et al, 2003)
have proved to be a useful way to represent the
content of document collections, e.g. (Chaney and
Blei, 2012; Ganguly et al, 2013; Gretarsson et
al., 2012; Hinneburg et al, 2012; Snyder et al,
2013). In these interfaces, topics need to be pre-
sented to users in an easily interpretable way. A
common way to represent topics is as set of key-
words generated from the n terms with the highest
marginal probabilities. For example, a topic about
the global financial crisis could be represented
by its top 10 most probable terms: FINANCIAL,
BANK, MARKET, GOVERNMENT, MORTGAGE,
BAILOUT, BILLION, STREET, WALL, CRISIS. But
interpreting such lists is not always straightfor-
ward, particularly since background knowledge
may be required (Chang et al, 2009).
Textual labels could assist with the interpre-
tations of topics and researchers have developed
methods to generate these automatically (Mei et
al., 2007; Lau et al, 2010; Lau et al, 2011). For
example, a topic which has keywords SCHOOL,
STUDENT, UNIVERSITY, COLLEGE, TEACHER,
CLASS, EDUCATION, LEARN, HIGH, PROGRAM,
could be labelled as EDUCATION and a suitable la-
bel for the topic shown above would be GLOBAL
FINANCIAL CRISIS. Approaches that make use of
alternative modalities, such as images (Aletras and
Stevenson, 2013), have also been proposed.
Mei et al (2007) label topics using statistically
significant bigrams identified in a reference collec-
tion. Magatti et al (2009) introduced an approach
for labelling topics that relied on two hierarchical
knowledge resources labelled by humans, while
Lau et al (2010) proposed selecting the most rep-
resentative word from a topic as its label. Hulpus
et al (2013) make use of structured data from DB-
pedia to label topics.
Lau et al (2011) proposed a method for auto-
matically labelling topics using information from
Wikipedia. A set of candidate labels is gener-
ated from Wikipedia article titles by querying us-
ing topic terms. Additional labels are then gen-
erated by chunk parsing the article titles to iden-
tify n-grams that represent Wikipedia articles as
well. Outlier labels (less relevant to the topic) are
identified and removed. Finally, the top-5 topic
terms are added to the candidate set. The la-
bels are ranked using Support Vector Regression
(SVR) (Vapnik, 1998) and features extracted us-
ing word association measures (i.e. PMI, t-test, ?
2
and Dice coefficient), lexical features and search
engine ranking. Lau et al (2011) report two ver-
sions of their approach, one unsupervised (which
is used as a baseline) and another which is super-
vised. They reported that the supervised version
achieves better performance than a previously re-
ported approach (Mei et al, 2007).
This paper introduces an alternative graph-
based approach which is unsupervised and less
computationally intensive than Lau et al (2011).
Our method uses topic keywords to form a query.
A graph is generated from the words contained in
the search results and these are then ranked using
the PageRank algorithm (Page et al, 1999; Mihal-
631
{?Description?: ?Microsoft will accelerate your journey to cloud computing with an
agile and responsive datacenter built from your existing technology investments.?,
?DisplayUrl?: ?www.microsoft.com/en-us/server-cloud/datacenter/virtualization.aspx?,
?ID?: ?a42b0908-174e-4f25-b59c-70bdf394a9da?,
?Title?: ?Microsoft | Server & Cloud | Datacenter | Virtualization ...?,
?Url?: ?http://www.microsoft.com/en-us/server-cloud/datacenter/virtualization.aspx?,
... }
Figure 1: Sample of the metadata associated with a search result.
cea and Tarau, 2004). Evaluation on a standard
data set shows that our method consistently out-
performs the best performing previously reported
method, which is supervised (Lau et al, 2011).
2 Methodology
We use the topic keywords to query a search en-
gine. We assume that the search results returned
are relevant to the topic and can be used to identify
and weigh relevant keywords. The most impor-
tant keywords can be used to generate keyphrases
for labelling the topic or weight pre-existing can-
didate labels.
2.1 Retrieving and Processing Text
Information
We use the approach described by Lau et al (2011)
to generate candidate labels from Wikipedia arti-
cles. The 10 terms with the highest marginal prob-
abilities in the topic are used to query Wikipedia
and the titles of the articles retrieved used as candi-
date labels. Further candidate labels are generated
by processing the titles of these articles to identify
noun chunks and n-grams within the noun chunks
that are themselves the titles of Wikipedia arti-
cles. Outlier labels, identified using a similarity
measure (Grieser et al, 2011), are removed. This
method has been proved to produce labels which
effectively summarise a topic?s main subject.
However, it should be noted that our method is
flexible and could be applied to any set of can-
didate labels. We have experimented with various
approaches to candidate label generation but chose
to report results using the approach described by
Lau et al (2011) to allow direct comparison of ap-
proaches.
Information obtained from web searches is used
to identify the best labels from the set of candi-
dates. The top n keywords, i.e. those with highest
marginal probability within the topic, are used to
form a query which was submitted to the Bing
1
search engine. Textual information included in the
Title field
2
of the search results metadata was ex-
tracted. Each title was tokenised using openNLP
3
and stop words removed.
Figure 1 shows a sample of the metadata asso-
ciated with a search result for the topic: VMWARE,
SERVER, VIRTUAL, ORACLE, UPDATE, VIRTU-
ALIZATION, APPLICATION, INFRASTRUCTURE,
MANAGEMENT, MICROSOFT.
2.2 Creating a Text Graph
We consider any remaining words in the search
result metadata as nodes, v ? V , in a graph
G = (V,E). Each node is connected to its neigh-
bouring words in a context window of ?n words.
In the previous example, the words added to the
graph from the Title of the search result are mi-
crosoft, server, cloud, datacenter and virtualiza-
tion.
We consider both unweighted and weighted
graphs. When the graph is unweighted we assume
that all the edges have a weight e = 1. In addi-
tion, we weight the edges of the graph by comput-
ing the relatedness between two nodes, v
i
and v
j
,
as their normalised Pointwise Mutual Information
(NPMI) (Bouma, 2009). Word co-occurrences are
computed using Wikipedia as a a reference cor-
pus. Pairs of words are connected with edges only
if NPMI(w
i
, w
j
) > 0.2 avoiding connections be-
tween words co-occurring by chance and hence in-
troducing noise.
2.3 Identifying Important Terms
Important terms are identified by applying the
PageRank algorithm (Page et al, 1999) in a sim-
ilar way to the approach used by Mihalcea and
1
http://www.bing.com/
2
We also experimented with using the Description field
but found that this reduced performance.
3
http://opennlp.apache.org/
632
Tarau (2004) for document keyphrase extraction.
The PageRank score (Pr) over G for a word (v
i
)
can be computed by the following equation:
Pr(v
i
) = d ?
?
v
j
?C(v
i
)
sim(v
i
, v
j
)
?
v
k
?C(v
j
)
sim(v
j
, v
k
)
Pr(v
j
)
+ (1? d)v (1)
where C(v
i
) denotes the set of vertices which are
connected to the vertex v
i
. d is the damping factor
which is set to the default value of d = 0.85 (Page
et al, 1999). In standard PageRank all elements
of the vector v are the same,
1
N
where N is the
number of nodes in the graph.
2.4 Ranking Labels
Given a candidate label L = {w
1
, ..., w
m
} con-
taining m keywords, we compute the score of L
by simply adding the PageRank scores of its con-
stituent keywords:
Score(L) =
m
?
i=1
Pr(w
i
) (2)
The label with the highest score amongst the set
of candidates is selected to represent the topic. We
also experimented with normalised versions of the
score, e.g. mean of the PageRank scores. How-
ever, this has a negative effect on performance
since it favoured short labels of one or two words
which were not sufficiently descriptive of the top-
ics. In addition, we expect that candidate labels
containing words that do not appear in the graph
(with the exception of stop words) are unlikely to
be good labels for the topic. In these cases the
score of the candidate label is set to 0. We also
experimented with removing this restriction but
found that it lowered performance.
3 Experimental Evaluation
3.1 Data
We evaluate our method on the publicly avail-
able data set published by Lau et al (2011). The
data set consists of 228 topics generated using
text documents from four domains, i.e. blog
posts (BLOGS), books (BOOKS), news articles
(NEWS) and scientific articles from the biomedi-
cal domain (PUBMED). Each topic is represented
by its ten most probable keywords. It is also as-
sociated with candidate labels and human ratings
denoting the appropriateness of a label given the
topic. The full data set consists of approximately
6,000 candidate labels (27 labels per topic).
3.2 Evaluation Metrics
Our evaluation follows the framework proposed
by Lau et al (2011) using two metrics, i.e. Top-
1 average rating and nDCG, to compare various
labelling methods.
Top-1 average rating is the average human rat-
ing (between 0 and 3) assigned to the top-ranked
label proposed by the system. This provides an in-
dication of the overall quality of the label the sys-
tem judges as the best one.
Normalised discounted cumulative gain
(nDCG) (J?arvelin and Kek?al?ainen, 2002; Croft et
al., 2009) compares the label ranking proposed
by the system to the ranking provided by human
annotators. The discounted cumulative gain
at position p, DCG
p
, is computed using the
following equation:
DCG
p
= rel
1
+
p
?
i=2
rel
i
log
2
(i)
(3)
where rel
i
is the relevance of the label to the topic
in position i. Then nDCG is computed as:
nDCG
p
=
DCG
p
IDCG
p
(4)
where IDCG
p
is the superviseed ranking of the
image labels, in our experiments this is the rank-
ing provided by the scores in the human annotated
data set.
3.3 Model Parameters
Our proposed model requires two parameters to
be set: the context window size when connecting
neighbouring words in the graph and the number
of the search results considered when constructing
the graph.
We experimented with different sizes of context
window, n, between?1 words to the left and right
and all words in the title. The best results were ob-
tained when n = 2 for all of the domains. In addi-
tion, we experimented with varying the number of
search results between 10 and 300. We observed
no noticeable difference in the performance when
the number of search results is equal or greater
than 30 (see below). We choose to report results
obtained using 30 search results for each topic. In-
cluding more results did not improve performance
but required additional processing.
633
Domain Model Top-1 Av. Rating nDCG-1 nDCG-3 nDCG-5
BLOGS
Lau et al (2011)-U 1.84 0.75 0.77 0.79
Lau et al (2011)-S 1.98 0.81 0.82 0.83
PR 2.05? 0.83 0.84 0.83
PR-NPMI 2.08? 0.84 0.84 0.83
Upper bound 2.45 1.00 1.00 1.00
BOOKS
Lau et al (2011)-U 1.75 0.77 0.77 0.79
Lau et al (2011)-S 1.91 0.84 0.81 0.83
PR 1.98? 0.86 0.88 0.87
PR-NPMI 2.01? 0.87 0.88 0.87
Upper bound 2.29 1.00 1.00 1.00
NEWS
Lau et al (2011)-U 1.96 0.80 0.79 0.78
Lau et al (2011)-S 2.02 0.82 0.82 0.84
PR 2.04? 0.83 0.81 0.81
PR-NPMI 2.05? 0.83 0.81 0.81
Upper bound 2.45 1.00 1.00 1.00
PUBMED
Lau et al (2011)-U 1.73 0.75 0.77 0.79
Lau et al (2011)-S 1.79 0.77 0.82 0.84
PR 1.88?? 0.80 0.80 0.80
PR-NPMI 1.90?? 0.81 0.80 0.80
Upper bound 2.31 1.00 1.00 1.00
Table 1: Results for Various Approaches to Topic Labelling (?: significant difference (t-test, p < 0.05)
to Lau et al (2011)-U; ?: significant difference (p < 0.05) to Lau et al (2011)-S).
4 Results and Discussion
Results are shown in Table 1. Performance when
PageRank is applied to the unweighted (PR) and
NPMI-weighted graphs (PR-NPMI) (see Section
2.2) is shown. Performance of the best unsuper-
vised (Lau et al (2011)-U) and supervised (Lau
et al (2011)-S) methods reported by Lau et al
(2011) are shown. Lau et al (2011)-U uses the av-
erage ?
2
scores between the topic keywords and
the label keywords while Lau et al (2011)-S uses
SVR to combine evidence from all features. In
addition, upper bound figures, the maximum pos-
sible value given the scores assigned by the anno-
tators, are also shown.
The results obtained by applying PageRank
over the unweighted graph (2.05, 1.98, 2.04 and
1.88) are consistently better than the supervised
and unsupervised methods reported by Lau et al
(2011) for the Top-1 Average scores and this im-
provement is observed in all domains. The differ-
ence is significant (t-test, p < 0.05) for the un-
supervised method. A slight improvement in per-
formance is observed when the weighted graph is
used (2.08, 2.01, 2.05 and 1.90). This is expected
since the weighted graph contains additional in-
formation about word relatedness. For example,
the word hardware is more related and, therefore,
closer in the graph to the word virtualization than
to the word investments.
Results from the nDCG metric imply that our
methods provide better rankings of the candidate
labels in the majority of the cases. It is outper-
formed by the best supervised approach in two do-
mains, NEWS and PUBMED, using the nDCG-
3 and nDCG-5 metrics. However, the best label
proposed by our methods is judged to be better
(as shown by the nDCG-1 and Top-1 Av. Rat-
ing scores), demonstrating that it is only the lower
ranked labels in our approach that are not as good
as the supervised approach.
An interesting finding is that, although limited
in length, the textual information in the search re-
sult?s metadata contain enough salient terms rel-
evant to the topic to provide reliable estimates of
634
50 100 150 200 250 300
1.7
1.8
1.9
2
2.1
2.2
Number of Search Results
T
o
p
-
1
A
v
.
R
a
t
i
n
g
(a) BLOGS
50 100 150 200 250 300
1.7
1.8
1.9
2
2.1
2.2
Number of Search Results
(b) BOOKS
50 100 150 200 250 300
1.7
1.8
1.9
2
2.1
2.2
Number of Search Results
T
o
p
-
1
A
v
.
R
a
t
i
n
g
(c) NEWS
50 100 150 200 250 300
1.7
1.8
1.9
2
2.1
2.2
Number of Search Results
Lau et al (2011)-U
Lau et al (2011)-S
PR
PR-NPMI
(d) PUBMED
Figure 2: Top-1 Average Rating obtained for different number of search results.
term importance. Consequently, it is not necessary
to measure semantic similarity between topic key-
words and candidate labels as previous approaches
have done. In addition, performance improvement
gained from using the weighted graph is mod-
est, suggesting that the computation of association
scores over a large reference corpus could be omit-
ted if resources are limited.
In Figure 2, we show the scores of Top-1 av-
erage rating obtained in the different domains by
experimenting with the number of search results
used to generate the text graph. The most inter-
esting finding is that performance is stable when
30 or more search results are considered. In addi-
tion, we observe that quality of the topic labels in
the four domains remains stable, and higher than
the supervised method, when the number of search
results used is between 150 and 200. The only
domain in which performance of the supervised
method is sometimes better than the approach pro-
posed here is NEWS. The main reason is that news
topics are more fine grained and the candidate
labels of better quality (Lau et al, 2011) which
has direct impact in good performance of ranking
methods.
5 Conclusion
We described an unsupervised graph-based
method to associate textual labels with automati-
cally generated topics. Our approach uses results
retrieved from a search engine using the topic
keywords as a query. A graph is generated from
the words contained in the search results metadata
and candidate labels ranked using the PageRank
algorithm. Evaluation on a standard data set
shows that our method consistently outperforms
the supervised state-of-the-art method for the task.
Acknowledgments
We would like to thank Jey Han Lau for providing
us with the labels selected by Lau et al (2011)-
U and Lau et al (2011)-S. We also thank Daniel
Preot?iuc-Pietro for his useful comments on early
drafts of this paper.
635
References
Nikolaos Aletras and Mark Stevenson. 2013. Rep-
resenting topics using images. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 158?167, At-
lanta, Georgia.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceed-
ings of GSCL.
Allison June-Barlow Chaney and David M. Blei. 2012.
Visualizing topic models. In Proceedings of the
Sixth International AAAI Conference on Weblogs
and Social Media, Dublin, Ireland.
Jonathan Chang, Jordan Boyd-Graber, and Sean Ger-
rish. 2009. Reading Tea Leaves: How Humans In-
terpret Topic Models. Neural Information, pages 1?
9.
Bruce W. Croft, Donald Metzler, and Trevor Strohman.
2009. Search engines: Information retrieval in
practice. Addison-Wesley.
Debasis Ganguly, Manisha Ganguly, Johannes Level-
ing, and Gareth J.F. Jones. 2013. TopicVis: A GUI
for Topic-based feedback and navigation. In Pro-
ceedings of the Thirty-Sixth Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval (SIGIR 13), Dublin,
Ireland.
Brynjar Gretarsson, John O?Donovan, Svetlin Bostand-
jiev, Tobias H?ollerer, Arthur Asuncion, David New-
man, and Padhraic Smyth. 2012. TopicNets: Visual
analysis of large text corpora with topic modeling.
ACM Trans. Intell. Syst. Technol., 3(2):23:1?23:26.
Karl Grieser, Timothy Baldwin, Fabian Bohnert, and
Liz Sonenberg. 2011. Using Ontological and Doc-
ument Similarity to Estimate Museum Exhibit Re-
latedness. Journal on Computing and Cultural Her-
itage (JOCCH), 3(3):10:1?10:20.
Alexander Hinneburg, Rico Preiss, and Ren?e Schr?oder.
2012. TopicExplorer: Exploring document collec-
tions with topic models. In Peter A. Flach, Tijl
Bie, and Nello Cristianini, editors, Machine Learn-
ing and Knowledge Discovery in Databases, volume
7524 of Lecture Notes in Computer Science, pages
838?841. Springer Berlin Heidelberg.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR ?99),
pages 50?57, Berkeley, California, United States.
Ioana Hulpus, Conor Hayes, Marcel Karnstedt, and
Derek Greene. 2013. Unsupervised graph-based
topic labelling using DBpedia. In Proceedings of the
6th ACM International Conference on Web Search
and Data Mining (WSDM ?13), pages 465?474,
Rome, Italy.
Kalervo J?arvelin and Jaana Kek?al?ainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Trans. Inf. Syst., 20(4):422?446.
Jey Han Lau, David Newman, Sarvnaz Karimi, and
Timothy Baldwin. 2010. Best topic word selec-
tion for topic labelling. In The 23rd International
Conference on Computational Linguistics (COLING
?10), pages 605?613, Beijing, China.
Jey Han Lau, Karl Grieser, David Newman, and Tim-
othy Baldwin. 2011. Automatic labelling of topic
models. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1536?
1545, Portland, Oregon, USA.
Davide Magatti, Silvia Calegari, Davide Ciucci, and
Fabio Stella. 2009. Automatic Labeling of Top-
ics. In Proceedings of the 9th International Confer-
ence on Intelligent Systems Design and Applications
(ICSDA ?09), pages 1227?1232, Pisa, Italy.
Qiaozhu Mei, Xuehua Shen, and Cheng Xiang Zhai.
2007. Automatic Labeling of Multinomial Topic
Models. In Proceedings of the 13th ACM Inter-
national Conference on Knowledge Discovery and
Data Mining (SIGKDD ?07), pages 490?499, San
Jose, California.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of Inter-
national Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP ?04), pages 404?
411, Barcelona, Spain.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The PageRank citation
ranking: Bringing order to the web. Technical Re-
port 1999-66, Stanford InfoLab.
Justin Snyder, Rebecca Knowles, Mark Dredze,
Matthew Gormley, and Travis Wolfe. 2013. Topic
models and metadata for visualizing text corpora. In
Proceedings of the 2013 NAACL-HLT Demonstra-
tion Session, pages 5?9, Atlanta, Georgia. Associa-
tion for Computational Linguistics.
Vladimir N Vapnik. 1998. Statistical learning theory.
Wiley, New York.
636
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 132?137, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UBC UOS-TYPED: Regression for Typed-similarity
Eneko Agirre
University of the Basque Country
Donostia, 20018, Basque Country
e.agirre@ehu.es
Nikolaos Aletras
University of Sheffield
Sheffield, S1 4DP, UK
n.aletras@dcs.shef.ac.uk
Aitor Gonzalez-Agirre
University of the Basque Country
Donostia, 20018, Basque Country
agonzalez278@ikasle.ehu.es
German Rigau
University of the Basque Country
Donostia, 20018, Basque Country
german.rigau@ehu.es
Mark Stevenson
University of Sheffield
Sheffield, S1 4DP, UK
m.stevenson@dcs.shef.ac.uk
Abstract
We approach the typed-similarity task using
a range of heuristics that rely on information
from the appropriate metadata fields for each
type of similarity. In addition we train a linear
regressor for each type of similarity. The re-
sults indicate that the linear regression is key
for good performance. Our best system was
ranked third in the task.
1 Introduction
The typed-similarity dataset comprises pairs of Cul-
tural Heritage items from Europeana1, a single ac-
cess point to digitised versions of books, paintings,
films, museum objects and archival records from in-
stitutions throughout Europe. Typically, the items
comprise meta-data describing a cultural heritage
item and, sometimes, a thumbnail of the item itself.
Participating systems need to compute the similarity
between items using the textual meta-data. In addi-
tion to general similarity, the dataset includes spe-
cific kinds of similarity, like similar author, similar
time period, etc.
We approach the problem using a range of sim-
ilarity techniques for each similarity types, these
make use of information contained in the relevant
meta-data fields.In addition, we train a linear regres-
sor for each type of similarity, using the training data
provided by the organisers with the previously de-
fined similarity measures as features.
We begin by describing our basic system in Sec-
tion 2, followed by the machine learning system in
1http://www.europeana.eu/
Section 3. The submissions are explained in Section
4. Section 5 presents our results. Finally, we draw
our conclusions in Section 6.
2 Basic system
The items in this task are taken from Europeana.
They cannot be redistributed, so we used the urls
and scripts provided by the organizers to extract the
corresponding metadata. We analysed the text in the
metadata, performing lemmatization, PoS tagging,
named entity recognition and classification (NERC)
and date detection using Stanford CoreNLP (Finkel
et al, 2005; Toutanova et al, 2003). A preliminary
score for each similarity type was then calculated as
follows:
? General: cosine similarity of TF.IDF vectors of
tokens, taken from all fields.
? Author: cosine similarity of TF.IDF vectors of
dc:Creator field.
? People involved, time period and location:
cosine similarity of TF.IDF vectors of loca-
tion/date/people entities recognized by NERC
in all fields.
? Events: cosine similarity of TF.IDF vectors of
event verbs and nouns. A list of verbs and
nouns possibly denoting events was derived us-
ing the WordNet Morphosemantic Database2.
? Subject and description: cosine similarity of
TF.IDF vectors of respective fields.
IDF values were calculated using a subset of Eu-
ropeana items (the Culture Grid collection), avail-
able internally. These preliminary scores were im-
2urlhttp://wordnetcode.princeton.edu/standoff-
files/morphosemantic-links.xls
132
proved using TF.IDF based on Wikipedia, UKB
(Agirre and Soroa, 2009) and a more informed time
similarity measure. We describe each of these pro-
cesses in turn.
2.1 TF.IDF
A common approach to computing document sim-
ilarity is to represent documents as Bag-Of-Words
(BOW). Each BOW is a vector consisting of the
words contained in the document, where each di-
mension corresponds to a word, and the weight is
the frequency in the corresponding document. The
similarity between two documents can be computed
as the cosine of the angle between their vectors. This
is the approached use above.
This approach can be improved giving more
weight to words which occur in only a few docu-
ments, and less weight to words occurring in many
documents (Baeza-Yates and Ribeiro-Neto, 1999).
In our system, we count document frequencies of
words using Wikipedia as a reference corpus since
the training data consists of only 750 items associ-
ated with short textual information and might not be
sufficient for reliable estimations. The TF.IDF sim-
ilarity between items a and b is defined as:
simtf.idf(a, b) =
?
w?a,b tfw,a ? tfw,b ? idf
2
w
??
w?a(tfw,a ? idfw)
2 ?
??
w?b(tfw,b ? idfw)
2
where tfw,x is the frequency of the term w in x ?
{a, b} and idfw is the inverted document frequency
of the word w measured in Wikipedia. We substi-
tuted the preliminary general similarity score by the
obtained using the TF.IDF presented in this section.
2.2 UKB
The semantic disambiguation UKB3 algorithm
(Agirre and Soroa, 2009) applies personalized
PageRank on a graph generated from the English
WordNet (Fellbaum, 1998), or alternatively, from
Wikipedia. This algorithm has proven to be very
competitive in word similarity tasks (Agirre et al,
2010).
To compute similarity using UKB we represent
WordNet as a graph G = (V,E) as follows: graph
nodes represent WordNet concepts (synsets) and
3http://ixa2.si.ehu.es/ukb/
dictionary words; relations among synsets are rep-
resented by undirected edges; and dictionary words
are linked to the synsets associated to them by di-
rected edges.
Our method is provided with a pair of vectors of
words and a graph-based representation of WordNet.
We first compute the personalized PageRank over
WordNet separately for each of the vector of words,
producing a probability distribution over WordNet
synsets. We then compute the similarity between
these two probability distributions by encoding them
as vectors and computing the cosine between the
vectors. We present each step in turn.
Once personalized PageRank is computed, it
returns a probability distribution over WordNet
synsets. The similarity between two vectors of
words can thus be implemented as the similarity be-
tween the probability distributions, as given by the
cosine between the vectors.
We used random walks to compute improved sim-
ilarity values for author, people involved, location
and event similarity:
? Author: UKB over Wikipedia using person en-
tities recognized by NERC in the dc:Creator
field.
? People involved and location: UKB over
Wikipedia using people/location entities recog-
nized by NERC in all fields.
? Events: UKB over WordNet using event nouns
and verbs recognized in all fields.
Results on the training data showed that perfor-
mance using this approach was quite low (with the
exception of events). This was caused by the large
number of cases where the Stanford parser did not
find entities which were in Wikipedia. With those
cases on mind, we combined the scores returned by
UKB with the similarity scores presented in Section
2 as follows: if UKB similarity returns a score, we
multiply both, otherwise we return the square of the
other similarity score. Using the multiplication of
the two scores, the results on the training data im-
proved.
2.3 Time similarity measure
In order to measure the time similarity between a
pair of items, we need to recognize time expres-
sions in both items. We assume that the year of
133
creation or the year denoting when the event took
place in an artefact are good indicators for time sim-
ilarity. Therefore, information about years is ex-
tracted from each item using the following pattern:
[1|2][0 ? 9]{3}. Using this approach, each item is
represented as a set of numbers denoting the years
mentioned in the meta-data.
Time similarity between two items is computed
based on the similarity between their associated
years. Similarity between two years is defined as:
simyear(y1, y2) = max{0, 1? |y1? y2| ? k}
k is a parameter to weight the difference between
two years, e.g. for k = 0.1 all items that have differ-
ence of 10 years or more assigned a score of 0. We
obtained best results for k = 0.1.
Finally, time similarity between items a and b is
computed as the maximum of the pairwise similarity
between their associated years:
simtime(a, b) = max?i?a
?j?b
{0, simyear(ai, bj)}
We substituted the preliminary time similarity
score by the measure obtained using the method pre-
sented in this section.
3 Applying Machine Learning
The above heuristics can be good indicators for the
respective kind of similarity, and can be thus applied
directly to the task. In this section, we take those
indicators as features, and use linear regression (as
made available by Weka (Hall et al, 2009)) to learn
models that fit the features to the training data.
We generated further similarity scores for gen-
eral similarity, including Latent Dirichlet Allocation
(LDA) (Blei et al, 2003), UKB and Wikipedia Link
Vector Model (WLVM)(Milne, 2007) using infor-
mation taken from all fields, as explained below.
3.1 LDA
LDA (Blei et al, 2003) is a statistical method that
learns a set of latent variables called topics from a
training corpus. Given a topic model, documents
can be inferred as probability distributions over top-
ics, ?. The distribution for a document i is denoted
as ?i. An LDA model is trained using the train-
ing set consisting of 100 topics using the gensim
package4. The hyperparameters (?, ?) were set to
1
num of topics . Therefore, each item in the test set is
represented as a topic distribution.
The similarity between a pair of items is estimated
by comparing their topic distributions following the
method proposed in Aletras et al (2012; Aletras and
Stevenson (2012). This is achieved by considering
each distribution as a vector (consisting of the topics
corresponding to an item and its probability) then
computing the cosine of the angle between them, i.e.
simLDA(a, b) =
~?a ? ~?b
|~?a| ? | ~?b|
where ~?a is the vector created from the probability
distribution generated by LDA for item a.
3.2 Pairwise UKB
We run UKB (Section 2.2) to generate a probabil-
ity distribution over WordNet synsets for all of the
words of all items. Similarity between two words
is computed by creating vectors from these distri-
butions and comparing them using the cosine of the
angle between the two vectors. If a words does not
appear in WordNet its similarity value to every other
word is set to 0. We refer to that similarity metric as
UKB here.
Similarity between two items is computed by per-
forming pairwise comparison between their words,
for each, selecting the highest similarity score:
sim(a, b) =
1
2
(?
w1?a
argmaxw2?b UKB(w1, w2)
|a|
+
?
w2?b
argmaxw1?a UKB(w2, w1)
|b|
)
where a and b are two items, |a| the number of
tokens in a and UKB(w1, w2) is the similarity be-
tween words w1 and w2.
3.3 WLVM
An algorithm described by Milne and Witten (2008)
associates Wikipedia articles which are likely to be
relevant to a given text snippet using machine learn-
ing techniques. We make use of that method to rep-
resent each item as a set of likely relevant Wikipedia
4http://pypi.python.org/pypi/gensim
134
articles. Then, similarity between Wikipedia arti-
cles is measured using the Wikipedia Link Vector
Model (WLVM) (Milne, 2007). WLVM uses both
the link structure and the article titles of Wikipedia
to measure similarity between two Wikipedia arti-
cles. Each link is weighted by the probability of it
occurring. Thus, the value of the weight w for a link
x? y between articles x and y is:
w(x? y) = |x? y| ? log
(
t?
z=1
t
z ? y
)
where t is the total number of articles in Wikipedia.
The similarity of articles is compared by forming
vectors of the articles which are linked from them
and computing the cosine of their angle. For exam-
ple the vectors of two articles x and y are:
x = (w(x? l1), w(x? l2), ..., w(x? ln))
y = (w(y ? l1), w(y ? l2), ..., w(y ? ln))
where x and y are two Wikipedia articles and x? li
is a link from article x to article li.
Since the items have been mapped to Wikipedia
articles, similarity between two items is computed
by performing pairwise comparison between articles
using WLVM, for each, selecting the highest simi-
larity score:
sim(a, b) =
1
2
(?
w1?a
argmaxw2?bWLVM(w1, w2)
|a|
+
?
w2?b
argmaxw1?aWLVM(w2, w1)
|b|
)
where a and b are two items, |a| the number of
Wikipedia articles in a and WLVM(w1, w2) is the
similarity between concepts w1 and w2.
4 Submissions
We selected three systems for submission. The first
run uses the similarity scores of the basic system
(Section 2) for each similarity types as follows:
? General: cosine similarity of TF.IDF vectors,
IDF based on Wikipedia (as shown in Section
2.1).
? Author: product of the scores obtained ob-
tained using TF.IDF vectors and UKB (as
shown in Section 2.2) using only the data ex-
tracted from dc:Creator field.
? People involved and location: product of co-
sine similarity of TF.IDF vectors and UKB (as
shown in Section 2.2) using the data extracted
from all fields.
? Time period: time similarity measure (as
shown in Section 2.3).
? Events: product of cosine similarity of TF.IDF
vectors and UKB (as shown in Section 2.2) of
event nouns and verbs recognized in all fields.
? Subject and description: cosine similarity of
TF.IDF vectors of respective fields (as shown
in Section 2).
For the second run we trained a ML model for
each of the similarity types, using the following fea-
tures:
? Cosine similarity of TF.IDF vectors as shown
in Section 2 for the eight similarity types.
? Four new values for general similarity: TF.IDF
(Section 2.1), LDA (Section 3.1), UKB and
WLVM (Section 3.3).
? Time similarity as shown in Section 2.3.
? Events similarity computed using UKB initial-
ized with the event nouns and verbs in all fields.
We decided not to use the product of TF.IDF
and UKB presented in Section 2.2 in this system
because our intention was to measure the power of
the linear regression ML algorithm to learn on the
given raw data.
The third run is similar, but includes all available
features (21). In addition to the above, we included:
? Author, people involved and location similar-
ity computed using UKB initialized with peo-
ple/location recognized by NERC in dc:Creator
field for author, and in all fields for people in-
volved and location.
? Author, people involved, location and event
similarity scores computed by the product of
TF.IDF vectors and UKB values as shown in
Section 2.2.
5 Results
Evaluation was carried out using the official scorer
provided by the organizers, which computes the
Pearson Correlation score for each of the eight sim-
ilarity types plus an additional mean correlation.
135
Team and run General Author People involved Time Location Event Subject Description Mean
UBC UOS-RUN1 0.7269 0.4474 0.4648 0.5884 0.4801 0.2522 0.4976 0.5389 0.5033
UBC UOS-RUN2 0.7777 0.6680 0.6767 0.7609 0.7329 0.6412 0.7516 0.8024 0.7264
UBC UOS-RUN3 0.7866 0.6941 0.6965 0.7654 0.7492 0.6551 0.7586 0.8067 0.7390
Table 1: Results of our systems on the training data, using cross-validation when necessary.
Team and run General Author People involved Time Location Event Subject Description Mean Rank
UBC UOS-RUN1 0.7256 0.4568 0.4467 0.5762 0.4858 0.3090 0.5015 0.5810 0.5103 6
UBC UOS-RUN2 0.7457 0.6618 0.6518 0.7466 0.7244 0.6533 0.7404 0.7751 0.7124 4
UBC UOS-RUN3 0.7461 0.6656 0.6544 0.7411 0.7257 0.6545 0.7417 0.7763 0.7132 3
Table 2: Results of our submitted systems.
5.1 Development
The three runs mentioned above were developed us-
ing the training data made available by the organiz-
ers. In order to avoid overfitting we did not change
the default parameters of the linear regressor, and
10-fold cross-validation was used for evaluating the
models on the training data. The results of our sys-
tems on the training data are shown on Table 1. The
table shows that the heuristics (RUN1) obtain low
results, and that linear regression improves results
considerably in all types. Using the full set of fea-
tures, RUN3 improves slightly over RUN2, but the
improvement is consistent across all types.
5.2 Test
The test dataset was composed of 750 pairs of items.
Table 2 illustrates the results of our systems in the
test dataset. The results of the runs are very similar
to those obtained on the training data, but the dif-
ference between RUN2 and RUN3 is even smaller.
Our systems were ranked #3 (RUN 3), #4 (RUN
2) and #6 (RUN 1) among 14 systems submitted
by 6 teams. Our systems achieved good correlation
scores for almost all similarity types, with the excep-
tion of author similarity, which is the worst ranked
in comparison with the rest of the systems.
6 Conclusions and Future Work
In this paper, we presented the systems submitted
to the *SEM 2013 shared task on Semantic Tex-
tual Similarity. We combined some simple heuris-
tics for each type of similarity, based on the appro-
priate metadata fields. The use of lineal regression
improved the results considerably across all types.
Our system fared well in the competition. We sub-
mitted three systems and the highest-ranked of these
achieved the third best results overall.
Acknowledgements
This work is partially funded by the PATHS
project (http://paths-project.eu) funded by the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 270082. Aitor Gonzalez-Agirre is supported by
a PhD grant from the Spanish Ministry of Education,
Culture and Sport (grant FPU12/06243).
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Proceed-
ings of the 12th conference of the European chapter of
the Association for Computational Linguistics (EACL-
2009), Athens, Greece.
Eneko Agirre, Montse Cuadros, German Rigau, and Aitor
Soroa. 2010. Exploring knowledge bases for sim-
ilarity. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC10). European Language Resources Associa-
tion (ELRA). ISBN: 2-9517408-6-7. Pages 373?377.?.
Nikolaos Aletras and Mark Stevenson. 2012. Computing
similarity between cultural heritage items using multi-
modal features. In Proceedings of the 6th Workshop
on Language Technology for Cultural Heritage, So-
cial Sciences, and Humanities, pages 85?93, Avignon,
France.
Nikolaos Aletras, Mark Stevenson, and Paul Clough.
2012. Computing similarity between items in a digi-
tal library of cultural heritage. J. Comput. Cult. Herit.,
5(4):16:1?16:19, December.
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern In-
formation Retrieval. Addison Wesley Longman Lim-
ited, Essex.
136
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022, March.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
D. Milne and I. Witten. 2008. Learning to Link
with Wikipedia. In Proceedings of the ACM Con-
ference on Information and Knowledge Management
(CIKM?2008), Napa Valley, California.
D. Milne. 2007. Computing semantic relatedness using
Wikipedia?s link structure. In Proceedings of the New
Zealand Computer Science Research Student Confer-
ence.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
137
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 85?93,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Computing Similarity between Cultural Heritage
Items using Multimodal Features
Nikolaos Aletras
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP, UK
n.aletras@dcs.shef.ac.uk
Mark Stevenson
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP, UK
m.stevenson@dcs.shef.ac.uk
Abstract
A significant amount of information about
Cultural Heritage artefacts is now available
in digital format and has been made avail-
able in digital libraries. Being able to iden-
tify items that are similar would be use-
ful for search and navigation through these
data sets. Information about items in these
repositories is often multimodal, such as
pictures of the artefact and an accompa-
nying textual description. This paper ex-
plores the use of information from these
various media for computing similarity be-
tween Cultural Heritage artefacts. Results
show that combining information from im-
ages and text produces better estimates of
similarity than when only a single medium
is considered.
1 Introduction and Motivation
In recent years a vast amount of Cultural Heritage
(CH) artefacts have been digitised and made avail-
able on-line. For example, the Louvre and the
British Museum provide information about ex-
hibits on their web pages1. In addition, informa-
tion is also available via sites that aggregate CH
information from multiple resources. A typical
example is Europeana2, a web-portal to collec-
tions from several European institutions that pro-
vides access to over 20 million items including
paintings, films, books, archives and museum ex-
hibits.
However, online information about CH arte-
facts is often unstructured and varies by collec-
1http://www.louvre.fr/,
http://www.britishmuseum.org/
2http://www.europeana.eu
tion. This makes it difficult to identify informa-
tion of interest in sites that aggregate informa-
tion from multiple sources, such as Europeana,
or to compare information across multiple collec-
tions (such as the Louvre and British Museum).
These problems form a significant barrier to ac-
cessing the information available in these online
collections. A first step towards improving access
would be to identify similar items in collections.
This could assist with several applications that are
of interest to those working in CH including rec-
ommendation of interesting items (Pechenizkzy
and Calders, 2007; Wang et al, 2008), generation
of virtual tours (Joachims et al, 1997; Wang et al,
2009), visualisation of collections (Kauppinen et
al., 2009; Hornbaek and Hertzum, 2011) and ex-
ploratory search (Marchionini, 2006; Amin et al,
2008).
Information in digital CH collections often in-
cludes multiple types of media such as text, im-
ages and audio. It seems likely that informa-
tion from all of these types would help humans
to identify similar items and that it could help to
identify them automatically. However, previous
work on computing similarity in the CH domain
has been limited and, in particular, has not made
use of information from multiple types of media.
For example, Grieser et al (2011) computed sim-
ilarity between exhibits in Melbourne Museum
by applying a range of text similarity measures
but did not make use of other media. Tech-
niques for exploiting information from multi-
media collections have been developed and are
commonly applied to a wide range of problems
such as Content-based Image Retrieval (Datta et
al., 2008) and image annotation (Feng and Lap-
ata, 2010).
85
This paper makes use of information from two
media (text and images) to compute the similar-
ity between items in a large collection of CH
items (Europeana). A range of similarity mea-
sures for text and images are compared and com-
bined. Evaluation is carried out using a set of
items from Europeana with similarity judgements
that were obtained in a crowdsourcing experi-
ment. We find that combining information from
both media produces better results than when ei-
ther is used alone.
The main contribution of this paper is to
demonstrate the usefulness of applying informa-
tion from more than one medium when compar-
ing CH items. In addition, it explores the effec-
tiveness of different similarity measures when ap-
plied to this domain and introduces a data set of
similarity judgements that can be used as a bench-
mark.
The remainder of this paper is structured as fol-
lows. Section 2 describes some relevant previous
work. Sections 3, 4 and 5 describe the text and im-
age similarity measures applied in this paper and
how they are combined. Sections 6 describes the
experiments used in this paper and the results are
reported in Section 7. Finally, Section 8 draws
the conclusions and provides suggestions for fu-
ture work.
2 Background
2.1 Text Similarity
Two main approaches for determining the similar-
ity between two texts have been explored: corpus-
based and knowledge-based methods. Corpus-
based methods rely on statistics that they learn
from corpora while knowledge-based methods
make use of some external knowledge source,
such as a thesaurus, dictionary or semantic net-
work (Agirre et al, 2009; Gabrilovich and
Markovitch, 2007).
A previous study (Aletras et al, 2012) com-
pared the effectiveness of various methods for
computing the similarity between items in a CH
collection based on text extracted from their
descriptions, including both corpus-based and
knowledge-based approaches. The corpus-based
approaches varied from simple word counting ap-
proaches (Manning and Schutze, 1999) to more
complex ones based on techniques from Infor-
mation Retrieval (Baeza-Yates and Ribeiro-Neto,
1999) and topic models (Blei et al, 2003). The
knowledge-based approaches relied on Wikipedia
(Milne, 2007). Aletras et al (2012) concluded
that corpus-based measures were more effective
than knowledge-based ones for computing simi-
larity between these items.
2.2 Image Similarity
Determining the similarity between images has
been explored in the fields such as Computer Vi-
sion (Szeliski, 2010) and Content-based Image
Retrieval (CBIR) (Datta et al, 2008). A first step
in computing the similarity between images is to
transform them into an appropriate set of features.
Some major feature types which have been used
are colour, shape, texture or salient points. Fea-
tures are also commonly categorised into global
and local features.
Global features characterise an entire image.
For example, the average of the intensities of red,
green and blue colours gives an estimation of the
overall colour distribution in the image. The main
advantages of global features are that they can be
computed efficiently. However, they are unable
to represent information about elements in an im-
age (Datta et al, 2008). On the other hand, lo-
cal features aim to identify interesting areas in
the image, such as where significant differences
in colour intensity between adjacent pixels is de-
tected.
Colour is one of the most commonly used
global features and has been applied in sev-
eral fields including image retrieval (Jacobs et
al., 1995; Sebe and Michael S. Lew, 2001;
Yu et al, 2002), image clustering (Cai et al,
2004; Strong and Gong, 2009), database index-
ing (Swain and Ballard, 1991) and, object/scene
recognition (Schiele and Crowley, 1996; Ndjiki-
Nya et al, 2004; Sande et al, 2008). A common
method for measuring similarity between images
is to compare the colour distributions of their his-
tograms. A histogram is a graphical representa-
tion of collected counts for predefined categories
of data. To create a histogram we have to specify
the range of the data values, the number of dimen-
sions and the bins (intervals into which ranges
of values are combined). A colour histogram
records the number of the pixels that fall in the
interval of each bin. Schiele and Crowley (1996)
describe several common metrics for comparing
colour histograms including ?2, correlation and
86
intersection.
2.3 Combining Text and Image Features
The integration of information from text and im-
age features has been explored in several fields.
In Content-based Image Retrieval image features
are combined together with words from captions
to retrieve images relevant to a query (La Cas-
cia et al, 1998; Srihari et al, 2000; Barnard
and Forsyth, 2001; Westerveld, 2002; Zhou and
Huang, 2002; Wang et al, 2004). Image cluster-
ing methods have been developed to combine in-
formation from images and text to create clusters
of similar images (Loeff et al, 2006; Bekkerman
and Jeon, 2007). Techniques for automatic image
annotation that generate models as a mixture of
word and image features have also been described
(Jeon et al, 2003; Blei and Jordan, 2003; Feng
and Lapata, 2010).
2.4 Similarity in Cultural Heritage
Despite the potential usefulness of similarity in
CH, there has been little previous work on the
area. An exception is the work of Grieser et al
(2011). They computed the similarity between a
set of 40 exhibits from Melbourne Museum by
analysing the museum?s web pages and physi-
cal layout. They applied a range of text similar-
ity techniques (see Section 2.1) to the web pages
as well as similarity measures that made use of
Wikipedia. However, the Wikipedia-based tech-
niques relied on a manual mapping between the
items and an appropriate Wikipedia article. Al-
though the web pages often contained images of
the exhibits, Grieser et al (2011) did not make use
of them.
3 Text Similarity
We make use of various corpus-based approaches
for computing similarity between CH items since
previous experiments (see Section 2.1) have
shown that these outperformed knowledge-based
methods in a comparison of text-based similarity
methods for the CH domain.
We assume that we wish to compute the simi-
larity between a pair of items, A and B, and that
each item has both text and an image associated
with it. The text is denoted as At and Bt while
the images are denoted by Ai and Bi.
3.1 Word Overlap
A common approach to computing similarity is to
count the number of common words (Lesk, 1986).
The text associated with each item is compared
and the similarity is computed as the number of
words (tokens) they have in common normalised
by the combined total:
simWO(A,B) =
|At ?Bt|
|At ?Bt|
3.2 N-gram Overlap
The Word Overlap approach is a bag of words
method that does not take account of the order
in which words appear, despite the fact that this
is potentially useful information for determining
similarity. One way in which this information can
be used is to compare n-grams derived from a text.
Patwardhan et al (2003) used this approach to ex-
tend the Word Overlap measure. This approach
identifies n-grams in common between the two
text and increases the score by n2 for each one
that is found, where n is the length of the n-gram.
More formally,
simngram(A,B) =
?
n ? n?gram(At,Bt)
n2
|At ?Bt|
where n?gram(At, Bt) is the set of n-grams that
occur in both At and Bt.
3.3 TF.IDF
The word and n-gram overlap measures assign
the same importance to each word but some are
more important for determining similarity be-
tween texts than others. A widely used approach
to computing similarity between documents is to
represent them as vectors in which each term is
assigned a weighting based on its estimated im-
portance (Manning and Schutze, 1999). The vec-
tors can then be compared using the cosine met-
ric. A widely used scheme for weighting terms
is tf.idf, which takes account of the frequency of
each term in individual documents and the num-
ber of documents in a corpus in which it occurs.
3.4 Latent Dirichlet Allocation
Topic models (Blei et al, 2003) are a useful tech-
nique for representing the underlying content of
documents. LDA is a widely used topic model
87
that assumes each document is composed of a
number of topics. For each document LDA re-
turns a probability distribution over a set of topics
that have been derived from an unlabeled corpus.
Similarity between documents can be computed
by converting these distributions into vectors and
using the cosine metric.
4 Image Similarity
Two approaches are compared for computing the
similarity between images. These are largely
based on colour features and are more suitable for
the images in the data set we use for evaluation
(see Section 6).
4.1 Colour Similarity (RGB)
The first approach is based on comparison of
colour histograms derived from images.
In the RGB (Red Green Blue) colour model,
each pixel is represented as an integer in range of
0-255 in three dimensions (Red, Green and Blue).
One histogram is created for each dimension. For
grey-scale images it is assumed that the value of
each dimension is the same in each pixel and a
single histogram, called the luminosity histogram,
is created. Similarity between the histograms in
each colour channel is computed using the inter-
section metric. The intersection metric (Swain
and Ballard, 1991) measures the number of cor-
responding pixels that have same colour in two
images. It is defined as follows:
Inter(h1, h2) =
?
I
min(h1(I), h2(I))
where hi is the histogram of image i, I is the set
of histogram bins and min(a, b) is the minimum
between corresponding pixel colour values.
The final similarity score is computed as the av-
erage of the red, green and blue histogram simi-
larity scores:
simRGB(Ai, Bi) =
?
i?{R,G,B}
Inter(hAi , hBi)
3
4.2 Image Querying Metric (imgSeek)
Jacobs et al (1995) described an image similar-
ity metric developed for Content-based Image Re-
trieval. It makes use of Haar wavelet decompo-
sition (Beylkin et al, 1991) to create signatures
of images that contain colour and basic shape in-
formation. Images are compared by determining
the number of significant coefficients they have in
common using the following function:
distimgSeek(Ai, Bi) = w0|CAi(0, 0)? CBi(0, 0)|
+
?
i,j:C?Ai (i,j) 6=0
wbin(i,j)(C?Ai(i, j) 6= C?Bi(i, j))
where wb are weights, CI represents a single
colour channel for an image I , CI(0, 0) are scal-
ing function coefficients of the overall average in-
tensity of the colour channel and C?I(i, j) is the
(i, j)-th truncated, quantised wavelet coefficient
of image I . For more details please refer to Ja-
cobs et al (1995).
Note that this function measures the distance
between two images with low scores indicating
similar images and high scores dis-similar ones.
We assign the negative sign to this metric to assign
high scores to similar images. It is converted into
a similarity metric as follows:
simimgSeek(Ai, Bi) = ?distimgSeek(Ai, Bi)
5 Combining Text and Image Similarity
A simple weighted linear combination is used to
combine the results of the text and image similar-
ities, simimg and simt. The similarity between a
pair of items is computed as follows
simT+I(A,B) = w1 ? simt(At, Bt)
+ w2 ? simimg(Ai, Bi)
where wi are weights learned using linear regres-
sion (see Section 6.4).
6 Evaluation
This section describes experiments used to evalu-
ate the similarity measures described in the previ-
ous sections.
6.1 Europeana
The similarity measures are evaluated using infor-
mation from Europeana3, a web-portal that pro-
vides access to information CH artefacts. Over
2,000 institutions through out Europe have con-
tributed to Europeana and the portal provides ac-
cess to information about over 20 million CH arte-
facts, making it one of the largest repositories
3http://www.europeana.eu
88
of digital information about CH currently avail-
able. It contains information about a wide vari-
ety of types of artefacts including paintings, pho-
tographs and newspaper archives. The informa-
tion is in a range of European languages, with
over 1 million items in English. The diverse na-
ture of Europeana makes it an interesting resource
for exploring similarity measures.
The Europeana portal provides various types of
information about each artefact, including textual
information, thumbnail images of the items and
links to additional information available for the
providing institution?s web site. The textual in-
formation is derived from metadata obtained from
the providing institution and includes title, de-
scription as well as details of the subject, medium
and creator.
An example artefact from the Europeana por-
tal is shown in Figure 1. This particular artefact
is an image showing detail of an architect?s office
in Nottingham, United Kingdom. The informa-
tion provided for this item is relatively rich com-
pared to other items in Europeana since the title is
informative and the textual description is of rea-
sonable length. However, the amount of informa-
tion associated with items in Europeana is quite
varied and it is common for items to have short
titles, which may be uninformative, or have very
limited textual descriptions. In addition, the meta-
data associated with items in Europeana is poten-
tially a valuable source of information that could
be used for, among other things, computing simi-
larity between items. However, the various pro-
viding institutions do not use consistent coding
schemes to populate these fields which makes it
difficult to compare items provided by different
institutions. These differences in the information
provided by the various institutions form a signif-
icant challenge in processing the Europeana data
automatically.
6.2 Evaluation Data
A data set was created by selecting 300 pairs of
items added to Europeana by two providers: Cul-
ture Grid4 and Scran5. The items added to Eu-
ropeana by these providers represent the major-
ity that are in English and they contain different
types of items such as objects, archives, videos
and audio files. We removed five pairs that did
4http://www.culturegrid.org.uk/
5http://www.scran.ac.uk/
not have any images associated with one of the
items. (These items were audiofiles.) The result-
ing dataset consists of 295 pairs of items and is
referred to as Europeana295.
Each item corresponds to a metadata record
consisting of textual information together with a
URI and a link to its thumbnail. Figure 1 shows an
item taken from the Europeana website. The title,
description and subject fields have been shown
to be useful information for computing similar-
ity (Aletras et al, 2012). These are extracted and
concatenated to form the textual information as-
sociated with each item. In addition, the accom-
panying thumbnail image (or ?preview?) was also
extracted to be used as the visual information. The
size of these images varies from 7,000 to 10,000
pixels.
We have pre-processed the data by removing
stop words and applying stemming. For the
tf.idf and LDA the training corpus was a total of
759,896 Europeana items. We have filtered out
all items that have no description and have a ti-
tle shorter than 4 words, or have a title which has
been repeated more than 100 times.
6.3 Human Judgements of Similarity
Crowdflower6, a crowdsourcing platform, was
used to obtain human judgements of the simi-
larity between each pair of items. Participants
were asked to rate each item pair using a 5 point
scale where 4 indicated that the pair of items were
highly similar or near-identical while 0 indicated
that they were completely different. Participants
were presented with a page containing 10 pairs of
items and asked to rate all of them. Participants
were free to rate as many pages as they wanted up
to a maximum of 30 pages (i.e. the complete Eu-
ropeana295 data set). To ensure that the annota-
tors were not returning random answers each page
contained a pair for which the similarity had been
pre-identified as being at one end of the similarity
scale (i.e. either near-identical or completely dif-
ferent). Annotations from participants that failed
to answer correctly these questions or participants
that have given same rating to all of their answers
were removed. A total of 3,261 useful annotations
were collected from 99 participants and each pair
was rated by at least 10 participants.
The final similarity score for each pair was gen-
6http://crowdflower.com/
89
Figure 1: Example item from Europeana portal showing how both textual and image information are displayed.
(Taken from http://www.europeana.eu/portal/)
erated by averaging the ratings. Inter-annotator
agreement was computed as the average of the
Pearson correlation between the ratings of each
participant and the average ratings of the other
participants, a methodology used by Grieser et
al. (2011). The inter-annotator agreement for the
data set was ? = +0.553, which is comparable
with the agreement score of ? = +0.507 previ-
ously reported by Grieser et al (2011).
6.4 Experiments
Experiments were carried out comparing the re-
sults of the various techniques for computing text
and image similarity (Sections 3 and 4) and their
combination (Section 5). Performance is mea-
sured as the Pearson?s correlation coefficient with
the gold-standard data.
The combination of text and image similarity
(Section 5) relies on a linear combination of text
and image similarities. The weights for this com-
bination are obtained using a linear regression
model. The input values were the results obtained
for the individual text and similarity methods and
the target value was the gold-standard score for
each pair in the dataset. 10-fold cross-validation
was used for evaluation.
7 Results
An overview of the results obtained is shown in
Table 1. Results for the text and image similarity
methods used alone are shown in the left and top
part of the table while the results for their combi-
Image Similarity
RGB imgSeek
Text Similarity 0.254 0.370
Word Overlap 0.487 0.450 0.554
tf.idf 0.437 0.426 0.520
N-gram overlap 0.399 0.384 0.504
LDA 0.442 0.419 0.517
Table 1: Performance of similarity measures applied
to Europeana295 data set (Pearson?s correlation coef-
ficient).
nation are in the main body.
The best performance for text similarity (0.487)
is achieved by Word Overlap and the lowest by
N-gram Overlap (0.399). The results are surpris-
ing since the simplest approach produces the best
results. It is likely that the reason for these re-
sults is the nature of the textual data in Europeana.
The documents are often short, in some cases the
description missing or the subject information is
identical to the title.
For image similarity, results using imgSeek are
higher than RGB (0.370 and 0.254 respectively).
There is also a clear difference between the per-
formance of the text and image similarity meth-
ods and results obtained from both image similar-
ity measures is lower than all four that are based
on text. The reason for these results is the nature
of the Europeana images. There are a large num-
ber of black-and-white image pairs which means
that colour information cannot be obtained from
90
many of them. In addition, the images are low
resolution, since they are thumbnails, which lim-
its the amount of shape information that can be
derived from them, restricting the effectiveness of
imgSeek. However, the fact that performance is
better for imgSeek and RGB suggests that it is still
possible to obtain useful information about shape
from these images.
When the image and text similarity measures
are combined the highest performance is achieved
by the combination of the Word Overlap and
imgSeek (0.554), the best performing text and im-
age similarity measures when applied individu-
ally. The performance of all text similarity mea-
sures improves when combined with imgSeek.
All results are above 0.5 with the highest gain
observed for N-gram Overlap (from 0.399 to
0.504), the worst performing text similarity mea-
sure when applied individually. On the other
hand, combining text similarity measures with
RGB consistently leads to performance that is
lower than when the text similarity measure is
used alone.
These results demonstrate that improvements
in similarity scores can be obtained by making
use of information from both text and images. In
addition, better results are obtained for the text
similarity methods and this is likely to be caused
by the nature of the images which are associated
with the items in our data set. It is also impor-
tant to make use of an appropriate image similar-
ity method since combing text similarity methods
with RGB reduces performance.
8 Conclusion
This paper demonstrates how information from
text and images describing CH artefacts can be
combined to improve estimates of the similarity
between them. Four corpus-based and two image-
based similarity measures are explored and eval-
uated on a data set consisting of 295 manually-
annotated pairs of items from Europeana. Results
showed that combing information from text and
image similarity improves performance and that
imgSeek similarity method consistently improves
performance of text similarity methods.
In future work we intend to make use of other
types of image features including the low-level
ones used by approaches such as Scale Invari-
ant Feature Transformation (SIFT) (Lowe, 1999;
Lowe, 2004) and the bag-of-visual words model
(Szeliski, 2010). In addition we plan to apply
these approaches to higher resolution images to
determine how the quality and size of an image
affects similarity algorithms.
Acknowledgments
The research leading to these results was
carried out as part of the PATHS project
(http://paths-project.eu) funded by
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 270082.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL ?09), pages 19?27, Boulder, Colorado.
Nikolaos Aletras, Mark Stevenson, and Paul Clough.
2012. Computing similarity between items in a dig-
ital library of cultural heritage. Submitted.
Alia Amin, Jacco van Ossenbruggen, Lynda Hard-
man, and Annelies van Nispen. 2008. Understand-
ing Cultural Heritage Experts? Information Seeking
Needs. In Proceedings of the 8th ACM/IEEE-CS
Joint Conference on Digital Libraries, pages 39?47,
Pittsburgh, PA.
Ricardo Baeza-Yates and Berthier Ribeiro-Neto.
1999. Modern Information Retrieval. Addison
Wesley Longman Limited, Essex.
Kobus Barnard and David Forsyth. 2001. Learn-
ing the Semantics of Words and Pictures. Pro-
ceedings Eighth IEEE International Conference on
Computer Vision (ICCV ?01), 2:408?415.
Ron Bekkerman and Jiwoon Jeon. 2007. Multi-modal
clustering for multimedia collections. In IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion (CVPR ?07), pages 1?8.
Gregory Beylkin, Ronald Coifman, and Vladimir
Rokhlin. 1991. Fast Wavelet Transforms and Nu-
merical Algorithms I. Communications on Pure
and Applied Mathematics, 44:141?183.
David M. Blei and Michael I. Jordan. 2003. Modeling
Annotated Data. Proceedings of the 26th annual
international ACM SIGIR conference on Research
and Development in Information Retrieval (SIGIR
?03), pages 127?134.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
91
Deng Cai, Xiaofei He, Zhiwei Li, Wei-Ying Ma, and
Ji-Rong Wen. 2004. Hierarchical Clustering of
WWW Image Search Results Using Visual, Textual
and Link Information. Proceedings of the 12th an-
nual ACM international conference on Multimedia
(MULTIMEDIA ?04), pages 952?959.
Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z.
Wang. 2008. Image Retrieval: Ideas, Influences,
and Trends of the New Age. ACM Computing Sur-
veys, 40(2):1?60.
Yansong Feng and Mirella Lapata. 2010. Topic
Models for Image Annotation and Text Illustration.
In Proceedings of Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 831?839, Los Angeles, California,
June.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the International Joint Conference on Artificial In-
telligence (IJCAI ?07), pages 1606?1611.
Karl Grieser, Timothy Baldwin, Fabian Bohnert, and
Liz Sonenberg. 2011. Using Ontological and Doc-
ument Similarity to Estimate Museum Exhibit Re-
latedness. Journal on Computing and Cultural Her-
itage (JOCCH), 3(3):10:1?10:20.
Kasper Hornbaek and Morten Hertzum. 2011. The
notion of overview in information visualization. In-
ternational Journal of Human-Computer Studies,
69:509?525.
Charles E. Jacobs, Adam Finkelstein, and David H.
Salesin. 1995. Fast multiresolution image query-
ing. In Proceedings of the 22nd annual conference
on Computer Graphics and Interactive Techniques
(SIGGRAPH ?95), pages 277?286, New York, NY,
USA.
Jiwoon Jeon, Victor Lavrenko, and Raghavan Man-
matha. 2003. Automatic image annotation and re-
trieval using cross-media relevance models. In Pro-
ceedings of the 26th annual international ACM SI-
GIR Conference on Research and Development in
Information Retrieval (SIGIR ?03), pages 119?126,
New York, NY, USA.
Thorsten Joachims, Dayne Freitag, and Tom Mitchell.
1997. Webwatcher: A tour guide for the world wide
web. In Proceedings of the International Joint Con-
ference on Artificial Intelligence (IJCAI ?97), pages
770?777.
Tomi Kauppinen, Kimmo Puputti, Panu Paakkarinen,
Heini Kuittinen, Jari Va?a?ta?inen, and Eero Hyvo?nen.
2009. Learning and visualizing cultural heritage
connections between places on the semantic web.
In Proceedings of the Workshop on Inductive Rea-
soning and Machine Learning on the Semantic Web
(IRMLeS2009) and the 6th Annual European Se-
mantic Web Conference (ESWC2009), Heraklion,
Crete, Greece.
Marco La Cascia, Sarathendu Sethi, and Stan Sclaroff.
1998. Combining textual and visual cues for
content-based image retrieval on the world wide
web. In IEEE Workshop on Content-Based Access
of Image and Video Libraries, pages 24?28.
Michael Lesk. 1986. Automatic Sense Disambigua-
tion using Machine Readable Dictionaries: how to
tell a pine cone from an ice cream cone. In Proceed-
ings of the ACM Special Interest Group on the De-
sign of Communication Conference (SIGDOC ?86),
pages 24?26, Toronto, Canada.
Nicolas Loeff, Cecilia Ovesdotter Alm, and David A.
Forsyth. 2006. Discriminating image senses by
clustering with multimodal features. In Proceed-
ings of the COLING/ACL on Main Conference
Poster Sessions (COLING-ACL ?06), pages 547?
554, Stroudsburg, PA, USA.
David G. Lowe. 1999. Object Recognition from Local
Scale-invariant Features. Proceedings of the Sev-
enth IEEE International Conference on Computer
Vision, pages 1150?1157.
David G. Lowe. 2004. Distinctive Image Fea-
tures from Scale-Invariant Keypoints. International
Journal of Computer Vision, 60(2):91?110.
Christopher D. Manning and Hinrich Schutze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press.
Gary Marchionini. 2006. Exploratory Search: from
Finding to Understanding. Communications of the
ACM, 49(1):41?46.
David Milne. 2007. Computing Semantic Relatedness
using Wikipedia Link Structure. In Proceedings of
the New Zealand Computer Science Research Stu-
dent Conference.
Patrick Ndjiki-Nya, Oleg Novychny, and Thomas Wie-
gand. 2004. Merging MPEG 7 Descriptors for Im-
age Content Analysis. In Proceedings of IEEE In-
ternational Conference on Acoustics, Speech, and
Signal Processing (ICASSP ?04), pages 5?8.
Siddhard Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2003. Using Measures of Semantic Re-
latedness for Word Sense Disambiguation. In Pro-
ceedings of the 4th International Conference on In-
telligent Text Processing and Computational Lin-
guistics, pages 241?257.
Mykola Pechenizkzy and Toon Calders. 2007. A
framework for guiding the museum tours personal-
ization. In Proceedings of the Workshop on Person-
alised Access to Cultural Heritage (PATCH ?07),
pages 11?28.
Koen E.A. Sande, Theo Gevers, and Cees G. M.
Snoek. 2008. Evaluation of Color Descriptors for
Object and Scene Recognition. In Proceedings of
the IEEE Computer Society Conference on Com-
puter Vision and Pattern Recognition (CVPR ?08),
pages 1?8.
92
Bernt Schiele and James L. Crowley. 1996. Object
recognition using multidimensional receptive field
histograms. In Proceedings of the 4th European
Conference on Computer Vision (ECCV ?96), pages
610?619, London, UK.
Nicu Sebe and Michael S. Lew. 2001. Color-based
Retrieval. Pattern Recognition Letters, 22:223?
230, February.
Rohini K. Srihari, Aibing Rao, Benjamin Han,
Srikanth Munirathnam, and Xiaoyun Wu. 2000. A
model for multimodal information retrieval. In Pro-
ceedings of the IEEE International Conference on
Multimedia and Expo (ICME ?00), pages 701?704.
Grant Strong and Minglun Gong. 2009. Organizing
and Browsing Photos using Different Feature Vec-
tors and their Evaluations. Proceedings of the ACM
International Conference on Image and Video Re-
trieval (CIVR ?09), pages 3:1?3:8.
Michael J. Swain and Dana H. Ballard. 1991. Color
indexing. International Journal of Computer Vi-
sion, 7:11?32.
Richard Szeliski. 2010. Computer Vision: Algorithms
and Applications. Springer-Verlag Inc. New York.
Xin-Jing Wang, Wei-Ying Ma, Gui-Rong Xue, and
Xing Li. 2004. Multi-model similarity propaga-
tion and its application for web image retrieval.
In Proceedings of the 12th annual ACM Interna-
tional Conference on Multimedia (MULTIMEDIA
?04), pages 944?951, New York, NY, USA.
Yiwen Wang, Natalia Stash, Lora Aroyo, Peter
Gorgels, Lloyd Rutledge, and Guus Schreiber.
2008. Recommendations based on semantically-
enriched museum collections. Journal of Web Se-
mantics: Science, Services and Agents on the World
Wide Web, 6(4):43?50.
Yiwen Wang, Lora Aroyo, Natalia Stash, Rody Sam-
beek, Schuurmans Yuri, Guus Schreiber, and Pe-
ter Gorgels. 2009. Cultivating personalized mu-
seum tours online and on-site. Journal of Interdis-
ciplinary Science Reviews, 34(2):141?156.
Thijs Westerveld. 2002. Probabilistic multimedia re-
trieval. In Proceedings of the 25th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR ?02),
pages 437?438, New York, NY, USA.
Hui Yu, Mingjing Li, Hong-Jiang Zhang, and Jufu
Feng. 2002. Color Texture Moments for Content-
based Image Retrieval. In Proceedings of the
IEEE International Conference on Image Process-
ing (ICIP ?02), pages 929?932.
Xiang Sean Zhou and Thomas S. Huang. 2002. Uni-
fying keywords and visual contents in image re-
trieval. IEEE Multimedia, 9(2):23 ?33.
93
