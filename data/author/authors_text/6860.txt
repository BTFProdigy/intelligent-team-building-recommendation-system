Statistical Modeling for Unit Selection in Speech Synthesis
Cyril Allauzen and Mehryar Mohri and Michael Riley?
AT&T Labs ? Research
180 Park Avenue, Florham Park, NJ 07932, USA
{allauzen, mohri, riley}@research.att.com
Abstract
Traditional concatenative speech synthesis systems
use a number of heuristics to define the target and
concatenation costs, essential for the design of the
unit selection component. In contrast to these ap-
proaches, we introduce a general statistical model-
ing framework for unit selection inspired by auto-
matic speech recognition. Given appropriate data,
techniques based on that framework can result in a
more accurate unit selection, thereby improving the
general quality of a speech synthesizer. They can
also lead to a more modular and a substantially more
efficient system.
We present a new unit selection system based on
statistical modeling. To overcome the original ab-
sence of data, we use an existing high-quality unit
selection system to generate a corpus of unit se-
quences. We show that the concatenation cost can
be accurately estimated from this corpus using a sta-
tistical n-gram language model over units. We used
weighted automata and transducers for the repre-
sentation of the components of the system and de-
signed a new and more efficient composition algo-
rithm making use of string potentials for their com-
bination. The resulting statistical unit selection is
shown to be about 2.6 times faster than the last re-
lease of the AT&T Natural Voices Product while
preserving the same quality, and offers much flex-
ibility for the use and integration of new and more
complex components.
1 Motivation
A concatenative speech synthesis system (Hunt and
Black, 1996; Beutnagel et al, 1999a) consists of
three components. The first component, the text-
analysis frontend, takes text as input and outputs
a sequence of feature vectors that characterize the
acoustic signal to synthesize. The first element of
each of these vectors is the predicted phone or half-
phone; other elements are features such as the pho-
netic context, acoustic features (e.g., pitch, dura-
tion), or prosodic features.
? This author?s new address is: Google, Inc, 1440 Broadway,
New York, NY 10018, riley@google.com.
The second component, unit selection, deter-
mines in a set of recorded acoustic units corre-
sponding to phones (Hunt and Black, 1996) or half-
phones (Beutnagel et al, 1999a) the sequence of
units that is the closest to the sequence of fea-
ture vectors predicted by the text analysis frontend.
The final component produces an acoustic signal
from the unit sequence chosen by unit selection
using simple concatenation or other methods such
as PSOLA (Moulines and Charpentier, 1990) and
HNM (Stylianou et al, 1997).
Unit selection is performed by defining two cost
functions: the target cost that estimates how the
features of a recorded unit match the specified fea-
ture vector and the concatenation cost that estimates
how well two units will be perceived to match when
appended. Unit selection then consists of finding,
given a specified sequence of feature vectors, the
unit sequence that minimizes the sum of these two
costs.
The target and concatenation cost functions have
traditionally been formed from a variety of heuris-
tic or ad hoc quality measures based on features of
the audio and text. In this paper, we follow a differ-
ent approach: our goal is a system based purely on
statistical modeling. The starting point is to assume
that we have a training corpus of utterances labeled
with the appropriate unit sequences. Specifically,
for each training utterance, we assume available a
sequence of feature vectors f = f1 . . . fn and the
corresponding units u = u1 . . . un that should be
used to synthesize this utterance. We wish to esti-
mate from this corpus two probability distributions,
P (f |u) and P (u). Given these estimates, we can
perform unit selection on a novel utterance using:
u = argmax
u
P (u|f) (1)
= argmin
u
(? logP (f |u) ? logP (u)) (2)
Equation 1 states that the most likely unit se-
quence is selected given the probabilistic model
used. Equation 2 follows from the definition of
conditional probability and that P (f) is fixed for a
given utterance. The two terms appearing in Equa-
tion 2 can be viewed as the statistical counterparts
of the target and concatenation costs in traditional
unit selection.
The statistical framework just outlined is simi-
lar to the one used in speech recognition (Jelinek,
1976). We also use several techniques that have
been very successfully applied to speech recogni-
tion. For instance, in this paper, we show how
? logP (u) (the concatenation cost) can be accu-
rately estimated using a statistical n-gram language
model over units. Two questions naturally arise.
(a) How can we collect a training corpus for build-
ing a statistical model? Ideally, the training cor-
pus could be human-labeled, as in speech recog-
nition and other natural language processing tasks.
But this seemed impractical given the size of the
unit inventory, the number of utterances needed for
good statistical estimates, and our limited resources.
Instead, we chose to use a training corpus gener-
ated by an existing high-quality unit selection sys-
tem, that of the AT&T Natural Voices Product. Of
course, building a statistical model on that output
can, at best, only match the quality of the origi-
nal. But, it can serve as an exploratory trial to mea-
sure the quality of our statistical modeling. As we
will see, it can also result in a synthesis system that
is significantly faster and modular than the original
since there are well-established algorithms for rep-
resenting and optimizing statistical models of the
type we will employ. To further simplify the prob-
lem, we will use the existing traditional target costs,
providing statistical estimates only of the concate-
nation costs (? logP (u)).
(b) What are the benefits of a statistical modeling
approach?
(1) High-quality cost functions. One issue
with traditional unit selection systems is that
their cost functions are the result of the following
compromise: they need to be complex enough
to have a perceptual meaning but simple enough
to be computed efficiently. With our statistical
modeling approach, the labeling phase could be
performed offline by a highly accurate unit selec-
tion system, potentially slow and complex, while
the run-time statistical system could still be fast.
Moreover, if we had audio available for our training
corpus, we could exploit that in the initial label-
ing phase for the design of the unit selection system.
(2) Weighted finite-state transducer representa-
tion. In addition to the already mentioned synthesis
speed and the opportunity of high-quality measures
in the initial offline labeling phase, another benefit
of this approach is that it leads to a natural represen-
tation by weighted transducers, and hence enables
us to build a unit selection system using general
and flexible representations and methods already in
use for speech recognition, e.g., those found in the
FSM (Mohri et al, 2000), GRM (Allauzen et al,
2004) and DCD (Allauzen et al, 2003) libraries.
Other unit selection systems based on weighted
transducers were also proposed in (Yi et al, 2000;
Bulyko and Ostendorf, 2001).
(3) Unit selection algorithms and speed-up. We
present a new unit selection system based on sta-
tistical modeling. We used weighted automata and
transducers for the representation of the compo-
nents of the system and designed a new and efficient
composition algorithm making use of string poten-
tials for their combination. The resulting statistical
unit selection is shown to be about 2.6 times faster
than the last release of the AT&T Natural Voices
Product while preserving the same quality, and of-
fers much flexibility for the use and integration of
new and more complex components.
2 Unit Selection Methods
2.1 Overview of a Traditional Unit Selection
System
This section describes in detail the cost functions
used in the AT&T Natural Voices Product that we
will use as the baseline in our experimental results,
see (Beutnagel et al, 1999a) for more details about
this system. In this system, unit selection is based
on (Hunt and Black, 1996) but using units corre-
sponding to halfphones instead of phones. Let U
be the set of recorded units. Two cost functions
are defined: the target cost Ct(fi, ui) is used to
estimate the mismatch between the features of the
feature vector fi and the unit ui; the concatena-
tion cost Cc(ui, uj) is used to estimate the smooth-
ness of the acoustic signal when concatenating the
units ui and uj . Given a sequence f = f1 . . . fn
of feature vectors, unit selection can then be formu-
lated as the problem of finding the sequence of units
u = u1 . . . un that minimizes these two costs:
u = argmin
u?Un
(
n?
i=1
Ct(fi, ui) +
n?
i=2
Cc(ui?1, ui))
In practice, not all unit sequences of a given length
are considered. A preselection method such as the
one proposed by (Conkie et al, 2000) is used. The
computation of the target cost can be split in two
parts: the context cost Cp that is the component of
the target cost corresponding to the phonetic con-
text, and the feature cost Cf that corresponds the
other components of the target cost:
Ct(fi, ui) = Cp(fi, ui) + Cf (fi, ui) (3)
For each phonetic context ? of length 5, a list L(?)
of the units that are the most frequently used in the
phonetic context ? is computed. For each feature
vector fi in f , the candidate units for fi are com-
puted in the following way. Let ?i be the 5-phone
context of fi in f . The context costs between fi and
all the units in the preselection list of the phonetic
context ?i are computed and the M units with the
best context cost are selected:
Ui = M-best
ui?L(?i)
(Cp(fi, ui))
The feature costs between fi and the units in Ui are
then computed and the N units with the best target
cost are selected:
U ?i = N-bestui?Ui
(Cp(fi, ui) + Cf (fi, ui))
The unit sequence u verifying:
u = argmin
u?U ?1???U ?n
(
n?
i=1
Ct(fi, ui) +
n?
i=2
Cc(ui?1, ui))
is determined using a classical Viterbi search. Thus,
for each position i, the N2 concatenation costs be-
tween the units in U ?i and U ?i+1 need to be com-
puted. The caching method for concatenation costs
proposed in (Beutnagel et al, 1999b) can be used to
improve the efficiency of the system.
2.2 Statistical Modeling Approach
Our statistical modeling approach was described
in Section 1. As already mentioned, our general
approach would consists of deriving both the tar-
get cost ? logP (f |u) and the concatenation cost
? logP (u) from appropriate training data using
general statistical methods. To simplify the prob-
lem, we will use the existing target cost provided by
the traditional unit selection system and concentrate
on the problem of estimating the concatenation cost.
We used the unit selection system presented in the
previous section to generate a large corpus of more
than 8M unit sequences, each unit corresponding to
a unique recorded halfphone. This corpus was used
to build an n-gram statistical language model us-
ing Katz backoff smoothing technique (Katz, 1987).
This model provides us with a new cost function, the
grammar cost Cg, defined by:
Cg(uk|u1...uk?1) = ? log(P (uk|u1...uk?1))
where P is the probability distribution estimated by
our model. We used this new cost function to re-
place both the concatenation and context costs used
in the traditional approach. Unit selection then con-
sists of finding the unit sequence u such that:
u = argmin
u?Un
n?
i=1
(Cf (fi, ui)+Cg(ui|ui?k . . . ui?1))
In this approach, rather than using a preselection
method such as that of (Conkie et al, 2000), we are
using the statistical language model to restrict the
candidate space (see Section 4.2).
3 Representation by Weighted Finite-State
Transducers
An important advantage of the statistical frame-
work we introduced for unit selection is that the re-
sulting components can be naturally represented by
weighted finite-state transducers. This casts unit se-
lection into a familiar schema, that of a Viterbi de-
coder applied to a weighted transducer.
3.1 Weighted Finite-State Transducers
We give a brief introduction to weighted finite-state
transducers. We refer the reader to (Mohri, 2004;
Mohri et al, 2000) for an extensive presentation of
these devices and will use the definitions and nota-
tion introduced by these authors.
A weighted finite-state transducer T is an 8-tuple
T = (?,?, Q, I, F,E, ?, ?) where ? is the finite
input alphabet of the transducer, ? is the finite out-
put alphabet, Q is a finite set of states, I ? Q the
set of initial states, F ? Q the set of final states,
E ? Q ? (? ? {}) ? (? ? {}) ? R ? Q a fi-
nite set of transitions, ? : I ? R the initial weight
function, and ? : F ? R the final weight function
mapping F to R. In our statistical framework, the
weights can be interpreted as log-likelihoods, thus
there are added along a path. Since we use the stan-
dard Viterbi approximation, the weight associated
by T to a pair of strings (x, y) ? ?? ? ?? is given
by:
[[T ]](x, y) = min
pi?R(I,x,y,F )
?[p[pi]] + w[pi] + ?[n[pi]]
where R(I, x, y, F ) denotes the set of paths from an
initial state p ? I to a final state q ? F with input
label x and output label y, w[pi] the weight of the
path pi, ?[p[pi]] the initial weight of the origin state
of pi, and ?[n[pi]] the final weight of its destination.
A Weighted automaton A = (?, Q, I, F,E, ?, ?)
is defined in a similar way by simply omitting the
output (or input) labels. We denote by ?2(T ) the
0 1a 2b 3c 4d
(a)
0
1a:x
5
a:u
2b:y
6b:v
3c:z 4d:t
7c:w 8a:s
(b)
0
1a:x
2
a:u
3b:y
4b:v
5c:z
6c:w
7d:t
(c)
Figure 1: (a) Weighted automaton T1. (b) Weighted
transducer T2. (c) T1 ? T2, the result of the compo-
sition of T1 and T2.
weighted automaton obtained from T by removing
its input labels.
A general composition operation similar to
the composition of relations can be defined for
weighted finite-state transducers (Eilenberg, 1974;
Berstel, 1979; Salomaa and Soittola, 1978; Kuich
and Salomaa, 1986). The composition of two trans-
ducers T1 and T2 is a weighted transducer denoted
by T1 ? T2 and defined by:
[[T1 ? T2]](x, y) = min
z???
{[[T1]](x, z) + [[T2]](z, y)}
There exists a simple algorithm for constructing
T = T1 ? T2 from T1 and T2 (Pereira and Riley,
1997; Mohri et al, 1996). The states of T are iden-
tified as pairs of a state of T1 and a state of T2. A
state (q1, q2) in T1?T2 is an initial (final) state if and
only if q1 is an initial (resp. final) state of T1 and q2
is an initial (resp. final) state of T2. The transitions
of T are the result of matching a transition of T1
and a transition of T2 as follows: (q1, a, b, w1, q?1)
and (q2, b, c, w2, q?2) produce the transition
((q1, q2), a, c, w1 + w2, (q?1, q?2)) (4)
in T . The efficiency of this algorithm was critical to
that of our unit selection system. Thus, we designed
an improved composition that we will describe later.
Figure 1(c) gives the resulting of the composition of
the weighted transducers given figure 2(a) and (b).
3.2 Language Model Weighted Transducer
The n-gram statistical language model we construct
for unit sequences can be represented by a weighted
automaton G which assigns to each sequence u its
log-likelihood:
[[G]](u) = ? log(P (u)). (5)
according to our probability estimate P . Since
a unit sequence u uniquely determines the corre-
sponding halfphone sequence x, the n-gram statis-
tical model equivalently defines a model of the joint
distribution of P (x, u). G can be augmented to
define a weighted transducer G? assigning to pairs
(x, u) their log-likelihoods. For any halfphone se-
quence x and unit sequence u, we define G? by:
[[G?]](x, u) = ? logP (u) (6)
The weighted transducer G? can be used to generate
all the unit sequences corresponding to a specific
halfphone sequence given by a finite automaton p,
using composition: p ? G?. In our case, we also wish
to use the language model transducer G? to limit the
number of candidate unit sequences considered. We
will do that by giving a strong precedence to n-
grams of units that occurred in the training corpus
(see Section 4.2).
Example Figure 2(a) shows the bigram model G
estimated from the following corpus:
<s> u1 u2 u1 u2 </s>
<s> u1 u3 </s>
<s> u1 u3 u1 u2 </s>
where ?s? and ?/s? are the symbols marking the
start and the end of an utterance. When the unit u1
is associated to the halfphone p1 and both units u1
and u2 are associated to the halfphone p2, the corre-
sponding weighted halfphone-to-unit transducer G?
is the one shown in Figure 2(b).
3.3 Unit Selection with Weighted Finite-State
Transducers
From each sequence f = f1 . . . fn of feature vec-
tors specified by the text analysis frontend, we can
straightforwardly derive the halfphone sequence to
be synthesized and represent it by a finite automa-
ton p, since the first component of each feature vec-
tor fi is the corresponding halfphone. Let W be the
weighted automaton obtained by composition of p
with G? and projection on the output:
W = ?2(p ? G?) (7)
W represents the set of candidate unit sequences
with their respective grammar costs. We can then
use a speech recognition decoder to search for the
best sequence u since W can be thought of as the
</s>
u3
</s>/0.703
.
?/3.647 u1
u1/0.703
</s>/1.466
u3/1.871
u1/0.955
u2
u2/1.466
u3/0.921
?/5.034
u2/0.514
</s>/0.410
?/4.053
u1/1.108
<s>
?/5.216
u1/0.003
</s>
u3
?:</s>/0.703
.
?:?/3.647 u1
p1:u1/0.703
?:</s>/1.466
p2:u3/1.871
p1:u1/0.955
u2
p2:u2/1.466
p2:u3/0.921
?:?/5.034
p2:u2/0.514
?:</s>/0.410
?:?/4.053
p1:u1/1.108
<s>
?:?/5.216
p1:u1/0.003
(a) (b)
Figure 2: (a) n-gram language model G for unit sequences. (b) Corresponding halfphone-to-unit weighted
transducer G?.
counterpart of a speech recognition transducer, f
the equivalent of the acoustic features and Cf the
analogue of the acoustic cost. Our decoder uses a
standard beam search of W to determine the best
path by computing on-the-fly the feature cost be-
tween each unit and its corresponding feature vec-
tor.
Composition constitutes the most costly opera-
tion in this framework. Section 4 presents several
of the techniques that we used to speed up that al-
gorithm in the context of unit selection.
4 Algorithms
4.1 Composition with String Potentials
In general, composition may create non-
coaccessible states, i.e., states that do not admit a
path to a final state. These states can be removed
after composition using a standard connection (or
trimming) algorithm that removes unnecessary
states. However, our purpose here is to avoid the
creation of such states to save computational time.
To that end, we introduce the notion of string
potential at each state.
Let i[pi] (o[pi]) be the input (resp. output) label of
a path pi, and denote by x ? y the longest common
prefix of two strings x and y. Let q be a state in a
weighted transducer. The input (output) string po-
tential of q is defined as the longest common prefix
of the input (resp. output) labels of all the paths in
T from q to a final state:
pi(q) =
?
pi??(q,F )
i[pi]
po(q) =
?
pi??(q,F )
o[pi]
The string potentials of the states of T can be com-
puted using the generic shortest-distance algorithm
of (Mohri, 2002) over the string semiring. They can
be used in composition in the following way. We
will say that two strings x and y are comparable if
x is a prefix of y or y is a prefix of x.
Let (q1, q2) be a state in T = T1 ? T2. Note
that (q1, q2) is a coaccessible state only if the out-
put string potential of q1 in T1 and the input string
potential of q2 in T2 are comparable, i.e., po(q1) is
a prefix of pi(q2) or pi(q2) is a prefix of po(q1).
Hence, composition can be modified to create only
those states for which the string potentials are com-
patible.
As an example, state 2 = (1, 5) of the transducer
T = T1 ? T2 in Figure 1 needs not be created since
po(1) = bcd and pi(5) = bca are not comparable
strings.
The notion of string potentials can be extended
to further reduce the number of non-coaccessible
states created by composition. The extended input
string potential of q in T , is denoted by p?i(q) and is
the set of strings defined by:
p?i(q) = pi(q) ? ?i(q) (8)
where ?i(q) ? ? and is such that for every ? ?
?i(q), there exist a path pi from q to a final state such
that pi(q)? is a prefix of the input label of pi. The ex-
tended output string potential of q, p?o(q), is defined
similarly. A state (q1, q2) in T1 ? T2 is coaccessible
only if
(p?o(q1) ? ??) ? (p?i(q2) ? ??) 6= ? (9)
Using string potentials helped us substantially im-
prove the efficiency of composition in unit selection.
4.2 Language Model Transducer ? Backoff
As mentioned before, the transducer G? represents
an n-gram backoff model for the joint probability
distribution P (x, u). Thus, backoff transitions are
used in a standard fashion when G? is viewed as an
automaton over paired sequences (x, u). Since we
use G? as a transducer mapping halfphone sequences
to unit sequences to determine the most likely unit
sequence u given a halfphone sequence x 1we need
to clarify the use of the backoff transitions in the
composition p ? G?.
Denote by O(V ) the set of output labels of a set
of transitions V . Then, the correct use derived from
the definition of the backoff transitions in the joint
model is as follows. At a given state s of G? and for
a given input halfphone a, the outgoing transitions
with input a are the transitions V of s with input
label a, and for each b 6? O(V ), the transition of the
first backoff state of s with input label a and output
b.
For the purpose of our unit selection system, we
had to resort to an approximation. This is because in
general, the backoff use just outlined leads to exam-
ining, for a given halfphone, the set of all units pos-
sible at each state, which is typically quite large.2
Instead, we restricted the inspection of the backoff
states in the following way within the composition
p ? G?. A state s1 in p corresponds in the composed
transducer p ? G? to a set of states (s1, s2), s2 ? S2,
where S2 is a subset of the states of G?. When
computing the outgoing transitions of the states in
(s1, s2) with input label a, the backoff transitions of
a state s2 are inspected if and only if none of the
states in S2 has an outgoing transition with input la-
bel a.
1This corresponds to the conditional probability P (u|x) =
P (x, u)/P (x).
2Note that more generally the vocabulary size of our statis-
tical language models, about 400,000, is quite large compared
to the usual word-based models.
4.3 Language Model Transducer ? Shrinking
A classical algorithm for reducing the size of an
n-gram language model is shrinking using the
entropy-based method of (Stolcke, 1998) or the
weighted difference method (Seymore and Rosen-
feld, 1996), both quite similar in practice. In our
experiments, we used a modified version of the
weighted difference method. Let w be a unit and
let h be its conditioning history within the n-gram
model. For a given shrink factor ?, the transition
corresponding to the n-gram hw is removed from
the weighted automaton if:
log(P? (w|h)) ? log(?hP? (w|h?)) ?
?
c(hw) (10)
where h? is the backoff sequence associated with h.
Thus, a higher-order n-gram hw is pruned when
it does not provide a probability estimate signifi-
cantly different from the corresponding lower-order
n-gram sequence h?w.
This standard shrinking method needs to be mod-
ified to be used in the case of our halfphone-to-unit
weighted transducer model with the restriction on
the traversal of the backoff transitions described in
the previous section. The shrinking methods must
take into account all the transitions sharing the same
input label at the state identified with h and its back-
off state h?. Thus, at each state identified with h in
G?, a transition with input label x is pruned when the
following condition holds:
?
w?Xxh
log(P? (w|h)) ?
?
w?Xxh?
log(?hP? (w|h?)) ?
?
c(hw)
where h? is the backoff sequence associate with h
and Xxk is the set of output labels of all the outgoing
transitions with input label x of the state identified
with k.
5 Experimental results
We used the AT&T Natural Voices Product speech
synthesis system to synthesize 107,987 AP news ar-
ticles, generating a large corpus of 8,731,662 unit
sequences representing a total of 415,227,388 units.
We used this corpus to build several n-gram Katz
backoff language models with n = 2 or 3. Ta-
ble 1 gives the size of the resulting language model
weighted automata. These language models were
built using the GRM Library (Allauzen et al, 2004).
We evaluated these models by using them to syn-
thesize an AP news article of 1,000 words, corre-
sponding to 8250 units or 6 minutes of synthesized
speech. Table 2 gives the unit selection time (in sec-
onds) taken by our new system to synthesize this AP
Model No. of states No. of transitions
2-gram, unshrunken 293,935 5,003,336
3-gram, unshrunken 4,709,404 19,027,244
3-gram, ? = ?4 2,967,472 14,223,284
3-gram, ? = ?1 2,060,031 12,133,965
3-gram, ? = 0 1,681,233 10,217,164
3-gram, ? = 1 1,370,220 9,146,797
3-gram, ? = 4 934,914 7,844,250
Table 1: Size of the stochastic language models for
different n-gram order and shrinking factor.
Model composition search total time
baseline system - - 4.5s
2-gram, unshrunken 2.9s 1.0s 3.9s
3-gram, unshrunken 1.2s 0.5s 1.7s
3-gram, ? = ?4 1.3s 0.5s 1.8s
3-gram, ? = ?1 1.5s 0.5s 2.0s
3-gram, ? = 0 1.7s 0.5s 2.2s
3-gram, ? = 1 2.1s 0.6s 2.7s
3-gram, ? = 4 2.7s 0.9s 3.6s
Table 2: Computation time for each unit selection
system when used to synthesize the same AP news
article.
news article. Experiments were run on a 1GHz Pen-
tium III processor with 256KB of cache and 2GB of
memory. The baseline system mentioned in this ta-
ble is the AT&T Natural Voices Product which was
also used to generate our training corpus using the
concatenation cost caching method from (Beutnagel
et al, 1999b). For the new system, both the compu-
tation times due to composition and to the search
are displayed. Note that the AT&T Natural Voices
Product system was highly optimized for speed. In
our new systems, the standard research software li-
braries already mentioned were used. The search
was performed using the standard speech recog-
nition Viterbi decoder from the DCD library (Al-
lauzen et al, 2003). With a trigram language model,
our new statistical unit selection system was about
2.6 times faster than the baseline system.
A formal test using the standard mean of opinion
score (MOS) was used to compare the quality of the
high-quality AT&T Natural Voices Product synthe-
sizer and that of the synthesizers based on our new
unit selection system with shrunken and unshrunken
trigram language models. In such tests, several lis-
teners are asked to rank the quality of each utterance
from 1 (worst score) to 5 (best). The MOS results of
the three systems with 60 utterances tested by 21 lis-
teners are reported in Table 3 with their correspond-
Model raw score normalized score
baseline system 3.54? .20 3.09? .22
3-gram, unshrunken 3.45? .20 2.98? .21
3-gram, ? = ?1 3.40? .20 2.93? .22
Table 3: Quality testing results: we report for each
system, the mean and standard error of the raw and
the listener-normalized scores.
ing standard error. The difference of scores between
the three systems is not statistically significant (first
column), in particular, the absolute difference be-
tween the two best systems is less than .1.
Different listeners may rank utterances in dif-
ferent ways. Some may choose the full range of
scores (1?5) to rank each utterance, others may se-
lect a smaller range near 5, near 3, or some other
range. To factor out such possible discrepancies in
ranking, we also computed the listener-normalized
scores (second column of the table). This was done
for each listener by removing the average score over
the full set of utterances, dividing it by the stan-
dard deviation, and by centering it around 3. The
results show that the difference between the normal-
ized scores of the three systems is not significantly
different. Thus, the MOS results show that the three
systems have the same quality.
We also measured the similarity of the two best
systems by comparing the number of common units
they produce for each utterance. On the AP news ar-
ticle already mentioned, more than 75% of the units
were common.
6 Conclusion
We introduced a statistical modeling approach to
unit selection in speech synthesis. This approach is
likely to lead to more accurate unit selection sys-
tems based on principled learning algorithms and
techniques that radically depart from the heuristic
methods used in the traditional systems. Our pre-
liminary experiments using a training corpus gener-
ated by the AT&T Natural Voices Product demon-
strates that statistical modeling techniques can be
used to build a high-quality unit selection system.
It also shows other important benefits of this ap-
proach: a substantial increase of efficiency and a
greater modularity and flexibility.
Acknowledgments
We thank Mark Beutnagel for helping us clarify
some of the details of the unit selection system in
the AT&T Natural Voices Product speech synthe-
sizer. Mark also generated the training corpora and
set up the listening test used in our experiments.
We also acknowledge discussions with Brian Roark
about various statistical language modeling topics
in the context of unit selection.
References
Cyril Allauzen, Mehryar Mohri, and Michael
Riley. 2003. DCD Library - Decoder Li-
brary, software collection for decoding and re-
lated functions. In AT&T Labs - Research.
http://www.research.att.com/sw/tools/dcd.
Cyril Allauzen, Mehryar Mohri, and Brian
Roark. 2004. A General Weighted Gram-
mar Library. In Proceedings of the Ninth
International Conference on Automata (CIAA
2004), Kingston, Ontario, Canada, July.
http://www.research.att.com/sw/tools/grm.
Jean Berstel. 1979. Transductions and Context-
Free Languages. Teubner Studienbucher:
Stuttgart.
Mark Beutnagel, Alistair Conkie, Juergen
Schroeter, and Yannis Stylianou. 1999a.
The AT&T Next-Gen system. In Proceedings of
the Joint Meeting of ASA, EAA and DAGA, pages
18?24, Berlin, Germany.
Mark Beutnagel, Mehryar Mohri, and Michael Ri-
ley. 1999b. Rapid unit selection from a large
speech corpus for concatenative speech synthesis.
In Proceedings of Eurospeech, volume 2, pages
607?610.
Ivan Bulyko and Mari Ostendorf. 2001. Unit selec-
tion for speech synthesis using splicing costs with
weighted finite-state trasnducers. In Proceedings
of Eurospeech, volume 2, pages 987?990.
Alistair Conkie, Mark Beutnagel, Ann Syrdal, and
Philip Brown. 2000. Preselection of candidate
units in a unit selection-based text-to-speech syn-
thesis system. In Proceedings of ICSLP, vol-
ume 3, pages 314?317.
Samuel Eilenberg. 1974. Automata, Languages
and Machines, volume A. Academic Press.
Andrew Hunt and Alan Black. 1996. Unit selec-
tion in a concatenative speech synthesis system.
In Proceedings of ICASSP?96, volume 1, pages
373?376, Atlanta, GA.
Frederick Jelinek. 1976. Continuous speech recog-
nition by statistical methods. IEEE Proceedings,
64(4):532?556.
Slava M. Katz. 1987. Estimation of probabilities
from sparse data for the language model com-
ponent of a speech recogniser. IEEE Transac-
tions on Acoustic, Speech, and Signal Processing,
35(3):400?401.
Werner Kuich and Arto Salomaa. 1986. Semir-
ings, Automata, Languages. Number 5 in EATCS
Monographs on Theoretical Computer Science.
Springer-Verlag, Berlin, Germany.
Mehryar Mohri, Fernando C. N. Pereira, and
Michael Riley. 1996. Weighted automata in text
and speech processing. In Proceedings of the
12th European Conference on Artificial Intelli-
gence (ECAI 1996), Workshop on Extended fi-
nite state models of language, Budapest, Hun-
gary. John Wiley and Sons, Chichester.
Mehryar Mohri, Fernando C. N. Pereira, and
Michael Riley. 2000. The Design Principles
of a Weighted Finite-State Transducer Library.
Theoretical Computer Science, 231(1):17?32.
http://www.research.att.com/sw/tools/fsm.
Mehryar Mohri. 2002. Semiring Frameworks
and Algorithms for Shortest-Distance Problems.
Journal of Automata, Languages and Combina-
torics, 7(3):321?350.
Mehryar Mohri. 2004. Weighted Finite-State
Transducer Algorithms: An Overview. In Car-
los Mart??n-Vide, Victor Mitrana, and Gheorghe
Paun, editors, Formal Languages and Applica-
tions, volume 148, VIII, 620 p. Springer, Berlin.
Eric Moulines and Francis Charpentier. 1990.
Pitch-synchronous waveform processing tech-
niques for text-to-speech synthesis using di-
phones. Speech Communication, 9(5-6):453?
467.
Fernando C. N. Pereira and Michael D. Riley. 1997.
Speech Recognition by Composition of Weighted
Finite Automata. In Finite-State Language Pro-
cessing, pages 431?453. MIT Press.
Arto Salomaa and Matti Soittola. 1978. Automata-
Theoretic Aspects of Formal Power Series.
Springer-Verlag: New York.
Kristie Seymore and Ronald Rosenfeld. 1996.
Scalable backoff language models. In Pro-
ceedings of ICSLP, volume 1, pages 232?235,
Philadelphia, Pennsylvania.
Andreas Stolcke. 1998. Entropy-based pruning
of backoff language models. In Proc. DARPA
Broadcast News Transcription and Understand-
ing Workshop, pages 270?274.
Yannis Stylianou, Thierry Dutoit, and Juergen
Schroeter. 1997. Diphone conactenation using
a harmonic plus noise model of speech. In Pro-
ceedings of Eurospeech.
Jon Yi, James Glass, and Lee Hetherington. 2000.
A flexible scalable finite-state transducer archi-
tecture for corpus-based concatenative speech
synthesis. In Proceedings of ICSLP, volume 3,
pages 322?325.
Proceedings of NAACL HLT 2009: Tutorials, pages 9?10,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
OpenFst: An Open-Source, Weighted Finite-State Transducer Library
and its Applications to Speech and Language
Michael Riley, Cyril Allauzen, and Martin Jansche, Google Inc.
Finite-state methods are well established in language and speech processing. OpenFst (available from
www.openfst.org) is a free and open-source software library for building and using finite automata, in
particular, weighted finite-state transducers (FSTs). This tutorial is an introduction to weighted finite-
state transducers and their uses in speech and language processing. While there are other weighted
finite-state transducer libraries, OpenFst (a) offers, we believe, the most comprehensive, general and
efficient set of operations; (b) makes available full source code; (c) exposes high- and low-level C++
APIs that make it easy to embed and extend; and (d) is a platform for active research and use among
many colleagues.
1 Tutorial Outline
1. Introduction to OpenFst
The first part of the tutorial introduces operations on weighted automata such as determiniza-
tion and intersection/composition as well as the corresponding OpenFst binaries and library-level
APIs that implement those operations. We describe how to read FSTs from simple textual de-
scriptions and combine them into larger and more complex machines, and optimize them using
simple command-line and library calls.
? Introduction
? Motivating examples
? Finite-state methods in NLP and Speech
? A quick tour of OpenFst
? Finite-state machines and operations
? OpenFst binaries
? High-level C++ API
? Human-readable file formats
? Comparison of OpenFst and competing libraries
? Comparison with the AT&T FSM LibraryTM
? Brief comparison with SFST and related libraries
? Advanced usage of OpenFst
? Low-level C++ API
? Constructing and modifying Fst objects programmatically
? Implementing new concrete Fst classes
? Adding new weight semirings
? Customizing matchers and filters for composition
9
2. Applications
The second part of the tutorial focuses on several application areas of interest to the NAACL HLT
audience, including speech recognition, speech synthesis, and general text processing. In each
application area we discuss a straightforward example in detail, then delve into an advanced ex-
ample that highlights important features of OpenFst, common pitfalls, efficiency considerations,
new research directions, etc. These examples are drawn from our extensive experience in applying
OpenFst to problems in speech and language processing.
? Automatic speech recognition
? Context dependency transducers
? Language models and grammars
? Natural Language Processing
? Unicode processing
? Text analysis, text normalization
? Pronunciation models
? Other areas
? Computational biology
? Pattern matching
2 Target Audience
This tutorial is intended for students, researchers, and practitioners interested in applying finite-state
methods. It is suitable for participants of a variety of backgrounds, including those without any back-
ground in finite-state techniques as well as advanced users of existing software packages. For those
unfamiliar with finite-state transducers, the first portion of the tutorial provides an introduction to the
theory and application areas. Users of other finite-state libraries will particularly benefit from the con-
trastive description of the unique features of OpenFst, its C++ API, and the examples drawn from
real-world applications.
3 Presenters
Michael Riley began his career at Bell Labs and AT&T Labs where he, together with Mehryar Mohri and
Fernando Pereira, introduced and developed the theory and use of weighted finite-state transducers (WF-
STs) in speech and language. This work was recognized in best paper awards from the journals Speech
Communication and Computer Speech and Language. He has been a research scientist at Google, Inc.
since 2003. He is a principal author of the OpenFst library and the AT&T FSM LibraryTM. He has given
several tutorials on WFSTs before: at ACL 1994, Coling 1997 and Interspeech 2002.
Cyril Allauzen is another key author of the OpenFst library. His main research interests are in finite-state
methods and their applications to text, speech and natural language processing and machine learning.
Before joining Google, he worked as a researcher at AT&T Labs ? Research and at NYU?s Courant
Institute of Mathematical Sciences.
Martin Jansche has applied the OpenFst library to several speech and language problems at Google. His
FST-related interests are in text processing for speech tasks and in learning and applying pronunciation
and transliteration models.
10
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1373?1383,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Hierarchical Phrase-Based Translation Representations
Gonzalo Iglesias? Cyril Allauzen? William Byrne?
Adria` de Gispert? Michael Riley?
?Department of Engineering, University of Cambridge, Cambridge, CB2 1PZ, U.K.
{gi212,wjb31,ad465}@eng.cam.ac.uk
? Google Research, 76 Ninth Avenue, New York, NY 10011
{allauzen,riley}@google.com
Abstract
This paper compares several translation rep-
resentations for a synchronous context-free
grammar parse including CFGs/hypergraphs,
finite-state automata (FSA), and pushdown
automata (PDA). The representation choice is
shown to determine the form and complex-
ity of target LM intersection and shortest-path
algorithms that follow. Intersection, shortest
path, FSA expansion and RTN replacement al-
gorithms are presented for PDAs. Chinese-to-
English translation experiments using HiFST
and HiPDT, FSA and PDA-based decoders,
are presented using admissible (or exact)
search, possible for HiFST with compact
SCFG rulesets and HiPDT with compact LMs.
For large rulesets with large LMs, we intro-
duce a two-pass search strategy which we then
analyze in terms of search errors and transla-
tion performance.
1 Introduction
Hierarchical phrase-based translation, using a syn-
chronous context-free translation grammar (SCFG)
together with an n-gram target language model
(LM), is a popular approach in machine transla-
tion (Chiang, 2007). Given a SCFG G and an n-
gram language model M , this paper focuses on how
to decode with them, i.e. how to apply them to the
source text to generate a target translation. Decod-
ing has three basic steps, which we first describe
in terms of the formal languages and relations in-
volved, with data representations and algorithms to
follow.
1. Translating the source sentence s with G
to give target translations: T = {s} ? G,
a (weighted) context-free language resulting
from the composition of a finite language and
the algebraic relation G for SCFG G.
2. Applying the language model to these target
translations: L=T ?M, a (weighted) context-
free language resulting from the intersection
of a context-free language and the regular lan-
guage M for M .
3. Searching for the translation and language
model combination with the highest-probablity
path: L?=argmaxl?LL
Of course, decoding requires explicit data represen-
tations and algorithms for combining and searching
them. In common to the approaches we will con-
sider here, s is applied to G by using the CYK algo-
rithm in Step 1 and M is represented by a finite au-
tomaton in Step 2. The choice of the representation
of T in many ways determines the remaining de-
coder representations and algorithms needed. Since
{s} is a finite language and we assume through-
out that G does not allow unbounded insertions,
T and L are, in fact, regular languages. As such,
T and L have finite automaton representations Tf
and Lf . In this case, weighted finite-state intersec-
tion and single-source shortest path algorithms (us-
ing negative log probabilities) can be used to solve
Steps 2 and 3 (Mohri, 2009). This is the approach
taken in (Iglesias et al, 2009a; de Gispert et al,
2010). Instead T and L can be represented by hy-
pergraphs Th and Lh (or very similarly context-free
rules, and-or trees, or deductive systems). In this
case, hypergraph intersection with a finite automa-
ton and hypergraph shortest path algorithms can be
used to solve Steps 2 and 3 (Huang, 2008). This
is the approach taken by Chiang (2007). In this
paper, we will consider another representation for
context-free languages T and L as well, pushdown
automata (PDA) Tp and Lp, familiar from formal
1373
language theory (Aho and Ullman, 1972). We will
describe PDA intersection with a finite automaton
and PDA shortest-path algorithms in Section 2 that
can be used to solve Steps 2 and 3. It cannot be
over-emphasized that the CFG, hypergraph and PDA
representations of T are used for their compactness
rather than for expressing non-regular languages.
As presented so far, the search performed in Step
3 is admissible (or exact) ? the true shortest path
is found. However, the search space in MT can be
quite large. Many systems employ aggressive prun-
ing during the shortest-path computation with little
theoretical or empirical guarantees of correctness.
Further, such pruning can greatly complicate any
complexity analysis of the underlying representa-
tions and algorithms. In this paper, we will exclude
any inadmissible pruning in the shortest-path algo-
rithm itself. This allows us in Section 3 to compare
the computational complexity of using these differ-
ent representations. We show that the PDA represen-
tation is particularly suited for decoding with large
SCFGs and compact LMs.
We present Chinese-English translation results
under the FSA and PDA translation representations.
We describe a two-pass translation strategy which
we have developed to allow use of the PDA repre-
sentation in large-scale translation. In the first pass,
translation is done using a lattice-generating version
of the shortest path algorithm. The full translation
grammar is used but with a compact, entropy-pruned
version (Stolcke, 1998) of the full language model.
This first-step uses admissible pruning and lattice
generation under the compact language model. In
the second pass, the original, unpruned LM is simply
applied to the lattices produced in the first pass. We
find that entropy-pruning and first-pass translation
can be done so as to introduce very few search errors
in the overall process; we can identify search errors
in this experiment by comparison to exact transla-
tion under the full translation grammar and language
model using the FSA representation. We then inves-
tigate a translation grammar which is large enough
that exact translation under the FSA representation
is not possible. We find that translation is possible
using the two-pass strategy with the PDA translation
representation and that gains in BLEU score result
from using the larger translation grammar.
1.1 Related Work
There is extensive prior work on computational ef-
ficiency and algorithmic complexity in hierarchical
phrase-based translation. The challenge is to find al-
gorithms that can be made to work with large trans-
lation grammars and large language models.
Following the original algorithms and analysis of
Chiang (2007), Huang and Chiang (2007) devel-
oped the cube-growing algorithm, and more recently
Huang and Mi (2010) developed an incremental de-
coding approach that exploits left-to-right nature of
the language models.
Search errors in hierarchical translation, and in
translation more generally, have not been as exten-
sively studied; this is undoubtedly due to the diffi-
culties inherent in finding exact translations for use
in comparison. Using a relatively simple phrase-
based translation grammar, Iglesias et al (2009b)
compared search via cube-pruning to an exact FST
implementation (Kumar et al, 2006) and found that
cube-pruning suffered significant search errors. For
Hiero translation, an extensive comparison of search
errors between the cube pruning and FSA imple-
mentation was presented by Iglesias et al (2009a)
and de Gispert et al (2010). Relaxation techniques
have also recently been shown to finding exact so-
lutions in parsing (Koo et al, 2010) and in SMT
with tree-to-string translation grammars and trigram
language models (Rush and Collins, 2011), much
smaller models compared to the work presented in
this paper.
Although entropy-pruned language models have
been used to produce real-time translation sys-
tems (Prasad et al, 2007), we believe our use of
entropy-pruned language models in two-pass trans-
lation to be novel. This is an approach that is widely-
used in automatic speech recognition (Ljolje et al,
1999) and we note that it relies on efficient represen-
tation of very large search spaces T for subsequent
rescoring, as is possible with FSAs and PDAs.
2 Pushdown Automata
In this section, we formally define pushdown au-
tomata and give intersection, shortest-path and re-
lated algorithms that will be needed later.
Informally, pushdown automata are finite au-
tomata that have been augmented with a stack. Typ-
1374
0
1a
2
?
(
3)b
0
1
a
2
?
(
?
3
)
?
b
(a) (b)
0
1
(
3
?
2
a
4(
)
5
b
)
0,?
1,(
?
3,?
?
2,(a
4,(?
?
5,(
b
?
(c) (d)
Figure 1: PDA Examples: (a) Non-regular PDA accept-
ing {anbn|n ? N}. (b) Regular (but not bounded-stack)
PDA accepting a?b?. (c) Bounded-stack PDA accepting
a?b? and (d) its expansion as an FSA.
ically this is done by adding a stack alphabet and la-
beling each transition with a stack operation (a stack
symbol to be pushed onto, popped or read from the
stack) in additon to the usual input label (Aho and
Ullman, 1972; Berstel, 1979) and weight (Kuich
and Salomaa, 1986; Petre and Salomaa, 2009). Our
equivalent representation allows a transition to be la-
beled by a stack operation or a regular input symbol
but not both. Stack operations are represented by
pairs of open and close parentheses (pushing a sym-
bol on and popping it from the stack). The advantage
of this representation is that is identical to the finite
automaton representation except that certain sym-
bols (the parentheses) have special semantics. As
such, several finite-state algorithms either immedi-
ately generalize to this PDA representation or do so
with minimal changes. The algorithms described in
this section have been implemented in the PDT ex-
tension (Allauzen and Riley, 2011) of the OpenFst
library (Allauzen et al, 2007).
2.1 Definitions
A (restricted) Dyck language consist of ?well-
formed? or ?balanced? strings over a finite num-
ber of pairs of parentheses. Thus the string
( [ ( ) ( ) ] { } [ ] ) ( ) is in the Dyck language over 3
pairs of parentheses.
More formally, let A and A be two finite alpha-
bets such that there exists a bijection f from A to
A. Intuitively, f maps an open parenthesis to its cor-
responding close parenthesis. Let a? denote f(a) if
a ? A and f?1(a) if a ? A. The Dyck language
DA over the alphabet A? = A ? A is then the lan-
guage defined by the following context-free gram-
mar: S ? , S ? SS and S ? aSa? for all a?A.
We define the mapping cA : A?? ? A?? as follow.
cA(x) is the string obtained by iteratively deleting
from x all factors of the form aa? with a ? A. Ob-
serve that DA=c?1A ().
Let A and B be two finite alphabets such that
B ? A, we define the mapping rB : A? ? B?
by rB(x1 . . . xn) = y1 . . . yn with yi = xi if xi ?B
and yi= otherwise.
A weighted pushdown automaton (PDA) T over
the tropical semiring (R ? {?},min,+,?, 0) is
a 9-tuple (?,?,?, Q,E, I, F, ?) where ? is the fi-
nite input alphabet, ? and ? are the finite open and
close parenthesis alphabets, Q is a finite set of states,
I ?Q the initial state, F ? Q the set of final states,
E ? Q ? (? ? ?? ? {}) ? (R ? {?}) ? Q a fi-
nite set of transitions, and ? : F ? R ? {?} the
final weight function. Let e= (p[e], i[e], w[e], n[e])
denote a transition in E.
A path pi is a sequence of transitions pi=e1 . . . en
such that n[ei]=p[ei+1] for 1 ? i < n. We then de-
fine p[pi] = p[e1], n[pi] = n[en], i[pi] = i[e1] ? ? ? i[en],
and w[pi]=w[e1] + . . . + w[en].
A path pi is accepting if p[pi] = I and n[pi] ? F .
A path pi is balanced if r??(i[pi]) ?D?. A balanced
path pi accepts the string x ? ?? if it is a balanced
accepting path such that r?(i[pi])=x.
The weight associated by T to a string x ? ??
is T (x) = minpi?P (x) w[pi] + ?(n[pi]) where P (x)
denotes the set of balanced paths accepting x. A
weighted language is recognizable by a weighted
pushdown automaton iff it is context-free. We de-
fine the size of T as |T |= |Q|+|E|.
A PDA T has a bounded stack if there exists K ?
N such that for any sub-path pi of any balanced path
in T : |c?(r??(i[pi]))| ? K . If T has a bounded stack,
then it represents a regular language. Figure 1 shows
non-regular, regular and bounded-stack PDAs.
A weighted finite automaton (FSA) can be viewed
as a PDA where the open and close parentheses al-
phabets are empty, see (Mohri, 2009) for a stand-
alone definition.
1375
2.2 Expansion Algorithm
Given a bounded-stack PDA T , the expansion of T
is the FSA T ? equivalent to T defined as follows.
A state in T ? is a pair (q, z) where q is a state in T
and z ???. A transition (q, a, w, q?) in T results in
a transition ((q, z), a?, w, (q?, z?)) in T ? only when:
(a) a?? ? {}, z? =z and a? =a, (b) a??, z? =za
and a? = , or (c) a ? ?, z? is such that z = z?a
and a? = . The initial state of T ? is I ? = (I, ). A
state (q, z) in T ? is final if q is final in T and z = 
(??((q, ))=?(q)). The set of states of T ? is the set of
pairs (q, z) that can be reached from an initial state
by transitions defined as above. The condition that
T has a bounded stack ensures that this set is finite
(since it implies that for any (q, z), |z| ? K).
The complexity of the algorithm is linear in
O(|T ?|)=O(e|T |). Figure 1d show the result of the
algorithm when applied to the PDA of Figure 1c.
2.3 Intersection Algorithm
The class of weighted pushdown automata is closed
under intersection with weighted finite automata
(Bar-Hillel et al, 1964; Nederhof and Satta, 2003).
Considering a pair (T1, T2) where one element is an
FSA and the other element a PDA, then there exists
a PDA T1?T2, the intersection of T1 and T2, such
that for all x ? ??: (T1?T2)(x) = T1(x)+T2(x).
We assume in the following that T2 is an FSA. We
also assume that T2 has no input- transitions. When
T2 has input- transitions, an epsilon filter (Mohri,
2009; Allauzen et al, 2011) generalized to handle
parentheses can be used.
A state in T =T1?T2 is a pair (q1, q2) where q1 is
a state of T1 and q2 a state of T2. The initial state is
I=(I1, I2). Given a transition e1=(q1, a, w1, q?1) in
T1, transitions out of (q1, q2) in T are obtained using
the following rules.
If a ? ?, then e1 can be matched with a tran-
sition (q2, a, w2, q?2) in T2 resulting a transition
((q1, q2), a, w1+w2, (q?1, q?2)) in T .
If a = , then e1 is matched with staying in q2
resulting in a transition ((q1, q2), , w1, (q?1, q2)).
Finally, if a ? ??, e1 is also matched
with staying in q2, resulting in a transition
((q1, q2), a, w1, (q?1, q2)) in T .
A state (q1, q2) in T is final when both q1 and q2
are final, and then ?((q1, q2))=?1(q1)+?2(q2).
SHORTESTDISTANCE(T )
1 for each q ? Q and a ? ? do
2 B[q, a]? ?
3 GETDISTANCE(T, I)
4 return d[f, I ]
RELAX(q, s, w,S)
1 if d[q, s] > w then
2 d[q, s]? w
3 if q 6? S then
4 ENQUEUE(S , q)
GETDISTANCE(T,s)
1 for each q ? Q do
2 d[q, s]??
3 d[s, s]? 0
4 Ss ? s
5 while Ss 6=? do
6 q ? HEAD(Ss)
7 DEQUEUE(Ss)
8 for each e ? E[q] do
9 if i[e] ? ? ? {} then
10 RELAX(n[e], s, d[q, s] + w[e],Ss)
11 elseif i[e] ? ? then
12 B[s, i[e]]? B[s, i[e]] ? {e}
13 elseif i[e] ? ? then
14 if d[n[e], n[e]] is undefined then
15 GETDISTANCE(T, n[e])
16 for each e? ? B[n[e], i[e]] do
17 w? d[q, s] +w[e] + d[p[e?], n[e]] + w[e?]
18 RELAX(n[e?], s, w,Ss)
Figure 2: PDA shortest distance algorithm. We assume
that F ={f} and ?(f)=0 to simplify the presentation.
The complexity of the algorithm is in O(|T1||T2|).
2.4 Shortest Distance and Path Algorithms
A shortest path in a PDA T is a balanced accepting
path with minimal weight and the shortest distance
in T is the weight of such a path. We show that when
T has a bounded stack, shortest distance and short-
est path can be computed in O(|T |3 log |T |) time
(assuming T has no negative weights) and O(|T |2)
space.
Given a state s in T with at least one incoming
open parenthesis transition, we denote by Cs the set
of states that can be reached from s by a balanced
path. If s has several incoming open parenthesis
transitions, a naive implementation might lead to the
states in Cs to be visited up to exponentially many
times. The basic idea of the algorithm is to memo-
ize the shortest distance from s to states in Cs. The
1376
pseudo-code is given in Figure 2.
GETDISTANCE(T, s) starts a new instance of the
shortest-distance algorithm from s using the queue
Ss, initially containing s. While the queue is not
empty, a state is dequeued and its outgoing transi-
tions examined (line 5-9). Transitions labeled by
non-parenthesis are treated as in Mohri (2009) (line
9-10). When the considered transition e is labeled by
a close parenthesis, it is remembered that it balances
all incoming open parentheses in s labeled by i[e]
by adding e to B[s, i[e]] (line 11-12). Finally, when
e is labeled with an open parenthesis, if its destina-
tion has not already been visited, a new instance is
started from n[e] (line 14-15). The destination states
of all transitions balancing e are then relaxed (line
16-18).
The space complexity of the algorithm is
quadratic for two reasons. First, the number of
non-infinity d[q, s] is |Q|2. Second, the space re-
quired for storing B is at most in O(|E|2) since
for each open parenthesis transition e, the size of
|B[n[e], i[e]]| is O(|E|) in the worst case. This
last observation also implies that the cumulated
number of transitions examined at line 16 is in
O(N |Q| |E|2) in the worst case, where N denotes
the maximal number of times a state is inserted in
the queue for a given call of GETDISTANCE. As-
suming the cost of a queue operation is ?(n) for a
queue containing n elements, the worst-case time
complexity of the algorithm can then be expressed
as O(N |T |3 ?(|T |)). When T contains no negative
weights, using a shortest-first queue discipline leads
to a time complexity in O(|T |3 log |T |). When all
the Cs?s are acyclic, using a topological order queue
discipline leads to a O(|T |3) time complexity.
In effect, we are solving a k-sources shortest-
path problem with k single-source solutions. A po-
tentially better approach might be to solve the k-
sources or k-pairs problem directly (Hershberger et
al., 2003).
When T has been obtained by converting an RTN
or an hypergraph into a PDA (Section 2.5), the poly-
nomial dependency in |T | becomes a linear depen-
dency both for the time and space complexities. In-
deed, for each q in T , there exists a unique s such
that d[q, s] is non-infinity. Moreover, for each close
parenthesis transistion e, there exists a unique open
parenthesis transition e? such that e?B[n[e?], i[e?]].
When each component of the RTN is acyclic, the
complexity of the algorithm is hence in O(|T |) in
time and space.
The algorithm can be modified to compute the
shortest path by keeping track of parent pointers.
2.5 Replacement Algorithm
A recursive transition network (RTN) can be speci-
fied by (N,?, (T?)??N , S) where N is an alphabet
of nonterminals, ? is the input alphabet, (T?)??N is
a family of FSAs with input alphabet ? ? N , and
S?N is the root nonterminal.
A string x ? ?? is accepted by R if there exists
an accepting path pi in TS such that recursively re-
placing any transition with input label ? ?N by an
accepting path in T? leads to a path pi? with input x.
The weight associated by R is the minimum over all
such pi? of w[pi?]+?S(n[pi?]).
Given an RTN R, the replacement of R is the
PDA T equivalent to R defined by the 9-tuple
(?,?,?, Q,E, I, F, ?, ?) with ?=Q=???N Q? ,
I = IS , F = FS , ?= ?S , and E =
?
??N
?
e?E? Ee
where Ee = {e} if i[e] 6? N and Ee =
{(p[e], n[e], w[e], I?), (f, n[e], ??(f), n[e])|f ?F?}
with ?= i[e]?N otherwise.
The complexity of the construction is in O(|T |).
If |F? | = 1, then |T | = O(
?
??N |T? |) = O(|R|).
Creating a superfinal state for each T? would lead to
a T whose size is always linear in the size of R.
3 Hierarchical Phrase-Based Translation
Representation
In this section, we compare several different repre-
sentations for the target translations T of the source
sentence s by synchronous CFG G prior to language
model M application. As discussed in the introduc-
tion, T is a context-free language. For example, sup-
pose it corresponds to:
S?abXdg, S?acXfg, and X?bc.
Figure 3 shows several alternative representations of
T : Figure 3a shows the hypergraph representation of
this grammar; there is a 1:1 correspondence between
each production in the CFG and each hyperedge in
the hypergraph. Figure 3b shows the RTN represen-
tation of this grammar with a 1:1 correspondence be-
tween each production in the CFG and each path in
the RTN; this is the translation representation pro-
1377
SX
3 3
a
1 1
b
2
1
c
2
2
d
4
f
4
g
5 5
(a) Hypergraph
0
1a
6
a
2b
7c
3X 4d 5g
8X 9f 10g 11 12b 13c
S X
(b) RTN
0
1a
6
a
2b
7c
11
(
12b
3 4d 5g
[
8 9f 10g
13c
)
]
(c) PDA
0,?
1,?a
6,?
a
2,?b
7,?c
11,(?
11,[?
12,(b
12,[b
13,(c
13,[c
3,??
8,??
4,?d
9,?f
5,?g
10,?g
(d) FSA
Figure 3: Alternative translation representations
duced by the HiFST decoder (Iglesias et al, 2009a;
de Gispert et al, 2010). Figure 3c shows the push-
down automaton representation generated from the
RTN with the replacement algorithm of Section 2.5.
Since s is a finite language and G does not allow
unbounded insertion, Tp has a bounded stack and
T is, in fact, a regular language. Figure 3d shows
the finite-state automaton representation of T gen-
erated by the PDA using the expansion algorithm
of Section 2.2. The HiFST decoder converts its
RTN translation representation immediately into the
finite-state representation using an algorithm equiv-
alent to converting the RTN into a PDA followed by
PDA expansion.
As shown in Figure 4, an advantage of the RTN,
PDA, and FSA representations is that they can bene-
fit from FSA epsilon removal, determinization and
minimization algorithms applied to their compo-
nents (for RTNs and PDAs) or their entirety (for
FSAs). For the complexity discussion below, how-
ever, we disregard these optimizations. Instead we
focus on the complexity of each MT step described
in the introduction:
1. SCFG Translation: Assuming that the parsing
of the input is performed by a CYK parse, then
the CFG, hypergraph, RTN and PDA represen-
0 1a
2b
3
c
4X
5X
6
d
f 7
g
0 1b 2c
S X
(a) RTN
0 1a
2b
3
c 8
(
[ 9
b
4
6
d
7g
5
f1 0c
)
]
(b) PDA
0 1a
2b
3
c
4b
5b
6c
7c
8
d
f 9
g
(c) FSA
Figure 4: Optimized translation representations
tations can be generated in O(|s|3|G|) time and
space (Aho and Ullman, 1972). The FSA rep-
resentation can require an additional O(e|s|3|G|)
time and space since the PDA expansion can be
exponential.
2. Intersection: The intersection of a CFG Th
with a finite automaton M can be performed by
the classical Bar-Hillel algorithm (Bar-Hillel
et al, 1964) with time and space complex-
ity O(|Th||M |3).1 The PDA intersection algo-
rithm from Section 2.3 has time and space com-
plexity O(|Tp||M |). Finally, the FSA intersec-
tion algorithm has time and space complexity
O(|Tf ||M |) (Mohri, 2009).
3. Shortest Path: The shortest path algorithm on
the hypergraph, RTN, and FSA representations
requires linear time and space (given the under-
lying acyclicity) (Huang, 2008; Mohri, 2009).
As presented in Section 2.4, the PDA rep-
resentation can require time cubic and space
quadratic in |M |.2
Table 1 summarizes the complexity results. Note
the PDA representation is equivalent in time and su-
perior in space to the CFG/hypergraph representa-
tion, in general, and it can be superior in both space
1The modified Bar-Hillel construction described by Chi-
ang (2007) has time and space complexity O(|Th||M |4); the
modifications were introduced presumably to benefit the subse-
quent pruning method employed (but see Huang et al (2005)).
2The time (resp. space) complexity is not cubic (resp.
quadratic) in |Tp||M |. Given a state q in Tp, there exists a
unique sq such that q belongs to Csq . Given a state (q1, q2)
in Tp ?M , (q1, q2) ? C(s1,s2) only if s1 = sq1 , and hence
(q1, q2) belongs to at most |M | components.
1378
Representation Time Complexity Space Complexity
CFG/hypergraph O(|s|3 |G| |M |3) O(|s|3 |G| |M |3)
PDA O(|s|3 |G| |M |3) O(|s|3 |G| |M |2)
FSA O(e|s|3|G| |M |) O(e|s|3|G| |M |)
Table 1: Complexity using various target translation rep-
resentations.
and time to the FSA representation depending on the
relative SCFG and LM sizes. The FSA representa-
tion favors smaller target translation sets and larger
language models. Should a better complexity PDA
shortest path algorithm be found, this conclusion
could change. In practice, the PDA and FSA rep-
resentations benefit hugely from the optimizations
mentioned above, these optimizations improve the
time and space usage by one order of magnitude.
4 Experimental Framework
We use two hierarchical phrase-based SMT de-
coders. The first one is a lattice-based decoder im-
plemented with weighted finite-state transducers (de
Gispert et al, 2010) and described in Section 3. The
second decoder is a modified version using PDAs
as described in Section 2. In order to distinguish
both decoders we call them HiFST and HiPDT, re-
spectively. The principal difference between the two
decoders is where the finite-state expansion step is
done. In HiFST, the RTN representation is immedi-
ately expanded to an FSA. In HiPDT, this expansion
is delayed as late as possible - in the output of the
shortest path algorithm. Another possible configu-
ration is to expand after the LM intersection step but
before the shortest path algorithm; in practice this is
quite similar to HiFST.
In the following sections we report experiments
in Chinese-to-English translation. For translation
model training, we use a subset of the GALE 2008
evaluation parallel text;3 this is 2.1M sentences and
approximately 45M words per language. We re-
port translation results on a development set tune-nw
(1,755 sentences) and a test set test-nw (1,671 sen-
tences). These contain translations produced by the
GALE program and portions of the newswire sec-
tions of MT02 through MT06. In tuning the sys-
3See http://projects.ldc.upenn.edu/gale/data/catalog.html.
We excluded the UN material and the LDC2002E18,
LDC2004T08, LDC2007E08 and CUDonga collections.
0 7.5? 10?9 7.5? 10?8 7.5? 10?7
207.5 20.2 4.1 0.9
Table 2: Number of ngrams (in millions) in the 1st pass 4-gram
language models obtained with different ? values (top row).
tems, standard MERT (Och, 2003) iterative param-
eter estimation under IBM BLEU4 is performed on
the development set.
The parallel corpus is aligned using MTTK (Deng
and Byrne, 2008) in both source-to-target and
target-to-source directions. We then follow stan-
dard heuristics (Chiang, 2007) and filtering strate-
gies (Iglesias et al, 2009b) to extract hierarchical
phrases from the union of the directional word align-
ments. We call a translation grammar the set of rules
extracted from this process. We extract two transla-
tion grammars:
? A restricted grammar where we apply the fol-
lowing additional constraint: rules are only
considered if they have a forward translation
probability p > 0.01. We call this G1. As will
be discussed later, the interest of this grammar
is that decoding under it can be exact, that is,
without any pruning in search.
? An unrestricted one without the previous con-
straint. We call this G2. This is a superset of
the previous grammar, and exact search under
it is not feasible for HiFST: pruning is required
in search.
The initial English language model is a Kneser-
Ney 4-gram estimated over the target side of the par-
allel text and the AFP and Xinhua portions of mono-
lingual data from the English Gigaword Fourth Edi-
tion (LDC2009T13). This is a total of 1.3B words.
We will call this language model M1. For large lan-
guage model rescoring we also use the LM M2 ob-
tained by interpolating M1 with a zero-cutoff stupid-
backoff (Brants et al, 2007) 5-gram estimated using
6.6B words of English newswire text.
We next describe how we build translation sys-
tems using entropy-pruned language models.
1. We build a baseline HiFST system that uses M1
and a hierarchical grammar G, parameters be-
ing optimized with MERT under BLEU.
4See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl
1379
2. We then use entropy-based pruning of the lan-
guage model (Stolcke, 1998) under a relative
perplexity threshold of ? to reduce the size of
M1. We will call the resulting language model
as M?1 . Table 2 shows the number of n-grams
(in millions) obtained for different ? values.
3. We translate with M?1 using the same param-
eters obtained in MERT in step 1, except for
the word penalty, tuned over the lattices under
BLEU performance. This produces a transla-
tion lattice in the topmost cell that contains hy-
potheses with exact scores under the translation
grammar and M?1 .
4. Translation lattices in the topmost cell are
pruned with a likelihood-based beam width ?.
5. We remove the M?1 scores from the pruned
translation lattices and reapply M1, moving the
word penalty back to the original value ob-
tained in MERT. These operations can be car-
ried out efficiently via standard FSA opera-
tions.
6. Additionally, we can rescore the translation lat-
tices obtained in steps 1 or 5 with the larger
language model M2. Again, this can be done
via standard FSA operations.
Note that if ?=? or if ?=0, the translation lattices
obtained in step 1 should be identical to the ones of
step 5. While the goal is to increase ? to reduce the
size of the language model used at Step 3, ? will
have to increase accordingly so as to avoid pruning
away desirable hypotheses in Step 4. If ? defines
a sufficiently wide beam to contain the hypotheses
which would be favoured by M1, faster decoding
with M?1 would be possible without incurring search
errors M1. This is investigated next.
5 Entropy-Pruned LM in Rescoring
In Table 3 we show translation performance under
grammar G1 for different values of ?. Performance
is reported after first-pass decoding with M?1 (see
step 3 in Section 4), after rescoring with M1 (see
step 5) and after rescoring with M2 (see step 6). The
baseline (experiment number 1) uses ? = 0 (that is,
M1) for decoding.
Under translation grammar G1, HiFST is able to
generate an FSA with the entire space of possible
candidate hypotheses. Therefore, any degradation
in performance is only due to the M?1 involved in
decoding and the ? applied prior to rescoring.
As shown in row number 2, for ? ? 10?9 the
system provides the same performance to the base-
line when ? > 8, while decoding time is reduced
by roughly 40%. This is because M?1 is 10% of the
size of the original language model M1, as shown
in Table 2. As M?1 is further reduced by increas-
ing ? (see rows number 3 and 4), decoding time is
also reduced. However, the beam width ? required
in order to recover the good hypotheses in rescoring
increases, reaching 12 for experiment 3 and 15 for
experiment 4.
Regarding rescoring with the larger M2 (step 6
in Section 4), the system is also able to match the
baseline performance as long as ? is wide enough,
given the particular M?1 used in first-pass decoding.
Interestingly, results show that a similar ? value is
needed when rescoring either with M1 or M2.
The usage of entropy-pruned language models in-
crements speed at the risk of search errors. For in-
stance, comparing the outputs of systems 1 and 2
with ?=10 in Table 3 we find 45 different 1-best hy-
potheses, even though the BLEU score is identical.
In other words, we have 45 cases in which system 2
is not able to recover the baseline output because the
1st-pass likelihood beam ? is not wide enough. Sim-
ilarly, system 3 fails in 101 cases (? =12) and sys-
tem 4 fails in 95 cases. Interestingly, some of these
sentences would require impractically huge beams.
This might be due to the Kneser-Ney smoothing,
which interacts badly with entropy pruning (Chelba
et al, 2010).
6 Hiero with PDAs and FSAs
In this section we contrast HiFST with HiPDT under
the same translation grammar and entropy-pruned
language models. Under the constrained grammar
G1 their performance is identical as both decoders
can generate the entire search space which can then
be rescored with M1 or M2 as shown in the previous
section.
Therefore, we now focus on the unconstrained
grammar G2, where exact search is not feasible for
HiFST. In order to evaluate this problem, we run
both decoders over tune-nw, restricting memory us-
age to 10 gigabytes. If this limit is reached in decod-
1380
HiFST (G1 + M?1 ) +M1 +M2
# ? tune-nw test-nw time ? tune-nw test-nw tune-nw test-nw
1 0 (M1) 34.3 34.5 0.68 - - - 34.8 35.6
2 7.5? 10?9 32.0 32.8 0.38 10 34.8 35.6
9 34.3 34.5 34.9 35.5
8
3 7.5? 10?8 29.5 30.0 0.28 12 34.2 34.5 34.7 35.6
9 34.3 34.4 34.8 35.2
8 34.2 35.1
4 7.5? 10?7 26.0 26.4 0.20 15 34.2 34.5 34.7 35.6
12 34.4 35.5
Table 3: Results (lowercase IBM BLEU scores) under G1 with various M?1 as obtained with several values of ?.
Performance in subsequent rescoring with M1 and M2 after likelihood-based pruning of the translation lattices for
various ? is also reported. Decoding time, in seconds/word over test-nw, refers strictly to first-pass decoding.
Exact search for G2 + M?1 with memory usage under 10 GB
# ? HiFST HiPDT
Success Failure Success Failure
Expand Compose Compose Expand
2 7.5? 10?9 12 51 37 40 8 52
3 7.5? 10?8 16 53 31 76 1 23
4 7.5? 10?7 18 53 29 99.8 0 0.2
Table 4: Percentage of success in producing the 1-best translation under G2 with various M?1 when applying a hard
memory limitation of 10 GB, as measured over tune-nw (1755 sentences). If decoder fails, we report what step was
being done when the limit was reached. HiFST could be expanding into an FSA or composing the FSA with M?1 ;
HiPDT could be PDA composing with M?1 or PDA expanding into an FSA.
HiPDT (G2 + M?1 ) +M1 +M2
? tune-nw test-nw ? tune-nw test-nw tune-nw test-nw
7.5? 10?7 25.7 26.3 15 34.6 34.8 35.2 36.1
Table 5: HiPDT performance on grammar G2 with ? = 7.5 ? 10?7. Exact search with HiFST is not possible under
these conditions: pruning during search would be required.
ing, the process is killed5. We report what internal
decoding operation caused the system to crash. For
HiFST, these include expansion into an FSA (Ex-
pand) and subsequent intersection with the language
model (Compose). For HiPDT, these include PDA
intersection with the language model (Compose) and
subsequent expansion into an FSA (Expand), using
algorithms described in Section 2.
Table 4 shows the number of times each decoder
succeeds in finding a hypothesis given the memory
limit, and the operations being carried out when they
fail to do so, when decoding with various M?1 . With
?=7.5? 10?9 (row 2), HiFST can only decode 218
sentences, while HiPDT succeeds in 703 cases. The
5We used ulimit command. The experiment was carried out
over machines with different configurations and load. There-
fore, these numbers must be considered as approximate values.
differences between both decoders increase as the
M?1 is more reduced, and for ?=7.5?10?7 (row 4),
HiPDT is able to perform exact search over all but
three sentences.
Table 5 shows performance using the latter con-
figuration (Table 4, row 4). After large language
model rescoring, HiPDT improves 0.5 BLEU over
baseline with G1 (Table 3, row 1).
7 Discussion and Conclusion
HiFST fails to decode mainly because the expansion
into an FST leads to far too big search spaces (e.g.
fails 938 times under ? = 7.5 ? 10?8). If it suc-
ceeds in expanding the search space into an FST,
the decoder still has to compose with the language
model, which is also critical in terms of memory us-
1381
age (fails 536 times). In contrast, HiPDT creates a
PDA, which is a more compact representation of the
search space and allows efficient intersection with
the language model before expansion into an FST.
Therefore, the memory usage is considerably lower.
Nevertheless, the complexity of the language model
is critical for the PDA intersection and very specially
the PDA expansion into an FST (fails 403 times for
?=7.5? 10?8).
With the algorithms presented in this paper, de-
coding with PDAs is possible for any translation
grammar as long as an entropy pruned LM is used.
While this allows exact decoding, it comes at the
cost of making decisions based on less complex
LMs, although this has been shown to be an ad-
equate strategy when applying compact CFG rule-
sets. On the other hand, HiFST cannot decode under
large translation grammars, thus requiring pruning
during lattice construction, but it can apply an un-
pruned LM in this process. We find that with care-
fully designed pruning strategies, HiFST can match
the performance of HiPDT reported in Table 5. But
without pruning in search, expansion directly into an
FST would lead to an explosion in terms of memory
usage. Of course, without memory constraints both
strategies would reach the same performance.
Overall, these results suggest that HiPDT is more
robust than HiFST when using complex hierarchi-
cal grammars. Conversely, FSTs might be more
efficient for search spaces described by more con-
strained hierarchical grammars. This suggests that
a hybrid solution could be effective: we could use
PDAs or FSTs e.g. depending on the number of
states of the FST representing the expanded search
space, or other conditions.
8 Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7-ICT-2009-4) under grant
agreement number 247762, and was supported in
part by the GALE program of the Defense Advanced
Research Projects Agency, Contract No.HR0011-
06-C-0022, and a Google Faculty Research Award,
May 2010.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation and Compiling, volume 1-2.
Prentice-Hall.
Cyril Allauzen and Michael Riley, 2011. Pushdown
Transducers. http://pdt.openfst.org.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of CIAA, pages 11?23.
http://www.openfst.org.
Cyril Allauzen, Michael Riley, and Johan Schalkwyk.
2011. Filters for efficient composition of weighted
finite-state transducers. In Proceedings of CIAA, vol-
ume 6482 of LNCS, pages 28?38. Springer.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Y. Bar-Hillel, editor, Language and Information: Se-
lected Essays on their Theory and Application, pages
116?150. Addison-Wesley.
Jean Berstel. 1979. Transductions and Context-Free
Languages. Teubner.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP-ACL,
pages 858?867.
Ciprian Chelba, Thorsten Brants, Will Neveitt, and Peng
Xu. 2010. Study on interaction between entropy prun-
ing and kneser-ney smoothing. In Proceedings of In-
terspeech, pages 2242?2245.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hier-
archical phrase-based translation with weighted finite
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36(3).
Yonggang Deng and William Byrne. 2008. HMM word
and phrase alignment for statistical machine transla-
tion. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 16(3):494?507.
Manfred Drosde, Werner Kuick, and Heiko Vogler, ed-
itors. 2009. Handbook of Weighted Automata.
Springer.
John Hershberger, Subhash Suri, and Amit Bhosle. 2003.
On the difficulty of some shortest path problems. In
Proceedings of STACS, volume 2607 of LNCS, pages
343?354. Springer.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL, pages 144?151.
1382
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273?283.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005.
Machine translation as lexicalized parsing with hooks.
In Proceedings of the Ninth International Workshop
on Parsing Technology, Parsing ?05, pages 65?73,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Liang Huang. 2008. Advanced dynamic programming in
semiring and hypergraph frameworks. In Proceedings
of COLING, pages 1?18.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009a. Hierarchical phrase-based
translation with weighted finite state transducers. In
Proceedings of NAACL-HLT, pages 433?441.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009b. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
EACL, pages 380?388.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of EMNLP, pages 1288?1298.
Werner Kuich and Arto Salomaa. 1986. Semirings, au-
tomata, languages. Springer.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Andrej Ljolje, Fernando Pereira, and Michael Riley.
1999. Efficient general lattice generation and rescor-
ing. In Proceedings of Eurospeech, pages 1251?1254.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Drosde et al (Drosde et al, 2009), chapter 6, pages
213?254.
Mark-Jan Nederhof and Giorgio Satta. 2003. Probabilis-
tic parsing as intersection. In Proceedings of 8th In-
ternational Workshop on Parsing Technologies, pages
137?148.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167.
Ion Petre and Arto Salomaa. 2009. Algebraic systems
and pushdown automata. In Drosde et al (Drosde et
al., 2009), chapter 7, pages 257?289.
R. Prasad, K. Krstovski, F. Choi, S. Saleem, P. Natarajan,
M. Decerbo, and D. Stallard. 2007. Real-time speech-
to-speech translation for pdas. In Proceedings of IEEE
International Conference on Portable Information De-
vices, pages 1 ?5.
Alexander M. Rush and Michael Collins. 2011. Ex-
act decoding of syntactic translation models through
lagrangian relaxation. In Proceedings of ACL-HLT,
pages 72?82.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of DARPA
Broadcast News Transcription and Understanding
Workshop, pages 270?274.
1383
Pushdown Automata in Statistical
Machine Translation
Cyril Allauzen?
Google Research
Bill Byrne??
University of Cambridge
Adria` de Gispert??
University of Cambridge
Gonzalo Iglesias??
University of Cambridge
Michael Riley?
Google Research
This article describes the use of pushdown automata (PDA) in the context of statistical machine
translation and alignment under a synchronous context-free grammar. We use PDAs to com-
pactly represent the space of candidate translations generated by the grammar when applied to an
input sentence. General-purpose PDA algorithms for replacement, composition, shortest path,
and expansion are presented. We describe HiPDT, a hierarchical phrase-based decoder using the
PDA representation and these algorithms. We contrast the complexity of this decoder with a de-
coder based on a finite state automata representation, showing that PDAs provide a more suitable
framework to achieve exact decoding for larger synchronous context-free grammars and smaller
language models. We assess this experimentally on a large-scale Chinese-to-English alignment
and translation task. In translation, we propose a two-pass decoding strategy involving a weaker
language model in the first-pass to address the results of PDA complexity analysis. We study
in depth the experimental conditions and tradeoffs in which HiPDT can achieve state-of-the-art
performance for large-scale SMT.
? Google Research, 76 Ninth Avenue, New York, NY 10011. E-mail: {allauzen,riley}@google.com.
?? University of Cambridge, Department of Engineering. CB2 1PZ Cambridge, U.K. and SDL Research,
Cambridge U.K. E-mail: {wjb31,ad465,gi212}@eng.cam.ac.uk.
Submission received: 6 August 2012; revised version received: 20 February 2013; accepted for publication:
2 December 2013.
doi:10.1162/COLI a 00197
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
1. Introduction
Synchronous context-free grammars (SCFGs) are nowwidely used in statistical machine
translation, with Hiero as the preeminent example (Chiang 2007). Given an SCFG and
an n-gram language model, the challenge is to decode with them, that is, to apply them
to source text to generate a target translation.
Decoding is complex in practice, but it can be described simply and exactly in
terms of the formal languages and relations involved. We will use this description
to introduce and analyze pushdown automata (PDAs) for machine translation. This
formal description will allow close comparison of PDAs to existing decoders which are
based on other forms of automata. Decoding can be described in terms of the following
steps:
1. Translation: T = ?2({s}?G)
The first step is to compose the finite language {s}, which represents the
source sentence to be translated, with the algebraic relation G for the
translation grammar G. The result of this composition projected on the
output side is T , a weighted context-free grammar that contains all possible
translations of s under G. Following the usual definition of Hiero grammars,
we assume that G does not allow unbounded insertions so that T is a
regular language.
2. Language Model Application: L=T ?M
The next step is to compose the result of Step 1 with the weighted regular
grammarM defined by the n-gram language model, M. The result of this
composition is L, whose paths are weighted by the combined language
model and translation scores.
3. Search: l?=argmaxl?LL
The final step is to find the path through L that has the best combined
translation and language model score.
The composition {s} ? G in Step 1 that generates T can be performed by a modified
CYK algorithm (Chiang 2007). Our interest is in the different types of automata that can
be used to represent T as it is produced by this composition. We focus on three types
of representations: hypergraphs (Chiang 2007), weighted finite state automata (Iglesias
et al. 2009a; de Gispert et al. 2010), and PDAs. We will give a formal definition of PDAs
in Section 2, but we will first illustrate and compare these different representations by
a simple example.
Consider translating a source sentence ?s1 s2 s3? with a simple Hiero grammar G :
X??s1, t2 t3?
S??X s2 s3, t1 t2 X t4 t7?
S??X s2 s3, t1 t3 X t6 t7?
Step 1 yields the translations T = {?t1 t2 t2 t3 t4 t7? , ?t1 t3 t2 t3 t6 t7?}, and Figure 1 gives
examples of the different representations of these translations.We summarize the salient
features of these representations as they are used in decoding.
Hypergraphs. As described by Chiang (2007), a Hiero decoder can generate translations
in the form of a hypergraph, as in Figure 1a. As the figure shows, there is a
1:1 correspondence between each production in the CFG and each hyperedge in
the hypergraph.
688
Allauzen et al. Pushdown Automata in Statistical Machine Translation
(a) Hypergraph
0
1t1
6
t1
2t2
7t3
3X 4t4 5
t7
8X 9t6 10
t7 0 1t2 2t3
S X
(b) RTN
0
1t1
2
t1
3t2
4t3
5eps
6eps
7t2
8t2
9t3
10t3
11eps
12eps
13t4
14t6
15t7
16
t7
(c) FSA
0
1t1
6
t1
2t2
7t3
11
(
12t2
3 4t4 5
t7
[
8 9
t6
10t7
13t3
)
]
(d) PDA
Figure 1
Alternative representations of the regular language of possible translation candidates. Valid
paths through the PDA must have balanced parentheses.
Decoding proceeds by intersecting the translation hypergraph with a language
model, represented as a finite automaton, yielding L as a hypergraph. Step 3
yields a translation by finding the shortest path through the hypergraphL (Huang
2008).
Weighted Finite State Automata (WFSAs). Because T is a regular language and M is
represented by a finite automaton, it follows that T and L can themselves
be represented as finite automata. Consequently, Steps 2 and 3 can be solved
689
Computational Linguistics Volume 40, Number 3
using weighted finite-state intersection and single-source shortest path algo-
rithms, respectively (Mohri 2009). This is the general approach adopted in the
HiFST decoder (Iglesias et al. 2009a; de Gispert et al. 2010), which first represents
T as a Recursive Transition Network (RTN) and then performs expansion to
generate a WFSA.
Figure 1b shows the space of translations for this example represented as an RTN.
Like the hypergraph, it also has a 1:1 correspondence between each production
in the CFG and paths in the RTN components. The recursive RTN itself can be
expanded into a single WFSA, as shown in Figure 1c. Intersection and shortest
path algorithms are available for both of these WFSAs.
Pushdown Automata. Like WFSAs, PDAs are easily generated from RTNs, as will be
described later, and Figure 1d gives the PDA representation for this example. The
PDA represents the same language as the FSA, but with fewer states. Procedures
to carry out Steps 2 and 3 in decoding will be described in subsequent sections.
We will show that PDAs provide a general framework to describe key aspects
of several existing and novel translation algorithms. We note that PDAs have long
been used to describe parsing algorithms (Aho and Ullman 1972; Lang 1974), and it is
well known that pushdown transducers, the extended version of PDA with input and
output labels in each transition, do not have the expressive power needed to generate
synchronous context-free languages. For this reason, we do not use PDAs to implement
Step 1 in decoding: throughout this article a CYK-like parsing algorithm is always used
for Step 1. However, we do use PDAs to represent the regular languages produced in
Step 1 and in the intersection and shortest distance operations needed for Steps 2 and 3.
1.1 HiPDT: Hierarchical Phrase-Based Translation with PDAs
We introduce HiPDT, a hierarchical phrase-based decoder that uses a PDA representa-
tion for the target language. The architecture of the system is shown in Figure 2, where
CYK parse s with G
Build RTN
Expand RTN to FSA
Intersect FSA with LM
FSA 
Shortest 
Path
FSA 
Pruning
Lattice
1-Best
Hypothesis
RTN to PDA Replacement
Intersect PDA with LM
PDA 
(Pruned) 
Expansion
PDA 
Shortest 
Path
HiPDTHiFST
Figure 2
HiPDT versus HiFST: General flow and high-level operations.
690
Allauzen et al. Pushdown Automata in Statistical Machine Translation
we contrast it with HiFST (de Gispert et al. 2010). Both decoders parse the sentence with
a grammar G using a modified version of the CYK algorithm to generate the translation
search space as an RTN. Each decoder then follows a different path: HiFST expands
the RTN into an FSA, intersects it with the language model, and then prunes the result;
HiPDT performs the following steps:
1. Convert the RTN into PDA using the replacement algorithm. The PDA
representation for the example grammar in Section 1 is shown in Figure 1.
The algorithm will be described in Section 3.2.
2. Apply the language model scores to the PDA by composition. This operation
is described in Section 3.3.
3. Perform either one of the following operations:
(a) Shortest path through the PDA to get the exact best translation under
the model. Shortest distance/path algorithm is described in Section 3.4.
(b) Pruned expansion to an FSA. This expansion uses admissible pruning
and outputs a lattice. We do this for posterior rescoring steps. The
algorithm will be presented in detail in Sections 3.5 and 3.5.2.
The principal difference between the two decoders is the point at which finite-state
expansion is performed. In HiFST, the RTN representation is immediately expanded to
an FSA. In HiPDT, the PDA pruned expansion or shortest path computation is done
after the language model is applied, so that all computation is done with respect to both
the translation and language model scores.
The use of RTNs as an initial translation representation is somewhat influenced by
the development history of our FST and SMT systems. RTN algorithms were available
in OpenFST at the time HiFST was developed. HiPDT was developed as an extension to
HiFST using PDA algorithms, and these have subsequently been included in OpenFST.
A possible alternative approach could be to produce a PDA directly by traversing the
CYK grid. WFSAs could then be generated by PDA expansion, with a computational
complexity in speed and memory usage similar to the RTN-based approach.We present
RTNs as the initial translation representation because the generation of RTNs during
parsing is straightforward and has been previously presented (de Gispert et al. 2010).
We note, however, that RTN composition is algorithmically more complex than PDA
(and FSA) composition, so that RTNs themselves are not ideal representations of T if a
language model is to be applied. Composition of PDAs with FSAs will be discussed in
Section 3.3.
Figure 3 continues the simple translation example from earlier, showing how
HiPDT andHiFST both benefit from the compactness offeredbyWFSA epsilon removal,
determinization, andminimization operations. When applied to PDAs, these operations
treat parentheses as regular symbols. Compact representations of RTNs are shared by
both approaches. Figure 4 illustrates the PDA representation of the translation space
under a slightly more complex grammar that includes rules with alternative orderings
of nonterminals. The rule S??X1 s2 X2, t1 X1 X2? produces the sequence ?t1 t3 t4 t5 t6?,
and S??X1 s2 X2, t2 X2 X1? produces ?t2 t5 t6 t3 t4?. The PDA efficiently represents the
alternative orderings of the phrases ?t3 t4? and ?t5 t6? allowed under this grammar.
In addition to translation, this architecture can also be used directly to carry out
source-to-target alignment, or synchronous parsing, under the SCFG in a two-step
composition rather than one synchronous parsing stage. For example, by using M as the
automata that accepts ?t1 t2 t3 t6 t7?, Step 2 will yield all derivations that yield this string
691
Computational Linguistics Volume 40, Number 3
0 1t1
2t2
3
t3
4X
5X
6
t4
t6 7
t7
0 1t2 2t3
S X
(a) Optimized RTN
0 1t1
2t2
3
t3
4t2
5t2
6t3
7t3
8
t4
t6 9
t7
(b) Optimized FSA
0 1t1
2t2
3
t3 8
(
[ 9
t2
4
6
t4
7t7
5
t610t3
)
]
(c) Optimized PDA
Figure 3
Optimized representations of the regular language of possible translation candidates.
as a translation of the source string. This is the approach taken in Iglesias et al. (2009a)
and de Gispert et al. (2010) for the RTN/FSA and in Dyer (2010b) for hypergraphs. In
Section 4 we analyze how PDAs can be used for alignment.
1.2 Goals
We summarize here the aims of this article.
We will show how PDAs can be used as compact representations of the space T
of candidate translations generated by a hierarchical phrase-based SCFG when
applied to an input sentence s and intersected with a language model M.
We have described the architecture of HiPDT, a hierarchical phrase-based de-
coder based on PDAs, and have identified the general-purpose algorithms needed
0
1
t1
2t2
3(
4
[
5
t3
6t57t4 8t69)
10]
)
11]
(
[
X??s1, t3 t4?
X??s3, t5 t6?
S??X1 s2 X2, t1 X1 X2?
S??X1 s2 X2, t2 X2 X1?
Figure 4
Example of translation grammar with reordered nonterminals and the PDA representing the
result of applying the grammar to input sentence s1 s2 s3.
692
Allauzen et al. Pushdown Automata in Statistical Machine Translation
to perform translation and alignment; in doing so we have highlighted the
similarities and differences relative to translation with FSAs (Section 1.1). We
will provide a formal description of PDAs (Section 2) and present in detail
the associated PDA algorithms required to carry out Steps 2 and 3, including
RTN replacement, composition, shortest path, expansion, and pruned expansion
(Section 3).
We will show both theoretically and experimentally that the PDA representation is
well suited for exact decoding under a large SCFG and a small languagemodel.
An analysis of decoder complexity in terms of the automata used in the repre-
sentation is presented (Section 3). One important aspect of the translation task
is whether the search for the best translation is admissible (or exact) under the
translation and language models. Stated differently, we wish to know whether a
decoder produces the actual shortest path found or whether some form of pruning
might have introduced search errors. In our formulation, we can exclude inadmis-
sible pruning from the shortest-path algorithms, and doing so makes it straight-
forward to compare the computational complexity of a full translation pipeline
using different representations of T (Section 4). We empirically demonstrate that
a PDA representation is superior to an FSA representation in the ability to perform
exact decoding both in an inversion transduction grammar?style word alignment
task and in a translation task with a small language model (Section 4). In these
experiments we take HiFST as a contrastive system for HiPDT, but we do not
present experimental results with hypergraph representations. Hypergraphs are
widely used by the SMT community, and discussions and contrastive experiments
between HiFST and cube pruning decoders are available in the literature (Iglesias
et al. 2009a; de Gispert et al. 2010).
We will propose a two-pass translation decoding strategy for HiPDT based on
entropy-pruned first-pass language models.
Our complexity analysis prompts us to investigate decoding strategies based on
large translation grammars and small language models. We describe, implement,
and evaluate a two-pass decoding strategy for a large-scale translation task using
HiPDT (Section 5). We show that entropy-pruned languagemodels can be used in
first-pass translation, followed by admissible beam pruning of the output lattice
and subsequent rescoring with a full language model. We analyze the search
errors that might be introduced by a two-pass translation approach and show
that these can be negligible if pruning thresholds are set appropriately (Sec-
tion 5.2). Finally, we detail the experimental conditions and speed/performance
tradeoffs that allow HiPDT to achieve state-of-the-art performance for large-
scale SMT under a large grammar (Section 5.3), including lattice rescoring steps
under a vast 5-gram language model and lattice minimum Bayes risk decoding
(Section 5.4).
With this translation strategyHiPDT can yield very good translation performance.
For comparison, the performance of this Chinese-to-English SMT described in
Section 5.4 is equivalent to that of the University of Cambridge submission to the
NIST OpenMT 2012 Evaluation.1
1 For details see http://www.nist.gov/itl/iad/mig/openmt12.cfm.
693
Computational Linguistics Volume 40, Number 3
2. Pushdown Automata
Informally, pushdown transducers are finite-state transducers that have been aug-
mented with a stack. Typically this is done by adding a stack alphabet and labeling
each transition with a stack operation (a stack symbol to be pushed onto, popped, or
read from the stack) in addition to the usual input and output labels (Aho and Ullman
1972; Berstel 1979) and weight (Kuich and Salomaa 1986; Petre and Salomaa 2009). Our
equivalent representation allows a transition to be labeled by a stack operation or a
regular input/output symbol, but not both. Stack operations are represented by pairs
of open and close parentheses (pushing a symbol on and popping it from the stack).
The advantage of this representation is that it is identical to the finite automaton repre-
sentation except that certain symbols (the parentheses) have special semantics. As such,
several finite-state algorithms either immediately generalize to this PDA representation
or do so with minimal changes. In this section we formally define pushdown automata
and transducers.
2.1 Definitions
A (restricted) Dyck language consist of ?well-formed? or ?balanced? strings over a
finite number of pairs of parentheses. Thus the string ( [ ( ) ( ) ] { } [ ] ) ( ) is in the
Dyck language over three pairs of parentheses (see Berstel 1979 for a more detailed
presentation).
More formally, let A and A be two finite alphabets such that there exists a bijection
f from A to A. Intuitively, f maps an open parenthesis to its corresponding close
parenthesis. Let a? denote f (a) if a?A and f?1(a) if a?A. The Dyck language DA
over the alphabet A?=A ? A is then the language defined by the following context-free
grammar: S? ?, S? SS and S? aSa? for all a?A. We define the mapping cA : A?? ? A??
as follows. cA(x) is the string obtained by iteratively deleting from x all factors of the
form aa? with a ? A. Observe that DA=c?1A (?). Finally, for a subset B ? A, we define the
mapping rB : A? ? B? by rB(x1 . . . xn)=y1 . . . yn with yi=xi if xi?B and yi=? otherwise.
A semiring (K,?,?, 0, 1) is a ring that may lack negation. It is specified by a set
of values K, two binary operations ? and ?, and two designated values 0 and 1.
The operation ? is associative, commutative, and has 0 as identity. The operation ?
is associative, has identity 1, distributes with respect to ?, and has 0 as annihilator:
for all a ? K, a? 0 = 0? a = 0. If ? is also commutative, we say that the semiring is
commutative.
The probability semiring (R+,+,?, 0, 1) is used when the weights represent prob-
abilities. The log semiring (R ? {?},?log,+,?, 0), isomorphic to the probability semi-
ring via the negative-log mapping, is often used in practice for numerical stability. The
tropical semiring (R ? {?}, min,+,?, 0) is derived from the log semiring using the
Viterbi approximation. These three semirings are commutative.
A weighted pushdown automaton (PDA) T over a semiring (K,?,?, 0, 1) is an
8-tuple (?,?,?,Q,E, I, F, ?) where ? is the finite input alphabet, ? and ? are the finite
open and close parenthesis alphabets, Q is a finite set of states, I?Q the initial state,
F ? Q the set of final states, E ? Q? (? ? ?? ? {?})?K?Q a finite set of transitions,
and ? : F? K the final weight function. Let e= (p[e], i[e],w[e], n[e]) denote a transition
in E; for simplicity, (p[e], i[e], n[e]) denotes an unweighted transition (i.e., a transition
with weight 1?).
694
Allauzen et al. Pushdown Automata in Statistical Machine Translation
A path ? is a sequence of transitions ?=e1 . . . en such that n[ei]=p[ei+1] for 1 ? i <
n. We then define p[?]=p[e1], n[?]=n[en], i[?]= i[e1] ? ? ? i[en], and w[?]=w[e1]? . . .?
w[en]. A path ? is accepting if p[?]= I and n[?]?F. A path ? is balanced if r??(i[?])?D?.
A balanced path ? accepts the string x??? if it is a balanced accepting path such that
r?(i[?])=x.
The weight associated by T to a string x??? is
T(x)=
?
??P(x)
w[?]??(n[?]) (1)
where P(x) denotes the set of balanced paths accepting x. A weighted language is
recognizable by a weighted pushdown automaton iff it is context-free. We define the
size of T as |T|= |Q|+|E|.
A PDA T has a bounded stack if there exists K ? N such that for any path ? from I
such that c?(r??(i[?])) ? ??:
|c?(r??(i[?]))| ? K (2)
In other words, the number of open parentheses that are not closed along ? is bounded.
If T has a bounded stack, then it represents a regular language. Figure 5 shows non-
regular, regular, and bounded-stack PDAs. A weighted finite automaton (FSA) can be
viewed as a PDA where the open and close parentheses alphabets are empty (see Mohri
2009 for a stand-alone definition).
Finally, a weighted pushdown transducer (PDT) T over a semiring (K,?,?, 0, 1)
is a 9-tuple (?,?,?,?,Q,E, I, F, ?) where ? is the finite input alphabet, ? is the finite
output alphabet, ? and ? are the finite open and close parenthesis alphabets, Q is a
finite set of states, I?Q the initial state, F ? Q the set of final states, E ? Q? (? ? ?? ?
0
1a
2
?
(
3)b
0
1
a
2
?
(
?
3
)
?
b
0
1
(
3
?
2
a
4(
)
5
b
)
(a) (b) (c)
0,?
1,(
?
3,?
?
2,(a
4,(?
?
5,(
b
?
0
1a:c/1
2
?:?
(:(/1
3):)b:c/1
2
0
?:?
1
a:c/1
3
S:  /1?
b:c/1
TS
(d) (e) (f)
Figure 5
PDA Examples: (a) Non-regular PDA accepting {anbn|n ? N}. (b) Regular (but not
bounded-stack) PDA accepting a?b?. (c) Bounded-stack PDA accepting a?b? and (d) its
expansion as an FSA. (e) Weighted PDT T1 over the tropical semiring representing the
weighted transduction (anbn, c2n) 7? 3n and (f) equivalent RTN ({S},{a, b}, {c}, {TS},S).
695
Computational Linguistics Volume 40, Number 3
{?})?K?Q a finite set of transitions, and ? : F? K the final weight function. Let
e= (p[e], i[e], o[e],w[e], n[e]) denote a transition in E. Note that a PDA can be seen as
a particular case of a PDT where i[e] = o[e] for all its transitions. For simplicity, our
following presentation focuses on acceptors, rather than the more general case of trans-
ducers. This is adequate for the translation applications we describe, with the exception
of the treatment of alignment in Section 4.3, for which the intersection algorithm for
PDTs and FSTs is given in Appendix A.
3. PDT Operations
In this section we describe in detail the following PDA algorithms: Replacement, Com-
position, Shortest Path, and (Pruned) Expansion. Although these are needed to implement
HiPDT, these are general purpose algorithms, and suitable for many other applications
outside the focus of this article. The algorithms described in this section have been
implemented in the PDT extension (Allauzen and Riley 2011) of the OpenFst library
(Allauzen et al. 2007). In this section, in order to simplify the presentation we will only
consider machines over the tropical semiring (R+ ? {?}, min,+,?, 0). However, for
each operation, we will specify in which semirings it can be applied.
3.1 Recursive Transition Networks
We briefly give formal definitions for RTNs that will be needed to present the RTN
expansion operation. Examples are shown earlier in Figures 1(b) and 3(a). Informally,
an RTN is an automaton where some labels, nonterminals, are recursively replaced
by other automata. We give the formal definition for acceptors; the extension to RTN
transducers is straightforward.
An RTN R over the tropical semiring (R+ ? {?}, min,+,?, 0) is a 4-tuple
(N,?, (T?)??N, S) where N is the alphabet of nonterminals, ? is the input alpha-
bet, (T?)??N is a family of FSTs with input alphabet ? ?N, and S ? N is the root
nonterminal.
A sequence x ? ?? is accepted by (R,?) if there exists an accepting path ? in T? such
that ? = ?1e1 . . . ?nen?n+1 with i[?k] ? ??, i[ek] ? N and such that there exists sequences
xk such that xk is accepted by (R, i[ek]) and x = i[?1]x1 . . . i[?n]xni[?n+1]. We say that x is
accepted by R when it is accepted by (R, S). The weight associated by (R,?) (and by R)
to x can be defined in the same recursive manner.
As an example of testing whether an RTN accepts a sequence, consider the RTN R
of Figure 6 and the sequence x = a a b. The path in the automata TS can be written as
? = ?1 e1 ?2, with i[?1] = a, i[e1] = X1, and i[?2] = b. In addition, the machine (R, i[e1])
accepts x1 = a. Because x = i[?1] x1 i[?2], it follows that x is accepted by (R, S).
3.2 Replacement
This algorithm converts an RTN into a PDA. As explained in Section 1.1, this PDT
operation is applied by the HiPDT decoder in Step 1, and examples are given in earlier
sections (e.g., in figures 1 and 3).
Replacement acts on every transition of the RTN that is associated with a non-
terminal. The source and destination states of these transitions are used to define the
matched opening and closing parentheses, respectively, in the new PDA. Each RTN
nonterminal transition is deleted and replaced by two new transitions that lead to and
696
Allauzen et al. Pushdown Automata in Statistical Machine Translation
from the automaton indicated by the nonterminal. These new transitions have matched
parentheses, taken from the source and destination states of the RTN transition they
replace. Figure 6 gives a simple example.
Formally, given an RTN R, defined as (N,?, (T?)??N, S), its replacement is the PDA
T equivalent to R defined by the 8-tuple (?,?,?,Q,E, I, F, ?) with Q = ? =
?
??N Q?,
I = IS, F = FS, ? = ?S, and E =
?
??N
?
e?E? E
e where Ee = {e} if i[e] 6? N and
Ee={(p[e], n[e], ?,w[e], I?), (f, n[e], ?, ??(f ), n[e])|f ?F?} (3)
with ? = i[e] ? N otherwise.
The complexity of the construction is in O(|T|). If |F?| = 1 for all ? ? N, then
|T| = O(???N |T?|) = O(|R|). Creating a superfinal state for each T? would lead to a T
whose size is always linear in the size of R. In this article, we assume this optimization
is always performed. We note here that RTNs can be defined and the replacement
operation can be applied in any semiring.
3.3 Composition
Once we have created the PDA with translation scores, Step 2 in Section 1.1 applies the
language model scores to the translation space. This is done by composition with an
FSA containing the relevant language model weights.
The class of weighted pushdown transducers is closed under composition with
weighted finite-state transducers (Bar-Hillel, Perles, and Shamir 1964; Nederhof and
Satta 2003). OpenFST supports composition between automata T1 and T2, where T1
is a weighted pushdown transducer and T2 is a weighted finite-state transducer. If
both T1 and T2 are acceptors, rather than transducers, the composition of a PDA and
an FSA produces a PDA containing their intersection, and so no separate intersection
algorithm is required for these automata. Given this, we describe only the simpler,
special case of intersection between a PDA and an FSA, as this is sufficient for most
of the translation applications described in this article. The alignment experiments of
RTN R
1 2 3 4
a X1 b
TS
5 6
X2
a 7 8
b
TX1 TX2
R accepts a a b and a b b.
PDT T
1 2
a
5 6
3 4
7 8
a
b
b
6
3 3?
6?
T accepts a 3 a 3? b and a 3 6 b 6? 3? b.
Figure 6
Conversion of an RTN R to a PDA T by the replacement operation of Section 3.2. Using the
notation of Section 2.1, in this example ? = {3, 5} and ? = {3?, 5?}, with f (3) = 3? and f (5) = 5?.
The unweighted transition (2,X1, 3) in R is deleted and replaced by two new transitions (2, 3, 5)
and (6, 3?, 3); similarly, (5,X2, 6) is replaced by (5, 6, 7) and (8, 6?, 6). After application of the r?
mapping, the strings accepted by R and by T are the same.
697
Computational Linguistics Volume 40, Number 3
0 1ab 2
a
b 3
a
b 4
a
b
T2
0
1a
2
?
(
3)b T1
0,0
1,1a
2,0
?
0,1(
3,0)
1,2a
2,1
?
b
0,2(
3,1)
1,3a
2,2
?
b
0,3(
3,2)
1,4a
2,3
?
b
0,4(
3,3)
2,4
?
b
T
Figure 7
Composition example: Composition of a PDA T1 accepting {an, bn} with an FSA T2 accepting
{a, b}4 to produce a PDA T = T1 ? T2 . T has only one balanced path, and this path accepts
a(a(?)b)b. Composition is performed by the PDA-FSA intersection described in Section 3.3.
Section 4.3 do require composition of transducers; the algorithm for composition of
transducers is given in Appendix A.
An example of composition by intersection is given in Figure 7. The states of T are
created as the product of all the states in T1 and T2. Transitions are added as illustrated
in Figure 8. These correspond to all paths through T1 and T2 that can be taken by
a synchronized reading of strings from {a, b}?. The algorithm is very similar to the
composition algorithm for finite-state transducers, the difference being the handling
of the parentheses. The parenthesis-labeled transitions are treated similarly to epsilon
transitions, but the parenthesis labels are preserved in the result. This adds many
unbalanced paths to T. In this example, T has five paths but only one balanced path,
so that T accepts the string a a b b.
Formally, given a PDA T1 = (?,?,?,Q1,E1, I1, F1, ?1) and an FSA T2 =
(?,Q2,E2, I2, F2, ?2), intersection constructs a new PDA T = (?,?,?,Q,E, I, F, ?),
where T = T1 ? T2 as follows:
1. The new state space is in the product of the input state spaces: Q ? Q1 ?Q2.
2. The new initial and final states are I = (I1, I2), and F = {(q1, q2) : q1 ? F1, q2 ? F2}.
3. Weights are assigned to final states (q1, q2) ? Q as ?(q1, q2) = ?(q1)+ ?(q2).
4. For pairs of transitions (q1, a1,w1, q?1) ? E1 and (q2, a2,w2, q?2) ? E2, a transition
is added between states (q1, q2) and (q?1, q?2) as specified in Figure 8.
PDT T1 FSA T2 PDT T = T1 ? T2 Input Symbols
q1 q?1
a1/w1
q2 q?2
a2/w2
q1, q2 q?1, q?2
a1/w1 + w2
a1 ? ? and a1 = a2
q1, q2 q?1, q2
a1/w1
a1 ? ? ?? or a1 = ?
Transitions are added to T if and only if the conditions on the input symbols are satisfied.
Figure 8
PDA?FSA intersection under the tropical semiring. The PDA T is created by the intersection of
the PDA T1 and the FSA T2, i.e., T = T1 ? T2.
698
Allauzen et al. Pushdown Automata in Statistical Machine Translation
The intersection algorithm given here assumes that T2 has no input-? transitions.
When T2 has input-? transitions, an epsilon filter (Mohri 2009; Allauzen, Riley, and
Schalkwyk 2011) generalized to handle parentheses can be used. Note that Steps 1 and 2
do not require the construction of all possible pairs of states; only those states reachable
from the initial state and needed in Step 4 are actually generated. The complexity of
the algorithm is in O(|T1| |T2|) in the worst case, as will be discussed in Section 4.
Composition requires the semiring to be commutative.
3.4 Shortest Distance and Path Algorithms
With a PDA including both translation and language model weights, HiPDT can ex-
tract the best translation (Step 3a in Section 1.1). To this end, a general PDA shortest
distance/path algorithm is needed.
A shortest path in a PDA T is a balanced accepting path with minimal weight
and the shortest distance in T is the weight of such a path. We show that when
T has a bounded stack, shortest distance and shortest path can be computed in
O(|T|3 log |T|) time (assuming T has no negative weights) and O(|T|2) space. Figure 9
gives a pseudo-code description of the shortest-distance algorithm, which we now
discuss.
SHORTESTDISTANCE(T)
1 for each q ? Q and a ? ? do
2 B[q, a]? ?
3 for each q ? Q do
4 d[q, q]??
5 GETDISTANCE(T, I) ? I is the unique initial state
6 return d[I, f ] ? f is the unique final state
RELAX(s,q,w,S )
1 if d[s, q] > w then ? if w is a better estimate of the distance from s to q
2 d[s, q]? w ? update d[s, q]
3 if q 6? S then ? enqueue q in S if needed
4 ENQUEUE(S, q)
GETDISTANCE(T,s)
1 for each q ? Q do
2 d[s, q]??
3 d[s, s]? 0
4 Ss ? {s}
5 while Ss 6=? do
6 q? HEAD(Ss )
7 DEQUEUE(Ss )
8 for each e ? E[q] do ? E(q) is the set of transitions leaving state q
9 if i[e] ? ? ? {?} then ? i[e] is a regular symbol
10 RELAX(s,n[e], d[s, q]+w[e],Ss )
11 elseif i[e] ? ? then ? i[e] is a close parenthesis
12 B[s, i[e]]? B[s, i[e]] ? {e}
13 elseif i[e] ? ? then ? i[e] is an open parenthesis
14 if d[n[e], n[e]]=? then ? n[e] is the destination state of transition e
15 GETDISTANCE(T,n[e])
16 for each e? ? B[n[e], i[e]] do
17 w? d[s, q]+w[e]+ d[n[e], p[e?]]+ w[e?]
18 RELAX(s,n[e?],w,Ss )
Figure 9
PDT shortest distance algorithm.
699
Computational Linguistics Volume 40, Number 3
Given a PDA T = (?,?,?,Q,E, I, F, ?), the GETDISTANCE(T) algorithm computes
the shortest distance from the start state I to the final state2 f ? F. The algorithm
recursively calculates
d[q, q?] ? K ? the shortest distance from state q to state q? along a balanced path
At termination, the algorithm returns d[I, f ] as the cost of the shortest path through T.
The core of the shortest distance algorithm is the procedure GETDISTANCE(T, s)
which calculates the distances d[s, q] for all states q that can be reached from s. For an
FSA, this procedure is called once, as GETDISTANCE(T, I), to calculate d[I, q] ?q.
For a PDA, the situation is more complicated. Given a state s in T with at least
one incoming open parenthesis transition, we denote by Cs the set of states that can be
reached by a balanced path starting from s. If s has several incoming open parenthesis
transitions, a naive implementation might lead to the states in Cs to be visited exponen-
tially many times. This is avoided by memoizing the shortest distance from s to states in
Cs. To do this, GETDISTANCE(T, s) calculates d[s, s?] for all s? ? Cs, and it also constructs
sets of transitions
B[s, a] = {e ? E : p[e] ? Cs and i[e] = a} ?a ? ? (4)
These are the transitions with label a leaving states in Cs.
Consider any incoming transition to s, (q, a,w, s), with a ? ?. For every transition
e? = (s?, a,w?, q?), e? ? B[s, a] the following holds3
d[q, q?] = w+ d[s, s?]+ w? (5)
If d[s, s?] is available, the shortest distance from q to q? along any balanced path through
s can be computed trivially by Equation (5). For any state s with incoming open paren-
thesis transitions, only a single call to GETDISTANCE(T, s) is needed to precompute the
necessary values.
Figure 10 gives an example. When transition (2, (1, 0, 5) is processed,
GETDISTANCE(T, 5) is called. The distance d[5, 7] is computed, and following tran-
sitions are logged: B[5, (1]? {(7, )1, 0, 8)} and B[5, (2]? {(7, )2, 0, 9)}. Later, when the
transition (4, (2, 0, 5) is processed, its matching transition (7, )2, 0, 9) is extracted from
B[5, (2]. The distance d[4, 9] is then found by Equation (5) as d[5, 7]. This avoids redun-
dant re-calculation of distances along the shortest balanced path from state 4 to state 9.
We now briefly discuss the shortest distance pseudo-code given in Figure 9. The
description may be easier to follow after reading the worked example in Figure 10. Note
that the sets Cs are not computed explicitly by the algorithm.
The shortest distance calculation proceeds as follows. Self-distances, that is, d[q, q],
are set initially to ?; when GETDISTANCE(T, q) is called it sets d[q, q] = 0 to note that
q has been visited. GETDISTANCE(T, s) starts a new instance of the shortest-distance
algorithm from s using the queue Ss, initially containing s. While the queue is not empty,
a state is dequeued and its outgoing transitions examined (lines 7?11). Transitions
labeled by non-parenthesis are treated as in Mohri (2009) (lines 7?8). When a transition
e is labeled by a close parenthesis, e is added to B[s, i[e]] to indicate that this transition
2 For simplicity, we assume T has only one final state.
3 This assumes all paths from q to q? pass through s. The RELAX operation (Figure 9) handles the
general case.
700
Allauzen et al. Pushdown Automata in Statistical Machine Translation
0
1 2
5 6 7
8
10
3 4 9t1/20
t3/200
(2
t1/10
t2/100
(1 t2/1 t3/1
)1 t4/1, 000
)2
t6/1, 000
GETDISTANCE(T) runs
1. Initialization: d[q, q]??, ?q ? Q
2. GETDISTANCE(T, 0) is called
GETDISTANCE(T, 0) runs
3. Distances are calculated from state 0:
d[0, 0]? 0; d[0, 1]? d[0, 0]+ w[0, 1]; d[0, 2]? d[0, 1]+ w[1, 2]
4. Transition e1 = (2, (1, 0, 5) is reached. e1 has symbol i[e1] = (1 and destination state n[e1] = 5
5. d[5, 5] =? so GETDISTANCE(T, 5) is called
GETDISTANCE(T, 5) runs
6. Distances are calculated from state 5:
d[5, 5]? 0; d[5, 6]? d[5, 5]+ w[5, 6]; d[5, 7]? d[5, 6]+ w[6, 7]
7. The transitions (7, )1, 0, 8) and (7, )2, 0, 9) are reached and memoized
B[5, (1]? {(7, )1, 0, 8)}
B[5, (2]? {(7, )2, 0, 9)}
GETDISTANCE(T, 5) ends
GETDISTANCE(T, 0) resumes
8. Transition e1 = (2, (1, 0, 5) is still being processed, with p[e1] = 2, n[e1] = 5, and i[e1] = (1
9. Transition e2 = (7, )1, 0, 8) matching (1 is extracted from B[n[e1], i[e1]], with p[e2] = 7
and n[e2] = 8
10. Distance d[0, 8] is calculated as d[0, n[e2]] :
d[0, n[e2]]? d[0, p[e1]]+ w[p[e1],n[e1]]+ d[n[e1], p[e2]]+ w[p[e2],n[e2]]
10. Processing of e1 finishes, and calculation of distances from 0 continues:
d[0, 10]? d[0, 8]+ w[8, 10]
10 is a final state. Processing continues with transition (0, t1, 20, 3)
d[0, 3]? d[0, 0]+ w[0, 3]; d[0, 4]? d[0, 3]+ w[3, 4]
13. Transition e3 = (4, (2, 0, 5) is reached
e3 has symbol i[e3] = (2, source state p[e3] = 4, and destination state n[e3] = 5
14. GETDISTANCE(T, 5) is not called, since d[5, 5] = 0 indicates state 5 has been previously
visited
15. Transition e4 = (7, )2, 0, 9) matching (2 is extracted from B[n[e3], i[e3]], with p[e4] = 7 and
n[e4] = 9
16. Distance d[0, 9] is calculated as d[0, n[e4]], using cached values:
d[0, n[e4]]? d[0, p[e3]]+ w[p[e3],n[e3]]+ d[n[e3], p[e4]]+ w[p[e4],n[e4]]
17. d[0, 10] is less than? :
d[0, 10]? min(d[0, 10], d[0, 9]+ w[9, 10])
18. GETDISTANCE(T, 0) ends and returns d[0, 10]
GETDISTANCE(T) ends
Figure 10
Step-by-step description of the shortest distance calculation for the given PDA by the algorithm
of Figure 9. For simplicity, w[q, q?] indicates the weight of the transition connecting q and q?.
balances all incoming open parentheses into s labeled by i[e] (lines 9?10). Finally, if e has
an open parenthesis, and if its destination has not already been visited, a new instance of
GETDISTANCE is started from n[e] (lines 12?13). The destination states of all transitions
balancing e are then relaxed (lines 14?16).
The space complexity of the algorithm is quadratic for two reasons. First, the
number of non-infinity d[q, s] is |Q|2. Second, the space required for storing B is at
most in O(|E|2) because for each open parenthesis transition e, the size of |B[n[e], i[e]]|
701
Computational Linguistics Volume 40, Number 3
is O(|E|) in the worst case. This last observation also implies that the accumulated
number of transitions examined at line 16 is in O(Z|Q| |E|2) in the worst case, where
Z denotes the maximal number of times a state is inserted in the queue for a given
call of GETDISTANCE. Assuming the cost of a queue operation is ?(n) for a queue
containing n elements, the worst-case time complexity of the algorithm can then be
expressed as O(Z|T|3 ?(|T|)). When T contains no negative weights, using a shortest-
first queue discipline leads to a time complexity in O(|T|3 log |T|). When all the
Cs?s are acyclic, using a topological order queue discipline leads to a O(|T|3) time
complexity.
As was shown in Section 3.2, when T has been obtained by converting an RTN
or a hypergraph into a PDA, the polynomial dependency in |T| becomes a linear
dependency both for the time and space complexities. Indeed, for each q in T, there
exists a unique s such that d[s, q] is non-infinity. Moreover, for each open parenthesis
transition e, there exists a unique close parenthesis transition e? such that e??B[n[e], i[e]].
When each component of the RTN is acyclic, the complexity of the algorithm is O(|T|)
in time and space.
The algorithm can be modified (without changing the complexity) to compute the
shortest path by keeping track of parent pointers. The notion of shortest path requires
the semiring (K,?,?, 0, 1) to have the path property: for all a, b in K, a? b ? {a, b}. The
shortest-distance operation as presented here and the shortest-path operation can be
applied in any semiring having the path property by using the natural order defined by
?: a ? b iff a? b = a. However, the shortest distance algorithm given in Figure 9 can be
extended to work for k-closed semirings using the same techniques that were used by
Mohri (2002).
The shortest distance in the intersection of a string s and a PDA T determines if T
recognizes s. PDA recognition is closely related to CFG parsing; a CFG can be repre-
sented as a PDT whose input recognizes the CFG and whose output identifies the parse
(Aho and Ullman 1972). Lang (1974) showed that the cubic tabular method of Earley
can be naturally applied to PDAs; others give the weighted generalizations (Stolcke
1995; Nederhof and Satta 2006). Earley?s algorithm has its analogs in the algorithm in
Figure 9: the scan step corresponds to taking a non-parenthesis transition at line 10, the
predict step to taking an open parenthesis at lines 14?15, and the complete step to taking
the closed parentheses at lines 16?18.
Specialization to Translation. Following the formalism of Section 1, we are interested
in applying shortest distance and shortest path algorithms to automata created as
L = Tp ?M, where Tp, the translation representation, is a PDA derived from an RTN
(via replacement) and M, the language model, is a finite automaton.
For this particular case, the time complexity is O(|Tp||M|3) and the space complexity
is O(|Tp||M2|). The dependence on |Tp| is linear, rather than cubic or quadratic. The
reasoning is as follows. Given a state q in Tp, there exists a unique sq such that q belongs
to Csq. Given a state (q1, q2) in Tp?M, (q1, q2)?C(s1,s2 ) only if s1 = sq1 , and hence (q1, q2)
belongs to at most |M| components.
3.5 Expansion
As explained in Section 1.1, HiPDT can apply Step 3b to generate translation lattices.
This step is typically required for any posterior lattice rescoring strategies. We first
702
Allauzen et al. Pushdown Automata in Statistical Machine Translation
describe the unpruned expansion. However, in practice a pruning strategy of some sort
is required to avoid state explosion. Therefore, we also describe an implementation of
the PDA expansion that includes admissible pruning under a likelihood beam, thus
controlling on-the-fly the size of the output lattice.
3.5.1 Full Expansion. Given a bounded-stack PDA T, the expansion of T is the FSA T?
equivalent to T. A simple example is given in Figure 11.
Expansion starts from the PDA initial state. States and transitions are added to
the FSA as the expansion proceeds along paths through the PDA. In the new FSA,
parentheses are replaced by epsilons, and as open parentheses are encountered on
PDA transitions, they are ?pushed? into the FSA state labels; in this way the stack
depth is maintained along different paths through the PDA. Conversely, when a closing
parenthesis is encountered on a PDA path, a corresponding opening parenthesis is
?popped? from the FSA state label; if this is not possible, for example, as in state (5, ?)
in Figure 11, expansion along that path halts.
The resulting automata accept the same language. The FSA topology changes,
typically with more states and transitions than the original PDA, and the number of
added states is controlled only by the maximum stack depth of the PDA.
Formally, suppose the PDA T = (?,?,?,Q,E, I, F, ?) has a maximum stack depth
of K. The set of states in its FSA expansion T? are then
Q? = {(q, z) : q ? Q , z ? ?? and |z| ? K} (6)
and T? has initial state (I, ?) and final states F? = {(q, ?) : q ? F}. The condition that T
has a bounded stack ensures that Q? is finite. Transitions are added to T? as described in
Figure 12.
The full expansion operation can be applied to PDA over any semiring. The com-
plexity of the algorithm is linear in the size of T?. However, the size of T? can be
exponential in the size of T, which motivates the development of pruned expansion,
as discussed next.
0
1
2 3
4 5 6[ [
[
a
]
b ]
c
0,?
1, [ 2, [[ 3, [[ 4, [ 5, [ 6,??
? a ? b ?
2, [
3, [
?
a
c
4,? 5,?
?
b
Figure 11
Full expansion of a PDA to an equivalent FSA. The PDA maximum stack depth is 2; therefore
the FSA states belong to {0, .., 6} ? {?, [, [[}. Expansion can create incomplete paths in the FSA
(e.g., corresponding here to the unbalanced PDA path [ a ] b ]); however these are guaranteed to
be unconnected, namely, not to lead to a final state. Any unconnected states are removed after
expansion.
703
Computational Linguistics Volume 40, Number 3
Transition in PDA T New transition in FSA T? Conditions Explanation
q, z q?, z
a/w
a ? ? ? {?} a is not a parenthesis; stackdepth is unchanged
q q?
a/w
q, z q?, za
?
a ? ?
a is an open parenthesis; an
epsilon transition is added,
and a is ?pushed? into the
destination state, increas-
ing the stack depth
q, z?a q?, z?
?
a ? ?
a is a closing parenthe-
sis; an epsilon transition
is added, and the match-
ing open parenthesis a is
?popped? from the destina-
tion state, decreasing the
stack depth
Figure 12
PDA Expansion. A states (q, z) and (q?, z? ) in the FSA T? will be connected by a transition if and
only if the above conditions hold on the corresponding transition between q and q? in the PDA T.
3.5.2 Pruned Expansion. Given a bounded-stack PDA T, the pruned expansion of T with
threshold ? is an FST T?? obtained by deleting from T? all states and transitions that do
not belong to any accepting path ? in T? such that w[?]? ?[?] ? d+ ?, where d is the
shortest distance in T.
A naive implementation consisting of fully expanding T and then applying the
FST pruning algorithm would lead to a complexity in O(|T?| log |T?|)=O(e|T||T|).
Assuming that the reverse TR of T is also bounded-stack, an algorithm whose com-
plexity is in O(|T| |T??|+ |T|3 log |T|) can be obtained by first applying the shortest
distance algorithm from the previous section to TR and then using this to prune the
expansion as it is generated. To simplify the presentation, we assume that F={ f} and
?( f )=0.
The motivation for using reversed automaton in pruning is easily seen by looking
at FSAs. For an FSA, the cost of the shortest path through a transition (q, x,w, q?) can
be stated as d[I, q]+ w+ d[q?, f ]. Distances d[I, q] (i.e., distances from the start state) are
computed by the shortest distance algorithm, as discussed in Section 3.4. However,
distances of the form d[q?, f ] are not readily available. To compute these, a shortest
distance algorithm is run over the reversed automaton. Reversal preserves states and
transitions, but swaps the source and destination state (see Figure 13 for a PDA ex-
ample). The start state in the reversed machine is f , so that distances are computed
from f ; these are denoted dR[f, q] and correspond to d[q, f ] in the original FSA. The
cost of the shortest path through an FSA transition (q, x,w, q?) can then be computed as
d[I, q]+ w+ dR[f, q?].
Calculation for PDAs is more complex. Transitions with parentheses must be han-
dled such that distances through them are calculated over balanced paths. For example,
if T in Figure 13 was an FSA, the shortest cost of any path through the transition
e = (4, (2, 0, 5) could be calculated as d[0, 4]+ 0+ d[5, 10]. However, this is not correct,
because d[5, 10], the shortest distance from 5 to 10, is found via a path through the
transition (7, )1, 0, 8).
Correct calculation of the minimum cost of balanced paths through PDA transitions
can be done using quantities computed by the PDA shortest distance algorithm. For a
704
Allauzen et al. Pushdown Automata in Statistical Machine Translation
0
1 2
5 6 7
8
10
3 4 9t1/20
t3/200
(2
t1/10
t2/100
(1 t2/1 t3/1
)1 t4/1, 000
)2
t6/1, 000
T
0
1 2
5 6 7
8
10
3 4 9t1/20
t3/200
(2
t1/10
t2/100
(1 t2/1 t3/1
)1 t4/1, 000
)2
t6/1, 000
TR
Figure 13
PDA T and its reverse TR. TR has start state 10, final state 0, ?R = {)1, )2}, and ?
R = {(1, (2}.
PDA transition e = (q, a,w, q?), a ? ?, the cost of the shortest balanced path through e
can be found as4
c(e) = d[I, q]+ w[e]+ min
e??B[q?,a]
d[q?, p[e?]]+ w[e?]+ dR[n[e?], f ] (7)
where B[q?, a] and d[p[e?], q?] are computed by the PDA shortest distance algorithm over
T, and dR[n[e?], f ] is computed by the PDA shortest distance algorithm over TR.
In Figure 13, the shortest cost of paths through the transition e = (4, (2, 0, 5) is found
as follows: the shortest distance algorithm over T calculates d[0, 4] = 220 , d[5, 7] = 2,
and B[5, (2] = {7, )2, 0, 9}; the shortest distance algorithm over TR calculates dR[10, 9] =
1, 000 (trivially, here); the cost of the shortest path through e is
d[0, 4]+ w[e]+ d[5, 7]+ w[e?]+ dR[10, 9] = 220+ 0+ 2+ 0+ 1, 000
Pruned expansion is therefore able to avoid expanding transitions that would not
contribute to any path that would survive pruning. Prior to expansion of a PDA T to an
FSA T?, the shortest distance d in T is calculated. Transitions e = (q, a,w, q?), a ? ?, are
expanded as transitions e = ((q, z), q,w, (q?, za)) in T? only if c(e) ? d+ ?, as calculated
by Equation (7).
The pruned expansion algorithm implemented in OpenFST is necessarily more
complicated than the simple description given here. Pseudo-code describing the Open-
FST implementation is given in Appendix B.
The pruned expansion operation can be applied in any semiring having the path
property.
4 Note that d[p[e?], q?] could be replaced by dR[q?, p[e?]].
705
Computational Linguistics Volume 40, Number 3
4. HiPDT Analysis and Experiments: Computational Complexity
We now address the following questions:
r What are the differences between the FSA and PDA representations as
observed in a translation/alignment task?
r How do their respective decoding algorithms perform in relation to the
complexity analysis described here?
r How many times is exact decoding achievable in each case?
We will discuss the complexity of both HiPDT and HiFST decoders as well as the
hypergraph representation, with an emphasis on Hiero-style SCFGs. We assess our
analysis for FSA and PDA representations by contrasting HiFST and HiPDT with large
grammars for translation and alignment. For convenience, we refer to the hypergraph
representation as Th, and to the FSA and PDA representations as Tf and Tp.
We first analyze the complexity of each MT step described in the introduction:
1. SCFG Translation: Assuming that the parsing of the input is performed by a
CYK parse, then the CFG, hypergraph, RTN, and PDA representations can
be generated in O(|s|3|G|) time and space (Aho and Ullman 1972). The FSA
representation can require an additional O(e|s|3|G|) time and space because
the RTN expansion to FSA can be exponential.
2. Intersection: The intersection of a CFG Th with a finite automaton M can be
performed by the classical Bar-Hillel algorithm (Bar-Hillel, Perles, and
Shamir 1964) with time and space complexity O(|Th||M|l+1), where l is the
maximum number of symbols on the right-hand side of a grammar rule in
Th. Dyer (2010a) presents a more practical intersection algorithm that avoids
creating rules that are inaccessible from the start symbol. With deterministic
M, the intersection complexity becomes O(|Th||M|lN+1), where lN is the
rank of the SCFG (i.e., lN is the maximum number of nonterminals on the
right-hand side of a grammar rule). With Hiero-styles rules, lN = 2 so the
complexity is O(|Th||M|3) in that case.5 The PDA intersection algorithm
from Section 3.3 has time and space complexity O(|Tp||M|). Finally, the FSA
intersection algorithm has time and space complexity O(|Tf ||M|) (Mohri 2009).
3. Shortest Path: The shortest path algorithm on the hypergraph, RTN, and
FSA representations requires linear time and space (given the underlying
acyclicity) (Huang 2008; Mohri 2009). As presented in Section 3.4, the PDA
representation can require time cubic and space quadratic in |M|.
Table 1 summarizes the complexity results for SCFGs of rank 2. The PDA represen-
tation is equivalent in time and superior in space complexity to the CFG/hypergraph
representation, in general, and it can be superior in both space and time to the FSA
representation depending on the relative SCFG and language model (LM) sizes. The
FSA representation favors smaller target translation grammars and larger language
models.
5 The modified Bar-Hillel construction described by Chiang (2007) has time and space complexity
O(|Th||M|4 ); the modifications were introduced presumably to benefit the subsequent pruning method
employed (but see Huang, Zhong, & Gildea 2005).
706
Allauzen et al. Pushdown Automata in Statistical Machine Translation
Table 1
Translation complexity of target language representations for translation grammars of rank 2.
Representation Time Complexity Space Complexity
CFG/hypergraph O(|s|3 |G| |M|3) O(|s|3 |G| |M|3 )
PDA O(|s|3 |G| |M|3) O(|s|3 |G| |M|2 )
FSA O(e|s|3|G| |M|) O(e|s|3|G| |M|)
In practice, the PDA and FSA representations benefit greatly from the optimiza-
tions mentioned previously (Figure 3 and accompanying discussion). For the FSA
representation, these operations can offset the exponential dependencies in the worst-
case complexity analysis. For example, in a translation of a 15-word sentence taken
at random from the development sets described later, expansion of an RTN yields a
WFSA with 174? 106 states. By contrast, if the RTN is determinized and minimized
prior to expansion, the resulting WFSA has only 34? 103 states. Size reductions of this
magnitude are typical. In general, the original RTN, hypergraph, or CFG representation
can be exponentially larger than the RTN/PDT optimized as described.
Although our interest is primarily in Hiero-style translation grammars, which have
rank 2 and a relatively small number of nonterminals, this complexity analysis can be
extended to other grammars. For SCFGs of arbitrary rank lN, translation complexity in
time for hypergraphs becomes O(|G||s|lN+1|M|lN+1); with FSAs the time complexity be-
comes O(e|G||s|lN+1 |M|); and with PDAs the time complexity becomes O(|G||s|lN+1|M|3).
For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann
and Venugopal 2006) or GHKM (Galley et al. 2004), this suggests that PDA represen-
tations may offer computational advantages in the worst case relative to hypergraph
representations, although this must be balanced against other available strategies such
as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and
Langmead 2010). Of course, practical translation systems introduce various pruning
procedures to achieve much better decoding efficiency than the worst cases given here.
We will next describe the translation grammar and language model for our ex-
periments, which will be used throughout the remainder of this article (except when
stated otherwise). In the following sections we assess the complexity discussion with a
contrast between HiFST (FSA representation) and HiPDT (PDA representation) under
large grammars.
4.1 Translation Grammars and Language Models
Translation grammars are extracted from a subset of the GALE 2008 evaluation par-
allel text;6 this is 2.1M sentences and approximately 45M words per language. We
report translation results on a development set tune-nw (1,755 sentences) and a test set
test-nw (1,671 sentences). These contain translations produced by the GALE program
and portions of the newswire sections of the NIST evaluation setsMT02 throughMT06.7
6 See http://projects.ldc.upenn.edu/gale/data/catalog.html.We excluded the UN material and the
LDC2002E18, LDC2004T08, LDC2007E08, and CUDonga collections.
7 See http://www.itl.nist.gov/iad/mig/tests/mt/.
707
Computational Linguistics Volume 40, Number 3
Table 2
Number of n-grams with explicit conditional probability estimates assigned by the 4-gram
language models M?1 after entropy pruning of M1 at threshold values ?. Perplexities over the
(concatenated) tune-nw reference translations are also reported. The Kneser-Ney and Katz
4-gram LM have 416,190 unigrams, which are not removed by pruning.
? 0 7.5? 10?9 7.5? 10?8 7.5? 10?7 7.5? 10?6 7.5? 10?5 7.5? 10?4 7.5? 10?3
KN
2-grams 28M 10M 2.5M 442K 37K 1.3K 21 0
3-grams 61M 6M 969K 74K 2.7K 38 0 0
4-grams 117M 3M 219K 5K 44 0 0 0
perplexity 98.1 122.2 171.5 290.4 605.1 1270.2 1883.6 2200.0
KATZ
2-grams 28M 7M 2M 391K 52K 4K 117 1
3-grams 64M 10M 1.5M 148K 8.4K 197 1 0
4-grams 117M 4.6M 398K 19K 510 1 0 0
perplexity 106.7 120.4 146.9 210.5 336.6 596.5 905.0 1046.1
In tuning the systems, MERT (Och 2003) iterative parameter estimation under IBM
BLEU8 is performed on the development set.
The parallel corpus is aligned using MTTK (Deng and Byrne 2008) in both source-
to-target and target-to-source directions. We then follow published procedures (Chiang
2007; Iglesias et al. 2009b) to extract hierarchical phrases from the union of the
directional word alignments. We call a translation grammar (G) the set of rules
extracted from this process. For reference, the number of rules in G that can apply to the
tune-nw is 1.1M, of which 593K are standard non-hierarchical phrases and 511K are
strictly hierarchical rules.
We will use two English language models in these translation experiments. The
first language model, denoted M1, is a 4-gram estimated over 1.3B words taken
from the target side of the parallel text and the AFP and Xinhua portions of the
English Gigaword Fourth Edition (LDC2009T13). We use both Kneser-Ney (Kneser
and Ney 1995) and Katz (Katz 1987) smoothing in estimating M1. Where language
model reduction is required, we apply Stolcke entropy pruning (Stolcke 1998) to M1
under the relative perplexity threshold ?. The resulting language model is labeled
as M?1 .
The reduction in size in terms of component n-grams is summarized in Table 2.
For aggressive enough pruning, the original 4-gram model can be effectively reduced
to a trigram, bigram, or unigram model. For both the Katz and the Kneser-Ney 4-gram
language models: at ? = 7.5E? 05 the number of 4-grams in the LM is effectively
reduced to zero; at ? = 7.5E? 4 the number of 3-grams is effectively 0; and at
? = 7.5E? 3, only unigrams remain. Development set perplexities increase as entropy
pruning becomes more aggressive, with the Katz smoothed model performing better
under pruning (Chelba et al. 2010; Roark, Allauzen, and Riley 2013).
We will also use a larger language model, denoted M2, obtained by interpolat-
ing M1 with a zero-cutoff stupid-backoff 5-gram model (Brants et al. 2007) estimated
over 6.6B words of English newswire text; M2 is estimated as needed for the n-grams
required for the test sets.
8 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl.
708
Allauzen et al. Pushdown Automata in Statistical Machine Translation
Table 3
Success in finding the 1-best translation under G with various M?1 under a memory size limit of
10GB as measured over tune-nw (1,755 sentences). We note which operations in translation
exceeded the memory limit: either Expansion and Intersection for HiFST, or Intersection and
Shortest Path operation for HiPDT.
Decoding with G + M?1 under a 10GB memory size limit
# ? HiFST HiPDT
Success Failure Success Failure
Expansion Intersection Intersection Shortest Path
2 7.5? 10?9 12% 51% 37% 40% 8% 52%
3 7.5? 10?8 16% 53% 31% 76% 1% 23%
4 7.5? 10?7 18% 53% 29% 99.8% 0% 0.2%
4.2 Exact Decoding with Large Grammars and Small LanguageModels
We now compare HiFST and HiPDT in translation with our large grammar G. In this
case we know that exact search is often not feasible for HiFST.
We run both decoders over tune-nw with a restriction on memory use of 10 GB.
If this limit is reached in decoding, the process is killed.9 Table 3 shows the number
of times each decoder succeeds in finding a hypothesis under the memory limit when
decoding with various entropy-pruned LMs M?1 . With ?=7.5? 10?9 (row 2), HiFST
can only decode 218 sentences, and HiPDT succeeds in 703 cases. The difference in
success rates between the decoders is more pronounced as the language model is more
aggressively pruned: for ?=7.5? 10?7 HiPDT succeeds for all but three sentences.
As Table 3 shows, HiFST fails most frequently in its initial expansion from RTN
to FSA; this operation depends only on the translation grammar and does not benefit
from any reduction in the language model size. Subsequent intersection of the FSA
with the language model can still pose a challenge, although as the language model
is reduced, this intersection fails less often. By contrast, HiPDT intersects the translation
grammar with the language model prior to expansion and this operation nearly always
finishes successfully. The subsequent shortest path (or pruned expansion) operation is
prone to failure, but the risk of this can be greatly reduced by using smaller language
models.
In the next section we contrast both HiPDT and HiFST for alignment.
4.3 Alignment with Inversion Transduction Grammars
We continue to explore applications characterized by large translation grammars G
and small language models M. As an extreme instance of a problem involving a large
translation grammar and a simple target language model, we consider parallel text
alignment under an Inversion Transduction Grammar (ITG) (Wu 1997). This task, or
something like it, is often done in translation grammar induction. The process should
yield the set of derivations, with scores, that generate the target sentence as a translation
9 We use the UNIX ulimit command. The experiment was carried out over machines with different
configurations and loads, so these numbers should be considered as approximate values.
709
Computational Linguistics Volume 40, Number 3
of the source sentence. In alignment the target language model is extremely simple:
It is simply an acceptor for the target language sentence so that |M| is linear in the
length of the target sentence. In contrast, the search space needs now to be represented
with pushdown transducers (instead of pushdown automata) keeping track of both
translations and derivations, that is, indices of the rules in the grammar (Iglesias et al.
2009a; de Gispert et al. 2010; Dyer 2010b).
We define a word-based translation grammar GITG for the alignment problem as
follows. First, we obtain word-to-word translation rules of the form X??s, t? based
on probabilities from IBM Model 1 translation tables estimated over the parallel text,
where s and t are one source and one target word, respectively (?16M rules). Then,
we allow monotonic and inversion transduction of two adjacent nonterminals in the
usual ITG style (i.e., add X??X1 X2, X1 X2? and X??X1 X2, X2 X1?). Additionally,
we allow unrestricted source word deletions (X??s, ??), and restricted target word
insertions (X??X1 X2, X1 t X2?). This restriction, which is solely motivated by ef-
ficiency reasons, disallows the insertion of two consecutive target words. We make
no claims about the suitability or appropriateness of this specific grammar for either
alignment or translation; we introduce this grammar only to define a challenging
alignment task.
A set of 2,500 sentence pairs of up to 50 source and 75 target words was chosen
for alignment. These sentences come from the same Chinese-to-English parallel data
described in Section 4.1. Hard limits on memory usage (10GB) and processing time
(10 minutes) were imposed for processing each sentence pair. If HiPDT or HiFST ex-
ceeded either limit in aligning any sentence pair, alignment was stopped and a ?mem-
ory/time failure? was noted. Even if the resource limits are not exceeded, alignment
may fail due to limitations in the grammar. This happens when either a particular word
pair rule that is not in our Model 1 table, or more than one consecutive target insertions
are needed to reach alignment. In such cases, we record a ?grammar failure,? as opposed
to a ?memory/time failure.?
Results are reported in Table 4. Of the 2,500 sentence pairs, HiFST successfully
aligns only 41% of the sentence pairs under these time and memory constraints. The
reason for this low success rate is that HiFST must generate and expand all possible
derivations under the ITG for a given sentence pair. Even if it is strictly enforced
that the FSA in every CYK cell contains only partial derivations which produce sub-
strings of the target sentence, expansion often exceeds the memory/time constraints.
In contrast, HiPDT succeeds in aligning all sentence pairs that can be aligned under
the grammar (89%), because it never fails due to memory or time constraints. In this
experiment, if alignment is at all possible, HiPDT will find the best derivation. Align-
ment success rate (or coverage) could trivially be improved by modifying the ITG to
allow more consecutive target insertions, or by increasing the number of word-to-word
Table 4
Percentages of success and failure in aligning 2,500 sentence pairs under GITG with HiFST and
HiPDT. HiPDT finds an alignment whenever it is possible under the translation grammar.
HiFST HiPDT
Success Failure Success Failure
memory/time grammar memory/time grammar
41% 53% 6% 89% 0% 11%
710
Allauzen et al. Pushdown Automata in Statistical Machine Translation
rules, but that would not change the conclusion in the contrast between HiFST and
HiPDT.
The computational analysis from the beginning of this section applies to alignment.
The language model M is replaced by an acceptor for the target sentence, and if we
assume that the target sentence length is proportional to the source sentence length, it
follows that |M| ? |s| and the worst-case complexity for HiPDT in alignment mode is
O(|s|6|G|). This is comparable to ITG alignment (Wu 1997) and the intersection algo-
rithm of Dyer (2010b).
Our experimental results support the complexity analysis summarized in Table 1.
HiPDT is more efficient in ITG alignment and this is consistent with its linear depen-
dence on the grammar size, whereas HiFST suffers from its exponential dependence.
This use of PDAs in alignment does not rely on properties specific either to Hiero
or to ITGs. We expect that the approach should be applicable with other types of
SCFGs, although we note that alignment under SCFGs with an arbitrary number of
nonterminals can be NP-hard (Satta and Peserico 2005).
5. HiPDT Two-Pass Translation Architecture and Experiments
The previous complexity analysis suggests that PDAs should excel when used with
large translation grammars and relatively small n-gram language models. In hierar-
chical phrase-based translation, this is a somewhat unusual scenario: It is far more
typical that translation tasks requiring a large translation grammar also require large
language models. To accommodate these requirements we have developed a two-
pass decoding strategy in which a weak version of a large language model is ap-
plied prior to the expansion of the PDA, after which the full language model is
applied to the resulting WFSA in a rescoring pass. An effective way of generating
weak language models is by means of entropy pruning under a threshold ?; these are
the language models M?1 of Section 4.1. Such a two-pass strategy is widely used in
automatic speech recognition (Ljolje, Pereira, and Riley 1999). The steps in two-pass
translation using entropy-pruned language models are given here, and depicted in
Figure 14.
Step 1. We translate with M?1 and G using the same parameters obtained by MERT
for the baseline system, with the exception that the word penalty parameter
is adjusted to produce hypotheses of roughly the correct length. This produces
translation lattices that contain hypotheses with exact scores under G and M?1 :
?2({s} ? G) ?M?1 .
Step 2. These translation lattices are pruned at beamwidth ?: [?2({s} ? G) ?M?1 ]?.
Step 3. We remove the M?1 scores from the pruned translation lattices, reapply the full
language model M1, and restore the word penalty parameter to the baseline
value obtained by MERT. This gives an approximation to ?2({s} ? G) ?M1:
scores are correctly assigned underG andM1, but only hypotheses that survived
pruning at Step 2 are included.
We can rescore the lattices produced by the baseline system or by the two-pass
system with the larger language model M2. If ?=? or if ?=0, the translation lattices
obtained in Step 3 should be identical to lattices produced by the baseline system (i.e.,
the rescoring step is no longer needed). The aim is to increase ? to shrink the language
model used at Step 1, but ? will then have to increase accordingly to avoid pruning
away desirable hypotheses in Step 2.
711
Computational Linguistics Volume 40, Number 3
CYK parse 
s with G Build RTN
RTN to PDA 
Replacement
Intersect PDA 
with WFSA M1
?
PDA to FSA 
Pruned Expansion,
threshold B
Intersect FSA with LM M1
FSA 
Shortest 
Path
FSA 
Pruning
Lattice1-Best Hypothesis
Remove 
LM scores 
Entropy Pruning, 
threshold ?
LM M1
(as WFSA) 
further rescoring
Figure 14
Two-pass HiPDT translation with an entropy pruned language model.
5.1 Efficient Removal of First-Pass Language Model Scores Using
Lexicographic Semirings
The two-pass translation procedure requires removal of the weak language model
scores used in the initial expansion of the translation search space; this is done so
that only the translation scores under G remain after pruning. In the tropical semiring,
the weak LM scores can be ?subtracted? at the path level from the lattice, but this
involves a determinization of an unweighted translation lattice, which can be very
inefficient.
As an alternative we can define a lexicographic semiring (Shafran et al. 2011;
Roark, Sproat, and Shafran 2011) ?w1,w2? over the tropical weights w1 and w2 with the
operations ? and ?:
?w1,w2? ? ?w3,w4? =
{
?w1,w2? if w1 < w3 or (w1 = w3 and w2 < w4)
?w3,w4? otherwise (8)
?w1,w2? ? ?w3,w4? = ?w1 + w3,w2 + w4? (9)
The PDA algorithms described in Section 3 are valid under this new semiring because
it is commutative and has the path property. In particular, the PDA representing {s} ? G
is constructed so that the translation grammar score appears in both w1 and w2 (i.e., it is
duplicated). In the first-pass language model, w1 has the n-gram language model scores
and the w2 are 0. After composition, the resulting automata have the combined trans-
lation grammar score and language model score in the first dimension, and the second
dimension contains the translation grammar scores alone. Pruning can be performed
under the lexicographic semiring with a threshold set so that only the combined scores
in the first dimension are considered. The resulting automata can easily be mapped back
into the regular tropical semiring such that only the translation scores in the second
712
Allauzen et al. Pushdown Automata in Statistical Machine Translation
dimension are retained (this is a linear operation done by the fstmap operation in the
OpenFST library).
5.2 Translation Quality and Modeling Errors in Two-Pass Decoding
We wish to analyze the degree to which the two-pass decoding strategy introduces
?modeling errors? into translation. A modeling error occurs in two-pass decoding
whenever the decoder produces a translation whose score is less than the best attainable
under the grammar and language model (i.e., whenever the best possible translation
is discarded by pruning at Step 2). We refer to these as modeling errors, rather than
search errors, because they are due to differences in scores assigned by the models
M1 and M?1 .
Ideally, we would compare the two-pass translation system against a baseline sys-
tem that performs exact translation, without pruning in search, under the grammar G
and language model M1. This would allow us to address the following questions:
r Is a two-pass decoding procedure that uses entropy-pruned language
models adequate for translation? How many modeling errors are
introduced? Does two-pass decoding impact on translation quality?
r Which smoothing/discounting technique is best suited for the first-pass
language model in two-pass translation, and which smoothing/
discounting technique is best at avoiding modeling errors?
Our grammar G is not suitable for these experiments, as we do not have a system
capable of exact decoding under both G and M1. To create a suitable baseline we there-
fore reduce G by excluding rules that have a forward translation probability p < 0.01,
and refer to this reduced grammar as Gsmall. This process reduces the number of strictly
hierarchical rules that apply to our tune-nw set from 511K to 189K, while the number of
standard phrases is unchanged.
Under Gsmall, both HiFST and HiPDT are able to exactly compose the entire space of
possible candidate hypotheses with the language model and to extract the shortest path
hypothesis. Because an exact decoding baseline is thus available, we can empirically
evaluate the proposed two-pass strategy. Any degradation in translation quality can
only be due to the modeling errors introduced by pruning under ? with respect to the
entropy-pruned M?1 .
Figure 15 shows translation performance under grammar Gsmall for different values
of entropy pruning threshold ?. Performance is reported after first-pass decoding with
M?1 (Step 1, Section 5), and after rescoring with M1 (Step 3, Section 5) the first-pass
lattices pruned at alternative ? beams. The first column reports the baseline for either
Kneser-Ney andKatz languagemodels, which are found by translation without entropy
pruning, that is, performed with M1. Both yield 34.5 on test-nw.
The first and main conclusion from this figure is that the two-pass strategy is ade-
quate because we are always able to recover the baseline performance. As expected, the
harsher the entropy-pruning ofM1 (as we lower ?) the greater?must be to recover from
the significant degradation in first-pass decoding. But even at a harsh ? = 7.5? 10?7,
when first-pass performance drops over 7 BLEU points, a relatively-low value of ? = 15
can recover the baseline performance.
Although this is true independently of the LM smoothing approach, a second
conclusion from the figure is that the choice of LM smoothing does impact first-pass
713
Computational Linguistics Volume 40, Number 3
Figure 15
Results (lower case IBM BLEU scores over test-nw) under Gsmall with various M?1 as obtained
with several values of ?. Performance in subsequent rescoring with M1 after likelihood-based
pruning of the resulting translation lattices for various ? is also reported. In the pipeline, M1
(and M?1 ) are estimated with either Katz or Kneser-Ney smoothing.
translation performance. For entropy pruning at ? = 7.5? 10?7, the Katz LMs perform
better for smaller beamwidths ?. These results are consistent with the test set
perplexities of the entropy pruned LMs (Table 2), and are also in line with other studies
of Kneser-Ney smoothing and entropy pruning (Chelba et al. 2010; Roark, Allauzen,
and Riley 2013).
Modeling errors are reported in Table 5 at the entropy pruning threshold ? = 7.5?
10?7. As expected, modeling errors decrease as the beamwidth ? increases, although
we find that the language model with Katz smoothing has fewer modeling errors.
However, modeling errors do not necessarily impact corpus level BLEU scores. For wide
beamwidths (e.g., ? = 15 here), there are still some modeling errors, but these are either
few enough or subtle enough that two-pass decoding under either smoothing method
yields the same corpus level BLEU score as the exact decoding baseline.
Table 5
Two-pass translation modeling errors as a function of RTN expansion pruning threshold ?. A
modeling error occurs whenever the score of a hypothesis produced by the two-pass translation
differs from the score found by the exact baseline system. Errors are tabulated over systems
reported in Figure 15, at ? = 7.5? 10?7.
? Kneser-Ney Katz
8 814 619
12 343 212
15 240 110
714
Allauzen et al. Pushdown Automata in Statistical Machine Translation
5.3 HIPDT Two-Pass Decoding Speed and Translation Performance
r What are the speed and quality tradeoffs for HiPDT as a function of
first-pass LM size and translation grammar complexity?
r How do these compare against the predicted computational complexity?
In this section we turn back to the original large grammar, for which HiFST cannot
perform exact decoding (see Table 3). In contrast, HiPDT is able to do exact decoding
so we study tradeoffs in speed and translation performance. The speed of two-pass
decoding can be increased by decreasing ? and/or increasing ?, but at the risk of
degradation in translation performance. For grammar G and language model M1 we
plot in Figure 16 the BLEU score against speed as a function of ? for a selection of ?
values. BLEU score is measured over the entire test set test-nw but speed is calculated
only on sentences of length up to 20 words (?500 sentences). In computing speed we
measure not only the PDA operations, but the entire HiPDT decoding process described
in Figure 14, including CYK parsing and the application of M1. We note in passing that
these unusually slow decoding speeds are a consequence of the large grammars, lan-
guage models, and broad pruning thresholds chosen for these experiments; in practice,
translation with either HiPDT or HiFST is much faster.
In these experiments we find that the language model entropy pruning threshold
? and the likelihood beamwidth ? work together to balance speed against translation
quality. For every entropy pruning threshold ? value considered, there is a value of ?
for which there is no degradation in translation quality. For example, suppose we want
to attain a translation quality of 34.5 BLEU: then ? should be set to 12 or greater. If the
goal is to find the fastest system at this level, then we choose ? = 7.5? 10?5.
The interaction between pruning in expansion and pruning of the language model
is explained by Figure 17, where decoding and rescoring times are shown for various
Figure 16
HiPDT translation quality versus speed (decoding with G, M?1 + rescoring with M1) under
different entropy pruning thresholds ? and for likelihood beamwidths ? = 15, 12, 9, 8, 7.
715
Computational Linguistics Volume 40, Number 3
Figure 17
Accumulated decoding+rescoring times for HiPDT under different entropy pruning thresholds,
reaching a performance of at least 34.5 BLEU, for which ? is set to 12.
values of ? and ? that achieve at least the translation quality target of 34.5. As ?
increases, decoding time decreases because a smaller language model is easier to apply;
however, rescoring times increase, because the larger values of ? lead to larger WFSAs
after expansion, and these are costly to rescore. The balance occurs at ? = 7.5? 10?5
and a translation rate of 3.0 words/sec. In this case, entropy pruning yields a severely
shrunken bigram language model, but this may vary depending on the translation
grammar and the original, unpruned LM.
5.4 Rescoring with 5-Gram Language Models and LMBR Decoding
r Does the HiPDT two-pass decoding generate lattices that can be useful in
rescoring?
We now report on rescoring experiments using WFSAs produced by the two-pass
HiPDT translation system under the large translation grammar G. We demonstrate that
HiPDT can be used to generate large, compact representations of the translation space
that are suitable for rescoring with large language models or by alternative decoding
procedures. We investigate translation performance by applying versions of the lan-
guage model M2 estimated with stupid backoff. We also investigate minimum Bayes
risk (MBR) decoding (Kumar and Byrne 2004) as an alternative search strategy. We are
particularly interested in lattice MBR (LMBR) (Tromble et al. 2008), which is well suited
for the large WFSAs that the system can generate; we use the implementation described
by Blackwood, de Gispert, & Byrne (2010). There are two parameters to be tuned: a
scaling parameter to normalize the evidence scores and a word penalty applied to the
hypotheses space; these are tuned jointly on the tune-nw set. Results are reported in
Figure 18.
We note first that rescoring with the large language model M2, which is effectively
interpolated with M1, gives consistent gains over initial results obtained with M1 alone.
After 5-gram rescoring there is already +0.5 BLEU improvement compared with Gsmall.
With a richer translation grammar we have generated a richer lattice that allows gains
to be gotten by our lattice rescoring techniques.
716
Allauzen et al. Pushdown Automata in Statistical Machine Translation
Figure 18
HiPDT decoding with G. Decoding language model M?1 and first pass rescoring language model
M1 are Katz. Results on test-nw are given for ML-Decoding under the 5-gram stupid backoff
language model (?5gML?) and for LMBR and for LMBR decoding. Parameter values are
? = 15, 12, 9, 8 and ? = 7.5? 10?7 , 7.5? 10?5, 7.5? 10?3.
We also find that BLEU scores degrade smoothly as ? decreases and the expansion
pruning beamwidth narrows, and at all values of ? LMBR gives improvement over
the MAP hypotheses. Because LMBR relies on posterior distributions over n-grams, we
conclude that HiPDT is able to generate compact representations of large search spaces
with posteriors that are robust to pruning conditions.
Finally, we find that increasing ? degrades performance quite smoothly for ? ? 9.
Again, with appropriate choices of ? and ? we can easily reach a compromise between
decoding speed and final performance of our HiPDT system. For instance, with ? =
7.5? 10?7 and? = 12, for whichwe decode at a rate of 3words/sec as seen in Figure 16,
we are losing only 0.5 BLEU after LMBR compared to ? = 7.5? 10?7 and ? = 15.
6. RelatedWork
There is extensive prior work on computational efficiency and algorithmic complexity
in hierarchical phrase-based translation. The challenge is to find algorithms that can be
made to work with large translation grammars and large language models.
Following the original algorithms and analysis of Chiang (2007), Huang and
Chiang (2007) developed the cube-growing algorithm, and more recently Huang and
Mi (2010) developed an incremental decoding approach that exploits the left-to-right
nature of n-gram language models.
Search errors in hierarchical translation, and in translation more generally, have
not been as extensively studied; this is undoubtedly due to the difficulties inherent in
finding exact translations for use in comparison. Using a relatively simple phrase-based
translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an
exact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruning
suffered significant search errors. For Hiero translation, an extensive comparison of
search errors between the cube pruning and FSA implementation was presented by
Iglesias et al. (2009a) and de Gispert et al. (2010). The effect of search errors has also been
717
Computational Linguistics Volume 40, Number 3
studied in phrase-based translation by Zens andNey (2008). Relaxation techniques have
also recently been shown to find exact solutions in parsing (Koo et al. 2010), phrase-
based SMT (Chang and Collins 2011), and in tree-to-string translation under trigram
language models (Rush and Collins 2011); this prior work involved much smaller
grammars and languages models than have been considered here.
Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been
studied previously by Dyer (2010b), who showed that a single synchronous parsing al-
gorithm (Wu 1997) can be significantly improved upon in practice through hypergraph
compositions. We developed similar procedures for our HiFST decoder (Iglesias et al.
2009a; de Gispert et al. 2010) via a different route, after noting that with the space of
translations represented as WFSAs, alignment can be performed using operations over
WFSTs (Kumar and Byrne 2005).
Although entropy-pruned language models have been used to produce real-time
translation systems (Prasad et al. 2007), we believe our use of entropy-pruned language
models in two-pass translation to be novel. This is an approach that is widely used in
automatic speech recognition (Ljolje, Pereira, and Riley 1999) and we note that it relies
on efficient representation of very large search spaces T for subsequent rescoring, as is
possible with FSAs and PDAs.
7. Conclusion
In this article, we have described a novel approach to hierarchical machine translation
using pushdown automata. We have presented fundamental PDA algorithms including
composition, shortest-path, (pruned) expansion, and replacement and have shown how
these can be used in PDA-based machine translation decoding and how this relates to
and compares with hypergraph and FSA-based decoding.
On the basis of the experimental results presented in the previous sections, we can
now address the questions laid out in Sections 4 and 5:
r A two-pass translation decoding procedure in which translation is first
performed with a weak entropy-pruned language model and followed by
admissible likelihood-based pruning and rescoring with a full language
model can yield good quality translations. Translation performance does
not degrade significantly unless the first-pass language model is very
heavily pruned.
r As predicted by the analysis of algorithmic complexity, intersection and
expansion algorithms based on the PDA representation are able to
perform exact decoding with large translation and weak language models.
By contrast, RTN to FSA expansion fails with large translation grammars,
regardless of the size of the language model. With large translation
grammars, language model composition prior to expansion may be more
attractive than expansion prior to language model composition.
r Our experimental results suggest that for a translation grammar and a
language model of a particular size, and given a value of language model
entropy pruning threshold ?, there is a value of the pruned expansion
parameter ? for which there is no degradation in translation quality with
HiPDT. This makes exact decoding under large translation grammars
possible. The values of ? and ? will be grammar- and task-dependent.
718
Allauzen et al. Pushdown Automata in Statistical Machine Translation
r Although there is some interaction between parameter tuning, pruning
thresholds, and language modeling strategies, the variation is not
significant enough to indicate that a particular language model or
smoothing technique is best. This is particularly true if minimum Bayes
risk decoding is applied to the output translation lattices.
Several questions naturally arise about the decoding strategies presented here. One
is whether inadmissible pruning methods can be applied to the PDA-based systems that
are analogous to those used in current hypergraph-based systems such as cube-pruning
(Chiang 2007). Another is whether a hybrid PDA?FSA system, where some parts of the
PDA are pre-expanded and some not, could provide benefits over full pre-expansion
(FSA) or none (PDA). We leave these questions for future work.
Appendix A. Composition of a Weighted PDT and a Weighted FST
Given a pair (T1,T2) where T1 is a weighted pushdown transducer and the T2 is a
weighted finite-state transducer, and such that T1 has input and output alphabets ?
and ? and T2 has input and output alphabets ? and ?, then there exists a weighted
pushdown transducer T1 ? T2, which is the composition of T1 and T2, such that for all
(x, y) ? ?? ? ??:
T = (T1 ? T2)(x, y) = minz???(T1(x, z)+ T2(z, y)) (A.10)
We also assume that T2 has no input-? transitions, noting that for T2 with input-? tran-
sitions, an epsilon filter (Mohri 2009; Allauzen, Riley, and Schalkwyk 2011) generalized
to handle parentheses could be used.
A state in T is a pair (q1, q2) where q1 is a state of T1 and q2 a state of T2. Given a
transition e1 = (q1, a, b,w1, q?1) in T1, transitions out of (q1, q2) in T are obtained using the
following rules. If b ? ?, then e1 can be matched with a transition (q2, b, c,w2, q?2) in T2
resulting in a transition ((q1, q2), a, c,w1 + w2, (q?1, q?2)) in T. If b = ?, then e1 is matched
with staying in q2 resulting in a transition ((q1, q2), a, ?,w1, (q?1, q2)). Finally, if b = a ? ??,
e1 is also matched with staying in q2, resulting in a transition ((q1, q2), a, a,w1, (q?1, q2)) in
T. The initial state is (I1, I2) and a state (q1, q2) in T is final when both q1 and q2 are both
final. Weight values are assigned as ?((q1, q2)) = ?1(q1)+ ?2(q2).
Appendix B. Pruned Expansion
Let dR and BR be the data structures computed by the shortest-distance algorithm
applied to TR. For a state q in T? (or equivalently T??), let d[q] denote the shortest distance
from the initial state to q, d[q] denote the shortest distance from q to the final state, and
s[q] denote the destination state of the last unbalanced open-parenthesis transition on a
shortest path from the initial state to q.
The algorithm is based on the following property: Letting e denote a transition in
T? such that p[e] = (q, z) and z = z?a, the weight of a shortest path through e can be
expressed as:
d[(q, z)]+ w[e]+ min
e??BR[qs ,a]
dR[n[e], p[e?]]+ w[e?]+ d[(n[e?], z?)] (B.11)
719
Computational Linguistics Volume 40, Number 3
PRUNEDEXPANSION(T, ?)
1 (dR,BR)? SHORTESTDISTANCE (TR )
2 ?? dR[I, f ]+? ? Compute the pruning threshold
3 B? REVERSE (BR ) ? Compute the balance information in T from the one in TR
4 (I?, f ? )? ((I,?), ( f,?)) ? I? and f ? are the initial and final states of the pruned expansion
5 (F?,??( f ?))? ({ f ?}, 0)
6 (d[I?], s[I?])? (0, I? )
7 (d[I?], d[ f ?])? (dR[I, f ], 0)
8 (zD,D[ f ])? (?, 0)
9 S? Q?? {I?}
10 while S 6=? do
11 (q, z)? HEAD(S)
12 DEQUEUE (S)
13 if s[(q, z)]= (q, z) then
14 if z 6= zD then ? If the stack has changed, D needs to be cleared and recomputed
15 CLEAR (D)
16 zD? z
17 for each e ? B[q, z|z|] do ? For each close paren. transition balancing the incoming z|z|-labeled open paren. transition in q
18 D[p[e]]? min(D[p[e]],w[e]+ d[(n[e], z1 ? ? ? z|z|?1 )])
19 for each e ? E[q] do
20 if i[e] ? ?? {?} then ? If i[e] is a regular symbol
21 if RETAINPATH (q, z,w[e],n[e]) then
22 E?? E? ? {((q, z), i[e], o[e],w[e], (n[e], z))}
23 elseif i[e] ?? then ? If i[e] is an open parenthesis
24 z?? zi[e]
25 r? false
26 for each e? ? B[n[e], i[e]] do ? For each close paren. transition e? that balances e
27 w? w[e]+ dR[n[e],p[e?]]+w[e?] ? w: weight of the shortest bal. path beginning by e and ending by e? in T
28 r? r? RETAINPATH (q, z,w,n[e?]) ? Does the expansion of that path belong to an accepting path below threshold?
29 wF?min(wF, dR[n[e], p[e?]]+w[e?]+ d[(n[e?], z)])
30 if r then ? If any of the paths considered above are below threshold
31 E?? E? ? {((q, z),?,?,w[e], (n[e], z? ))}
32 PROCESSSTATE ((n[e], z?))
33 s[(n[e], z?)]? (n[e], z? )
34 d[(n[e], z? )]?min(d[(n[e], z? )], d[(q, z)]+w[e])
35 d[(n[e], z? )]?min(d[(n[e], z? )],wF )
36 elseif i[e] ?? and c?(zi[e]) ? ?? then ? If i[e] is the close parenthesis matching the top of the stack
37 z?? c?(zi[e])
38 if d[(q, z)]+w[e]+ d[(n[e], z? )] ? ? then
39 E?? E? ? {((q, z),?,?,w[e], (n[e], z? ))}
40 return (?,?,?,?,Q?,E?, I?,F?,?? )
RETAINPATH(q, z,w, q? )
1 ? Returns true iff a path from (q, z) to (q?, z) with weight w belongs to an accepting path below threshold
2 wI? d[(q, z)]+w ? Shortest distance from I to (q?, z) when taking a path from (q, z) to (q?, z) of weight w
3 wF? min{dR[q?, t]+D[t]|D[t] 6=?}? Current estimate of s. d. from (q?, z) to f ?
4 if wI < d[(q? , z)] then ? If wI is a better estimate of s.-d. from I? to (q?, z), update d[(q?, z)] and s[(q?, z)]
5 d[(q? , z)]? wI
6 s[(q?, z)]? s[(q, z)]
7 if wF < d[(q?, z)] then ? If wF is a better estimate of s. d. from (q?, z) to f ? , update d[(q?, z)]
8 d[(q?, z)]? wF
9 if ? < wI +wF then ? wI +wF: min. weight of an accepting path taking a path of weight w from (q, z) to (q?, z)
10 return false
11 PROCESSSTATE ((q? , z))
12 return true
PROCESSSTATE((q, z))
1 if (q, z) 6? Q? then ? If state (q, z) does not exist yet, create it and add it to the queue
2 Q?? Q? ?{(q, z)}
3 ENQUEUE (S, (q, z))
Figure 19
PDT pruned expansion algorithm. We assume that F={ f} and ?( f )=0 to simplify the
presentation.
720
Allauzen et al. Pushdown Automata in Statistical Machine Translation
where (qs, z) = s[(q, z)]. This implies that assuming when (q, z) is visited, d[(n[e?], z?)] is
known; we then have all the required information for deciding whether e should be
pruned or retained. In order to ensure that each state is visited once, we need to ensure
that d[(q, z)] is known when (q, z) is visited so we can apply an A? queue discipline
among the states sharing the same stack.
Both conditions can be achieved by using a queue discipline defined by a partial
order? such that
z is a prefix of z? ? (q, z) ? (q?, z?) (B.12)
d[(q, z)]+ d[(q, z)] < d[(q?, z)]+ d[(q?, z)]? (q, z) ? (q?, z) (B.13)
We also assume that all states sharing the same stack will be dequeued consecutively
(z 6= z? ? for all (q, q?), (q, z) ? (q?, z?) or for all (q, q?), (q?, z?) ? (q, z)). This allows us to
cache some computations (the D data structure as described subsequently).
The pseudo code of the algorithm is given in Figure 19. First, the shortest distance
algorithm is applied to TR and the absolute pruning threshold is computed accordingly
(lines 1?2). The resulting balanced data information is then reversed (line 3). The initial
and final states are created (lines 4?5) and the d, d, and D data structures are initialized
accordingly (lines 6?8). The default value in these data structures is assumed to be?.
The queue is initialized containing the initial state (line 9).
The state (q, z) at the head of the queue is dequeued (lines 10?12). If (q, z) admits an
incoming open-parenthesis transition, B contains the balance information for that state
and D can be updated accordingly (lines 13?18).
If e is a regular transition, the resulting transition ((q, z), i[e], o[e],w[e], (n[e], z)) in T?
can be pruned using the criterion derived from Equation (B.11). If it is retained, the
transition is created as well as its destination state (n[e], z) if needed (lines 20?22).
If e is an open-parenthesis transition, each balanced path starting by the resulting
transition in T? and ending by a close-parenthesis transition is treated as a meta-
transition and pruned using the same criterion as regular transitions (lines 23?29). If any
of these meta-transitions is retained, the transition ((q, z), ?, ?,w[e], (n[e], zi[e])) resulting
from e is created as well as its destination state (n[e], zi[e]) if needed (lines 30?35).
If e is a closed-parenthesis transition, it is created if it belongs to a balanced path
below the threshold (lines 36?39).
Finally, the resulting transducer T?? is returned (line 40).
Acknowledgments
The research leading to these results has
received funding from the European Union
Seventh Framework Programme
(FP7-ICT-2009-4) under grant agreement
number 247762, and was supported in part
by the GALE program of the Defense
Advanced Research Projects Agency,
contract no. HR0011-06-C-0022, and a
May 2010 Google Faculty Research Award.
References
Aho, Alfred V. and Jeffrey D. Ullman. 1972.
The Theory of Parsing, Translation and
Compiling, volume 1-2. Prentice-Hall.
Allauzen, Cyril and Michael Riley, 2011.
Pushdown Transducers. http://pdt.
openfst.org.
Allauzen, Cyril, Michael Riley, and Johan
Schalkwyk. 2011. Filters for efficient
composition of weighted finite-state
transducers. In Proceedings of CIAA,
volume 6482 of LNCS, pages 28?38. Blois.
Allauzen, Cyril, Michael Riley, Johan
Schalkwyk, Wojciech Skut, and Mehryar
Mohri. 2007. OpenFst: A general and
efficient weighted finite-state transducer
library. In Proceedings of CIAA,
pages 11?23. http://www.openfst.org.
Bar-Hillel, Y., M. Perles, and E. Shamir. 1964.
On formal properties of simple phrase
721
Computational Linguistics Volume 40, Number 3
structure grammars. In Y. Bar-Hillel,
editor, Language and Information: Selected
Essays on their Theory and Application.
Addison-Wesley, pages 116?150.
Berstel, Jean. 1979. Transductions and
Context-Free Languages. Teubner.
Blackwood, Graeme, Adria` de Gispert,
and William Byrne. 2010. Efficient path
counting transducers for minimum
Bayes-risk decoding of statistical machine
translation lattices. In Proceedings of the
ACL: Short Papers, pages 27?32, Uppsala.
Brants, Thorsten, Ashok C. Popat, Peng Xu,
Franz J. Och, and Jeffrey Dean. 2007.
Large language models in machine
translation. In Proceedings of EMNLP-ACL,
pages 858?867, Prague.
Chang, Yin-Wen and Michael Collins. 2011.
Exact decoding of phrase-based translation
models through lagrangian relaxation.
In Proceedings of EMNLP, pages 26?37,
Edinburgh.
Chelba, Ciprian, Thorsten Brants, Will
Neveitt, and Peng Xu. 2010. Study
on interaction between entropy
pruning and Kneser-Ney smoothing.
In Proceedings of Interspeech,
pages 2,242?2,245, Makuhari.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
de Gispert, Adria`, Gonzalo Iglesias, Graeme
Blackwood, Eduardo R. Banga, and
William Byrne. 2010. Hierarchical
phrase-based translation with weighted
finite state transducers and shallow-n
grammars. Computational Linguistics,
36(3):201?228.
Deng, Yonggang and William Byrne. 2008.
HMM word and phrase alignment for
statistical machine translation. IEEE
Transactions on Audio, Speech, and Language
Processing, 16(3):494?507.
Dyer, Chris. 2010a. A Formal Model of
Ambiguity and its Applications in Machine
Translation. Ph.D. thesis, University of
Maryland.
Dyer, Chris. 2010b. Two monolingual parses
are better than one (synchronous parse). In
Proceedings of NAACL-HLT, pages 263?266,
Los Angeles, CA.
Galley, M., M. Hopkins, K. Knight, and
D. Marcu. 2004. What?s in a translation
rule. In Proceedings of HLT-NAACL,
pages 273?280, Boston, MA.
Hopkins, M. and G. Langmead. 2010. SCFG
decoding without binarization. In
Proceedings of EMNLP, pages 646?655,
Cambridge, MA.
Huang, Liang. 2008. Advanced dynamic
programming in semiring and hypergraph
frameworks. In Proceedings of COLING,
pages 1?18, Manchester.
Huang, Liang and David Chiang. 2007.
Forest rescoring: Faster decoding with
integrated language models. In Proceedings
of ACL, pages 144?151, Prague.
Huang, Liang and Haitao Mi. 2010. Efficient
incremental decoding for tree-to-string
translation. In Proceedings of EMNLP,
pages 273?283, Cambridge, MA.
Huang, Liang, Hao Zhang, and Daniel
Gildea. 2005. Machine translation as
lexicalized parsing with hooks. In
Proceedings of the Ninth International
Workshop on Parsing Technology,
Parsing ?05, pages 65?73, Vancouver.
Iglesias, Gonzalo, Adria` de Gispert,
Eduardo R. Banga, and William Byrne.
2009a. Hierarchical phrase-based
translation with weighted finite state
transducers. In Proceedings of NAACL-HLT,
pages 433?441, Boulder, CO.
Iglesias, Gonzalo, Adria` de Gispert,
Eduardo R. Banga, and William Byrne.
2009b. Rule filtering by pattern for efficient
hierarchical translation. In Proceedings of
EACL, pages 380?388, Athens.
Katz, Slava M. 1987. Estimation of
probabilities from sparse data for the
language model component of a speech
recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3):400?401.
Kneser, Reinhard and Herman Ney. 1995.
Improved backing-off for m-gram
language modeling. In Proceedings of
ICASSP, volume 1, pages 181?184,
Detroit, MI.
Koo, Terry, Alexander M. Rush, Michael
Collins, Tommi Jaakkola, and David
Sontag. 2010. Dual decomposition for
parsing with non-projective head
automata. In Proceedings of EMNLP,
pages 1,288?1,298, Cambridge, MA.
Kuich, Werner and Arto Salomaa. 1986.
Semirings, automata, languages. Springer.
Kumar, Shankar and William Byrne.
2004. Minimum Bayes-risk decoding
for statistical machine translation. In
Proceedings of HLT-NAACL, pages 169?176,
Boston, MA.
Kumar, Shankar and William Byrne.
2005. Local phrase reorderingmodels
for statistical machine translation.
In Proceedings of EMNLP-HLT,
pages 161?168, Rochester, NY.
Kumar, Shankar, Yonggang Deng, and
William Byrne. 2006. A weighted finite
722
Allauzen et al. Pushdown Automata in Statistical Machine Translation
state transducer translation template
model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Lang, Bernard. 1974. Deterministic
techniques for efficient non-deterministic
parsers. In Proceedings of ICALP,
pages 255?269, Saarbru?cken.
Ljolje, Andrej, Fernando Pereira, and
Michael Riley. 1999. Efficient general lattice
generation and rescoring. In Proceedings of
Eurospeech, pages 1,251?1,254, Budapest.
Mohri, Mehryar. 2002. Semiring frameworks
and algorithms for shortest-distance
problems. Journal of Automata, Languages
and Combinatorics, 7:321?350.
Mohri, Mehryar. 2009. Weighted automata
algorithms. In M. Drosde, W. Kuick,
and H. Vogler, editors, Handbook of
Weighted Automata. Springer, chapter 6,
pages 213?254.
Nederhof, Mark-Jan and Giorgio Satta. 2003.
Probabilistic parsing as intersection. In
Proceedings of 8th International Workshop on
Parsing Technologies, pages 137?148, Nancy.
Nederhof, Mark-Jan and Giorgio Satta. 2006.
Probabilistic parsing strategies. Journal of
the ACM, 53(3):406?436.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL, pages 160?167,
Sapporo.
Petre, Ion and Arto Salomaa. 2009.
Algebraic systems and pushdown
automata. In M. Drosde, W. Kuick,
and H. Vogler, editors, Handbook of
Weighted Automata. Springer, chapter 7,
pages 257?289.
Prasad, R., K. Krstovski, F. Choi, S. Saleem,
P. Natarajan, M. Decerbo, and D. Stallard.
2007. Real-time speech-to-speech
translation for PDAs. In Proceedings
of IEEE International Conference on
Portable Information Devices, pages 1?5,
Orlando, FL.
Roark, Brian, Cyril Allauzen, and
Michael Riley. 2013. Smoothed marginal
distribution constraints for language
modeling. In Proceedings of ACL,
pages 43?52, Sofia.
Roark, Brian, Richard Sproat, and Izhak
Shafran. 2011. Lexicographic semirings for
exact automata encoding of sequence
models. In Proceedings of ACL-HLT,
pages 1?5, Portland, OR.
Rush, Alexander M. and Michael Collins.
2011. Exact decoding of syntactic
translation models through lagrangian
relaxation. In Proceedings of ACL-HLT,
pages 72?82, Portland, OR.
Satta, Giorgio and Enoch Peserico. 2005.
Some computational complexity results
for synchronous context-free grammars.
In Proceedings of HLT-EMNLP,
pages 803?810, Vancouver.
Shafran, Izhak, Richard Sproat, Mahsa
Yarmohammadi, and Brian Roark.
2011. Efficient determinization of
tagged word lattices using categorial
and lexicographic semirings. In
Proceedings of ASRU, pages 283?288,
Honolulu, HI.
Stolcke, Andreas. 1995. An efficient
probabilistic context-free parsing
algorithm that computes prefix
probabilities. Computational Linguistics,
21(2):165?201.
Stolcke, Andreas. 1998. Entropy-based
pruning of backoff language models.
In Proceedings of DARPA Broadcast
News Transcription and Understanding
Workshop, pages 270?274, Landsdowne,
VA.
Tromble, Roy, Shankar Kumar, Franz J. Och,
and Wolfgang Macherey. 2008. Lattice
minimum Bayes-risk decoding for
statistical machine translation. In
Proceedings of EMNLP, pages 620?629,
Edinburgh.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23:377?403.
Xiao, Tong, Mu Li, Dongdong Zhang,
Jingbo Zhu, and Ming Zhou. 2009. Better
synchronous binarization for machine
translation. In Proceedings of EMNLP,
pages 362?370, Singapore.
Zens, Richard and Hermann Ney. 2008.
Improvements in dynamic programming
beam search for phrase-based statistical
machine translation. In Proceedings of
IWSLT, pages 195?205, Honolulu, HI.
Zhang, Hao, Liang Huang, Daniel Gildea,
and Kevin Knight. 2006. Synchronous
binarization for machine translation. In
Proceedings of HLT-NAACL, pages 256?263,
New York, NY.
Zollmann, Andreas and Ashish Venugopal.
2006. Syntax augmented machine
translation via chart parsing. In Proceedings
of NAACL Workshop on Statistical Machine
Translation, pages 138?141, New York, NY.
723

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 957?965,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Expected Sequence Similarity Maximization
Cyril Allauzen1, Shankar Kumar1, Wolfgang Macherey1, Mehryar Mohri2,1 and Michael Riley1
1Google Research, 76 Ninth Avenue, New York, NY 10011
2Courant Institute of Mathematical Sciences, 251 Mercer Street, New York, NY 10012
Abstract
This paper presents efficient algorithms for
expected similarity maximization, which co-
incides with minimum Bayes decoding for a
similarity-based loss function. Our algorithms
are designed for similarity functions that are
sequence kernels in a general class of posi-
tive definite symmetric kernels. We discuss
both a general algorithm and a more efficient
algorithm applicable in a common unambigu-
ous scenario. We also describe the applica-
tion of our algorithms to machine translation
and report the results of experiments with sev-
eral translation data sets which demonstrate a
substantial speed-up. In particular, our results
show a speed-up by two orders of magnitude
with respect to the original method of Tromble
et al (2008) and by a factor of 3 or more
even with respect to an approximate algorithm
specifically designed for that task. These re-
sults open the path for the exploration of more
appropriate or optimal kernels for the specific
tasks considered.
1 Introduction
The output of many complex natural language pro-
cessing systems such as information extraction,
speech recognition, or machine translation systems
is a probabilistic automaton. Exploiting the full in-
formation provided by this probabilistic automaton
can lead to more accurate results than just using the
one-best sequence.
Different techniques have been explored in the
past to take advantage of the full lattice, some based
on the use of a more complex model applied to
the automaton as in rescoring, others using addi-
tional data or information for reranking the hypothe-
ses represented by the automaton. One method for
using these probabilistic automata that has been suc-
cessful in large-vocabulary speech recognition (Goel
and Byrne, 2000) and machine translation (Kumar
and Byrne, 2004; Tromble et al, 2008) applications
and that requires no additional data or other com-
plex models is the minimum Bayes risk (MBR) de-
coding technique. This returns that sequence of the
automaton having the minimum expected loss with
respect to all sequences accepted by the automaton
(Bickel and Doksum, 2001). Often, minimizing the
loss function L can be equivalently viewed as max-
imizing a similarity function K between sequences,
which corresponds to a kernel function when it is
positive definite symmetric (Berg et al, 1984). The
technique can then be thought of as an expected se-
quence similarity maximization.
This paper considers this expected similarity max-
imization view. Since different similarity functions
can be used within this framework, one may wish to
select the one that is the most appropriate or relevant
to the task considered. However, a crucial require-
ment for this choice to be realistic is to ensure that
for the family of similarity functions considered the
expected similarity maximization is efficiently com-
putable. Thus, we primarily focus on this algorith-
mic problem in this paper, leaving it to future work
to study the question of determining how to select
the similarity function and report on the benefits of
this choice.
A general family of sequence kernels including
the sequence kernels used in computational biology,
text categorization, spoken-dialog classification, and
many other tasks is that of rational kernels (Cortes
et al, 2004). We show how the expected similarity
maximization can be efficiently computed for these
kernels. In section 3, we describe more specifically
the framework of expected similarity maximization
in the case of rational kernels and the correspond-
ing algorithmic problem. In Section 4, we describe
both a general method for the computation of the ex-
pected similarity maximization, and a more efficient
method that can be used with a broad sub-family
of rational kernels that verify a condition of non-
ambiguity. This latter family includes the class of
n-gram kernels which have been previously used to
957
apply MBR to machine translation (Tromble et al,
2008). We examine in more detail the use and ap-
plication of our algorithms to machine translation
in Section 5. Section 6 reports the results of ex-
periments applying our algorithms in several large
data sets in machine translation. These experiments
demonstrate the efficiency of our algorithm which
is shown empirically to be two orders of magnitude
faster than Tromble et al (2008) and more than 3
times faster than even an approximation algorithm
specifically designed for this problem (Kumar et al,
2009). We start with some preliminary definitions
and algorithms related to weighted automata and
transducers, following the definitions and terminol-
ogy of Cortes et al (2004).
2 Preliminaries
Weighted transducers are finite-state transducers in
which each transition carries some weight in addi-
tion to the input and output labels. The weight set
has the structure of a semiring.
A semiring (K,?,?, 0, 1) verifies all the axioms
of a ring except from the existence of a negative el-
ement ?x for each x ? K, which it may verify or
not. Thus, roughly speaking, a semiring is a ring
that may lack negation. It is specified by a set of
values K, two binary operations ? and ?, and two
designated values 0 and 1. When ? is commutative,
the semiring is said to be commutative.
The real semiring (R+,+,?, 0, 1) is used when
the weights represent probabilities. The log
semiring (R ? {??,+?},?log,+,?, 0) is iso-
morphic to the real semiring via the negative-
log mapping and is often used in practice
for numerical stability. The tropical semiring
(R?, {??,+?},min,+,?, 0) is derived from
the log semiring via the Viterbi approximation and
is often used in shortest-path applications.
Figure 1(a) shows an example of a weighted
finite-state transducer over the real semiring
(R+,+,?, 0, 1). In this figure, the input and out-
put labels of a transition are separated by a colon
delimiter and the weight is indicated after the slash
separator. A weighted transducer has a set of initial
states represented in the figure by a bold circle and
a set of final states, represented by double circles. A
path from an initial state to a final state is an accept-
ing path.
The weight of an accepting path is obtained by
first ?-multiplying the weights of its constituent
0                          a:b/1
1
a:b/2
2/1
a:b/4
3/8
b:a/6
b:a/3
b:a/5
0                        b/1
1
b/2
2/1
b/4
3/8
a/6
a/3
a/5
(a) (b)
Figure 1: (a) Example of weighted transducer T over the
real semiring (R+,+,?, 0, 1). (b) Example of weighted
automaton A. A can be obtained from T by projection on
the output and T (aab, bba) = A(bba) = 1? 2? 6? 8+
2? 4? 5? 8.
transitions and?-multiplying this product on the left
by the weight of the initial state of the path (which
equals 1 in our work) and on the right by the weight
of the final state of the path (displayed after the slash
in the figure). The weight associated by a weighted
transducer T to a pair of strings (x, y) ? ?? ??? is
denoted by T (x, y) and is obtained by ?-summing
the weights of all accepting paths with input label x
and output label y.
For any transducer T , T?1 denotes its inverse,
that is the transducer obtained from T by swapping
the input and output labels of each transition. For all
x, y ? ??, we have T?1(x, y) = T (y, x).
The composition of two weighted transducers T1
and T2 with matching input and output alphabets ?,
is a weighted transducer denoted by T1 ? T2 when
the semiring is commutative and the sum:
(T1 ? T2)(x, y) =
?
z???
T1(x, z)? T2(z, y) (1)
is well-defined and in K for all x, y (Salomaa and
Soittola, 1978).
Weighted automata can be defined as weighted
transducers A with identical input and output labels,
for any transition. Since only pairs of the form (x, x)
can have a non-zero weight associated to them by
A, we denote the weight associated by A to (x, x)
by A(x) and call it the weight associated by A to
x. Similarly, in the graph representation of weighted
automata, the output (or input) label is omitted. Fig-
ure 1(b) shows an example of a weighted automa-
ton. When A and B are weighted automata, A ? B
is called the intersection of A and B. Omitting the
input labels of a weighted transducer T results in a
weighted automaton which is said to be the output
projection of T .
958
3 General Framework
Let X be a probabilistic automaton representing the
output of a complex model for a specific query input.
The model may be for example a speech recognition
system, an information extraction system, or a ma-
chine translation system (which originally motivated
our study). For machine translation, the sequences
accepted by X are the potential translations of the
input sentence, each with some probability given by
X .
Let ? be the alphabet for the task considered, e.g.,
words of the target language in machine translation,
and let L : ?? ? ?? ? R denote a loss function
defined over the sequences on that alphabet. Given
a reference or hypothesis set H ? ??, minimum
Bayes risk (MBR) decoding consists of selecting a
hypothesis x ? H with minimum expected loss with
respect to the probability distribution X (Bickel and
Doksum, 2001; Tromble et al, 2008):
x? = argmin
x?H
E
x??X
[L(x, x?)]. (2)
Here, we shall consider the case, frequent in prac-
tice, where minimizing the loss L is equivalent to
maximizing a similarity measure K : ????? ? R.
When K is a sequence kernel that can be repre-
sented by weighted transducers, it is a rational ker-
nel (Cortes et al, 2004). The problem is then equiv-
alent to the following expected similarity maximiza-
tion:
x? = argmax
x?H
E
x??X
[K(x, x?)]. (3)
When K is a positive definite symmetric rational
kernel, it can often be rewritten as K(x, y) = (T ?
T?1)(x, y), where T is a weighted transducer over
the semiring (R+?{+?},+,?, 0, 1). Equation (3)
can then be rewritten as
x? = argmax
x?H
E
x??X
[(T ? T?1)(x, x?)] (4)
= argmax
x?H
?A(x) ? T ? T?1 ?X?, (5)
where we denote by A(x) an automaton accepting
(only) the string x and by ??? the sum of the weights
of all accepted paths of a transducer.
4 Algorithms
4.1 General method
Equation (5) could suggest computing A(x) ? T ?
T?1 ? X for each possible x ? H . Instead, we
can compute a composition based on an automa-
ton accepting all sequences in H , A(H). This leads
to a straightforward method for determining the se-
quence maximizing the expected similarity having
the following steps:
1. compute the composition X ? T , project on
the output and optimize (epsilon-remove, de-
terminize, minimize (Mohri, 2009)) and let Y2
be the result;1
2. compute the composition Y1 = A(H) ? T ;
3. compute Y1 ? Y2 and project on the input, let Z
be the result;2
4. determinize Z;
5. find the maximum weight path with the label of
that path giving x?.
While this method can be efficient in various scenar-
ios, in some instances the weighted determinization
yielding Z can be both space- and time-consuming,
even though the input is acyclic. The next two sec-
tions describe more efficient algorithms.
Note that in practice, for numerical stability, all
of these computations are done in the log semiring
which is isomorphic to (R+?{+?},+,?, 0, 1). In
particular, the maximum weight path in the last step
is then obtained by using a standard single-source
shortest-path algorithm.
4.2 Efficient method for n-gram kernels
A common family of rational kernels is the family
of n-gram kernels. These kernels are widely use as
a similarity measure in natural language processing
and computational biology applications, see (Leslie
et al, 2002; Lodhi et al, 2002) for instance.
The n-gram kernel Kn of order n is defined as
Kn(x, y) =
?
|z|=n
cx(z)cy(z), (6)
where cx(z) is the number of occurrences of z in
x. Kn is a positive definite symmetric rational ker-
nel since it corresponds to the weighted transducer
Tn ? T?1n where the transducer Tn is defined such
that Tn(x, z) = cx(z) for all x, z ? ?? with |z| = n.
1Equivalent to computing T?1 ? X and projecting on the
input.
2Z is then the projection on the input of A(H)?T ?T?1?X .
959
0a:?
b:?
1a:a
b:b
2a:a
b:b
a:?
b:?
0
1a/0.5
2
b/0.5
3b/1
4b/1
5a/1
6a/1
7a/0.4
8
b/0.6
b/1
9/1
b/1
a/1
(a) (b)
0
1a
2
b
3b
4b
5a
6a
7a
8
b
b
9
b
a
0
1a/1
2
b/1 3/1
a/0.2
b/1.5
a/1.8
b/0.5
(c) (d)
?
a/0
a/0
b/0
b/0
a/0.2
b/1.5
a/1.8
b/0.5
0
1a/0
2
b/0
3b/1.5
4b/0.5
5a/1.8
6a/1.8
7a/0.2
8
b/1.5
b/0.5
9/0
b/1.5
a/1.8
(e) (f)
Figure 2: Efficient method for bigram kernel: (a) Counting transducer T2 for ? = {a, b} (over the real semiring). (b)
Probabilistic automaton X (over the real semiring). (c) The hypothesis automaton A(H) (unweighted). (d) Automaton
Y2 representing the expected bigram counts in X (over the real semiring). (e) Automaton Y1: the context dependency
model derived from Y2 (over the tropical semiring). (f) The composition A(H) ? Y1 (over the tropical semiring).
The transducer T2 for ? = {a, b} is shown in Fig-
ure 2(a).
Taking advantage of the special structure of n-
gram kernels and of the fact that A(H) is an un-
weighted automaton, we can devise a new and sig-
nificantly more efficient method for computing x?
based on the following steps.
1. Compute the expected n-gram counts in X: We
compute the composition X ?T , project on out-
put and optimize (epsilon-remove, determinize,
minimize) and let Y2 be the result. Observe that
the weighted automaton Y2 is a compact repre-
sentation of the expected n-gram counts in X ,
i.e. for an n-gram w (i.e. |w| = n):
Y2(w) =
?
x???
X(x)cx(w)
= E
x?X
[cx(w)] = cX(w).
(7)
2. Construct a context-dependency model: We
compute the weighted automaton Y1 over the
tropical semiring as follow: the set of states is
Q = {w ? ??| |w| ? n and w occurs in X},
the initial state being ? and every state being fi-
nal; the set of transitions E contains all 4-tuple
(origin, label, weight, destination) of the form:
? (w, a, 0, wa) with wa ? Q and |w| ? n?
2 and
? (aw, b, Y2(awb), wb) with Y2(awb) 6= 0
and |w| = n? 2
where a, b ? ? and w ? ??. Observe that
w ? Q when wa ? Q and that aw,wb ? Q
when Y2(awb) 6= 0. Given a string x, we have
Y1(x) =
?
|w|=n
cX(w)cx(w). (8)
Observe that Y1 is a deterministic automaton,
hence Y1(x) can be computed in O(|x|) time.
3. Compute x?: We compute the composition
A(H) ? Y1. x? is then the label of the accepting
path with the largest weight in this transducer
and can be obtained by applying a shortest-path
algorithm to ?A(H) ? Y1 in the tropical semir-
ing.
The main computational advantage of this method
is that it avoids the determinization of Z in the
960
0 1a/1 2/1a/c1
b/c2
0 1a
2/c1
a
3/c2
b
0
b
1a
2/c1
a
3/c2
b
b
a
b
a
0
b/0
1a/0
2
a/0
3
b/0
2?/0b/0 a/0
?      /c1
3?/0?      /c2
b/0
a/0
(a) (b) (c) (d)
Figure 3: Illustration of the construction of Y1 in the unambiguous case. (a) Weighted automaton Y2 (over the real
semiring). (b) Deterministic tree automaton Y ?2 accepting {aa, ab} (over the tropical semiring). (c) Result of deter-
minization of ??Y ?2 (over the tropical semiring). (d) Weighted automaton Y1 (over the tropical semiring).
(+,?) semiring, which can sometimes be costly.
The method has also been shown empirically to be
significantly faster than the one described in the pre-
vious section.
The algorithm is illustrated in Figure 2. The al-
phabet is ? = {a, b} and the counting transducer
corresponding to the bigram kernel is given in Fig-
ure 2(a). The evidence probabilistic automaton X
is given in Figure 2(b) and we use as hypothesis
set the set of strings that were assigned a non-zero
probability by X; this set is represented by the deter-
ministic finite automaton A(H) given in Figure 2(c).
The result of step 1 of the algorithm is the weighted
automaton Y2 over the real semiring given in Fig-
ure 2(d). The result of step 2 is the weighted au-
tomaton Y1 over the tropical semiring is given in
Figure 2(e). Finally, the result of the composition
A(H) ? Y1 (step 3) is the weighted automaton over
the tropical semiring given in Figure 2(f). The re-
sult of the expected similarity maximization is the
string x? = ababa, which is obtained by applying
a shortest-path algorithm to ?A(H) ? Y1. Observe
that the string x with the largest probability in X is
x = bbaba and is hence different from x? = ababa in
this example.
4.3 Efficient method for the unambiguous case
The algorithm presented in the previous section for
n-gram kernels can be generalized to handle a wide
variety of rational kernels.
Let K be an arbitrary rational kernel defined by a
weighted transducer T . Let XT denote the regular
language of the strings output by T . We shall as-
sume that XT is a finite language, though the results
of this section generalize to the infinite case. Let
? denote a new alphabet defined by ? = {#x : x ?
XT } and consider the simple grammar G of context-
dependent batch rules:
? ? #x/x ?. (9)
Each such rule inserts the symbol #x immediately
after an occurrence x in the input string. For batch
context-dependent rules, the context of the applica-
tion for all rules is determined at once before their
application (Kaplan and Kay, 1994). Assume that
this grammar is unambiguous for a parallel applica-
tion of the rules. This condition means that there is
a unique way of parsing an input string using the
strings of XT . The assumption holds for n-gram
sequences, for example, since the rules applicable
are uniquely determined by the n-grams (making the
previous section a special case).
Given an acyclic weighted automaton Y2 over the
tropical semiring accepting a subset of XT , we can
construct a deterministic weighted automaton Y1 for
??L(Y2) when this grammar is unambiguous. The
weight assigned by Y1 to an input string is then the
sum of the weights of the substrings accepted by Y2.
This can be achieved using weighted determiniza-
tion.
This suggests a new method for generalizing Step
2 of the algorithm described in the previous section
as follows (see illustration in Figure 3):
(i) use Y2 to construct a deterministic weighted
tree Y ?2 defined on the tropical semiring ac-
cepting the same strings as Y2 with the same
weights, with the final weights equal to the to-
tal weight given by Y2 to the string ending at
that leaf;
(ii) let Y1 be the weighted automaton obtained by
first adding self-loops labeled with all elements
of ? at the initial state of Y ?2 and then deter-
minizing it, and then inserting new transitions
leaving final states as described in (Mohri and
Sproat, 1996).
961
Step (ii) consists of computing a deterministic
weighted automaton for ??Y ?2 . This step corre-
sponds to the Aho-Corasick construction (Aho and
Corasick, 1975) and can be done in time linear in
the size of Y ?2 .
This approach assumes that the grammar G of
batch context-dependent rules inferred by XT is un-
ambiguous. This can be tested by constructing the
finite automaton corresponding to all rules in G. The
grammar G is unambiguous iff the resulting automa-
ton is unambiguous (which can be tested using a
classical algorithm). An alternative and more ef-
ficient test consists of checking the presence of a
failure or default transition to a final state during
the Aho-Corasick construction, which occurs if and
only if there is ambiguity.
5 Application to Machine Translation
In machine translation, the BLEU score (Papineni et
al., 2001) is typically used as an evaluation metric.
In (Tromble et al, 2008), a Minimum Bayes-Risk
decoding approach for MT lattices was introduced.3
The loss function used in that approach was an ap-
proximation of the log-BLEU score by a linear func-
tion of n-gram matches and candidate length. This
loss function corresponds to the following similarity
measure:
KLB(x, x?) = ?0|x?|+
?
|w|?n
?|w|cx(w)1x?(w).
(10)
where 1x(w) is 1 if w occurs in x and 0 otherwise.
(Tromble et al, 2008) implements the MBR de-
coder using weighted automata operations. First,
the set of n-grams is extracted from the lat-
tice. Next, the posterior probability p(w|X) of
each n-gram is computed. Starting with the un-
weighted lattice A(H), the contribution of each n-
gram w to (10) is applied by iteratively compos-
ing with the weighted automaton corresponding to
w(w/(?|w|p(w|X))w)? where w = ?? \ (??w??).
Finally, the MBR hypothesis is extracted as the best
path in the automaton. The above steps are carried
out one n-gram at a time. For a moderately large lat-
tice, there can be several thousands of n-grams and
the procedure becomes expensive. This leads us to
investigate methods that do not require processing
the n-grams one at a time in order to achieve greater
efficiency.
3Related approaches were presented in (DeNero et al, 2009;
Kumar et al, 2009; Li et al, 2009).
0
1?:?
2
?:?
b:?
3
a:a
a:? b:b
a:?
b:?
Figure 4: Transducer T 1 over the real semiring for the
alphabet {a, b}.
The first idea is to approximate the KLB similar-
ity measure using a weighted sum of n-gram ker-
nels. This corresponds to approximating 1x?(w) by
cx?(w) in (10). This leads us to the following simi-
larity measure:
KNG(x, x?) = ?0|x?|+
?
|w|?n
?|w|cx(w)cx?(w)
= ?0|x?|+
?
1?i?n
?iKi(x, x?)
(11)
Intuitively, the larger the length of w the less likely
it is that cx(w) 6= 1x(w), which suggests comput-
ing the contribution to KLB(x, x?) of lower-order
n-grams (|w| ? k) exactly, but using the approxima-
tion by n-gram kernels for the higher-order n-grams
(|w| > k). This gives the following similarity mea-
sure:
KkNG(x, x?) = ?0|x?|+
?
1?|w|?k
?|w|cx(w)1x?(w)
+
?
k<|w|?n
?|w|cx(w)cx?(w)
(12)
Observe that K0NG = KNG and KnNG = KLB .
All these similarity measures can still be com-
puted using the framework described in Section 4.
Indeed, there exists a transducer Tn over the real
semiring such that Tn(x, z) = 1x(z) for all x ? ??
and z ? ?n. The transducer T 1 for ? = {a, b} is
given by Figure 4. Let us define the similarity mea-
sure Kn as:
Kn(x, x?) = (Tn?T?1n )(x, x?) =
?
|w|=n
cx(w)1x?(w).
(13)
Observe that the framework described in Section 4
can still be applied even though Kn is not symmet-
ric. The similarity measures KLB , KNG and KkNG
962
zhen aren
nist02 nist04 nist05 nist06 nist08 nist02 nist04 nist05 nist06 nist08
no mbr 38.7 39.2 38.3 33.5 26.5 64.0 51.8 57.3 45.5 43.8
exact 37.0 39.2 38.6 34.3 27.5 65.2 51.4 58.1 45.2 45.0
approx 39.0 39.9 38.6 34.4 27.4 65.2 52.5 58.1 46.2 45.0
ngram 36.6 39.1 38.1 34.4 27.7 64.3 50.1 56.7 44.1 42.8
ngram1 37.1 39.2 38.5 34.4 27.5 65.2 51.4 58.0 45.2 44.8
Table 1: BLEU score (%)
zhen aren
nist02 nist04 nist05 nist06 nist08 nist02 nist04 nist05 nist06 nist08
exact 3560 7863 5553 6313 5738 12341 23266 11152 11417 11405
approx 168 422 279 335 328 504 1296 528 619 808
ngram 28 72 34 70 43 85 368 105 63 66
ngram1 58 175 96 99 89 368 943 308 167 191
Table 2: MBR Time (in seconds)
can then be expressed as the relevant linear combi-
nation of Ki and Ki.
6 Experimental Results
Lattices were generated using a phrase-based MT
system similar to the alignment template system de-
scribed in (Och and Ney, 2004). Given a source sen-
tence, the system produces a word lattice A that is a
compact representation of a very large N -best list of
translation hypotheses for that source sentence and
their likelihoods. The lattice A is converted into a
lattice X that represents a probability distribution
(i.e. the posterior probability distribution given the
source sentence) following:
X(x) = exp(?A(x))?
y??? exp(?A(y))
(14)
where the scaling factor ? ? [0,?) flattens the dis-
tribution when ? < 1 and sharpens it when ? > 1.
We then applied the methods described in Section 5
to the lattice X using as hypothesis set H the un-
weighted lattice obtained from X .
The following parameters for the n-gram factors
were used:
?0 =
?1
T and ?n =
1
4Tprn?1 for n ? 1. (15)
Experiments were conducted on two language
pairs Arabic-English (aren) and Chinese-English
(zhen) and for a variety of datasets from the NIST
Open Machine Translation (OpenMT) Evaluation.4
The values of ?, p and r used for each pair are given
4http://www.nist.gov/speech/tests/mt
? p r
aren 0.2 0.85 0.72
zhen 0.1 0.80 0.62
Table 3: Parameters used for performing MBR.
in Table 3. We used the IBM implementation of the
BLEU score (Papineni et al, 2001).
We implemented the following methods using the
OpenFst library (Allauzen et al, 2007):
? exact: uses the similarity measure KLB based
on the linearized log-BLEU, implemented as
described in (Tromble et al, 2008);
? approx: uses the approximation to KLB from
(Kumar et al, 2009) and described in the ap-
pendix;
? ngram: uses the similarity measure KNG im-
plemented using the algorithm of Section 4.2;
? ngram1: uses the similarity measure K1NG
also implemented using the algorithm of Sec-
tion 4.2.
The results from Tables 1-2 show that ngram1
performs as well as exact on all datasets5 while be-
ing two orders of magnitude faster than exact and
overall more than 3 times faster than approx.
7 Conclusion
We showed that for broad families of transducers
T and thus rational kernels, the expected similar-
5We consider BLEU score differences of less than 0.4% not
significant (Koehn, 2004).
963
ity maximization problem can be solved efficiently.
This opens up the option of seeking the most appro-
priate rational kernel or transducer T for the spe-
cific task considered. In particular, the kernel K
used in our machine translation applications might
not be optimal. One may well imagine for exam-
ple that some n-grams should be further emphasized
and others de-emphasized in the definition of the
similarity. This can be easily accommodated in the
framework of rational kernels by modifying the tran-
sition weights of T . But, ideally, one would wish
to select those weights in an optimal fashion. As
mentioned earlier, we leave this question to future
work. However, we can offer a brief look at how
one could tackle this question. One method for de-
termining an optimal kernel for the expected sim-
ilarity maximization problem consists of solving a
problem similar to that of learning kernels in classi-
fication or regression. Let X1, . . . , Xm be m lattices
with Ref(X1), . . . ,Ref(Xm) the associated refer-
ences and let x?(K,Xi) be the solution of the ex-
pected similarity maximization for lattice Xi when
using kernel K. Then, the kernel learning optimiza-
tion problem can be formulated as follows:
min
K?K
1
m
m?
i=1
L(x?(K,Xi),Ref(Xi))
s. t. K = T ? T?1 ? Tr[K] ? C,
where K is a convex family of rational kernels and
Tr[K] denotes the trace of the kernel matrix. In
particular, we could choose K as a family of linear
combinations of base rational kernels. Techniques
and ideas similar to those discussed by Cortes et al
(2008) for learning sequence kernels could be di-
rectly relevant to this problem.
A Appendix
We describe here the approximation of the KLB
similarity measure from Kumar et al (2009). We
assume in this section that the lattice X is determin-
istic in order to simplify the notations. The posterior
probability of n-gram w in the lattice X can be for-
mulated as:
p(w|X) =
?
x???
1x(w)P (x|s) =
?
x???
1x(w)X(x)
(16)
where s denotes the source sentence. When using
the similarity measure KLB defined Equation (10),
Equation (3) can then be reformulated as:
x? = argmax
x??H
?0|x?|+
?
w
?|w|cx?(w)p(w|X). (17)
The key idea behind this new approximation algo-
rithm is to rewrite the n-gram posterior probability
(Equation 16) as follows:
p(w|X) =
?
x???
?
e?EX
f(e, w, ?x)X(x) (18)
where EX is the set of transitions of X , ?x is
the unique accepting path labeled by x in X and
f(e, w, ?) is a score assigned to transition e on path
? containing n-gram w:
f(e, w, ?) =
?
?
?
1 if w ? e, p(e|X) > p(e?|X),
and e? precedes e on ?
0 otherwise.
(19)
In other words, for each path ?, we count the tran-
sition that contributes n-gram w and has the highest
transition posterior probability relative to its prede-
cessors on the path ?; there is exactly one such tran-
sition on each lattice path ?.
We note that f(e, w, ?) relies on the full path ?
which means that it cannot be computed based on
local statistics. We therefore approximate the quan-
tity f(e, w, ?) with f?(e, w,X) that counts the tran-
sition e with n-gram w that has the highest arc poste-
rior probability relative to predecessors in the entire
lattice X . f?(e, w,X) can be computed locally, and
the n-gram posterior probability based on f? can be
determined as follows:
p(w|G) =
?
x???
?
e?EX
f?(e, w,X)X(x)
=
?
e?Ex
1w?ef?(e, w,X)
?
x???
1pix(e)X(x)
=
?
e?EX
1w?ef?(e, w,X)P (e|X),
(20)
where P (e|X) is the posterior probability of a lat-
tice transition e ? EX . The algorithm to perform
Lattice MBR is given in Algorithm 1. For each state
t in the lattice, we maintain a quantity Score(w, t)
for each n-gram w that lies on a path from the initial
state to t. Score(w, t) is the highest posterior prob-
ability among all transitions on the paths that termi-
nate on t and contain n-gram w. The forward pass
requires computing the n-grams introduced by each
transition; to do this, we propagate n-grams (up to
maximum order ?1) terminating on each state.
964
Algorithm 1 MBR Decoding on Lattices
1: Sort the lattice states topologically.
2: Compute backward probabilities of each state.
3: Compute posterior prob. of each n-gram:
4: for each transition e do
5: Compute transition posterior probability P (e|X).
6: Compute n-gram posterior probs. P (w|X):
7: for each n-gram w introduced by e do
8: Propagate n? 1 gram suffix to he.
9: if p(e|X) > Score(w, T (e)) then
10: Update posterior probs. and scores:
p(w|X) += p(e|X) ? Score(w, T (e)).
Score(w, he) = p(e|X).
11: else
12: Score(w, he) = Score(w, T (e)).
13: end if
14: end for
15: end for
16: Assign scores to transitions (given by Equation 17).
17: Find best path in the lattice (Equation 17).
References
Alfred V. Aho and Margaret J. Corasick. 1975. Efficient
String Matching: An Aid to Bibliographic Search.
Communications of the ACM, 18(6):333?340.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: a
general and efficient weighted finite-state transducer
library. In CIAA 2007, volume 4783 of LNCS, pages
11?23. Springer. http://www.openfst.org.
Christian Berg, Jens Peter Reus Christensen, and Paul
Ressel. 1984. Harmonic Analysis on Semigroups.
Springer-Verlag: Berlin-New York.
Peter J. Bickel and Kjell A. Doksum. 2001. Mathemati-
cal Statistics, vol. I. Prentice Hall.
Corinna Cortes, Patrick Haffner, and Mehryar Mohri.
2004. Rational Kernels: Theory and Algorithms.
Journal of Machine Learning Research, 5:1035?1062.
Corinna Cortes, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2008. Learning sequence kernels. In Pro-
ceedings of MLSP 2008, October.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of ACL and IJCNLP, pages 567?575.
Vaibhava Goel and William J. Byrne. 2000. Minimum
Bayes-risk automatic speech recognition. Computer
Speech and Language, 14(2):115?135.
Ronald M. Kaplan and Martin Kay. 1994. Regular mod-
els of phonological rule systems. Computational Lin-
guistics, 20(3).
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In EMNLP,
Barcelona, Spain.
Shankar Kumar and William J. Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL, Boston, MA, USA.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Asso-
ciation for Computational Linguistics and IJCNLP.
Christina S. Leslie, Eleazar Eskin, and William Stafford
Noble. 2002. The Spectrum Kernel: A String Kernel
for SVM Protein Classification. In Pacific Symposium
on Biocomputing, pages 566?575.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proceedings of ACL and IJCNLP, pages 593?
601.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watskins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?44.
Mehryar Mohri and Richard Sproat. 1996. An Efficient
Compiler for Weighted Rewrite Rules. In Proceedings
of ACL ?96, Santa Cruz, California.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Manfred Droste, Werner Kuich, and Heiko Vogler,
editors, Handbook of Weighted Automata, chapter 6,
pages 213?254. Springer.
Franz J. Och and Hermann Ney. 2004. The align-
ment template approach to statistical mchine transla-
tion. Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Report
RC22176 (W0109-022), IBM Research Division.
Arto Salomaa and Matti Soittola. 1978. Automata-
Theoretic Aspects of Formal Power Series. Springer.
Roy W. Tromble, Shankar Kumar, Franz J. Och, and
Wolfgang Macherey. 2008. Lattice minimum Bayes-
risk decoding for statistical machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 620?
629.
965
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 61?66,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
The OpenGrm open-source finite-state grammar software libraries
Brian Roark? Richard Sproat?? Cyril Allauzen? Michael Riley? Jeffrey Sorensen? & Terry Tai?
?Oregon Health & Science University, Portland, Oregon ?Google, Inc., New York
Abstract
In this paper, we present a new collection
of open-source software libraries that pro-
vides command line binary utilities and library
classes and functions for compiling regular
expression and context-sensitive rewrite rules
into finite-state transducers, and for n-gram
language modeling. The OpenGrm libraries
use the OpenFst library to provide an efficient
encoding of grammars and general algorithms
for building, modifying and applying models.
1 Introduction
The OpenGrm libraries1 are a (growing) collec-
tion of open-source software libraries for build-
ing and applying various kinds of formal gram-
mars. The C++ libraries use the OpenFst library2
for the underlying finite-state representation, which
allows for easy inspection of the resulting grammars
and models, as well as straightforward combination
with other finite-state transducers. Like OpenFst,
there are easy-to-use command line binaries for fre-
quently used operations, as well as a C++ library
interface, allowing library users to create their own
algorithms from the basic classes and functions pro-
vided.
The libraries can be used for a range of com-
mon string processing tasks, such as text normal-
ization, as well as for building and using large sta-
tistical models for applications like speech recogni-
tion. In the rest of the paper, we will present each of
the two libraries, starting with the Thrax grammar
compiler and then the NGram library. First, though,
we will briefly present some preliminary (infor-
mal) background on weighted finite-state transduc-
ers (WFST), just as needed for this paper.
1http://www.opengrm.org/
2http://www.openfst.org/
2 Informal WFST preliminaries
A weighted finite-state transducer consists of a set
of states and transitions between states. There is an
initial state and a subset of states are final. Each tran-
sition is labeled with an input symbol from an input
alphabet; an output symbol from an output alpha-
bet; an origin state; a destination state; and a weight.
Each final state has an associated final weight. A
path in the WFST is a sequence of transitions where
each transition?s destination state is the next transi-
tion?s origin state. A valid path through the WFST is
a path where the origin state of the first transition is
an initial state, and the the last transition is to a final
state. Weights combine along the path according to
the semiring of the WFST.
If every transition in the transducer has the same
input and output symbol, then the WFST represents
a weighted finite-state automaton. In the OpenFst
library, there are a small number of special sym-
bols that can be used. The  symbol represents the
empty string, which allows the transition to be tra-
versed without consuming any symbol. The ? (or
failure) symbol on a transition also allows it to be
traversed without consuming any symbol, but it dif-
fers from  in only allowing traversal if the symbol
being matched does not label any other transition
leaving the same state, i.e., it encodes the semantics
of otherwise, which is useful for language models.
For a more detailed presentation of WFSTs, see Al-
lauzen et al (2007).
3 The Thrax Grammar Compiler
The Thrax grammar compiler3 compiles grammars
that consist of regular expressions, and context-
dependent rewrite rules, into FST archives (fars) of
weighted finite state transducers. Grammars may
3The compiler is named after Dionysius Thrax (170?
90BCE), the reputed first Greek grammarian.
61
be split over multiple files and imported into other
grammars. Strings in the rules may be parsed
in one of three different ways: as a sequence of
bytes (the default), as utf8 encodings, or accord-
ing to a user-provided symbol table. With the
--save symbols flag, the transducers can be
saved out into fars with appropriate symbol tables.
The Thrax libraries provide full support for dif-
ferent weight (semiring) classes. The command-line
flag --semiring allows one to set the semiring,
currently to one of: tropical (default), log or log64
semirings.
3.1 General Description
Thrax revolves around rules which, typically, con-
struct an FST based on a given input. In the simplest
case, we can just provide a string that represents a
(trivial) transducer and name it using the assignment
operator:
pear = "pear";
In this example, we have an FST consisting of the
characters ?p?, ?e?, ?a?, and ?r? in a chain, assigned
to the identifier pear:
This identifier can be used later in order to build
further FSTs, using built-in operators or using cus-
tom functions:
kiwi = "kiwi";
fruits = pear | kiwi; # union
In Thrax, string FSTs are enclosed by double-quotes
(") whereas simple strings (often used as pathnames
for functions) are enclosed in single-quotes (?).
Thrax provides a set of built-in functions that
aid in the construction of more complex expres-
sions. We have already seen the disjunction ?|? in
the previous example. Other standard regular op-
erations are expr*, expr+, expr? and expr{m,n},
the latter repeating expr between m and n times,
inclusive. Composition is notated with ?@? so
that expr1 @ expr2 denotes the composition of
expr1 and expr2. Rewriting is denoted with ?:?
where expr1 : expr2 rewrites strings that match
expr1 into expr2. Weights can be added to expres-
sions using the notation ?<>?: thus, expr<2.4>
adds weight 2.4 to expr. Various operations on
FSTs are also provided by built-in functions, includ-
ing Determinize, Minimize, Optimize and
Invert, among many others.
3.2 Detailed Description
A Thrax grammar consists of a set of one or more
source files, each of which must have the extension
.grm. The compiler compiles each source file to a
single FST archive with the extension .far. Each
grammar file has sections: Imports and Body, each
of which is optional. The body section can include
statements interleaved with functions, as specified
below. Comments begin with a single pound sign
(#) and last until the next newline.
3.2.1 Imports
The Thrax compiler compiles source files (with
the extension .grm) into FST archive files (with
the extension .far). FST archives are an Open-
Fst storage format for a series of one or more FSTs.
The FST archive and the original source file then
form a pair which can be imported into other source
files, allowing a Python-esque include system that is
hopefully familiar to many. Instead of working with
a monolithic file, Thrax allows for a modular con-
struction of the final rule set as well as sharing of
common elements across projects.
3.2.2 Functions
Thrax has extensive support for functions that can
greatly augment the capabilities of the language.
Functions in Thrax can be specified in two ways.
The first is inline via the func keyword within grm
files. These functions can take any number of input
arguments and must return a single result (usually an
FST) to the caller via the return keyword:
func DumbPluralize[fst] {
# Concatenate with "s"...
result = fst "s";
# ...and then return to caller.
return result;
}
Alternatively, functions can be written C++ and
added to the language. Regardless of the func-
tion implementation method (inline in Thrax or
subclassed in C++), functions are integrated into
the Thrax environment and can be called directly
by using the function name and providing the
necessary arguments. Thus, assuming someone has
written a function called NetworkPluralize
that retrieves the plural of a word from some web-
site, one could write a grammar fragment as follows:
62
apple = "apple";
plural_apple = DumbPluralize[apple];
plural_tomato = NetworkPluralize[
"tomato",
?http://server:port/...?];
3.2.3 Statements
Functions can be interleaved with grammar state-
ments that generate the FSTs that are exported to the
FST archive as output. Each statement consists of an
assignment terminating with a semicolon:
foo = "abc";
export bar = foo | "xyz";
Statements preceded with the export keyword will
be written to the final output archive. Statements
lacking this keyword define temporaries that be used
later, but are themselves not output.
The basic elements of any grammar are string
FSTs, which, as mentioned earlier, are defined by
text enclosed by double quotes ("), in contrast to
raw strings, which are enclosed by single quotes (?).
String FSTs can be parsed in one of three ways,
which are denoted using a dot (.) followed by ei-
ther byte, utf8, or an identifier holding a symbol ta-
ble. Note that within strings, the backslash character
(\) is used to escape the next character. Of partic-
ular note, ?\n? translates into a newline, ?\r? into
a line feed, and ?\t? into the tab character. Literal
left and right square brackets also need escaping, as
they are used to generate symbols (see below). All
other characters following the backslash are unin-
terpreted, so that we can use \? and \? to insert an
actual quote (double) quote symbol instead of termi-
nating the string.
Strings, by default, are interpreted as sequences
of bytes, each transition of the resulting FST
corresponding to a single 1-byte character of the
input. This can be specified either by leaving off the
parse mode ("abc") or by explicitly using the byte
mode ("abc".byte). The second way is to use
UTF8 parsing by using the special keyword, e.g.:
Finally, we can load a symbol table and split
the string using the fst field separator flag
(found in fst/src/lib/symbol-table.cc)
and then perform symbol table lookups. Symbol ta-
bles can be loaded using the SymbolTable built-in
function:
arctic_symbol_table =
SymbolTable[?/path/to/bears.symtab?];
pb = "polar bear".arctic_symbol_table;
One can also create temporary symbols on the
fly by enclosing a symbol name inside brackets
within an FST string. All of the text inside the
brackets will be taken to be part of the symbol
name, and future encounters of the same symbol
name will map to the same label. By default, la-
bels use ?Private Use Area B? of the unicode ta-
ble (0x100000 - 0x10FFFD), except that the last two
code points 0x10FFFC and 0x10FFFD are reserved
for the ?[BOS]? and ?[EOS]? tags discussed below.
cross_pos = "cross" ("" : "_[s_noun]");
pluralize_nouns = "_[s_noun]" : "es";
3.3 Standard Library Functions and
Operations
Built-in functions are provided that operate on FSTs
and perform most of the operations that are available
in the OpenFst library. These include: closure, con-
catenation, difference, composition and union. In
most cases the notation of these functions follows
standard conventions. Thus, for example, for clo-
sure, the following syntax applies: fst* (accepts fst
0 or more times); fst+ (accepts fst 1 or more times);
fst? (accepts fst 0 or 1 times) fst{x,y} (accepts fst at
least x but no more than y times).
The operator ?@? is used for composition: a @
b denotes a composed with b. A ?:? is used to de-
note rewrite, where a : b denotes a transducer
that deletes a and inserts b. Most functions can also
be expressed using functional notation:
b = Rewrite["abc", "def"];
The delimiters< and> add a weight to an expres-
sion in the chosen semiring: a<3> adds the weight
3 (in the tropical semiring by default) to a.
Functions lacking operators (hence only called
by function name) include: ArcSort, Connect,
Determinize, RmEpsilon, Minimize,
Optimize, Invert, Project and Reverse.
Most of these call the obvious underlying OpenFst
function.
One function in particular, CDRewrite is worth
further discussion. This function takes a transducer
and two context acceptors (and the alphabet ma-
chine), and generates a new FST that performs a
context dependent rewrite everywhere in the pro-
vided contexts. The context-dependent rewrite algo-
rithm used is that of Mohri and Sproat (1996), and
63
see also Kaplan and Kay (1994). The fourth argu-
ment (sigma star) needs to be a minimized ma-
chine. The fifth argument selects the direction of
rewrite; we can either rewrite left-to-right or right-
to-left or simultaneously. The sixth argument selects
whether the rewrite is optional.
CDRewrite[tau, lambda, rho,
sigma_star,
?ltr?|?rtl?|?sim?,
?obl?|?opt?]
For context-dependent rewrite rules, two built-in
symbols ?[BOS]? and ?[EOS]? have a special mean-
ing in the context specifications: they refer to the
beginning and end of string, respectively.
There are also built-in functions that perform
other tasks. In the interest of space we concentrate
here on the StringFile function, which loads a
file consisting of a list of strings, or tab-separated
pairs of strings, and compiles them to an acceptor
that represents the union of the strings.
StringFile[?strings_file?]
While it is equivalent to the union of the individual
string (pairs), StringFile uses an efficient algo-
rithm for constructing a prefix tree (trie) from the
list and can be significantly more efficient than com-
puting a union for large lists. If a line consists of a
tab-separated pair of strings a, b, a transducer equiv-
alent to Rewrite[a, b] is compiled.
The optional keywords byte (default), utf8 or
the name of a symbol table can be used to specify
the parsing mode for the strings. Thus
StringFile[?strings_file?, utf8, my_symtab]
would parse a sequence of tab-separated pairs, using
utf8 parsing for the left-hand string, and the symbol
table my symtab for the right-hand string.
4 NGram Library
The OpenGrm NGram library contains tools for
building, manipulating and using n-gram language
models represented as weighted finite-state trans-
ducers. The same finite-state topology is used to en-
code raw counts as well as smoothed models. Here
we briefly present this structure, followed by details
on the operations manipulating it.
An n-gram is a sequence of n symbols: w1 . . . wn.
Each state in the model represents a prefix history
of the n-gram (w1 . . . wn?1), and transitions in the
model represent either n-grams or backoff transi-
tions following that history. Figure 1 lists conven-
tions for states and transitions used to encode the
n-grams as a WFST.
This representation is similar to that used in other
WFST-based n-gram software libraries, such as the
AT&T GRM library (Allauzen et al, 2005). One
key difference is the implicit representation of <s>
and </s>, as opposed to encoding them as symbols
in the grammar. This has the benefit of including all
start and stop symbol functionality while avoiding
common pitfalls that arise with explicit symbols.
Another difference from the GRM library repre-
sentation is explicit inclusion of failure links from
states to their backoff states even in the raw count
files. The OpenGrm n-gram FST format is consis-
tent through all stages of building the models, mean-
ing that model manipulation (e.g., merging of two
Figure 1: List of state and transition conventions used to encode collection of n-grams in WFST.
An n-gram is a sequence of n symbols: w1 . . . wn. Its proper prefixes include all sequences w1 . . . wk for k < n.
? There is a unigram state in every model, representing the empty string.
? Every proper prefix of every n-gram in the model has an associated state in the model.
? The state associated with an n-gram w1...wn has a backoff transition (labeled with ) to the state associated
with its suffix w2...wn.
? An n-gram w1...wn is represented as a transition, labeled with wn, from the state associated with its prefix
w1...wn?1 to a destination state defined as follows:
? If w1...wn is a proper prefix of an n-gram in the model, then the destination of the transition is the state
associated with w1...wn
? Otherwise, the destination of the transition is the state associated with the suffix w2...wn.
? Start and end of the sequence are not represented via transitions in the automaton or symbols in the symbol
table. Rather
? The start state of the automaton encodes the ?start of sequence? n-gram prefix (commonly denoted<s>).
? The end of the sequence (often denoted </s>) is included in the model through state final weights, i.e.,
for a state associated with an n-gram prefix w1...wn, the final weight of that state represents the weight
of the n-gram w1...wn</s>.
64
(a)
?
?
?
a/0
a/-1.1
b/-1.1
b/0
b/-0.69
a/-0.69
0
0
(b)
?/0.69
?/0.916
a/0.6
a/0.Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43?52,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Smoothed marginal distribution constraints for language modeling
Brian Roark??, Cyril Allauzen? and Michael Riley?
?Oregon Health & Science University, Portland, Oregon ?Google, Inc., New York
roarkbr@gmail.com, {allauzen,riley}@google.com
Abstract
We present an algorithm for re-estimating
parameters of backoff n-gram language
models so as to preserve given marginal
distributions, along the lines of well-
known Kneser-Ney (1995) smoothing.
Unlike Kneser-Ney, our approach is de-
signed to be applied to any given smoothed
backoff model, including models that have
already been heavily pruned. As a result,
the algorithm avoids issues observed when
pruning Kneser-Ney models (Siivola et al,
2007; Chelba et al, 2010), while retain-
ing the benefits of such marginal distribu-
tion constraints. We present experimen-
tal results for heavily pruned backoff n-
gram models, and demonstrate perplexity
and word error rate reductions when used
with various baseline smoothing methods.
An open-source version of the algorithm
has been released as part of the OpenGrm
ngram library.1
1 Introduction
Smoothed n-gram language models are the de-
facto standard statistical models of language for
a wide range of natural language applications, in-
cluding speech recognition and machine transla-
tion. Such models are trained on large text cor-
pora, by counting the frequency of n-gram col-
locations, then normalizing and smoothing (reg-
ularizing) the resulting multinomial distributions.
Standard techniques store the observed n-grams
and derive probabilities of unobserved n-grams via
their longest observed suffix and ?backoff? costs
associated with the prefix histories of the unob-
served suffixes. Hence the size of the model grows
with the number of observed n-grams, which is
very large for typical training corpora.
1www.opengrm.org
Natural language applications, however, are
commonly used in scenarios requiring relatively
small footprint models. For example, applica-
tions running on mobile devices or in low latency
streaming scenarios may be required to limit the
complexity of models and algorithms to achieve
the desired operating profile. As a result, statisti-
cal language models ? an important component of
many such applications ? are often trained on very
large corpora, then modified to fit within some
pre-specified size bound. One method to achieve
significant space reduction is through random-
ized data structures, such as Bloom (Talbot and
Osborne, 2007) or Bloomier (Talbot and Brants,
2008) filters. These data structures permit effi-
cient querying for specific n-grams in a model
that has been stored in a fraction of the space
required to store the full, exact model, though
with some probability of false positives. Another
common approach ? which we pursue in this pa-
per ? is model pruning, whereby some number of
the n-grams are removed from explicit storage in
the model, so that their probability must be as-
signed via backoff smoothing. One simple prun-
ing method is count thresholding, i.e., discarding
n-grams that occur less than k times in the corpus.
Beyond count thresholding, the most widely used
pruning methods (Seymore and Rosenfeld, 1996;
Stolcke, 1998) employ greedy algorithms to re-
duce the number of stored n-grams by comparing
the stored probabilities to those that would be as-
signed via the backoff smoothing mechanism, and
removing those with the least impact according to
some criterion.
While these greedy pruning methods are highly
effective for models estimated with most com-
mon smoothing approaches, they have been shown
to be far less effective with Kneser-Ney trained
language models (Siivola et al, 2007; Chelba et
al., 2010), leading to severe degradation in model
quality relative to other standard smoothing meth-
43
4-gram models Backoff Interpolated
Perplexity n-grams Perplexity n-grams
Smoothing method full pruned (?1000) full pruned (?1000)
Absolute Discounting (Ney et al, 1994) 120.5 197.3 383.4 119.8 198.1 386.2
Witten-Bell (Witten and Bell, 1991) 118.8 196.3 380.4 121.6 202.3 396.4
Ristad (1995) 126.4 203.6 395.6 ??- N/A ??-
Katz (1987) 119.8 198.1 386.2 ??- N/A ??-
Kneser-Ney (Kneser and Ney, 1995) 114.5 285.1 388.2 115.8 274.3 398.7
Mod. Kneser-Ney (Chen and Goodman, 1998) 116.3 280.6 396.2 112.8 270.7 399.1
Table 1: Reformatted version of Table 3 in Chelba et al (2010), demonstrating perplexity degradation of Kneser-Ney
smoothed models in contrast to other common smoothing methods. Data: English Broadcast News, 128M words training;
692K words test; 143K word vocabulary. 4-gram language models, pruned using Stolcke (1998) relative entropy pruning to
approximately 1.3% of the original size of 31,095,260 n-grams.
ods. Thus, while Kneser-Ney may be the preferred
smoothing method for large, unpruned models
? where it can achieve real improvements over
other smoothing methods ? when relatively sparse,
pruned models are required, it has severely dimin-
ished utility.
Table 1 presents a slightly reformatted version
of Table 3 from Chelba et al (2010). In their
experiments (see Table 1 caption for specifics on
training/test setup), they trained 4-gram Broad-
cast News language models using a variety of
both backoff and interpolated smoothing methods
and measured perplexity before and after Stol-
cke (1998) relative entropy based pruning. With
this size training data, the perplexity of all of
the smoothing methods other than Kneser-Ney
degrades from around 120 with the full model
to around 200 with the heavily pruned model.
Kneser-Ney smoothed models have lower perplex-
ity with the full model than the other methods by
about 5 points, but degrade with pruning to far
higher perplexity, between 270-285.
The cause of this degradation is Kneser-Ney?s
unique method for estimating smoothed language
models, which will be presented in more detail in
Section 3. Briefly, the smoothing method reesti-
mates lower-order n-gram parameters in order to
avoid over-estimating the likelihood of n-grams
that already have ample probability mass allocated
as part of higher-order n-grams. This is done via
a marginal distribution constraint which requires
the expected frequency of the lower-order n-grams
to match their observed frequency in the training
data, much as is commonly done for maximum
entropy model training. Goodman (2001) proved
that, under certain assumptions, such constraints
can only improve language models. Lower-order
n-gram parameters resulting from Kneser-Ney are
not relative frequency estimates, as with other
smoothing methods; rather they are parameters
estimated specifically for use within the larger
smoothed model.
There are (at least) a couple of reasons why such
parameters do not play well with model pruning.
First, the pruning methods commonly use lower
order n-gram probabilities to derive an estimate
of state marginals, and, since these parameters are
no longer smoothed relative frequency estimates,
they do not serve that purpose well. For this rea-
son, the widely-used SRILM toolkit recently pro-
vided switches to modify their pruning algorithm
to use another model for state marginal estimates
(Stolcke et al, 2011). Second, and perhaps more
importantly, the marginal constraints that were ap-
plied prior to smoothing will not in general be con-
sistent with the much smaller pruned model. For
example, if a bigram parameter is modified due
to the presence of some set of trigrams, and then
some or all of those trigrams are pruned from the
model, the bigram associated with the modified
parameter will be unlikely to have an overall ex-
pected frequency equal to its observed frequency
anymore. As a result, the resulting model degrades
dramatically with pruning.
In this paper, we present an algorithm that
imposes marginal distribution constraints of the
sort used in Kneser-Ney modeling on arbitrary
smoothed backoff n-gram language models. Our
approach makes use of the same sort of deriva-
tion as the original Kneser-Ney modeling, but,
among other differences, relies on smoothed es-
timates of the empirical relative frequency rather
than the unsmoothed observed frequency. The al-
gorithm can be applied after the smoothed model
has been pruned, hence avoiding the pitfalls asso-
ciated with Kneser-Ney modeling. Furthermore,
while Kneser-Ney is conventionally defined as a
variant of absolute discounting, our method can
be applied to models smoothed with any backoff
smoothing, including mixtures of models, widely
44
used for domain adaptation.
We next establish formal preliminaries and
our smoothed marginal distribution constraints
method.
2 Preliminaries
N-gram language models are typically presented
mathematically in terms of words w, the strings
(histories) h that precede them, and the suffixes
of the histories (backoffs) h? that are used in the
smoothing recursion. Let V be a vocabulary (al-
phabet), and V ? a string of zero or more symbols
drawn from V . Let V k denote the set of strings
w ? V ? of length k, i.e., |w| = k. We will use
variables u, v, w, x, y, z ? V to denote single sym-
bols from the vocabulary; h, g ? V ? to denote his-
tory sequences preceding the specific word; and
h?, g? ? V ? the respective backoff histories of h
and g as typically defined (see below). For a string
w = w1 . . . w|w| we can calculate the smoothed
conditional probability of each word wi in the se-
quence given the k words that preceded it, de-
pending on the order of the Markov model. Let
hki = wi?k . . . wi?1 be the previous k words in
the sequence. Then the smoothed model is defined
recursively as follows:
P(wi | hki ) =
{
P(wi | hki ) if c(hkiwi) > 0
?(hki ) P(wi | hk?1i ) otherwise
where c(hkiwi) is the count of the n-gram sequence
wi?k . . . wi in the training corpus; P is a regular-
ized probability estimate that provides some prob-
ability mass for unobserved n-grams; and ?(hki )
is a factor that ensures normalization. Note that
for h = hki , the typically defined backoff history
h? = hk?1i , i.e., the longest suffix of h that is not h
itself. When we use h? and g? (for notational con-
venience) in future equations, it is this definition
that we are using.
There are many ways to estimate P, includ-
ing absolute discounting (Ney et al, 1994), Katz
(1987) and Witten and Bell (1991). Interpolated
models are special cases of this form, where the P
is determined using model mixing, and the ? pa-
rameter is exactly the mixing factor value for the
lower order model.
N-gram language models allow for a sparse rep-
resentation, so that only a subset of the possible n-
grams must be explicitly stored. Probabilities for
the rest of the n-grams are calculated through the
?otherwise? semantics in the equation above. For
an n-gram language model G, we will say that an
n-gram hw ? G if it is explicitly represented in
the model; otherwise hw 6? G. In the standard n-
gram formulation above, the assumption is that if
c(hkiwi) > 0 then the n-gram has a parameter; yet
with pruning, we remove many observed n-grams
from the model, hence this is no longer the ap-
propriate criterion. We reformulate the standard
equation as follows:
P(wi|hki ) =
{
?(hkiwi) if hkiwi ? G
?(hki , hk?1i ) P(wi|hk?1i ) otherwise
(1)
where ?(hkiwi) is the parameter associated with
the n-gram hkiwi and ?(hki , hk?1i ) is the backoff
cost associated with going from state hki to state
hk?1i . We assume that, if hw ? G then all prefixes
and suffixes of hw are also in G.
Figure 1 presents a schema of an automaton rep-
resentation of an n-gram model, of the sort used in
the OpenGrm library (Roark et al, 2012). States
represent histories h, and the words w, whose
probabilities are conditioned on h, label the arcs,
leading to the history state for the subsequent
word. State labels are provided in Figure 1 as
a convenience, to show the (implicit) history en-
coded by the state, e.g., ?xyz? indicates that the
state represents a history with the previous three
symbols being x, y and z. Failure arcs, labeled
with a ? in Figure 1, encode an ?otherwise? se-
mantics and have as destination the origin state?s
backoff history. Many higher order states will
back off to the same lower order state, specifically
those that share the same suffix.
Note that, in general, the recursive definition of
backoff may require the traversal of several back-
yz
z
xyz
u/?(xyzu)
w/?(yzw)
w/?(zw)
?/?(xyz,yz)
?/?(yz,z)
zw
yyz
?/?(yyz,yz)
yzw
?
yzu yzv
v/?(yyzv)
w/?(yyzw)
?/?(z,?)
?/?(yzw,zw)
z/?(z)
Figure 1: N-gram weighted automaton schema. State labels
are presented for convenience, to specify the history implic-
itly encoded by the state.
45
off arcs before emitting a word, e.g., the highest
order states in Figure 1 needing to traverse a cou-
ple of ? arcs to reach state ?z?. We can define
the backoff cost between a state hki and any of its
suffix states as follows. Let ?(h, h) = 1 and for
m > 1,
?(hki , hk?mi ) =
m?
j=1
?(hk?j+1i , h
k?j
i ).
If hkiw 6? G then the probability of that n-gram
will be defined in terms of backoff to its longest
suffix hk?mi w ? G. Let hwG denote the longest
suffix of h such that hwGw ? G. Note that this
is not necessarily a proper suffix, since hwG could
be h itself or it could be . Then
P(w | h) = ?(h, hwG) ?(hwGw) (2)
which is equivalent to equation 1.
3 Marginal distribution constraints
Marginal distribution constraints attempt to match
the expected frequency of an n-gram with its ob-
served frequency. In other words, if we use the
model to randomly generate a very large corpus,
the n-grams should occur with the same rela-
tive frequency in both the generated and original
(training) corpus. Standard smoothing methods
overgenerate lower-order n-grams. Using standard
n-gram notation (where g? is the backoff history
for g), this constraint is stated in Kneser and Ney
(1995) as
P?(w | h?) =
?
g:g?=h?
P(g, w | h?) (3)
where P? is the empirical relative frequency esti-
mate. Taking this approach, certain base smooth-
ing methods end up with very nice, easy to cal-
culate solutions based on counts. Absolute dis-
counting (Ney et al, 1994) in particular, using the
above approach, leads to the well-known Kneser-
Ney smoothing approach (Kneser and Ney, 1995;
Chen and Goodman, 1998). We will follow this
same approach, with a couple of changes. First,
we will make use of regularized estimates of rela-
tive frequency P rather than raw relative frequency
P?. Second, rather than just looking at observed
histories h that back off to h?, we will look at
all histories (observed or not) of the length of
the longest history in the model. For notational
simplicity, suppose we have an n+1-gram model,
hence the longest history in the model is of length
n. Assume the length of the particular backoff his-
tory |h?| = k. Let V n?kh? be the set of strings
h ? V n with h? as a suffix. Then we can restate
the marginal distribution constraint in equation 3
as
P(w | h?) =
?
h?V n?kh?
P(h,w | h?) (4)
Next we solve for ?(h?w) parameters used in
equation 1. Note that h? is a suffix of any h ?
V n?kh?, so conditioning probabilities on h and h?
is the same as conditioning on just h. Each of
the following derivation steps simply relies on the
chain rule or definition of conditional probability,
as well as pulling terms out of the summation.
P(w | h?) =
?
h?V n?kh?
P(h,w | h?)
=
?
h?V n?kh?
P(w | h, h?) P(h | h?)
=
?
h?V n?kh?
P(w | h) P(h)?
g?V n?kh?
P(g)
= 1?
g?V n?kh?
P(g)
?
h?V n?kh?
P(w | h) P(h) (5)
Then, multiplying both sides by the normaliz-
ing denominator on the right-hand side and using
equation 2 to substitute ?(h, hwG) ?(hwGw) for
P(w | h):
P(w | h?)
?
g?V n?kh?
P(g) =
?
h?V n?kh?
P(w | h) P(h)
=
?
h?V n?kh?
?(h, hwG) ?(hwGw) P(h) (6)
Note that we are only interested in h?w ? G,
hence there are two disjoint subsets of histories
h ? V n?kh? that are being summed over: those
such that hwG = h? and those such that |hwG| >
|h?|. We next separate these sums in the next step
of the derivation:
P(w | h?)
?
g?V n?kh?
P(g) =
?
h?V n?kh?:|hwG|>|h?|
?(h, hwG) ?(hwGw) P(h) +
?
h?V n?kh?:hwG=h?
?(h, h?) ?(h?w) P(h) (7)
Finally, we solve for ?(h?w) in the second sum
on the right-hand side of equation 7, yielding the
formula in equation 8. Note that this equation is
the correlate of equation (6) in Kneser and Ney
46
?(h?w) =
P(w | h?)
?
g?V n?kh?
P(g) ?
?
h?V n?kh?:|hwG|>|h?|
?(h, hwG) ?(hwGw) P(h)
?
h?V n?kh?:hwG=h?
?(h, h?) P(h)
(8)
(1995), modulo the two differences noted earlier:
use of smoothed probability P rather than raw rel-
ative frequency; and summing over all history sub-
strings in V n?kh? rather than just those with count
greater than zero, which is also a change due to
smoothing. Keep in mind, P is the target expected
frequency from a given smoothed model. Kneser-
Ney models are not useful input models, since
their P n-gram parameters are not relative fre-
quency estimates. This means that we cannot sim-
ply ?repair? pruned Kneser-Ney models, but must
use other smoothing methods where the smoothed
values are based on relative frequency estimation.
There are, in addition, two other important dif-
ferences in our approach from that in Kneser and
Ney (1995), which would remain as differences
even if our target expected frequency were the
unsmoothed relative frequency P? instead of the
smoothed estimate P. First, the sum in the nu-
merator is over histories of length n, the highest
order in the n-gram model, whereas in the Kneser-
Ney approach the sum is over histories that imme-
diately back off to h?, i.e., from the next highest
order in the n-gram model. Thus the unigram dis-
tribution is with respect to the bigram model, the
bigram model is with respect to the trigram model,
and so forth. In our optimization, we sum in-
stead over all possible history sequences of length
n. Second, an early assumption made in Kneser
and Ney (1995) is that the denominator term in
their equation (6) (our Eq. 8) is constant across all
words for a given history, which is clearly false.
We do not make this assumption. Of course, the
probabilities must be normalized, hence the final
values of ?(h?w) will be proportional to the val-
ues in Eq. 8.
We briefly note that, like Kneser-Ney, if the
baseline smoothing method is consistent, then the
amount of smoothing in the limit will go to zero
and our resulting model will also be consistent.
The smoothed relative frequency estimate P and
higher order ? values on the right-hand side of Eq.
8 are given values (from the input smoothed model
and previous stages in the algorithm, respectively),
implying an algorithm that estimates highest or-
ders of the model first. In addition, steady state
history probabilities P(h) must be calculated. We
turn to the estimation algorithm next.
4 Model constraint algorithm
Our algorithm takes a smoothed backoff n-gram
language model in an automaton format (see Fig-
ure 1) and returns a smoothed backoff n-gram lan-
guage model with the same topology. For all n-
grams in the model that are suffixes of other n-
grams in the model ? i.e., that are backed-off to
? we calculate the weight provided by equation 8
and assign it (after normalization) to the appropri-
ate n-gram arc in the automaton. There are several
important considerations for this algorithm, which
we address in this section. First, we must provide
a probability for every state in the model. Second,
we must memoize summed values that are used
repeatedly. Finally, we must iterate the calcula-
tion of certain values that depend on the n-gram
weights being re-estimated.
4.1 Steady state probability calculation
The steady state probability P(h) is taken to be the
probability of observing h after a long word se-
quence, i.e., the state?s relative frequency in a long
sequence of randomly-generated sentences from
the model:
P(h) = lim
m??
?
w1...wm
P?(w1 . . . wmh) (9)
where P? is the corpus probability derived as fol-
lows: The smoothed n-gram probability model
P(w | h) is naturally extended to a sentence
s = w0 . . . wl, where w0 = <s> and wl = </s>
are the sentence initial and final words, by P(s) =?l
i=1 P(wi | hni ). The corpus probability s1 . . . sr
is taken as:
P?(s1 . . . sr) = (1? ?)?r?1
r?
i=1
P(si) (10)
where ? parameterizes the corpus length distri-
bution.2 Assuming the n-gram language model
automaton G has a single final state </s> into
2P? models words in a corpus rather than a single sen-
tence since Equation 9 tends to zero as m ? ? otherwise.
In Markov chain terms, the corpus distribution is made irre-
ducible to allow a non-trivial stationary distribution.
47
which all </s> arcs enter, adding a ? weighted
 arc from the </s> state to the initial state and
having a final weight 1 ? ? in order to leave the
automaton at the </s> state will model this cor-
pus distribution. According to Eq. 9, P (h) is then
the stationary distribution of the finite irreducible
Markov Chain defined by this altered automaton.
There are many methods for computing such a sta-
tionary distribution; we use the well-known power
method (Stewart, 1999).
One difficulty remains to be resolved. The
backoff arcs have a special interpretation in the
automaton: they are traversed only if a word fails
to match at the higher order. These failure arcs
must be properly handled before applying stan-
dard stationary distribution calculations. A simple
approach would be for each word w? and state h
such that hw? /? G, but h?w? ? G, add a w? arc
from state h to h?w? with weight ?(h, h?)?(h?w?)
and then remove all failure arcs (see Figure 2a).
This however results in an automaton with |V | arcs
leaving every state, which is unwieldy with larger
vocabularies and n-gram orders. Instead, for each
word w and state h such that hw ? G, add a w arc
from state h to h?w with weight ??(h, h?)?(h?w)
and then replace all failure labels with  labels (see
Figure 2b). In this case, the added negatively-
weighted arcs compensate for the excess probabil-
ity mass allowed by the epsilon arcs3. The number
of added arcs is no more than found in the original
model.
4.2 Accumulation of higher order values
We are summing over all possible histories of
length n in equation 8, and the steady state prob-
ability calculation outlined in the previous section
includes the probability mass for histories h 6? G.
The probability mass of states not inG ends up be-
ing allocated to the state representing their longest
suffix that is explicitly in G. That is the state that
would be active when these histories are encoun-
tered. Hence, once we have calculated the steady
state probabilities for each state in the smoothed
model, we only need to sum over states explicitly
in the model.
As stated earlier, the use of ?(hwGw) in the
numerator of equation 8 for hwG that are larger
than h? implies that the longer n-grams must be
3Since each negatively-weighted arc leaving a state
exactly cancels an epsilon arc followed by a matching
positively-weighted arc in each iteration of the power
method, convergence is assured.
(a) (b)
K
K

w/?(Kw)
w
/?(K
w
)
?/?(K,K
)
Kw
K
w

w
/?(K,K
)?(K
w
)
K
K

w/?(Kw)
w/?(K
w)
?/?(K,K
)
Kw
K
w
w/?(K,K
)?(K
w)
Figure 2: Schemata showing failure arc handling: (a) ?
removal: add w? arc (red), delete ? arc; (b) ? replacement:
add w arc (red), replace ? by  (red)
re-estimated first. Thus we process each history
length in descending order, finishing with the un-
igram state. Since we assume that, for every n-
gram hw ? G, every prefix and suffix is also
in G, we know that if h?w 6? G then there is
no history h such that h? is a suffix of h and
hw ? G. This allows us to recursively accumu-
late the ?(h, h?) P(h) in the denominator of Eq. 8.
For every n-gram, we can accumulate values re-
quired to calculate the three terms in equation 8,
and pass them along to calculate lower order n-
gram values. Note, however, that a naive imple-
mentation of an algorithm to assign these values is
O(|V |n). This is due to the fact that the denom-
inator factor must be accumulated for all higher
order states that do not have the given n-gram.
Hence, for every state h directly backing off to
h? (order |V |), and for every n-gram arc leaving
state h? (order |V |), some value must be accumu-
lated. This can be particularly clearly seen at the
unigram state, which has an arc for every unigram
(the size of the vocabulary): for every bigram state
(also order of the vocabulary), in the naive algo-
rithm we must look for every possible arc. Since
there are O(|V |n?2) lower order histories in the
model in the worst case, we have overall complex-
ity O(|V |n). However, we know that the number
of stored n-grams is very sparse relative to the pos-
sible number of n-grams, so the typical case com-
plexity is far lower. Importantly, the denominator
is calculated by first assuming that all higher order
states back off to the current n-gram, then subtract
out the mass associated with those that are already
observed at the higher order. In such a way, we
need only perform work for higher order n-grams
hw that are explicitly in the model. This opti-
mization achieves orders-of-magnitude speedups,
so that models take seconds to process.
Because smoothing is not necessarily con-
48
strained across n-gram orders, it is possible that
higher-order n-grams could be smoothed less than
lower order n-grams, so that the numerator of
equation 8 can be less than zero, which is not valid.
A value less than zero means that the higher or-
der n-grams will already produce the n-gram more
frequently than its smoothed expected frequency.
We set a minimum value  for the numerator, and
any n-gram numerator value less than  is replaced
with  (for the current study,  = 0.001). We
find this to be relatively infrequent, about 1% of
n-grams for most models.
4.3 Iteration
Recall that P and ? terms on the right-hand side of
equation 8 are given and do not change. But there
are two other terms in the equation that change as
we update the n-gram parameters. The ?(h, h?)
backoff weights in the denominator ensure nor-
malization at the higher order states, and change
as the n-gram parameters at the current state are
modified. Further, the steady state probabilities
will change as the model changes. Hence, at each
state, we must iterate the calculation of the denom-
inator term: first adjust n-gram weights and nor-
malize; then recalculate backoff weights at higher
order states and iterate. Since this only involves
the denominator term, each n-gram weight can be
updated by multiplying by the ratio of the old term
and the new term.
After the entire model has been re-estimated,
the steady state probability calculation presented
in Section 4.1 is run again and model estimation
happens again. As we shall see in the experimen-
tal results, this typically converges after just a few
iterations. At this time, we have no convergence
proofs for either of these iterative components to
the algorithm, but expect that something can be
said about this, which will be a priority in future
work.
5 Experimental results
All results presented here are for English Broad-
cast News. We received scripts for replicating the
Chelba et al (2010) results from the authors, and
we report statistics on our replication of their pa-
per?s results in Table 2. The scripts are distributed
in such a way that the user supplies the data from
LDC98T31 (1996 CSR HUB4 Language Model
corpus) and the script breaks the collection into
training and testing sets, normalizes the text, and
Smoothing Perplexity n-grams (?1000)
method full pruned model diff
Abs.Disc. 120.4 197.1 382.3 -1.1
Witten-Bell 118.7 196.1 379.3 -1.1
Ristad 126.2 203.4 394.6 -1.1
Katz 119.7 197.9 385.1 -1.1
Kneser-Ney? 114.4 234.1 375.4 -12.7
Table 2: Replication of Chelba et al (2010) using provided
script. Using the script, the size of the unpruned model is
31,091,219 ngrams, 4,041 fewer than Chelba et al (2010).
? Kneser-Ney model pruned using -prune-history-lm
switch in SRILM.
trains and prunes the language models using the
SRILM toolkit (Stolcke et al, 2011). Presumably
due to minor differences in text normalization, re-
sulting in very slightly fewer n-grams in all con-
ditions, we achieve negligibly lower perplexities
(one or two tenths of a point) in all conditions, as
can be seen when comparing with Table 1. All
of the same trends result, thus that paper?s result
is successfully replicated here. Note that we ran
our Kneser-Ney pruning (noted with a ? in the ta-
ble), using the new -prune-history-lm switch in
SRILM ? created in response to the Chelba et al
(2010) paper ? which allows the use of another
model to calculate the state marginals for pruning.
This fixes part of the problem ? perplexity does not
degrade as much as the Kneser-Ney pruned model
in Table 1 ? but, as argued earlier in this paper, this
is not the sole reason for the degradation and the
perplexity remains extremely inflated.
We follow Chelba et al (2010) in training and
test set definition, vocabulary size, and parame-
ters for reporting perplexity. Note that unigrams
in the models are never pruned, hence all models
assign probabilities over an identical vocabulary
and perplexity is comparable across models. For
all results reported here, we use the SRILM toolkit
for baseline model training and pruning, then con-
vert from the resulting ARPA format model to
an OpenFst format (Allauzen et al, 2007), as
used in the OpenGrm n-gram library (Roark et al,
2012). We then apply the marginal distribution
constraints, and convert the result back to ARPA
format for perplexity evaluation with the SRILM
toolkit. All models are subjected to full normaliza-
tion sanity checks, as with typical model functions
in the OpenGrm library.
Recall that our algorithm assumes that, for ev-
ery n-gram in the model, all prefix and suffix n-
grams are also in the model. For pruned mod-
els, the SRILM toolkit does not impose such a
requirement, hence explicit arcs are added to the
49
Perplexity n-grams
Smoothing Pruned Pruned (?1000)
Method Model +MDC ? in WFST
Abs.Disc. 197.1 187.4 9.7 389.2
Witten-Bell 196.1 185.7 10.4 385.0
Ristad 203.4 190.3 13.1 395.9
Katz 197.9 187.5 10.4 390.8
AD,WB,Katz
Mixture 196.6 186.3 10.3 388.7
Table 3: Perplexity reductions achieved with marginal dis-
tribution constraints (MDC) on the heavily pruned models
from Chelba et al (2010), and a mixture model. WFST n-
gram counts are slightly higher than ARPA format in Table 2
due to adding prefix and suffix n-grams.
model during conversion, with probability equal to
what they would receive in the the original model.
The resulting model is equivalent, but with a small
number of additional arcs in the explicit repre-
sentation (around 1% for the most heavily pruned
models).
Table 3 presents perplexity results for models
that result from applying our marginal distribution
constraints to the four heavily pruned models from
Table 2. In all four cases, we get perplexity reduc-
tions of around 10 points. We present the num-
ber of n-grams represented explicitly in the WFST,
which is a slight increase from those presented in
Table 2 due to the reintroduction of prefix and suf-
fix n-grams.
In addition to the four models reported in
Chelba et al (2010), we produced a mixture model
by interpolating (with equal weight) smoothed n-
gram probabilities from the full (unpruned) ab-
solute discounting, Witten-Bell and Katz models,
which share the same set of n-grams. After renor-
malizing and pruning to approximately the same
size as the other models, we get commensurate
gains using this model as with the other models.
Figure 3 demonstrates the importance of iterat-
ing the steady state history calculation. All of the
methods achieve perplexity reductions with sub-
sequent iterations. Katz and absolute discounting
achieve very little reduction in the first iteration,
but catch back up in the second iteration.
The other iterative part of the algorithm, dis-
cussed in Section 4.3, is the denominator of equa-
tion 8, which changes due to adjustments in the
backoff weights required by the revised n-gram
probabilities. If we do not iteratively update the
backoff weights when reestimating the weights,
the ?Pruned+MDC? perplexities in Table 3 in-
crease by between 0.2?0.4 points. Hence, iterat-
ing the steady state probability calculation is quite
important, as illustrated by Figure 3; iterating the
0 1 2 3 4 5 6180
185
190
195
200
205
Iterations of estimation (recalculating steady state probs)
Perp
lexit
y
 
 Witten?BellRistadKatzAbsolute DiscountingWB,AD,Katz mixture
Figure 3: Models resulting from different numbers of pa-
rameter re-estimation iterations. Iteration 0 is the baseline
pruned model.
denominator calculation much less so, at least for
these models. We noted in Section 3 that a key dif-
ference between our approach and Kneser and Ney
(1995) is that their approach treated the denomina-
tor as a constant. If we do this, the ?Pruned+MDC?
perplexities increase by between 4.5?5.6 points,
i.e., about half of the perplexity reduction is lost
for each method. Thus, while iteration of denomi-
nator calculation may not be critical, it should not
be treated as a constant.
We now look at the impacts on system perfor-
mance we can achieve with these new models4,
and whether the perplexity differences that we ob-
serve translate to real error rate reductions.
For automatic speech recognition experiments,
we used as test set the 1997 Hub4 evaluation set
consisting of 32,689 words. The acoustic model
is a tied-state triphone GMM-based HMM whose
input features are 9-frame stacked 13-dimensional
PLP-cepstral coefficients projected down to 39 di-
mensions using LDA. The model was trained on
the 1996 and 1997 Hub4 acoustic model train-
ing sets (about 150 hours of data) using semi-tied
covariance modeling and CMLLR-based speaker
adaptive training and 4 iterations of boosted MMI.
We used a multi-pass decoding strategy: two
quick passes for adaptation supervision, CMLLR
and MLLR estimation; then a slower full decoding
pass running about 3 times slower than real time.
Table 4 presents recognition results for the
heavily pruned models that we have been con-
sidering, both for first pass decoding and rescor-
ing of the resulting lattices using failure transi-
tions rather than epsilon backoff approximations.
4For space purposes, we exclude the Ristad method from
this point forward since it is not competitive with the others.
50
Word error rate (WER)
First pass Rescoring
Smoothing Pruned Pruned Pruned Pruned
Method Model +MDC Model +MDC
Abs.Disc. 20.5 19.7 20.2 19.6
Witten-Bell 20.5 19.9 20.1 19.6
Katz 20.5 19.7 20.2 19.7
Mixture 20.5 19.6 20.2 19.6
Kneser-Neya 22.1 22.2
Kneser-Neyb 20.5 20.6
Table 4: WER reductions achieved with marginal dis-
tribution constraints (MDC) on the heavily pruned models
from Chelba et al (2010), and a mixture model. Kneser-
Ney results are shown for: a) original pruning; and b) with
-prune-history-lm switch.
The perplexity reductions that were achieved for
these models do translate to real word error rate
reductions at both stages of between 0.5 and 0.9
percent absolute. All of these gains are sta-
tistically significant at p < 0.0001 using the
stratified shuffling test (Yeh, 2000). For pruned
Kneser-Ney models, fixing the state marginals
with the -prune-history-lm switch reduces the
WER versus the original pruned model, but no re-
ductions were achieved vs. baseline methods.
Table 5 presents perplexity and WER results
for less heavily pruned models, where the prun-
ing thresholds were set to yield approximately
1.5 million n-grams (4 times more than the pre-
vious models); and another set at around 5 mil-
lion n-grams, as well as the full, unpruned mod-
els. While the robust gains we?ve observed up to
now persist with the 1.5M n-gram models (WER
reductions significant, Witten-Bell at p < 0.02,
others at p < 0.0001), the larger models yield
diminishing gains, with no real WER improve-
ments. Performance of Witten-Bell models with
the marginal distribution constraints degrade badly
for the larger models, indicating that this method
of regularization, unmodified by aggressive prun-
ing, does not provide a well suited distribution for
this sort of optimization. We speculate that this
is due to underregularization, having noted some
floating point precision issues when allowing the
backoff recalculation to run indefinitely.
6 Summary and Future Directions
The presented method reestimates lower order
n-gram model parameters for a given smoothed
backoff model, achieving perplexity and WER re-
ductions for many smoothed models. There re-
main a number of open questions to investigate
in the future. Recall that the numerator in Eq.
8 can be less than zero, meaning that no param-
eterization would lead to a model with the target
frequency of the lower order n-gram, presumably
due to over- or under-regularization. We antic-
ipate a pre-constraint on the baseline smoothing
method, that would recognize this problem and ad-
just the smoothing to ensure that a solution does
exist. Additionally, it is clear that different reg-
ularization methods yield different behaviors, no-
tably that large, relatively lightly pruned Witten-
Bell models yield poor results. We will look to
identify the issues with such models and provide
general guidelines for prepping models prior to
processing. Finally, we would like to perform ex-
tensive controlled experimentation to examine the
relative contribution of the various aspects of our
approach.
Acknowledgments
Thanks to Ciprian Chelba and colleagues for the
scripts to replicate their results. This work was
supported in part by a Google Faculty Research
Award and NSF grant #IIS-0964102. Any opin-
ions, findings, conclusions or recommendations
expressed in this publication are those of the au-
thors and do not necessarily reflect the views of
the NSF.
M Less heavily pruned model Moderately pruned model Full model
Smoothing D ngrams WER ngrams WER ngrams WER
Method C (?106) PPL FP RS (?106) PPL FP RS (?106) PPL FP RS
Abs. N 1.53 146.6 18.1 17.9 5.19 129.1 17.0 16.6 31.1 120.4 16.2 16.1
Disc. Y 141.2 17.2 17.2 126.3 16.6 16.6 31.1 117.0 16.0 16.0
Witten- N 1.54 145.8 18.1 17.6 5.08 129.4 17.3 16.8 31.1 118.7 16.3 16.1
Bell Y 139.7 17.9 17.4 126.4 18.4 17.3 31.1 118.4 18.1 17.6
Katz N 1.57 146.6 17.8 17.7 5.10 128.9 16.8 16.6 31.1 119.7 16.2 16.1
Y 141.1 17.3 17.3 125.7 16.6 16.6 31.1 114.7 16.2 16.1
Mixture N 1.55 145.5 18.1 17.7 5.11 128.2 16.9 16.6 31.1 118.5 16.3 16.1
Y 139.2 17.3 17.2 123.6 16.6 16.4 31.1 114.6 17.3 16.4
Kneser-Ney backoff model, unpruned: 31.1 114.4 15.8 15.9
Table 5: Perplexity (PPL) and both first pass (FP) and rescoring (RS) WER reductions for less heavily pruned models using
marginal distribution constraints (MDC).
51
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Twelfth International
Conference on Implementation and Application of
Automata (CIAA 2007), Lecture Notes in Computer
Science, volume 4793, pages 11?23.
Ciprian Chelba, Thorsten Brants, Will Neveitt, and
Peng Xu. 2010. Study on interaction between en-
tropy pruning and Kneser-Ney smoothing. In Pro-
ceedings of Interspeech, page 24222425.
Stanley Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report, TR-10-98, Harvard
University.
Joshua Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech and Language,
15(4):403?434.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recogniser. IEEE Transactions on Acoustic,
Speech, and Signal Processing, 35(3):400?401.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), pages
181?184.
Hermann Ney, Ute Essen, and Reinhard Kneser. 1994.
On structuring probabilistic dependences in stochas-
tic language modeling. Computer Speech and Lan-
guage, 8:1?38.
Eric S. Ristad. 1995. A natural law of succession.
Technical Report, CS-TR-495-95, Princeton Univer-
sity.
Brian Roark, Richard Sproat, Cyril Allauzen, Michael
Riley, Jeffrey Sorensen, and Terry Tai. 2012. The
OpenGrm open-source finite-state grammar soft-
ware libraries. In Proceedings of the ACL 2012 Sys-
tem Demonstrations, pages 61?66.
Kristie Seymore and Ronald Rosenfeld. 1996. Scal-
able backoff language models. In Proceedings of
the International Conference on Spoken Language
Processing (ICSLP).
Vesa Siivola, Teemu Hirsimaki, and Sami Virpioja.
2007. On growing and pruning kneserney smoothed
n-gram models. IEEE Transactions on Audio,
Speech, and Language Processing, 15(5):1617?
1624.
William J Stewart. 1999. Numerical methods for com-
puting stationary distributions of finite irreducible
markov chains. Computational Probability, pages
81?111.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. Srilm at sixteen: Update and out-
look. In Proceedings of the IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU).
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proc. DARPA Broad-
cast News Transcription and Understanding Work-
shop, pages 270?274.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings of ACL-08: HLT, pages 505?513.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on
the cheap. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 468?476.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabilities of
novel events in adaptive text compression. IEEE
Transactions on Information Theory, 37(4):1085?
1094.
A. Yeh. 2000. More accurate tests for the statistical
significance of result differences. In Proceedings of
the 18th International COLING, pages 947?953.
52
