Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1034?1043, Prague, June 2007. c?2007 Association for Computational Linguistics
Validation and Evaluation of Automatically Acquired Multiword
Expressions for Grammar Engineering
Aline Villavicencio??, Valia Kordoni?, Yi Zhang?,
Marco Idiart? and Carlos Ramisch?
?Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?Department of Computer Sciences, Bath University (UK)
?Department of Computational Linguistics, Saarland University, and DFKI GmbH (Germany)
?Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
avillavicencio@inf.ufrgs.br, {yzhang,kordoni}@coli.uni-sb.de
idiart@if.ufrgs.br, ceramisch@inf.ufrgs.br
Abstract
This paper focuses on the evaluation of meth-
ods for the automatic acquisition of Multiword
Expressions (MWEs) for robust grammar engi-
neering. First we investigate the hypothesis that
MWEs can be detected by the distinct statistical
properties of their component words, regardless
of their type, comparing 3 statistical measures:
mutual information (MI), ?2 and permutation
entropy (PE). Our overall conclusion is that at
least two measures, MI and PE, seem to differen-
tiate MWEs from non-MWEs. We then investi-
gate the influence of the size and quality of differ-
ent corpora, using the BNC and the Web search
engines Google and Yahoo. We conclude that, in
terms of language usage, web generated corpora
are fairly similar to more carefully built corpora,
like the BNC, indicating that the lack of con-
trol and balance of these corpora are probably
compensated by their size. Finally, we show a
qualitative evaluation of the results of automat-
ically adding extracted MWEs to existing lin-
guistic resources. We argue that such a process
improves qualitatively, if a more compositional
approach to grammar/lexicon automated exten-
sion is adopted.
1 Introduction
The task of automatically identifying Multiword
Expressions (MWEs) like phrasal verbs (break
down) and compound nouns (coffee machine)
using statistical measures has been the focus
of considerable investigative effort, (e.g. Pearce
(2002), Evert and Krenn (2005) and Zhang et
al. (2006)). Given the heterogeneousness of
the different phenomena that are considered to
be MWEs, there is no consensus about which
method is best suited for which type of MWE,
and if there is a single method that can be suc-
cessfully used for any kind of MWE.
Another difficulty for work on MWE identifi-
cation is that of the evaluation of the results ob-
tained (Pearce, 2002; Evert and Krenn, 2005),
starting from the lack of consensus about a pre-
cise definition for MWEs (Villavicencio et al,
2005).
In this paper we investigate some of the is-
sues involved in the evaluation of automatically
extracted MWEs, from their extraction to their
subsequent use in an NLP task. In order to do
that, we present a discussion of different statisti-
cal measures, and the influence that the size and
quality of different data sources have. We then
perform a comparison of these measures and dis-
cuss whether there is a single measure that has
good overall performance for MWEs in general,
regardless of their type. Finally, we perform a
qualitative evaluation of the results of adding
automatically extracted MWEs to a linguistic
resource, taking as basis for the evaluation the
approach proposed by Zhang et al (2006). We
argue that such results can improve in quality
if a more compositional approach to MWE en-
coding is adopted for the grammar extension.
Having more accurate means of deciding for an
appropriate method for identifying and incor-
porating MWEs is critical for maintaining the
quality of linguistic resources for precise NLP.
This paper starts with a discussion of MWEs
(? 2), of their coverage in linguistic resources
(? 3), and of some methods proposed for auto-
matically identifying them (? 4). This is fol-
lowed by a detailed investigation and compar-
ison of measures for MWE identification (? 5).
1034
After that we present an approach for predicting
appropriate lexico-syntactic categories for their
inclusion in a linguistic resource, and an evalu-
ation of the results in a parsing task(? 7). We
finish with some conclusions and discussion of
future work.
2 Multiword Expressions
The term Multiword Expressions has been used
to describe expressions for which the syntactic or
semantic properties of the whole expression can-
not be derived from its parts (Sag et al, 2002),
including a large number of related but distinct
phenomena, such as phrasal verbs (e.g. come
along), nominal compounds (e.g. frying pan),
institutionalised phrases (e.g. bread and butter),
and many others. Jackendoff (1997) estimates
the number of MWEs in a speaker?s lexicon to
be comparable to the number of single words.
However, due to their heterogeneous character-
istics,MWEs present a tough challenge for both
linguistic and computational work (Sag et al,
2002). For instance, some MWEs are fixed, and
do not present internal variation, such as ad hoc,
while others allow different degrees of internal
variability and modification, such as spill beans
(spill several/musical/mountains of beans).
Sag et al (2002) discuss two main ap-
proaches commonly employed in NLP for treat-
ing MWEs: the words-with-spaces approach
models an MWE as a single lexical entry and it
can adequately capture fixed MWEs like by and
large. A compositional approach treats MWEs
by general and compositional methods of lin-
guistic analysis, being able to capture more syn-
tactically flexible MWEs, like rock boat, which
cannot be satisfactorily captured by a words-
with-spaces approach, since it would require lex-
ical entries to be added for all the possible
variations of an MWE (e.g. rock/rocks/rocking
this/that/his... boat). Therefore, to provide a
unified account for the detection and encoding
of these distinct but related phenomena is a real
challenge for NLP systems.
3 Grammar and Lexicon Coverage in
Deep Processing
Many NLP tasks and applications, like Parsing
and Machine Translation, depend on large-scale
linguistic resources, such as electronic dictionar-
ies and grammars for precise results. Several
substantial resources exist: e.g., hand-crafted
large-scale grammars like the English Resource
Grammar (ERG - Flickinger (2000)) and the
Dutch Alpino Grammar (Bouma et al, 2001).
Unfortunately, the construction of these re-
sources is the manual result of human efforts and
therefore likely to contain errors of omission and
commission (Briscoe and Carroll, 1997). Fur-
thermore, due to the open-ended and dynamic
nature of languages, such linguistic resources are
likely to be incomplete, and manual encoding of
new entries and constructions is labour-intensive
and costly.
Take, for instance, the coverage test results
for the ERG (a broad-coverage precision HPSG
grammar for English) on the British National
Corpus (BNC). Baldwin et al (2004), among
many others, have investigated the main causes
of parse failure, parsing a random sample of
20,000 strings from the written component of
the BNC using the ERG. They have found that
the large majority of failures is caused by miss-
ing lexical entries, with 40% of the cases, and
missing constructions, with 39%, where missing
MWEs accounted for 8% of total errors. That is,
even by a margin, the lexical coverage is lower
than the grammar construction coverage.
This indicates the acute need for robust (semi-
)automated ways of acquiring lexical informa-
tion for MWEs, and this is the one of the goals
of this work. In the next section we discuss
some approaches that have been developed in re-
cent years to (semi-)automatically detect and/or
repair lexical and grammar errors in linguistic
grammars and/or extend their coverage.
4 Acquiring MWEs
The automatic acquisition of specific types of
MWE has attracted much interest (Pearce,
2002; Baldwin and Villavicencio, 2002; Evert
and Krenn, 2005; Villavicencio, 2005; van der
1035
Beek, 2005; Nicholson and Baldwin, 2006). For
instance, Baldwin and Villavicencio (2002) pro-
posed a combination of methods to extract Verb-
Particle Constructions (VPCs) from unanno-
tated corpora, that in an evaluation on the
Wall Street Journal achieved 85.9% precision
and 87.1% recall. Nicholson and Baldwin (2006)
investigated the prediction of the inherent se-
mantic relation of a given compound nominaliza-
tion using as statistical measure the confidence
interval.
On the other hand, Zhang et al (2006) looked
at MWEs in general investigating the semi-
automated detection of MWE candidates in
texts using error mining techniques and vali-
dating them using a combination of the World
Wide Web as a corpus and some statistical mea-
sures. 6248 sentences were then extracted from
the BNC; these contained at least one of the 311
MWE candidates verified with World Wide Web
in the way described in Zhang et al (2006). For
each occurrence of the MWE candidates in this
set of sentences, the lexical type predictor pro-
posed in Zhang and Kordoni (2006) predicted a
lexical entry candidate. This resulted in 373 ad-
ditional MWE lexical entries for the ERG gram-
mar using a words-with-spaces approach. As re-
ported in Zhang et al (2006), this addition to
the grammar resulted in a significant increase in
grammar coverage of 14.4%. However, no fur-
ther evaluation was done of the results of the
measures used on the identification of MWEs or
of the resulting grammar, as not all MWEs can
be correctly handled by the simple words-with-
spaces approach (Sag et al, 2002). And these
are the starting points of the work we are re-
porting on here.
5 Evaluation of the Identification of
MWEs
One way of viewing the MWE identification task
is, given a list of sequences of words, to distin-
guish those that are genuine MWEs (e.g. in the
red), from those that are just sequences of words
that do not form any kind of meaningful unit
(e.g. of alcohol and). In order to do that, one
commonly used approach is to employ statisti-
cal measures (e.g. Pearce (2002) for collocations
and Zhang et al (2006) for MWEs in general).
When dealing with statistical analysis there are
two important statistical questions that should
be addressed: How reliable is the corpus used?
and How precise is the chosen statistical measure
to distinguish the phenomena studied?.
In this section we look at these issues, for the
particular case of trigrams, by testing different
corpora and different statistical measures. For
that we use 1039 trigrams that are the output
of Zhang et al (2006) error mining system, and
frequencies collected from the BNC and from
the World Wide Web. The former were col-
lected from two different portions of the BNC,
namely the fragment of the BNC (BNCf ) used
in the error-mining experiments, and the com-
plete BNC (from the site http://pie.usna.edu/),
to test whether a larger sample of a more ho-
mogeneous and well balanced corpus improves
results significantly. For the latter we used two
different search engines: Google and Yahoo, and
the frequencies collected reflect the number of
pages that had exact matches of the n-grams
searched, using the API tools for each engine.
5.1 Comparing Corpora
A corpus for NLP related work should be a re-
liable sample of the linguistic output of a given
language. For this work in particular, we expect
that the relative ordering in frequency for differ-
ent n-grams is preserved across corpora, in the
same domain (e.g. a corpus of chemistry arti-
cles). For, if this is not the case, different con-
clusions are certain to be drawn from different
corpora.
The first test we performed was a direct com-
parison of the rank plots of the relative fre-
quency of trigrams for the four corpora. We
ranked 1039 MWE-candidate trigrams accord-
ing to their occurrence in each corpus and we
normalised this value by the total number of
times any one of the 1039 trigrams appeared
for each corpus. These normalisation values
were: 66,101 times in BNCf , 322,325 in BNC,
224,479,065 in Google and 6,081,786,313 in Ya-
hoo. It is possible to have an estimate of the size
of each corpus from these numbers: the trigrams
1036
account for something like 0.3% of the BNC cor-
pora, while for Google and Yahoo nothing can
be said since their sizes are not reliable numbers.
Figure 1 displays the results. The overall rank-
ing distribution is very similar for these corpora
showing the expected Zipf like behaviour in spite
of their different sizes.
10-5
10-4
10-3
10-2
10-1
1 10 100 1000
re
la
tiv
e 
fre
qu
en
cy
rank
BNCf
BNC
Google
Yahoo
Figure 1: Relative frequency rank for the 1039
trigrams analysed.
Of course, the information coming from Fig-
ure 1 is not sufficient for our purposes. The or-
der of the trigrams could be very different inside
each corpus. Therefore a second test is needed
to compare the rankings of the n-grams in each
corpus. In order to do that we measure the
Kendall?s ? scores between corpora. Kendall?s ?
is a non-parametric method for estimating cor-
relation between datasets (Press et al, 1992).
For the number of trigrams studied here the
Kendall?s scores obtained imply a significant cor-
relation between the corpora with p<0.000001.
The significance indicates that the data are cor-
related and the null hypothesis of statistical
independence is certainly disproved. Unfortu-
nately disproving the null hypothesis does not
give much information about the degree of cor-
relation; it only asserts that it exists. Thus, it
could be a very insignificant correlation. In ta-
ble 1, we display a more intuitive measure to
estimate the correlation, the probability Q that
any 2 trigrams chosen from two corpora have
the same relative ordering in frequency. This
probability is related to Kendall?s ? through the
expression Q = (1 + ?)/2 .
BNC Google Yahoo
BNCf 0.81 0.73 0.78
BNC 0.73 0.77
Google 0.86
Table 1: The probability Q of 2 trigrams hav-
ing the same frequency rank order for different
corpora.
The results show that the four corpora are
certainly correlated, and can probably be used
interchangeably to access most of the statisti-
cal properties of the trigrams. Interestingly, a
higher correlation was observed between Yahoo
and Google than between BNCf and BNC, even
though BNCf is a fragment of BNC, and there-
fore would be expected to have a very high cor-
relation. This suggests that as corpora sizes
increase, so do the correlations between them,
meaning that they are more likely to agree on
the ranking of a given MWE.
5.2 Comparing statistical measures -
are they equivalent?
Here we concentrate on a single corpus, BNCf ,
and compare the three statistical measures for
MWE identification: Mutual Information (MI),
?2 and Permutation Entropy (PE)(Zhang et al,
2006), to investigate if they order the trigrams
in the same fashion.
MI and ?2 are typical measures of associa-
tion that compare the joint probability of occur-
rence of a certain group of events p(abc) with
a prediction derived from the null hypothesis
of statistical independence between these events
p?(abc) = p(a)p(b)p(c) (Press et al, 1992). In
our case the events are the occurrences of words
in a given position in an n-gram. For a trigram
with words w1w2w3, ?2 is calculated as:
?2 =
?
a,b,c
[ n(abc)? n?(abc) ]2
n?(abc)
where a corresponds either to the word w1 or to
?w1 (all but the word w1) and so on. n(abc)
is the number of trigrams abc in the corpus,
n?(abc) = n(a)n(b)n(c)/N2 is the predicted
number from the null hypothesis, n(a) is the
1037
number of unigrams a, and N the number of
words in the corpus. Mutual Information, in
terms of these numbers, is:
MI =
?
a,b,c
n(abc)
N log2
[ n(abc)
n?(abc)
]
The third measure, permutation entropy, is a
measure of order association. Given the words
w1, w2, and w3, PE is calculated in this work as:
PE = ?
?
(i,j,k)
p(wiwjwk) ln [ p(wiwjwk) ]
where the sum runs over all the permutations
of the indexes and, therefore, over all possible
positions of the selected words in the trigram.
The probabilities are estimated from the number
of occurrences of each permutation of a trigram
(e.g. by and large, large by and, and large by,
and by large, large and by, and by large and) as:
p(w1w2w3) =
n(w1w2w3)
?
(i,j,k)
n(wiwjwk)
PE was proposed by Zhang et al (2006) as a
possible measure to detect MWEs, under the
hypothesis that MWEs are more rigid to per-
mutations and therefore present smaller PEs.
Even though it is quite different from MI and
?2, PE can also be thought as an indirect mea-
sure of statistical independence, since the more
independent the words are the closer PE is from
its maximal value (ln 6, for trigrams). One pos-
sible advantage of this measure over the others
is that it does not rely on single word counts,
which are less accurate in Web based corpora.
Given the rankings produced for each one of
these three measures we again use Kendall?s ?
test to assess correlation and its significance.
Table 2 displays the Q probability of finding
the same ordering in these three measures. The
general conclusion from the table is that even
though there is statistical significance in the cor-
relations found (the p values are not displayed,
but they are very low as before) the differ-
ent measures order the trigrams very differently.
There is a 70% chance of getting the same order
from MI and ?2, but it is safe to say that these
measures are very different from the PE, since
their Q values are very close to pure chance.
MI??2 MI?PE ?2?PE
Q 0.71 0.55 0.45
Table 2: The probability Q of having 2 trigrams
with the same rank order for different statistical
measures.
5.3 Comparing Statistical Measures -
are they useful?
The use of statistical measures is widespread in
NLP but there is no consensus about how good
these measures are for describing natural lan-
guage phenomena. It is not clear what exactly
they capture when analysing the data.
In order to evaluate if they would make good
predictors for MWEs, we compare the measures
distributions for MWEs and non-MWEs. For
that we selected as gold standard a set of around
400 MWE candidates annotated by a native
speaker1 as MWEs or not. We then calculated
the histograms for the values of MI, ?2 and
PE for the two groups. MI and ?2 were cal-
culated only for BNCf . Table 3 displays the re-
sults of the Kolmogorov-Smirnof test (Press et
al., 1992) for these histograms, where the first
value is Kolmogorov-Smirnov D value (D?[0,1]
and large D values indicate large differences be-
tween distributions) and the second is the signif-
icance probability (p) associated to D given the
sizes of the data sets, in this case 90 for MWEs
and 292 for non-MWEs.
MIBNCf ?2BNCf PEY ahoo PEGoogle
D 0.27 0.13 0.27 0.24
p< 0.0001 0.154 0.0001 0.0005
Table 3: Comparison of MI, ?2 and PE
The surprising result is that there is no statis-
tical significance, at least using the Kolmogorov-
Smirnov test, that indicates that being or not
an MWE has some effect in the value of the tri-
gram?s ?2. The same does not happen for MI
or PE. They do seem to differentiate between
MWEs and non-MWEs. As discussed before the
statistical significance implies the existence of an
1The native speaker is a linguist expert in MWEs.
1038
effect but has very little to say about the inten-
sity of the effect. As in the case of this work our
interest is to use the effect to predict MWEs,
the intensity is very important. In the figures
that follow we show the normalised histograms
for MI, ?2(for the BNCf ) and PE (for the case
of Yahoo) for MWEs and non-MWEs. The ideal
scenario would be to have non overlapping dis-
tributions for the two cases, so a simple thresh-
old operation would be enough to distinguish
MWEs. This is not the case in any of the plots.
Starting from Figure 3 it clearly illustrates the
negative result for ?2 in table 3. The other two
distributions show a visible effect in the form of
a slight displacement of the distributions to the
left for MWEs. In particular for the distribution
of PE, the large peak on the right, representing
the n-grams whose word order is irrelevant with
respect to its occurrence, has an important re-
duction for MWEs.
The statistical measures discussed here are
all different forms of measuring correlations be-
tween the component words of MWEs. There-
fore, as some types of MWEs may have stronger
constraints on word order, we believe that more
visible effects can be seen in these measures if we
look at their application for individual types of
MWEs, which is planned for future work. This
will bring an improvement to the power of MWE
prediction of these measures.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 0.2
-5.5 -5 -4.5 -4 -3.5 -3 -2.5 -2
Pr
ob
ab
ilit
y
log(MI)
MWEs
non-MWEs
Figure 2: Normalised histograms of MI values
for MWEs and non-MWEs in BNCf .
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 2  3  4  5  6  7  8
Pr
ob
ab
ilit
y
log(?2)
MWEs
non-MWEs
Figure 3: Normalised histograms of ?2 values
for MWEs and non-MWEs in BNCf .
 0
 0.05
 0.1
 0.15
 0.2
 0.25
-3.5 -3 -2.5 -2 -1.5 -1 -0.5  0  0.5
Pr
ob
ab
ilit
y
log(PE(Yahoo))
MWEs
non-MWEs
Figure 4: Normalised histograms of PE values
for MWEs and non-MWEs in Yahoo.
6 Evaluation of the Extensions to
the Grammar
Our ultimate goal is to maximally automate
the process of discovering and handling MWEs.
With good statistical measures, we are able
to distinguish genuine MWE from non-MWEs
among the n-gram candidates. However, from
the perspective of grammar engineering, even
with a good candidate list of MWEs, great ef-
fort is still required in order to incorporate such
word units into a given grammar automatically
and in a precise way.
Zhang et al (2006) tried a simple ?word with
spaces? approach. By acquiring new lexical en-
tries for the MWEs candidates validated by the
statistical measures, the grammar coverage was
shown to improve significantly. However, no fur-
ther investigation on the parser accuracy was re-
ported there.
Taking a closer look at the MWE candidates
1039
proposed, we find that only a small proportion of
them can be handled appropriately by the?word
with spaces? approach of Zhang et al (2006).
Simply adding new lexical entries for all MWEs
can be a workaround for enhancing the parser
coverage, but the quality of the parser output is
clearly linguistically less interesting.
On the other hand, we also find that a large
proportion of MWEs that cannot be correctly
handled by the grammar can be covered prop-
erly in a constructional way by adding one lex-
ical entry for the head (governing) word of the
MWE. For example, the expression foot the bill
will be correctly handled with a standard head-
complement rule, if there is a transitive verb
reading for the word foot in the lexicon. Some
other examples are: to put forward, the good of,
in combination with, . . . , where lexical exten-
sion to the words in bold will allow the gram-
mar to cover these MWEs. In this paper, we
employ a constructional approach for the acqui-
sition of new lexical entries for the head words
of the MWEs.2
It is arguable that such an approach may lead
to some potential grammar overgeneration, as
there is no selectional restriction expressed in
the new lexical entry. However, as far as the
parsing task is concerned, such overgeneration
is not likely to reduce the accuracy of the gram-
mar significantly as we show later in this paper
through a thorough evaluation.
6.1 Experimental Setup
With the complete list of 1039 MWE candidates
discussed in section 5, we rank each n-gram
according to each of the three statistical mea-
sures. The average of all the rankings is used
as the combined measure of the MWE candi-
dates. Since we are only interested in acquiring
new lexical entries for MWEs which are not cov-
ered by the grammar, we used the error mining
results (Zhang et al, 2006; van Noord, 2004)
to only keep those candidates with parsability
? 0.1. The top 30 MWE candidates are used in
2The combination of the ?word with space? approach
of Zhang et al (2006) with the constructional approach
we propose here is an interesting topic that we want to
investigate in future research.
this experiment.
We used simple heuristics in order to extract
the head words from these MWEs:
? the n-grams are POS-tagged with an auto-
matic tagger;
? finite verbs in the n-grams are extracted as
head words;
? nouns are also extracted if there is no verb
in the n-gram.
Occasionally, the tagger errors might introduce
wrong head words. However, the lexical type
predictor of Zhang and Kordoni (2006) that we
used in our experiments did not generate inter-
esting new entries for them in the subsequent
steps, and they were thus discarded, as discussed
below.
With the 30 MWE candidates, we extracted
a sub-corpus from the BNC with 674 sentences
which included at least one of these MWEs. The
lexical acquisition technique described in Zhang
and Kordoni (2006) was used with this sub-
corpus in order to acquire new lexical entries for
the head words. The lexical acquisition model
was trained with the Redwoods treebank (Oepen
et al, 2002), following Zhang et al (2006).
The lexical prediction model predicted for
each occurrence of the head words a most plau-
sible lexical type in that context. Only those
predictions that occurred 5 times or more were
taken into consideration for the generation of the
new lexical entries. As a result, we obtained 21
new lexical entries.
These new lexical entries were later merged
into the ERG lexicon. To evaluate the grammar
performance with and without these new lexical
entries, we
1. parsed the sub-corpus with/without new
lexical entries and compared the grammar
coverage;
2. inspected the parser output manually and
evaluated the grammar accuracy.
In parsing the sub-corpus, we used the PET
parser (Callmeier, 2001). For the manual eval-
1040
uation of the parser output, we used the tree-
banking tools of the [incr tsdb()] system (Oepen,
2001).
6.2 Grammar Performance
Table 4 shows that the grammar coverage im-
proved significantly (from 7.1% to 22.7%) with
the acquired lexical entries for the head words
of the MWEs. This improvement in coverage
is largely comparable to the result reported in
(Zhang et al, 2006), where the coverage was re-
ported to raise from 5% to 18% with the ?word
with spaces? approach (see also section 4).
It is also worth mentioning that Zhang et al
(2006) added 373 new lexical entries for a to-
tal of 311 MWE candidates, with an average
of 1.2 entries per MWE. In our experiment, we
achieved a similar coverage improvement with
only 21 new entries for 30 different MWE candi-
dates, with an average of 0.7 entries per MWE.
This suggests that the lexical entries acquired
in our experiment are of much higher linguistic
generality.
To evaluate the grammar accuracy, we man-
ually checked the parser outputs for the sen-
tences in the sub-corpus which received at least
one analysis from the grammar before and af-
ter the lexical extension. Before the lexical ex-
tension, 48 sentences are parsed, among which
32 (66.7%) sentences contain at least one cor-
rect reading (table 4). After adding the 21 new
lexical entries, 153 sentences are parsed, out of
which 124 (81.0%) sentences contain at least one
correct reading.
Baldwin et al (2004) reported in an earlier
study that for BNC data, about 83% of the sen-
tences covered by the ERG have a correct parse.
In our experiment, we observed a much lower
accuracy on the sub-corpus of BNC which con-
tains a lot of MWEs. However, after the lexical
extension, the accuracy of the grammar recovers
to the normal level.
It is also worth noticing that we did not re-
ceive a larger average number of analyses per
sentence (table 4), as it was largely balanced
by the significant increase of sentences covered
by the new lexical entries. We also found
that the disambiguation model as described by
Toutanova et al (2002) performed reasonably
well, and the best analysis is ranked among top-
5 for 66% of the cases, and top-10 for 75%.
All of these indicate that our approach of lexi-
cal acquisition for head words of MWEs achieves
a significant improvement in grammar coverage
without damaging the grammar accuracy. Op-
tionally, the grammar developers can check the
validity of the lexical entries before they are
added into the lexicon. Nonetheless, even a
semi-automatic procedure like this can largely
reduce the manual work of grammar writers.
7 Conclusions
In this paper we looked at some of the issues
involved in the evaluation of the identification
of MWEs. In particular we evaluated the use
of three statistical measures for automatically
identifying MWEs. The results suggest that at
least two of them (MI and PE) can distinguish
MWEs. In terms of the corpora used, a sur-
prisingly higher level of agreement was found
between different corpora (Google and Yahoo)
than between two fragments of the same one.
This tells us two lessons. First that even though
Google and Yahoo were not carefully built to be
language corpora their sizes compensate for that
making them fairly good samples of language
usage. Second, a fraction of a smaller well bal-
anced corpus may not necessarily be as balanced
as the whole.
Furthermore, we argued that for precise gram-
mar engineering it is important to perform a
careful evaluation of the effects of including au-
tomatically acquired MWEs to a grammar. We
looked at the evaluation of the effects in cover-
age, size of the grammar and accuracy of the
parses after adding the MWE-candidates. We
adopted a compositional approach to the en-
coding of MWEs, using some heuristics to de-
tect the head of an MWE, and this resulted in
a smaller grammar than that by Zhang et al
(2006), still achieving a similar increase in cov-
erage and maintaining a high level of accuracy of
parses, comparable to that reported by Baldwin
et al (2004).
The statistical measures are currently only
1041
item # parsed # avg. analysis # coverage %
ERG 674 48 335.08 7.1%
ERG + MWE 674 153 285.01 22.7%
Table 4: ERG coverage with/without lexical acquisition for the head words of MWEs
used in a preprocessing step to filter the non-
MWEs for the lexical type predictor. Alterna-
tively, the statistical outcomes can be incorpo-
rated more tightly, i.e. to combine with the lex-
ical type predictor and give confidence scores on
the resulting lexical entries. These possibilities
will be explored in future work.
References
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the unextractable: A case study on verb-
particles. In Proc. of the 6th Conference on Nat-
ural Language Learning (CoNLL-2002), Taipei,
Taiwan.
Timothy Baldwin, Emily M. Bender, Dan Flickinger,
Ara Kim, and Stephan Oepen. 2004. Road-testing
the English Resource Grammar over the British
National Corpus. In Proceedings of the Fourth
International Conference on Language Resources
and Evaluation (LREC 2004), Lisbon, Portugal.
Gosse Bouma, Gertjan van Noord, and Robert Mal-
ouf. 2001. Alpino: Wide-coverage computational
analysis of dutch. In Computational Linguistics in
The Netherlands 2000.
Ted Briscoe and John Carroll. 1997. Automatic
extraction of subcategorization from corpora. In
Fifth Conference on Applied Natural Language
Processing, Washington, USA.
Ulrich Callmeier. 2001. Efficient parsing with large-
scale unification grammars. Master?s thesis, Uni-
versita?t des Saarlandes, Saarbru?cken, Germany.
Stefan Evert and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of sta-
tistical association measures. Computer Speech
and Language, 19(4):450?466.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28.
Ray Jackendoff. 1997. Twistin? the night away. Lan-
guage, 73:534?59.
Jeremy Nicholson and Timothy Baldwin. 2006. In-
terpretation of compound nominalisations using
corpus and web statistics. In Proceedings of the
Workshop on Multiword Expressions: Identifying
and Exploiting Underlying Properties, pages 54?
61, Sydney, Australia. Association for Computa-
tional Linguistics.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and
Thorsten Brants. 2002. The LinGO Redwoods
treebank: Motivation and preliminary applica-
tions. In Proceedings of COLING 2002: The 17th
International Conference on Computational Lin-
guistics: Project Notes, Taipei.
Stephan Oepen. 2001. [incr tsdb()] ? competence
and performance laboratory. User manual. Tech-
nical report, Computational Linguistics, Saarland
University, Saarbru?cken, Germany.
Darren Pearce. 2002. A comparative evaluation of
collocation extraction techniques. In Third Inter-
national Conference on Language Resources and
Evaluation, Las Palmas, Canary Islands, Spain.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 1992. Numerical
Recipes in C: The Art of Scientific Computing.
Second edition. Cambridge University Press.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-2002), pages 1?15, Mexico City,
Mexico.
Kristina Toutanova, Christoper D. Manning, Stu-
art M. Shieber, Dan Flickinger, and Stephan
Oepen. 2002. Parse ranking for a rich HPSG
grammar. In Proceedings of the First Workshop
on Treebanks and Linguistic Theories (TLT2002),
pages 253?263, Sozopol, Bulgaria.
Leonoor van der Beek. 2005. The extraction of
determinerless pps. In Proceedings of the ACL-
SIGSEM Workshop on The Linguistic Dimensions
of Prepositions and their Use in Computational
Linguistics Formalisms and Applications, Colch-
ester, UK.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of
1042
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages
446?453, Barcelona, Spain, July.
Aline Villavicencio, Francis Bond, Anna Korhonen,
and Diana McCarthy. 2005. Introduction to the
special issue on multiword expressions: having a
crack at a hard nut. Journal of Computer Speech
and Language Processing, 19(4):365?377.
Aline Villavicencio. 2005. The availability of verb-
particle constructions in lexical resources: How
much is enough? Journal of Computer Speech
and Language Processing, 19.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts process-
ing. In Proceedings of the Fifth International
Conference on Language Resources and Evalua-
tion (LREC 2006), Genoa, Italy.
Yi Zhang, Valia Kordoni, Aline Villavicencio, and
Marco Idiart. 2006. Automated multiword ex-
pression prediction for grammar engineering. In
Proceedings of the Workshop on Multiword Ex-
pressions: Identifying and Exploiting Underlying
Properties, pages 36?44, Sydney, Australia. Asso-
ciation for Computational Linguistics.
1043
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 36?44,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automated Multiword Expression Prediction for Grammar Engineering
Yi Zhang & Valia Kordoni
Dept. of Computational Linguistics
Saarland University
D-66041 Saarbru?cken, Germany
{yzhang,kordoni}@coli.uni-sb.de
Aline Villavicencio & Marco Idiart
Institutes of Informatics & Physics
Federal University of Rio Grande do Sul
Av. Bento Gonc?alves, 9500
Porto Alegre - RS, Brazil
avillavicencio@inf.ufrgs.br
idiart@if.ufrgs.br
Abstract
However large a hand-crafted wide-
coverage grammar is, there are always go-
ing to be words and constructions that
are not included in it and are going to
cause parse failure. Due to their hetero-
geneous and flexible nature, Multiword
Expressions (MWEs) provide an endless
source of parse failures. As the number
of such expressions in a speaker?s lexi-
con is equiparable to the number of single
word units (Jackendoff, 1997), one ma-
jor challenge for robust natural language
processing systems is to be able to deal
with MWEs. In this paper we propose
to semi-automatically detect MWE can-
didates in texts using some error mining
techniques and validating them using a
combination of the World Wide Web as a
corpus and some statistical measures. For
the remaining candidates possible lexico-
syntactic types are predicted, and they are
subsequently added to the grammar as new
lexical entries. This approach provides
a significant increase in the coverage of
these expressions.
1 Introduction
Hand-crafted large-scale grammars like the En-
glish Resource Grammar (Flickinger, 2000), the
Pargram grammars (Butt et al, 1999) and the
Dutch Alpino Grammar (Bouma et al, 2001)
are extremely valuable resources that have been
used in many NLP applications. However, due
to the open-ended and dynamic nature of lan-
guages, and the difficulties of grammar engineer-
ing, such grammars are likely to contain errors
and be incomplete. An error can be roughly clas-
sified as under-generating (if it prevents a gram-
matical sentence to be generated/parsed) or over-
generating (if it allows an ungrammatical sen-
tence to be generated/parsed). In the context of
wide-coverage parsing, we focus on the under-
generating errors which normally lead to parsing
failure.
Traditionally, the errors of the grammar are to
be detected manually by the grammar develop-
ers. This is usually done by running the grammar
over a carefully designed test suite and inspecting
the outputs. This procedure becomes less reliable
as the grammar gets larger, and is especially dif-
ficult when the grammar is developed in a dis-
tributed manner. Baldwin et al (2004), among
many others, for instance, have investigated the
main causes of parse failure, parsing a random
sample of 20,000 strings from the written com-
ponent of the British National Corpus (hencefor-
ward BNC) using the English Resource Gram-
mar (Flickinger, 2000), a broad-coverage preci-
sion HPSG grammar for English. They have found
that the large majority of failures are caused by
missing lexical entries, with 40% of the cases, and
missing constructions, with 39%.
To this effect, as mentioned above, in recent
years, some approaches have been developed in
order to (semi)automatically detect and/or repair
the errors in linguistic grammars. van Noord
(2004), for instance, takes a statistical approach
towards semi-automated error detection using the
parsability metric for word sequences. He reports
on a simple yet practical way of identifying gram-
mar errors. The method is particularly useful for
discovering systematic problems in a large gram-
mar with reasonable coverage. The idea behind it
is that each (under-generating) error in the gram-
36
mar leads to the parsing failure of some specific
grammatical sentences. By running the grammar
over a large corpus, the corpus can be split into
two subsets: the set of sentences covered by the
grammar and the set of sentences that failed to
parse. The errors can be identified by comparing
the statistical difference between these two sets
of sentences. By statistical difference, any kind
of uneven distribution of linguistic phenomena is
meant. In the case of van Noord (2004), the word
sequences are used, mainly because the cost to
compute and count the word sequences is mini-
mum. The parsability of a sequence wi . . . wj is
defined as:
R(wi . . . wj) =
C(wi . . . wj, OK)
C(wi . . . wj)
(1)
where C(wi . . . wj) is the number of sentences
in which the sequence wi . . . wj occurs, and
C(wi . . . wj , OK) is the number of sentences with
a successful parse which contain the sequence.
A frequency cut is used to eliminate the infre-
quent sequences. With suffix arrays and perfect
hashing automata, the parsability of all word se-
quences (with arbitrary length) can be computed
efficiently. The word sequences are then sorted
according to their parsabilities. Those sequences
with the lowest parsabilities are taken as direct in-
dication of grammar errors.
Among them, one common error, and sub-
sequently very common cause of parse failure
is due to Multiword Expressions (MWEs), like
phrasal verbs (break down), collocations (bread
and butter), compound nouns (coffee machine),
determiner-less PPs (in hospital), as well as so-
called ?frozen expressions? (by and large), as dis-
cussed by both Baldwin et al (2004) and van No-
ord (2004). Indicatively, in the experiments re-
ported in Baldwin et al (2004), for instance, from
all the errors due to missing lexical entries, one
fifth were due to missing MWEs (8% of total er-
rors). If an MWE is syntactically marked, the stan-
dard grammatical rules and lexical entries cannot
generate the string, as for instance in the case of
a phrasal verb like take off, even if the individual
words that make up the MWE are contained in the
lexicon.
In this paper we investigate semi-automatic
methods for error mining and detection of miss-
ing lexical entries, following van Noord (2004),
with the subsequent handling of the MWEs among
them. The output of the error mining phase pro-
poses a set of n-grams, which also contain MWEs.
Therefore, the task is to distinguish the MWEs
from the other cases. To do this, first we propose
to use the World Wide Web as a very large corpus
from which we collect evidence that enables us to
rule out noisy cases (due to spelling errors, for in-
stance), following Grefenstette (1999), Keller et
al. (2002), Kilgarriff and Grefenstette (2003) and
Villavicencio (2005). The candidates that are kept
can be semi-automatically included in the gram-
mar, by employing a lexical type predictor, whose
output we use in order to add lexical entries to the
lexicon, with a possible manual check by a gram-
mar writer. This procedure significantly speeds up
the process of grammar development, relieving the
grammar developer of some of the burden by au-
tomatically detecting parse failures and providing
semi-automatic means for handling them.
The paper starts with a discussion of MWEs and
of some of the characteristics that make them so
challenging for NLP, in section 2. This is followed
by a more detailed discussion of the technique
employed for error detection, in section 3. The
approach used for distinguishing noisy sequences
from MWE-related constructions using the World
Wide Web is then presented. How this information
is used for extending the grammar and the results
obtained are then addressed in section 5.
2 Multiword Expressions
The term Multiword Expressions (MWEs) has
been used to describe expressions for which the
syntactic or semantic properties of the whole ex-
pression cannot be derived from its parts ((Sag et
al., 2002), (Villavicencio et al, 2005)), including
a large number of related but distinct phenomena,
such as phrasal verbs (e.g. come along), nomi-
nal compounds (e.g. frying pan), institutionalised
phrases (e.g. bread and butter), and many oth-
ers. They are used frequently in language, and
in English, Jackendoff (1997) estimates the num-
ber of MWES in a speaker?s lexicon to be com-
parable to the number of single words. This is re-
flected in several existing grammars and lexical re-
sources, where almost half of the entries are Mul-
tiword Expressions. However, due to their hetero-
geneous characteristics, MWEs present a tough
challenge for both linguistic and computational
work (Sag et al, 2002). Some MWEs are fixed,
and do not present internal variation, such as ad
37
hoc, while others allow different degrees of inter-
nal variability and modification, such as touch a
nerve (touch/find a nerve) and spill beans (spill
several/musical/mountains of beans). In terms of
semantics, some MWEs are more opaque in their
meaning (e.g. to kick the bucket as to die), while
others have more transparent meanings that can be
inferred from the words in the MWE (e.g. eat up,
where the particle up adds a completive sense to
eat). Therefore, to provide a unified account for
the detection of these distinct but related phenom-
ena is a real challenge for NLP systems.
3 Detection of Errors: Overview
van Noord (2004) reports on various errors that
have been discovered for the Dutch Alpino Gram-
mar (Bouma et al, 2001) semi-automatically, us-
ing the Twente Nieuws Corpus. The idea pur-
sued by van Noord (2004) has been to locate those
n-grams in the input that might be the cause of
parsing failure. By processing a huge amount
of data, the parsability metrics briefly presented
in section 1 have been used to successfully lo-
cate various errors introduced by the tokenizer,
erroneous/incomplete lexical descriptions, frozen
expressions with idiosyncratic syntax, or incom-
plete grammatical descriptions. However, the re-
covery of these errors has been shown to still re-
quire significant efforts from the grammar devel-
oper. Moreover, there is no concrete data given
about the distribution of the different types of er-
rors discovered.
As also mentioned before, among the n-grams
that usually cause parse failures, there is a large
number of missing MWEs in the lexicon such
as phrasal verbs, collocations, compound nouns,
frozen expressions (e.g. by and large, centre of
attention, put forward by, etc).
For the purpose of the detection of MWEs, we
are interested in seeing what the major types of er-
ror for a typical large-scale deep grammar are. In
this context, we have run the error mining experi-
ment reported by van Noord with the English Re-
source Grammar (ERG; (Flickinger, 2000))1 and
the British National Corpus 2.0 (BNC; (Burnard,
2000)).
We have used a subset of the BNC written com-
ponent. The sentences in this collection contain
no more than 20 words and only ASCII characters.
1ERG is a large-scale HPSG grammar for English. In this
paper, we have used the January 2006 release of the grammar.
That is about 1.8M distinct sentences.
These sentences have then be fed into an effi-
cient HPSG parser (PET; (Callmeier, 2000)) with
ERG loaded. The parser has been configured with
a maximum edge number limit of 100K and has
run in the best-only mode so that it does not ex-
haustively find all the possible parses. The result
of each sentence is marked as one of the following
four cases:
? P means at least one parse is found for the
sentence;
? L means the parser halted after the morpho-
logical analysis and has not been able to con-
struct any lexical item for the input token;
? N means the search has finished normally
and there is no parse found for the sentence;
? E means the search has finished abnormally
by exceeding the edge number limit.
It is interesting to notice that when the ambigu-
ity packing mechanism (Oepen and Carroll, 2000)
is used and the unpacking is turned off 2, E does
not occur at all for our test corpus. Running the
parsability checking over the entire collection of
sentences has taken the parser less than 2 days on
a 64bit machine with 3GHz CPU. The results are
shown in Table 1.
Result # Sentences Percentage
P 644,940 35.80%
L 969,452 53.82%
N 186,883 10.38%
Table 1: Distribution of Parsing Results
?From the results shown in Table 1, one can see
that ERG has full lexical span for less than half of
the sentences. For these sentences, about 80% are
successfully parsed. These numbers show that the
grammar coverage has a significant improvement
as compared to results reported by Baldwin et al
(2004) and Zhang and Kordoni (2006), mainly at-
tributed to the increase in the size of the lexicon
and the new rules to handle punctuations and frag-
ments.
Obviously, L indicates the unknown words in
the input sentence. But for N , it is not clear where
2For the experiment of error mining, only the parsability
checking is necessary. There is no need to record the exact
parses.
38
and what kind of error has occurred. In order
to pinpoint the errors, we used the error mining
techniques proposed by van Noord (2004) on the
grammar and corpus. We have taken the sentences
marked as N (because the errors in L sentences
are already determined) and calculate the word se-
quence parsabilities against the sentences marked
as P . The frequency cut is set to be 5. The whole
process has taken no more than 20 minutes, result-
ing in total the parsability scores for 35K n-grams
(word sequences). The distribution of n-grams in
length with parsability below 0.1 is shown in Ta-
ble 2.
Number Percentage
uni-gram 798 20.84%
bi-gram 2,011 52.52%
tri-gram 937 24.47%
Table 2: Distribution of N-gram in Length in Error
Mining Results (R(x) < 0.1)
Although pinpointing the problematic n-grams
still does not tell us what the exact errors are, it
does shed some light on the cause. From Table 2
we see quite a lot of uni-grams with low parsabil-
ities. Table 3 gives some examples of the word
sequences. By intuition, we make the bold as-
sumption that the low parsability of uni-grams is
caused by the missing appropriate lexical entries
for the corresponding word.3
For the bi-grams and tri-grams, we do see a lot
of cases where the error can be repaired by just
adding a multiword lexical entry into the grammar.
N-gram Count
professionals 248
the flat 62
indication of 21
tone of voice 19
as always is 7
Table 3: Some Examples of the N-grams in Error
Mining Results
In order to distinguish those n-grams that can
be added into the grammar as MWE lexical en-
tries from the other cases, we propose to vali-
date them using evidence collected from the World
Wide Web.
3It has later been confirmed with the grammar developer
that almost all of the errors detected by these low parsability
uni-grams can be fixed by adding correct lexical entries.
4 Detection of MWEs and related
constructions
Recently, many researchers have started using the
World Wide Web as an extremely large corpus,
since, as pointed out by Grefenstette (1999), the
Web is the largest data set available for NLP
((Grefenstette, 1999), (Keller et al, 2002), (Kil-
garriff and Grefenstette, 2003) and (Villavicencio,
2005)). For instance, Grefenstette employs the
Web to do example-based machine translation of
compounds from French into English. The method
he employs would suffer considerably from data
sparseness, if it were to rely only on corpus data.
So for compounds that are sparse in the BNC he
also obtains frequencies from the Web. The scale
of the Web can help to minimise the problem of
data sparseness, that is especially acute for MWEs,
and Villavicencio (2005) uses the Web to find ev-
idence to verify automatically generated VPCs.
This work is built on these, in that we propose
to employ the Web as a corpus, using frequencies
collected from the Web to detect MWEs among
the n-grams that cause parse failure. We concen-
trate on the 482 most frequent candidates, to verify
t he method.
The candidate list has been pre-processed to re-
move systematic unrelated entries, like those in-
cluding acronyms, names, dates and numbers, fol-
lowing Bouma and Villada (2002). Using Google
as a search engine, we have looked for evidence
on the Web for each of the candidate MWEs, that
have occurred as an exact match in a webpage. For
each candidate searched, Google has provided us
with a measure of frequency in the form of the
number of pages in which it appears. Table 4
shows the 10 most frequent candidates, and among
these there are parts of formulae, frozen expres-
sions and collocations. Table 5 on the other hand,
shows the 10 least frequent candidates. From the
total of candidates, 311 have been kept while the
other have been discarded as noise.
A manual inspection of the candidates has re-
vealed that indeed the list contains a large amount
of MWEs and frozen expressions like taking into
account the, good and evil, by and large, put for-
ward by and breach of contract. Some of these
cases, like come into effect in, have very spe-
cific subcategorisation requirements, and this is re-
flected by the presence of the prepositions into and
in in the ngram. Other cases seem to be part of
formulae, like but also in, as part of not only X but
39
Table 4: Top 10 Candidate Multiword Expressions
MWE Pages Entropy Prob(%)
the burden of 36600000 0.366 79.4
and cost effective 34400000 0.372 70.7
the likes of 34400000 0.163 93.1
but also in 27100000 0.038 98.9
to bring together 25700000 0.086 96.6
points of view 24500000 0.017 99.6
and the more 23700000 0.512 61.5
with and without 23100000 0.074 97.4
can do for 22300000 0.003 99.9
taking into account the 22100000 0.009 99.6
but what about 21000000 0.045 98.7
the ultimate in 17400000 0.199 90.0
Table 5: Bottom 10 Candidate Multiword Expressions
MWE Pages Entropy Prob (%)
stand by and 1350000 0.399 65.5
discharged from hospital 553000 0.001 99.9
shock of it 92300 0.541 44.6
was woken by 91400 0.001 99.9
telephone rang and 43700 0.026 99.2
glanced across at 36900 0.003 99.9
the citizens charter 22900 0.070 97.9
input is complete 13900 0.086 97.2
from of government 706 0.345 0.1
the to infinitive 561 0.445 1.4
40
also Y, but what about, and the more the (part of
the more the Yer).
However, among the candidates there still re-
main those that are not genuine MWEs, like of al-
cohol and and than that in, which contain very fre-
quent words that enable them to obtain a very high
frequency count without being an MWE. There-
fore, to detect these cases, the remainder of the
candidates could be further analysed using some
statistical techniques to try to distinguish them
from the more likely MWEs among the candi-
dates. This is done by Bouma and Villada (2002)
who investigated some measures that have been
used to identify certain kinds of MWEs, focusing
on collocational prepositional phrases, and on the
tests of mutual information, log likelihood and ?2.
One significant difference here is that this work is
not constrained to a particular type of MWEs, but
has to deal with them in general. Moreover, the
statistical measures used by Bouma and Villada
demand the knowledge of single word frequencies
which can be a problem when using Google espe-
cially for common words like of and a.
In Tables 4 and 5 we present two alternative
measures that combined can help to detect false
candidates. The rational is similar to the statis-
tical tests, without the need of searching for the
frequency of each of the words that make up the
MWE. We assume that if a candidate is just a
result of the random occurrence of very frequent
words most probably the order of the words in the
ngram is not important. Therefore, given a can-
didate, such as the likes of, we measure the fre-
quency of occurrence of all its permutations (e.g.
the of likes, likes the of, etc) and we calculate the
candidate?s entropy as
S = ? 1logN
N
?
k=1
Pi logPi (2)
where Pi is the probability of occurrence of a
given permutation, and N the total number of per-
mutations. The entropy above defined has its max-
imum at S = 1 when all permutations are equally
probably, which indicates a clear signature of a
random nature. On the other hand, when order is
very important and only a single configuration is
allowed the entropy has its minimum, S = 0. An
ngram with low entropy has good chances of being
an MWE. A close inspection on Table 4 shows that
the top two candidate ngrams have relatively high
entropies ( here we consider high entropy when
S > 0.3 ). In the first case this can be explained
by the fact that the word the can appear after the
word of without compromising the MWE mean-
ing as in the burden of the job. In the second case
it shows that the real MWE is cost effective and
the word and can be either in the beginning or in
the end of the trigram. In fact for a trigram with
only two acceptable permutations the entropy is
S = log 2/ log 6 ' 0.39, very close to what is
obtained .
We also show the probability of occurrence
of each candidate ngram among its permutations
(P1). Most of the candidates in the list are more
frequent than their permutations. In Table 4 we
find two exceptions which are clearly spelling er-
rors in the last 2 ngrams. Therefore low P1 can
be a good indicative of a noisy candidate. Another
good predictor is the relative frequency between
the candidates. Given the occurrence values for
the most frequent candidates, we consider that by
using a threshold of 20,000 occurrences, it is pos-
sible to remove the more noisy cases.
We note that the grammar can also impose some
restrictions in the order of the elements in the
ngram, in the sense that some of the generated
permutations are ungrammatical (e.g. the of likes)
and will most probably have null or very low fre-
quencies. Therefore, on top of the constraints on
the lexical order there are also constraints on the
constituent order of a candidate which will be re-
flected in these measures.4
The remainder candidates can be semi-
automatically included in the grammar, by using
a lexical type predictor, as described in the next
section. With this information, each candidate is
added as a lexical entry, with a possible manual
check by a grammar writer prior to inclusion in
the grammar.
4Google ignores punctuation between the elements of the
ngram. This can lead to some hits being returned for some
of the ungrammatical permuted ngrams, such as one one by
in the sentence We?re going to catch people one by one. One
day,... from www.beertravelers.com/lists/drafttech.html. On
the other hand, Google only returns the number of pages
where a given ngram occurred, but not the number of times it
occurred in that page. This can result in a huge underestima-
tion especially for very frequent ngrams and words, which
can be used mo re than once in a given page. Therefore,
a conservative view of these frequencies must be adopted,
given that for some ngrams they might be inflated and for
others deflated.
41
5 Automated Deep Lexical Acquisition
In section (3), we have seen that more than 50%
of the sentences contain one or more unknown
words. And about half of the other parsing failures
are also due to lexicon missing. In this section, we
propose a statistical approach towards lexical type
prediction for unknown words, including multi-
word expressions.
5.1 Atomic Lexical Types
Lexicalist grammars are normally composed of a
limited number of rules and a lexicon with rich
linguistic features attached to each entry. Some
grammar formalisms have a type inheriting system
to encode various constraints, and a flat structure
of the lexicon with each entry mapped onto one
type in the inheritance hierarchy. The following
discussion is based on Head-driven Phrase Struc-
ture Grammar (HPSG) (Pollard and Sag, 1994),
but should be easily adapted to other formalisms,
as well.
The lexicon of HPSG consists of a list of well-
formed Typed Feature Structures (TFSs) (Carpen-
ter, 1992), which convey the constraints on spe-
cific words by two ways: the type compatibility,
and the feature-value consistency. Although it is
possible to use both features and types to con-
vey the constraints on lexical entries, large gram-
mars prefer the use of types in the lexicon because
the inheritance system prevents the redundant def-
inition of feature-values. And the feature-value
constraints in the lexicon can be avoided by ex-
tending the types. Say we have n lexical entries
Li :t
[
F a1
] . . . Ln :t
[
F an
]
. They share the same
lexical type t, but take different values for the fea-
ture F . If a1, . . . , an are the only possible values
for F in the context of type t, we can extend the
type t with subtypes ta1 :t
[
F a1
] . . . tan :t
[
F an
]
and modify the lexical entries to use these new
types, respectively. Based on the fact that large
grammars normally have a very restricted num-
ber of feature-values constraints for each lexical
type, the increase of the types is acceptable. It is
also typical that the types assigned to lexical en-
tries are maximum on the type hierarchy, which
means that they have no further subtypes. We will
call the maximum lexical types after extension the
atomic lexical types. Then the lexicon will be a
multi-valued mapping from the word stems to the
atomic lexical types.
Needless to underline here that all we have
mentioned above is not applicable exclusively to
HPSG, but to many other formalisms based on
TFSs, which makes our assumptions about atomic
lexical types all the more relevant for a wide range
of systems and applications.
5.2 Statistical Lexical Type Predictor
Given that the lexicon of deep grammars can be
modelled by a mapping from word stems to atomic
lexical types, we now go on designing the statisti-
cal methods that can automatically ?guess? such
mappings for unknown words.
Similar to Baldwin (2005), we also treat the
problem as a classification task. But there is an im-
portant difference. While Baldwin (2005) makes
predictions for each unknown word, we create a
new lexical entry for each occurrence of the un-
known word. The assumption behind this is that
there should be exactly one lexical entry that cor-
responds to the occurrence of the word in the given
context5.
We use a single classifier to predict the atomic
lexical type. There are normally hundreds of
atomic lexical types for a large grammar. So the
classification model should be able to handle a
large number of output classes. We choose the
Maximum Entropy-based model because it can
easily handle thousands of features and a large
number of possible outputs. It also has the ad-
vantages of general feature representation and no
independence assumption between features. With
the efficient parameter estimation algorithms dis-
cussed by Malouf (2002), the training of the model
is now very fast.
For our prediction model, the probability of a
lexical type t given an unknown word and its con-
text c is:
p(t|c) = exp(
?
i ?ifi(t, c))
?
t??T exp(
?
i ?ifi(t?, c))
(3)
where feature fi(t, c) may encode arbitrary char-
acteristics of the context. The parameters <
?1, ?2, . . . > can be evaluated by maximising the
pseudo-likelihood on a training corpus (Malouf,
2002). The detailed design and feature selec-
tion for the lexical type predictor are described in
Zhang and Kordoni (2006).
5Lexical ambiguity is not considered here for the un-
knowns. In principle, this constraint can be relaxed by allow-
ing the classifier to return more than one results by, setting a
confidence threshold, for example.
42
In the experiment described here, we have used
the latest version of the Redwoods Treebank in or-
der to train the lexical type predictor with morpho-
logical features and context words/POS tags fea-
tures 6. We have then extracted from the BNC
6248 sentences, which contain at least one of the
311 MWE candidates verified with World Wide
Web in the way described in the previous section.
For each occurrence of the MWE candidates in
this set of sentences, our lexical type predictor has
predicted a lexical entry candidate. This has re-
sulted in 1936 distinct entries. Only those entries
with at least 5 counts have been added into the
grammar. This has resulted in an extra 373 MWE
lexical entries for the grammar.
This addition to the grammar has resulted in a
significant increase in coverage (table 6) of 14.4%.
This result is very promising, as only a subset of
the candidate MWEs has been analysed, and could
result in an even greater increase in coverage, if
these techniques were applied to the complete set
of candidates.
However, we should also point out that the cov-
erage numbers reported in Table 6 are for a set
of ?difficult? sentences which contains a lot of
MWEs. When compared to the numbers reported
in Table 1, the coverage of the parser on this data
set after adding the MWE entries is still signifi-
cantly lower. This indicates that not all the MWEs
can be correctly handled by simply adding more
lexical entries. Further investigation is still re-
quired.
6 Conclusions
One of the important challenges for robust natural
language processing systems is to be able to deal
with the systematic parse failures caused in great
part by Multiword Expressions and related con-
structions. Therefore, in this paper we have pro-
posed an approach for the semi-automatic exten-
sion of grammars by using an error mining tech-
nique for the detection of MWE candidates in texts
and for predicting possible lexico-syntactic types
for them. The approach presented is based on that
of van Noord (2004) and proposes a set of MWE
candidates. For this set of candidates, using the
World Wide Web as a large corpus, frequencies are
gathered for each candidate. These in conjunction
with some statistical measures are employed for
ruling out noisy cases like spelling mistakes (from
6The POS tags are produced with the TnT tagger.
of government) and frequent non-MWE sequences
like input is complete.
With this information the remaining sequences
are analysed by a statistical type predictor that as-
signs the most likely lexical type for each of the
candidates in a given context. By adding these to
the grammar as new lexical entries, a considerable
increase in coverage of 14.4% was obtained.
The approach proposed employs simple and
self-contained techniques that are language-
independent and can help to semi-automatically
extend the coverage of a grammar without rely-
ing on external resources, like electronic dictio-
naries and ontologies that are expensive to obtain
and not available for all languages. Therefore, it
provides an inexpensive and reusable manner of
helping and speeding up the grammar engineer-
ing process, by relieving the grammar developer
of some of the burden of extending the coverage
of the grammar.
As future work we intend to investigate further
statistical measures that can be applied robustly to
different types of MWEs for refining even more
the list of candidates and distinguishing false pos-
itives, like of alcohol and from MWEs, like put
forward by. The high frequency with which the
former occur in corpora and the more accute prob-
lem of data sparseness that affects the latter make
this a difficult task.
References
Timothy Baldwin, Emily M. Bender, Dan Flickinger,
Ara Kim, and Stephan Oepen. 2004. Road-testing
the English Resource Grammar over the British Na-
tional Corpus. In Proceedings of the Fourth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2004), Lisbon, Portugal.
Timothy Baldwin. 2005. Bootstrapping deep lexical
resources: Resources for courses. In Proceedings
of the ACL-SIGLEX Workshop on Deep Lexical Ac-
quisition, pages 67?76, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Gosse Bouma and Begon?a Villada. 2002. Corpus-
based acquisition of collocational prepositional
phrases. In Proceedings of the Computational Lin-
guistics in the Netherlands (CLIN) 2001, University
of Twente.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide-coverage computational anal-
ysis of dutch. In Computational Linguistics in The
Netherlands 2000.
43
Entries Added Item # Covered # Coverage
ERG 0 6246 268 4.3%
ERG+MWE(Web) 373 6246 1168 18.7%
Table 6: Parser coverage on ?difficult? sentences before/after adding MWE lexical entries
Lou Burnard. 2000. User Reference Guide for the
British National Corpus. Technical report, Oxford
University Computing Services.
M. Butt, S. Dipper, A. Frank, and T.H. King. 1999.
Writing large-scale parallel grammars for english,
french, and german. In Proceedings of the LFG99
Conference. CSLI Publications.
Ulrich Callmeier. 2000. PET ? a platform for ex-
perimentation with efficient HPSG processing tech-
niques. Journal of Natural Language Engineering,
6(1):99?108.
Bob Carpenter. 1992. The Logic of Typed Fea-
ture Structures. Cambridge University Press, Cam-
bridge, England.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28.
Gregory Grefenstette. 1999. The World Wide Web
as a resource for example-based machine transla-
tion tasks. In Proceedings of ASLIB, Conference on
Translating and the Computer, London.
Ray Jackendoff. 1997. Twistin? the night away. Lan-
guage, 73:534?59.
Frank Keller, Maria Lapata, and Olga Ourioupina.
2002. Using the Web to overcome data sparse-
ness. In Jan Hajic? and Yuji Matsumoto, editors, Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 230?237,
Philadelphia.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on web as corpus.
Computational Linguistics, 29.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Conferencde on Natural Lan-
guage Learning (CoNLL-2002), pages 49?55.
Stephan Oepen and John Carroll. 2000. Ambiguity
packing in constraint-based parsing ? practical re-
sults. In Proceedings of the 1st Conference of the
North American Chapter of the ACL, pages 162?
169, Seattle, WA.
Carl J. Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press, Chicago, Illinois.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. In Proceed-
ings of the 3rd International Conference on Intelli-
gent Text Processing and Computational Linguistics
(CICLing-2002), pages 1?15, Mexico City, Mexico.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of
the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
446?453, Barcelona, Spain, July.
Aline Villavicencio, Francis Bond, Anna Korhonen,
and Diana McCarthy. 2005. Introduction to the spe-
cial issue on multiword expressions: having a crack
at a hard nut. Journal of Computer Speech and Lan-
guage Processing, 19.
Aline Villavicencio. 2005. The availability of verb-
particle constructions in lexical resources: How
much is enough? Journal of Computer Speech and
Language Processing, 19.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts processing.
In Proceedings of the Fifth International Confer-
ence on Language Resources and Evaluation (LREC
2006), Genoa, Italy.
44
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419?424,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Nothing like Good Old Frequency:
Studying Context Filters for Distributional Thesauri
Muntsa Padr
?
o,
?
Marco Idiart
?
, Carlos Ramisch
?
, Aline Villavicencio
?
?
Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?
Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
?
Aix Marseille Universit?e, CNRS, LIF UMR 7279, 13288, Marseille (France)
muntsa.padro@inf.ufrgs.br, marco.idiart@gmail.com,
carlos.ramisch@lif.univ-mrs.fr, avillavicencio@inf.ufrgs.br
Abstract
Much attention has been given to the
impact of informativeness and similar-
ity measures on distributional thesauri.
We investigate the effects of context fil-
ters on thesaurus quality and propose the
use of cooccurrence frequency as a sim-
ple and inexpensive criterion. For eval-
uation, we measure thesaurus agreement
with WordNet and performance in answer-
ing TOEFL-like questions. Results illus-
trate the sensitivity of distributional the-
sauri to filters.
1 Introduction
Large-scale distributional thesauri created auto-
matically from corpora (Grefenstette, 1994; Lin,
1998; Weeds et al., 2004; Ferret, 2012) are an
inexpensive and fast alternative for representing
semantic relatedness between words, when man-
ually constructed resources like WordNet (Fell-
baum, 1998) are unavailable or lack coverage. To
construct a distributional thesaurus, the (colloca-
tional or syntactic) contexts in which a target word
occurs are used as the basis for calculating its sim-
ilarity with other words. That is, two words are
similar if they share a large proportion of contexts.
Much attention has been devoted to refin-
ing thesaurus quality, improving informativeness
and similarity measures (Lin, 1998; Curran and
Moens, 2002; Ferret, 2010), identifying and de-
moting bad neighbors (Ferret, 2013), or using
more relevant contexts (Broda et al., 2009; Bie-
mann and Riedl, 2013). For the latter in particular,
as words vary in their collocational tendencies, it
is difficult to determine how informative a given
context is. To remove uninformative and noisy
contexts, filters have often been applied like point-
wise mutual information (PMI), lexicographer?s
mutual information (LMI) (Biemann and Riedl,
2013), t-score (Piasecki et al., 2007) and z-score
(Broda et al., 2009). However, the selection of a
measure and of a threshold value for these filters
is generally empirically determined. We argue that
these filtering parameters have a great influence on
the quality of the generated thesauri.
The goal of this paper is to quantify the im-
pact of context filters on distributional thesauri.
We experiment with different filter methods and
measures to assess context significance. We pro-
pose the use of simple cooccurrence frequency as
a filter and show that it leads to better results than
more expensive measures such as LMI or PMI.
Thus we propose a cheap and effective way of fil-
tering contexts while maintaining quality.
This paper is organized as follows: in ?2 we
discuss evaluation of distributional thesauri. The
methodology adopted in the work and the results
are discussed in ?3 and ?4. We finish with some
conclusions and discussion of future work.
2 Related Work
In a nutshell, the standard approach to build a dis-
tributional thesaurus consists of: (i) the extraction
of contexts for the target words from corpora, (ii)
the application of an informativeness measure to
represent these contexts and (iii) the application of
a similarity measure to compare sets of contexts.
The contexts in which a target word appears can
be extracted in terms of a window of cooccurring
(content) words surrounding the target (Freitag et
al., 2005; Ferret, 2012; Erk and Pado, 2010) or in
terms of the syntactic dependencies in which the
target appears (Lin, 1998; McCarthy et al., 2003;
Weeds et al., 2004). The informativeness of each
context is calculated using measures like PMI, and
t-test while the similarity between contexts is cal-
culated using measures like Lin?s (1998), cosine,
Jensen-Shannon divergence, Dice or Jaccard.
Evaluation of the quality of distributional the-
sauri is a well know problem in the area (Lin,
419
1998; Curran and Moens, 2002). For instance, for
intrinsic evaluation, the agreement between the-
sauri has been examined, looking at the average
similarity of a word in the thesauri (Lin, 1998),
and at the overlap and rank agreement between the
thesauri for target words like nouns (Weeds et al.,
2004). Although much attention has been given to
the evaluation of various informativeness and sim-
ilarity measures, a careful assessment of the ef-
fects of filtering on the resulting thesauri is also
needed. For instance, Biemann and Riedl (2013)
found that filtering a subset of contexts based on
LMI increased the similarity of a thesaurus with
WordNet. In this work, we compare the impact of
using different types of filters in terms of thesaurus
agreement with WordNet, focusing on a distribu-
tional thesaurus of English verbs. We also propose
a frequency-based saliency measure to rank and
filter contexts and compare it with PMI and LMI.
Extrinsic evaluation of distributional thesauri
has been carried out for tasks such as En-
glish lexical substitution (McCarthy and Navigli,
2009), phrasal verb compositionality detection
(McCarthy et al., 2003) and the WordNet-based
synonymy test (WBST) (Freitag et al., 2005). For
comparative purposes in this work we adopt the
latter.
3 Methodology
We focus on thesauri of English verbs constructed
from the BNC (Burnard, 2007)
1
. Contexts are ex-
tracted from syntactic dependencies generated by
RASP (Briscoe et al., 2006), using nouns (heads
of NPs) which have subject and direct object rela-
tions with the target verb. Thus, each target verb
is represented by a set of triples containing (i) the
verb itself, (ii) a context noun and (iii) a syntac-
tic relation (object, subject). The thesauri were
constructed using Lin?s (1998) method. Lin?s ver-
sion of the distributional hypothesis states that two
words (verbs v
1
and v
2
in our case) are similar if
they share a large proportion of contexts weighted
by their information content, assessed with PMI
(Bansal et al., 2012; Turney, 2013).
In the literature, little attention is paid to context
filters. To investigate their impact, we compare
two kinds of filters, and before calculating similar-
ity using Lin?s measure, we apply them to remove
1
Even though larger corpora are available, we use a tradi-
tional carefully constructed corpus with representative sam-
ples of written English to control the quality of the thesaurus.
potentially noisy triples:
? Threshold (th): we remove triples that oc-
cur less than a threshold th. Threshold values
vary from 1 to 50 counts per triple.
? Relevance (p): we keep only the top p most
relevant contexts for each verb, were rele-
vance is defined according to the following
measures: (a) frequency, (b) PMI, and (c)
LMI (Biemann and Riedl, 2013). Values of
p vary between 10 and 1000.
In this work, we want to answer two ques-
tions: (a) Do more selective filters improve intrin-
sic evaluation of thesaurus? and (b) Do they also
help in extrinsic evaluation?
For intrinsic evaluation, we determine agree-
ment between a distributional thesaurus and Word-
Net as the path similarities for the first k distri-
butional neighbors of a verb. A single score is
obtained by averaging the similarities of all verbs
with their k first neighbors. The higher this score
is, the closer the neighbors are to the target in
WordNet, and the better the thesaurus. Several
values of k were tested and the results showed ex-
actly the same curve shapes for all values, with
WordNet similarity decreasing linearly with k. For
the remainder of the paper we adopt k = 10, as it
is widely used in the literature.
For extrinsic evaluation, we use the WBST set
for verbs (Freitag et al., 2005) with 7,398 ques-
tions and an average polysemy of 10.4. The task
consists of choosing the most suitable synonym
for a word among a set of four options. The the-
saurus is used to rank the candidate answers by
similarity scores, and select the first one as the
correct synonym. As discussed by Freitag et al.
(2005), the upper bound reached by English na-
tive speakers is 88.4% accuracy, and simple lower
bounds are 25% (random choice) and 34.5% (al-
ways choosing the most frequent option).
4 Results
Figure 1 shows average WordNet similarities for
thesauri built filtering by frequency threshold th
and by p most frequent contexts. Table 1 sum-
marizes the parametrization leading to the best
WordNet similarity for each kind of filter. In all
cases we show the results obtained for different
frequency ranges
2
as well as the results when av-
eraging over all verbs.
2
In order to study the influence of verb frequency on the
results, we divide the verbs in three groups: high-frequency
420
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 1  10
W
N
 
s
i
m
i
l
a
r
i
t
y
th 
WordNet path Similarity for different frequency ranges, k=10Filtering triples with frequency under th
all verbshigh frequent verbsmid frequent verbslow frequent verbs  0
 0.05
 0.1
 0.15
 0.2
 0.25
 10  100  1000
W
N
 
s
i
m
i
l
a
r
i
t
y
p 
WordNet path Similarity for different frequency ranges, k=10Keeping p most frequent triples per verb
all verbshigh frequent verbsmid frequent verbslow frequent verbs
Figure 1: WordNet scores for verb frequency ranges, filtering by frequency threshold th (left) and p most
frequent contexts (right).
Filter All verbs Frequency range
Low Mid High
No filter - 0.148 - 0.101 - 0.144 - 0.198
Filter low freq. contexts th = 50 0.164 th = 50 0.202 th = 50 0.154 th = 1 0.200
Keep p contexts (freq.) p = 200 0.158 p = 500 0.138 p = 200 0.149 p = 200 0.206
Keep p contexts (PMI) p = 1000 0.139 p = 1000 0.101 p = 1000 0.136 p = 1000 0.181
Keep p contexts (LMI) p = 200 0.155 p = 100 0.112 p = 200 0.147 p = 200 0.208
Table 1: Best scores obtained for each filter for all verbs and frequency ranges. Scores are given in terms
of WordNet path. Confidence interval is arround ? 0.002 in all cases.
When using a threshold filter (Figure 1 left),
high values lead to better performance for mid-
and low-frequency verbs. This is because, for high
th values, there are few low and mid-frequency
verbs left, since a verb that occurs less has less
chances to be seen often in the same context. The
similarity for verbs with no contexts over the fre-
quency threshold cannot be assessed and as a con-
sequence those verbs are not included in the fi-
nal thesaurus. As Figure 2 shows, the number
of verbs decreases much faster for low and mid
frequency verbs when th increases.
3
For exam-
ple, for th = 50, there are only 7 remaining low-
frequency verbs in the thesaurus and these tend
to be idiosyncratic multiword expressions. One
example is wreak, and the only triple contain-
ing this verb that appeared more than 50 times is
wreak havoc (71 occurrences). The neighbors of
this verb are cause and play, which yield a good
similarity score in WordNet. Therefore, although
higher thresholds result in higher similarities for
low and mid-frequency verbs, this comes at a cost,
as the number of verbs included in the thesaurus
decreases considerably.
(||v|| ? 500), mid-frequency (150 ? ||v|| < 500) and low-
frequency (||v|| < 150).
3
For p most salient contexts, the number of verbs does not
vary and is the same shown in Figure 2 for th = 1 (no filter).
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 1  10
N
u
m
b
e
r
 
o
f
 
v
e
r
b
s
th 
Number of verbs in WordNetFiltering triples with frequency under th
all verbshigh frequent verbsmid frequent verbslow frequent verbs
Figure 2: Number of verbs per frequency ranges
when filtering by context frequency threshold th
As expected, the best performance is obtained
for high-frequency verbs and no filter, since it re-
sults in more context information per verb. In-
creasing th decreases similarity due to the removal
of some of these contexts. In average, higher th
values lead to better overall similarity among the
frequency ranges (from 0.148 with th = 1 to
0.164 with th = 50). The higher the threshold,
the more high-frequency verbs will prevail in the
thesauri, for which the WordNet path similarities
are higher.
On the other hand, when adopting a relevance
421
 0
 0.2
 0.4
 0.6
 0.8
 1
 1  10
P
,
 
R
,
 
F
1
th 
WBST task: P, R and F1Filtering triples with frequency under th
PrecisionRecallF1  0
 0.2
 0.4
 0.6
 0.8
 1
 10  100  1000
P
,
 
R
,
 
F
1
p 
WBST task: P, R and F1Keeping p most frequent triples per verb
PrecisionRecallF1
Figure 3: WBST task scores filtering by frequency threshold th (left) and p most frequent contexts
(right).
filter of keeping the p most relevant contexts for
each verb (Figure 1 right), we obtain similar re-
sults, but more stable thesauri. The number of
verbs remains constant, since we keep a fixed
number of contexts for each verb and verbs are not
removed when the threshold is modified. Word-
Net similarity increases as more contexts are taken
into account, for all frequency ranges. There is a
maximum around p = 200, though larger values
do not lead to a drastic drop in quality. This sug-
gests that the noise introduced by low-frequency
contexts is compensated by the increase of infor-
mativeness for other contexts. An ideal balance
is reached by the lowest possible p that maintains
high WordNet similarity, since the lower the p the
faster the thesaurus construction.
In terms of saliency measure, when keeping
only the p most relevant contexts, sorting them
with PMI leads to much worse results than LMI
or frequency, as PMI gives too much weight to
infrequent combinations. This is consistent with
results of Biemann and Riedl (2013). Regarding
LMI versus frequency, the results using the latter
are slightly better (or with no significant differ-
ence, depending on the frequency range). The ad-
vantage of using frequency instead of LMI is that
it makes the process simpler and faster while lead-
ing to equal or better performance in all frequency
ranges. Therefore for the extrinsic evaluation us-
ing WBST task, we use frequency to select the
p most relevant contexts and then compute Lin?s
similarity using only those contexts.
Figure 3 shows the performance of the thesauri
in the WBST task in terms of precision, recall and
F1.
4
For precision, the best filter is to remove con-
4
Filters based on LMI and PMI were also tested with the
texts occurring less than th times, but, this also
leads to poor recall, since many verbs are left out
of the thesauri and their WSBT questions cannot
be answered. On the other hand, keeping the most
relevant p contexts leads to more stable results and
when p is high (right plot), they are similar to those
shown in the left plot of Figure 3.
4.1 Discussion
The answer to our questions in Section 3 is yes,
more selective filters improve intrinsic and extrin-
sic thesaurus quality. The use of both filtering
methods results in thesauri in which the neighbors
of target verbs are closer in WordNet and get better
scores in TOEFL-like tests. However, the fact that
filtering contexts with frequency under th removes
verbs in the final thesaurus is a drawback, as high-
lighted in the extrinsic evaluation on the WBST
task.
Furthermore, we demonstrated that competitive
results can be obtained keeping only the p most
relevant contexts per verb. On the one hand, this
method leads to much more stable thesauri, with
the same verbs for all values of p. On the other
hand, it is important to highlight that the best re-
sults to assess the relevance of the contexts are ob-
tained using frequency while more sophisticated
filters such as LMI do not improve thesaurus qual-
ity. Although an LMI filter is relatively fast com-
pared to dimensionality reduction techniques such
as singular value decomposition (Landauer and
Dumais, 1997), it is still considerably more expen-
sive than a simple frequency filter.
In short, our experiments indicate that a reason-
same results as intrinsic evaluation: sorting contexts by fre-
quency leads to better results.
422
able trade-off between noise, coverage and com-
putational efficiency is obtained for p = 200 most
frequent contexts, as confirmed by intrinsic and
extrinsic evaluation. Frequency threshold th is
not recommended: it degrades recall because the
contexts for many verbs are not frequent enough.
This result is useful for extracting distributional
thesauri from very large corpora like the UKWaC
(Ferraresi et al., 2008) by proposing an alterna-
tive that minimizes the required computational re-
sources while efficiently removing a significant
amount of noise.
5 Conclusions and Future Work
In this paper we addressed the impact of filters
on the quality of distributional thesauri, evaluat-
ing a set of standard thesauri and different filtering
methods. The results suggest that the use of fil-
ters and their parameters greatly affect the thesauri
generated. We show that it is better to use a filter
that selects the most relevant contexts for a verb
than to simply remove rare contexts. Furthermore,
the best performance was obtained with the sim-
plest method: frequency was found to be a simple
and inexpensive measure of context salience. This
is especially important when dealing with large
amounts of data, since computing LMI for all con-
texts would be computationally costly. With our
proposal to keep just the p most frequent contexts
per verb, a great deal of contexts are cheaply re-
moved and thus the computational power required
for assessing similarity is drastically reduced.
As future work, we plan to use these filters to
build thesauri from larger corpora. We would like
to generalize our findings to other syntactic con-
figurations (e.g. noun-adjective) as well as to other
similarity and informativeness measures. For in-
stance, ongoing experiments indicate that the same
parameters apply when Lin?s similarity is replaced
by cosine. Finally, we would like to compare the
proposed heuristics with more sophisticated filter-
ing strategies like singular value decomposition
(Landauer and Dumais, 1997) and non-negative
matrix factorization (Van de Cruys, 2009).
Acknowledgments
We would like to thank the support of projects
CAPES/COFECUB 707/11, PNPD 2484/2009,
FAPERGS-INRIA 1706-2551/13-7, CNPq
312184/2012-3, 551964/2011-1, 482520/2012-4
and 312077/2012-2.
References
Mohit Bansal, John DeNero, and Dekang Lin. 2012.
Unsupervised translation sense clustering. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
773?782, Montr?eal, Canada, June. Association for
Computational Linguistics.
Chris Biemann and Martin Riedl. 2013. Text: Now
in 2D! a framework for lexical expansion with con-
textual similarity. Journal of Language Modelling,
1(1).
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In James
Curran, editor, Proc. of the COLING/ACL 2006 In-
teractive Presentation Sessions, pages 77?80, Sid-
ney, Australia, Jul. ACL.
Bartosz Broda, Maciej Piasecki, and Stan Szpakow-
icz. 2009. Rank-based transformation in mea-
suring semantic relatedness. In Proceedings of
the 22nd Canadian Conference on Artificial Intel-
ligence: Advances in Artificial Intelligence, Cana-
dian AI ?09, pages 187?190, Berlin, Heidelberg.
Springer-Verlag.
Lou Burnard. 2007. User Reference Guide for the
British National Corpus. Technical report, Oxford
University Computing Services, Feb.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Proc.of
the ACL 2002 Workshop on Unsupervised Lexical
Acquisition, pages 59?66, Philadelphia, Pennsylva-
nia, USA. ACL.
Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In Proc. of the
ACL 2010 Conference Short Papers, pages 92?97,
Uppsala, Sweden, Jun. ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). MIT Press, May. 423 p.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluat-
ing UKWaC, a very large web-derived corpus of En-
glish. In In Proceedings of the 4th Web as Corpus
Workshop (WAC-4.
Olivier Ferret. 2010. Testing semantic similarity mea-
sures for extracting synonyms from a corpus. In
Proc. of the Seventh LREC (LREC 2010), pages
3338?3343, Valetta, Malta, May. ELRA.
Olivier Ferret. 2012. Combining bootstrapping and
feature selection for improving a distributional the-
saurus. In ECAI, pages 336?341.
Olivier Ferret. 2013. Identifying bad semantic neigh-
bors for improving distributional thesauri. In Proc.
of the 51st ACL (Volume 1: Long Papers), pages
561?571, Sofia, Bulgaria, Aug. ACL.
423
Dayne Freitag, Matthias Blume, John Byrnes, Ed-
mond Chow, Sadik Kapadia, Richard Rohwer, and
Zhiqiang Wang. 2005. New experiments in distri-
butional representations of synonymy. In Ido Dagan
and Dan Gildea, editors, Proc. of the Ninth CoNLL
(CoNLL-2005), pages 25?32, University of Michi-
gan, MI, USA, Jun. ACL.
Gregory Grefenstette. 1994. Explorations in Au-
tomatic Thesaurus Discovery. Springer, Norwell,
MA, USA.
Thomas K Landauer and Susan T. Dumais. 1997. A
solution to platos problem: The latent semantic anal-
ysis theory of acquisition, induction, and represen-
tation of knowledge. Psychological review, pages
211?240.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proc. of the 36th ACL and
17th COLING, Volume 2, pages 768?774, Montreal,
Quebec, Canada, Aug. ACL.
Diana McCarthy and Roberto Navigli. 2009. The en-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In Francis Bond, Anna Korhonen,
Diana McCarthy, and Aline Villavicencio, editors,
Proc. of the ACL Workshop on MWEs: Analysis, Ac-
quisition and Treatment (MWE 2003), pages 73?80,
Sapporo, Japan, Jul. ACL.
Maciej Piasecki, Stanislaw Szpakowicz, and Bartosz
Broda. 2007. Automatic selection of heterogeneous
syntactic features in semantic similarity of polish
nouns. In Proceedings of the 10th international
conference on Text, speech and dialogue, TSD?07,
pages 99?106, Berlin, Heidelberg. Springer-Verlag.
Peter D. Turney. 2013. Distributional semantics be-
yond words: Supervised learning of analogy and
paraphrase. 1:353?366.
Tim Van de Cruys. 2009. A non-negative tensor factor-
ization model for selectional preference induction.
In Proceedings of the Workshop on Geometrical
Models of Natural Language Semantics, pages 83?
90, Athens, Greece, March. Association for Compu-
tational Linguistics.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proc. of the 20th COLING (COL-
ING 2004), pages 1015?1021, Geneva, Switzerland,
Aug. ICCL.
424
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1321?1330,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Language Acquisition and Probabilistic Models: keeping it simple
Aline Villavicencio?, Marco Idiart?Robert Berwick?, Igor Malioutov?
?Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
?LIDS, Dept. of EECS, Massachusetts Institute of Technology (USA)
? CSAIL, Dept. of EECS, Massachusetts Institute of Technology (USA)
avillavicencio@inf.ufrgs.br, marco.idiart@if.ufrgs.br
berwick@csail.mit.edu, igorm@mit.edu
Abstract
Hierarchical Bayesian Models (HBMs)
have been used with some success
to capture empirically observed pat-
terns of under- and overgeneralization
in child language acquisition. How-
ever, as is well known, HBMs are
?ideal? learning systems, assuming ac-
cess to unlimited computational re-
sources that may not be available
to child language learners. Conse-
quently, it remains crucial to carefully
assess the use of HBMs along with al-
ternative, possibly simpler, candidate
models. This paper presents such
an evaluation for a language acquisi-
tion domain where explicit HBMs have
been proposed: the acquisition of En-
glish dative constructions. In particu-
lar, we present a detailed, empirically-
grounded model-selection compari-
son of HBMs vs. a simpler alternative
based on clustering along with max-
imum likelihood estimation that we
call linear competition learning (LCL).
Our results demonstrate that LCL can
match HBM model performance with-
out incurring on the high computa-
tional costs associated with HBMs.
1 Introduction
In recent years, with advances in probability
and estimation theory, there has been much
interest in Bayesian models (BMs) (Chater,
Tenenbaum, and Yuille, 2006; Jones and
Love, 2011) and their application to child lan-
guage acquisition with its challenging com-
bination of structured information and in-
complete knowledge, (Perfors, Tenenbaum,
and Wonnacott, 2010; Hsu and Chater, 2010;
Parisien, Fazly, and Stevenson, 2008; Parisien
and Stevenson, 2010) as they offer several ad-
vantages in this domain. They can readily
handle the evident noise and ambiguity of ac-
quisition input, while at the same time pro-
viding efficiency via priors that mirror known
pre-existing language biases. Further, hierar-
chical Bayesian Models (HBMs) can combine
distinct abstraction levels of linguistic knowl-
edge, from variation at the level of individ-
ual lexical items, to cross-item variation, using
hyper-parameters to capture observed pat-
terns of both under- and over-generalization
as in the acquisition of e.g. dative alterna-
tions in English (Hsu and Chater, 2010; Per-
fors, Tenenbaum, and Wonnacott, 2010), and
verb frames in a controlled artificial language
(Wonnacott, Newport, and Tanenhaus, 2008).
HBMs can thus be viewed as providing a
?rational? upper bound on language learn-
ability, yielding optimal models that account
for observed data while minimizing any re-
quired prior information. In addition, the
clustering implicit in HBM modeling intro-
duces additional parameters that can be tuned
to specific data patterns. However, this comes
at a well-known price: HBMs generally are
also ideal learning systems, known to be
computationally infeasible (Kwisthout, Ware-
ham, and van Rooij, 2011). Approximations
proposed to ensure computational tractabil-
ity, like reducing the number of classes that
need to be learned may also be linguisti-
cally and cognitively implausible. For in-
stance, in terms of verb learning, this could
1321
take the form of reducing the number of sub-
categorization frames to the relevant subset,
as in (Perfors, Tenenbaum, and Wonnacott,
2010), where only 2 frames are considered for
?take?, when in fact it is listed in 6 frames
by Levin (1993). Finally, comparison of vari-
ous Bayesian models of the same task is rare
(Jones and Love, 2011) and Bayesian infer-
ence generally can be demonstrated as sim-
ply one class of regularization or smooth-
ing techniques among many others; given the
problem at hand, there may well be other,
equally compelling regularization methods
for dealing with the bias-variance dilemma
(e.g., SVMs (Shalizi, 2009)). Consequently, the
relevance of HBMs for cognitively accurate ac-
counts of human learning remains uncertain
and needs to be carefully assessed.
Here we argue that the strengths of HBMs
for a given task must be evaluated in light of
their computational and cognitive costs, and
compared to other viable alternatives. The fo-
cus should be on finding the simplest statis-
tical models consistent with a given behav-
ior, particularly one that aligns with known
cognitive limitations. In the case of many
language acquisition tasks this behavior often
takes the form of overgeneralization, but with
eventual convergence to some target language
given exposure to more data.
In particular, in this paper we consider how
children acquire English dative verb construc-
tions, comparing HBMs to a simpler alterna-
tive, a linear competition learning (LCL) al-
gorithm that models the behavior of a given
verb as the linear competition between the ev-
idence for that verb, and the average behav-
ior of verbs belonging to its same class. The
results show that combining simple cluster-
ing methods along with ordinary maximum
likelihood estimation yields a result compara-
ble to HBM performance, providing an alter-
native account of the same facts, without the
computational costs incurred by HBM models
that must rely, for example, on Markov Chain
Monte Carlo (MCMC) methods for numeri-
cally integrating complex likelihood integrals,
or on Chinese Restaurant Process (CRP) for
producing partitions.
In terms of Marr?s hierarchy (Marr, 1982)
learning verb alternations is an abstract com-
putational problem (Marr?s type I), solvable
by many type II methods combining repre-
sentations (models, viz. HBMs or LCLs) with
particular algorithms. The HBM convention
of adopting ideal learning amounts to invok-
ing unbounded algorithmic resources, solv-
ability in principle, even though in practice
such methods, even approximate ones, are
provably NP-hard (cf. (Kwisthout, Wareham,
and van Rooij, 2011)). Assuming cognitive
plausibility as a desideratum, we therefore ex-
amine whether HBMs can also be approxi-
mated by another type II method (LCLs) that
does not demand such intensive computa-
tion. Any algorithm that approximates an
HBM can be viewed as implementing a some-
what different underlying model; if it repli-
cates HBM prediction performance but is sim-
pler and less computationally complex then
we assume it is preferable.
This paper is organized as follows: we start
with a discussion of formalizations of lan-
guage acquisition tasks, ?2. We present our
experimental framework for the dative acqui-
sition task, formalizing a range of learning
models from simple MLE methods to HBM
techniques, ?3, and a computational evalua-
tion of each model, ?4. We finish with conclu-
sions and possibilities for future work, ?5.
2 Evidence in Language Acquisition
A familiar problem for language acquisition is
how children learn which verbs participate in
so-called dative alternations, exemplified by
the child-produced sentences 1 to 3, from the
Brown (1973) corpus in CHILDES (MacWhin-
ney, 1995).
1. you took me three scrambled eggs (a direct object da-
tive (DOD) from Adam at age 3;6)
2. Mommy can you fix dis for me ? (a prepositional da-
tive (PD) from Adam at age 4;7)
3. *Mommy, fix me my tiger (from Adam at age 5;2)
Examples like these show that children gen-
eralize their use of verbs. For example, in sen-
tence (1), the child Adam uses take as a DOD
before any recorded occurrence of a similar
use of take in adult speech to Adam. Such
verbs alternate because they can also occur
with a prepositional form, as in sentence (2).
However, sometimes a child?s use of verbs like
1322
these amounts to an overgeneralization ? that
is, their productive use of a verb in a pattern
that does not occur in the adult grammar, as in
sentence (3), above. Faced with these two verb
frames the task for the learner is to decide for a
particular verb if it is a non-alternating DOD
only verb, a PD only verb, or an alternating
verb that allows both forms.
This ambiguity raises an important learn-
ability question, conventionally known as
Baker?s paradox (Baker, 1979). On the as-
sumption that children only receive positive
examples of verb forms, then it is not clear
how they might recover from the overgener-
alization exhibited in sentence (3) above, be-
cause they will never receive positive sen-
tences from adults like (3), using fix in a DOD
form. As has long been noted, if negative ex-
amples were systematically available to learn-
ers, then this problem would be solved, since
the child would be given evidence that the
DOD form is not possible in the adult gram-
mar. However, although parental correction
could be considered to be a source of negative
evidence, it is neither systematic nor generally
available to all children (Marcus, 1993). Even
when it does occur, all careful studies have in-
dicated that it seems mostly concerned with
semantic appropriateness rather than syntax.
In the cases where it is related to syntax, it
is often difficult to determine what the cor-
rection refers to in the utterance and besides
children seem to be oblivious to the correction
(Brown and Hanlon, 1970; Ingram, 1989).
One alternative solution to Baker?s paradox
that has been widely discussed at least since
Chomsky (1981) is the use of indirect negative
evidence. On the indirect negative evidence
model, if a verb is not found where it would
be expected to occur, the learner may con-
clude it is not part of the adult grammar. Cru-
cially, the indirect evidence model is inher-
ently statistical. Different formalizations of in-
direct negative evidence have been incorpo-
rated in several computational learning mod-
els for learning e.g. grammars (Briscoe, 1997;
Villavicencio, 2002; Kwiatkowski et al, 2010);
dative verbs (Perfors, Tenenbaum, and Won-
nacott, 2010; Hsu and Chater, 2010); and mul-
tiword verbs (Nematzadeh, Fazly, and Steven-
son, 2013). Since a number of closely related
models can all implement the indirect nega-
tive evidence approach, the decision of which
one to choose for a given task may not be en-
tirely clear. In this paper we compare a range
of statistical models consistent with a certain
behavior: early overgeneralization, with even-
tual convergence to the correct target on the
basis of exposure to more data.
3 Materials and Methods
3.1 Dative Corpora
To emulate a child language acquisition en-
vironment we use naturalistic longitudinal
child-directed data, from the Brown corpus in
CHILDES, for one child (Adam) for a subset
of 19 verbs in the DOD and PD verb frames,
figure 1. This dataset was originally reported
in Perfors, Tenenbaum, and Wonnacott (2010),
and longitudinal and incremental aspects to
acquisition are approximated by dividing the
data available into 5 incremental epochs (E1 to
E5 in the figures), where at the final epoch the
learner has seen the full corpus.
Model comparison requires a gold standard
database for acquisition, reporting which
frames have been learned for which verbs at
each stage, and how likely a child is of mak-
ing creative uses of a particular verb in a new
frame. An independent gold standard with
developmental information (e.g. Gropen et
al. (1989)) would clearly be ideal. Absent
this, a first step is demonstrating that sim-
pler alternative models can replicate HBM
performance on their own terms. Therefore,
the gold standard we use for evaluation is
the classification predicted by Perfors, Tenen-
baum, and Wonnacott (2010). The evaluations
reported in our analysis take into account in-
trinsic characteristics of each model in rela-
tion to the likelihoods of the verbs, to deter-
mine the extent to which the models go be-
yond the data they were exposed to, discussed
in section 2. Further, since it has been ar-
gued that very low frequency verbs may not
yet be firmly placed in a child?s lexicon (Yang,
2010; Gropen et al, 1989), at each epoch we
also impose a low-frequency threshold of 5
occurrences, considering only verbs that the
learner has seen at least 5 times. This use of a
low-frequency threshold for learning has ex-
tensive support in the literature for learning
1323
of all kinds in both human and non-human
animals, e.g. (Gallistel, 2002). A cut-off fre-
quency in this range has also commonly been
used in NLP tasks like POS tagging (Ratna-
parkhi, 1999).
3.2 The learners
We selected a set of representative statistical
models that are capable in principle of solv-
ing this classification task, ranging from what
is perhaps the simplest possible, a simple bi-
nomial, all the way to multi-level hierarchical
Bayesian approaches.
A Binomial distribution serves as the sim-
plest model for capturing the behavior of a
verb occurring in either DOD or PD frame.
Representing the probability of DOD as ?, af-
ter n occurrences of the verb the probability
that y of them are DOD is:
p( y| ?,n) =
(n
y
)
?y (1 ? ?)n?y (1)
Considering that p(y| ?,n) is the likelihood
in a Bayesian framework, the simplest and the
most intuitive estimator of ?, given y in n verb
occurrences, is the Maximum Likelihood Esti-
mator (MLE):
?MLE =
y
n (2)
?MLE is viable as a learning model in the sensethat its accuracy increases as the amount of ev-
idence for a verb grows (n ? ?), reflecting
the incremental, on-line character of language
learning. However, one well known limita-
tion of MLE is that it assigns zero probability
mass to unseen events. Ruling out events on
the grounds that they did not occur in a finite
data set early in learning may be too strong ?
though it should be noted that this is simply
one (overly strong) version of the indirect neg-
ative evidence position.
Again as is familiar, to overcome zero
count problem, models adopt one or another
method of smoothing to assign a small prob-
ability mass to unseen events. In a Bayesian
formulation, this amounts to assigning non-
zero probability mass to some set of priors;
smoothing also captures the notion of gener-
alization, making predictions about data that
has never been seen by the learner. In the
context of verb learning smoothing could be
based on several principles:
? an (innate) expectation as to how verbs in
general should behave;
? an acquired class-based expectation of
the behavior of a verb, based on its associ-
ation to similar but more frequent verbs.
The former can be readily implemented
in terms of prior probability estimates. As
we discuss below, class-based estimates arise
from one or another clustering method, and
can produce more accurate estimates for less
frequent verbs based on patterns already
learned for more frequent verbs in the same
class; see (Perfors, Tenenbaum, and Wonna-
cott, 2010). In this case, smoothing is a side-
effect of the behavior of a class as a whole.
When learning begins, the prior probability
is the only source of information for a learner
and, as such, dominates the value of the poste-
rior probability. However, in the large sample
limit, it is the likelihood that dominates the
posterior distribution regardless of the prior.
In Hierarchical Bayesian Models both effects
are naturally incorporated. The prior distri-
bution is structured as a chain of distributions
of parameters and hyper-parameters, and the
data may be divided into classes that share
some of the hyper-parameters, as defined be-
low for the case of a three levels model:
? ? Exponential(1)
? ? Exponential(1)
?k ? Exponential(?)
?k ? Beta(?, ?)
?ik ? Beta(?k?k, ?k(1 ? ?k))
yi|ni ? Binomial(?ik)
The indices refer to the possible hierarchies
among the hyper-parameters. ? and ? are in
the top, and they are shared by all verbs. Then
there are classes of different ?k, ?k, and theprobabilities for the DOD frame for the dif-
ferent verbs (?ik) are drawn according to theclasses k assigned to them. An estimate for
(?ik) for a given configuration of clusters isgiven by
1324
Figure 1: Verb tokens per epoch (E1 to E5)
Figure 2: Verb tokens ? 5 per epoch (E1 to E5)
where P(Y) is the evidence of the data,
the unnormalized posterior for the hyper-
parameters is
and the likelihood for ? and ? is
The Hierarchical Bayesian Model prediction
for?i is the average of the estimate?ikHBM overall possible partitions of the verbs in the task.
To simplify the notation we can write
?HBM = E
[ y + ??
n + ?
]
(3)
where in the expression E[. . . ] are included
the integrals described above and the average
of all possible class partitions. Due to this
complexity, in practice even small data sets re-
quire the use of MCMC methods, and statisti-
cal models for partitions, like CRP (Gelman et
al., 2003; Perfors, Tenenbaum, and Wonnacott,
2010). This complexity also calls into question
the cognitive fidelity of such approaches.
Eq.3 is particularly interesting because by
fixing? and ? (instead of averaging over them)
it is possible to deduce simpler (and classical)
models: MLE corresponds to ? = 0; the so
called ?add-one? smoothing (referred in this
paper as L1) corresponds to ? = 2 and ? = 1/2.
From Eq.3 it is also clear that if ? and ? (or
their distributions) are unchanged, as the evi-
dence of a verb grows (n??), the HBM esti-
mate approaches MLE?s, (?HBM ? ?MLE). Onthe other hand, when ? >> n, ?HBM ? ?, sothat ? can be interpreted as a prior value for ?
in the low frequency limit.
Following this reasoning, we propose an
alternative approach, a linear competition
learner (LCL), that explicitly models the be-
havior of a given verb as the linear competi-
tion between the evidence for the verb, and
the average behavior of verbs of the same
class. As clustering is defined independently
from parameter estimation, the advantages of
the proposed approach are twofold. First, it
is computationally much simpler, not requir-
ing approximations by Monte Carlo meth-
ods. Second, differently from HBMs where
the same attributes are used for clustering and
parameter estimation (in this case the DOD
and PD counts for each verb), in LCL cluster-
1325
ing may be done using more general contexts
that employ a variety of linguistic and envi-
ronmental attributes.
For LCL the prior and class-based informa-
tion are incorporated as:
?LCL =
yi + ?C?C
ni + ?C (4)
where ?C and ?C are defined via justifiableheuristic expressions dependent solely on the
statistics of the class attributed to each verb i.
The strength of the prior (?C) is a mono-tonic function of the number of elements (mC)in the class C, excluding the target verb vi.To approximate the gold standard behavior of
the HBM for this task (Perfors, Tenenbaum,
and Wonnacott, 2010) we chose the following
function for ?C:
?C = mC3/2(1 ?mC?1/5) + 0.1 (5)
with the strength of the prior for the LCL
model depending on the number of verbs in
the class, not on their frequency. Eq.5 was
chosen as a good fit to HBMs, without incur-
ring their complexity. The powers are simple
fractions, not arbitrary numbers. A best fit
was not attempted due to the lack of assess-
ment of how accurate HBMs are on real data.
The prior value (?C) is a smoothed estima-tion of the probability of DOD in a given class,
combining the evidence for all verbs in that
class:
?C =
YC + 1/2
NC + 1 (6)
in this case YC is the number of DOD occur-rences in the class, and NC the total numberof verb occurrences in the class, in both cases
excluding the target verb vi.The interpretation of these parameters is
as follows: ?C is the estimate of ? in the ab-sence of any data for a verb; and ?C controlsthe crossover between this estimate and MLE,
with a large ?C requiring a larger sample (ni)to overcome the bias given by ?C.For comparative purposes, in this paper we
examine alternative models for (a) probability
estimation and (b) clustering. The models are
the following:
? two models without clusters: MLE and
L1;
? two models where clusters are performed
independently: LCL and MLE??; and
? the full HBM described before.
MLE?? corresponds to replacing ?, ? in eq.3by their maximal likelihood values calculated
from P({yi,ni}i?k|?, ?) described before.For models without clustering, estimation
is based solely on the observed behavior of
verbs. With clustering, same-cluster verbs
share some parameters, influencing one an-
other. HBMs place distributions over pos-
sible clusters, with estimation derived from
averages over distributions. In HBMs, clus-
tering and probability estimation are calcu-
lated jointly. In the other models these two
estimates are calculated separately, permit-
ting ?plug-and-play? use of external cluster-
ing methods, like X-means (Pelleg and Moore,
2000)1. However, to further assess the impact
of cluster assignment on alternative model
performance, we also used the clusters that
maximize the evidence of the HBM for the
DOD and PD counts of the target verbs, and
we refer to these as Maximum Evidence (ME)
clusters. In MWE clusters, verbs are separated
into 3 classes: one if they have counts for both
frames; another for only the DOD frame; and
a final for only the PD frame.
4 Evaluation
The learning task consists of estimating the
probability that a given verb occurs in a partic-
ular frame, using previous occurrences as the
basis for this estimation. In this context, over-
generalization can be viewed as the model?s
predictions that a given verb seen only in one
frame (say, a PD) can also occur in the other
(say, a DOD) as well, and it decreases as the
learner receives more data. In one extreme
we have MLE, which does not overgeneralize,
and in the other the L1 model, which assigns
uniform probability for all unseen cases. The
other 3 models fall somewhere in between,
overgeneralizing beyond the observed data,
using the prior and class-based smoothing to
assign some (low) probability mass to an un-
seen verb-frame pair. The relevant models?
1Other clustering algorithms were also used; here
we report X-means results as representative of these
models. X-means is available from http://www.cs.
waikato.ac.nz/ml/weka/
1326
predictions for each of the target verbs in the
DOD frame, given the full corpus, are in fig-
ure 3. In either end of the figure are the verbs
that were attested in only one of the frames
(PD only at the left-hand end, and DOD only
at the right-hand end). For these verbs, LCL
and HBM exhibit similar behavior. When the
low-frequency threshold is applied, MLE??,HBM and LCL work equally well, figure 4.
Figure 4: Probability of verbs in DOD frame,
Low Frequency Threshold.
To examine how overgeneralization pro-
gresses during the course of learning as the
models were exposed to increasing amounts
of data, we used the corpus divided by cumu-
lative epochs, as described in ?3.1. For each
epoch, verbs seen in only one of the frames
were divided in 5 frequency bins, and the
models were assessed as to how much over-
generalization they displayed for each of these
verbs. Following Perfors, Tenenbaum, and
Wonnacott (2010) overgeneralization is calcu-
lated as the absolute difference between the
models predicted ? and ?MLE, for each of theepochs, figure 5, and for comparative pur-
poses their alternating/non-alternating clas-
sification is also adopted. For non-alternating
verbs, overgeneralization reflects the degree
of smoothing of each model. As expected, the
more frequent a verb is, the more confident
the model is in the indirect negative evidence
it has for that verb, and the less it overgeneral-
izes, shown in the lighter bars in all epochs. In
addition, the overall effect of larger amounts
of data are indicated by a reduction in over-
generalization epoch by epoch. The effects of
class-based smoothing can be assessed com-
paring L1, a model without clustering which
displays a constant degree of overgeneraliza-
tion regardless of the epoch, while HBM uses
a distribution over clusters and the other mod-
els X-means. If a low-frequency threshold is
applied, the differences between the models
decrease significantly and so does the degree
of overgeneralization in the models? predic-
tions, as shown in the 3 lighter bars in the fig-
ure.
Figure 5: Overgeneralization, per epoch, per
frequency bin, where 0.5 corresponds to the
maximum overgeneralization.
While the models differ somewhat in their
predictions, the quantitative differences need
to be assessed more carefully. To compare
the models and provide an overall difference
measure, we use the predictions of the more
complex model, HBM, as a baseline and then
calculate the difference between its predic-
tions and those of the other models. We
used three different measures for comparing
models, one for their standard difference; one
that prioritizes agreement for high frequency
verbs; and one that focuses more on low fre-
quency verbs.
The first measure, denoted Difference, cap-
tures a direct comparison between two mod-
els, M1 and M2 as the average prediction dif-ference among the verbs, and is defined as:
This measure treats all differences uniformly,
regardless of whether they relate to high or
low frequency verbs in the learning sample
(e.g. for bring with 150 counts and serve with
only 1 have the same weight). To focus on high
frequency verbs, we also define the Weighted
Difference between two models as:
Here we expect Dn < D since models tend to
1327
Figure 3: Probability of verbs in DOD frame.
agree as the amount of evidence for each verb
increases. Conversely, our third measure, de-
noted Inverted, prioritizes the agreement be-
tween two models on low frequency verbs, de-
fined as follows:
D1/n captures the degree of similarity in over-generalization between two models. The re-
sults of applying these three difference mea-
sures are shown in figure 6 for the relevant
models, where grey is for D(M1,M2), blackfor Dn(M1,M2) and white for D1/n(M1,M2).Given the probabilistic nature of Monte Carlo
methods, there is also a variation between dif-
ferent runs of the HBM model (HBM to HBM-
2), and this indicates that models that per-
form within these bounds can be considered
to be equivalent (e.g. HBMs and ME-MLE??for Weighted Difference, and the HBMs and
X-MLE?? for the Inverted Difference).
Comparing the prediction agreement, the
strong influence of clustering is clear: the
models that have compatible clusters have
similar performances. For instance, all the
models that adopt the ME clusters for the
data perform closest to HBMs. Moreover, the
weighted differences tend to be smaller than
0.01 and around 0.02 for the inverted differ-
ences. The results for these measures become
even closer in most cases when the low fre-
quency threshold is adopted, figure 7, as the
Figure 6: Model Comparisons.
Figure 7: Model Comparison - Low Frequency
Threshold.
0 5 10 15 20 25 30 35 40 45 500.5
0.6
0.7
0.8
0.9
1
number of examples
DO
D p
rob
abi
lity
 
 
MLE
L1
HBM
LCLMLE
L1
HBM
LCL
Figure 8: DOD probability evolution for mod-
els with increase in evidence
evidence reduces the influence of the prior.
To examine the decay of overgeneralization
with the increase in evidence for these mod-
els, two simulated scenarios are defined for a
single generic verb: one where the evidence
for DOD amounts to 75% of the data (dashed
lines) and in the other to 100% (solid lines),
figures 9 and 8. Unsurprisingly, the perfor-
mance of the models is dependent on the
amount of evidence available. This is a con-
sequence of the decrease in the influence of
the priors as the sample size increases in a rate
of 1/N, as shown in figure 9 for the decrease
in overgeneralization. Ultimately it is the ev-
1328
100 101 102
10?4
10?3
10?2
10?1
100
number of examples
ove
rge
ner
aliz
atio
n
 
 L1HBMLCLL1HBMLCL
Figure 9: Overgeneralization reduction with
increase in evidence
idence that dominates the posterior probabil-
ity. Although the Bayesian model exhibits fast
convergence, after 10 examples, the simpler
model L1 is only approximately 3% below the
Bayesian model in performance for scenario 1
and is still 90% accurate in scenario 2, figure 8.
These results suggest that while these mod-
els all differ slightly in the degree of overgen-
eralization for low frequency data and noise,
these differences are small, and as evidence
reaches approximately 10 examples per verb,
the overall performance for all models ap-
proaches that of MLE.
5 Conclusions and Future Work
HBMs have been successfully used for a
number of language acquisition tasks captur-
ing both patterns of under- and overgeneral-
ization found in child language acquisition.
Their (hyper)parameters provide robustness
for dealing with low frequency events, noise,
and uncertainty and a good fit to the data,
but this fidelity comes at the cost of complex
computation. Here we have examined HBMs
against computationally simpler approaches
to dative alternation acquisition, which imple-
ment the indirect negative approach. We also
advanced several measures for model com-
parison in order to quantify their agreement
to assist in the task of model selection. The re-
sults show that the proposed LCL model, in
particular, that combines class-based smooth-
ing with maximum likelihood estimation, ob-
tains results comparable to those of HBMs,
in a much simpler framework. Moreover,
when a cognitively-viable frequency thresh-
old is adopted, differences in the performance
of all models decrease, and quite rapidly ap-
proach the performance of MLE.
In this paper we used standard clustering
techniques grounded solely on verb counts to
enable comparison with previous work. How-
ever, a variety of additional linguistic and dis-
tributional features could be used for cluster-
ing verbs into more semantically motivated
classes, using a larger number of frames and
verbs. This will be examined in future work.
We also plan to investigate the use of cluster-
ing methods more targeted to language tasks
(Sun and Korhonen, 2009).
Acknowledgements
We would like to thank the support of
projects CAPES/COFECUB 707/11, CNPq
482520/2012-4, 478222/2011-4, 312184/2012-
3, 551964/2011-1 and 312077/2012-2. We also
want to thank Amy Perfors for kindly sharing
the input data.
References
Baker, Carl L. 1979. Syntactic Theory and the Pro-
jection Problem. Linguistic Inquiry, 10(4):533?
581.
Briscoe, Ted. 1997. Co-evolution of language and
the language acquisition device. In Proceedings
of the 35th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 418?427.
Morgan Kaufmann.
Brown, Roger. 1973. A first language: Ehe early
stages. Harvard University Press, Cambridge,
Massachusetts.
Brown, Roger and Camille Hanlon. 1970. Deriva-
tional complexity and the order of acquisition of
child?s speech. In J. Hays, editor, Cognition and
the Development of Language. NY: John Wiley.
Chater, Nick, Joshua B. Tenenbaum, and Alan
Yuille. 2006. Probabilistic models of cogni-
tion: where next? Trends in Cognitive Sciences,
10(7):292 ? 293.
Chomsky, Noam. 1981. Lectures on government and
binding. Mouton de Gruyter.
1329
Gallistel, Charles R. 2002. Frequency, contin-
gency, and the information processing theory of
conditioning. In P.Sedlmeier and T. Betsch, ed-
itors, Frequency processing and cognition. Oxford
University Press, pages 153?171.
Gelman, Andrew, John B. Carlin, Hal S. Stern, and
Donald B. Rubin. 2003. Bayesian Data Analy-
sis, Second Edition (Chapman & Hall/CRC Texts in
Statistical Science). Chapman and Hall/CRC, 2
edition.
Gropen, Jess, Steve Pinker, Michael Hollander,
Richard Goldberg, and Ronald Wilson. 1989.
The learnability and acquisition of the dative al-
ternation in English. Language, 65(2):203?257.
Hsu, Anne S. and Nick Chater. 2010. The logi-
cal problem of language acquisition: A proba-
bilistic perspective. Cognitive Science, 34(6):972?
1016.
Ingram, David. 1989. First Language Acquisition:
Method, Description and Explanation. Cambridge
University Press.
Jones, Matt and Bradley C. Love. 2011. Bayesian
Fundamentalism or Enlightenment? On the ex-
planatory status and theoretical contributions
of Bayesian models of cognition. Behavioral and
Brain Sciences, 34(04):169?188.
Kwiatkowski, Tom, Luke Zettlemoyer, Sharon
Goldwater, and Mark Steedman. 2010. Induc-
ing probabilistic CCG grammars from logical
form with higher-order unification. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1223?1233.
Kwisthout, Johan, Todd Wareham, and Iris van
Rooij. 2011. Bayesian intractability is not an
ailment that approximation can cure. Cognitive
Science, 35(5):779?1007.
Levin, B. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. University of
Chicago Press, Chicago, IL.
MacWhinney, Brian. 1995. The CHILDES project:
tools for analyzing talk. Hillsdale, NJ: Lawrence
Erlbaum Associates, second edition.
Marcus, Gary F. 1993. Negative evidence in lan-
guage acquisition. Cognition, 46:53?85.
Marr, D. 1982. Vision. San Francisco, CA: W. H.
Freeman.
Nematzadeh, Aida, Afsaneh Fazly, and Suzanne
Stevenson. 2013. Child acquisition of multi-
word verbs: A computational investigation. In
A. Villavicencio, T. Poibeau, A. Korhonen, and
A. Alishahi, editors, Cognitive Aspects of Com-
putational Language Acquisition. Springer, pages
235?256.
Parisien, Christopher, Afsaneh Fazly, and Suzanne
Stevenson. 2008. An incremental bayesian
model for learning syntactic categories. In Pro-
ceedings of the Twelfth Conference on Computational
Natural Language Learning, CoNLL ?08, pages
89?96, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Parisien, Christopher and Suzanne Stevenson.
2010. Learning verb alternations in a usage-
based bayesian model. In Proceedings of the 32nd
Annual Conference of the Cognitive Science Society.
Pelleg, Dan and Andrew Moore. 2000. X-means:
Extending k-means with efficient estimation of
the number of clusters. In Proceedings of the
Seventeenth International Conference on Machine
Learning, pages 727?734, San Francisco. Morgan
Kaufmann.
Perfors, Amy, Joshua B. Tenenbaum, and Eliz-
abeth Wonnacott. 2010. Variability, nega-
tive evidence, and the acquisition of verb argu-
ment constructions. Journal of Child Language,
(37):607?642.
Ratnaparkhi, Adwait. 1999. Learning to parse nat-
ural language with maximum entropy models.
Machine Learning, pages 151?175.
Shalizi, Cosma R. 2009. Dynamics of bayesian
updating with dependent data and misspeci-
fied models. ElectroCosmanic Journal of Statistics,
3:1039?1074.
Sun, Lin and Anna Korhonen. 2009. Improving
verb clustering with automatically acquired se-
lectional preferences. In EMNLP, pages 638?
647.
Villavicencio, Aline. 2002. The Acquisition of a
Unification-Based Generalised Categorial Grammar.
Ph.D. thesis, Computer Laboratory, University
of Cambridge.
Wonnacott, Elizabeth, Elissa L. Newport, and
Michael K. Tanenhaus. 2008. Acquiring and
processing verb argument structure: Distribu-
tional learning in a miniature language. Cogni-
tive Psychology, 56:165?209.
Yang, Charles. 2010. Three factors in language
variation. Lingua, 120:1160?1177.
1330
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 49?56
Manchester, August 2008
Picking them up and Figuring them out:
Verb-Particle Constructions, Noise and Idiomaticity
Carlos Ramisch
??
, Aline Villavicencio
??
, Leonardo Moura
?
and Marco Idiart
?
?
Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?
GETALP Laboratory, Joseph Fourier University - Grenoble INP (France)
?
Department of Computer Sciences, Bath University (UK)
?
Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
{ceramisch,avillavicencio,lfsmoura}@inf.ufrgs.br, idiart@if.ufrgs.br
Abstract
This paper investigates, in a first stage,
some methods for the automatic acquisi-
tion of verb-particle constructions (VPCs)
taking into account their statistical prop-
erties and some regular patterns found in
productive combinations of verbs and par-
ticles. Given the limited coverage pro-
vided by lexical resources, such as dictio-
naries, and the constantly growing number
of VPCs, possible ways of automatically
identifying them are crucial for any NLP
task that requires some degree of semantic
interpretation. In a second stage we also
study whether the combination of statis-
tical and linguistic properties can provide
some indication of the degree of idiomatic-
ity of a given VPC. The results obtained
show that such combination can success-
fully be used to detect VPCs and distin-
guish idiomatic from compositional cases.
1 Introduction
Considerable investigative effort has focused on
the automatic identification of Multiword Expres-
sions (MWEs), like compound nouns (science fic-
tion) and phrasal verbs (carry out) (e.g. Pearce
(2002), Evert and Krenn (2005) and Zhang et
al. (2006)). Some of them employ language
and/or type dependent linguistic knowledge for
the task, while others employ independent statis-
tical methods, such as Mutual Information and
Log-likelihood (e.g. Pearce (2002) and, Zhang et
al. (2006)), or even a combination of them (e.g.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Baldwin (2005) and Sharoff (2004)), as basis for
helping to determine whether a given sequence
of words is in fact an MWE. Although some re-
search aims at developing methods for dealing
with MWEs in general (e.g. Zhang et al (2006),
Ramisch et al (2008)), there is also some work that
deals with specific types of MWEs (e.g. Pearce
(2002) on collocations and Villavicencio (2005)
on verb-particle constructions (VPCs)) as each of
these MWE types has distinct distributional and
linguistic characteristics.
VPCs are combinations of verbs and particles,
such as take off in Our plane took off late, that due
to their complex characteristics and flexible na-
ture, provide a real challenge for NLP. In particu-
lar, there is a lack of adequate resources to identify
and treat them, and those that are available provide
only limited coverage, in face of the huge number
of combinations in use. For tasks like parsing and
generation, it is essential to know whether a given
VPC is possible or not, to avoid for example us-
ing combinations that sound unnatural or ungram-
matical to native speakers (e.g. give/lend/?grant
out for the conveying of something to someone or
some place - (Fraser, 1976)).
1
Thus, the knowl-
edge of which combinations are possible is cru-
cial for precision grammar engineering. In ad-
dition, as the semantics of VPCs varies from the
idiomatic to the more compositional cases, meth-
ods for the automatic detection and handling of id-
iomaticity are very important for any NLP task that
involves some degree of semantic interpretation
such as Machine Translation (in this case avoiding
the problem of producing an unrelated translation
for a source sentence). Automatic methods for the
identification of idiomaticity in MWEs have been
1
See Baldwin et al (2004) for a discussion of the effects of
multiword expressions like VPCs on a parser?s performance.
49
proposed using a variety of approaches such as
statistical, substitutional, distributional, etc. (e.g.
McCarthy et al (2003), Bannard (2005) and Fa-
zly and Stevenson (2006)). In particular, Fazly
and Stevenson (2006) look at the correlation be-
tween syntactic fixedness (in terms of e.g. pas-
sivisation, choice of determiner type and pluralisa-
tion) and non-compositionality of verb-noun com-
pounds such as shoot the breeze.
In this work we investigate the automatic extrac-
tion of VPCs, looking into a variety of methods,
combining linguistic with statistical information,
ranging from frequencies to association measures:
Mutual Information (MI), ?
2
and Entropy. We also
investigate the determination of compositionality
of VPCs verifying whether the degree of semantic
flexibility of a VPC combined with some statisti-
cal information can be used to determine if it is
idiomatic or compositional.
This paper starts with a brief description of
VPCs, research on their automatic identification
and determination of their semantics (? 2). We then
explain the research questions and the assumptions
that serve as the basis for the application of statis-
tical measures (? 3) on the dataset (? 4). Our meth-
ods and experiments are then detailed (? 5), and
the results obtained are analysed (? 6). We con-
clude with a discussion of the contributions that
this work brings to the research on verb-particle
constructions (? 7).
2 Verb-Particle Constructions in Theory
and Practice
Particles in VPCs are characterised by containing
features of motion-through-location and of com-
pletion or result in their core meaning (Bolinger,
1971). VPCs can range from idiosyncratic or semi-
idiosyncratic combinations, such as get on (in e.g.
Bill got on well with his new colleagues), to more
regular ones, such as tear up (e.g. in In a rage she
tore up the letter Jack gave her). A three way clas-
sification is adopted by (Deh?e, 2002) and (Jack-
endoff, 2002), where a VPC can be classified as
compositional, idiomatic or aspectual, depending
on its sense. In compositional VPCs the meaning
of the construction is determined by the literal in-
terpretations of the particle and the verb. These
VPCs usually involve particles with directional or
spatial meaning, and these can often be replaced
by the appropriate directional PPs (e.g. carry in
in Sheila carried the bags in/into the house Deh?e
(2002)). Idiomatic VPCs, on the other hand, can-
not have their meaning determined by interpreting
their components literally (e.g. get on, meaning to
be on friendly terms with someone). The third class
is that of aspectual VPCs, which have the parti-
cle providing the verb with an endpoint, suggesting
that the action described by the verb is performed
completely, thoroughly or continuously (e.g. tear
up meaning to tear something into a lot of small
pieces).
From a syntactic point of view, a given combi-
nation can occur in several different subcategorisa-
tion frames. For example, give up can occur as an
intransitive VPC (e.g. in I give up! Tell me the an-
swer), where no other complement is required, or
it may occur as a transitive VPC which requires a
further NP complement (e.g. in She gave up alco-
hol while she was pregnant ). Since in English par-
ticles tend to be homographs with prepositions (up,
out, in), a verb followed by a preposition/particle
and an NP can be ambiguous between a transitive
VPC and a prepositional verb (e.g. rely on, in He
relies on his wife for everything). Some criteria
that characterise VPCs are discussed by Bolinger
(1971):
2
C1 In a transitive VPC the particle may come ei-
ther before or after the NP (e.g. He backed
up the team vs. He backed the team up).
However, whether a particle can be separated
or not from the verb may depend on the de-
gree of bonding between them, the size of the
NP, and the kind of NP. This is considered by
many to be sufficient condition for diagnos-
ing a VPC, as prepositions can only appear in
a position contiguous to the verb (e.g. *He
got the bus off ).
C2 Unstressed personal pronouns must precede
the particle (e.g. They ate it up but not *They
ate up it).
C3 If the particle precedes a simple definite NP,
the particle does not take the NP as its object
(e.g. in He brought along his girlfriend) un-
like with PP complements or modifiers (e.g.
in He slept in the hotel). This means that in
the first example the NP is not a complement
of the particle along, while in the second it is.
2
The distinction between a VPC and a prepositional verb
may be quite subtle, and as pointed out by Bolinger, many
of the criteria proposed for diagnosing VPCs give different
results for the same combination, frequently including un-
wanted combinations and excluding genuine VPCs.
50
In this paper we use the first two criteria, therefore
the candidates may contain noise (in the form of
prepositional verbs and related constructions).
VPCs have been the subject of a considerable
amount of interest, and some analysis has been
done on the subject of productive VPCs. In many
cases the particle seems to be compositionally
adding a specific meaning to the construction and
following a productive pattern (e.g. in tear up,
cut up and split up, where the verbs are seman-
tically related and up adds a sense of completion
to the action of these verbs). Fraser (1976) points
out that semantic properties of verbs can affect
their ability to combine with particles: for exam-
ple, bolt/cement/clamp/glue/paste/nail are seman-
tically similar verbs where the objects represented
by the verbs are used to join material, and they can
all combine with down. There is clearly a com-
mon semantic thread running through this list, so
that a new verb that is semantically similar to them
can also be reasonably assumed to combine with
down. Indeed, frequently new VPCs are formed by
analogy with existing ones, where often the verb is
varied and the particle remains (e.g. hang on, hold
on and wait on). Similarly, particles from a given
semantic class can be replaced by other particles
from the same class in compositional combina-
tions: send up/in/back/away (Wurmbrand, 2000).
By identifying classes of verbs that follow patterns
such as these in VPCs, we can help in the identi-
fication of a new unknown candidate combination,
using the degree of productivity of a class to which
the verb belongs as a back-off strategy.
In terms of methods for automatic identifica-
tion of VPCs from corpora, Baldwin (2005) pro-
poses the extraction of VPCs with valence infor-
mation from raw text, exploring a range of tech-
niques (using (a) a POS tagger, (b) a chunker, (c) a
chunk grammar, (d) a dependency parser, and (e) a
combination of all methods). Villavicencio (2005)
uses the Web as a corpus and productive patterns
of combination to generate and validate candidate
VPCs. The identification of compositionality in
VPCs is addressed by McCarthy et al (2003) who
examine the overlap of similar words in an auto-
matically acquired distributional thesaurus for verb
and VPCs, and by Bannard (2005) who uses a
distributional approach to determine when and to
what extent the components of a VPC contribute
their simplex meanings to the interpretation of the
VPC. Both report a correlation between some of
the measures and compositionality judgements.
3 The Underlying Hypotheses
The problem of the automatic detection and classi-
fication of VPCs can be summarised as, for a given
VPC candidate, to answer to the questions:
Q1 Is it a real VPC or some free combination
of verb and preposition/adverb or a preposi-
tional verb?
Q2 If it is a true VPC, is it idiomatic or composi-
tional?
In order to answer the first question, we use two
assumptions. Firstly, we consider that the elements
of a true VPC co-occur above chance. The greater
the correlation between the verb and the particle
the greater the chance that the candidate is a true
VPC. Secondly, based on criterion C1 we also as-
sume that VPCs have more flexible syntax and are
more productive than non-VPCs. This second as-
sumption goes against what is usually adopted for
general MWEs, since it is the prepositional verbs
that allow less syntactic configurations than VPCs
and are therefore more rigid (? 2). To further dis-
tinguish VPCs from prepositional verbs and other
related constructions we also verify the possibil-
ity of the particle to be immediately followed by
an indirect prepositional complement (like in The
plane took off from London), which is a good in-
dicator/delimiter of a VPC since in non-VPC con-
structions like prepositional verbs the preposition
needs to have an NP complement. Therefore, we
will assume that a true VPC occurs in the following
configurations, according to Villavicencio (2005)
and Ramisch et al (2008):
S1 VERB + PARTICLE + DELIMITER, for intran-
sitive VPCs;
S2 VERB + NP + PARTICLE + DELIMITER, for
transitive split VPCs and;
S3 VERB + PARTICLE + NP + DELIMITER, for
transitive joint VPCs.
In order to answer Q2, we look at the link be-
tween productivity and compositionality and as-
sume that a compositional VPC accepts the sub-
stitution of one of its members by a semantically
related term. This is in accordance to Fraser
(1976), who shows that semantic properties of
51
verbs can affect their ability to combine with par-
ticles: for example verbs of hunting combining
with the resultative down (hunt/track/trail/follow
down) and verbs of cooking with the aspectual up
(bake/cook/fry/broil up), forming essentially pro-
ductive VPCs. Idiomatic VPCs, however, will
not accept the substitution of one of its members
by a related term (e.g. get and its synonyms in
get/*obtain/*receive over), even if at first glance
this could seem natural. In our experiments, we
will consider that a VPC is compositional if it ac-
cepts: the replacement of the verb by a synonym,
or of the preposition by another preposition. Sum-
marising our hypothesis, we get:
? For Q1: Is the candidate syntactically flexi-
ble, i.e. does it allow the configurations S1
through S3?
? NO: non-VPC
? YES: VPC
? For Q2: Is the candidate semantically flexi-
ble, allowing the substitution of a member by
a related word?
? NO: idiomatic VPC
? YES: compositional VPC
4 Data Sources
To generate a gold standard, we used the Bald-
win VPC candidates dataset (henceforth Baldwin
CD)
3
, which contains 3,078 English VPC candi-
dates annotated with information about idiomatic-
ity (14.5% are considered idiomatic). We fur-
ther annotated this dataset with information about
whether each candidate is a genuine VPC or not,
where a candidate is consider genuine if it be-
longs to at least one of a set of machine-readable
dictionaries: the Alvey Natural Language Tools
(ANLT) lexicon (Carroll and Grover, 1989), the
Comlex lexicon (Macleod and Grishman, 1998),
and the LinGO English Resource Grammar (ERG)
(Copestake and Flickinger, 2000)
4
. With this crite-
rion 81.8% of them are considered genuine VPCs.
To gather information about the candidates in
this work we employ both a fragment of 1.8M
sentences from the British National Corpus (BNC
Burnard (2000)) and the Web as corpora. The
BNC fragment is used to calculate the correlation
3
This dataset was provided by Timothy Baldwin for the
MWE2008 Workshop.
4
Version of November 2001.
measures since they require a corpus with known
size. The Web is used to generate frequencies
for the entropy measures, as discussed in ? 5.2.
Web frequencies are approximated by the number
of pages containing a candidate and indexed by
Yahoo Search API. In order to keep the searches
as simple and self-sufficient as possible, no addi-
tional sources of information are used (Villavicen-
cio, 2005). Therefore, the frequencies are quite
conservative in the sense that by employing in-
flected forms of verbs, potentially much more evi-
dence could be gathered.
For the generation of semantic variational pat-
terns, we use both Wordnet 3.0 (Fellbaum, 1998)
and Levin?s English Verb Classes and Alternations
(Levin, 1993). Wordnet is organised as a graph of
concepts, called synsets, linked by relations of syn-
onymy, hyponymy, etc. Each synset contains a list
of words that represent the concept. The verbs in
a synset and its synonym synsets are used to gen-
erate variations of a VPC candidate. Likewise we
use Levin?s classes, which define 190 fine-grained
classes for English verbs, based on their syntactic
and semantic features.
It is important to highlight that the generation
of the semantic variations strongly relies on these
resources. Therefore, cross-language extension
would depend on the availability of similar tools
for the target language.
5 Carrying out the experiments
Our experiments are composed of two stages, each
one consisting of three steps (corresponding to the
next three sections). The first stage filters out ev-
ery candidate that is evaluated as not being a VPC,
while the second one intends to identify the id-
iomatic VPCs among the remaining candidates of
the previous stage.
5.1 Generating candidates
For each of the 3,078 items in the Baldwin CD we
generated 2 sets of variations, syntactic and seman-
tic, and we will refer to these as alternative forms
or variations of a candidate.
The syntactic variations are generated using the
patterns S1 to S3 described in section 3. Following
the work of Villavicencio (2005) 3 frequently used
prepositions for, from and with are used as delim-
iters and we search for NPs in the form of pronouns
like this and definite NPs like the boy. The use of
alternative search patterns also helps to give an in-
52
dication about the syntactic distribution of a can-
didate VPC, and consequently if it has a preferred
syntactic realisation. For instance, for eat up and
the delimiter with, we propose a list of Web search
queries for its respective variations v
i
, shown with
their corresponding Web frequencies in table 1.
5
Variation (v
i
) Frequency (n
Y ahoo
(v
i
))
eat up with 49200
eat the * up with 2240
eat this up with 1120
eat up the * with 3110
Table 1: Distribution of syntactic variations for the
candidate eat up.
For the semantic variations, in order to capture
the idiomaticity of VPCs we generate the alterna-
tive forms by replacing the verb by its synonym
verbs as follows:
WNS Wordnet Strict variations. When using Word-
net, we consider any verb that belongs to the
same synset of the candidate as a synonym.
WNL Wordnet Loose variations. This is an indi-
rect synonymy relation capturing any verb
in Wordnet that belongs either to the same
synset or to a synset that is synonym of the
synset in which the candidate verb is con-
tained.
Levin These include all verbs in the same Levin
class as the candidate.
Multiword synonyms are ignored in this step to
avoid noisy search patterns, (e.g. *eat up up). The
examples for these variations are shown in table 2
for the candidate act in.
Wordnet and Levin are considered ambiguous
resources because one verb is potentially contained
in several synsets or classes. However, as Word
Sense Disambiguation is not within the scope of
this work we employ some heuristics to select a
given sense for the candidate verb. In order to test
the effect of frequency, the first heuristic adopts the
first synset in the list, as Wordnet organises synsets
in descending order of frequency (denoted as first).
To study the influence of the number of synonyms,
the second and third heuristics use respectively the
biggest (max) and smallest (min) synsets. The last
5
The Yahoo wildcard used in these searches matches any
word occurring in that particular position.
Variation (v
i
) Source n
Y ahoo
(v
i
)
act in ? 2690
playact in WNS 0
play in WNS 167000
behave in WNL 98
do in WNL 24600
pose in Levin 1610
qualify in Levin 358
rank in Levin 706
rate in Levin 16700
serve in Levin 2240
Table 2: Distribution of syntactic variations for the
candidate eat up.
heuristic is the union of all synonyms (all). These
heuristics are indicated using a subscript notation,
where e.g. WNS
all
symbolizes the WNS varia-
tions set using the union of all synsets as disam-
biguation heuristic. Finally, we generated two
additional sets of candidates by replacing the par-
ticle by one of the 48 prepositions listed in the
ANLT dictionary (prep) and also by one of 9 cho-
sen locative prepositions (loc-prep). It is impor-
tant to also verify possible variations of the prepo-
sition because compositional VPCs combine pro-
ductively with one or more groups of particles, e.g.
locatives, and present consequently a wider prob-
ability distribution among the variations, while an
idiomatic VPC presents a higher frequency for a
chosen preposition.
5.2 Working the statistical measures out
The classifications of the candidate VPCs are done
using a set of measures: the frequencies of the
VPC candidates and of their individual words,
their Mutual Information (MI), ?
2
and Entropies.
We calculate the MI and ?
2
indices of a candidate
formed by a verb and a particle based on their in-
dividual frequencies and on their co-occurrence in
the BNC fragment.
The Entropy measure is given by
H(V ) = ?
n
?
i=1
p(v
i
) ln [ p(v
i
) ]
where
p(v
i
) =
n(v
i
)
?
? v
j
?V
n(v
j
)
is the probability of the variation v
i
to occur
among the set of all possible variations V =
53
H(V ) ? 0.001081
n
BNC
(p) ? 51611
n
Y ahoo
(v
transitive
) ? 1
n
Y ahoo
(v) ? 2020000000 : yes
n
Y ahoo
(v) > 2020000000
?
2
? 25.99
? ? ?
Figure 1: Fragment of the decision tree that filters
out non-VPCs.
{v
1
, v
2
, . . . , v
n
}, and n(v
i
) is the Web frequency
for the variation v
i
.
The entropy of a probability distribution gives
us some clues about its shape. A very low en-
tropy is a sign of a heterogeneous distribution that
contains a peak. On the other hand, a distribution
that presents uniformity will lead to a high entropy
value.
The interest of H(V ) for the detection of VPCs
is in that true instances are more likely to not prefer
a canonical form, more widely distributing proba-
bilities over all alternative syntactic frames (S1 to
S3), while non-VPCs are more likely to choose one
frame and present low frequencies for the proposed
variations.
For the semantic variations, the entropy is cal-
culated from a set V of variations generated by the
Wordnet synset, Levin class and preposition sub-
stitutions described in ? 5.1. The interpretation of
the entropy at this point is that high entropy indi-
cates compositionality while low entropy indicates
idiomaticity, since compositional VPCs are more
productive and distribute well over a class of verbs
or a class of prepositions and idiomatic VPCs pre-
fer a specific verb or preposition.
5.3 Bringing estimations together
Once we got a set of measures to predict
VPCs and another to predict their idiomatic-
ity/compositionality, we would like to know which
measures are useful. Therefore, we combine our
measures automatically by building a decision tree
with the J48 algorithm, a version of the traditional
entropy-based C4.5 algorithm implemented in the
Weka package.
6
6 Weighting the results up
The first stage of our experiments applied to the
3,078 VPC candidates generated a decision tree us-
6
http://www.cs.waikato.ac.nz/ml/weka/
ing 10-fold cross validation that is partially repro-
duced in figure 1. From these, 2,848 candidates
were considered genuine VPCs, with 2,419 true
positives, 100 false negatives and 429 false posi-
tives. This leads to a recall of 96% of the VPCs
being kept in the list with a precision of 84.9%,
and an f-measure of 90.1%. We interpret this as a
very positive result since although some false neg-
atives have been filtered out, the remaining candi-
dates are now less noisy.
Figure 1 shows that the entropy of the variations
is the best predictor since it is at the root of the
tree. We can also see that there are several types
of raw frequencies being used before a correlation
measure appears (?
2
). We can conclude that the
frequency of each transitive, intransitive and split
configurations are also good predictors to detect
false from true VPCs. At this point, MI does not
seem to contribute to the classification task.
For our second stage, we generated Wordnet
synonym, Levin class and preposition variations
for a list of the 2,867 VPC candidates classified
as genuine cases. We also took into account the
proportion of synonyms that are MWEs (vpc-syn)
and the proportion of synonyms that contain the
candidate itself (self-syn).
In order to know what kind of contribution each
measure gives to the construction of the decision
tree, we used a simple iterative algorithm that con-
structs the set U of useful attributes. It first ini-
tialises U with all attributes, then calculates the
precision for each class (yes and no)
7
on a cross
validation using all attributes in U . For each at-
tribute a ? U , it ignores a and recalculates preci-
sions. If both precisions decrease, the contribution
of a is positive, if both increase then a is negative,
else its contribution remains unknown. All fea-
tures that contribute negatively are removed from
U , and the algorithm is repeated until there is no
negative attribute left.
The step-by-step execution of the algorithm
can be observed in table 3, where the inconclu-
sive steps are hidden. We found out that the
optimal features are U
?
= {self-syn, H(prep),
H(Levin
first
), H(WNS
first
), H(WNS
min
),
H(Levin
max
), H(Levin
min
).} The self-syn in-
formation seems to be very important, as without
it precisions of both classes decrease considerably
7
We use the precision as a quality estimator since it gives
a good idea of the amount of work that a grammar engineer
or lexicographer must perform in order to clear the list from
false positives.
54
Precision
# Ignored No Yes +/?
1
st
iteration
0 ? 86.6% 54.9%
1 vpc-syn 86.7% 56.6% ?
2 self-syn 85.2% 28.7% +
4 H(loc-prep) 86.7% 56.1% ?
6 H(WNS
max
) 87.5% 57.4% ?
9 H(WNLfirst) 86.7% 57.9% ?
10 H(WNL
max
) 86.7% 57.8% ?
11 H(WNL
min
) 86.9% 57.6% ?
16 H(Levin
all
) 86.7% 55.1% ?
2
nd
iteration
17 ? 87.7% 60.3%
18 H(prep) 87.6% 59.2% +
21 H(WNS
all
) 87.8% 61.6% ?
22 H(WNL
all
) 87.8% 61.0% ?
23 H(Levin
first
) 87.5% 60.2% +
3
rd
iteration
26 ? 87.8% 61.9%
27 H(WNS
first
) 87.8% 61.9% ?
28 H(WNS
min
) 87.7% 61.1% +
29 H(Levin
max
) 87.8% 61.6 ?
30 H(Levin
min
) 87.7% 61.5% +
Table 3: Iterative attributes selection process. Pre-
cision in each class is used as quality estimator.
(experiment #2).
All entropies of the WNL heuristics are of little
or no utility. This could probably be explained by
either the choice of simple WSD heuristics for se-
lecting synsets, or because the indirect synonymy
information is too far related to the original verb to
be used in variational patterns. Inspecting the gen-
erated variations, we notice that most of the syn-
onym synsets are related to secondary senses or
very specific uses of a verb and are thus not cor-
rectly disambiguated.
In what concerns the WNS sets, only the small-
est and first synset were kept, suggesting again that
it may not be a good idea to maximise the syn-
onyms set and for future work, we intent to es-
tablish a threshold for a synset to be taken into
account. In addition, we can also infer a posi-
tive contribution of the frequency of a sense with
the choice of the first synset returned by Word-
net resulting in a reasonable WSD heuristic (which
is compatible with the results by McCarthy et al
(2004)).
On the other hand, the algorithm selected the
first, the smallest and the biggest of the Levin?s
sets. This probably happens because the major-
ity of these verbs belongs only to one or two, but
never to a great number of classes. Since the gran-
ularity of the classes is coarser than for synsets,
the heuristics often offer four equal or very close
entropies and thus redundant information. As an
overall result, the last iteration shown in table 3
indicates a precision of 61.9% for the classifier in
detecting idiomatic VPCs, that is to say that we au-
tomatically retrieved 176 VPCs where 67 are false
positives and 109 are truly idiomatic. This value is
a quality estimator for the resulting VPCs that will
potentially be used in the construction of a lexi-
con. Recall of idiomatic VPCs goes from 16.7%
to 24.9%.
7 Conclusions
One of the important challenges for robust natu-
ral language processing systems is to be able to
successfully deal with Multiword Expressions and
related constructions. We investigated the identifi-
cation of VPCs using a combination of statistical
methods and linguistic information, and whether
there is a correlation between the productivity of
VPCs and their semantics that could help us detect
if a VPC is idiomatic or compositional.
The results confirm that the use of statistical
and linguistic information to automatically iden-
tify verb-particle constructions presents a reason-
able way of improving coverage of existing lexi-
cal resources in a very simple and straightforward
manner. In terms of grammar engineering, the in-
formation about compositional candidates belong-
ing to productive classes provides us with the ba-
sis for constructing a family of fine-grained redun-
dancy rules for these classes. These rules are ap-
plied in a constrained way to verbs already in the
lexicon, according to their semantic classes. The
VPCs identified as idiomatic, on the other hand,
need to be explicitly added to the lexicon, after
their semantic is determined. This study can also
be complemented with the results of investigations
into the semantics of VPCs, as discussed by both
Bannard (2005) and McCarthy et al (2003).
In addition, the use of clustering methods is an
interesting possibility for automatically identify-
ing clusters of productive classes of both verbs and
of particles that combine well together.
55
Acknowledgments
This research was partly supported by the CNPq
research project Recuperac??ao de Informac??oes
Multil??ng?ues (CNPq Universal 484585/2007-0).
References
Baldwin, Timothy, Emily M. Bender, Dan Flickinger, Ara
Kim, and Stephan Oepen. 2004. Road-testing the English
Resource Grammar over the British National Corpus. In
Fourth International Conference on Language Resources
and Evaluation (LREC 2004), Lisbon, Portugal.
Baldwin, Timothy. 2005. Deep lexical acquisition of verb-
particle constructions. Computer Speech and Language,
19(4):398?414.
Bannard, Colin J. 2005. Learning about the meaning of verb-
particle constructions from corpora. Computer Speech and
Language, 19(4):467?478.
Bolinger, Dwight. 1971. The phrasal verb in English. Har-
vard University Press, Harvard, USA.
Burnard, Lou. 2000. User reference guide for the British Na-
tional Corpus. Technical report, Oxford University Com-
puting Services.
Carroll, John and Claire Grover. 1989. The derivation of a
large computational lexicon of English from LDOCE. In
Boguraev, B. and E. Briscoe, editors, Computational Lexi-
cography for Natural Language Processing. Longman.
Copestake, Ann and Dan Flickinger. 2000. An open-source
grammar development environment and broad-coverage
English grammar using HPSG. In Proceedings of the
2nd International Conference on Language Resources and
Evaluation (LREC 2000).
Deh?e, Nicole. 2002. Particle verbs in English: syntax, in-
formation structure and intonation. John Benjamins, Am-
sterdam/Philadelphia.
Evert, Stefan and Brigitte Krenn. 2005. Using small random
samples for the manual evaluation of statistical association
measures. Computer Speech and Language, 19(4):450?
466.
Fazly, Afsaneh and Suzanne Stevenson. 2006. Automatically
constructing a lexicon of verb phrase idiomatic combina-
tions. In EACL. The Association for Computer Linguis-
tics.
Fellbaum, Christiane, editor. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Communica-
tion). The MIT Press, May.
Fraser, Bruce. 1976. The Verb-Particle Combination in En-
glish. Academic Press, New York, USA.
Jackendoff, Ray. 2002. English particle constructions, the
lexicon, and the autonomy of syntax. In N. Deh?e, R. Jack-
endoff, A. McIntyre and S. Urban, editors, Verb-Particle
Explorations. Berlin: Mouton de Gruyter.
Levin, Beth. 1993. English Verb Classes and Alternations:
a preliminary investigation. University of Chicago Press,
Chicago and London.
Macleod, Catherine and Ralph Grishman. 1998. Comlex syn-
tax reference manual, Proteus Project.
McCarthy, Diana, Bill Keller, and John Carroll. 2003. De-
tecting a continuum of compositionality in phrasal verbs.
In Proceedings of the ACL 2003 workshop on Multiword
expressions, pages 73?80, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Finding predominant word senses in untagged
text. In Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics, page 279. Associa-
tion for Computational Linguistics.
Pearce, Darren. 2002. A comparative evaluation of colloca-
tion extraction techniques. In Third International Confer-
ence on Language Resources and Evaluation, Las Palmas,
Canary Islands, Spain.
Ramisch, Carlos, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An evaluation of methods for the ex-
traction of multiword expressions. In Proceedings of the
LREC Workshop - Towards a Shared Task for Multiword
Expressions (MWE 2008), pages 50?53, Marrakech, Mo-
rocco, June.
Sharoff, Serge. 2004. What is at stake: a case study of rus-
sian expressions starting with a preposition. pages 17?23,
Barcelona, Spain.
Villavicencio, Aline. 2005. The availability of verb-particle
constructions in lexical resources: How much is enough?
Journal of Computer Speech and Language Processing,
19(4):415?432.
Wurmbrand, S. 2000. The structure(s) of particle verbs. Ms.,
McGill University.
Zhang, Yi, Valia Kordoni, Aline Villavicencio, and Marco
Idiart. 2006. Automated multiword expression prediction
for grammar engineering. In Proceedings of the Workshop
on Multiword Expressions: Identifying and Exploiting Un-
derlying Properties, pages 36?44, Sydney, Australia. As-
sociation for Computational Linguistics.
56
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 23?25,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
An annotated English child language database
Aline Villavicencio??, Beracah Yankama?, Rodrigo Wilkens?,
Marco A. P. Idiart?, Robert Berwick?
?Federal University of Rio Grande do Sul (Brazil)
?MIT (USA)
alinev@gmail.com, beracah@mit.edu, rswilkens@gmail.com, marco.idiart@gmail.com, berwick@csail.mit.edu
1 Introduction
The use of large-scale naturalistic data has been
opening up new investigative possibilities for lan-
guage acquisition studies, providing a basis for
empirical predictions and for evaluations of alter-
native acquisition hypotheses. One widely used
resource is CHILDES (MacWhinney, 1995) with
transcriptions for over 25 languages of interac-
tions involving children, with the English corpora
available in raw, part-of-speech tagged, lemma-
tized and parsed formats (Sagae et al, 2010; But-
tery and Korhonen, 2005). With a recent increase
in the availability of lexical and psycholinguistic
resources and robust natural language processing
tools, it is now possible to further enrich child-
language corpora with additional sources of infor-
mation.
In this paper we describe the English CHILDES
Verb Database (ECVD), which extends the orig-
inal lexical and syntactic annotation of verbs
in CHILDES with information about frequency,
grammatical relations, semantic classes, and other
psycholinguistic and statistical information. In
addition, these corpora are organized in a search-
able database that allows the retrieval of data ac-
cording to complex queries that combine different
sources of information. This database is also mod-
ular and can be straightforwardly extended with
additional annotation levels. In what follows, we
discuss the tools and resources used for the anno-
tation (?2), and conclude with a discussion of the
implications of this initial work along with direc-
tions for future research (?3).
2 Linguistic and Statistical
Properties
The English CHILDES Verb Database con-
tains information about the English corpora in
CHILDES parsed using three different pipelines:
(1) MEGRASP; (2) RASP; and (3) the CHILDES
Treebank. In the first, made available as part of
the CHILDES distribution1, the corpora are POS
1http://childes.psy.cmu.edu/
tagged (in %mor), and parsed using MEGRASP
(Sagae et al, 2010) which provides information
about dependency parses and grammatical rela-
tions (in %gra):2
*MOT: I said (.) Adam you could have a banana
and offer Robin and Ursula one (.)would you
?
%mor: pro|I v|say&PAST n:prop|Adam pro|you
aux|could v|have det|a n|banana ...
%gra: 1|2|SUBJ 2|6|CJCT 3|2|OBJ 4|6|SUBJ
5|6|AUX 6|9|COORD 7|8|DET 8|6|OBJ ...
In the second pipeline, the RASP system
(Briscoe et al, 2006) is used for tokenisation,
tagging, lemmatization and parsing of the input
sentences, outputting syntactic trees (in %ST)
and grammatical relations (%GR).3 In both
examples each GR denotes a relation, along with
its head and dependent:
*MOT: oh no # he didn?t say anything about win-
dow .
%ST: (T Oh:1 no:2 ,:3 (S he:4 (VP do+ed:5
not+:6 say:7 anything:8 (PP about:9 (N1
window:10)))) .:11)
%GR: (|ncsubj| |say:7 VV0| |he:4 PPHS1| )
(|aux| |say:7 VV0| |do+ed:5 VDD|)
(|ncmod| |say:7 VV0| |not+:6 XX|)
(|iobj| |say:7 VV0| |about:9 II|) (|dobj|
|say:7 VV0| |anything:8 PN1|) (|dobj|
|about:9 II| |window:10 NN1|)
The third focuses on the Adam corpus from
the Brown data set (Brown, 1973) and uses
the Charniak parser with Penn Treebank style
part of speech tags and output, followed by
hand-curation, as described by Pearl and Sprouse
(2012):
(S1 (SBARQ (WHNP (WP who)) (SQ (VP (COP is)
(NP (NN that)))) (. ?)))
2In an evaluation MEGRASP produced correct depen-
dency relations for 96% of the relations in the gold stan-
dard, with the dependency relations being labelled with the
correct GR 94% of the time.
3The data was kindly provided by P. Buttery and A.
Korhonen and generated as described in (Buttery and Ko-
rhonen, 2005).
23
The use of annotations from multiple parsers
enables the combination of the complementary
strengths of each in terms of coverage and ac-
curacy, similar to inter-annotator agreement ap-
proaches. These differences are also useful for op-
timizing search patterns in terms of the source
which produces the best accuracy for a particu-
lar case. Information about corpora sizes and the
annotated portions for each of the parsers is dis-
played in table 1.
Information Sentences
Total Raw 4.84 million
MEGRASP & RASP Raw 2.5 million
MEGRASP Parsed 109,629
RASP Parsed 2.21 million
CHILDES Treebank 26,280
MEGRASP & RASP Parsed 98,456
Table 1: Parsed Sentences
The verbs in each sentence are also annotated
with information about shared patterns of mean-
ing and syntactic behavior from 190 fine-grained
subclasses that cover 3,100 verb types (Levin,
1993). This annotation allows searches defined
in terms of verb classes, and include all sentences
that contain verbs that belong to a given class.
For instance, searching for verbs of running would
return sentences containing not only run but also
related verbs like slide, roll and stroll.
Additional annotation of properties linked to
language use and recognition include extrinsic fac-
tors such as word frequency and intrinsic factors
such as the length of a word in terms of sylla-
bles; age of acquisition; imageability; and familiar-
ity. Some of this annotation is obtained from the
MRC Psycholinguistic Database (Coltheart, 1981)
which contains 150,837 entries with information
about 26 properties, although not all properties
are available for every word (e.g. IMAG is only
available for 9,240 words).
For enabling complex search functionalities
that potentially combine information from several
sources, the annotated sentences were organized
in a database, and Tables 2 and 3 list some of the
available annotations. Given the focus on verbs,
for search efficiency each sentence is indexed ac-
cording to the verbs it contains. In addition, verbs
and nouns are further annotated with information
shown in table 3 whenever it is available in the
existing resources.
These levels of annotation allow for complex
searches involving for example, a combination of
information about a verb?s lemma, target gram-
matical relations, and occurrence of Levin?s classes
in the corpora.
Not all sentences have been successfully ana-
lyzed, and the comments field contains informa-
Fields
Sentence ID
Corpus
Speaker
File
Raw sentence
MOR and POST tags
MEGRASP dep. and GRs
RASP syntactic tree
RASP dep. and GRs
Comments
Table 2: Information about Sentences
Fields
Word ID
Sentence ID
Levin?s classes
Age of acquisition
Familiarity
Concreteness
Frequency
Imageability
Number of syllables
Table 3: Information about Words
tion about the missing annotations and cases of
near perfect matches that arise from the parsers
using different heuristics for e.g. non-words, meta-
characters and punctuation. These required more
complex matching procedures for identifying the
corresponding cases in the annotations of the
parsers.
3 Conclusions and future work
This paper describes the construction of the En-
glish CHILDES Verb Database. It combines in-
formation from different parsing systems to capi-
talize on their complementary recall and precision
strengths and ensure the accuracy of the searches.
It also includes information about Levin?s classes
for verbs, and some psycholinguistic information
for some of the words, like age of acquisition,
familiarity and imageability. The result is a
large-scale integrated resource that allows com-
plex searches involving different annotation lev-
els. This database can be used to inform analysis,
for instance, about the complexity of the language
employed with and by a child as her age increases,
that can shed some light on discussions about the
poverty of the stimulus. This is an ongoing project
to make the annotated data available to the re-
search community in a user-friendly interface that
allows complex patterns to be specified in a simple
way.
Acknowledgements
This research was partly supported by CNPq
Projects 551964/2011-1, 202007/2010-3,
24
305256/2008-4 and 309569/2009-5.
References
E. Briscoe, J. Carroll, and R. Watson. 2006. The
second release of the rasp system. In Proceedings
of the COLING/ACL 2006 Interactive Presentation
Sessions, Sydney, Australia.
R. Brown. 1973. A first language: The early
stages. Harvard University Press, Cambridge, Mas-
sachusetts.
P. Buttery and A. Korhonen. 2005. Large-scale anal-
ysis of verb subcategorization differences between
child directed speech and adult speech. In Proceed-
ings of the Interdisciplinary Workshop on the Iden-
tification and Representation of Verb Features and
Verb Classes.
M. Coltheart. 1981. The MRC psycholinguistic
database. Quarterly Journal of Experimental Psy-
chology, 33A:497?505.
B. Levin. 1993. English verb classes and alterna-
tions - a preliminary investigation. The University
of Chicago Press.
B. MacWhinney. 1995. The CHILDES project: tools
for analyzing talk. Hillsdale, NJ: Lawrence Erlbaum
Associates, second edition.
L. Pearl and J. Sprouse, 2012. Experimental Syntax
and Islands Effects, chapter Computational Models
of Acquisition for Islands. Cambridge University
Press.
K. Sagae, E. Davis, A. Lavie, B. MacWhinney, and
S. Wintner. 2010. Morphosyntactic annotation of
CHILDES transcripts. Journal of Child Language,
37(03):705?729.
25
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 43?50,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
Get out but don?t fall down: verb-particle constructions in child language
Aline Villavicencio??, Marco A. P. Idiart?, Carlos Ramisch?,
V??tor Arau?jo?, , Beracah Yankama?, Robert Berwick?
?Federal University of Rio Grande do Sul (Brazil)
?MIT (USA)
alinev@gmail.com, marco.idiart@gmail.com, ceramisch@inf.ufrgs.br,
vbuaraujo@inf.ufrgs.br, beracah@mit.edu, berwick@csail.mit.edu
Abstract
Much has been discussed about the chal-
lenges posed by Multiword Expressions
(MWEs) given their idiosyncratic, flexi-
ble and heterogeneous nature. Nonethe-
less, children successfully learn to use them
and eventually acquire a number of Mul-
tiword Expressions comparable to that of
simplex words. In this paper we report
a wide-coverage investigation of a partic-
ular type of MWE: verb-particle construc-
tions (VPCs) in English and their usage
in child-produced and child-directed sen-
tences. Given their potentially higher com-
plexity in relation to simplex verbs, we
examine whether they appear less promi-
nently in child-produced than in child-
directed speech, and whether the VPCs
that children produce are more conserva-
tive than adults, displaying proportionally
reduced lexical repertoire of VPCs or of
verbs in these combinations. The results
obtained indicate that regardless of any ad-
ditional complexity VPCs feature widely in
children data following closely adult usage.
Studies like these can inform the develop-
ment of computational models for language
acquisition.
1 Introduction
There has been considerable discussion about
the challenges imposed by Multiword Expres-
sions (MWEs) which in addition to crossing word
boundaries act as a single lexical unit at some lev-
els of linguistic analysis (Calzolari et al, 2002;
Sag et al, 2002; Fillmore, 2003). They include a
wide range of grammatical constructions such as
verb-particle constructions (VPCs), idioms, com-
pound nouns and listable word configurations,
such as terminology and formulaic linguistic units
(Wray, 2009). Depending on the definition, they
may also include less traditional sequences like
copy of in They gave me a copy of the book (Fill-
more et al, 1988), greeting formulae like how
do you do?, and lexical bundles such as I dont
know whether or memorized poems and famil-
iar phrases from TV commercials (Jackendoff,
1997). These expressions may have reduced syn-
tactic flexibility, and be semantically more opaque
so that their semantics may not be easily inferred
from their component words. For instance, to play
down X means to (try to) make X seem less im-
portant than it really is and not literally a playing
event.
These expressions may also breach general
syntactic rules, sometimes spanning phrasal
boundaries and often having a high degree of lex-
icalisation and conventionality. They form a com-
plex of features that interact in various, often un-
tidy, ways and represent a broad continuum be-
tween non-compositional (or idiomatic) and com-
positional groups of words (Moon, 1998). In ad-
dition, they are usually sequences or groups of
words that co-occur more often than would be ex-
pected by chance, and have been argued to appear
in the same order of magnitude in a speaker?s lex-
icon as the simplex words (Jackendoff, 1997).
In terms of language acquisition difficulties
may arise as the interpretation of these expres-
sions often demands more knowledge than just
about (1) unitary words and (2) word-to-word re-
lations. This introduces a distinction between
what a learner is able to computationally disam-
biguate or figure out automatically from language
and what must be explicitly stored/memorized
and retrieved whole from memory at the time of
43
use, rather than being subject to generation or
analysis by the language grammar (Wray, 2009,
p. 9). Yet, according to Fillmore et al (1988),
in an ideal learning environment, most of the
knowledge about how to use a language should
be computable while explicitly memorized se-
quences should be kept to a minimum.
Due to these idiosyncrasies they have been
noted as easily phonetically mislearned: e.g. by
and large mistaken for by in large, to all in-
tents and purposes for to all intensive purposes,
and an arm and a leg for a nominal egg (Fill-
more, 2003). For second language (L2) learn-
ers in particular (Wray, 2002) MWEs are in-
deed a well-known cause of problems and less
likely to be used by them than by native speak-
ers in informal spoken contexts (Siyanova and
Schmitt, 2007). Even if L2 learners may be capa-
ble of producing a large number of MWEs, their
underlying intuitions and fluency do not match
those of native speakers (Siyanova and Schmitt,
2008) and they may produce marked combina-
tions that are not conventionally used together
(e.g. plastic surgery/?operation, strong/?powerful
tea) (Pearce, 2002; Siyanova and Schmitt, 2007).
Given the potential additional sources of com-
plexity of MWEs for learning, in this paper we
investigate whether children shy away from us-
ing them when they communicate. We focus on
a particular type of MWEs, VPCs, which present
a wide range of syntactic and semantic idyosin-
crasies examining whether children produce pro-
portionally less VPCs than adults. In addition, we
analyze whether any potential added processing
costs for VPCs are reflected in a reduced choice
of VPCs or verbs to form these combinations in
child-produced sentences compared to adult us-
age. Finally, given the possibility of flexible word
orders in VPCs with the verb and particle not only
occurring adjacently but also with an NP object
between them, we compare these two groups in
terms of distances between the verb and the par-
ticle in these combinations, to determine whether
there is a preference for a joint or a split config-
uration and if children and adults adopt distinct
strategies for their usage. By profiling the VPC
usage by children our aim is to provide the basis
for a computational modeling of the acquisition of
these constructions.
This paper is structured as follows: in sec-
tion 2 describes VPCs and related works; sec-
tion 3 presents the resources and methods used in
this paper. The analyses of VPCs in children and
adults sentences are in section 4. We finish with
conclusions and possibilities of future works.
2 Related Work
VPCs are combinations of verbs and prepositional
(up, down, ...), adverbial (away, back,...), adjecti-
val (short,...) or verbal (go, be,...) particles, and in
this work we focus on VPCs with prepositional or
adverbial particles like put off and move on. From
a language acquisition perspective, the complex-
ity of VPCs arises from their wide syntactic as
semantic variability.
Syntactically, like simplex verbs, VPCs can oc-
cur in different subcategorisation frames (e.g. in-
transitive in break down and transitive in print NP
up). However, the type of verb and the num-
ber of arguments of a VPC seem to have an
impact in learning as both children with typical
development and with specific language impair-
ments (SLI) seem to use obligatory arguments and
inflectional morphology more consistently with
general all purpose verbs, like make, go, do, put,
than with more specific verbs. Moreover, as the
number of obligatory arguments increases chil-
dren with SLI seem to produce more general and
fewer specific verbs (Boynton-Hauerwas, 1998).
Goldberg (1999b) refers to these verbs as light
verbs, suggesting that due to their frequency of
use, they are acquired earlier by children, and sub-
sequently act as centers of gravity from which
more specific instances can be learnt. These verbs
are very common and frequent in the everyday
communication, that could be used in place of
more specialized instances (e.g. make instead of
build).
In transitive VPCs there is the additional diffi-
culty of the particle appearing in different word
orders in relation to the verb: in a joint configu-
ration, adjacent to the verb (e.g. make up NP) or
in a split configuration after the NP complement
(make NP up) (Lohse et al, 2004). While some
VPCs can appear in both configurations, others
are inseparable (run across NP), and a learner has
to successfully account for these. Gries (2002)
using a multifactorial analysis to investigate 25
variables that could be linked to particle place-
ment like size of the direct object (in syllables
and words), type of NP (pronoun or lexical), type
of determiner (indefinite or definite). For a set
44
of 403 VPCs from the British National Corpus
he obtains 84% success in predicting (adult) na-
tive speakers? choice. Lohse et al (2004) propose
that these factors can be explained by consider-
ations of processing efficiency based on the size
of the object NP and on semantic dependencies
among the verb, the particle, and the object. In a
similar study for children Diessel and Tomasello
(2005) found that the type of the NP (pronoun vs
lexical NP) and semantics of the particle (spatial
vs non-spatial) were good predictors of placement
on child language data.
Semantically, one source of difficulties for
learners comes from the wide spectrum of compo-
sitionality that VPCs present. On one end of the
spectrum some combinations like take away com-
positionally combine the meaning of a verb with
the core meaning of a particle giving a sense of
motion-through-location (Bolinger, 1971). Other
VPCs like boil up are semi-idiomatic (or aspec-
tual) and the particle modifies the meaning of the
verb adding a sense of completion or result. At the
other end of the spectrum, idiomatic VPCs like
take off, meaning to imitate have an opaque mean-
ing that cannot be straightforwardly inferred from
the meanings of each of the components literally.
Moreover, even if some verbs form combinations
with almost every particle (e.g., get, fall, go,...),
others are selectively combined with only a few
particles (e.g., book and sober with up), or do not
combine well with them at all (e.g., know, want,
resemble,...) (Fraser, 1976). Although there are
some semi-productive patterns in these combina-
tions, like verbs of cooking and the aspectual up
(cook up, boil up, bake up), and stative verbs not
forming VPCs, for a learner it may not be clear
whether an unseen combination of verb and parti-
cle is indeed a valid VPC that can be produced or
not. Sawyer (1999) longitudinal analysis of VPCs
in child language found that children seem to treat
aspectual and compositional combinations differ-
ently, with the former being more frequent and
employing a larger variety of types than the lat-
ter. The sources of errors also differ and while
for compositional cases the errors tend to be lexi-
cal, for aspectuals there is a predominance of syn-
tactic errors such as object dropping, which ac-
counts for 92% of the errors in split configura-
tion for children under 5 (Sawyer, 1999). Chil-
dren with SLI tended to produce even more object
dropping errors for VPCs than children with typ-
ical development, despite both groups producing
equivalent numbers of VPCs (Juhasz and Grela,
2008). Given that compositionality seems to have
an impact on learning, to help reduce avoidance
of phrasal verbs Sawyer (2000) proposes a seman-
tic driven approach for second language learning
where transparent compositional cases would be
presented first to help familiarization with word
order variation, semi-idiomatic cases would be
taught next in groups according to the contribu-
tion of the particle (e.g telicity or completive-
ness), and lastly the idiomatic cases that need to
be memorized.
In this paper we present a wide coverage ex-
amination of VPC distributions in child produced
and child-directed sentences, comparing whether
children reproduce the linguistic environment to
which they are exposed or whether they present
distinct preferences in VPC usage.
3 Materials and Methods
For this work we use the English corpora from
the CHILDES database (MacWhinney, 1995)
containing transcriptions of child-produced and
child-directed speech from interactions involving
children of different age groups and in a variety
of settings, from naturalistic longitudinal studies
to task oriented latitudinal cases. These corpora
are available in raw, part-of-speech-tagged, lem-
matized and parsed formats (Sagae et al, 2010).
Moreover the English CHILDES Verb Construc-
tion Database (ECVCD) (Villavicencio et al,
2012) also adds for each sentence the RASP pars-
ing and grammatical relations (Briscoe and Car-
roll, 2006), verb semantic classes (Levin, 1993),
age of acquisition, familiarity, frequency (Colt-
heart, 1981) and other psycholinguistic and dis-
tributional characteristics. These annotated sen-
tences are divided into two groups according to
the speaker annotation available in CHILDES, the
Adults Set and the Children Set contain respec-
tively all the sentences spoken by adults and by
children1, as shown in table 1 as Parsed.
VPCs in these corpora are detected by look-
ing in the RASP annotation for all occurrences
of verbs followed by particles, prepositions and
adverbs up to 5 words to the right, following
Baldwin (2005), shown as Sentences with VPCs
1For the latter sentences which did not contain informa-
tion about age were removed.
45
Sentences Children Set Adults Set
Parsed 482,137 988,101
with VPCs 44,305 83,098
with VPCs Cleaned 38,326 82,796
% with VPCs 7.95 8.38
Table 1: VPCs in English Corpora in the Children
and Adults Sets
in table 1. The resulting sentences are subse-
quently automatically processed to remove noise
and words mistagged as verbs. For these candi-
dates with non-alphabetic characters, like @ in
a@l up, were removed as were those that did not
involve verbs (e.g. di, dat,), using the Comlex
Lexicon as reference for verb validity (Macleod
and Grishman, 1998). The resulting sets are listed
as Sentences with VPCs Cleaned in table 1. The
analyses reported in this paper use these sen-
tences, and the distribution of VPCs per children
age group is shown in table 2. Given the non-
uniform amounts of VPC for each age group, and
the larger proportion of VPC sentences in younger
ages in these corpora, we consider children as a
unique group. For these, the individual frequen-
cies of the verb, the particle and the VPC are col-
lected separately in the children set and in the
adult set, using the mwetoolkit (Ramisch et al,
2010).
Age in months VPC Sentences
0-24 2,799
24-48 26,152
48-72 8,038
72-96 1,337
>96 514
No age 4,841
Table 2: VPCs in Children Set per Age
To evaluate the VPCs in these sets, we use:
? English VPC dataset (Baldwin, 2008); which
lists 3,078 VPCs with valency (intransitive
and transitive) information;
? Comlex lexicon (Macleod and Grishman,
1998) containing 10,478 phrasal verbs;
? the Alvey Natural Language Tools (ANLT)
lexicon (Carroll and Grover, 1989) with
6,351 phrasal verbs.
4 VPCs in Child Language
To investigate whether any extra complexity in the
acquisition of VPCs is reflected in their reduced
presence in child-produced than in child-directed
sentences, we compare the proportion of VPCs in
the Children and Adults Sets, table 3. In absolute
terms adults produced more than double the num-
ber of VPCs that children did. However, given
the differences in size of the two sets, in relative
terms there was a similar proportion of VPC us-
age in these corpora for each of the groups: 7.95%
of the sentences produced by children contained
VPCs vs 8.38% of those by adults. Moreover, the
frequencies with which these VPCs are used by
both children and adults reflects the Zipfian distri-
bution found for the use of words in natural lan-
guages, with a large part of the VPCs occurring
just once in the data, table 4. In addition, in terms
of frequency, children?s production of VPCs re-
sembles that of the adults.
Total VPC Children Set Adults Set
Tokens 38,326 82,796
Types 1,579 2,468
Table 3: VPC usage in CHILDES
Frequency Children Set Adults Set
1 42.62% 43.03%
2 13.05% 15%
3 8.36% 6.48%
4 4.05% 4.5%
?5 31.92% 31%
Table 4: VPC types per frequency
Another possible source of divergence between
children and adults is in the lexical variety found
in VPCs. The potential difficulties with VPCs
may be manifested in children producing a re-
duced repertoire of VPCs or using a smaller set
of verbs to form these combinations. As shown in
table 3, adults, as expected, employ a larger VPC
vocabulary with 1.56 more types than children.
However, an examination of the distributions of
types reveals that they only differ by a scale. As
a result when children frequencies are multiplied
by a factor of 2.16, which corresponds to the ra-
tio between VPC tokens used by adults and chil-
dren (table 3), the resulting distribution has a very
46
good match with the adult distribution, see fig-
ure 1. Therefore, the lower number of VPC types
used by children can be explained totally by the
lower number of sentences they produced, and the
hypothesis that difficulties in VPCs would lead to
their avoidance is not confirmed by the data.
Nonetheless, there is a discrepancy between
the distributions found for the higher frequency
VPCs. Children have a more uniform distribution
and adults tend to repeat more often the higher
frequency combinations (top left corner of fig-
ure 1). An evidence that this discrepancy is partic-
ular for high frequency VPCs, and not their con-
stituent verbs, is shown in figure 2. This figure
displays the rank plot for the verbs present in the
VPCs, for both adults and children. The same
scale factor used in figure 1 is applied to compen-
sate for the lower number of VPC sentences in the
children set. This time the match is extraordinary,
spanning the whole vocabulary.
100 102 10410
0
101
102
103
104
105
rank
freq
uen
cy
VPC Usage
 
 adults
children*
Figure 1: VPC Usage Frequency vs Ranking. The
children frequency is scaled to match adult total
VPC usage.
Ranks however, might not tell the whole story.
It is important to verify if the same VPCs and
verbs are present in the both vocabularies, and fur-
ther if their orders in the ranks are similar. The
two groups have very similar preferences for VPC
usage, with a Kendall ? score of 0.63 which indi-
cates that they are highly correlated, as Kendall
? ranges from -1 to 1. Furthermore they use a
very similar set of verbs in VPCs, with a Kendall
100 102 10410
0
101
102
103
104
105
rank
freq
uen
cy
Verbs in VPCs Usage
 
 adultschildren*
Figure 2: Verbs in VPCs Usage Frequency vs
Ranking. The children frequency is scaled to
match adult total VPC usage.
? score of 0.84 pointing to a very strong corre-
lation. We find less agreement between the or-
ders of VPCs and verbs for both children and
adults, indicating that the order of the verbs in
the data is not predictive of the relative frequen-
cies of VPCs. We examined (a) if children?s VPC
ranks followed their verb ranks, (b) if adults VPC
ranks followed their verb ranks and (c) if chil-
dren?s VPC ranks followed adults? verb ranks.
The resulting Kendall scores were around 0.2 for
all three cases. Moreover, if the lower frequency
VPCs are removed to avoid potential cases of
noise, the Kendall ? score for VPCs by adults and
children increases with the threshold, second line
from the top in Figure 3, while it remains constant
for all the other cases. As an example, the top 10
VPC types used by children and adults are listed
in table 5. From these, 9 out of the 10 are the
same differing only in the order in which they ap-
pear. Most of these combinations are listed in one
of the dictionaries used for evaluation: 72% for
adults and 75.87% for children. When a thresh-
old of at least 5 counts is applied these values go
up to 87.72% for adults and 79.82% for children,
as would be expected. This indicates that besides
any possible lack of coverage for child-directed
VPCs in the lexicons or noise, it is in the lower
frequency combinations that novel and domains
specific non-standard usages can be found. Some
47
Rank Chidren Children Adult Adult Child
VPC Freq VPC Freq Rank
1 put on 2005 come on 6244 7
2 go in 1608 put on 4217 1
3 get out 1542 go on 2660 9
4 take off 1525 get out 2251 3
5 fall down 1329 take off 2249 4
6 put in 1284 put in 2177 6
7 come on 1001 sit down 2133 8
8 sit down 981 go in 1661 2
9 go on 933 come out 1654 10
10 come out 872 pick up 1650 18
Table 5: Top VPCs for Children and Adults
of the combinations not found in these dictionar-
ies include crawl in and creep up by adults and
erase off and crash down by children.
0
0.2
0.4
0.6
0.8
1
0 5 10 20
Lexical Choices for VPCs
K
e
n
d
a
l
l
 
t
a
u
threshold
Children / Adults VPCs Children VPCs / Verbs
Adults VPC / Verbs Children VPCs / Adult Verbs
Children /Adult Verbs
Figure 3: Kendall ? score per VPC frequency
threshold
Finally, despite adults having a larger verb vo-
cabulary used in VPCs than children, the two
groups have similar ratios of verb per VPCs: 2.81
VPCs for children and 2.79 for adults, table 6.
The top verbs used in VPCs types are also respon-
sible for very frequent VPC tokens (e.g. go, get,
come, take, put, make and move) accounting for
5.83% VPC types and 43.76% tokens for adults
and 7.02% of the types and 47.81% of the to-
kens for children, confirming the discrepancy dis-
cussed earlier. These are very general verbs and
some of the most frequent in the data, reported
among the first to be learned (Goldberg, 1999a)
which may facilitate their acquisition and use in
VPCs.
Comparing VPC types used by children and by
adults, this trend is confirmed: a large proportion
(72.32%) of the VPC types that children use is
also used by adults, Children ? Adult in table 6.
When low frequency VPCs types are removed,
this proportion increases (89.48%). Moreover,
when the VPCs used only by the adults are con-
sidered, most of these (93.44%) occur with fre-
quency lower than 5. This suggests that children
tend to follow quite closely the combinations em-
ployed by adults, and the lower frequency cases
may not yet be incorporated in their active vocab-
ulary.
In terms of the distance between verb and par-
ticle, there is a strong preference in the data for
joint combinations for both children and adults,
table 7. For the split cases, the majority contains
only one word between the verb and the particle.
Children in particular display a slight disprefer-
ence for longer distances between verbs and parti-
cles, and over 97% of VPCs have at most 2 words
between them.
Distance Children Set Adults Set
0 65.13% 64.14%
1 23.48% 22.15%
2 9.33% 10.90%
3 1.65% 2.15%
4 0.29% 0.47%
5 0.09% 0.16%
Table 7: Distance between verb and particle
5 Conclusions and future work
In this paper we presented an investigation of
VPCs in child-produced and child-directed sen-
tences in English to determine whether potential
complexities in the nature of these combinations
48
Children Adult Children ?Adult Children Adult
VPCs VPCs VPCs only VPCs only VPCs
VPCs 1579 2468 1142 437 1243
Verb in VPCs 561 884 401 160 483
Particle in VPCs 28 35 24 4 9
VPCs ? 5 504 766 451 53 278
Verb in VPCs ? 5 207 282 183 24 99
Particle in VPCs ? 5 18 20 17 1 3
Table 6: Number of VPC, Verb and Particle types by group, common usages
are reflected in their reduced usage by children.
The combination of these results shows that, de-
spite any additional difficulties, VPCs are as much
a feature in children?s data as in adults?. Children
follow very closely adult usage in terms of the
types and are sensitive to their frequencies, dis-
playing similar distributions to adults. They also
seem to use them in a similar manner in terms of
particle placement. Therefore no correction for
VPC complexity was found in this data.
Despite these striking similarities in many of
the distributions, there are still some discrepan-
cies between these two groups. In particular in the
VPC ranks, children present a more uniform dis-
tribution for higher frequency VPCs when com-
pared to adults. Moreover, there is a modest but
significant dispreference for longer distances be-
tween verb and particle for children. Whether
these reflect different strategies or efficiency con-
siderations deserves to be further investigated.
Acknowledgements
This research was partly supported by CNPq
Projects 551964/2011-1, 202007/2010-3,
305256/2008-4 and 309569/2009-5.
References
Timothy Baldwin. 2005. Deep lexical acquisition
of verb-particle constructions. Computer Speech &
Language Special issue on MWEs, 19(4):398?414.
Timothy Baldwin. 2008. A resource for evaluating
the deep lexical acquisition of english verb-particle
constructions. In Proceedings of the LREC Work-
shop Towards a Shared Task for Multiword Expres-
sions (MWE 2008), pages 1?2, Marrakech, Mo-
rocco, June.
Dwight Bolinger. 1971. The phrasal verb in English.
Harvard University Press, Harvard, USA.
L. S. Boynton-Hauerwas. 1998. The role of general
all purpose verbs in language acquisition: A com-
parison of children with specific language impair-
ments and their language-matched peers. 59.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on
the PARC depbank. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL 2006),
pages 41?48, Sidney, Australia, July. Association
for Computational Linguistics.
Nicoleta Calzolari, Charles Fillmore, Ralph Grishman,
Nancy Ide, Alessandro Lenci, Catherine Macleod,
and Antonio Zampolli. 2002. Towards best prac-
tice for multiword expressions in computational
lexicons. In Third International Conference on
Language Resources and Evaluation (LREC 2002),
pages 1934?1940, Las Palmas, Canary Islands,
Spain. European Language Resources Association.
John Carroll and Claire Grover. 1989. The derivation
of a large computational lexicon of English from
LDOCE. In B. Boguraev and E. Briscoe, editors,
Computational Lexicography for Natural Language
Processing. Longman.
M. Coltheart. 1981. The MRC psycholinguistic
database. Quarterly Journal of Experimental Psy-
chology, 33A:497?505.
Holger Diessel and Michael Tomasello. 2005. Particle
placement in early child language : A multifactorial
analysis. Corpus Linguistics and Linguistic Theory,
1(1):89?112.
Charles J. Fillmore, Paul Kay, and Mary C. O?Connor.
1988. Regularity and idiomaticity in grammatical
constructions: The case of Let Alone. Language,
64(3):510?538.
Charles Fillmore. 2003. Multiword expressions: An
extremist approach. Presented at Collocations and
idioms 2003: linguistic, computational, and psy-
cholinguistic perspectives.
Bruce Fraser. 1976. The Verb-Particle Combination
in English. Academic Press, New York, USA.
49
Adele E. Goldberg, 1999a. The Emergence of Lan-
guage, chapter Emergence of the semantics of
argument structure constructions, pages 197?212.
Carnegie Mellon Symposia on Cognition Series.
Adele E. Goldberg. 1999b. The emergence of the
semantics of argument structure constructions. In
B. MacWhinney, editor, Emergence of language.
Lawrence Erlbaum Associates, Hillsdale, NJ.
Stefan Gries. 2002. The influence of processing on
syntactic variation: Particle placement in english.
In Nicole Dehe?, Ray Jackendoff, Andrew McIn-
tyre, and Silke Urban, editors, Verb-Particle Ex-
plorations, pages 269?288. New York: Mouton de
Gruyter.
Ray Jackendoff. 1997. Twistin? the night away. Lan-
guage, 73:534?559.
C. R. Juhasz and B. Grela. 2008. Verb particle errors
in preschool children with specific language impair-
ment. Contemporary Issues in Communication Sci-
ence & Disorders, 35:76?83.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a preliminary investigation. University of
Chicago Press, Chicago, USA.
Barbara Lohse, John A Hawkins, and Thomas Wa-
sow. 2004. Domain minimization in english verb-
particle constructions. Language, 80(2):238?261.
Catherine Macleod and Ralph Grishman. 1998.
COMLEX syntax reference manual, Proteus
Project.
B. MacWhinney. 1995. The CHILDES project: tools
for analyzing talk. Hillsdale, NJ: Lawrence Erl-
baum Associates, second edition.
Rosamund E. Moon. 1998. Fixed Expressions and
Idioms in English: A Corpus-based Approach. Ox-
ford University Press.
Darren Pearce. 2002. A comparative evaluation of
collocation extraction techniques. In Third Inter-
national Conference on Language Resources and
Evaluation (LREC 2002), Las Palmas, Canary Is-
lands, Spain. European Language Resources Asso-
ciation.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. mwetoolkit: a framework for mul-
tiword expression identification. In Proceedings of
the Seventh International Conference on Language
Resources and Evaluation (LREC 2010), Malta,
May. European Language Resources Association.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-
word expressions: A pain in the neck for NLP.
In Proceedings of the 3rd International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing-2002), volume 2276/2010 of
Lecture Notes in Computer Science, pages 1?15,
Mexico City, Mexico, February. Springer.
K. Sagae, E. Davis, A. Lavie, B. MacWhinney, and
S. Wintner. 2010. Morphosyntactic annotation of
CHILDES transcripts. Journal of Child Language,
37(03):705?729.
J.H. Sawyer. 1999. Verb adverb and verb particle
constructions: their syntax and acquisition. s.n.
Joan H. Sawyer. 2000. Comments on clayton m. dar-
win and loretta s. gray?s ?going after the phrasal
verb: An alternative approach to classification?. a
reader reacts. TESOL Quarterly, 34(1):151?159.
Anna Siyanova and Norbert Schmitt. 2007. Na-
tive and nonnative use of multi-word vs. one-word
verbs. International Review of Applied Linguistics,
45:109139.
Anna Siyanova and Norbert Schmitt. 2008. L2 learner
production and processing of collocation: A multi-
study perspective. Canadian Modern Language Re-
view, 64(3):429458.
Aline Villavicencio, Beracah Yankama, Robert
Berwick, and Marco Idiart. 2012. A large scale
annotated child language construction database. In
Proceedings of the 8th LREC, Istanbul, Turkey.
Alison Wray. 2002. Formulaic Language and the Lex-
icon. Cambridge University Press, Cambridge, UK.
Alison Wray. 2009. Formulaic language in learn-
ers and native speakers. Language Teaching,
32(04):213?231.
50
