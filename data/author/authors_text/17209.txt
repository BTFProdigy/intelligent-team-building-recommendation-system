Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 66?73, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
NTNU-CORE: Combining strong features for semantic similarity
Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov, Bjo?rn Gamba?ck, Andre? Lynum
Norwegian University of Science and Technology
Department of Computer and Information and Science
Sem S?lands vei 7-9
NO-7491 Trondheim, Norway
{emarsi,hansmoe,bungum,sizov,gamback,andrely}@idi.ntnu.no
Abstract
The paper outlines the work carried out at
NTNU as part of the *SEM?13 shared task
on Semantic Textual Similarity, using an ap-
proach which combines shallow textual, dis-
tributional and knowledge-based features by
a support vector regression model. Feature
sets include (1) aggregated similarity based
on named entity recognition with WordNet
and Levenshtein distance through the calcula-
tion of maximum weighted bipartite graphs;
(2) higher order word co-occurrence simi-
larity using a novel method called ?Multi-
sense Random Indexing?; (3) deeper seman-
tic relations based on the RelEx semantic
dependency relationship extraction system;
(4) graph edit-distance on dependency trees;
(5) reused features of the TakeLab and DKPro
systems from the STS?12 shared task. The
NTNU systems obtained 9th place overall (5th
best team) and 1st place on the SMT data set.
1 Introduction
Intuitively, two texts are semantically similar if they
roughly mean the same thing. The task of formally
establishing semantic textual similarity clearly is
more complex. For a start, it implies that we have
a way to formally represent the intended meaning of
all texts in all possible contexts, and furthermore a
way to measure the degree of equivalence between
two such representations. This goes far beyond the
state-of-the-art for arbitrary sentence pairs, and sev-
eral restrictions must be imposed. The Semantic
Textual Similarity (STS) task (Agirre et al, 2012,
2013) limits the comparison to isolated sentences
only (rather than complete texts), and defines sim-
ilarity of a pair of sentences as the one assigned by
human judges on a 0?5 scale (with 0 implying no
relation and 5 complete semantic equivalence). It is
unclear, however, to what extent two judges would
agree on the level of similarity between sentences;
Agirre et al (2012) report figures on the agreement
between the authors themselves of about 87?89%.
As in most language processing tasks, there are
two overall ways to measure sentence similarity, ei-
ther by data-driven (distributional) methods or by
knowledge-driven methods; in the STS?12 task the
two approaches were used nearly equally much.
Distributional models normally measure similarity
in terms of word or word co-occurrence statistics, or
through concept relations extracted from a corpus.
The basic strategy taken by NTNU in the STS?13
task was to use something of a ?feature carpet bomb-
ing approach? in the way of first automatically ex-
tracting as many potentially useful features as possi-
ble, using both knowledge and data-driven methods,
and then evaluating feature combinations on the data
sets provided by the organisers of the shared task.
To this end, four different types of features were
extracted. The first (Section 2) aggregates similar-
ity based on named entity recognition with WordNet
and Levenshtein distance by calculating maximum
weighted bipartite graphs. The second set of features
(Section 3) models higher order co-occurrence sim-
ilarity relations using Random Indexing (Kanerva
et al, 2000), both in the form of a (standard) sliding
window approach and through a novel method called
?Multi-sense Random Indexing? which aims to sep-
arate the representation of different senses of a term
66
from each other. The third feature set (Section 4)
aims to capture deeper semantic relations using ei-
ther the output of the RelEx semantic dependency
relationship extraction system (Fundel et al, 2007)
or an in-house graph edit-distance matching system.
The final set (Section 5) is a straight-forward gath-
ering of features from the systems that fared best in
STS?12: TakeLab from University of Zagreb (S?aric?
et al, 2012) and DKPro from Darmstadt?s Ubiqui-
tous Knowledge Processing Lab (Ba?r et al, 2012).
As described in Section 6, Support Vector Regres-
sion (Vapnik et al, 1997) was used for solving the
multi-dimensional regression problem of combining
all the extracted feature values. Three different sys-
tems were created based on feature performance on
the supplied development data. Section 7 discusses
scores on the STS?12 and STS?13 test data.
2 Compositional Word Matching
Compositional word matching similarity is based
on a one-to-one alignment of words from the two
sentences. The alignment is obtained by maximal
weighted bipartite matching using several word sim-
ilarity measures. In addition, we utilise named entity
recognition and matching tools. In general, the ap-
proach is similar to the one described by Karnick
et al (2012), with a different set of tools used. Our
implementation relies on the ANNIE components in
GATE (Cunningham et al, 2002) and will thus be
referred to as GateWordMatch.
The processing pipeline for GateWordMatch
is: (1) tokenization by ANNIE English Tokeniser,
(2) part-of-speech tagging by ANNIE POS Tagger,
(3) lemmatization by GATE Morphological Anal-
yser, (4) stopword removal, (5) named entity recog-
nition based on lists by ANNIE Gazetteer, (6) named
entity recognition based on the JAPE grammar by
the ANNIE NE Transducer, (7) matching of named
entities by ANNIE Ortho Matcher, (8) computing
WordNet and Levenstein similarity between words,
(9) calculation of a maximum weighted bipartite
graph matching based on similarities from 7 and 8.
Steps 1?4 are standard preprocessing routines.
In step 5, named entities are recognised based on
lists that contain locations, organisations, compa-
nies, newspapers, and person names, as well as date,
time and currency units. In step 6, JAPE grammar
rules are applied to recognise entities such as ad-
dresses, emails, dates, job titles, and person names
based on basic syntactic and morphological features.
Matching of named entities in step 7 is based on
matching rules that check the type of named entity,
and lists with aliases to identify entities as ?US?,
?United State?, and ?USA? as the same entity.
In step 8, similarity is computed for each pair
of words from the two sentences. Words that are
matched as entities in step 7 get a similarity value
of 1.0. For the rest of the entities and non-entity
words we use LCH (Leacock and Chodorow, 1998)
similarity, which is based on a shortest path between
the corresponding senses in WordNet. Since word
sense disambiguation is not used, we take the simi-
larity between the nearest senses of two words. For
cases when the WordNet-based similarity cannot be
obtained, a similarity based on the Levenshtein dis-
tance (Levenshtein, 1966) is used instead. It is nor-
malised by the length of the longest word in the pair.
For the STS?13 test data set, named entity matching
contributed to 4% of all matched word pairs; LCH
similarity to 61%, and Levenshtein distance to 35%.
In step 9, maximum weighted bipartite matching
is computed using the Hungarian Algorithm (Kuhn,
1955). Nodes in the bipartite graph represent words
from the sentences, and edges have weights that cor-
respond to similarities between tokens obtained in
step 8. Weighted bipartite matching finds the one-to-
one alignment that maximizes the sum of similarities
between aligned tokens. Total similarity normalised
by the number of words in both sentences is used as
the final sentence similarity measure.
3 Distributional Similarity
Our distributional similarity features use Random
Indexing (RI; Kanerva et al, 2000; Sahlgren, 2005),
also employed in STS?12 by Tovar et al (2012);
Sokolov (2012); Semeraro et al (2012). It is an
efficient method for modelling higher order co-
occurrence similarities among terms, comparable to
Latent Semantic Analysis (LSA; Deerwester et al,
1990). It incrementally builds a term co-occurrence
matrix of reduced dimensionality through the use of
a sliding window and fixed size index vectors used
for training context vectors, one per unique term.
A novel variant, which we have called ?Multi-
67
sense Random Indexing? (MSRI), inspired by
Reisinger and Mooney (2010), attempts to capture
one or more ?senses? per unique term in an unsu-
pervised manner, each sense represented as an indi-
vidual vector in the model. The method is similar to
classical sliding window RI, but each term can have
multiple context vectors (referred to as ?sense vec-
tors? here) which are updated individually. When
updating a term vector, instead of directly adding the
index vectors of the neighbouring terms in the win-
dow to its context vector, the system first computes a
separate window vector consisting of the sum of the
index vectors. Then cosine similarity is calculated
between the window vector and each of the term?s
sense vectors. Each similarity score is in turn com-
pared to a set similarity threshold: if no score ex-
ceeds the threshold, the sentence vector is added as
a new separate sense vector for the term; if exactly
one score is above the threshold, the window vector
is added to that sense vector; and if multiple scores
are above the threshold, all the involved senses are
merged into one sense vector, together with the win-
dow vector. This accomplishes an incremental clus-
tering of senses in an unsupervised manner while re-
taining the efficiency of classical RI.
As data for training the models we used the
CLEF 2004?2008 English corpus (approx. 130M
words). Our implementation of RI and MSRI is
based on JavaSDM (Hassel, 2004). For classical
RI, we used stopword removal (using a customised
versions of the English stoplist from the Lucene
project), window size of 4+4, dimensionality set to
1800, 4 non-zeros, and unweighted index vector in
the sliding window. For MSRI, we used a simi-
larity threshold of 0.2, a vector dimensionality of
800, a non-zero count of 4, and window size of
5+5. The index vectors in the sliding window were
shifted to create direction vectors (Sahlgren et al,
2008), and weighted by distance to the target term.
Rare senses with a frequency below 10 were ex-
cluded. Other sliding-window schemes, including
unweighted non-shifted vectors and Random Permu-
tation (Sahlgren et al, 2008), were tested, but none
outperformed the sliding-window schemes used.
Similarity between sentence pairs was calcu-
lated as the normalised maximal bipartite similar-
ity between term pairs in each sentence, resulting
in the following features: (1) MSRI-Centroid:
each term is represented as the sum of its sense
vectors; (2) MSRI-MaxSense: for each term
pair, the sense-pair with max similarity is used;
(3) MSRI-Context: for each term, its neigh-
bouring terms within a window of 2+2 is used as
context for picking a single, max similar, sense
from the target term to be used as its represen-
tation; (4) MSRI-HASenses: similarity between
two terms is computed by applying the Hungarian
Algorithm to all their possible sense pair mappings;
(5) RI-Avg: classical RI, each term is represented
as a single context vector; (6) RI-Hungarian:
similarity between two sentences is calculated us-
ing the Hungarian Algorithm. Alternatively, sen-
tence level similarity was computed as the cosine
similarity between sentence vectors composed of
their terms? vectors. The corresponding features
are (1) RI-SentVectors-Norm: sentence vec-
tors are created by summing their constituent terms
(i.e., context vectors), which have first been normal-
ized; (2) RI-SentVectors-TFIDF: same as be-
fore, but TF*IDF weights are added.
4 Deeper Semantic Relations
Two deep strategies were employed to accompany
the shallow-processed feature sets. Two existing
systems were used to provide the basis for these fea-
tures, namely the RelEx system (Fundel et al, 2007)
from the OpenCog initiative (Hart and Goertzel,
2008), and an in-house graph-edit distance system
developed for plagiarism detection (R?kenes, 2013).
RelEx outputs syntactic trees, dependency graphs,
and semantic frames as this one for the sentence
?Indian air force to buy 126 Rafale fighter jets?:
Commerce buy:Goods(buy,jet)
Entity:Entity(jet,jet)
Entity:Name(jet,Rafale)
Entity:Name(jet,fighter)
Possibilities:Event(hyp,buy)
Request:Addressee(air,you)
Request:Message(air,air)
Transitive action:Beneficiary(buy,jet)
Three features were extracted from this: first, if
there was an exact match of the frame found in s1
with s2; second, if there was a partial match until the
first argument (Commerce buy:Goods(buy);
and third if there was a match of the frame category
68
(Commerce buy:Goods).
In STS?12, Singh et al (2012) matched Universal
Networking Language (UNL) graphs against each
other by counting matches of relations and univer-
sal words, while Bhagwani et al (2012) calculated
WordNet-based word-level similarities and created
a weighted bipartite graph (see Section 2). The
method employed here instead looked at the graph
edit distance between dependency graphs obtained
with the Maltparser dependency parser (Nivre et al,
2006). Edit distance is the defined as the minimum
of the sum of the costs of the edit operations (in-
sertion, deletion and substitution of nodes) required
to transform one graph into the other. It is approx-
imated with a fast but suboptimal algorithm based
on bipartite graph matching through the Hungarian
algorithm (Riesen and Bunke, 2009).
5 Reused Features
The TakeLab ?simple? system (S?aric? et al, 2012) ob-
tained 3rd place in overall Pearson correlation and
1st for normalized Pearson in STS?12. The source
code1 was used to generate all its features, that is,
n-gram overlap, WordNet-augmented word overlap,
vector space sentence similarity, normalized differ-
ence, shallow NE similarity, numbers overlap, and
stock index features.2 This required the full LSA
vector space models, which were kindly provided
by the TakeLab team. The word counts required for
computing Information Content were obtained from
Google Books Ngrams.3
The DKPro system (Ba?r et al, 2012) obtained first
place in STS?12 with the second run. We used the
source code4 to generate features for the STS?12
and STS?13 data. Of the string-similarity features,
we reused the Longest Common Substring, Longest
Common Subsequence (with and without normaliza-
tion), and Greedy String Tiling measures. From the
character/word n-grams features, we used Charac-
ter n-grams (n = 2, 3, 4), Word n-grams by Con-
tainment w/o Stopwords (n = 1, 2), Word n-grams
1http://takelab.fer.hr/sts/
2We did not use content n-gram overlap or skip n-grams.
3http://storage.googleapis.com/books/
ngrams/books/datasetsv2.html, version 20120701,
with 468,491,999,592 words
4http://code.google.com/p/
dkpro-similarity-asl/
by Jaccard (n = 1, 3, 4), and Word n-grams by Jac-
card w/o Stopwords (n = 2, 4). Semantic similarity
measures include WordNet Similarity based on the
Resnik measure (two variants) and Explicit Seman-
tic Similarity based on WordNet, Wikipedia or Wik-
tionary. This means that we reused all features from
DKPro run 1 except for Distributional Thesaurus.
6 Systems
Our systems follow previous submissions to the STS
task (e.g., S?aric? et al, 2012; Banea et al, 2012) in
that feature values are extracted for each sentence
pair and combined with a gold standard score in or-
der to train a Support Vector Regressor on the result-
ing regression task. A postprocessing step guaran-
tees that all scores are in the [0, 5] range and equal 5
if the two sentences are identical. SVR has been
shown to be a powerful technique for predictive data
analysis when the primary goal is to approximate a
function, since the learning algorithm is applicable
to continuous classes. Hence support vector regres-
sion differs from support vector machine classifica-
tion where the goal rather is to take a binary deci-
sion. The key idea in SVR is to use a cost function
for building the model which tries to ignore noise in
training data (i.e., data which is too close to the pre-
diction), so that the produced model in essence only
depends on a more robust subset of the extracted fea-
tures.
Three systems were created using the supplied
annotated data based on Microsoft Research Para-
phrase and Video description corpora (MSRpar and
MSvid), statistical machine translation system out-
put (SMTeuroparl and SMTnews), and sense map-
pings between OntoNotes and WordNet (OnWN).
The first system (NTNU1) includes all TakeLab and
DKPro features plus the GateWordMatch feature
with the SVR in its default setting.5 The training
material consisted of all annotated data available,
except for the SMT test set, where it was limited to
SMTeuroparl and SMTnews. The NTNU2 system is
similar to NTNU1, except that the training material
for OnWN and FNWN excluded MSRvid and that
the SVR parameter C was set to 200. NTNU3 is
similar to NTNU1 except that all features available
are included.
5RBF kernel,  = 0.1, C = #samples, ? = 1#features
69
Data NTNU1 NTNU2 NTNU3
MSRpar 0.7262 0.7507 0.7221
MSRvid 0.8660 0.8882 0.8662
SMTeuroparl 0.5843 0.3386 0.5503
SMTnews 0.5840 0.5592 0.5306
OnWN 0.7503 0.6365 0.7200
mean 0.7022 0.6346 0.6779
Table 1: Correlation score on 2012 test data
7 Results
System performance is evaluated using the Pearson
product-moment correlation coefficient (r) between
the system scores and the human scores. Results on
the 2012 test data (i.e., 2013 development data) are
listed in Table 1. This basically shows that except
for the GateWordMatch, adding our other fea-
tures tends to give slightly lower scores (cf. NTNU1
vs NTNU3). In addition, the table illustrates that op-
timizing the SVR according to cross-validated grid
search on 2012 training data (here C = 200), rarely
pays off when testing on unseen data (cf. NTNU1
vs NTNU2).
Table 2 shows the official results on the test data.
These are generally in agreement with the scores on
the development data, although substantially lower.
Our systems did particularly well on SMT, holding
first and second position, reasonably good on head-
lines, but not so well on the ontology alignment data,
resulting in overall 9th (NTNU1) and 12th (NTNU3)
system positions (5th best team). Table 3 lists the
correlation score and rank of the ten best individual
features per STS?13 test data set, and those among
the top-20 overall, resulting from linear regression
on a single feature. Features in boldface are gen-
uinely new (i.e., described in Sections 2?4).
Overall the character n-gram features are the most
informative, particularly for HeadLine and SMT.
The reason may be that these not only capture word
overlap (Ahn, 2011), but also inflectional forms and
spelling variants.
The (weighted) distributional similarity features
based on NYT are important for HeadLine and SMT,
which obviously contain sentence pairs from the
news genre, whereas the Wikipedia based feature is
more important for OnWN and FNWN. WordNet-
based measures are highly relevant too, with variants
NTNU1 NTNU2 NTNU3
Data r n r n r n
Head 0.7279 11 0.5909 59 0.7274 12
OnWN 0.5952 31 0.1634 86 0.5882 32
FNWN 0.3215 45 0.3650 27 0.3115 49
SMT 0.4015 2 0.3786 9 0.4035 1
mean 0.5519 9 0.3946 68 0.5498 12
Table 2: Correlation score and rank on 2013 test data
relying on path length outperforming those based on
Resnik similarity, except for SMT.
As is to be expected, basic word and lemma uni-
gram overlap prove to be informative, with overall
unweighted variants resulting in higher correlation.
Somewhat surprisingly, higher order n-gram over-
laps (n > 1) seem to be less relevant. Longest com-
mon subsequence and substring appear to work par-
ticularly well for OnWN and FNWN, respectively.
GateWordMatch is highly relevant too, in
agreement with earlier results on the development
data. Although treated as a single feature, it is ac-
tually a combination of similarity features where an
appropriate feature is selected for each word pair.
This ?vertical? way of combining features can po-
tentially provide a more fine-grained feature selec-
tion, resulting in less noise. Indeed, if two words are
matching as named entities or as close synonyms,
less precise types of features such as character-based
and data-driven similarity should not dominate the
overall similarity score.
It is interesting to find that MSRI outper-
forms both classical RI and ESA (Gabrilovich and
Markovitch, 2007) on this task. Still, the more ad-
vanced features, such as MSRI-Context, gave in-
ferior results compared to MSRI-Centroid. This
suggests that more research on MSRI is needed
to understand how both training and retrieval can
be optimised. Also, LSA-based features (see
tl.weight-dist-sim-wiki) achieve better
results than both MSRI, RI and ESA. Then again,
larger corpora were used for training the LSA mod-
els. RI has been shown to be comparable to LSA
(Karlgren and Sahlgren, 2001), and since a relatively
small corpus was used for training the RI/MSRI
models, there are reasons to believe that better
scores can be achieved by both RI- and MSRI-based
features by using more training data.
70
HeadLine OnWN FNWN SMT Mean
Features r n r n r n r n r n
CharacterNGramMeasure-3 0.72 2 0.39 2 0.44 3 0.70 1 0.56 1
CharacterNGramMeasure-4 0.69 3 0.38 5 0.45 2 0.67 6 0.55 2
CharacterNGramMeasure-2 0.73 1 0.37 9 0.34 10 0.69 2 0.53 3
tl.weight-dist-sim-wiki 0.58 14 0.39 3 0.45 1 0.67 5 0.52 4
tl.wn-sim-lem 0.69 4 0.40 1 0.41 5 0.59 10 0.52 5
GateWordMatch 0.67 8 0.37 11 0.34 11 0.60 9 0.50 6
tl.dist-sim-nyt 0.69 5 0.34 28 0.26 23 0.65 8 0.49 7
tl.n-gram-match-lem-1 0.68 6 0.36 16 0.37 8 0.51 14 0.48 8
tl.weight-dist-sim-nyt 0.57 17 0.37 14 0.29 18 0.66 7 0.47 9
tl.n-gram-match-lc-1 0.68 7 0.37 10 0.32 13 0.50 17 0.47 10
MCS06-Resnik-WordNet 0.49 26 0.36 22 0.28 19 0.68 3 0.45 11
TWSI-Resnik-WordNet 0.49 27 0.36 23 0.28 20 0.68 4 0.45 12
tl.weight-word-match-lem 0.56 18 0.37 16 0.37 7 0.50 16 0.45 13
MSRI-Centroid 0.60 13 0.36 17 0.37 9 0.45 19 0.45 14
tl.weight-word-match-olc 0.56 19 0.38 8 0.32 12 0.51 15 0.44 15
MSRI-MaxSense 0.58 15 0.36 15 0.31 14 0.45 20 0.42 16
GreedyStringTiling-3 0.67 9 0.38 6 0.31 15 0.34 29 0.43 17
ESA-Wikipedia 0.50 25 0.30 38 0.32 14 0.54 12 0.42 18
WordNGramJaccard-1 0.64 10 0.37 12 0.25 25 0.33 30 0.40 19
WordNGramContainment-1-stopword 0.64 25 0.38 7 0.25 24 0.32 31 0.40 20
RI-Hungarian 0.58 16 0.33 31 0.10 34 0.42 22 0.36 24
RI-AvgTermTerm 0.56 20 0.33 32 0.11 33 0.37 28 0.34 25
LongestCommonSubstring 0.40 29 0.30 39 0.42 4 0.37 27 0.37 26
ESA-WordNet 0.11 43 0.30 40 0.41 6 0.49 18 0.33 29
LongestCommonSubsequenceNorm 0.53 21 0.39 4 0.19 27 0.18 37 0.32 30
MultisenseRI-ContextTermTerm 0.39 31 0.33 33 0.28 21 0.15 38 0.29 33
MultisenseRI-HASensesTermTerm 0.39 32 0.33 34 0.28 22 0.15 39 0.29 34
RI-SentVectors-Norm 0.34 35 0.35 26 -0.01 51 0.24 35 0.23 39
RelationSimilarity 0.31 39 0.35 27 0.24 26 0.02 41 0.23 40
RI-SentVectors-TFIDF 0.27 40 0.15 50 0.08 40 0.23 36 0.18 41
GraphEditDistance 0.33 38 0.25 46 0.13 31 -0.11 49 0.15 42
Table 3: Correlation score and rank of the best features
8 Conclusion and Future Work
The NTNU system can be regarded as continuation
of the most successful systems from the STS?12
shared task, combining shallow textual, distribu-
tional and knowledge-based features into a support
vector regression model. It reuses features from the
TakeLab and DKPro systems, resulting in a very
strong baseline.
Adding new features to further improve
performance turned out to be hard: only
GateWordMatch yielded improved perfor-
mance. Similarity features based on both classical
and innovative variants of Random Indexing were
shown to correlate with semantic textual similarity,
but did not complement the existing distributional
features. Likewise, features designed to reveal
deeper syntactic (graph edit distance) and semantic
relations (RelEx) did not add to the score.
As future work, we would aim to explore a
vertical feature composition approach similar to
GateWordMatch and contrast it with the ?flat?
composition currently used in our systems.
Acknowledgements
Thanks to TakeLab for source code of their ?simple?
system and the full-scale LSA models. Thanks to the
team from Ubiquitous Knowledge Processing Lab
for source code of their DKPro Similarity system.
71
References
Agirre, E., Cer, D., Diab, M., and Gonzalez-Agirre,
A. (2012). SemEval-2012 Task 6: A pilot on se-
mantic textual similarity. In *SEM (2012), pages
385?393.
Agirre, E., Cer, D., Diab, M., Gonzalez-Agirre, A.,
and Guo, W. (2013). *SEM 2013 Shared Task:
Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second
Joint Conference on Lexical and Computational
Semantics. Association for Computational Lin-
guistics.
Ahn, C. S. (2011). Automatically detecting authors?
native language. PhD thesis, Monterey, Califor-
nia. Naval Postgraduate School.
Banea, C., Hassan, S., Mohler, M., and Mihalcea, R.
(2012). UNT: a supervised synergistic approach
to semantic text similarity. In *SEM (2012),
pages 635?642.
Ba?r, D., Biemann, C., Gurevych, I., and Zesch, T.
(2012). UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity
measures. In *SEM (2012), pages 435?440.
Bhagwani, S., Satapathy, S., and Karnick, H. (2012).
sranjans : Semantic textual similarity using maxi-
mal weighted bipartite graph matching. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics ? Volume 1: Proceed-
ings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation (Sem-
Eval 2012), pages 579?585, Montre?al, Canada.
Association for Computational Linguistics.
Cunningham, H., Maynard, D., Bontcheva, K., and
Tablan, V. (2002). GATE: A framework and
graphical development environment for robust
NLP tools and applications. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 168?175, Philadel-
phia, Pennsylvania. ACL.
Deerwester, S., Dumais, S., Furnas, G., Landauer,
T., and Harshman, R. (1990). Indexing by latent
semantic analysis. Journal of the American Soci-
ety for Information Science, 41(6):391?407.
Fundel, K., Ku?ffner, R., and Zimmer, R. (2007).
RelEx - Relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Gabrilovich, E. and Markovitch, S. (2007). Comput-
ing semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of The
Twentieth International Joint Conference for Ar-
tificial Intelligence., pages 1606?1611.
Hart, D. and Goertzel, B. (2008). Opencog: A soft-
ware framework for integrative artificial general
intelligence. In Proceedings of the 2008 confer-
ence on Artificial General Intelligence 2008: Pro-
ceedings of the First AGI Conference, pages 468?
472, Amsterdam, The Netherlands, The Nether-
lands. IOS Press.
Hassel, M. (2004). JavaSDM package.
Kanerva, P., Kristoferson, J., and Holst, A. (2000).
Random indexing of text samples for latent se-
mantic analysis. In Gleitman, L. and Josh, A.,
editors, Proceedings of the 22nd Annual Confer-
ence of the Cognitive Science Society, page 1036.
Erlbaum.
Karlgren, J. and Sahlgren, M. (2001). From Words
to Understanding. In Uesaka, Y., Kanerva, P., and
Asoh, H., editors, Foundations of real-world in-
telligence, chapter 26, pages 294?311. Stanford:
CSLI Publications.
Karnick, H., Satapathy, S., and Bhagwani, S. (2012).
sranjans: Semantic textual similarity using max-
imal bipartite graph matching. In *SEM (2012),
pages 579?585.
Kuhn, H. (1955). The Hungarian method for the as-
signment problem. Naval research logistics quar-
terly, 2:83?97.
Leacock, C. and Chodorow, M. (1998). Combin-
ing local context and WordNet similarity for word
sense identification. WordNet: An electronic lexi-
cal . . . .
Levenshtein, V. I. (1966). Binary codes capable of
correcting deletions, insertions and reversals. So-
viet Physics Doklady, 10(8):707?710.
Nivre, J., Hall, J., and Nilsson, J. (2006). Malt-
parser: A data-driven parser-generator for depen-
dency parsing. In In Proc. of LREC-2006, pages
2216?2219.
72
Reisinger, J. and Mooney, R. (2010). Multi-
prototype vector-space models of word meaning.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
number June, pages 109?117.
Riesen, K. and Bunke, H. (2009). Approximate
graph edit distance computation by means of bi-
partite graph matching. Image and Vision Com-
puting, 27(7):950?959.
R?kenes, H. (2013). Graph-Edit Distance Applied to
the Task of Detecting Plagiarism. Master?s the-
sis, Norwegian University of Science and Tech-
nology.
Sahlgren, M. (2005). An introduction to random in-
dexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International
Conference on Terminology and Knowledge En-
gineering, TKE, volume 5.
Sahlgren, M., Holst, A., and Kanerva, P. (2008). Per-
mutations as a Means to Encode Order in Word
Space. Proceedings of the 30th Conference of the
Cognitive Science Society.
S?aric?, F., Glavas?, G., Karan, M., S?najder, J., and
Bas?ic?, B. D. (2012). TakeLab: systems for mea-
suring semantic text similarity. In *SEM (2012),
pages 441?448.
*SEM (2012). Proceedings of the First Joint Con-
ference on Lexical and Computational Seman-
tics (*SEM), volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation,
Montreal, Canada. Association for Computational
Linguistics.
Semeraro, G., Aldo, B., and Orabona, V. E. (2012).
UNIBA: Distributional semantics for textual sim-
ilarity. In *SEM (2012), pages 591?596.
Singh, J., Bhattacharya, A., and Bhattacharyya, P.
(2012). janardhan: Semantic textual similarity us-
ing universal networking language graph match-
ing. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Vol-
ume 1: Proceedings of the main conference and
the shared task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evalu-
ation (SemEval 2012), pages 662?666, Montre?al,
Canada. Association for Computational Linguis-
tics.
Sokolov, A. (2012). LIMSI: learning semantic simi-
larity by selecting random word subsets. In *SEM
(2012), pages 543?546.
Tovar, M., Reyes, J., and Montes, A. (2012). BUAP:
a first approximation to relational similarity mea-
suring. In *SEM (2012), pages 502?505.
Vapnik, V., Golowich, S. E., and Smola, A. (1997).
Support vector method for function approxima-
tion, regression estimation, and signal process-
ing. In Mozer, M. C., Jordan, M. I., and Petsche,
T., editors, Advances in Neural Information Pro-
cessing Systems, volume 9, pages 281?287. MIT
Press, Cambridge, Massachusetts.
73
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 83?90,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Towards Dynamic Word Sense Discrimination with Random Indexing
Hans Moen, Erwin Marsi, Bjo?rn Gamba?ck
Norwegian University of Science and Technology
Department of Computer and Information and Science
Sem S?lands vei 7-9
NO-7491 Trondheim, Norway
{hansmoe,emarsi,gamback}@idi.ntnu.no
Abstract
Most distributional models of word sim-
ilarity represent a word type by a single
vector of contextual features, even though,
words commonly have more than one
sense. The multiple senses can be captured
by employing several vectors per word in a
multi-prototype distributional model, pro-
totypes that can be obtained by first con-
structing all the context vectors for the
word and then clustering similar vectors
to create sense vectors. Storing and clus-
tering context vectors can be expensive
though. As an alternative, we introduce
Multi-Sense Random Indexing, which per-
forms on-the-fly (incremental) clustering.
To evaluate the method, a number of mea-
sures for word similarity are proposed,
both contextual and non-contextual, in-
cluding new measures based on optimal
alignment of word senses. Experimental
results on the task of predicting semantic
textual similarity do, however, not show
a systematic difference between single-
prototype and multi-prototype models.
1 Introduction
Many terms have more than one meaning, or
sense. Some of these senses are static and can
be listed in dictionaries and thesauri, while other
senses are dynamic and determined by the con-
texts the terms occur in. Work in Word Sense Dis-
ambiguation often concentrate on the static word
senses, making the task of distinguishing between
them one of classification into a predefined set of
classes (i.e., the given word senses); see, e.g., Erk
et al (2013; Navigli (2009) for overviews of cur-
rent work in the area. The idea of fixed generic
word senses has received a fair amount of criti-
cism in the literature (Kilgarriff, 2000).
This paper instead primarily investigates dy-
namically appearing word senses, word senses that
depend on the actual usage of a term in a cor-
pus or a domain. This task is often referred to as
Word Sense Induction or Word Sense Discrimina-
tion (Schu?tze, 1998). This is, in contrast, essen-
tially a categorisation problem, distinguished by
different senses being more or less similar to each
other at a given time, given some input data. The
dividing line between Word Sense Disambigua-
tion and Discrimination is not necessarily razor
sharp though: also different senses of a term listed
in a dictionary tend to have some level of overlap.
In recent years, distributional models have been
widely used to infer word similarity. Most such
models represent a word type by a single vector of
contextual features obtained from co-occurrence
counts in large textual corpora. By assigning a
single vector to each term in the corpus, the re-
sulting model assumes that each term has a fixed
semantic meaning (relative to all the other terms).
However, due to homonomy and polysemy, word
semantics cannot be adequately represented by a
single-prototype vector.
Multi-prototype distributional models in con-
trast employ different vectors to represent different
senses of a word (Reisinger and Mooney, 2010).
Multiple prototypes can be obtained by first con-
structing context vectors for all words and then
clustering similar context vectors to create a sense
vector. This may be expensive, as vectors need to
stored and clustered. As an alternative, we propose
a new method called Multi-Sense Random Index-
ing (MSRI), which is based on Random Indexing
(Kanerva et al, 2000) and performs an on-the-fly
(incremental) clustering.
MSRI is a method for building a multi-
prototype / multi-sense vector space model, which
attempts to capture one or more senses per unique
term in an unsupervised manner, where each sense
is represented as a separate vector in the model.
83
This differs from the classical Random Indexing
(RI) method which assumes a static sense inven-
tory by restricting each term to have only one vec-
tor (sense) per term, as described in Section 2. The
MSRI method is introduced in Section 3.
Since the induced dynamic senses do not neces-
sarily correspond to the traditional senses distin-
guished by humans, we perform an extrinsic eval-
uation by applying the resulting models to data
from the Semantic Textual Similarity shared task
(Agirre et al, 2013), in order to compare MSRI
to the classical RI method. The experimental set-
up is the topic of Section 4, while the results of
the experiments are given in Section 5. Section 6
then sums up the discussion and points to ways in
which the present work could be continued.
2 Vector Space Models
With the introduction of LSA, Latent Semantic
Analysis (Deerwester et al, 1990), distributed
models of lexical semantics, built from unla-
belled free text data, became a popular sub-field
within the language processing research commu-
nity. Methods for building such semantic mod-
els rely primarily on term co-occurrence infor-
mation, and attempt to capture latent relations
from analysing large amounts of text. Most of
these methods represent semantic models as multi-
dimensional vectors in a vector space model.
After LSA, other methods for building seman-
tic models have been proposed, one of them being
Random Indexing (Kanerva et al, 2000). Com-
mon to these methods is that they generate a con-
text vector for each unique term in the training data
which represents the term?s ?contextual? meaning
in the vector space. By assigning a single con-
text vector to each term in the corpus, the resulting
model assumes that each term has a fixed semantic
meaning (relative to all other terms).
Random Indexing incrementally builds a co-
occurrence matrix of reduced dimensionality, by
first assigning index vectors to each unique term.
The vectors are of a predefined size (typically
around 1000), and consist of a few randomly
placed 1s and -1s. Context vectors of the same size
are also assigned to each term, initially consisting
of only zeros. When traversing a document corpus
using a sliding window of a fixed size, the context
vectors are continuously updated: the term in the
centre of the window (the target term), has the in-
dex vectors of its neighbouring terms (the ones in
the window) added to its context vector using vec-
tor summation. Then the cosine similarity mea-
sure can be used on term pairs to calculate their
similarity (or ?contextual similarity?).
Random Indexing has achieved promising re-
sults in various experiments, for example, on the
TOEFL test (?Test of English as a Foreign Lan-
guage?) (Kanerva et al, 2000). However, it is ev-
ident that many terms have more than one mean-
ing or sense, some being static and some dynamic,
that is, determined by the contexts the terms occur
in. Schu?tze (1998) proposed a method for clus-
tering the contextual occurrences of terms into in-
dividual ?prototype? vectors, where one term can
have multiple prototype vectors representing sep-
arate senses of the term. Others have adopted
the same underlying idea, using alternative meth-
ods and techniques (Reisinger and Mooney, 2010;
Huang et al, 2012; Van de Cruys et al, 2011; Dinu
and Lapata, 2010).
3 Multi-Sense Random Indexing, MSRI
Inspired by the work of Schu?tze (1998) and
Reisinger and Mooney (2010), this paper intro-
duces a novel variant of Random Indexing, which
we have called ?Multi-Sense Random Indexing?.
MSRI attempts to capture one or more senses per
unique term in an unsupervised and incremental
manner, each sense represented as an separate vec-
tor in the model. The method is similar to classical
sliding window RI, but each term can have mul-
tiple context vectors (referred to as sense vectors
here) which are updated separately.
When updating a term vector, instead of directly
adding the index vectors of the neighbouring terms
in the window to its context vector, the system first
computes a separate window vector consisting of
the sum of the index vectors. The similarity be-
tween the window vector and each of the term?s
sense vectors is calculated. Each similarity score
is then compared to a pre-set similarity threshold:
? if no score exceeds the threshold, the window
vector becomes a new separate sense vector
for the term,
? if exactly one score is above the threshold,
the window vector is added to that sense vec-
tor, and
? if multiple scores are above the threshold, all
the involved senses are merged into one sense
vector, together with the window vector.
84
Algorithm 1 MSRI training
for all terms t in a document D do
generate window vector ~win from the neigh-
bouring words? index vectors
for all sense vectors ~si of t do
sim(si) = CosSim( ~win,~si)
end for
if sim(si..k) ? ? then
Merge ~si..k and ~win through summing
else
if sim(si) ? ? then
~si+ = ~win
end if
else
if sim(si..n) < ? then
Assign ~win as new sense vector of t
end if
end if
end for
See Algorithm 1 for a pseudo code version. Here
? represents the similarity threshold.
This accomplishes an incremental (on-line)
clustering of senses in an unsupervised manner,
while retaining the other properties of classical RI.
Even though the algorithm has a slightly higher
complexity than classical RI, this is mainly a mat-
ter of optimisation, which is not the focus of this
paper. The incremental clustering that we apply
is somewhat similar to what is used by Lughofer
(2008), although we are storing in memory only
one element (i.e., vector) for each ?cluster? (i.e.,
sense) at any given time.
When looking up a term in the vector space, a
pre-set sense-frequency threshold is applied to fil-
ter out ?noisy? senses. Hence, senses that have
occurred less than the threshold are not included
when looking up a term and its senses for, for ex-
ample, similarity calculations.
As an example of what the resulting models
contain in terms of senses, Table 1 shows four dif-
ferent senses of the term ?round? produced by the
MSRI model. Note that these senses do not nec-
essarily correspond to human-determined senses.
The idea is only that using multiple prototype
vectors facilitates better modelling of a term?s
meaning than a single prototype (Reisinger and
Mooney, 2010).
round1 round2 round3 round4
finish camping inch launcher
final restricted bundt grenade
match budget dough propel
half fare thick antitank
third adventure cake antiaircraft
Table 1: Top-5 most similar terms for four dif-
ferent senses of ?round? using the Max similarity
measure to the other terms in the model.
3.1 Term Similarity Measures
Unlike classical RI, which only has a single con-
text vector per term and thus calculates similarity
between two terms directly using cosine similarity,
there are multiple ways of calculating the similar-
ity between two terms in MSRI. Some alternatives
are described in Reisinger and Mooney (2010). In
the experiment in this paper, we test four ways of
calculating similarity between two terms t and t?
in isolation, with the Average and Max methods
stemming from Reisinger and Mooney (2010).
Let ~si..n and ~s?j..m be the sets of sense vectors
corresponding to the terms t and t? respectively.
Term similarity measures are then defined as:
Centroid
For term t, compute its centroid vector by
summing its sense vectors ~si..n. The same is
done for t? with its sense vectors ~s?j..m. These
centroids are in turn used to calculate the co-
sine similarity between t and t?.
Average
For all ~si..n in t, find the pair ~si, ~s?j with high-
est cosine similarity:
1
n
n?
i=1
CosSimmax(~si, ~s?j)
Then do the same for all ~s?j..m in t?:
1
m
m?
j=1
CosSimmax(~s?j , ~si)
The similarity between t and t? is computed
as the average of these two similarity scores.
Max
The similarity between ti and t?i equals the
similarity of their most similar sense:
Sim(t, t?) = CosSimmaxij (~si, ~s?i)
85
Hungarian Algorithm
First cosine similarity is computed for each
possible pair of sense vectors ~si..n and ~s?j..m,
resulting in a matrix of similarity scores.
Finding the optimal matching from senses ~si
to ~s?j that maximises the sum of similarities
is known as the assignment problem. This
combinatorial optimisation problem can be
solved in polynomial time through the Hun-
garian Algorithm (Kuhn, 1955). The over-
all similarity between terms t and t? is then
defined as the average of the similarities be-
tween their aligned senses.
All measures defined so far calculate similarity be-
tween terms in isolation. In many applications,
however, terms occur in a particular context that
can be exploited to determine their most likely
sense. Narrowing down their possible meaning to
a subset of senses, or a single sense, can be ex-
pected to yield a more adequate estimation of their
similarity. Hence a context-sensitive measure of
term similarity is defined as:
Contextual similarity
Let ~C and ~C ? be vectors representing the con-
texts of terms t and t? respectively. These
context vectors are constructed by summing
the index vectors of the neighbouring terms
within a window, following the same proce-
dure as used when training the MSRI model.
We then find s? and s? ? as the sense vectors
best matching the context vectors:
s? = argmaxi CosSim(~si, ~C)
s? ? = argmaxj CosSim(~sj , ~C ?)
Finally, contextual similarity is defined as the
similarity between these sense vectors:
Simcontext(t, t
?) = CosSim(s?, s? ?)
3.2 Sentence Similarity Features
In the experiments reported on below, a range of
different ways to represent sentences were tested.
Sentence similarity was generally calculated by
the average of the maximum similarity between
pairs of terms from both sentences, respectively.
The different ways of representing the data in
combination with some sentence similarity mea-
sure will here be referred to as similarity features.
1. MSRI-TermCentroid:
In each sentence, each term is represented as
the sum of its sense vectors. This is similar
to having one context vector, as in classical
RI, but due to the sense-frequency filtering,
potentially ?noisy? senses are not included.
2. MSRI-TermMaxSense:
For each bipartite term pair in the two sen-
tences, their sense-pairs with maximum co-
sine similarity are used, one sense per term.
3. MSRI-TermInContext:
A 5 + 5 window around each (target) term
is used as context for selecting one sense of
the term. A window vector is calculated by
summing the index vectors of the other terms
in the window (i.e., except for the target term
itself). The sense of the target term which is
most similar to the window vector is used as
the representation of the term.
4. MSRI-TermHASenses:
Calculating similarity between two terms is
done by applying the Hungarian Algorithm
to all their bipartite sense pairs.
5. RI-TermAvg:
Classical Random Indexing ? each term is
represented as a single context vector.
6. RI-TermHA:
Similarity between two sentences is calcu-
lated by applying the Hungarian Algorithm to
the context vectors of each constituent term.
The parameters were selected based on a com-
bination of surveying previous work on RI (e.g.,
Sokolov (2012)), and by analysing how sense
counts evolved during training. For MSRI, we
used a similarity threshold of 0.2, a vector dimen-
sionality of 800, a non-zero count of 6, and a win-
dow size of 5 + 5. Sense vectors resulting from
less than 50 observations were removed. For clas-
sical RI, we used the same parameters as for MSRI
(except for a similarity threshold).
4 Experimental Setup
In order to explore the potential of the MSRI
model and the textual similarity measures pro-
posed here, experiments were carried out on data
from the Semantic Textual Similarity (STS) shared
task (Agirre et al, 2012; Agirre et al, 2013).
86
Given a pair of sentences, systems participating
in this task shall compute how semantically sim-
ilar the two sentences are, returning a similar-
ity score between zero (completely unrelated) and
five (completely semantically equivalent). Gold
standard scores are obtained by averaging multi-
ple scores obtained from human annotators. Sys-
tem performance is then evaluated using the Pear-
son product-moment correlation coefficient (?) be-
tween the system scores and the human scores.
The goal of the experiments reported here was
not to build a competitive STS system, but rather
to investigate whether MSRI can outperform clas-
sical Random Indexing on a concrete task such as
computing textual similarity, as well as to identify
which similarity measures and meaning represen-
tations appear to be most suitable for such a task.
The system is therefore quite rudimentary: a sim-
ple linear regression model is fitted on the training
data, using a single sentence similarity measure
as input and the similarity score as the dependent
variable. The implementations of RI and MSRI
are based on JavaSDM (Hassel, 2004).
As data for training random indexing models,
we used the CLEF 2004?2008 English corpus,
consisting of approximately 130M words of news-
paper articles (Peters et al, 2004). All text was
tokenized and lemmatized using the TreeTagger
for English (Schmid, 1994). Stopwords were re-
moved using a customized version of the stoplist
provided by the Lucene project (Apache, 2005).
Data for fitting and evaluating the linear re-
gression models came from the STS development
and test data, consisting of sentence pairs with
a gold standard similarity score. The STS 2012
development data stems from the Microsoft Re-
search Paraphrase corpus (MSRpar, 750 pairs),
the Microsoft Research Video Description cor-
pus (MSvid, 750 pairs), and statistical machine
translation output based on the Europarl corpus
(SMTeuroparl, 734 pairs). Test data for STS
2012 consists of more data from the same sources:
MSRpar (750 pairs), MSRvid (750 pairs) and
SMTeuroparl (459 pairs). In addition, different
test data comes from translation data in the news
domain (SMTnews, 399 pairs) and ontology map-
pings between OntoNotes and WordNet (OnWN,
750 pairs). When testing on the STS 2012 data, we
used the corresponding development data from the
same domain for training, except for OnWN where
we used all development data combined.
The development data for STS 2013 consisted
of all development and test data from STS 2012
combined, whereas test data comprised machine
translation output (SMT, 750 pairs), ontology
mappings both between WordNet and OntoNotes
(OnWN, 561 pairs) and between WordNet and
FrameNet (FNWN, 189 pairs), as well as news ar-
ticle headlines (HeadLine, 750 pairs). For sim-
plicity, all development data combined were used
for fitting the linear regression model, even though
careful matching of development and test data sets
may improve performance.
5 Results and Discussion
Table 2 shows Pearson correlation scores per fea-
ture on the STS 2012 test data using simple linear
regression. The most useful features for each data
set are marked in bold. For reference, the scores of
the best performing STS systems for each data set
are also shown, as well as baseline scores obtained
with a simple normalized token overlap measure.
There is large variation in correlation scores,
ranging from 0.77 down to 0.27. Part of this vari-
ation is due to the different nature of the data sets.
For example, sentence similarity in the SMT do-
main seems harder to predict than in the video
domain. Yet there is no single measure that ob-
tains the highest score on all data sets. There is
also no consistent difference in performance be-
tween the RI and MSRI measures, which seem
to yield about equal scores on average. The
MSRI-TermInContext measure has the low-
est score on average, suggesting that word sense
disambiguation in context is not beneficial in its
current implementation.
The corresponding results on the STS 2013 test
data are shown in Table 3. The same observations
as for the STS 2012 data set can be made: again
there was no consistent difference between the RI
and MSRI features, and no single best measure.
All in all, these results do not provide any ev-
idence that MSRI improves on standard RI for
this particular task (sentence semantic similarity).
Multi-sense distributional models have, however,
been found to outperform single-sense models on
other tasks. For example, Reisinger and Mooney
(2010) report that multi-sense models significantly
increase the correlation with human similarity
judgements. Other multi-prototype distributional
models may yield better results than their single-
prototype counterparts on the STS task.
87
Features: MSRpar MSRvid SMTeuroparl SMTnews OnWN Mean
Best systems 0.73 0.88 0.57 0.61 0.71 0.70
Baseline 0.43 0.30 0.45 0.39 0.59 0.43
RI-TermAvg 0.44 0.71 0.50 0.42 0.65 0.54
RI-TermHA 0.41 0.72 0.44 0.35 0.56 0.49
MSRI-TermCentroid 0.45 0.73 0.50 0.33 0.64 0.53
MSRI-TermHASenses 0.40 0.77 0.47 0.39 0.68 0.54
MSRI-TermInContext 0.33 0.55 0.36 0.27 0.42 0.38
MSRI-TermMaxSense 0.44 0.71 0.50 0.32 0.64 0.52
Table 2: Pearson correlation scores per feature on STS 2012 test data using simple linear regression
Feature Headlines SMT FNWN OnWN Mean
Best systems 0.78 0.40 0.58 0.84 0.65
Baseline 0.54 0.29 0.21 0.28 0.33
RI-TermAvg 0.60 0.37 0.21 0.52 0.42
RI-TermHA 0.65 0.36 0.27 0.52 0.45
MSRI-TermCentroid 0.60 0.35 0.37 0.45 0.44
MSRI-TermHASenses 0.63 0.35 0.33 0.54 0.46
MSRI-TermInContext 0.20 0.29 0.19 0.36 0.26
MSRI-TermMaxSense 0.58 0.35 0.31 0.45 0.42
Table 3: Pearson correlation scores per feature on STS 2013 test data using simple linear regression
Notably, the more advanced features used in our
experiment, such as MSRI-TermInContext,
gave very clearly inferior results when compared
to MSRI-TermHASenses. This suggests that
more research on MSRI is needed to understand
how both training and retrieval can be fully uti-
lized and optimized.
6 Conclusion and Future Work
The paper introduced a new method called Multi-
Sense Random Indexing (MSRI), which is based
on Random Indexing and performs on-the-fly
clustering, as an efficient way to construct multi-
prototype distributional models for word similar-
ity. A number of alternative measures for word
similarity were proposed, both context-dependent
and context-independent, including new measures
based on optimal alignment of word senses us-
ing the Hungarian algorithm. An extrinsic eval-
uation was carried out by applying the resulting
models to the Semantic Textual Similarity task.
Initial experimental results did not show a sys-
tematic difference between single-prototype and
multi-prototype models in this task.
There are many questions left for future work.
One of them is how the number of senses per word
evolves during training and how the distribution
of senses in the final model looks like. So far we
only know that on average the number of senses
keeps growing with more training material, cur-
rently resulting in about 5 senses per word at the
end of training (after removing senses with fre-
quency below the sense-frequency threshold). It
is worth noting that this depends heavily on the
similarity threshold for merging senses, as well as
on the weighting schema used.
In addition there are a number of model para-
meters that have so far only been manually tuned
on the development data, such as window size,
number of non-zeros, vector dimensionality, and
the sense frequency filtering threshold. A system-
atic exploration of the parameter space is clearly
desirable. Another thing that would be worth
looking into, is how to compose sentence vectors
and document vectors from the multi-sense vector
space in a proper way, focusing on how to pick
the right senses and how to weight these. It would
also be interesting to explore the possibilities for
combining the MSRI method with the Reflective
Random Indexing method by Cohen et al (2010)
in an attempt to model higher order co-occurrence
relations on sense level.
The fact that the induced dynamic word senses
do not necessarily correspond to human-created
senses makes evaluation in traditional word sense
disambiguation tasks difficult. However, correla-
88
tion to human word similarity judgement may pro-
vide a way of intrinsic evaluation of the models
(Reisinger and Mooney, 2010). The Usim bench
mark data look promising for evaluation of word
similarity in context (Erk et al, 2013).
It is also worth exploring ways to optimise the
algorithm, as this has not been the focus of our
work so far. This would also allow faster training
and experimentation on larger text corpora, such
as Wikipedia. In addition to the JavaSDM pack-
age (Hassel, 2004), Lucene (Apache, 2005) with
the Semantic Vectors package (Widdows and Fer-
raro, 2008) would be an alternative framework for
implementing the proposed MSRI algorithm.
Acknowledgements
This work was partly supported by the Re-
search Council of Norway through the EviCare
project (NFR project no. 193022) and by the
European Community?s Seventh Framework Pro-
gramme (FP7/20072013) under grant agreement
nr. 248307 (PRESEMT). Part of this work has
been briefly described in our contribution to the
STS shared task (Marsi et al, 2013).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics (*SEM), volume 2: Proceed-
ings of the Sixth International Workshop on Seman-
tic Evaluation, pages 385?393, Montreal, Canada,
June. Association for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, pages 32?43, Atlanta, Georgia, June. As-
sociation for Computational Linguistics.
Apache. 2005. Apache Lucene open source package.
http://lucene.apache.org/.
Trevor Cohen, Roger Schvaneveldt, and Dominic Wid-
dows. 2010. Reflective random indexing and indi-
rect inference: A scalable method for discovery of
implicit connections. Journal of Biomedical Infor-
matics, 43(2):240?256, April.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing distributional similarity in context. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1162?1172,
Cambridge, Massachusetts, October. Association for
Computational Linguistics.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2013. Measuring word meaning in context. Com-
putational Linguistics, 39(3):501?544.
Martin Hassel. 2004. JavaSDM package. http:
//www.nada.kth.se/?xmartin/java/.
School of Computer Science and Communication;
Royal Institute of Technology (KTH); Stockholm,
Sweden.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ?12, pages 873?
882, Jeju Island, Korea. Association for Computa-
tional Linguistics.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In Proceedings of the 22nd An-
nual Conference of the Cognitive Science Society,
page 1036, Philadelphia, Pennsylvania. Erlbaum.
Adam Kilgarriff. 2000. I don?t believe in word senses.
Computers and the Humanities, 31(2):91?113.
Harold W. Kuhn. 1955. The Hungarian method for
the assignment problem. Naval Research Logistics
Quarterly, 2:83?97.
Edwin Lughofer. 2008. Extensions of vector quantiza-
tion for incremental clustering. Pattern Recognition,
41(3):995?1011, March.
Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov,
Bjo?rn Gamba?ck, and Andre? Lynum. 2013. NTNU-
CORE: Combining strong features for semantic sim-
ilarity. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 66?73, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Roberto Navigli. 2009. Word Sense Disambiguation:
a survey. ACM Computing Surveys, 41(2):1?69.
Carol Peters, Paul Clough, Julio Gonzalo, Gareth J.F.
Jones, Michael Kluck, and Bernardo Magnini, ed-
itors. 2004. Multilingual Information Access
for Text, Speech and Images, 5th Workshop of the
Cross-Language Evaluation Forum, CLEF 2004,
volume 3491 of Lecture Notes in Computer Science.
Springer-Verlag, Bath, England.
89
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 109?117, Los Angeles, California, June.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
1st International Conference on New Methods in
Natural Language Processing, pages 44?49, Univer-
sity of Manchester Institute of Science and Technol-
ogy, Manchester, England, September.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123, March.
Artem Sokolov. 2012. LIMSI: learning semantic
similarity by selecting random word subsets. In
Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics (*SEM), volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation, pages 543?546, Montreal,
Canada, June. Association for Computational Lin-
guistics.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1012?1022, Edinburgh, Scotland,
July. Association for Computational Linguistics.
Dominic Widdows and Kathleen Ferraro. 2008. Se-
mantic vectors: a scalable open source package and
online technology management application. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08), pages 1183?
1190, Marrakech, Morocco.
90
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 116?124,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Care Episode Retrieval
Hans Moen
1
, Erwin Marsi
1
, Filip Ginter
2
,
Laura-Maria Murtola
3,4
, Tapio Salakoski
2
, Sanna Salanter
?
a
3,4
,
1
Dept. of Computer and Information Science,
Norwegian University of Science and Technology, Norway
2
Dept. of Information Technology, University of Turku, Finland
3
Dept. of Nursing Science, University of Turku, Finland
4
Turku University Hospital, Finland
{hans.moen,emarsi}@idi.ntnu.no, ginter@cs.utu.fi,
{lmemur,tapio.salakoski,sansala}@utu.fi
Abstract
The documentation of a care episode con-
sists of clinical notes concerning patient
care, concluded with a discharge sum-
mary. Care episodes are stored electron-
ically and used throughout the health care
sector by patients, administrators and pro-
fessionals from different areas, primarily
for clinical purposes, but also for sec-
ondary purposes such as decision support
and research. A common use case is, given
a ? possibly unfinished ? care episode,
to retrieve the most similar care episodes
among the records. This paper presents
several methods for information retrieval,
focusing on care episode retrieval, based
on textual similarity, where similarity is
measured through domain-specific mod-
elling of the distributional semantics of
words. Models include variants of random
indexing and a semantic neural network
model called word2vec. A novel method is
introduced that utilizes the ICD-10 codes
attached to care episodes to better induce
domain-specificity in the semantic model.
We report on an experimental evaluation
of care episode retrieval that circumvents
the lack of human judgements regarding
episode relevance by exploiting (1) ICD-
10 codes of care episodes and (2) seman-
tic similarity between their discharge sum-
maries. Results suggest that several of the
methods proposed outperform a state-of-
the art search engine (Lucene) on the re-
trieval task.
1 Introduction
Information retrieval (IR) aims at retrieving and
ranking documents relative to a textual query ex-
pressing the information need of a user (Manning
et al., 2008). IR has become a crucial technology
for many organisations that deal with vast amounts
of partly structured and unstructured (free text)
data stored in electronic format, including hospi-
tals and other health care providers. IR is an es-
sential part of the clinical practice; e.g., on-line IR
systems are associated with substantial improve-
ments in clinicians decision-making concerning
clinical problems (Westbrook et al., 2005).
The different stages of the clinical care of a pa-
tient are documented in clinical care notes, con-
sisting mainly of free text. A care episode consists
of a sequence of individual clinical care notes,
concluded by a discharge summary, as illustrated
in Figure 1. Care episodes are stored in elec-
tronic format in electronic health record (EHR)
systems. These systems are used throughout the
health care sector by patients, administrators and
professionals from different areas, primarily for
clinical purposes, but also for secondary purposes
such as decision support and research (H?ayrinen et
al., 2008). IR from EHR in general is therefore a
common and important task.
This paper focuses on the particular task of re-
trieving those care episodes that are most similar
to the sequence of clinical notes for a given pa-
tient, which we will call care episode retrieval.
In conventional IR, the query typically consists of
several keywords or a short phrase, while the re-
trievable units are typically documents. In con-
trast, in care episode retrieval, the query consist of
the clinical notes contained in a care episode. The
discharge summary is used separately for evalu-
116
A
BTime
Clinical notes Dischargesummary
Figure 1: Illustration of care episode retrieval. The
two care episodes (A and B) are composed of
a number of individual clinical notes and a sin-
gle discharge summary. Given an ongoing care
episode (minus the discharge summary), the task
is to retrieve other, similar care episodes.
ation purposes, and is assumed to be unavailable
for constructing a query at retrieval time. Retriev-
able units are thus complete care episodes without
summaries.
We envision a number of different use cases for
a care episode retrieval system. Firstly, it could fa-
cilitate clinicians in decision-making. For exam-
ple, given a patient that is being treated in a hos-
pital, an involved clinician may want to find previ-
ous patients that are similar in terms of their health
history, symptoms or received treatments. Supple-
mentary input from the clinician would enable the
system to give heightened weight to keywords of
particular interest within the care episodes, which
would further be emphasized in the semantic sim-
ilarity calculation during IR. It may help consider-
ably to see what similar patients have received in
terms of medication and further treatment, what
related issues such as bi-conditions or risks oc-
curred, how other clinicians have described cer-
tain aspects, what clinical practice guidelines have
been utilized, and so on. This relates to the un-
derlying principle in textual case-based reasoning
(Lenz et al., 1998). Secondly, it could help man-
agement to get almost real time information con-
cerning the overall situation on the unit for a spe-
cific follow-up period. Such a system could for ex-
ample support managerial decision-making with
statistical information concerning care trends on
the unit, adverse events or infections. Thirdly, it
could facilitate knowledge discovery and research.
For instance, it could enable researchers to map
or cluster similar care episodes to find common
symptoms or conditions. In sum, care episode re-
trieval is likely to improve care quality and consis-
tency in hospitals.
From the perspective of NLP, care episode re-
trieval ? and IR from EHRs in general ? is a
challenging task. It differs from general-purpose
web search in that the vocabulary, the informa-
tion needs and the queries of clinicians are highly
specialised (Yang et al., 2011). Clinical notes
contain highly domain-specific terminology (Rec-
tor, 1999; Friedman et al., 2002; Allvin et al.,
2010) and generic text processing resources are
therefore often suboptimal or inadequate (Shatkay,
2005). At the same time, development of dedi-
cated clinical NLP tools and resources is often dif-
ficult and costly. For example, popular data-driven
approaches to NLP are based on supervised learn-
ing, which requires substantial amounts of tailored
training data, typically built through manual anno-
tation by annotators who need both linguistic and
clinical knowledge. Additionally, variations in the
language and terminology used in sub-domains
within and across health care organisations greatly
limit the scope of applicability of such training
data (Rector, 1999).
Recent work has shown that distributional mod-
els of semantics, induced in an unsupervised man-
ner from large corpora of clinical and/or medical
text, are well suited as a resource-light approach
to capturing and representing domain-specific ter-
minology (Pedersen et al., 2007; Koopman et al.,
2012; Henriksson et al., 2014). This raises the
question to what extent distributional models of
semantics can alleviate the aforementioned prob-
lems of NLP in the clinical domain. The work
reported here investigates to what extent distribu-
tional models of semantics, built from a corpus of
clinical text in an fully unsupervised manner, can
be used for care episode retrieval. Models include
several variants of random indexing and a seman-
tic neural network model called word2vec, which
will be described in more detail in Section 4.
It has been argued that clinical NLP should ex-
ploit existing knowledge resources such as knowl-
edge bases about medications, treatments, dis-
eases, symptoms and care plans, despite these not
having been explicitly built for doing clinical NLP
(Friedman et al., 2013). Along these lines, a novel
method is proposed here that utilizes the ICD-10
codes ? diagnostic labels attached to care episodes
by clinicians ? to better induce domain-specificity
in the semantic model. Experimental results sug-
gest that this method outperforms a state-of-the art
search engine (Lucene) on the task of care episode
117
retrieval.
Apart from issues related to clinical terminol-
ogy, another problem in care episode retrieval is
the lack of benchmark data, such as the relevance
scores produced by human judges commonly used
for evaluation of IR systems. Although collec-
tions of care episodes may be available, producing
gold standard similarity scores required for evalu-
ation is costly. Another contribution of this paper
is the proposal of evaluation procedures that cir-
cumvent the lack of human judgements regarding
episode similarity. This is accomplished by ex-
ploiting either (1) ICD-10 codes of care episodes
or (2) semantic similarity between their discharge
summaries. Despite our focus on the specific task
of care episode retrieval, we hypothesize that the
methods and models proposed here have the po-
tential to increase performance of IR on clinical
text in general.
2 Data
The data set used in this study consists of the elec-
tronic health records from patients with any type
of heart related problem that were admitted to one
particular university hospital in Finland between
the years 2005-2009. Of these, only the clini-
cal notes written by physician are used. A sup-
porting statement for the research was obtained
from the Ethics Committee of the Hospital District
(17.2.2009 ?67) and permission to conduct the re-
search was obtained from the Medical Director of
the Hospital District (2/2009). The total set consist
of 66884 care episodes, which amounts to 398040
notes and 64 million words in total. This full set
was used for training of the semantic models. To
make the experimentation more convenient, we
chose to use a subset for evaluation. This com-
prises 26530 care episodes, amounting to 155562
notes and 25.7 million words in total.
Notes are mostly unstructured, consisting of
free text in Finnish. Some meta-data ? such as
names of the authors, dates, wards, and so on ? is
present, but is not used for retrieval.
Care episodes have been manually labeled ac-
cording to the 10th revision of the International
Classification of Diseases (ICD-10) (World Health
Organization and others, 2013), a standardised
tool of diagnostic codes for classifying diseases.
Codes are normally applied at the end of the pa-
tient?s stay, or even after the patient has been dis-
charged from the hospital. Care episodes have
one primary ICD-10 code attached and optionally
a number of additionally relevant codes. In this
study, only the primary one is used, because ex-
traction of the secondary codes is non-trivial.
ICD-10 codes have an internal structure that re-
flects the classification system ranging from broad
categories down to fine-grained subjects. For ex-
ample, the first character (J) of the code J21.1
signals that it belongs to the broad category Dis-
eases of the respiratory system. The next two
digits (21) classify the subject as belonging to
the subcategory Acute bronchiolitis. Finally, the
last digit after the dot (1) means that it belongs
to the sub-subclass Acute bronchiolitis due to hu-
man metapneumovirus. There are 356 unique ?pri-
mary? ICD-10 codes in the evaluation data set.
3 Task
The task addressed in this study is retrieval of care
episodes that are similar to each other. In con-
trast to the normal IR setting, where the search
query is derived from a text stating the user?s in-
formation need, here the query is based on an-
other care episode, which we refer to as the query
episode. As the query episode may document on-
going treatment, and thus lack a discharge sum-
mary and ICD-10 code, neither of these informa-
tion sources can be relied upon for constructing
the query. The task is therefore to retrieve the most
similar care episodes using only the information
contained in the free text of the clinical notes in
the query episode.
Evaluation of retrieval results generally re-
quires an assessment of their relevancy to the
query. Since similarity judgements by humans
are currently lacking, and obtaining these is time-
consuming and costly, we explored alternative
ways of evaluating performance on the task. The
first alternative is to assume that care episodes are
similar if they have the same ICD-10 code. That is,
a retrieved care episode is considered correct if its
ICD-10 code is identical to the code of the query
episode. It should be noted that ICD-10 codes are
not used in the query in any of the experiments.
Closer inspection shows that the free text con-
tent in care episodes with the same ICD-10 code
is indeed quite similar in many cases, but not al-
ways. Considering all of them equally similar
amounts to an arguably coarse approximation of
relevance. The second alternative tries to remedy
this issue by measuring the similarity between dis-
118
charge summaries. That is, if the discharge sum-
mary of a retrieved episode is semantically simi-
lar to the discharge summary of the query episode,
the retrieved episode is assumed to be correct.
In practice, textual similarity between discharge
summaries, and therefore the relevance score, is
continuous rather than binary. It is measured using
the same models of distributional semantics used
for retrieval, which will be described in Section 4.
It should be stressed that the discharge summaries
are not taken into consideration during retrieval in
any of the experiments and are only used for eval-
uation.
4 Method
4.1 Semantic models
A crucial part in retrieving similar care episodes
is having a good similarity measure. Here similar-
ity between care episodes is measured as the sim-
ilarity between the words they contain (see Sec-
tion 4.2). Semantic similarity between words is in
turn measured through the use of word space mod-
els (WSM), without performing an explicit query
expansion step. Several variants of these models
were tested, utilizing different techniques and pa-
rameters for building them. The models trained
and tested in this paper are: (1) classic random
indexing with a sliding window using term in-
dex vectors and term context vectors (RI-Word);
(2) random indexing with index vectors for doc-
uments (RI-Doc); (3) random indexing with in-
dex vectors for ICD-10 codes (RI-ICD); (4) a ver-
sion of random indexing where only the term in-
dex vectors are used (RI-Index); and (5) a seman-
tic neural network model, using word2vec to build
word context vectors (Word2vec).
RI-Word
Random Indexing (RI) (Kanerva et al., 2000) is
a method for building a (pre) compressed WSM
with a fixed dimensionality, done in an incremen-
tal fashion. RI consist of the following two steps:
First, instead of allocating one dimension in the
multidimensional vector space to a single word,
each word is assigned an ?index vector? as its
unique signature in the vector space. Index vectors
are generated vectors consisting of mostly zeros
together with a randomly distributed set of several
1?s and -1?s, uniquely distributed for each unique
word; The second step is to induce ?context vec-
tors? for each word. A context vector represents
the contextual meaning of a word in the WSM.
This is done using a sliding window of a fixed size
to traverse a training corpus, inducing context vec-
tors for the center/target word of the sliding win-
dow by summing the index vectors of the neigh-
bouring words in the window.
As the dimensionality of the index vectors is
fixed, the dimensionality of the vector space will
not grow beyond the size W ?Dim, where W is
the number of unique words in the vocabulary, and
Dim being the pre-selected dimensionality to use
for the index vectors. As a result, RI models are
significantly smaller than plain word space mod-
els, making them a lot less computationally expen-
sive. Additionally, the method is fully incremental
(additional training data can be added at any given
time without having to retrain the existing model),
easy to parallelize, and scalable, meaning that it is
fast and can be trained on large amounts of text in
an on-line fashion.
RI-Doc
Contrary to sliding window approach used in RI-
Word, a RI model built with document index vec-
tors first assigns unique index vectors to every
document in the training corpus. In the training
phase, each word in a document get the respective
document vector added to its context vector. The
resulting WSM is thus a compressed version of a
term-by-document matrix.
RI-ICD
Based on the principle of RI with document index
vectors, we here explore a novel way of construct-
ing a WSM by exploiting the ICD-10 code classi-
fication done by clinicians. Instead of using doc-
ument index vectors, we here use ICD-code index
vectors. First, a unique index vector is assigned to
each chapter and sub-chapter in the ICD-10 taxon-
omy. This means assigning a unique index vector
to each ?node? in the ICD-10 taxonomy, as illus-
trated in Figure 2. For each clinical note in the
training corpus, the index vector of the their pri-
mary ICD-10 code is added to all words within it.
In addition, all the index vectors for the ICD-codes
higher in the taxonomy are added, each weighted
according to their position in the hierarchy. A
weight of 1 is given to the full code, while the
weight is halved for each step upwards in the hi-
erarchy. The motivation for the latter is to capture
a certain degree of similarity between codes that
share an initial path in the taxonomy. As a result,
119
J	 ?
0.125	 ? 0.25	 ? 0.5	 ? 1	 ? Weight	 ?
2	 ?
1	 ?
0	 ?
1	 ?
1	 ?
0	 ?
J21.1	 ?
0	 ?
Figure 2: Weighting applied to ICD-code index
vectors when training WSMs based on ICD-10
codes (RI-ICD).
this similarity is encoded in the resulting WSM.
As a example: for a clinical note labelled with the
code J21.1, we add the following index vectors
to the context vectors of all its constituting words:
iv(J)? 0.125, iv(J2)? 0.25, iv(J21)? 0.5 and
iv(J21.1) ? 1.0. The underlying hypothesis for
building a WSM in this way is that it may cap-
ture relations between words in a way that bet-
ter reflects the clinical domain, compared to the
other domain-independent methods for construct-
ing a WSM.
RI-Index
As an alternative to using word?s (semantic) con-
text vectors, we simply only use their index vec-
tors as their ?contextual meaning?. When con-
structing document vectors directly from word in-
dex vectors (see Section 4.2), the resulting docu-
ment vectors represent a compressed version of a
document-by-term matrix.
Word2vec
Recently, a novel method for inducing WSMs was
introduced by Mikolov et al. (2013a), stemming
from the research in deep learning and neural net-
work language models. While the overall objec-
tive of learning a continuous vector space repre-
sentation for each word based on its textual con-
text remains, the underlying algorithms are sub-
stantially different from traditional methods such
as Latent Semantic Analysis and RI. Considering,
in turn, every word in the training data as a target
word, the method induces the representations by
training a simplified neural network to predict the
nearby context words of each target word (skip-
gram architecture), or alternatively the target word
based on all words in its immediate context (BoW
architecture). The vector space representation is
subsequently extracted from the learned weights
within the neural network. One of the main prac-
tical advantages of the word2vec method lies in
its scalability, allowing quick training on large
amounts of text, setting it apart from the majority
of other methods of distributional semantics. Ad-
ditionally, the word2vec method has been shown
to produce representations that surpass in quality
traditional methods such as Latent Semantic Anal-
ysis, especially on tasks measuring the preserva-
tion of important linguistic regularities (Mikolov
et al., 2013b).
4.2 Computing care episode similarity
After having computed a WSM, the next step is
to build episode vectors to use for the actual re-
trieval task. This is done by first normalizing the
word vectors and multiplying them with a word?s
TF*IDF weight. An episode vector is then ob-
tained by summing the word vectors of all its
words and dividing the result by the total num-
ber of words in the episode. Similarity between
episodes is determined by computing the cosine
similarity between their vectors.
4.3 Baselines
Two baselines were used in this study. The first
one is random retrieval of care episodes, which
can be expected to give very low scores and serves
merely as a sanity check. The second one is
Apache Lucene (Cutting, 1999), a state-of-the-art
search engine based on look-up of similar docu-
ments through a reverse index and relevance rank-
ing based on a TF*IDF-weighted vector space
model. Care episodes were indexed using Lucene.
Similar to the other models/methods, all of the free
text in the query episode, excluding the discharge
summary, served as the query string provided to
Lucene. Being a state-of-the-art IR system, the
scores achieved by Lucene in these experiments
should indicate the difficulty of the task.
5 Experiments
In these experiments we strove to have a setup
that was as comparable as possible for all models
and systems, both in terms of text pre-processing
and in terms of the target model dimensionality
when inducing the vector space models. The clin-
120
ical notes are split into sentences, tokenized, and
lemmatized using a Constraint-Grammar based
morphological analyzer and tagger extended with
clinical vocabulary (Karlsson, 1995). After stop
words were removed
1
, the total training corpus
contained 39 million words (minus the query
episodes), while the evaluation subset contained
18.5 million words. The vocabulary consisted of
0.6 million unique terms. Twenty care episodes
were randomly selected to serve as the query
episodes during testing, with the requirement that
each had different ICD-10 codes and consisted of a
minimum of six clinical notes. The average num-
ber of words per query episode is 830.
RI-based and word2vec models have a prede-
fined dimensionality of 800. For RI-based mod-
els, 4 non-zeros were used in the index vectors.
For the RI-Word model, a narrow context win-
dow was employed (5 left + 5 right), weighting
index vectors according to their distance to the tar-
get word (weight
i
= 2
1?dist
it
). In addition, the
index vectors were shifted once left or right de-
pending on what side of the target word they were
located, similar to direction vectors as described
in (Sahlgren et al., 2008) These parameters for RI
were chosen based on previous work on semantic
textual similarity (Moen et al., 2013). Also a much
larger window of 20+20 was tested, but without
noteworthy improvements. The word2vec model
is trained with the BoW architecture and otherwise
default parameters. In addition to Apache Lucene
(version 4.2.0)
2
, the word2vec tool
3
was used to
train the word2vec model, and the RI-based meth-
ods utilized the JavaSDM package
4
. Scores were
calculated using the trec eval tool
5
.
5.1 Experiment 1: ICD-10 code overlap
In this experiment retrieved episodes with a pri-
mary ICD-10 code identical to that of the query
episode were considered to be correct. The num-
ber of correct episodes varies between 49 and
1654. The total is 7721, and the average is
386. The high total is mainly due to three query
episodes with ICD-10 codes that occur very fre-
quently in the episode collection (896, 1590, and
1
http://www.nettiapina.fi/
finnish-stopword-list/
2
http://archive.apache.org/dist/
lucene/java/
3
https://code.google.com/p/word2vec/
4
http://www.nada.kth.se/
?
xmartin/java/
5
http://trec.nist.gov/trec_eval/
IR model MAP P@10
Lucene 0.1379 0.3000
RI-Word 0.0911 0.2650
RI-Doc 0.1015 0.3300
RI-ICD 0.3261 0.5150
RI-Index 0.1187 0.3200
Word2vec 0.1768 0.3350
Random 0.0154 0.0200
Table 1: Mean average precision and precision at
10 for retrieval of care episodes with the same pri-
mary ICD-10 code as the query episode
1654 times). When conducting the experiment all
care episodes were retrieved for each of the 20
query episodes.
Performance was measured in terms of mean
average precision (MAP) and precision among
the top-10 results (P@10), averaged over all 20
queries, as shown in in Table 1. The best MAP
score is achieved by RI-ICD, almost twice that of
word2vec, which achieved the second best MAP
score, whereas RI-Word performed worst of all.
All models score well above the random baseline,
whereas RI-ICD outperforms Lucene by a large
margin. P@10 scores follow the same ranking.
The latter scores are more representative for most
use cases where users will only inspect the top-n
retrieval results.
5.2 Experiment 2: Discharge summary
overlap
In this experiment retrieved episodes with a dis-
charge summary similar to that of the query
episode were considered to be correct. Using the
discharge summaries of the query episodes, the
top 100 care episodes with the most similar dis-
charge summary were selected as the most simi-
lar care episodes (disregarding the query episode).
This was repeated for each of the methods ? i.e.
the five different semantic models and Lucene ?
resulting in six different tests. The top 100 was
used rather than a threshold on the similarity score,
because otherwise six different thresholds would
have to be chosen. This procedure thus resulted in
six different test collections, each consisting of 20
query episodes with their corresponding 100 most
similar collection episodes.
Subsequently a 6-by-6 experimental design was
followed where each retrieval method was tested
against each test set construction method. At re-
trieval time, for each query episode, the system re-
trieves and ranks 1000 care episodes. It can be ex-
pected that when identical methods are used for re-
121
trieval and test set construction, the resulting bias
gives rise to relatively high scores. In contrast,
averaging over the scores for all six construction
methods is assumed to be a less biased indicator
of performance.
Table 2 shows the number of correctly retrieved
episodes by the different models, with the maxi-
mum being 2000 (20 queries times 100 most sim-
ilar episodes). This gives an indication of the re-
call among a 1000 retrieved episodes per query,
but without caring about precision or ranking. In
general, the numbers are relatively good when the
same model is used for both retrieval and construc-
tion of the test set (cf. values on the diagonal), al-
though in a couple of cases (e.g. with word2vec)
results are better with different models. The RI-
ICD model performs best when used for both re-
trieval and test construction. Looking at the av-
erages, which presumably are less biased indica-
tors, RI-ICD and word2vec seem to have compa-
rable performance, with both of them outperform-
ing Lucene. Other models are less successful, al-
though still much better than the random baseline.
The MAP scores in Table 3 show similar re-
sults, although here RI-ICD yields the best aver-
age score. Both models RI-ICD and word2vec
outperform Lucene. Again the RI-ICD model per-
forms exceptionally well when used for both re-
trieval and test construction.
Finally Table 4 presents precision for top-10 re-
trieved care episodes. Here RI-Doc yields the best
average scores, while RI-ICD and word2vec both
perform slightly worse.
6 Discussion
The goal of the experiments was primarily to
determine which distributional semantic models
work best for care episode retrieval. The exper-
imental results show that several models outper-
form Lucene at the care episode retrieval task.
This suggests that models of higher order seman-
tics contribute positively to calculating document
similarities in the clinical domain, compared with
straight forward boolean word matching (cf. RI-
Index and Lucene).
The relatively good performance of the RI-ICD
model, particularly in Experiment 1, suggests that
exploiting structured or encoded information in
building semantic models for clinical NLP is a
promising direction that calls for further investi-
gation. This approach concurs with the arguments
in favor of reuse of existing information sources
in Friedman et al. (2013). On the one hand, it
may not be surprising that the RI-ICD model is
performing well on Experiment 1, given how it in-
duces semantic relations between words occurring
in episodes with the same ICD-10 code. On the
other hand, being able to accurately retrieve care
episodes with similar ICD-10 codes evidently has
practical value from a clinical perspective.
The different ranking of models in experiments
1 versus 2 confirms that there is a difference be-
tween the two indicators of episode similarity,
i.e. similarity in terms of their ICD-10 codes
versus similarity with regard to their discharge
summaries. In our data a single care episode
can potentially span across several hospital wards.
A better correlation between the similarity mea-
sures is to be expected when narrowing the def-
inition of a care episode to only a single ward.
Also, taking into consideration all ICD-10 codes
for care episodes ? not only the primary one ?
could potentially improve discrimination among
care episodes. This could be useful in two ways:
(1) to create more precise test sets of the type used
in Experiment 1; (2) to extend RI-ICD models
with index vectors also for the secondary ICD-10
codes.
Input to the models for training was limited to
the free text in the clinical notes, with the ex-
ception of the use of ICD-10 codes in the RI-
ICD model. Other sources of information could,
and probably should, be utilized in a practical
care episode retrieval system applied in a hospi-
tal, such as the structured and coded information
commonly found in EHR systems. Another po-
tential information source is the internal structure
of the care episodes, as episodes containing sim-
ilar notes in the same sequential order are intu-
itively more likely to be similar. We tried comput-
ing exhaustive pairwise similarities between the
individual notes from two episodes and then tak-
ing the average of these as a similarity measure
for the episodes. However, this did not improve
performance on any measure. An alternative ap-
proach may be to apply sequence alignment algo-
rithms, as commonly used in bioinformatics (Gus-
field, 1997), in order to detect if both episodes
contain similar notes in the same temporal order.
We leave this to future work.
122
IR model \ Test set Lucene RI-Word RI-Doc RI-ICD RI-Index Word2vec Average Rank
Lucene 889 700 670 687 484 920 725 2
RI-Word 643 800 586 600 384 849 644 5
RI-Doc 665 630 859 697 436 795 680 4
RI-ICD 635 459 659 1191 490 813 707 3
RI-Index 690 491 607 654 576 758 629 6
Word2vec 789 703 702 870 516 1113 782 1
Random 74 83 86 67 84 85 79 7
Table 2: Number of correctly retrieved episodes (max 2000) for different IR models (rows) when using
different models for measuring discharge summary similarity (columns)
IR model \ Test set Lucene RI-Word RI-Doc RI-ICD RI-Index Word2vec Average Rank
Lucene 0.0856 0.0357 0.0405 0.0578 0.0269 0.0833 0.0550 3
RI-Word 0.0392 0.0492 0.0312 0.0412 0.0151 0.0735 0.0416 6
RI-Doc 0.0493 0.0302 0.0677 0.0610 0.0220 0.0698 0.0500 4
RI-ICD 0.0497 0.0202 0.0416 0.1704 0.0261 0.0712 0.0632 1
RI-Index 0.0655 0.0230 0.0401 0.0504 0.0399 0.0652 0.0473 5
Word2vec 0.0667 0.0357 0.0404 0.0818 0.0293 0.1193 0.0622 2
Random 0.0003 0.0003 0.0005 0.0002 0.0003 0.0004 0.0003 7
Table 3: Mean average precision for different IR models (rows) when using different models for measur-
ing discharge summary similarity (columns)
IR model \ Test set Lucene RI-Word RI-Doc RI-ICD RI-Index Word2vec Average Rank
Lucene 0.2450 0.1350 0.1200 0.1650 0.0950 0.1900 0.1583 5
RI-Word 0.1350 0.1500 0.1000 0.1350 0.0600 0.2100 0.1316 6
RI-Doc 0.2000 0.1250 0.2050 0.2200 0.0900 0.2400 0.1800 1
RI-ICD 0.1700 0.0650 0.1350 0.3400 0.0950 0.2050 0.1683 2
RI-Index 0.2000 0.1250 0.1550 0.1250 0.1700 0.2050 0.1633 3
Word2vec 0.1800 0.1200 0.1150 0.2100 0.0850 0.2650 0.1625 4
Random 0.0000 0.0000 0.0050 0.0000 0.0000 0.0000 0.0008 7
Table 4: Precision at top-10 retrieved episodes for different IR models (rows) when using different
models for measuring discharge summary similarity (columns)
7 Conclusion and future work
In this paper we proposed the task of care episode
retrieval as a way of evaluating several distribu-
tional semantic models in their performance at IR.
As manually constructing a proper test set of clas-
sified care episodes is costly, we experimented
with building test sets by exploiting either ICD-10
code overlap or semantic similarity of discharge
summaries. A novel method for generating se-
mantic models utilizing the ICD-10 codes of care
episodes in the training corpus was presented (RI-
ICD). The models, as well as the Lucene search
engine, were applied to the care episode retrieval
task and their performance was evaluated against
the test sets using different evaluation measures.
The results suggest that the RI-ICD model is bet-
ter suited to IR tasks in the clinical domain com-
pared with models trained on local distributions of
words, or those relying on direct word matching.
The word2vec model performed relatively well
and outperformed Lucene in both experiments.
In the results reported here, the internal se-
quence of clinical notes is ignored. Future work
should focus on exploring the temporal (sub-) se-
quence similarities between care episode pairs for
doing care episode retrieval. Further work should
also focus on expanding on the RI-ICD method
by exploiting other types of structured and/or en-
coded information related to clinical notes for
training semantic models tailored for NLP in the
clinical domain.
Acknowledgments
This study was partly supported by the Research
Council of Norway through the EviCare project
(NFR project no. 193022), the Turku University
Hospital (EVO 2014), and the Academy of Fin-
land (project no. 140323). The study is a part
of the research projects of the Ikitik consortium
(http://www.ikitik.fi). We would like
to thank Juho Heimonen for assisting us in pre-
processing the data and the reviewers for their
helpful comments.
123
References
Helen Allvin, Elin Carlsson, Hercules Dalianis, Ri-
itta Danielsson-Ojala, Vidas Daudaravi?cius, Mar-
tin Hassel, Dimitrios Kokkinakis, Helj?a Lundgren-
Laine, Gunnar Nilsson, ?ystein Nytr?, et al. 2010.
Characteristics and analysis of finnish and swedish
clinical intensive care nursing narratives. In Pro-
ceedings of the NAACL HLT 2010 Second Louhi
Workshop on Text and Data Mining of Health Docu-
ments, pages 53?60. Association for Computational
Linguistics.
Doug Cutting. 1999. Apache Lucene open source
package.
Carol Friedman, Pauline Kra, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of zellig harris. Journal of
biomedical informatics, 35(4):222?235.
Carol Friedman, Thomas C Rindflesch, and Mil-
ton Corn. 2013. Natural language process-
ing: State of the art and prospects for significant
progress, a workshop sponsored by the national li-
brary of medicine. Journal of biomedical informat-
ics, 46(5):765?773.
Dan Gusfield. 1997. Algorithms on strings, trees and
sequences: computer science and computational bi-
ology. Cambridge University Press.
Kristiina H?ayrinen, Kaija Saranto, and Pirkko
Nyk?anen. 2008. Definition, structure, content, use
and impacts of electronic health records: a review
of the research literature. International journal of
medical informatics, 77(5):291?304.
Aron Henriksson, Hans Moen, Maria Skeppstedt, Vi-
das Daudaravi, Martin Duneld, et al. 2014. Syn-
onym extraction and abbreviation expansion with
ensembles of semantic spaces. Journal of biomed-
ical semantics, 5(1):6.
Pentti Kanerva, Jan Kristofersson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In Proceedings of 22nd Annual
Conference of the Cognitive Science Society, page
1036.
Fred Karlsson. 1995. Constraint grammar: a
language-independent system for parsing unre-
stricted text. Mouton de Gruyter, Berlin and New
York.
Bevan Koopman, Guido Zuccon, Peter Bruza, Lauri-
anne Sitbon, and Michael Lawley. 2012. An evalu-
ation of corpus-driven measures of medical concept
similarity for information retrieval. In Proceedings
of the 21st ACM international conference on Infor-
mation and knowledge management, pages 2439?
2442. ACM.
Mario Lenz, Andr?e H?ubner, and Mirjam Kunze. 1998.
Textual cbr. In Case-based reasoning technology,
pages 115?137. Springer.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751. Associa-
tion for Computational Linguistics, June.
Hans Moen, Erwin Marsi, and Bj?orn Gamb?ack. 2013.
Towards dynamic word sense discrimination with
random indexing. ACL 2013, page 83.
Ted Pedersen, Serguei VS Pakhomov, Siddharth Pat-
wardhan, and Christopher G Chute. 2007. Mea-
sures of semantic similarity and relatedness in the
biomedical domain. Journal of biomedical infor-
matics, 40(3):288?299.
Alan L Rector. 1999. Clinical terminology: why is
it so hard? Methods of information in medicine,
38(4/5):239?252.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode order in
word space. In Proceedings of the Annual Meeting
of the Cognitive Science Society.
Hagit Shatkay. 2005. Hairpins in bookstacks: infor-
mation retrieval from biomedical text. Briefings in
Bioinformatics, 6(3):222?238.
Johanna I Westbrook, Enrico W Coiera, and A So-
phie Gosling. 2005. Do online information retrieval
systems help experienced clinicians answer clinical
questions? Journal of the American Medical Infor-
matics Association, 12(3):315?321.
World Health Organization and others. 2013. Interna-
tional classification of diseases (icd).
Lei Yang, Qiaozhu Mei, Kai Zheng, and David A
Hanauer. 2011. Query log analysis of an electronic
health record search engine. In AMIA Annual Sym-
posium Proceedings, volume 2011, page 915. Amer-
ican Medical Informatics Association.
124
