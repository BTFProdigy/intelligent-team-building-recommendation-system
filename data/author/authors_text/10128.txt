Proceedings of the 12th Conference of the European Chapter of the ACL, pages 799?807,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
MINT: A Method for Effective and Scalable Mining of  
Named Entity Transliterations from Large Comparable Corpora 
Raghavendra Udupa         K Saravanan         A Kumaran        Jagadeesh Jagarlamudi*          
Microsoft Research India 
Bangalore 560080 INDIA 
 [raghavu,v-sarak,kumarana,jags}@microsoft.com 
 
Abstract 
In this paper, we address the problem of min-
ing transliterations of Named Entities (NEs) 
from large comparable corpora. We leverage 
the empirical fact that multilingual news ar-
ticles with similar news content are rich in 
Named Entity Transliteration Equivalents 
(NETEs). Our mining algorithm, MINT, uses 
a cross-language document similarity model to 
align multilingual news articles and then 
mines NETEs from the aligned articles using a 
transliteration similarity model. We show that 
our approach is highly effective on 6 different 
comparable corpora between English and 4 
languages from 3 different language families. 
Furthermore, it performs substantially better 
than a state-of-the-art competitor.   
1 Introduction 
Named Entities (NEs) play a critical role in many 
Natural Language Processing and Information 
Retrieval (IR) tasks.  In Cross-Language Infor-
mation Retrieval (CLIR) systems, they play an 
even more important role as the accuracy of their 
transliterations is shown to correlate highly with 
the performance of the CLIR systems (Mandl 
and Womser-Hacker, 2005, Xu and Weischedel, 
2005).  Traditional methods for transliterations 
have not proven to be very effective in CLIR. 
Machine Transliteration systems (AbdulJaleel 
and Larkey, 2003; Al-Onaizan and Knight, 2002; 
Virga and Khudanpur, 2003) usually produce 
incorrect transliterations and translation lexcions 
such as hand-crafted or statistical dictionaries are 
too static to have good coverage of NEs1 occur-
ring in the current news events. Hence, there is a 
critical need for creating and continually updat-
                                                 
* Currently with University of Utah. 
1 New NEs are introduced to the vocabulary of a lan-
guage every day. On an average, 260 and 452 new 
NEs appeared daily in the XIE and AFE segments of 
the LDC English Gigaword corpora respectively. 
ing multilingual Named Entity transliteration 
lexicons. 
The ubiquitous availability of comparable 
news corpora in multiple languages suggests a 
promising alternative to Machine Transliteration, 
namely, the mining of Named Entity Translitera-
tion Equivalents (NETEs) from such corpora. 
News stories are typically rich in NEs and there-
fore, comparable news corpora can be expected 
to contain NETEs (Klementiev and Roth, 2006; 
Tao et al, 2006). The large quantity and the per-
petual availability of news corpora in many of 
the world?s languages, make mining of NETEs a 
viable alternative to traditional approaches. It is 
this opportunity that we address in our work. 
    In this paper, we detail an effective and scala-
ble mining method, called MINT (MIning 
Named-entity Transliteration equivalents), for 
mining of NETEs from large comparable corpo-
ra. MINT addresses several challenges in mining 
NETEs from large comparable corpora: exhaus-
tiveness (in mining sparse NETEs), computa-
tional efficiency (in scaling on corpora size), 
language independence (in being applicable to 
many language pairs) and linguistic frugality (in 
requiring minimal external linguistic resources).   
Our contributions are as follows: 
? We give empirical evidence for the hypo-
thesis that news articles in different languages 
with reasonably similar content are rich sources 
of NETEs (Udupa, et al, 2008).  
? We demonstrate that the above insight can 
be translated into an effective approach for min-
ing NETEs from large comparable corpora even 
when similar articles are not known a priori. 
? We demonstrate MINT?s effectiveness on 
4 language pairs involving 5 languages (English, 
Hindi, Kannada, Russian, and Tamil) from 3 dif-
ferent language families, and its scalability on 
corpora of vastly different sizes (2,000 to 
200,000 articles).  
? We show that MINT?s performance is sig-
nificantly better than a state of the art method 
(Klementiev and Roth, 2006). 
 
799
We discuss the motivation behind our ap-
proach in Section 2 and present the details in 
Section 3.  In Section 4, we describe the evalua-
tion process and in Section 5, we present the re-
sults and analysis.  We discuss related work in 
Section 6.  
2 Motivation 
MINT is based on the hypothesis that news ar-
ticles in different languages with similar content 
contain highly overlapping set of NEs. News 
articles are typically rich in NEs as news is about 
events involving people, locations, organizations, 
etc2. It is reasonable to expect that multilingual 
news articles reporting the same news event 
mention the same NEs in the respective languag-
es. For instance, consider the English and Hindi 
news reports from the New York Times and the 
BBC on the second oath taking of President Ba-
rack Obama (Figure 1). The articles are not pa-
rallel but discuss the same event. Naturally, they 
mention the same NEs (such as Barack Obama, 
John Roberts, White House) in the respective 
languages, and hence, are rich sources of NETEs.    
Our empirical investigation of comparable 
corpora confirmed the above insight. A study of 
                                                 
2 News articles from the BBC corpus had, on an 
average, 12.9 NEs and new articles from the The 
New Indian Express, about 11.8 NEs. 
 
200 pairs of similar news articles published by 
The New Indian Express in 2007 in English and 
Tamil showed that 87% of the single word NEs 
in the English articles had at least one translitera-
tion equivalent in the conjugate Tamil articles.  
The MINT method leverages this empirically 
backed insight to mine NETEs from such compa-
rable corpora.   
However, there are several challenges to the 
mining process: firstly, vast majority of the NEs 
in comparable corpora are very sparse; our anal-
ysis showed that 80% of the NEs in The New 
Indian Express news corpora appear less than 5 
times in the entire corpora.  Hence, any mining 
method that depends mainly on repeated occur-
rences of the NEs in the corpora is likely to miss 
vast majority of the NETEs.  Secondly, the min-
ing method must restrict the candidate NETEs 
that need to be examined for match to a reasona-
bly small number, not only to minimize false 
positives but also to be computationally efficient.  
Thirdly, the use of linguistic tools and resources 
must be kept to a minimum as resources are 
available only in a handful of languages.  Finally, 
it is important to use as little language-specific 
knowledge as possible in order to make the min-
ing method applicable across a vast majority of 
languages of the world.  The MINT method pro-
posed in this paper addresses all the above is-
sues. 
 
800
3 The MINT Mining Method 
MINT has two stages. In the first stage, for 
every document in the source language side, the 
set of documents in the target language side with 
similar news content are found using a cross-
language document similarity model. In the 
second stage, the NEs in the source language 
side are extracted using a Named Entity Recog-
nizer (NER) and, subsequently, for each NE in a 
source language document, its transliterations are 
mined from the corresponding target language 
documents. We present the details of the two 
stages of MINT in the remainder of this section. 
3.1 Finding Similar Document Pairs  
The first stage of MINT method (Figure 2) works 
on the documents from the comparable corpora 
(CS, CT) in languages S and T and produces a col-
lection AS,T  of similar article pairs (DS, DT).  Each 
article pair (DS, DT) in AS,T consists of an article 
(DS) in language S and an article (DT) in language 
T, that have similar content. The cross-language 
similarity between DS and DT, as measured by the 
cross-language similarity model MD, is at least ? 
> 0. 
 
Cross-language Document Similarity Model: 
The cross-language document similarity model 
measures the degree of similarity between a pair 
of documents in source and target languages.  
We use the negative KL-divergence between 
source and target document probability distribu-
tions as the similarity measure. 
  Given two documents DS, DT in source and tar-
get languages respectively, with 
TS VV , denoting 
the vocabulary of source and target languages, 
the similarity between the two documents is giv-
en by the KL-divergence measure, -KL(DS || DT), 
as: 
?
? TTw ST
TT
ST
V Dwp
DwpDwp )|(
)|(log)|(
  
where p(w | D) is the likelihood of word w in D. 
As we are interested in target documents which 
are similar to a given source document, we can 
ignore the numerator as it is independent of the 
target document.  Finally, expanding p(wT | Ds) 
as 
)|()|( SVw TSS wwpDwpSS??
we specify the 
cross-language similarity score as follows: 
 
Cross-language similarity =       
)|(log)|()|( TTSTw w SS DwpwwpDwpTVT SVS? ?? ?
 
 
3.2 Mining NETEs from Document Pairs  
The second stage of the MINT method works on 
each pair of articles (DS, DT) in the collection AS,T  
and produces a set PS,T of NETEs. Each pair (?S, 
?T) in PS,T  consists of an NE ?S in language S, and 
a token ?T in language T, that are transliteration 
equivalents of each other.  Furthermore, the 
transliteration similarity between ?S and ?T, as 
measured by the transliteration similarity model 
MT, is at least ? > 0. Figure 3 outlines this algo-
rithm.  
 
Discriminative Transliteration Similarity 
Model:  
The transliteration similarity model MT measures 
the degree of transliteration equivalence between 
a source language and a target language term.  
Input: Comparable news corpora (CS, CT) in languages (S,T)  
           Crosslanguage Document Similarity Model MD for (S, T) 
           Threshold score ?. 
Output: Set AS,T of pairs of similar articles (DS, DT) from (CS, CT). 
1 AS,T  ? ? ;         // Set of Similar articles (DS, DT) 
2 for each article DS in CS do 
3     XS   ? ? ;       // Set of candidates for DS. 
4      for each article dT  in CT  do 
5         score = CrossLanguageDocumentSimilarity(DS,dT,MD); 
6         if (score ? ?) then XS  ? XS  ? (dT , score) ; 
7      end 
8     DT  = BestScoringCandidate(XS); 
9    if (DT  ? ?) then AS,T  ? AS,T  ? (DS, DT) ; 
10 end 
CrossLanguageSimilarDocumentPairs 
Figure 2. Stage 1 of MINT 
Input:  
      Set AS,T  of similar documents (DS, DT)  in languages  
(S,T),   
      Transliteration Similarity Model MT for (S, T),  
      Threshold score ?. 
Output: Set PS,T  of NETEs (?S, ?T) from  AS,T ; 
1   PS,T  ? ? ;  
2   for each pair of articles (DS, DT) in AS,T  do 
3        for each named entity ?S in DS do  
4            YS ? ? ; // Set of candidates for ?S. 
5            for each candidate eT  in DT  do 
6                 score = TransliterationSimilarity(?S, eT, MT) ; 
7                 if (score ? ?)  then   YS  ?  YS ? (eT , score) ; 
8            end 
9            ?T  = BestScoringCandidate(YS) ;  
10          if (?T  ? null) then PS,T  ?  PS,T  ? (?S, ?T) ; 
11      end 
12 end 
TransliterationEquivalents 
Figure 3. Stage 2 of MINT 
801
We employ a logistic function as our translitera-
tion similarity model MT, as follows: 
 
 TransliterationSimilarity (?S,eT,MT) = 
),( TS1
1
ewte ?????
 
where ? (?S, eT) is the feature vector for the pair 
(?S, eT) and w is the weights vector.  Note that the 
transliteration similarity takes a value in the 
range [0..1]. The weights vector w is learnt dis-
criminatively over a training corpus of known 
transliteration equivalents in the given pair of 
languages. 
 
Features: The features employed by the model 
capture interesting cross-language associations 
observed in (?S, eT): 
 
? All unigrams and bigrams from the 
source and target language strings. 
? Pairs of source string n-grams and target 
string n-grams such that difference in the 
start positions of the source and target n-
grams is at most 2. Here n ? ?2,1? . 
? Difference in the lengths of the two 
strings.  
 
Generative Transliteration Similarity Model: 
We also experimented with an extension of He?s 
W-HMM model (He, 2007). The transition prob-
ability depends on both the jump width and the 
previous source character as in the W-HMM 
model. The emission probability depends on the 
current source character and the previous target 
character unlike the W-HMM model (Udupa et 
al., 2009). Instead of using any single alignment 
of characters in the pair (wS, wT), we marginalize 
over all possible alignments: 
? ? ? ? ? ?11
1
11 ,|,|| 1 ??
?
???? jajajj
A
m
j
nm tstpsaapstP
jj
 
 
Here, 
jt
(and resp. 
is ) denotes the j
th (and resp. 
ith) character in wT (and resp. wS) and maA 1? is 
the hidden alignment between wT and wS where 
jt
is aligned to 
jas
, ,m,j ?1? . We estimate 
the parameters of the model using the EM algo-
rithm. The transliteration similarity score of a 
pair (wS, wT) is log P(wT  | wS) appropriately trans-
formed. 
 
 
4 Experimental Setup 
Our empirical investigation consists of experi-
ments in three data environments, with each en-
vironment providing answer to specific set of 
questions, as listed below: 
 
1. Ideal Environment (IDEAL): Given a collec-
tion AS,T of oracle-aligned article pairs (DS, DT) 
in S and T, how effective is Stage 2 of MINT in 
mining NETE from AS,T? 
2. Near Ideal Environment (NEAR-IDEAL): 
Let AS,T  be a collection of similar article pairs 
(DS, DT) in S and T. Given comparable corpora 
(CS, CT) consisting of only articles from AS,T, but 
without the knowledge of pairings between the 
articles,  
a. How effective is Stage 1 of MINT in re-
covering AS,T  from (CS, CT) ? 
b. What is the effect of Stage 1 on the 
overall effectiveness of MINT? 
3. Real Environment (REAL): Given large 
comparable corpora (CS, CT), how effective is 
MINT, end-to-end? 
 
The IDEAL environment is indeed ideal for 
MINT since every article in the comparable cor-
pora is paired with exactly one similar article in 
the other language and the pairing of articles in 
the comparable corpora is known in advance.  
We want to emphasize here that such corpora are 
indeed available in many domains such as tech-
nical documents and interlinked multilingual 
Wikipedia articles. In the IDEAL environment, 
only Stage 2 of MINT is put to test, as article 
alignments are given.  
In the NEAR-IDEAL data environment, every 
article in the comparable corpora is known to 
have exactly one conjugate article in the other 
language though the pairing itself is not known 
in advance.  In such a setting, MINT needs to 
discover the article pairing before mining NETEs 
and therefore, both stages of MINT are put to 
test.  The best performance possible in this envi-
ronment should ideally be the same as that of 
IDEAL, and any degradation points to the short-
coming of the Stage 1 of MINT.  These two en-
vironments quantify the stage-wise performance 
of the MINT method.    
Finally, in the data environment REAL, we 
test MINT on large comparable corpora, where 
even the existence of a conjugate article in the 
target side for a given article in the source side of 
the comparable corpora is not guaranteed, as in 
802
any normal large multilingual news corpora. In 
this scenario both the stages of MINT are put to 
test.  This is the toughest, and perhaps the typical 
setting in which MINT would be used.  
4.1 Comparable Corpora 
In our experiments, the source language is Eng-
lish whereas the 4 target languages are from 
three different language families (Hindi from the 
Indo-Aryan family, Russian from the Slavic fam-
ily, Kannada and Tamil from the Dravidian fami-
ly). Note that none of the five languages use a 
common script and hence identification of cog-
nates, spelling variations, suffix transformations, 
and other techniques commonly used for closely 
related languages that have a common script are 
not applicable for mining NETEs.  Table 1 sum-
marizes the 6 different comparable corpora that 
were used for the empirical investigation; 4 for 
the IDEAL and NEAR-IDEAL environments (in 
4 language pairs), and 2 for the REAL environ-
ment (in 2 language pairs). 
 
Cor-
pus 
Source -
Target 
Data 
Environ-
ment 
Articles (in 
Thousands) 
Words (in 
Millions) 
Src Tgt Src Tgt 
EK-S 
English- 
Kannada 
IDEAL& 
NEAR-IDEAL 
2.90 2.90 0.42 0.34 
ET-S 
English- 
Tamil 
IDEAL& 
NEAR-IDEAL 
2.90 2.90 0.42 0.32 
ER-S 
English- 
Russian 
IDEAL& 
NEAR-IDEAL 
2.30 2.30 1.03 0.40 
EH-S 
English- 
Hindi 
IDEAL& 
NEAR-IDEAL 
11.9 11.9 3.77 3.57 
EK-L 
English- 
Kannada 
REAL 103.8 111.0 27.5 18.2 
ET-L 
English- 
Tamil 
REAL 103.8 144.3 27.5 19.4 
Table 1: Comparable Corpora 
 
The corpora can be categorized into two sepa-
rate groups, group S (for Small) consisting of 
EK-S, ET-S, ER-S, and EH-S and group L (for 
Large) consisting of EK-L and ET-L. Corpora in 
group S are relatively small in size, and contain 
pairs of articles that have been judged by human 
annotators as similar. Corpora in group L are two 
orders of magnitude larger in size than those in 
group S and contain a large number of articles 
that may not have conjugates in the target side. 
In addition the pairings are unknown even for the 
articles that have conjugates. All comparable 
corpora had publication dates, except EH-S, 
which is known to have been published over the 
same year. 
The EK-S, ET-S, EK-L and ET-L corpora are 
from The New Indian Express news paper, whe-
reas the EH-S corpora are from Web Dunia and 
the ER-S corpora are from BBC/Lenta News 
Agency respectively. 
4.2 Cross-language Similarity Model  
The cross-language document similarity model 
requires a bilingual dictionary in the appropriate 
language pair. Therefore, we generated statistical 
dictionaries for 3 language pairs (from parallel 
corpora of the following sizes: 11K sentence 
pairs in English-Kannada, 54K in English-Hindi, 
and 14K in English-Tamil) using the GIZA++ 
statistical alignment tool (Och et al, 2003), with 
5 iterations each of IBM Model 1 and HMM.  
We did not have access to an English-Russian 
parallel corpus and hence could not generate a 
dictionary for this language pair. Hence, the 
NEAR-IDEAL experiments were not run for the 
English-Russian language pair.   
Although the coverage of the dictionaries was 
low, this turned out to be not a serious issue for 
our cross-language document similarity model as 
it might have for topic based CLIR (Ballesteros 
and Croft, 1998). Unlike CLIR, where the query 
is typically smaller in length compared to the 
documents, in our case we are dealing with news 
articles of comparable size in both source and 
target languages.  
When many translations were available for a 
source word, we considered only the top-4 trans-
lations.  Further, we smoothed the document 
probability distributions with collection frequen-
cy as described in (Ponte and Croft, 1998). 
4.3 Transliteration Similarity Model  
The transliteration similarity models for each of 
the 4 language pairs were produced by learning 
over a training corpus consisting of about 16,000 
single word NETEs, in each pair of languages.  
The training corpus in English-Hindi, English-
Kannada and English-Tamil were hand-crafted 
by professionals, the English-Russian name pairs 
were culled from Wikipedia interwiki links and 
were cleaned heuristically.  Equal number of 
negative samples was used for training the mod-
els. To produce the negative samples, we paired 
each source language NE with a random non-
matching target language NE.  No language spe-
cific features were used and the same feature set 
was used in each of the 4 language pairs making 
MINT language neutral.   
In all the experiments, our source side lan-
guage is English, and the Stanford Named Entity 
Recognizer (Finkel et al 2005) was used to ex-
tract NEs from the source side article.  It should 
be noted here that while the precision of the NER 
803
used was consistently high, its recall was low, 
(~40%) especially in the New Indian Express 
corpus, perhaps due to the differences in the data 
used for training the NER and the data on which 
we used it.   
4.4 Performance Measures  
Our intention is to measure the effectiveness of 
MINT by comparing its performance with the 
oracular (human annotator) performance.  As 
transliteration equivalents must exist in the 
paired articles to be found by MINT, we focus 
only on those NEs that actually have at least one 
transliteration equivalent in the conjugate article. 
Three performance measures are of interest to 
us: the fraction of distinct NEs from source lan-
guage for which we found at least one translitera-
tion in the target side (Recall on distinct NEs), 
the fraction of distinct NETEs (Recall on distinct 
NETEs) and the Mean Reciprocal Rank (MRR) 
of the NETEs mined.  Since we are interested in 
mining not only the highly frequent but also the 
infrequent NETEs, recall metrics measure how 
effective our method is in mining NETEs ex-
haustively. The MRR score indicates how effec-
tive our method is in preferring the correct ones 
among candidates. 
To measure the performance of MINT, we 
created a test bed for each of the language pairs. 
The test beds are summarized in Table 2.  
The test beds consist of pairs of similar ar-
ticles in each of the language pairs. It should be 
noted here that as transliteration equivalents must 
exist in the paired articles to be found by MINT, 
we focus only on those NEs that actually have at 
least one transliteration equivalent in the conju-
gate article. 
5 Results & Analysis 
In this section, we present qualitative and quan-
titative performance of the MINT algorithm, in 
mining NETEs from comparable news corpora. 
All the results in Sections 5.1 to 5.3 were ob-
tained using the discriminative transliteration 
similarity model described in Section 3.2. The 
results using the generative transliteration simi-
larity model are discussed in Section 5.4. 
5.1 IDEAL Environment 
Our first set of experiments investigated the ef-
fectiveness of Stage 2 of MINT, namely the min-
ing of NETEs in an IDEAL environment. As 
MINT is provided with paired articles in this ex-
periment, all experiments for this environment 
were run on test beds created from group S cor-
pora (Table 2).  
 
 
Results in the IDEAL Environment:  
The recall measures for distinct NEs and distinct 
NETEs for the IDEAL environment are reported 
in Table 3.  
 
Test 
Bed 
Recall (%) 
Distinct NEs Distinct NETEs 
EK-ST 97.30 95.07 
ET-ST 99.11 98.06 
EH-ST 98.55 98.66 
ER-ST 93.33 85.88 
 Table 3: Recall of MINT in IDEAL 
 
Note that in the first 3 language pairs MINT was 
able to mine a transliteration equivalent for al-
most all the distinct NEs. The performance in 
English-Russian pair was relatively worse, per-
haps due to the noisy training data.   
In order to compare the effectiveness of 
MINT with a state-of-the-art NETE mining ap-
proach, we implemented the time series based 
Co-Ranking algorithm based on (Klementiev and 
Roth, 2006).  
 
Table 4 shows the MRR results in the IDEAL 
environment ? both for MINT and the Co-
Ranking baseline: MINT outperformed Co-
Ranking on all the language pairs, despite not 
using time series similarity in the mining 
process.  The high MRRs (@1 and @5) indicate 
that in almost all the cases, the top-ranked candi-
date is a correct NETE.  Note that Co-Ranking 
could not be run on the EH-ST test bed as the 
articles did not have a date stamp. Co-Ranking is 
crucially dependent on time series and hence re-
quires date stamps for the articles. 
 
Test Bed 
Comparable 
Corpora 
Article 
Pairs 
Distinct 
NEs 
Distinct 
NETEs 
EK-ST EK-S 200 481 710 
ET-ST ET-S 200 449 672 
EH-ST EH-S 200 347 373 
ER-ST ER-S 100 195 347 
Table 2: Test Beds for IDEAL & NEAR-IDEAL 
Test 
Bed 
MRR@1 MRR@5 
MINT CoRanking MINT CoRanking 
EK-ST 0.94 0.26 0.95 0.29 
ET-ST 0.91 0.26 0.94 0.29 
EH-ST 0.93 - 0.95 - 
ER-ST 0.80 0.38 0.85 0.43 
Table 4: MINT & Co-Ranking in IDEAL 
804
5.2 NEAR-IDEAL Environment 
The second set of experiments investigated the 
effectiveness of Stage 1 of MINT on comparable 
corpora that are constituted by pairs of similar 
articles, where the pairing information between 
the articles is with-held.  MINT reconstructed the 
pairings using the cross-language document si-
milarity model and subsequently mined NETEs. 
As in previous experiments, we ran our experi-
ments on test beds described in Section 4.4. 
 
Results in the NEAR-IDEAL Environment: 
There are two parts to this set of experiments. In 
the first part, we investigated the effectiveness of 
the cross-language document similarity model 
described in Section 3.1. Since we know the 
identity of the conjugate article for every article 
in the test bed, and articles can be ranked accord-
ing to the cross-language document similarity 
score, we simply computed the MRR for the 
documents identified in each of the test beds, 
considering only the top-2 results. Further, where 
available, we made use of the publication date of 
articles to restrict the number of target articles 
that are considered in lines 4 and 5 of the MINT 
algorithm in Figure 2.  Table 5 shows the results 
for two date windows ? 3 days and 1 year. 
 
 Test 
Bed 
MRR@1 MRR@2 
3 days 1 year 3 days 1 year 
EK-ST 0.99 0.91 0.99 0.93 
ET-ST 0.96 0.83 0.97 0.87 
EH-ST - 0.81 - 0.82 
Table 5: MRR of Stage 1 in NEAR-IDEAL 
 
Subsequently, the output of the Stage 1 was giv-
en as the input to the Stage 2 of the MINT me-
thod. In Table 6 we report the MRR @1 and @5 
for the second stage, for both time windows (3 
days & 1 year). 
 
It is interesting to compare the results of MINT 
in NEAR-IDEAL data environment (Table 6) 
with MINT?s results in IDEAL environment 
(Table 4). The drop in MRR@1 is small: ~2% 
for EK-ST and ~3% for ET-ST. For EH-ST the 
drop is relatively more (~12%) as may be ex-
pected since the time window (3 days) could not 
be applied for this test bed.  
5.3 REAL Environment 
The third set of experiments investigated the ef-
fectiveness of MINT on large comparable corpo-
ra. We ran the experiments on test beds created 
from group L corpora.   
 
 Test-beds for the REAL Environment: The 
test beds for the REAL environment (Table 7) 
consisted of only English articles since we do not 
know in advance whether these articles have any 
similar articles in the target languages. 
 
 Results in the REAL Environment: In real 
environment, we examined the top 2 articles of 
returned by Stage 1 of MINT, and mined NETEs 
from them. We used a date window of 3 in Stage 
1. Table 8 summarizes the results for the REAL 
environment. 
 
We observe that the performance of MINT is 
impressive, considering the fact that the compa-
rable corpora used in the REAL environment is 
two orders of magnitude larger than those used in 
IDEAL and NEAR-IDEAL environments. This 
implies that MINT is able to effectively mine 
NETEs whenever the Stage 1 algorithm was able 
to find a good conjugate for each of the source 
language articles.  
5.4 Generative Transliteration Similarity 
Model 
We employed the extended W-HMM translitera-
tion similarity model in MINT and used it in the 
IDEAL data environment.  Table 9 shows the 
results. 
Test 
Bed 
MRR@1 MRR@5 
3 days 1 year 3 days 1 year 
EK-ST 0.92 0.87 0.94 0.90 
ET-ST 0.88 0.74 0.91 0.78 
EH-ST - 0.82 - 0.87 
Table 6: MRR of Stage 2 in NEAR-IDEAL 
Test 
Bed 
Comparable 
Corpora 
Articles 
Distinct  
NEs 
EK-LT EK-L 100 306 
ET-LT ET-L 100 228 
Table 7: Test Beds for REAL 
 
Test Bed 
MRR 
@1 @5 
EK-LT 0.86 0.88 
ET-LT 0.82 0.85 
Table 8: MRR of Stage 2 in REAL 
Test Bed 
MRR 
@1 @5 
EK-S 0.85 0.86 
ET-S 0.81 0.82 
EH-S 0.91 0.93 
Table 9:  MRR of Stage 2 in IDEAL using genera-
tive transliteration similarity model 
805
We see that the results for the generative transli-
teration similarity model are good but not as 
good as those for the discriminative translitera-
tion similarity model. As we did not stem either 
the English NEs or the target language words, 
the generative model made more mistakes on 
inflected words compared to the discriminative 
model.   
5.5  Examples of Mined NETEs 
Table 10 gives some examples of the NETEs 
mined from the comparable news corpora.  
 
6  Related Work 
CLIR systems have been studied in several 
works (Ballesteros and Croft, 1998; Kraiij et al 
2003). The limited coverage of dictionaries has 
been recognized as a problem in CLIR and MT 
(Demner-Fushman & Oard, 2002; Mandl & 
Womser-hacker, 2005; Xu &Weischedel, 2005).  
In order to address this problem, different 
kinds of approaches have been taken, from learn-
ing transformation rules from dictionaries and 
applying the rules to find cross-lingual spelling 
variants (Pirkola et al, 2003), to  learning trans-
lation lexicon from monolingual and/or compa-
rable corpora (Fung, 1995; Al-Onaizan and 
Knight, 2002; Koehn and Knight, 2002; Rapp, 
1996). While these works have focused on find-
ing translation equivalents of all class of words, 
we focus specifically on transliteration equiva-
lents of NEs.  (Munteanu and Marcu, 2006; 
Quirk et al, 2007) addresses mining of parallel 
sentences and fragments from nearly parallel 
sentences. In contrast, our approach mines 
NETEs from article pairs that may not even have 
any parallel or nearly parallel sentences.   
NETE discovery from comparable corpora 
using time series and transliteration model was 
proposed in (Klementiev and Roth, 2006), and 
extended for NETE mining for several languages 
in (Saravanan and Kumaran, 2007).  However, 
such methods miss vast majority of the NETEs 
due to their dependency on frequency signatures.   
In addition, (Klementiev and Roth, 2006) may 
not scale for large corpora, as they examine 
every word in the target side as a potential trans-
literation equivalent. NETE mining from compa-
rable corpora using phonetic mappings was pro-
posed in (Tao et al, 2006), but the need for lan-
guage specific knowledge restricts its applicabili-
ty across languages.  We proposed the idea of 
mining NETEs from multilingual articles with 
similar content in (Udupa, et al, 2008). In this 
work, we extend the approach and provide a de-
tailed description of the empirical studies. 
7  Conclusion 
In this paper, we showed that MINT, a simple 
and intuitive technique employing cross-
language document similarity and transliteration 
similarity models, is capable of mining NETEs 
effectively from large comparable news corpora. 
Our three stage empirical investigation showed 
that MINT performed close to optimal on com-
parable corpora consisting of pairs of similar ar-
ticles when the pairings are known in advance. 
MINT induced fairly good pairings and performs 
exceedingly well even when the pairings are not 
known in advance. Further, MINT outperformed 
a state-of-the-art baseline and scaled to large 
comparable corpora.  Finally, we demonstrated 
the language neutrality of MINT, by mining 
NETEs from 4 language pairs (between English 
and one of Russian, Hindi, Kannada or Tamil) 
from 3 vastly different linguistic families. 
As a future work, we plan to use the ex-
tended W-HMM model to get features for the 
discriminative transliteration similarity model. 
We also want to use a combination of the cross-
language document similarity score and the 
transliteration similarity score for scoring the 
NETEs. Finally, we would like to use the mined 
NETEs to improve the performance of the first 
stage of MINT. 
Acknowledgments 
We thank Abhijit Bhole for his help and Chris 
Quirk for valuable comments. 
 
Language 
Pair 
Source NE Transliteration 
English-
Kannada 
Woolmer ??????? 
Kafeel ????? 
Baghdad ???????? 
English-Tamil Lloyd ??????  
Mumbai ?????????? 
Manchester ??????????? 
English-Hindi Vanhanen ??????? 
Trinidad ???????????  
Ibuprofen ?????????? 
English-
Russian 
Kreuzberg ?????????? 
Gaddafi ??????? 
Karadzic ???????? 
Table 10: Examples of Mined NETEs 
806
References 
AbdulJaleel, N. and Larkey, L.S. 2003. Statistical translite-
ration for English-Arabic cross language information re-
trieval. Proceedings of CIKM 2003.  
Al-Onaizan, Y. and Knight, K. 2002. Translating named 
entities using monolingual and bilingual resources. Pro-
ceedings of the 40th Annual Meeting of ACL. 
Ballesteros, L. and Croft, B. 1998. Dictionary Methods for 
Cross-Lingual Information Retrieval. Proceedings of 
DEXA?96.  
Chen, H., et al 1998. Proper Name Translation in Cross-
Language Information Retrieval. Proceedings of the 36th 
Annual Meeting of the ACL. 
Demner-Fushman, D., and Oard, D. W. 2002. The effect of 
bilingual term list size on dictionary-based cross-
language information retrieval. Proceedings of the 36th 
Hawaii International Conference on System Sciences.  
Finkel, J. Trond Grenager, and Christopher Manning. 2005. 
Incorporating Non-local Information into Information 
Extraction Systems by Gibbs Sampling. Proceedings of 
the 43nd Annual Meeting of the ACL. 
Fung, P. 1995. Compiling bilingual lexicon entries from a 
non-parallel English-Chinese corpus. Proceedings of the 
3rd Workshop on Very Large Corpora. 
Fung, P. 1995. A pattern matching method for finding noun 
and proper noun translations from noisy parallel corpora.  
Proceedings of ACL 1995.  
He. X. 2007: Using word dependent transition models in 
HMM based word alignment for statistical machine 
translation. In Proceedings of 2nd ACL Workshop on Sta-
tistical Machine Translation . 
Hermjakob, U., Knight, K., and Daume, H. 2008. Name 
translation in statistical machine translation: knowing 
when to transliterate. Proceedings ACL 2008. 
Klementiev, A. and Roth, D. 2006. Weakly supervised 
named entity transliteration and discovery from multilin-
gual comparable corpora. Proceedings of the 44th Annual 
Meeting of the ACL.  
Knight, K. and Graehl, J. 1998. Machine Transliteration. 
Computational Linguistics.  
Koehn, P. and Knight, K. 2002. Learning a translation lex-
icon from monolingual corpora. Proceedings of Unsu-
pervised Lexical Acquisition. 
Kraiij, W., Nie, J-Y. and  Simard, M. 2003. Emebdding 
Web-based Statistical Translation Models in Cross-
Language Information Retrieval. Computational Linguis-
tics., 29(3):381-419. 
Mandl, T., and Womser-Hacker, C.  2004. How do named 
entities contribute to retrieval effectiveness? Proceedings 
of the 2004 Cross Language Evaluation Forum Cam-
paign 2004. 
Mandl, T., and Womser-Hacker, C.  2005. The Effect of 
named entities on effectiveness in cross-language infor-
mation retrieval evaluation. ACM Symposium on Applied 
Computing.  
Munteanu, D. and Marcu D. 2006. Extracting parallel sub-
sentential fragments from non-parallel corpora. Proceed-
ings of the ACL 2006. 
Och, F. and Ney, H. 2003. A systematic comparison of var-
ious statistical alignment models. Computational Lin-
guistics. 
Pirkola, A., Toivonen, J., Keskustalo, H., Visala, K. and 
Jarvelin, K. 2003. Fuzzy translation of cross-lingual 
spelling variants. Proceedings of SIGIR 2003.  
Ponte, J. M. and Croft, B. 1998. A Language Modeling 
Approach to Information Retrieval. Proceedings of ACM 
SIGIR 1998.  
Quirk, C., Udupa, R. and Menezes, A. 2007. Generative 
models of noisy translations with applications to parallel 
fragments extraction. Proceedings of the 11th MT Sum-
mit. 
Rapp, R. 1996. Automatic identification of word transla-
tions from unrelated English and German corpora. Pro-
ceedings of ACL?99 
Saravanan, K. and Kumaran, A. 2007. Some experiments in 
mining named entity transliteration pairs from compara-
ble corpora. Proceedings of the 2nd International Work-
shop on Cross Lingual Information Access. 
Tao, T., Yoon, S., Fister, A., Sproat, R. and Zhai, C. 2006. 
Unsupervised named entity transliteration using temporal 
and phonetic correlation. Proceedings of EMNLP 2006.  
Udupa, R., Saravanan, K., Kumaran, A. and Jagarlamudi, J.  
2008.  Mining Named Entity Transliteration Equivalents 
from Comparable Corpora. Proceedings of the CIKM 
2008. 
Udupa, R., Saravanan, K., Bakalov, A. and Bhole, A.  2009.  
?They are out there if you know where to look?: Mining 
transliterations of OOV terms in cross-language informa-
tion retrieval. Proceedings of the ECIR 2009. 
Virga, P. and Khudanpur, S. 2003. Transliteration of proper 
names in cross-lingual information retrieval. Proceedings 
of the ACL Workshop on Multilingual and Mixed Lan-
guage Named Entity Recognition.  
Xu, J. and Weischedel, R. 2005. Empirical studies on the 
impact of lexical resources on CLIR performance. In-
formation Processing and Management. 
807
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 930?940,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Improving Bilingual Projections via Sparse Covariance Matrices
Jagadeesh Jagarlamudi
University of Maryland
College Park, USA
jags@umiacs.umd.edu
Raghavendra Udupa
Microsoft Research
Bangalore, India
raghavu@microsoft.com
Hal Daume? III
University of Maryland
College Park, USA
hal@umiacs.umd.edu
Abhijit Bhole
Microsoft Research
Bangalore, India
v-abbhol@microsoft.com
Abstract
Mapping documents into an interlingual rep-
resentation can help bridge the language bar-
rier of cross-lingual corpora. Many existing
approaches are based on word co-occurrences
extracted from aligned training data, repre-
sented as a covariance matrix. In theory, such
a covariance matrix should represent seman-
tic equivalence, and should be highly sparse.
Unfortunately, the presence of noise leads to
dense covariance matrices which in turn leads
to suboptimal document representations. In
this paper, we explore techniques to recover
the desired sparsity in covariance matrices in
two ways. First, we explore word association
measures and bilingual dictionaries to weigh
the word pairs. Later, we explore different
selection strategies to remove the noisy pairs
based on the association scores. Our experi-
mental results on the task of aligning compa-
rable documents shows the efficacy of sparse
covariance matrices on two data sets from two
different language pairs.
1 Introduction
Aligning documents from different languages arises
in a range of tasks such as parallel phrase extrac-
tion (Gale and Church, 1991; Rapp, 1999), mining
translations for out-of-vocabulary words for statis-
tical machine translation (Daume III and Jagarla-
mudi, 2011) and document retrieval (Ballesteros and
Croft, 1996; Munteanu and Marcu, 2005). In this
task, we are given a comparable corpora and some
documents in one language are assumed to have a
comparable document in the other language and the
goal is to recover this hidden alignment. In this pa-
per, we address this problem by mapping the docu-
ments into a common subspace (interlingual repre-
sentation). This common subspace generalizes the
notion of vector space model for cross-lingual ap-
plications (Turney and Pantel, 2010).
Most of the existing approaches use manually
aligned document pairs to find a common subspace
in which the aligned document pairs are maximally
correlated. The sub-space can be found using ei-
ther generative approaches based on topic modeling
(Mimno et al, 2009; Jagarlamudi and Daume? III,
2010; Zhang et al, 2010; Vu et al, 2009) or dis-
criminative approaches based on variants of Princi-
pal Component Analysis (PCA) and Canonical Cor-
relation Analysis (CCA) (Susan T. Dumais, 1996;
Vinokourov et al, 2003; Platt et al, 2010; Haghighi
et al, 2008). Both styles rely on document level
term co-occurrences to find the latent representation.
The discriminative approaches capture essential
word co-occurrences in terms of two monolingual
covariance matrices and a cross-covariance matrix.
Subsequently, they use these covariance matrices to
find projection directions in each language such that
aligned documents lie close to each other (Sec. 2).
The strong reliance of these approaches on the co-
variance matrices leads to problems, especially with
the noisy data caused either by the noisy words
in a document or the noisy document alignments.
Noisy data is not uncommon and is usually the case
with data collected from community based resources
such as Wikipedia. This degrades performance of a
930
variety of tasks, such as transliteration Mining (Kle-
mentiev and Roth, 2006; Hermjakob et al, 2008;
Ravi and Knight, 2009) and multilingual web search
(Gao et al, 2009).
In this paper, we address the problem of identi-
fying and removing noisy entries in the covariance
matrices. We address this problem in two stages.
In the first stage, we explore the use of word asso-
ciation measures such as Mutual Information (MI)
and Yule?s ? (Reis and Judd, 2000) in computing
the strength of a word pair (Sec. 3.1). We also
explore the use of bilingual dictionaries developed
from cleaner resources such as parallel data. In the
second stage, we use the association strengths in fil-
tering out the noisy word pairs from the covariance
matrices. We pose this as a word pair selection prob-
lem and explore multiple strategies (Sec. 3.2).
We evaluate the utility of sparse covariance ma-
trices in improving the bilingual projections incre-
mentally (Sec. 4). We first report results on syn-
thetic multi-view data where the true correspon-
dences between features of different views are avail-
able. Moreover, this also lets us systematically ex-
plore the effect of noise level on the accuracy. Our
experimental results show a significant improvement
when the true correspondences are available. Later,
we report our experimental results on the document
alignment task on Europarl and Wikipedia data sets
and on two language pairs. We found that sparsify-
ing the covariance matrices helps in general, but us-
ing cleaner resource such bilingual dictionaries per-
formed best.
2 Canonical Correlation Analysis (CCA)
In this section, we describe how Canonical Correla-
tion Analysis is used to solve the problem of align-
ing bilingual documents. We mainly focus on repre-
senting the solution of CCA in terms of covariance
matrices. Since most of the existing discriminative
approaches are variants of CCA, showing the advan-
tage of recovering sparseness in CCA makes it appli-
cable to the other variants as well.
Given a training data of n aligned document pairs,
CCA finds projection directions for each language,
so that the documents when projected along these di-
rections are maximally correlated (Hotelling, 1936).
Let X (d1?n) and Y (d2?n) be the representation
of data in both the languages and further assume that
the data is centered (subtract the mean vector from
each document i.e. xi?xi??x and yi ? yi??y).
Then CCA finds projection directions a and b which
maximize:
aTXY Tb?
aTXXTa
?
bTY Y Tb
s.t. aTXXTa = 1 & bTY Y Tb = 1
The projection directions are obtained by solving the
generalized eigen system:
[
0 Cxy
Cyx 0
] [
a
b
]
=
[
(1-?)Cxx+?I 0
0 (1-?)Cyy+?I
] [
a
b
]
(1)
where Cxx = XXT , Cyy = Y Y T are the monolin-
gual covariance matrices, Cxy = XY T is the cross-
covariance matrix and ? is the regularization param-
eter. Using these eigenvectors as columns, we form
the projection matrices A and B. These projection
matrices are used to map documents in both the lan-
guages into interlingual representation.
Given any new pair of documents, their similarity
is computed by first mapping them into the lower di-
mensions space and computing the cosine similarity
between their projections. In general, using all the
eigenvectors is sub optimal and thus retaining top
eigenvectors leads to better generalizability.
3 Covariance Selection
As shown above, the underlying objective function
in most of the discriminative approaches is of the
form aTXY Tb. This can be rewritten as :
aTXY Tb =
n?
k=1
?xk,a??yk,b?
=
n?
k=1
( d1?
i=1
Xi,kai ?
d2?
j=1
Yj,kbj
)
=
d1?
i=1
d2?
j=1
aibj
( n?
k=1
Xi,kYj,k
)
=
d1,d2?
i,j=1
aibjCxyij (2)
Similarly, the constraints can also be rewritten as?d1
i,j=1 aiajCxxij = 1 and
?d2
i,j=1 bibjC
yy
ij = 1.
931
Maximizing this objective function, under the
constraints, involves a careful selection of the vec-
tors a and b such that aibj is high whenever Cxyij
is high. So, every non-zero entry of the cross-
covariance matrix restricts the choice of the pro-
jection directions. While this may not be a severe
problem when the training data is clean, but this is
very uncommon especially in the case of high di-
mensional data like text documents. Moreover, the
inherent ambiguity of natural languages increases
the chances of seeing a noisy word in any docu-
ment. Every occurrence of a noisy word will have a
non-zero contribution towards the covariance matrix
making it dense, which in turn prevents the selection
of appropriate projection directions.
In this section, we describe some techniques to
recover the sparsity by removing the noisy entries
from the covariance matrices. We break this task
into two sub problems: computing an association
score for every word pair and then using an appro-
priate strategy to identify the noisy pairs based on
their weights. We explore multiple ways to address
both the steps in the following two sections. For
the sake of convenience and clarity, we describe our
techniques in the context of cross-covariance ma-
trix between English and Spanish language pair. But
these techniques extend directly to monolingual co-
variance matrices, and to different language pairs as
well.
3.1 Computing Word Pair Association
The first step in filtering out the noisy word co-
occurrences is to use an appropriate measure to com-
pute the strength of word pairs (English and Span-
ish words). This is a well studied problem and sev-
eral association measures have been proposed in the
NLP literature (Dunning, 1993; Inkpen and Hirst,
2002; Moore, 2004). These association measures
can be divided into groups based on the statistics
they use (Hoang et al, 2009). Here we explore a few
of them for sparsifying the cross-covariance matrix.
3.1.1 Covariance
The first option is to use the cross-covariance
matrix itself. As noted above, when the data ma-
trix is centered, the cross-covariance of an English
word (ei) with a Spanish word (fj) is given by?n
k=1 XikYjk. It measures the strength with which
two words co-occur together. This measure uses in-
formation about the occurrence of a word pair in
aligned documents and doesn?t use other statistics
such as ?how often this pair doesn?t co-occur to-
gether? and so on.
3.1.2 Mutual Information
Association measures like covariance and Point-
wise Mutual Information, which only use the fre-
quency with which a word pair co-occurs, often
overestimate the strength of low frequent words
(Moore, 2004). On the other hand, measures
like Log-likelihood ratio (Dunning, 1993) and Mu-
tual Information (MI) use other statistics like the
marginal probabilities of each of the words.
For any two words, ei and fj , let n11, n10, n01
and n00 denote the number of documents in which
both the words co-occur, only English word occurs,
only Spanish word occurs and none of the words oc-
cur. Then the Mutual Information of this word pair
is given by:
MI(ei, fj) =
1
n
?
i,j?{0,1}
nij log
nij ? n
ninj
(3)
where ni and nj denote the number of documents
in which the English and the Spanish word occurs
and n is the total number of documents. We treat
the occurrence of a word in a document slightly dif-
ferent from others, we treat a word as occurring in
a document if it has occurred more than its average
frequency in the corpus. Log-likelihood ratio and
the MI differ only in terms of the constant they use,
so we use only MI in our experiments.
3.1.3 Yule?s ?
Yule?s ? is another popular association measure
used in psychology (Reis and Judd, 2000). It uses
same statistics used by Mutual Information but dif-
fers in the way in which they are combined. MI con-
verts the frequencies into probabilities before com-
puting the association measure where as Yule?s ?
uses the observed frequencies directly, and doesn?t
make any assumptions about the underlying proba-
bility distributions. Given the same interpretation of
the variables as introduced in the previous section,
the Yule?s ? is estimated as:
? =
?n00n11 ?
?n01n10?n00n11 +
?n01n10
(4)
932
This way of combining the frequencies bears simi-
larity with the log-odds ratio.
3.1.4 Bilingual Dictionary
The above three association measures use the
same training data that is available to compute the
covariance matrices in CCA. Thus, their utility in
bringing additional information, which is not cap-
tured by the covariance matrices, is arguable (our
experiments show that they are indeed helpful).
Moreover, they use document level co-occurrence
information which is coarse compared to the co-
occurrence at sentence level or the translational in-
formation provided by a bilingual dictionary. So,
we use bilingual dictionaries as our final resource to
weigh the word co-occurrences. Notice that, using
bilingual information brings in information gleaned
from an external corpus.
We use translation tables learnt using Giza++
(Och and Ney, 2003) on Europarl data set. Since the
translation tables are asymmetric, we combine trans-
lation tables from both the directions. We first use a
threshold on the conditional probability to filter out
the low probability ones and then convert them into
joint probabilities before combining. For each word
pair (ei, fj), we compute the score as:
1
2
(
P (ei|fj)P (fj) + P (fj|ei)P (ei)
)
While the first three association measures can also
be applied to monolingual data, bilingual dictionary
can?t be used for weighting monolingual word pairs.
So in this case, we use either of the above mentioned
techniques for weighting monolingual word pairs.
3.2 Selection Strategies
The next step after computing association measure
for all word pairs is to use them in selecting the pairs
that need to be retained. In this section, we describe
some approaches such as thresholding and matching
for the word pair selection.
3.2.1 Thresholding
A straight forward way to remove the noisy word
co-occurrences is to zero out the entries of the
cross-covariance matrix that are lower than a thresh-
old. To understand the motivation, consider the
rewritten objective function of CCA, aTXY Tb =
?
ij C
xy
ij aibj . This is linear in terms of the individ-
ual components of the cross-covariance matrix. So,
if we want to remove some of the entries of the co-
variance matrix with minimal change in the value of
the objective function, then the optimal choice is to
sort the entries of the covariance matrix and filter out
the less confident word pairs.
3.2.2 Relative Thresholding
While the thresholding strategy described in the
above section is very simple, it is often biased by
the frequent words. Since a frequent word co-occurs
with other words often, it naturally tends to have
high association with most of the other words. As
a result, absolute thresholding tends to remove all
the less frequent word pairs while leaving the co-
occurrences of the frequent words untouched. Even-
tually, this may lead to zeroing out some of the rows
or the columns of the cross-covariance matrix.
To circumvent this, we try thresholding at word
level. For every English word, we choose a few
Spanish words that have high association and vice
versa. Since the nearest neighbour property is asym-
metric, we take the union of all the selected word
pairs. That is, we retain a word pair, if either the
Spanish word is in the top ranked list of the English
word or vice versa.
3.2.3 Maximal Matching
Though relative thresholding overcomes the prob-
lem of zeroing out entire rows or columns posed by
direct thresholding, it is still biased by the frequent
words. The high association measure of a frequent
English word with many Spanish words, makes it a
nearest neighbour for lot of Spanish words. One way
to prevent this is to discourage an already selected
English word from associating with a new Spanish
word. This requires a global knowledge of all the
selected pairs and can not be done by looking at the
individual words, as is the case with the greedy strat-
egy employed by the relative thresholding.
We use matching to solve this problem. We for-
mulate the selection of the word pairs as a network
flow problem (Jagarlamudi et al, 2011). The objec-
tive is to select word pairs that have high association
measure while constraining each word to be asso-
ciated with only a few words from other language.
Let Iij denote an indicator variable taking a value of
933
0 or 1 depending on if the word pair (ei, fj) is se-
lected or not. We want each word to be associated
with k words from other language, i.e.?j Iij = k
and
?
i Iij = k. Moreover, we want word pairs
with high association score to be selected. We can
encode this objective and the constraints as the fol-
lowing optimization problem:
argmax
I
d1,d2?
i,j=1
Cxyij Iij (5)
?i
?
j
Iij = k; ?j
?
i
Iij = k; ?i, j Iij ? {0, 1}
If k = 1, then this problem reduces to a linear as-
signment problem and can be solved optimally us-
ing the Hungarian algorithm (Jonker and Volgenant,
1987). For other values of k, this can be solved by
relaxing the constraint Iij ? {0, 1} to 0 ? Iij ? 1.
The optimal solution of the relaxed problem can be
found efficiently using linear programming (Ravin-
dra et al, 1993). The uni-modular nature of the
constraints guarantees an integral solution (Schri-
jver, 2003), so relaxing the original integer problem
doesn?t introduce any error in the optimal solution.
3.2.4 Monolingual Augmentation
The above three selection strategies operate on the
covariance matrices independently. In this section
we propose to combine them. Specifically, we pro-
pose to augment the set of selected bilingual word
pairs using the monolingual word pairs. We first use
any of the above mentioned strategies to select bilin-
gual and monolingual word pairs. Let Ixy, Ixx and
Iyy be the binary matrices that indicate the selected
word pairs based on the bilingual and monolingual
association scores. Then the monolingual augmen-
tation strategy updates Ixy in the following way:
Ixy ? Binarize(IxxIxyIyy)
i.e., we multiply Ixy with the monolingual selection
matrices and then binarize the resulting matrix. Our
monolingual augmentation is motivated by the fol-
lowing probabilistic interpretation:
P (x, y) =
?
x?,y?
P (x|x?)P (y|y?)P (x?, y?)
which can be rewritten as P ? T xP (T y)T where
T x and T y are monolingual state transition matrices.
3.3 Our Approach
In this section we summarize our approach for the
task of finding aligned documents from a cross-
lingual comparable corpora. The training phase in-
volves finding projection directions for documents
of both the languages. We compute the covariance
matrices using the training data. Then we use any
of the word association measures (Sec. 3.1) along
with a selection criteria (Sec. 3.2) to recover the
sparseness in either only the cross-covariance or all
of the covariance matrices. Let Ixy, Ixx and Iyy
be the binary matrices which represent the word
pairs that are selected based on the chosen sparsi-
fication technique. Now, we replace the covariance
matrices in Eq. 1 as follows: Cxx ? Cxx ? Ixx,
Cyy ? Cyy ? Iyy and Cxy ? Cxy ? Ixy where
? denotes the element-wise matrix product. Subse-
quently, we solve the generalized eigenvalue prob-
lem shown in Eq. 1 to obtain the projection direc-
tions. Let A and B be the matrices formed with top
eigenvectors of Eq. 1 as the columns. These pro-
jection matrices are used to map documents into the
interlingual representation. Such an interlingual rep-
resentation is useful in many tasks like cross-lingual
text categorization (Bel et al, 2003) multilingual
web search (Gao et al, 2009) and so on.
During the testing, given an English document x,
finding an aligned Spanish document involves solv-
ing:
argmax
y
xT
(
(ABT )? Ixy
)
y
?
xT
(
(AAT )? Ixx
)
x
?
yT
(
(BBT )? Iyy
)
y
If the documents are normalized before hand, then
the above equation reduces to computing only the
numerator.
4 Experiments
4.1 Experimental Setup
We experiment with the task of finding aligned doc-
uments from a cross-lingual comparable corpora. In
this task, we are given comparable corpora consist-
ing of two document collections, each in a differ-
ent language. As the corpora are comparable, some
documents in one collection have a comparable doc-
ument in the other collection. The task is to recover
934
this hidden alignment. The recovered alignment is
compared against the ground truth.
We evaluate our idea of sparsifying the covari-
ance matrices incrementally. We first evaluate the
effectiveness of our approach on synthetic data, as
it enables us to systematically study the effect of
noise. Subsequently, we evaluate each of the above
discussed sparsification strategies on real world data
sets. We have discussed four possible ways for
computing word association measure and three ap-
proaches for word pair selection. That leaves us 12
different ways for sparsifying the covariance matri-
ces, with each method having parameters to control
the amount of sparseness. We use a small amount of
development data for model selection and parameter
tuning and choose a few promising models. Finally,
we compare these selected models with state-of-the-
art baselines on two language pairs and on two dif-
ferent data sets.
In each case, we use the training data to learn
the projection directions. And then, for each of the
test documents, we find the aligned document from
other language. We report average accuracy of the
top ranked document and also the Mean Reciprocal
Rank (MRR) of the true aligned document.
4.2 Synthetic Data
We follow the generative story introduced in Bach
and Jordan (2005) to generate synthetic multi-view
data. Their method does not assume any correspon-
dence between the feature dimensions of both the
views. We modify their approach slightly so that
we know the actual correspondence between the fea-
tures. We use these true feature correspondences for
sparsification of the cross-covariance matrix.
We first generate a d dimensional vector in the
common latent space and then use the projection
matrices to map it into the individual feature spaces
as follows:
z ? N (0, Id)
x|z ? (W1z + ?1) + ? N (0, Id1)
y|z ? (W1z + ?2) + ? N (0, Id2)
Notice that we use the same projection matrix W1
for both the views, this ensures a one-to-one corre-
spondence between the features of both the spaces.
Moreover, we also introduce a parameter ? which
controls the amount of noise in the data.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  1.5  2  2.5  3  3.5  4
Sparse MRR
Sparse Accuracy
CCA MRR
CCA Accuracy
Figure 1: Accuracy of CCA and our sparsified version
with the noise parameter.
We generate a total of 3000 pairs of points and use
2000 of them for training the models and the rest
for evaluation. We use the true feature correspon-
dences to form the cross-covariance selection ma-
trix Ixy (Sec. 3.3). For this experiment, we use the
full monolingual covariance matrices. We train both
CCA and our sparse version on the training data and
evaluate them on the test data. We repeat this mul-
tiple times and report the average accuracies. Fig. 1
shows the performance of CCA and our sparse CCA,
as we vary the noise parameter ? from 1 to 4. It
is very clear that the sparse version performs sig-
nificantly better than CCA. As the noise increases,
the performance of CCA drops quickly. This exper-
iment demonstrates a significant performance gain
when the true correspondences are available. But
this information is not available in the case of real
world data sets, so we try to approximate it.
4.3 Model Selection
As we have discussed, there are several choices for
computing the association measure and for selecting
the word pairs to be retained. And each of them have
sparsity parameters, giving raise to many possible
models. For model selection, we use approximately
5000 document pairs collected from the Wikipedia
between English and Spanish. We use the cross-
language links provided as the ground truth. We to-
kenize the documents, retain only the most frequent
2000 words in each language and convert the docu-
935
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 2000  4000  6000  8000  10000  12000  14000  16000  18000  20000
MI+Match
Yule+Match
Cov+Match
MI+RelThreshold
Yule+RelThreshold
Cov+RelThreshold
MI+Threshold
Yule+Threshold
Cov+Threshold
CCA
Figure 2: Comparison of the word association measures
along with different selection criteria. The x-axis plots
the number of non-zero entries in the covariance matrices
and the y-axis plots the accuracy of top-ranked document.
ments into TFIDF vectors. We use 60% of the data
for training different models and the rest for evaluat-
ing the models. We choose a few promising models
based on this development set results and evaluate
them on bigger data sets.
4.3.1 Selection Strategies
In the first experiment, we combine the three
association measures, Covariance (Cov), MI and
Yule?s ?, with the three selection criteria, Thresh-
old, Relative Threshold (RelThreshold) and Match-
ing (Match). Fig. 2 shows the performance of these
different combinations with varying levels of spar-
sity in the covariance matrices. The horizontal line
represents the performance of CCA on this data set.
We start with 2000 non-zero entries in the covari-
ance matrices and experiment up to 20,000 non-zero
entries. Since our data set has 2000 words in each
language, 2000 non-zero entries in a covariance ma-
trix implies that, on an average, every word is as-
sociated with only one word. This results in highly
sparse covariance matrices.
Overall, we observe that reducing the level of
sparsity , i.e. selecting more number of elements in
the covariance matrices, increases the performance
slightly and then decreases again. From the figure, it
seems that sparsifying the covariance matrices might
help in improving the performance of the task. But
it is interesting to note that not all the models per-
form better than CCA. In fact, both the models that
achieve better scores use Matching as the selection
criteria. This suggests that, apart from the weighting
of the word pairs, appropriate selection of the word
pairs is also equally important. In the rest of the ex-
periments we mainly report results with Matching as
the selection criterion. From this figure, we observe
that Mutual Information and Yule?s ? perform com-
petitively but they consistently outperform models
that use covariance as the association measure. So
in the rest of the experiments we report results with
MI or Yule?s ?.
4.3.2 Amount of Sparsity
In the previous experiment, we used same level
of sparsity for all the covariance matrices, i.e. same
number of associations were selected for each word
in all the three covariance matrices. In the following
experiment, we use different levels of sparsity for
the individual covariance matrices. Fig. 3 shows the
performance of Yule+Match and Dictionary+Match
combinations with different levels of sparsity. In
the Yule+Match combination, we use Yule?s ? as-
sociation measure for weighting the word pairs and
use matching for selection. In the Dictionary+Match
combination, we use bilingual dictionary for sparsi-
fying cross-covariance matrix, i.e. we keep all the
word pairs whose conditional translation probabil-
ity is above a threshold. And for monolingual word
pairs, we use MI for weighting and matching for
word pair selection.
For each level of sparsity of the cross-covariance
matrix, we experiment with different levels of spar-
sity on the monolingual covariance matrices. ?Only
XY? indicates we use the full monolingual covari-
ance matrices. In ?Match(k)? runs, we allow each
word to be associated with a total of k words (Eq. 5).
?Aug? indicates that we use monolingual augmen-
tation to refine the sparsity of the cross-covariance
matrix (Sec. 3.2.4).
From both the figures 3(a) and 3(b), we observe
that ?Only XY? run (dark blue) performs poorly
compared to the other runs, indicating that sparsify-
ing all the covariance matrices is better than spar-
sifying only the cross-covariance matrix. In the
936
(a) Performance of Yule+Match combination. The x-axis plots
the number of Spanish words selected per each English word
and vice versa. This determines the sparsity of Cxy. Matching
is used as selection criteria for all the covariance matrices.
(b) Performance of Dictionary+Match combination. The x-axis
plots the threshold on bilingual translation probability and it deter-
mines the sparsity of Cxy. Matching is used to select only the mono-
lingual sparsity.
Figure 3: Comparison of Yule+Match and Dictionary+Match combination with different levels of sparsity for the
covariance matrices. In both the figures, the x-axis plots the sparsity of the cross-covariance matrix and for each
value we try different levels of sparsity on the monolingual covariance matrices (which are grouped together). The
description of these individual runs is provided in the relevant parts of the text. The y-axis plots the accuracy of the
top-ranked document. CCA achieves 61% accuracy on this data set.
Yule+Match combination, Fig. 3(a), all the runs
seem to be performing better when each English
word is allowed to associate with 2 or 3 Spanish
words and vice versa. Among different ways of se-
lecting the monolingual word pairs, Match(2)+Aug
performs better than the remaining runs. So we use
Match(2)+Aug combination for the Yule?s ? mea-
sure.
Unlike the Yule+Match combinations, there is no
clear winner for Dictionary+Match combinations.
First of all, the performance increase as we increase
the translation probability threshold and then de-
creases again (indicated by the ?Average? perfor-
mance in Fig. 3(b)). On an average, all the sys-
tems perform better with a threshold of 0.01, which
we use in our final experiments. In this case, both
Match(1) and Match(2)+Aug runs (orange and green
bars respectively) perform competitively so we use
both of these models in our final experiments.
In both the above experiments, the performance
bars are very similar when we use MI instead of
Yule and vice versa for weighting monolingual word
pairs. Thus, to illustrate the main ideas we chose
Yule?s ? for the former combination and MI for the
latter combination.
4.3.3 Promising Models
Based on the above experiments, we choose the
following combinations for our final experiments.
Yule(l)+Match(k), where l ? {2, 3} is the number
of Spanish words allowed for each English word
and vice versa and k=2 is the number of monolin-
gual word associations for each word. We also run
both these combinations with monolingual augmen-
tation, indicated by Yule(l)+Match(k)+Aug. For
dictionary based weighting, Dictionary+Match(k),
we choose a translation probability threshold of 0.01
and try k ? {1, 2}. Again, we run these combina-
tions with monolingual augmentation identified by
Dictionary+Match(k)+Aug.
4.4 Results
For our final results, we choose data in two language
pairs (English-Spanish and English-German) from
two different resources, Europarl (Koehn, 2005) and
Wikipedia. For Europarl data sets, we artificially
make them comparable by considering the first half
937
Wikipedia Europarl
English-Spanish English-German English-Spanish English-German
Acc. MRR Acc. MRR Acc. MRR Acc. MRR
CCA 0.776 0.852 0.570 0.699 0.872 0.920 0.748 0.831
OPCA 0.781 0.856 0.570 0.700 0.870 0.920 0.748 0.831
Yule(2)+Match(2) 0.798? 0.866? 0.576 0.703 0.901? 0.939? 0.780? 0.853?
Yule(2)+Match(2)+Aug 0.811? 0.876? 0.602? 0.723? 0.883 0.927 0.771? 0.847?
Yule(3)+Match(2) 0.803? 0.870? 0.572 0.700 0.856 0.907 0.747 0.830
Yule(3)+Match(2)+Aug 0.793? 0.861? 0.610? 0.726? 0.878+ 0.925+ 0.763+ 0.843?
Dictionary+Match(1) 0.811? 0.875? 0.656? 0.762? 0.928? 0.957? 0.874? 0.922?
Dictionary+Match(2) 0.811? 0.876? 0.623? 0.736? 0.923? 0.955? 0.853? 0.907?
Dictionary+Match(2)+Aug 0.825? 0.885? 0.630? 0.735? 0.897? 0.935? 0.866? 0.917?
Table 1: Performance of our models in comparison with CCA and OPCA on English-Spanish and English-German
language pairs. ? and + indicate statistical significance measured by paired t-test at p=0.01 and 0.05 levels respectively.
When an improvement is significant at p=0.01 it is automatically significant at p=0.05 and hence is not shown.
of English document and the second half of its
aligned foreign language document (Mimno et al,
2009). For Wikipedia data set, we use the cross-
language link as the ground truth. For each of these
data sets, we choose approximately 5000 aligned
document pairs. We remove the stop words and keep
all the words that occur in at least five documents.
After the preprocessing, on an average, we are left
with 4700 words in each language. Subsequently we
convert the documents into their TFIDF representa-
tion.
In Platt et al (2010), the authors compare differ-
ent systems on the comparable document retrieval
task and show that discriminative approaches work
better compared to their generative counter parts.
So, here we compare only with the state-of-the-
art discriminative systems such as CCA and OPCA
(Platt et al, 2010). For each of the systems, we re-
port the average results of five-fold cross validation.
We divide the data into 3:1:1 ratio for training, vali-
dation and test sets. The validation data set is used to
select the best number of dimensions of the common
sub space. For both CCA and our models, we set the
regularization parameter ? to 0.3 which we found
works well in a relevant but different experiments.
For OPCA, we manually tried different regulariza-
tion parameters ranging from 0.0001 to 1 and found
that a value of 0.001 worked best.
The results are shown in Table 1. On these data
sets, both CCA and OPCA performed competitively.
OPCA takes advantage of the common vocabulary
in both the languages. But in our data sets, vocab-
ulary of both the languages is treated differently, so
it is not surprising that they give almost the same
results. From the results, it is clear that sparsify-
ing the covariance matrices helps improving the ac-
curacies significantly. In all the four data sets, the
best performing method always used dictionary for
cross-lingual sparsity selection. This indicates that
using fine granular information such as a bilingual
dictionary gleaned from an external source is very
helpful in improving the accuracies. Among the
models that rely solely on the training data, models
that use monolingual augmentation performed bet-
ter on Wikipedia data set, while models that do not
use augmentation performed better on Europarl data
sets. This suggests that, when the aligned documents
are clean (closer to being parallel) the statistics com-
puted from cross-lingual corpora are trustworthy. As
the documents become comparable, we need to use
monolingual statistics to refine the bilingual statis-
tics. Moreover, these models achieve higher gains in
the case of Wikipedia data set compared to the gains
in Europarl. This conforms with our initial hunch
that, when the training data is clean the covariance
matrices tend to be less noisy.
5 Discussion
In this paper, we have proposed the idea of sparsi-
fyng covariance matrices to improve bilingual pro-
938
jection directions. We are not aware of any NLP
research that attempts to recover the sparseness of
the covariance matrices to improve the projection
directions. Our work is different from the sparse
CCA (Hardoon and Shawe-Taylor, 2011; Rai and
Daume? III, 2009) proposed in the Machine Learning
literature. Their objective is to find projection di-
rections such that the original documents are repre-
sented as a sparse vectors in the common sub-space.
Another seemingly relevant but different direction
is the sparse covariance matrix selection research
(Banerjee et al, 2005). The objective in this work
is to find matrices such that the inverse of the co-
variance matrix is sparse which has applications in
Gaussian processes.
In this paper, we tried sparsification in the con-
text of CCA only but our technique is general and
can be applied to its variants like OPCA. Our ex-
perimental results show that using external informa-
tion such as bilingual dictionaries which is gleaned
from cleaner resources brings significant improve-
ments. Moreover, we also observe that computing
word pair association measures from the same train-
ing data along with an appropriate selection criteria
can also yield significant improvements. This is cer-
tainly encouraging and in future we would like to
explore more sophisticated techniques to recover the
sparsity based on the training data itself.
6 Acknowledgments
We thank the anonymous reviewers for their help-
ful comments. This material is partially supported
by the National Science Foundation under Grant No.
1139909.
References
Francis R. Bach and Michael I. Jordan. 2005. A proba-
bilistic interpretation of canonical correlation analysis.
Technical report, Dept Statist Univ California Berke-
ley CA Tech.
Lisa Ballesteros and W. Bruce Croft. 1996. Dictio-
nary methods for cross-lingual information retrieval.
In Proceedings of the 7th International Conference
on Database and Expert Systems Applications, DEXA
?96, pages 791?801, London, UK. Springer-Verlag.
Onureena Banerjee, Alexandre d?Aspremont, and Lau-
rent El Ghaoui. 2005. Sparse covariance selection
via robust maximum likelihood estimation. CoRR,
abs/cs/0506023.
Nuria Bel, Cornelis H. A. Koster, and Marta Villegas.
2003. Cross-lingual text categorization.
Hal Daume III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 407?412, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Comput. Linguist.,
19(1):61?74, March.
William A. Gale and Kenneth W. Church. 1991. A pro-
gram for aligning sentences in bilingual corpora. In
Proceedings of the 29th annual meeting on Associ-
ation for Computational Linguistics, pages 177?184,
Morristown, NJ, USA. Association for Computational
Linguistics.
Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong.
2009. Exploiting bilingual information to improve
web search. In Proceedings of Human Language Tech-
nologies: The 2009 Conference of the Association for
Computational Linguistics, ACL-IJCNLP ?09, pages
1075?1083, Morristown, NJ, USA. ACL.
Aria Haghighi, Percy Liang, Taylor B. Kirkpatrick, and
Dan Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In Proceedings of ACL-08:
HLT, pages 771?779, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
David R. Hardoon and John Shawe-Taylor. 2011. Sparse
canonical correlation analysis. Journal of Machine
Learning, 83(3):331?353.
Ulf Hermjakob, Kevin Knight, and Hal Daume? III. 2008.
Name translation in statistical machine translation -
learning when to transliterate. In Proceedings of ACL-
08: HLT, pages 389?397, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Hung Huu Hoang, Su Nam Kim, and Min-Yen Kan.
2009. A Re-examination of Lexical Association Mea-
sures. In Proceedings of ACL-IJCNLP 2009 Workshop
on Multiword Expressions: Identification, Interpre-
tation, Disambiguation and Applications, Singapore,
August. Association for Computational Linguistics.
H. Hotelling. 1936. Relation between two sets of vari-
ables. Biometrica, 28:322?377.
Diana Zaiu Inkpen and Graeme Hirst. 2002. Ac-
quiring collocations for lexical choice between near-
synonyms. In Proceedings of the ACL-02 workshop
on Unsupervised lexical acquisition - Volume 9, ULA
?02, pages 67?76, Stroudsburg, PA, USA. Association
for Computational Linguistics.
939
Jagadeesh Jagarlamudi and Hal Daume? III. 2010. Ex-
tracting multilingual topics from unaligned compara-
ble corpora. In Advances in Information Retrieval,
32nd European Conference on IR Research, ECIR,
volume 5993, pages 444?456, Milton Keynes, UK.
Springer.
Jagadeesh Jagarlamudi, Hal Daume III, and Raghavendra
Udupa. 2011. From bilingual dictionaries to interlin-
gual document representations. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 147?152, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
R. Jonker and A. Volgenant. 1987. A shortest augment-
ing path algorithm for dense and sparse linear assign-
ment problems. Computing, 38(4):325?340.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 817?824, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT, AAMT.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 880?889, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Robert C. Moore. 2004. On Log-Likelihood-Ratios
and the Significance of Rare Events. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 333?340, Barcelona, Spain, July. Association
for Computational Linguistics.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Comput. Linguist., 31:477?
504, December.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
John C. Platt, Kristina Toutanova, and Wen-tau Yih.
2010. Translingual document representations from
discriminative projections. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 251?261,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Piyush Rai and Hal Daume? III. 2009. Multi-label pre-
diction via sparse infinite cca. In Advances in Neural
Information Processing Systems, Vancouver, Canada.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, ACL ?99, pages 519?526,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Sujith Ravi and Kevin Knight. 2009. Learning phoneme
mappings for transliteration without parallel data. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 37?45, Boulder, Colorado, June. Association for
Computational Linguistics.
K. Ahuja Ravindra, L. Magnanti Thomas, and B. Orlin
James. 1993. Network Flows: Theory, Algorithms,
and Applications. Prentice-Hall, Inc.
Harry T Reis and Charles M Judd. 2000. Handbook of
Research Methods in Social and Personality Psychol-
ogy. Cambridge University Press.
Alexander Schrijver. 2003. Combinatorial Optimization.
Springer.
Michael L. Littman Susan T. Dumais, Thomas K. Lan-
dauer. 1996. Automatic cross-linguistic information
retrieval using latent semantic indexing. In Working
Notes of the Workshop on Cross-Linguistic Informa-
tion Retrieval, SIGIR, pages 16?23, Zurich, Switzer-
land. ACM.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
J. Artif. Intell. Res. (JAIR), 37:141?188.
Alexei Vinokourov, John Shawe-taylor, and Nello Cris-
tianini. 2003. Inferring a semantic representation
of text via cross-language correlation analysis. In
Advances in Neural Information Processing Systems,
pages 1473?1480, Cambridge, MA. MIT Press.
Thuy Vu, AiTi Aw, and Min Zhang. 2009. Feature-based
method for document alignment in comparable news
corpora. In EACL, pages 843?851.
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010.
Cross-lingual latent topic extraction. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1128?1137, Up-
psala, Sweden, July. Association for Computational
Linguistics.
940
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 12?23, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Regularized Interlingual Projections:
Evaluation on Multilingual Transliteration
Jagadeesh Jagarlamudi
University of Maryland
College Park, USA, 20742
jags@umiacs.umd.edu
Hal Daum? III
University of Maryland
College Park, USA, 20742
hal@umiacs.umd.edu
Abstract
In this paper, we address the problem of build-
ing a multilingual transliteration system using
an interlingual representation. Our approach
uses international phonetic alphabet (IPA) to
learn the interlingual representation and thus
allows us to use any word and its IPA repre-
sentation as a training example. Thus, our ap-
proach requires only monolingual resources: a
phoneme dictionary that lists words and their
IPA representations.1 By adding a phoneme
dictionary of a new language, we can readily
build a transliteration system into any of the
existing previous languages, without the ex-
pense of all-pairs data or computation. We
also propose a regularization framework for
learning the interlingual representation, which
accounts for language specific phonemic vari-
ability, and thus it can find better mappings
between languages. Experimental results on
the name transliteration task in five diverse
languages show a maximum improvement of
29% accuracy and an average improvement of
17% accuracy compared to a state-of-the-art
baseline system.
1 Introduction
Because of the wide usage of English, many natu-
ral language processing (NLP) tasks have bilingual
resources from English into other languages. For ex-
ample, significantly larger parallel texts are available
1It is arguable that getting words and their IPA representa-
tion require knowledge about both words and IPA symbols, but
it still is specific to one language and, in this sense, we refer to
it as a monolingual resource.
between English and other languages. Similarly,
bilingual dictionaries and transliteration data sets are
more accessible from a language into English than
into a different language. This situation has caused
the NLP community to develop approaches which
use a resource rich language (Q say English) as pivot
to build resources/applications between a new lan-
guage pair P and R. Previous studies in machine
translation (Utiyama and Isahara, 2007; Paul and
Sumita, 2011), transliteration (Khapra et al2010),
and dictionary mining (Saralegi et al2011) show
that these bridge language approaches perform com-
petitively with approaches that use resources be-
tween P and R. In this paper, we propose a regular-
ization framework for bridge language approaches
and show its effectiveness for name transliteration
task. The key idea of our approach is that it accounts
for language specific variation in the bridge lan-
guage resources (i.e. between P ? Q and Q? R)
and aims to minimize this variation as much as pos-
sible. Though our technique is general, for clarity
we describe it in the context of named entity (NE)
transliteration.
Named entity (NE) transliteration involves
transliterating a name in one language into another
language and is shown to be crucial for machine
translation (MT) (Knight and Graehl, 1998; Al-
Onaizan and Knight, 2002; Hermjakob et al
2008; Li et al2009) and cross-lingual information
retrieval (CLIR) (AbdulJaleel and Larkey, 2003;
Mandl and Womser-Hacker, 2005; Udupa et al
2009). There exists a large body of literature in
transliteration, especially in the bilingual setting,
well summarized by Ravi and Knight (2009). We
12
English Bulgarian
Word IPA Word IPA
bashful /?b??f?l/ ????? /??ib?m/
tuesday /?tu?zde?/ ??? /luk/
craft /k??ft/ ??? /kak/
book /b?k/ ????? /m??zej/
head /h?d/ ????? /sp??k?/
Table 1: Example phoneme dictionaries in English and
Bulgarian. The English translations for the Bulgarian
words are switch, onion, how, museum, and spekle.
summarize the approaches that are most relevant to
us in Sec. 5. In this paper, we operate in the context
of transliteration mining (Klementiev and Roth,
2006; Sproat et al2006) where we assume that we
are given a source language name and a list of target
language candidate transliterations and the task is to
identify the correct transliteration.
Given a set of l languages, we address the prob-
lem of building a transliteration system between
every pair of languages. A straight forward su-
pervised learning approach would require training
data of name pairs between every pair of languages
(Knight and Graehl, 1998) or a set of common
names transliterated from every language into a
pivot language. Though it is relatively easy to ob-
tain names transliterated into a pivot language (such
as English), it is unlikely that such data sets contain
the same names. Bridge language approaches over-
come the need for common names and build translit-
eration systems for resource poor languages (Khapra
et al2010). However, such approaches still require
training data consisting of bilingual name translit-
erations (orthographic name-to-name mappings). In
this paper, we relax the need for name translitera-
tions by using international phonetic alphabet (IPA)
in a manner akin to a ?bridge language.?
2 IPA for Transliteration
We assume that we have a list of words and their
IPA representations in each of the l languages. The
words in different languages need not have any rela-
tionship to each other. Table 1 shows few words and
their IPA representations in English and Bulgarian
languages. We refer to the set of (word, IPA) pairs
as phoneme dictionary in this paper. Notice that the
common symbols in the IPA sequences indicate a
vague phonetic correspondence between the charac-
ter sequences of English and Bulgarian. For exam-
ple, both the words ?bashful? and ??????? have the
symbol ??? in their IPA sequences which indicate a
possible mapping between the character sequences
?sh? and ???.
The use of IPA as the bridge language offers mul-
tiple advantages. As shown in Table 1, it allows us
to include any (word, IPA) pair in the training data
and thus it relaxes the need for name pairs as the
training data. Since we only need a phoneme dic-
tionary in each language, our approach does not re-
quire any bilingual resources to build the transliter-
ation system. Moreover, since our training data can
contain any word (not only the NEs), it is easier to
obtain such a resource, for e.g. the phoneme dic-
tionaries obtained from Wiktionary contain at least
2000 words in 21 languages and we will see in Sec. 6
that we can build a decent transliteration systemwith
2000 words.2 Finally, unlike other transliteration ap-
proaches, by simply adding a phoneme dictionary of
(l+1)st language we can readily get a transliteration
system into any of the existing l languages and thus
avoid the need for all-pairs data or computation.
Using IPA as the bridge language poses some
new challenges such as the language specific phone-
mic inventory. For example, Mandarin doesn?t
have /v/, so it is frequently substituted with /w/ or
/f/. Similarly, !X?? (Southern Khoisan, spoken in
Botswana) has 122 consonants, mostly consisting
of a large inventory of different word-initial click
sounds (Haspelmath et al2005), many of which
do not exist in any other documented languages.
Besides this language specific phonemic inventory,
names have different IPA representations in differ-
ent languages. For example, as shown in Table 2,
the IPA sequences for ?China? in English and Dutch
have common IPA symbols but the English IPA se-
quence has additional symbols. Moreover, a name
can have multiple pronunciations with in a language,
e.g. ?France? has two different IPA sequences in En-
glish (Table 2).
In order to handle this phonemic diversity, our
method explicitly models language-specific variabil-
ity and attempts to minimize this phonemic variabil-
2In our experiments, we consider languages with small
(2000) and big (>30K) phoneme dictionaries.
13
Word IPA sequence
China /?t?a?.n?/ (En), /??ina/ (Du), /??i?na?/ (De)
America /??m?r?k?/ (En), /a?me.ri.ka/ (Ro)
France /?f???ns/ (En), /?f??nts/ (En), /f???s/ (Fr)
Table 2: IPA sequences of few words in different lan-
guages indicated using language codes in the parenthesis
(?En? for English, ?Du? for Dutch, ?De? for German, ?Ro?
for Romanian, and ?Fr? for French).
ity as much as possible. At a high level, our ap-
proach uses the phoneme dictionaries of each lan-
guage to learn mapping functions into an interlin-
gual representation (also referred as common sub-
space). Subsequently, given a pair of languages, a
query name in one of the languages and a list of
candidate transliterations in the other language, we
use the mapping functions of those two language to
identify the correct name transliteration. The map-
ping functions explicitly model the language specific
variability and thus account for fine grained differ-
ences. Our experimental results on four language
pairs from two different language families show a
maximum improvement of 29% accuracy and an av-
erage improvement of 17% accuracy compared to
a state-of-the-art baseline approach. An important
advantage of our approach is that, it extends eas-
ily to more than two languages and in fact adding
phoneme dictionary from a different, but related,
language improves the accuracies of a given lan-
guage pair. Our main contributions are: 1) build-
ing a transliteration system using (word, IPA) pairs
and hence using only monolingual resources and 2)
proposing a regularization framework which is more
general and applies to other bridge language applica-
tions such as lexicon mining (Mann and Yarowsky,
2001).
3 Low Dimensional Projections
Our approach is inspired by the Canonical Correla-
tion Analysis (CCA) (Hotelling, 1936) and its appli-
cation to transliteration mining (Udupa and Khapra,
2010).
First, we convert the phoneme dictionary of each
language into feature vectors, i.e. we convert each
word into a feature vector of n-gram character se-
quences and similarly, we also, convert the IPA
representations into feature vectors of n-gram IPA
symbol sequences. For example, if we use uni-
gram and bigram sequences as features, then the
feature vectors of ?head? and its IPA sequence
`h?d' are given by {h, e, a, d,#h, he, ea, ad, d$}
and {h,?, d,#h, h?,?d, d$}. For brevity, we refer
to the spaces of n-gram character and IPA symbol
sequences as character and phonemic spaces respec-
tively. The character space is specific to each lan-
guage while the phonemic space is shared across all
the languages. Since we use IPA as bridge, even
though two languages share orthography (e.g. En-
glish and French) it is irrelevant for our approach.
Then, for each language, we find mappings
(
Ai
and Ui
)
from the character and phonemic spaces
into a common k-dimensional subspace such that the
correct transliterations lie closer to each other in this
subspace. Before moving into the details of our ap-
proach, we will describe the notation and then give
an overview of the process by which our approach
finds the transliteration.
3.1 Notation
Let x(m)i ? Rdi and p
(m)
i ? Rc be the feature vec-
tors of the mth word and its IPA sequence in the
ith
(
1 ? ? ? l
)
language, where di is the size (i.e. no. of
features) of the character space of the language and
c is the size of the common phonemic space. Let
Xi (di?ni) and Pi (c?ni) denote the ith language
data matrices with x(m)i and p
(m)
i m = 1 ? ? ?ni as the
columns respectively. We consistently use subscript
to indicate the language and superscript to indicate
the index of an example point.
3.2 Method Overview
During the training stage, for each language, we find
mappings (or projection directions) Ai ? R(di?k)
and Ui ? R(c?k) from the character and phonemic
spaces into a k-dimensional subspace (or an interlin-
gual representation) such that a name gets mapped
to the same k-dimensional vector irrespective of the
language. That is, given a name xi it gets mapped
to the vector ATi xi and similarly its IPA sequence
pi gets mapped to UTi pi. During the testing stage,
given a name xi in the source (ith) language, we find
its transliteration in the target (jth) language xj by
solving the following decoding problem:
arg min
xj
L
(
xi, xj
)
(1)
14
Figure 1: A single name (Gandhi) is shown in all the in-
put feature spaces. The alignment between the character
and phonemic space is indicated with double dimensional
arrows. Bridge-CCA uses a single mapping function U
from the phonemic space into the common subspace (the
2-dimensional green space at the top), where as our ap-
proach uses two mapping functions U1 and U2, one for
each language, to map the IPA sequences into the com-
mon subspace.
where L
(
xi, xj
)
is given by
min
p?Rc
?ATi xi ? UTi p?2 + ?ATj xj ? UTj p?2 (2)
This formulation uses the source language mappings
(Ai and Ui) to find the IPA sequence p that is clos-
est to the source name and then uses it, along with
the target language mappings (Aj and Uj), to iden-
tify the correct transliteration from a list of candidate
transliterations.
At a high level, existing bridge language ap-
proaches such as Bridge-CCA (Khapra et al2010)
assume that Ui ? Uj thus ignoring the language
specific variation. To understand its implication
consider the example shown in Fig. 1. The mid-
dle portion of the Fig. shows the name Gandhi
(represented as point) in the character spaces of
English and Hindi, three-dimensional spaces, and
its IPA sequences in the phonemic space (the two-
dimensional space in the middle). Notice that, be-
cause of the phonemic variation, the same name is
represented by two distinct points in the common
phonemic space.3 Now, since Bridge-CCA uses a
single mapping function for both the IPA sequences,
it fails to map these two distinct points into a com-
mon point in the interlingual subspace.
Our new formulation, as explained above, relaxes
this hard constraint and learns different mapping
functions (Ui and Uj) and hence our approach can
potentially map both the distinct IPA sequences into
a single point. As a result our approach success-
fully handles the language specific phonemic vari-
ation. At the same time we constrain the projec-
tion directions such that they behave similarly for
the phonemic sounds that are observed in majority
of the languages. In the example shown in Fig. 1,
our model (called Regularized Projections) finds two
different mapping functions U1 and U2, one for each
language, from the phonemic space into the com-
mon two-dimensional space at the bottom.
3.3 Regularized Projections
In this section we first formulate the problem of find-
ing the mapping functions (Ai and Ui) of each lan-
guage as an optimization problem. In the following
section (Sec. 4), we develop a method for solving the
optimization problem and also derive closed form
solution for the prediction problem given in Eq. 1.
For simplicity, we describe our approach in terms of
single projection vectors, ai ? Rdi and ui ? Rc,
rather than full matrices, but the generalization is
trivial.
Inspired by the Canonical Correlation Analysis
(CCA) (Hotelling, 1936), we find projection direc-
tions in the character and phonemic spaces of each
language such that, after projection, a word is closer
to its aligned IPA sequence. To understand this, as-
sume that we have a name (say ?Barack Obama?) in
all the languages4 and its feature vectors are given
by xi and pi i = 1 ? ? ? l in the character and phone-
3In reality, as explained in the previous section, the phone-
mic variation that is commonly observed is that different fea-
tures are triggered for different languages. But for visualization
purpose, we showed the IPA sequences as if they differ in the
feature values.
4Our model does not require same names in different lan-
guages; this is used only for easier understanding.
15
mic spaces respectively. Then, we might try to find
projection directions ai in each language and u in
the common phonemic space such that:
arg min
ai,u
l
?
i=1
(
?xi, ai? ? ?pi,u?
)2
(3)
where ??, ?? denotes the dot product between two
vectors. This model assumes that the projection di-
rection u is same for the phonemic space of all the
languages. This is a hard constraint and does not
handle the language specific variability as discussed
in the previous section. We model the language
specificity by relaxing this hard constraint.
In our model, intuitively, the parameters corre-
sponding to the phonemic sounds that occur in ma-
jority of the languages are shared across the lan-
guages while the parameters of the language spe-
cific sounds are modeled per each language. This
is achieved by modeling the projection directions of
the ith language phonemic space ui ? u + ri. The
vector u ? Rc is common to the phonemic spaces
of all the languages and thus handles sounds that
are observed in multiple languages while ri ? Rc,
the residual vector, is specific to each language and
accounts for the language specific phonemic varia-
tions. Then the new formulation is given by:
arg min
ai,u,ri
l
?
i=1
??xi, ai? ? ?pi,u + ri??2 + ??pi, ri?2
where ? is the residual parameter. The first term of
this summation ensures that a word and its IPA se-
quence gets mapped to closer points in the subspace
while the second term forces the residual vectors to
be as small as possible. By enforcing the residual
vectors to be small, this formulation encourages the
sounds that occur in majority of the languages to be
accounted by u and the sounds that are specific to the
given language by ri. The final optimization prob-
lem is obtained by summing these terms over all the
examples and all the languages and is given by:
min
ai,u,ri
l
?
i=1
( ||XTi ai ? P Ti (u + ri)||2
ni
+ ? ?P Ti ri?2
)
(4)
s.t.
l
?
i=1
1
ni
?XTi ai?2 = 1 and
l
?
i=1
1
ni
?P Ti u?2 = 1
The constraints of the above optimization problem
avoid the trivial solution of setting all the vectors to
zero and are referred to as length constraints.
4 Model Optimization
In this section, we derive the solutions for the opti-
mization problems presented in the previous section.
4.1 Training the Model
We follow the standard procedure of forming the La-
grangian and setting its derivative to zero. The La-
grangian L of the optimization problem in Eq. 4 is
given by:
L =
?
i
1
ni
||XTi ai?P Ti (u+ri)||2+?
?
i
?P Ti ri?2
+?
(
?
i
1
ni
?XTi ai?2?1
)
+?
(
?
i
1
ni
?P Ti u?2?1
)
where ? and ? are Lagrangian multipliers corre-
sponding to the length constraints. Differentiating L
with respect to ai, ri and u and setting the derivatives
to zero yields the following equations, respectively:
(1 + ?)XiXTi ai ?XiP Ti ri = XiP Ti u
?PiXTi ai + (1 + ?ni)PiP Ti ri = ?PiP Ti u
?
i
1
ni
(
PiXTi ai?PiP Ti ri
)
= (1+?)
?
i
1
ni
PiP Ti u
We can rewrite these equations in matrix form, as
shown in Eqs. 5 and 6, since the solution becomes
clear in this form. For brevity, let Ei = (1 +
?)XiXTi , Fi = ?XiP Ti and Gi = (1 + ?ni)PiP Ti .
Then, u can be solved for using the generalized
eigenvalue problem shown in Eq. 7. This step in-
volves computing an inverse of a (di+c) matrix and
an eigenvalue problem of size c which can be ex-
pensive since solving each of these problems involve
cubic time. This can be reduced further into a prob-
lem of smaller size by using inverse of a partitioned
matrix as shown in Eq. 8. This identity reduces the
matrix inverse computation from a problem of size
di + c into two smaller problems of size di and c
each. This reduces the time complexity considerably
since the inverse computation is cubic in the size of
the matrix.
16
[(1 + ?)XiXTi ?XiP Ti
?PiXTi (1 + ?ni)PiP Ti
] [
ai
ri
]
=
[
XiP Ti
?PiP Ti
]
u (5)
?
i
1
ni
[
PiXTi ?PiP Ti
]
[
ai
ri
]
= (1 + ?)
?
i
1
ni
PiP Ti u (6)
?
i
1
ni
[
?F Ti
?Gi
1+?ni
]
[
Ei Fi
F Ti Gi
]?1 [ ?Fi
?Gi
1+?ni
]
u = (1 + ?)
?
i
1
ni
PiP Ti u (7)
If Mi =
(
Ei ? FiG?1i F
T
i
)?1, then
[
Ei Fi
F Ti Gi
]?1
=
[
Mi ?MiFiG?1i
?G?1i F Ti Mi G
?1
i + G
?1
i F Ti MiFiG
?1
i
]
(8)
Substituting Eq. 8 into Eq. 7 and further simplify-
ing results in the following eigenvalue problem for
solving u:
?
i
Gi + (?ni)2F Ti MiFi
ni(1 + ?ni)2
u = (1 + ?)
?
i
PiP Ti
ni
u
where Mi =
(
Ei ? FiG?1i F Ti
)?1. Notice that the
termEi = (1+?)XiXTi depends on the Lagrangian
multiplier ?. Because of this, we cannot solve for
both the parameters and the Lagrangian multipliers
at the same time. One possible approach is to do an
alternate optimization over the parameters and La-
grangian multipliers, but in this paper we fix ? and
solve for u. The value of ? denotes the correlation
and its maximum value is 1. In practice, we often
observe that the top few correlations take the value
of 1. Based on this observation we fix the value of ?
to 1 (Sec. 6).
Subsequently, we use u to solve for ai and ri as
follows:
ai = ?
?niMiFi
1 + ?ni
u (9)
ri =
?niG?1i F Ti MiFi ? I
1 + ?ni
u (10)
In order to increase the stability of the system we
regularizeGi andEi by adding ?I . We use the top k
eigenvectors u and their corresponding ai and ri vec-
tors as columns and form the mappingsU ,Ai andRi
respectively. These mappings are used in predicting
the transliteration of a name in one language into
any other language, which will be described in the
following section.
4.2 Transliteration Mining (Prediction)
During the testing phase, given a source name and
a list of candidate transliterations, we solve the de-
coding problem shown in Eq. 1 to find the appropri-
ate target language transliteration. Formally, given
a word xi in ith language we find its transliteration
into jth language xj , by solving the optimization
problem shown in Eq. 1, where Ui = U + Ri and
Uj = U + Rj . Similar to the previous case, the
closed form solution can be found by computing the
first derivative with respect to the unknown phoneme
sequence and the target language transliteration and
setting it to zero. First, the IPA sequence p? that
minimizes L
(
xi, xj
)
is given by:
p? = C?1ij
(
UiATi xi + UjATj xj
)
(11)
whereCij = UiUTi +UjUTj . We substitute this back
in Eq. 2 and then solve for xj , the best transliteration
in the jth language, as:
Aj
(
I?UTj C?1ij Uj
)
ATj xj = AjUTj C?1ij UiA
T
i xi (12)
Since Ui and Uj are not full rank matrices, to in-
crease the numerical stability of the prediction step,
we useCij = UiUTi +UjUTj +0.001 I where I is an
identity matrix. Notice that this solution doesn?t de-
pend on the p? and hence we don?t need to compute
it explicitly.
5 Related Work
There is a large body of the literature in named entity
transliteration, so we will describe only the most rel-
evant ones to our approach. In transliteration, gener-
ative approaches aim to generate the target language
17
transliteration of a given source name (Knight and
Graehl, 1998; Jung et al2000; Haizhou et al2004;
Al-Onaizan and Knight, 2002) while discriminative
approaches assume a list of target language names,
obtained from other sources, and try to identify the
correct transliteration (Klementiev and Roth, 2006;
Sproat et al2006). The effectiveness of the dis-
criminative approaches depend on the list of target
language candidates. Sproat et al2006) report an
oracle accuracy of 85%, but it depends on the source
of the candidate transliterations. Nevertheless, all
these approaches require either bilingual name pairs
or phoneme sequences to learn to transliterate be-
tween two languages. Thus, if we want to build
a transliteration system between every pair of lan-
guages in a given set of languages then these ap-
proaches need resources between every pair of lan-
guages which can be prohibitive.
Bridge language approaches propose an alterna-
tive and use a resource rich language such as English
as common language (Khapra et al2010) but they
still need bilingual resources. Moreover Bridge-
CCA (Khapra et al2010) uses a single mapping
function for the phonemic space of all the languages
and thus it can not handle language specific variabil-
ity. In the original setting, authors use English as the
pivot and since the feature space of English is fixed,
irrespective of the target language, this may not be a
serious concern but it becomes crucial when we use
IPA as the bridge language.
Approaches that map words in different languages
into the common phonemic space have also been
well studied. But most of these approaches use lan-
guage specific resources such as CMU pronuncia-
tion dictionary (Gao et al2004) or a carefully con-
structed cost matrices for addition, substitution, and
deletion of phonemes between a pair of languages
(Tao et al2006; Yoon et al2007). Variants of
soundex algorithm (Odel and Russel, 1918) such as
Kodex (Kang and Choi, 2000) use hand constructed
consonant to soundex code tables for name translit-
eration. Similar to our approach these variants only
require soundex mappings of a new language to
build transliteration system, but our model does not
require explicit mapping between n-gram characters
and the IPA symbols instead it learns them auto-
matically using phoneme dictionaries. Alternatively
unsupervised approaches have also been explored
(Ravi and Knight, 2009), but their accuracies are
fairly low compared to the supervised and weekly
supervised approaches.
6 Experiments
Our experiments are designed to evaluate the follow-
ing three aspects of our model, and of our approach
to transliteration in general:
IPA as bridge: Unlike other phonemic based ap-
proaches (Sec. 5), we do not explicitly model the
phoneme modifications between pairs of languages.
Moreover, the phoneme dictionary in each language
is crawled from Wiktionary (Sec. 6.1), which is
likely to be noisy. So, the first aspect we want
to evaluate is the effectiveness of using IPA as the
bridge language. Here, we also compare our method
with other bridge language approaches and establish
the importance of modeling language specific vari-
ance.
Multilinguality: In our method, simply adding a
phoneme dictionary of a new language allows us to
extend our transliteration system into any of the ex-
isting languages. We evaluate the effect of data from
a different, but related, languages on a transliteration
system between a given pair.
Complementarity: Using IPA as bridge language
allows us to build transliteration system into re-
source poor languages. But we also want to eval-
uate whether such an approach can help improving
a transliteration system trained directly on bilingual
name-pairs.
6.1 Data Sets
We use data sets from five languages in order to eval-
uate the effectiveness of our approach. The phoneme
dictionaries (list of words and their IPA represen-
tations as shown in Table 1) are obtained from
Wiktionary. The Wiktionary dump downloaded in
October 2011 has at least 2000 (word, IPA) pairs
in 21 languages which also includes some resource
poor languages (e.g. Armenian, Taiwanese, Turkish,
etc.).
In principle, our method allows us to build
transliteration system into any of these language
pairs without any additional information. But, in this
paper, we use English (En), Bulgarian (Bg), Rus-
sian (Ru), French (Fr), and Romanian (Ro) for eval-
18
En. Bg. Ru. Fr. Ro.
Train 31K 36K 1141 36K 5211
Dev. ? 1264 2000 2717 430
Test ? 1264 2000 2717 431
# Features 5000 3998 2900 5000 3465
Table 3: Statistics of different data sets. Training
data is monolingual phoneme dictionaries while develop-
ment/test sets are bilingual name pairs between English
and the respective language.
uation purposes, as they suffice to showcase all the
three aspects mentioned in the previous section. Ta-
ble 3 shows the sizes of phoneme dictionaries used
for training the models. The phoneme dictionar-
ies of English, Bulgarian, and Russian contain more
than 30K (word,IPA) pairs while the remaining two
languages have smaller phoneme dictionaries. The
development and test sets between English and the
remaining language pairs are obtained from geon-
ames data base.5 These are geographic location
names from different countries written in multiple
languages.
6.2 Experimental Setup
We convert the phoneme dictionaries of each lan-
guage into feature vectors. We use unigram and
bigram features in the phonemic space and uni-
gram, bigram and trigram features in the character
space. An example for feature generation is shown
in Sec. 3. After converting the data into feature vec-
tors, we retain the most frequent 5000 features. We
only keep the frequent 5000 features since we ob-
served, elsewhere, that including infrequent features
leads CCA based methods to learn projection direc-
tions with perfect correlations which are not effec-
tive for downstream applications. The last row of
Table 3 shows the number of features in the char-
acter space of each of the languages. The phone-
mic space is common to all the languages and has
3777 features. Though the phonemic features are
common to all the languages, as discussed in Sec. 2,
only a subset of features will be observed in a given
language. For example, in our data sets, of the total
3777 common phonetic features only 3312, 882, and
1009 features are observed in English, Bulgarian,
5http://www.geonames.org/
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  10  20  30  40  50  60  70  80  90  100lambda
Valid. Acc.Valid. MRR
Figure 2: Performance of transliteration system with
residual parameter ? on English-Bulgarian development
data set.
and Russian languages respectively. This indicates
the diversity in the phonemic inventory of different
languages.
We compare our approach against Bridge-CCA, a
state-of-the-art bridge language transliteration sys-
tem which is known to perform competitively with
other discriminative approaches (Khapra et al
2010). We use the phoneme dictionaries in each lan-
guage to train our approach, as well as the baseline
system. The projection directions learnt during the
training are used to find the transliteration for a test
name as described in Sec. 4.2. We report the perfor-
mance in terms of the accuracy (exact match) of the
top ranked transliteration and the mean reciprocal
rank (MRR) of the correct transliteration. We find
transliterations in both the directions (i.e. target lan-
guage transliterations given a source name and vice
versa) and report average accuracies. The regular-
ization parameter (? ) and the size of the interlingual
representation (k) in both our approach and Bridge-
CCA are tuned on the development set.
6.3 Description of Results
In this section we report experimental results on the
three aspects mentioned above.
6.3.1 IPA as Bridge
Fig. 2 shows the performance of our system with
the residual parameter ? (in Eq. 4) on the develop-
19
En-Bg En-Ru En-Fr En-Ro
Acc. MRR Acc. MRR Acc. MRR Acc. MRR
1 Bridge-CCA 68.83 77.22 44.50 53.22 41.55 52.89 71.69 79.59
2 Ours (cosine) 67.68 76.52 45.07 53.63 42.45 53.06 74.13 81.28
3 Ours (Eq. 12) 83.70 88.32 63.47 73.01 70.68 78.13 77.38 84.22
4 Ours (cosine + Multi.) 68.91 77.44 49.15 57.20 42.55 53.02 77.49 84.04
5 Ours (Eq. 12 + Multi.) 84.45 88.43 66.70 75.85 71.09 78.43 77.49 84.04
Table 4: Results of our approach and the baseline system on the test set. The second block shows the results when our
approach is trained only on phoneme dictionaries of the language pair, the third block shows results when we include
other language data as well.
ment data set. When ? is small, the model does not
attempt to constrain the projection directions Ui?s
and hence they tend to map names to completely
unrelated vectors. As we increase the residual pa-
rameter, it forces the residual vectors (Ri) to be
smaller and thus the subspaces identified for each
language are closely tied together. Thus, it models
the commonalities across languages and also the lan-
guage specific variability. Based on the performance
curves on the development data, we fix ? = 50 in the
rest of the experiments.
Table 4 shows the results of Bridge-CCA and our
approach on the four language pairs. We report the
results of our approach with the decoding proposed
in Sec. 4.2 and a simple cosine similarity measure
in the common-subspace, i.e. cos
(
ATi xi, ATj xj
)
.
Comparison of the accuracies in rows 1, 2 and 3,
shows that simply using cosine similarity performs
almost same as the Bridge-CCA approach. How-
ever, using the decoding suggested in Eq. 12 gives
significant improvements. To understand why the
cosine angle between ATi xi and ATj xj is not the ap-
propriate measure, assume that the vectors xi and
xj are feature vectors of same name in two lan-
guages and let p be its true IPA representation. Then,
since our model learns projection directions such
that ATi xi ? UTi p,
cos(ATi xi, ATj xj) = cos
(
(U+Ri)Tp, (U+Rj)Tp
)
The additional residual matrices Ri and Rj make
the cosine measure inappropriate. At the same time,
our model forces the residual matrices to be small
and this is probably the reason why it performs
competitively with the Bridge-CCA. On the other
hand, our decoding method, as shown in Eq. 1, in-
tegrates over the best possible phoneme sequence
and thus yields significant improvements. In the rest
of the paper, we report results with the decoding
in Eq. 12 unless specified explicitly. Our approach
achieves a maximum improvement of 29.13% ac-
curacy over Bridge-CCA in English-French and on
an average it achieves 17.17% and 15.19% improve-
ment in accuracy and MRR respectively. Notice that
even though our Russian phoneme dictionary has
only 1141 (word, IPA) pairs, our approach is able
to achieve an accuracy of 63.47% and an MRR of
73% indicating that the correct name transliteration
is, on an average, at rank 1 or 2.
6.3.2 Multilinguality
The fourth and fifth rows of Table 4 also show the
multilingual results. In particular, we train our sys-
tem on data from the three languages En, Bg, and
Ru and test it on En-Bg and En-Ru test sets. Simi-
larly, we train a different system on data from En, Fr
and Ro and evaluate it on En-Fr and En-Ro test sets.
We split the languages based on the language family,
Russian and Bulgarian are Slavonic languages while
French and Romanian are Romance languages, and
expect that languages in same family have similar
pronunciations. Comparing the performance of our
system with and without the multilingual data set, it
is clear that having data from other languages helps
improve the accuracy.
6.3.3 Complementarity
In the final experiment, we want to compare
the performance of our approach, which uses only
monolingual resources, with a transliteration system
trained using bilingual name pairs. We train a CCA
based transliteration system (Udupa and Khapra,
20
En-Bg En-Fr
Acc. MRR Acc. MRR
CCA 95.57 96.76 95.82 96.67
Ours+CCA 95.69 96.90 96.14 96.90
? Err 2.7% 4.2% 7.5% 6.8%
Ours+CCA(t) 95.80 96.95 96.34 97.04
? Err 5.4% 5.8% 12.3% 11.3%
Table 5: Comparison with a system trained on bilingual
name pairs. The (t) in the third row indicates parame-
ters are tuned for test set. We also show the percentage
error reduction achieved by a linear combination of our
approach and CCA.
2010) on a training data of 3792 and 8151 location
name pairs. Notice that the training and test data for
this system are from the same domain and thus it has
an additional advantage over our approach, which is
trained on whatever happens to be on Wiktionary.
The second row of Table 5 shows the results of
CCA on English-Bulgarian and English-French lan-
guage pairs. CCA achieves high accuracies even
though the training data is relatively small, most
likely because of the domain match between train-
ing and test data sets. As another baseline, we tried
using Google machine translation API to transliter-
ate the English names of the En-Bg test set. We
hoped that since these are names, the translation en-
gine would simply transliterate them and return the
result. Of the output, we observed that about 500
names are passed through the MT system unchanged
and so we ignore them. On the remaining names,
it achieved an accuracy of 76.15% and the average
string edit distance of the returned transliteration to
the true transliteration is about 3.74. These accura-
cies are not directly comparable to the results shown
in Table 5 because, presumably, it is a transliteration
generation system unlike CCA which is a transliter-
ation mining approach. For lack fair comparison, we
don?t report the accuracies of the Google transliter-
ation output in Table 5.
Table 5 also shows the results of our system when
combined with the CCA approach. For a given En-
glish word, we score the candidate transliterations
using our approach and then linearly combine their
scores with the scores assigned by CCA. We per-
form a line search between [0, 1] for the appropriate
weight combination. The third and fourth rows of
Table 5 show the results of the linear combination
when the weight is tuned for the development and
test sets respectively. The improvements, though
not significant, are encouraging and suggest that a
sophisticated way of combining these different sys-
tems may yield significant improvements. This ex-
periment shows that a transliteration system trained
on word and IPA representations can actually aug-
ment a system trained on bilingual name pairs lead-
ing to an improved performance.
7 Conclusion
In this paper we proposed a regularization technique
for the bridge language approaches and showed
its effectiveness on the name transliteration task.
Our approach learns interlingual representation us-
ing only monolingual resources and hence can be
used to build transliteration system between re-
source poor languages. We show that, by account-
ing the language specific phonemic variation, we
can get a significant improvements. Our experimen-
tal results suggest that a transliteration system built
using IPA data can also help improve the accuracy
of a transliteration system trained on bilingual name
pairs.
Thought we used IPA as a bridge language there
are other viable options. For example, as shown
in Khapra et al2010) we can use English as the
bridge language. Since name transliteration prob-
lem is being studied for a considerable time, many
resources already exist between English and other
languages. So, one can argue the appropriateness of
IPA as bridge language compared to, say, English.
While this is an important question, in this paper,
we are primarily interested in showing the impor-
tance of handling language specific phenomenon in
the bridge language approaches. In future, we would
like to study the appropriateness of IPA vs. English
as the bridge language and also the generalizability
of our technique to other scenarios.
Acknowledgements
This work is partially funded by NSF grant IIS-
1153487 and the BOLT program of the Defense
Advanced Research Projects Agency, Contract No.
HR0011-12-C-0015.
21
References
Nasreen AbdulJaleel and Leah S. Larkey. 2003. Statis-
tical transliteration for english-arabic cross language
information retrieval. In Proceedings of the twelfth in-
ternational conference on Information and knowledge
management, CIKM ?03, pages 139?146, New York,
NY, USA. ACM.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proceed-
ings of the ACL-02 workshop on Computational ap-
proaches to semitic languages, SEMITIC ?02, pages
1?13, Stroudsburg, PA, USA. ACL.
Wei Gao, Kam fai Wong, and Wai Lam. 2004. Phoneme-
based transliteration of foreign names for OOV prob-
lem. In Proceedings of the 1st International Joint Con-
ference on Natural Language Processing (IJCNLP),
pages 374?381.
Li Haizhou, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?04, Strouds-
burg, PA, USA. ACL.
Martin Haspelmath, Matthew Dryer, David Gil, and
Bernard Comrie, editors. 2005. The World Atlas of
Language Structures. Oxford University Press.
Ulf Hermjakob, Kevin Knight, and Hal Daum? III. 2008.
Name translation in statistical machine translation -
learning when to transliterate. In Proceedings of ACL-
08: HLT, pages 389?397, Columbus, Ohio, June.
ACL.
Harold Hotelling. 1936. Relation between two sets of
variables. Biometrica, 28:322?377.
Sung Young Jung, SungLim Hong, and Eunok Paek.
2000. An english to korean transliteration model of
extended markov window. In Proceedings of the 18th
conference on Computational linguistics - Volume 1,
COLING ?00, pages 383?389, Stroudsburg, PA, USA.
ACL.
Byung-Ju Kang and Key-Sun Choi. 2000. Two ap-
proaches for the resolution of word mismatch prob-
lem caused by english words and foreign words in ko-
rean information retrieval. In Proceedings of the 5th
international workshop on on Information retrieval
with Asian languages, IRAL ?00, pages 133?140, New
York, NY, USA. ACM.
Mitesh M. Khapra, Raghavendra Udupa, A. Kumaran,
and Pushpak Bhattacharyya. 2010. Pr + rq ? pq:
Transliteration mining using bridge language. In Pro-
ceedings of the Twenty-Fourth AAAI Conference on
Artificial Intelligence, AAAI 2010, Atlanta, Georgia,
USA, July. AAAI Press.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics, ACL-
44, pages 817?824, Stroudsburg, PA, USA. ACL.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
Haizhou Li, A. Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009. Report of news 2009 machine
transliteration shared task. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Transliter-
ation, NEWS ?09, pages 1?18, Stroudsburg, PA, USA.
ACL.
Thomas Mandl and Christa Womser-Hacker. 2005. The
effect of named entities on effectiveness in cross-
language information retrieval evaluation. In Proceed-
ings of the 2005 ACM symposium on Applied comput-
ing, SAC ?05, pages 1059?1064, New York, NY, USA.
ACM.
Gideon S. Mann and David Yarowsky. 2001. Multipath
translation lexicon induction via bridge languages. In
Proceedings of the 2nd meeting of the North American
Chapter of the Association for Computational Linguis-
tics on Language technologies, NAACL ?01, pages 1?
8, Stroudsburg, PA, USA. ACL.
M. K. Odel and R. C. Russel. 1918. U.s. patent numbers,
1,261,167 (1918) and 1,435,663(1922).
Michael Paul and Eiichiro Sumita. 2011. Translation
quality indicators for pivot-based statistical mt. In
Proceedings of 5th International Joint Conference on
Natural Language Processing, pages 811?818, Chiang
Mai, Thailand, November. AFNLP.
Sujith Ravi and Kevin Knight. 2009. Learning phoneme
mappings for transliteration without parallel data. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 37?45, Boulder, Colorado, June. ACL.
Xabier Saralegi, Iker Manterola, and I?aki San Vicente.
2011. Analyzing methods for improving precision of
pivot based bilingual dictionaries. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 846?856, Edinburgh,
Scotland, UK., July. ACL.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable corpora.
In Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
ACL-44, pages 73?80, Stroudsburg, PA, USA. ACL.
Tao Tao, Su-Youn Yoon, Andrew Fister, Richard Sproat,
and ChengXiang Zhai. 2006. Unsupervised named
22
entity transliteration using temporal and phonetic cor-
relation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?06, pages 250?257, Stroudsburg, PA, USA.
ACL.
Raghavendra Udupa and Mitesh M. Khapra. 2010.
Transliteration equivalence using canonical correlation
analysis. In ECIR?10, pages 75?86.
Raghavendra Udupa, Saravanan K, Anton Bakalov, and
Abhijit Bhole. 2009. "they are out there, if you know
where to look": Mining transliterations of oov query
terms for cross-language information retrieval. In Pro-
ceedings of the 31th European Conference on IR Re-
search on Advances in Information Retrieval, ECIR
?09, pages 437?448, Berlin, Heidelberg. Springer-
Verlag.
Masao Utiyama and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statisti-
cal machine translation. In Proceedings of Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 484?491, Rochester, New York, April.
ACL.
Su-Youn Yoon, Kyoung-Young Kim, and Richard Sproat.
2007. Multilingual transliteration using feature based
phonetic method. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 112?119, Prague, Czech Republic, June.
ACL.
23
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 204?213,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Incorporating Lexical Priors into Topic Models
Jagadeesh Jagarlamudi
University of Maryland
College Park, USA
jags@umiacs.umd.edu
Hal Daume? III
University of Maryland
College Park, USA
hal@umiacs.umd.edu
Raghavendra Udupa
Microsoft Research
Bangalore, India
raghavu@microsoft.com
Abstract
Topic models have great potential for help-
ing users understand document corpora.
This potential is stymied by their purely un-
supervised nature, which often leads to top-
ics that are neither entirely meaningful nor
effective in extrinsic tasks (Chang et al
2009). We propose a simple and effective
way to guide topic models to learn topics
of specific interest to a user. We achieve
this by providing sets of seed words that a
user believes are representative of the un-
derlying topics in a corpus. Our model
uses these seeds to improve both topic-
word distributions (by biasing topics to pro-
duce appropriate seed words) and to im-
prove document-topic distributions (by bi-
asing documents to select topics related to
the seed words they contain). Extrinsic
evaluation on a document clustering task
reveals a significant improvement when us-
ing seed information, even over other mod-
els that use seed information na??vely.
1 Introduction
Topic models such as Latent Dirichlet Allocation
(LDA) (Blei et al 2003) have emerged as a pow-
erful tool to analyze document collections in an
unsupervised fashion. When fit to a document
collection, topic models implicitly use document
level co-occurrence information to group seman-
tically related words into a single topic. Since the
objective of these models is to maximize the prob-
ability of the observed data, they have a tendency
to explain only the most obvious and superficial
aspects of a corpus. They effectively sacrifice per-
formance on rare topics to do a better job in mod-
eling frequently occurring words. The user is then
left with a skewed impression of the corpus, and
perhaps one that does not perform well in extrin-
sic tasks.
To illustrate this problem, we ran LDA on
the most frequent five categories of the Reuters-
21578 (Lewis et al 2004) text corpus. This doc-
ument distribution is very skewed: more than half
of the collection belongs to the most frequent cat-
egory (?Earn?). The five topics identified by the
LDA are shown in Table 1. A brief observation
of the topics reveals that LDA has roughly allo-
cated topics 1 & 2 for the most frequent class
(?Earn?) and one topic for the subsequent two
frequent classes (?Acquisition? and ?Forex?) and
merged the least two frequent classes (?Crude?
and ?Grain?) into a single topic. The red colored
words in topic 5 correspond to the ?Crude? class
and blue words are from the ?Grain? class.
This leads to the situation where the topics
identified by LDA are not in accordance with the
underlying topical structure of the corpus. This
is a problem not just with LDA: it is potentially
a problem with any extension thereof that have
focused on improving the semantic coherence of
the words in each topic (Griffiths et al 2005;
Wallach, 2005; Griffiths et al 2007), the doc-
ument topic distributions (Blei and McAuliffe,
2008; Lacoste-Julien et al 2008) or other aspects
(Blei. and Lafferty., 2009).
We address this problem by providing some ad-
ditional information to the model. Initially, along
with the document collection, a user may provide
higher level view of the document collection. For
instance, as discussed in Section 4.4, when run
on historical NIPS papers, LDA fails to find top-
ics related to Brain Imaging, Cognitive Science or
Hardware, even though we know from the call for
204
mln, dlrs, billion, year, pct, company, share, april, record, cts, quarter, march, earnings, stg, first, pay
mln, NUM, cts, loss, net, dlrs, shr, profit, revs, year, note, oper, avg, shrs, sales, includes
lt, company, shares, corp, dlrs, stock, offer, group, share, common, board, acquisition, shareholders
bank, market, dollar, pct, exchange, foreign, trade, rate, banks, japan, yen, government, rates, today
oil, tonnes, prices, mln, wheat, production, pct, gas, year, grain, crude, price, corn, dlrs, bpd, opec
Table 1: Topics identified by LDA on the frequent-5 categories of the Reuters corpus. The categories are Earn,
Acquisition, Forex, Grain and Crude (in the order document frequency).
1 company, billion, quarter, shrs, earnings
2 acquisition, procurement, merge
3 exchange, currency, trading, rate, euro
4 grain, wheat, corn, oilseed, oil
5 natural, gas, oil, fuel, products, petrol
Table 2: An example for sets of seed words (seed top-
ics) for the frequent-5 categories of the Reuters-21578
categorization corpus. We use them as running exam-
ple in the rest of the paper.
papers that such topics should exist in the corpus.
By allowing the user to provide some seed words
related to these underrepresented topics, we en-
courage the model to find evidence of these top-
ics in the data. Importantly, we only encourage
the model to follow the seed sets and do not force
it. So if it has compelling evidence in the data
to overcome the seed information then it still has
the freedom to do so. Our seeding approach in
combination with the interactive topic modeling
(Hu et al 2011) will allow a user to both explore
a corpus, and also guide the exploration towards
the distinctions that he/she finds more interesting.
2 Incorporating Seeds
Our approach to allowing a user to guide the topic
discovery process is to let him provide seed infor-
mation at the level of word type. Namely, the user
provides sets of seed words that are representative
of the corpus. Table 2 shows an example of seed
sets one might use for the Reuters corpus. This
kind of supervision is similar to the seeding in
bootstrapping literature (Thelen and Riloff, 2002)
or prototype-based learning (Haghighi and Klein,
2006). Our reliance on seed sets is orthogonal
to existing approaches that use external knowl-
edge, which operate at the level of documents
(Blei and McAuliffe, 2008), tokens (Andrzejew-
ski and Zhu, 2009) or pair-wise constraints (An-
drzejewski et al 2009).
We build a model that uses the seed words
in two ways: to improve both topic-word and
document-topic probability distributions. For
ease of exposition, we present these ideas sep-
arately and then in combination (Section 2.3).
To improve topic-word distributions, we set up
a model in which each topic prefers to gener-
ate words that are related to the words in a seed
set (Section 2.1). To improve document-topic
distributions, we encourage the model to select
document-level topics based on the existence of
input seed words in that document (Section 2.2).
Before moving on to the details of our mod-
els, we briefly recall the generative story of the
LDA model and the reader is encouraged to refer
to (Blei et al 2003) for further details.
1. For each topic k = 1 ? ? ? T,
? choose ?k ? Dir(?).
2. For each document d, choose ?d ? Dir(?).
? For each token i = 1 ? ? ?Nd:
(a) Select a topic zi ? Mult(?d).
(b) Select a word wi ? Mult(?zi).
where T is the number of topics, ?, ? are hyper-
parameters of the model and ?k and ?d are topic-
word and document-topic Multinomial probabil-
ity distributions respectively.
2.1 Word-Topic Distributions (Model 1)
In regular topic models, each topic k is defined
by a Multinomial distribution ?k over words. We
extend this notion and instead define a topic as a
mixture of two Multinomial distributions: a ?seed
topic? distribution and a ?regular topic? distribu-
tion. The seed topic distribution is constrained to
only generate words from a corresponding seed
set. The regular topic distribution may generate
any word (including seed words). For example,
seed topic 4 (in Table 2) can only generate the
five words in its set. The word ?oil? can be gener-
ated by seed topics 4 and 5, as well as any regular
205
?sT?rT?s1?r1
doc
z=1 z=2 z=T? ? ? ? ? ? ? ? ?
?11 ? ?1 ?T1? ?T
Figure 1: Tree representation of a document in Model
1.
topic. We want to emphasize that, like any regular
topic, each seed topic is a non-uniform probabil-
ity distribution over the words in its set. The user
only inputs the sets of seed words and the model
will infer their probability distributions.
For the sake of simplicity, we describe our
model by assuming a one-to-one correspondence
between seed and regular topics. This assumption
can be easily relaxed by duplicating the seed top-
ics when there are more regular topics. As shown
in Fig. 1, each document is a mixture over T top-
ics, where each of those topics is a mixture of
a regular topic (?r? ) and its associated seed topic
(?s? ) distributions. The parameter ?k controls the
probability of drawing a word from the seed topic
distribution versus the regular topic distribution.
For our first model, we assume that the corpus is
generated based on the following generative pro-
cess (its graphical notation is shown in Fig. 2(a)):
1. For each topic k=1? ? ? T,
(a) Choose regular topic ?rk ? Dir(?r).
(b) Choose seed topic ?sk ? Dir(?s).
(c) Choose ?k ? Beta(1, 1).
2. For each document d, choose ?d ? Dir(?).
? For each token i = 1 ? ? ?Nd:
(a) Select a topic zi ? Mult(?d).
(b) Select an indicator xi ? Bern(?zi)
(c) if xi is 0
? Select a word wi ? Mult(?rzi).
// choose from regular topic
(d) if xi is 1
? Select a word wi ? Mult(?szi).
// choose from seed topic
The first step is to generate Multinomial distribu-
tions for both seed topics and regular topics. The
seed topics are drawn in a way that constrains
their distribution to only generate words in the
corresponding seed set. Then, for each token in a
document, we first generate a topic. After choos-
ing a topic, we flip a (biased) coin to pick either
the seed or the regular topic distribution. Once
this distribution is selected we generate a word
from it. It is important to note that although there
are 2?T topic-word distributions in total, each
document is still a mixture of only T topics (as
shown in Fig. 1). This is crucial in relating seed
and regular topics and is similar to the way top-
ics and aspects are tied in TAM model (Paul and
Girju, 2010).
To understand how this model gathers words
related to seed words, consider a seed topic (say
the fourth row in Table 2) with seed words {grain,
wheat, corn, etc. }. Now by assigning all the re-
lated words such as ?tonnes?, ?agriculture?, ?pro-
duction? etc. to its corresponding regular topic,
the model can potentially put high probability
mass on topic z = 4 for agriculture related doc-
uments. Instead, if it places these words in an-
other regular topic, say z = 3, then the document
probability mass has to be distributed among top-
ics 3 and 4 and as a result the model will pay a
steeper penalty. Thus the model uses seed topic
to gather related words into its associated regu-
lar topic and as a consequence the document-topic
distributions also become focussed.
We have experimented with two ways of choos-
ing the binary variable xi (step 2b) of the gener-
ative story. In the first method, we fix this sam-
pling probability to a constant value which is in-
dependent of the chosen topic (i.e. ?i = ??, ?i =
1 ? ? ? T). And in the second method we learn the
probability as well (Sec. 4).
2.2 Document-Topic distributions (Model 2)
In the previous model we used seed words to im-
prove topic-word probability distributions. Here
we propose a model to explore the use of seed
words to improve document-topic probability dis-
tributions. Unlike the previous model, we will
present this model in the general case where the
number of seed topics is not equal to the number
of regular topics. Hence, we associate each seed
set (we refer seed set as group for conciseness)
with a Multinomial distribution over the regular
topics which we call group-topic distribution.
To give an overview of our model, first, we
transfer the seed information from words onto
206
DT
? ?
?r ?r
?s
Nd
x z
w
(a) Model 1
DT
?
?
? ?
?r
~b
?r Nd
?
?
z
w
g
(b) Model 2
DT
?
?
? ?
?r
~b
?r
?s
Nd
?
?
x z
w
g
(c) SeededLDA
Figure 2: The graphical notation of all the three models. In Model 1 we use seed topics to improve the topic-word
probability distributions. In Model 2, the seed topic information is first transfered to the document level based
on the document tokens and then it is used to improve document-topic distributions. In the final, SeededLDA,
model we combine both the models. In Model 1 and SeededLDA, we dropped the dependency of ?s on hyper
parameter ?s since it is observed. And, for clarity, we also dropped the dependency of x on ?.
the documents that contain them. Then, the
document-topic distribution is drawn in a two step
process: we sample a seed set (g for group) and
then use its group-topic distribution (?g) as prior
to draw the document-topic distribution (?d). We
used this two step process, to allow flexible num-
ber of seed and regular topics, and to tie the topic
distributions of all the documents within a group.
We assume the following generative story and its
graphical notation is shown in Fig. 2(b).
1. For each k = 1? ? ? T,
(a) Choose ?rk ? Dir(?r).
2. For each seed set s = 1? ? ? S,
(a) Choose group-topic distribution ?s ?
Dir(?). // the topic distribution for sth
group (seed set) ? a vector of length T.
3. For each document d,
(a) Choose a binary vector~b of length S.
(b) Choose a document-group distribution
?d ? Dir(?~b).
(c) Choose a group variable g ? Mult(?d)
(d) Choose ?d ? Dir(?g). // of length T
(e) For each token i = 1 ? ? ?Nd:
i. Select a topic zi ? Mult(?d).
ii. Select a word wi ? Mult(?rzi).
We first generate T topic-word distributions
(?k) and S group-topic distributions (?s). Then
for each document, we generate a list of seed sets
that are allowed for this document. This list is
represented using the binary vector ~b. This bi-
nary vector can be populated based on the docu-
ment words and hence it is treated as an observed
variable. For example, consider the (very short!)
document ?oil companies have merged?. Accord-
ing to the seed sets from Table 2, we define a bi-
nary vector that denotes which seed topics contain
words in this document. In this case, this vec-
tor ~b = ?1, 1, 0, 1, 1?, indicating the presence of
seeds from sets 1, 2, 4 and 5.1 As discussed in
(Williamson et al 2010), generating binary vec-
tor is crucial if we want a document to talk about
topics that are less prominent in the corpus.
The binary vector ~b, that indicates which seeds
exist in this document, defines a mean of a
Dirichlet distribution from which we sample a
document-group distribution, ?d (step 3b). We
set the concentration of this Dirichlet to a hy-
perparamter ? , which we set by hand (Sec. 4);
thus, ?d ? Dir(?~b). From the resulting multino-
mial, we draw a group variable g for this docu-
ment. This group variable brings clustering struc-
ture among the documents by grouping the docu-
ments that are likely to talk about same seed set.
Once the group variable (g) is drawn, we
choose the document-topic distribution (?d) from
a Dirichlet distribution with the group?s-topic dis-
tribution as the prior (step 3d). This step ensures
that the topic distributions of documents within
each group are related. The remaining sampling
1As a special case, if no seed word is found in the docu-
ment,~b is defined as the all-ones vector.
207
process proceeds like LDA. We sample a topic
for each word and then generate a word from its
corresponding topic-word distribution. Observe
that, if the binary vector is all ones and if we
set ?d = ?d then this model reduces to the LDA
model with ? and ?r as the hyperparameters.
2.3 SeededLDA
Both of our models use seed words in different
ways to improve topic-word and document-topic
distributions respectively. We can combine both
the above models easily. We refer to the combined
model as SeededLDA and its generative story is
as follows (its graphical notation is shown in Fig.
2(c)). The variables have same semantics as in the
previous models.
1. For each k=1? ? ? T,
(a) Choose regular topic ?rk ? Dir(?r).
(b) Choose seed topic ?sk ? Dir(?s).
(c) Choose ?k ? Beta(1, 1).
2. For each seed set s = 1? ? ? S,
(a) Choose group-topic distribution ?s ?
Dir(?).
3. For each document d,
(a) Choose a binary vector~b of length S.
(b) Choose a document-group distribution
?d ? Dir(?~b).
(c) Choose a group variable g ? Mult(?d).
(d) Choose ?d ? Dir(?g). // of length T
(e) For each token i = 1 ? ? ?Nd:
i. Select a topic zi ? Mult(?d).
ii. Select an indicator xi ? Bern(?zi).
iii. if xi is 0
? Select a word wi ? Mult(?rzi).
iv. if xi is 1
? Select a word wi ? Mult(?szi).
In the SeededLDA model, the process for gen-
erating group variable of a document is same as
the one described in the Model 2. And like in the
Model 2, we sample a document-topic probability
distribution as a Dirichlet draw with the group-
topic distribution of the chosen group as prior.
Subsequently, we choose a topic for each token
and then flip a biased coin. We choose either the
seed or the regular topic based on the result of the
coin toss and then generate a word from its distri-
bution.
2.4 Automatic Seed Selection
In (Andrzejewski and Zhu, 2009; Andrzejewski
et al 2009), the seed information is provided
manually. Here, we describe the use of feature se-
lection techniques, prevalent in the classification
literature, to automatically derive the seed sets. If
we want the topicality structure identified by the
LDA to align with the underlying class structure,
then the seed words need to be representative of
the underlying topicality structure. To enable this,
we first take class labeled data (doesn?t need to
be multi-class labeled data unlike (Ramage et al
2009)) and identify the discriminating features for
each class. Then we choose these discriminating
features as the initial sets of seed words. In prin-
ciple, this is similar to the prototype driven unsu-
pervised learning (Haghighi and Klein, 2006).
We use Information Gain (Mitchell, 1997) to
identify the required discriminating features. The
Information Gain (IG) of a word (w) in a class (c)
is given by
IG(c, w) = H(c) ?H(c|w)
whereH(c) is the entropy of the class andH(c|w)
is the conditional entropy of the class given the
word. In computing Information Gain, we bina-
rize the document vectors and consider whether a
word occurs in any document of a given class or
not. Thus obtained ranked list of words for each
class are filtered for ambiguous words and then
used as initial sets of seed words to be input to the
model.
3 Related Work
Seed-based supervision is closely related to the
idea of seeding in the bootstrapping literature for
learning semantic lexicons (Thelen and Riloff,
2002). The goals are similar as well: growing
a small set of seed examples into a much larger
set. A key difference is the type of semantic in-
formation that the two approaches aim to capture:
semantic lexicons are based on much more spe-
cific notions of semantics (e.g. all the country
names) than the generic ?topic? semantics of topic
models. The idea of seeding has also been used
in prototype-driven learning (Haghighi and Klein,
2006) and shown similar efficacies for these semi-
supervised learning approaches.
LDAWN (Boyd-Graber et al 2007) models
sets of words for the word sense disambiguation
208
task. It assumes that a topic is a distribution
over synsets and relies on the Wordnet to obtain
the synsets. The most related prior work is that
of (Andrzejewski et al 2009), who propose the
use Dirichlet Forest priors to incorporate Must
Link and Cannot Link constraints into the topic
models. This work is analogous to constrained
K-means clustering (Wagstaff et al 2001; Basu
et al 2008). A must link between a pair word
types represents that the model should encourage
both the words to have either high or low prob-
ability in any particular topic. A cannot link be-
tween a word pair indicates both the words should
not have high probability in a single topic. In the
Dirichlet Forest approach, the constraints are first
converted into trees with words as the leaves and
edges having pre-defined weights. All the trees
are joined to a dummy node to form a forest. The
sampling for a word translates into a random walk
on the forest: starting from the root and selecting
one of its children based on the edge weights until
you reach a leaf node.
While the Dirichlet Forest method requires su-
pervision in terms of Must link and Cannot link
information, the Topics In Sets (Andrzejewski and
Zhu, 2009) model proposes a different approach.
Here, the supervision is provided at the token
level. The user chooses specific tokens and re-
strict them to occur only with in a specified list of
topics. While this needs minimal changes to the
inference process of LDA, it requires information
at the level of tokens. The word type level seed
information can be converted into token level in-
formation (like we do in Sec. 4) but this prevents
their model from distinguishing the tokens based
on the word senses.
Several models have been proposed which use
supervision at the document level. Supervised
LDA (Blei and McAuliffe, 2008) and DiscLDA
(Lacoste-Julien et al 2008) try to predict the cat-
egory labels (e.g. sentiment classification) for
the input documents based on a document labeled
data. Of these models, the most related one to
SeededLDA is the LabeledLDA model (Ramage
et al 2009). Their model operates on multi-class
labeled corpus. Each document is assumed to be
a mixture over a known subset of topics (classes)
with each topic being a distribution over words.
The process of generating document topic distri-
bution in LabeledLDA is similar to the process
of generating group distribution in our Model 2
(Sec. 2.2). However our model differs from La-
beledLDA in the subsequent steps. Rather than
using the group distribution directly, we sam-
ple a group variable and use it to constrain the
document-topic distributions of all the documents
within this group. Moreover, in their model the
binary vector is observed directly in the form of
document labels while, in our case, it is automat-
ically populated based on the document tokens.
Interactive topic modeling brings the user into
the loop, by allowing him/her to make suggestions
on how to improve the quality of the topics at each
iteration (Hu et al 2011). In their approach, the
authors use Dirichlet Forest method to incorpo-
rate the user?s preferences. In our experiments
(Sec. 4), we show that SeededLDA performs bet-
ter than Dirichlet Forest method, so SeededLDA
when used with their framework can allow an user
to explore a document collection in a more mean-
ingful manner.
4 Experiments
We evaluate different aspects of the model sep-
arately. Our experimental setup proceeds as fol-
lows: a) Using an existing model, we evaluate the
effectiveness of automatically derived constraints
indicating the potential benefits of adding seed
words into the topic models. b) We evaluate each
of our proposed models in different settings and
compare with multiple baseline systems.
Since our aim is to overcome the domi-
nance of majority topics by encouraging the
topicality structure identified by the topic mod-
els to align with that of the document cor-
pus, we choose extrinsic evaluation as the
primary evaluation method. We use docu-
ment clustering task and use frequent-5 cate-
gories of Reuters-21578 corpus (Lewis et al
2004) and four classes from the 20 News-
groups data set (i.e.?rec.autos?, ?sci.electronics?,
?comp.hardware? and ?alt.atheism?). For both
the corpora we do the standard preprocessing
of removing stopwords and infrequent words
(Williamson et al 2010).
For all the models, we use a Collapsed Gibbs
sampler (Griffiths and Steyvers, 2004) for the in-
ference process. We use the standard hyperparam-
eters values ? = 1.0, ? = 0.01 and ? = 1.0 and
run the sampler for 1000 iterations, but one can
use techniques like slice sampling to estimate the
hyperparameters (Johnson and Goldwater, 2009).
209
Reuters 20 Newsgroups
F-measure VI F-measure VI
LDA 0.64 (?.05) 1.26 (?.16) 0.77 (?.06) 0.9 (?.13)
Dirichlet Forest 0.67? (?.02) 1.17 (?.11) 0.79(?.01) 0.83?(?.03)
? over LDA (+4.68%) (-7.1%) (+2.6%) (-7.8%)
Table 3: The effect of adding constraints by Dirichlet Forest Encoding. For Variational Information (VI) a lower
score indicates a better clustering. ? indicates statistical significance at p = 0.01 as measured by the t-test. All
the four improvements are significant at p = 0.05.
We run all the models with the same number of
topics as the number of clusters. Then, for each
document, we find the topic that has maximum
probability in the posterior document-topic distri-
bution and assign it to that cluster. The accuracy
of the document clustering is measured in terms
of F-measure and Variation of Information. F-
measure is calculated based on the pairs of doc-
uments, i.e. if two documents belong to a cluster
in both ground truth and the clustering proposed
by the system then it is counted as correct, other-
wise it is counted as wrong. Variational Informa-
tion (VI) of two clusterings X and Y is given as
(Meila?, 2007):
VI(X,Y ) = H(X) +H(Y ) ? 2I(X,Y )
whereH(X) denotes the entropy of the clustering
X and I(X,Y ) denotes the mutual information
between the two clusterings. For VI, a lower value
indicates a better clustering. All the accuracies are
averaged over 25 different random initializations
and all the significance results are measured using
the t-test at p = 0.01.
4.1 Seed Extraction
The seeds were extracted automatically (Sec. 2.4)
based on a small sample of labeled data other than
the test data. We first extract 25 seeds words per
each class and then remove the seed words that
appear in more than one class. After this filtering,
on an average, we are left with 9 and 15 words per
each seed topic for Reuters and 20 Newsgroups
corpora respectively.
We use the existing Dirichlet Forest method to
evaluate the effectiveness of the automatically ex-
tracted seed words. The Must and Cannot links
required for the supervision (Andrzejewski et al
2009) are automatically obtained by adding a
must-link between every pair of words belonging
to the same seed set and a split constraint between
every pair of words belonging to different sets.
The accuracies are averaged over 25 different ran-
dom initializations and are shown in Table 3. We
have also indicated the relative performance gains
compared to LDA. The significant improvement
over the plain LDA demonstrates the effectiveness
of the automatic extraction of seed words in topic
models.
4.2 Document Clustering
In the next experiment, we compare our models
with LDA and other baselines. The first baseline
(maxCluster) simply counts the number of tokens
in each document from each of the seed topics and
assigns the document to the seed topic that has
most tokens. This results in a clustering of doc-
uments based on the seed topic they are assigned
to. This baseline evaluates the effectiveness of the
seed words with respect to the underlying cluster-
ing. Apart from the maxCluster baseline, we use
LDA and z-labels (Andrzejewski and Zhu, 2009)
as our baselines. For z-labels, we treat all the to-
kens of a seed word in the same way. Table 4
shows the comparison of our models with respect
to the baseline systems.2 Comparing the perfor-
mance of maxCluster to that of LDA, we observe
that the seed words themselves do a poor job in
clustering the documents.
We experimented with two variants of Model 1.
In the first run (Model 1) we sample the ?k value,
i.e. the probability of choosing a seed topic for
each topic. While in the ?Model 1 (?? = 0.7)? run,
we fix this probability to a constant value of 0.7 ir-
respective of the topic.3 Though both the models
2The code used for LDA baseline in Tables 3 and 4
are different. For Table 3, we use the code available from
http://pages.cs.wisc.edu/?andrzeje/research/df lda.html.
We use our own version for Table 4. We tried to produce
a comparable baseline by running the former for more
iterations and with different hyperparameters. In Table 3,
we report their best results.
3We chose this value based on intuition; it is not tuned.
210
Reuters 20 Newsgroups
F-measure VI F-measure VI
maxCluster 0.53 1.75 0.58 1.44
LDA 0.66 (?.04) 1.2 (?.12) 0.76 (?.06) 0.9 (?.14)
z-labels 0.73 (?.01) 1.04 (?.01) 0.8 (?.00) 0.82 (?.01)
? over LDA (+10.6%) (-13.3%) (+5.26%) (-8.8%)
Model 1 0.69 (?.00) 1.13 (?.01) 0.8 (?.01) 0.81 (?.02)
Model 1 (?? = 0.7) 0.73 (?.00) 1.09 (?.01) 0.8 (?.01) 0.81 (?.02)
Model 2 0.66 (?.04) 1.22 (?.1) 0.77 (?.07) 0.85 (?.12)
SeededLDA 0.76? (?.01) 0.99? (?.03) 0.81? (?.01) 0.75? (?.02)
? over LDA (+15.5%) (-17.5%) (+6.58%) (-16.7%)
Table 4: Accuracies on document clustering task with different models. ? indicates significant improvement
compared to the z-labels approach, as measured by the t-test with p = 0.01. The relative performance gains are
with respect to the LDA model and are provided for comparison with Dirichlet Forest method (in Table 3.)
performed better than LDA, fixing the probabil-
ity gave better results. When we attempt to learn
this value, the model chooses to explain some of
the seed words by the regular topics. On the other
hand, when ? is fixed, it explains almost all the
seed words based on the seed topics. The next
row (Model 2) indicates the performance of our
second model on the same data sets. The first
model seems to be performing better than the sec-
ond model, which is justifiable since the latter
uses seed topics indirectly. Though the variants
of Model 1 and Model 2 performed better than
the LDA, they fell short of the z-labels approach.
Table 4 also shows the performance of our com-
bined model (SeededLDA) on both the corpora.
When the models are combined, the performance
improves over each of them and is also better than
the baseline systems. As explained before, our in-
dividual models improve both the topic-word and
document-topic distributions respectively. But it
turns out that the knowledge learnt by both the in-
dividual models is complementary to each other.
As a result the combined model performed better
than the individual models and other baseline sys-
tems. Comparing the last rows of Tables 4 and 3,
we notice that the relative performance gains ob-
served in the case of SeededLDA is significantly
higher than the performance gains obtained by
incorporating the constraints using the Dirichlet
Forest method. Moreover, as indicated in the Ta-
ble 4, SeededLDA achieves significant gains over
the z-labels approach as well.
We have also provided the standard intervals
for each of the approaches. A quick inspection of
these intervals reveals the superior performance
of SeededLDA compared to all the baselines. The
standard deviation of the F-measures over dif-
ferent random initializations of our our model is
about 1% for both the corpora while it is 4% and
6% for the LDA on Reuters and 20 Newsgroups
corpora respectively. The reduction in the vari-
ance, across all the approaches that use seed infor-
mation, shows the increased robustness of the in-
ference process when using seed words. From the
accuracies in both the tables, it is clear that Seed-
edLDA model out-performs other models which
try to incorporate seed information into the topic
models.
4.3 Effect of Ambiguous Seeds
In the following experiment we study the effect
of ambiguous seeds. We allow a seed word to oc-
cur in multiple seed sets. Table 6 shows the cor-
responding results. The performance drops when
we add ambiguous seed words, but it is still higher
than that of the LDA model. This suggests that the
quality of the seed topics is determined by the dis-
criminative power of the seed words rather than
the number of seed words in each seed topic. The
topics identified by the SeededLDA on Reuters
corpus are shown in the Table 5. With the help of
the seed sets, the model is able to split the ?Grain?
and ?Crude? into two separate topics which were
merged into a single topic by the plain LDA.
4.4 Qualitative Evaluation on NIPS papers
We ran LDA and SeededLDA models on the NIPS
papers from 2001 to 2010. For this corpus, the
seed words are chosen from the call for proposal.
211
group, offer, common, cash, agreement, shareholders, acquisition, stake, merger, board, sale
oil, price, prices, production, lt, gas, crude, 1987, 1985, bpd, opec, barrels, energy, first, petroleum
0, mln, cts, net, loss, 2, dlrs, shr, 3, profit, 4, 5, 6, revs, 7, 9, 8, year, note, 1986, 10, 0, sales
tonnes, wheat, mln, grain, week, corn, department, year, export, program, agriculture, 0, soviet, prices
bank, market, pct, dollar, exchange, billion, stg, today, foreign, rate, banks, japan, yen, rates, trade
Table 5: Topics identified by SeededLDA on the frequent-5 categories of Reuters corpus
Reuters 20 Newsgroups
F VI F VI
LDA 0.66 1.2 0.76 0.9
SeededLDA 0.76 0.99 0.81 0.75
SeededLDA 0.71 1.08 0.79 0.78(amb)
Table 6: Effect of ambiguous seed words on Seed-
edLDA.
There are 10 major areas with sub areas under
each of them. We ran both the models with 10 top-
ics. For SeededLDA, the words in each of the ar-
eas are selected as seed words and we filter out the
ambiguous seed words. Upon a qualitative obser-
vation of the output topics, we found that LDA has
identified seven major topics and left out ?Brain
Imaging?, ?Cognitive Science and Artificial In-
telligence? and ?Hardware Technologies? areas.
Not surprisingly, but reassuringly, these areas are
underrepresented among the NIPS papers. On the
other hand, SeededLDA successfully identifies all
of the major topics. The topics identified by LDA
and SeededLDA are shown in the supplementary
material.
5 Discussion
In traditional topic models, a symmetric Dirich-
let distribution is used as prior for topic-word dis-
tributions. A first attempt method to incorporate
seed words into the model is to use an asymmetric
Dirichlet distribution as prior for the topic-word
distributions (also called as Informed priors). For
example, to encourage Topic 5 to align with a seed
set we can choose an asymmetric prior of the form
~?5 = {?, ? ? ? , ? + c, ? ? ? , ?}, i.e. we increase
the component values corresponding to the seed
words by a positive constant value. This favors
the desired seed words to be drawn with a higher
probability from this topic. But, it is argued else-
where that words drawn from such distributions
rarely pick words other than the seed words (An-
drzejewski et al 2009). Moreover, since, in our
method each seed topic is a distribution over the
seed words, the convex combination of regular
and seed topics can be seen as adding different
weights (ci) to different components of the prior
vector. Thus our Model 1 can be seen as an asym-
metric generalization of the Informed priors.
For comparability purposes, in this paper, we
experimented with same number of regular topics
as the number of seed topics. But as explained in
the modeling part, our model is general enough
to handle situation with unequal number of seed
and regular topics. In this case, we assume that
the seed topics indicate a higher level of topical-
ity structure of the corpus and associate each seed
topic (or group) with a distribution over the regu-
lar topics. On the other hand, in many NLP appli-
cations, we tend to have only a partial information
rather than high-level supervision. In such cases,
one can create some empty seed sets and tweak
the model 2 to output a 1 in the binary vector cor-
responding to these seed sets. In this paper, we
used information gain to select the discriminating
seed words. But in the real world applications,
one can use publicly available ODP categorization
data to obtain the higher level seed words and thus
explore the corporal in a more meaningful way.
In this paper, we have explored two methods
to incorporate lexical prior into the topic mod-
els, combining them into a single model that we
call SeededLDA. From our experimental analysis,
we found that automatically derived seed words
can improve clustering performance significantly.
Moreover, we found out that allowing a seed word
to be shared across multiple sets of seed words de-
grades the performance.
6 Acknowledgments
We thank the anonymous reviewers for their help-
ful comments. This material is partially supported
by the National Science Foundation under Grant
No. IIS-1153487.
212
References
Andrzejewski, D. and Zhu, X. (2009). Latent dirichlet
allocation with topic-in-set knowledge. In Proceed-
ings of the NAACL HLT 2009 Workshop on Semi-
Supervised Learning for Natural Language Pro-
cessing, SemiSupLearn ?09, pages 43?48, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Andrzejewski, D., Zhu, X., and Craven, M. (2009). In-
corporating domain knowledge into topic modeling
via dirichlet forest priors. In ICML ?09: Proceed-
ings of the 26th Annual International Conference
on Machine Learning, pages 25?32, New York, NY,
USA. ACM.
Basu, S., Ian, D., and Wagstaff, K. (2008). Con-
strained Clustering : Advances in Algorithms, The-
ory, and Applications. Chapman & Hall/CRC Pres.
Blei, D. and McAuliffe, J. (2008). Supervised topic
models. In Advances in Neural Information Pro-
cessing Systems 20, pages 121?128, Cambridge,
MA. MIT Press.
Blei., D. M. and Lafferty., J. (2009). Topic models. In
Text Mining: Theory and Applications. Taylor and
Francis.
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). La-
tent dirichlet alcation. Journal of Maching Learn-
ing Research, 3:993?1022.
Boyd-Graber, J., Blei, D. M., and Zhu, X. (2007). A
topic model for word sense disambiguation. In Em-
pirical Methods in Natural Language Processing.
Chang, J., Boyd-Graber, J., Wang, C., Gerrish, S., and
Blei, D. M. (2009). Reading tea leaves: How hu-
mans interpret topic models. In Neural Information
Processing Systems.
Griffiths, T., Steyvers, M., and Tenenbaum, J. (2007).
Topics in semantic representation. Psychological
Review, 114(2):211?244.
Griffiths, T. L. and Steyvers, M. (2004). Finding sci-
entific topics. Proceedings of National Academy of
Sciences USA, 101 Suppl 1:5228?5235.
Griffiths, T. L., Steyvers, M., Blei, D. M., and Tenen-
baum, J. B. (2005). Integrating topics and syntax.
In Advances in Neural Information Processing Sys-
tems, volume 17, pages 537?544.
Haghighi, A. and Klein, D. (2006). Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguis-
tics, HLT-NAACL ?06, pages 320?327, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Hu, Y., Boyd-Graber, J., and Satinoff, B. (2011). In-
teractive topic modeling. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
- Volume 1, HLT ?11, pages 248?257, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Johnson, M. and Goldwater, S. (2009). Improving
nonparameteric bayesian inference: experiments
on unsupervised word segmentation with adap-
tor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ?09, pages
317?325, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Lacoste-Julien, S., Sha, F., and Jordan, M. (2008).
DiscLDA: Discriminative learning for dimensional-
ity reduction and classification. In Proceedings of
NIPS ?08.
Lewis, D. D., Yang, Y., Rose, T. G., and Li, F. (2004).
Rcv1: A new benchmark collection for text catego-
rization research. J. Mach. Learn. Res., 5:361?397.
Meila?, M. (2007). Comparing clusterings?an infor-
mation based distance. J. Multivar. Anal., 98:873?
895.
Mitchell, T. M. (1997). Machine Learning. McGraw-
Hill, New York.
Paul, M. and Girju, R. (2010). A two-dimensional
topic-aspect model for discovering multi-faceted
topics. In AAAI.
Ramage, D., Hall, D., Nallapati, R., and Manning,
C. D. (2009). Labeled LDA: a supervised topic
model for credit attribution in multi-labeled cor-
pora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 1 - Volume 1, EMNLP ?09, pages 248?
256, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Thelen, M. and Riloff, E. (2002). A bootstrapping
method for learning semantic lexicons using extrac-
tion pattern contexts. In In Proc. 2002 Conf. Empir-
ical Methods in NLP (EMNLP).
Wagstaff, K., Cardie, C., Rogers, S., and Schro?dl, S.
(2001). Constrained k-means clustering with back-
ground knowledge. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing, ICML ?01, pages 577?584, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Wallach, H. M. (2005). Topic modeling: beyond bag-
of-words. In NIPS 2005 Workshop on Bayesian
Methods for Natural Language Processing.
Williamson, S., Wang, C., Heller, K. A., and Blei,
D. M. (2010). The IBP compound dirichlet pro-
cess and its application to focused topic modeling.
In ICML, pages 1151?1158.
213
 
Cross-Lingual Information Retrieval System for Indian 
Languages 
 
Jagadeesh Jagarlamudi and A Kumaran 
 
Abstract 
 
This paper describes our first participation in the Indian language sub-task of 
the main Adhoc monolingual and bilingual track in CLEF competition. In 
this track, the task is to retrieve relevant documents from an English corpus 
in response to a query expressed in different Indian languages including 
Hindi, Tamil, Telugu, Bengali and Marathi. Groups participating in this 
track are required to submit a English to English monolingual run and a 
Hindi to English bilingual run with optional runs in rest of the languages. 
We had submitted a monolingual English run and a Hindi to English cross-
lingual run. 
 
We used a word alignment table that was learnt by a Statistical Machine 
Translation (SMT) system trained on aligned parallel sentences, to map a 
query in source language into an equivalent query in the language of the 
target document collection. The relevant documents are then retrieved using 
a Language Modeling based retrieval algorithm. On CLEF 2007 data set, our 
official cross-lingual performance was 54.4% of the monolingual 
performance and in the post submission experiments we found that it can be 
significantly improved up to 73.4%. 
 
 
 
 
 
 
 
 
 
 
 
 
 
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 699?709,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Low-Dimensional Discriminative Reranking
Jagadeesh Jagarlamudi
Department of Computer Science
University of Maryland
College Park, MD 20742, USA
jags@umiacs.umd.edu
Hal Daume? III
Department of Computer Science
University of Maryland
College Park, MD 20742, USA
hal@umiacs.umd.edu
Abstract
The accuracy of many natural language pro-
cessing tasks can be improved by a reranking
step, which involves selecting a single output
from a list of candidate outputs generated by
a baseline system. We propose a novel fam-
ily of reranking algorithms based on learning
separate low-dimensional embeddings of the
task?s input and output spaces. This embed-
ding is learned in such a way that prediction
becomes a low-dimensional nearest-neighbor
search, which can be done computationally ef-
ficiently. A key quality of our approach is that
feature engineering can be done separately on
the input and output spaces; the relationship
between inputs and outputs is learned auto-
matically. Experiments on part-of-speech tag-
ging task in four languages show significant
improvements over a baseline decoder and ex-
isting reranking approaches.
1 Introduction
Mapping inputs to outputs lies at the heart of many
Natural Language Processing applications. For ex-
ample, given a sentence as input: part-of-speech
(POS) tagging involves finding the appropriate POS
tag sequence (Thede and Harper, 1999); pars-
ing involves finding the appropriate tree structure
(Kubler et al, 2009) and statistical machine trans-
lation (SMT) involves finding correct target lan-
guage translation (Brown et al, 1993). The accuracy
achieved on such tasks can often be improved signif-
icantly with the help of a discriminative reranking
step (Collins and Koo, 2005; Charniak and John-
son, 2005; Shen et al, 2004; Watanabe et al, 2007).
For the POS tagging, reranking is relative less ex-
plored due to the already higher accuracies in En-
glish (Collins, 2002), but it is shown to improve ac-
curacies in other languages such as Chinese (Huang
et al, 2007). In this paper, we propose a novel ap-
proach to discriminative reranking and show its ef-
fectiveness in POS tagging. Reranking allows us to
use arbitrary features defined jointly on input and
output spaces that are often difficult to incorporate
into the baseline decoder due to the computational
tractability issues. The effectiveness of reranking
depends on the joint features defined over both input
and output spaces. This has led the community to
spend substantial efforts in defining joint features for
reranking (Fraser et al, 2009; Chiang et al, 2009).
Unfortunately, developing joint features over the
input and output space can be challenging, espe-
cially in problems for which the exact mapping be-
tween the input and the output is unclear (for in-
stance, in automatic caption generation for images,
semantic parsing or non-literal translation). In con-
trast to prior work, our approach uses features de-
fined separately within the input and output spaces,
and learns a mapping function that can map an ob-
ject from one space into the other. Since our ap-
proach requires within-space features, it makes the
feature engineering relatively easy.
For clarity, we will discuss our approach in the
context of POS tagging, though of course it gener-
alizes to any reranking problem. At test time, in
POS tagging, we receive a sentence and a list of
candidate output POS sequences as input. We run
a feature extractor on the input sentence to obtain
a representation x ? Rd1 ; we run an independent
699
feature extractor on each of the m-many outputs
to obtain representations y?1, . . . , y?m ? Rd2 . We
will project all of these points down to a low k-
dimensional space by means of matrices A ? Rd1?k
(for x as ATx) and B ? Rd2?k (for y? as BT y?).
We then select as the output the y?j that maximizes
cosine similar to x in the lower-dimensional space:
maxj cos(ATx, BT y?j). The goal is to learn the pro-
jection matrices A and B so that the result of this
operation is a low-loss output.
Given training data of sentences and their refer-
ence tag sequences, our approach implicitly uses all
possible pairwise feature combinations across the
views and learns the matrices A and B that can map
a given sentence (as its feature vector) to its cor-
responding tag sequence. Considering all possible
pairwise combinations enables our model to auto-
matically handle long range dependencies such as
a word at a position effecting the tag choice at any
other position.
Experiments performed on four languages (En-
glish, Chinese, French and Swedish) show the ef-
fectiveness of our approach in comparison to the
baseline decoder and to the existing reranking ap-
proaches (Sec. 4). Using only the within-space fea-
tures, our models are able to beat reranking ap-
proaches that use more informative joint features.
While it is possible to include joint features into our
models, we leave this for future work.
2 Models for Low-Dimensional Reranking
In this section, we describe our approach to learning
low-dimensional representations for reranking. We
first fix some notation, then discuss the intuition be-
hind the problem we wish to solve. We propose both
generative-style and discriminative-style approaches
to formalizing this intuition, as well as a softened
variant of the discriminative model. In the subse-
quent section, we discuss computational issues re-
lated to these models.
2.1 Notation
Let xi ? Rd1 and yi ? Rd2 be the feature vectors
representing the ith(1 ? ? ?n) sentence and its refer-
ence tag sequence from the training data. Each sen-
tence is also associated with mi number of candi-
date tag sequences, output by the baseline decoder,
and are represented as y?ij ? Rd2 j = 1 ? ? ?mi. Each
candidate tag sequence (y?ij) is also associated with
a non-negative loss Lij . Note that we place abso-
lutely no constraints on the loss function. Moreover,
letX (d1?n) and Y (d2?n) denote the data matri-
ces with xi and yi as columns respectively. Finally,
let ?u,v? denote the dot product of the two vectors
u and v.
2.2 Intuition
As stated in the introduction, our goal is to learn
projections A ? Rd1?k and B ? Rd2?k in such a
way that test-time predictions are made with high
accuracy (or low loss). At test time, the output will
be chosen by maximizing cosine similarity between
the input and the output, after projecting these vec-
tors into a low-dimensional space using A and B,
respectively. The cosine similarity in our context is:
xTABT y?j
?
xTAATx
?
y?Tj BBT y?j
(1)
Our goal is to learn A and B in such a way that the
y?j with maximum cosine similarity to an x is ac-
tually the correct output. In what follows, we will
describe our models to find one-dimensional projec-
tion vectors a ? Rd1 and b ? Rd2 , but the general-
ization to matrices A and B is very trivial.
2.3 A Generative-Style Model
The first model we propose is akin to a gener-
ative probabilistic model, in the sense that it at-
tempts to model the relationship between an input
and its desired output, without taking alternate pos-
sible outputs into account. In the context of the in-
tuition sketched in the previous section, the idea is
to choose A and B so as to maximize the cosine
similarities on the training data between each input
and it?s correct (or minimal-loss) output. This model
intentionally ignores the information present in the
alternative, incorrect outputs. The hope is that by
making the cosine similarities with the best output
as high as possible, all the alternate outputs will look
bad in comparison.
Given a training data of sentences and their
reference tag sequences represented as X and Y
(Sec. 2.1), our generative model finds projection di-
rections, in word and tag spaces, along which the
700
aligned sentence and tag sequence pairs have maxi-
mum cosine similarity. In the one-dimensional set-
ting, it finds directions a ? Rd1 and b ? Rd2 such
that the correlation as defined in Eq. 2 is maximized.
aTXY Tb?
aTXXTa
?
bTY Y Tb
(2)
Since the objective is invariant to the scaling of vec-
tors a and b, it can be rewritten as:
argmax
a,b
aTXY Tb (3)
s.t. aTXXTa = 1 and bTY Y Tb = 1(4)
We refer to the constraints in Eq. 4 as length con-
straints in the rest of this paper.
To understand why maximizing this objective
function learns a good mapping function between
the sentence and the tag sequence, consider decom-
posing the objective function as follows:
aTXY Tb =
n
?
i=1
?xi,a??yi,b?
=
n
?
i=1
(
d1
?
l=1
xlial ?
d2
?
m=1
ymi bm
)
=
n
?
i=1
(
d1
?
l=1
d2
?
m=1
xlial ymi bm
)
=
n
?
i=1
(
d1,d2
?
l,m=1
wlm?lmi
)
(5)
where we replaced the scalars xliymi and albm with
?lmi and wlm respectively. So finally, the objective
can be expressed as aTXY Tb =
?
i?w, ?(xi,yi)?
where w is the weight vector and ?(xi,yi) is a vec-
tor of size (d1 ? d2) and is given by the Kronecker
product of the two feature vectors xi and yi.
In this form, the generative objective function
bears similarity to the linear boundary surface
widely used in machine learning, except that the
weights are restricted to be the outer product of two
vectors. From the reduced expressions, it is clear
that our generative model considers all possible pair-
wise combinations of the input features (d1?d2) and
learns which of them are more important than others.
Intuitively, it puts higher weight on a word and tag
pair that co-occur frequently in the training data, at
the same time each of these are infrequent in their
own views.
2.4 A Discriminative-Style Model
The primary disadvantage of our generative model is
that it only uses input sentences and their reference
tag sequences and does not use the incorrect candi-
date tag sequences of a given sentence at all. In what
follows, we describe a model that utilize the incor-
rect candidate tag sequences as negative examples
to improve the projection directions (a and b). Our
goal is to address this by adding constraints to our
model that explicitly penalize ranking high-loss out-
puts higher than low-loss outputs, as is often done in
the context of maximum-margin structure prediction
techniques (Taskar et al, 2004).
In this section, we describe a discriminative
model that keeps track of the margin deviations and
finds the projection directions iteratively. Intuitively,
after the projection into the lower dimensional sub-
space, the cosine similarity of a sentence to its refer-
ence tag sequence must be greater than that of its
incorrect candidate tag sequences. Moreover, the
margin between these similarities should be propor-
tional to the loss of the candidate translation, i.e. the
more dissimilar a candidate tag sequence to its ref-
erence is, the farther it should be from the reference
in the projected space.
From the decomposition shown in Eq. 5, for a
given pair of source sentence xi and a tag sequence
yj , the generative model assigns a score of :
?a,xi??b,yj? = aTxiyTj b
Each input sentence is also associated with a list
of candidate tag sequences and since each of these
candidate sequences are incorrect they should be as-
signed a score less than that of the reference tag se-
quence. Drawing ideas from structure prediction lit-
erature (Bakir et al, 2007), we modify the objec-
tive function in order to include these terms. This
idea can be captured using a loss augmented mar-
gin constraint for each sentence, tag sequence pair
(Tsochantaridis et al, 2004). Let ?i denote a non-
negative slack variable, then we define our new op-
timization problem as:
arg max
a,b,??0
1? ?
? a
TXY Tb?
?
i
?i (6)
s.t. aTXXTa = 1 and bTY Y Tb = 1
?i ?j aTxiyTi b? aTxiy?Tijb ? 1?
?i
Lij
701
where 0 ? ? ? 1 is a weight parameter. This ob-
jective function is ensuring that the margin between
the reference and the candidate tag sequences in the
projected space (as given by aTxiyTi b?aTxiy?Tijb)
is proportional to its loss (Lij). Notice that the slack
is defined for each sentence and it remains the same
for all of its candidate tag sequences.
2.5 A Softened Discriminative Model
One disadvantage of the discriminative model de-
scribed in the previous section is that it cannot be
optimized in closed form (as discussed in the next
section). In this section, we consider a model that
lies between the generative model and the (fully)
discriminative model. This softened model has at-
tractive computational properties (it is easy to com-
pute) and will also form a building block for the op-
timization of the full discriminative model.
For each sentence xi, its reference tag sequence
yi should be assigned a higher score than any of its
candidate tag sequences y?ij i.e. we want to maxi-
mize aTxiyTi b?aTxiy?Tijb. In the fully discrimina-
tive model, we enforce that this is at least one (mod-
ulo slack). In the relaxed version, we instead require
that this hold on average. In order to achieve this
we add the following terms to the objective function:
?j = 1 ? ? ?mi
aTxiyTi b? aTxiy?Tijb = aTxirTijb (7)
where rij = yi ? y?ij is the residual vector between
the reference and the candidate sequences. Now,
we simply sum all these terms for a given sentence
weighted by their loss and encourage it to be as high
as possible, i.e. we maximize
1
mi
mi
?
j=1
Lij
(
aTxirTijb
)
= aTxi
( 1
mi
mi
?
j=1
LijrTij
)
b (8)
The normalization bymi takes care of unequal num-
bers of candidate tag sequences that often arises be-
cause of the difference in the lengths of the input
sentences. Now let R denote a matrix of the same
size as that of Y (i.e. d2 ? n) with its ith column as
given by 1mi
?mi
j=1 Lijrij , then we add the following
term to the generative objective function:
n
?
i=1
aTxi
( 1
mi
mi
?
j=1
LijrTij
)
b = aTXRTb (9)
Finally, the projection directions are obtained by
solving the following optimization problem :
argmax
a,b
(1? ?)aTXY Tb+ ? aTXRTb (10)
s.t. aTXXTa = 1 and bTY Y Tb = 1
where 0 ? ? ? 1 is the weight parameter to be
tuned on the development set.
3 Optimization
In this section, we describe how we solve the opti-
mization problems associated with our models. First
we discuss the solution of the generative model.
Next, we discuss the softened discriminative model,
since its solution will be used as a subroutine in our
final discussion of the fully discriminative model.
3.1 Optimizing the Generative Model
The optimization problem corresponding to the gen-
erative model turns out to be identical to that of
canonical correlation analysis (CCA) (Hotelling,
1936; Hardoon et al, 2004), which immediately
suggests a solution by solving an eigensystem. In
particular, the projection directions are obtained by
solving the following generalized eigensystem:
(
0 Cxy
Cyx 0
)(
a
b
)
=
(
Cxx 0
0 Cyy
)(
a
b
)
(11)
where Cxx = (1 ? ?)XXT + ?I , Cyy = (1 ?
?)Y Y T + ?I are autocovariance matrices, Cxy =
XY T is the cross-covariance matrix, Cyx = CTxy,
? is a regularization parameter and I is the identity
matrix of appropriate size. Using these eigenvectors
as columns, we form projection matrices A and B.
These projection matrices are used to project sen-
tences and tag sequences into a common lower di-
mensional subspace. In general, using all the eigen-
vectors is sub-optimal from the generalization per-
spective so we retain only top k eigenvectors.
3.2 Optimizing the Softened Model
In the softened discriminative version, the summa-
tion of all the difference terms over all candidate tag
sequences and sentences (Eq. 9), enables a simpler
objective function whose optimum can be derived
by following a procedure very similar to that of the
702
generative model. In particular, the projection direc-
tions are obtained by solving Eq. 11 except that Cxy
is replaced with X((1? ?)Y T + ?RT ).
3.3 Optimizing the Discriminative Model
To solve the discriminative model, we begin by con-
structing the Lagrange dual. Let ?1, ?2 and ?ij
be the Lagrangian multipliers corresponding to the
length and the margin constraints respectively, then
the Lagrangian of Eq. 6 is given by:
L = 1? ?? a
TXY Tb?
n
?
i=1
?i
? ?1
(
aTXXTa? 1
)
? ?2
(
bTY Y Tb? 1
)
+
n,mi
?
i=1,j=1
?ij
(
aTxirTijb? 1 +
?i
Lij
)
Differentiating the Lagrangian with respect to the
parameters a,b and setting them to zero yields
the solution for the parameters in terms of the La-
grangian multipliers ?ij as follows:
(
0 C?xy
C?yx 0
)(
a
b
)
=
(
Cxx 0
0 Cyy
)(
a
b
)
(12)
where C?xy = X
(
1??
? Y T + RT
)
and R is a ma-
trix of size d2 ? n with ith column as given by
1
mi
?mi
j=1 ?ijrij . We use superscript ? on the cross-
covariance matrix to indicate that it is dependent on
the Lagrangian multipliers ?ij . In other words, the
solution is similar to that of the previous formulation
except that the residual vectors are weighted by the
Lagrangian multipliers instead of the loss function.
Unlike the max margin formulations of SVM, it is
not easy to rewrite the parameters a,b in terms of
the Lagrangian multipliers ?ij as C?xy itself depends
on ?ij?s. Hence, rewriting the parameters in terms
of the Lagrangian multipliers and then solving the
dual is not amenable in this case.
In order to solve this optimization problem, we
resort to an alternate optimization technique in the
primal space. It proceeds in two stages. In the first
stage, we keep the Lagrangian multipliers ?ij fixed
and then solve for the parameters a,b, ?1, ?2 and
?i. Projection directions a,b and their Lagrangian
multipliers ?1, ?2 are obtained by solving the gen-
eralized eigenvalue problem given in Eq. 12. Using
Algorithm 1 Alternate optimization algorithm for
solving the parameters of Discriminative Model.
Input: X,Y, Y? , L, ?, ?
Output: A,B
1: ?i, j ?ij = Lij ;
2: rij = yi ? y?ij ; Cxx = (1 ? ?)XXT + ?I;
Cyy = (1? ?)Y Y T + ?I
3: repeat
4: Form R with ith column as 1mi
?mi
j=1 ?ijrij
5: C?xy = X
(
1??
? Y T +RT
)
6: Solve for the eigenvectors of Eq. 12. .
7: Form matrices A,B with top k eigenvectors
as columns; k is determined using dev. set.
8: Let An & Bn be normalized versions of A
and B s.t. they follow the length constraints.
9: for each sentence i = 1 ? ? ?n do
10: j = 1? ? ?mi, ?ij =
(
1? xTi AnBTn rij
)
Lij
11: ?i = min
{
0 , ?ij | s.t. ?ij > 0
}
12: if ?i > 0 then
13: dij = xTi AnBTn rij ?
(
1? ?iLij
)
14: ?ij = ?ij ? ? dij
15: end if
16: end for
17: until slack values doesn?t change
18: return A,B
these projection directions, we determine the slack
variable ?i for each sentence. In the second stage
of the alternate optimization, we fix a,b and ?i and
take a gradient descent step along ?ij?s to minimize
the function. We repeat this process until conver-
gence. In our experiments, we noticed that this al-
gorithm converges within five iterations, so we only
run it for five iterations.
The pseudocode of our approach is shown in
Alg. 1. First we initialize the Lagrangian multipli-
ers proportional to the loss of the candidate tag se-
quences (step 1). This ensures that the eigenvectors
solved in step 6 are same as the output given by the
softened model (Sec. 2.5). In general, in our experi-
ments, we observed that this is a good starting point.
After solving the generalized eigenvalue problem in
step. 6, we consider the top k eigenvectors, as de-
termined by the error on the development set and
normalize them so that they follow the length con-
straints (steps 7 and 8). In the rest of the algorithm,
703
we use these normalized projection directions to find
the slack values which are in turn used to find the up-
date direction for the Lagrangian variables.
In step 10, we compute the potential slack value
(?ij) for each constraint so that it is satisfied and
then choose the minimum of the positive ?ij val-
ues as the slack for this sentence (step 11). If the
chosen slack value is equal to zero, it implies that
?ij ? 0 ?j = 1 ? ? ?mi which in turn implies that
all the constraints of a given input sentence are sat-
isfied by the current projection directions and hence
there is no need to update the Lagrangian multipli-
ers. Otherwise, some of the constraints are still not
satisfied and hence we will update their correspond-
ing Lagrangian multipliers in steps 13 and 14. In
specific, step 13 computes the deviation of the mar-
gin constraints with the new slack value and step 14
updates the Lagrangian multipliers along the gradi-
ent direction.
In principle, our approach is similar to the cutting
plane algorithm used to optimize slack re-scaling
version of Structured SVM (Tsochantaridis et al,
2004), but it differs in selecting the slack variable
(step 11). The cutting plane method chooses ?i as
the maximum of {0, ?ij} where as we choose the
minimum of the positive ?ij values as the slack. In-
tuitively, this means that the cutting plane algorithm
chooses a constraint that is most violated which re-
sults in fewer constraints. This is crucial in struc-
tured SVM, because solving the dual problem is cu-
bic in terms of the number of examples and con-
straints. In contrast, our approach selects the slack
such that at least one of the constraints is satisfied
and adds all the remaining constraints to the active
set. Since step 6 considers a weighted average of all
these constraints the complexity depends only on the
number of training examples and not the constraints.
3.4 Combining with Viterbi Decoding Score
All the three formulations discussed until now do not
consider the Viterbi decoding score assigned to each
candidate tag sequence. As explained in Collins and
Koo (2005), the decoding score plays an important
role in reranking the candidate sentences. Here, we
describe a simple linear combination of the Viterbi
decoding score and the score obtained by projecting
into the low-dimensional subspace, using projection
directions obtained by any of the above models.
For a given sentence xi and candidate tag se-
quence pair y?ij , let sij and pij (Eq. 1) be the scores
assigned by Viterbi decoding and the lower dimen-
sional projections respectively. Then we define the
final score for this pair as a simple linear combina-
tion of these two scores as:
Score(xi, y?ij) = sij + w pij (13)
The weight w is optimized using a grid search on
the development data set, we search for w from 0 to
100 with an increment of 1 and choose the value for
which the error is minimum on the development set.
3.5 Reranking for POS Tagging
To summarize our approach, we convert the train-
ing data into feature vectors and use any of the
three methods discussed above to find the lower di-
mensional projection directions (a and b). Each of
those approaches involve solving a similar general-
ized eigenvalue problem (Eq. 11) with the cross co-
variance matrix Cxy defined differently in the three
approaches. This problem can be solved in differ-
ent ways, but we use the following approach since it
reduces the size of the eigenvalue problem.
C?1yy CTxyC?1xx Cxy b = ? b (14)
a =
1?
? C
?1
xx Cxy b (15)
where ? is the eigenvalue. Assuming that d2 ? d1,
which is usually true in POS tagging because of
the smaller tag vocabulary, these equations solve
a smaller eigenvalue problem. After solving the
eigenvalue problem, we form matricesA andB with
columns as the top k eigenvectors a and b respec-
tively. Given a new sentence and candidate tag se-
quence pair (xi, y?ij), their similarity is obtained us-
ing Eq. 1. Now, based on the development data set
we find the weight (w) for the linear combination of
the projection and Viterbi decoding scores (Eq. 13).
During the reranking stage, we first use Eq. 1 to
compute the projection score for all the candidate
tag sequences and then use Eq. 13 to combine this
scores with the decoding score. The candidate tag
sequences are reranked based on this final score.
4 Experiments
In this section, we report POS tagging experiments
on four languages: English, Chinese, French and
704
Train. Dev. Test
English (En.) # sent. 15K 2K 1791# words 362K 47K 43K
Chinese (Zh.) # sent. 50K 4K 3647# words 292K 26K 25K
French (Fr.) # sent. 9K 2K 1351# words 254K 57K 40K
Swedish (Sv.) # sent. 8K 2K 1431# words 137K 31K 28K
Table 1: Training and test data statistics.
Swedish. The data in all these languages is obtained
from the CoNLL 2006 shared task on multilingual
dependency parsing (Buchholz and Marsi, 2006).
We only consider the word and its fine grained POS
tag (columns 2 and 5 respectively) and ignore the
dependency links in the data. Table 1 shows the data
statistics in each of these languages.
We use a second order Hidden Markov Model
(Thede and Harper, 1999) based tagger as a baseline
tagger in our experiments. This model uses trigram
transition and emission probabilities and is shown
to achieve good accuracies in English and other lan-
guages (Huang et al, 2007). We refer to this as the
baseline tagger in the rest of this paper and is used to
produce n-best list for each candidate sentence. The
n-best list for training data is produced using multi-
fold cross-validation like Collins and Koo (2005)
and Charniak and Johnson (2005). The first block of
Table 2 shows the accuracies of the top-ranked tag
sequence (according to the Viterbi decoding score)
and the oracle accuracies on the 10-best list. As
expected the accuracies on English and French are
high and are on par with the state-of-the-art systems.
From the oracle scores, it is clear that though there is
a chance for improvement using reranking, the scope
for improvement in English is less compared to the
5 point improvement reported for parsing (Charniak
and Johnson, 2005). This indicates the difficulty
of the reranking problem for POS tagging in well-
resourced languages.
4.1 Reranking Features and Baselines
In this paper, except for Chinese, we use suffixes of
length two to four as features in the word view and
unigram and bigram tag sequences as features in the
tag view. That is, we convert each word of the sen-
tence into suffixes of length two to four and then
treat each sentence as a bag of suffixes. Similarly,
we treat a candidate POS tag sequence as a bag of
unigram and bigram tag features. For Chinese, we
use character sequences of length one and two as
features for the sentences and use unigram and bi-
gram POS tag sequences on the tag view. We did
not include any alignment based features, i.e. fea-
tures that depend on the position.
We compare our models with a boosting-based
discriminative approach (Collins and Koo, 2005)
and its regularized version (Huang et al, 2007). In
order to enable a fair comparison, we use suffix and
tag pairs as features for both these models. For ex-
ample, we would generate the following features for
the word ?selling? in the phrase ?the/DT selling/NN
pressure/NN?: (ng, NN), (ng, DT NN), (ing,NN),
(ing,DT NN), (ling,NN), (ling,DT NN). For com-
parison purposes, we also show results by running
the baseline rerankers with n-gram features.
4.2 Results
There are following hyper parameters in each of our
models, regularization parameter ? , weight parame-
ter ? in the discriminative and softened discrimina-
tive models, the linear combination weight w with
the Viterbi decoding score, and finally, the size of
the lower dimensional subspace (k). We use grid
search to tune these parameters based on the devel-
opment data set. The optimal hyperparameter values
differ based on the model and the language, but the
tagging accuracy is relatively robust with respect to
these parameter values. For English, the best values
for the discriminative model are ? = 0.95, ? = 0.3
and k = 75. For the same language, Fig. 1 shows
the performance with respect to ? and ? parameters,
respectively, with other parameters fixed to their op-
timal values. Notice that, although the performance
varies it is always more than the accuracy of the
baseline tagger (96.74%).
Table 2 shows the results of different models on
the development and test data sets. On the test data
set, the baseline reranking approaches perform bet-
ter than the HMM decoder in Chinese and Swedish
languages, but they underperform in English and
French languages. This is justifiable because the in-
dividual characters are good indicators of POS tag
705
 96.74
 96.76
 96.78
 96.8
 96.82
 96.84
 96.86
 0.78  0.8  0.82  0.84  0.86  0.88  0.9  0.92  0.94  0.96  0.98  1
Discriminative
Softened-Disc
Generative
?
 96.74
 96.76
 96.78
 96.8
 96.82
 96.84
 96.86
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8
Discriminative
Softened-Disc
?
Figure 1: Tagging accuracy with hyperparameters ? and ? on English development data set.
Development Set Test set
English Chinese French Swedish English Chinese French Swedish
Baseline 96.74 92.55 96.94 93.22 96.15 92.31 97.41 93.23
Oracle 98.85 98.41 98.61 96.96 98.39 98.19 99.00 96.48
Collins (Sufx) 96.66 93.00 96.87 93.50 96.06 92.81 97.35 93.44
Regularized (Sufx) 96.60 93.12 96.90 93.36 96.00 92.88 97.38 93.35
Generative 96.82 93.14 96.97 93.46 96.24 92.95 97.43 93.26
Softened-Disc 96.85 93.14 97.04 93.49 96.32 92.87 97.53 93.24
Discriminative 96.85 93.17 97.03 93.50 96.3 92.91 97.53 93.36
Collins (n-gm) 96.74 93.14 97.06 93.44 96.13 92.74 97.54 93.45
Regularized (n-gm) 96.78 93.14 97.01 93.45 96.14 92.80 97.52 93.40
Table 2: Accuracy of the baseline HMM tagger and different reranking approaches. For comparison purposes, we also
showed the results of Collins and Koo (2005) its regularized versions with n-gram features. The improvements of our
discriminative models are statistically significant at p = 0.01 and p = 0.05 levels on Chinese and English respectively.
information for Chinese and this additional informa-
tion is being exploited by the reranking approaches.
Swedish, on the other hand, is a Germanic language
with compound word phenomenon which makes the
baseline HMM decoder weaker compared to English
and French.
The fourth block shows the performance of our
models. Except in Swedish, one of our models out-
perform the baseline decoder and the other rerank-
ing approaches. The fact that our models outperform
the baseline system and other reranking approaches
indicate that, by considering all the pairwise com-
binations of the input features our models capture
dependencies that are left by other models. Among
the different formulations of our approach, maxi-
mizing the margin between the correct and incorrect
candidates performed better than generative, and en-
suring that the margin is proportional to the loss of
the candidate sequence (discriminative) led to even
more improved results. Except in Chinese, our dis-
criminative version performed at least as well as the
other variants. Compared to the baseline decoder,
the discriminative version achieves a maximum im-
provement of 0.6 points in Chinese while achieving
0.15, 0.12 and 0.13 points of improvement in En-
glish, French and Swedish languages respectively.
We also reported the results of the baseline
rerankers with n-gram features in the fifth block of
Table 2. We remind the reader that our models use
only suffix features, so for a fair comparison the
706
En. Zh. Fr. Sv.
Generative 94.83 89.89 96.1 91.89
Softened-Disc 95.04 89.61 95.97 91.95
Discriminative 94.95 89.76 95.82 92.11
Table 3: Accuracies without combining with Viterbi de-
coding score.
reader should compare our results with the baseline
rerankers run with the suffix features. The perfor-
mance of these baseline rankers improved when we
include the n-gram features but it is still less than
the discriminative model in most cases.
Finally, Table 3 shows the performance of our
models without combining with the Viterbi decod-
ing score. As shown, the performance drops signif-
icantly and is in accordance with the behavior ob-
served elsewhere (Collins and Koo, 2005).
5 Related Work
In this section, we discuss approaches that are most
relevant to our problem and the approach.
In NLP literature, discriminative reranking has
been well explored for parsing (Collins and Koo,
2005; Charniak and Johnson, 2005; Shen and Joshi,
2003; McDonald et al, 2005; Johnson and Ural,
2010) and statistical machine translation (Shen et
al., 2004; Watanabe et al, 2007; Liang et al, 2006).
Collins (2002) proposed two reranking approaches,
namely boosting algorithm and a voted perceptron,
for the POS tagging task. Later Huang et al (2007)
propose a regularized version of the objective used
by Collins (2002) and show an improved perfor-
mance for Chinese. In all of the above reranking
approaches, the feature functions are defined jointly
on the input and output, whereas in our approach,
the features are defined separately within each view
and the algorithm learns the relationship between
them automatically. This is the primary difference
between our approach and the existing rerankers.
In principle, our margin formulations are similar
to the max margin formulations of CCA (Szedmak
et al, 2007) and maximum margin regression (Szed-
mak et al, 2006; Wang et al, 2007). These ap-
proaches solve the following optimization problem:
min ?W?2 + C1T ? (16)
s.t. ?yi,W?(x)i? ? 1? ?i ?i = 1 ? ? ?n
Our approach differs from these formulations in two
main ways: the score assigned by our generative
model (equivalent to CCA) for an input-output pair
(xTi abTyi) can be converted into this format by
substituting W ? baT but in doing so we are
ignoring the rank constraint. It is often observed
that, dimensionality reduction leads to an improved
performance and thus the rank constraint becomes
crucial. Another major difference is that, the con-
straints in Eq. 16 represent that any input and out-
put pair should have at least a margin of 1 (modulo
slack), whereas in our approach, the constraints in-
clude incorrect outputs along with their loss value.
In other words, our formulation is more suitable for
the reranking problem while Eq. 16 is more suitable
for regression or classification tasks. Our genera-
tive model is very similar to the supervised semantic
hashing work (Bai et al, 2010) but the way we opti-
mize is completely different from theirs.
6 Discussion
In this paper, we proposed a novel family of mod-
els for discriminative reranking problem and showed
improvements for the POS tagging task in four dif-
ferent languages. Here, we restricted our scope to
showing the utility of our technique and, hence, did
not experiment with different features, though it is
an important direction. By using only within space
features, our models are able to beat the rerank-
ing approaches that use potentially more informa-
tive alignment-based features. It is also possible to
include alignment-based features into our models by
posing the problem as a feature selection problem on
the covariance matrices (Jagarlamudi et al, 2011).
Our approach involves an inverse computation and
an eigenvalue problem. Although our models scale
to medium size data sets (our Chinese data set has
50K examples and 33K features), these operations
can be expensive. But there are alternative approx-
imation techniques that scale well to large data sets
(Halko et al, 2009). We leave this for future work.
Acknowledgments
We thank Zhongqiang Huang for providing the code
for the baseline systems, Raghavendra Udupa and
the anonymous reviewers for their insightful com-
ments. This work is partially funded by NSF grants
IIS-1153487 and IIS-1139909.
707
References
Bing Bai, Jason Weston, David Grangier, Ronan
Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier
Chapelle, and Kilian Weinberger. 2010. Learning
to rank with (a lot of) word features. Inf. Retr.,
13(3):291?314, June.
Gu?khan H. Bakir, Thomas Hofmann, Bernhard
Scho?lkopf, Alexander J. Smola, Ben Taskar, and
S. V. N. Vishwanathan. 2007. Predicting Structured
Data (Neural Information Processing). The MIT
Press.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19:263?311, June.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning, CoNLL-X ?06, pages 149?
164, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 173?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 218?226, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31:25?70, March.
Michael Collins. 2002. Ranking algorithms for named-
entity extraction: boosting and the voted perceptron.
In Proceedings of the 40th Annual Meeting on As-
sociation for Computational Linguistics, ACL ?02,
pages 489?496, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Alexander Fraser, Renjing Wang, and Hinrich Schu?tze.
2009. Rich bitext projection features for parse rerank-
ing. In Proceedings of the 12th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, EACL ?09, pages 282?290, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Nathan Halko, Per-Gunnar. Martinsson, and A. Joel
Tropp. 2009. Finding structure with randomness:
Stochastic algorithms for constructing approximate
matrix decompositions. Technical report, California
Institute of Technology.
David R. Hardoon, Sandor R. Szedmak, and John R.
Shawe-taylor. 2004. Canonical correlation analy-
sis: An overview with application to learning methods.
Neural Comput., 16:2639?2664, December.
Harold Hotelling. 1936. Relation between two sets of
variables. Biometrica, 28:322?377.
Zhongqiang Huang, Mary Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and discrim-
inative reranking. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1093?1102,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Jagadeesh Jagarlamudi, Raghavendra Udupa, Hal
Daume? III, and Abhijit Bhole. 2011. Improving
bilingual projections via sparse covariance matrices.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
930?940, Edinburgh, Scotland, UK., July. Association
for Computational Linguistics.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown parsers. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 665?668,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Sandra Kubler, Ryan McDonald, Joakim Nivre, and
Graeme Hirst. 2009. Dependency Parsing. Morgan
and Claypool Publishers.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 761?768, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 91?98, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Libin Shen and Aravind K. Joshi. 2003. An SVM based
voting algorithm with application to parse reranking.
In Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003 - Volume 4,
CONLL ?03, pages 9?16, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
708
Libin Shen, Anoop Sarkar, and Franz Och. 2004. Dis-
criminative reranking for machine translation. In Hu-
man Language Technology Conference and the 5th
Meeting of the North American Association for Com-
putational Linguistics: HLT-NAACL 2004, Boston,
USA, May.
S. Szedmak, J. Shawe-Taylor, and E. Parado-Hernandez.
2006. Learning via linear operators: Maximum mar-
gin regression; multiclass and multiview learning at
one-class complexity. Technical report, University of
Southampton.
Sandor Szedmak, Tijl De Bie, and David R. Hardoon.
2007. A metamorphosis of canonical correlation anal-
ysis into multivariate maximum margin learning. In
Proceedings of the fifteenth European Symposium on
Artificial Neural Networks.
Ben Taskar, Carlos. Guestrin, and Daphne Koller. 2004.
Max margin markov networks. In Proceedings of
NIPS 16.
Scott M. Thede and Mary P. Harper. 1999. A second-
order Hidden Markov Model for part-of-speech tag-
ging. In Proceedings of the Annual Meeting on Asso-
ciation for Computational Linguistics, pages 175?182.
Association for Computational Linguistics.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the twenty-first inter-
national conference on Machine learning, ICML ?04,
pages 104?, New York, NY, USA. ACM.
Zhuoran Wang, John Shawe-Taylor, and Sandor Szed-
mak. 2007. Kernel regression based machine trans-
lation. In Human Language Technologies 2007: The
Conference of the North American Chapter of the As-
sociation for Computational Linguistics; Companion
Volume, Short Papers, NAACL-Short ?07, pages 185?
188, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online Large-Margin Training for Sta-
tistical Machine Translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic, June. Association for
Computational Linguistics.
709
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 147?152,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
From Bilingual Dictionaries to Interlingual Document Representations
Jagadeesh Jagarlamudi
University of Maryland
College Park, USA
jags@umiacs.umd.edu
Hal Daume? III
University of Maryland
College Park, USA
hal@umiacs.umd.edu
Raghavendra Udupa
Microsoft Research India
Bangalore, India
raghavu@microsoft.com
Abstract
Mapping documents into an interlingual rep-
resentation can help bridge the language bar-
rier of a cross-lingual corpus. Previous ap-
proaches use aligned documents as training
data to learn an interlingual representation,
making them sensitive to the domain of the
training data. In this paper, we learn an in-
terlingual representation in an unsupervised
manner using only a bilingual dictionary. We
first use the bilingual dictionary to find candi-
date document alignments and then use them
to find an interlingual representation. Since
the candidate alignments are noisy, we de-
velop a robust learning algorithm to learn
the interlingual representation. We show that
bilingual dictionaries generalize to different
domains better: our approach gives better per-
formance than either a word by word transla-
tion method or Canonical Correlation Analy-
sis (CCA) trained on a different domain.
1 Introduction
The growth of text corpora in different languages
poses an inherent problem of aligning documents
across languages. Obtaining an explicit alignment,
or a different way of bridging the language barrier,
is an important step in many natural language pro-
cessing (NLP) applications such as: document re-
trieval (Gale and Church, 1991; Rapp, 1999; Balles-
teros and Croft, 1996; Munteanu and Marcu, 2005;
Vu et al, 2009), Transliteration Mining (Klementiev
and Roth, 2006; Hermjakob et al, 2008; Udupa et
al., 2009; Ravi and Knight, 2009) and Multilingual
Web Search (Gao et al, 2008; Gao et al, 2009).
Aligning documents from different languages arises
in all the above mentioned problems. In this pa-
per, we address this problem by mapping documents
into a common subspace (interlingual representa-
tion)1. This common subspace generalizes the no-
tion of vector space model for cross-lingual applica-
tions (Turney and Pantel, 2010).
There are two major approaches for solving the
document alignment problem, depending on the
available resources. The first approach, which
is widely used in the Cross-lingual Information
Retrieval (CLIR) literature, uses bilingual dictio-
naries to translate documents from one language
(source) into another (target) language (Ballesteros
and Croft, 1996; Pirkola et al, 2001). Then stan-
dard measures such as cosine similarity are used to
identify target language documents that are close to
the translated document. The second approach is to
use training data of aligned document pairs to find a
common subspace such that the aligned document
pairs are maximally correlated (Susan T. Dumais,
1996; Vinokourov et al, 2003; Mimno et al, 2009;
Platt et al, 2010; Haghighi et al, 2008) .
Both kinds of approaches have their own strengths
and weaknesses. Dictionary based approaches treat
source documents independently, i.e., each source
language document is translated independently of
other documents. Moreover, after translation, the re-
lationship of a given source document with the rest
of the source documents is ignored. On the other
hand, supervised approaches use all the source and
target language documents to infer an interlingual
1We use the phrases ?common subspace? and ?interlingual
representation? interchangeably.
147
representation, but their strong dependency on the
training data prevents them from generalizing well
to test documents from a different domain.
In this paper, we propose a technique that com-
bines the advantages of both these approaches. At a
broad level, our approach uses bilingual dictionaries
to identify initial noisy document alignments (Sec.
2.1) and then uses these noisy alignments as train-
ing data to learn a common subspace. Since the
alignments are noisy, we need a learning algorithm
that is robust to the errors in the training data. It is
known that techniques like CCA overfit the training
data (Rai and Daume? III, 2009). So, we start with an
unsupervised approach such as Kernelized Sorting
(Quadrianto et al, 2009) and develop a supervised
variant of it (Sec. 2.2). Our supervised variant learns
to modify the within language document similarities
according to the given alignments. Since the origi-
nal algorithm is unsupervised, we hope that its su-
pervised variant is tolerant to errors in the candidate
alignments. The primary advantage of our method is
that, it does not use any training data and thus gen-
eralizes to test documents from different domains.
And unlike the dictionary based approaches, we use
all the documents in computing the common sub-
space and thus achieve better accuracies compared
to the approaches which translate documents in iso-
lation.
There are two main contributions of this work.
First, we propose a discriminative technique to learn
an interlingual representation using only a bilingual
dictionary. Second, we develop a supervised variant
of Kernelized Sorting algorithm (Quadrianto et al,
2009) which learns to modify within language doc-
ument similarities according to a given alignment.
2 Approach
Given a cross-lingual corpus, with an underlying un-
known document alignment, we propose a technique
to recover the hidden alignment. This is achieved
by mapping documents into an interlingual repre-
sentation. Our approach involves two stages. In the
first stage, we use a bilingual dictionary to find ini-
tial candidate noisy document alignments. The sec-
ond stage uses a robust learning algorithm to learn a
common subspace from the noisy alignments iden-
tified in the first step. Subsequently, we project all
the documents into the common subspace and use
maximal matching to recover the hidden alignment.
During this stage, we also learn mappings from the
document spaces onto the common subspace. These
mappings can be used to convert any new document
into the interlingual representation. We describe
each of these two steps in detail in the following two
sub sections (Sec. 2.1 and Sec. 2.2).
2.1 Noisy Document Alignments
Translating documents from one language into an-
other language and finding the nearest neighbours
gives potential alignments. Unfortunately, the re-
sulting alignments may differ depending on the di-
rection of the translation owing to the asymmetry
of bilingual dictionaries and the nearest neighbour
property. In order to overcome this asymmetry, we
first turn the documents in both languages into bag
of translation pairs representation.
We follow the feature representation used in Ja-
garlamudi and Daume? III (2010) and Boyd-Graber
and Blei (2009). Each translation pair of the bilin-
gual dictionary (also referred as a dictionary en-
try) is treated as a new feature. Given a docu-
ment, every word is replaced with the set of bilin-
gual dictionary entries that it participates in. If
D represents the TFIDF weighted term ? docu-
ment matrix and T is a binary matrix matrix of size
no of dictionary entries? vocab size, then convert-
ing documents into a bag of dictionary entries is
given by the linear operation X(t) ? TD.2
After converting the documents into bag of dic-
tionary entries representation, we form a bipartite
graph with the documents of each language as a
separate set of nodes. The edge weight Wij be-
tween a pair of documents x(t)i and y
(t)
j (in source
and target language respectively) is computed as the
Euclidean distance between those documents in the
dictionary space. Let piij indicate the likeliness of
a source document x(t)i is aligned to a target doc-
ument y(t)j . We want each document to align to at
least one document from other language. Moreover,
we want to encourage similar documents to align
to each other. We can formulate this objective and
the constraints as the following minimum cost flow
2Superscript (t) indicates that the data is in the form of bag
of dictionary entries
148
problem (Ravindra et al, 1993):
argmin
pi
m,n
?
i,j=1
Wijpiij (1)
?i
?
j
piij = 1 ; ?j
?
i
piij = 1
?i, j 0 ? piij ? C
where C is some user chosen constant, m and n
are the number of documents in source and target
languages respectively. Without the last constraint
(piij ? C) this optimization problem always gives an
integral solution and reduces to a maximum match-
ing problem (Jonker and Volgenant, 1987). Since
this solution may not be accurate, we allow many-to-
many mapping by setting the constant C to a value
less than one. In our experiments (Sec. 3), we
found that setting C to a value less than 1 gave bet-
ter performance analogous to the better performance
of soft Expectation Maximization (EM) compared
to hard-EM. The optimal solution of Eq. 1 can be
found efficiently using linear programming (Ravin-
dra et al, 1993).
2.2 Supervised Kernelized Sorting
Kernelized Sorting is an unsupervised technique to
align objects of different types, such as English and
Spanish documents (Quadrianto et al, 2009; Ja-
garalmudi et al, 2010). The main advantage of this
method is that it only uses the intra-language doc-
ument similarities to identify the alignments across
languages. In this section, we describe a supervised
variant of Kernelized Sorting which takes a set of
candidate alignments and learns to modify the intra-
language document similarities to respect the given
alignment. Since Kernelized Sorting does not rely
on the inter-lingual document similarities at all, we
hope that its supervised version is robust to noisy
alignments.
Let X and Y be the TFIDF weighted term ?
document matrices in both the languages and let
Kx and Ky be their linear dot product kernel ma-
trices, i.e. , Kx = XTX and Ky = Y TY .
Let ? ? {0, 1}m?n denote the permutation matrix
which captures the alignment between documents of
different languages, i.e. piij = 1 indicates docu-
ments xi and yj are aligned. Then Kernelized Sort-
ing formulates ? as the solution of the following op-
timization problem (Gretton et al, 2005):
argmax
?
tr(Kx?Ky?T ) (2)
= argmax
?
tr(XTX ? Y TY ?T ) (3)
In our supervised version of Kernelized Sorting,
we fix the permutation matrix (to say ??) and mod-
ify the kernel matrices Kx and Ky so that the ob-
jective function is maximized for the given permu-
tation. Specifically, we find a mapping for each lan-
guage, such that when the documents are projected
into their common subspaces they are more likely to
respect the alignment given by ??. Subsequently, the
test documents are also projected into the common
subspace and we return the nearest neighbors as the
aligned pairs.
Let U and V be the mappings for the required sub-
space in both the languages, then we want to solve
the following optimization problem:
argmax
U,V
tr(XTUUTX ?? Y TV V TY ??T )
s.t. UTU = I & V TV = I (4)
where I is an identity matrix of appropriate size. For
brevity, let Cxy denote the cross-covariance matrix
(i.e. Cxy = X??Y T ) then the above objective func-
tion becomes:
argmax
U,V
tr(UUTCxyV V TCTxy)
s.t. UTU = I & V TV = I (5)
We have used the cyclic property of the trace func-
tion while rewriting Eq. 4 to Eq. 5. We use alterna-
tive maximization to solve for the unknowns. Fixing
V (to say V0), rewriting the objective function using
the cyclic property of the trace function, forming the
Lagrangian and setting its derivative to zero results
in the following solution:
CxyV0V T0 CTxy U = ?u U (6)
For the initial iteration, we can substitute V0V T0 as
identity matrix which leaves the kernel matrix un-
changed. Similarly, fixing U (to U0) and solving the
optimization problem for V results:
CTxyU0UT0 Cxy V = ?v V (7)
149
In the special case where both V0V T0 and U0UT0
are identity matrices, the above equations reduce to
CxyCTxy U = ?u U and CTxyCxy V = ?v V . In
this particular case, we can simultaneously solve for
both U and V using Singular Value Decomposition
(SVD) as:
USV T = Cxy (8)
So for the first iteration, we do the SVD of the cross-
covariance matrix and get the mappings. For the
subsequent iterations, we use the mappings found by
the previous iteration, as U0 and V0, and solve Eqs.
6 and 7 alternatively.
2.3 Summary
In this section, we describe our procedure to recover
document alignments. We first convert documents
into bag of dictionary entries representation (Sec.
2.1). Then we solve the optimization problem in Eq.
1 to get the initial candidate alignments. We use the
LEMON3 graph library to solve the min-cost flow
problem. This step gives us the piij values for every
cross-lingual document pair. We use them to form
a relaxed permutation matrix (??) which is, subse-
quently, used to find the mappings (U and V ) for
the documents of both the languages (i.e. solv-
ing Eq. 8). We use these mappings to project both
source and target language documents into the com-
mon subspace and then solve the bipartite matching
problem to recover the alignment.
3 Experiments
For evaluation, we choose 2500 aligned docu-
ment pairs from Wikipedia in English-Spanish and
English-German language pairs. For both the data
sets, we consider only words that occurred more
than once in at least five documents. Of the words
that meet the frequency criterion, we choose the
most frequent 2000 words for English-Spanish data
set. But, because of the compound word phe-
nomenon of German, we retain all the frequent
words for English-German data set. Subsequently
we convert the documents into TFIDF weighted vec-
tors. The bilingual dictionaries for both the lan-
guage pairs are generated by running Giza++ (Och
and Ney, 2003) on the Europarl data (Koehn, 2005).
3https://lemon.cs.elte.hu/trac/lemon
En ? Es En ? De
Word-by-Word 0.597 0.564
CCA (? = 0.3) 0.627 0.485
CCA (? = 0.5) 0.628 0.486
CCA (? = 0.8) 0.637 0.487
OPCA 0.688 0.530
Ours (C = 0.6) 0.67 0.604
Ours (C = 1.0) 0.658 0.590
Table 1: Accuracy of different approaches on the
Wikipedia documents in English-Spanish and English-
German language pairs. For CCA, we regularize the
within language covariance matrices as (1??)XXT+?I
and the regularization parameter ? value is also shown.
We follow the process described in Sec. 2.3 to re-
cover the document alignment for our method.
We compare our approach with a dictionary based
approach, such as word-by-word translation, and
supervised approaches, such as CCA (Vinokourov
et al, 2003; Hotelling, 1936) and OPCA (Platt
et al, 2010). Word-by-word translation and our
approach use bilingual dictionary while CCA and
OPCA use a training corpus of aligned documents.
Since the bilingual dictionary is learnt from Eu-
roparl data set, for a fair comparison, we train su-
pervised approaches on 3000 document pairs from
Europarl data set. To prevent CCA from overfitting
to the training domain, we regularize it heavily. For
OPCA, we use a regularization parameter of 0.1 as
suggested by Platt et al (2010). For all the systems,
we construct a bipartite graph between the docu-
ments of different languages, with edge weight be-
ing the cross-lingual similarity given by the respec-
tive method and then find maximal matching (Jonker
and Volgenant, 1987). We report the accuracy of the
recovered alignment.
Table 1 shows accuracies of different methods on
both Spanish and German data sets. For comparison
purposes, we trained and tested CCA on documents
from same domain (Wikipedia). It achieves 75% and
62% accuracies for the two data sets respectively
but, as expected, it performed poorly when trained
on Europarl articles. On the English-German data
set, a simple word-by-word translation performed
better than CCA and OPCA. For both the language
pairs, our model performed better than word-by-
word translation method and competitively with the
150
supervised approaches. Note that our method does
not use any training data.
We also experimented with few values of the pa-
rameter C for the min-cost flow problem (Eq. 1).
As noted previously, setting C = 1 will reduce the
problem into a linear assignment problem. From
the results, we see that solving a relaxed version of
the problem gives better accuracies but the improve-
ments are marginal (especially for English-German).
4 Discussion
For both language pairs, the accuracy of the first
stage of our approach (Sec. 2.1) is almost same as
that of word-by-word translation system. Thus, the
improved performance of our system compared to
word-by-word translation shows the effectiveness of
the supervised Kernelized sorting.
The solution of our supervised Kernelized sorting
(Eq. 8) resembles Latent Semantic Indexing (Deer-
wester, 1988). Except, we use a cross-covariance
matrix instead of a term ? document matrix. Effi-
cient algorithms exist for solving SVD on arbitrarily
large matrices, which makes our approach scalable
to large data sets (Warmuth and Kuzmin, 2006). Af-
ter solving Eq. 8, the mappings U and V can be
improved by iteratively solving the Eqs. 6 and 7 re-
spectively. But it leads the mappings to fit the noisy
alignments exactly, so in this paper we stop after
solving the SVD problem.
The extension of our approach to the situation
with different number of documents on each side is
straight forward. The only thing that changes is the
way we compute alignment after finding the projec-
tion directions. In this case, the input to the bipar-
tite matching problem is modified by adding dummy
documents to the language that has fewer documents
and assigning a very high score to edges that connect
to the dummy documents.
5 Conclusion
In this paper we have presented an approach to re-
cover document alignments from a comparable cor-
pora using a bilingual dictionary. First, we use the
bilingual dictionary to find a set of candidate noisy
alignments. These noisy alignments are then fed into
supervised Kernelized Sorting, which learns to mod-
ify within language document similarities to respect
the given alignments.
Our approach exploits two complimentary infor-
mation sources to recover a better alignment. The
first step uses cross-lingual cues available in the
form of a bilingual dictionary and the latter step
exploits document structure captured in terms of
within language document similarities. Experimen-
tal results show that our approach performs better
than dictionary based approaches such as a word-
by-word translation and is also competitive with su-
pervised approaches like CCA and OPCA.
References
Lisa Ballesteros and W. Bruce Croft. 1996. Dictio-
nary methods for cross-lingual information retrieval.
In Proceedings of the 7th International Conference
on Database and Expert Systems Applications, DEXA
?96, pages 791?801, London, UK. Springer-Verlag.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Uncertainty
in Artificial Intelligence.
Scott Deerwester. 1988. Improving Information Re-
trieval with Latent Semantic Indexing. In Christine L.
Borgman and Edward Y. H. Pai, editors, Proceed-
ings of the 51st ASIS Annual Meeting (ASIS ?88), vol-
ume 25, Atlanta, Georgia, October. American Society
for Information Science.
William A. Gale and Kenneth W. Church. 1991. A pro-
gram for aligning sentences in bilingual corpora. In
Proceedings of the 29th annual meeting on Associ-
ation for Computational Linguistics, pages 177?184,
Morristown, NJ, USA. Association for Computational
Linguistics.
Wei Gao, John Blitzer, and Ming Zhou. 2008. Using
english information in non-english web search. In iN-
EWS ?08: Proceeding of the 2nd ACM workshop on
Improving non english web searching, pages 17?24,
New York, NY, USA. ACM.
Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong.
2009. Exploiting bilingual information to improve
web search. In Proceedings of Human Language Tech-
nologies: The 2009 Conference of the Association for
Computational Linguistics, ACL-IJCNLP ?09, pages
1075?1083, Morristown, NJ, USA. ACL.
Arthur Gretton, Arthur Gretton, Olivier Bousquet, Olivier
Bousquet, Er Smola, Bernhard Schlkopf, and Bern-
hard Schlkopf. 2005. Measuring statistical depen-
dence with hilbert-schmidt norms. In Proceedings of
Algorithmic Learning Theory, pages 63?77. Springer-
Verlag.
151
Aria Haghighi, Percy Liang, Taylor B. Kirkpatrick, and
Dan Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In Proceedings of ACL-08:
HLT, pages 771?779, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Ulf Hermjakob, Kevin Knight, and Hal Daume? III. 2008.
Name translation in statistical machine translation -
learning when to transliterate. In Proceedings of ACL-
08: HLT, pages 389?397, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
H. Hotelling. 1936. Relation between two sets of vari-
ables. Biometrica, 28:322?377.
Jagadeesh Jagaralmudi, Seth Juarez, and Hal Daume? III.
2010. Kernelized sorting for natural language process-
ing. In Proceedings of AAAI Conference on Artificial
Intelligence.
Jagadeesh Jagarlamudi and Hal Daume? III. 2010. Ex-
tracting multilingual topics from unaligned compara-
ble corpora. In Advances in Information Retrieval,
32nd European Conference on IR Research, ECIR,
volume 5993, pages 444?456, Milton Keynes, UK.
Springer.
R. Jonker and A. Volgenant. 1987. A shortest augment-
ing path algorithm for dense and sparse linear assign-
ment problems. Computing, 38(4):325?340.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 817?824, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 880?889, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Comput. Linguist., 31:477?
504, December.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Ari Pirkola, Turid Hedlund, Heikki Keskustalo, and
Kalervo Jrvelin. 2001. Dictionary-based cross-
language information retrieval: Problems, methods,
and research findings. Information Retrieval, 4:209?
230.
John C. Platt, Kristina Toutanova, and Wen-tau Yih.
2010. Translingual document representations from
discriminative projections. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 251?261,
Stroudsburg, PA, USA.
Novi Quadrianto, Le Song, and Alex J. Smola. 2009.
Kernelized sorting. In D. Koller, D. Schuurmans,
Y. Bengio, and L. Bottou, editors, Advances in Neural
Information Processing Systems 21, pages 1289?1296.
Piyush Rai and Hal Daume? III. 2009. Multi-label pre-
diction via sparse infinite cca. In Advances in Neural
Information Processing Systems, Vancouver, Canada.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, ACL ?99, pages 519?526,
Stroudsburg, PA, USA.
Sujith Ravi and Kevin Knight. 2009. Learning phoneme
mappings for transliteration without parallel data. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 37?45, Boulder, Colorado, June.
K. Ahuja Ravindra, L. Magnanti Thomas, and B. Orlin
James. 1993. Network flows: Theory, algorithms, and
applications.
Michael L. Littman Susan T. Dumais, Thomas K. Lan-
dauer. 1996. Automatic cross-linguistic information
retrieval using latent semantic indexing. In Working
Notes of the Workshop on Cross-Linguistic Informa-
tion Retrieval, SIGIR, pages 16?23, Zurich, Switzer-
land. ACM.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
J. Artif. Intell. Res. (JAIR), 37:141?188.
Raghavendra Udupa, K. Saravanan, A. Kumaran, and Ja-
gadeesh Jagarlamudi. 2009. Mint: A method for ef-
fective and scalable mining of named entity transliter-
ations from large comparable corpora. In EACL, pages
799?807. The Association for Computer Linguistics.
Alexei Vinokourov, John Shawe-taylor, and Nello Cris-
tianini. 2003. Inferring a semantic representation
of text via cross-language correlation analysis. In
Advances in Neural Information Processing Systems,
pages 1473?1480, Cambridge, MA. MIT Press.
Thuy Vu, AiTi Aw, and Min Zhang. 2009. Feature-based
method for document alignment in comparable news
corpora. In EACL, pages 843?851.
Manfred K. Warmuth and Dima Kuzmin. 2006. Ran-
domized pca algorithms with regret bounds that are
logarithmic in the dimension. In Neural Information
Processing Systems, pages 1481?1488.
152
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 407?412,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Domain Adaptation for Machine Translation by Mining Unseen Words
Hal Daume? III
University of Maryland
Collge Park, USA
hal@umiacs.umd.edu
Jagadeesh Jagarlamudi
University of Maryland
College Park, USA
jags@umiacs.umd.edu
Abstract
We show that unseen words account for a
large part of the translation error when mov-
ing to new domains. Using an extension of
a recent approach to mining translations from
comparable corpora (Haghighi et al, 2008),
we are able to find translations for otherwise
OOV terms. We show several approaches
to integrating such translations into a phrase-
based translation system, yielding consistent
improvements in translations quality (between
0.5 and 1.5 Bleu points) on four domains and
two language pairs.
1 Introduction
Large amounts of data are currently available to
train statistical machine translation systems. Un-
fortunately, these training data are often qualita-
tively different from the target task of the transla-
tion system. In this paper, we consider one specific
aspect of domain divergence (Jiang, 2008; Blitzer
and Daume? III, 2010): the out-of-vocabulary prob-
lem. By considering four different target domains
(news, medical, movie subtitles, technical documen-
tation) in two source languages (German, French),
we: (1) Ascertain the degree to which domain di-
vergence causes increases in unseen words, and the
degree to which this degrades translation perfor-
mance. (For instance, if all unknown words are
names, then copying them verbatim may be suffi-
cient.) (2) Extend known methods for mining dic-
tionaries from comparable corpora to the domain
adaptation setting, by ?bootstrapping? them based
on known translations from the source domain. (3)
Develop methods for integrating these mined dictio-
naries into a phrase-based translation system (Koehn
et al, 2007).
As we shall see, for most target domains, out of
vocabulary terms are the source of approximately
half of the additional errors made. The only excep-
tion is the news domain, which is sufficiently sim-
ilar to parliament proceedings (Europarl) that there
are essentially no new, frequent words in news. By
mining a dictionary and naively incorporating it into
a translation system, one can only do slightly bet-
ter than baseline. However, with a more clever inte-
gration, we can close about half of the gap between
baseline (unadapted) performance and an oracle ex-
periment. In most cases this amounts to an improve-
ment of about 1.5 Bleu points (Papineni et al, 2002)
and 1.5 Meteor points (Banerjee and Lavie, 2005).
The specific setting we consider is the one in
which we have plentiful parallel (?labeled?) data in a
source domain (eg., parliament) and plentiful com-
parable (?unlabeled?) data in a target domain (eg.,
medical). We can use the unlabeled data in the tar-
get domain to build a good language model. Finally,
we assume access to a very small amount of parallel
(?labeled?) target data, but only enough to evaluate
on, or run weight tuning (Och, 2003). All knowl-
edge about unseen words must come from the com-
parable data.
2 Background and Challenges
Domain adaptation is a well-studied field, both in the
NLP community as well as the machine learning and
statistics communities. Unlike in machine learning,
in the case of translation, it is not enough to simply
407
adjust the weights of a learned translation model to
do well on a new domain. As expected, we shall
see that unseen words pose a major challenge for
adapting translation systems to distant domains. No
machine learning approach to adaptation could hope
to attenuate this problem.
There have been a few attempts to measure or per-
form domain adaptation in machine translation. One
of the first approaches essentially performs test-set
relativization (choosing training samples that look
most like the test data) to improve translation per-
formance, but applies the approach only to very
small data sets (Hildebrand et al, 2005). Later
approaches are mostly based on a data set made
available in the 2007 StatMT workshop (Koehn and
Schroeder, 2007), and have attempted to use mono-
lingual (Civera and Juan, 2007; Bertoldi and Fed-
erico, 2009) or comparable (Snover et al, 2008) cor-
pus resources. These papers all show small, but sig-
nificant, gains in performance when moving from
Parliament domain to News domain.
3 Data
Our source domain is European Parliament
proceedings (http://www.statmt.org/
europarl/). We use three target domains: the
News Commentary corpus (News) used in the MT
Shared task at ACL 2007, European Medicines
Agency text (Emea), the Open Subtitles data
(Subs) and the PHP technical document data,
provided as part of the OPUS corpus http:
//urd.let.rug.nl/tiedeman/OPUS/).
We extracted development and test sets from each
of these corpora, except for news (and the source
domain) where we preserved the published dev and
test data. The ?source? domain of Europarl has 996k
sentences and 2130k words.) We count the number
of words and sentences in the English side of the
parallel data, which is the same for both language
pairs (i.e. both French-English and German-English
have the same English). The statistics are:
Comparable Tune Test
sents words sents sents
News 35k 753k 1057 2007
Emea 307k 4220k 1388 4145
Subs 30k 237k 1545 2493
PHP 6k 81k 1007 2000
Dom Most frequent OOV Words
News
(17%)
behavior, favor, neighbors, fueled, neigh-
boring, abe, wwii, favored, nicolas, fa-
vorable, zhao, ahmedinejad, bernanke,
favorite, phelps, ccp, skeptical, neighbor,
skeptics, skepticism
Emea
(49%)
renal, hepatic, subcutaneous, irbesartan,
ribavirin, olanzapine, serum, patienten,
dl, eine, sie, pharmacokinetics, riton-
avir, hydrochlorothiazide, erythropoietin,
efavirenz, hypoglycaemia, epoetin, blis-
ter, pharmacokinetic
Subs
(68%)
gonna, yeah, f...ing, s..., f..., gotta, uh,
wanna, mom, lf, ls, em, b....h, daddy, sia,
goddamn, sammy, tyler, bye, bigweld
PHP
(44%)
php, apache, sql, integer, socket, html,
filename, postgresql, unix, mysql, color,
constants, syntax, sesam, cookie, cgi, nu-
meric, pdf, ldap, byte
Table 1: For each domain, the percentage of target do-
main word tokens that are unseen in the source domain,
together with the most frequent English words in the tar-
get domains that do not appear in the source domain. (In
the actual data the subtitles words do not appear cen-
sored.)
All of these data sets actually come with parallel
target domain data. To obtain comparable data, we
applied to standard trick of taking the first 50% of
the English text as English and the last 50% of the
German text as German. While such data is more
parallel than, say, Wikipedia, it is far from parallel.
To get a better sense of the differences between
these domains, we give some simple statistics about
out of vocabulary words and examples in Table 1.
Here, for each domain, we show the percentage of
words (types) in the target domain that are unseen in
the Parliament data. As we can see, it is markedly
higher in Emea, Subs and PHP than in News.
4 Dictionary Mining
Our dictionary mining approach is based on Canon-
ical Correlation Analysis, as used previously by
(Haghighi et al, 2008). Briefly, given a multi-view
data set, Canonical Correlation Analysis is a tech-
nique to find the projection directions in each view
so that the objects when projected along these di-
408
rections are maximally aligned (Hotelling, 1936).
Given any new pair of points, the similarity between
the them can be computed by first projecting onto
the lower dimensions space and computing the co-
sine similarity between their projections. In general,
using all the eigenvectors is sub optimal and thus
retaining top eigenvectors leads to an improved gen-
eralizability.
Here we describe the use of CCA to find the trans-
lations for the OOV German words (Haghighi et al,
2008). From the target domain corpus we extract the
most frequent words (approximately 5000) for both
the languages. Of these, words that have translation
in the bilingual dictionary (learnt from Europarl) are
used as training data. We use these words to learn
the CCA projections and then mine the translations
for the remaining frequent words. The dictionary
mining involves multiple stages. In the first stage,
we extract feature vectors for all the words. We
use context and orthographic features. In the sec-
ond stage, using the dictionary probabilities of seen
words, we identify pairs of words whose feature vec-
tors are used to learn the CCA projection directions.
In the final stage, we project all the words into the
sub-space identified by CCA and mine translations
for the OOV words. We will describe each of these
steps in detail in this section.
For each of the frequent words we extract the con-
text vectors using a window of length five. To over-
come data sparsity issue, we truncate each context
word to its first seven characters. We discard all the
context features which co-occur with less than five
words. Among the remaining features, we consider
only the most frequent 2000 features in each lan-
guage. We convert the frequency vectors into TFIDF
vectors, center the data and then binarize the vec-
tors depending on if the feature value is positive of
not. We convert this data into word similarities us-
ing linear dot product kernel. We also represent each
word using the orthographic features, with n-grams
of length 1-3 and convert them into TFIDF form and
subsequently turn them into word similarities (again
using the linear kernel). Since we convert the data
into word similarities, the orthographic features are
relevant even though the script of source and tar-
get languages differ. Where as using the features
directly rending them useless for languages whose
script is completely different like Arabic and En-
waste blutdruckabfall 0.274233
bleeding blutdruckabfall 0.206440
stroke blutdruckabfall 0.190345
dysphagia dysphagie 0.233743
encephalopathy dysphagie 0.215684
lethargy dysphagie 0.203176
ribavirin ribavirin 0.314273
viraferonpeg ribavirin 0.206194
bioavailability verfgbarkeit 0.409260
xeristar xeristar 0.325458
cymbalta xeristar 0.284616
Table 2: Random unseen Emea words in German and
their mined translations.
glish. For each language we linearly combine the
kernel matrices obtained using the context vectors
and the orthographic features. We use incomlete
cholesky decomposition to reduce the dimension-
ality of the kernel matrices. We do the same pre-
processng for all words, the training words and the
OOV words. And the resulting feature vectors for
each word are used for learning the CCA projections
Since a word can have multiple translations, and
that CCA uses only one translation, we form a bipar-
tite graph with the training words in each language
as nodes and the edge weight being the translation
probability of the word pair. We then run Hungar-
ian algorithm to extract maximum weighted bipar-
tite matching (Jonker and Volgenant, 1987). We
then run CCA on the resulting pairs of the bipartite
matching to get the projection directions in each lan-
guage. We retain only the top 35% of the eigenvec-
tors. In other relevant experiments, we have found
that this setting of CCA outperforms the baseline ap-
proach.
We project all the frequent words, including the
training words, in both the languages into the lower
dimensional spaces and for each of the OOV word
return the closest five points from the other language
as potential new translations. The dictionary min-
ing, viewed subjectively and intrinsically, performs
quite well. In Table 2, we show four randomly se-
lected unseen German words from Emea (that do not
occur in the Parliament data), together with the top
three translations and associated scores (which are
not normalized). Based on a cursory evaluation of
5 randomly selected words in French and German
409
by native speakers (not the authors!), we found that
8/10 had correct mined translations.
5 Integration into MT System
The output of the dicionary mining approach is a list
of pairs (f, e) of foreign words and predicted En-
glish translations. Each of these comes with an as-
sociated score. There are two obvious ways to in-
tegrate such a dictionary into a phrase-based trans-
lation system: (1) Provide the dictionary entries as
(weighted) ?sentence? pairs in the parallel corpus.
These ?sentences? would each contain exactly one
word. The weighting can be derived from the trans-
lation probability from the dictionary mining. (2)
Append the phrase table of a baseline phrase-based
translation model trained only on source domain
data with the word pairs. Use the mining probability
as the phrase translation probabilities.
It turned out in preliminary experiments (on Ger-
man/Emea) that neither of these approaches worked
particularly well. The first approach did not work
at all, even with fairly extensive hand-tuning of the
sentence weights. It often hurt translation perfor-
mance. The second approach did not hurt transla-
tion performance, but did not help much either. It
led to an average improvement of only about 0.5
Bleu points, on development data. This is likely be-
cause weight tuning tuned a single weight to account
for the import of the phrase probabilities across both
?true? phrases as well as these ?mined? phrases.
We therefore came up with a slightly more com-
plex, but still simple, method for adding the dic-
tionary entries to the phrase table. We add four
new features to the model, and set the plain phrase-
translation probabilities for the dictionary entries to
zero. These new features are:
1. The dictionary mining translation probability.
(Zero for original phrase pairs.)
2. An indicator feature that says whether all Ger-
man words in this phrase pair were seen in
the source data. (This will always be true for
source phrases and always be false for dictio-
nary entries.)
3. An indicator that says whether all German
words in this phrase pair were seen in target
data. (This is not the negation of the previous
feature, because there are plenty of words in the
target data that had also been seen. This feature
might mean something like ?trust this phrase
pair a lot.?)
4. The conjunction of the previous two features.
Interestingly, only adding the first feature was
not helpful (performance remained about 0.5 Bleu
points above baseline). Adding only the last three
features (the indicator features) alone did not help at
all (performance was roughly on par with baseline).
Only when all four features were included did per-
formance improve significantly. In the results dis-
cussed in Section 6.2, we report results on test data
using the combination of these four features.
6 Experiments
In all of our experiments, we use two trigram lan-
guage models. The first is trained on the Gigaword
corpus. The second is trained on the English side of
the target domain corpus. The two language models
are traded-off against each other during weight tun-
ing. In all cases we perform parameter tuning with
MERT (Och, 2003), and results are averaged over
three runs with different random initializations.
6.1 Baselines and Oracles
Our first set of experiments is designed to establish
baseline performance for the domains. In these ex-
periments, we built a translation model based only
on the Parliament proceedings. We then tune it us-
ing the small amount of target-domain tuning data
and test on the corresponding test data. This is row
BASELINE in Table 3. Next, we build an oracle,
based on using the parallel target domain data. This
system, OR in Table 3 is constructed by training
a system on a mix of Parliament data and target-
domain data. The last line in this table shows the
percent improvement when moving to this oracle
system. As we can see, the gains range from tiny
(4% relative Bleu points, or 1.2 absolute Bleu points
for news, which may just be because we have more
data) to quite significant (73% for medical texts).
Finally, we consider how much of this gain we
could possible hope to realize by our dictionary min-
ing technique. In order to estimate this, we take
the OR system, and remove any phrases that con-
tain source-language words that appear in neither
410
BLEU Meteor
News Emea Subs PHP News Emea Subs PHP
BASELINE 23.00 26.62 10.26 38.67 34.58 27.69 15.96 24.66
German ORACLE-OOV 23.77 33.37 11.20 39.77 34.83 30.99 17.03 25.82
ORACLE 24.62 42.77 11.45 41.01 35.46 36.40 17.80 25.85
BASELINE 27.30 40.46 16.91 28.12 37.31 35.62 20.61 20.47
French ORACLE-OOV 27.92 50.03 19.17 29.48 37.57 39.55 21.79 20.91
ORACLE 28.55 59.49 19.81 30.15 38.12 45.55 23.52 21.77
ORACLE-OOV CHANGE +2% +24% +11% +5% +0% +12% +6% +7%
ORACLE CHANGE +4% +73% +15% +2% +2% +29% +13% +6%
Table 3: Baseline and oracle scores. The last two rows are the change between the baseline and the two types of
oracles, averaged over the two languages.
German French
BLEU Meteor BLEU Meteor
News 23.80 35.53 27.66 37.41
+0.80 +0.95 +0.36 +0.10
Emea 28.06 29.18 46.17 37.38
+1.44 +1.49 +1.51 +1.76
Subs 10.39 16.27 17.52 21.11
+0.13 +0.31 +0.61 +0.50
PHP 38.95 25.53 28.80 20.82
+0.28 +0.88 +0.68 +0.35
Table 4: Dictionary-mining system results. The italicized
number beneath each score is the improvement over the
BASELINE approach from Table 3.
the Parliament proceedings nor our list of high fre-
quency OOV terms. In other words, if our dictio-
nary mining system found as-good translations for
the words in its list as the (cheating) oracle system,
this is how well it would do. This is referred to
as OR-OOV in Table 3. As we can see, the upper
bound on performance based only on mining unseen
words is about halfway (absolute) between the base-
line and the full Oracle. Except in news, when it
is essentially useless (because the vocabulary differ-
ences between news and Parliament proceedings are
negligible). (Results using Meteor are analogous,
but omitted for space.)
6.2 Mining Results
The results of the dictionary mining experiment, in
terms of its effect on translation performance, are
shown in Table 4. As we can see, there is a mod-
est improvement in Subtitles and PHP, a markedly
large improvement in Emea, and a modest improve-
ment in News. Given how tight the ORACLE results
were to the BASELINE results in Subs and PHP, it is
quite impressive that we were able to improve per-
formance as much as we did. In general, across
all the data sets and both languages, we roughly
split the difference (in absolute terms) between the
BASELINE and ORACLE-OOV systems.
7 Discussion
In this paper we have shown that dictionary mining
techniques can be applied to mine unseen words in
a domain adaptation task. We have seen positive,
consistent results across two languages and four do-
mains. The proposed approach is generic enough to
be integrated into a wide variety of translation sys-
tems other than simple phrase-based translation.
Of course, unseen words are not the only cause
of translation divergence between two domains. We
have not addressed other issues, such as better es-
timation of translation probabilities or words that
change word sense across domains. The former is
precisely the area to which one might apply do-
main adaptation techniques from the machine learn-
ing community. The latter requires significant ad-
ditional work, since it is quite a bit more difficult
to spot foreign language words that are used in new
senses, rather that just never seen before. An alter-
native area of work is to extend these results beyond
simply the top-most-frequent words in the target do-
main.
411
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In In Proceed-
ings of Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization at ACL.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In StatMT ?09: Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation.
John Blitzer and Hal Daume? III. 2010. Do-
main adaptation. Tutorial at the International
Conference on Machine Learning, http:
//adaptationtutorial.blitzer.com/.
Jorge Civera and Alfons Juan. 2007. Domain adap-
tation in statistical machine translation with mixture
modelling. In StatMT ?07: Proceedings of the Second
Workshop on Statistical Machine Translation.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of the Con-
ference of the Association for Computational Linguis-
tics (ACL).
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
model for statistical machine translation based on in-
formation retrieval. In European Association for Ma-
chine Translation.
H. Hotelling. 1936. Relation between two sets of vari-
ables. Biometrica, 28:322?377.
J. Jiang. 2008. A literature survey on domain
adaptation of statistical classifiers. Available at
http://sifaka.cs.uiuc.edu/jiang4/
domain_adaptation/survey.
R. Jonker and A. Volgenant. 1987. A shortest augment-
ing path algorithm for dense and sparse linear assign-
ment problems. Computing, 38(4):325?340.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In StatMT ?07: Proceedings of the Second Workshop
on Statistical Machine Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
Conference of the Association for Computational Lin-
guistics (ACL), Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
Conference of the Association for Computational Lin-
guistics (ACL), pages 311?318.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation us-
ing comparable corpora. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
412
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435?1445,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
SenseSpotting: Never let your parallel data tie you to an old domain
Marine Carpuat1, Hal Daume? III2, Katharine Henry3,
Ann Irvine4, Jagadeesh Jagarlamudi5, Rachel Rudinger6
1 National Research Council Canada, marine.carpuat@nrc.gc.ca
2 CLIP, University of Maryland, me@hal3.name
3 CS, University of Chicago, kehenry@uchicago.edu
4 CLSP, Johns Hopkins University, anni@jhu.edu
5 IBM T.J. Watson Research Center, jags@us.ibm.com
6 CLSP, Johns Hopkins University, rachel.rudinger@aya.yale.edu
Abstract
Words often gain new senses in new do-
mains. Being able to automatically iden-
tify, from a corpus of monolingual text,
which word tokens are being used in a pre-
viously unseen sense has applications to
machine translation and other tasks sensi-
tive to lexical semantics. We define a task,
SENSESPOTTING, in which we build sys-
tems to spot tokens that have new senses
in new domain text. Instead of difficult
and expensive annotation, we build a gold-
standard by leveraging cheaply available
parallel corpora, targeting our approach to
the problem of domain adaptation for ma-
chine translation. Our system is able to
achieve F-measures of as much as 80%,
when applied to word types it has never
seen before. Our approach is based on
a large set of novel features that capture
varied aspects of how words change when
used in new domains.
1 Introduction
As Magnini et al (2002) observed, the domain of
the text that a word occurs in is a useful signal for
performing word sense disambiguation (e.g. in a
text about finance, bank is likely to refer to a finan-
cial institution while in a text about geography, it
is likely to refer to a river bank). However, in the
classic WSD task, ambiguous word types and a set
of possible senses are known in advance. In this
work, we focus on the setting where we observe
texts in two different domains and want to iden-
tify words in the second text that have a sense that
did not appear in the first text, without any lexical
knowledge in the new domain.
To illustrate the task, consider the French noun
rapport. In the parliament domain, this means
e?tat rapport re?gime
Govt. geo. state report (political) regime
Medical state (mind) report dietgeo. state ratio (political) regime
Science geo. state ratio (political) regimereport diet
Movies geo. state report (political) regimediet
Table 1: Examples of French words and their most
frequent senses (translations) in four domains.
(and is translated as) ?report.? However, in mov-
ing to a medical or scientific domain, the word
gains a new sense: ?ratio?, which simply does not
exist in the parliament domain. In a science do-
main, the ?report? sense exists, but it is dominated
about 12:1 by ?ratio.? In a medical domain, the
?report? sense remains dominant (about 2:1), but
the new ?ratio? sense appears frequently.
In this paper we define a new task that we call
SENSESPOTTING. The goal of this task is to iden-
tify words in a new domain monolingual text that
appeared in old domain text but which have a
new, previously unseen sense1. We operate un-
der the framework of phrase sense disambiguation
(Carpuat and Wu, 2007), in which we take au-
tomatically align parallel data in an old domain
to generate an initial old-domain sense inventory.
This sense inventory provides the set of ?known?
word senses in the form of phrasal translations.
Concrete examples are shown in Table 1. One of
our key contributions is the development of a rich
set of features based on monolingual text that are
indicative of new word senses.
This work is driven by an application need.
When machine translation (MT) systems are ap-
plied in a new domain, many errors are a result
of: (1) previously unseen (OOV) source language
words, or (2) source language words that appear
with a new sense and which require new transla-
1All features, code, data and raw results are at: github.
com/hal3/IntrinsicPSDEvaluation
1435
tions2 (Carpuat et al, 2012). Given monolingual
text in a new domain, OOVs are easy to identify,
and their translations can be acquired using dictio-
nary extraction techniques (Rapp, 1995; Fung and
Yee, 1998; Schafer and Yarowsky, 2002; Schafer,
2006; Haghighi et al, 2008; Mausam et al, 2010;
Daume? III and Jagarlamudi, 2011), or active learn-
ing (Bloodgood and Callison-Burch, 2010). How-
ever, previously seen (even frequent) words which
require new translations are harder to spot.
Because our motivation is translation, one sig-
nificant point of departure between our work and
prior related work (?3) is that we focus on word
tokens. That is, we are not interested only in the
question of ?has this known word (type) gained
a new sense??, but the much more specific ques-
tion of ?is this particular (token) occurrence of this
known word being used in a new sense?? Note
that for both the dictionary mining setting and the
active learning setting, it is important to consider
words in context when acquiring their translations.
2 Task Definition
Our task is defined by two data components. De-
tails about their creation are in ?5. First, we need
an old-domain sense dictionary, extracted from
French-English parallel text (in our case, parlia-
mentary proceedings). Next, we need new-domain
monolingual French text (we use medical text, sci-
entific text and movie subtitle text). Given these
two inputs, our challenge is to find tokens in the
new-domain text that are being used in a new sense
(w.r.t. the old-domain dictionary).
We assume that we have access to a small
amount of new domain parallel ?tuning data.?
From this data, we can extract a small new do-
main dictionary (?5). By comparing this new do-
main dictionary to the old domain dictionary, we
can identify which words have gained new senses.
In this way, we turn the SENSESPOTTING problem
into a supervised binary classification problem: an
example is a French word in context (in the new
domain monolingual text) and its label is positive
when it is being used in a sense that did not ex-
ist in the old domain dictionary. In this task, the
classifier is always making predictions on words
2Sense shifts do not always demand new translations;
some ambiguities are preserved across languages. E.g.,
fene?tre can refer to a window of a building or on a moni-
tor, but translates as ?window? either way. Our experiments
use bilingual data with an eye towards improving MT perfor-
mance: we focus on words that demand new translations.
outside this tuning data on word types it has never
seen before! From an applied perspective, the as-
sumption of a small amount of parallel data in the
new domain is reasonable: if we want an MT sys-
tem for a new domain, we will likely have some
data for system tuning and evaluation.
3 Related Work
While word senses have been studied extensively
in lexical semantics, research has focused on word
sense disambiguation, the task of disambiguating
words in context given a predefined sense inven-
tory (e.g., Agirre and Edmonds (2006)), and word
sense induction, the task of learning sense inven-
tories from text (e.g., Agirre and Soroa (2007)). In
contrast, detecting novel senses has not received as
much attention, and is typically addressed within
word sense induction, rather than as a distinct
SENSESPOTTING task. Novel sense detection
has been mostly motivated by the study of lan-
guage change over time. Most approaches model
changes in co-occurrence patterns for word types
when moving between corpora of old and modern
language (Sagi et al, 2009; Cook and Stevenson,
2010; Gulordava and Baroni, 2011).
Since these type-based models do not capture
polysemy in the new language, there have been a
few attempts at detecting new senses at the token-
level as in SENSESPOTTING. Lau et al (2012)
leverage a common framework to address sense
induction and disambiguation based on topic mod-
els (Blei et al, 2003). Sense induction is framed
as learning topic distributions for a word type,
while disambiguation consists of assigning topics
to word tokens. This model can interestingly be
used to detect newly coined senses, which might
co-exist with old senses in recent language. Bam-
man and Crane (2011) use parallel Latin-English
data to learn to disambiguate Latin words into En-
glish senses. New English translations are used as
evidence that Latin words have shifted sense. In
contrast, the SENSESPOTTING task consists of de-
tecting when senses are unknown in parallel data.
Such novel sense induction methods require
manually annotated datasets for the purpose of
evaluation. This is an expensive process and there-
fore evaluation is typically conducted on a very
small scale. In contrast, our SENSESPOTTING task
leverages automatically word-aligned parallel cor-
pora as a source of annotation for supervision dur-
ing training and evaluation.
1436
The impact of domain on novel senses has also
received some attention. Most approaches oper-
ate at the type-level, thus capturing changes in the
most frequent sense of a word when shifting do-
mains (McCarthy et al, 2004; McCarthy et al,
2007; Erk, 2006; Chan and Ng, 2007). Chan and
Ng (2007) notably show that detecting changes in
predominant sense as modeled by domain sense
priors can improve sense disambiguation, even af-
ter performing adaptation using active learning.
Finally, SENSESPOTTING has not been ad-
dressed directly in MT. There has been much inter-
est in translation mining from parallel or compara-
ble corpora for unknown words, where it is easy to
identify which words need translations. In con-
trast, SENSESPOTTING detects when words have
new senses and, thus, frequently a new translation.
Work on active learning for machine translation
has focused on collecting translations for longer
unknown segments (e.g., Bloodgood and Callison-
Burch (2010)). There has been some interest in
detecting which phrases that are hard to translate
for a given system (Mohit and Hwa, 2007), but dif-
ficulties can arise for many reasons: SENSESPOT-
TING focuses on a single problem.
4 New Sense Indicators
We define features over both word types and word
tokens. In our classification setting, each instance
consists of a French word token in context. Our
word type features ignore this context and rely on
statistics computed over our entire new domain
corpus. In contrast, our word token features con-
sider the context of the particular instance of the
word. If it were the case that only one sense ex-
isted for all word tokens of a particular type within
a single domain, we would expect our word type
features to be able to spot new senses without the
help of the word token features. However, in fact,
even within a single domain, we find that often a
word type is used with several senses, suggesting
that word token features may also be useful.
4.1 Type-level Features
Lexical Item Frequency Features A very ba-
sic property of the new domain that we hope to
capture is that word frequencies change, and such
changes might be indicative of a domain shift. As
such, we compute unigram log probabilities (via
smoothed relative frequencies) of each word un-
der consideration in the old domain and the new
domain. We then add as features these two log
probabilities as well as their difference. These are
our Type:RelFreq features.
N-gram Probability Features The goal of the
Type:NgramProb feature is to capture the fact
that ?unusual contexts? might imply new senses.
To capture this, we can look at the log probability
of the word under consideration given its N-gram
context, both according to an old-domain language
model (call this `oldng ) and a new-domain language
model (call this `newng ). However, we do not sim-
ply want to capture unusual words, but words that
are unlikely in context, so we also need to look at
the respective unigram log probabilities: `oldug and
`newug . From these four values, we compute corpus-
level (and therefore type-based) statistics of the
new domain n-gram log probability (`newng , the dif-
ference between the n-gram probabilities in each
domain (`newng ? `oldng ), the difference between the
n-gram and unigram probabilities in the new do-
main (`newng ? `newug ), and finally the combined differ-
ence: `newng ? `newug + `oldug ? `oldng ). For each of these
four values, we compute the following type-based
statistics over the monolingual text: mean, stan-
dard deviation, minimum value, maximum value
and sum. We use trigram models.
Topic Model Feature The intuition behind the
topic model feature is that if a word?s distribu-
tion over topics changes when moving into a new
domain, it is likely to also gain a new sense.
For example, suppose that in our old domain, the
French word enceinte is only used with the sense
?wall,? but in our new domain, enceinte may have
senses corresponding to either ?wall? or to ?preg-
nant.? We would expect to see this reflected in
enceinte?s distribution over topics: the topic that
places relatively high probabilities on words such
as ?be?be?? (English ?baby?) and enfant (English
?child?) will also place a high probability on en-
ceinte when trained on new domain data. In the
old domain, however, we would not expect a sim-
ilar topic (if it exists) to give a high probabil-
ity to enceinte. Based on this intuition, for all
words w, where To and Tn are the set of old
and new topics and Po and Pn are the old and
new distributions defined over them, respectively,
and cos is the cosine similarity between a pair
of topics, we define the feature Type:TopicSim:?
t?Tn,t??To Pn(t|w)Po(t?|w) cos(t, t?). For aword w, the feature value will be high if, for
each new domain topic t that places high proba-
bility on w, there is an old domain topic t? that
1437
is similar to t and also places a high probabil-
ity on w. Conversely, if no such topic exists, the
score will be low, indicating the word has gained
a new sense. We use the online LDA (Blei et
al., 2003; Hoffman et al, 2010), implemented
in http://hunch.net/?vw/ to compute topics on
the two domains separately. We use 100 topics.
Context Feature It is expected that words acquir-
ing new senses will tend to neighbor different sets
of words (e.g. different arguments, prepositions,
parts of speech, etc.). Thus, we define an addi-
tional type level feature to be the ratio of the num-
ber of new domain n-grams (up to length three)
that contain word w and which do not appear in
the old domain to the total number of new domain
n-grams containing w. With Nw indicating the set
of n-grams in the new domain which contain w,
Ow indicating the set of n-grams in the old domain
which contain w, and |Nw ? Ow| indicating the
n-grams which contain w and appear in the new
but not the old domain, we define Type:Contextas
|Nw?Ow|
|Nw| . We do not count n-grams containingOOVs, as they may simply be instances of apply-
ing the same sense of a word to a new argument
4.2 Token-level Features
N-gram Probability Features Akin to the N-
gram probability features at the type level (namely,
Token:NgramProb), we compute the same val-
ues at the token level (new/old domain and un-
igram/trigram). Instead of computing statistics
over the entire monolingual corpus, we use the in-
stantaneous values of these features for the token
under consideration. The six features we construct
are: unigram (and trigram) log probabilities in the
old domain, the new domain, and their difference.
Context Features Following the type-level n-
gram feature, we define features for a particular
word token based on its n-gram context. For token
wi, in position i in a given sentence, we consider
its context words in a five word window: wi?2,
wi?1, wi+1, and wi+2. For each of the four con-
textual words in positions p = {?2,?1, 1, 2},
relative to i, we define the following feature, To-
ken:CtxCnt: log(cwp) where cwp is the number
of times word wp appeared in position p relative
to wi in the OLD-domain data. We also define a
single feature which is the percent of the four con-
textual words which had been seen in the OLD-
domain data, Token:Ctx%.
Token-Level PSD Features These features aim
to capture generalized characteristics of a context.
Towards this end, first, we pose the problem as a
phrase sense disambiguation (PSD) problem over
the known sense inventory. Given a source word in
a context, we train a classifier to predict the most
likely target translation. The ground truth labels
(target translation for a given source word) for this
classifier are generated from the phrase table of
the old domain data. We use the same set of fea-
tures as in Carpuat and Wu (2007). Second, given
a source word s, we use this classifier to com-
pute the probability distribution of target transla-
tions (p(t|s)). Subsequently, we use this prob-
ability distribution to define new features for the
SENSESPOTTING task. The idea is that, if a word
is used in one of the known senses then its con-
text must have been seen previously and hence we
hope that the PSD classifier outputs a spiky dis-
tribution. On the other hand, if the word takes a
new sense then hopefully it is used in an unseen
context resulting in the PSD classifier outputting
an uniform distribution. Based on this intuition,
we add the following features: MaxProb is the
maximum probability of any target translation:
maxt p(t|s). Entropy is the entropy of the proba-
bility distribution: ??t p(t|s) log p(t|s). Spread
is the difference between maximum and mini-
mum probabilities of the probability distribution:(
maxt p(t|s) ? mint p(t|s)
). Confusion is the
uncertainty in the most likely prediction given the
source token: mediantp(t|s)maxt p(t|s) . The use of median inthe numerator rather than the second best is mo-
tivated by the observation that, in most cases, top
ranked translations are of the same sense but differ
in morphology.
We train the PSD classifier in two modes:
1) a single global classifier that predicts the
target translation given any source word; 2) a
local classifier for each source word. When
training the global PSD classifier, we include
some lexical features that depend on the source
word. For both modes, we use real valued
and binned features giving rise to four families
of features Token:G-PSD, Token:G-PSDBin,
Token:L-PSD and Token:L-PSDBin.
Prior vs. Posterior PSD Features When the
PSD classifier is trained in the second mode, i.e.
one classifier per word type, we can define ad-
ditional features based on the prior (with out the
word context) and posterior (given the word?s
context) probability distributions output by the
classifier, i.e. pprior(t|s) and ppost.(t|s) respec-
1438
Domain Sentences Lang Tokens Types
Hansard 8,107,356 fr 161,695,309 191,501en 144,490,268 186,827
EMEA 472,231 fr 6,544,093 34,624en 5,904,296 29,663
Science 139,215 fr 4,292,620 117,669en 3,602,799 114,217
Subs 19,239,980 fr 154,952,432 361,584en 174,430,406 293,249
Table 2: Basic characteristics of the parallel data.
tively. We compute the following set of fea-
tures referred to as Token:PSDRatio: SameMax
checks if both the prior and posterior distri-
butions have the same translation as the most
likely translation. SameMin is same as the
above feature but check if the least likely trans-
lation is same. X-OR MinMax is the exclusive-
OR of SameMax and SameMin features. KL
is the KL-divergence between the two distri-
butions. Since KL-divergence is asymmetric,
we use KL(pprior||ppost.) and KL(ppost.||pprior).
MaxNorm is the ratio of maximum probabilities
in prior and posterior distributions. SpreadNorm
is the ratio of spread of the prior and posterior dis-
tributions, where spared is the difference between
maximum and minimum probabilities of the dis-
tribution as defined earlier. ConfusionNorm is the
ratio of confusion of the prior and posterior distri-
butions, where confusion is defined as earlier.
5 Data and Gold Standard
The first component of our task is a parallel cor-
pus of old domain data, for which we use the
French-English Hansard parliamentary proceed-
ings (http://www.parl.gc.ca). From this, we
extract an old domain sense dictionary, using the
Moses MT framework (Koehn et al, 2007). This
defines our old domain sense dictionary. For new
domains, we use three sources: (1) the EMEA
medical corpus (Tiedemann, 2009), (2) a corpus of
scientific abstracts, and (3) a corpus of translated
movie subtitles (Tiedemann, 2009). Basic statis-
tics are shown in Table 2. In all parallel corpora,
we normalize the English for American spelling.
To create the gold standard truth, we followed
a lexical sample apparoach and collected a set
of 300 ?representative types? that are interest-
ing to evaluate on, because they have multiple
senses within a single domain or whose senses
are likely to change in a new domain. We used
a semi-automatic approach to identify represen-
tative types. We first used the phrase table from
Parallel Repr. Repr. % New
Sents fr-tok Types Tokens Sense
EMEA 24k 270k 399 35,266 52.0%
Science 22k 681k 425 8,355 24.3%
Subs 36k 247k 388 22,598 43.4%
Table 3: Statistics about representative words and
the size of the development sets. The columns
show: the total amount of parallel development
data (# of sentences and tokens in French), # of
representative types that appear in this corpus, the
corresponding # of tokens, and the percentage of
these tokens that correspond to ?new senses.?
the Moses output to rank phrases in each domain
using TF-IDF scores with Okapi BM25 weight-
ing. For each of the three new domains (EMEA,
Science, and Subs), we found the intersection of
phrases between the old and the new domain. We
then looked at the different translations that each
had in the phrase table and a French speaker se-
lected a subset that have multiple senses.3
In practice, we limited our set alost entirely
to source words, and included only a single multi-
word phrase, vue des enfants, which usually trans-
lates as ?for children? in the old domain but al-
most always translates as ?sight of children? in
the EMEA domain (as in ?. . . should be kept out
of the sight of children?). Nothing in the way we
have defined, approached, or evaluated the SENS-
ESPOTTING task is dependent on the use of rep-
resentative words instead of longer representative
phrases. We chose to consider mostly source lan-
guage words for simplicity and because it was eas-
ier to identify good candidate words.
In addition to the manually chosen words, we
also identified words where the translation with
the highest lexical weight varied in different do-
mains, with the intuition being that are the words
that are likely to have acquired a new sense. The
top 200 words from this were added to the man-
ually selected representative words to form a list
of 450. Table 3 shows some statistics about these
words across our three test domains.
6 Experiments
6.1 Experimental setup
Our goal in evaluation is to be able to under-
stand what our approach is realistically capa-
ble of. One challenge is that the distribution
3In order to create the evaluation data, we used both sides
of the full parallel text; we do not use the English side of the
parallel data for actually building systems.
1439
of representative words is highly skewed.4 We
present results in terms of area under the ROC
curve (AUC),5 micro-averaged precision/recall/f-
measure and macro-averaged precision/recall/f-
measure. For macro-averaging, we compute a sin-
gle confusion matrix over all the test data and
determining P/R/F from that matrix. For micro-
averaging, we compute a separate confusion ma-
trix for each word type on the French side, com-
pute P/R/F for each of these separately, and then
average the results. (Thus, micro-F is not a
function of micro-P and micro-R.) The AUC and
macro-averaged scores give a sense of how well
the system is doing on a type-level basis (es-
sentially weighted by type frequency), while the
micro-averaged scores give a sense as to how well
the system is doing on individual types, not taking
into account their frequencies.
For most of our results, we present standard
deviations to help assess significance (?2? is
roughly a 90% confidence interval). For our re-
sults, in which we use new-domain training data,
we compute these results via 16-fold cross valida-
tion. The folds are split across types so the sys-
tem is never being tested on a word type that it has
seen before. We do this because it more closely re-
sembles our application goals. We do 16-fold for
convenience, because we divide the data into bi-
nary folds recursively (thus having a power-of-two
is easier), with an attempt to roughly balance the
size of the training sets in each fold (this is tricky
because of the skewed nature of the data). This en-
tire 16-fold cross-validation procedure is repeated
10 times and averages and standard deviations are
over the 160 replicates.
We evaluate performance using our type-level
features only, TYPEONLY, our token-level fea-
tures only, TOKENONLY, and using both our type
and our token level features, ALLFEATURES.
We compare our results with two baselines:
RANDOM and CONSTANT. RANDOM predicts
new-sense or not-new-sense randomly and with
equal probability. CONSTANT always predicts
new-sense, achieving 100% recall and a macro-
level precision that is equal to the percent of repre-
sentative words which do have a new sense, mod-
ulo cross-validation splits (see Table 3). Addi-
4The most frequent (voie) appears 3881 times; there are
60 singleton words on average across the three new domains.
5AUC is the probability that the classifier will assign a
higher score to a randomly chosen positive example than to a
randomly chosen negative example (Wikipedia, 2013).
tionally, we compare our results with a type-level
oracle, TYPEORACLE. For all tokens of a given
word type, the oracle predicts the majority label
(new-sense or not-new-sense) for that word type.
These results correspond to an upper bound for the
TYPEONLY experiments.
6.2 Classification Setup
For all experiments, we use a linear classifier
trained by stochastic gradient descent to optimize
logistic loss. We also did some initial experi-
ments on development data using boosted deci-
sion trees instead and other loss functions (hinge
loss, squared loss), but they never performed as
well. In all cases, we perform 20 passes over
the training data, using development data to per-
form early stopping (considered at the end of each
pass). We also use development data to tune a
regularizer (either `1 or `2) and its regularization
weight.6 Finally, all real valued features are au-
tomatically bucketed into 10 consecutive buckets,
each with (approximately) the same number of
elements. Each learner uses a small amount of
development data to tune a threshold on scores
for predicting new-sense or not-a-new-sense, us-
ing macro F-measure as an objective.
6.3 Result Summary
Table 4 shows our results on the SENSESPOT-
TING task. Classifiers based on the features
that we defined outperform both baselines in all
macro-level evaluations for the SENSESPOTTING
task. Using AUC as an evaluation metric, the
TOKENONLY, TYPEONLY, and ALLFEATURES
models performed best on EMEA, Science, and
Subtitles data, respectively. Our token-level fea-
tures perform particularly poorly on the Science
and Subtitles data. Although the model trained on
only those features achieves reasonable precision
(72.59 and 70.00 on Science and Subs, respec-
tively), its recall is very low (20.41 and 35.15), in-
dicating that the model classifies many new-sense
words as not-new-sense. Most of our token-level
features capture the intuition that when a word to-
ken appears in new or infrequent contexts, it is
likely to have gained a new sense. Our results indi-
cate that this intuition was more fruitful for EMEA
than for Science or Subs.
In contrast, the type-only features (TYPEONLY)
6We use http://hunch.net/?vw/ version 7.1.2,and run it with the following arguments that affect learning
behavior: --exact adaptive norm --power t 0.5
1440
Macro Micro
AUC P R F P R F
EMEA
RANDOM 50.34 ? 0.60 51.24 ? 0.59 50.09 ? 1.18 50.19 ? 0.75 47.04 ? 0.60 56.07 ? 1.99 37.27 ? 0.91
CONSTANT 50.00 ? 0.00 50.99 ? 0.00 100.0 ? 0.00 67.09 ? 0.00 45.80 ? 0.00 100.0 ? 0.00 52.30 ? 0.00
TYPEONLY 55.91 ? 1.13 69.76 ? 3.45 43.13 ? 1.42 41.61 ? 1.07 77.92 ? 2.04 50.12 ? 2.35 31.26 ? 0.63
TYPEORACLE 88.73 ? 0.00 87.32 ? 0.00 86.76 ? 0.00 87.04 ? 0.00 90.01 ? 0.00 67.46 ? 0.00 59.39 ? 0.00
TOKENONLY 78.80 ? 0.52 69.83 ? 1.59 75.58 ? 2.61 69.40 ? 1.92 59.03 ? 1.70 62.53 ? 1.66 43.39 ? 0.94
ALLFEATURES 79.60 ? 1.20 68.11 ? 1.19 79.84 ? 2.27 71.64 ? 1.83 55.28 ? 1.11 71.50 ? 1.62 46.83 ? 0.62
Science
RANDOM 50.18 ? 0.78 24.48 ? 0.57 50.32 ? 1.33 32.92 ? 0.79 46.99 ? 0.51 60.32 ? 1.06 34.72 ? 1.03
CONSTANT 50.00 ? 0.00 24.34 ? 0.00 100.0 ? 0.00 39.15 ? 0.00 44.39 ? 0.00 100.0 ? 0.00 50.44 ? 0.00
TYPEONLY 77.06 ? 1.23 66.07 ? 2.80 36.28 ? 4.10 34.50 ? 4.06 84.97 ? 0.82 36.81 ? 2.33 24.22 ? 1.70
TYPEORACLE 88.76 ? 0.00 78.43 ? 0.00 69.29 ? 0.00 73.54 ? 0.00 84.19 ? 0.00 67.41 ? 0.00 52.67 ? 0.00
TOKENONLY 66.62 ? 0.47 60.50 ? 3.11 28.05 ? 2.06 30.81 ? 2.75 76.21 ? 1.78 36.57 ? 2.23 24.68 ? 1.36
ALLFEATURES 73.91 ? 0.66 50.59 ? 2.08 60.60 ? 2.04 47.54 ? 1.52 66.72 ? 1.19 62.30 ? 1.36 40.22 ? 1.03
Subs
RANDOM 50.26 ? 0.69 42.47 ? 0.60 50.17 ? 0.84 45.68 ? 0.68 52.18 ? 1.32 54.63 ? 2.01 39.87 ? 2.10
CONSTANT 50.00 ? 0.00 42.51 ? 0.00 100.0 ? 0.00 59.37 ? 0.00 50.63 ? 0.00 100.0 ? 0.00 58.67 ? 0.00
TYPEONLY 67.16 ? 0.73 76.41 ? 1.51 31.91 ? 3.15 36.37 ? 2.58 90.03 ? 0.61 34.78 ? 1.12 26.20 ? 0.61
TYPEORACLE 81.35 ? 0.00 83.12 ? 0.00 70.23 ? 0.00 76.12 ? 0.00 90.62 ? 0.00 52.37 ? 0.00 44.43 ? 0.00
TOKENONLY 63.30 ? 0.99 63.17 ? 2.31 45.38 ? 2.07 43.30 ? 1.29 76.38 ? 1.68 49.70 ? 1.76 37.92 ? 1.20
ALLFEATURES 69.26 ? 0.60 63.48 ? 1.77 56.22 ? 2.66 52.78 ? 1.96 67.55 ? 0.83 62.18 ? 1.45 43.85 ? 0.90
Table 4: Complete SENSESPOTTING results for all domains. The scores are from cross-validation on
a single domain; in all cases, higher is better. Two standard deviations of performance over the cross-
validation are shown in small type. For all domains and metrics, the highest (not necessarily statistically
significant) non-oracle results are bolded.
are relatively weak for predicting new senses on
EMEA data but stronger on Subs (TYPEONLY
AUC performance is higher than both baselines)
and even stronger on Science data (TYPEONLY
AUC and f-measure performance is higher
than both baselines as well as the ALLFEA-
TURESmodel). In our experience with the three
datasets, we know that the Science data, which
contains abstracts from a wide variety of scientific
disciplines, is the most diverse, followed by the
Subs data, and then EMEA, which mostly consists
of text from drug labels and tends to be quite repet-
itive. Thus, it makes sense that type-level features
would be the most informative for the least homo-
geneous dataset. Representative words in scien-
tific text are likely to appear in variety of contexts,
while in the EMEA data they may only appear in
a few, making it easier to contrast them with the
distributions observed in the old domain data.
For all domains, in micro-level evaluation, our
models fail to outperform the CONSTANT base-
line. Recall that the micro-level evaluation com-
putes precision, recall, and f-measure for all word
tokens of a given word type and then averages
across word types. We observe that words that are
less frequent in both the old and the new domains
are more likely to have a new sense than more fre-
quent words, which causes the CONSTANT base-
line to perform reasonably well. In contrast, it is
more difficult for our models to make good pre-
dictions for less frequent words. A low frequency
in the new domain makes type level features (esti-
mated over only a few instances) noisy and unreli-
able. Similarly, a low frequency in the old domain
makes the our token level features, which all con-
trast with old domain instances of the word type.
6.4 Feature Ablation
In the previous section, we observed that (with one
exception) both Type-level and Token-level fea-
tures are useful in our task (in some cases, essen-
tial). In this section, we look at finer-grained fea-
ture distinctions through a process of feature ab-
lation. In this setting, we begin with all features
in a model and remove one feature at a time, al-
ways removing the feature that hurts performance
least. For these experiments, we determine which
feature to remove using AUC. Note that we?re ac-
tually able to beat (by 2-4 points AUC) the scores
from Table 4 by removing features!
The results here are somewhat mixed. In EMEA
and Science, one can actually get by (accord-
ing to AUC) with very few features: just two
(Type:NgramProband Type:Context) are suffi-
cient to achieve optimal AUC scores. To get
higher Macro-F scores requires nearly all the fea-
tures, though this is partially due to the choice of
1441
EMEA AUC MacF
ALLFEATURES 79.60 71.64
?Token:L-PSDBin 77.09 70.50
?Type:RelFreq 78.43 72.19
?Token:G-PSD 79.66 72.11
?Type:Context 79.66 72.45
?Token:Ctx% 78.91 73.37
?Type:TopicSim 78.05 71.33
?Token:CtxCnt 76.90 71.72
?Token:L-PSD 76.03 73.35
?Type:NgramProb 73.32 69.54
?Token:G-PSDBin 74.41 69.76
?Token:NgramProb 69.78 68.89
?Token:PSDRatio 48.38 3.45
Science AUC MacF
ALLFEATURES 73.91 47.54
?Token:L-PSDBin 76.26 53.69
?Token:G-PSD 77.04 53.56
?Token:G-PSDBin 77.44 54.54
?Token:L-PSD 77.85 56.05
?Token:PSDRatio 77.92 57.34
?Token:CtxCnt 77.85 54.42
?Type:Context 78.17 55.45
?Token:Ctx% 78.06 55.04
?Type:TopicSim 77.83 54.57
?Token:NgramProb 76.98 51.02
?Type:RelFreq 74.25 49.57
?Type:NgramProb 50.00 0.00
Subs AUC MacF
ALLFEATURES 69.26 52.78
?Type:NgramProb 69.13 53.33
?Token:G-PSDBin 70.23 54.72
?Token:CtxCnt 71.23 58.35
?Token:L-PSDBin 72.07 57.85
?Token:G-PSD 72.17 57.33
?Type:TopicSim 72.31 58.41
?Token:Ctx% 72.17 56.17
?Token:NgramProb 71.35 59.26
?Token:PSDRatio 70.33 46.88
?Token:L-PSD 69.05 53.31
?Type:RelFreq 65.25 48.22
?Type:Context 50.00 0.00
Table 5: Feature ablation results for all three corpora. Selection criteria is AUC, but Macro-F is presented
for completeness. Feature selection is run independently on each of the three datasets. The features
toward the bottom were the first selected.
AUC Macro-F Micro-F
EMEA
TYPEONLY 71.43 ? 0.94 52.62 ? 3.41 38.67 ? 1.35
TOKENONLY 73.75 ? 1.11 67.77 ? 4.18 45.49 ? 3.96
ALLFEATURES 72.19 ? 4.07 67.26 ? 7.88 49.29 ? 3.55
XV-ALLFEATURES 79.60 ? 1.20 71.64 ? 1.83 46.83 ? 0.62
Science
TYPEONLY 75.19 ? 0.89 51.53 ? 2.55 37.14 ? 4.41
TOKENONLY 71.24 ? 1.45 47.27 ? 1.11 40.48 ? 1.84
ALLFEATURES 74.14 ? 0.93 48.86 ? 3.94 43.20 ? 3.16
XV-ALLFEATURES 73.91 ? 0.66 47.54 ? 1.52 40.22 ? 1.03
Subs
TYPEONLY 60.90 ? 1.47 39.21 ? 14.78 24.77 ? 2.78
TOKENONLY 62.00 ? 1.16 49.74 ? 6.30 42.95 ? 3.92
ALLFEATURES 60.12 ? 2.11 50.16 ? 8.63 38.56 ? 5.20
XV-ALLFEATURES 69.26 ? 0.60 52.78 ? 1.96 43.85 ? 0.90
Table 6: Cross-domain test results on the SENS-
ESPOTTING task. Two standard deviations are
shown in small type. Only AUC, Macro-F and
Micro-F are shown for brevity.
AUC as the measure on which to ablate. It?s quite
clear that for Science, all the useful information
is in the type-level features, a result that echoes
what we saw in the previous section. While for
EMEA and Subs, both type- and token-level fea-
tures play a significant role. Considering the six
most useful features in each domain, the ones that
pop out as frequently most useful are the global
PSD features, the ngram probability features (ei-
ther type- or token-based), the relative frequency
features and the context features.
6.5 Cross-Domain Training
One disadvantage to the previous method for eval-
uating the SENSESPOTTING task is that it requires
parallel data in a new domain. Suppose we have no
parallel data in the new domain at all, yet still want
to attack the SENSESPOTTING task. One option is
to train a system on domains for which we do have
parallel data, and then apply it in a new domain.
This is precisely the setting we explore in this sec-
tion. Now, instead of performing cross-validation
in a single domain (for instance, Science), we take
the union of all of the training data in the other
domains (e.g., EMEA and Subs), train a classifier,
and then apply it to Science. This classifier will al-
most certainly be worse than one trained on NEW
(Science) but does not require any parallel data in
that domain. (Hyperparameters are chosen by de-
velopment data from the OLD union.)
The results of this experiment are shown in
Table 6. We include results for TOKENONLY,
TYPEONLY and ALLFEATURES; all of these are
trained in the cross-domain setting. To ease com-
parison to the results that do not suffer from do-
main shift, we also present ?XV-ALLFEATURES?,
which are results copied from Table 4 in which
parallel data from NEW is used. Overall, there is a
drop of about 7.3% absolute in AUC, moving from
XV-ALLFEATURES to ALLFEATURES, including
a small improvement in Science (likely because
Science is markedly smaller than Subs, and ?more
difficult? than EMEA with many word types).
6.6 Detecting Most Frequent Sense Changes
We define a second, related task: MOSTFRE-
QSENSECHANGE. In this task, instead of predict-
ing if a given word token has a sense which is
brand new with respect to the old domain, we pre-
dict whether it is being used with a a sense which
is not the one that was observed most frequently
in the old domain. In our EMEA, Science, and
Subtitles data, 68.2%, 48.3%, and 69.6% of word
tokens? predominant sense changes.
1442
6 12 25 50 100.32
.40
.50
.63
Science
Macro
?F
% of data 6 12 25 50 100
.40
.50
.63
.79 EMEA
% of data 
 
6 12 25 50 100
.40
.50
.63
Subs
% of data 
 
TypeOracleRandomAllFeatures
Figure 1: Learning curves for the three domains. X-axis is percent of data used, Y-axis is Macro-F score.
Both axes are in log scale to show the fast rate of growth. A horizontal bar corresponding to random
predictions, and the TYPEORACLE results are shown for comparison.
AUC Macro-F Micro-F
EMEA
RANDOM 50.54 ? 0.41 58.23 ? 0.34 49.69 ? 0.85
CONSTANT 50.00 ? 0.00 82.15 ? 0.00 74.43 ? 0.00
TYPEONLY 55.05 ? 1.00 67.45 ? 1.35 65.72 ? 0.59
TYPEORACLE 88.36 ? 0.00 90.64 ? 0.00 77.46 ? 0.00
TOKENONLY 66.42 ? 1.07 80.27 ? 0.50 68.96 ? 0.58
ALLFEATURES 58.64 ? 3.45 80.57 ? 0.45 69.40 ? 0.51
Science
RANDOM 50.13 ? 0.78 49.05 ? 0.82 48.19 ? 1.47
CONSTANT 50.00 ? 0.00 65.21 ? 0.00 73.22 ? 0.00
TYPEONLY 68.32 ? 1.05 54.70 ? 2.35 57.04 ? 1.52
TYPEORACLE 91.41 ? 0.00 86.71 ? 0.00 74.26 ? 0.00
TOKENONLY 68.49 ? 0.59 62.76 ? 0.89 64.40 ? 1.08
ALLFEATURES 68.31 ? 0.93 64.73 ? 1.93 67.20 ? 1.65
Subs
RANDOM 50.27 ? 0.27 56.93 ? 0.29 50.93 ? 1.11
CONSTANT 50.00 ? 0.00 79.96 ? 0.00 76.26 ? 0.00
TYPEONLY 60.36 ? 0.90 67.78 ? 1.98 61.58 ? 1.78
TYPEORACLE 82.16 ? 0.00 87.96 ? 0.00 73.87 ? 0.00
TOKENONLY 59.49 ? 1.04 77.79 ? 0.82 73.51 ? 0.68
ALLFEATURES 54.97 ? 0.89 77.30 ? 1.58 72.29 ? 1.68
Table 7: Cross-validation results on the MOST-
FREQSENSECHANGE task. Two standard devia-
tions are shown in small type.
We use the same set of features and learn-
ing framework to generate and evaluate models
for this task. While the SENSESPOTTING task
has MT utility in suggesting which new domain
words demand a new translation, the MOSTFRE-
QSENSECHANGE task has utility in suggesting
which words demand a new translation proba-
bility distribution when shifting to a new do-
main. Table 7 shows the results of our MOSTFRE-
QSENSECHANGE task experiments.
Results on the MOSTFREQSENSECHANGE
task are somewhat similar to those for the SENS-
ESPOTTING task. Again, our models perform bet-
ter under a macro-level evaluation than under a
micro-level evaluation. However, in contrast to
the SENSESPOTTING results, token-level features
perform quite well on their own for all domains.
It makes sense that our token level features have a
better chance of success on this task. The impor-
tant comparison now is between a new domain to-
ken in context and the majority of the old domain
tokens of the same word type. This comparison
is likely to be more informative than when we are
equally interested in identifying overlap between
the current token and any old domain senses. Like
the SENSESPOTTING results, when doing a micro-
level evaluation, our models do not perform as
well as the CONSTANT baseline, and, as before,
we attribute this to data sparsity.
6.7 Learning Curves
All of the results presented so far use classi-
fiers trained on instances of representative types
(i.e. ?representative tokens?) extracted from fairly
large new domain parallel corpora (see Table 3),
consisting of between 22 and 36 thousand parallel
sentences, which yield between 8 and 35 thousand
representative tokens. Although we expect some
new domain parallel tuning data to be available
in most MT settings, we would like to know how
many representative types are required to achieve
good performance on the SENSESPOTTING task.
Figure 6.5 shows learning curves over the num-
ber of representative tokens that are used to train
SENSESPOTTING classifiers. In fact, only about
25-50% of the data we used is really necessary to
achieve the performance observed before.
Acknowledgments We gratefully acknowledge the support
of the JHU summer workshop program (and its funders), the
entire DAMT team (http://hal3.name/DAMT/), San-
jeev Khudanpur, support from the NRC for Marine Carpuat,
as well as DARPA CSSG Grant D11AP00279 for Hal Daume?
III and Jagadeesh Jagarlamudi.
1443
References
E. Agirre and P.G. Edmonds. 2006. Word Sense Dis-
ambiguation: Algorithms and Applications. Text,
Speech, and Language Technology Series. Springer
Science+Business Media B.V.
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007
task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 7?12.
David Bamman and Gregory Crane. 2011. Measuring
historical word sense variation. In Proceedings of
the 2011 Joint International Conference on Digital
Libraries (JCDL 2011), pages 1?10.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet
allocation. Journal of Machine Learning Research
(JMLR), 3.
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 854?864,
Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving
Statistical Machine Translation using Word Sense
Disambiguation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2007), pages 61?
72, Prague, June.
Marine Carpuat, Hal Daume? III, Alexander Fraser,
Chris Quirk, Fabienne Braune, Ann Clifton, Ann
Irvine, Jagadeesh Jagarlamudi, John Morgan, Ma-
jid Razmara, Ales? Tamchyna, Katharine Henry, and
Rachel Rudinger. 2012. Domain adaptation in ma-
chine translation: Final report. In 2012 Johns Hop-
kins Summer Workshop Final Report.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In Proceedings of the Association for
Computational Linguistics.
Paul Cook and Suzanne Stevenson. 2010. Automati-
cally identifying changes in the semantic orientation
of words. In Proceedings of the 7th International
Conference on Language Resources and Evaluation,
pages 28?34, Valletta, Malta.
Hal Daume? III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by min-
ing unseen words. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Katrin Erk. 2006. Unknown word sense detection as
outlier detection. In Proceedings of the main confer-
ence on Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, pages 128?135.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Kristina Gulordava and Marco Baroni. 2011. A distri-
butional similarity approach to the detection of se-
mantic change in the google books ngram corpus. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 67?71, Edinburgh, UK, July. Association for
Computational Linguistics.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Matthew Hoffman, David Blei, and Francis Bach.
2010. Online learning for latent dirichlet alocation.
In Advances in Neural Information Processing Sys-
tems (NIPS).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, Timothy Baldwin, and Lexical Computing.
2012. Word sense induction for novel sense de-
tection. In Proceedings of the 13th Conference of
the European Chapter of the Association for compu-
tational Linguistics (EACL 2012), pages 591?601.
Citeseer.
Bernardo Magnini, Carlo Strapparava, Giovanni Pez-
zulo, and Alfio Gliozzo. 2002. The role of domain
information in word sense disambiguation. Natural
Language Engineering, 8(04):359?373.
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Kobi Reiter, Michael Skinner, Marcus Sam-
mer, and Jeff Bilmes. 2010. Panlingual lexical
translation via probabilistic inference. Artificial In-
telligence, 174:619?637, June.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, page 279. Association for Computational Lin-
guistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553?590.
1444
Behrang Mohit and Rebecca Hwa. 2007. Localiza-
tion of difficult-to-translate phrases. In proceedings
of the 2nd ACL Workshop on Statistical Machine
Translations.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and phonetic space. In Proceedings
of the EACL 2009 Workshop on GEMS: GEometical
Models of Natural Language Semantics, pages 104?
111, Athens, Greece, March.
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Charles Schafer. 2006. Translation Discovery Using
Diverse Similarity Measures. Ph.D. thesis, Johns
Hopkins University.
Jo?rg Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces. In N. Nicolov, K. Bontcheva, G. An-
gelova, and R. Mitkov, editors, Recent Advances in
Natural Language Processing (RANLP).
Wikipedia. 2013. Receiver operating characteristic.
http://en.wikipedia.org/wiki/Receiver_
operating_characteristic#Area_Under_
the_Curve, February.
1445
Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop, pages 17?25,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Sketching Techniques for Large Scale NLP
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III, and Suresh Venkatasubramanian
University of Utah, School of Computing
{amitg,jags,hal,suresh}@cs.utah.edu
Abstract
In this paper, we address the challenges
posed by large amounts of text data by
exploiting the power of hashing in the
context of streaming data. We explore
sketch techniques, especially the Count-
Min Sketch, which approximates the fre-
quency of a word pair in the corpus with-
out explicitly storing the word pairs them-
selves. We use the idea of a conservative
update with the Count-Min Sketch to re-
duce the average relative error of its ap-
proximate counts by a factor of two. We
show that it is possible to store all words
and word pairs counts computed from 37
GB of web data in just 2 billion counters
(8 GB RAM). The number of these coun-
ters is up to 30 times less than the stream
size which is a big memory and space gain.
In Semantic Orientation experiments, the
PMI scores computed from 2 billion coun-
ters are as effective as exact PMI scores.
1 Introduction
Approaches to solve NLP problems (Brants et al,
2007; Turney, 2008; Ravichandran et al, 2005) al-
ways benefited from having large amounts of data.
In some cases (Turney and Littman, 2002; Pat-
wardhan and Riloff, 2006), researchers attempted
to use the evidence gathered from web via search
engines to solve the problems. But the commer-
cial search engines limit the number of automatic
requests on a daily basis for various reasons such
as to avoid fraud and computational overhead.
Though we can crawl the data and save it on disk,
most of the current approaches employ data struc-
tures that reside in main memory and thus do not
scale well to huge corpora.
Fig. 1 helps us understand the seriousness of
the situation. It plots the number of unique word-
s/word pairs versus the total number of words in
5 10 15 20 25
5
10
15
20
25
Log2 of # of words
Lo
g 2 
of 
# o
f u
niq
ue
 Ite
ms
 
 
Items=word?pairs
Items=words
Figure 1: Token Type Curve
a corpus of size 577 MB. Note that the plot is in
log-log scale. This 78 million word corpus gen-
erates 63 thousand unique words and 118 million
unique word pairs. As expected, the rapid increase
in number of unique word pairs is much larger
than the increase in number of words. Hence, it
shows that it is computationally infeasible to com-
pute counts of all word pairs with a giant corpora
using conventional main memory of 8 GB.
Storing only the 118 million unique word pairs
in this corpus require 1.9 GB of disk space. This
space can be saved by avoiding storing the word
pair itself. As a trade-off we are willing to tolerate
a small amount of error in the frequency of each
word pair. In this paper, we explore sketch tech-
niques, especially the Count-Min Sketch, which
approximates the frequency of a word pair in the
corpus without explicitly storing the word pairs
themselves. It turns out that, in this technique,
both updating (adding a new word pair or increas-
ing the frequency of existing word pair) and query-
ing (finding the frequency of a given word pair) are
very efficient and can be done in constant time1.
Counts stored in the CM Sketch can be used to
compute various word-association measures like
1depend only on one of the user chosen parameters
17
Pointwise Mutual Information (PMI), and Log-
Likelihood ratio. These association scores are use-
ful for other NLP applications like word sense
disambiguation, speech and character recognition,
and computing semantic orientation of a word. In
our work, we use computing semantic orientation
of a word using PMI as a canonical task to show
the effectiveness of CM Sketch for computing as-
sociation scores.
In our attempt to advocate the Count-Min
sketch to store the frequency of keys (words or
word pairs) for NLP applications, we perform both
intrinsic and extrinsic evaluations. In our intrinsic
evaluation, first we show that low-frequent items
are more prone to errors. Second, we show that
computing approximate PMI scores from these
counts can give the same ranking as Exact PMI.
However, we need counters linear in size of stream
to achieve that. We use these approximate PMI
scores in our extrinsic evaluation of computing se-
mantic orientation. Here, we show that we do not
need counters linear in size of stream to perform
as good as Exact PMI. In our experiments, by us-
ing only 2 billion counters (8GB RAM) we get the
same accuracy as for exact PMI scores. The num-
ber of these counters is up to 30 times less than the
stream size which is a big memory and space gain
without any loss of accuracy.
2 Background
2.1 Large Scale NLP problems
Use of large data in the NLP community is not
new. A corpus of roughly 1.6 Terawords was used
by Agirre et al (2009) to compute pairwise sim-
ilarities of the words in the test sets using the
MapReduce infrastructure on 2, 000 cores. Pan-
tel et al (2009) computed similarity between 500
million terms in the MapReduce framework over a
200 billion words in 50 hours using 200 quad-core
nodes. The inaccessibility of clusters for every one
has attracted the NLP community to use stream-
ing, randomized, approximate and sampling algo-
rithms to handle large amounts of data.
A randomized data structure called Bloom fil-
ter was used to construct space efficient language
models (Talbot and Osborne, 2007) for Statis-
tical Machine Translation (SMT). Recently, the
streaming algorithm paradigm has been used to
provide memory and space-efficient platform to
deal with terabytes of data. For example, We
(Goyal et al, 2009) pose language modeling as
a problem of finding frequent items in a stream
of data and show its effectiveness in SMT. Subse-
quently, (Levenberg and Osborne, 2009) proposed
a randomized language model to efficiently deal
with unbounded text streams. In (Van Durme and
Lall, 2009b), authors extend Talbot Osborne Mor-
ris Bloom (TOMB) (Van Durme and Lall, 2009a)
Counter to find the highly ranked k PMI response
words given a cue word. The idea of TOMB is
similar to CM Sketch. TOMB can also be used to
store word pairs and further compute PMI scores.
However, we advocate CM Sketch as it is a very
simple algorithm with strong guarantees and good
properties (see Section 3).
2.2 Sketch Techniques
A sketch is a summary data structure that is used
to store streaming data in a memory efficient man-
ner. These techniques generally work on an input
stream, i.e. they process the input in one direc-
tion, say from left to right, without going back-
wards. The main advantage of these techniques
is that they require storage which is significantly
smaller than the input stream length. For typical
algorithms, the working storage is sublinear in N ,
i.e. of the order of logk N , where N is the input
size and k is some constant which is not explicitly
chosen by the algorithm but it is an artifact of it..
Sketch based methods use hashing to map items in
the streaming data onto a small-space sketch vec-
tor that can be easily updated and queried. It turns
out that both updating and querying on this sketch
vector requires only a constant time per operation.
Streaming algorithms were first developed in
the early 80s, but gained in popularity in the late
90s as researchers first realized the challenges of
dealing with massive data sets. A good survey
of the model and core challenges can be found in
(Muthukrishnan, 2005). There has been consid-
erable work on coming up with different sketch
techniques (Charikar et al, 2002; Cormode and
Muthukrishnan, 2004; Li and Church, 2007). A
survey by (Rusu and Dobra, 2007; Cormode and
Hadjieleftheriou, 2008) comprehensively reviews
the literature.
3 Count-Min Sketch
The Count-Min Sketch (Cormode and Muthukr-
ishnan, 2004) is a compact summary data structure
used to store the frequencies of all items in the in-
put stream. The sketch allows fundamental queries
18
on the data stream such as point, range and in-
ner product queries to be approximately answered
very quickly. It can also be applied to solve the
finding frequent items problem (Manku and Mot-
wani, 2002) in a data stream. In this paper, we are
only interested in point queries. The aim of a point
query is to estimate the count of an item in the in-
put stream. For other details, the reader is referred
to (Cormode and Muthukrishnan, 2004).
Given an input stream of word pairs of length N
and user chosen parameters ? and ?, the algorithm
stores the frequencies of all the word pairs with the
following guarantees:
? All reported frequencies are within the true
frequencies by at most ?N with a probability
of at least ?.
? The space used by the algorithm is
O(1? log 1? ).
? Constant time of O(log(1? )) per each update
and query operation.
3.1 CM Data Structure
A Count-Min Sketch with parameters (?,?) is rep-
resented by a two-dimensional array with width w
and depth d :
?
?
?
sketch[1,1] ? ? ? sketch[1,w]
.
.
.
.
.
.
.
.
.
sketch[d,1] ? ? ? sketch[d,w]
?
?
?
Among the user chosen parameters, ? controls the
amount of tolerable error in the returned count and
? controls the probability with which the returned
count is not within the accepted error. These val-
ues of ? and ? determine the width and depth of the
two-dimensional array respectively. To achieve
the guarantees mentioned in the previous section,
we set w=2? and d=log(1? ). The depth d denotes
the number of pairwise-independent hash func-
tions employed by the algorithm and there exists
an one-to-one correspondence between the rows
and the set of hash functions. Each of these hash
functions hk:{1 . . . N} ? {1 . . . w} (1 ? k ? d)
takes an item from the input stream and maps it
into a counter indexed by the corresponding hash
function. For example, h2(w) = 10 indicates that
the word pair w is mapped to the 10th position in
the second row of the sketch array. These d hash
functions are chosen uniformly at random from a
pairwise-independent family.
Figure 2: Update Procedure for CM sketch and conserva-
tive update (CU)
Initially the entire sketch array is initialized
with zeros.
Update Procedure: When a new item (w,c) ar-
rives, where w is a word pair and c is its count2,
one counter in each row, as decided by its corre-
sponding hash function, is updated by c. Formally,
?1 ? k ? d
sketch[k,hk(w)]? sketch[k,hk(w)] + c
This process is illustrated in Fig. 2 CM. The item
(w,2) arrives and gets mapped to three positions,
corresponding to the three hash functions. Their
counts before update were (4,2,1) and after update
they become (6,4,3). Note that, since we are using
a hash to map a word into an index, a collision can
occur and multiple word pairs may get mapped to
the same counter in any given row. Because of
this, the values stored by the d counters for a given
word pair tend to differ.
Query Procedure: The querying involves find-
ing the frequency of a given item in the input
stream. Since multiple word pairs can get mapped
into same counter and the observation that the
counts of items are positive, the frequency stored
by each counter is an overestimate of the true
count. So in answering the point query, we con-
sider all the positions indexed by the hash func-
tions for the given word pair and return the mini-
mum of all these values. The answer to Query(w)
is:
c? = mink sketch[k,hk(w)]
Note that, instead of positive counts if we had neg-
ative counts as well then the algorithm returns the
median of all the counts and the bounds we dis-
cussed in Sec. 3 vary. In Fig. 2 CM, for the word
pair w it takes the minimum over (6,4,3) and re-
turns 3 as the count of word pair w.
2In our setting, c is always 1. However, in other NLP
problem, word pairs can be weighted according to recency.
19
Both update and query procedures involve eval-
uating d hash functions and a linear scan of all the
values in those indices and hence both these pro-
cedures are linear in the number of hash functions.
Hence both these steps require O(log(1? )) time. In
our experiments (see Section 4.2), we found that a
small number of hash functions are sufficient and
we use d=3. Hence, the update and query oper-
ations take only a constant time. The space used
by the algorithm is the size of the array i.e. wd
counters, where w is the width of each row.
3.2 Properties
Apart from the advantages of being space efficient,
and having constant update and constant querying
time, the Count-Min sketch has also other advan-
tages that makes it an attractive choice for NLP
applications.
? Linearity: given two sketches s1 and s2 com-
puted (using the same parameters w and d)
over different input streams, the sketch of
the combined data stream can be easily ob-
tained by adding the individual sketches in
O(1? log 1? ) time which is independent of the
stream size.
? The linearity is especially attractive because,
it allows the individual sketches to be com-
puted independent of each other. Which
means that it is easy to implement it in dis-
tributed setting, where each machine com-
putes the sketch over a sub set of corpus.
? This technique also extends to allow the dele-
tion of items. In this case, to answer a point
query, we should return the median of all the
values instead of the minimum value.
3.3 Conservative Update
Estan and Varghese introduce the idea of conser-
vative update (Estan and Varghese, 2002) in the
context of networking. This can easily be used
with CM Sketch to further improve the estimate
of a point query. To update an item, word pair, w
with frequency c, we first compute the frequency
c? of this item from the existing data structure and
the counts are updated according to: ?1 ? k ? d
sketch[k,hk(w)]? max{sketch[k,hk(w)], c? + c}
The intuition is that, since the point query returns
the minimum of all the d values, we will update
a counter only if it is necessary as indicated by
the above equation. Though this is a heuristic, it
avoids the unnecessary updates of counter values
and thus reduces the error.
The process is also illustrated in Fig. 2CU.
When an item ?w? with a frequency of 2 arrives
in the stream, it gets mapped into three positions
in the sketch data structure. Their counts before
update were (4,2,1) and the frequency of the item
is 1 (the minimum of all the three values). In this
particular case, the update rule says that increase
the counter value only if its updated value is less
than c? + 2 = 3. As a result, the values in these
counters after the update become (4,3,3).
However, if the value in any of the counters
is already greater than 3 e.g. 4, we cannot at-
tempt to correct it by decreasing, as it could con-
tain the count for other items hashed at that posi-
tion. Therefore, in this case, for the first counter
we leave the value 4 unchanged. The query pro-
cedure remains the same as in the previous case.
In our experiments, we found that employing the
conservative update reduces the Average Relative
Error (ARE) of these counts approximately by a
factor of 2. (see Section 4.2). But unfortunately,
this update prevents deletions and items with neg-
ative updates cannot be processed3.
4 Intrinsic Evaluations
To show the effectiveness of the Count-Min sketch
in the context of NLP, we perform intrinsic evalu-
ations. The intrinsic evaluations are designed to
measure the error in the approximate counts re-
turned by CMS compared to their true counts. By
keeping the total size of the data structure fixed,
we study the error by varying the width and the
depth of the data structure to find the best setting
of the parameters for textual data sets. We show
that using conservative update (CU) further im-
proves the quality of counts over CM sketch.
4.1 Corpus Statistics
Gigaword corpus (Graff, 2003) and a copy of web
crawled by (Ravichandran et al, 2005) are used
to compute counts of words and word pairs. For
both the corpora, we split the text into sentences,
tokenize and convert into lower-case. We generate
words and word pairs (items) over a sliding win-
dow of size 14. Unlike previous work (Van Durme
3Here, we are only interested in the insertion case.
20
Corpus Sub Giga 50% 100%
set word Web Web
Size
.15 6.2 15 31GB
# of sentences 2.03 60.30 342.68 686.63(Million)
# of words 19.25 858.92 2122.47 4325.03(Million)
Stream Size 0.25 19.25 18.63 39.0510 (Billion)
Stream Size 0.23 25.94 18.79 40.0014 (Billion)
Table 1: Corpus Description
and Lall, 2009b) which assumes exact frequen-
cies for words, we store frequencies of both the
words and word pairs in the CM sketch4. Hence,
the stream size in our case is the total number of
words and word pairs in a corpus. Table 1 gives
the characteristics of the corpora.
Since, it is not possible to compute exact fre-
quencies of all word pairs using conventional main
memory of 8 GB from a large corpus, we use a
subset of 2 million sentences (Subset) from Giga-
word corpus for our intrinsic evaluation. We store
the counts of all words and word pairs (occurring
in a sliding window of length 14) from Subset us-
ing the sketch and also the exact counts.
4.2 Comparing CM and CU counts and
tradeoff between width and depth
To evaluate the amount of over-estimation in CM
and CU counts compared to the true counts, we
first group all items (words and word pairs) with
same true frequency into a single bucket. We then
compute the average relative error in each of these
buckets. Since low-frequent items are more prone
to errors, making this distinction based on fre-
quency lets us understand the regions in which the
algorithm is over-estimating. Average Relative er-
ror (ARE) is defined as the average of absolute dif-
ference between the predicted and the exact value
divided by the exact value over all the items in
each bucket.
ARE = 1N
N
?
i=1
|Exacti ? Predictedi|
Exacti
Where Exact and Predicted denotes values of exact
and CM/CU counts respectively; N denotes the
number of items with same counts in a bucket.
In Fig. 3(a), we fixed the number of counters
to 50 million with four bytes of memory per each
4Though a minor point, it allows to process more text.
counter (thus it only requires 200 MB of main
memory). Keeping the total number of counters
fixed, we try different values of depth (2, 3, 5 and
7) of the sketch array and in each case the width
is set to 50Md . The ARE curves in each case are
shown in Fig. 3(a). There are three main observa-
tions: First it shows that most of the errors occur
on low frequency items. For frequent items, in al-
most all the different runs the ARE is close to zero.
Secondly, it shows that ARE is significantly lower
(by a factor of two) for the runs which use conser-
vative update (CUx run) compared to the runs that
use direct CM sketch (CMx run). The encouraging
observation is that, this holds true for almost all
different (width,depth) settings. Thirdly, in our ex-
periments, it shows that using depth of 3 gets com-
paratively less ARE compared to other settings.
To be more certain about this behavior with re-
spect to different settings of width and depth, we
tried another setting by increasing the number of
counters to 100 million. The curves in 3(b) follow
a pattern which is similar to the previous setting.
Low frequency items are more prone to error com-
pared to the frequent ones and employing conser-
vative update reduces the ARE by a factor of two.
In this setting, depth 3 and 5 do almost the same
and get lowest ARE. In both the experiments, set-
ting the depth to three did well and thus in the rest
of the paper we fix this parameter to three.
Fig. 4 studies the effect of the number of coun-
ters in the sketch (the size of the two-dimensional
sketch array) on the ARE. Using more number of
counters decreases the ARE in the counts. This is
intuitive because, as the length of each row in the
sketch increases, the probability of collision de-
creases and hence the array is more likely to con-
tain true counts. By using 200 million counters,
which is comparable to the length of the stream
230 million (Table. 1), we are able to achieve al-
most zero ARE over all the counts including the
rare ones5. Note that the actual space required
to represent the exact counts is almost two times
more than the memory that we use here because
there are 230 million word pairs and on an aver-
age each word is eight characters long and requires
eight bytes (double the size of an integer). The
summary of this Figure is that, if we want to pre-
serve the counts of low-frequent items accurately,
then we need counters linear in size of stream.
5Even with other datasets we found that using counters
linear in the size of the stream leads to ARE close to zero ?
counts.
21
0 2 4 6 8 10 12
0
0.5
1
1.5
2
2.5
3
3.5
4
Log2 of true frequency counts of words/word?pairs
Av
era
ge 
Re
lati
ve 
Err
or
 
 
CM7
CM5
CM3
CM2
CU7
CU5
CU3
CU2
(a) 50M counters
0 2 4 6 8 10 12
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Log2 of true frequency counts of words/word?pairs
Av
era
ge 
Re
lati
ve 
Err
or
 
 
CM7
CM5
CM3
CM2
CU7
CU5
CU3
CU2
(b) 100M counters
Figure 3: Comparing 50 and 100 million counter models with different (width,depth) settings. The notation CMx represents
the Count-Min Sketch with a depth of ?x? and CUx represents the CM sketch along with conservative update and depth ?x?.
0 2 4 6 8 10 12
0
1
2
3
4
5
6
Log2 of true frequency counts of words/word?pairs
Av
er
ag
e 
Re
lat
ive
 E
rro
r
 
 
20M
50M
100M
200M
Figure 4: Comparing different size models with depth 3
4.3 Evaluating the CU PMI ranking
In this experiment, we compare the word pairs as-
sociation rankings obtained using PMI with CU
and exact counts. We use two kinds of measures,
namely accuracy and Spearman?s correlation, to
measure the overlap in the rankings obtained by
both these approaches.
4.3.1 PointWise Mutual Information
The Pointwise Mutual Information (PMI) (Church
and Hanks, 1989) between two words w1 and w2
is defined as:
PMI(w1, w2) = log2
P (w1, w2)
P (w1)P (w2)
Here, P (w1, w2) is the likelihood that w1 and w2
occur together, and P (w1) and P (w2) are their in-
dependent likelihoods respectively. The ratio be-
tween these probabilities measures the degree of
statistical dependence between w1 and w2.
4.3.2 Description of the metrics
Accuracy is defined as fraction of word pairs that
are found in both rankings to the size of top ranked
word pairs.
Accuracy = |CP-WPs ? EP-WPs||EP-WPs|
Where CP-WPs represent the set of top ranked K
word pairs under the counts stored using the CU
sketch and EP-WPs represent the set of top ranked
word pairs with the exact counts.
Spearman?s rank correlation coefficient (?)
computes the correlation between the ranks of
each observation (i.e. word pairs) on two variables
(that are top N CU-PMI and exact-PMI values).
This measure captures how different the CU-PMI
ranking is from the Exact-PMI ranking.
? = 1? 6
? d2i
F (F 2 ? 1)
Where di is the difference between the ranks of
a word pair in both rankings and F is the number
of items found in both sets.
Intuitively, accuracy captures the number of
word pairs that are found in both the sets and then
Spearman?s correlation captures if the relative or-
der of these common items is preserved in both the
rankings. In our experimental setup, both these
measures are complimentary to each other and
measure different aspects. If the rankings match
exactly, then we get an accuracy of 100% and a
correlation of 1.
4.3.3 Comparing CU PMI ranking
The results with respect to different sized counter
(50, 100 and 200 million) models are shown in Ta-
ble 2. Table 2 shows that having counters linear
22
Counters 50M 100M 200M
Top K Acc ? Acc ? Acc ?
50 .20 -0.13 .68 .95 .92 1.00
100 .18 .31 .77 .80 .96 .95
200 .21 .68 .73 .86 .97 .99
500 .24 .31 .71 .97 .95 .99
1000 .33 .17 .74 .87 .95 .98
5000 .49 .38 .82 .82 .96 .97
Table 2: Evaluating the PMI rankings obtained using CM
Sketch with conservative update (CU) and Exact counts
in size of stream (230M ) results in better rank-
ing (i.e. close to the exact ranking). For example,
with 200M counters, among the top 50 word pairs
produced using the CU counts, we found 46 pairs
in the set returned by using exact counts. The ?
score on those word pairs is 1 means that the rank-
ing of these 46 items is exactly the same on both
CU and exact counts. We see the same phenom-
ena for 200M counters with other Top K values.
While both accuracy and the ranking are decent
with 100M counters, if we reduce the number of
counters to say 50M , the performance degrades.
Since, we are not throwing away any infrequent
items, PMI will rank pairs with low frequency
counts higher (Church and Hanks, 1989). Hence,
we are evaluating the PMI values for rare word
pairs and we need counters linear in size of stream
to get alost perfect ranking. Also, using coun-
ters equal to half the length of the stream is decent.
However, in some NLP problems, we are not inter-
ested in low-frequency items. In such cases, even
using space less than linear in number of coun-
ters would suffice. In our extrinsic evaluations, we
show that using space less than the length of the
stream does not degrades the performance.
5 Extrinsic Evaluations
5.1 Experimental Setup
To evaluate the effectiveness of CU-PMI word
association scores, we infer semantic orientation
(S0) of a word from CU-PMI and Exact-PMI
scores. Given a word, the task of finding the SO
(Turney and Littman, 2002) of the word is to iden-
tify if the word is more likely to be used in positive
or negative sense. We use a similar framework as
used by the authors6 to infer the SO. We take the
seven positive words (good, nice, excellent, posi-
tive, fortunate, correct, and superior) and the nega-
tive words (bad, nasty, poor, negative, unfortunate,
6We compute this score slightly differently. However, our
main focus is to show that CU-PMI scores are useful.
wrong, and inferior) used in (Turney and Littman,
2002) work. The SO of a given word is calculated
based on the strength of its association with the
seven positive words, and the strength of its asso-
ciation with the seven negative words. We com-
pute the SO of a word ?w? as follows:
SO-PMI(W) = PMI(+, w)? PMI(?, w)
PMI(+,W) =
?
p?Pwords
log hits(p, w)hits(p) ? hits(w)
PMI(-,W) =
?
n?Nwords
log hits(n,w)hits(n) ? hits(w)
Where, Pwords and Nwords denote the seven pos-
itive and negative prototype words respectively.
We compute SO score from different sized cor-
pora (Section 4.1). We use the General Inquirer
lexicon7 (Stone et al, 1966) as a benchmark to
evaluate the semantic orientation scores similar to
(Turney and Littman, 2002) work. Words with
multiple senses have multiple entries in the lexi-
con, we merge these entries for our experiment.
Our test set consists of 1619 positive and 1989
negative words. Accuracy is used as an evaluation
metric and is defined as the fraction of number of
correctly identified SO words.
Accuracy = Correctly Identified SO Words ? 100Total SO words
5.2 Results
We evaluate SO of words on three different sized
corpora: Gigaword (GW) 6.2GB, GigaWord +
50% of web data (GW+WB1) 21.2GB and Gi-
gaWord + 100% of web data (GW+WB2) 31GB.
Note that computing the exact counts of all word
pairs on these corpora is not possible using main
memory, so we consider only those pairs in which
one word appears in the prototype list and the
other word appears in the test set.
We compute the exact PMI (denoted using Ex-
act) scores for pairs of test-set words w1 and proto-
type words w2 using the above data-sets. To com-
pute PMI, we count the number of hits of individ-
ual words w1 and w2 and the pair (w1,w2) within a
sliding window of sizes 10 and 14 over these data-
sets. After computing the PMI scores, we compute
SO score for a word using SO-PMI equation from
Section 5.1. If this score is positive, we predict
the word as positive. Otherwise, we predict it as
7The General Inquirer lexicon is freely available at
http://www.wjh.harvard.edu/ inquirer/
23
Model Accuracy window 10 Accuracy window 14
#of counters Mem. Usage GW GW+WB1 GW+WB2 GW GW+WB1 GW+WB2
Exact n/a 64.77 75.67 77.11 64.86 74.25 75.30
500M 2GB 62.98 71.09 72.31 63.21 69.21 70.35
1B 4GB 62.95 73.93 75.03 63.95 72.42 72.73
2B 8GB 64.69 75.86 76.96 65.28 73.94 74.96
Table 3: Evaluating Semantic Orientation of words with different # of counters of CU sketch with increasing amount of data
on window size of 10 and 14. Scores are evaluated using Accuracy metric.
negative. The results on inferring correct SO for
a word w with exact PMI (Exact) are summarized
in Table 3. It (the second row) shows that increas-
ing the amount of data improves the accuracy of
identifying the SO of a word with both the win-
dow sizes. The gain is more prominent when we
add 50% of web data in addition to Gigaword as
we get an increase of more than 10% in accuracy.
However, when we add the remaining 50% of web
data, we only see an slight increase of 1% in accu-
racy8. Using words within a window of 10 gives
better accuracy than window of 14.
Now, we use our CU Sketches of 500 million
(500M ), 1 billion (1B) and 2 billion (2B) coun-
ters to compute CU-PMI. These sketches contain
the number of hits of all words/word pairs (not just
the pairs of test-set and prototype words) within a
window size of 10 and 14 over the whole data-
set. The results in Table 3 show that even with
CU-PMI scores, the accuracy improves by adding
more data. Again we see a significant increase in
accuracy by adding 50% of web data to Gigaword
over both window sizes. The increase in accuracy
by adding the rest of the web data is only 1%.
By using 500M counters, accuracy with CU-
PMI are around 4% worse than the Exact. How-
ever, increasing the size to 1B results in only 2
% worse accuracy compared to the Exact. Go-
ing to 2B counters (8 GB of RAM), results in ac-
curacy almost identical to the Exact. These re-
sults hold almost the same for all the data-sets
and for both the window sizes. The increase in
accuracy comes at expense of more memory Us-
age. However, 8GB main memory is not large as
most of the conventional desktop machines have
this much RAM. The number of 2B counters is
less than the length of stream for all the data-sets.
For GW, GW+WB1 and GW+WB2, 2B counters
are 10, 20 and 30 times smaller than the stream
size. This shows that using counters less than the
stream length does not degrade the performance.
8These results are similar to the results reported in (Tur-
ney and Littman, 2002) work.
The advantage of using Sketch is that it con-
tains counts for all words and word pairs. Suppose
we are given a new word to label it as positive or
negative. We can find its exact PMI in two ways:
First, we can go over the whole corpus and com-
pute counts of this word with positive and nega-
tive prototype words. This procedure will return
PMI in time needed to traverse the whole corpus.
If the corpus is huge, this could be too slow. Sec-
ond option is to consider storing counts of all word
pairs but this is not feasible as their number in-
creases rapidly with increase in data (see Fig. 1).
Therefore, using a CM sketch is a very good al-
ternative which returns the PMI in constant time
by using only 8GB of memory. Additionally, this
Sketch can easily be used for other NLP applica-
tions where we need word-association scores.
6 Conclusion
We have explored the idea of the CM Sketch,
which approximates the frequency of a word pair
in the corpus without explicitly storing the word
pairs themselves. We used the idea of a conserva-
tive update with the CM Sketch to reduce the av-
erage relative error of its approximate counts by
a factor of 2. It is an efficient, small-footprint
method that scales to at least 37 GB of web data
in just 2 billion counters (8 GB main memory). In
our extrinsic evaluations, we found that CU Sketch
is as effective as exact PMI scores.
Word-association scores from CU Sketch can be
used for other NLP tasks like word sense disam-
biguation, speech and character recognition. The
counts stored in CU Sketch can be used to con-
struct small-space randomized language models.
In general, this sketch can be used for any applica-
tion where we want to query a count of an item.
Acknowledgments
We thank the anonymous reviewers for helpful
comments. This work is partially funded by NSF
grant IIS-0712764 and Google Research Grant
Grant for Large-Data NLP.
24
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and wordnet-based approaches. In NAACL
?09: Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Moses Charikar, Kevin Chen, and Martin Farach-
colton. 2002. Finding frequent items in data
streams.
K. Church and P. Hanks. 1989. Word Association
Norms, Mutual Information and Lexicography. In
Proceedings of the 27th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 76?83,
Vancouver, Canada, June.
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
Graham Cormode and S. Muthukrishnan. 2004. An
improved data stream summary: The count-min
sketch and its applications. J. Algorithms.
Cristian Estan and George Varghese. 2002. New direc-
tions in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
Amit Goyal, Hal Daume? III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language modeling. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
D. Graff. 2003. English Gigaword. Linguistic Data
Consortium, Philadelphia, PA, January.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
756?764, Singapore, August. Association for Com-
putational Linguistics.
Ping Li and Kenneth W. Church. 2007. A sketch algo-
rithm for estimating two-way and multi-way associ-
ations. Comput. Linguist., 33(3).
G. S. Manku and R. Motwani. 2002. Approximate
frequency counts over data streams. In Proceedings
of the 28th International Conference on Very Large
Data Bases.
S. Muthukrishnan. 2005. Data streams: Algorithms
and applications. Foundations and Trends in Theo-
retical Computer Science, 1(2).
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-
scale distributional similarity and entity set expan-
sion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 938?947, Singapore, August. Association
for Computational Linguistics.
S. Patwardhan and E. Riloff. 2006. Learning Domain-
Specific Information Extraction Patterns from the
Web. In Proceedings of the ACL 2006 Workshop on
Information Extraction Beyond the Document.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: using
locality sensitive hash function for high speed noun
clustering. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics.
Florin Rusu and Alin Dobra. 2007. Statistical analysis
of sketch estimators. In SIGMOD ?07. ACM.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on
the cheap. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EM NLP-CoNLL).
Peter D. Turney and Michael L. Littman. 2002.
Unsupervised learning of semantic orientation
from a hundred-billion-word corpus. CoRR,
cs.LG/0212012.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In Pro-
ceedings of COLING 2008.
Benjamin Van Durme and Ashwin Lall. 2009a. Prob-
abilistic counting with randomized storage. In IJ-
CAI?09: Proceedings of the 21st international jont
conference on Artifical intelligence, pages 1574?
1579.
Benjamin Van Durme and Ashwin Lall. 2009b.
Streaming pointwise mutual information. In Ad-
vances in Neural Information Processing Systems
22.
25
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 51?56,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Sketch Techniques for Scaling Distributional Similarity to the Web
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III, and Suresh Venkatasubramanian
School of Computing
University of Utah
Salt Lake City, UT 84112
{amitg,jags,hal,suresh}@cs.utah.edu
Abstract
In this paper, we propose a memory, space,
and time efficient framework to scale dis-
tributional similarity to the web. We
exploit sketch techniques, especially the
Count-Min sketch, which approximates
the frequency of an item in the corpus
without explicitly storing the item itself.
These methods use hashing to deal with
massive amounts of the streaming text. We
store all item counts computed from 90
GB of web data in just 2 billion coun-
ters (8 GB main memory) of CM sketch.
Our method returns semantic similarity
between word pairs in O(K) time and
can compute similarity between any word
pairs that are stored in the sketch. In our
experiments, we show that our framework
is as effective as using the exact counts.
1 Introduction
In many NLP problems, researchers (Brants et al,
2007; Turney, 2008) have shown that having large
amounts of data is beneficial. It has also been
shown that (Agirre et al, 2009; Pantel et al, 2009;
Ravichandran et al, 2005) having large amounts
of data helps capturing the semantic similarity be-
tween pairs of words. However, computing distri-
butional similarity (Sec. 2.1) between word pairs
from large text collections is a computationally ex-
pensive task. In this work, we consider scaling dis-
tributional similarity methods for computing se-
mantic similarity between words to Web-scale.
The major difficulty in computing pairwise sim-
ilarities stems from the rapid increase in the num-
ber of unique word-context pairs with the size of
text corpus (number of tokens). Fig. 1 shows that
5 10 15 20 25
5
10
15
20
Log2 of # of words
Lo
g 2
 
o
f #
 o
f u
ni
qu
e 
Ite
m
s
 
 
word?context pairs
words
Figure 1: Token Type Curve
the number of unique word-context pairs increase
rapidly compared to the number words when plot-
ted against the number of tokens1. For example,
a 57 million word corpus2 generates 224 thousand
unique words and 15 million unique word-context
pairs. As a result, it is computationally hard to
compute counts of all word-context pairs with a gi-
ant corpora using conventional machines (say with
main memory of 8 GB). To overcome this, Agirre
et al (2009) used MapReduce infrastructure (with
2, 000 cores) to compute pairwise similarities of
words on a corpus of roughly 1.6 Terawords.
In a different direction, our earlier work (Goyal
et al, 2010) developed techniques to make the
computations feasible on a conventional machines
by willing to accept some error in the counts. Sim-
ilar to that work, this work exploits the idea of
Count-Min (CM) sketch (Cormode and Muthukr-
ishnan, 2004) to approximate the frequency of
word pairs in the corpus without explicitly stor-
ing the word pairs themselves. In their, we stored
1Note that the plot is in log-log scale.
2
?Subset? column of Table 1 in Section 5.1
51
counts of all words/word pairs in fixed amount of
main memory. We used conservative update with
CM sketch (referred as CU sketch) and showed
that it reduces the average relative error of its ap-
proximate counts by a factor of two. The approx-
imate counts returned by CU Sketch were used
to compute approximate PMI between word pairs.
We found their that the approximate PMI values
are as useful as the exact PMI values for com-
puting semantic orientation (Turney and Littman,
2002) of words. In addition, our intrinsic evalua-
tions in their showed that the quality of approxi-
mate counts and approximate PMI is good.
In this work, we use CU-sketch to store counts
of items (words, contexts, and word-context pairs)
using fixed amount of memory of 8 GB by using
only 2B counters. These approximate counts re-
turned by CU Sketch are converted into approx-
imate PMI between word-context pairs. The top
K contexts (based on PMI score) for each word
are used to construct distributional profile (DP) for
each word. The similarity between a pair of words
is computed based on the cosine similarity of their
respective DPs.
The above framework of using CU sketch to
compute semantic similarity between words has
five good properties. First, this framework can re-
turn semantic similarity between any word pairs
that are stored in the CU sketch. Second, it can
return the similarity between word pairs in time
O(K). Third, because we do not store items ex-
plicitly, the overall space required is significantly
smaller. Fourth, the additive property of CU
sketch (Sec. 3.2) enables us to parallelize most
of the steps in the algorithm. Thus it can be easily
extended to very large amounts of text data. Fifth,
this easily generalizes to any kind of association
measure and semantic similarity measure.
2 Background
2.1 Distributional Similarity
Distributional Similarity is based on the distribu-
tional hypothesis (Firth, 1968; Harris, 1985) that
words occur in similar contexts tend to be sim-
ilar. The context of a word is represented by
the distributional profile (DP), which contains the
strength of association between the word and each
of the lexical, syntactic, semantic, and/or depen-
dency units that co-occur with it3. The association
3In this work, we only consider lexical units as context.
is commonly measured using conditional proba-
bility, pointwise mutual information (PMI) or log
likelihood ratios. Then the semantic similarity be-
tween two words, given their DPs, is calculated
using similarity measures such as Cosine, ?-skew
divergence, and Jensen-Shannon divergence. In
our work, we use PMI as association measure and
cosine similarity to compute pairwise similarities.
2.2 Large Scale NLP problems
Pantel et al (2009) computed similarity between
500 million word pairs using the MapReduce
framework from a 200 billion word corpus using
200 quad-core nodes. The inaccessibility of clus-
ters for every one has attracted NLP community to
use streaming, and randomized algorithms to han-
dle large amounts of data.
Ravichandran et al (2005) used locality sensi-
tive hash functions for computing word-pair simi-
larities from large text collections. Their approach
stores a enormous matrix of all unique words and
their contexts in main memory which makes it
hard for larger data sets. In our work, we store
all unique word-context pairs in CU sketch with a
pre-defined size4.
Recently, the streaming algorithm paradigm has
been used to provide memory and time-efficient
platform to deal with terabytes of data. For
example, we (Goyal et al, 2009); Levenberg
and Osborne (2009) build approximate language
models and show their effectiveness in SMT. In
(Van Durme and Lall, 2009b), a TOMB Counter
(Van Durme and Lall, 2009a) was used to find the
top-K verbs ?y? with the highest PMI for a given
verb ?x?. The idea of TOMB is similar to CU
Sketch. However, we use CU Sketch because of
its simplicity and attractive properties (see Sec. 3).
In this work, we go one step further, and compute
semantic similarity between word-pairs using ap-
proximate PMI scores from CU sketch.
2.3 Sketch Techniques
Sketch techniques use a sketch vector as a data
structure to store the streaming data compactly in
a small-memory footprint. These techniques use
hashing to map items in the streaming data onto a
small sketch vector that can be easily updated and
queried. These techniques generally process the
input stream in one direction, say from left to right,
4We use only 2 billion counters which takes up to 8 GB
of main memory.
52
without re-processing previous input. The main
advantage of using these techniques is that they
require a storage which is significantly smaller
than the input stream length. A survey by (Rusu
and Dobra, 2007; Cormode and Hadjieleftheriou,
2008) comprehensively reviews the literature.
3 Count-Min Sketch
The Count-Min Sketch (Cormode and Muthukr-
ishnan, 2004) is a compact summary data struc-
ture used to store the frequencies of all items in
the input stream.
Given an input stream of items of length N
and user chosen parameters ? and ?, the algorithm
stores the frequencies of all the items with the fol-
lowing guarantees:
? All reported frequencies are within ?N of
true frequencies with probability of atleast ?.
? Space used by the algorithm is O(1? log 1? ).
? Constant time of O(log(1? )) per each update
and query operation.
3.1 CM Data Structure
A Count-Min Sketch with parameters (?,?) is rep-
resented by a two-dimensional array with width w
and depth d :
?
?
?
sketch[1, 1] ? ? ? sketch[1, w]
.
.
.
.
.
.
.
.
.
sketch[d, 1] ? ? ? sketch[d,w]
?
?
?
Among the user chosen parameters, ? controls the
amount of tolerable error in the returned count and
? controls the probability with which the returned
count is not within this acceptable error. These
values of ? and ? determine the width and depth
of the two-dimensional array respectively. To
achieve the guarantees mentioned in the previous
section, we set w=2? and d=log(1? ). The depth d
denotes the number of pairwise-independent hash
functions employed by the algorithm and there
exists an one-to-one correspondence between the
rows and the set of hash functions. Each of these
hash functions hk:{x1 . . . xN} ? {1 . . . w}, 1 ?
k ? d takes an item from the input stream and
maps it into a counter indexed by the correspond-
ing hash function. For example, h2(x) = 10 indi-
cates that the item ?x? is mapped to the 10th posi-
tion in the second row of the sketch array. These
d hash functions are chosen uniformly at random
from a pairwise-independent family.
Initialize the entire sketch array with zeros.
Update Procedure: When a new item ?x? with
count c arrives5, one counter in each row, as de-
cided by its corresponding hash function, is up-
dated by c. Formally, ?1 ? k ? d
sketch[k,hk(x)]? sketch[k,hk(x)] + c
Query Procedure: Since multiple items can be
hashed to the same counter, the frequency stored
by each counter is an overestimate of the true
count. Thus, to answer the point query, we con-
sider all the positions indexed by the hash func-
tions for the given item and return the minimum
of all these values. The answer to Query(x) is:
c? = mink sketch[k, hk(x)].
Both update and query procedures involve eval-
uating d hash functions. Hence, both these proce-
dures are linear in the number of hash functions. In
our experiments (see Section5), we use d=3 simi-
lar to our earlier work (Goyal et al, 2010). Hence,
the update and query operations take only constant
time.
3.2 Properties
Apart from the advantages of being space efficient
and having constant update and querying time, the
CM sketch has other advantages that makes it at-
tractive for scaling distributional similarity to the
web:
1. Linearity: given two sketches s1 and s2 com-
puted (using the same parameters w and d)
over different input streams, the sketch of the
combined data stream can be easily obtained
by adding the individual sketches.
2. The linearity allows the individual sketches
to be computed independent of each other.
This means that it is easy to implement it in
distributed setting, where each machine com-
putes the sketch over a subset of the corpus.
3.3 Conservative Update
Estan and Varghese introduce the idea of conserva-
tive update (Estan and Varghese, 2002) in the con-
text of networking. This can easily be used with
CM Sketch (CU Sketch) to further improve the es-
timate of a point query. To update an item, w with
frequency c, we first compute the frequency c? of
5In our setting, c is always 1.
53
this item from the existing data structure and the
counts are updated according to: ?1 ? k ? d
sketch[k,hk(x)]? max{sketch[k,hk(x)], c? + c}
The intuition is that, since the point query returns
the minimum of all the d values, we will update a
counter only if it is necessary as indicated by the
above equation. This heuristic avoids the unneces-
sary updating of counter values and thus reduces
the error.
4 Efficient Distributional Similarity
To compute distributional similarity efficiently, we
store counts in CU sketch. Our algorithm has three
main steps:
1. Store approximate counts of all words, con-
texts, and word-context pairs in CU-sketch
using fixed amount of counters.
2. Convert these counts into approximate PMI
scores between word-context pairs. Use these
PMI scores to store top K contexts for a word
on the disk. Store these top K context vectors
for every word stored in the sketch.
3. Use cosine similarity to compute the similar-
ity between word pairs using these approxi-
mate top K context vectors constructed using
CU sketch.
5 Word pair Ranking Evaluations
As discussed earlier, the DPs of words are used to
compute similarity between a pair of words. We
used the following four test sets and their corre-
sponding human judgements to evaluate the word
pair rankings.
1. WS-353: WordSimilarity-3536 (Finkelstein
et al, 2002) is a set of 353 word pairs.
2. WS-203: A subset of WS-353 containing 203
word pairs marked according to similarity7
(Agirre et al, 2009).
3. RG-65: (Rubenstein and Goodenough, 1965)
is set of 65 word pairs.
4. MC-30: A smaller subset of the RG-65
dataset containing 30 word pairs (Miller and
Charles, 1991).
6http://www.cs.technion.ac.il/ gabr/resources/data/word-
sim353/wordsim353.html
7http://alfonseca.org/pubs/ws353simrel.tar.gz
Each of these data sets come with human ranking
of the word pairs. We rank the word pairs based
on the similarity computed using DPs and evalu-
ate this ranking against the human ranking. We
report the spearman?s rank correlation coefficient
(?) between these two rankings.
5.1 Corpus Statistics
The Gigaword corpus (Graff, 2003) and a copy of
the web crawled by (Ravichandran et al, 2005)
are used to compute counts of all items (Table. 1).
For both the corpora, we split the text into sen-
tences, tokenize, convert into lower-case, remove
punctuations, and collapse each digit to a sym-
bol ?0? (e.g. ?1996? gets collapsed to ?0000?).
We store the counts of all words (excluding num-
bers, and stop words), their contexts, and counts
of word-context pairs in the CU sketch. We de-
fine the context for a given word ?x? as the sur-
rounding words appearing in a window of 2 words
to the left and 2 words to the right. The context
words are concatenated along with their positions
-2, -1, +1, and +2. We evaluate ranking of word
pairs on three different sized corpora: Gigaword
(GW), GigaWord + 50% of web data (GW-WB1),
and GigaWord + 100% of web data (GW-WB2).
Corpus Sub GW GW- GW-
set WB1 WB2
Size
.32 9.8 49 90(GB)
# of sentences 2.00 56.78 462.60 866.02(Million)
Stream Size
.25 7.65 37.93 69.41(Billion)
Table 1: Corpus Description
5.2 Results
We compare our system with two baselines: Ex-
act and Exact1000 which use exact counts. Since
computing the exact counts of all word-context
pairs on these corpora is not possible using main
memory of only 8 GB , we generate context vec-
tors for only those words which appear in the test
set. The former baseline uses all possible contexts
which appear with a test word, while the latter
baseline uses only the top 1000 contexts (based on
PMI value) for each word. In each case, we use
a cutoff (of 10, 60 and 120) on the frequency of
word-context pairs. These cut-offs were selected
based on the intuition that, with more data, you
get more noise, and not considering word-context
pairs with frequency less than 120 might be a bet-
54
Data GW GW-WB1 GW-WB2
Model Frequency cutoff Frequency cutoff Frequency cutoff10 60 120 10 60 120 10 60 120
? ? ?
WS-353
Exact .25 .25 .22 .29 .28 .28 .30 .28 .28
Exact1000 .36 .28 .22 .46 .43 .37 .47 .44 .41
Our Model .39 .28 .22 -0.09 .48 .40 -0.03 .04 .47
WS-203
Exact .35 .36 .33 .38 .38 .37 .40 .38 .38
Exact1000 .49 .40 .35 .57 .55 .47 .56 .56 .52
Our Model .49 .39 .35 -0.08 .58 .47 -0.06 .03 .55
RG-65
Exact .21 .12 .08 .42 .28 .22 .39 .31 .23
Exact1000 .14 .09 .08 .45 .16 .13 .47 .26 .12
Our Model .13 .10 .09 -0.06 .32 .18 -0.05 .08 .31
MC-30
Exact .26 .23 .21 .45 .33 .31 .46 .39 .29
Exact1000 .27 .18 .21 .63 .42 .32 .59 .47 .36
Our Model .36 .20 .21 -0.08 .52 .39 -0.27 -0.29 .52
Table 2: Evaluating word pairs ranking with Exact and CU counts. Scores are evaluated using ? metric.
ter choice than a cutoff of 10. The results are
shown in Table 2
From the above baseline results, first we learn
that using more data helps in better capturing
the semantic similarity between words. Second,
it shows that using top (K) 1000 contexts for
each target word captures better semantic similar-
ity than using all possible contexts for that word.
Third, using a cutoff of 10 is optimal for all differ-
ent sized corpora on all test-sets.
We use approximate counts from CU sketch
with depth=3 and 2 billion (2B) counters (?Our
Model?)8. Based on previous observation, we re-
strict the number of contexts for a target word to
1000. Table 2 shows that using CU counts makes
the algorithm sensitive to frequency cutoff. How-
ever, with appropriate frequency cutoff for each
corpus, approximate counts are nearly as effective
as exact counts. For GW, GW-WB1, and GW-
WB2, the frequency cutoffs of 10, 60, and 120 re-
spectively performed the best. The reason for de-
pendence on frequency cutoffs is due to the over-
estimation of low-frequent items. This is more
pronounced with bigger corpus (GW-WB2) as the
size of CU sketch is fixed to 2B counters and
stream size is much bigger (69.41 billion) com-
pared to GW where the stream size is 7.65 billion.
The advantages of using our model is that the
sketch contains counts for all words, contexts, and
word-context pairs stored in fixed memory of 8
GB by using only 2B counters. Note that it is not
8Our goal is not to build the best distributional similarity
method. It is to show that our framework scales easily to large
corpus and it is as effective as exact method.
feasible to keep track of exact counts of all word-
context pairs since their number increases rapidly
with increase in data (see Fig. 1). We can use our
model to create context vectors of size K for all
possible words stored in the Sketch and computes
semantic similarity between two words in O(K)
time. In addition, the linearity of sketch allows
us to include new incoming data into the sketch
without building the sketch from scratch. Also,
it allows for parallelization using the MapReduce
framework. We can generalize our framework to
any kind of association and similarity measure.
6 Conclusion
We proposed a framework which uses CU Sketch
to scale distributional similarity to the web. It can
compute similarity between any word pairs that
are stored in the sketch and returns similarity be-
tween them in O(K) time. In our experiments, we
show that our framework is as effective as using
the exact counts, however it is sensitive to the fre-
quency cutoffs. In future, we will explore ways to
make this framework robust to the frequency cut-
offs. In addition, we are interested in exploring
this framework for entity set expansion problem.
Acknowledgments
We thank the anonymous reviewers for helpful
comments. This work is partially funded by NSF
grant IIS-0712764 and Google Research Grant
Grant for Large-Data NLP.
55
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and wordnet-based approaches. In NAACL
?09: Proceedings of HLT-NAACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
Graham Cormode and S. Muthukrishnan. 2004. An
improved data stream summary: The count-min
sketch and its applications. J. Algorithms.
Cristian Estan and George Varghese. 2002. New direc-
tions in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. In
ACM Transactions on Information Systems.
J. Firth. 1968. A synopsis of linguistic theory 1930-
1955. In F. Palmer, editor, Selected Papers of J. R.
Firth. Longman.
Amit Goyal, Hal Daume? III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language modeling. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daume? III,
and Suresh Venkatasubramanian. 2010. Sketching
techniques for Large Scale NLP. In 6th Web as Cor-
pus Workshop in conjunction with NAACL-HLT.
D. Graff. 2003. English Gigaword. Linguistic Data
Consortium, Philadelphia, PA, January.
Z. Harris. 1985. Distributional structure. In J. J. Katz,
editor, The Philosophy of Linguistics, pages 26?47.
Oxford University Press, New York.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In
Proceedings of EMNLP, August.
G.A. Miller and W.G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: using
locality sensitive hash function for high speed noun
clustering. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual correlates of synonymy. Computational Lin-
guistics, 8:627?633.
Florin Rusu and Alin Dobra. 2007. Statistical analysis
of sketch estimators. In SIGMOD ?07. ACM.
P.D. Turney and M.L. Littman. 2002. Unsupervised
learning of semantic orientation from a hundred-
billion-word corpus.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In Pro-
ceedings of COLING 2008.
Benjamin Van Durme and Ashwin Lall. 2009a. Prob-
abilistic counting with randomized storage. In IJ-
CAI?09: Proceedings of the 21st international jont
conference on Artifical intelligence.
Benjamin Van Durme and Ashwin Lall. 2009b.
Streaming pointwise mutual information. In Ad-
vances in Neural Information Processing Systems
22.
56
