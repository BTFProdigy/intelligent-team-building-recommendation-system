Proceedings of the Workshop on Linguistic Distances, pages 100?108,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Variants of tree similarity in a Question Answering task
Martin Emms
Dept of Computer Science
Trinity College
Ireland
Abstract
The results of experiments on the appli-
cation of a variety of distance measures
to a question-answering task are reported.
Variants of tree-distance are considered,
including whole-vs-sub tree, node weight-
ing, wild cards and lexical emphasis. We
derive string-distance as a special case
of tree-distance and show that a particu-
lar parameterisation of tree-distance out-
performs the string-distance measure.
1 Introduction
This paper studies the deployment in a ques-
tion answering task of methods which assess the
similarity of question and answer representations.
Given questions such as
Q1 what does malloc return ?
Q2 What year did poet Emily Dickinson die?
and a collection of sentences (eg. a computer man-
ual, a corpus of newspaper articles), the task is to
retrieve the sentences that answer the question, eg.
A1 the malloc function returns a null pointer
A2 In 1886 , poet Emily Dickinson died in Amherst , Mass
One philosophy for finding answers to ques-
tions would be to convert questions and candidate
answers into logical forms and to compute answer-
hood by apply theorem-proving methods. Another
philosophy is to assume that the answers are simi-
lar to the questions, where similarity might be de-
fined in many different ways. While not all an-
swers to all questions will be similar, there?s an
intuition that most questions can be answered in
a way which shares quite a bit with the question,
and that accordingly with a large enough corpus, a
similarity-based approach could be fruitful.
2 Distance Measures
In pursuing such a similarity-based approach to
question-answering, the key decisions to be made
are the representations of the questions and an-
swers, and relatedly, distance measures between
them.
We will primarily be concerned with measures
which refer to a linguistic structure assigned to a
word sequence ? variants of tree-distance, but we
will also consider string-distance.
2.1 Tree Measures
Following (Zhang and Shasha, 1989), one can ar-
rive at tree-distance in the following way. Given
source and target ordered, labelled trees, S and
T , consider the set H(S, T ) of all 1-to-1 par-
tial maps, ?, from S into T , which are homo-
morphisms preserving left-to-right order and an-
cestry1. Let the alignment, ??, be the enlarg-
ment of the map ? with pairs (Si, ?) for nodes
Si 6? dom(?) and (?, Tj) for nodes Tj 6? ran(?).
Let D define deletion costs for the (Si, ?), I inser-
tion costs for the (?, Tj), and R replacement costs
for the (Si, Tj) which represent nodes with non-
identical labels. Then a total cost for the align-
ment, C(??) can be defined as the sum of these
components costs, and the tree distance can then
be defined as the cost of the least-cost map:
?(S, T ) = min({C(??) | ? ? H(S, T )})
For any 3 trees, T 1, T 2, T 3, the triangle inequal-
ity holds ?(T 1, T 3) ? ?(T 1, T 2) + ?(T 2, T 3).
1If Tj1 = ?(Si1) and Tj2 = ?(Si2) then (i) Si1 is to the
left of Si2 iff Tj1 is to the left of Tj2 and (ii) Si1 is a descen-
dant of Si2 iff Tj1 is a descendant of Tj2 , with descendency
understood as the transitive closure of the daugher-mother re-
lation.
100
Briefly the argument is as follows. Given map-
pings ? ? H(T 1, T 2), and ? ? H(T 2, T 3), ??? ?
H(T 1, T 3)2, so (? ? ?)? is an alignment between
T 1 and T 3, and ?(T 1, T 3) ? C((? ? ?)?). The
cost of the composition is less than the sum of the
costs of the composed maps: ??s insertions and re-
placements contribute only if they fall in dom(?),
? ?s deletions and replacements contribute only if
they act on ran(?).
From this basic definition, one can depart in
a number of directions. First of all, there is a
part-vs-whole dimension of variation. Where
?(S, T ) gives the cost of aligning the whole
source tree S with the target T , one can consider
variants where one minimises over a set of sub-
parts of S. This is equivalent to letting all but the
nodes belonging to the chosen sub-part to delete
at zero cost3. Let ?(S, T ) be the sub-tree dis-
tance. Let ~?(S, T ), be the sub-traversal distance,
in which sub-traversals of the left-to-right, post-
order traversal of S are considered. As for ?, the
triangle inequality holds for ? and ~? ? one needs
to extend the notion of alignment with a set of free
deletions. Unlike ?, ? and ~? are not symmetric.
All of ?, ? and ~? are implicitly parametrised by
the cost functions, D, I and R. In the work below
4 other parameters are explored
Node weighting W: this is a function which
assigns a real-number weight to each each
node. The cost function then refers to the
weights. In experiments reported below,
Dw((Si, w), ?) = w, Iw(?, (Tj , w)) = w,
Rw((Si, ws), (Tj , wt)) = max(ws, wt), if
Si and Tj have unequal labels. The experi-
ments reported below use 2 weighting func-
tion ST R, and LEX . ST R assign weights
according to the syntactic structure, via a
classification of nodes as heads vs. comple-
ments vs. adjuncts vs. the rest, with es-
sentially adjuncts given 1/5th the weights of
heads and complements, and other daughters
1/2, via essentially the following top-down
algorithm:
Str(node, rank) :
assign weight 1/rank to node
for each daughter d
2?x ? T1?z ? T3((x, z) ? ? ? ? iff ?y ? T2((x, y) ?
?, (y, z) ? ? )
3Note that if one minimises also over sub-parts of the tar-
get, you do not get an interesting notion, as the minimum will
inevitably involve at most one node of source and target.
if (d is head or complement){
assign weight = 1/rank,
Str(rank, d) }
else if (d is adjunct) {
assign weight = 1/(5 ? rank),
Str(5 ? rank, d)}
else {
assign weight = 1/(2 ? rank)
Str(2 ? rank, d) }
LEX is a function which can be composed
with ST R, and scales up the weights of leaf
nodes by a factor of 3.
Target wild cards T (?): this is a function which
classifies certain target sub-trees as wild-
card. If source Si is mapped to target Tj , and
Tj is the root of a wild-card tree, all nodes
within the Si sub-tree can be deleted for 0
cost, and all those within the Tj sub-tree can
be inserted for 0 cost. A wild card np tree
might can be put in the position of the gap in
wh-questions, allowing for example what is
memory allocation, to closely match any sen-
tences with memory allocation as their ob-
ject, no matter what their subject ? see Fig-
ure 3.
Source self-effacers S/?: this is a function
which classifies source sub-trees as self-
effacers. Such trees can be deleted in
their entirety for zero cost. If S/? clas-
sifies all source sub-trees as self-effacing,
then ?(S/?) will coincide with notion of
?tree-distance with Cut? given in (Zhang and
Shasha, 1989).
Target self-inserters ?/T : this is a function
which classifies certain target sub-trees as
self-inserters. Such trees can be inserted in
their entirety for zero cost. A candidate might
be optional adjuncts.4
2.2 Sequence Measures
The tree-distance measures work with an elabora-
tion of the original questions and answers. (Lev-
enshtein, 1966) defined the 1 dimensional precur-
sor of tree distance, which works directly on the
2 word sequences for the answer and question.
For two sequences, s, t, and vertical (or hori-
zontal) tree encodings l tree(s) and l tree(t), if
4Thus a target wild-card is somewhat like a target self-
effacer, but one which also licenses the classification of a
matched source sub-tree as a being self-effacer.
101
process
n
np
s
vp
rhs
lhs be rhs
lhs
vp
call be
memory
n
n
n
allocation
np something
pro
np
s
vp
lhs rhs
memory
n
n
n
allocation
np
sub tree matching dist=5.0
Figure 1: Sub tree example
we define ?(s, t), as ?(l tree(s), l tree(t)), and
?(s, t), as ~?(l tree(s), l tree(t)), then ? and ?
coincide with the standard sequence edit distance
and sub-sequence edit distance. As special cases
of ? and ?, ? and ? inherit the triangle inequality
property.
To illustrate some of the tree-distance defini-
tions, in the following example, a ? distance of
3 between 2 trees is obtained, assuming unit costs
for deletions (shown in red and double outline), in-
sertions (shown in green and double outline), and
substitutions (shown in blue and linked with an ar-
row):
a
b
b
a
a
c
b
a
b b a
a
whole tree matching dist=3.0
Note also in this picture that nodes that are mapped
without a relabelling are shown at the same hori-
zontal level, with no linking arrow.
Figure 1 shows a sub-tree example ? ?. The
source tree nodes which do not belong to the cho-
sen sub-tree are shown in grey. The lowest vp sub-
tree in the source is selected, and mapped to the
vp in the target. The remaining target nodes must
be inserted, but this costs less than a match which
starts higher and necessitates some deletions and
substitutions.
Figure 2 shows a sub-tree example where the
structural weighting ST R has been used: size of
a node reflects the weight. 4 of the nodes in the
source represent the use of an auxiliary verb, and
receive low weight, changing the optimum match
to one covering the whole source tree. There is
some price paid in matching the dissimilar subject
nps.
process something
n pro
np
s
vp
rhs
lhs be rhs
lhs
vp
call be
memory
n
n
n
allocation
np
np
s
vp
lhs rhs
memory
n
n
n
allocation
np
sub tree matching dist=3.6
Figure 2: Structurally weighted example
Figure 3 continues the example, but this time
in the subject position there is a sub-tree which is
classified as a wild-card np tree, and it matches at
0 cost with the subject np in the source tree.
process
n
np np_wild
s
vp
rhs
lhs be rhs
lhs
vp
call be
memory
n
n
n
allocation
np
something
pro
s
vp
lhsrhs
memory
n
n
n
allocation
np
sub tree matching dist=1.6
Figure 3: Wild-card example
The basis of the algorithm used to calculate ?
is the ZhangShasha algorithm (Zhang and Shasha,
1989): the Appendix summarises it. The im-
102
plementation is based on code implementing ?
(Fontana et al, 2004), adapting it to allowing for
the ? and ~? variants and T (?), S/?, and ?/T pa-
rameters, and to generate the human-readable dis-
plays of the alignments (such as seen in figures 1,2
and 3).
2.3 Order invariant measures
Assessing answer/question similarity by variants
of tree distance or sequence edit-distance, means
that distance will not be word-order invariant.
There are also measures which are word-order in-
variant, sometimes called token-based measures.
These measures are usually couched in a vector
representation of questions and answers, where
vector dimensions are words from (some cho-
sen enumeration) of words (see (Salton and Lesk,
1968)). In the simplest case the values on each
dimensions are in {0, 1}, denoting presence or ab-
sence of a word. If ? is vector product and aw
is the set of words in a sequence a, then ~a ?~b =
|aw ? bw|, for the binary vectors representing aw,
bw. Three well known measures based on this are
given below, both in terms vectors, and for binary
vectors, the equivalent formulation with sets:
Dice 2(~a ?~b)/(~a ? ~a) + (~b ?~b))
= 2(|aw ? bw|)/(|aw |+ |bw|)
Jaccard (~a ?~b)/(~a ? ~a) +~b ?~b? ~a ?~b)
= (|aw ? bw|)/(|aw ? bw)
Cosine (~a ?~b)/(~a ? ~a).5(~b ?~b).5
= (|aw ? bw|)/((|aw |)0.5(|bw|)0.5)
These measure similarity, not difference, ranging
for 1 for identical aw,bw, to 0 for disjoint. In
the binary case, Dice/Jaccard similarity can be
related to the alignment-based, difference count-
ing perspective of the edit-distances. If we de-
fine ?w(a, b) as |aw ? bw| ? |aw ? bw| ? the size
of the symmetric difference between aw and bw ?
this can be seen as a set-based version of edit dis-
tance5, which (i) considers mappings on the sets of
words, aw, bw, not the sequences a, b, and (ii) sets
replacement cost to infinity. A difference measure
(ranging from 0 for identical aw,bw to 1 for dis-
joint) results if ?w(a, b) is divided by |aw| + |bw|
(resp. |aw ? bw|) and this difference measures will
give the reverse of a ranking by Dice (resp. Jac-
card) similarity.
The Cosine is a measure of the angle be-
tween the vectors ~a,~b, and is not relatable in the
5?w(a, b) could be equivalently defined as |(~a ?~b)|2
binary-case to the alignment-based, difference-
counting perspective of the edit-distances: di-
viding ?w(a, b), the symmetric difference, by
|aw|.5|bw|.5 does not give a measure with maxi-
mum value 1 for the disjoint case, and does not
give the reverse of a ranking by Cosine similarity.6
Below we shall use ? to denote the Cosine dis-
tance.
3 The Question Answering Tasks
For a given representation r (parse trees, word se-
quences etc.), and distance measure d, we shall
generically take a Question Answering by Dis-
tance (QAD) task to be given by a set of queries,
Q, and for each query q, a corpus of potential an-
swer sentences, CORq. For each a ? CORq, the
system determines d(r(a), r(q)), the distance be-
tween the representations of a and q, then uses this
to sort CORq into Aq. This sorting is then evalu-
ated in the following way. If ac ? Aq is the correct
answer, then the correct-answer-rank is the rank
of ac in Aq:
| {a ? Aq : d(r(a), r(q)) ? d(r(ac), r(q))} |
whilst the correct-answer-cutoff is the proportion
of Aq cut off by the correct answer ac:
| {a ? Aq : d(r(a), r(q)) ? d(r(ac), r(q))} | / | Aq |
Lower values for this connote better performance.
Another figure of merit is the reciprocal correct-
answer-rank. Higher values of this connote better
performance.
Note the notion of answerhood is not one requir-
ing answers to be the sub-sentential phrases asso-
ciated with wh-phrases in the question. Also not
all the questions are wh-questions.
Note also that the set of candidate answers
CORq is sorted by the answer-to-query distance,
d(r(a), r(q)), not the query-to-answer distance,
d(r(q), r(a)). The intuition is that the queries are
short and the answers longer, with sub-part that re-
ally contains the answer.
The performance of some of the above men-
tioned distance measures on 2 examples of QAD
tasks has been measured:
GNU Library Manual QAD Task: in
this case Q is a set of 88 hand-created
6if the vectors are normalised by their length, then you
can show |(~a/|~a| ?~b/|~b|)|2 reverses the Cosine ranking
103
queries, and CORq, shared by all the
queries, is the sentences of the manual
of the GNU C Library7 (| CORq |?
31, 000).
The TREC 11 QAD task: In this
case Q was the 500 questions of the
TREC11 QA track (Voorhees and Buck-
land, 2002), whose answers are drawn
from a large corpus of newspaper arti-
cles. CORq was taken to be the sen-
tences of the top 50 from the top-1000
ranking of articles provided by TREC11
for each question (| CORq |? 1000).
Answer correctness was determined us-
ing the TREC11 answer regular expres-
sions.
For the tree-distance measures, 2 parsing sys-
tems have been used. For convenience of refer-
ence, we will call the first parser, the trinity parser.
This is a home-grown parser combining a disam-
biguating part-of-speech tagger with a bottom-up
chartparser, refering to CFG-like syntax rules and
a subcategorisation system somewhat in a catego-
rial grammar style. Right-branching analyses are
prefered and a final selection of edges from all
available is made using a leftmost/longest selec-
tion strategy ? there is always an output regardless
of whether there is a single input-encompassing
edge. Preterminal node labels are a combination
of a main functor with other feature terms, but the
replacement cost function R is set to ignore the
feature terms. Terminal node labels are base forms
of words, not inflected forms. For the structural
weighting algorithm, ST R, the necessary node
distinctions are furnished directly by the parser for
vp, and by a small set of structure matching rules
for other structures (nps, pps etc). The structures
output for wh-questions are essentially deep struc-
tures, re-ordering an auxiliary inversion, and plac-
ing a tree in the position of a gap.
The Collins parser (Collins, 1999) (Model 3
variant) is a probabilistic parser, using a model of
trees as built top-down with a repertoire of moves,
learnt from the Penn Treebank. The preterminal
node labels are a combination of a Penn Tree-
bank label with other information pertaining to the
head/complement/adjunct distinction, but the re-
placement cost function R is set to ignore all but
the Penn Treebank part of the label. The termi-
7http://www.gnu.org
nal node labels are inflected forms of words, not
base forms. For the structural weighting algo-
rithm, ST R, the necessary node distinctions are
furnished directly by the parser. For the question
parses, a set of transformations is applied to the
parses directly given by the parser, which compa-
rable to the trinity parser, re-order auxiliary inver-
sion, and place a tree in the position of a gap.
4 Relating Parse Quality to Retrieval
Performance
As a kind of sanity-check on the idea of the us-
ing syntactic structures in retrieving answers, we
performed some experiments in which we var-
ied the sophistication of the parse trees that the
parsers could produce, the expectation being that
the less sophisticated the parse, the less successful
would be question-answering performance. The
left-hand data in Table 1 refers to various reduc-
tions of the linguistic knowledge bases of the trin-
ity parser(thin50 = random removal of 50% subset,
manual = manual removal of a subset, flat = en-
tirely flat parses, gold = hand-correction of query
parses and their correct answers). The right-hand
data in Table 1 refers to experiments in which the
repertoire of moves available to the Collins parser,
as defined by its grammar file, was reduced to dif-
ferent sized random subsets of itself.
Figure 4 shows the empirical cumulative den-
sity function (ecdf) of the correct-answer-cutoff
obtained with the weighted sub-tree with wild
cards measure. For each possible value c of
correct-answer-cutoff, it plots the percentage of
queries with a correct-answer-cutoff ? c.
0.0 0.1 0.2 0.3 0.4 0.5
0.
0
0.
4
0.
8
gold
full
thin50
manual
flat
Figure 4: Success vs Cut-off for different parse settings:
x = correct-answer-cutoff, y = proportion of queries whose
correct-answer-cutoff ? x (ranking by weighted sub-tree
with wild cards) (Library task)
What these experiments show is that the ques-
104
Table 1: Distribution of Correct Cutoff across query set Q in different parse settings. Left-hand data =
GNU task, trinity parser, right-hand data = TREC11 task, Collins parser
Parsing 1st Qu. Median Mean 3rd Qu.
flat 0.1559 0.2459 0.2612 0.3920
manual 0.0215 0.2103 0.2203 0.3926
thin50 0.01418 0.02627 0.157 0.2930
full 0.00389 0.04216 0.1308 0.2198
gold 0.00067 0.0278 0.1087 0.1669
Parsing 1st Qu. Median Mean 3rd Qu.
55 0.3157 0.6123 0.5345 0.766400
75 0.02946 0.1634 0.2701 0.4495
85 0.0266 0.1227 0.2501 0.4380
100 0.01256 0.08306 0.2097 0.2901
tion answering performance is a function of the so-
phistication of the parses that the parsers are able
to produce.
5 Comparing Distance Measures
Table 2 gives results on the Library task, using the
trinity parser, for some variations of the distance
measure.
Considering the results in 2, the best perform-
ing measure (mrr = 0.27) was the sub-traversal
distance, ~?, assigning weights structurally using
ST R, with lexical emphasis LEX , and treating a
gap position as an np wild card. This slightly out
performs the sub-tree measure, ? (mrr = 0.25).
An alternative approach to discounting parts of
the answer tree, allowing any sub-tree of the an-
swer the option to delete for free (?(W = Str ?
Lex, T (?) = np gap, S/? = ?)) performs con-
siderably worse (mrr = 0.16). Presumably this is
because it is too enthusiastic to assemble the query
tree from disparate parts of the answer tree. By
comparison, ~? and ? can only assembly the query
tree from parts of the answer tree that are more
closely connected.
The tree-distance measures (~?, ?) using struc-
tural weights, lexical emphasis and wild cards
(mrr = 0.27) out-perform the sub-sequence mea-
sure, ? (mrr = 0.197). It also out-performs the
cosine measure, ? (mrr = 0.190). But ? and ?
either out-perform or perform at about the same
level as the tree-distance measure if the lexical
emphasis is removed (see ?(W = Str, T (?) =
np gap), mrr = 0.160).
The tree-distance measure ? works better if
structural weighting is used (mrr = 0.09) than
if it is not (mrr = 0.04).
The tree-distance measure ? works better with
wild-cards (see ?(W = Str, T (?) = np gap),
mrr = 0.160, than without (see ?(W = Str),
mrr = 0.090).
Table 3 gives some results on the TREC11 task,
using the Collins parser. Fewer comparisons have
been made here.
The sub-traversal measure, using structural
weighting, lexical emphasis, and wild-cards per-
forms better (mrr = 0.150) than the sub-sequence
measure (mrr = 0.09), which in turn performs
better than the basic sub-traversal measure, with-
outh structural weighting, lexical emphasis or
wild-cards (mrr = 0.076). The cosine distance,
?, performed best.
6 Discussion
For the parsers used, you could easily have 2
sentences with completely different words, and
very different meanings, but which would have the
same pre-terminal syntactic structure: the preter-
minal syntactic structure is not a function of the
meaning. Given this, it is perhaps not surpris-
ing that there will be cases that the sequence dis-
tance easily spots as dissimilar, but which the tree
distance measure, without any lexical emphasis,
will regard as quite similar, and this perhaps ex-
plains why, without any lexical emphasis, the tree-
distance measure performs at similar level to, or
worse than, the sub-sequence distance measure.
With some kind of lexical emphasis in place,
the tree-distance measures out-perform the sub-
sequence measures. We can speculate as to the
reason for this. There are two kinds of case
where the tree-distance measures could be ex-
pected to spot a similarity which the sequence-
distance measures will fail to spot. One is when
the question and answer are more or less simi-
lar on their head words, but differ in determiners,
auxiliaries and adjuncts. The sequence distance
measure will pay more of a price for these differ-
ences than the structurally weighted tree-distance.
Another kind of case is when the answer supplies
words which match a wild-card in the middle of
the query tree, as might happen for example in:
Q: what do child processes inherit from their par-
ent processes
A: a child process inherits the owner and permis-
sions from the ancestor process
105
Table 2: For different distance measures (Library task, trinity parser), distrution of correct-answer-
cutoff, mean reciprocal rank mrr
cutoff
distance type 1st Qu. Median Mean mrr
~?(W = Str ? Lex, T (?) = np gap) 8.630-05 8.944-04 2.460-02 0.270
?(W = Str ? Lex, T (?) = np gap) 9.414e-05 1.428e-03 7.133e-02 0.255
? bases 1.569e-04 2.087e-03 5.181e-02 0.197
? bases 1.569e-04 8.630e-04 1.123e-02 0.190
?(W = Str ? Lex, T (?) = np gap, S/? = ?) 4.080e-04 9.352-03 5.853-02 0.160
?(W = Str, T (?) = np gap) 3.923e-04 1.964e-02 1.162e-01 0.160
?(W = Str) 5.060e-03 3.865e-02 1.303e-01 0.090
? 1.324e-03 1.046e-01 1.852e-01 0.040
? 8.398e-02 2.633e-01 3.531e-01 0.003
Table 3: For different distance measures (TREC task, collins parser) the distribution of correct-answer-
cutoff and mean reciprocal rank (mrr)
cutoff
distance type 1st Qu. Median Mean mrr
? forms 7.847e-03 2.631e-02 1.068e-01 0.167
~?(W = Str ? Lex, T (?) = np gap) 8.452e-03 4.898e-02 1.558e-01 0.150
? forms 2.113e-02 7.309-02 2.051e-01 0.092
~? 1.815e-02 1.030e-01 3.269e-01 0.076
The tree-distance measures will see these as
similar, but the sub-sequence measure will pay a
large price for words in the answer that match the
gap position in the query. Thus one can argue that
the use of structural weighting, and wild-card trees
in the query analysis will tend to equate things
which the sequence distance sees as dissimilar.
Another possible reason that the tree-distance
measure out-performs the sub-sequence measure
is that it may be able to distinguish things which
the sequence distance will tend to treat as equiva-
lent. A question might make the thematic role of
some entity very clear, but use very few significant
words as in:
what does malloc do ?
Using tree distance will favour answer sen-
tences with malloc as the subject, such as mal-
loc returns a null pointer. The basic problem for
the sequence distance here is that it does not have
much to work with and will only be able to parti-
tion the answer set into a small set of equivalence
classes.
These are speculations as to why tree-distance
would out-perform sequence distance. Whether
these equating and discriminating advantages
which theoretically should accrue to ?, ~? actually
will do so, will depend on the accuracy of the pars-
ing: if there is too much bad parsing, then we will
be equating that which we should keep apart, and
discriminating that which we should equate.
In the two tasks, the relationship between the
tree-distance measures and the order-invariant co-
sine measure worked out differently. The reasons
for this are not clear at the moment. One pos-
sibility is that our use of the Collins parser has
not yet resulted in good enough parses, especially
question parses ? recall that the indication from
4 was that improved parse quality will give better
retrieval performance. Also it is possible that rel-
ative to the queries in the Library task, the amount
of word-order permutation between question and
answer is greater in the TREC task. This is also
indicated by the fact that on the TREC task, the
sub-sequence measure, ?, falls considerably be-
hind the cosine measure, ?, whereas for the Li-
brary task they perform at similar levels.
Some other researchers have also looked at
the use of tree-distance measures in semantically-
oriented tasks. Punyakonok(2004) report work
106
using tree-distance to do question-answering on
the TREC11 data. Their work differs from that
presented here in several ways. They take the
parse trees which are output by Collins parser and
convert them into dependency trees between the
leaves. They compute the distance from query to
the answer, rather than from answer to query, us-
ing essentially the variant of tree-distance that al-
lows arbitrary sub-trees of the target to insert for
zero-cost. Presumably this directionality differ-
ence is not a significant one, and with distances
calculated from answers to queries, this would cor-
respond to the variant that allows arbitrary source
sub-trees to delete with zero cost. The cost func-
tions are parameterised to refer in the case of wild-
card replacements to (i) information derived from
Named Entity recognisers so different kinds of wh
wild-cards can be given low-cost replacment with
vocabulary categorised as belong to the right kind
by NE recognition and (ii) base-form information.
There is no way to make a numerical compar-
ison because they took a different answer corpus
CORq ? the articles containing the answers sug-
gested by TREC11 participants ? and a different
criterion of correctness ? an answer was correct if
it belonged to an article which the TREC11 adju-
dicators judges to contain a correct answer.
Their adaptation of cost functions to refer to es-
sentially semantic annotations of tree nodes is an
avenue we intend to explore in future work. What
this paper has sought to do is to investigate intrin-
sic syntactic parameters that might influence per-
formance. The hope is that these parameters still
play a role in an enriched system.
7 Conclusion and Future Work
For two different parsers, and two different
question-answering tasks, we have shown that im-
proved parse quality leads to better performance,
and that a tree-distance measure out-performs a
sequence distance measure. We have focussed on
intrinsic, syntactic properties of parse-trees. It is
not realistic to expect that exclusively using tree-
distance measures in this rather pure way will give
state-of-the-art question-answering performance,
but the contribution of this paper is the (start of
an) exporation of the syntactic parameters which
effect the use of tree-distance in question answer-
ing. More work needs to be done in systematically
varying the parsers, question-answering tasks, and
parametrisations of tree-distance over all the pos-
sibilities.
There are many possibilities to be explored in-
volving adapting cost functions to enriched node
descriptions. Already mentioned above, is the pos-
sibility to involve semantic information in the cost
functions. Another avenue is introducing weight-
ings based on corpus-derived statistics, essentially
making the distance comparision refer to extrin-
sic factors. One open question is whether anal-
ogously to idf , cost functions for (non-lexical)
nodes should depend on tree-bank frequencies.
Another question needing further exploration is
the dependency-vs-constituency contrast. Interest-
ingly Punyakonok(2004) themselves speculate:
each node in a tree represents only a
word in the sentence; we believe that ap-
propriately combining nodes into mean-
ingful phrases may allow our approach
to perform better.
We found working with constituency trees that
it was the sub-traversal distance measure that per-
formed best, and it needs to be seen whether this
holds also for dependency trees. Also to be ex-
plored is the role of structural weighting in a sys-
tem using dependency trees.
A final speculation that it would be interesting
to explore is whether one can use feed-back from
performance on a QATD task as a driver in the
machine-learning of probabilities for a parser, in
an approach analogous to the use of the language-
model in parser training.
References
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Walter Fontana, Ivo L. Hofacker, and Pe-
ter F. Stadler. 2004. Vienna rna package.
www.tbi.univie.ac.at/?ivo/RNA.
V. I. Levenshtein. 1966. Binary codes capable of cor-
recting insertions and reversals. Sov. Phys. Dokl,
10:707?710.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2004.
Natural language inference via dependency tree
mapping: An application to question answering.
Computational Linguistics.
G. Salton and M. E. Lesk. 1968. Computer evala-
tion of indexing and text processing. Journal of the
ACM, 15:8?36, January.
107
Ellen Voorhees and Lori Buckland, editors. 2002. The
Eleventh Text REtrieval Conference (TREC 2002).
Department of Commerce, National Institute of
Standards and Technology.
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM Journal of Computing, 18:1245?
1262.
Appendix
This appendix briefly summarises the algorithm
to compute the tree-distance, based on (Zhang
and Shasha, 1989) (see Section 2.1 for definition
of tree-distance). The algorithm operates on the
left-to-right post-order traversals of trees. Given
source and target trees S and T , the output is a
table T , indexed vertically by the traversal of S
and horizontally by the traversal of T , and posi-
tion T [i][j] is the tree-distance from the S subtree
rooted at i, to the T subtree rooted at j. Thus the
bottom righthand corner of the table represents the
tree distance between S and T .
If k is the index of a node of the tree, the left-
most leaf, l(k), is the index of the leaf reached
by following the left-branch down. For a given
leaf there is a highest node of which it is the
left-most leaf. Let such a node be called a key-
root. Let KR(T ) be the sequence of key-roots
in T . The algorithm is a doubly nested loop as-
cending throught the key-roots of S and T , in
which for each pair of key-roots (i, j), a routine
tree dist(i, j) updates the T table.
Suppose i is any node of S. Then for any is
with l(i) ? is ? i, the subsequence of S from
l(i) to is can be seen as a forest of subtrees of S,
denoted F (l(i), is). tree dist(i, j) creates a ta-
ble F , indexed vertically from l(i) to i and hori-
zontally from l(j) to j, such that F [is][jt] repre-
sents the distance between the forests F (l(i), is)
and F (l(j), jt). Also the F table should be seen
as having an extra left-most column, representing
for each is, l(i) ? is ? i, the F (l(i), is) to ? map-
ping (pure deletion), and an extra uppermost row
representing for each for each jt, l(j) ? jt ? j,
the ? to F (l(j), jt) mapping (pure insertion).
tree dist(i, j){
initialize:
F [l(i)][?], . . . ,F [i][?] = 1, . . . , i ? l(i) + 1
F [?][l(j)], . . . ,F [?][j] = 1, . . . , j ? l(j) + 1
loop: ?is, l(i) ? is ? i,?jt, l(j) ? jt ? j
{
case 1: l(is) = l(i) and l(jt) = l(j)
T [is][jt] = F [is][jt] = min of swap, delete,
insert, where
swap = F [is ? 1][jt ? 1] + swap(is, jt)
delete = F [is ? 1][jt] + delete(is)
insert = F [is][jt ? 1] + insert(jt)
case 2: either l(is) 6= l(i) or l(jt) 6= l(j)
F [is][jt] = min of delete, insert, for + tree,
where
swap, delete, insert as before and
for + tree = F [l(is) ? 1][l(jt) ? 1] + T [is][jt]
}
}
In case 1, the ?forests? F (l(i), is) and
F (l(j), jt) are both single trees and the computed
forest distance is transferred to the tree-distance
table T . In case 2, at least one of F (l(i), is) or
F (l(j), jt) represents a forest of more than one
tree. This means there is the possibility that the
final trees in the two forests are mapped to each
other. This quantity is found from the T table.
This formulation gives the whole-tree distance
between S and T . For the sub-tree distance, you
take the minimum of the final column of T . For
the sub-traversal case, you do the same but on the
final iteration, you set the pure deletion column
of F to all 0s, and take the minimum of the final
column of F .
To accommodate wild-card target trees, case
1 in the above is extended to allow T [is][jt] =
F [is][jt] = 0 in case jt is the root of a wild-card
tree. To accommodate self-effacing source trees,
case 2 in the above is extended to also consider
for + tree del = F [l(is)? 1, jt].
108
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 73?78,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring Multilingual Semantic Role Labeling 
 
Baoli Li, Martin Emms, Saturnino Luz, Carl Vogel 
Department of Computer Science 
Trinity College Dublin 
Dublin 2, Ireland 
{baoli.li,mtemms,luzs,vogel}@cs.tcd.ie 
 
 
 
Abstract 
This paper describes the multilingual semantic 
role labeling system of Computational Lin-
guistics Group, Trinity College Dublin, for the 
CoNLL-2009 SRLonly closed shared task. 
The system consists of two cascaded compo-
nents: one for disambiguating predicate word 
sense, and the other for identifying and classi-
fying arguments. Supervised learning tech-
niques are utilized in these two components. 
As each language has its unique characteris-
tics, different parameters and strategies have 
to be taken for different languages, either for 
providing functions required by a language or 
for meeting the tight deadline. The system ob-
tained labeled F1 69.26 averaging over seven 
languages (Catalan, Chinese, Czech, English, 
German, Japanese, and Spanish), which ranks 
the system fourth among the seven systems 
participating the SRLonly closed track. 
1 Introduction 
Semantic role labeling, which aims at computa-
tionally identifying and labeling arguments of 
predicate words, has become a leading research 
problem in computational linguistics with the ad-
vent of various supporting resources (e.g. corpora 
and lexicons) (M?rquez et al, 2008). Word seman-
tic dependencies derived by semantic role labeling 
are assumed to facilitate automated interpretation 
of natural language texts. Moreover, techniques for 
automatic annotation of semantic dependencies can 
also play an important role in adding metadata to 
corpora for the purposes of machine translation 
and speech processing. We are currently investi-
gating such techniques as part of our research into 
integrated language technology in the Center for 
Next Generation Localization (CNGL, 
http://www.cngl.ie). The multilingual nature of the 
CoNLL-2009 shared task on syntactic and seman-
tic dependency analysis, which includes Catalan, 
Chinese, Czech, English, German, Japanese, and 
Spanish (Haji? et al, 2009), makes it a good test-
bed for our research. 
We decided to participate in the CoNLL-2009 
shared task at the beginning of March, signed the 
agreement for getting the training data on March 
2nd, 2009, and obtained all the training data (espe-
cially the part from LDC) on March 4th, 2009. Due 
to the tight time constraints of the task, we chose to 
use existing packages to implement our system. 
These time constraints also meant that we had to 
resort to less computationally intensive methods to 
meet the deadline, especially for some large data-
sets (such as the Czech data). In spite of these dif-
ficulties and resource limitations, we are proud to 
be among the 21 teams who successfully submitted 
the results1. 
As a new participant, our goals in attending the 
CoNLL-2009 SRLonly shared task were to gain 
more thorough knowledge of this line of research 
and its state-of-the-art, and to explore how well a 
system quickly assembled with existing packages 
can fare at this hard semantic analysis problem.  
Following the successful approaches taken by 
the participants of the CoNLL-2008 shared task 
(Surdeanu et al, 2008) on monolingual syntactic 
and semantic dependency analysis, we designed 
and implemented our CoNLL-2009 SRLonly sys-
tem with pipeline architecture. Two main compo-
nents are cascaded in this system: one is for 
disambiguating predicate word sense 2 , and the 
other for identifying and classifying arguments for 
                                                          
1
 According to our correspondence with Dr. Jan Haji?, totally 
31 teams among 60 registered ones signed and got the evalua-
tion data. 
2
 As predicate words are marked in the CoNLL-2009 datasets, 
we don?t need to identify predicate words. 
73
predicate words. Different supervised learning 
techniques are utilized in these two components. 
For predicate word sense disambiguation (WSD), 
we have experimented with three algorithms: SVM, 
kNN, and Na?ve Bayes. Based on experimental 
results on the development datasets, we chose 
SVM and kNN to produce our submitted official 
results. For argument identification and classifica-
tion, we used a maximum entropy classifier for all 
the seven datasets. As each language has its unique 
characteristics and peculiarities within the dataset, 
different parameters and strategies have to be taken 
for different languages (as detailed below), either 
for providing functions required by a language or 
for meeting the tight deadline. Our official submis-
sion obtained 69.26 labeled F1 averaging over the 
seven languages, which ranks our system fourth 
among the seven systems in the SRLonly closed 
track. 
The rest of this paper is organized as follows. 
Section 2 discusses the first component of our sys-
tem for predicate word sense disambiguation. Sec-
tion 3 explains how our system detects and 
classifies arguments with respect to a predicate 
word. We present experiments in Section 4, and 
conclude in Section 5. 
2 Predicate Word Sense Disambiguation 
This component tries to determine the sense of a 
predicate word in a specific context. As a sense of 
a predicate word is often associated with a unique 
set of possible semantic roles, this task is also 
called role set determination. Based on the charac-
teristics of different languages, we take different 
strategies in this step, but the same feature set is 
used for different languages. 
2.1 Methods 
Intuitively, each predicate word should be treated 
individually according to the list of its possible 
senses. We therefore designed an initial solution 
based on the traditional methods in WSD: repre-
sent each sense as a vector from its definition or 
examples; describe the predicate word for disam-
biguation as a vector derived from its context; and 
finally output the sense which has the highest simi-
larity with the current context. We also considered 
using singular value decomposition (SVD) to over-
come the data sparseness problem. Unfortunately, 
we found this solution didn?t work well in our pre-
liminary experiments. The main problem is that the 
definition of each sense of a predicate word is not 
available. What we have is just a few example con-
texts for one sense of a predicate word, and these 
contexts are often not informative enough for 
WSD. On the other hand, our limited computing 
resources could not afford SVD operation on a 
huge matrix. 
We finally decided to take each sense tag as a 
class tag across different words and transform the 
disambiguation problem into a normal multi-class 
categorization problem. For example, in the Eng-
lish datasets, all predicates with ?01? as a sense 
identifier were counted as examples for the class 
?01?. With this setting, a predicate word may be 
assigned an invalid sense tag. It is an indirect solu-
tion, but works well. We think there are at least 
two possible reasons: firstly, most predicate words 
take their popular sense in running text. For exam-
ple, in the English dataset (training and develop-
ment), 160,477 of 185,406 predicate occurrences 
(about 86.55%) take their default sense ?01?. Sec-
ondly, predicates may share some common role 
sets, even though their senses may not be exactly 
the same, e.g. ?tell? and ?inform?. 
Unlike the datasets in other languages, the Japa-
nese dataset doesn?t have specialized sense tags 
annotated for each predicate word, so we simply 
copy the predicted lemma of a predicate word to its 
PRED field. For other datasets, we derived a train-
ing sample for each predicate word, whose class 
tag is its sense tag. Then we trained a model from 
the generated training data with a supervised learn-
ing algorithm, and applied the learned model for 
predicting the sense of a predicate word. This is 
our base solution. 
When transforming the datasets, the Czech data 
needs some special processing because of its 
unique annotation format. The sense annotation for 
a predicate word in the Czech data does not take 
the form ?LEMMA.SENSE?. In most cases, no 
specialized sense tags are annotated. The PRED 
field of these words only contains ?LEMMA?. In 
other cases, the disambiguated senses are anno-
tated with an internal representation, which is 
given in a predicate word lexicon. We decomposed 
the internal representation of each predicate word 
into two parts: word index id and sense tag. For 
example, from ?zv??en? v-w10004f2? we know ?v-
w10004? is the index id of word ?zv??en??, and 
?f2? is its sense tag. We then use these derived 
74
sense tags as class tags and add a class tag ?=? for 
samples without specialized sense tag. 
For each predicate word, we derive a vector de-
scribing its context and attributes, each dimension 
of which corresponds to a feature. We list the fea-
ture types in the next subsection. Features appear-
ing only once are removed. The TF*IDF weighting 
schema is used to calculate the weight of a feature. 
Three different algorithms were tried during the 
development period: support vector machines 
(SVM), distance-weighted k-Nearest Neighbor 
(kNN) (Li et al, 2004), and Na?ve Bayes with mul-
tinomial model (Mccallum and Nigam, 1998). As 
to the SVM algorithm, we used the robust 
LIBSVM package (Chang and Lin, 2001), with a 
linear kernel and default values for other parame-
ters. The algorithms achieving the best results in 
our preliminary experiments are chosen for differ-
ent languages: SVM for Catalan, Chinese, and 
Spanish; kNN for German (k=20). 
We used kNN for English (k=20) and Czech 
(k=10) because we could not finish training with 
SVM on these two datasets in limited time. Even 
with kNN algorithm, we still had trouble with the 
English and Czech datasets, because thousands of 
training samples make the prediction for the 
evaluation data unacceptably slow. We therefore 
had to further constrain the search space for a new 
predicate word to those samples containing the 
same predicate word. If there are not samples con-
taining the same predicate word in the training data, 
we will assign it the most popular sense tag (e.g. 
?01? for English). 
How to use the provided predicate lexicons is a 
challenging issue. Lexicons for different languages 
take different formats and the information included 
in different lexicons is quite different. We derived 
a sense list lexicon from the original predicate 
lexicon for Chinese, Czech, English, and German. 
Each entry in a sense list lexicon contains a predi-
cate word, its internal representation (especially for 
Czech), and a list of sense tags that the predicate 
can have. Then we obtained a variant of our base 
solution, which uses the sense list of a predicate 
word to filter impossible senses. It works as fol-
lows: 
- Disambiguate a new predicate with the base 
solution; 
- Choose the most possible sense from all the 
candidate senses obtained in step 1: if the 
base classifier doesn?t output a vector of 
probabilities for classes, only check 
whether the predicted one is a valid sense 
for the predicate; 
- If there is not a valid sense for a new predi-
cate (including the cases where the predi-
cate does not have an entry in the sense list 
lexicon), output the most popular sense tag; 
Unfortunately, preliminary experiments on the 
German and Chinese datasets didn?t support to in-
clude such a post-processing stage. The perform-
ance with this filtering became a little worse. 
Therefore, we decided not to use it generally, but 
one exception is for the Czech data. 
With kNN algorithm, we can greatly reduce the 
time for training the Czech data, but we do have 
problem with prediction, as there are totally 
469,754 samples in the training dataset. It?s a time-
consuming task to calculate the similarities be-
tween a new sample and all the samples in the 
training dataset to find its k nearest neighbors, thus 
we have to limit the search space to those samples 
that contain the predicate word for disambiguation. 
To process unseen predicate words, we used the 
derived sense list lexicon: if a predicate word for 
disambiguation is out of the sense list lexicon, we 
simply copy its predicted lemma to the PRED field; 
if no sample in the training dataset has the same 
predicate word, we take its first possible sense in 
the sense list lexicon. With this strategy, our sys-
tem can process the huge Czech dataset in short 
time. 
2.2 Features 
The features we used in this step include3: 
 
a. [Lemma | (Lemma with POS)] of all words in the sen-
tence; 
b. Attributes of predicate word, which is obtained from 
PFEAT field by splitting the field at symbol ?|? and 
removing the invalid attribute of ?*?; 
c. [Lemma | POS] bi-grams of predicate word and its 
[previous | following] one word; 
d. [Lemma | POS] tri-grams of predicate word and its 
[previous | following] two words; 
e. [Lemma | (Lemma with POS)] of its most [left | right] 
child; 
f. [(Lemma+Dependency_Relation+Lemma) | (POS 
+Dependency_Relation+POS)] of predicate word and 
its most [left | right] child; 
                                                          
3
 We referred to those CoNLL-2008 participants? reports, e.g. 
(Ciaramita et al, 2008), when we designed the feature sets for 
the two components. 
75
g. [Lemma | (Lemma with POS)] of the head of the pre-
dicate word; 
h. [(Lemma+Dependency_Relation+Lemma) | (POS+D-
ependency_Relation+POS)] of predicate word and its 
head; 
i. [Lemma | (Lemma with POS)] of its [previous | fol-
lowing] two brothers; 
j. [Lemma | POS | (Dependency relation)] bi-gram of 
predicate word and its [previous | following] one 
brother; 
k. [Lemma | POS | (Dependency relation)] tri-gram of 
predicate word and its [previous | following] two 
brothers. 
3 Argument Identification and Classifica-
tion  
The second component of our system is used to 
detect and classify arguments with respect to a 
predicate word. We take a joint solution rather than 
solve the problem in two consecutive steps: argu-
ment identification and argument classification. 
3.1 Methods  
By introducing an additional argument type tag ?_? 
for non-arguments, we transformed the two tasks 
(i.e. argument identification and argument classifi-
cation) into one multi-class classification problem. 
As a word can play different roles with respect to 
different predicate words and a predicate word can 
be an argument of itself, we generate a training set 
by deriving a training example from each word-
predicate pair. For example, if a sentence with two 
predicates has 7 words, we will derive 7*2=14 
training examples. Therefore, the number of train-
ing examples generated in this step will be around 
L times larger than that obtained in the previous 
step, where L is the average length of sentences. 
We chose to use maximum entropy algorithm in 
this step because of its success in the CoNLL-2008 
shared task (Surdeanu et al, 2008). Le Zhang?s 
maximum entropy package (Zhang, 2006) is inte-
grated in our system. 
The Czech data cause much trouble again for us, 
as the training data derived by the above strategy 
became even larger. We had to use a special strat-
egy for the Czech data: we selectively chose word-
predicate pairs for generating the training dataset. 
In other words, not all possible combinations are 
used. We chose the following words with respect 
to each predicate: the first and the last two words 
of a sentence; the words between the predicate and 
any argument of it; two words before the predicate 
or any argument; and two words after the predicate 
or any argument. 
In the Czech and Japanese data, some words 
may play multiple roles with respect to a predicate 
word. We thus have to consider multi-label classi-
fication problem (Tsoumakas and Katakis, 2007) 
for these two languages? data. We tried the follow-
ing two solutions: 
? Take each role type combination as a class 
and transform the multi-label problem to a 
single-label classification problem; 
? Classify a word with a set of binary classi-
fiers: consider each role type individually 
with a binary classifier; any possible role 
type will be output; if no role type is ob-
tained after considering all the role types, 
the role type with the highest confidence 
value will be output; and, if ?_? is output 
with any other role type, remove it. 
We used the second solution in our official 
submission, but we finally found these two solu-
tions perform almost the same. The performance 
difference is very small. We found the cases with 
multi-labels (actually at most two) in the training 
data are very limited: 690 of 414,326 in the Czech 
data and 113 of 46,663 in the Japanese data. 
3.2 Features 
The features we used in this step include: 
 
a. Whether the current word is a predicate; 
b. [Lemma | POS] of current word and its [previous | fol-
lowing] one word; 
c. [Lemma | POS] bi-grams of current word and its [pre-
vious | following] one word; 
d. POS tri-grams of current word, its previous word and 
its following word; 
e. Dependency relation of current word to its head; 
f. [Lemma | POS] of the head of current word; 
g. [Lemma | POS] bi-grams of current word and its head; 
h. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of current word and its 
head; 
i. [Lemma | POS] of its most [left | right] child; 
j. [Lemma | POS] bi-grams of current word and its most 
[left | right] child; 
k. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS) of current word and its 
most [left | right] child; 
l. The number of children of the current word and the 
predicate word; 
m. Attributes of the current word, which is obtained from 
PFEAT field by splitting the field at symbol ?|? and 
removing the invalid attribute of ?*?; 
n. The sense tag of the predicate word; 
76
o. [Lemma | POS] of the predicate word and its head; 
p. Dependency relation of the predicate word to its head; 
q. [Lemma | POS] bi-grams of the predicate word and its 
head; 
r. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of the predicate word and 
its head; 
s. [Lemma | POS] of the most [left | right] child of the 
predicate word; 
t. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of predicate word and its 
head; 
u. [Lemma | POS] bi-gram of the predicate word and its 
most [left | right] child; 
v. [(Lemma+Dependency_Relation+Lemma) | (POS+De 
pendency_Relation+POS)] of the predicate word and 
its most [left | right] child; 
w. The relative position of the current word to the predi-
cate one: before, after, or on; 
x. The distance of the current word to the predicate one; 
y. The relative level (up, down, or same) and level dif-
ference on the syntactic dependency tree of the current 
word to the predicate one; 
z. The length of the shortest path between the current 
word and the predicate word. 
4 Experiments 
4.1 Datasets  
The datasets of the CoNLL-2009 shared task con-
tain seven languages: Catalan (CA), Chinese (CN), 
Czech (CZ), English (EG), German (GE), Japanese 
(JP), and Spanish (SP). The training and evaluation 
data of each language (Taul? et al, 2008; Xue et 
al., 2008; Haji? et al, 2006; Palmer et al, 2002; 
Burchardt et al, 2006; Kawahara et al, 2002) have 
been converted to a uniform CoNLL Shared Task 
format. Each participating team is required to 
process all seven language datasets.  
 
Lanuage CA CN CZ EN GE JP SP 
Size (KB) 48974 41340 94284 58155 41091 8948 52430 
# of Sen-
tences 14924 24039 43955 40613 38020 4643 15984 
# of Predi-
cate words 42536 110916 469754 185404 17988 27251 48900 
Avg. # of 
Predicates 
per sentence 
2.85 4.61 10.69 4.57 0.47 5.87 3.06 
popular 
sense tag 
a2 
(37%) 
01 
(90%) 
= 
(81%) 
01 
(87%) 
1 
(75%) 
= 
(100%) 
a2 
(39%) 
Table 1. Statistical information of the seven language 
datasets (training and development). 
 
Table 1 shows some statistical information of 
both training and development data for each lan-
guage. The total size of the uncompressed original 
data without lexicons is about 345MB. The Czech 
dataset is the largest one containing 43,955 sen-
tences and 469,754 predicate words, while the 
Japanese dataset the smallest one. On average, 
10.69 predicate words appear in a Czech sentence, 
while only 0.47 predicate words exist in a German 
sentence. The most popular sense tag in the Czech 
datasets is ?=?, which means the PRED field has 
the same value as the PLEMMA field or the 
FORM field. About 81% of Czech predicate words 
take this value. 
4.2 Experimental Results  
F1 is used as the main evaluation metric in the 
CoNLL-2009 shared task. As to the SRLonly track, 
a joint semantic labeled F1, which considers predi-
cate word sense disambiguation and argument la-
beling equally, is used to rank systems. 
 
Avg. CA CN CZ EG GE JP SP 
69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54 
Table 2. Official results of our system. 
 
Table 2 gives the official results of our system 
on the evaluation data. The system obtained the 
best result (74.06) on the Catalan data, but per-
formed very poor (57.46) on the Czech data. Ex-
cept the Czech data, our system performs quite 
stable on the other six language data with mean of 
71.23 and standard deviation of 2.42. 
 
 Avg. CA CN CZ EG GE JP SP 
Over-
all F1 69.47 74.12 70.52 57.57 70.24 67.97 72.17 73.68 
Pred. 
WSD 
F1 
86.9 84.42 94.54 72.23 92.98 81.09 99.07 83.96 
Arg 
I&C 
F1 
57.24 69.29 57.71 33.19 58.25 60.64 52.72 68.86 
Arg 
I&C 
PR 
69.77 73.43 72.48 62.14 70.14 66.63 69.37 74.23 
Arg 
I&C 
RE 
49.77 65.6 47.94 22.64 49.81 55.64 42.52 64.21 
Table 3. Results of our system after fixing a minor bug. 
 
After submitting the official results, we found 
and fixed a minor bug in the implementation of the 
second component. Table 3 presents the results of 
our system after fixing this bug. The overall per-
formance doesn?t change much. We further ana-
lyzed the bottlenecks by checking the performance 
of different components. 
At the predicate WSD part, our system works 
reasonable with labeled F1 86.9, but the perform-
ance on the Czech data is lower than that of a base-
line system that constantly chooses the most 
popular sense tag. If we use this baseline solution, 
77
we can get predicate WSD F1 78.66, which further 
increases the overall labeled F1 on the Czech data 
to 61.68 from 57.57 and the overall labeled F1 
over the seven languages to 70.05 from 69.47. 
From table 3, we can see our system performs 
relatively poorly for argument identification and 
classification (57.24 vs. 86.9). The system seems 
too conservative for argument identification, which 
makes the recall very lower. We explored some 
strategies for improving the performance of the 
second component, e.g. separating argument iden-
tification and argument classification, and using 
feature selection (with DF threshold) techniques, 
but none of them helps much. We are thinking the 
features currently used may not be effective 
enough, which deserves further study. 
5 Conclusion and Future Work  
In this paper, we describe our system for the 
CoNLL-2009 shared task -- SRLonly closed track. 
Our system was built on existing packages with a 
pipeline architecture, which integrated two cas-
caded components: predicate word sense disam-
biguation and argument identification and 
classification. Our system performs well at disam-
biguating the sense of predicate words, but poorly 
at identifying and classifying arguments. In the 
future, we plan to explore much effective features 
for argument identification and classification. 
Acknowledgments 
This research was funded by Science Foundation 
Ireland under the CNGL grant. We used the IITAC 
Cluster in our initial experiments. We thank IITAC, 
the HEA, the National Development Plan and the 
Trinity Centre for High Performance Computing 
for their support. We are also obliged to John 
Keeney for helping us running our system on the 
CNGL servers. 
References  
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea 
Kowalski, Sebastian Pad? and Manfred Pinkal. 2006. 
The SALSA Corpus: a German Corpus Resource for 
Lexical Semantics. Proceedings of the 5th Interna-
tional Conference on Language Resources and Eval-
uation (LREC-2006). Genoa, Italy. 
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: 
a library for support vector machines. Software 
available at http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
Massimiliano Ciaramita, Giuseppe Attardi, Felice 
Dell?Orletta, and Mihai Surdeanu. 2008. DeSRL: A 
Linear-Time Semantic Role Labeling System. Pro-
ceedings of the CoNLL-2008. 
Jan Haji?, Massimiliano Ciaramita, Richard Johansson, 
Daisuke Kawahara, Maria Antonia Mart?, Llu?s 
M?rquez, Adam Meyers, Joakim Nivre, Sebastian 
Pad?, Jan ?t?p?nek, Pavel Stra??k, Mihai Surdeanu, 
Nianwen Xue and Yi Zhang. 2009. The CoNLL-2009 
Shared Task: Syntactic and Semantic Dependencies 
in Multiple Languages. Proceedings of the 13th 
Conference on Computational Natural Language 
Learning (CoNLL-2009). Boulder, Colorado, USA. 
Jan Haji?, Jarmila Panevov?, Eva Haji?ov?, Petr Sgall, 
Petr Pajas, Jan ?t?p?nek, Ji?? Havelka, Marie 
Mikulov? and Zden?k ?abokrtsk?. 2006. The Prague 
Dependency Treebank 2.0. Linguistic Data 
Consortium, USA. ISBN 1-58563-370-4. 
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 
2002. Construction of a Japanese Relevance-tagged 
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). Las Palmas, Spain. 
Baoli Li, Qin Lu and Shiwen Yu. 2004. An Adaptive k-
Nearest Neighbor Text Categorization Strategy. ACM 
Transactions on Asian Language Information 
Processing, 3(4): 215-226. 
Llu?s M?rquez, Xavier Carreras, Kenneth C. Litkowski 
and Suzanne Stevenson. 2008. Semantic Role Label-
ing: An Introduction to the Special Issue. Computa-
tional Linguistics, 34(2):145-159. 
Andrew Mccallum and Kamal Nigam. 1998. A Com-
parison of Event Models for Naive Bayes Text Clas-
sification. Proceedings of AAAI/ICML-98 Workshop 
on Learning for Text Categorization. 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Lluis Marquez and Joakim Nivre. 2008. The CoNLL-
2008 Shared Task on Joint Parsing of Syntactic and 
Semantic Dependencies. Proceedings of the 12th 
Conference on Computational Natural Language 
Learning (CoNLL-2008).  
Mariona Taul?, Maria Ant?nia Mart? and Marta 
Recasens. 2008. AnCora: Multilevel Annotated 
Corpora for Catalan and Spanish. Proceedings of the 
6th International Conference on Language Resources 
and Evaluation (LREC-2008). Marrakech, Morocco. 
Grigorios Tsoumakas and Ioannis Katakis. 2007. Multi-
Label Classification: An Overview. International 
Journal of Data Warehousing and Mining, 3(3):1-13. 
Nianwen Xue and Martha Palmer. 2009. Adding 
semantic roles to the Chinese Treebank.  Natural 
Language Engineering, 15(1):143-172.  
Le Zhang. 2006. Maximum Entropy Modeling Toolkit 
for Python and C++. Software available at 
http://homepages.inf.ed.ac.uk/s0450736/maxent_tool
kit.html. 
78
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 619?623,
Dublin, Ireland, August 23-24, 2014.
TCDSCSS: Dimensionality Reduction to Evaluate Texts of Varying
Lengths - an IR Approach
Arun Jayapal
Dept of Computer Science
Trinity College Dublin
jayapala@cs.tcd.ie
Martin Emms
Dept of Computer Science
Trinity College Dublin
martin.emms@cs.tcd.ie
John D.Kelleher
School of Computing
Dublin Institute of Technology
john.d.kelleher@dit.ie
Abstract
This paper provides system description of
the cross-level semantic similarity task for
the SEMEVAL-2014 workshop. Cross-
level semantic similarity measures the de-
gree of relatedness between texts of vary-
ing lengths such as Paragraph to Sen-
tence and Sentence to Phrase. Latent Se-
mantic Analysis was used to evaluate the
cross-level semantic relatedness between
the texts to achieve above baseline scores,
tested on the training and test datasets. We
also tried using a bag-of-vectors approach
to evaluate the semantic relatedness. This
bag-of-vectors approach however did not
produced encouraging results.
1 Introduction
Semantic relatedness between texts have been
dealt with in multiple situations earlier. But it is
not usual to measure the semantic relatedness of
texts of varying lengths such as Paragraph to Sen-
tence (P2S) and Sentence to Phrase (S2P). This
task will be useful in natural language process-
ing applications such as paraphrasing and summa-
rization. The working principle of information re-
trieval system is the motivation for this task, where
the queries are not of equal lengths compared to
the documents in the index. We attempted two
ways to measure the semantic similarity for P2S
and S2P in a scale of 0 to 4, 4 meaning both texts
are similar and 0 being dissimilar. The first one
is Latent Semantic Analysis (LSA) and second, a
bag-of-vecors (BV) approach. An example of tar-
get similarity ratings for comparison type S2P is
provided in table 1.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails: http://creativecommons.org/licenses/
by/4.0/
Sentence: Schumacher was undoubtedly one of
the very greatest racing drivers there has ever
been, a man who was routinely, on every lap, able
to dance on a limit accessible to almost no-one
else.
Score Phrase
4 the unparalleled greatness
of Schumachers driving abilities
3 driving abilities
2 formula one racing
1 north-south highway
0 orthodontic insurance
Table 1: An Example - Sentence to Phrase simi-
larity ratings for each scale
2 Data
The task organizers provided training data, which
included 500 pairs of P2S, S2P, Phrase to Word
(P2W) and their similarity scores. The training
data for P2S and S2P included text from different
genres such as Newswire, Travel, Metaphoric and
Reviews. In the training data for P2S, newswire
text constituted 36% of the data, while reviews
constituted 10% of the data and rest of the three
genres shared 54% of the data.
Considering the different genres provided in the
training data, a chunk of data provided for NIST
TAC?s Knowledge Base Population was used for
building a term-by-document matrix on which
to base the LSA method. The data included
newswire text and web-text, where the web-text
included data mostly from blogs. We used 2343
documents from the NIST dataset
1
, which were
available in eXtended Markup Language format.
Further to the NIST dataset, all the paragraphs
in the training data
2
of paragraph to sentence were
added to the dataset. To add these paragraphs to
the dataset, we converted each paragraph into a
1
Distributed by LDC (Linguistic Data Consortium)
2
provided by the SEMEVAL task-3 organizers
619
new document and the documents were added to
the corpus. The unique number of words identi-
fied in the corpus were approximately 40000.
3 System description
We tried two different approaches for evaluating
the P2S and S2P. Latent Semantic Analysis (LSA)
using SVD worked better than the Bag-of-Vectors
(BV) approach. The description of both the ap-
proaches are discussed in this section.
3.1 Latent Semantic Analysis
LSA has been used for information retrieval al-
lowing retrieval via vectors over latent, arguably
conceptual, dimensions, rather than over surface
word dimensions (Deerwester et al., 1990). It was
thought this would be of advantage for comparison
of texts of varying length.
3.1.1 Representation
The data corpus was converted into a mxn term-
by-document matrix, A, where the counts (c
m,n
)
of all terms (w
m
) in the corpus are represented
in rows and the respective documents (d
n
) in
columns:
A =
?
?
?
?
?
d
1
d
2
? ? ? d
n
w
1
c
1,1
c
1,2
? ? ? c
1,n
w
2
c
2,1
c
2,2
? ? ? c
2,n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w
m
c
m,1
c
m,2
? ? ? c
m,n
?
?
?
?
?
The document indexing rules such as text tok-
enization, case standardization, stop words re-
moval, token stemming, and special characters and
punctuations removal were followed to get the ma-
trix A.
Singular Value Decomposition (SVD) decom-
poses the matrix into U , ? and V matrices (ie.,
A = U?V
T
) such that U and V are orthonormal
matrices and ? is a diagonal matrix with singular
values. Retaining just the first k columns of U and
V , gives an approximation of A
A ? A
k
= U
k
?
k
V
T
k
(1)
According to LSA, the columns of U
k
are thought
of as representing latent, semantic dimensions,
and an arbitrary m-dimensional vector
#?
v can be
projected onto this semantic space by taking the
dot-product with each column of U
k
; we will call
the result
#      ?
v
sem
.
In the experiments reported later, the m-
dimensional vector
#?
v is sometimes a vector of
word counts, and sometimes a thresholded or
?boolean? version, mapping all non-zero numbers
to 1.
3.1.2 Similarity Calculation
To evaluate the similarity of a paragraph, p, and a
sentence, s, first these are represented as vectors of
word counts,
#?
p and
#?
s , then these are projected in
the latent semantic space, to give
#      ?
p
sem
and
#      ?
s
sem
,
and then between these the cosine similarity met-
ric is calculated:
cos(
#      ?
p
sem
.
#      ?
s
sem
) =
#      ?
p
sem
.
#      ?
s
sem
|
#      ?
p
sem
|.|
#      ?
s
sem
|
(2)
The cosine similarity metric provides a similarity
value in the range of 0 to 1, so to match the target
range of 0 to 4, the cosine values were multiplied
by 4. Exactly the same procedure is used for the
sentence to phrase comparison.
Further, the number of retained dimensions of
U
k
was varied, giving different dimensionalities
of the LSA space. The results of testing at the re-
duced dimensions are discussed in 4.1
3.2 Bag-of-Vectors
Another method we experimented on could be
termed a ?bag-of-vectors? (BV) approach: each
word in an item to be compared is replaced by a
vector representing its co-occurrence behavior and
the obtained bags of vectors enter into the compar-
ison process.
3.2.1 Representation
For the BV approach, the same data sources as was
used for the LSA approach is turned into a m?m
term-by-term co-occurrence matrix C:
C =
?
?
?
?
?
?
?
w
1
w
2
? ? ? w
m
w
1
c
1,1
c
1,2
? ? ? c
1,m
w
2
c
2,1
c
2,2
? ? ? c
2,m
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
w
m
c
m,1
c
m,2
? ? ? c
m,m
?
?
?
?
?
?
?
The same preprocessing steps as for the LSA ap-
proach applied (text tokenization, case standard-
ization, stop words removal, special characters and
punctuations removal). Via C, if one has a bag-
of-words representing a paragraph, sentence or
phrase, one can replace it by a bag-of-vectors, re-
placing each word w
i
by the corresponding row of
C ? we will call these rows word-vectors.
620
3.2.2 Similarity Calculation
For calculating P2S similarity, the procedure is
as follows. The paragraph and sentence are tok-
enized, and stop-words were removed and are rep-
resented as two vectors
#?
p and
#?
s .
For each word p
i
from
#?
p , its word vector from
C is found, and this is compared to the word vector
for each word s
i
in
#?
s , via the cosine measure. The
highest similarity score for each word p
i
in
#?
p is
stored in a vector
# ?
S
p
shown in (3). The overall
semantic similarity score between paragraph and
sentence is then the mean value of the vector
# ?
S
p
?
4 ? see (4).
S
p
=
[
S
p
1
S
p
2
? ? ? S
p
i
]
(3)
S
sim
=
?
n
i=1
S
p
i
n
? 4 (4)
Exactly corresponding steps are carried out for the
S2P similarity. Although experiments were car-
ried out this particular BV approach, the results
were not encouraging. Details of the experiments
carried out are explained in 4.2.
4 Experiments
Different experiments were carried out using LSA
and BV systems described in sections 3.1 and 3.2
on the dataset described in section 2. Pearson
correlation and Spearman?s rank correlation were
the metrics used to evaluate the performance of
the systems. Pearson correlation provides the de-
gree of similarity between the system?s score for
each pair and the gold standard?s score for the said
pair while Spearman?s rank correlation provides
the degree of similarity between the rankings of
the pairs according to similarity.
4.1 LSA
The LSA model was used to evaluate the semantic
similarity between P2S and S2P.
4.1.1 Paragraph to Sentence
An initial word-document matrix A was built by
extracting tokens just based on spaces, stop words
removed and tokens sorted in alphabetical order.
As described in 3.1.1, via the SVD of A, a ma-
trix U
k
is obtained which can be used to project an
m dimensional vector into a k dimensional one.
In one setting the paragraph and sentence vec-
tors which are projected into the LSA space have
unique word counts for their dimensions. In an-
other setting before projection, these vectors are
Dimensions 100% 90% 50% 30% 10%
Basic word-doc representation 0.499 - 0.494 0.484 0.426
Evaluation-boolean counts 0.548 - 0.533 0.511 0.420
Constrained tokenization 0.368 0.564 0.540 0.516 0.480
Added data 0.461 0.602 0.568 0.517 0.522
Table 2: Pearson scores at different dimensions -
Paragraph to Sentence
thresholded into ?boolean? versions, with 1 for ev-
ery non-zero count.
The Pearson scores for these settings are in the
first and second rows of table 2. They show the
variation with the number of dimensions of the
LSA representation (that is the number of columns
of U that are kept)
3
. An observation is that the
usage of boolean values instead of word counts
showed improved results.
Further experiments were conducted, retaining
the boolean treatment of the vectors to be pro-
jected. In a new setting, further improvements
were made to the pre-processing step, creating a
new word-document matrix A using constrained
tokenization rules, removing unnecessary spaces
and tabs, and tokens stemmed
4
. The performance
of the similarity calculation is shown as the third
row of Table 2: there is a trend of increase in cor-
relation scores with respect to the increase in di-
mensionality up to a maximum of 0.564, reached
at 90% dimension.
0 20 40 60 80 1000.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
Percent Dimensions maintained
Sem
anti
c si
mila
rity
 
 
Basic word?doc representationEvaluation with Boolean valuesConstrained TokenizationAdded data representation
Figure 1: Paragraph to Sentence - Pearson corre-
lation scores for four different experiments at dif-
ferent dimensions
3
(represented in percent) of U
k
Not convinced with the pearson scores, more
3
Here, the dimension X% means k = (X/100) ? N ,
whereN is the total number of columns in A in the unreduced
SVD.
4
Stemmed using Porter Stemmer module availabe from
http://tartarus.org/?martin/PorterStemmer/
621
documents were added to the dataset to build a
new word-document matrix representation A. The
documents included all the paragraphs from the
training set. Each paragraph provided in the train-
ing set was added to the dataset as a separate docu-
ment. The experiment was performed maintaining
the settings from the previous experiment and the
results are shown in the fourth row of table 2. The
increase in trend of correlation scores with respect
to the increase in dimensionality is followed by the
new U produced from A after applying SVD. Fig-
ure 2 provides the distribution of similarity scores
evaluated at 90% dimension of the model with re-
spect to the gold standard.
Further to compare the performance of different
experiments, all the experiment results are plotted
in Figure 1. It can be observed that every subse-
quent model built has shown improvements in per-
formance. The first two experiments shown in the
first two rows of table 2 are shown in red and blue
lines in the figure. It can be observed that in both
the settings, the pearson correlation scores were
increasing as the the number of dimensions main-
tained also increased, whereas in the other two set-
tings, the pearson correlation scores reached their
maximum at 90% and came down at 100% di-
mension, which is unexpected and so is not jus-
tified. It is observed from Figure 2 that the scores
0 100 200 300 400 5000
0.5
1
1.5
2
2.5
3
3.5
4
Training data Examples
Sim
ilari
ty s
core
s
Figure 2: Semantic similarity scores - Gold stan-
dard (Line plot) vs System scores (Scatter plot) for
examples in training data
of the system in scatter plot are not always clus-
tered around the gold standard scores, plotted as a
line. As the gold standard score goes up, the sys-
tem prediction accuracy has come down. One rea-
son for this pattern can be attributed to the train-
ing set which had data mostly data from Newswire
Dimensions 100% 90% 70% 50% 30% 10%
Basic word-doc
representation 0.493 - - 0.435 0.423 0.366
Evaluation
boolean counts 0.472 - - 0.449 0.430 0.363
Constrained
tokenization 0.498 0.494 0.517 0.485 0.470 0.434
Added
data 0.493 0.504 0.498 0.498 0.488 0.460
Table 3: Pearson scores at different dimensions
3
-
Sentence to Phrase
and webtext. Therefore, during evaluation all the
words from paragraph and/or sentence would not
have got a position while getting projected on the
latent semantic space, which we believe has pulled
down the accuracy.
4.1.2 Sentence to Phrase
The experiments carried out for P2S provided in
4.1.1 were conducted for S2P examples as well.
The pearson scores produced by different experi-
ments at different dimensions are provided in ta-
ble 3. This table shows that the latest word-
document representation made with added docu-
ments, did not have any impact on the correlation
scores, while the earlier word-document represen-
tation provided in 3
rd
row, which used the original
dataset preprocessed with constrained tokeniza-
tion rules, removing unnecessary spaces and tabs,
and tokens stemmed, provided better correlation
score at 70% dimension. Further the comparison
of different experiments carried out at different
settings are plotted in Figure 3.
0 20 40 60 80 1000.35
0.4
0.45
0.5
0.55
Percent Dimensions maintained
Sem
anti
c si
mila
rity
 
 
Basic word?doc representationEvaluation with Boolean valuesConstrained TokenizationAdded data representation
Figure 3: Sentence to Phrase - Pearson correlation
scores for four different experiments at different
dimensions
3
(represented in percentage) of U
k
622
4.2 Bag of Vectors
BV was tested in two different settings. The
first representation was created with bi-gram co-
occurance count as mentioned in section 3.2.1 and
experiments were carried out as mentioned in sec-
tion 3.2.2. This produced negative Pearson corre-
lation scores for P2S and S2P. Then we created an-
other representation by getting co-occurance count
in a window of 6 words in a sentence, on evalua-
tion produced correlation scores of 0.094 for P2S
and 0.145 for S2P. As BV showed strong negative
results, we did not continue using the method for
evaluating the test data. But we strongly believe
that the BV approach can produce better results if
we could compare the sentence to the paragraph
rather than the paragraph to the sentence as men-
tioned in section 3.2.2. During similarity calcula-
tion, when comparing sentence to the paragraph,
for each word in the sentence, we look for the best
semantic match from the paragraph, which would
increase the mean value by reducing the number of
divisions representing the number of words in the
sentence. In the current setting, it is believed that
while computing the similarity for the paragraph
to sentence, the words in the paragraph (longer
text) will consider a few words in the sentence to
be similar multiple times. This could not be right
when we compare the texts of varying lengths.
5 Conclusion and Discussion
On manual verification, it was identified that the
dataset used to build the representation did not
have documents related to the genres Metaphoric,
CQA and Travel. The original dataset mostly had
documents from Newswire text and blogs which
included reviews as well. Further, it can be identi-
fied from tables 2 and 3, the word-document rep-
resentation with added documents from the train-
ing set improved Pearson scores. This allowed to
assume that the dataset did not have completely
relevant set of documents to evaluate the training
set which included data from different genres. For
evaluation of the model on test data, we submitted
two runs and best of them reported Pearson score
of 0.607 and 0.552 on P2S and S2P respectively.
In the future work, we should be able to experi-
ment with more relevant data to build the model
using LSI and also use statistically strong unsu-
pervised classifier pLSI (Hofmann T, 2001) for the
same task. Further to this, as discussed in 4.2 we
would be able to experiment with the BV approach
by comparing the sentence to the paragraph, which
we believe will yield promising results to compare
the texts of varying lengths.
References
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer and Richard Harshman
1990. Indexing by latent semantic analysis Jour-
nal of the American society for information science,
41(6):391?401
Thomas Hofmann 2001. Unsupervised Learning
by Probabilistic Latent Semantic Analysis Journal
Machine Learning, Volume 42 Issue 1-2, January-
February 2001 Pages 177 - 196
623
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 48?53,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Measuring the compositionality of collocations via
word co-occurrence vectors: Shared task system description
Alfredo Maldonado-Guerra and Martin Emms
School of Computer Science and Statistics
Trinity College Dublin
Ireland
{maldonaa, mtemms}@scss.tcd.ie
Abstract
A description of a system for measuring the
compositionality of collocations within the
framework of the shared task of the Distribu-
tional Semantics and Compositionality work-
shop (DISCo 2011) is presented. The system
exploits the intuition that a highly composi-
tional collocation would tend to have a consid-
erable semantic overlap with its constituents
(headword and modifier) whereas a colloca-
tion with low compositionality would share
little semantic content with its constituents.
This intuition is operationalised via three con-
figurations that exploit cosine similarity mea-
sures to detect the semantic overlap between
the collocation and its constituents. The sys-
tem performs competitively in the task.
1 Introduction
Collocations or multiword expressions vary in the
degree to which a native speaker is able to under-
stand them based on the interaction of their con-
stituents? individual meanings. The concept of com-
positionality of a collocation captures this notion.
The shared task of the DISCo 2011 workshop (Bie-
mann and Giesbrecht, 2011) consists in comparing
systems? compositionality scores against composi-
tionality scores based on human judgements. Sys-
tems were evaluated on the match of the compo-
sitional scores generated by the system and those
based on human judgements ? specifically taking the
mean of the absolute difference of these scores. Ad-
ditionally the organisers also classified the human-
derived scores into three coarse categories of com-
positionality: non-compositional (low), somewhat
compositional (medium) and compositional (high).
Systems were required to produce an additional
compositionality labelling into these three coarse
categories and were evaluated on the precision of
this labelling.
The methods used by our system for measuring
compositionality take inspiration from the work of
McCarthy et al (2003), who measured the simi-
larity between a phrasal verb (a main verb and a
preposition like blow up) and its main verb (blow)
by comparing the words that are closely semanti-
cally related to each, and use this similarity as an
indicator of compositionality. Our method for mea-
suring compositionality is considerably different as
it instead directly compares the semantic similar-
ity between the headword and the collocation and
between the modifier and the collocation by com-
puting a cosine similarity score between word co-
occurrence vectors that represent the headword, the
modifier and the collocation (see 3.2). Our system
can be regarded as fully unsupervised as it does not
employ any parsers in its processing or any external
data other than the corpus and the collocation lists
provided by the organisers.
The rest of the paper is organised as follows: Sec-
tion 2 describes the corpora and the collocation list
provided by the task organisers. Section 3 intro-
duces some definitions and describes the three con-
figurations in detail. Section 4 presents the results
and concludes.
2 Data
Shared task participants were provided with a list of
collocations of three grammatical forms: adjective-
48
noun collocations (A-N), subject-verb collocations
(S-V) and verb-object collocations (V-O). Our sys-
tem assumes that each collocation consists of a
headword and a modifier and it interprets these con-
stituents in each grammatical form as follows: A-N:
adjective - modifier, noun - headword; S-V: subject
- modifier, verb - headword; V-O: verb - headword,
object - modifier.
As a corpus, our system uses a random sample of
500,000 documents from the plain-text, non-parsed
version of the English ukWaC corpus (Baroni et al,
2009).
3 System description
Our system can be employed in three different con-
figurations. All three rely in representing words
and collocations as word co-occurrence vectors and
measure semantic similarity using the cosine mea-
sure.
3.1 Preliminary definitions
These definitions are largely based on the con-
struction of first-order context vectors, word co-
occurrence vectors and second-order context vectors
via global selection as described in Sch?tze (1998)
and in Purandare and Pedersen (2004) by consider-
ing context windows of 20 words centred at a target
word.
The first-order context vector is a vector repre-
senting a token of a word, or equivalently a position
p in a document. Dimensions of the vector are word
types w, and the value on dimension w is a count
of the frequency with which w occurs in a specified
window around p in a given document doc.
C1(p)(w) = ?
p? 6=p
p?10?p?
p??p+10
(1 if w = doc(p?), else 0) (1)
In this work the dimensions are the 2,000 non-
function words that are most frequent in the corpus1.
The word co-occurrence vector (or simply word
vector) is a vector recording the co-occurrence be-
haviour of a particular word type w in a corpus. As
1We employ a modified version of the stop word
list supplied with Ted Pedersen?s Text-NSP package
(http://www.d.umn.edu/~tpederse/nsp.html)
such it can be defined by summation over first-order
context vectors:
W(w) =?
p
(1 if w = doc(p), else 0) ?C1(p) (2)
And the second-order context vector is a further
vector representing an instance of a word. For a par-
ticular location p, it is defined to be sum of the word
vectors of words in a given window around p
C2(p) = ?
p? 6=p
p?10?p?
p??p+10
W(doc(p)) (3)
Although the above are defined for types and to-
kens of words, they can be generalised to multiword
expressions in various ways. In this work, for any
multiword expression type x y, its tokens are taken
to be occurrences of the sequence x?y, where ? can
be any sequence of intervening words of length l,
0? l ? 3. By taking the position of x as the position
of the multiword token, and taking the first position
after the token as position p + 1, the definitions of
C1, W and C2 can be carried over to multiword ex-
pressions.
All the configurations described below use the co-
sine measure between vectors, defined in the stan-
dard way
cos(v,w) =
?Ni=1 viwi?
?Ni=1 v
2
i ?
N
i=1 w
2
i
(4)
3.2 System configurations
For each collocation in the test set, the first configu-
ration of our system starts off by building word vec-
tors for the collocation, its headword and its modi-
fier.
The first configuration of the system outputs the
average of two cosine similarity measures as the
compositionality score for the collocation:
c1 =
1
2
[
cos(W(x y) ,W(x))
+cos(W(x y) ,W(y))
]
(5)
where W(x y) is the word vector representing the
collocation whose constituents are x and y, and
W(x) and W(y) are the word vectors representing
each constituent x and y, respectively.
49
The second configuration of our system consid-
ers the occurrences of the headword when accompa-
nied by the modifier forming the collocation sepa-
rately from occurrences of the headword appearing
on its own and compares them. If y is the headword
of a collocation and coll(p) is a Boolean function
that determines whether the word at position p forms
a collocation with x, let
Wx(y) =?
p
(1 if
doc(p) = y
coll(p,x)
, else 0) ?C1(p) (6)
be the word vector computed from all the occur-
rences of the headword y that form a collocation
with x and conversely, let
Wx?(y) =?
p
(1 if
doc(p) = y
qcoll(p,x)
, else 0) ?C1(p) (7)
be the word vector representing the occurrences of y
not engaging in a collocation with x. In this configu-
ration, the compositionality score is then computed
by
c2 = cos
(
Wx (y) ,Wx? (y)
)
(8)
The intuition behind this configuration is that if
the headword tends to co-occur with more or less the
same words in both cases (producing a high cosine
score), then the meaning of the headword is simi-
lar regardless of whether the collocation?s modifier
is present or not, implying a high degree of com-
positionality. If on the other hand, the headword
co-occurs with somewhat differing words in the two
cases (a low cosine score), then we assume that the
presence of the collocation?s modifier is markedly
changing the meaning of the headword, implying a
low degree of compositionality.
In its third configuration, our system employs
clustering techniques in order to exploit semantic
differences that may naturally emerge from each
context in which the collocation and its constituents
are used. Different senses of a collocation might
have different compositionality measures as can be
seen in these two example sentences employing the
collocation great deal:
1. Two cans of soup for the price of one is such a
great deal!
C2(y) C2(y) C2(x)
C2(x y)C2(x)
C2(x y)
C2(y)
C2(y)
C2(x)C2(x y)
C2(x)
C2(x)
C2(y) C2(x y)
Figure 1: Example of a clustered second-order context
vector space.
2. The tsunami caused a great deal of damage to
the country?s infrastructure.
In Word Sense Induction, clustering is used to group
occurrences of a target word according to its sense or
usage in context (see e.g. Pedersen (2010)) as it is
expected that each cluster will represent a different
sense or usage of the target word. However, since
the contexts that human annotators referred to when
judging the compositionality of the collocations was
not provided, our system employs a workaround that
uses a weighted average when measuring composi-
tionality. This workaround is explained in what fol-
lows.
In this configuration, the system first builds word
vectors for the 20,000 most frequent words in the
corpus (equation 2), and then uses these to compute
the second-order context vectors for each occurrence
of the collocation and its constituents in the corpus
(equation 3). After context vectors for all occur-
rences have been computed, they are clustered using
CLUTO?s repeated bisections algorithm2. The vec-
tors are clustered across a small number K of clus-
ters (we employed K = 4). We expect that each clus-
ter will represent a different contextual usage of the
collocation, its headword and its modifier. Figure 1
depicts how a context vector space could be parti-
tioned with K = 4.
The system then for each cluster k builds the word
vectors (equation 2) Wk(x y), Wk(x), and Wk(y) for
the collocation, its headword and its modifier, from
the contexts grouped within the cluster k. The com-
positionality measure for the third configuration is
then basically a weighted average over the clusters
2http://glaros.dtc.umn.edu/gkhome/views/cluto/
50
of the c1 score using each cluster, that is:
c3 =
K
?
k=1
?k?
N
1
2
[
cos(Wk(x y),Wk(x))
+cos(Wk(x y),Wk(y))
]
(9)
where ?k? is the number of contexts in cluster k
and N is the total number of contexts across all clus-
ters.
For all three configurations, the value reported as
the numeric compositionality score was the corre-
sponding value obtained from equations (5), (8) or
(9), multiplied by 100. Each configuration?s nu-
meric scores ci were binned into the three coarse
compositionality classes by comparing them with
the configuration?s maximum value through equa-
tion (10).
coarse(ci) =
?
??
??
high if 23 max? ci
medium if 13 max < ci <
2
3 max
low if ci ? 13 max
(10)
4 Results and conclusion
Table 1 shows the evaluation results for the three
system configurations and two baselines. The left-
hand side of the table shows the average difference
between the gold-standard numeric score and each
configuration?s numeric score. The right-hand side
reports the precision on binning the numeric scores
into the coarse classes. Evaluation scores are re-
ported on all collocations and on the collocation sub-
types separately. Row R is the baseline suggested
by the workshop organisers, assigning random nu-
meric scores, in turn binned into the coarse cate-
gories. Row A shows the performance of a con-
stant output baseline, assigning all collocations the
mean gold-standard numeric score from the training
set: 66.45, and then applying the binning strategy
of equation (10) to this ? which always assigns the
coarse category high.
The first thing to note from this table is that con-
figurations 1 and 2 generally outperform configu-
ration 3, both on the mean difference and coarse
scores. Configuration 1 slightly outperforms con-
figuration 2 on the mean numeric difference scores,
whilst configuration 2 is very close to and slightly
C Average differences (numeric) Precision (coarse)
ALL A-N S-V V-O ALL A-N S-V V-O
1 17.95 18.56 20.80 15.58 53.4 63.5 19.2 62.5
2 18.35 19.62 20.20 15.73 54.2 63.5 19.2 65.0
3 25.59 24.16 32.04 23.73 44.9 40.4 42.3 52.5
R 32.82 34.57 29.83 32.34 29.7 28.8 30.0 30.8
A 16.86 17.73 15.54 16.52 58.5 65.4 34.6 65.0
Table 1: Evaluation results of the three system configura-
tions and two baselines on the test dataset. Best system
scores on each grammatical subtype highlighted in bold.
better than configuration 1 on the coarse precision
scores. The exception is that configuration 3 was the
best performer on the coarse precision scoring for
the S-V subtype.
The R baseline is outperformed by configurations
1, 2 and 3; roughly speaking where 1 and 2 out-
perform R by d, configuration 3 outperforms R by
around d/2. The A baseline generally outperforms
all our system configurations. It seems to be also a
quite competitive baseline for other systems partici-
pating in the shared task.
The other trend apparent from the table is that per-
formance on the V-O and A-N subtypes tends to ex-
ceed that on the the S-V subtype.
An examination of the gold standard test
files shows that the distribution over the
low/medium/high categories is similar for both
V-O and A-N, in both cases close to 0.08/0.27/0.65,
with high covering nearly two-thirds of cases,
whilst for S-V the distribution is quite different:
0.0/0.654/0.346, with medium covering nearly
two-thirds of cases. This is reflected in the A
baseline precision scores, as for each subtype these
will necessarily be the proportion of gold-standard
high cases. This explains for example why the A
baseline is much poorer on the S-V cases (34.6)
than on the other cases (65.0, 65.4).
Looking further into the differences between the
three subtypes, Figure 2 shows the gold standard nu-
meric score distribution across the three collocation
subtypes (Test GS), and the corresponding distribu-
tions for scores from the system?s first configuration
(Conf 1). This shows in more detail the nature of
the poorer performance on S-V, with the gold stan-
dard having a peak around 50-60, and the system
having a peak around 70-80. For the other subtypes
51
Perc
ent 
of To
tal
0
10
20
30
0 20 40 60 80 100
A?NConf 1 S?VConf 1
0 20 40 60 80 100
V?OConf 1
A?NTest GS
0 20 40 60 80 100
S?VTest GS
0
10
20
30
V?OTest GS
Figure 2: The distribution of the gold standard numeric
score vs. the distribution of the system?s first configura-
tion numeric scores.
A-N S-V V-O
Instances 177254 11092 121317
Avg intervening 0.0684 0.3867 0.4612
Table 2: Some corpus statistics: the number of matched
collocations per subtype (Instances) and the average
number of intervening words per subtype (Avg interven-
ing).
the contrast in the distributions seems broadly con-
sistent with the mean numeric difference scores of
Table 1.
One can speculate on the reasons for the system?s
poorer performance on the S-V subtype. The sys-
tem treats intervening words in a collocation in a
particular way, namely by ignoring them. This is
one option, and another would be to include them as
features counted in the vectors. Table 2 shows the
average intervening words in the occurrences of the
collocations. S-V and V-O are alike in this respect,
both being much more likely to present intervening
words than collocations of the A-N subtype. So the
explanation of the poorer performance on S-V can-
not lie there. Also because the average number of
intervening words is low, we believe it is unlikely
that including them as features will impact perfor-
mance significantly.
Table 2 also gives the number of matched collo-
cations per subtype. The number for the S-V collo-
cations is an order of magnitude smaller than for the
other subtypes. Although the collocations supplied
by the organisers are in their base form, the system
attempts to match them ?as is? in the unlemmatised
version of the corpus. Whilst for A-N and V-O the
base-form sequences relatively frequently do double
service as inflected forms, this is far less frequently
the case for the S-V sequences (e.g. user see (S-
V) is far less common than make money (V-O) ).
This much smaller number of occurrences for S-V
cases, or the fact that they are drawn from syntac-
tically special contexts, may be a factor in the rel-
atively poorer performance. This perhaps is also a
factor in the earlier noted fact that although config-
uration 3 was generally outperformed, on the S-V
subtype the reverse occurs.
The unlemmatised version of the corpus was used
because initial experimentation with the validation
set produced slightly better results when employing
raw words as features rather than lemmas. A possi-
bility for future work would be to to refer to lemmas
for matching collocations in the corpus, but to con-
tinue to use unlemmatised words as features.
Other areas for future investigation involve the ef-
fects of weighting schemes (such as IDF) and the
use of similarity measures other than cosine, as
well as alternatives in configurations 2 and 3. For
example, configuration 2 could involve the modifier
in the computation of the compositionality score,
and configuration 3 could create separate clustering
spaces for collocation, headword and modifier and
compute similarity scores based on vectors represen-
ting these clusters.
In sum, the simplest configuration of a totally un-
supervised system yielded surprisingly good results
at measuring compositionality of collocations in raw
corpora, and whereas there is scope for further de-
velopment and refinement, the system as it is consti-
tutes a robust baseline to compare against more ela-
borate systems.
5 Acknowledgements
We would like to thank our anonymous reviewers for
their insightful comments and ideas. This research is
supported by the Science Foundation Ireland (Grant
07/CE/I1142) as part of the Centre for Next Gene-
ration Localisation (www.cngl.ie) at Trinity College
Dublin.
52
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226, February.
Chris Biemann and Eugenie Giesbrecht. 2011. Distribu-
tional Semantics and Compositionality 2011: Shared
Task Description and Results. In Proceedings of the
Distributional Semantics and Compositionality work-
shop (DISCo 2011) in conjunction with ACL 2011,
Portland, Oregon.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phra-
sal verbs. In Proceedings of the ACL 2003 workshop
on Multiword expressions: analysis, acquisition and
treatment-Volume 18, pages 73?80, Sapporo. Associa-
tion for Computational Linguistics.
Ted Pedersen. 2010. Duluth-WSI: SenseClusters applied
to the sense induction task of SemEval-2. In Procee-
dings of the 5th International Workshop on Seman-
tic Evaluation, number July, pages 363?366, Uppsala,
Sweden. Association for Computational Linguistics.
Amruta Purandare and Ted Pedersen. 2004. Word
sense discrimination by clustering contexts in vector
and similarity spaces. Proceedings of the Conference
on Computational Natural Language Learning, pages
41?48.
Hinrich Sch?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
53
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 89?93,
Gothenburg, Sweden, 26-27 April 2014. c?2014 Association for Computational Linguistics
Detecting change and emergence for multiword expressions
Martin Emms
Department of Computer Science
Trinity College, Dublin
Ireland
Martin.Emms@tcd.ie
Arun Jayapal
Department of Computer Science
Trinity College, Dublin
Ireland
jayapala@tcd.ie
Abstract
This work looks at a temporal aspect of
multiword expressions (MWEs), namely
that the behaviour of a given n-gram and
its status as a MWE change over time. We
propose a model in which context words
have particular probabilities given a us-
age choice for an n-gram, and those us-
age choices have time dependent probabil-
ities, and we put forward an expectation-
maximisation technique for estimating the
parameters from data with no annotation
of usage choice. For a range of MWE
usages of recent coinage, we evaluate
whether the technique is able to detect the
emerging usage.
1 Introduction
When an n-gram is designated a ?multiword ex-
pression?, or MWE, its because it possesses prop-
erties which are not straightforwardly predictable
given the component words of the n-gram ? that
red tape can refer to bureaucratic regulation would
be a simple example. A further aspect is that while
some tokens of the n-gram type may be examples
of the irregular MWE usage, others may not be ?
so red tape can certainly also be used in a fash-
ion which is transparent relative to its parts. A
further aspect is temporal: that tokens of the n-
gram can be sought in language samples from dif-
ferent times. It seems reasonable to assume that
the irregular MWE usage of red tape at some time
emerged, and was predated by the more transpar-
ent usage. This paper concerns the possibility of
finding automatic, unsupervised means to detect
the emergence of a MWE usage of a given n-gram.
To illustrate further, consider the following ex-
amples (these are all taken from the data set on
which we worked)
(a) the wind lifted his three-car garage and
smashed it to the ground. (1995)
(a?) sensational group CEO, totally smashed
it in the BGT (Britain Got Talent) (2013)
(b) my schedule gave me time to get ad-
justed (1990)
(b?) it?s important to set time out and enjoy
some me time (2013)
(1)
(a) and (a?) feature the n-gram smashed it. (a)
uses the standard destructive sense of smashed,
and it refers to an object undergoing the destruc-
tive transformation. In (a?) the n-gram is used dif-
ferently and is roughly replaceable by ?excelled?,
a usage not via the standard sense of smashed, nor
one where it refers to any object at all. Where in
both (a) and (a?) the n-gram would be regarded as
a phrase, (b) and (b?) involving the n-gram me time
show another possibility. In (b), me and time are
straightforward dependants of gave. In (b?), the
two words form a noun-phrase, meaning some-
thing like ?personal time?. The usage is arguably
more acceptable than would be the case with other
object pronouns, and if addressed to a particular
person, the me would refer to the addressee, which
is not the usual function of a first-person pronoun.
For smashed it and me time, the second (primed)
example illustrates an irregular usage-variant of
the n-gram, whilst the first illustrates a regular
usage-variant, and the irregular example is drawn
from a later time than the regular usage. Lan-
guage is a dynamic phenomenon, with the range
of ways a given n-gram might contribute subject
to change over time, and for these n-grams, it
would seem to be the case that the availability
of the ?me time? = ?personal time? and ?smashed
it = ?excelled? usage-variants is a relatively re-
cent innovation1 , predated by the regular usage-
variants. It seems that in work on multiword ex-
1That is to say, recent in British English according to the
89
pressions, there has been little attention paid to
this dynamic aspect, whereby a particular multi-
word usage starts to play a role in a language at a
particular point in time. Building on earlier work
(Emms, 2013), we present some work concerning
unsupervised means to detect this. Section 2 de-
scribes our data, section 3 our EM-based method
and section 4 discusses the results obtained.
2 Data
To investigate such emergence phenomena some
kind of time-stamped corpus is required. The ap-
proach we took to this was to exploit a search fa-
cility that Google has offered for some time ? cus-
tom date range ? whereby it is possible to specify
a time period for text matching the searched item.
To obtain data for a given n-gram, we repeatedly
set different year-long time spans and saved the
first 100 returned ?hits? as potential examples of
the n-gram?s use. Each ?hit? has a text snippet and
an anchor text for a link to the online source from
which the snippet comes. If the text snippet or an-
chor string contains the n-gram it can furnish an
example of its use, and the longer of the two is
taken if both feature the n-gram.
A number of n-grams were chosen having the
properties that they have an irregular, MWE usage
alongside a regular one, with the MWE usage a re-
cent innovation. These were smashed it, me time
(illustrated in (1)) and going forward, and biolog-
ical clock, illustrated below.
(c) Going forward from the entrance,
you?ll come to a large room. (1995)
(c?) Going forward BJP should engage in
people?s movements (2009)
(d) A biological clock present in most eu-
karyotes imposes daily rhythms (1995)
(d?) How To Stop Worrying About Your Bi-
ological Clock . . . Pressure to have a
baby before 35 (2009)
(2)
Alongside the plain movement usage-variant seen
in (c), going forward has the more opaque usage-
variant in which it is roughly replaceable by ?in
the future?, seen in (c?). Alongside a technical use
in biology seen in (d), biological clock has come
to be used in a wider context to refer to a sense of
expiring time within which people may be able to
have a child, seen in (d?).
first author?s intuitions. It is not easy to find sources to cor-
roborate such intuitions
For each n-gram data was downloaded for suc-
cessive year-long time-spans from 1990 to 2013,
retaining the first 100 hits for each year. For some
of the earlier years there are less than 100 hits, but
mostly there are more than 100. This gives on the
order of 2000 examples for each n-gram, each with
a date stamp, but otherwise with no other annota-
tion. See Section 4 for some discussion of this
method of obtaining data.
3 Algorithm
For an n-gram with usage variants (as illustrated
by (1) and (2)), we take the Bayesian approach
that each variant gives different probabilities to
the words in its immediate vicinity, as has been
done in unsupervised word-sense disambiguation
(Manning and Schu?tze, 2003; de Marneffe and
Dupont, 2004). In those approaches, which ignore
any temporal dimension, it is also assumed that
there are prior probabilities on the usage-variants.
We bring in language change by having a succes-
sion of priors, one for each time period.
To make this more precise, where T is an oc-
currence of a particular n-gram, with W the se-
quence of words around T , let Y represent its
time-stamp. If we suppose there are k different
usage-variants of the n-gram, we simply model
this with a discrete variable S which can take on k
values. So S can be thought of as ranging over po-
sitions in an enumeration of the different ways that
the n-gram can contribute to the semantics. With
these variables we can say that we are consider-
ing a probability model for p(Y, S,W ). Apply-
ing the chain-rule this may be re-expresssed with-
out loss of generality as p(Y )p(S|Y )p(W |S, Y ).
We then make some assumptions: (i) that W
is conditionally independent of Y given S, so
p(W |S, Y ) = p(W |S), (ii) that p(W |S) may be
treated as
?
i
(p(W
i
|S) , and (iii) that p(Y ) is uni-
form. This then gives
p(Y, S,W ) = p(Y )p(S|Y )
?
i
(p(W
i
|S) (3)
The term p(S|Y ) directly models the fact that a
usage variant can vary its likelihood over time,
possibly having zero probability on some early
range of times. While (i) make context words
and times indepedent given a usage variant, con-
text words are still time-dependent: the sum
?
S
[p(S|Y )p(W |S)] varies with time Y due to
90
p(S|Y ). Assumption (i) reflects a plausibile idea
that given a concept being conveyed, the expected
accompanying vocabulary is substantially time-
independent. Moreover (i) drastically reduces the
number of parameters to be estimated: with 20
time spans and a 2-way usage choice, the word
probabilities are conditioned on 2 settings rather
than 40.
The parameters of the model in (3) have to be
estimated from data which is labelled only for
time ? the usage-variant variable is a hidden vari-
able ? and we tackle this with an EM procedure
(Dempster et al., 1977). Space precludes giving
the derivations of the update formulae but in out-
line there is an iteration of an E and an M step, as
follows:
(E step) based on current parameters, a table, ?,
is populated, such that for each data point d, and
possible S value s, ?[d][s] stores P (S = s|Y =
y
d
,W = w
d
).
(M step) based on ?, fresh parameter values are
re-estimated according to:
P (S = s|Y = y) =
?
d
(if Y d=y then ?[d][s] else 0)
?
d
(if Y d=y then 1 else 0)
P (w|S = s) =
?
d
(?[d][s]?freq(w?W d))
?
d
(?[d][s]?length(W d))
These updates can be shown to increase the
data probability, where the usage variable S is
summed-out.
4 Results and Discussion
Running the above-outlined EM procedure on the
downloaded data for a particular n-gram gener-
ates unsupervised estimates for p(S|Y ) ? inferred
usage distributions for each time span. To ob-
tain a reference with which to compare these in-
ferred distributions, approximately 10% of the
data per time-span was manually annotated and
used to give simple relative-frequency estimates of
p(S|Y ) ? which we will call empirical estimates.
Although the data was downloaded for year-long
time spans, it was decided to group the data into
successive spans of 3 year duration. This was to
make the empirical p(S|Y ) less brittle as they are
otherwise based on too small a quantity of data.
Figure 1 shows the outcomes, as usage-variant
probabilities in a succession of time spans, both
the empirical estimates obtained on a subset, and
the unsupervised estimates obtained on all the
data. The EM method can seek any number
of usage variants, and the results show the case
where 2 variants were sought. Where the man-
ually annotated subset used more variants these
were grouped to facilitate a comparison.
For smashed it, biological clock and going for-
ward, the ? line in the empirical plot is for the
MWE usage, and for me time it is the ? line, and
it has an upward trend. In the unsupervised case,
there is inevitable indeterminacy about which S
values may come to be associated with any objec-
tively real usage. Modulo this the unsupervised
and supervised graphs broadly concur.
One can also inspect the context-words which
come to have high probability in one semantic
variant relative to their probability in another. For
example, for smashed it, for the semantic usage
which is inferred to have an increasing proba-
bility in recent years, a selection from the most
favoured tokens includes !!, guys, really, com-
pletely, They, !, whilst for the other usage they in-
clude smithereens, bits, bottle, onto, phone. For
biological clock, a similar exercise gives for the
apparently increasing usage, tokens such as Ticks,
Ticking?, Health, Fertility and for the other usage
running, 24-hour, controlled, mammalian, mecha-
nisms. These associations would seem to be con-
sistent with the inferred semantic-usages being in
broad correspondence with the annotated usages.
As noted in section 2, as a means to obtain
data on relatively recent n-gram usages, we used
the custom date range search facility of Google.
One of the issues with such data is the potential
for the time-stamping (inferred by Google) to be
innaccurate. Though its not possible to exhaus-
tively verify the time-stamping, some inspection
was done, which revealed that although there are
some cases of documents which were incorrectly
stamped, this was tolerably infrequent. Then there
is the question of the representativeness of the
sample obtained. The mechanism we used gives
the first 100 from the at most 1000 ?hits? which
Google will return from amongst all index docu-
ments which match the n-gram and the date range,
so an uncontrollable factor is the ranking mech-
anism according to which these hits are selected
and ordered. The fact that the empirical usage
distributions accord reasonably well with prior in-
tuition is a modest indicator that the data is not
unusably unrepresentative. One could also argue
that for an initial test of the algorithms it suf-
fices for the methods to recover an apparent trend
91
smashed it (empirical and unsupervised)
2000 2005 2010
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0 smashed it
Year
Se
ns
e 
Pr
op
2000 2004 2008 2012
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Year
Se
ns
e 
Pr
op
biological clock (empirical and unsupervised)
1995 2005
0.
0
0.
4
0.
8
?
Year
Se
ns
e 
Pr
op
1995 2005
0.
0
0.
4
0.
8
bio clock
Year
Se
ns
e 
Pr
op
me time (empirical and unsupervised)
1995 2005
0.
0
0.
4
0.
8
me time
Year
Se
ns
e 
Pr
op
1995 2005
0.
0
0.
4
0.
8
me time
Year
Se
ns
e 
Pr
op
going forward (empirical and unsupervised)
1995 2005
0.
0
0.
4
0.
8
going forward
Year
Se
ns
e 
Pr
op
1995 2005
0.
0
0.
4
0.
8
going forward
Year
Se
ns
e 
Pr
op
Figure 1: For each n-gram the plots show the empirical usage-variant distributions per time-period in the
labelled subset and unsupervised usage-varaint distributions per time-period in the entire data set
in the downloaded data, even if the data is un-
representative. This being said, one direction for
further work will be to consider other sources of
time-stamped language use, such as the Google n-
grams corpus (Brants and Franz, 2012), or various
newswire corpora (Graff et al., 2007).
There does not seem to have been that much
work on unsupervised means to identify emer-
gence of new usage of a given expression ? there
is more work which groups all tokens of a type
together and uses change of context words to indi-
cate an evolving single meaning (Sagi et al., 2008;
Gulordava and Baroni, 2011). Lau et al. (2012)
though they do not address MWEs do look at the
emergence of new word senses, applying a word-
sense induction technique. Their testing was be-
tween two corpora taken to represent two different
time periods, the BNC and ukWac corpus, taken
to represent the late 20th century and 2007, re-
spectively, and they reported promising results on
5 words. The unsupervised method they used is
based on a Hierarchical Dirichlet Process model
(Yao and Van Durme, 2011), and a direction for
future work will be a closer comparison of the al-
gorithm presented here to that algorithm and other
related LDA-based methods in word sense induc-
tion (Brody and Lapata, 2009). Also the bag-
of-tokens model of the context words which we
adopted is a very simple one, and we wish to con-
sider more sophisticated models involving for ex-
ample part-of-tagging or syntactic structures.
The results are indicative at least that MWE
usage of an n-gram can be detected by unsuper-
vised means to be preceded by the other usages of
the n-gram. There has been some work on algo-
rithms which seek to quantify the degree of com-
positionality of particular n-grams (Maldonado-
Guerra and Emms, 2011; Biemann and Gies-
brecht, 2011) and it is hoped in future work to
consider the possible integration of some of these
techniques with those reported here. For a given
n-gram, it would be interesting to know if the col-
lection of its occurrences which the techniques of
the current paper suggest to belong to a more re-
cently emerging usage, are also a corpus of occur-
rences relative to which a compositionality mea-
sure would report the n-gram as being of low com-
positionality, and conversely for the apparently
less recent usage.
Acknowledgements
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the Centre for Next Generation Localisation
(www.cngl.ie) at Trinity College Dublin.
92
References
Chris Biemann and Eugenie Giesbrecht, editors. 2011.
Proceedings of the Workshop on Distributional Se-
mantics and Compositionality.
Thorsten Brants and Alex Franz. 2012. Google books
n-grams. ngrams.googlelabs.com.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In EACL 09: Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 103?111, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Marie-Catharine de Marneffe and Pierre Dupont. 2004.
Comparative study of statistical word sense discrim-
ination. In Ge?rald Purnelle, Ce?dric Fairon, and
Anne Dister, editors, Proceedings of JADT 2004 7th
International Conference on the Statistical Analysis
of Textual Data, pages 270?281. UCL Presses Uni-
versitaire de Louvain.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. J. Royal Statistical Society, B 39:1?
38.
Martin Emms. 2013. Dynamic EM in neologism evo-
lution. In Hujun Yin, Ke Tang, Yang Gao, Frank
Klawonn, Minho Lee, Thomas Weise, Bin Li, and
Xin Yao, editors, Proceedings of IDEAL 2013, vol-
ume 8206 of Lecture Notes in Computer Science,
pages 286?293. Springer.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English gigaword corpus. Linguis-
tic Data Consortium.
Kristina Gulordava and Marco Baroni. 2011. A distri-
butional similarity approach to the detection of se-
mantic change in the google books ngram corpus. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 67?71, Edinburgh, UK, July. Association for
Computational Linguistics.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense in-
duction for novel sense detection. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, EACL
?12, pages 591?601, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Alfredo Maldonado-Guerra and Martin Emms. 2011.
Measuring the compositionality of collocations via
word co-occurrence vectors: Shared task system de-
scription. In Proceedings of the Workshop on Dis-
tributional Semantics and Compositionality, pages
48?53, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Christopher Manning and Hinrich Schu?tze, 2003.
Foundations of Statistical Language Processing,
chapter Word Sense Disambiguation, pages 229?
264. MIT Press, 6 edition.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2008.
Tracing semantic change with latent semantic anal-
ysis. In Proceedings of ICEHL 2008.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods
for Natural Language Processing, pages 10?14. As-
sociation for Computational Linguistics.
93
