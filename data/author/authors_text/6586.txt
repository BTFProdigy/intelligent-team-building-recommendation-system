Performance Evaluation and Error Analysis for Multimodal Reference 
Resolution in a Conversation System 
Joyce Y. Chai          Zahar Prasov              Pengyu Hong 
Department of Computer Science and Engineering
Michigan State University  
East Lansing, MI 48864 
jchai@cse.msu.edu,  prasovza@cse.msu.edu 
Department of Statistics 
Harvard University 
Cambridge, MA 02138 
hong@stat.harvard.edu 
 
Abstract 
Multimodal reference resolution is a process 
that automatically identifies what users refer 
to during multimodal human-machine 
conversation. Given the substantial work on 
multimodal reference resolution; it is important 
to evaluate the current state of the art, 
understand the limitations, and identify 
directions for future improvement. We 
conducted a series of user studies to evaluate the 
capability of reference resolution in a 
multimodal conversation system.  This paper 
analyzes the main error sources during real-time 
human-machine interaction and presents key 
strategies for designing robust multimodal 
reference resolution algorithms. 
1 Introduction* 
Multimodal systems enable users to interact with 
computers through multiple modalities such as speech, 
gesture, and gaze (Bolt 1980; Cassell et al, 1999; Cohen et 
al., 1996; Chai et al, 2002; Johnston et al, 2002). One 
important aspect of building multimodal systems is for the 
system to understand the meanings of multimodal user 
inputs. A key element of this understanding process is 
reference resolution. Reference resolution is a process that 
finds the most proper referents to referring expressions. To 
resolve multimodal references, many approaches have 
been developed, from the use of a focus space model (Neal 
et al, 1998), a centering framework (Zancanaro et al 
1997), contextual factors (Huls et al, 1995); to recent 
approaches using unification (Johnston, 1998), finite state 
machines (Johnston and Bangalore 2000), and context-
based rules (Kehler 2000).   
Given the substantial work in this area; it is important 
to evaluate the state of the art, understand the limitations, 
                                                                
* This work was supported by grant IIS-0347548 from the 
National Science Foundation and grant IRGP-03-42111 from 
Michigan State University. 
 
and identify directions for future improvement. We 
conducted a series of user studies to evaluate the capability 
of reference resolution in a multimodal conversation 
system. In particular, this paper examines two important 
aspects: (1) algorithm requirements for handling a variety 
of references, and (2) technology requirements for 
achieving good real-time performance. In the following 
sections, we first give a brief description of our system. 
Then we analyze the main error sources during real-time 
human-machine interaction and discuss the key strategies 
for designing robust reference resolution algorithms. 
2 System Description  
We implemented a multimodal conversation system to 
study multimodal user referring behavior and to evaluate 
reference resolution algorithms. Users can use both speech 
and manual gestures (e.g., point and circle) to interact with 
a map-based graphic interface to find information about 
real estate properties.  
As shown in Figure 1, our system applies a semantic 
fusion approach that combines the semantic information 
identified from each modality. A key characteristic of the 
system is that, in addition to fusing information from 
different modalities, our system systematically 
incorporates information from the conversation context 
(e.g., the focus of attention from prior conversation), the 
Speech Input Gesture Input
Speech 
Recognizer 
NL
Parser 
Gesture
Recognizer
Multimodal Interpreter
(Graph-based Reference 
Resolution Component)
Conversation
Manager
Presentation
Manager
Conversation 
Context
Visual 
Context
Domain
Context
Multimedia Output  
Figure 1:  Overview of the system 
visual context (e.g., objects on the screen that are in the 
visual focus), and the domain context (i.e., the domain 
k
m
g
re
sp
re
e
in
re
re
g
o
h
c
p
is
a
G
p
R
re
m
d
3
W
p
g
th
sy
a
th
voice from each subject was trained individually to 
minimize speech recognition errors.  
e 
s 
t 
e 
e 
 
f 
, 
e 
 
 
e 
 
. 
s 
e 
 nowledge).   
The reference resolution approach is based on a graph-
atching algorithm. Specifically, two attribute relational 
raphs are used (Tsai and Fu, 1979). One graph is called 
ferring graph that captures referring expressions from 
eech utterances. Each node, corresponding to one 
ferring expression, consists of the semantic information 
xtracted from the expression and the temporal 
formation when the expression is uttered.  Each edge 
presents the semantic and temporal relation between two 
ferring expressions. The second graph is called referent 
raph that represents all potential referents (including 
bjects selected by the gesture, objects in the conversation 
istory, and objects in the visual focus). Each node 
aptures the semantic and temporal information about a 
otential referent (e.g., the time when the potential referent 
 selected by a gesture). Each edge captures the semantic 
nd temporal relations between two potential referents. 
iven these graph representations, the reference resolution 
roblem becomes a graph-matching problem (Gold and 
angarajan, 1996). The goal is to find a match between the 
3.1 Performance Evaluation 
Table 1 summarizes the referring behavior observed in th
studies and the performance of the system. The column
indicate whether there was no gesture, one gesture (poin
or circle), or multiple gestures involved in the input. Th
rows indicate the type of referring expressions in th
speech utterances. Each table entry shows the system
performance on resolving a particular combination o
speech and gesture inputs. For example, the entry at <S2
G4> indicates that 35 inputs consist of demonstrativ
singular noun phrases (as the referring expressions) and a
single circle gesture. Out of these inputs, 27 were correctly
recognized and eight were incorrectly recognized by th
speech recognizer. Out of the 27 correctly recognized
inputs, 26 were correctly assigned referents by the system
Out of the eight incorrectly recognized inputs, reference
in two inputs were correctly resolved.  
Consistent with earlier findings (Kehler 2000), th
majority of user references were simple which onlyferring graph and the referent graph that achieves the 
aximum compatibility between the two graphs.  The 
etails of this approach are described in (Chai et al, 2004). 
 Performance Evaluation and Analysis 
e conducted several user studies to evaluate the 
erformance of real time reference resolution using the 
raph-based approach. Eleven subjects participated in 
ese studies. Each of them was asked to interact with the 
stem using both speech and gestures (point and circle) to 
ccomplish five tasks. For example, one task was to find 
e least expensive house in the most populated town. The 
involved one referring expression and one gesture as 
shown in Table 1 (i.e., S1 to S8, with column G2 and G4). 
However, we have also found that 14% (31/219) of the 
inputs were complex, which involved multiple referring 
expressions from speech utterances (see the row S9). Some 
of these inputs did not have any accompanied gesture (e.g., 
<S9, G1>). Some were accompanied by one gesture (e.g., 
<S9, G4>) or multiple gestures (e.g., <S9, G3> and <S9, 
G5>). The referents to these referring expressions could 
come from user?s gestures, or from the conversation 
context, or from the graphic display. To resolve these types 
of references, the graph-based approach is effective by 
simultaneously considering the semantic, temporal, and 
1(1), 1(0)001(1), 1(0)000S5:(these|those)(num)*(adj)*(ones)*|them
129(111), 90(26)
15(9), 16(1)
2(0) 12(4)
4(2), 0(0)
7(7), 2(1)
7(4), 12(5)
22(21), 14(3)
64(61), 27(11)
7(6), 6(1)
Total 
Num
4(4), 11(0)9(7), 5(0)63(54), 36(8)7(4), 3(2)39(37),29(14)7(5), 6(2)Total Num
0(0), 3(0)8(6), 5(0)3(1), 7(1)4(2), 0(0)00(0), 1(0)S9: multiple expressions
0(0), 3(0)00(0), 3(0)0(0), 3(2)1(0), 3(2)1(0), 0(0)S8: proper nouns
002(0), 0(0)01(1), 0(0)1(1), 0(0)S7: empty expression
006(6), 0(0)01(1), 1(0)0(0), 1(1)S6: here|there
004(2), 4(1)02(2), 6(3)1(0), 2(1)S4: it|this|that| (this|that|the)(adj)*one
3(3), 2(0)019(18), 12(3)000S3: (these|those)(num)*(adj)*Ns
1(1), 2(0)1(1), 0(0)27(26), 8(2)3(2), 0(0)29(28), 16(9)3(3), 1(0)S2: (this|that) (adj*) N
0(0), 1(0)01(0), 1(1)05(5), 3(0)1(1), 1(0)S1: the (adj)*(N | Ns)
G6
Points and 
Circles
G5
Multiple 
Circles
G4
One Circle
G3
Multiple 
Points
G2
One Point
G1
No 
Gesture
Table 1: Performance evaluation of the graph-matching approach to multimodal reference resolution. In each entry form
?a(b), c(d)?,  ?a? indicates the number of inputs in which the referring expressions were correctly recognized by the speech
recognizer; ?b? indicates the number of inputs in which the referring expressions were  correctly recognized and were cor-
rectly resolved; ?c? indicates the number of inputs in which the referring expressions were not correctly recognized; ?d?
indicates the number of inputs in which the referring expressions were not correctly recognized, but were correctly resolved.
The sum of ?a? and ?c? gives the total number of inputs with a particular combination of speech and gesture. 
 
contextual constraints.   
3.2 Error Analysis 
As shown in Table 1, out of the total 219 inputs, 137 inputs 
had their referents correctly identified (A complex input 
with multiple referring expressions was considered 
correctly resolved only if the referents to all the referring 
expressions were correctly identified). For the remaining 
82 inputs in which the referents were not correctly 
identified, the errors mainly came from five sources as 
summarized in Table 2.  
A poor performance in speech recognition is a major 
error source. Although we have trained each user?s voice 
individually, the speech recognition rate is still very low. 
Only 59% (129/219) of inputs had correctly recognized 
referring expressions. This is partly due to the fact that 
more than half of our subjects are non-native speakers.  
Fusing inputs from multiple modalities together can 
sometimes compensate for the recognition errors (Oviatt 
1996). Among 90 inputs in which referring expressions 
were incorrectly recognized, 26 of them were correctly 
assigned referents due to the mutual disambiguation. 
However, poor speech recognition still accounted for 55% 
of the total errors. A mechanism to reduce the recognition 
errors, especially by utilizing information from other 
modalities will be important to provide a robust solution 
for real time multimodal reference resolution.  
The second source of errors (20% of the total errors) 
came from insufficient language understanding, especially 
the out-of-vocabularies. For example, ?area? was not in 
our vocabulary. So the additional semantic constraint 
expressed by ?area? was not captured. Therefore, the 
system could not identify whether a house or a town was 
referred when the user uttered ?this area?. It is important 
for the system to have a capability of acquire knowledge 
(e.g., vocabulary) dynamically by utilizing information 
from other modalities and the interaction context. 
Furthermore, the errors also came from a lack of 
understanding of spatial relations (as in ?the house just 
close to the red one?) and superlatives (as in ?the most 
expensive house?). Algorithms to align visual features to 
resolve spatial references as described in (Gorniak and Roy 
2003) are desirable.  
Among all errors, 13% came from unsynchronized 
inputs. Currently, we use an idle status (i.e., 2 seconds with 
no input from either speech or gesture) as the boundary to 
delimit an interaction turn. There are two types of out of 
synchronization. The first type is unsynchronized inputs 
from the user (such as a big pause between speech and 
gesture) and the other comes from the underlying system 
implementation.  The system captures speech inputs and 
gesture inputs from two different servers through TCP/IP 
protocol. A communication delay sometimes split one 
synchronized input into two separate turns of inputs (i.e., 
one turn was speech input alone and the other turn was 
gesture input alone). A better engineering mechanism to 
synchronize inputs is desired. 
The disfluencies from the users also accounted for 
about 7% of the total errors. Recent findings indicated that 
gesture patterns could be used as an additional source to 
identify different types of speech disfluencies during 
human-human conversation (Chen et al, 2002). As 
expected, speech disfluencies did not occur that much in 
our studies. Based on our limited cases, we found that 
gesture patterns could be indicators of speech disfluencies 
when they did occur. For example, if a user says ?show me 
the red house (point to house A), the green house (still 
point to the house A)?, then the behavior of pointing to the 
same house with different speech description usually 
indicates a repair. Furthermore, gestures also involve 
disfluencies, for example, repeatedly pointing to an object 
is a gesture repetition. Failure in identifying these 
disfluencies caused problems with reference resolution. It 
is important to have a mechanism that can identify these 
disfluencies using multimodal information.  
The remaining 5% errors came from the 
implementation of our approach in order to reduce the 
complexity of graph matching.  Currently, the referent 
graph only consists of potential referents from gestures, 
objects from the prior conversation, and the objects in the 
visual focus (i.e., highlighted on the screen). Therefore, it 
is insufficient to handle cases where users only use proper 
names (without any gestures) to refer to objects visible on 
the screen.  
From the error analysis, we learned that variations in 
user inputs (e.g., variations in vocabulary and 
synchronization patterns), disfluencies in speech utterances, 
and even small changes in the input quality or the 
environment could seriously impair the real-time 
performance. The future research effort should be devoted 
to developing adaptive approaches for reference resolution 
to deal with unexpected inputs (e.g., inputs that are outside 
of system knowledge).    
3.
Th ies in 
de rence 
re andle 
te dings 
(O tudy, 
ge were 
ut % of 
ca sture 
an rred. 
Fu poral 
5%Others
7%Disfluency
13%Out of synchronization
20%Language understanding errors
55%Speech recognition errors
Percentage
 
Table 2: The distribution of error sources 3 Design Strategies   
e evaluation also indicates three important strateg
signing effective algorithms for multimodal refe
solution. The first strategy concerns with how to h
mporal relations. Consistent with the previous fin
viatt et al 1997), in most cases (85%) in our s
stures occurred before the referring expressions 
tered. However, we did find some exceptions. In 7
ses, there was no overlap between speech and ge
d speech were uttered before gestures occu
rthermore, one user could have different tem
behavior at different stages in one interaction. In our study, 
five users exhibited varied temporal alignment during the 
interaction. Therefore, to accommodate different temporal 
variations, incorporating relative temporal relations 
between different modalities based on temporal closeness 
is preferred over incorporating absolute temporal relations 
or temporal orders.  
Second, in a multimodal conversation, the potential 
objects referred to by a user could come from different 
sources. They could be the objects gestured at, objects in 
the visual focus (e.g., highlighted), objects visible on the 
screen, or objects mentioned in a prior conversation. It is 
important for reference resolution algorithms to 
simultaneously combine semantic, temporal, and 
contextual constraints. This is particularly important for 
complex inputs that involve multiple referring expressions 
and multiple gestures as described earlier. 
Third, depending on the interface design and the 
underlying architecture for multimodal systems, different 
types of uncertainties occur during the process of input 
interpretation. For example, in our interface, each house 
icon is built on top of the town icon. Therefore, a pointing 
gesture could result in several possible objects.  Once a 
touch screen is used, a finger point may result in different 
possibilities. Furthermore, most systems like ours are 
based on the pipelined architecture as shown in Figure1. 
The pipelined processes can potentially lose low 
probability information (e.g., recognized alternatives with 
low probabilities) that could be very crucial when 
incorporated with other modalities and the interaction 
context. Therefore, it is important to retain information at 
different levels and systematically incorporate the 
imprecise information.  
4 Conclusion 
This paper presents an evaluation of graph-based 
multimodal reference resolution in a conversational system. 
The evaluation indicates that, the real-time performance is 
largely dependent on speech recognition performance, 
language processing capability, disfluency detection from 
both speech and gesture, as well as the system engineering 
issues.  Furthermore, the studies identify three important 
strategies for robust multimodal reference resolution 
algorithms: (1) using relative temporal constraints based 
on temporal closeness, (2) combining temporal, semantic, 
and contextual constraints simultaneously, and (3) 
incorporating imprecise information. A successful 
approach will need to consider both algorithmic 
requirements and technology limitations.  
Acknowledgement 
The authors would like to thank Keith Houck and    
Michelle Zhou at IBM T. J. Watson Research Center for 
their support in developing the system, and the anonymous 
reviewers for their helpful comments and suggestions. 
 
References 
Bolt, R.A. 1980. Put that there: Voice and Gesture at the 
Graphics Interface. Computer Graphics14(3): 262-270. 
Cassell, J., Bickmore, T., Billinghurst, M., Campbell, L., 
Chang, K., Vilhjalmsson, H. and Yan, H. 1999. Embodi-
ment in Conversational Interfaces: Rea. In Proceedings of 
the CHI'99 Conference, pp. 520-527. Pittsburgh, PA. 
Chai, J. Y., Hong, P., and Zhou, M. X. 2004. A probabilistic 
approach to reference resolution in multimodal user inter-
faces, Proceedings of 9th International Conference on Intel-
ligent User Interfaces (IUI): 70-77. Madeira, Portugal, 
January.  
Chai, J., Pan, S., Zhou, M., and Houck, K. 2002. Context-
based Multimodal Interpretation in Conversational Systems. 
Fourth International Conference on Multimodal Interfaces. 
Chen, L., Harper, M. and Quek, F. 2002. Gesture patterns 
during speech repairs. Proceedings of International Con-
ference on Multimodal Interfaces (ICMI).  
Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J., 
Smith, I., Chen, L., and Clow, J. 1996. Quickset: Multimo-
dal Interaction for Distributed Applications. Proceedings of 
ACM Multimedia, pp. 31? 40. 
Gold, S. and Rangarajan, A. 1996. A graduated assignment 
algorithm for graph-matching. IEEE Trans. Pattern Analy-
sis and Machine Intelligence, vol. 18, no. 4. 
Gorniak, P. and Roy, D. 2003.Grounded Semantic Composi-
tion for Visual Scenes. Journal of Artificial Intelligence 
Research. 
Huls, C., Bos, E., and Classen, W. 1995. Automatic Referent 
Resolution of Deictic and Anaphoric Expressions. Compu-
tational Linguistics, 21(1):59-79. 
Johnston, M. 1998. Unification-based Multimodal parsing, 
Proceedings of COLING-ACL.  
Johnston, M. and Bangalore, S. 2000. Finite-state multimodal 
parsing and understanding. Proceedings of COLING. 
Johnston, M.,  Bangalore, S.,  Visireddy G., Stent, A., Ehlen, 
P., Walker, M., Whittaker, S., and Maloor, P. 2002. 
MATCH: An Architecture for Multimodal Dialog Systems, 
in Proceedings of ACL. 
Kehler, A. 2000. Cognitive Status and Form of Reference in 
Multimodal Human-Computer Interaction, Proceedings of 
AAAI. 
Neal, J. G., Thielman, C. Y.,  Dobes, Z. Haller, S. M., and 
Shapiro, S. C. 1998. Natural Language with Integrated 
Deictic and Graphic Gestures. Intelligent User Interfaces, 
M. Maybury and W. Wahlster (eds.), 38-51.  
Oviatt, S., DeAngeli, A., and Kuhn, K. 1997. Integration and 
Synchronization of Input Modes during Multimodal Hu-
man-Computer Interaction, In Proceedings of Conference 
on Human Factors in Computing Systems: CHI '97. 
Tsai, W.H. and Fu, K.S.  1979. Error-correcting isomorphism 
of attributed relational graphs for pattern analysis. IEEE 
Transactions on Systems, Man and Cybernetics, vol. 9, pp. 
757?768. 
Zancanaro, M., Stock, O., and Strapparava, C. 1997. Multi-
modal Interaction for Information Access: Exploiting Co-
hesion. Computational Intelligence 13(7):439-464. 
Optimization in Multimodal Interpretation  
Joyce Y. Chai*         Pengyu Hong+ Michelle X. Zhou? Zahar Prasov* 
*Computer Science and Engineering 
Michigan State University  
East Lansing, MI 48824 
{jchai@cse.msu.edu,  
prasovz@cse.msu.edu} 
+Department of Statistics 
Harvard University 
Cambridge, MA 02138 
hong@stat.harvard.edu 
?Intelligent Multimedia Interaction  
  IBM T. J. Watson Research Ctr. 
Hawthorne, NY 10532 
mzhou@us.ibm.com 
Abstract 
    In a multimodal conversation, the way users 
communicate with a system depends on the 
available interaction channels and the situated 
context (e.g., conversation focus, visual feedback). 
These dependencies form a rich set of constraints 
from various perspectives such as temporal 
alignments between different modalities, 
coherence of conversation, and the domain 
semantics. There is strong evidence that 
competition and ranking of these constraints is 
important to achieve an optimal interpretation. 
Thus, we have developed an optimization approach 
for multimodal interpretation, particularly for 
interpreting multimodal references. A preliminary 
evaluation indicates the effectiveness of this 
approach, especially for complex user inputs that 
involve multiple referring expressions in a speech 
utterance and multiple gestures.   
1 Introduction 
Multimodal systems provide a natural and 
effective way for users to interact with computers 
through multiple modalities such as speech, 
gesture, and gaze (Oviatt 1996). Since the first 
appearance of ?Put-That-There? system (Bolt 
1980), a variety of multimodal systems have 
emerged, from early systems that combine speech, 
pointing (Neal et al, 1991), and gaze (Koons et al 
1993), to systems that integrate speech with pen 
inputs (e.g., drawn graphics) (Cohen et al, 1996; 
Wahlster 1998; Wu et al, 1999), and systems that 
engage users in intelligent conversation (Cassell et 
al., 1999; Stent et al, 1999; Gustafson et al, 2000; 
Chai et al, 2002; Johnston et al, 2002).   
One important aspect of building multimodal 
systems is multimodal interpretation, which is a 
process that identifies the meanings of user inputs. 
In a multimodal conversation, the way users 
communicate with a system depends on the 
available interaction channels and the situated 
context (e.g., conversation focus, visual feedback). 
These dependencies form a rich set of constraints 
from various aspects (e.g., semantic, temporal, and 
contextual). A correct interpretation can only be 
attained by simultaneously considering these 
constraints. In this process, two issues are 
important: first, a mechanism to combine 
information from various sources to form an 
overall interpretation given a set of constraints; and 
second, a mechanism that achieves the best 
interpretation among all the possible alternatives 
given a set of constraints. The first issue focuses on 
the fusion aspect, which has been well studied in 
earlier work, for example, through unification-
based approaches (Johnston 1998) or finite state 
approaches (Johnston and Bangalore, 2000). This 
paper focuses on the second issue of optimization.  
As in natural language interpretation, there is 
strong evidence that competition and ranking of 
constraints is important to achieve an optimal 
interpretation for multimodal language processing.  
We have developed a graph-based optimization 
approach for interpreting multimodal references. 
This approach achieves an optimal interpretation 
by simultaneously applying semantic, temporal, 
and contextual constraints. A preliminary 
evaluation indicates the effectiveness of this 
approach, particularly for complex user inputs that 
involve multiple referring expressions in a speech 
utterance and multiple gestures. In this paper, we 
first describe the necessities for optimization in 
multimodal interpretation, then present our graph-
based optimization approach and discuss how our 
approach addresses key principles in Optimality 
Theory used for natural language interpretation  
(Prince and Smolensky 1993).  
2 Necessities for Optimization in 
Multimodal Interpretation 
In a multimodal conversation, the way a user 
interacts with a system is dependent not only on 
the available input channels (e.g., speech and 
gesture), but also upon his/her conversation goals, 
the state of the conversation, and the multimedia 
feedback from the system. In other words, there is 
a rich context that involves dependencies from 
many different aspects established during the 
interaction. Interpreting user inputs can only be 
situated in this rich context. For example, the 
temporal relations between speech and gesture are 
important criteria that determine how the 
information from these two modalities can be 
combined. The focus of attention from the prior 
conversation shapes how users refer to those 
objects, and thus, influences the interpretation of 
referring expressions. Therefore, we need to 
simultaneously consider the temporal relations 
between the referring expressions and the gestures, 
the semantic constraints specified by the referring 
expressions, and the contextual constraints from 
the prior conversation. It is important to have a 
mechanism that supports competition and ranking 
among these constraints to achieve an optimal 
interpretation, in particular, a mechanism to allow 
constraint violation and support soft constraints.  
We use temporal constraints as an example to 
illustrate this viewpoint1.  The temporal constraints 
specify whether multiple modalities can be 
combined based on their temporal alignment. In 
earlier work, the temporal constraints are 
empirically determined based on user studies 
(Oviatt 1996). For example, in the unification-
based approach (Johnston 1998), one temporal 
constraint indicates that speech and gesture can be 
combined only when the speech either overlaps 
with gesture or follows the gesture within a certain 
time frame. This is a hard constraint that has to be 
satisfied in order for the unification to take place. 
If a given input does not satisfy these hard 
constraints, the unification fails.  
In our user studies, we found that, although the 
majority of user temporal alignment behavior may 
satisfy pre-defined temporal constraints, there are 
                                                                
1 We implemented a system using real estate as an application 
domain.  The user can interact with a map using both speech 
and gestures to retrieve information. All the user studies men-
tioned in this paper were conducted using this system.  
some exceptions. Table 1 shows the percentage of 
different temporal relations collected from our user 
studies. The rows indicate whether there is an 
overlap between speech referring expressions and 
their accompanied gestures. The columns indicate 
whether the speech (more precisely, the referring 
expressions) or the gesture occurred first. 
Consistent with the previous findings (Oviatt et al 
1997), in most cases (85% of time), gestures 
occurred before the referring expressions were 
uttered. However, in 15% of the cases the speech 
referring expressions were uttered before the 
gesture occurred. Among those cases, 8% had an 
overlap between the referring expressions and the 
ge
), 
al al 
(i ) 
in f 
in e 
1 n 
in 6 
m ?s 
sp d 
w d 
ah e 
us
sp
be
te
ac
T
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7
User
Pe
rc
en
ta
ge
Non-overlap Speech First Non-overlap Gesture First
Overlap Speech First Overlap Gesture First
Figure 1: Temporal relations between speech and gesture 
for individual users 
100%85%15%Total
48%40%8%Overlap
52%45%7%Non-overlap
TotalGesture FirstSpeech First
 
Table 1: Overall temporal relations between speech and 
gesture sture and 7% had no overlap.  
Furthermore, as shown in (Oviatt et al, 2003
though multimodal behaviors such as sequenti
.e., non-overlap) or simultaneous (e.g., overlap
tegration are quite consistent during the course o
teraction, there are still some exceptions. Figur
shows the temporal alignments from seve
dividual users in our study. User 2 and User 
aintained a consistent behavior in that User 2
eech referring expressions always overlappe
ith gestures and User 6?s gesture always occurre
ead of the speech expressions. The other fiv
ers exhibited varied temporal alignment between 
eech and gesture during the interaction.  It will 
 difficult for a system using pre-defined 
mporal constraints to anticipate and 
commodate all these different behaviors.  
herefore, it is desirable to have a mechanism that 
allows violation of these constraints and support 
soft or graded constraints.  
3 A Graph-based Optimization Approach  
To address the necessities described above, we 
developed an optimization approach for 
interpreting multimodal references using graph 
matching. The graph representation captures both 
salient entities and their inter-relations. The graph 
matching is an optimization process that finds the 
best matching between two graphs based on 
constraints modeled as links or nodes in these 
graphs. This type of structure and process is 
especially useful for interpreting multimodal 
references. One graph can represent all the 
referring expressions and their inter-relations, and 
the other graph can represent all the potential 
referents. The question is how to match them 
together to achieve a maximum compatibility 
given a particular context.  
3.1 Overview  
Graph-based Representation 
Attribute Relation Graph (ARG) (Tsai and Fu, 1979) 
is used to represent information in our approach. 
An ARG consists of a set of nodes that are 
connected by a set of edges. Each node represents 
an entity, which in our case is either a referring 
expression to be resolved or a potential referent.  
Each node encodes the properties of the 
corresponding entity including: 
? Semantic information that indicates the 
semantic type, the number of potential referents, 
and the specific attributes related to the 
corresponding entity (e.g., extracted from the 
referring expressions).  
? Temporal information that indicates the time 
when the corresponding entity is introduced into 
the discourse (e.g., uttered or gestured).  
Each edge represents a set of relations between 
two entities. Currently we capture temporal 
relations and semantic type relations. A temporal 
relation indicates the temporal order between two 
related entities during an interaction, which may 
have one of the following values:  
? Precede: Node A precedes Node B if the entity 
represented by Node A is introduced into the 
discourse before the entity represented by Node B.  
? Concurrent: Node A is concurrent with Node B if 
the entities represented by them are referred to or 
mentioned simultaneously. 
? Non-concurrent: Node A is non-concurrent with 
Node B if their corresponding objects/references 
cannot be referred/mentioned simultaneously.  
? Unknown: The temporal order between two entities 
is unknown. It may take the value of any of the 
above.  
A semantic type relation indicates whether two 
related entities share the same semantic type. It 
currently takes the following discrete values: Same, 
Different, and Unknown. It could be beneficial in the 
future to consider a continuous function measuring 
the rate of compatibility instead.  
Specially, two graphs are generated. One graph, 
called the referring graph, captures referring 
expressions from speech utterances. For example, 
suppose a user says Compare this house, the green 
house, and the brown one. Figure 2 show a referring 
graph that represents three referring expressions 
from this speech input. Each node captures the 
semantic information such as the semantic type 
(i.e., Semantic Type), the attribute (Color), the 
number (Number) of the potential referents, as well 
as the temporal information about when this 
referring expression is uttered (BeginTime and 
EndTime). Each edge captures the semantic (e.g., 
SemanticTypeRelation) and temporal relations (e.g., 
TemporalRelation) between the referring expressions. 
In this case, since the green house is uttered before 
the brown one, there is a temporal Precede 
relationship between these two expressions. 
Furthermore, according to our heuristic that 
objects-to-be-compared should share the same 
semantic type, therefore, the SemanticTypeRelation 
between two nodes is set to Same.  
Node 1
this house
Node 2
the green 
house
Node 3
the brown 
one
SemanticType: House
Number.: 1
Attribute: Color = $Green
BeginTime: 32244242ms
EndTime: ?
? ?
SemanticTypeRelation: Same
TemporalRelation: Precede
Direction: Node 2 -> Node 3
Speech: Compare this house, the green house    
and the brown one
 
Figure 2: An example of a referring graph 
Similarly, the second graph, called the referent 
graph, represents all potential referents from 
multiple sources (e.g., from the last conversation, 
gestured by the user, etc). Each node captures the 
semantic and temporal information about a 
potential referent (e.g., the time when the potential 
referent is selected by a gesture). Each edge 
captures the semantic and temporal relations 
between two potential referents.  For instance, 
suppose the user points to one position and then 
points to another position. The corresponding 
referent graph is shown in Figure 3. The objects 
inside the first dashed rectangle correspond to the 
potential referents selected by the first pointing 
gesture and those inside the second dashed 
rectangle correspond to the second pointing gesture. 
Each node also contains a probability that indicates 
the likelihood of its corresponding object being 
selected by the gesture. Furthermore, the salient 
objects from the prior conversation are also 
included in the referent graph since they could also 
be the potential referents (e.g., the rightmost 
dashed rectangle in Figure 32).  
To create these graphs, we apply a grammar-
based natural language parser to process speech 
inputs and a gesture recognition component to 
process gestures. The details are described in (Chai 
et al 2004a).  
                                                                
2 Each node from the conversation context is linked to every 
node corresponding to the first pointing and the second point-
ing.  
Graph-matching Process 
Given these graph representations, interpreting 
multimodal references becomes a graph-matching 
problem. The goal is to find the best match 
between a referring graph (Gs) and a referent graph 
(Gr). Suppose ? A referring graph Gs = ?{?m}, {?mn}?, where {?m} are 
nodes and {?mn} are edges connecting nodes ?m and ?n. Nodes in Gs are named referring nodes. 
? A referent graph Gr = ?{ax}, {rxy}?, where {ax} are 
nodes and {rxy} are edges connecting nodes ax and ay. 
Nodes in Gr are named referent nodes. 
   The following equation finds a match that 
achieves the maximum compatibility between Gr 
and Gs:  
),(),(),(
),(),(),(
mnxynymxx y m n
mxmxx msr
rEdgeSimaPaP
aNodeSimaPGGQ
???
??
? ? ? ?
? ? +=  (1)   
In Equation (1), Q(Gr,Gs) measures the degree of 
the overall match between the referent graph and 
the referring graph. P(ax,?m) is the matching 
probability between a node ax in the referent graph 
and a node ?m in the referring graph. The overall 
compatibility depends on the similarities between 
nodes (NodeSim) and the similarities between 
edges (EdgeSim). The function NodeSim(ax,?m) 
measures the similarity between a referent node ax 
and a referring node ?m by combining semantic 
constraints and temporal constraints. The function 
EdgeSim(rxy,?mn) measures the similarity between 
rxy and ?mn, which depends on the semantic and 
temporal constraints of the corresponding edges. 
These functions are described in detail in the next 
section.  
We use the graduated assignment algorithm 
(Gold and Rangarajan, 1996) to maximize Q(Gr,Gs) 
in Equation (1). The algorithm first initializes 
P(ax,?m) and then iteratively updates the values of 
P(ax,?m) until it converges. When the algorithm 
converges, P(ax,?m) gives the matching 
probabilities between the referent node ax and the 
referring node ?m that maximizes the overall 
compatibility function. Given this probability 
matrix, the system is able to assign the most 
probable referent(s) to each referring expression.  
3.2 Similarity Functions  
As shown in Equation (1), the overall 
compatibility between a referring graph and a 
referent graph depends on the node similarity 
Ossining
Chappaqua
Object ID: MLS2365478
SemanticType: House
Attribute: Color = $Brown
BeginTime: 32244292 ms
SelectionProb: 0.65
? ?
Semantic Type Relation: Diff
Temporal relation: Same
Direction: 
Gesture: Point to one position and point to 
another position
First pointing Second pointing Conversation
Context
Figure 3: An example of referent graph 
function and the edge similarity function. Next we 
give a detailed account of how we defined these 
functions. Our focus here is not on the actual 
definitions of those functions (since they may vary 
for different applications), but rather a mechanism 
that leads to competition and ranking of constraints.  
Node Similarity Function 
Given a referring expression (represented as ?m 
in the referring graph) and a potential referent 
(represented as ax in the referent graph), the node 
similarity function is defined based on the 
semantic and temporal information captured in ax 
and ?m through a set of individual compatibility 
functions: 
   NodeSim(ax,?m) = Id(ax,?m) SemType(ax,?m)  
                               ?k Attrk(ax,?m) Temp(ax,?m) 
Currently, in our system, the specific return 
values for these functions are empirically 
determined through iterative regression tests.  
Id(ax,?m) captures the constraint of the 
compatibilities between identifiers specified in ax 
and ?m. It indicates that the identifier of the 
potential referent, as expressed in a referring 
expression, should match the identifier of the true 
referent. This is particularly useful for resolving 
proper nouns. For example, if the referring 
expression is house number eight, then the correct 
referent should have the identifier number eight.  
We currently define this constraint as follows: 
Id(ax,?m) = 0 if the object identities of ax and ?m 
are different. Id(ax,?m) = 100 if they are the same. 
Id(ax,?m) = 1 if at least one of the identities of ax 
and ?m is unknown. The different return values 
enforce that a large reward is given to the case 
where the identifiers from the referring expressions 
match the identifiers from the potential referents.  
SemType(ax,?m) captures the constraint of 
semantic type compatibility between  ax and ?m. It 
indicates that the semantic type of a potential 
referent as expressed in the referring expression 
should match the semantic type of the correct 
referent. We define the following: SemType(ax,?m) 
= 0 if the semantic types of ax and ?m are different. 
SemType(ax,?m) = 1 if they are the same. 
SemType(ax,?m) = 0.5 if at least one of the 
semantic types of ax and ?m is unknown. Note that 
the return value given to the case where semantic 
types are the same (i.e., ?1?) is much lower than 
that given to the case where identifiers are the 
same (i.e., ?100?). This was designed to support 
constraint ranking. Our assumption is that the 
constraint on identifiers is more important than the 
constraint on semantic types. Because identifiers 
are usually unique, the corresponding constraint is 
a greater indicator of node matching if the 
identifier expressed from a referring expression 
matches the identifier of a potential referent. 
Attrk(ax,?m) captures the domain specific 
constraint concerning a particular semantic feature 
(indicated by the subscription k). This constraint 
indicates that the expected features of a potential 
referent as expressed in a referring expression 
should be compatible with features associated with 
the true referent. For example, in the referring 
expression the Victorian house, the style feature is 
Victorian.  Therefore, an object can only be a 
possible referent if the style of that object is 
Victorian.  Thus, we define the following: Ak(ax,?m) 
= 1 if both ax and ?m share the kth feature with the 
same value. Ak(ax,?m) = 0 if both ax and ?m have 
the feature k and the values of the feature k are not 
equal. Otherwise, when the kth feature is not 
present in either ax or ?m, then Ak (ax,?m) = 0.1.  
Note that these feature constraints are dependent 
on the specific domain model for a particular 
application.  
Temp(ax,?m) captures the temporal constraint 
between a referring expression ?m and a potential 
referent ax. As discussed in Section 2, a hard 
constraint concerning temporal relations between 
referring expressions and gestures will be 
incapable of handling the flexibility of user 
temporal alignment behavior. Thus the temporal 
constraint in our approach is a graded constraint, 
which is defined as follows: 
          
)
2000
|)()(|exp(),( mxmx
BeginTimeaBeginTimeaTemp ?? ??=  
This constraint indicates that the closer a 
referring expression and a potential referent in 
terms of their temporal alignment (regardless of 
the absolute precedence relationship), the more 
compatible they are.  
Edge Similarity Function 
The edge similarity function measures the 
compatibility of relations held between referring 
expressions (i.e., an edge ?mn in the referring graph) 
and relations between the potential referents (i.e., 
an edge rxy in the referent graph). It is defined by 
two individual compatibility functions as follows: 
EdgeSim(rxy, ?mn) = SemType(rxy, ?mn) Temp(rxy, ?mn)   
   SemType(rxy, ?mn) encodes the semantic type 
compatibility between an edge in the referring 
graph and an edge in the referent graph. It is 
defined in Table 2. This constraint indicates that 
the relation held between referring expressions 
should be compatible with the relation held 
between two correct referents. For example, 
consider the utterance How much is this green house 
and this blue house. This utterance indicates that the 
referent to the first expression this green house 
should share the same semantic type as the referent 
to the second expression this blue house. As shown 
in Table 2, if the semantic type relations of rxy and 
?mn are the same, SemType(rxy, ?mn) returns 1. If 
they are different, SemType(rxy, ?mn) returns zero. If 
either rxy or ?mn is unknown, then it returns 0.5.  
   Temp(rxy, ?mn) captures the temporal 
compatibility between an edge in the referring 
graph and an edge in the referent graph. It is 
defined in Table 3. This constraint indicates that 
the temporal relationship between two referring 
expressions (in one utterance) should be 
compatible with the relations of their 
corresponding referents as they are introduced into 
the context (e.g., through gesture). The temporal 
relation between referring expressions (i.e., ?mn) is 
either Precede or Concurrent. If the temporal 
relations of rxy and ?mn are the same, then Temp(rxy, 
?mn) returns 1. Because potential references could 
come from prior conversation, even if rxy and ?mn 
are not the same, the function does not return zero 
when ?mn is Precede.  
Next, we discuss how these definitions and the 
process of graph matching address optimization, in 
particular, with respect to key principles of 
Optimality Theory for natural language 
interpretation.  
3.3 Optimality Theory 
Optimality Theory (OT) is a theory of language 
and grammar, developed by Alan Prince and Paul 
Smolensky (Prince and Smolensky, 1993). In 
Optimality Theory, a grammar consists of a set of 
well-formed constraints. These constraints are 
applied simultaneously to identify linguistic 
structures. Optimality Theory does not restrict the 
content of the constraints (Eisner 1997). An 
innovation of Optimality Theory is the conception 
of these constraints as soft, which means violable 
and conflicting.  The interpretation that arises for 
an utterance within a certain context maximizes the 
degree of constraint satisfaction and is 
consequently the best alternative (hence, optimal 
interpretation) among the set of possible 
interpretations.  
The key principles or components of Optimality 
Theory can be summarized as the following three 
components (Blutner 1998): 1) Given a set of input, 
Generator creates a set of possible outputs for each 
input. 2) From the set of candidate output, Evaluator 
selects the optimal output for that input. 3) There is 
a strict dominance in term of the ranking of constraints. 
Constraints are absolute and the ranking of the 
constraints is strict in the sense that outputs that 
have at least one violation of a higher ranked 
constraint outrank outputs that have arbitrarily 
many violations of lower ranked constraints. 
Although Optimality Theory is a grammar-based 
framework for natural language processing, its key 
principles can be applied to other representations. 
At a surface level, our approach addresses these 
main principles. 
First, in our approach, the matching matrix 
P(ax,?m) captures the probabilities of all the 
possible matches between a referring node ?m and 
a referent node ax. The matching process updates 
these probabilities iteratively. This process 
corresponds to the Generator component in 
Optimality Theory.  
Second, in our approach, the satisfaction or 
violation of constraints is implemented via return 
values of compatibility functions. These 
0.50.50.5Unknown
0.510Different
0.501Same?mn
Unknown DifferentSame
rxySemType(rxy, ?mn)
 
Table 2: Definition of SemType(rxy, ?mn) 
 
0.5010Concurrent
0.50.70.51Precede?mn
Unknown Non-concurrentConcurrentPreceding
rxyTemp(rxy, ?mn)
Table 3: Definition of Temp(rxy, ?mn) 
constraints can be violated during the matching 
process. For example, functions Id(ax,?m), 
SemType(ax,?m), and Attrk(ax,?m) return zero if the 
corresponding intended constraints are violated. In 
this case, the overall similarity function will return 
zero. However, because of the iterative updating 
nature of the matching algorithm, the system will 
still find the most optimal match as a result of the 
matching process even some constraints are 
violated. Furthermore, A function that never 
returns zero such as Temp(ax,?m) in the node 
similarity function implements a gradient 
 
 
 
 
 
 
 
. 
 
 
, 
 
 
. 
 
3.4 Evaluation  
We conducted several user studies to evaluate 
the performance of this approach. Users could 
interact with our system using both speech and 
deictic gestures. Each subject was asked to 
complete five tasks. For example, one task was to 
find the cheapest house in the most populated town. 
Data from eleven subjects was collected and 
analyzed. 
Table 4 shows the evaluation results of 219 
inputs. These inputs were categorized in terms of 
the number of referring expressions in the speech 
input and the number of gestures in the gesture 
inputs. Out of the total 219 inputs, 137 inputs had 
their referents correctly interpreted. For the 
remaining 82 inputs in which the referents were 
not correctly identified, the problem did not come 
from the approach itself, but rather from other 
sources such as speech recognition and language 
understanding errors.  These were two major error 
sources, which were accounted for 55% and 20% 
of total errors respectively (Chai et al 2004b).    
In our studies, the majority of user references 
were simple in that they involved only one 
referring expression and one gesture as in earlier 
findings (Kehler 2000). It is trivial for our 
approach to handle these simple inputs since the 
size of the graph is usually very small and there is 
only one node in the referring graph. However, we 
did find 23% complex inputs (the row S3 and the 
column G3 in Table 4), which involved multiple 
referring expressions from speech utterances 
and/or multiple gestures. Our optimization 
approach is particularly effective to interpret these 
complex inputs by simultaneously considering 
semantic, temporal, and contextual constraints.  
4 Conclusion 
As in natural language interpretation addressed 
by Optimality Theory, the idea of optimizing 
constraints is beneficial and there is evidence in 
favor of competition and constraint ranking in 
multimodal language interpretation. We developed 
a graph-based approach to address optimization for 
multimodal interpretation; in particular, 
interpreting multimodal references. Our approach 
simultaneously applies temporal, semantic, and 
contextual constraints together and achieves the 
best interpretation among all alternatives. Although 
currently the referent graph corresponds to gesture 
129(111)
90(26)
20(15),
19(2)
102(91),
65(22)
7(5),
6(2)
Total Num
15(9),
16(1)
12(8),
8(0)
3(1),
7(1)
0(0),
1(0)
S3: Multiple referring 
expressions
110(90),
74(25)
8(7),
11(2)
96(89),
58(21)
6(4),
5(2)
S2: One referring
expression
4(2),
0(0)
03(1),
0(0)
1(1),
0(0)
S1:No referring
expression
Total
Num
G3: Multi-
Gestures
G2: One 
Gesture
G1: No 
Gesture
 
Table 4: Evaluation Results. In each entry form ?a(b), c(d)?,
?a? indicates the number of inputs in which the referring
expressions were correctly recognized by the speech recog-
nizer; ?b? indicates the number of inputs in which the refer-
ring expressions were  correctly recognized and were
correctly resolved; ?c? indicates the number of inputs in
which the referring expressions were not correctly recog-
nized; ?d? indicates the number of inputs in which the refer-
ring expressions also were not correctly recognized, but
were correctly resolved. The sum of ?a? and ?c? gives the
total number of inputs with a particular combination of
speech and gesture. constraint in Optimality Theory. Given these
compatibility functions, the graph-matching
algorithm provides an optimization process to find
the best match between two graphs. This process
corresponds to the Evaluator component of
Optimality Theory.  
Third, in our approach, different compatibility
functions return different values to address the
Constraint Ranking component in Optimality Theory
For example, as discussed earlier, once ax and ?m
share the same identifier, Id(ax,?m) returns 100. If
ax and ?m share the same semantic type
SemType(ax,?m) returns 1. Here, we consider the
compatibility between identifiers is more important
than the compatibility between semantic types
However, currently we have not yet addressed the
strict dominance aspect of Optimality Theory. 
input and conversation context, it can be easily 
extended to incorporate other modalities such as 
gaze inputs.  
We have only taken an initial step to investigate 
optimization for multimodal language processing. 
Although preliminary studies have shown the 
effectiveness of the optimization approach based 
on graph matching, this approach also has its 
limitations.  The graph-matching problem is a NP 
complete problem and it can become intractable 
once the size of the graph is increased. However, 
we have not experienced the delay of system 
responses during real-time user studies. This is 
because most user inputs were relatively concise 
(they contained no more than four referring 
expressions).  This brevity limited the size of the 
graphs and thus provided an opportunity for such 
an approach to be effective. Our future work will 
address how to extend this approach to optimize 
the overall interpretation of user multimodal inputs.  
Acknowledgements 
This work was partially supported by grant IIS-
0347548 from the National Science Foundation 
and grant IRGP-03-42111 from Michigan State 
University. The authors would like to thank John 
Hale and anonymous reviewers for their helpful 
comments and suggestions.  
References 
Bolt, R.A. 1980. Put that there: Voice and Gesture at the 
Graphics Interface. Computer Graphics, 14(3): 262-270.  
Blutner, R., 1998. Some Aspects of Optimality In Natural 
Language Interpretation. Journal of Semantics, 17, 189-216. 
Cassell, J., Bickmore, T., Billinghurst, M., Campbell, L., 
Chang, K., Vilhjalmsson, H. and Yan, H. 1999. Embodi-
ment in Conversational Interfaces: Rea. In Proceedings of 
the CHI'99 Conference, 520-527.  
Chai, J., Prasov, Z, and Hong, P. 2004b. Performance Evalua-
tion and Error Analysis for Multimodal Reference Resolu-
tion in a Conversational System. Proceedings of HLT-
NAACL 2004 (Companion Volumn).  
Chai, J. Y., Hong, P., and Zhou, M. X. 2004a. A Probabilistic 
Approach to Reference Resolution in Multimodal User In-
terfaces, Proceedings of 9th International Conference on 
Intelligent User Interfaces (IUI): 70-77.  
Chai, J., Pan, S., Zhou, M., and Houck, K. 2002. Context-
based Multimodal Interpretation in Conversational Systems. 
Fourth International Conference on Multimodal Interfaces. 
Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J., 
Smith, I., Chen, L., and Clow, J. 1996. Quickset: Multimo-
dal Interaction for Distributed Applications. Proceedings of 
ACM Multimedia. 
Eisner, Jason. 1997. Efficient Generation in Primitive Opti-
mality Theory. Proceedings of ACL?97. 
Gold, S. and Rangarajan, A. 1996. A Graduated Assignment 
Algorithm for Graph-matching. IEEE Trans. Pattern 
Analysis and Machine Intelligence, vol. 18, no. 4.  
Gustafson, J., Bell, L., Beskow, J., Boye J., Carlson, R., Ed-
lund, J., Granstrom, B., House D., and Wiren, M.  2000. 
AdApt ? a Multimodal Conversational Dialogue System in 
an Apartment Domain. Proceedings of 6th International 
Conference on Spoken Language Processing (ICSLP). 
Johnston, M, Cohen, P., McGee, D., Oviatt, S., Pittman, J. and 
Smith, I. 1997. Unification-based Multimodal Integration, 
Proceedings of ACL?97. 
Johnston, M. 1998. Unification-based Multimodal Parsing, 
Proceedings of COLING-ACL?98. 
Johnston, M. and Bangalore, S. 2000. Finite-state Multimodal 
Parsing and Understanding. Proceedings of COLING?00.  
Johnston, M.,  Bangalore, S.,  Visireddy G., Stent, A., Ehlen, 
P., Walker, M., Whittaker, S., and Maloor, P. 2002. 
MATCH: An Architecture for Multimodal Dialog Systems, 
Proceedings of ACL?02, Philadelphia, 376-383. 
Kehler, A. 2000. Cognitive Status and Form of Reference in 
Multimodal Human-Computer Interaction, Proceedings of 
AAAI?01, 685-689. 
Koons, D. B., Sparrell, C. J. and Thorisson, K. R. 1993. Inte-
grating Simultaneous Input from Speech, Gaze, and Hand 
Gestures. In Intelligent Multimedia Interfaces, M. Maybury, 
Ed. MIT Press: Menlo Park, CA. 
Neal, J. G., and Shapiro, S. C.  1991. Intelligent Multimedia 
Interface Technology. In Intelligent User Interfaces, J. Sul-
livan & S. Tyler, Eds. ACM: New York. 
Oviatt, S. L. 1996. Multimodal Interfaces for Dynamic Inter-
active Maps. In Proceedings of Conference on Human Fac-
tors in Computing Systems: CHI '96, 95-102.  
Oviatt, S., DeAngeli, A., and Kuhn, K., 1997. Integration and 
Synchronization of Input Modes during Multimodal Hu-
man-Computer Interaction, In Proceedings of Conference 
on Human Factors in Computing Systems: CHI '97. 
Oviatt, S., Coulston, R., Tomko, S., Xiao, B., Bunsford, R. 
Wesson, M., and Carmichael, L. 2003. Toward a Theory of 
Organized Multimodal Integration Patterns during Human-
Computer Interaction. In Proceedings of Fifth International 
Conference on Multimodal Interfaces, 44-51.  
Prince, A. and Smolensky, P. 1993. Optimality Theory. Con-
straint Interaction in Generative Grammar. ROA 537.  
http://roa.rutgers.edu/view.php3?id=845.  
Stent, A., J. Dowding, J. M. Gawron, E. O. Bratt, and R. 
Moore. 1999. The Commandtalk Spoken Dialog System. 
Proceedings of ACL?99,  183?190. 
Tsai, W.H. and Fu, K.S.  1979. Error-correcting Isomorphism 
of Attributed Relational Graphs for Pattern Analysis. IEEE 
Transactions on Systems, Man and Cybernetics., vol. 9. 
Wahlster, W., 1998. User and Discourse Models for Multimo-
dal Communication. Intelligent User Interfaces, M. 
Maybury and W. Wahlster (eds.),  359-370. 
Wu, L., Oviatt, S., and Cohen, P. 1999. Multimodal Integra-
tion ? A Statistical View, IEEE Transactions on Multime-
dia, Vol. 1, No. 4, 334-341.   
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 471?481,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Fusing Eye Gaze with Speech Recognition Hypotheses to
Resolve Exophoric References in Situated Dialogue
Zahar Prasov and Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824, USA
{prasovza,jchai}@cse.msu.edu
Abstract
In situated dialogue humans often utter lin-
guistic expressions that refer to extralinguistic
entities in the environment. Correctly resolv-
ing these references is critical yet challeng-
ing for artificial agents partly due to their lim-
ited speech recognition and language under-
standing capabilities. Motivated by psycholin-
guistic studies demonstrating a tight link be-
tween language production and human eye
gaze, we have developed approaches that in-
tegrate naturally occurring human eye gaze
with speech recognition hypotheses to resolve
exophoric references in situated dialogue in
a virtual world. In addition to incorporat-
ing eye gaze with the best recognized spo-
ken hypothesis, we developed an algorithm to
also handle multiple hypotheses modeled as
word confusion networks. Our empirical re-
sults demonstrate that incorporating eye gaze
with recognition hypotheses consistently out-
performs the results obtained from processing
recognition hypotheses alone. Incorporating
eye gaze with word confusion networks fur-
ther improves performance.
1 Introduction
Given a rapid growth in virtual world applications
for tutoring and training, video games and simu-
lations, and assistive technology, enabling situated
dialogue in virtual worlds has become increasingly
important. Situated dialogue allows human users to
navigate in a spatially rich environment and carry
a conversation with artificial agents to achieve spe-
cific tasks pertinent to the environment. Different
from traditional telephony-based spoken dialogue
systems and multimodal conversational interfaces,
situated dialogue supports immersion and mobility
in a visually rich environment and encourages so-
cial and collaborative language use (Byron et al,
2005; Gorniak et al, 2006). In situated dialogue, hu-
man users often need to make linguistic references,
known as exophoric referring expressions (e.g., the
book to the right), to extralinguistic entities
in the environment. Reliably resolving these ref-
erences is critical for dialogue success. However,
reference resolution remains a challenging problem,
partly due to limited speech and language process-
ing capabilities caused by poor speech recognition
(ASR), ambiguous language, and insufficient prag-
matic knowledge.
To address this problem, motivated by psycholin-
guistic studies demonstrating a close relationship
between language production and eye gaze, our
previous work has incorporated naturally occurring
eye gaze in reference resolution (Prasov and Chai,
2008). Our findings have shown that eye gaze can
partially compensate for limited language process-
ing and domain modeling. However, this work was
conducted in a setting where users only spoke to a
static visual interface. In situated dialogue, human
speech and eye gaze patterns are much more com-
plex. The dynamic nature of the environment and
the complexity of spatially rich tasks have a massive
influence on what the user will look at and say. It is
not clear to what degree prior findings can generalize
to situated dialogue. Therefore, this paper explores
new studies on incorporating eye gaze for exophoric
reference resolution in a fully situated virtual envi-
471
ronment ? a more realistic approximation of real
world interaction. In addition to incorporating eye
gaze with the best recognized spoken hypothesis, we
developed an algorithm to also handle multiple hy-
potheses modeled as word confusion networks.
Our empirical results have demonstrated the util-
ity of eye gaze for reference resolution in situ-
ated dialogue. Although eye gaze is much more
noisy given the mobility of the user, our results
have shown that incorporating eye gaze with recog-
nition hypotheses consistently outperform the re-
sults obtained from processing recognition hypothe-
ses alone. In addition, incorporating eye gaze with
word confusion networks further improves perfor-
mance. Our analysis also indicates that, although a
word confusion network appears to be more compli-
cated, the time complexity of its integration with eye
gaze is well within the acceptable range for real-time
applications.
2 Related Work
Prior work in reference resolution within situated di-
alogue has focused on using visual context to assist
reference resolution during interaction. In (Kelleher
and van Genabith, 2004) and (Byron et al, 2005), vi-
sual features of objects are used to model the focus
of attention. This attention modeling is subsequently
used to resolve references. In contrast to this line of
research, here we explore the use of human eye gaze
during real-time interaction to model attention and
facilitate reference resolution. Eye gaze provides a
richer medium for attentional information, but re-
quires processing of a potentially noisy signal.
Eye gaze has been used to facilitate human ma-
chine conversation and automated language process-
ing. For example, eye gaze has been studied in
embodied conversational discourse as a mechanism
to gather visual information, aid in thinking, or fa-
cilitate turn taking and engagement (Nakano et al,
2003; Bickmore and Cassell, 2004; Sidner et al,
2004; Morency et al, 2006; Bee et al, 2009).
Recent work has explored incorporating eye gaze
into automated language understanding such as au-
tomated speech recognition (Qu and Chai, 2007;
Cooke and Russell, 2008), automated vocabulary ac-
quisition (Liu et al, 2007; Qu and Chai, 2010), at-
tention prediction (Qvarfordt and Zhai, 2005; Fang
et al, 2009).
Motivated by previous psycholinguistic findings
that eye gaze is tightly linked with language pro-
cessing (Just and Carpenter, 1976; Tanenhous et al,
1995; Meyer and Levelt, 1998; Griffin and Bock,
2000), our prior work incorporates eye gaze into
reference resolution. Our results demonstrate that
such use of eye gaze can potentially compensate
for a conversational systems limited language pro-
cessing and domain modeling capability (Prasov and
Chai, 2008). However, this work is conducted in a
static visual environment and evaluated only on tran-
scribed spoken utterances. In situated dialogue, eye
gaze behavior is much more complex. Here, gaze
fixations may be made for the purpose of naviga-
tion or scanning the environment rather than refer-
ring to a particular object. Referring expressions can
be made to objects that are not in the user?s field of
view, but were previously visible on the interface.
Additionally, users may make egocentric spatial ref-
erences (e.g. ?the chair on the left?) which require
contextual knowledge (e.g. the users position in the
environment) in order to resolve. Therefore, the fo-
cus of our work here is on exploring these complex
user behaviors in situated dialogue and examining
how to combine eye gaze with ASR hypotheses for
improved reference resolution.
Alternative ASR hypotheses have been used in
many different ways in speech driven systems. Par-
ticularly, in (Mangu et al, 2000) multiple lattice
alignment is used for construction of word confusion
networks and in (Hakkani-Tu?r et al, 2006) word
confusion networks are used for named entity de-
tection. In the study presented here, we apply word
confusion networks (to represent ASR hypotheses)
along with eye gaze to the problem of reference res-
olution.
3 Data Collection
In this investigation, we created a 3D virtual world
(using the Irrlicht game engine1) to support situated
dialogue. We conducted a Wizard of Oz study in
which the user must collaborate with a remote arti-
ficial agent cohort (controlled by a human) to solve
a treasure hunting task. The cohort is an ?expert?
in treasure hunting and has some knowledge regard-
1http://irrlicht.sourceforge.net/
472
ing the locations of the treasure items, but cannot
see the virtual environment. The user, immersed in
the virtual world, must navigate the environment and
conduct a mixed-initiative dialogue with the agent to
find the hidden treasures. During the experiments,
a noise-canceling microphone was used to record
user speech and the Tobii 1750 display-mounted eye
tracker was used to record user eye movements.
A snapshot of user interaction with the treasure
hunting environment is shown in Figure 1. Here,
the user?s eye fixation is represented by the white
dot and saccades (eye movements) are represented
by white lines. The virtual world contains 10 rooms
with a total of 155 unique objects that encompass 74
different object types (e.g. chair or plant).
Figure 1: Snapshot of the situated treasure hunting envi-
ronment
Table 1 shows a portion of a sample dialogue be-
tween a user and the expert. Each Si represents a
system utterance and each Ui represents a user ut-
terance. We focus on resolving exophoric referring
expressions, which are enclosed in brackets here. In
our dataset, an exophoric referring expression is a
non-pronominal noun phrase that refers to an en-
tity in the extralinguistic environment. It may be
an evoking reference that initially refers to a new
object in the virtual world (e.g. an axe in utter-
ance U2) or a subsequent reference to an entity in the
virtual world which has previously been mentioned
in the dialogue (e.g. an axe in utterance U3). In
our study we focus on resolving exophoric referring
expressions because they are tightly coupled with a
user?s eye gaze behavior.
From this study, we constructed a parallel spoken
utterance and eye gaze corpus. Utterances, which
S1 Describe what you?re doing.
U1 I just came out from the room that
I started and i see [one long sword]
U2 [one short sword] and [an axe]
S2 Compare these objects.
U3 one of them is long and one of them
is really short, and i see [an axe]
Table 1: A conversational fragment demonstrating inter-
action with exophoric referring expressions.
Utterance: i just came out from the room that
i started and i see [one long sword]
Ht : . . . i 5210 see 5410 [one 5630
long 6080 sword 6460]
H1: . . . icy 5210 winds 5630 along 6080
so 6460 words 68000
H2: . . . icy 5210 [wine 5630] along 6080
so 6460 words 6800
. . .
H25: . . . icy 5210 winds 5630
[long 6080 sword 6460]
. . .
Table 2: Sample n-best list of recognition hypotheses
are separated by a long pause (500 ms) in speech,
are automatically recognized using the Microsoft
Speech SDK. Gaze fixations are characterized by
objects in the virtual world that are fixated via a
user?s eye gaze. When a fixation points to multi-
ple spatially overlapping objects, only the one in the
forefront is deemed to be fixated. The data corpus
was transcribed and annotated with 2204 exophoric
referring expressions amongst 2052 utterances from
15 users.
4 Word Confusion Networks
For each user utterance in our dataset, an n-best list
(with n = 100) of recognition hypotheses ranked
in order of likelihood is produced by the Microsoft
Speech SDK. One way to use the speech recogni-
tion results (as in most speech applications) is to use
the top ranked recognition hypothesis. This may not
be the best solution because a large amount of infor-
mation is being ignored. Table 2 demonstrates this
problem. Here, the number after the underscore de-
notes a timestamp associated with each recognized
spoken word. The strings enclosed in brackets de-
473
note recognized referring expressions. In this exam-
ple, the manual transcription of the original utter-
ance is shown by Ht. In this case, the system must
first identify one long sword as a referring ex-
pression and then resolve it to the correct set of en-
tities in the virtual world. However, not until the
twenty fifth ranked recognition hypothesis H25, do
we see a referring expression closest to the actual ut-
tered referring expression. Moreover, in utterances
with multiple referring expressions, there may not
be a single recognition hypothesis that contains all
referring expressions, but each referring expression
may be contained in some recognition hypothesis.
Thus, it is desirable to consider the entire n-best list
of hypotheses.
To address this issue, we adopted the word con-
fusion network (WCN): a compact representation
of a word lattice or n-best list (Mangu et al,
2000). A WCN captures alternative word hypothe-
ses and their corresponding posterior probabilities
in time-ordered sets. In addition to being compact,
an important feature for efficient post-processing
of recognition hypotheses for real-time systems,
WCNs are capable of representing more competing
hypotheses than either n-best lists or word lattices.
Figure 2 shows an example of a WCN for the utter-
ance ?. . . I see one long sword? along with a timeline
(in milliseconds) depicting the eye gaze fixations to
potential referent objects that correspond to the ut-
terance. The confusion network shows competing
word hypotheses along with corresponding proba-
bilities in log scale.
Using our data set, we can show that word con-
fusion networks contain significantly more words
that can compose a referring expression than the top
recognition hypothesis. The confusion network key-
word error rate (KWER) is 0.192 compared to a 1-
best list KWER of 0.318, where a keyword is a word
that can be contained in a referring expression. The
overall WER for word confusion networks and 1-
best lists are 0.315 and 0.460, respectively. The re-
ported WCN word error rates are all oracle word er-
ror rates reflecting the best WER that can be attained
using any path in the confusion network. One more
important feature of word confusion networks is that
they provide time alignment for words that occur at
approximately the same time interval in competing
hypotheses. This is not only useful for efficient syn-
tactic parsing, which is necessary for identifying re-
ferring expressions, but also critical for integration
with time aligned gaze streams.
5 Reference Resolution Algorithm
We have developed an algorithm that combines an
n-best list of speech recognition hypotheses with di-
alogue, domain, and eye-gaze information to resolve
exophoric referring expressions. There are three in-
puts to the multimodal reference resolution algo-
rithm for each utterance: (1) an n-best list of alter-
native speech recognition hypotheses (n = 100 for a
WCN and n = 1 for the top recognized hypothesis),
(2) a list of fixated objects (by eye gaze) that tempo-
rally correspond to the spoken utterance and (3) a set
of potential referent objects. Since during the trea-
sure hunting task people typically only speak about
objects that are visible or have recently been visible
on the screen, an object is considered to be a poten-
tial referent if it is present within a close proximity
(in the same room) of the user while an utterance is
spoken.
The multimodal reference resolution algorithm
proceeds with the following four steps:
Step 1: construct word confusion network A
word confusion network is constructed out of the in-
put n-best list of alternative recognition hypotheses
with the SRI Language Modeling (SRILM) toolkit
(Stolcke, 2002) using the procedure described in
(Mangu et al, 2000). This procedure aligns words
from the n-best list into equivalence classes. First,
instances of the same word containing approxi-
mately the same starting and ending timestamps are
clustered. Then, equivalence classes with common
time ranges are merged. For each competing word
hypothesis its probability is computed by summing
the posteriors of all utterance hypotheses containing
this word. In our work, instead of using the actual
posterior probability of each utterance hypothesis
(which was not available), we assigned each utter-
ance hypothesis a probability based on its position
in the ranked list. Figure 2 depicts a portion of the
resulting word confusion network (showing compet-
ing word hypotheses and their probabilities in log
scale) constructed from the n-best list in Table 2.
474
Figure 2: Sample parallel speech and eye gaze data streams, including a portion of the sample WCN
Step 2: extract referring expressions from WCN
The word confusion network is syntactically parsed
using a modified version of the CYK (Cooke and
Schwartz, 1970; Kasami, 1965; Younger, 1967)
parsing algorithm that is capable of taking a word
confusion network as input rather than a single
string. We call this the CYK-WCN algorithm. To
do the parsing, we applied a set of grammar rules
largely derived from a different domain in our pre-
vious work (Prasov and Chai, 2008). A parse chart
of the sample word confusion network is shown in
Table 3. Here, just as in the CYK algorithm the
chart is filled in from left to right then bottom to
top. The difference is that the chart has an added
dimension for competing word hypotheses. This
is demonstrated in position 15 of the WCN, where
one and wine are two nouns that constitute com-
peting words. Note that some words from the con-
fusion network are not in the chart (e.g. winds)
because they are out of vocabulary. The result of
the syntactic parsing is that the parts of speech of
all sub-phrases in the confusion network are identi-
fied. Next, a set of all exophoric referring expres-
sions (i.e. non-pronominal noun phrases) found in
the word confusion network are extracted. Each re-
ferring expression has a corresponding confidence
score, which can be computed in many many dif-
ferent ways. Currently, we simply take the mean of
the probability scores of the expression?s constituent
words. The sample WCN has four such phrases
(shown in bold in Table 3): wine at position 15 with
length 1, one long sword at position 15 with
length 3, long sword at position 16 with length
2, and sword at position 17 with length 1.
Step 3: resolve referring expressions Each re-
ferring expression rj is resolved to the top k po-
tential referent objects according to the probabil-
ity P (oi|rj), where k is determined by information
from the linguistic expressions. P (oi|rj) is deter-
mined using the following expression:
P (oi|rj) =
AS(oi)? ? Compat(oi, rj)1??
?
i
AS(oi)
? ? Compat(oi, rj)
1??
(1)
In this equation,
? AS: Attentional salience score of a particu-
475
...
5
4
length 3 NP? NUM Adj-NP
2 Adj-NP? ADJ N,
NP? Adj-NP
1 (1) N? wine, NP? N ADJ? long N? sword,
(2) NUM? one NP? N
... 14 15 16 17 18 ...
WCN position
Table 3: Syntactic parsing of word confusion network
lar object oi, which is determined based on
the gaze fixation intensity of an object at the
start time of referring expression rj . The fix-
ation intensity of an object is defined as the
amount of time that the object is fixated during
a predefined time windowW (Prasov and Chai,
2008). As in (Prasov and Chai, 2008), we set
W = [?1500..0] ms relative to the beginning
of referring expression rj .
? Compat: Compatibility score, which specifies
whether the object oi is compatible with the in-
formation specified by the referring expression
rj . Currently, the compatibility score is set to 1
if referring expression rj and object oi have the
same object type (e.g. chair), and 0 otherwise.
? ?: Importance weight, in the range [0..1], of
attentional salience relative to compatibility.
A high ? value indicates that the attentional
salience score based on eye gaze carries more
weight in deciding referents, while a low ?
value indicates that compatibility carries more
weight. In this work, we set ? = 0.5 to indicate
equal weighting between attentional salience
and compatibility. If we do not want to inte-
grate eye gaze in reference resolution, we can
set ? = 0.0. In this case, reference resolution
will be purely based on compatibility between
visual objects and information specified via lin-
guistic expressions.
Once all probabilities are calculated, each refer-
ring expression is resolved to a set of referent ob-
jects. Finally, this results in a set of (referring ex-
pression, referent object set) pairs with confidence
scores, which are determined by two components.
The first component is the confidence score of the
referring expression, which is explained in the Step
1 of the algorithm. The second component is the
probability that the referent object set is indeed the
referent of this expression (which is determined by
Equation 1). There are various ways to combine
these two components together to form an overall
confidence score for the pair. Here we simply mul-
tiply the two components. The confidence score for
the pair is used in the following step to prune un-
likely referring expressions.
Step 4: post-prune The resulting set of (referring
expression, referent object set) pairs is pruned to re-
move pairs that fall under one of the following two
conditions: (1) the pair has a confidence score equal
to or below a predefined threshold  (currently, the
threshold is set to 0 and thus keeps all resolved pairs)
and (2) the pair temporally overlaps with a higher
confidence pair. For example, in Table 3, the re-
ferring expressions one long sword and wine
overlap in position 15. Finally, the resulting (refer-
ring expression, referent object set) pairs are sorted
in ascending order according to their constituent re-
ferring expression timestamps.
6 Experimental Results
Using our data, described in Section 3, we applied
the multimodal reference resolution algorithm de-
scribed in Section 5. All of the data is used to
report the experimental results. Reference resolu-
tion model parameters are set based on our prior
work in a different domain (Prasov and Chai, 2008).
For each utterance we compare the reference reso-
lution performance with and without the integration
of eye gaze information. We also evaluate using a
476
word confusion network compared to a 1-best list to
model speech recognition hypotheses. For perspec-
tive, reference resolution with recognized speech in-
put is compared with transcribed speech.
6.1 Evaluation Metrics
The reference resolution algorithm outputs a list of
(referring expression, referent object set) pairs for
each utterance. We evaluate the algorithm by com-
paring the generated pairs to the annotated ?gold
standard? pairs using F-measure. We perform the
following two types of evaluation:
? Lenient Evaluation: Due to speech recognition
errors, there are many cases in which the al-
gorithm may not return a referring expression
that exactly matches the gold standard refer-
ring expression. It may only match based on
the object type. For example, the expressions
one long sword and sword are different,
but they match in terms of the intended object
type. For applications in which it is critical to
identify the objects referred to by the user, pre-
cisely identifying uttered referring expressions
may be unnecessary. Thus, we evaluate the ref-
erence resolution algorithm with a lenient com-
parison of (referring expression, referent object
set) pairs. In this case, two pairs are considered
a match if at least the object types specified via
the referring expressions match each other and
the referent object sets are identical.
? Strict Evaluation: For some applications it may
be important to identify exact referring ex-
pressions in addition to the objects they re-
fer to. This is important for applications that
attempt to learn a relationship between refer-
ring expressions and referenced objects. For
example, in automated vocabulary acquisition,
words other than object types must be identi-
fied so the system can learn to associate these
words with referenced objects. Similarly, in
systems that apply priming for language gen-
eration, identification of the exact referring ex-
pressions from human users could be impor-
tant. Thus, we also evaluate the reference reso-
lution algorithm with a strict comparison of (re-
ferring expression, referent object set) pairs. In
this case, a referring expression from the sys-
tem output needs to exactly match the corre-
sponding expression from the gold standard.
6.2 Role of Eye Gaze
We evaluate the effect of incorporating eye gaze
information into the reference resolution algorithm
using the top best recognition hypothesis (1-best),
the word confusion network (WCN), and the man-
ual speech transcription (Transcription). Speech
transcription, which contains no recognition errors,
demonstrates the upper bound performance of our
approach. When no gaze information is used, ref-
erence resolution solely depends on linguistic and
semantic processing of referring expressions. Table
4 shows the lenient reference resolution evaluation
using F-measure. This table demonstrates that le-
nient reference resolution is improved by incorpo-
rating eye gaze information. This effect is statisti-
cally significant in the case of transcription and 1-
best (p < 0.0001 and p < 0.009, respectively) and
marginal (p < 0.07) in the case of WCN.
Configuration Without Gaze With Gaze
Transcription 0.619 0.676
WCN 0.524 0.552
1-best 0.471 0.514
Table 4: Lenient F-measure Evaluation
Configuration Without Gaze With Gaze
Transcription 0.584 0.627
WCN 0.309 0.333
1-best 0.039 0.035
Table 5: Strict F-measure Evaluation
Table 5 shows the strict reference resolution eval-
uation using F-measure. As can be seen in the ta-
ble, incorporating eye gaze information significantly
(p < 0.0024) improves reference resolution per-
formance when using transcription and marginally
(p < 0.113) in the case of WCN optimized for strict
evaluation. However there is no difference for the 1-
best hypotheses which result in extremely low per-
formance. This observation is not surprising since 1-
best hypotheses are quite error prone and less likely
to produce the exact expressions.
477
Since eye gaze can be used to direct navigation in
a mobile environment as in situated dialogue, there
could be situations where eye gaze does not reflect
the content of the corresponding speech. In such
situations, integrating eye gaze in reference reso-
lution could be detrimental. To further understand
the role of eye gaze in reference resolution, we ap-
plied our reference resolution algorithm only to ut-
terances where speech and eye gaze are considered
closely coupled (i.e., eye gaze reflects the content of
speech). More specifically, following the previous
work (Qu and Chai, 2010), we define a closely cou-
pled utterance as one in which at least one noun or
adjective describes an object that has been fixated by
the corresponding gaze stream.
Table 6 and Table 7 show the performance based
on closely coupled utterances using lenient and strict
evaluation, respectively. In the lenient evaluation,
reference resolution performance is significantly im-
proved for all input configurations when eye gaze
information is incorporated (p < 0.0001 for tran-
scription, p < 0.015 for WCN, and p < 0.0022 for
1-best). In each case the closely coupled utterances
achieve higher performance than the entire set of ut-
terances evaluated in Table 5. Aside from the 1-best
case, the same is true when using strict evaluation
(p < 0.0006 for transcription and p < 0.046 for
WCN optimized for strict evaluation). This observa-
tion indicates that in situated dialogue, some mech-
anism to predict whether a gaze stream is closely
coupled with the corresponding speech content can
be beneficial in further improving reference resolu-
tion performance.
Configuration Without Gaze With Gaze
Transcription 0.616 0.700
WCN 0.523 0.570
1-best 0.473 0.537
Table 6: Lenient F-measure Evaluation for Closely Cou-
pled Utterances
6.3 Role of Word Confusion Network
The effect of incorporating eye gaze with WCNs
rather than 1-best recognition hypotheses into ref-
erence resolution can also be seen in Tables 4 and
5. Table 4 shows a significant improvement when
using WCNs rather than 1-best hypotheses for both
Configuration Without Gaze With Gaze
Transcription 0.579 0.644
WCN 0.307 0.345
1-best 0.045 0.038
Table 7: Strict F-measure Evaluation for Closely Coupled
Utterances
with (p < 0.015) and without (p < 0.0012) eye
gaze configurations. Similarly, Table 5 shows a sig-
nificant improvement in strict evaluation when us-
ing WCNs rather than 1-best hypotheses for both
with (p < 0.0001) and without (p < 0.0001) eye
gaze configurations. These results indicate that us-
ing word confusion networks improves both lenient
and strict reference resolution. This observation is
not surprising since identifying correct linguistic ex-
pressions will enable better search for semantically
matching referent objects.
Although WCNs lead to better performance, uti-
lizing WCNs is more computationally expensive
compared to 1-best recognition hypotheses. Never-
theless, in practice, WCN depth, which specifies the
maximum number of competing word hypotheses in
any position of the word confusion network, can be
limited to a certain value |d|. For example, in Figure
2 the depth of the shown WCN is 8 (there are 8 com-
peting word hypotheses in position 17 of the WCN).
The WCN depth can be limited by pruning word al-
ternatives with low probabilities until, at most, the
top |d| words remain in each position of the WCN.
It is interesting to observe how limiting WCN depth
can affect reference resolution performance. Figure
3 demonstrates this observation. In this figure the
resolution performance (in terms of lenient evalua-
tion) for WCNs of varying depth is shown as dashed
lines for with and without eye gaze configurations.
As a reference point, the performance when utiliz-
ing 1-best recognition hypotheses is shown as solid
lines. It can be seen that as the depth increases, the
performance also increases until the depth reaches 8.
After that, there is no performance improvement.
7 Discussion
In Section 6.2 we have shown that incorporating
eye gaze information improves reference resolu-
tion performance. Eye-gaze information is particu-
478
Figure 3: Lenient F-measure at each WCN Depth
larly helpful for resolving referring expressions that
are ambiguous from the perspective of the artificial
agent. Consider a scenario where the user utters a
referring expression that has an equivalent seman-
tic compatibility with multiple potential referent ob-
jects. For example, in a room with multiple books,
the user utters ?the open book to the right?, but only
the phrase ?the book? is recognized by the ASR. If
a particular book is fixated during interaction, there
is a high probability that it is indeed being referred
to by the user. Without eye gaze information, the se-
mantic compatibility alone could be insufficient to
resolve this referring expression. Thus, when eye
gaze information is incorporated, the main source of
performance improvement comes from better iden-
tification of potential referent objects.
In Section 6.3 we have shown that incorporating
multiple speech recognition hypotheses in the form
of a word confusion network further improves ref-
erence resolution performance. This is especially
true when exact referring expression identification
is required (F-measure of 0.309 from WCNs com-
pared to F-measure of 0.039 from 1-best hypothe-
ses). Using a WCN improves identification of low-
probability referring expressions. Consider a sce-
nario where the top recognition hypothesis of an ut-
terance contains no referring expressions or an in-
correct referring expression that has no semantically
compatible potential referent objects. If a referring
expression with a high compatibility value to some
potential referent object is present in a lower proba-
bility hypothesis, this referring expression can only
be identified when a WCN rather than a 1-best hy-
pothesis is utilized. Thus, when word confusion net-
works are incorporated, the main source of perfor-
mance improvement comes from better referring ex-
pression identification.
7.1 Computational Complexity
One potential concern of using word confusion net-
works rather than 1-best hypotheses is that they are
more computationally expensive to process. The
asymptotic computational complexity for resolving
the referring expressions using the algorithm pre-
sented in this work with a WCN is the summa-
tion of three components: (1) O(|G| ? |d|2 ? |w|3)
for confusion network construction and parsing, (2)
O(|r|?|O|?log(|O|)) for reference resolution, and (3)
O(|r|2) for selection of (referring expression, ref-
erent object set) pairs. Here, |w| is the number of
words in the input speech signal (or, more precisely,
the number of words in the longest ASR hypothesis
for a given utterance); |G| is the size of the parsing
grammar; |d| is the depth of the constructed word
confusion network; |O| is the number of potential
referent objects for each utterance; and |r| is the
number of referring expressions that are extracted
from the word confusion network.
The complexity is dominated by the word confu-
sion network construction and parsing. Also, both
the number of words in an input utterance ASR hy-
pothesis |w| and the number of referring expressions
in a word confusion network |r| are dependent on ut-
terance length. In our study, interactive dialogue is
encouraged and, thus, utterances are typically short;
with a mean length of 6.41 words and standard de-
viation of 4.35 words. The longest utterances in our
data set has 31 words. WCN depth |d| has a mean of
10.1, a standard deviation of 8.1, and a maximum 89
words. In practice, as shown in Section 6.3, limiting
|d| to 8 words achieves comparable reference resolu-
tion results as using a full word confusion network.
To demonstrate the applicability of our reference
resolution algorithm for real-time processing, we ap-
plied it on the data corpus presented in Section 3.
This corpus contains utterances with a mean input
time of 2927.5 ms and standard deviation of 1903.8
ms. On a 2.4 GHz AMD Athlon(tm) 64 X2 Dual
Core Processor, the runtimes resulted in a real time
factor of 0.0153 on average. Thus, on average, an
utterance from this corpus can be processed in just
under 45 ms, which is well within the range of ac-
479
ceptable real-time performance.
7.2 Error Analysis
As can be seen in Section 6, even when using tran-
scribed data, reference resolution performance still
has room for improvement (achieving the highest le-
nient F-measure of 0.700 when eye gaze is utilized
for resolving closely coupled utterances). In this
section, we elaborate on the potential error sources.
Specifically, we discuss two types of error: (1) a re-
ferring expression is incorrectly recognized or (2) a
recognized referring expression is not resolved to a
correct referent object set.
Given transcribed data, which simulates per-
fectly recognized utterances, all referring expression
recognition errors arise due to incorrect language
processing. Most of these errors occur because an
incorrect part of speech (POS) tag is assigned to a
word, or an out-of-vocabulary (OOV) word is en-
countered, or the parsing grammar has insufficient
coverage. A particularly interesting parsing prob-
lem occurs due to the nature of spoken language.
Since punctuation is sometimes unavailable, given
an utterance with several consecutive nouns, it is un-
clear which of these nouns should be treated as head
nouns and which should be treated as noun modi-
fiers. For example, in the utterance ?there is a desk
lamp table and two chairs? it is unclear if the itali-
cized expression should be parsed as a single phrase
or as a list of (two) phrases a desk and lamp.
Thus, some timing information should be used for
disambiguation.
Object set identification errors are more prevalent
than referring expression recognition errors. The
majority of these errors occur because a referring
expression is ambiguous from the perspective of the
conversational system and there is not enough in-
formation to choose amongst multiple potential ref-
erent objects due to limited speech recognition and
domain modeling. One reason for this is that a re-
ferring expression may be resolved to an incorrect
number of referent objects. Another reason is that a
pertinent object attribute or a distinguishing spatial
relationship between objects specified by the user
cannot be established by the system. For example,
during the utterance ?I see a vase left of the table?
there are two vases visible on the screen creating an
ambiguity if the phrase left of is not processed
correctly. This is caused by an inadequate repre-
sentation of spatial relationships and processing of
spatial language. One more reason for potential am-
biguity is the lack of pragmatic knowledge that can
support adequate inference. For example, when the
user refers to two sofa objects using the phrase ?an
armchair and a sofa?, the system lacks pragmatic
knowledge to indicate that arm chair refers to
the smaller of the two objects. Some of these errors
can be avoided when eye gaze information is avail-
able to the system. However, due to the noisy nature
of eye gaze data, many such referring expressions
remain ambiguous even when eye gaze information
is considered.
8 Conclusion
In this work, we have examined the utility of eye
gaze and word confusion networks for reference res-
olution in situated dialogue within a virtual world.
Our empirical results indicate that incorporating
eye gaze information with recognition hypotheses
is beneficial for the reference resolution task com-
pared to only using recognition hypotheses. Further-
more, using a word confusion network rather than
the top best recognition hypothesis further improves
reference resolution performance. Our findings also
demonstrate that the processing speed necessary to
integrate word confusion networks with eye gaze
information is well within the acceptable range for
real-time applications.
Acknowledgments
This work was supported by IIS-0347548 and IIS-
0535112 from the National Science Foundation. We
would like to thank anonymous reviewers for their
valuable comments and suggestions.
References
N. Bee, E. Andre?, and S. Tober. 2009. Breaking the
ice in human-agent communication: Eye-gaze based
initiation of contact with an embodied conversational
agent. In Proceedings of the 9th International Con-
ference on Intelligent Virtual Agents (IVA?09), pages
229?242. Springer.
T. Bickmore and J. Cassell, 2004. Social Dialogue with
Embodied Conversational Agents, chapter Natural, In-
480
telligent and Effective Interaction with Multimodal Di-
alogue Systems. Kluwer Academic.
D. K. Byron, T. Mampilly, and T. Sharma, V.and Xu.
2005. Utilizing visual attention for cross-modal coref-
erence interpretation. In Spring Lecture Notes in Com-
puter Science: Proceedings of CONTEXT-05, pages
83?96.
N. J. Cooke and M. Russell. 2008. Gaze-contingent au-
tomatic speech recognition. IET Signal Processing,
2(4):369?380, December.
J. Cooke and J. T. Schwartz. 1970. Programming lan-
guages and their compilers: Preliminary notes. Tech-
nical report, Courant Institute of Mathematical Sci-
ence.
R. Fang, J. Y. Chai, and F. Ferreira. 2009. Between lin-
guistic attention and gaze fixations in multimodal con-
versational interfaces. In The 11th International Con-
ference on Multimodal Interfaces (ICMI).
P. Gorniak, J. Orkin, and D. Roy. 2006. Speech, space
and purpose: Situated language understanding in com-
puter games. In Twenty-eighth Annual Meeting of
the Cognitive Science Society Workshop on Computer
Games.
Z. M. Griffin and K. Bock. 2000. What the eyes say
about speaking. In Psychological Science, volume 11,
pages 274?279.
D. Hakkani-Tu?r, F. Be?chet, G. Riccardi, and G. Tur.
2006. Beyond asr 1-best: Using word confusion net-
works in spoken language understanding. Computer
Speech and Language, 20(4):495?514.
M. A. Just and P. A. Carpenter. 1976. Eye fixations and
cognitive processes. In Cognitive Psychology, vol-
ume 8, pages 441?480.
T. Kasami. 1965. An efficient recognition and syntax-
analysis algorithm for context-free languages. Scien-
tific report AFCRL-65-758, Air Force Cambridge Re-
search Laboratory, Bedford, Massachusetts.
J. Kelleher and J. van Genabith. 2004. Visual salience
and reference resolution in simulated 3-d environ-
ments. Artificial Intelligence Review, 21(3).
Y. Liu, J. Y. Chai, and R. Jin. 2007. Automated vo-
cabulary acquisition and interpretation in multimodal
conversational systems. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL).
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding con-
sensus in speech recognition: word error minimization
and other applications of confusion networks. Com-
puter Speech and Language, 14(4):373?400.
A. S. Meyer and W. J. M. Levelt. 1998. Viewing and
naming objects: Eye movements during noun phrase
production. In Cognition, volume 66, pages B25?B33.
L.-P. Morency, C. M. Christoudias, and T. Darrell. 2006.
Recognizing gaze aversion gestures in embodied con-
versational discourse. In International Conference on
Multimodal Interfaces (ICMI).
Y. I. Nakano, G. Reinstein, T. Stocky, and J. Cassell.
2003. Towards a model of face-to-face grounding. In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL?03), pages
553?561.
Z. Prasov and J. Y. Chai. 2008. What?s in a gaze? the
role of eye-gaze in reference resolution in multimodal
conversational interfaces. In Proceedings of 13th In-
ternational Conference on Intelligent User interfaces
(IUI), pages 20?29.
S. Qu and J. Y. Chai. 2007. An exploration of eye gaze in
spoken language processing for multimodal conversa-
tional interfaces. In Proceedings of the Conference of
the North America Chapter of the Association of Com-
putational Linguistics (NAACL).
S. Qu and J. Y. Chai. 2010. Context-based word acquisi-
tion for situated dialogue in a virtual world. Journal of
Artificial Intelligence Research, 37:347?377, March.
P. Qvarfordt and S. Zhai. 2005. Conversing with the
user based on eye-gaze patterns. In Proceedings Of the
Conference on Human Factors in Computing Systems.
ACM.
C. L. Sidner, C. D. Kidd, C. Lee, and N. Lesh. 2004.
Where to look: A study of human-robot engagement.
In Proceedings of the 9th international conference
on Intelligent User Interfaces (IUI?04), pages 78?84.
ACM Press.
A. Stolcke. 2002. SRILM an extensible language model-
ing toolkit, confusion network. In International Con-
ference on Spoken Language Processing.
M. K. Tanenhous, M. Spivey-Knowlton, E. Eberhard, and
J. Sedivy. 1995. Integration of visual and linguistic
information during spoken language comprehension.
In Science, volume 268, pages 1632?1634.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
481
