Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1161?1165,
Prague, June 2007. c?2007 Association for Computational Linguistics
Pro3Gres Parser in the CoNLL Domain Adaptation Shared Task
Gerold Schneider and Kaarel Kaljurand and Fabio Rinaldi and Tobias Kuhn
Institute of Computational Linguistics, University of Zurich
Binzmu?hlestrasse 14
CH - 8050 Zurich, Switzerland
{gschneid,kalju,rinaldi,tkuhn}@ifi.uzh.ch
Abstract
We present Pro3Gres, a deep-syntactic, fast
dependency parser that combines a hand-
written competence grammar with proba-
bilistic performance disambiguation and that
has been used in the biomedical domain. We
discuss its performance in the domain adap-
tation open submission. We achieve aver-
age results, which is partly due to difficulties
in mapping to the dependency representation
used for the shared task.
1 Introduction
The Pro3Gres parser is a dependency parser that
combines a hand-written grammar with probabilis-
tic disambiguation. It is described in detail in
(Schneider, 2007). It uses tagger and chunker
pre-processors ? parsing proper happens only be-
tween heads of chunks ? and a post-processor graph
converter to capture long-distance dependencies.
Pro3Gres is embedded in a flexible XML pipeline.
It has been applied to many tasks, such as parsing
biomedical literature (Rinaldi et al, 2006; Rinaldi
et al, 2007) and the whole British National Cor-
pus, and has been evaluated in several ways. We
have achieved average results in the CoNLL do-
main adaptation track open submission (Marcus et
al., 1993; Johansson and Nugues, 2007; Kulick et
al., 2004; MacWhinney, 2000; Brown, 1973). The
performance of the parser is seriously affected by
mapping problems to the particular dependency rep-
resentation used in the shared task.
The paper is structured as follows. We give a brief
overview of the parser and its design policy in sec-
tion 2, we describe the domain adaptations that we
have used in section 3, comment on the results ob-
tained in section 4 and conclude in section 5.
2 Pro3Gres and its Design Policy
There has been growing interest in exploring the
space between Treebank-trained probabilistic gram-
mars (e.g. (Collins, 1999; Nivre, 2006)) and formal
grammar-based parsers integrating statistics (e.g.
(Miyao et al, 2005; Riezler et al, 2002)). We
have developed a parsing system that explores this
space, in the vein of systems like (Kaplan et al,
2004), using a linguistic competence grammar and
a probabilistic performance disambiguation allow-
ing us to explore interactions between lexicon and
grammar (Sinclair, 1996). The parser has been ex-
plicitly designed to be deep-syntactic like a formal
grammar-based parser, by using a dependency rep-
resentation that is close to LFG f-structure, but at
the same time mostly context-free and integrating
shallow approaches and aggressive pruning in or-
der to keep search-spaces small, without permitting
compromise on performance or linguistic adequacy.
(Abney, 1995) establishes the chunks and dependen-
cies model as a well-motivated linguistic theory. The
non-local linguistic constraints that a hand-written
grammar allows us to formulate, e.g. expressing
X-bar principles or barring very marked construc-
tions, further reduce parsing time by at least an order
of magnitude. Since the grammar is on Penn tags
(except for few closed classed words, e.g. allow-
ing including to function as preposition) the effort
for writing it manually is manageable. It has been
developed from scratch in about a person month,
1161
Figure 1: Pro3Gres parser flowchart
using traditional grammar engineering development
cycles. It contains about 1000 rules, the number is
largely so high due to tag combinatorics: for ex-
ample, the various subject attachment rules combin-
ing a subject ( NN, NNS, NNP, NNPS) and a verb
( VBZ, VBP, VBG, VBN, VBD) are all very simi-
lar.
The parser is fast enough for large-scale appli-
cation to unrestricted texts, and it delivers depen-
dency relations which are a suitable base for a
range of applications. We have used it to parse the
entire 100 million words British National Corpus
(http://www.natcorp.ox.ac.uk) and similar amounts
of biomedical texts. Its parsing speed is about
500,000 words per hour. The flowchart of the parser
can be seen in figure 1.
Pro3Gres (PRObabilistic PROlog-implemented
RObust Grammatical Role Extraction System) uses
a dependency representation that is close to LFG
f-structure, in order to give it an established lin-
guistic background. It uses post-processing graph
structure conversions and mild context-sensitivity to
capture long-distance dependencies. We have ar-
gued in (Schneider, 2005) that LFG f-structures can
be parsed for in a completely context-free fashion,
except for embedded WH-questions, where a de-
vice such as functional uncertainty (Kaplan and Za-
enen, 1989) or the equivalent Tree-Adjoining Gram-
mar Adjoining operation (Joshi and Vijay-Shanker,
1989) is used. In Dependency Grammar, this device
is also known as lifting (Kahane et al, 1998; Nivre
and Nilsson, 2005).
We use a hand-written competence grammar,
combined with performance-driven disambiguation
obtained from the Penn Treebank (Marcus et
al., 1993). The Maximum-Likelihood Estimation
(MLE) probability of generating a dependency re-
lation R given lexical heads (a and b) at distance (in
chunks) ? is calculated as follows.
p(R, ?|a, b) ?= p(R|a, b) ? p(?|R) =
#(R, a, b)
?n
i=1#(Ri, a, b)
?
#(R, ?)
#R
The counts are backed off (Collins, 1999; Merlo
and Esteve Ferrer, 2006). The backoff levels include
semantic classes from WordNet (Fellbaum, 1998):
we back off to the lexicographer file ID of the most
frequent word sense. An example output of the
parser is shown in figure 2.
3 Domain Adaptation
Based on our experience with parsing texts form the
biomedical domain, we have used the following two
adaptations to the domain of chemistry.
(Hindle and Rooth, 1993) exploit the fact that in
sentence-initial NP PP sequences the PP unambigu-
ously attaches to the noun. We have observed that in
sentence-initial NP PP PP sequences, also the sec-
ond PP frequently attaches to the noun, the noun
itself often being a relational noun. We have thus
used such sequences to learn relational nouns from
the unlabelled domain texts. Relational nouns are
allowed to attach several argument PPs in the gram-
mar, all other nouns are not.
Multi-word terms, adjective-preposition construc-
tions and frequent PP-arguments have strong collo-
cational force. We have thus used the collocation
extraction tool XTRACT (Smadja, 2003) to discover
collocations from large domain corpora. The prob-
ability of generating a dependency relation is aug-
mented for collocations above a certain threshold.
Since the tagging quality of the Chemistry testset
is high, the impact of multi-word term recognition
was lower than the biomedical domain when using a
standard tagger, as we have shown in (Rinaldi et al,
2007).
For the CHILDES domain, we have not used any
adaptation. The hand-written grammar fares quite
well on most types of questions, which are very fre-
quent in this domain. In the spirit of the shared
task, we have not attempted to correct tagging errors,
which were frequent in the CHILDES domain. We
have restricted the use of external resources to the
hand-written, domain-independent grammar, and to
WordNet. Due to serious problems in mapping our
1162
Figure 2: Example of original parser output
LFG f-structure based dependencies to the CoNLL
representation, much less time than expected was
available for the domain adaptation.
4 Our Results
We have achieved average results: Labeled attach-
ment score: 3151 / 5001 * 100 = 63.01, unlabeled at-
tachment score: 3327 / 5001 * 100 = 66.53, label ac-
curacy score: 3832 / 5001 * 100 = 76.62. These re-
sults are about 10 % below what we typically obtain
when using our own dependency representation or
GREVAL (Carroll et al, 2003), a deep-syntactic an-
notation scheme that is close to ours. Detailed eval-
uations are reported in (Schneider, 2007). Our map-
ping was quite poor, especially when conjunctions
are involved. Also punctuation is attached poorly.
5.7 % of all dependencies remained unmapped (un-
known in the figure). We give an overview of the the
relation-dependent results in figures 1 and 2.
Mapping problems include the following exam-
ples. First, headedness is handled very differently:
while we assume auxiliaries, prepositions and co-
ordinations to be dependents, the CoNNL repre-
sentation assumes the opposite, which leads to in-
correct mapping under complex interactions. Sec-
ond, the semantics of parentheticals (PRN) partly
remains unclear. In Quinidine elimination was
capacity limited with apparent Michaelis constant
(appKM) of 2.6 microM (about 1.2 mg/L) the gold
standard annotates the second parenthesis as paren-
thetical, but the first as nominal modification, al-
though both may be said to have appositional char-
acter. Third, we seem to have misinterpreted the
roles of ADV and AMOD, as they are often mutu-
ally exchanged. Fourth, the logical subject (LGS)
is sometimes marked on the by-PP (... are strongly
inhibited by-LGS carbon monoxide) and sometimes
on the participle (... are increased-LGS by pre-
deprel gold correct system recall (%) prec. (%)
ADV 366 212 302 57.92 70.20
AMOD 87 8 87 9.20 9.20
CC 11 0 0 0.00 NaN
COORD 402 233 342 57.96 68.13
DEP 9 0 0 0.00 NaN
EXP 2 0 0 0.00 NaN
GAP 14 0 0 0.00 NaN
IOBJ 3 0 0 0.00 NaN
LGS 37 0 0 0.00 NaN
NMOD 1813 1576 1763 86.93 89.39
OBJ 185 146 208 78.92 70.19
P 587 524 525 89.27 99.81
PMOD 681 533 648 78.27 82.25
PRN 34 13 68 38.24 19.12
ROOT 195 138 190 70.77 72.63
SBJ 279 217 296 77.78 73.31
VC 129 116 136 89.92 85.29
VMOD 167 116 149 69.46 77.85
unknown 0 0 287 NaN 0.00
Table 1: Prec.&recall of DEPREL
treatment) in the gold standard. Relations between
heads of chunks, which are central for predicate-
argument structures which Pro3Gres aims to re-
cover, such as SBJ, NMOD, ROOT, perform better
than those for which Pro3Gres was not originally
designed, particularly ADV, AMOD, PRN, P. Perfor-
mance on COORD was particularly disappointing.
Generally, mapping problems between different rep-
resentations would be smaller if one used a depen-
dency representation that maximally abstracts away
from form to function, for example (Carroll et al,
2003).
We have obtained results slightly above average
on the CHILDES domain, although we did not adapt
the parser to this domain in any way (unlabeled at-
tachment score: 3013 / 4999 * 100 = 60.27 %).
The hand-written grammar, which includes rules for
most types of questions, fares relatively well on this
domain since questions are rare in the Penn Tree-
bank (see (Hermjakob, 2001)). Pro3Gres has been
employed for question parsing at a TREC confer-
ence (Burger and Bayer, 2005).
1163
deprel gold correct system recall (%) prec. (%)
ADV 366 161 302 43.99 53.31
AMOD 87 5 87 5.75 5.75
CC 11 0 0 0.00 NaN
COORD 402 170 342 42.29 49.71
DEP 9 0 0 0.00 NaN
EXP 2 0 0 0.00 NaN
GAP 14 0 0 0.00 NaN
IOBJ 3 0 0 0.00 NaN
LGS 37 0 0 0.00 NaN
NMOD 1813 1392 1763 76.78 78.96
OBJ 185 140 208 75.68 67.31
P 587 221 525 37.65 42.10
PMOD 681 521 648 76.51 80.40
PRN 34 12 68 35.29 17.65
ROOT 195 138 190 70.77 72.63
SBJ 279 190 296 68.10 64.19
VC 129 116 136 89.92 85.29
VMOD 167 85 149 50.90 57.05
unknown 0 0 287 NaN 0.00
Table 2: Prec.&recall of DEPREL+ATTACHMENT
5 Conclusion
We have described the Pro3Gres parser. We have
achieved average results in the shared task with rel-
atively little adaptation. Mapping to different repre-
sentations is an often underestimated task. Our per-
formance on the CHILDES task, where we did not
adapt the parser, indicates that hand-written, care-
fully engineered competence grammars may be rel-
atively domain-independent while performance dis-
ambiguation is more domain-dependent. We will
adapt the parser to further domains and include more
unsupervised learning methods.
References
Steven Abney. 1995. Chunks and dependencies: Bring-
ing processing evidence to bear on syntax. In Jennifer
Cole, Georgia Green, and Jerry Morgan, editors, Com-
putational Linguistics and the Foundations of Linguis-
tic Theory, pages 145?164. CSLI.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
John D. Burger and Sam Bayer. 2005. MITRE?s Qanda
at TREC-14. In E. M. Voorhees and Lori P. Buck-
land, editors, The Fourteenth Text REtrieval Confer-
ence (TREC 2005) Notebook.
John Carroll, Guido Minnen, and Edward Briscoe. 2003.
Parser evaluation: using a grammatical relation anno-
tation scheme. In Anne Abeille?, editor, Treebanks:
Building and Using Parsed Corpora, pages 299?316.
Kluwer, Dordrecht.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, PA.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Ulf Hermjakob. 2001. Parsing and question classifica-
tion for question answering. In Proceedings of the
ACL 2001 Workshop on Open-Domain Question An-
swering, Toulouse, France.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19:103?120.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Aravind K. Joshi and K. Vijay-Shanker. 1989. Treat-
ment of long-distance dependencies in LFG and TAG:
Functional uncertainty in LFG is a corollary in TAG.
In Proceedings of ACL ?89.
Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In Proceedings of
COLINGACL, volume 1, pages 646?652, Montreal.
Ronald Kaplan and Annie Zaenen. 1989. Long-distance
dependencies, constituent structure, and functional un-
certainty. In Mark Baltin and Anthony Kroch, editors,
Alternative Concepts of Phrase Structrue, pages 17 ?
42. Chicago University Press.
Ron Kaplan, Stefan Riezler, Tracy H. King, John
T. Maxwell III, Alex Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Proceedings of HLT/NAACL
2004, Boston, MA.
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
Paola Merlo and Eva Esteve Ferrer. 2006. The notion of
argument in PP attachment. Computational Linguis-
tics, 32(2):341 ? 378.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2005. Corpus-oriented grammar development for ac-
quiring a Head-driven Phrase Structure Grammar from
1164
the Penn Treebank. In Keh-Yih Su, Jun?ichi Tsujii,
Jong-Hyeok Lee, and Oi Yee Kwong, editors, Natural
Language Processing - IJCNLP 2004, pages 684?693.
Springer.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 99?106, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Joakim Nivre. 2006. Constraints on non-projective de-
pendency parsing. In Proceedings of the European
Chapter of the Association of Computational Linguis-
tics (EACL) 2006, pages 73 ? 80, Trento, Italy. Asso-
ciation for Computational Linguistics.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?02), Philadephia, PA.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand,
Michael Hess, and Martin Romacker. 2006. . an en-
vironment for relation mining over richly annotated
corpora: the case of GENIA. BMC Bioinformatics,
7(Suppl 3):S3.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand,
Michael Hess, Christos Andronis, Ourania Konstanti,
and Andreas Persidis. 2007. Mining of functional
relations between genes and proteins over biomedical
scientific literature using a deep-linguistic approach.
Journal of Artificial Intelligence in Medicine, 39:127
? 136.
Gerold Schneider. 2005. A broad-coverage, representa-
tionally minimal LFG parser: chunks and F-structures
are sufficient. In Mriram Butt and Traci Holloway
King, editors, The 10th international LFG Conference
(LFG 2005), Bergen, Norway. CSLI.
Gerold Schneider. 2007. Hybrid Long-Distance Func-
tional Dependency Parsing. Doctoral Thesis, Institute
of Computational Linguistics, University of Zurich.
accepted for publication.
John Sinclair. 1996. The empty lexicon. International
Journal of Corpus Linguistics, 1, 1996.
Frank Smadja. 2003. Retrieving collocations from text:
Xtract. Computational Linguistics, 19:1, Special issue
on using large corpora:143?177.
1165
Exploiting Paraphrases in a Question Answering System
Fabio Rinaldi, James Dowdall,
Kaarel Kaljurand, Michael Hess
Institute of Computational Linguistics,
University of Zu?rich
Winterthurerstrasse 190
CH-8057 Zu?rich, Switzerland
{rinaldi,dowdall,kalju,hess}
@ifi.unizh.ch
Diego Molla?
Centre for Language Technology,
Macquarie University,
Sydney NSW 2109, Australia
{diego}@ics.mq.edu.au
Abstract
We present a Question Answering system
for technical domains which makes an in-
telligent use of paraphrases to increase the
likelihood of finding the answer to the user?s
question. The system implements a simple
and efficient logic representation of ques-
tions and answers that maps paraphrases
to the same underlying semantic represen-
tation. Further, paraphrases of technical
terminology are dealt with by a separate
process that detects surface variants.
1 Introduction
The problem of paraphrases conceals a number of
different linguistic problems, which in our opinion
need to be treated in separate ways. In fact, para-
phrases can happen at various levels in language. Us-
ing the examples provided in the call for papers for
this workshop, we would like to attempt a simple
classification, without any pretense of being exhaus-
tive:
1. Lexical synonymy.
Example: article, paper, publication
2. Morpho-syntactic variants.
a) Oswald killed Kennedy. / Kennedy was killed
by Oswald.
b) Edison invented the light bulb. / Edison?s
invention of the light bulb.
while (a) is purely syntactical (active vs pas-
sive), (b) involves a nominalisation.
3. PP-attachment.
a plant in Alabama / the Alabama plant
4. Comparatives vs superlatives.
be better than anybody else / be the best
5. Subordinate clauses vs separate sentences linked
by anaphoric pronouns.
The tree healed its wounds by growing new bark.
/ The tree healed its wounds. It grew new bark.
6. Inference.
The stapler costs $10. / The price of the stapler
is $10.
Where is Thimphu located? / Thimphu is capi-
tal of what country?
Of course combinations of the different types are
possible, e.g. Oswald killed Kennedy / Kennedy
was assassinated by Oswald is a combination of (1)
and (2).
Different types of knowledge and different linguis-
tic resources are needed to deal with each of the
above types. While type (1) can be dealt with us-
ing a resource such as WordNet (Fellbaum, 1998),
type (2) needs effective parsing and mapping of syn-
tactic structures into a common deeper structure,
possibly using a repository of nominalisations like
NOMLEX (Meyers et al, 1998). More complex
approaches are needed for the other types, up to
type (6) where generic world knowledge is required,
for instance to know that being a capital of a country
implies being located in that country. 1 Such world
knowledge could be expressed in the form of axioms,
like the following:
(X costs Y) iff (the price of X is Y)
In this paper we focus on the role of paraphrases
in a Question Answering (QA) system targeted at
1Note that the reverse is not true, and therefore this
is not a perfect paraphrase.
technical manuals. Technical documentation is char-
acterised by vast amounts of domain-specific termi-
nology, which needs to be exploited for providing in-
telligent access to the information contained in the
manuals (Rinaldi et al, 2002b). The approach taken
by QA systems is to allow a user to ask a query (for-
mulated in natural language) and have the system
search a background collection of documents in order
to locate an answer. The field of Question Answer-
ing has flourished in recent years2, in part, due to
the QA track of the TREC competitions (Voorhees
and Harman, 2001). These competitions evaluate
systems over a common data set alowing develop-
ers to benchmark performance in relation to other
competitors.
It is a common assumption that technical termi-
nology is subject to strict controls and cannot vary
within a given editing process. However this assump-
tion proves all too often to be incorrect. Unless edi-
tors are making use of a terminology control system
that forces them to use a specific version of a term,
they will naturally tend to use various paraphrases
to refer to the intended domain concept. Besides in
a query a user could use an arbitrary paraphrases of
the target term, which might happen to be one of
those used in the manual itself or might happen to
be a novel one.
We describe some potential solutions to this prob-
lem, taking our Question Answering system as an ex-
ample. We show which benefits our approach based
on paraphrases bring to the system. So far two dif-
ferent domains have been targeted by the system.
An initial application aims at answering questions
about the Unix man pages (Molla? et al, 2000a; Molla?
et al, 2000b). A more complex application targets
the Aircraft Maintenance Manual (AMM) of the Air-
bus A320 (Rinaldi et al, 2002b). Recently we have
started new work, using the Linux HOWTOs as a
new target domain.
In dealing with these domains we have identified
two major obstacles for a QA system, which we can
summarise as follows:
? The Parsing Problem
? The Paraphrase Problem
The Parsing Problem consists in the increased
difficulty of parsing text in a technical domain due to
domain-specific sublanguage. Various types of multi
word expressions characterise these domains, in par-
ticular referring to specific concepts like tools, parts
or procedures. These multi word expressions might
2Although early work in AI already touched upon the
topic, e.g. (Woods, 1977).
include lexical items which are either unknown to
a generic lexicon (e.g. coax cable) or have a spe-
cific meaning unique to this domain. Abbreviations
and acronyms are another common source of incon-
sistencies. In such cases the parser might either
fail to identify the compound as a phrase and con-
sequently fail to parse the sentence including such
items. Alternatively the parser might attempt to
?guess? their lexical category (in the set of open class
categories), leading to an exponential growth of the
number of possible syntactic parses. Not only the in-
ternal structure of the compound can be multi-way
ambiguous, even the boundaries of the compounds
might be difficult to detect and the parsers might
try odd combinations of the tokens belonging to the
compounds with neighbouring tokens.
The Paraphrase Problem resides in the imper-
fect knowledge of users of the systems, who can-
not be expected to be completely familiar with the
domain terminology. Even experienced users, who
know very well the domain, might not remember the
exact wording of a compound and use a paraphrase
to refer to the underlying domain concept. Besides
even in the manual itself, unless the editors have been
forced to use some strict terminology control system,
various paraphrases of the same compound will ap-
pear, and they need to be identified as co-referent.
However, it is not enough to identify all paraphrases
within the manual, novel paraphrases might be cre-
ated by the users each time they query the system.
In the rest of this paper we describe first our Ques-
tion Answering System (in Section 2) and briefly
show how we solved the first of the two problems
described above. Then, in Section 3 we show in de-
tail how the system is capable of coping with the
Paraphrase Problem. Finally in Section 4 we discuss
some related work.
2 A Question Answering System for
Technical Domains
Over the past few years our research group has devel-
oped an Answer Extraction system (ExtrAns) that
works by transforming documents and queries into a
semantic representation called Minimal Logical Form
(MLF) (Molla? et al, 2000a) and derives the answers
by logical proof from the documents. A full linguis-
tic (syntactic and semantic) analysis, complete with
lexical alternations (synonyms and hyponyms) is per-
formed. While documents are processed in an off-line
stage, the query is processed on-line.
Two real world applications have so far been im-
plemented with the same underlying technology. The
original ExtrAns system (Molla? et al, 2000b) is used
///// a.d electrical coax cable.n4 connects.v062 the.d external antenna.n1 to.o the.d ANT connection.n1 /////
-Wd
ff Dsu ff Ss
-
MVp
-Os
ff Ds
-Js
ff Ds
RW
Figure 1: An Example of LG Output
to extract answers to arbitrary user queries over the
Unix documentation files (?man pages?). A set of
500+ unedited man pages has been used for this ap-
plication. An on-line demo of ExtrAns can be found
at the project web page.3
 Knowledge 
Base
Document
Linguistic
Analysis
Term
processing
Figure 2: Off-line
Processing of Docu-
ments
More recently we tackled
a different domain, the Air-
plane Maintenance Manu-
als (AMM) of the Air-
bus A320 (Rinaldi et al,
2002b), which offered the
additional challenges of an
SGML-based format and a
much larger size (120MB).4
Despite being developed
initially for a specific do-
main, ExtrAns has demon-
strated a high level of do-
main independence.
As we work on relatively
small volumes of data we
can afford to process (in
an off-line stage) all the
documents in our collection
rather than just a few se-
lected paragraphs (see Fig-
ure 2). Clearly in some sit-
uations (e.g. processing in-
coming news) such an ap-
proach might not be fea-
sible and paragraph index-
ing techniques would need
to be used. Our current ap-
proach is particularly tar-
geted to small and medium sized collections.
In an initial phase all multi-word expressions
from the domain are collected and structured in
an external resource, which we will refer to as the
TermBase (Rinaldi et al, 2003; Dowdall et al, 2003).
The document sentences (and user queries) are syn-
tactically processed with the Link Grammar (LG)
parser (Sleator and Temperley, 1993) which uses a
3http://www.ifi.unizh.ch/cl/extrans/
4Still considerably smaller than the size of the docu-
ment collections used for TREC
grammar with a wide coverage of English and has
a robust treatment of ungrammatical sentences and
unknown words. The multi-word terms from the the-
saurus are identified and passed to the parser as sin-
gle tokens. This prevents (futile) analysis of the in-
ternal structure of terms (see Figure 1), simplifying
parsing by 46%. This solves the first of the problems
that we have identified in the introduction (?The
Parsing Problem?).
In later stages of processing, a corpus-based ap-
proach (Brill and Resnik, 1994) is used to deal with
ambiguities that cannot be solved with syntactic in-
formation only, in particular attachments of preposi-
tional phrases, gerunds and infinitive constructions.
ExtrAns adopts an anaphora resolution algorithm
(Molla? et al, 2003) that is based on Lappin and Le-
ass? approach (Lappin and Leass, 1994). The original
algorithm, which was applied to the syntactic struc-
tures generated by McCord?s Slot Grammar (Mc-
Cord et al, 1992), has been ported to the output of
Link Grammar. So far the resolution is restricted to
sentence-internal pronouns but the same algorithm
can be applied to sentence-external pronouns too.
A lexicon of nominalisations based on NOMLEX
(Meyers et al, 1998) is used for the most important
cases. The main problem here is that the semantic
relationship between the base words (mostly, but not
exclusively, verbs) and the derived words (mostly,
but not exclusively, nouns) is not sufficiently sys-
tematic to allow a derivation lexicon to be compiled
automatically. Only in relatively rare cases is the
relationship as simple as with to edit <a text> ?
editor of <a text> / <text> editor, as the effort
that went into building resources such as NOMLEX
also shows.
User queries are processed on-line and converted
into MLFs (possibly expanded by synonyms) and
proved by refutation over the document knowledge
base (see Figure 3). Pointers to the original text at-
tached to the retrieved logical forms allow the system
to identify and highlight those words in the retrieved
sentence that contribute most to that particular an-
swer. When the user clicks on one of the answers
provided, the corresponding document will be dis-
played with the relevant passages highlighted.
 Knowledge 
Base
ANSWERSQuery
Document
Linguistic
Analysis
Paraphrase
Identification
Figure 3: On-line Processing of Queries
The meaning of the documents and of the queries
produced by ExtrAns is expressed by means of Mini-
mal Logical Forms (MLFs). The MLFs are designed
so that they can be found for any sentence (using
robust approaches to treat very complex or ungram-
matical sentences), and they are optimized for NLP
tasks that involve the semantic comparison of sen-
tences, such as Answer Extraction.
The expressivity of the MLFs is minimal in the
sense that the main syntactic dependencies between
the words are used to express verb-argument rela-
tions, and modifier and adjunct relations. However,
complex quantification, tense and aspect, temporal
relations, plurality, and modality are not expressed.
One of the effects of this kind of underspecification
is that several natural language queries, although
slightly different in meaning, produce the same logi-
cal form.
The main feature of the MLFs is the use of reifi-
cation (the expression of abstract concepts as con-
crete objects) to achieve flat expressions (Molla? et
al., 2000b). The MLFs are expressed as conjunc-
tions of predicates with all the variables existentially
bound with wide scope. For example, the MLF of
the sentence ?cp will quickly copy the files? is:
(1) holds(e4), object(cp,o1,[x1]),
object(s command,o2,[x1]),
evt(s copy,e4,[x1,x6]),
object(s file,o3,[x6]),
prop(quickly,p3,[e4]).
In other words, there is an entity x1 which rep-
resents an object of type cp and of type command,
there is an entity x6 (a file), there is an entity e4,
which represents a copying event where the first ar-
gument is x1 and the second argument is x6, there
is an entity p3 which states that e4 is done quickly,
and the event e4, that is, the copying, holds. The
entities o1, o2, o3, e4, and p3 are the result of reifi-
cation. The reification of the event, e4, has been used
to express that the event is done quickly. The other
entities are not used in this MLF, but other more
complex sentences may need to refer to the reifica-
tion of properties (adjective-modifying adverbs) or
object predicates (non-intersective adjectives such as
the alleged suspect).
ExtrAns finds the answers to the questions by
forming the MLFs of the questions and then run-
ning Prolog?s default resolution mechanism to find
those MLFs that can prove the question. When no
direct proof for the user query is found, the system
is capable of relaxing the proof criteria in a stepwise
manner. First, hyponyms of the query terms will be
added as disjunctions in the logical form of the ques-
tion, thus making it more general but still logically
correct. If that fails, the system will attempt approx-
imate matching, in which the sentence (or sentences)
with the highest overlap of predicates with the query
is retrieved. The (partially) matching sentences are
scored and the best fits are returned. In the case
that this method finds too many answers because
the overlap is too low, the system will attempt key-
word matching, in which syntactic criteria are aban-
doned and only information about word classes is
used. This last step corresponds approximately to a
traditional passage-retrieval methodology with con-
sideration of the POS tags.
3 Dealing with Paraphrases
The system is capable of dealing with paraphrases
at two different levels. On the phrase level, differ-
ent surface realizations (terms) which refer to the
same domain concept will be mapped into a com-
mon identifier (synset identifier). On the sentence
level, paraphrases which involve a (simple) syntactic
transformation will be dealt with by mapping them
into the same logical form. In this section we will
describe these two approaches and discuss ways to
cope with complex types of parapharases.
3.1 Identifying Terminological Paraphrases
During the construction of the MLFs, thesaurus
terms are replaced by their synset identifiers. This
results in an implicit ?terminological normalization?
for the domain. The benefit to the QA process is
an assurance that a query and answer need not in-
volve exactly the same surface realization of a term.
Utilizing the synsets in the semantic representation
means that when the query includes a term, ExtrAns
returns sentences that logically answer the query, in-
Fastr
Term
Extraction
Hyponymy
Thesaurus ExtrAns
Document
Figure 4: Term Processing
volving any known paraphrase of that term.
For example, the logical form of the query Where
are the stowage compartments installed? is trans-
lated internally into the Horn query (2).
(2) evt(install,A,[B,C]),
object(D,E,[B]),
object(s stowage compartment,G,[C])
This means that a term (belonging to the same
synset as stowage compartment) is involved in an in-
stall event with an anonymous object. If there is
an MLF from the document that can match exam-
ple (2), then it is selected as a candidate answer and
the sentence it originates from is shown to the user.
The process of terminological variation is well
investigated (Ibekwe-SanJuan and Dubois, 2002;
Daille et al, 1996; Ibekwe-Sanjuan, 1998). The
primary focus has been to use linguistically based
variation to expand existing term sets through cor-
pus investigation or to produce domain representa-
tions. A subset of such variations identifies terms
which are strictly synonymous. ExtrAns gathers
these morpho-syntactic variations into synsets. The
sets are augmented with terms exhibiting three
weaker synonymy relations described by Hamon &
Nazarenko (2001). These synsets are organized into
a hyponymy (isa) hierarchy, a small example of which
can be seen in Figure 5. Figure 4 shows a schematic
representation of this process.
The first stage is to normalize any terms that con-
tain punctuation by creating a punctuation free ver-
sion and recording the fact that that the two are
strictly synonymous. Further processing is involved
in terms containing brackets to determine if the
bracketed token is an acronym or simply optional. In
the former case an acronym-free term is created and
the acronym is stored as a synonym of the remain-
ing tokens which contain it as a regular expression.
So evac is synonymous with evacuation and ohsc is
synonymous with overhead stowage compartment. In
cases such as emergency (hard landings) the brack-
eted tokens can not be interpreted as acronyms and
so are not removed.
The synonymy relations are identified using the
terminology tool Fastr (Jacquemin, 2001). Every to-
ken of each term is associated with its part-of-speech,
its morphological root, and its synonyms. Phrasal
rules represent the manner in which tokens combine
to form multi-token terms, and feature-value pairs
carry the token specific information. Metarules li-
cense the relation between two terms by constrain-
ing their phrase structures in conjunction with the
morphological and semantic information on the indi-
vidual tokens.
The metarules can identify simple paraphrases
that result from morpho-syntactic variation (cargo
compartment door ?? doors of the cargo compart-
ment), terms with synonymous heads (electrical ca-
ble ?? electrical line), terms with synonymous mod-
ifiers (fastener strip ?? attachment strip) and both
(functional test ?? operational check). For a de-
scription of the frequency and range of types of vari-
ation present in the AMM see Rinaldi et al (2002a).
3.2 Identifying Syntactic Paraphrases
An important effect of using a simplified semantic-
based representation such as the Minimal Logical
Forms is that various types of syntactic variations
are automatically captured by a common representa-
tion. This ensures that many potential paraphrases
in a user query can map to the same answer into the
manual.
For example the question shown in Figure 6 can
be answered thanks to the combination of two fac-
tors. On the lexical level ExtrAns knows that APU
is an abbreviation of Auxiliary Power Unit, while on
the syntactic level the active and passive voices (sup-
plies vs supplied with) map into the same underlying
representation (the same MLF).
Another type of paraphrase which can be detected
at this level is the kind that was classified as type (3)
in the introduction. For example the question: Is
the sensor connected to the APU ECB?, can locate
the answer This sensor is connected to the Elec-
tronic Control Box (ECB) of the APU. This has been
achieved by introducing meaning postulates that op-
erate at the level of the MLFs (such as ?any predicate
that affects an object will also affect the of -modifiers
of that object?).
3.3 Weaker Types of Paraphrases
When the thesaurus definition of terminological syn-
onymy fails to locate an answer from the docu-
ment collection, ExtrAns explores weaker types of
paraphrases, where the equivalence between the two
terms might not be complete.
TERM
doors of the cargo compartment
cargo compartment door
cargo comparment doors
cargo-compartment door
emergency ( hard landings )
emergency hard landings
emergency hard landing
emergency evacuation (evac)
emergency evacuation
evacuation
evac
electrical cable
electrical line
fastner strip
attachment strip
functional test
operational check
door functional test
stowage compartment
overhead stowage compartment
OHSC
1
2
3
5
6
7
10
9
8
11
Figure 5: A Sample of the TermBase
Figure 6: Active vs Passive Voice
First, ExtrAns makes use of the hyponymy rela-
tions, which can be considered as sort of unidirec-
tional paraphrases. Instead of looking for synset
members, the query is reformulated to included hy-
ponyms and hyperonyms of the terms:
(3) (object(s stowage compartment,A,[B]);
object(s overhead stowage compartment,A,[B])),
evt(install,C,[D,B]),
object(E,F,[D|G])
Now the alternative objects are in a logical OR rela-
tion. This query finds the answer in Figure 7 (where
stowage compartment is a hyperonym of overhead
stowage compartment).
We have implemented a very simple ad-hoc algo-
rithm to determine lexical hyponymy between terms.
Term A is a hyponym of term B if (i) A has more to-
kens than B, (ii) all the tokens of B are present in A,
and (iii) both terms have the same head. There are
three provisions. First, ignore terms with dashes and
brackets as cargo compartment is not a hyponym of
cargo - compartment and this relation (synonymy) is
already known from the normalisation process. Sec-
ond, compare lemmatised versions of the terms to
capture that stowage compartment is a hyperonym
of overhead stowage compartments. Finally, the head
of a term is the rightmost non-symbol token (i.e. a
word) which can be determined from the part-of-
speech tags. This hyponymy relation is compara-
ble to the insertion variations defined by Daille et
al. (1996).
The expressivity of the MLF can further be ex-
panded through the use of meaning postulates of the
type: ?If x is installed in y, then x is in y?. This
ensures that the query Where are the equipment and
furnishings? extracts the answer The equipment and
furnishings are installed in the cockpit.
4 Related Work
The importance of detecting paraphrasing in Ques-
tion Answering has been shown dramatically in
TREC9 by the Falcon system (Harabagiu et al,
2001), which made use of an ad-hoc module capable
of caching answers and detecting question similar-
ity. As in that particular evaluation the organisers
deliberately used a set of paraphrases of the same
questions, such approach certainly helped in boost-
ing the performance of the system. In an environ-
ment where the same question (in different formula-
tions) is likely to be repeated a number of times, a
module capable of detecting paraphrases can signif-
icantly improve the performance of a Question An-
Figure 7: Overhead stowage compartment is a Hyponym of Stowage compartment
swering system.
Another example of application of paraphrases for
Question Answering is given in (Murata and Isahara,
2001), which further argues for the importance of
paraphrases for other applications such Summarisa-
tion, error correction and speech generation.
Our approach for the acquisition of terminological
paraphrases might have some points in common with
the approach described in (Terada and Tokunaga,
2001). The motivation that they bring forward for
the necessity of identifying abbreviations is related to
the problem that we have called ?the Parsing Prob-
lem?.
A very different approach to paraphrases is taken
in (Takahashi et al, 2001) where they formulate the
problem as a special case of Machine Translation,
where the source and target language are the same
but special rules, based on different parameters, li-
cense different types of surface realizations.
Hamon & Nazarenko (2001) explore the termino-
logical needs of consulting systems. This type of IR
guides the user in query/keyword expansion or pro-
poses various levels of access into the document base
on the original query. A method of generating three
types of synonymy relations is investigated using gen-
eral language and domain specific dictionaries.
5 Conclusion
Automatic recognition of paraphrases is an effec-
tive technique to ease the information access bur-
den in a technical domain. We have presented some
techniques that we have adopted in a Question An-
swering system for dealing with paraphrases. These
techniques range from the detection of lexical para-
phrases and terminology variants, to the use of a
simplified logical form that provides the same repre-
sentation for morpho-syntactic paraphrases, and the
use of meaning postulates for paraphrases that re-
quire inferences.
References
Eric Brill and Philip Resnik. 1994. A rule-based
approach to prepositional phrase attachment dis-
ambiguation. In Proc. COLING ?94, volume 2,
pages 998?1004, Kyoto, Japan.
Beatrice Daille, Benot Habert, Christian Jacquemin,
and Jean Royaute?. 1996. Empirical observation of
term variations and principles for their description.
Terminology, 3(2):197?258.
James Dowdall, Fabio Rinaldi, Fidelia Ibekwe-
SanJuan, and Eric SanJuan. 2003. Complex
structuring of term variants for Question Answer-
ing. In Proc. ACL-2003 Workshop on Multiword
Expressions, Sapporo, Japan.
Christiane Fellbaum 1998. WordNet: an electronic
lexical database. MIT Press, Cambridge, MA.
Thierry Hamon and Adeline Nazarenko. 2001. De-
tection of synonymy links between terms: Experi-
ment and results. In Didier Bourigault, Christian
Jacquemin, and Marie-Claude L?Homme, editors,
Recent Advances in Computational Terminology,
pages 185?208. John Benjamins Publishing Com-
pany.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca,
Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu,
Roxana G??rju, Vasile Rus, and Paul Morarescu.
2001. Falcon: Boosting knowledge for answer
engines. In Voorhees and Harman (Voorhees and
Harman, 2001).
Fidelia Ibekwe-SanJuan and Cyrille Dubois. 2002.
Can Syntactic Variations Highlight Semantic
Links Between Domain Topics? In Proceedings
of the 6th International Conference on Terminol-
ogy and Knowledge Engineering (TKE02), pages
57?64, Nancy, August.
Fidelia Ibekwe-Sanjuan. 1998. Terminological Vari-
ation, a Means of Identifying Research Topics from
Texts. In Proceedings of COLING-ACL, pages
571?577, Quebec,Canada, August.
Christian Jacquemin. 2001. Spotting and Discover-
ing Terms through Natural Language Processing.
MIT Press.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Com-
putational Linguistics, 20(4):535?561.
Michael McCord, Arendse Bernth, Shalom Lap-
pin, and Wlodek Zadrozny. 1992. Natural lan-
guage processing within a slot grammar frame-
work. International Journal on Artificial Intelli-
gence Tools, 1(2):229?277.
Adam Meyers, Catherine Macleod, Roman Yangar-
ber, Ralph Grishman, Leslie Barrett, and Ruth
Reeves. 1998. Using NOMLEX to produce
nominalization patterns for information extrac-
tion. In Proceedings: the Computational Treat-
ment of Nominals, Montreal, Canada, (Coling-
ACL98 workshop), August.
Diego Molla?, Gerold Schneider, Rolf Schwitter, and
Michael Hess. 2000a. Answer Extraction using
a Dependency Grammar in ExtrAns. Traitement
Automatique de Langues (T.A.L.), Special Issue
on Dependency Grammar, 41(1):127?156.
Diego Molla?, Rolf Schwitter, Michael Hess, and
Rachel Fournier. 2000b. Extrans, an answer ex-
traction system. T.A.L. special issue on Informa-
tion Retrieval oriented Natural Language Process-
ing.
Diego Molla?, Rolf Schwitter, Fabio Rinaldi, James
Dowdall, and Michael Hess. 2003. Anaphora res-
olution in ExtrAns. In Proceedings of the Interna-
tional Symposium on Reference Resolution and Its
Applications to Question Answering and Summa-
rization, 23?25 June, Venice, Italy.
Masaki Murata and Hitoshi Isahara. 2001. Univer-
sal model for paraphrasing - using transformation
based on a defined criteria. In Proceedings of the
NLPRS2001 Workshop on Automatic Paraphras-
ing: Theories and Applications.
Fabio Rinaldi, James Dowdall, Michael Hess, Kaarel
Kaljurand, Mare Koit, Kadri Vider, and Neeme
Kahusk. 2002a. Terminology as Knowledge in An-
swer Extraction. In Proceedings of the 6th Interna-
tional Conference on Terminology and Knowledge
Engineering (TKE02), pages 107?113, Nancy, 28?
30 August.
Fabio Rinaldi, James Dowdall, Michael Hess, Diego
Molla?, and Rolf Schwitter. 2002b. Towards An-
swer Extraction: an application to Technical Do-
mains. In ECAI2002, European Conference on Ar-
tificial Intelligence, Lyon, 21?26 July.
Fabio Rinaldi, James Dowdall, Michael Hess, Kaarel
Kaljurand, and Magnus Karlsson. 2003. The Role
of Technical Terminology in Question Answering.
In Proceedings of TIA-2003, Terminologie et In-
telligence Artificielle, Strasbourg, April.
Daniel D. Sleator and Davy Temperley. 1993. Pars-
ing English with a link grammar. In Proc. Third
International Workshop on Parsing Technologies,
pages 277?292.
Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, and
Kentaro Inui. 2001. Kura: A revision-based
lexico-structural paraphrasing engine. In Proceed-
ings of the NLPRS2001 Workshop on Automatic
Paraphrasing: Theories and Applications.
Akira Terada and Takenobu Tokunaga. 2001. Au-
tomatic disabbreviation by using context informa-
tion. In Proceedings of the NLPRS2001 Workshop
on Automatic Paraphrasing: Theories and Appli-
cations.
Ellen M. Voorhees and Donna Harman, editors.
2001. Proceedings of the Ninth Text REtrieval
Conference (TREC-9), Gaithersburg, Maryland,
November 13-16, 2000.
W.A. Woods. 1977. Lunar rocks in natural English:
Explorations in Natural Language Question An-
swering. In A. Zampolli, editor, Linguistic Struc-
tures Processing, volume 5 of Fundamental Studies
in Computer Science, pages 521?569. North Hol-
land.
Complex structuring of term variants for Question Answering
James Dowdall, Fabio Rinaldi
Institute of Computational Linguistics
University of Zurich
fdowdall,rinaldig@cl.unizh.ch
Fidelia Ibekwe-SanJuan
ERSICO
University of Lyon3
ibekwe@univ-lyon3.fr
Eric SanJuan
LITA EA3097
University of Metz
eric.sanjuan@iut.univ-metz.fr
Abstract
Question Answering provides a method of
locating precise answers to specic ques-
tions but in technical domains the amount
of Multi-Word Terms complicates this task.
This paper outlines the Question Answer-
ing task in such a domain and explores two
ways of detecting relations between Multi-
Word Terms. The rst targets specic se-
mantic relations, the second uses a cluster-
ing algorithm, but they are both based on
the idea of syntactic variation. The pa-
per demonstrates how the combination of
these two methodologies provide sophisti-
cated access to technical domains.
1 Introduction
Nominal compounds are inherently ambiguous on
both the syntactic and semantic fronts. Whilst the
number of syntactic possibilities increase exponen-
tially with word length (Isabelle, 1984), semantic in-
terpretation is at best contextually dependent and in
the worst cases determined by extra-linguistic (prag-
matic) factors.
1
Technical documentation is an at-
tractive domain in which to explore nominal com-
pounds for two reasons. First, they present an abun-
dance of compounds, secondly they restrict semantic
interpretation by excluding compounds with opaque
(extra-linguistic) interpretation. The result is multi-
word terms (MWT) which are both compositional,
their formation is a function of their constituent ele-
ments (Kageura, 2002) and endocentric, the com-
pound is a hyponym of its head (Barker and Sz-
pakowicz, 1998).
1
For example, \apple juice place" (Levi, 1979)
This paper addresses the issue of structuring the
Multi-Word Terms (MWTs) for Question Answer-
ing (QA) in technical domains. The central prob-
lem is that unfamiliarity with MWTs that character-
ize such domains creates an eective barrier against
users nding answers.
Section 2 outlines the domain of focus, the
MWT extraction method and examples character-
istic MWTs. Section 3 explores the QA task in tech-
nical domains by describing the ExtrAns system,
and how it structures the MWTs for the task. Sec-
tion 4 presents TermWatch which identies syntac-
tic variants and uses a hierarchical clustering algo-
rithm to build classes of term variants. The common
ground between these two approaches is in the use
of syntactic variants to structure the terminology as
a whole. Section 5 explores how the resulting struc-
tures can be used in the QA task. After surveying
some related work in Section 6 the paper ends by
drawing conclusions on the approaches presented.
2 MWT Extraction
Before the MWTs can be structured, the terms
need to be extracted from a corpus of texts. This
stage was performed using the INTEX linguistic
parser (Silberztein, 1993). INTEX is a nite state
transducer parser. The corpus used in the present
study concerns scientic publications on the bread-
making process. It was made available by the French
Institute of Scientic Information (INIST). Without
going into much detail regarding the candidate term
extraction rules, the approach adopted can be sum-
marized as selective NLP followed by shallow pars-
ing, much in the same way as (Evans et al, 1992).
We dened morpho-syntactic properties of complex
nominal syntagms written as nite state automata,
implemented in the INTEX linguistic toolbox. IN-
TEX is equipped with linguistic resources to perform
 Knowledge 
Base
Document
Linguistic
Analysis
MWT
processing
(a)
Oine
 Knowledge 
Base
ANSWERS
Query
Document
Linguistic
Analysis
MWT
Recognition
(b)
Online
Figure 1: Schematic: ExtrAns Processing Stages
an overall morpho-syntactic analysis on the texts.
The NP automata are applied in an iterative way
on the corpus until we reach a satisfactory medium-
grained noun phrase splitting. Our concern was to
extract more or less complex terms as they appeared
in the text corpus and not atomic NP extraction.
The rationale was to conserve the associations be-
tween terms as the scientists (authors) made them
during their write-up. Examples of candidate terms
extracted at this stage are: \hydrophilic powdered
lecithin, traditional sour dough starter cultures, de-
velopment of traditional bread avour". More details
on the NP splitting rules can be found in (Ibekwe-
SanJuan, 2001). Manual validation by a domain ex-
pert produced 3651 MWTs.
3 ExtrAns
Question Answering systems attempt to extract
small snippets of text in response to a natural lan-
guage query. Briey, ExtrAns achieves this in two
distinct stages:
O-line the entire document collection is sub-
jected to linguistic analysis, which produces a full
syntactic parse for each sentence. After some inter-
mediate steps, such as anaphora resolution and dis-
ambiguation, the syntactic parse is translated into a
semantic representation designed to capture the core
meaning of the sentences. These representations are
stored in a Knowledge Base.
On-line user queries are subjected to the same
linguistic analysis. The resulting semantic rep-
resentation of the query is `matched' against the
knowledge base. These `matches' can be identied
in their original document location, so users can
contextualize these potential answers. Interest in
the specics of this process should be directed
toward (Rinaldi et al, 2002) (Dowdall et al, 2002).
In dealing with technical domains we have iden-
tied two major obstacles for a QA system which
can be summarize as the Parsing Problem and the
Paraphrase Problem.
The Parsing Problem consists in the increased
di?culty of parsing text in a technical domain due to
domain-specic sublanguage. Various types of MWT
characterize these domains, in particular referring
to specic concepts like tools, parts or procedures.
These multi word expressions might include lexical
items which are either unknown to a generic lexicon
(e.g. \acetic acid") or have a specic meaning unique
to this domain. Abbreviations and acronyms are an-
other common source of incosistencies. In such cases
the parser might either fail to identify the compound
as a phrase and consequently fail to parse the sen-
tence including such items. Alternatively the parser
might attempt to `guess' their lexical category (in
the set of open class categories), leading to an ex-
ponential growth of the number of possible syntactic
parses. Not only the internal structure of the com-
pound can be multi-way ambiguous, even the bound-
aries of the compounds might be di?cult to detect
and the parsers might try odd combinations of the
tokens belonging to the compounds with neighbour-
ing tokens.
The Paraphrase Problem resides in the imper-
fect knowledge of users of the systems, who cannot be
expected to be completely familiar with the domain
terminology. Even experienced users, who know the
domain very well, might not remember the exact
wording of a MWT and use a paraphrase to refer to
the underlying domain concept. Besides even in the
documents themselves, unless the editors have been
forced to use some strict terminology control system,
various paraphrases of the same compound will ap-
pear, and they need to be identied as co-referent.
However, it is not enough to identify all paraphrases
within the manual, novel paraphases might be cre-
ated by the users each time they query the system.
The task of QA in technical domains is to identify:
`what' needs to be known about `which' multi-word
term. Then to extract sentences that provide the
answer. How to nd the `what' is dependant on the
approach. ExtrAns uses linguistic processing which
results in a semantic representation. However, in
the TREC domain of newswire, considerable success
has been achieved by statistical measures and even
pattern matching. Here, these distinctions are unim-
portant.
What is of concern is in how to meet the two com-
peting search needs of answering specic questions
and navigating through a domain of specialized, un-
familiar MWTs.
Designed specically for technical domains, Ex-
trAns involves strategies for exploiting the abundant
MWTs that these domains hold. The approach uti-
lizes WordNet to gather the MWTs into synonymy
sets based on variation rules. The terminology is also
related through an hyponymy hierarchy.
Synonymy between MWTs is either strict, or
detected through WordNet. Strictly synonymous
MWTs coreference a single object/concept. This
link is a result of morpho-syntactic variation taking
\chemical improver action" and producing the anit-
symmetrical term \action of chemical improver".
The process simply involves inverting the Head and
introducing modiers with a preposition.
WordNet synonymy, on the other hand, comes in
three types of symmetrical variation depending on
which tokens from two MWTs can be found in the
same synset:
 WordNet Head substitution, (\bread ingestion"
and \bread consumption")
 WordNet modier substitution (\quantity of
yeast" and \amount of yeast")
 WordNet Modier and head substitution (\key
ingredient" and \functional component").
However, synonymy identied through WordNet is
dened by WordNet. As a general lexical database
not designed for specilized domains it represents
common synonymy between words. The resulting
links created between multi-word terms translates
into concepts non-specialists cannot easily distin-
guish. These links produced 1277 synsets the vast
majority of which contain two MWTs.
Hyponymy The MWTs are organized into a lex-
ical hyponymy (is a) hierarchy that exploits their
endocentricity (Barker and Szpakowicz, 1998). The
hyponymy relation is identied through two types of
rules, Left Expansions which further modies \dough
stickiness" to be \intense" producing \intense dough
stickiness". Here the original head-modier rela-
tions of the hypernym are unaltered in the hyponym.
However, with Insertion rules these relations are
stickiness
dough
stickiness
surface
stickiness
dough
increase
stickiness
wheat
dough
surface
stickiness
measure
surface
stickiness
intense
dough
stickiness
diminished
dough
stickiness
wheat
dough
stickiness
Figure 2: Hyponymy Hierarchy
changed in the potential hyponym. For example,
whatever is going on in \wheat dough stickiness", in-
serting the word \surface" to produce \wheat dough
surface stickiness" has altered the original head-
modier relations. So a generic/specic relation is
less certain. For the moment such links are permit-
ted.
This process allows multiple parents for a given
term. So \wheat dough surface stickiness" is also
a hyponym of \surface stickiness" through a left-
expansion rule. An example of this kind of hierarchy
can be seen in gure 2.
These two structures are exploited in the search
process during `matching' of queries against answers.
The strengths they bring and the limitations imposed
are explored in Section 5 after description of an al-
ternative approach to term variant structuring.
4 The TermWatch system
TermWatch (Ibekwe-SanJuan and SanJuan, 2003)
clusters term variants into classes, thus producing
a three-level structuring of terms: term, connected
component and class levels. It integrates a visual
interface developed with the Aisee graphic visualiza-
tion to enable the user explore the classes and browse
through the links between terms. Earlier stages of
this work were presented in (Ibekwe-SanJuan, 1998).
The system comprises of two major modules: a
syntactic variant identier and a clustering module
whose results are loaded onto the Aisee visualization
tool.
2
4.1 Variants identier module
Automatic term variant identication has been ex-
tensively explored in (Jacquemin, 2001). In the sec-
tions below, we will recall briey the denitions of
the variation types we identify and give examples
each type.
2
http://www.aisee.com/
Expansions are subdivided along the grammati-
cal axis: those that aect the modier words in a
term and those that aect the head word. Modier
expansions (L-Exp) describes two elementary op-
erations: left-expansion (L-Exp) and Insertion (Ins).
They both denote the addition at the leftmost po-
sition (L-Exp) or inside a term (Insertion or Ins) of
new modier elements. For instance, \gas holding
property of dough" is a left-expansion of \gas holding
property" because by transformation to a nominal
compound structure, we obtain \dough gas holding
property". Likewise, \bread dough quality character-
istics" is an insertion variant (Ins) of \bread char-
acteristics". Head expansions (R-Exp) describes
the addition of one or more nominals in the head po-
sition of a term, thus shifting the former headword
to a modier position. Thus \frozen sweet dough
baking" is a R-Exp of \frozen sweet dough". A com-
bination of the two expansion types yield left-right
expansion (LR-Exp) in that it describes addition of
words both in the modier and head positions. For
example, the relation between \nonstarch polysac-
charide" and \functional property of rye nonstarch
polysaccharide" (\rye nonstarch polysaccharide func-
tional property"). These relations are constrained in
that the added or inserted words have to be con-
tiguous, otherwise, we may not have the expected
semantic relations. Only nominal elements are con-
sidered (nouns, adjectives).
Substitutions are also dened along the gram-
matical axis to yield two sub-types : modier and
head substitution. Modier substitution (M-Sub)
describes the replacing of one modier word in term
t
1
by another word in term t
2
. Thus \bread dough
leavening" is a modier substitution (M-Sub) of
\composite dough leavening". Head substitution
(H-Sub) relates terms which share the same modi-
ers but dierent heads : \eect of xanthan gum"
and \addition of xanthan gum". These relations
are equally constrained in that they can only link
terms of equal length where one and only one item
is dierent, thus guaranteeing the interpretability of
the relations. Substitutions, since they denote non-
directional relations between terms of equal length,
engender symmetrical relations between terms on the
formal level: t
1
t
2
. Their transitive closure cre-
ates classes of terms. For instance, a set of terms
related by modier substitution (M-Sub) seem to
point to a class of \properties/attributes" shared by
a same concept (the head word) as in \bread texture,
endosperm texture, good texture" for binary terms
and \sour corn bread, sour dough bread, sour maize
bread" for ternary terms. In this last case, the chang-
ing properties seem to point to the possible special-
izations (\sour-") of the concept (\bread"). Head
substitution on the other hand gathers together sets
of terms that share the same \properties" (the mod-
ier words), thus creating a class of \concepts". For
instance, the set of term variants \frozen dough bak-
ing, frozen dough characteristics, frozen dough prod-
ucts". The common attribute is \frozen dough",
shared by this class of concepts \products, char-
acteristics, baking". (Ibekwe-SanJuan, 1998) al-
ready put forward the idea of these semantic rela-
tions and (Jacquemin, 1995) reported similar con-
ceptual relations for his insertion and coordination
variants.
4.2 Variant Clustering Module
The second module of TermWatch is a hierarchical
clustering algorithm, CPCL (Classication by Pref-
erential Clustered Link), which clusters terms based
on the variations described above. The six elemen-
tary variation relations are represented as a di-graph.
Clustering is a two-stage process. First the algorithm
builds connected components using a subset of the
variation relations, usually the modier relations (L-
Exp, Ins, M-Sub), these are the COMP relations.
The transitive closure COMP* of COMP partitions
the whole set of terms into components. These con-
nected components are sub-graphs of term variants
that share the same headword. At the second stage,
the connected components are clustered into classes
using the head relations (R-Exp, LR-Exp, H-sub),
this subset of relations is called CLAS. At this stage,
components whose terms are in one of the CLAS re-
lations are grouped basing on an edge dierentiation
coe?cient computed thus:
d
ij
=
X
R2CLAS
n
R
(i; j)
jRj
where CLAS is the set of binary head relations
(Exp D, Exp GD, Sub C), and n
R
(i; j) is the num-
ber of variants of type R between components i and
j. This coe?cient is higher when terms of two com-
ponents share many CLAS relations of a rare type
in the corpus. Components with the highest d
ij
are
clustered rst. The CPCL algorithm can be iterated
several times to suit the user's requirement or un-
til it converges. This means that the user is free to
either set the number of iterations or leave the algo-
rithm to do all the iterations until convergence. The
user only has to specify which set of variations s/he
wants to play the COMP and the CLAS role. In
theory, this distinction is already made in the sys-
tem but the user can change it. On the linguistic
Component 1 component 2
bromate measurement dough stickiness
dough stickiness measurement diminished dough stickiness
dough surface stickiness measurement dough increase stickiness
stickiness measurement intense dough stickiness
measure surface stickiness
soft red winter wheat lines dough stickiness
surface stickiness
wheat dough stickiness
wheat dough surface stickiness
Table 1: Example of a class built by TermWatch.
level, a class contains at least two connected com-
ponents, each comprising of sets of term variants
around the same head word. Class here should be
understood in a formal way: it corresponds to group-
ings of connected components resulting from a hier-
archical clustering algorithm. They are not strictly
dened semantically. Although, we nd semanti-
cally related terms within these classes, the exact
semantic relations involved between pairs of terms
are not explicitly tagged. So on the semantic level,
a class here comprises subsets of term variants re-
ecting, \class of" relations (engendered by substitu-
tions) and \hypernym/hyponym" relations (engen-
dered by modier expansions). For instance, Table
1 displays the term variants found in one class.
This class was built around two components, one
structured around the concept of \stickiness mea-
surement" (most frequent repeated segment) and the
other around the concept of \dough stickiness". We
can observe the COMP relations between term vari-
ants inside each component. The variants that ini-
tiated this class formation are in italics (the ones
sharing CLAS relations).
The TermWatch programs have been implemented
in the AWK language and can run on a Unix or
Windows system. The system is computationally
tractable and processing time is quite acceptable for
real-life applications. For instance, it took 40 sec-
onds on a normal PC running Windows to process a
graph of 3651 term variants and to load the results
onto the Aisee graphic interface. 33 classes of vari-
able sizes were produced at the 3rd iteration of the
clustering algorithm. The smallest class had 4 terms
and the biggest 218 terms! So class size depends very
much on the number and types of variation relations
present in the initial graph.
3
3
TermWatch was initially designed as a scientic and
technology watch system, hence the choices made in
syntactic term variant denitions, the clustering algo-
rithm and visualization mechanisms are tightly related
to this application. A WWW interface is currently under
construction to facilitate the return to the source texts
5 Combining the two systems
The two outlined methodologies use the existence
of syntactic variation between multi-word terms to
structure the terminology as a whole. However, each
approach reects a dierent aspect of this structure.
The ExtrAns approach is designed to identify
explicit relations between terms. The results are
(relatively) small synsets and a hierarchy of types.
For TermWatch, the organizing principle results in
larger classes of terms built around dierent head
words related by syntactic substitution or expansion.
Whilst, not specically targeting semantic relations
the classes do exhibit related terms. Some of these
relations are denable within the classes. For exam-
ple, the class presented in Table 1 contains all of the
hyponyms of \stickiness" identied in ExtrAns (g-
ure 2), but the relations are not rendered explicit in
the class. Also the class contains other terms not
involved in a specic hyponymy relation.
The utility of the classes is in capturing more
\fuzzy" relations between terms whilst avoiding the
problems of trying to dene the relation. For exam-
ple, how can the relation between t
1
: \frozen sweet
dough" and t
2
: \frozen sweet dough baking" be de-
ned ? The most obvious candidate is a part whole
relation but this is defendable only on a formal level:
i.e. t
1
is a subset of t
2
, but does that make t
1
really
a part of t
2
in any semantic sense? In other words, is
\frozen sweet dough" really a part of \frozen sweet
dough baking"?
The TermWatch system does not grapple with this
issue. The interest of these classes for the QA task is
that they exhibit these fuzzy relations. These repre-
sent wider categories of terms to be used for specic
search types. For example, when looking for gen-
eral information on \frozen sweet dough" a user may
well be interested in \baking" it, but when extract-
ing specic information on the same term the rela-
tion is inappropriate. TermWatch was designed orig-
inally for scientic and technological watch (STW).
through hyperlinks.
Term
Extraction
Term
Structure
ExtrAns
Document
synonymy
hyponymy
TermWatch
WordNet
Figure 3: Using the structures in ExtrAns
In this type of application, the expert is less inter-
ested in strict semantic relations between terms in
a taxonomy but more in capturing the association
of research topics in his/her eld. So such \fuzzy"
relations become all important.
Currently ExtrAns uses the synsets and hyponymy
hierarchy during the `matching' of queries against
documents. However, when this fails to locate any-
thing the process is nished without providing users
with any information or any further access into the
domain. What is required is to \relax" the denition
of semantic relation, or facilitate domain investiga-
tion through visualization of the terminology.
The combination of the two methodologies (de-
picted in gure 3) results in a terminology structured
along four levels of granularity. This structure repre-
sents MWTs that are: Strictly synonymous, Word-
Net related, Hierarchy of types and Clustered by
Class.
These levels can be eectively exploited in lo-
cating answers. First, extract potential answers
that involve strictly synonymous MWTs. Second,
look for potential answers with WordNet related
MWTs. Third, try hypernyms/hyponyms of the
search MWT. Finally, allow the user to browse the
classes of MWTs to identify which are of interest in
answer to the question.
TermWatch allows a user-friendly navigation of
the clustering results. Classes are mapped out as
nodes connected by edges whose length denote the
distance between them. The longer the length, the
farther the classes are from one another and thus the
lower their edge coe?cient (d
ij
). The Aisee inter-
face oers standard navigation functions which allow
users to unfold a class into its components and then
into the terms they contain. It thus reects the three-
level structuring eected by the TermWatch mod-
ules.
Figure 4 gives the graphic representation of results
obtained on the corpus. Note that only classes linked
to others are shown in this gure. Classes are la-
beled automatically by the most active term. The
layout points out central or core classes, here classes
(32, 22) which can represent the dominant terminol-
ogy, and by extension, core research topics in the
eld. This layout also brings out interesting con-
gurations like complete graphs and linear graphs.
Complete graphs. The four classes labeled by the
terms \dough behaviour" (32), \wheat our bread"
(29), \wheat bran" (6) and \dough improver" (20)
form a complete graph. They are all linked by sym-
metrical head substitution relations. We found in
these classes term variants like \wheat our dough"
(class 32); \wheat our bread" (class 29), \wheat
our supplementation, wheat our blend, wheat our
fractionation" (class 6), and nally \wheat our com-
position" (class 20). This complete graph is thus
structured around the two modier elements \wheat
our" which can reect a property shared by the
concepts of these four classes. Linear graphs. The
anti-symmetrical relations engendered by insertions
and expansions generate linear graphs, i.e., chains of
relatively long vertices starting from a central class to
the border of the graph. The visualization tool natu-
rally aligns the elements of these linear graphs, thus
highlighting them. For instance, the linear graph
formed by the three classes \dough behaviour" (32),
\frozen dough baking" (10), \dough procedure" (21)
is structured around the set of variants: \frozen
sweet dough (32) ! \frozen sweet dough baking (10)
 \frozen dough baking" (10). The last term \frozen
dough baking" establishes a strong variation relation
with terms in the third class (21) in which we found
the modiers \frozen dough" associated to three dif-
ferent head words: \characteristic, method, prod-
uct".
Given that the syntactic variations which helped
group terms give o semantic links, and given our
restricted denitions of variation relation (see 4.1), a
user seeking information can be oered these class's
contents at this stage in order to see loosely related
terms semantically which a terminological resource
(thesaurus) or WordNet may not have identied.
For instance, in the class shown in Table 1, many
of the terms may not have been related by any se-
mantic relation in WordNet (bromate measurement
and dough stickiness) because none of the head or
the modier words are in any synsets. The clus-
tering algorithm, brings these terms in one class
because \bromate measurement" is a modier sub-
stitution of \stickiness measurement" which is why
they are in the same component. Both tell us some-
thing about \measurement (or rather about measur-
able objects). On the other hand, \dough surface
stickiness measurement", in the same component, is
a left expansion of \stickiness measurement". The
Figure 4: Navigating the clusters of MWTs
two could point to a `hypernym/hyponym' relation.
Thus, from link to link, these terms are connected
to terms of the second component owing to the one
anti-symmetrical link between \dough surface stick-
iness measurement" and \surface stickiness".
From this kind of investigation, a user can choose
the MWTs of interest. This set then becomes the
basis of a second round of answering specic ques-
tions. In this way the system can provide high preci-
sion access to answers, whilst facilitating navigation
through a domain of unfamiliar MWTs.
6 Related Work
The importance of multi-word expressions (MWE)
in various natural language tasks such as auto-
matic indexing, machine translation, information
retrieval/extraction and technology watch need no
longer be proved.
The Multi-word Expression Project aims at study-
ing the properties of a wide range of expressions
including collocations, metaphors and terminology.
The motivation is in explicitly dening the character-
istics of such phrases. The results of the project will
suggest e?cient strategies for overcoming the prob-
lems MWEs cause for NLP applications (Sag et al,
2002)
Much work has been dedicated to the process of
nominal compounding (Levi, 1979) and the seman-
tic interpretation of nominal compounds (Downing,
1977) (Finin, 1980). Other works have addressed
the specic problem of extracting nominal multi-
word expressions for IR applications (Evans et al,
1992) (Smeaton and Sheridan, 1992) (Smadja, 1993)
or of representing them semantically in order to en-
hance IR systems (Popowich et al, 1992) (Gay and
Croft, 1990).
Many systems are dedicated towards structur-
ing terminology for ontology building or terminol-
ogy knowledge base construction (Aussenac-Gilles
et al, 2003). These approaches use the corpus
to identify linguistic markers which in turn point
to certain semantic relations between terms (hy-
pernym/hyponym, synonyms, meronyms). The ap-
proaches we describe are dierent in that relations
are gained through syntactic variations between the
terms.
Active research by the computational terminol-
ogy community (Jacquemin, 2001) (Bourigault et
al., 2001) (Pearson, 1998) has highlighted the im-
portance of discourse as a means of capturing the
essence of terms, hence as a good basis for struc-
turing them. Jacquemin's extensive study has also
highlighted the fact that terms are given to varia-
tions in discourse, so any endeavor to capture the re-
lations between terminological units should integrate
the variation paradigm.
7 Conclusions
Dening and identifying semantic relations between
terms is problematic but can be utilized as part of the
QA process. However, clustering MWTs based on
syntactic variation uncovers classes of terms which
reect more \fuzzy" semantic relations. These are
ideally suited to enabling navigation through the do-
main identifying terms to be used in the Question
Answering process, oering sophisticated access to a
domain. The resulting term structure can be utilized
as a computational thesaurus or incorporated as part
of a larger domain ontology.
References
N. Aussenac-Gilles, B. Biebow, and S. Szulman.
2003. D'une methode a un guide pratique de
modelisation de connaissances a partir de textes.
In Proc. of the 5th Conference on Terminologie
et Intelligence Articielle, Strasbourg, March 31 -
April 1.
K. Barker and S. Szpakowicz. 1998. Semi-Automatic
Recognition of Noun Modier Relationships. In
Proc. of COLING-ACL98, Montreal, Quebec,
Canada, August 10-14.
D. Bourigault, C. Jacquemin, and M-C. L'Homme,
editors. 2001. Recent Advances in Computational
Terminology, volume 2. John Benjamins.
J. Dowdall, M. Hess, N. Kahusk, K. Kaljurand,
M. Koit, F. Rinaldi, and K. Vider. 2002. Tech-
nical Terminology as a Critical Resource. In Proc.
of LREC-02, Las Palmas, 29 { 31 May.
P. Downing. 1977. On the creation and use of english
compound nouns. Language, (53):810 { 842.
D.A Evans, R.G. Leerts, G. Grefenstette, S.K. Han-
derson, W.R. Hersh, and A.A.Archbold. 1992.
CLARIT TREC design, experiments and results.
Technical report, Carnegie Mellon University.
T. Finin. 1980. The semantic interpretation of nom-
inal compounds. In Proceedings "Articial Intelli-
gence, pages 310 { 312. Stanford.
L.S. Gay and W.B. Croft. 1990. Interpreting nomi-
nal compounds for information retrieval. Informa-
tion Processing and Management, 26(1):21 { 38.
F. Ibekwe-SanJuan and E. SanJuan. 2003. From
term variants to research topics. Journal of
Knowledge Organization (ISKO), special issue on
Human Language Technology, 29(3/4).
F. Ibekwe-SanJuan. 1998. Terminological variation,
a means of identifying research topics from texts.
In Proc. of Joint ACL-COLING'98, pages 564 {
570, Quebec, 10-14 August.
F. Ibekwe-SanJuan. 2001. Extraction termi-
nologique avec intex. In Proc.of the 4th Annual
INTEX Workshop, Bordeaux, 10-11 June.
P. Isabelle. 1984. Another look at nominal com-
pounds. In Proc. of the 10th International Con-
ference on Computational Linguistics (COLING
'84), pages 509{516, Stanford, USA.
C. Jacquemin. 1995. A symbolic and surgical ac-
quisition of terms through variation. In Proc. of
IJCAI95, Montreal.
C. Jacquemin. 2001. Spotting and discovering terms
through Natural Language Processing. MIT Press.
K. Kageura. 2002. The dynamics of Terminology: A
descriptive theory of term formation and termino-
logical growth. John Benjamins, Amsterdam.
J. N. Levi. 1979. The syntax and semantics of com-
plex nominals. Academic press, New York.
J. Pearson. 1998. Terms in Context. John Ben-
jamins, Amsterdam.
F. Popowich, P. Mcfetridge, D. Fass, and G. Hall.
1992. Processing complex noun phrases in a natu-
ral language interface to a statistical database. In
Proceedings COLING'92, pages 46 { 51, Nantes,
August 23 { 28.
F. Rinaldi, M. Hess, D. Molla, R. Schwitter, J. Dow-
dall, G. Schneider, and R. Fournier. 2002. Answer
Extraction in Technical Domains. In Proc. of CI-
CLing 2002, Mexico City, February.
I. A. Sag, T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger. 2002. Multiword Expressions: a
Pain in the Neck for NLP. In Proc. of CICLing
2002, Mexico City, February.
M. Silberztein. 1993. Dictionnaires Electroniques
et Analyse Lexicale du Francais - Le Systeme IN-
TEX. Masson, Paris.
F. Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, (19):143 { 177.
A. F. Smeaton and P. Sheridan. 1992. The appli-
cation of morpho-syntactic language processing to
eective phrase-matching. Information Processing
and Management, 28(3):349 { 369.
Parmenides: an opportunity for ISO TC37 SC4?
Fabio Rinaldi
1
, James Dowdall
1
, Michael Hess
1
, Kaarel Kaljurand
1
, Andreas Persidis
2
,
Babis Theodoulidis
3
, Bill Black
3
, John McNaught
3
, Haralampos Karanikas
3
, Argyris Vasilakopoulos
3
,
Kelly Zervanou
3
, Luc Bernard
3
, Gian Piero Zarri
4
, Hilbert Bruins Slot
5
, Chris van der Touw
5
,
Margaret Daniel-King
6
, Nancy Underwood
6
, Agnes Lisowska
6
, Lonneke van der Plas
6
,
Veronique Sauron
6
, Myra Spiliopoulou
7
, Marko Brunzel
7
, Jeremy Ellman
8
,
Giorgos Orphanos
9
, Thomas Mavroudakis
10
, Spiros Taraviras
10
.
Abstract
Despite the many initiatives in recent years
aimed at creating Language Engineering
standards, it is often the case that dierent
projects use dierent approaches and often
dene their own standards. Even within the
same project it often happens that dierent
tools will require dierent ways to represent
their linguistic data.
In a recently started EU project focusing
on the integration of Information Extrac-
tion and Data Mining techniques, we aim
at avoiding the problem of incompatibility
among dierent tools by dening a Com-
mon Annotation Scheme internal to the
project. However, when the project was
started (Sep 2002) we were unaware of the
standardization eort of ISO TC37/SC4,
and so we commenced once again trying to
dene our own schema. Fortunately, as this
work is still at an early stage (the project
will last till 2005) it is still possible to redi-
rect it in a way that it will be compati-
ble with the standardization work of ISO.
In this paper we describe the status of the
work in the project and explore possible
synergies with the work in ISO TC37 SC4.
1 1
Institute of Computational Linguistics, Uni-
versity of Zurich, Switzerland;
2
Biovista, Athens,
Greece;
3
Centre for Research in Information Manage-
ment, UMIST, Manchester, UK;
4
CNRS, Paris, France;
5
Unilever Research and Development, Vlaardingen,
The Netherlands;
6
TIM/ISSCO, University of Geneva,
Switzerland;
7
Uni Magdeburg, Germany;
8
Wordmap
Ltd., Bath, UK;
9
Neurosoft, Athens, Greece;
10
The
Greek Ministry of National Defense, Athens, Greece
1 Introduction
It is by now widely accepted that some W3C stan-
dards (such as XML and RDF) provide a con-
venient and practical framework for the creation
of eld-specic markup languages (e.g. MathML,
VoiceXML). However XML provides only a common
\alphabet" for interchange among tools, the steps
that need to be taken before there is any real shar-
ing are still many (just as many human languages
share the same alphabets, that does not mean that
they can be mutually intelligible). The necessary
step to achieve mutual understanding in Language
Resources is to create a common data model.
The existence of a standard brings many other
advantages, like the ability to automatically com-
pare the results of dierent tools which provide the
same functionality, from the very basic (e.g. tok-
enization) to the most complex (e.g. discourse rep-
resentation). Some of the NIST-supported competi-
tive evaluations (e.g. MUC) greatly beneted by the
existence of scoring tools, which could automatically
compare the results of each participant against a gold
standard. The creation of such tools (and their ef-
fectiveness) was possible only because the organizing
institute had pre-dened and \imposed" upon the
participants the annotation scheme. However, that
sort of \brute force" approach might not always pro-
duce the best results. It is important to involve the
community in the denition of such standards at an
early stage, so that all the possible concerns can be
met and a wider acceptance can be achieved.
Another clear benet of agreed standards is that
they will increase interoperability among dierent
tools. It is not enough to have publicly available
APIs to ensure that dierent tools can be integrated.
In fact, if their representation languages (their \data
vocabulary") are too divergent, no integration will
be possible (or at least it will require a considerable
mapping eort). For all the above reasons we enthu-
siastically support any concertation work, aimed at
establishing common foundations for the eld.
In a recently started EU project (\Parmenides")
focusing on the integration of Information Extrac-
tion and Data Mining techniques (for Text Mining)
we aim at avoiding the problem of incompatibility
among dierent tools by dening a Common Annota-
tion Scheme internal to the project. However, when
the project was started (Sep 2002) we were unaware
of the standardization eort of ISO TC37 SC4, and
so we commenced once again trying to dene our own
schema. Fortunately, as this work is still at an early
stage (the project will last till 2005) it is still possible
to redirect it in a way that it will be compatible with
the standardization work of ISO.
In this paper we will describe the approach fol-
lowed so far in the denition of the Parmenides Com-
mon Annotation Scheme, even if its relation with ISO
is still only supercial. In the forthcoming months
our intention is to explore possible synergies between
our work and the current initiatives in ISO TC37
SC4, with the aim to get at a Parmenides annota-
tion scheme which is conformant to the approach cur-
rently discussed in the standardization committee.
2 The Parmenides Lingua Franca
In this section we will describe the XML-based anno-
tation scheme proposed for the Parmenides project.
In general terms the project is concerned with or-
ganisational knowledge management, specically, by
developing an ontology driven systematic approach
to integrating the entire process of information gath-
ering, processing and analysis.
The annotation scheme is intended to work as the
projects' lingua franca: all the modules will be re-
quired to be able to accept as input and generate
as output documents conformant to the (agreed) an-
notation scheme. The specication will be used to
create data-level compatibility among all the tools
involved in the project.
Each tool might choose to use or ignore part of
the information dened by the markup: some infor-
mation might not yet be available at a given stage
of processing or might not be required by the next
module. Facilities will need to be provided for lter-
ing annotations according to a simple conguration
le. This is in fact one of the advantages of using
XML: many readily available o-the-shelf tools can
be used for parsing and ltering the XML annota-
tions, according to the needs of each module.
The annotation scheme will be formally dened by
a DTD and an equivalent XML schema denition.
Ideally the schema should remain exible enough to
allow later additional entities when and if they are
needed. However the present document has only an
illustrative purpose, in particular the set of anno-
tation elements introduced needs to be further ex-
panded and the attributes of all elements need to be
veried.
There are a number of simplications which have
been taken in this document with the purpose of
keeping the annotation scheme as simple as possible,
however they might be put into question and more
complex approaches might be required. For instance
we assume that we will be able to identify a unique
set of tags, suitable for all the applications. If this
proves to be incorrect, a possible way to deal with
the problem is the use of XML namespaces. Our
assumptions allow us (for the moment) to keep all
XML elements in the same namespace (and there-
fore ignore the issue altogether).
2.1 Corpus Development
The annotation scheme will be used to create a de-
velopment corpus - a representative sample of the
domain, provided by the users as typical of the doc-
uments they manually process daily. In this phase,
the documents are annotated by domain experts for
the information of interest. This provides the bench-
mark against which algorithms can be developed and
tested to automate extraction as far as possible.
Of primary importance to the annotation process
is the consolidation of the \information of interest",
the text determined as the target of the Information
Extraction modules. Given the projects' goals, this
target will be both diverse and complex necessitating
clarity and consensus.
2.2 Sources Used for this Document
Parmenides aims at using consolidated Information
Extraction techniques, such as Named Entity Ex-
traction, and therefore this work builds upon well-
known approaches, such as the Named Entity anno-
tation scheme from MUC7 (Chinchor, 1997). Cru-
cially, attention will be paid to temporal annota-
tions, with the aim of using extracted temporal in-
formation for detection of trends (using Data Min-
ing techniques). Therefore we have investigated all
the recently developed approaches to such a problem,
and have decided for the adoption of the TERQAS
tagset (Ingria and Pustejovsky, 2002; Pustejovsky et
al., 2002).
Other sources that have been considered include
the GENIA tagset (GENIA, 2003), TEI (TEI Con-
sortium, 2003) and the GDA
1
tagset. The list of
entities introduced so far is by no means complete
1
http://www.i-content.org/GDA/tagset.html
but serves as the starting point, upon which to build
a picture of the domains from information types they
contain. The domain of interests (e.g. Biotechnol-
ogy) are also expected to be terminology-rich and
therefore require proper treatment of terminology.
To supplement the examples presented, a com-
plete document has been annotated according to the
outlined specication.
2
There are currently three
methods of viewing the document which oer dif-
fering ways to visualize the annotations. These
are all based on transformation of the same XML
source document, using XSLT and CSS (and some
Javascript for visualization of attributes). For exam-
ple, the basic view can be seen in gure (1).
3 Levels of Annotation
The set of Parmenides annotations is organized into
three levels:
 Structural Annotations
Used to dene the physical structure of the doc-
ument, it's organization into head and body,
into sections, paragraphs and sentences.
3
 Lexical Annotations
Associated to a short span of text (smaller than
a sentence), and identify lexical units that have
some relevance for the Parmenides project.
 Semantic Annotations
Not associated with any specic piece of text
and as such could be free-oating within the
document, however for the sake of clarity, they
will be grouped into a special unit at the end
of the document. They refer to lexical anno-
tations via co-referential Ids. They (partially)
correspond to what in MUC7 was termed `Tem-
plate Elements' and `Template Relations'.
Structural annotations apply to large text spans,
lexical annotations to smaller text spans (sub-
sentence). Semantic annotations are not directly
linked to a specic text span, however, they are
linked to text units by co-referential identiers.
All annotations are required to have an unique ID
and thus will be individually addressable, this allows
semantic annotations to point to the lexical annota-
tions to which they correspond. Semantic Annota-
tions themselves are given a unique ID, and therefore
can be elements of more complex annotations (\Sce-
nario Template" in MUC parlance).
2
available at http://www.ifi.unizh.ch/Parmenides
3
Apparently the term 'structure' is used with a dif-
ferent meaning in the ISO documentation, referring
to morpho-syntactical structure rather than document
structure.
Structural Annotations The structure of the
documents will be marked using an intuitively appro-
priate scheme which may require further adaptations
to specic documents. For the moment, the root
node is <ParDoc> (Parmenides Document) which
can contain <docinfo>, <body>, <ParAnn>. The
<docinfo> might include a title, abstract or sum-
mary of the documents contents, author informa-
tion and creation/release time. The main body
of the documents (<body>) will be split into sec-
tions (<sec>) which can themselves contain sec-
tions as well as paragraphs (<para>). Within the
paragraphs all sentences will be identied by the
<sentence> tag. The Lexical Annotations will
(normally) be contained within sentences. The -
nal section of all documents will be <ParAnn> (Par-
menides Annotations) where all of the semantic an-
notations that subsume no text are placed. Figure
(2) demonstrates the annotation visualization tool
displaying the documents structure (using nested
boxes).
Lexical Annotations Lexical Annotations are
used to mark any text unit (smaller than a sentence),
which can be of interest in Parmenides. They include
(but are not limited to):
1. Named Entities in the classical MUC sense
2. New domain-specic Named Entities
3. Terms
4. Temporal Expressions
5. Events
6. Descriptive phrases (chunks)
The set of Lexical Annotations described in this
document will need to be further expanded to cover
all the requirements of the project, e.g. names of
products (Acme Arms International's KryoZap (TM)
tear gas riot control gun), including e.g. names of
drugs (Glycocortex's Siderocephalos).
When visualizating the set of Lexical Tags in a
given annotated document, clicking on specic tags
displays the attribute values (see gure (3)).
Semantic Annotations The relations that exist
between lexical entities are expressed through the
semantic annotations. So lexically identied peo-
ple can be linked to their organisation and job ti-
tle, if this information is contained in the document
(see gure (4)). In terms of temporal annotations, it
is the explicit time references and events which are
identied lexically, the temporal relations are then
captured through the range of semantic tags.
Figure 1: Basic Annotation Viewing
3.1 Example
While the structural annotations and lexical annota-
tions should be easy to grasp as they correspond to
accepted notions of document structure and of con-
ventional span-based annotations, an example might
help to illustrate the role of semantic annotations.
(1) The recent ATP award is
<ENAMEX id="e8" type="ORGANIZATION">
Dyax
</ENAMEX>
's second, and follows a
<NUMEX id="n5" type="MONEY">
$4.3 million
</NUMEX>
<ENAMEX id="e9" type="ORGANIZATION">
NIST
</ENAMEX>
grant to
<ENAMEX id="e10" type="ORGANIZATION">
Dyax
</ENAMEX>
and
<ENAMEX id="e11" type="ORGANIZATION">
CropTech Development Corporation
</ENAMEX>
in
<TIMEX3 tid="t4" type="DATE" value="1997">
1997
</TMEX3>
There are two occurrences of Dyax in this short
text: the two Lexical Entities e8 and e10, but clearly
they correspond to the same Semantic Entity. To
capture this equivalence, we could use the syntactic
notion of co-reference (i.e. Identify the two as co-
referent). Another possible approach is to make a
step towards the conceptual level, and create a se-
mantic entity, of which both e8 and e10 are lexical
expressions (which could be dierent, e.g. \Dyax",
\Dyax Corp.", \The Dyax Corporation"). The sec-
ond approach can be implemented using an empty
XML element, created whenever a new entity is men-
tioned in text. For instance, in (2) we can use the tag
Figure 2: Visualization of Structural Annotations
<PEntity> (which stands for Parmenides Entity).
(2) <PEntity peid="obj1" type="ORGANIZATION"
mnem="Dyax" refid="e1 e3 e6 e8 e10 e12"/>
The new element is assigned (as usual) a unique
identication number and a type. The attribute mnem
contains just one of the possible ways to refer to the
semantic entity (a mnemonic name, possibly chosen
randomly). However, it also takes as the value of
the refid attribute as many coreferent ids as are
warranted by the document. In this way all lexical
manifestations of a single entity are identied. All
the lexical entities which refer to this semantic entity,
are possible ways to `name' it (see also g. 4).
Notice that the value of the `type' attribute has
been represented here as a string for readability pur-
poses, in the actual specication it will be a pointer
to a concept in a domain-specic Ontology.
Other semantic entities from (1) are:
(3) <PEntity peid="obj2" type="ORGANIZATION"
mnem="NIST" refid="e2 e4 e7 e9"/>
<PEntity peid="obj3" type="ORGANIZATION"
mnem="CropTech" refid="e11"/>
The newly introduced semantic entities can then
be used to tie together people, titles and organiza-
tions on the semantic level. Consider for example
the text fragment (4), which contains only Lexical
Annotations.
(4) ... said
<ENAMEX id="e17" type="PERSON">
Charles R. Wescott
</ENAMEX>
, Ph.D.,
<ROLE type='x' id="x5">
Senior Scientist
</ROLE>
at
<ENAMEX id="e60" type="ORGANIZATION">
Dyax Corp
</ENAMEX>
The Lexical Entity e17 requires the introduction
of a new semantic entity, which is given the arbitrary
identier `obj5':
(5) <PEntity peid="obj5" type="PERSON"
mnem="Charles R. Wescott" refid="e17"/>
Figure 3: Visualization of Lexical Annotations and their attributes
In turn, this entity is linked to the entity obj1
from (1) by a relation of type `workFor' (PRelation
stands for Parmenides Relation):
(6) <PRelation prid="rel2" source="obj5"
target="obj1" type="worksFor" role="Senior
Scientist" evidence="x5"/>
4 Discussion
As the status of the Parmenides annotation scheme
is still preliminary, we aim in this section to pro-
vide some justication for the choices done so far
and some comparison with existing alternatives.
4.1 Named Entities
One of the purposes of Named Entities is to instanti-
ate frames or templates representing facts involving
these elements. A minor reason to preserve the clas-
sic named entities is so that we can test an IE system
against the MUC evaluation suites and know how
it is doing compared to the competition and where
there may be lacunae. As such, the MUC-7 speci-
cation (Chinchor, 1997) is adopted with the minor
extension of a non-optional identication attribute
on each tag.
4.2 Terminology
A term is a means of referring to a concept of a spe-
cial subject language; it can be a single wordform,
a multiword form or a phrase, this does not matter.
The only thing that matters is that it has special
reference: the term is restricted to refer to its con-
cept of the special domain. The act of (analytically)
dening xes the special reference of a term to a con-
cept. Thus, it makes no sense to talk of a term not
having a denition. A concept is described by den-
ing it (using other certain specialised linguistic forms
(terms) and ordinary words), by relating it to other
concepts, and by assigning a linguistic form (term)
to it.
If we are interested in fact extraction from densely
terminological texts with few named entities apart
from perhaps names of authors, names of laborato-
ries, and probably many instances of amounts and
measures, then we would need to rely much more on
prior identication of terms in the texts, especially
where these are made up of several word forms.
A term can have many variants: even standard-
ised terms have variants e.g. singular, plural forms
of a noun. Thus we should perhaps more correctly
refer to a termform, at least when dealing with text.
Among variants one can also include acronyms and
reduced forms. You therefore nd a set of variants,
typically, all referring to the same concept in a special
domain: they are all terms (or termforms). Again
this problem pinpoints the need for a separation of
the lexical annotations (the surface variants within
the document) and semantic annotations (pointing
abstractly to the underlying concept).
4.3 Approaches to Temporal Annotations
TIDES (Ferro et al, 2001) is a temporal annota-
tion scheme that was developed at the MITRE Cor-
poration and it can be considered as an extension
of the MUC7 Named Entity Recognition (Tempo-
ral Entity Recognition - TIMEX Recognition) (Chin-
chor, 1997). It aims at annotating and normalizing
explicit temporal references. STAG (Setzer, 2001)
is an annotation scheme developed at the University
of She?eld. It has a wider focus than TIDES in
the sense that it combines explicit time annotation,
event annotation and the ability to annotate tempo-
ral relations between events and times.
TimeML (Ingria and Pustejovsky, 2002) stands for
\Time Markup Language" and represents the inte-
gration and consolidation of both TIDES and STAG.
It was created at the TERQAS Workshop
4
and is
designed to combine the advantages of the previous
temporal annotations schemes. It contains a set of
tags which are used to annotate events, time expres-
sions and various types of event-event, event-time
and time-time relations. TimeML is specically tar-
geted at the temporal attributes of events (time of
occurrence, duration etc.).
As the most complete and recent, TimeML should
be adopted for the temporal annotations. Broadly,
its organization follows the Parmenides distinction
between lexical/semantic annotations. Explicit tem-
poral expressions and events receive an appropriate
(text subsuming) lexical tag. The temporal rela-
tions existing between these entities are then cap-
tured through a range of semantic (non-text subsum-
ing) tags.
For example, each event introduces a correspond-
ing semantic tag. There is a distinction be-
tween event \tokens" and event \instances" moti-
vated by predicates that represent more than one
event. Accordingly, each event creates a semantic
<MAKEINSTANCE> tag that subsumes no text. Ei-
ther, one tag for each realised event or a single tag
with the number of events expressed as the value of
the cardinality attribute. The tag is introduced and
the event or to which it refers is determined by the
attributes eventID.
5 Conclusion
We believe that ISO TC37/SC4 provides a very in-
teresting framework within which specic research
concerns can be addressed without the risk of rein-
venting the wheel or creating another totally new
4
http://www.cs.brandeis.edu/~jamesp/arda/time
and incompatible annotation format. The set of an-
notations that we have been targeting so far in Par-
menides is probably a small subset of what is tar-
geted by ISO TC37/SC4. Although we had only lim-
ited access to the documentation available, we think
our approach is compatible with the work being done
in ISO.
It is, we believe, extremely important for a project
like ours, to be involved directly in the ongoing dis-
cussion. Moreover we are at precisely the right stage
for a more direct `exposure' to the ISO TC37/SC4
discussion, as we have completed the exploratory
work but no irrevocable modeling commitment has
so far been taken. Therefore we would hope to be-
come more involved in order to make our proposal
t exactly into that framework. The end result of
this process might be that Parmenides could become
a sort of \Guinea Pig" for at least a subset of ISO
TC37 SC4.
Acknowledgments
The Parmenides project is funded by the European
Commission (contract No. IST-2001-39023) and
by the Swiss Federal O?ce for Education and Sci-
ence (BBW/OFES). All the authors listed have con-
tributed to the (ongoing) work described in this pa-
per. Any remaining errors are the sole responsibility
of the rst author.
References
Nancy Chinchor. 1997. MUC-7 Named Entity Task Denition, Version
3.5. http://www.itl.nist.gov/iaui/894.02/
related projects/muc/proceedings/ne task.html.
Lisa Ferro, Inderjeet Mani, Beth Sundheim, and George Wilson. 2001.
Tides temporal annotation guidelines, version 1.0.2. Technical re-
port, The MITRE Corporation.
GENIA. 2003. Genia project home page. http://www-tsujii.is.s.u-
tokyo.ac.jp/~genia.
Bob Ingria and James Pustejovsky. 2002. TimeML
Specication 1.0 (internal version 3.0.9), July.
http://www.cs.brandeis.edu/%7Ejamesp/arda
/time/documentation/TimeML-Draft3.0.9.html.
James Pustejovsky, Roser Sauri, Andrea Setzer, Rob Giazauskas, and
Bob Ingria. 2002. TimeML Annotation Guideline 1.00 (internal
version 0.4.0), July. http://www.cs.brandeis.edu/%7Ejamesp/arda
/time/documentation/TimeML-Draft3.0.9.html.
Andrea Setzer. 2001. Temporal Information in Newswire Articles: An
Annotation Scheme and Corpus Study. Ph.D. thesis, University of
She?eld.
TEI Consortium. 2003. The text encoding initiative. http://www.tei-
c.org/.
Figure 4: Visualization of Semantic Annotations
Answering Questions in the Genomics Domain
Fabio Rinaldi, James Dowdall, Gerold Schneider
Institute of Computational Linguistics,
University of Zurich, CH-8057 Zurich
Switzerland
{rinaldi, dowdall, gschneid}@cl.unizh.ch
Andreas Persidis
Biovista, 34 Rodopoleos Str.,
Ellinikon, GR-16777 Athens,
Greece
andreasp@biovista.com
Abstract
In this paper we describe current efforts aimed at
adapting an existing Question Answering system to
a new document set, namely research papers in the
genomics domain. The system has been originally
developed for another restricted domain, however it
has already proved its portability. Nevertheless, the
process is not painless, and the specific purpose of
this paper is to describe the problems encountered.
1 Introduction
One of the core problems in exploiting scientific
papers in research and clinical settings is that the
knowledge that they contain is not easily acces-
sible. Although various resources which attempt
to consolidate such knowledge are being created
(e.g. UMLS1, SWISS-PROT, OMIM, GeneOntol-
ogy, GenBank, LocusLink), the amount of informa-
tion available keeps growing exponentially (Stapley
and Benoit, 2000).
There is accordingly a pressing need for intelli-
gent systems capable of accessing that information
in an efficient and user-friendly way. Question An-
swering systems aim at providing a focused way
to access the information contained in a document
collection. Specific research in the area of Ques-
tion Answering has been prompted in the last few
years in particular by the Question Answering track
of the Text REtrieval Conference (TREC-QA) com-
petitions (Voorhees, 2001). The TREC-QA compe-
titions focus on open-domain systems, i.e. systems
that can (potentially) answer any generic question.
As these competitions are based on large volumes
of text, the competing systems (normally) resort to a
relatively shallow text analysis.2 In contrast a ques-
tion answering system working on a restricted do-
main can take advantage of the formatting and style
1
http://www.nlm.nih.gov/research/umls/
2With some notable exception, e.g. (Harabagiu et al, 2001).
conventions in the text, can make use of the specific
domain-dependent terminology, and of full parsing.
In many restricted domains, including technical
documentation and research papers, terminology
plays a pivotal role. This is in fact one of the
major differences between restricted domains and
open domain texts. While in open domain systems
Named Entities play a major role, in technical doc-
umentation, as well as in research papers, they have
a secondary role, by contrast a far greater role is
played by domain terminology. Terminology is a
major obstacle for processing research papers and
at the same time a key access path to the knowledge
encoded in those papers. Terminology provides the
means to name and access domain-specific concepts
and objects.
Restricted domains present the additional prob-
lem of ?domain navigation?. Users of the system
cannot always be expected to be completely fa-
miliar with the domain terminology. Unfamiliar-
ity with domain terminology might lead to ques-
tions which contain imperfect formulations of do-
main terms. It becomes therefore essential to be
able to detect terminological variants and exploit the
relations between terms (like synonymy, meronymy,
antonymy). The process of variation is well in-
vestigated in terminological research (Daille et al,
1996). In the Biomedical domain, an example of a
system that deals with terminological variants (also
called ?aliases?) can be found in (Pustejovsky et al,
2002).
In the rest of this paper we will first briefly de-
scribe our existing Question Answering system, Ex-
trAns (section 2). In the following section (3) we
detail the specific problems encountered in the new
domain and the steps that we have taken to solve
them. We conclude the paper with an overview of
related research (section 4).
Figure 1: Example of document to be analyzed
2 The original Question Answering system
ExtrAns is a Question Answering system aimed at
restricted domains, in particular terminology-rich
domains. While open domain Question Answering
systems typically are targeted at large text collec-
tions and use relatively little linguistic information,
ExtrAns answers questions over such domains by
exploiting linguistic knowledge from the documents
and terminological knowledge about a specific do-
main. Various applications of the ExtrAns system
have been developed, from the original prototype
aimed at the Unix documentation files (Molla? et al,
2000) to a version targeting the Aircraft Mainte-
nance Manuals (AMM) of the Airbus A320 (Molla?
et al, 2003; Rinaldi et al, 2004). In the present pa-
per we describe current work in applying the system
to a different domain and text type: research papers
in the genomics area.
Our approach to Question Answering is particu-
larly computationally intensive; this allows a deeper
linguistic analysis to be performed, at the cost of
higher processing time. The documents are an-
alyzed in an off-line stage and transformed in a
semantic representation (called ?Minimal Logical
Forms? or MLFs), which is stored in a Knowledge
Base (KB). In an on-line phase (see fig. 2) the user
queries are analyzed using the same basic machin-
ery (however the cost of processing them is neg-
ligible, so that there is no visible delay) and their
semantic representation is matched in the KB. If a
match is encountered, the sentences that gave origin
to the match are presented as possible answer to the
question.
Documents (and queries) are first tokenized, then
they go through a terminology-processing module.
If a term belonging to a synset in the terminolog-
ical knowledge base is detected, then the term is
replaced by a synset identifier in the logical form.
This results in a canonical form, where the synset
identifier denotes the concept that each of the terms
in the synset names. In this way any term contained
in a user query is automatically mapped to all its
variants. This approach amounts to an implicit ?ter-
minological normalization? for the domain, where
the synset identifier can be taken as a reference to
SemanticMatching
DocumentKB
document logicalform
AnswersinDocument
DocumentLinguisticProcessingQUERY QueryFiltering
Thesaurus
QUERY+Synset
Figure 2: Schematic representation of the core QA engine
the ?concept? that each of the terms in the synset de-
scribes (Kageura, 2002).
ExtrAns depends heavily on its use of logical
forms, which are designed so that they are easy to
build and to use, yet expressive enough for the task
at hand (Molla?, 2001). The logical forms and asso-
ciated semantic interpretation methods are designed
to cope with problematic sentences, which include
very long sentences, even sentences with spelling
mistakes, and structures that are not recognized by
the syntactic analyzer. An advantage of ExtrAns?
Minimal Logical Forms (MLFs) is that they can be
produced with minimal domain knowledge. This
makes our technology easily portable to different
domains. The only true impact of the domain is
during the preprocessing stage of the input text and
during the creation of a thesaurus that reflects the
specific terms used in the chosen domain, their lex-
ical relations and their word senses.
Unlike sentences in documents, user queries
are processed on-line and the resulting MLFs are
proved by deduction over the MLFs of document
sentences stored in the KB. When no direct answer
for a user query can be found, the system is able to
relax the proof criteria in a stepwise manner. First,
hyponyms are added to the query terms. This makes
the query more general but maintains its logical cor-
rectness. If no answers can be found or the user
determines that they are not good answers, the sys-
tem will attempt approximate matching, in which
the sentence that has the highest overlap of predi-
cates with the query is retrieved. The matching sen-
tences are scored and the best matches are returned.
The MLFs contain pointers to the original text
which allow ExtrAns to identify and highlight those
words in the retrieved sentence that contribute most
to a particular answer. An example of the output of
ExtrAns can be seen in fig. 3. When the user clicks
on one of the answers provided, the corresponding
document will be displayed with the relevant pas-
sages highlighted. Another click displays the an-
swer in the context of the document and allows the
user to verify the justification of the answer.
3 Moving to the new domain
The first step in adapting the system to a new do-
main is identifying the specific set of documents to
be analyzed. We have experimented with two dif-
ferent collections in the genomics domain. The first
collection (here called the ?Biovista? corpus) has
been generated from Medline using two seed term
lists of genes and pathways (biological process) to
extract an initial corpus of research papers (full ar-
ticles). The second collection is constituted by the
GENIA corpus (Kim et al, 2003)3, which contains
2000 abstracts from Medline (a total of 18546 sen-
tences). The advantage of the latter is that domain-
specific terminology is already manually annotated.
However focusing only on that case would mean
disregarding a number of real-world problems (in
particular terminology detection).
3.1 Formatting information
An XML based filtering tool has been used to select
zones of the documents that need to be processed
in a specific fashion. Consider for instance the case
of bibliography. The initial structure of the docu-
ment allows to identify easily each bibliographical
item. Isolating the authors, titles and publication in-
formation is then trivial (because it follows a regular
structure). The name of the authors (together with
the html cross-references) can then be used to iden-
tify the citations within the main body of the paper.
If a preliminary zone identification (as described) is
not performed, the names of the authors used in the
citations would appear as spurious elements within
sentences, making their analysis very difficult.
Another common case is that of titles. Normally
they are Nominal Phrases rather than sentences. If
3
http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
Figure 3: Example of interaction with the system
the parser was expecting to find a sentence it would
fail. However using the knowledge that a title is
being processed, we can modify the configuration
of the parser so that it accepts an NP as a correct
parse.
3.2 Terminology
The high frequency of terminology in technical text
produces various problems when locating answers.
A primary problem is the increased difficulty of
parsing text in a technical domain due to domain-
specific sublanguage. Various types of multi-word
terms characterize these domains, in particular re-
ferring to specific concepts (e.g. genome sequences,
proteins). These multi-word expressions might in-
clude lexical items which are either unknown to a
generic lexicon (e.g. ?argentine methylation?), have
a specific meaning unique to this domain or dever-
bal adjectives (and nouns) are often mistagged as
verbs (e.g. ?mediated activation?, ?cell killing?).
Abbreviations and acronyms, often complex (e.g.
bracketed inside NPs, like ?adenovirus (ad) infec-
tion?) are another common source of inconsisten-
cies. In such cases the parser might either fail to
identify the compound as a phrase and consequently
fail to parse the sentence including such items. Al-
ternatively a parser might attempt to ?guess? their
lexical category (in the set of open class categories),
leading to an exponential growth of the number of
possible syntactic parses and often incorrect deci-
sions. Not only the internal structure of the com-
pound can be multi-way ambiguous, also the bound-
aries of the compounds are difficult to detect and the
parsers may try odd combinations of the tokens be-
longing to the compounds with neighboring tokens.
We have described in (Rinaldi et al, 2002) some
approaches that might be taken towards terminology
extraction for a specific domain. The GENIA cor-
pus removes these problems completely by provid-
ing pre-annotated terminological units. This allows
attention to be focused on other challenges of the
QA task, rather than getting ?bogged down? with
terminology extraction and organization.
In the case of the Biovista corpus, we had to
perform a phase of terminology discovery, which
was facilitated by the existence of the seed lists of
genes and pathways. We first marked up those terms
which appear in the corpus using additional xml
tags. This identified 900 genes and 218 pathways
that occur in the corpus - represented as boxed to-
kens in fig. 4. Next the entire corpus is chunked into
nominal and verbal chunks using LT Chunk (Finch
and Mikheev, 1997). Ignoring prepositions and
gerunds the chunks are a minimal phrasal group -
represented as the square braces in fig. 4. The cor-
pus terms are then expanded to the boundary of the
phrasal chunk they appear in. For example, NP3 in
fig. 4 contains two terms of interest producing the
new term ?IFN-induced transcription?. The 1118
corpus terms were expanded into 6697 new candi-
date terms. 1060 involve a pathway in head position
and 1154 a gene. The remaining 4483 candidate
terms involve a novel head with at least one gene or
pathway as a modifier.
Once the terminology is available, it is necessary
to detect relations among terms in order to exploit
Argentine methylation of  STAT1  modulates   IFN -induced  transcription
NP1 VBZ
subj
NP2 NP3
prepmodpp
obj
Figure 4: An example of syntactic analysis
it. We have focused our attention in particular to
the relations of synonymy and hyponymy, which
are detected as described in (Dowdall et al, 2003)
and gathered in a Thesaurus. The organizing unit is
the WordNet style synset which includes strict syn-
onymy as well as three weaker synonymy relations.
These sets are further organized into a isa hierarchy
based on two definitions of hyponymy.
One of the most serious problems that we have
encountered in working in restricted domains is
the syntactic ambiguity generated by multi-word
units, in particular technical terms. Any generic
parser, unless developed specifically for the do-
main at hand, will have serious problems dealing
with those multi-words. The solution that we have
adopted is to parse multi-word terms as single syn-
tactic units. The tokenizer detects the terms (pre-
viously collected in the Thesaurus) as they appear in
the input stream, and packs them into single lexical
tokens prior to syntactical analysis, assigning them
the syntactic properties of their head word. In previ-
ous work this approach has proved to be particularly
effective, bringing a reduction in the complexity of
parsing of 46% (Rinaldi et al, 2002).
3.3 Parsing
The deep syntactic analysis builds upon the chunks
to identify sentence level syntactic relations be-
tween the heads of the chunks. The output is a
hierarchical structure of syntactic relations - func-
tional dependency structures - represented as the di-
rected arrows in fig. 4. The parser (Pro3Gres) uses
hand-written declarative rules to encode acknowl-
edged facts, such as verbs typically take one but
never two subjects, combined with a statistical lan-
guage model that calculates lexicalized attachment
probabilities, similar to (Collins, 1999). Parsing is
seen as a decision process, the probability of a total
parse is the product of probabilities of the individual
decisions at each ambiguous point in the derivation.
Probabilistic parsers generally have the advan-
tage that they are fast and robust, and that they
resolve syntactic ambiguities with high accuracy.
Both of these points are prerequisites for a statistical
analysis that is feasible over large amounts of text
and beneficial to the Q&A system?s performance.
In comparison to shallow processing methods,
parsing has the advantage that relations spanning
long stretches of text can still be recognized, and
that the parsing context largely contributes to the
disambiguation. In comparison to deep linguistic,
formal grammar-based parsers, however, the output
of probabilistic parsers is relatively shallow, pure
context-free grammar (CFG) constituency output,
tree structures that do not include grammatical func-
tion annotation nor co-indexation and empty nodes
annotation expressing long-distance dependencies
(LDD). In a simple example sentence ?John wants
to leave?, a deep-linguistic syntactic analysis ex-
presses the identity of the explicit matrix clause
subject and implicit subordinate clause subject by
means of co-indexing the explicit and the empty im-
plicit subject trace t: ?[John1 wants [t1 to leave]]?.
A parser that fails to recognize these implicit sub-
jects, so-called control subjects, misses very impor-
tant information, quantitatively about 3 % of all sub-
jects.
Although LDD annotation is actually provided in
Treebanks such as the Penn Treebank (Marcus et al,
1993) over which they are typically trained, most
probabilistic parsers largely or fully ignore this in-
formation. This means that the extraction of LDDs
and the mapping to shallow semantic representa-
tions such as MLF is not always possible, because
first co-indexation information is not available, sec-
ond a single parsing error across a tree fragment
containing an LDD makes its extraction impossible,
third some syntactic relations cannot be recovered
Figure 5: Dependency Tree output of the SWI Prolog graphical implementation of the parser
on configurational grounds only.
We therefore adapt ExtrAns to use a new statis-
tical broad-coverage parser that is as fast as a prob-
abilistic parser but more deep-linguistic because it
delivers grammatical relation structures which are
closer to predicate-argument structures and shallow
semantic structures like MLF, and more informative
if non-local dependencies are involved (Schneider,
2003). It has been evaluated and shown to have
state-of-the-art performance.
The parser expresses distinctions that are es-
pecially important for a predicate-argument based
shallow semantic representation, as far as they
are expressed in the Penn Treebank training data,
such as PP-attachment, most LDDs, relative clause
anaphora, participles, gerunds, and the argu-
ment/adjunct distinction for NPs.
In some cases functional relations distinctions
that are not expressed in the Penn Treebank are
made. Commas are e.g. disambiguated between
apposition and conjunction, or the Penn tag IN is
disambiguated between preposition and subordinat-
ing conjunction. Other distinctions that are less rel-
evant or not clearly expressed in the Treebank are
left underspecified, such as the distinction between
PP arguments and adjuncts, or a number of types of
subordinate clauses. The parser is robust in that it
returns the most promising set of partial structures
when it fails to find a complete parse for a sentence.
For sentences syntactically more complex than this
illustrative example, as many hierarchical relations
are returned as possible. A screenshot of its graphi-
cal interface can be seen in fig. 5. Its parsing speed
is about 300,000 words per hour.
Fig. 4 displays the three levels of analysis that are
performed on a simple sentence. Term expansion
yields NP3 as a complete candidate term. However,
NP1 and NP2 form two distinct, fully expanded
noun phrase chunks. Their formation into a noun
phrase with an embedded prepositional phrase is re-
covered from the parser?s syntactic relations giv-
ing the maximally projected noun phrase involv-
ing a term: ?Argentine methylation of STAT1? (or
juxtaposed ?STAT1 Argentine methylation?). Fi-
nally, the highest level syntactic relations (subj
and obj) identifies a transitive predicate relation
between these two candidate terms.
3.4 MLFs
The deep-linguistic dependency based parser partly
simplifies the construction of MLF. First, the map-
ping between labeled dependencies and a surface
semantic representation is often more direct than
across a complex constituency subtree (Schneider,
2003), and often more accurate (Johnson, 2002).
Dedicated labels can directly express complex re-
lations, the lexical participants needed for the con-
struction are more locally available.
Let us look at the example sentence ?Aden-
ovirus infection and transfection were used to model
changes in susceptibility to cell killing caused by
E1A expression?. The control relation (infection
is the implicit subject of model) and the PP rela-
tion (including the description noun) are available
locally. The reduced relative clause killing caused
by is expressed by a local dedicated label (modpart).
Only the conjunction infection and transfection, ex-
pressed here by bracketing, needs to be searched
across the syntactic hierarchy.
This leads to the following MLFs:
object(infection, o1, [o1]).
object(transfection, o2, [o2]).
object(change, o3, [o3]).
object(susceptibility, o4, [o4]).
object(killing, o5, [o5]).
object(expression, o6, [o6]).
object(cell, o7, [o7]).
evt(cause, e3, [o6]).
evt(model, e1, [(o1,o2), o3]).
evt(use, e2, [(o1,o2), e1]).
by(e3, o6).
in(o5, o7).
to(o4, o5).
in(o3, o4).
4 Related Work
Question Answering in Biomedicine is surveyed in
detail in (Zweigenbaum, 2003), in particular regard-
ing clinical questions. An example of a system ap-
plied to such questions is presented in (Niu et al,
2003), where it is applied in a setting for Evidence-
Based Medicine. This system identifies specific
?roles? within the document sentences and the ques-
tions, determining the answers is then a matter of
comparing the roles in each. To this aim, natural
language questions are translated into the PICO for-
mat (Sackett et al, 2000).
Automatic knowledge extraction (or strategies for
improving these methods) over Medline articles are
numerous. For example, (Craven and Kumlien,
1999) identifies possible drug-interaction relations
(predicates) between proteins and chemicals using
a ?bag of words? approach applied to the sentence
level. This produces inferences of the type: drug-
interactions (protein, pharmacologic-agent) where
an agent has been reported to interact with a pro-
tein.
(Sekimizu et al, 1998) uses frequently occurring
predicates and identifies the subject and object ar-
guments in the predication, in contrast (Rindflesch
et al, 2000) uses named entity recognition tech-
niques to identify drugs and genes, then identifies
the predicates which connect them. This type of
?object-relation-object? inference may also be im-
plied (Cimino and Barnet, 1993). This method
uses ?if then? rules to extract semantic relationships
between the medical entities depending on which
MeSH headings these entities appear under. For
example, if a citation has ?Electrocardiography?
with the subheading ?Methods? and has ?Myocar-
dial Infarction? with the subheading ?Diagnosis?
then ?Electrocardiography? diagnoses ?Myocardial
Infarction?.
(Spasic? et al, 2003) uses domain-relevant verbs
to improve on terminology extraction. The co-
occurrence in sentences of selected verbs and can-
didate terms reinforces their termhood. But where
such linguistic inferences are stored in a KB as facts,
statistical inferences are only used to visualize pos-
sible relations between objects for further investiga-
tion. (Stapley and Benoit, 2000) measures statistical
gene name co-occurrence and graphically displays
the results for an expert to investigate the dominant
patterns. The PubMed4 system uses the UMLS to
relate metathesaurus concepts against a controlled
vocabulary used to index the abstracts. This allows
efficient retrieval of abstracts from medical journals,
but it makes use of hyponymy and lexical synonymy
to organize the terms. It collects terminologies from
differing sub-domains in a metathesaurus of con-
cepts.
All such inferences (especially statistical) need to
be verified by an expert to ensure their validity. Syn-
tactic parsing, if any, is reserved to shallow NP iden-
tifying strategies (Sekimizu et al, 1998), or possi-
bly supplemented with PP information (Rindflesch
et al, 2000). Semantic interpretation of the docu-
ments is only attempted through their MeSH head-
ings (Mendonca and Cimino, 1999).
5 Conclusion
This paper documents our approach towards QA in
the genomics domain. Although some aspects of
the work described in this paper are still experimen-
tal, we think that the description of the problems
that we have encountered and the specific solutions
adopted or planned will provide an interesting con-
tribution to the workshop. We conclude by observ-
ing that Question Answering is currently seen as an
?advanced? topic in the Genomics Track of TREC5,
due to be targeted for the first time in Year 2 (2005).
Acknowledgments
The authors wish to thank the organizers of the workshop
and the anonymous reviewers for their helpful comments
and suggestions.
References
J.J. Cimino and G.O. Barnet. 1993. Automatic Knowl-
edge Acquisition from Medline. Methods of Informa-
tion in Medicine, 32(2):120?130.
Michael Collins. 1999. Head-Statistical Models for Nat-
ural Language Processing. Ph.D. thesis, University of
Pennsylvania, Philadelphia, USA.
M. Craven and J. Kumlien. 1999. Constructing biologi-
cal knowledge bases by extracting information from
4
http://www.ncbi.nlm.nih.gov/pubmed/
5
http://medir.ohsu.edu/?genomics/roadmap.html
text sources. Proceedings of the 8th International
Conference on Intelligent Systems for Molecular Bi-
ology (ISMB-99).
B. Daille, B. Habert, C. Jacquemin, and J. Roy-
aute?. 1996. Empirical observation of term varia-
tions and principles for their description. Termino-
logy, 3(2):197?258.
James Dowdall, Fabio Rinaldi, Fidelia Ibekwe-Sanjuan,
and Eric Sanjuan. 2003. Complex Structuring of
Term Variants for Question Answering. In Proc. of the
ACL 03, Workshop on Multiword Expression: Analy-
sis, Acquisition and Treatment, Sapporo, Japan, July.
Steve Finch and Andrei Mikheev. 1997. A Workbench
for Finding Structure in Texts. In Proceedings of Ap-
plied Natural Language Processing, Washington, DC,
April.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, Rada
Mihalcea, Mihai Surdeanu, Razvan Bunescu, Rox-
ana G??rju, Vasile Rus, and Paul Morarescu. 2001.
FALCON: Boosting knowledge for answer engines.
In Ellen M. Voorhees and Donna Harman, editors,
Proceedings of the Ninth Text REtrieval Conference
(TREC-9).
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Meeting of the
ACL, University of Pennsylvania, Philadelphia.
Kyo Kageura. 2002. The Dynamics of Terminology, A
descriptive theory of term formation and terminologi-
cal growth. Terminology and Lexicography, Research
and Practice. John Benjamins Publishing.
J.D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GE-
NIA corpus - a semantically annotated corpus for bio-
textmining. Bioinformatics, 19(1):180?182.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The
penn treebank. Computational Linguistics, 19:313?
330.
E. A. Mendonca and J. J. Cimino. 1999. Automated
Knowledge Extraction from Medline Citations. Med-
ical Informatics.
Diego Molla?, Rolf Schwitter, Michael Hess, and Rachel
Fournier. 2000. ExtrAns, an answer extraction sys-
tem. T.A.L. special issue on Information Retrieval ori-
ented Natural Language Processing, pages 495?522.
Diego Molla?, Fabio Rinaldi, Rolf Schwitter, James Dow-
dall, and Michael Hess. 2003. Answer Extraction
from Technical Texts. IEEE Intelligent Systems.
Diego Molla?. 2001. Ontologically promiscuous flat log-
ical forms for NLP. In Harry Bunt, Ielka van der Sluis,
and Elias Thijsse, editors, Proceedings of IWCS-4,
pages 249?265. Tilburg University.
Yun Niu, Graeme Hirst, Gregory McArthur, and Patricia
Rodriguez-Gianolli. 2003. Answering clinical ques-
tions with role identification. In Sophia Ananiadou
and Jun?ichi Tsujii, editors, Proceedings of the ACL
2003 Workshop on Natural Language Processing in
Biomedicine, pages 73?80.
J. Pustejovsky, J. Castan?o, R. Saur?i, A. Rumshisky,
J. Zhang, and W. Luo. 2002. Medstract: Creating
Large-scale Information Servers for Biomedical Li-
braries. In ACL 2002 Workshop on Natural Language
Processing in the Biomedical Domain. Philadel-
phia, PA. Available at http://www.medstract.
org/publications.html.
Fabio Rinaldi, James Dowdall, Michael Hess, Kaarel
Kaljurand, Mare Koit, Kadri Vider, and Neeme
Kahusk. 2002. Terminology as Knowledge in An-
swer Extraction. In Proceedings of the 6th Inter-
national Conference on Terminology and Knowledge
Engineering (TKE02), pages 107?113, Nancy, 28?30
August.
Fabio Rinaldi, Michael Hess, James Dowdall, Diego
Molla?, and Rolf Schwitter. 2004. Question answering
in terminology-rich technical domains. In Mark May-
bury, editor, New Directions in Question Answering.
AAAI Press.
T.C. Rindflesch, L. Tanabe, J. N. Weinstein, and
L. Hunter. 2000. Edgar: Extraction of drugs, genes
and relations from the biomedical literature. In Pacific
Symposium on Biocomputing, pages 514?25.
D. L. Sackett, S. E. Straus, W. S. Richardson,
W. Rosenberg, and R. B. Haynes. 2000. Evidence
Based Medicine: How to Practice and Teach EBM.
Churchill Livingstone.
Gerold Schneider. 2003. Extracting and Using Trace-
Free Functional Dependencies from the Penn Tree-
bank to Reduce Parsing Complexity. In Proceedings
of The Second Workshop on Treebanks and Linguis-
tic Theories (TLT 2003), Va?xjo?, Sweden, November
14-15.
T. Sekimizu, H. Park, and J Tsujii. 1998. Identifying the
interaction between genes and gene products based on
frequently seen verbs in Medline abstracts. Genome
Informatics, Universal Academy Press.
Irena Spasic?, Goran Nenadic?, and Sophia Ananiadou.
2003. Using domain-specific verbs for term classifi-
cation. In Sophia Ananiadou and Jun?ichi Tsujii, edi-
tors, Proceedings of the ACL 2003 Workshop on Nat-
ural Language Processing in Biomedicine, pages 17?
24.
B.J. Stapley and G. Benoit. 2000. Bibliometrics: infor-
mation retrieval and visualization from co-occurrence
of gene names in medline abstracts. In Proceedings
of the Pacific Symposium on Biocomputing (Oahu,
Hawaii), pages 529?540.
Ellen M. Voorhees. 2001. The TREC question answer-
ing track. Natural Language Engineering, 7(4):361?
378.
Pierre Zweigenbaum. 2003. Question answering in
biomedicine. In Proc. of EACL 03 Workshop: Natu-
ral Language Processing for Question Answering, Bu-
dapest.
Fast, Deep-Linguistic Statistical Dependency Parsing
Gerold Schneider, Fabio Rinaldi, James Dowdall
Institute of Computational Linguistics, University of Zurich
fgschneid,rinaldig@ifi.unizh.ch, j.m.dowdall@sussex.ac.uk
Abstract
We present and evaluate an implemented sta-
tistical minimal parsing strategy exploiting DG
charateristics to permit fast, robust, deep-
linguistic analysis of unrestricted text, and com-
pare its probability model to (Collins, 1999) and
an adaptation, (Dubey and Keller, 2003). We
show that DG allows for the expression of the
majority of English LDDs in a context-free way
and oers simple yet powerful statistical mod-
els.
1 Introduction
We present a fast, deep-linguistic statistical
parser that prots from DG characteristics and
that uses am minimal parsing strategy. First,
we rely on nite-state based approaches as long
as possible, secondly where parsing is neces-
sary we keep it context-free as long as possible
1
.
For low-level syntactic tasks, tagging and base-
NP chunking is used, parsing only takes place
between heads of chunks. Robust, successful
parsers (Abney, 1995; Collins, 1999) have shown
that this division of labour is particularly at-
tractive for DG.
Deep-linguistic, Formal Grammar parsers
have carefully crafted grammars written by pro-
fessional linguists. But unrestricted real-world
texts still pose a problem to NLP systems that
are based on Formal Grammars. Few hand-
crafted, deep linguistic grammars achieve the
coverage and robustness needed to parse large
corpora (see (Riezler et al, 2002), (Burke et al,
2004) and (Hockenmaier and Steedman, 2002)
for exceptions), and speed remains a serious
challenge. The typical problems can be grouped
as follows.
Grammar complexity Fully comprehensive
grammars are di?cult to maintain and consid-
1
Non-subject WH-question pronouns and support
verbs cannot be treated context-free with our approach.
We use a simple pre-parsing step to analyze them
erably increase parsing complexity.
Parsing complexity Typical formal gram-
mar parser complexity is much higher than
the O(n
3
) for CFG. The complexity of some
formal grammars is still unknown.
2
Pars-
ing algorithms able to treat completely un-
restricted long-distance dependencies are NP-
complete (Neuhaus and Broker, 1997).
Ranking Returning all syntactically possible
analyses for a sentence is not what is expected
of a syntactic analyzer. A clear indication of
preference is needed.
Pruning In order to keep search spaces man-
ageable it is necessary to discard unconvincing
alternatives already during the parsing process.
A number of robust statistical parsers that
oer solutions to these problems have become
available (Charniak, 2000; Collins, 1999; Hen-
derson, 2003). In a statistical parser, the rank-
ing of intermediate structures occurs naturally
and based on empirical grounds, while most
rule-based systems rely on ad hoc heuristics.
With an aggressive beam for parse-time prun-
ing (so in our parser), real-world parsing time
can be reduced to near-linear. If one were to
assume a constantly full xed beam, or uses an
oracle (Nivre, 2004) it is linear in practice
3
.
Also worst-case complexity for exhaustive
parsing is low, as these parsers are CFG-
based (Eisner, 2000)
4
. But they typically pro-
duce CFG constituency data as output, trees
that do not express long-distance dependen-
cies. Although grammatical function and empty
2
For Tree-Adjoining Grammars (TAG) it is O(n
7
) or
O(n
8
) depending on the implementation (Eisner, 2000).
(Sarkar et al, 2000) state that the theoretical bound of
worst time complexity for Head-Driven Phrase Structure
Grammar (HPSG) parsing is exponential.
3
In practical terms, beam or oracle approach have
very similar eects
4
Parsing complexity of the original Collins Models is
O(n
5
), but theoretically O(n
3
) would be possible
Antecedent POS Label Count Description Example
1 NP NP * 22,734 NP trace Sam was seen *
2 NP * 12,172 NP PRO * to sleep is nice
3 WHNP NP *T* 10,659 WH trace the woman who you saw *T*
(4) *U* 9,202 Empty units $ 25 *U*
(5) 0 7,057 Empty complementizers Sam said 0 Sasha snores
(6) S S *T* 5,035 Moved clauses Sam had to go, Sasha said *T*
7 WHADVP ADVP *T* 3,181 WH-trace Sam explained how to leave *T*
(8) SBAR 2,513 Empty clauses Sam had to go, said Sasha (SBAR)
(9) WHNP 0 2,139 Empty relative pronouns the woman 0 we saw
(10) WHADVP 0 726 Empty relative pronouns the reason 0 to leave
Table 1: The distribution of the 10 most frequent types of empty nodes and their antecedents in
the Penn Treebank (adapted from (Johnson, 2002)). Bracketed line numbers only involve LDDs as
grammar artifact
nodes annotation expressing long-distance de-
pendencies are provided in Treebanks such as
the Penn Treebank (Marcus et al, 1993), most
statistical Treebank trained parsers fully or
largely ignore them
5
, which entails two prob-
lems: rst, the training cannot prot from valu-
able annotation data. Second, the extraction
of long-distance dependencies (LDD) and the
mapping to shallow semantic representations is
not always possible from the output of these
parsers. This limitation is aggravated by a lack
of co-indexation information and parsing errors
across an LDD. In fact, some syntactic relations
cannot be recovered on congurational grounds
only. For these reasons, (Johnson, 2002) refers
to them as \half-grammars".
An approach that relies heavily on DG char-
acteristics is explored in this paper. It uses
a hand-written DG grammar and a lexicalized
probability model. It combines the low com-
plexity of a CFG parser, the pruning and rank-
ing advantages of statistical parsers and the
ability to express the majority of LDDs of For-
mal Grammars. After presenting the DG bene-
ts, we dene our DG and introduce our statis-
tical model. Then, we give an evaluation.
2 The Benet of DG Characteristics
In addition to some obvious benets, such as
the integration of chunking and parsing (Abney,
1995), where a chunk largely corresponds to a
nucleus (Tesniere, 1959), or that in an endocen-
tric theory projection can never fail, we present
eight characteristics in more detail, which in
their combination allow us to treat the majority
of English long-distance dependencies (LDD) in
our DG parser Pro3Gres in a context-fee way.
5
(Collins, 1999) Model 2 uses some of the functional
labels, and Model 3 some long-distance dependencies
The ten most frequent types of empty nodes
cover more than 60,000 of the approximately
64,000 empty nodes of sections 2-21 of the Penn
Treebank. Table 1, reproduced from (Johnson,
2002) [line numbers and counts from the whole
Treebank added], gives an overview.
2.1 No Empty Nodes
The fact that traditional DG does not know
empty nodes allows a DG parser to use the e?-
cient 0(n
3
) CYK algorithm.
2.2 Only Content Words are Nuclei
Only content words can be nuclei in a tradi-
tional DG. This means that empty units, empty
complementizers and empty relative pronouns
[lines 4,5,9,10] pose no problem for DG as they
are optional, non-head material. For example, a
complementizer is an optional dependent of the
subordinated verb.
2.3 No External Argument, ID/LP
Moved clauses [line 6] are mostly PPs or clausal
complements of verbs of utterance. Only verbs
of utterance allow subject-verb inversion in af-
rmative clauses [line 8]. Our hand-written
grammar provides rules with appropriate re-
strictions for them, allowing an inversion of the
\canonical" dependency direction under well-
dened conditions, distinguishing between or-
dre lineaire (linear precedence(LP)) and ordre
structural (immediate dominance(ID)). Fronted
positions are available locally to the verb in a
theory that does not posit a distinction between
internal and external arguments.
2.4 Exploiting Functional DG Labels
The fact that dependencies are often labeled is
a main dierence between DG and constituency.
We exploit this by using dedicated labels to
model a range of constituency LDDs, relations
Relation Label Example
verb{subject subj he sleeps
verb{rst object obj sees it
verb{second object obj2 gave (her) kisses
verb{adjunct adj ate yesterday
verb{subord. clause sentobj saw (they) came
verb{prep. phrase pobj slept in bed
noun{prep. phrase modpp draft of paper
noun{participle modpart report written
verb{complementizer compl to eat apples
noun{preposition prep to the house
Table 2: Important Pro3Gres Dependency
types
spanning several constituency levels, including
empty nodes and functional Penn Treebank la-
bels, by a purely local DG relation
6
. The selec-
tive mapping patterns for MLE counts of pas-
sive subjects and control subjects from the Penn
Treebank, the most frequent NP traces [line 1],
are e.g. (@ stands for arbitrary nestedness):
?
h
h
h
h
(
(
(
(
NP-SBJ-X@
noun
VP@
h
h
h
(
(
(
V
passive verb
NP
-NONE-
*-X
?
h
h
h
h
(
(
(
(
NP-SBJ-X@
noun
VP@
h
h
h
(
(
(
V
control-verb
S
NP-SBJ
-NONE-
*-X
Our approach employs nite-state approxima-
tions of long-distance dependencies, described
in (Schneider, 2003) for DG and (Cahill et al,
2004) for Lexical Functional Grammar (LFG)It
leaves empty nodes underspecied but largely
recoverable. Table 2 gives an overview of im-
portant dependencies.
2.5 Monostratalism and Functionalism
While multistratal DGs exist and several de-
pendency levels can be distinguished (Mel'cuk,
1988) we follow a conservative view close to the
original (Tesniere, 1959), which basically parses
directly for a simple LFG f-structure without
needing a c-structure detour.
6
In addition to taking less decisions due to the gained
high-level shallowness, it is ensured that the lexical in-
formation that matters is available in one central place,
allowing the parser to take one well-informed decision in-
stead of several brittle decisions plagued by sparseness.
Collapsing deeply nested structures into a single depen-
dency relation is less complex but has a similar eect as
selecting what goes in to the parse history in history-
based approaches.
2.6 Graphs
DG theory often conceives of DG structures
as graphs instead of trees (Hudson, 1984). A
statistical lexicalized post-processing module
in Pro3Gres transforms selected subtrees into
graphs, e.g. in order to express control.
2.7 Transformation to Semantic Layer
Pro3Gres is currently being applied in a Ques-
tion Answering system specically targeted at
technical domains (Rinaldi et al, 2004b). One
of the main advantages of a DG parser such as
Pro3Gres over other parsing approaches is that
a mapping from the syntactic layer to a seman-
tic layer (meaning representation) is partly sim-
plied (Molla et al, 2000).
2.8 Tesniere's Translations
The possible functional changes of a word called
translations (Tesniere, 1959) are an exception
to endocentricity. They are an important con-
tribution to a traceless theory. Gerunds (af-
ter winning/VBG the race) or innitives [line
2] may function as nouns, obviating the need
for an empty subject. In nounless NPs such as
the poor, adjectives function as nouns, obviating
the need for an empty noun head. Participles
may function as adjectives (Western industrial-
ized/VBN countries), again obviating the need
for an empty subject.
3 The Statistical Dependency Model
Most successful deep-linguistic Dependency
Parsers (Lin, 1998; Tapanainen and Jarvinen,
1997) do not have a statistical base. But one
DG advantage is precisely that it oers simple
but powerful statistical Maximum Likelihood
Estimation (MLE) models. We now dene our
DG and the probability model.
The rules of a context-free, unlabeled DG
are equivalent to binary-branching CFG rewrite
rules in which the head and the mother node are
isomorphic. When converting DG structures to
CFG, the order of application of these rules is
not necessarily known, but in a labeled DG, the
set of rules can specify the order (Covington,
1994). Fig. 1 shows such two structures, equiv-
alent except for the absence of functional la-
bels in CFG. Subj (but not PP ) has been used
in this example conversion to specify the appli-
cation order, hence we get a repetition of the
eat/V node, mirroring a traditional CFG S and
VP distinction.
In a binary CFG, any two constituents A and
B which are adjacent during parsing are candi-
ROOT the man eats apples with a fork
W
SENT

Subj

Det
W
Obj
W
PP
W
PObj

Det
eat/V
h
h
h
h
h
(
(
(
(
(
man/N
X
X


the/D
the
man/N
man
eat/V
h
h
h
h
h

(
(
(
(
(
eat/V
eats
apple/N
apples
with/P
h
h
(
(
with/P
with
fork/N
X
X


a/D
a
fork/N
fork
Figure 1: DG and CFG representation
dates for the RHS of a rewrite rule. As terminal
types we use word tags.
X ! AB; e:g:NP ! DT NN (1)
In DG, one of these is isomorphic to the LHS,
i.e. the head. This grammar is also a Bare
Phrase Structure grammar known from Mini-
malism (Chomsky, 1995).
B ! AB; e:g: NN ! DT NN (2)
A ! AB; e:g: V B ! V B PP (3)
Labeled DG rules additionally use a syntactic
relation label R. A non-lexicalized model would
be:
p(RjA ! AB)

=
#(R;A ! AB)
#(A ! AB)
(4)
Research on PCFG and PP-attachment has
shown the importance of probabilizing on lexical
heads (a and b).
p(RjA ! AB;a; b)

=
#(R;A ! AB; a; b)
#(A ! AB; a; b)
(5)
All that A ! AB expresses is that the depen-
dency relation is towards the right.
p(Rjright; a; b)

=
#(R; right; a; b)
#(right; a; b)
(6)
e.g. for the Verb-PP attachment relation pobj
(following (Collins and Brooks, 1995) including
the description noun
7
)
p(pobjjright; verb; prep; desc:noun)

=
#(pobj; right; verb; prep; desc:noun)
#(right; verb; prep; desc:noun)
The distance (measured in chunks) between a
head and a dependent is a limiting factor for the
probability of a dependency between them.
p(R; distjright; a; b)

=
#(R; dist; right; a; b)
#(right; a; b)
(7)
7
PP is considered to be an exocentric category, since
both the preposition and the description noun can be
seen as head; in LFG they appear as double-head
Many relations are only allowed towards one di-
rection, the left/right factor is absent for them.
Typical distances mainly depend on the rela-
tion. Objects usually immediately follow the
verb, while a PP attached to the verb may easily
follow only at the second or third position, after
the object and other PPs etc. By application of
the chain rule and assuming that distance is in-
dependent of the lexical heads we get:
p(R; distja; b)

=
#(R; a; b)
#(a; b)

#(R; dist)
#R
(8)
We now explore Pro3Gres' main probability
model by comparing it to (Collins, 1999), and
an adaptation of it, (Dubey and Keller, 2003).
3.1 Relation of Pro3Gres to Collins
Model 1
We will rst consider the non-generative Model
1 (Collins, 1999). Both (Collins, 1999) Model
1 and Pro3Gres are mainly dependency-based
statistical parsers over heads of chunks, a
close relation can thus be expected. The
(Collins, 1999) Model 1 MLE estimation is:
P (Rjha; atagi; hb; btagi; dist)

=
#(R; ha; atagi; hb; btagi; dist)
#(ha; atagi; hb; btagi; dist)
(9)
Dierences in comparison to (8) are:
 Pro3Gres does not use tag information.
This is because, rst, the licensing hand-
written grammar is based on Penn tags.
 The second reason for not using tag infor-
mation is because Pro3Gres backs o to se-
mantic WordNet classes (Fellbaum, 1998)
for nouns and to Levin classes (Levin, 1993)
for verbs instead of to tags, which has the
advantage of being more ne-grained.
 Pro3Gres uses real distances, measured in
chunks, instead of a feature vector. Dis-
tance is assumed to be dependent only on
R, which reduces the sparse data problem.
(Chung and Rim, 2003) made similar ob-
servations for Korean.
 The co-occurrence count in the MLE de-
nominator is not the sentence-context, but
the sum of counts of competing relations.
E.g. the object and adjunct relation are
in competition, as they are licensed by the
same tag sequence V B NN. Pro3Gres
models attachment (thus decision) proba-
bilities, viewing parsing as a decision pro-
cess.
 Relations (R) have a Functional DG de-
nition, including LDDs.
3.2 Relation to Collins Model 2
(Collins, 1999) Model 2 extends the parser to in-
clude a complement/adjunct distinction for NPs
and subordinated clauses, and it includes a sub-
categorisation frame model.
For the subcategorisation-dependent genera-
tion of dependencies in Model 2, rst the prob-
abilities of the possible subcat frames are calcu-
lated and the selected subcat frame is added as
a condition. Once a subcategorized constituent
has been found, it is removed from the subcat
frame, ensuring that non-subcategorized con-
stituents cannot be attached as complement,
which is one of the two major function of a
subcat frame. The other major function of a
subcat frame is to nd all the subcategorized
constituents. In order to ensure this, the prob-
ability when a rewrite rule can stop expanding
is calculated. Importantly, the probability of
a rewrite rule with a non-empty subcat frame
to stop expanding is low, the probability of a
rewrite rule with an empty subcat frame to stop
expanding is high.
Pro3Gres includes a complement/adjunct dis-
tinction for NPs. The examples given in sup-
port of the subcategorisation frame model in
(Collins, 1999) Model 2 are dealt with by the
hand-written grammar in Pro3Gres.
Every complement relation type, namely
subj, obj, obj2, sentobj, can only occur once per
verb, which ensures one of the two major func-
tions of a subcat frame, that non-subcategorized
constituents cannot be attached as comple-
ments. This amounts to keeping separate sub-
cat frames for each relation type, where the se-
lection of the appropriate frame and removing
the found constituent coincide, which has the
advantage of a reduced search space: no hy-
pothesized, but unfound subcat frame elements
need to be managed. As for the second major
function of subcat frames { to ensure that if pos-
sible all subcategorized constituents are found {
the same principle applies: selection of subcat
frame and removing of found constituents coin-
cide; lexical information on the verb argument
candidate is available at frame selection time al-
ready. This implies that Collins Model 2 takes
an unnecessary detour.
As for the probability of stopping the expan-
sion of a rule { since DG rules are always binary
{ it is always 0 before and 1 after the attach-
ment. But what is needed in place of interrela-
tions of constituents of the same rewrite rule is
proper cooperation of the dierent subcat types.
For example, the grammar rules only allow a
noun to be obj2 once obj has been found, or a
verb is required to have a subject unless it is
non-nite or a participle, or all objects need to
be closer to the verb than a subordinate clause.
3.3 Relation to Dubey & Keller 03
(Dubey and Keller, 2003) address the ques-
tion whether models such as Collins also im-
prove performance on freer word order lan-
guages, in their case German. German is con-
siderably more inectional which means that
discarding functional information is more harm-
ful, and which explains why the NEGRA an-
notation has been conceived to be quite at
(Skut et al, 1997). (Dubey and Keller, 2003)
observe that models such as Collins when ap-
plied directly perform worse than an unlexical-
ized PCFG baseline. The fact that learning
curves converge early indicates that this is not
mainly a sparse data eect. They suggest a lin-
guistically motivated change, which is shown to
outperform the baseline.
The (Collins, 1999) Model 2 rule generation
model for P ! L
m
:::L
1
HR
1
:::R
n
, is
P (RHSjLHS) = P
h
(HjP; t(P ); l(P ))

m
Y
i=0
P
l
(L
i
; t(L
i
); l(L
i
)jP;H; t(H); l(H); d(i))

n
Y
i=0
P
r
(R
i
; t(R
i
); l(R
i
)jP;H; t(H); l(H); d(i))
P
h
P of head t(H) tag of H head word
LHS left-hand side RHS right-hand side
P
l:1::m
P(words left of head) P
r:1::n
P(words right of head)
H LHS Head Category P RHS Mother Category
L left Constit. Cat. R right Constit. Cat.
l(H) head word of H d distance measure
Dubey & Keller suggest the following change
in order to respect the NEGRA atness: P
h
is
left unchanged, but P
l
and P
r
are conditioned
on the preceding sister instead of on the head:
P (RHSjLHS) = P
h
(HjP; t(P ); l(P ))

m
Y
i=0
P
l
(L
i
; t(L
i
); l(L
i
)jP;L
i 1
; t(L
i 1
); l(L
i 1
); d(i))

n
Y
i=0
P
r
(R
i
; t(R
i
); l(R
i
)jP;R
i 1
; t(R
i 1
); l(R
i 1
); d(i))
Their new model performs considerably better
and also outperforms the unlexicalized baseline.
The authors state that \[u]sing sister-head re-
lationships is a way of counteracting the at-
ness of the grammar productions; it implicitly
adds binary branching to the grammar." (ibid.).
DG is binary branching by denition; adding
binary branching implicitly converts the CFG
rules into an ad-hoc DG.
Whether the combination ((Chomsky, 1995)
merge) of two binary constituents directly
projects to a \real" CFG rule LHS or an im-
plicit intermediate constituent does not matter.
Observations
 What counts is each individual Functional
DG dependency, no matter whether it is ex-
pressed as a sister-head or a head-head de-
pendency, or stretches across several CFG
levels (control, modpart etc.)
 Not adjacency (i,i-1) but headedness
counts. Instead of conditioning on the pre-
ceding (i-1) sister, conditioning on the real
DG head is linguistically more motivated
8
.
 Not adjacency (i,i-1) but the type of GR
counts: the question why Dubey & Keller
did not use the NEGRA GR labels has to
arise when discussing a strongly inectional
language such as German.
 The use of a generative model, calculating
the probability of a rule and ultimately the
probability of producing a sentence given
the grammar only has theoretical advan-
tages. For practical purposes, modeling
parsetime decision probabilities is as valid.
With these observations in mind, we can com-
pare Pro3Gres to (Dubey and Keller, 2003).
As for the Base-NP Model, Pro3Gres only re-
spects the best tagging & chunking result re-
ported to it { a major source of errors (see sec-
tion 4). In DG, projection (although not ex-
pansion) is deterministic. H and P are usually
isomorphic, if not Tesniere-translations are rule-
based. Since in DG, only lexical nodes are cat-
egories, P=t(P). P
h
is thus l(h), the prior, we
ignore it for maximizing. In analogy, also cat-
egory (L/R) and their tags are identical. The
revised formula is
P (RHSjLHS)

=
l(h)

m
Y
i=0
P
l
(t(L
i
); l(L
i
)jP; t(L
i 1
); l(L
i 1
); d(i))

n
Y
i=0
P
r
(t(R
i
); l(R
i
)jP; t(R
i 1
); l(R
i 1
); d(i))
If a DG rule is head-right, P is L
i
or R
i
, if
it is head-left, P is L
i 1
or R
i 1
, respectively.
8
In primarily right-branching languages such as En-
glish or German (i-1) actually amounts to being the head
in the majority of, but not all cases. In a more functional
DG perspective such as the one taken in Pro3Gres, these
languages turn out to be less right-branching, however,
with prepositions or determiners analyzed as markers to
the nominal head or complementizers or relative pro-
nouns as markers to the verbal head of the subclause.
Headedness and not direction matters. L
i
/R
i
is replaced by H
i
and L/R
i 1=i+1
by H'. H' is
understood to be the DG dependent, although,
as mentioned, H' could also be the DG head in
this implicit ad-hoc DG.
P (RHSjLHS)

=
l(h)

n+m
Y
i=0
P
l;r
(t(H
i
); l(H
i
)jt(H
i
); t(H
0
i
); l(H
0
i
); d(i))
P (t(H
i
)jt(H
i
); t(H
0
i
)) is a projection or
attachment grammar model modeling the
unlexicalized probability of t(H) and t(H')
participating in a binary rule with t(H) as
head { the merge probability in Bare Phrase
Structure (Chomsky, 1995); an unlabeled ver-
sion of (4). P (t(H
i
); l(H
i
)jt(H
i
); t(H
0
i
); l(H
0
i
))
is a lexicalized version of the same pro-
jection or attachment grammar model;
P (t(H
i
); l(H
i
)jt(H
i
); t(H
0
i
); l(H
0
i
; d(i))) in
addition conditions on the distance
9
. Pro3Gres
expresses the unlexicalized rules by licensing
grammar rules for relation R. Tags are not used
in Pro3Gres' model, because semantic backos
and tag-based licensing rules are used.
P (d(i)jl(H
i
); l(H
0
i
)) (10)
The Pro3Gres main MLE estimation (8)
(l(H) = a; l(H
0
) = b) diers from (10) by using
labeled DG, and thus from the Dubey & Keller
Model by using a consistent functional DG.
4 Evaluation
(Lin, 1995; Carroll et al, 1999) suggest eval-
uating on the linguistically meaningful level of
dependency relations. Two such evaluations are
reported now.
First, a general-purpose evaluation using a
hand-compiled gold standard corpus (Carroll et
al., 1999), which contains the grammatical re-
lation data of 500 random sentences from the
Susanne corpus. The performance (table 3), ac-
cording to (Preiss, 2003), is similar to a large
selection of statistical parsers and a grammat-
ical relation nder. Relations involving LDDs
form part of these relations. A selection of them
is also given: WH-Subject (WHS), WH-Object
(WHO), passive Subject (PSubj), control Sub-
ject (CSubj), and the anaphor of the relative
clause pronoun (RclSubjA).
9
Since normalized probabilities are used
P (t(H
i
); l(H
i
)jt(H
i
); t(H
0
i
); l(H
0
i
; d(i))) =
P (t(H
i
); d(i)jt(H
i
); t(H
0
i
); l(H
i
); l(H
0
i
))
CARROLL Percentages for some relations, general, on Carroll testset only LDD-involving
Subject Object noun-PP verb-PP subord. clause WHS WHO PSubj CSubj RclSubjA
Precision 91 89 73 74 68 92 60 n/a 80 89
Recall 81 83 67 83 n/a 90 86 83 n/a 63
GENIA Percentages for some relations, general, on GENIA corpus
Subject Object noun-PP verb-PP subord. clause
Precision 90 94 83 82 71
Recall 86 95 82 84 75
Table 3: Evaluation on Carroll's test suite on subj, obj, PP-attachment and clause subord. relations
and a selection of 5 LDD relations, and on the terminology-annotated GENIA corpus
Secondly, to answer how the parser performs
over domains markedly dierent to the train-
ing corpus, to test whether terminology is the
key to a successful parsing system, and to assess
the impact of chunking errors, the parser has
been applied to the GENIA corpus (Kim et al,
2003), 2000 MEDLINE abstracts of more than
400,000 words describing the results of Biomed-
ical research, which is annotated for multi-word
terms and thus contains near-perfect chunking.
100 random sentences from the GENIA corpus
have been manually annotated and compared to
the parser output (Rinaldi et al, 2004a).
5 Conclusions
We have discussed how DG allows the expres-
sion of the majority of LDDs in a context-
free way and shown that DG allows for simple
but powerful statistical models. An evaluation
shows that the performance of its implementa-
tion is state-of-the-art
10
. Its parsing speed of
about 300,000 words per hour is very good for a
deep-linguistic parser and makes it fast enough
for unlimited application.
References
Steven Abney. 1995. Chunks and dependen-
cies: Bringing processing evidence to bear
on syntax. In Jennifer Cole, Georgia Green,
and Jerry Morgan, editors, Computational
Linguistics and the Foundations of Linguis-
tic Theory, pages 145{164. CSLI.
M. Burke, A. Cahill, R. O'Donovan, J. van
Genabith, and A. Way. 2004. Treebank-
based acquisistion of wide-coverage, proba-
bilistic LFG resources: Project overview, re-
sults and evaluation. In The First Interna-
tional Joint Conference on Natural Language
Processing (IJCNLP-04), Workshop "Beyond
shallow analyses - Formalisms and statisti-
cal modeling for deep analyses", Sanya City,
China.
10
We are currently starting evaluation on the PARC
700 corpus
Aoife Cahill, Michael Burke, Ruth O'Donovan,
Josef van Genabith, and Andy Way. 2004.
Long-distance dependency resolution in au-
tomatically acquired wide-coverage PCFG-
based LFG approximations. In Proceedings of
ACL-2004, Barcelona, Spain.
John Carroll, Guido Minnen, and Ted Briscoe.
1999. Corpus annotation for parser evalua-
tion. In Proceedings of the EACL-99 Post-
Conference Workshop on Linguistically Inter-
preted Corpora, Bergen, Norway.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North
American Chapter of the ACL, pages 132{
139.
Noam Chomsky. 1995. The Minimalist Pro-
gram. The MIT Press, Cambridge, Mas-
sachusetts.
Hoojung Chung and Hae-Chang Rim. 2003. A
new probabilistic dependency parsing model
for head-nal, free word order languages. IE-
ICE Transaction on Information & System,
E86-D, No. 11:2490{2493.
Michael Collins and James Brooks. 1995.
Prepositional attachment through a backed-
o model. In Proceedings of the Third Work-
shop on Very Large Corpora, Cambridge,
MA.
Michael Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania, Philadel-
phia, PA.
Michael A. Covington. 1994. An empirically
motivated reinterpretation of Dependency
Grammar. Technical Report AI1994-01, Uni-
versity of Georgia, Athens, Georgia.
Amit Dubey and Frank Keller. 2003. Proba-
bilistic parsing for German using sister-head
dependencies. In Proceedings of the 41st An-
nual Meeting of the Association for Compu-
tational Linguistics, Sapporo.
Jason Eisner. 2000. Bilexical grammars and
their cubic-time parsing algorithms. In Harry
Bunt and Anton Nijholt, editors, Advances in
Probabilistic and Other Parsing Technologies.
Kluwer.
Christiane Fellbaum, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
James Henderson. 2003. Inducing history
representations for broad coverage statisti-
cal parsing. In Proceedings of HLT-NAACL
2003, Edmonton, Canada.
Julia Hockenmaier and Mark Steedman. 2002.
Generative models for statistical parsing with
combinatory categorial grammar. In Proceed-
ings of 40th Annual Meeting of the Associa-
tion for Computational Linguistics, Philadel-
phia.
Richard Hudson. 1984. Word Grammar. Basil
Blackwell, Oxford.
Mark Johnson. 2002. A simple pattern-
matching algorithm for recovering empty
nodes and their antecedents. In Proceedings
of the 40th Meeting of the ACL, University of
Pennsylvania, Philadelphia.
J.D. Kim, T. Ohta, Y. Tateisi, and J. Tsu-
jii. 2003. Genia corpus - a semantically an-
notated corpus for bio-textmining. Bioinfor-
matics, 19(1):i180{i182.
Beth C. Levin. 1993. English Verb Classes
and Alternations: a Preliminary Investiga-
tion. University of Chicago Press, Chicago,
IL.
Dekang Lin. 1995. A dependency-based
method for evaluating broad-coverage
parsers. In Proceedings of IJCAI-95, Mon-
treal.
Dekang Lin. 1998. Dependency-based evalua-
tion of MINIPAR. In Workshop on the Eval-
uation of Parsing Systems, Granada, Spain.
Mitch Marcus, Beatrice Santorini, and M.A.
Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: the Penn Treebank.
Computational Linguistics, 19:313{330.
Igor Mel'cuk. 1988. Dependency Syntax: theory
and practice. State University of New York
Press, New York.
Diego Molla, Gerold Schneider, Rolf Schwit-
ter, and Michael Hess. 2000. Answer
Extraction using a Dependency Grammar
in ExtrAns. Traitement Automatique de
Langues (T.A.L.), Special Issue on Depen-
dency Grammar, 41(1):127{156.
Peter Neuhaus and Norbert Broker. 1997. The
complexity of recognition of linguistically ad-
equate dependency grammars. In Proceedings
of the 35th ACL and 8th EACL, pages 337{
343, Madrid, Spain.
Joakim Nivre. 2004. Inductive dependency
parsing. In Proceedings of Promote IT, Karl-
stad University.
Judita Preiss. 2003. Using grammatical rela-
tions to compare parsers. In Proc. of EACL
03, Budapest, Hungary.
Stefan Riezler, Tracy H. King, Ronald M. Ka-
plan, Richard Crouch, John T. Maxwell,
and Mark Johnson. 2002. Parsing the Wall
Street Journal using a Lexical-Functional
Grammar and discriminative estimation tech-
niques. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Lin-
guistics (ACL'02), Philadephia, PA.
Fabio Rinaldi, James Dowdall, Gerold Schnei-
der, and Andreas Persidis. 2004a. Answer-
ing Questions in the Genomics Domain. In
ACL 2004 Workshop on Question Answering
in restricted domains, Barcelona, Spain, 21{
26 July.
Fabio Rinaldi, Michael Hess, James Dowdall,
Diego Molla, and Rolf Schwitter. 2004b.
Question answering in terminology-rich tech-
nical domains. In Mark Maybury, edi-
tor, New Directions in Question Answering.
MIT/AAAI Press.
Anoop Sarkar, Fei Xia, and Aravind Joshi.
2000. Some experiments on indicators of
parsing complexity for lexicalized grammars.
In Proc. of COLING.
Gerold Schneider. 2003. Extracting and using
trace-free Functional Dependencies from the
Penn Treebank to reduce parsing complex-
ity. In Proceedings of Treebanks and Linguis-
tic Theories (TLT) 2003, Vaxjo, Sweden.
Wojciech Skut, Brigitte Krenn, Thorsten
Brants, and Hans Uszkoreit. 1997. An anno-
tation scheme for free word order languages.
In Proceedings of the Fifth Conference on Ap-
plied Natural Language Processing (ANLP-
97), Washington, DC.
Pasi Tapanainen and Timo Jarvinen. 1997. A
non-projective dependency parser. In Pro-
ceedings of the 5th Conference on Applied
Natural Language Processing, pages 64{71.
Association for Computational Linguistics.
Lucien Tesniere. 1959. Elements de Syntaxe
Structurale. Librairie Klincksieck, Paris.
A Robust and Hybrid Deep-Linguistic Theory Applied to
Large-Scale Parsing
Gerold Schneider, James Dowdall, Fabio Rinaldi
Institute of Computational Linguistics, University of Zurich
{gschneid,rinaldi}@ifi.unizh.ch, j.m.dowdall@sussex.ac.uk
Abstract
Modern statistical parsers are robust and quite
fast, but their output is relatively shallow when
compared to formal grammar parsers. We sug-
gest to extend statistical approaches to a more
deep-linguistic analysis while at the same time
keeping the speed and low complexity of a sta-
tistical parser. The resulting parsing architec-
ture suggested, implemented and evaluated here
is highly robust and hybrid on a number of
levels, combining statistical and rule-based ap-
proaches, constituency and dependency gram-
mar, shallow and deep processing, full and near-
full parsing. With its parsing speed of about
300,000 words per hour and state-of-the-art per-
formance the parser is reliable for a number of
large-scale applications discussed in the article.
1 Introduction
Robustness in Computational Linguistics has
been recently recognized as a central issue for
the design of reliable, large-scale Natural Lan-
guage Processing (NLP) systems. While the
highest possible linguistic coverage is desirable,
speed and robustness are equally important in
practical applications.
Formal Grammar Parser have carefully
crafted grammars written by professional lin-
guists. In addition to expressing local relations,
i.e. relations between a mother and a direct
daughter node, a number of non-local relations,
i.e. relations involving more than two genera-
tions, are also modeled. An example of a non-
local relation is the subject control relation in
the sentence John wants to leave, where John is
not only the explicit subject of want, but equally
the implicit subject of leave. A parser that fails
to recognize control subjects misses important
information, quantitatively about 3 % of all sub-
jects.
But unrestricted real-world texts still pose a
problem to NLP systems that are based on For-
mal Grammars. Few hand-crafted, deep linguis-
tic grammars achieve the coverage and robust-
ness needed to parse large corpora (see (Riezler
et al, 2002) for an exception, and (Burke et al,
2004; Hockenmaier and Steedman, 2002) for ap-
proaches extracting formal grammars from the
Treebank), and speed remains a serious chal-
lenge. The typical problems can be grouped as
follows.
Grammar complexity Fully comprehensive
grammars are difficult to maintain and consid-
erably increase parsing complexity. Note that
statistical parsers can equally suffer from this
problem, see e.g. (Kaplan et al, 2004).
Parsing complexity Typical formal gram-
mar parser complexity is much higher than
the O(n3) for CFG (Eisner, 1997). The com-
plexity of some formal grammars is still un-
known. For Tree-Adjoining Grammars (TAG)
it is O(n7) or O(n8) depending on the im-
plementation (Eisner, 2000). (Sarkar et al,
2000) state that the theoretical bound of worst
time complexity for Head-Driven Phrase Struc-
ture Grammar (HPSG) parsing is exponential.
Parsing algorithms able to treat completely un-
restricted long-distance dependencies are NP-
complete (Neuhaus and Bro?ker, 1997).
Ranking Returning all syntactically possible
analyses for a sentence is not really what is ex-
pected of a syntactic analyzer if it should be of
practical use, since for a human there is usually
only one ?correct? interpretation. A clear in-
dication of preference, by means of ranking the
analyses in a preference order is needed.
Pruning In order to keep search spaces man-
ageable it is in fact necessary to discard uncon-
vincing alternatives already during the parsing
process. In a statistical parser, the ranking of
intermediate structures occurs naturally, while
a rule-based system has to rely on ad hoc heuris-
tics. With a beam search in a parse-time prun-
ing system, which means that the total number
of alternatives kept is constant from a certain
search complexity onwards, real-world parsing
time can be reduced to near-linear. If one were
to assume a constantly full beam, or uses an
oracle (Nivre, 2004) it is linear in practice.
A number of robust statistical parsers that
offer solutions to these problems have now be-
come available (Charniak, 2000; Collins, 1999;
Henderson, 2003), but they typically produce
CFG constituency data as output, trees that do
not express long-distance dependencies.
Although grammatical function and empty
nodes annotation expressing long-distance de-
pendencies are provided in Treebanks such as
the Penn Treebank (Marcus et al, 1993), most
statistical Treebank trained parsers fully or
largely ignore them1, which entails two prob-
lems: first, the training cannot profit from valu-
able annotation data. Second, the extraction
of long-distance dependencies (LDD) and the
mapping to shallow semantic representations is
not always possible from the output of these
parsers. This limitation is aggravated by a
lack of co-indexation information and parsing
errors across an LDD. In fact, some syntac-
tic relations cannot be recovered on configura-
tional grounds only. For these reasons, (John-
son, 2002) provocatively refers to them as ?half-
grammars?.
The paper is organized as follows. We first ex-
plore a deep-linguistic grammar theory for En-
glish that is inherently designed to be robust
by extending the low processing complexity and
the robustness of statistical approaches to a
more deep-linguistic level, by making careful
use of underspecification, grammar compression
techniques and using a grammar that directly
delivers simple predicate-argument structures.
This allow us to use a context-free grammar
at parse-time while successfully treating long-
distance dependencies using low-complexity ap-
proaches before and after parsing. Our ap-
proach is to use finite-state approximations
of long-distance dependencies, as they are de-
scribed in (Schneider, 2003a) for Dependency
Grammar (DG) and (Cahill et al, 2004) for
Lexical Functional Grammar (LFG). (Dienes
and Dubey, 2003) show that finite-state pre-
processing modules can successfully deal with
LDDs. Our approach is similar in also amount-
ing to a preprocessing recognition of LDDs.
Then we show that the implementation
(Pro3Gres) profits from hybridness and is fast
1(Collins, 1999) Model 2 uses some of the functional
labels, and Model 3 some long-distance dependencies
and robust enough to do large-scale parsing of
totally unrestricted texts and give an overview
of its applications. To conclude, two evaluations
are given.
2 A Robust Deep-Linguistic Theory
Generally, a linguistic analysis model aims at
complete and correct analysis, which means
that the mapping between the text data and its
syntactic and semantic analysis is sound (the
model extracts correct readings) and complete
(the model deals with all language phenomena).
In practice, however, both goals cannot be to-
tally reached. The main obstacle for soundness
is the all-pervasive characteristic of natural lan-
guage to be ambiguous, where ambiguities can
often only be resolved with world knowledge.
Statistical disambiguation such as (Collins
and Brooks, 1995) for PP-attachment or
(Collins, 1997; Charniak, 2000) for generative
parsing greatly improve disambiguation, but as
they model by imitation instead of by under-
standing, complete soundness has to remain elu-
sive.
As for completeness, already early ?na??ve?
statistical approaches have shown that the prob-
lem of grammar size is not solved but even ag-
gravated by a naive probabilistic parser imple-
mentation, in which e.g. all CFG rules permit-
ted in the Penn Treebank are extracted. From
his 300,000 words training part of the Penn
Treebank (Charniak, 1996) obtains more than
10,000 CFG rules, of which only about 3,000
occur more than once. It is therefore necessary
to either discard infrequent rules, do manual
editing, use a different rule format such as indi-
vidual dependencies (Collins, 1996) or gain full
linguistic control and insight by using a hand-
written grammar ? each of which sacrifices total
completeness.
2.1 Near-full Parsing
The approach we have chosen is to use a
manually-developed wide-coverage tag sequence
grammar (Abney, 1995; Briscoe and Carroll,
2002), and to exclude or restrict rare, marked
and error-prone phenomena. For example,
while it is generally possible for nouns to be
modified by more than one PP, only nouns seen
in the Treebank with several PPs are allowed to
have several PPs. Or, while it is generally possi-
ble for a subject to occur to the immediate right
of a verb (said she), this is only allowed for verbs
seen with a subject to the right in the train-
ing corpus, typically verbs of utterance, and
only in a comma-delimited or sentence-final con-
text. This entails that the parser profits from
a lean grammar but finds a complete structure
spanning the entire sentence in the majority of
real-world sentences and needs to resorts to col-
lecting partial parses in the remaining minority.
Starting from the most probable longest span,
recursively the most probable longest span to
left and right is searched.
Near-full parsing only leads to a very small
loss. If an analysis consists of two partial parses,
on the dependency relation level only the one,
usually high-level relation between the heads
of the two partial parses remains unexpressed.
The risk of returning ?garden path?, locally
correct but globally wrong, analyses diminishes
with increasing span length.
2.2 Functional Dependency Grammar
We follow the broad architecture suggested by
(Abney, 1995) which naturally integrates chunk-
ing and dependency parsing and has proven
to be practical, fast and robust (Collins, 1996;
Basili and Zanzotto, 2002). Tagging and chunk-
ing are very robust, finite-state approaches,
parsing then only occurs between heads of
chunks.2 The perspicuous rules of a hand-
written dependency grammar build up the pos-
sible syntactic structures, which are ranked and
pruned by calculating lexical attachment proba-
bilities for the majortiy of the dependency rela-
tions used in the grammar. The grammar con-
tains around 1000 rules containing the depen-
dent?s and the head?s tag, the direction of the
dependency, lexical information for closed class
words, and context restrictions3. Context re-
strictions express e.g. that only a verb which
has an object in its context is allowed to attach
a secondary object.
Our approach can be seen as an extension of
(Collins and Brooks, 1995) from PP-attachment
to most dependency relations. Training data
is a partial mapping of the Penn Treebank to
deep-linguistic dependency structures, similar
to (Basili et al, 1998).
Robustness also depends on the grammar
formalism. While many formalisms fail to
2Practical experiments using a toy NP and verb-
group grammar have shown that parsing between heads
of chunks only is about four times faster than parsing
between every word, i.e. without chunking.
3the number of rules is high because of tag combina-
torics leading to many almost identical rules. A subject
relations is e.g. possible between the 6 verb tags and the
4 noun tags
project when subcategorized arguments cannot
be found, in a grammar like DG, in which maxi-
mal projections and terminal nodes are isomor-
phic, projection can never fail.
In classical DG, only content words can be
heads, and there is no distinction between syn-
tactic and semantic dependency ? semantic de-
pendency is used as far as possible. These as-
sumptions entail that there are no functional
and no empty nodes, which means that low com-
plexity O(n3) algorithms such as CYK, which is
used here, can be employed.
The classical dependency grammar distinc-
tion between ordre line?aire and ordre struc-
tural, basically an immediate dominance / linear
precedence distinction (ID/LP) also has the ad-
vantage that a number of phenomena classically
assumed to involve long-distance dependencies,
fronted or inversed constituents, can be treated
locally. They only need rules that allow an in-
version of the ?canonical? dependency direction
under well-defined conditions. As for fronted el-
ements, since DG does not distinguish between
external and internal arguments, front positions
are always locally available to the verb.
2.3 Underspecification and
Disambiguation
The cheapest approach to dealing with the all-
pervasive NL ambiguity is to underspecifiy ev-
erything, which leads to a sound and complete
mapping, but one that is content-free and ab-
surd. But in few, carefully selected areas where
distinctions do not matter for the task at hand,
where the disambiguation task is particularly
unreliable, or where inter-annotator agreement
is very low, underspecification can serve as a
tool to greatly facilitate linguistic analysis. For
example, intra-base NP ambiguities, such as
quantifier scope ambiguities do not matter for
a parser like ours aiming at predicate-argument
structure, and are thus not attempted to an-
alyze. There is one part-of-speech distinction
where inter-annotator agreement is quite low
and the performance of taggers generally very
poor: the distinction between verbal particles
and prepositions. We currently leave the dis-
tinction underspecified, but a statistical disam-
biguator is being developed.
Conversely, the Penn Treebank annotation is
sometimes not specific enough. The parser dis-
tinguishes between the reading of the tag IN as
a complementizer or as a preposition, and dis-
ambiguates commas as far as it can, between
apposition, subordination and conjunction.
Some typical tagging errors can be robustly
corrected by the hand-written grammar. For
example, the distinction between verb past
tense VBD and participle VBN is unreliable,
but can usually be disambiguated in the pars-
ing process by leaving this tag distinction un-
derspecified for a number of constructions.
2.4 Long-distance Dependencies
Long-distance dependencies exponentially
increase parsing complexity (Neuhaus and
Bro?ker, 1997). We therefore use an approach
that preprocesses, post-processes and partly
underspecifies them, allowing us to use a
context-free grammar at parse time.
In detail, (1) before the parsing we model
dedicated patterns across several levels of con-
stituency subtrees partly leading to dedicated,
compressed and fully local dependency rela-
tions, (2) we use statistical lexicalized post-
processing, and (3) we rely on traditional De-
pendency Grammar assumptions (section 2.2).
2.4.1 Pre-processing
(Johnson, 2002) presents a pattern-matching al-
gorithm for post-processing the output of sta-
tistical parsers to add empty nodes to their
parse trees. While encouraging results are re-
ported for perfect parses, performance drops
considerably when using trees produced by a
statistical parser. ?If the parser makes a sin-
gle parsing error anywhere in the tree fragment
matched by the pattern, the pattern will no
longer match. This is not unlikely since the sta-
tistical model used by the parser does not model
these larger tree fragments. It suggests that one
might improve performance by integrating pars-
ing, empty node recovery and antecedent find-
ing in a single system ... ? (Johnson, 2002).
We have applied structural patterns to the
Penn Treebank, where like in perfect parses pre-
cision and recall are high, and where in addi-
tion functional labels and empty nodes are avail-
able, so that patterns similar to Johnson?s but
? like (Jijkoun, 2003) ? relying on functional
labels and empty nodes reach precision close to
100%. Unlike in Johnson, also patterns for local
dependencies are used; non-local patterns sim-
ply stretch across more subtree-levels. We use
the extracted lexical counts as lexical frequency
training material. Every dependency relation
has a group of structural extraction patterns
associated with it. This amounts to a partial
mapping of the Penn Treebank to Functional
Relation Label Example
verb?subject subj he sleeps
verb?first object obj sees it
verb?second object obj2 gave (her) kisses
verb?adjunct adj ate yesterday
verb?subord. clause sentobj saw (they) came
verb?prep. phrase pobj slept in bed
noun?prep. phrase modpp draft of paper
noun?participle modpart report written
verb?complementizer compl to eat apples
noun?preposition prep to the house
Table 1: The most important dependency types
used by the parser
?










NP-SBJ-X@
noun
VP@






V
passive verb
NP
-NONE-
*-X
?










NP-SBJ-X@
noun
VP@






V
control-verb
S
NP-SBJ
-NONE-
*-X
Figure 1: The extaction patterns for passive
subjects (top) and subject control (bottom)
DG (Hajic?, 1998), (Tapanainen and Ja?rvinen,
1997). Table 1 gives an overview of the most
important dependencies.
The subj relation, for example, has the head
of an arbitrarily nested NP with the functional
tag SBJ as dependent, and the head of an ar-
bitrarily nested VP as head for all active verbs.
In passive verbs, however, a movement involv-
ing an empty constituent is assumed, which cor-
responds to the extraction pattern in figure 1,
where VP@ is an arbitrarily nested VP, and NP-
SBJ-X@ the arbitrarily nested surface subject
and X the co-indexed, moved element. Move-
ments are generally supposed to be of arbitrary
length, but a closer investigation reveals that
this type of movement is fixed.
The same argument can be made for other
relations, for example control structures, which
have the extraction pattern shown in figure 1.
Grammatical role labels, empty node labels and
tree configurations spanning several local sub-
trees are used as integral part of some of the
patterns. This leads to much flatter trees, as
typical for DG, which has the advantages that
(1) it helps to alleviate sparse data by map-
ping nested structures that express the same
dependency relation, (2) the costly overhead for
dealing with unbounded dependencies can be
largely avoided, (3) it is ensured that the lex-
ical information that matters is available in one
central place, allowing the parser to take one
well-informed decision instead of several brittle
decisions plagued by sparseness, which greatly
reduces complexity and the risk of errors (John-
son, 2002). Collapsing deeply nested structures
into a single dependency relation is less complex
but has the same effect as carefully selecting
what goes in to the parse history in history-
based approaches. ?Much of the interesting
work is determining what goes into [the history]
H(c)?(Charniak, 2000).
(Schneider, 2003a) shows that the vast ma-
jority of LDDs can be treated in this way,
essentially compressing non-local subtrees into
dedicated relations even before grammar writ-
ing starts. The compressed trees correspond
to a simple LFG f-structure. The trees ob-
tained from parsing can be decompressed into
traditional constituency trees including empty
nodes and co-indexation, or into shallow seman-
tic structures such as Minimal Logical Forms
(MLF) (Rinaldi et al, 2004b; Schneider et al,
2000; Schwitter et al, 1999). This approach
leaves LDDs underspecified, but recoverable,
and makes no claims as to whether empty nodes
at an automonous syntactic level exist or not.
2.4.2 Post-Processing
After parsing, shared constituents can be ex-
tracted again. The parser explicitly does this
for control, raising and semi-auxiliary relations,
because the grammar does not distinguish be-
tween subordinating clauses with and without
control. A probability model based on the verb
semantics is invoked if a subordinate clause
without overt subject is seen, in order to decide
whether the matrix clause subject or object is
shared.
2.4.3 What do we lose?
Among the 10 most frequent types of empty
nodes, which cover more than 60,000 of the
64,000 empty nodes in the Penn treebank, there
are only two problematic LDD types: WH
Traces and indexed gerunds.
WH traces Only 113 of the 10,659 WHNP
antecedents in the Penn Treebank are actually
question pronouns. The vast majority, over
9,000, are relative pronouns. For them, an in-
version of the direction of the relation they have
to the verb is allowed if the relative pronoun
Figure 2: Pro3Gres flowchart
precedes the subject. This method succeeds in
most cases, but linguistic non-standard assump-
tions need to be made for stranded prepositions.
Only non-subject WH-question pronouns and
support verbs need to be treated as ?real?
non-local dependencies. In question sentences,
before the main parsing is started, the sup-
port verb is attached to any lonely participle
chunk in the sentence, and the WH-pronoun
pre-parses with any verb.
Indexed Gerunds Unlike in control, rais-
ing and semi-auxiliary constructions, the an-
tecedent of an indexed gerund cannot be es-
tablished easily. The fact that almost half of
the gerunds are non-indexed in the Penn Tree-
bank indicates that information about the un-
expressed participant is rather semantic than
syntactic in nature, much like in pronoun res-
olution. Currently, the parser does not try to
decide whether the target gerund is an indexed
or non-indexed gerund nor does it try to find the
identity of the lacking participant in the latter
case. This is an important reason why recall
values for the subject and object relations are
lower than the precision values.
3 Robustness ?in the small?
In addition to a robust deep-linguistic design
(robustness ?in the large?, section 2), the im-
plemented parser, Pro3Gres, uses a number of
practical robust approaches ?in the small? at
each processing level, such as relying on finite-
state tagging and chunking or collecting par-
tial parses if no complete analysis can be found,
or using incrementally more aggressive pruning
techniques in very long sentences. During the
parsing process, only a certain number of alter-
natives for each possible span are kept. Experi-
ments have shown that using a fixed number or
a number dependent on the parsing complex-
ity in terms of global chart entries lead to very
similar results. Using reasonable beam sizes in-
creases parsing speed by an order of magnitude
while hardly affecting parser performance. For
the fixed number model, performance starts to
collapse only when less than 4 alternatives per
span are kept.
When a certain complexity has been reached
(currently 1000 chart entries), only reductions
above a certain probability threshold are per-
missible. The threshold starts very low, but
is a function of the total number of chart en-
tries. This entails that even sentences with
hundreds of words can be parsed quickly, but
it is not aimed at finding complete parses for
them, rather a graceful degradation of perfor-
mance (Menzel, 1995) is intended.
4 A hybrid approach on many levels
Pro3Gres profits from being hybrid on many
levels. Hybridness means that the most robust
approach can be chosen for each task and each
processing level.
statistical vs. rule-based the most obvious
way in which Pro3Gres is a hybrid (Schneider,
2003b). Unlike formal grammars to which post-
hoc statistical disambiguators can be added,
Pro3Gres has been designed to be hybrid, care-
fully distinguishing between tasks that can best
be solved by finite-state methods, rule-based
methods and statistical methods. While e.g.
grammar writing is easy for a linguist, and a
naive Treebank grammar suffers from similar
complexity problems as a comprehensive for-
mal grammar, the scope of application and the
amount of ambiguity a rule creates is often be-
yond our imagination and best handled by a
statistical system.
shallow vs. deep the designing philosophy
for Pro3Gres has been to stay as shallow as pos-
sible to obtain reliable results at each level.
Treebank constituency vs. DG the obser-
vation that a DG that expresses grammatical
relations is more informative, but also more in-
tuitive to interpret for a non-expert, and that
Functional DG can avoid a number of LDD
types has made DG the formalism of our choice.
For lexicalizing the grammar, a partial mapping
from the largest manually annotated corpus
available, the Penn Treebank, was necessary, ex-
hibiting a number of mapping challenges.
history-based vs. mapping-based
Pro3Gres is not a parse-history-based ap-
proach. Instead of manually selecting what
goes into the history, as is usually done (see
(Henderson, 2003) for an exception), we man-
ually select how to linguistically meaningfully
map Treebank structures onto dependency re-
lations by the use of mapping patterns adapted
from (Johnson, 2002).
probabilistic vs. statistical Pro3Gres is
not a probabilistic system in the sense of a
PCFG. From a practical viewpoint, knowing the
probability of a certain rule expansion per se
is of little interest. Pro3Gres models decision
probabilities, the probability of a parse is un-
derstood to be the product of all the decision
probabilities taken during the derivation.
local subtress vs. DOP psycholinguistic
experiments and Data-Oriented Parsing (DOP)
(Bod et al, 2003) suggest that people store
subtrees of various sizes, from two-word frag-
ments to entire sentences. But (Goodman,
2003) suggests that the large number of sub-
trees can be reduced to a compact grammar that
makes DOP parsing computationally tractable.
In Pro3Gres, a subset of non-local fragments
which, based on linguistic intuition are espe-
cially important, are used.
generative vs. structure-generating DG
generally, although generative in the sense that
connected complete structures are generated, is
not generative in the sense that it is always
guaranteed to terminate if used for random gen-
eration of language. Since a complete or partial
hierarchical structure that follows CFG assump-
tions due to the employed grammar is built up
for each sentence. Pro3Gres? constraint to allow
each complement dependency type only once
per verb can be seen as a way of rendering it
generative in practice.
syntax vs. semantics instead of using
a back-off to tags (Collins, 1999), semantic
classes, Wordnet for nouns and Levin classes
for verbs, are used, in the hope that they better
manage better to express selectional restrictions
than tags. Practical experiments have shown,
however, that, in accordance to (Gildea, 2001)
on head-lexicalisation, there is almost no in-
crease in performance.
5 Applications and Evaluation
Pro3Gres is currently being applied in a Ques-
tion Answering system specifically targeted at
Figure 3: Dependency Tree output of the SWI Prolog graphical implementation of the parser
technical domains (Rinaldi et al, 2004b). One
of the main advantages of a dependency-based
parser such as Pro3Gres over other parsing ap-
proaches is that a mapping from the syntactic
layer to a semantic layer (meaning representa-
tion) is partly simplified (Molla? et al, 2000; Ri-
naldi et al, 2002).
The original version of the QA system used
the Link Grammar (LG) parser (Sleator and
Temperley, 1993), which however had a number
of significant shortcomings. In particular the
set of the dependency relations used in LG is
very idiosyncratic, which makes any syntactic-
semantic mapping created for LG necessarily
unportable and difficult to extend and maintain.
A recent line of research concerns applications
for the Semantic Web. The documents avail-
able in the World Wide Web are mostly written
in natural language. As such, they are under-
standable only to humans. One of the directions
of Semantic Web research is about adding a
layer to the documents that somehow formalizes
their content, making it understandable also to
software agents. Such Semantic Web annota-
tions can be seen as a way to mark explicitly
the meaning of certain parts of the documents.
The dependency relations provided by a parser
such as Pro3Gres, combined with domain spe-
cific axioms, allow the creation of (some of) the
semantic annotations, as described in (Rinaldi
et al, 2003; Kaljurand et al, 2004).
The modified QA system (using Pro3Gres) is
being exploited in the area of ?Life Sciences?, for
applications concerning Knowledge Discovery
over Medline abstracts (Rinaldi et al, 2004a;
Dowdall et al, 2004). We illustrate some of the
differences between general-purpose parsing and
the parsing of highly technical texts like Med-
line and give two evaluations.
5.1 General unrestricted texts
We first report an evaluation on sentences from
an open domain, which gives a good impression
of the performance of the parser on general, un-
restricted text.
In traditional constituency approaches,
parser evaluation is done in terms of the corre-
spondence of the bracketting between the gold
standard and the parser output. (Lin, 1995;
Carroll et al, 1999) suggest evaluating on the
linguistically more meaningful level of syntactic
relations. Two evaluations on the syntactic
relation level are reported in the following.
First, a general-purpose evaluation using a
hand-compiled gold standard corpus (Carroll
et al, 1999), which contains the grammatical
relation data of 500 random sentences from the
Susanne corpus.
The performance, shown in table 2, is, accord-
ing to (Preiss, 2003), similar to a large selection
of statistical parsers and a grammatical relation
finder. Relations involving long-distance depen-
dencies form part of these relations. In order to
measure specifically their performance, a selec-
tion of them is also given: WH-Subject (WHS),
WH-Object (WHO), passive Subject (PSubj),
control Subject (CSubj), and the anaphor of the
relative clause pronoun (RclSubjA).
5.2 Parsing highly technical language
While measuring general parsing performance is
fundamental in the development of any parsing
system there is a danger of fostering domain de-
pendence in concentrating on a single domain.
In order to answer how the parser performs
over domains markedly different to the training
corpus , the parser has been applied to the GE-
NIA corpus (Kim et al, 2003), 2000 MEDLINE
abstracts of more than 400,000 words describing
the results of Biomedical research.
Average sentence length is 27 words, the lan-
Percentage Values for some relations, general, on Carroll corpus only LDD-involving
Subject Object noun-PP verb-PP subord. cl. WHS WHO PSubj CSubj RclSubjA
Precision 91 89 73 74 68 92 60 n/a 80 89
Recall 81 83 67 83 n/a 90 86 83 n/a 63
Table 2: Results of evaluating the parser output on Carroll?s test suite on subject, object, PP-
attachment and clause subordination relations, and a selective evaluation of 5 relations involving
long-distance dependencies (LDD)
Percentage Values for some relations, general, on the GENIA corpus
Subject Object noun-PP verb-PP subord. clause
Precision 90 94 83 82 71
Recall 86 95 82 84 75
Table 3: Results of evaluating 100 random sentences from the terminology-annotated GENIA
corpus, on subject, object, PP-attachment and clause subordination relations
guage is very technical and extremely domain-
specific. But the most striking characteristic
of this domain is the frequency of MultiWord
Terms (MWT) which are known to cause seri-
ous problems for NLP systems (Sag et al, 2002),
(Dowdall et al, 2003). The token to chunk ra-
tio: NPs = 2.3 , VPs = 1.3 (number of tokens
divided by the number of chunks) is unusually
high.
The GENIA corpus does not include any syn-
tactic annotation (making standard evaluation
more difficult) but approx. 100, 000 MWTs are
annotated and assigned a semantic type from
the GENIA ontology.
This novel parsing application is designed to
determine how parsing performance interacts
with MWT recognition as well as the applica-
bility and possible improvements to the proba-
blistic model over this domain, to test the hy-
pothesis if terminology is the key to a successful
parsing system. We do not discard this infor-
mation, thus simulating a situation in which a
near-perfect terminology-recognition tool is at
one?s disposal. MWT are regarded as chunks,
the parsing thus takes place between between
the heads of MWT, words and chunks.
100 random sentences from the GENIA cor-
pus have been manually annotated for this eval-
uation and compared to the parser output. De-
spite the extreme complexity and technical lan-
guage, parsing performance under these condi-
tions is considerably better than on the Carroll
corpus when using automated chunking, as ta-
ble 3 reveals.
It is worth noting that 10 of the 17 subject
precision errors (out of 171 subjects) are ?hard?
cases involving long-distance dependencies (1
control, 4 relative pronouns) and 5 verb group
chunking errors. Equally interesting, 2 of the 4
object recall errors (out of 79 objects) are due
to 1 mistagging and 1 mischunking.
In practice, MWT extraction is still not au-
tomated to the level of chunking or Name En-
tity recognition simulated in this experiment
(for a comprehensive review of the state-of-the-
art see (Castellv et al, 2001)). This is, in
a large part, due to the lack of definitive or-
thographic, morphological and syntactic char-
acteristics to differentiante between MWTs and
canonical phrases. So MWT extraction remains
a semi-automated task performed in cycles with
the result of each cycle requiring manual valida-
tion. The return for this time consuming activ-
ity are the characteristics of MWTs which can
be use to fine tune the algorithms during the
next extraction cycle.
6 Conclusion
We have suggested a robust, deep-linguistic
grammar theory delivering grammatical rela-
tion structures as output, which are closer to
predicate-argument structures than pure con-
stituency structures, and more informative if
non-local dependencies are involved. We have
presented an implementation of the theory that
is used for large-scale parsing. An evaluation
at the grammatical relation level shows that its
performance is state-of-the-art.
References
Steven Abney. 1995. Chunks and dependencies:
Bringing processing evidence to bear on syntax.
In Jennifer Cole, Georgia Green, and Jerry Mor-
gan, editors, Computational Linguistics and the
Foundations of Linguistic Theory, pages 145?164.
CSLI.
Roberto Basili and Fabio Massimo Zanzotto. 2002.
Parsing engineering and empirical robustness.
Journal of Natural Language Engineering, 8/2-3.
Roberto Basili, Maria Teresa Pazienza, and
Fabio Massimo Zanzotto. 1998. Evaluating a ro-
bust parser for Italian language. In Proceedings of
Evaluations of Parsing Systems Workshop, held
jointly with 1st LREC, Granada,Spain.
Rens Bod, Remko Scha, and Khalil Sima?an, editors.
2003. Data-Oriented Parsing. Center for the
Study of Language and Information, Studies in
Computational Linguistics (CSLI-SCL). Chicago
University Press.
Ted Briscoe and John Carroll. 2002. Robust accu-
rate statistical annotation of general text. In Pro-
ceedings of the 3rd International Conference on
Language Resources and Evaluation, pages 1499?
1504, Las Palmas, Gran Canaria.
M. Burke, A. Cahill, R. O?Donovan, J. van Gen-
abith, and A. Way. 2004. Treebank-based ac-
quisistion of wide-coverage, probabilistic LFG re-
sources: Project overview, results and evalua-
tion. In The First International Joint Confer-
ence on Natural Language Processing (IJCNLP-
04), Workshop ?Beyond shallow analyses - For-
malisms and statistical modeling for deep analy-
ses?, Sanya City, Hainan Island, China.
Aoife Cahill, Michael Burke, Ruth O?Donovan,
Josef van Genabith, and Andy Way. 2004.
Long-distance dependency resolution in automat-
ically acquired wide-coverage PCFG-based LFG
approximations. In Proceedings of ACL-2004,
Barcelona, Spain.
John Carroll, Guido Minnen, and Ted Briscoe.
1999. Corpus annotation for parser evaluation.
In Proceedings of the EACL-99 Post-Conference
Workshop on Linguistically Interpreted Corpora,
Bergen, Norway.
M. Teresa Cabre? Castellv, Rosa Estopa?, and
Jordi Vivaldi Palatresi, 2001. Recent Advances in
Computational Terminology, chapter Automatic
term detection: A review of current systems,
pages 53?87. John Benjamins.
Eugene Charniak. 1996. Tree-bank grammar. Tech-
nical Report Technical Report CS-96-02, Depart-
ment of Computer Science, Brown University.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North
American Chapter of the ACL, pages 132?139.
Michael Collins and James Brooks. 1995. Preposi-
tional attachment through a backed-off model. In
Proceedings of the Third Workshop on Very Large
Corpora, Cambridge, MA.
Michael Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the Thirty-Fourth Annual Meeting of the
Association for Computational Linguistics, pages
184?191, Philadelphia.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proc. of the 35th
Annual Meeting of the ACL, pages 16?23, Madrid,
Spain.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Pter Dienes and Amit Dubey. 2003. Antecedent
recovery: Experiments with a trace tagger. In
Proceedings of the 2003 Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP), Sapporo, Japan.
James Dowdall, Fabio Rinaldi, Fidelia Ibekwe-
Sanjuan, and Eric SanJuan. 2003. Complex
structuring of term variants for question answer-
ing. In Proceedings of the ACL workshop on Mul-
tiWord Expressions: Analysis, Acquisition and
Treatment, Sapporo, Japan, July.
James Dowdall, Fabio Rinaldi, Andreas Persidis,
Kaarel Kaljurand, Gerold Schneider, and Michael
Hess. 2004. Terminology expansion and re-
lation identification between genes and path-
ways. In Workshop on Terminology, Ontology
and Knowledge Representation. Universite Jean
Moulin (Lyon 3).
Jason Eisner. 1997. Bilexical grammars and a cubic-
time probabilistic parser. In Proceedings of the
5th International Workshop on Parsing Technolo-
gies, pages 54?65, MIT, Cambridge, MA, Septem-
ber.
Jason Eisner. 2000. Bilexical grammars and their
cubic-time parsing algorithms. In Harry Bunt and
Anton Nijholt, editors, Advances in Probabilis-
tic and Other Parsing Technologies. Kluwer Aca-
demic Publishers.
Daniel Gildea. 2001. Corpus variation and parser
performance. In Proceedings of the 2001 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 167?202, Pittsburgh,
PA.
Joshua Goodman. 2003. Efficient parsing of DOP
with PCFG-reductions. In Bod et al (Bod et al,
2003).
Jan Hajic?. 1998. Building a syntactically annotated
corpus: The Prague Dependency Treebank. In
Eva Hajic?ova?, editor, Issues of Valency and Mean-
ing. Studies in Honor of Jarmila Panevova?, pages
106?132. Karolinum, Charles University Press,
Prague.
James Henderson. 2003. Inducing history repre-
sentations for broad coverage statistical parsing.
In Proceedings of HLT-NAACL 2003, Edmonton,
Canada.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with combi-
natory categorial grammar. In Proceedings of 40th
Annual Meeting of the Association for Computa-
tional Linguistics, Philadelphia.
Valentin Jijkoun. 2003. Finding non-local depen-
dencies: beyond pattern matching. In Proceedings
of the ACL 03 Student Workshop, Budapest.
Mark Johnson. 2002. A simple pattern-matching
algorithm for recovering empty nodes and their
antecedents. In Proceedings of the 40th Meeting
of the ACL, University of Pennsylvania, Philadel-
phia.
Kaarel Kaljurand, Fabio Rinaldi, James Dowdall,
and Michael Hess. 2004. Exploiting language re-
sources for semantic web annotations. In Proceed-
ings of LREC 2004, Lisbon, May 24-30. accepted
for publication.
Ron Kaplan, Stefan Riezler, Tracy H. King, John
T. Maxwell III, Alex Vasserman, and Richard
Crouch. 2004. Speed and accuracy in shallow
and deep stochastic parsing. In Proceedings of
HLT/NAACL 2004, Boston, MA.
J.D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003.
Genia corpus - a semantically annotated corpus
for bio-textmining. Bioinformatics, 19(1):i180?
i182.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of IJCAI-95, Montreal.
Mitch Marcus, Beatrice Santorini, and M.A.
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Wolfgang Menzel. 1995. Robust processing of natu-
ral language. Lecture Notes in Computer Science,
981:19?34.
Diego Molla?, Gerold Schneider, Rolf Schwitter, and
Michael Hess. 2000. Answer Extraction using a
Dependency Grammar in ExtrAns. Traitement
Automatique de Langues (T.A.L.), Special Issue
on Dependency Grammar, 41(1):127?156.
Peter Neuhaus and Norbert Bro?ker. 1997. The com-
plexity of recognition of linguistically adequate
dependency grammars. In Proceedings of the 35th
ACL and 8th EACL, pages 337?343, Madrid,
Spain.
Joakim Nivre. 2004. Inductive dependency parsing.
In Proceedings of Promote IT, Karlstad Univer-
sity.
Judita Preiss. 2003. Using grammatical relations to
compare parsers. In Proc. of EACL 03, Budapest,
Hungary.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Journal
using a Lexical-Functional Grammar and discrim-
inative estimation techniques. In Proc. of the 40th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?02), Philadephia, PA.
Fabio Rinaldi, James Dowdall, Michael Hess, Diego
Molla?, and Rolf Schwitter. 2002. Towards Answer
Extraction: an application to Technical Domains.
In ECAI2002, European Conference on Artificial
Intelligence, Lyon, pages 460?464.
Fabio Rinaldi, Kaarel Kaljurand, James Dowdall,
and Michael Hess. 2003. Breaking the deadlock.
In ODBASE 2003 (International Conference on
Ontologies, Databases and Applications of Seman-
tics) Catania, Italy., volume 2889 of Lecture Notes
in CS. Springer Verlag.
Fabio Rinaldi, James Dowdall, Gerold Schneider,
and Andreas Persidis. 2004a. Answering Ques-
tions in the Genomics Domain. In ACL 2004
Workshop on Question Answering in restricted
domains, Barcelona, Spain, 21?26 July.
Fabio Rinaldi, Michael Hess, James Dowdall, Diego
Molla?, and Rolf Schwitter. 2004b. Question an-
swering in terminology-rich technical domains. In
Mark Maybury, editor, New Directions in Ques-
tion Answering. MIT/AAAI Press.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: a Pain in the Neck for NLP. In Pro-
ceedings of the Third International Conference,
CICLing 2002, pages 1?15, Mexico City, Febru-
rary.
Anoop Sarkar, Fei Xia, and Aravind Joshi. 2000.
Some experiments on indicators of parsing com-
plexity for lexicalized grammars. In Proc. of
COLING.
Gerold Schneider, Diego Molla` Aliod, and Michael
Hess. 2000. Inkrementelle minimale logische for-
men fr die antwortextraktion. In Proceedings of
34th Linguistic Colloquium, September 1999, Uni-
versity of Mainz, FASK.
Gerold Schneider. 2003a. Extracting and using
trace-free Functional Dependencies from the Penn
Treebank to reduce parsing complexity. In Pro-
ceedings of Treebanks and Linguistic Theories
(TLT) 2003, Va?xjo?, Sweden.
Gerold Schneider. 2003b. A low-complexity, broad-
coverage probabilistic dependency parser for En-
glish. In Proceedings of HLT-NAACL 2003 Stu-
dent session, Edmonton, Canada.
Rolf Schwitter, Diego Molla? Aliod, and Michael
Hess. 1999. ExtrAns - answer extraction from
technical documents by minimal logical forms
and selective highlighting. In Proceedings of The
Third International Tbilisi Symposium on Lan-
guage, Logic and Computation, Batumi, Georgia.
Daniel D. Sleator and Davy Temperley. 1993. Pars-
ing English with a link grammar. In Proc. Third
International Workshop on Parsing Technologies,
pages 277?292.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A non-
projective dependency parser. In Proceedings of
the 5th Conference on Applied Natural Language
Processing, pages 64?71. Association for Compu-
tational Linguistics.
Proceedings of the Workshop on BioNLP, pages 80?88,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
TX Task:
Automatic Detection of Focus Organisms in Biomedical Publications
Thomas Kappeler, Kaarel Kaljurand, Fabio Rinaldi?
Institute of Computational Linguistics, University of Zurich
kappeler@bluewin.ch, kalju@cl.uzh.ch, rinaldi@cl.uzh.ch
Abstract
In biomedical information extraction (IE), a
central problem is the disambiguation of am-
biguous names for domain specific entities,
such as proteins, genes, etc. One important
dimension of ambiguity is the organism to
which the entities belong: in order to disam-
biguate an ambiguous entity name (e.g. a pro-
tein), it is often necessary to identify the spe-
cific organism to which it refers.
In this paper we present an approach to the
detection and disambiguation of the focus or-
ganism(s), i.e. the organism(s) which are the
subject of the research described in scientific
papers, which can then be used for the disam-
biguation of other entities.
The results are evaluated against a gold stan-
dard derived from IntAct annotations. The
evaluation suggests that the results may al-
ready be useful within a curation environment
and are certainly a baseline for more complex
approaches.
1 Introduction
The task of identifying the organisms which are in-
volved in research described in biomedical articles
is extremely important for the field of biomedical in-
formation extraction (IE), both in itself and in con-
nection with other tasks. In itself, because the con-
cept of biological taxonomy is basic for every re-
searcher: organisms and their taxonomic classifica-
tion can be used very effectively in various contexts,
for example to restrict searches, a classical infor-
mation retrieval (IR) task. At the same time, any
biomedical text mining system would be incomplete
without the possibility to use organisms as concepts,
e.g. in finding (statistical) associations, which can
?Corresponding author
then be used to form hypotheses about causal rela-
tions.
The necessity of identifying organisms is even
more evident as part of other important entity recog-
nition tasks in biomedical information extraction
(IE), e.g. identification and disambiguation of pro-
teins mentioned in the literature. For example,
within the PPI task (identification of protein-protein
interactions) of Biocreative II (Krallinger et al,
2008), the identification of the focus organism was
seen by many participants as an essential subtask in
order to properly disambiguate protein names. Pro-
tein interactions are fundamental for most biological
processes, therefore they are at the focus of a huge
and fast growing number of biomedical papers. As
these cannot all be read or even inspected by the re-
searchers, databases such as IntAct (Kerrien et al,
2006) or MINT (Zanzoni et al, 2002) try to create a
reliable catalogue of experimentally detected inter-
actions by extracting them ?manually? from the lit-
erature through the usage of human experts. This is
known as ?curation?, a costly and time-consuming
process, which could be speeded up much by effi-
cient, robust and precise extraction tools.
One of the most important obstacles for efficient
automatic identification of proteins is the extreme
ambiguity of the commonly used protein names in
the literature. The fragmentation of the biomedical
scientific community into lots of extremely special-
ized sub-communities seems to be the main reason
for this ambiguity. In most cases, the ambiguity is
between homologous proteins of different species.
Any human reader belonging to the sub-community
concerned can, in general, disambiguate an ambigu-
ous protein name like ?goat? (which can refer to
proteins found in four different organisms: human,
rat, mouse and zebrafish), as the species is obvious
to them from the context. However, this ambiguity
80
remains problematic for IE systems (and even for
curators in some cases) and needs to be solved be-
fore more complex tasks, such as protein interaction
detection, can be effectively tackled (Rinaldi et al,
2008).
Our goal is to be able to identify automatically
the focus organisms, i.e. the organisms that are
mentioned in the paper as the hosts of the exper-
iments described, or as the sources of the entities
involved. This information can then be used for tag-
ging papers for more efficient organism-based infor-
mation retrieval, or, more commonly, for the dis-
ambiguation of other entities mentioned in the same
paper. Since organism recognition is normally per-
formed with reference to a taxonomical organization
(of Linnean origin) of all known organisms (in our
case, the NCBI taxonomy) this task is often referred
to as ?TX task?.
In the rest of this paper we describe in section 2
the resources used and the approach followed in or-
der to extract and rank candidate organisms. In sec-
tion 3 we present our results and propose a more fine
grained interpretation of the task, which we again
evaluate. Finally in section 4 we compare our ap-
proach to previous work and discuss its limitations.
2 Methods
Our approach can be described briefly as (1) find all
explicit mentions of organisms either by their scien-
tific or ?common? names; (2) count these mentions
and combine the resulting numbers with a simple
use of statistics to arrive at a ranked list or a sim-
ple set of organisms which can be used, among other
things, to disambiguate ambiguous protein names in
the article under investigation.
2.1 Resources Used
The first step for this approach was to choose a
widely accepted taxonomy which not just includes
unambiguous identifiers for all known organisms,
but also provides a sufficiently large list of names
for them. The taxonomy selected for this was the
NCBI Taxonomy1.
1Available as archive taxdmp.zip from
ftp://ftp.ncbi.nih.gov/pub/taxonomy/. We worked with a
version downloaded on July 10th 2008. The file nodes.dmp
contains the taxonomy as a set of 443,299 nodes for the taxa
and immediate-dominance-relations between them. The file
As most of these organism are unlikely to ever oc-
cur in biomedical literature, we decided to restrict
our interest to the organisms for which a UniProt
organism mnemonic identifier exists. UniProt
(UniProt Consortium, 2007) is a database containing
detailed information about known proteins, obtained
by a process of curation of the biomedical literature.
For every protein, a ?mnemonic? identifier is de-
fined (e.g. HBA HUMAN for ?Human Hemoglobin
A?) which is composed by a shorthand for the pro-
tein name and a simple unique identifier for the or-
ganism. Within the UniProt entry for the protein,
the organism is also referred to by its NCBI iden-
tifier, allowing the construction of a mapping from
the mnemonic identifiers for the organisms used by
UniProt to their equivalent NCBI identifiers.
The set of organism that have a UniProt
mnemonic identifier (11,444 organisms) probably
covers the near totality of organisms that have been
subject to research in molecular biology. In the
NCBI taxonomy 31,733 names are defined for that
subset of organisms. Although several classes of
names are defined by NCBI, for the purpose of
this work we distinguish only between ?scientific
names? and the other classes (pooled together as
?common names?).2
As an additional source of information, we used
the IntAct database of protein interactions3 for two
different purposes:
? to derive statistical measures used later by the
program, most importantly the frequency of
each focus organism in papers curated by Int-
Act (using the IntAct annotations as the sources
of the ?focus?).
? to derive a gold standard against which our pro-
grams could be tested
IntAct provides an annotated set of protein in-
teractions. Each interaction is enriched with de-
tailed information about the two proteins involved
names.dmp connects one or several names (619,325) of differ-
ent nameclasses (such as ?scientific? or ?common?) to each
node. The nodes (taxa) are referred to by numeric identifiers.
2While there are no ambiguous ?scientific names? in this
taxonomy, there are several ambiguous ?common names?, but
only very few of these occurred in our sample, e.g. ?mink?,
?barley?, ?green monkey?, and they are very rare.
3Version of May 2008, downloaded from
http://www.ebi.ac.uk/intact/site/contents/downloads.jsf
81
(from which the reference organisms can be recov-
ered), and with the identifier of the paper from which
the interaction was originally derived in the curation
process. This allows to build a gold standard by as-
sociating each paper to its focus organisms.
The sample used in our experiments is a set of 621
PubMed-indexed full text articles, dating from 1995
to 2007, for which IntAct annotations are available.4
2.2 First Experiments and Normalization
As an initial experiment, we performed a simple
lexical lookup of the names of the 11,444 organ-
isms under consideration. In previous applications
of IE techniques for biomedical literature (Kappeler
et al, 2008; Rinaldi et al, 2008) we found that
simple techniques for the generation of variants of
the known names significantly benefited the recall
of the application. For example, multiword protein
names can be subject to a number of minor variants,
such as the introduction of hyphens or the separation
of compound words, which make automatic recog-
nition more challenging. In the case of organism
names, although our initial expectations were sim-
ilar, we found the benefit (in terms of additional re-
call) of such variants to be extremely limited, possi-
bly because names of species are used more consis-
tently than the names of proteins or genes.
Therefore it was possible to implement a simpler
approach to recognition of organism names, based
on lexical lookup against a database containing all
names of interest, coupled with a simple normaliza-
tion step which removes trivial orthographic differ-
ences (such as hyphens) between the key word in
the database and the lookup word from the docu-
ment (for details see (Kaljurand et al, 2009)). The
inclusion of other biomedical NE?s (such as pro-
tein names, method names, cell line names) in the
database together with a strict implementation of
the ?longest match? principle leads to better preci-
sion by eliminating false positives caused by match-
ing organism names with a fragment of a multiword
term for another entity (such as the method ?yeast
two-hybrid?).
As mentioned, the names provided by the NCBI
4The reason of this particular choice is that the same subset
was used for experiments related to the automatic detection of
experimental methods, also using IntAct annotations as a gold
standard, described in (Kappeler et al, 2008).
taxonomy have been classified into ?scientific
names? or ?common names?. Using only ?scientific
names? appeared as an effective way to obtain better
precision, but we soon discovered that precision of
the common names suffered most by a few very bad
names, such as ?Li?, which is a ?common name? for
LIV (Louping ill virus) in the taxonomy, but appears
only (and very frequently) as Chinese surname in the
texts. By eliminating about 25 of similar misleading
?common names? the results of this class rose to the
same level as the ?scientific names?, so there was
no reason to exclude the whole class (as that would
have harmed recall).
Since the bibliography might contain spurious
mentions of other organisms, we automatically re-
moved it from the main text. However, contrary to
expectations, this did not lead to better results for
this task (at least after the elimination of the mislead-
ing ?common names? mentioned above), but was
not reversed because of its effects on other tasks. An
intuition from other tasks was to use the abstracts
instead of the full text of the articles, because that
would tend to exclude accidental mentions of organ-
isms leading to false positives. But a main problem
of this approach is that many abstracts do not yield
any organism mentions. Whenever they do though,
their precision is high. So there is a strong case for
giving the mentions there a higher weight, but obvi-
ously the rest of the article plays an important role
as well. We experimentally found that counting an
?abstract mention? as equivalent to 25 ?fulltext men-
tions? worked best.
2.3 Measures Improving Recall
An experiment using all names provided by NCBI
and considering all mentions of those names in the
fulltext version of each article led to a recall of 83%,
leading us to conclude that either the taxonomy does
not contain all names used, or some organisms are
suggested to the human reader by the context and/or
his anticipations. The first of these problems was
adressed by adding some generated names to the
termbase, the second by the use of a default.
Several possible ways of generating new names
automatically from the names in the database were
considered, but only two were applied successfully,
as described below. One of them was the automatic
generation of additional names from the nameclass
82
?scientific name? (for organisms of species or sub-
species level) by the process of replacing the first
word (which would be the genus name in the classi-
cal Linnean binomial nomenclature) by its first letter
and a dot. The resulting names, such as ?E. coli?, are
widely used, but not included in the taxonomy. A
seemingly large disadvantage of this approach is its
potential for ambiguity: 338 of the resulting names
refer to more than one organism. But the test on
our sample showed that of these only 4 occurred at
all, only 1 more than once: ?C. elegans? (potentially
referring to the organisms identified in UniProt as
CAEEL, CENEL, CESEL and CUNEL) which al-
ways stood for CAEEL, i.e. ?Caenorhabditis ele-
gans?. So excluding the other options for ?C. ele-
gans? eliminated the ambiguity (at least in our sam-
ple). We observed that this type of name is in fre-
quent use only for few species and in this case the
unabbreviated name is often used first, so the addi-
tion of this generated nameclass added little to re-
call.
The other type of name missing from the taxon-
omy is the use of the (Linnean) genus name for a
very frequent species, e.g. ?Arabidopsis? used for
?Arabidopsis thaliana?. Experiments showed that
this type could not be reliably generated automat-
ically from the ?scientific names?, as this name-
class includes many names which do not follow
the rules of Linnean binomial nomenclature, mostly
virus names such as ?Human papillomavirus type
me180? where the first word is generally not a
genus name, but a host name. So the problem of
(potentially huge) ambiguity in this type of names
was not even researched, instead the names of this
type for the most frequent organisms were gener-
ated manually and those which improved the results
were included into the termbase (Saccharomyces,
Arabidopsis, Drosophila, Escherichia, Xenopus and
Synechocystis). The addition of this generated
nameclass did not add much to recall for the same
reason as for the first group: in most cases the un-
abbreviated name appears in the paper as well. To-
gether both groups improved recall by about 3.4%.
As HUMAN is the most frequent organism in
this context, it was obvious that a default HUMAN
would take care of many cases where human readers
disambiguate ambiguous protein names even with-
out any explicit mentions of this species. As there
Table 1: Most frequent organisms in IntAct (derived from
interactor proteins and host organisms)
ORG freq
HUMAN 0.281
YEAST 0.272
MOUSE 0.091
ARATH 0.056
CERAE 0.037
RAT 0.033
DROME 0.028
SCHPO 0.023
ECOLX 0.020
ECOLI 0.013
are no cases (with the current termbase and sample)
of articles with no organism mentions in the full text,
we chose to have a default triggered by no findings
in the abstract. Experiments showed that ? contrary
to intuition ? a weight of the default proportional to
the total number of mentions (just adding a percent-
age to HUMAN) would lead to worse results than an
absolute value for the default.5
2.4 Measures Improving Precision
The simple approach of considering every mention
of each organism (after excluding the misleading
common names, as described above), leads to a pre-
cision of only 27.6%, therefore the list of organism
identifiers obtained in this way has to be considered
as a ?candidates list? from which a selection has to
be made.
Candidates can be of course ranked according to
number of mentions in each article. A ranking based
on the mention counts, taking into account the cor-
rection factor of 25 for mentions in the abstract (as
described in section 2.2), was still far from opti-
mal, so we multiplied the mentions with the relative
frequencies of the organisms in a micro-averaged
frequency table (table 1) computed over all of Int-
Act (not just our sample, to avoid overfitting) and
smoothed roughly by attributing 1% of the probabil-
ity mass to all unseen organisms (over 11,000). This
ranking did far better than expected and after nor-
5 A tentative explanation: In a small paper, the effect of ac-
cidental mentions of ?wrong? organisms is much larger than in
big papers (where the important organisms are mentioned again
and again). This detrimental effect may be counterbalanced by
a relatively stronger default.
83
malizing the whole list to 1, a minimal threshold for
the score could be set up to maximize the f-score by
improving precision at the cost of recall. The actual
value of the threshold (currently 0.04) is of course
arbitrary, depending on what measure one wants to
maximize.
Another problem to be tackled is that different pa-
pers will have different numbers of focus organisms,
ranging from one (in about 70% of the cases), to sev-
eral hundreds (in a few very infrequent cases). It
could be assumed that being able to correctly guess
the number of focus organisms would lead to im-
provement in the TX task, as we could pick only as
many candidate organisms (in their ranking order)
as the expected number for the paper. However, an
experiment using the gold standard as an oracle to
predict the number of organisms to be returned as
a result, instead of using a threshold in the ranking,
did not perform much better (recall was about 1.7%
higher), so we decided not to spend any energy on
exploring ways to predict the number of organisms
as the effect would be minimal, even with perfect
prediction.
Further experiments, such as giving different
weights to mentions of names of different name-
classes, did not lead to better results. Including in-
formation about the precision or recall of the names
encountered in our test set (or the organisms pre-
dicted by them) in the formula for the weights6 did
not lead to better results either.
3 Evaluation and analysis of results
So finally the program in its current form considers
all organism mentions, as delivered by the termbase
search, eliminates the problematic common names,
counts the mentions for each organism in fulltext
and abstracts, multiplies the latter by 25 and adds
them to the fulltext mentions. In case of no abstract
mentions, a default of 28 fulltext mentions is added
to HUMAN (equivalent to about one abstract men-
tion).
The result for each organism is multiplied by the
relative frequency of the organism in IntAct and di-
vided by the sum of the results over all organisms to
6An idea suggested by its successful use in the detection of
experimental methods in (Kappeler et al, 2008) and (Rinaldi et
al., 2008).
Table 2: Most frequent false positives for the best results
with our sample
ORG freq
HUMAN 121
YEAST 104
MOUSE 68
ECOLX 18
DROME 13
ARATH 11
RAT 9
Table 3: Most frequent false negatives for the best results
with our sample
ORG freq
CERAE 73
MOUSE 59
RAT 40
YEAST 21
BOVIN 14
ECOLI 13
ECOLX 13
normalize the sum of the values to 1 (100%). All or-
ganisms under the threshold of 0.04 (or 4%) are then
eliminated from the list.
Our best results (max. f-score) for the task of find-
ing all organisms in the gold standard combining or-
ganisms of interacting proteins and host organisms
are: precision: 0.742; recall: 0.738; f-score: 0.740.
An analysis of the most frequent false positives
is reported in table 2. The ranking is more or less
identical with the frequency table (table 1), which is
what we would expect. Manual inspection of some
of the papers causing these false positives gave the
following results:
? Some names of experimental methods contain-
ing organism names (which could avoid false
positives if recognized as methods) were not
yet included in the termbase.
? Some organisms (or their proteins respectively)
are discussed in the paper, but not as results of
the authors own experiments, so they do not ap-
pear in the gold standard. Obviously the cura-
tors consider only the novel findings reported
in the paper, and all background information is
ignored.
84
Table 4: Most frequent organisms in IntAct (derived from
interactor proteins only)
ORG freq
HUMAN 0.380
MOUSE 0.123
YEAST 0.108
ARATH 0.080
RAT 0.047
DROME 0.040
SCHPO 0.032
ECOLI 0.019
BOVIN 0.016
CAEEL 0.014
? While in some cases the annotators seem to de-
cide that an organism is just used as part of
the method and does not merit an inclusion, in
other cases the annotators do not seem to treat
the problem the same way.
An analysis of the most frequent false negatives
is reported in table 3. The ranking is certainly not
identical with the frequency table (table 1), which
was unexpected. Manual inspection of some of the
papers causing these false negatives gave the follow-
ing results:
? Some common names such as ?mice?, and ad-
jectives such as ?murine?, were absent from the
taxonomy (while ?transgenic mice? e.g. was
present).
? There are probably more hints to recognize
ECOLI (Escherichia coli K12) than just the
presence of the string ?K12? (or ?K-12?). Our
program tends to attribute all mentions of ?Es-
cherichia coli? without this string to ECOLX,
generating false negatives for ECOLI and false
positives for ECOLX.
? The extremely high false negative rate for
CERAE (Chlorocebus aethiops, also known as
Cercopithecus aethiops) is a consequence of its
very different frequencies as source of interac-
tor proteins and as a host organism.
The problem with CERAE suggests that it might
be necessary to consider separately organisms in
their roles as sources of the interactor proteins and as
hosts for the experiments. CERAE is only frequent
as a host organism, but in this role it does not appear
in the papers by any of the organism names given
by the taxonomy (such as ?Chlorocebus aethiops?,
?Cercopithecus aethiops?, ?African green monkey?,
?grivet?, ?savanah monkey? or ?vervet monkey?).
The reason is that often only the names of cell lines
(e.g. ?Vero?) derived from the organism appear in
the paper.7 To a lesser degree, this is true as well for
papers where YEAST appears in this role.
A first step to deal with this problem consisted
in creating different frequency tables for organisms
as source of interactor proteins and as hosts of the
experiment (tables 4 and 5). As these frequency ta-
bles are very different from each other and from the
combined one (table 1) and as the combined task
of identifying ?protein organisms? and ?host organ-
isms? seems to be artificial in any case, we decided
to split the problem accordingly: (a) identify organ-
isms from which interacting proteins are derived; (b)
identify host organisms. The results for each of these
new tasks are not yet as good as the result for the
combined task we described above, but as the infor-
mation we are looking for now is more specific, this
was to be expected.
3.1 Identification of ?Interactor Organisms?
In order to obtain a solution for this more specific
task, we just kept the formula as for the original task,
but replaced the frequency table for ?interactor and
host organisms? (table 1) by a new one for ?interac-
tors only? (table 4). At the same time we raised the
threshold to 18%: as the new freqency tables tended
to nearly eliminate several typical host organisms,
the remaining candidates for ?interactor organisms?
profited by this, so the threshold had to be raised
to maximize f-score. The rest of the parameters re-
mained identical.
Obviously, a new gold standard for ?interactors
only? had to be derived from IntAct. Our best results
for this new task are: precision: 0.697; recall: 0.693;
f-score: 0.695.
3.2 Identification of ?Host Organisms?
For this alternative task we also had to improve the
input, not just the formula, as we noticed that of-
7 The Vero lineage is a very popular cell line isolated from
kidney epithelial cells extracted from an African green monkey
(?Cercopithecus aethiops?).
85
Table 5: Most frequent organisms in IntAct (host organ-
isms only), freq* is computed excluding ?in vitro?
ORG freq freq*
?in vitro? 0.363 -
YEAST 0.262 0.412
HUMAN 0.167 0.264
CERAE 0.036 0.057
MOUSE 0.035 0.055
ARATH 0.021 0.034
DROME 0.021 0.034
SCHPO 0.020 0.031
ECOLX 0.017 0.027
RAT 0.010 0.015
ten species which were given as hosts by IntAct
were not mentioned by any of their names (most
importantly CERAE). So we decided to include an-
other category of biological named entities in our
termbase, namely cell line names. These were de-
rived from one of the largest collections of cell
lines information: the Cell Lines Knowledge Base
(CLKB, (Sarntivijai et al, 2008)). However, a few
cell line names which are type-ambiguous with other
types of NE?s in our termbase (normally proteins)
had to be ignored to avoid conflicts. Another new
input to the formula was the mention of ?in vitro?,
contained in our termbase as a method, but used by
the IntAct annotators as annotation for the ?host or-
ganism?.
The following adaptations to the ranking formula
were necessary. The frequency table for ?interactor
and host organisms? (table 1) was replaced by a new
one for ?hosts only?, including ?in vitro? (table 5).
At the same time the default had to be changed to
?in vitro? and was given a nearly identical weight
of 30 fulltext mentions (instead of 28), the thresh-
old remained at 4% and the abstract mentions were
given a weight of 35 fulltext mentions. The new cell
line mentions were given a weight of 3 fulltext men-
tions for their respective organisms. Of course, a
new gold standard for ?interactors only? was derived
from IntAct also in this case. Our best results yet for
this new task are: precision: 0.689; recall: 0.737;
f-score: 0.712.
4 Related Work and Discussion
The task of organism recognition is only recently
starting to emerge as an independent subtask in
biomedical IE. For example, the latest BioCreative
competitive evaluation of text mining system for bi-
ology8 included a task of protein-protein interaction
detection (Krallinger et al, 2008). Although organ-
ism recognition was not officially evaluated, many
participants found that it was an indispensable step
in order to perform accurate protein recognition and
disambiguation. As a consequence, the BioCreative
meta-server (Leitner et al, 2008), offers organism
recognition as one of its services (called ?TX task?).
(Wang and Matthews, 2008) is perhaps the most
comprehensive study to date dealing with species
disambiguation for term disambiguation. They com-
bine a rule-based species disambiguation approach
with a maximum entropy classifier based on con-
textual features of the term to be disambiguated.
They evaluate in detail the contribution of both ap-
proaches over two separate corpora. While previous
work has shown the benefits of using species infor-
mation for term disambiguation (Alex et al, 2008;
Rinaldi et al, 2008), this is perhaps the first study
which also provides a separate evaluation of species
disambiguation in itself. Since their purpose is to
use the organism mentions to disambiguate entities,
they evaluate how far their system can identify the
organisms associated with each entity mention in
the document. They report a level of accuracy that
reaches 74.24% on one of their test corpora.
Since our results are for whole articles, not single
entity mentions, they are not directly comparable.
The advantage of our approach resides in its simplic-
ity, since it does not require a specifically designed
training set, being based only on publicly available
standard databases. This reduces not only the cost
compared to building own resources, but also en-
sures that their quality is monitored.
In this paper we have not discussed how our re-
sults can be used in the disambiguation of entities.
As long as only one organism is selected as the fo-
cus of a given research publication, this is a rather
trivial task. However, as mentioned already in sec-
tion 2.4, it is often the case that multiple organisms
are considered within the same publication. In that
8http://www.biocreative.org/
86
case, organism mentions would need to be ?local-
ized? within the article in order to serve for disam-
biguation purposes, as done in (Wang and Matthews,
2008). Our own approach to this problem is pre-
sented and discussed in (Kaljurand et al, 2009).
One important limitation of our approach is its
reliance on explicit mentions of organisms by their
names as stored in the termbase (or minor variants
thereof). Using all the names available to us (in-
cluding cell lines) and their variants we could so far
achieve only a maximal value of 88% recall, which
means that 12% of the organisms are not referred to
by any name in our resources. This may be due to
either missing names in the termbase (the organisms
are mentioned, but by different names) or because
they are identified by human readers through other
contextual hints which may consist of any sort of in-
formation,9 and may presuppose massive amounts
of background knowledge. The first problem might
be adressed by adding other sources of names to our
termbase. The second problem might be adressed
by using a machine learning approach, which how-
ever brings with it a whole set of new problems, such
as selection and representation of the features rele-
vant for training, as well as the fact that a sufficiently
large training corpus needs to be available.
Another limitation of our approach is the fact that
its development and testing rests on its application
to the identification of either organisms of protein
interactors or host organisms. The original formu-
lation of the goal that motivated this work was ?to
identify automatically the organisms forming part of
the subject matter of scientific papers?. This leaves
open the question of the application of the results,
and is deliberately vague in the wording ?part of the
subject matter?, which includes but is not confined
to the cases mentioned above. This formulation was
motivated by a desire to keep the task as generic as
possible, so that the resulting application could not
only be used as a module for the protein disambigua-
tion task, but also for other tasks of NE disambigua-
tion with respect to organisms, as well as for organ-
ism identification as an independent task. Addition-
ally, the ranked list of candidate organisms delivered
by our program could also be presented to human
9A trivial example would be a publication in a journal which
specializes in research on a single organism.
users, who might want to use them in novel ways,
for example in an assisted curation environment.
However, the gold standard by which we test our
results is tailored to its application as a protein dis-
ambiguation module, just as the frequency tables we
use. Even apart from this, the appropriateness of the
gold standard is partly questionable, as it does not
only prefer organisms involved in protein interac-
tions to those that are not, but also ?new? knowledge
to ?old? knowledge, etc. Our approach, based on
?correcting? simple counts of organism mentions us-
ing frequency tables, can only be successful as long
as there is a gold standard for the specific applica-
tion that is being pursued. We can derive from Int-
Act useful gold standards for organisms from which
protein interactors are derived or host organisms, but
we have no gold standard for ?organism identifica-
tion? as an independent task.
5 Conclusion
In this paper we discussed an approach to the prob-
lem of ?organism identification? as an independent
task, based only on standard resources. While
the initial results were interesting, the experimental
setup led us to identify more specific aspects of the
problem, and in particular to distinguish organisms
mentioned in their roles as sources of the interact-
ing proteins and as hosts of the experiments. We
have shown that a clear identification of the different
functional roles played by organism mentions can
lead to more accurate results.
Although a fully automated disambiguation pro-
cess based on organism mentions is not within im-
mediate reach, the results described in this paper
appear already potentially useful for protein name
disambiguation in a curation environment. An-
other possible application would be in biomedi-
cal curation-based databases, for the semi-automatic
tagging of publications with their focus organisms.
Acknowledgements
This research is partially funded by the Swiss National
Science Foundation (grant 100014-118396/1). Addi-
tional support is provided by Novartis Pharma AG, NI-
TAS, Text Mining Services, CH-4002, Basel, Switzer-
land. We thank the anonymous reviewers for their in-
sightful comments.
87
References
[Alex et al2008] Beatrice Alex, Claire Grover, Barry
Haddow, Mijail Kabadjov, Ewan Klein, Michael
Matthews, Richard Tobin, and Xinglong Wang. 2008.
Automating curation using a natural language process-
ing pipeline. Genome Biology, 9(Suppl 2):S10.
[Kaljurand et al2009] Kaarel Kaljurand, Fabio Rinaldi,
Thomas Kappeler, and Gerold Schneider. 2009. Us-
ing existing biomedical resources to detect and ground
terms in biomedical literature. In 12th Conference on
Artificial Intelligence in Medicine (AIME?09), Verona,
Italy, 18?22 July.
[Kappeler et al2008] Thomas Kappeler, Simon
Clematide, Kaarel Kaljurand, Gerold Schneider,
and Fabio Rinaldi. 2008. Towards automatic de-
tection of experimental methods from biomedical
literature. In Third International Symposium on
Semantic Mining in Biomedicine (SMBM).
[Kerrien et al2006] S. Kerrien, Y. Alam-Faruque,
B. Aranda, I. Bancarz, A. Bridge, C. Derow, E. Dim-
mer, M. Feuermann, A. Friedrichsen, R. Huntley,
C. Kohler, J. Khadake, C. Leroy, A. Liban, C. Lieftink,
L. Montecchi-Palazzi, S. Orchard, J. Risse, K. Robbe,
B. Roechert, D. Thorneycroft, Y. Zhang, R. Apweiler,
and H. Hermjakob. 2006. IntAct ? Open Source
Resource for Molecular Interaction Data. Nucleic
Acids Research.
[Krallinger et al2008] Martin Krallinger, Florian Leit-
ner, Carlos Rodriguez-Penagos, and Alfonso Valencia.
2008. Overview of the protein-protein interaction an-
notation extraction task of BioCreative II. Genome Bi-
ology, 9(Suppl 2):S4.
[Leitner et al2008] Florian Leitner, Martin Krallinger,
Carlos Rodriguez-Penagos, Jo?rg Hakenberg, Con-
rad Plake, Cheng-Ju Kuo, Chun-Nan Hsu, Richard
Tzong-Han Tsai, Hsi-Chuan Hung, William W. Lau,
Calvin A. Johnson, Rune Saetre, Kazuhiro Yoshida,
Yan Hua Chen, Sun Kim, Soo-Yong Shin, Byoung-Tak
Zhang, William A. Baumgartner, Lawrence Hunter,
Barry Haddow, Michael Matthews, Xinglong Wang,
Patrick Ruch, Fre?de?ric Ehrler, Arzucan O?zgu?r, Gu?nes
Erkan, Dragomir R. Radev, Michael Krauthammer,
ThaiBinh Luong, Robert Hoffmann, Chris Sander, and
Alfonso Valencia. 2008. Introducing meta-services
for biomedical information extraction. Genome Biol-
ogy, 9(Suppl 2):S6.
[Rinaldi et al2008] Fabio Rinaldi, Thomas Kappeler,
Kaarel Kaljurand, Gerold Schneider, Manfred Klen-
ner, Simon Clematide, Michael Hess, Jean-Marc von
Allmen, Pierre Parisot, Martin Romacker, and Therese
Vachon. 2008. OntoGene in BioCreative II. Genome
Biology, 9(Suppl 2):S13.
[Sarntivijai et al2008] Sirarat Sarntivijai, Alexander S.
Ade, Brian D. Athey, and David J. States. 2008. A
bioinformatics analysis of the cell line nomenclature.
Bioinformatics, 24(23):2760?2766.
[UniProt Consortium2007] UniProt Consortium. 2007.
The universal protein resource (UniProt). Nucleic
Acids Research, 35:D193?7.
[Wang and Matthews2008] Xinglong Wang and Michael
Matthews. 2008. Distinguishing the species of
biomedical named entities for term identification.
BMC Bioinformatics, 9(Suppl 11):S6.
[Zanzoni et al2002] A. Zanzoni, L. Montecchi-Palazzi,
M. Quondam, G. Ausiello, M. Helmer-Citterich, and
G. Cesareni. 2002. MINT: a Molecular INTeraction
database. FEBS Letters, 513(1):135?140.
88
Proceedings of the Workshop on BioNLP: Shared Task, pages 28?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
UZurich in the BioNLP 2009 Shared Task
Kaarel Kaljurand
Institute of
Computational Linguistics
University of Zurich
Switzerland
kalju@cl.uzh.ch
Gerold Schneider
Institute of
Computational Linguistics
University of Zurich
Switzerland
gschneid@cl.uzh.ch
Fabio Rinaldi?
Institute of
Computational Linguistics
University of Zurich
Switzerland
rinaldi@cl.uzh.ch
Abstract
We describe a biological event detection
method implemented for the BioNLP 2009
Shared Task 1. The method relies entirely on
the chunk and syntactic dependency relations
provided by a general NLP pipeline which was
not adapted in any way for the purposes of
the shared task. The method maps the syn-
tactic relations to event structures while be-
ing guided by the probabilities of the syntactic
features of events which were automatically
learned from the training data. Our method
achieved a recall of 26% and a precision of
44% in the official test run, under ?strict equal-
ity? of events.
1 Introduction
This paper describes the adaptation of an existing
text mining system to the BioNLP shared task. The
system has been originally created for participation
in the BioCreative1 protein-protein interaction task
(Rinaldi et al, 2008) and further developed for an
internal project based on the IntAct dataset of pro-
tein interactions (Kerrien et al, 2006). We decided
to participate only in Task 1 of the BioNLP shared
task, mainly because of lack of time and resources.
Our event annotation method relied on various
preprocessing steps and an existing state of the art
dependency parser, which provided the input to the
event annotator. As all the linguistic processing was
performed by the preprocessor and the parser, the
ideas implemented for the event annotator could re-
main simple while still producing reasonable results.
?Corresponding author
1http://www.biocreative.org/
Thus, the event annotator performed a straightfor-
ward rewriting of syntactic structures to event struc-
tures, guided by the information on the syntactic
nature of events that we obtained from the train-
ing data. In this sense our system can be used as
a reference for a comparison to other systems that
rely completely on a dependency parser delivered
analysis that is rewritten into event structures using
knowledge gained from the training data.
Our system consists of a preprocessing phase that
uses a pipeline of NLP tools, described in section 2
of this paper. Linguistic resources are learned auto-
matically from the preprocessed training data (sec-
tion 3). A Prolog-implemented event generator is
applied directly to the preprocessing results and is
guided by the relative frequencies of syntactic fea-
tures provided in the resources (section 4). This
is followed by a postprocessing step that removes
some unlikely event structures, makes sure that all
events that violate the well-formedness rules are fil-
tered out, and finally serializes the event structures
into the requested output format. In section 5 we
present an illustrative example of the events gener-
ated by this approach and discuss some implications
of the event model adopted in the shared task. In
section 6, we describe the evaluation that we per-
formed during the training period, the final official
results on the test data, and some alternative evalu-
ations performed in parallel to the official one. In
section 7 we draw conclusions and describe future
work.
2 Preprocessing
Aside from a format conversion step necessary to
deal with the data provided by the shared task, the
28
preprocessing phase is largely based on an existing
pipeline of NLP tools, that we have developed in the
OntoGene project2 (Rinaldi et al, 2006; Rinaldi et
al., 2008).
2.1 Tokenization, sentence splitting,
part-of-speech tagging
For tokenization, sentence splitting, and part-of-
speech (POS) tagging we used LingPipe3. Ling-
Pipe produces very granular tokens by default, e.g.
a character sequence from abstract 10395645
caspase-3-like (CPP32/Yama/apopain)
which contains multiple hyphens and slashes (as
usual for biomedical texts) is split into 12 (rather
than just 4) tokens
caspase, -, 3, -, like, (, CPP32, /, Yama, /,
apopain, )
allowing a more detailed detection of terms
(shown in boldface in the examples) and trigger-
words which would stay token-internal if a less gran-
ular tokenization was used.
The models used for sentence splitting and POS-
tagging come with the LingPipe distribution and are
trained on the GENIA corpus (Kim et al, 2003),
thus providing a biomedical text aware sentence
splitting and POS-tagging.
2.2 Term annotation
Correctly detecting multi-word terms in the text can
substantially improve the parsing results, because
long noun sequences would be grouped together and
the parser can only focus on the heads of the groups
and ignore the rest. In this task, however, we de-
cided to keep things simple and rely on chunking as
the only means of noun grouping.
Thus, we only annotated the terms provided by
the task organizers in the a1-files (i.e. protein men-
tions). We made the assumption that terms are se-
quences of tokens as defined by the LingPipe tok-
enizer. Whereas in the vast majority of cases this co-
incides with the tokenization used by the organizers,
there are 10 cases in the training data where this as-
sumption is violated (e.g. ?IkappaB-alphaS32/36A?
2http://www.ontogene.org/
3http://alias-i.com/lingpipe/
contains the term ?IkappaB-alpha? but according to
LingPipe, the tokens are ?IkappaB?, ?-?, ?alphaS32?,
?/?, ?36A?).
As the last step of term annotation, we recon-
nected tokens which were separated by hyphens and
slashes, unless the tokens were part of terms. This
allowed for a more reliable processing with tools
which are not optimized to deal with symbols like
hyphens and slashes if these are padded with white-
space.
2.3 Lemmatization using Morpha
Lemmatization was performed using Morpha (Min-
nen et al, 2001), which provides an accurate lemma-
tization given that the input contains part-of-speech
information. We used the lemma information even-
tually only as part of the input to the dependency
parser, i.e. for the other aspects of event annotation
lemmas were ignored.
2.4 Chunking using LTCHUNK
Chunking can considerably reduce parsing com-
plexity, while hardly affecting performance (Prins,
2005). In order to group contiguous sequences of
nouns and verbs, we used LTCHUNK (Mikheev,
1997). LTCHUNK annotates all noun and verb
groups in the sentences. A chunk is an important
unit in the analysis of biomedical texts. Consider an
NP chunk like
T cell-receptor-induced FasL upregula-
tion
which contains two event triggers, amounting to a
mention of a complex event.
After applying LTCHUNK, we also detected
chunk heads, with a simple algorithm ? select last
noun in noun groups, select last verb in verb groups.
This selection is done on the basis of POS-tags.
2.5 Dependency parsing using Pro3Gres
Pro3Gres (Schneider, 2008) is a robust, deep-
syntactic, broad-coverage probabilistic dependency
parser, which identifies grammatical relations be-
tween the heads of chunks, including the majority
of long-distance dependencies. The output is a hi-
erarchical structure of relations (represented as the
directed arrows in the example shown in figure 1).
29
Figure 1: Dependency-syntax tree of the title of abstract 9360945: ?Transcription factor NF-kappaB regulates in-
ducible Oct-2 gene expression in precursor B lymphocytes.? The dependency relations link together the heads of the
5 chunks.
The parser uses a hand-written grammar express-
ing linguistic competence, and a statistical language
model that calculates lexicalized attachment proba-
bilities, thus expressing linguistic performance. The
parser expresses distinctions that are especially im-
portant for a predicate-argument based deep syntac-
tic representation, as far as they are expressed in
the training data generated from the Penn Treebank
(Marcus et al, 1993). This includes prepositional
phrase attachments, control structures, appositions,
relative clause anaphora, participles, gerunds, and
argument/adjunct distinctions. The dependency la-
bel set is similar to the one used in the Stanford
scheme, the parser achieves state-of-the-art perfor-
mance (Haverinen et al, 2008).
We have slightly adapted Pro3Gres to the biomed-
ical domain. A class of nouns that varies consider-
ably in the biomedical domain are relational nouns.
They are syntactically marked because they can have
several prepositional phrase arguments. Biomedical
relational nouns like ?overexpression? or ?transcrip-
tion? are absent from the Penn Treebank or rare. We
have used an unsupervised approach based on (Hin-
dle, D and Rooth, M, 1991) to learn relational nouns
from Medline.
A new relation type, hyph, has been added to con-
nect tokens to hyphens and slashes, and thus better
deal with these characters in biomedical texts.
2.6 Preprocessor output
The preprocessor produces 5 Prolog-formatted files
for each abstract. Each of these files is token-
centered and affiliates a token ID with a group (ei-
ther sentence, chunk, or term) that contains this to-
ken, or maps it to a syntactically related (either as
the head or the dependent) token.
? Tokens maps each token to its lemma, POS-
tag, and character offsets
? Chunks maps each token to its containing
chunk, chunk?s type (noun or verb group), and
chunk?s head
? Terms maps each token to its containing term,
term?s type, term?s ID (assigned by the a1-file,
or the a2-file in case of processing the training
data)
? Sentences maps each sentence ID to the list of
IDs of the tokens in the sentence
? Dependencies maps each token to its imme-
diate head and dependent, and to the types of
these dependency relations
These files are the input to the resource generator
described below, and later (together with the gener-
ated resources), the input to the event annotator.
3 Resources
The 800 abstracts of the training data were used
during development for the generation of three re-
sources which are described in this section. For the
official testing we used the concatenation of training
and development data (i.e. 950 abstracts). The re-
sources were generated automatically from the a1-
and a2-files; and from the preprocessed version of
txt-, a1- and a2-files. The resulting data files include
frequencies of the total occurrence of an item (e.g.
word, syntactic configuration) and the frequency of
its occurrence in an event.
All the words in the resources were lowercased
but not lemmatized. Resources were stored as
Prolog-formatted files.
30
Frequency Event type Event arguments
149 Gene expression Theme(T)
28 Transcription Theme(T)
2 Localization Theme(T), AtLoc(T)
1 Positive regulation Theme(T)
1 Positive regulation Theme(E)
Table 1: Frequency distribution of the event structures
that are triggered by the word form ?expressed? which in
total triggered an event 181 times in the training data. ?T?
means that the argument is filled by a term, ?E? means
that the argument is filled by an event.
3.1 Words
The word frequencies file provides a simple prob-
abilistic model for excluding stopwords, as we ob-
served that many different function words some-
times triggered events in the training data. We
wanted to exclude such words to obtain a better pre-
cision. The words-resource can be queried using a
simple interface
word_to_freq(+Word, -F)
which maps every word to its frequency.
3.2 Event types and arguments
Using the training data, we created a mapping from
each candidate trigger-word to the possible event
types and the permissible event frames. A sample of
this mapping is illustrated in table 1. The arguments
have a type (e.g. Theme) but their filler is abstracted
to be either ?T? (for terms) or ?E? (for events).
This resource can be queried via the interface
eword_to_event(+EventWord,
-EventType, -EventArgs, -F1, -F2)
which maps every trigger-word to its possible
event type and arguments. The returned frequencies
show how often the event structure was triggered
by the trigger-word, and how often the trigger-word
triggered an event in total.
3.3 Domination paths between terms
The most sophisticated of the resources that we
generated recorded the syntactic paths between the
terms (from a1- and a2-files) observed in the train-
ing data, and counted how often these paths were
present in events, connecting triggers with event ar-
gument fillers. With each term, also its type (e.g.
Positive regulation, Protein) was recorded.
For the syntactic paths, we only considered dom-
ination paths where one of the terms is the head and
the other the dependent, defined as follows.
Definition 1 (Domination between chunks)
Term t1 dominates term t2 if t1 ? c1 and
t2 ? c2 and there exists a directed syntactic path
h(c1) ? . . . ? h(c2), where h(?) is the head of the
given chunk.
For example, in figure 1, the term ?regulates?
dominates all the other tokens, among them the term
?expression? (which is the head of its chunk), and the
Protein-term ?Oct-2?. Note that this definition does
not require the terms to be in the chunk head posi-
tion. However, this decision did not affect the results
significantly.
The chunk-internal domination relation is defined
for terms which are chunk-internal and thus ?invisi-
ble? to the dependency parser because the parser ig-
nores everything but the head of the chunk. This re-
lation captures the default syntactic dependency be-
tween nouns in noun groups where the head noun
usually follows its dependents.
Definition 2 (Chunk-internal domination) Term
t1 dominates term t2 if t1, t2 ? c and i(t1) > i(t2),
where i(?) is the sequential index of the given term
in the chunk.
For example, in figure 1, in the 3rd chunk, the
term ?expression? dominates the terms ?Oct-2? and
?inducible?; and furthermore, ?Oct-2? dominates ?in-
ducible?.
The stored syntactic path is a list of dependency
relations from the dependent to the head, or an
empty list if both terms are in the same chunk.
Instead of domination, we also considered using
the asymmetric relation of ?connectedness?, where
two terms are connected if either of the terms dom-
inates the other, or if both are dominated by some
token in the tree. This relation, however, seemed to
decrease precision much more than increase recall.
In order to query the domination resource we
designed a simple query interface that allows for
partially instantiated input. For example the query
(where the underscores denote uninstantiated parts)
?- find_path_freq(bind, ?Binding?,
_, ?Protein?,
[modpp | _ ],
F1, F2).
31
asks how often there is a domination relation be-
tween the head term ?bind? if it has the type Bind-
ing and some dependent term with type Protein,
such that the dependency path starts with the rela-
tion modpp. The frequency counts resulting from
this query tell the frequency of this configuration in
events (F1), and in total (F2). This information al-
lows the computation of the conditional probability
of an argument of an event given the event type, the
trigger-word, the argument word, the argument type,
and the syntactic path between trigger and argument.
4 Event generation
The event generation relied fully on the syntax tree
and chunk information that was delivered by the pre-
processing module. No fall-back to a surface co-
occurrence of words was used. We only considered
words and structures seen in the training data as pos-
sible parts of events. Such a design entails relatively
good precision at lower recall.
For each of the generation steps described below,
a probability threshold decided whether to continue
the ?building? of the event given the trigger-word,
the event arguments template or the argument in-
stantiation. The thresholds were set manually after
some experimentation. We did not try to automat-
ically decide the best performing thresholds. Deci-
sions are taken locally, possibly cutting some local
minima. A simple maximum-likelihood estimation
(MLE) approach was used.
4.1 Trigger generation
Trigger candidates were generated from the token
list of each sentence in the analyzed abstract. Fig-
ure 2 shows a browser-based visualization approach
that we created as a support in our work. In the case
of the training data, the annotations come the a1-
and a2-files provided by the organizers. In the case
of the development and test data, the annotations for
the triggers are those generated by the system.
We only considered one-token trigger-words be-
cause multi-token triggers were less frequent in the
training data, where only about 8% of the trigger-
word forms contained a space character. Also, many
of these multiword triggers contain a token that ex-
ists as a trigger on its own (e.g. ?transcriptional reg-
ulation? triggers the Regulation-event in the training
data, as does ?regulation?), allowing us to generate a
sensible event structure even if it does not match a
gold standard event under the ?strict equality?. To-
kens that had been seen to trigger an event in the
training data with probability higher than 0.12 were
considered further.
In MLE terms, we calculate the probability of a
given token to be a trigger as follows:
p(Trigger |Token) = f(Token ? (Token = Trigger))f(Token)
(1)
4.2 Event type and arguments template
generation
Next, trigger-words were mapped to event type and
argument template structures. In MLE terms, we
calculated the probability of an event structure (i.e.
the combination of event type and arguments tem-
plate) given the trigger-word.
p(EventStruct |Trigger) = f(Trigger ? EventStruct)f(Trigger)
(2)
Again, only high probability structures were con-
sidered further. We used the probability threshold of
0.25 for simple event structures (i.e. not containing
nested events), and 0.1 for complex event structures
(only regulation events in the shared task).
4.3 Event argument filling
The inclusion of a protein as an argument of an event
was based on the syntactic domination of the trigger
of the event over the term of the protein. We at-
tempted to generate simple events of all types seen
in the training data.
For complex events, the trigger-words of the main
and the embedded events had to be in a domination
relationship. We generated regulation-events with
only 1-level embedding. Although more complex
embeddings are possible (see example below), these
are not very frequent.
prevents T cell-receptor-induced FasL
upregulation
In order to flexibly deal with sparse data, we per-
formed a sequence of queries, one less instantiated
32
Figure 2: Example of an annotated sentence from abstract 10080948 in the training data.
than the previous one, weighted the results accord-
ingly and calculated the weighted mean to be the fi-
nal probability for including the argument.
find_path_freq(HWord, HType, DWord, DType, Path,
C1_1, C2_1),
find_path_freq(_, HType, _, DType, Path,
C1_2, C2_2),
find_path_freq(_, HType, _, DType, _,
C1_3, C2_3)
In MLE terms, we calculate the probability that
a syntactic configuration fills an argument slot.
Syntactic configurations consist of the head word
HWord, the head event type HType, the dependent
word DWord, the dependent event type DType, and
the syntactic path Path between them.
p(Arg |HWord, HType, DWord, DType, Path) =
1
w1+w2+w3 ? (
w1 ? f(HWord, HType, DWord, DType, Path?Arg)f(HWord, HType, DWord, DType, Path) +
w2 ? f(HType, DType, Path?Arg)f(HType, DType, Path) +
w3 ? f(HType, DType?Arg)f(HType, DType) ) (3)
The weigths were set as w1 = 3, w2 = 2 and
w3 = 1.2. The fact that the weights decrease ap-
proximates a back-off model. Only if the final prob-
ability was higher than 0.3 the event was further con-
sidered. For complex events, we used formula 3 as
given, but for simple events, where DWord is a pro-
tein, DWord was always left uninstantiated.
4.4 Postprocessing
During the postprocessing step some unlikely event
structures were filtered out. This filtering is delayed
until all the events have been generated, because ex-
cluding the unwanted events is difficult during cre-
ation time as sometimes extrospection is required.
Also, the postprocessing step acts as a safety net
that filters out well-formedness errors (e.g. argu-
ment sharing violations), thus making sure that the
submission to the evaluation system is not rejected
by the system. Finally, the set of generated events is
serialized into the BioNLP a2-format.
5 Example and discussion
As an example of application of our approach, con-
sider again the syntactic tree shown in figure 1.
Our approach results in the generation of the events
shown in figure 3, given that ?regulates?, ?inducible?,
and ?expression? are trigger-words, and ?Oct-2? is an
a1-annotated protein.
Figure 3: Visualization of two simple event structures
regulates(Oct-2) and expression(Oct-2), and a complex
structure regulates(expression(Oct-2)).
We call events like regulates(Oct-2) ?shortcut
events?, as there exists an alternative and longer
path ? regulates(expression) and expression(Oct-2)
? that connects the trigger to its event argument.
These ?shortcut events? are filtered out in the post-
processing step as unlikely events.
It is useful to observe that the particular view of
event structures defined by the BioNLP shared task
is by no means unchallenged. Whether nested events
are necessary in a representation of biological rele-
vant relations is a question which is open to debate.
While from the linguistic perspective they do offer a
more adequate representation of the content matter
of the text, from the biological point of view these
structures are redundant in many cases. The exam-
ple used in this section is illustrative.
From the biologist?s perspective, ?A regulates the
expression of B? is a way to express that A regu-
lates B. Obviously such a short-circuit is not in all
cases possible, but the point is that the biologist
33
might be interested only in the direct biological in-
teractions, and be inclined to ignore the linguistic
representation of that interaction. This is the point
of view taken for example in the Protein-Protein
Interaction task of the latest BioCreative competi-
tion (Krallinger et al, 2008). In that case, all lin-
guistic structures used to better characterize the in-
teraction are purposefully ignored, and only the bare
interaction is preserved.
Since BioCreative aimed at simulating the pro-
cess of database curation, and was based on datasets
provided by real-word interaction databases such as
IntAct (Kerrien et al, 2006) and MINT (Zanzoni et
al., 2002), there is reasonable motivation for taking
this alternative view into consideration. At the very
least, a mapping from complex events to simple in-
teractions should always be provided.
The difference in the approach towards interpre-
tation of literature fragments has a direct impact on
the resources used and the success of each approach.
Our own development in the past couple of years has
been driven by the BioCreative model (Rinaldi et al,
2008), and therefore we tended to ignore intermedi-
ate structures in protein interactions. For example,
in (Schneider et al, 2009) we present a lexical re-
source that aims at capturing ?transparent? relations,
i.e. words that express a relation that from the bio-
logical point of view can be ignored because of its
transitivity properties, such as ?expression of Oct-
2? in the example above. This resource, although
certainly useful from the biological point of view,
proved to be useless in the shared task, due to the
different level of granularity in the representation of
events.
6 Official evaluation and additional
experiments
We mainly trained and evaluated using the ?strict
equality? evaluation criteria as our reference. The
results on the development data are shown in table
2. With more relaxed equality definitions, the results
were always a few percentage points better. Our re-
sults in the official testrun are shown in table 3.
Good results for some event structures (notably
Phosphorylation) are due to the simple textual repre-
sentation of these events. For example, Phosphory-
lation is always triggered by a form or derivation of
?phosphorylate?, and these forms rarely trigger any
other types of events. Furthermore, according to the
parsed training data, the probability of a Phospho-
rylation-event, given a syntactic domination relation
between a Phosphorylation-trigger and a protein is
0.92. Also, 56% of these domination paths are ei-
ther chunk-internal or over a single modpp depen-
dency relation, making them easy to detect.
In parallel to the approach used in our official sub-
mission we considered some variants, aimed at max-
imizing either recall or precision, as well as an alter-
native approach based on machine learning.
A high recall baseline method, which generates
all possible event structures in a given sentence,
achieves 81% recall on simple events, with preci-
sion dropping to 11%. One of the reasons why this
method does not reach 100% recall is the fact that
it only annotates event candidates with single-token
triggers that have been seen in the training data.
The filter described in section 4.3 has a major ef-
fect on precision. If it is removed, precision drops
by 11%, while the gain in recall is only 3% ? re-
call 35.10%, precision 37.88%, F-score 36.44%. In-
stead, if we keep w1 but set w2 = w3 = 0 in formula
3, precision increases to 56%, while recall drops to
27%. Increasing the probability thresholds to further
improve precision results in the precision of 60% but
this remains the ceiling in our experiments.
Additionally, we performed separate experiments
with a machine-learning approach which considers
a more varied set of features, including surface in-
formation and syntax coming from an ensemble of
parsers. However, the limited time and resources
available to us during the competition did not al-
low us to go beyond the results achieved using the
approach described in detail in this paper. Since
our best score on the development data was 27%
(about 10% inferior to our consolidated approach),
we opted for not considering this approach in our
official submission.
The fact that this approach was based on a de-
composition of events into their arguments led us to
realize some fundamental limitations in the official
evaluation measures. In particular, none of the orig-
inally implemented measures would give credit to
the partial recognition of an event (i.e. correct trig-
ger word and at least one correct argument, but not
all). We contend that such partial recognition can be
34
Event class Precision Recall F-Score True pos. False pos. False neg.
Simple events 56.71 48.20 52.11 389 297 418
Complex events 38.03 19.25 25.56 189 308 793
All events 48.86 32.31 38.90 578 605 1211
Table 2: Results on the development data of 150 abstracts, measured using ?strict equality?.
Event class gold (match) answer (match) Recall Precision F-Score
Localization 174 (31) 34 (31) 17.82 91.18 29.81
Binding 347 (102) 287 (102) 29.39 35.54 32.18
Gene expression 722 (370) 515 (370) 51.25 71.84 59.82
Transcription 137 (28) 148 (28) 20.44 18.92 19.65
Protein catabolism 14 (8) 16 (8) 57.14 50.00 53.33
Phosphorylation 135 (78) 84 (78) 57.78 92.86 71.23
Simple events total 1529 (617) 1084 (617) 40.35 56.92 47.23
Regulation 291 (29) 120 (29) 9.97 24.17 14.11
Positive regulation 983 (138) 533 (138) 14.04 25.89 18.21
Negative regulation 379 (55) 158 (55) 14.51 34.81 20.48
Complex events total 1653 (222) 811 (222) 13.43 27.37 18.02
All events total 3182 (839) 1895 (839) 26.37 44.27 33.05
Table 3: Results on the test data of 260 abstracts, measured using ?strict equality?, as reported by the BioNLP 2009
online evaluation system.
useful in a practical annotation task, and yet the of-
ficial scores doubly punish such an outcome (once
as a FP and once as a FN). This is a problem already
observed in previous evaluation challenges, however
we believe that a simple solution in this case consists
in decomposing the events (for evaluation purposes)
in their constituent roles and arguments. In other
words, each event is given as much ?weight? as its
number of roles. The correct recognition of an event
with two roles would therefore lead to two TP, but its
partial recognition (one argument) would still lead
to one TP, which we think is a more fair evaluation
in case of partial recognition. Our suggestion was
later implemented by the organizers as an additional
scoring criteria.
7 Conclusions and future work
We have described a biological event detection
method that relies on the chunk and syntactic de-
pendency relations obtained during the preprocess-
ing stage. No fall-back strategy that is based on e.g.
surface patterns was designed for this task. This is
consistent with our approach to biomedical event de-
tection ? relation extraction is entirely based on ex-
isting syntactic information about the sentences, and
can be ported easily if the definition of relations and
events is changed, as in the case of other competi-
tions which use a different notion of relations (e.g.
BioCreative).
As the chunker and the dependency parser form
a core of the described system, their limitations and
improvements have a fundamental effect on the fur-
ther processing. In parallel to a thorough error anal-
ysis which can drive further development of our con-
solidated approach, we intend to further explore the
enhanced flexibility provided by the machine learn-
ing approach briefly mentioned in section 6. In both
cases, we intend to use the BioNLP shared task eval-
uation site as a reference in order to compare them,
not only against each other, but also against the re-
sults of other participants.
Acknowledgements
This research is partially funded by the Swiss Na-
tional Science Foundation (grant 100014-118396/1).
Additional support is provided by Novartis Pharma
AG, NITAS, Text Mining Services, CH-4002, Basel,
Switzerland. The authors would like to thank the
two anonymous reviewers of BioNLP 2009 for their
valuable feedback.
35
References
[Haverinen et al2008] Katri Haverinen, Filip Ginter,
Sampo Pyysalo, and Tapio Salakoski. 2008. Accu-
rate conversion of dependency parses: targeting the
stanford scheme. In Proceedings of Third Interna-
tional Symposium on Semantic Mining in Biomedicine
(SMBM 2008), Turku, Finland.
[Hindle, D and Rooth, M1991] Hindle, D and Rooth, M.
1991. Structural Ambiguity and Lexical Relations.
Meeting of the Association for Computational Linguis-
tics, pages 229?236.
[Kerrien et al2006] S. Kerrien, Y. Alam-Faruque,
B. Aranda, I. Bancarz, A. Bridge, C. Derow, E. Dim-
mer, M. Feuermann, A. Friedrichsen, R. Huntley,
C. Kohler, J. Khadake, C. Leroy, A. Liban, C. Lieftink,
L. Montecchi-Palazzi, S. Orchard, J. Risse, K. Robbe,
B. Roechert, D. Thorneycroft, Y. Zhang, R. Apweiler,
and H. Hermjakob. 2006. IntAct ? Open Source
Resource for Molecular Interaction Data. Nucleic
Acids Research.
[Kim et al2003] J.D. Kim, T. Ohta, Y. Tateisi, and J. Tsu-
jii. 2003. GENIA corpus ? a semantically annotated
corpus for bio-textmining. Bioinformatics, 19(1):180?
182.
[Krallinger et al2008] Martin Krallinger, Florian Leit-
ner, Carlos Rodriguez-Penagos, and Alfonso Valencia.
2008. Overview of the protein-protein interaction an-
notation extraction task of BioCreative II. Genome Bi-
ology, 9(Suppl 2):S4.
[Marcus et al1993] M Marcus, B Santorini, and
M Marcinkiewicz. 1993. Building a Large An-
notated Corpus of English: the Penn Treebank.
Computational Linguistics, 19:313?330.
[Mikheev1997] A Mikheev. 1997. Automatic rule induc-
tion for unknown word guessing. Computational Lin-
guistics, 23(3):405?423.
[Minnen et al2001] G Minnen, J Carroll, and D Pearce.
2001. Applied morphological processing of English.
Natural Language Engineering, 7(3):207?223.
[Prins2005] Robbert Prins. 2005. Finite-State Pre-
Processing for Natural Language Analysis. Ph.D. the-
sis, Behavioral and Cognitive Neurosciences (BCN)
research school, University of Groningen.
[Rinaldi et al2006] Fabio Rinaldi, Gerold Schneider,
Kaarel Kaljurand, Michael Hess, and Martin Ro-
macker. 2006. An Environment for Relation Mining
over Richly Annotated Corpora: the case of GENIA.
BMC Bioinformatics, 7(Suppl 3):S3.
[Rinaldi et al2008] Fabio Rinaldi, Thomas Kappeler,
Kaarel Kaljurand, Gerold Schneider, Manfred Klen-
ner, Simon Clematide, Michael Hess, Jean-Marc von
Allmen, Pierre Parisot, Martin Romacker, and Therese
Vachon. 2008. OntoGene in BioCreative II. Genome
Biology, 9(Suppl 2):S13.
[Schneider et al2009] Gerold Schneider, Kaarel Kalju-
rand, Thomas Kappeler, and Fabio Rinaldi. 2009.
Detecting protein-protein interactions in biomedical
texts using a parser and linguistic resources. In CI-
CLing 2009, 10th International Conference on Intel-
ligent Text Processing and Computational Linguistics,
Mexico City, Mexico.
[Schneider2008] Gerold Schneider. 2008. Hybrid Long-
Distance Functional Dependency Parsing. Ph.D. the-
sis, Faculty of Arts, University of Zurich.
[Zanzoni et al2002] A. Zanzoni, L. Montecchi-Palazzi,
M. Quondam, G. Ausiello, M. Helmer-Citterich, and
G. Cesareni. 2002. MINT: a Molecular INTeraction
database. FEBS Letters, 513(1):135?140.
36
Proceedings of BioNLP Shared Task 2011 Workshop, pages 151?152,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
An Incremental Model for the Coreference Resolution Task of BioNLP 2011
Don Tuggener, Manfred Klenner, Gerold Schneider, Simon Clematide, Fabio Rinaldi
Institute of Computational Linguistics, University of Zurich, Switzerland
{tuggener,klenner,gschneid,siclemat,rinaldi}@cl.uzh.ch
Abstract
We introduce our incremental coreference res-
olution system for the BioNLP 2011 Shared
Task on Protein/Gene interaction. The benefits
of an incremental architecture over a mention-
pair model are: a reduction of the number
of candidate pairs, a means to overcome the
problem of underspecified items in pair-wise
classification and the natural integration of
global constraints such as transitivity. A fil-
tering system takes into account specific fea-
tures of different anaphora types. We do not
apply Machine Learning, instead the system
classifies with an empirically derived salience
measure based on the dependency labels of the
true mentions. The OntoGene pipeline is used
for preprocessing.
1 Introduction
The Coreference Resolution task of BioNLP fo-
cused on finding anaphoric references to proteins
and genes. Only antecedent-anaphora pairs are con-
sidered in evaluation and not full coreference sets.
Although it might not seem to be necessary to gen-
erate full coreference sets, anaphora resolution still
benefits from their establishment. Our incremental
approach (Klenner et al, 2010) naturally enforces
transitivity constraints and thereby reduces the num-
ber of potential antecedent candidates. The system
achieved good results in the BioNLP 2011 shared
task (Fig. 1)
Team R P F1
A 22.18 73.26 34.05
Our model 21.48 55.45 30.96
B 19.37 63.22 29.65
C 14.44 67.21 23.77
D 3.17 3.47 3.31
E 0.70 0.25 0.37
Figure 1: Protein/Gene Coreference Task
2 Preprocessing: The OntoGene Pipeline
OntoGene?s text mining system is based on an
internally-developed fast, broad-coverage, deep-
syntactic parsing system (Schneider, 2008). The
parser is wrapped into a pipeline which uses a num-
ber of other NLP tools. The parser is a key compo-
nent in a pipeline of NLP tools (Rinaldi et al, 2010),
used to process input documents. First, in a pre-
processing stage, the input text is transformed into
a custom XML format, and sentences and tokens
boundaries are identified. The OntoGene pipeline
also includes a step of term annotation and disam-
biguation, which are not used for the BioNLP shared
task, since relevant terms are already provided in
both the training and test corpora. The pipeline also
includes part-of-speech taggers, a lemmatizer and a
syntactic chunker.
When the pipeline finishes, each input sentence
has been annotated with additional information,
which can be briefly summarized as follows: sen-
tences are tokenized and their borders are detected;
each sentence and each token has been assigned an
ID; each token is lemmatized; tokens which be-
long to terms are grouped; each term is assigned a
normal-form and a semantic type; tokens and terms
are then grouped into chunks; each chunk has a
type (NP or VP) and a head token; each sentence
is described as a syntactic dependency structure. All
this information is represented as a set of predicates
and stored into the Knowledge Base of the system,
which can then be used by different applications,
such as the OntoGene Relation Miner (Rinaldi et al,
2006) and the OntoGene Protein-Protein Interaction
discovery tool (Rinaldi et al, 2008).
3 Our Incremental Model for Coreference
Resolution
1 for i=1 to length(I)
2 for j=1 to length(C)
3 rj := virtual prototype of coreference set Cj
4 Cand := Cand ? rj if compatible(rj ,mi)
5 for k= length(B) to 1
6 bk:= the k-th licensed buffer element
7 Cand := Cand ? bk if compatible(bk,mi)
8 if Cand = {} then B := B ?mi
9 if Cand 6= {} then
10 antei := most salient element of Cand
11 C := augment(C,antei,mi)
Figure 2: Incremental model: base algorithm
151
Fig. 2 shows the base algorithm. Let I be the
chronologically ordered list of NPs, C be the set
of coreference sets and B a buffer, where NPs are
stored, if they are not anaphoric (but might be valid
antecedents). Furthermore mi is the current NP and
? means concatenation of a list and a single item.
The algorithm proceeds as follows: a set of an-
tecedent candidates is determined for each NP mi
(steps 1 to 7) from the coreference sets (rj) and the
buffer (bk). A valid candidate rj or bk must be com-
patible with mi. The definition of compatibility de-
pends on the POS tags of the anaphor-antecedent
pair. The most salient available candidate is selected
as antecedent for mi.
3.1 Restricted Accessibility of Antecedent
Candidates
In order to reduce underspecification, mi is com-
pared to a virtual prototype of each coreference set
(similar to e.g. (Luo et al, 2004; Yang et al, 2004;
Rahman and Ng, 2009)). The virtual prototype bears
morphologic and semantic information accumulated
from all elements of the coreference set. Access to
coreference sets is restricted to the virtual prototype.
This reduces the number of considered pairs (from
the cardinality of a set to 1).
3.2 Filtering based on Anaphora Type
Potentionally co-refering NPs are extracted from the
OntoGene pipeline based on POS tags. We then ap-
ply filtering based on anaphora type: Reflexive pro-
nouns must be bound to a NP that is governed by the
same verb. Relative pronouns are bound to the clos-
est NP in the left context. Personal and possessive
pronouns are licensed to bind to morphologically
compatible antecedent candidates within a window
of two sentences. Demonstrative NPs containing the
lemmata ?protein? or ?gene? are licensed to bind to
name containing mentions. Demonstrative NPs not
containing the trigger lemmata can be resolved to
string matching NPs preceding them1.
3.3 Binding Theory as a Filter
We know through binding theory that ?modulator?
and ?it? cannot be coreferent in the sentence ?Over-
expression of protein inhibited stimulus-mediated
transcription, whereas modulator enhanced it?.
Thus, the pair ?modulator?-?it? need not be consid-
ered at all. We have not yet implemented a full-
1As we do not perform anaphoricity determination of nom-
inal NPs, we do not consider bridging anaphora (anaphoric
nouns that are connected to their antecedents through seman-
tic relations and cannot be identified by string matching).
blown binding theory. Instead, we check if the an-
tecedent and the anaphor are governed by the same
verb.
4 An Empirically-based Salience Measure
Our salience measure is a partial adaption of the
measure from (Lappin and Leass, 1994). The
salience of a NP is solely defined by the salience
of the dependency label it bears. The salience of a
dependency label, D, is estimated by the number of
true mentions (i.e. co-refering NPs) that bear D (i.e.
are connected to their heads with D), divided by the
total number of true mentions (bearing any D). The
salience of the label subject is thus calculated by:
Number of truementions bearing subject
Total number of truementions
We get a hierarchical ordering of the dependency la-
bels (subject > object > pobject > ...) according to
which antecedents are ranked and selected.
References
Manfred Klenner, Don Tuggener, and Angela Fahrni. 2010. Inkre-
mentelle koreferenzanalyse fu?r das deutsche. In Proceedings der
10. Konferenz zur Verarbeitung Natu?rlicher Sprache.
Shalom Lappin and Herbert J Leass. 1994. An algorithm for pronomi-
nal anaphora resolution. Computational Linguistics, 20:P. 535?561.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and
Salim Roukos. 2004. A mention-synchronous coreference resolu-
tion algorithm based on the bell tree. In Proceedings of the 42nd
Annual Meeting on Association for Computational Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised models for corefer-
ence resolution. In Proceedings of the 2009 Conference on Empir-
ical Methods in Natural Language Processing: Volume 2 - Volume
2, EMNLP ?09, pages 968?977, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand, Michael Hess, and
Martin Romacker. 2006. An Environment for Relation Mining over
Richly Annotated Corpora: the case of GENIA. BMC Bioinformat-
ics, 7(Suppl 3):S3.
Fabio Rinaldi, Thomas Kappeler, Kaarel Kaljurand, Gerold Schnei-
der, Manfred Klenner, Simon Clematide, Michael Hess, Jean-Marc
von Allmen, Pierre Parisot, Martin Romacker, and Therese Vachon.
2008. OntoGene in BioCreative II. Genome Biology, 9(Suppl
2):S13.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand, Simon Clematide,
Therese Vachon, and Martin Romacker. 2010. OntoGene in
BioCreative II.5. IEEE/ACM Transactions on Computational Bi-
ology and Bioinformatics, 7(3):472?480.
Gerold Schneider. 2008. Hybrid Long-Distance Functional Depen-
dency Parsing. Doctoral Thesis, Institute of Computational Linguis-
tics, University of Zurich.
Xiaofeng Yang, Jian Su, Guodong Zhou, and Chew Lim Tan. 2004. An
np-cluster based approach to coreference resolution. In Proceedings
of the 20th international conference on Computational Linguistics.
152
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 116?120,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
UZH in the BioNLP 2013 GENIA Shared Task
Gerold Schneider, Simon Clematide, Tilia Ellendorff, Don Tuggener, Fabio Rinaldi,
{rinaldi,gschneid,siclemat,ellendorff,tuggener}@cl.uzh.ch
Institute of Computational Linguistics, University of Zurich, Switzerland
Gintare? Grigonyte?
Stockholm University, Department of Linguistics, Section for Computational Linguistics
gintare@ling.su.se
Abstract
We describe a biological event detec-
tion method implemented for the Genia
Event Extraction task of BioNLP 2013.
The method relies on syntactic depen-
dency relations provided by a general NLP
pipeline, supported by statistics derived
from Maximum Entropy models for can-
didate trigger words, for potential argu-
ments, and for argument frames.
1 Introduction
The OntoGene team at the University of Zurich
has developed text mining applications based on
a combination of deep-linguistic analysis and ma-
chine learning techniques (Rinaldi et al, 2012b;
Clematide and Rinaldi, 2012; Rinaldi et al, 2010).
Our approaches have proven competitive in sev-
eral shared task evaluations (Rinaldi et al, 2013;
Clematide et al, 2011; Rinaldi et al, 2008). Addi-
tionally, we have developed advanced systems for
the curation of the biomedical literature (Rinaldi
et al, 2012a).
Our participation in the Genia Event Extraction
task of BioNLP 2013 (Kim et al, 2013) was moti-
vated by the desire of testing our technologies on
a more linguistically motivated task. In the course
of our participation we revised several modules of
our document processing pipeline, however we did
not have sufficient resources to completely revise
the final module which generates the event struc-
tures, and we still relied on a module which we
had developed for our previous participation to the
BioNLP shared task.
The final submission was composed by our
standard preprocessing module (described briefly
in section 2) and novel probability models (section
3), combined within the old event generator (sec-
tion 4).
2 Preprocessing
The OntoGene environment is based on a pipeline
of several NLP tools which all operate on a com-
mon XML representation of the original docu-
ment.
Briefly, the pipeline includes modules for
sentence-splitting, tokenization, part-of-speech
tagging, lemmatization, stemming, term-
recognition (not used for the BioNLP shared
task), chunking, dependency-parsing and event
generation. Different variants of those modules
have been used in different instantiations of the
pipeline. For the BioNLP 2013 participation,
lingpipe was used for sentence splitting, tok-
enization and PoS tagging, morpha (Minnen et
al., 2001) was used for lemmatization, a python
implementation of the Porter stemmer for stem-
ming, LTTT (Grover et al, 2000), was used for
chunking, and the Pro3Gres parser (Schneider,
2008) for dependency analysis.
As we have made good experiences with a
rule based system for anaphora resolution in the
BioNLP 2011 shared task (Tuggener et al, 2011),
we implemented a similar approach that resolves
anaphors to terms identified during preprocessing.
Rules contain patterns like ?X such as Y? or ?X
is a Y?, and pronouns are resolved to the nearest
grammatical subject or object. Anaphora resolu-
tion led to an improvement of 0.2% recall on the
development set, while precision was hardly af-
fected.
3 Probability models
Several probability models have been computed
from the training data in order to be used to score
and filter candidate events generated by the sys-
tem. The following models played a role in the
final submission:
P (eventType | trigger candidate) (1)
116
P (frame ? eventType | trigger candidate) (2)
P (role ? eventType | protein) (3)
P (role(t, d) | synpath(t, d)) (4)
For all of them we computed global Maximum
Likelihood Estimations (MLE), using the training
and development datasets from the 2013 and 2011
challenges. For all of the models above, except
for the last one, we also estimated the probabili-
ties by a Maximum Entropy (ME) approach. The
MegaM tool (Daume? III, 2004) allows for a super-
vised training of binary classifiers where the class
probability is optimized by adjusting the feature
weights and not just the binary classification deci-
sion itself. This helps to deal with the imbalanced
classes such as the distribution of true or false trig-
gerword candidates.
For the classification of trigger candidates
(Equation 1), a binary ME classifier for each event
type is separately trained, based on local and
global features as described below. The trigger-
word candidates are collected from the training
data using their stemmed representation as a selec-
tion criterion. We generally exclude triggerword
candidates that occur in less than 1% as true trig-
gers in the training set. Within the data, we found
that triggers that consist of more than one word are
rather rare (less than 5% of all triggers, most of
them occurring once). However, we transformed
these multiword triggers to singleword triggers,
replacing them by their first content word.
The choice of ME features, partly inspired by
(Ekbal et al, 2013), can be grouped into features
derived from the triggerword itself (word), fea-
tures from the sentence of the triggerword (con-
text), and features from article-wide information
(global).
Word features: (1) The text, lemma, part of
speech (PoS), stem and local syntactic dependency
of the triggerword candidate as computed by the
Pro3Gres parser. (2) Information whether a trig-
gerword candidate is head of a chunk as well as
whether the chunk is nominal or verbal
Context features: Unigrams and bigrams in a
window of variable size to left and right of the trig-
gerword candidate; three types of uni- and bigrams
are used: PoS, lemmas and stems; for unigrams we
also include the lower-cased words; for bigrams,
the triggerword candidate itself is included in the
first bigram to either side.
Global features: (1) Presence or absence of a
protein in a window of a given size around the
triggerword candidate (Boolean feature); only the
most frequent proteins of an article are considered.
(2) The zone in an article where the triggerword
candidate appears, e.g. Title/Abstract, Introduc-
tion, Background, Material and Methods, Results
and Discussion, Caption and Conclusion.
Feature engineering was done by testing differ-
ent combinations of settings (window size, thresh-
olds) with the aim of finding an optimal overall
ME model which reaches the lowest error rates for
all event types. The error rate of the candidate set
was measured as the cumulative error mass com-
puted from the assigned class probability as fol-
lows: if the trigger candidate is a true positive, the
error is 1 minus the probability assigned by the
classifier. If the candidate is a false positive, the
error is the probability assigned by the classifier.
Our approach does not allow us to compute an er-
ror rate for false negatives, because we simply rely
on the set of trigger words seen in the training data
as possible candidates.
In these experiments, we discovered that for
most event types an optimal setting for the context
features considers a wide span of about 20 tokens
to the left and right of the triggerword. Includ-
ing bigrams of lemmas, stems and PoS delivered
the best results compared to including only one or
two of these bigram types. Context features can be
parameterized according to how much positional
information they contain: the distance of a word
to the right and left of the trigger, only the direc-
tion (left or right) or no position information at all
(bag of unigrams/bigrams). We found that the ex-
act positional information is only important for the
first word to the left and right (adjacent to the trig-
gerword), whereas for all words that are further
away it is favorable to only use the direction in re-
lation to the trigger. A window size of 10 words
within which proteins are found in the context of
a triggerword gave the best results. The optimal
number of the most frequent proteins considered
within this window was found to be the 10 most
frequent proteins within an article.
The second type of ME classifier (Equation 2)
has the purpose of calculating the probabilities of
event frames for all event types given a trigger
word. We use the term frame for a combination of
arguments that an event is able to accept as theme
and cause and whether these arguments are real-
117
ized as proteins or subevents.
For the classification of proteins (Equation 3),
again separate binary ME classifiers were built in
order to estimate the probability that a protein has
a role (theme or cause) in an event of a given type.
4 Event Generation
We tested two independent event generation mod-
ules, one based on a revision of our previous 2009
submission (Kaljurand et al, 2009) and one which
is a totally new implementation. We could do only
preliminary tests with the second module, which
however showed promising results, in particular
with much better recall than the older module (up
to 65.23%), despite the very little time that we
could invest in its development. The best F-score
that we could reach was still slightly inferior to the
one of the old module at the deadline for submis-
sion of results. In the rest of this paper we will
describe only the module which was used in the
official submission.
The event extraction process consists of three
phases. First, event candidates are generated,
based on trigger words and their context, using the
ME and MLE probabilities pT (equation 1).
Second, individual arguments of an event are
generated. We calculate the MLE probability pR
of an argument role (e.g. Theme) to occur as part
of a given event type, as follows:
pR(Role |EventType) =
f(Role ? EventType)
f(EventType)
(5)
We obtained the best results on the development
corpus when combining the probabilities as:
pA =
pT ? pT ? pR
pT + pT + pR
(6)
We generate arguments, using an MLE syntac-
tic path and an ME argument model, as follows.
The syntactic path between the trigger word and
every term (protein or subordinate event) is con-
sidered. If they are syntactically connected, and
if the probability of a syntactic path to express an
event is above a threshold, it is selected. As this is
a filtering step, it negatively affects recall.
We calculate the MLE probability ppath that
a syntactic configuration fills an argument slot.
Syntactic configurations consist of the head word
(trigger) HWord, the head event type HType, the
dependent word DWord, the dependent event type
DType, and the syntactic path Path between them.
In order to deal with sparse data, we use a
smoothed model.
ppath(Arg |HWord, HType, DWord, DType, Path) =
1
w1+w2+w3
? (
w1 ?
f(HWord, HType, DWord, DType, Path?Arg)
f(HWord, HType, DWord, DType, Path) +
w2 ?
f(HType, DType, Path?Arg)
f(HType, DType, Path) +
w3 ?
f(HType, DType?Arg)
f(HType, DType) ) (7)
The weights were emprically set as w1 = 4,
w2 = 2 and w3 = 1.5. The fact that the weights
decrease approximates a back-off model. The final
probability had to be larger than 0.2.
We have also used an ME model which delivers
the probability parg that a term is the argument of
a specific event, see formula 3. If this ME model
predicts with a probability of above 80% that the
term is not an argument, the search fails. Other-
wise, the probabilities are combined. On the de-
velopment corpus, we achieved best results when
using the harmonic mean:
pargument = 2 ?
ppath ? parg
ppath + parg
(8)
As a last step, the several arguments of an event
are combined into a frame. We have tested mod-
els predicting an entire frame directly, and models
combining the individual arguments generated in
the previous step. The latter approach performed
better. Any permutation of the argument candi-
dates could constitute a frame. Only frames seen
in the training corpus for a given event type are
considered. We have again used an ME and an
MLE model for predicting frames.
The ME model predicts pframe.ME , see for-
mula 2. We have also used two MLE models:
the first one delivers the probability pframe.MLE
based on the event type only, the second one
pframeword.MLE also considers the trigger word
and is much sparser (a low default is thus used for
unseen words). The probability of the individual
arguments also needs to be taken into considera-
tion. We used the mean of the individual argu-
ments? probabilities (pargs?mean).
5 Evaluation
In our analysis of errors, we noticed that frames
with more than one argument are created ex-
tremely rarely. The problem is that frames with
several arguments are rarer because the context
often does not offer the possibility to attach sev-
eral arguments. Therefore, we consistently un-
dergenerated with pargs?mean as outlined above.
118
Event Class gold (match) answer (match) recall prec. fscore
SVT-TOTAL 1117 ( 619) 851 ( 619) 55.42 72.74 62.91
EVT-TOTAL 1490 ( 698) 1103 ( 698) 46.85 63.28 53.84
REG-TOTAL 1694 ( 168) 618 ( 168) 9.92 27.18 14.53
All events total 3184 ( 866) 1721 ( 866) 27.20 50.32 35.31
Table 1: Results on the development set, measured using ?strict equality?.
Event Class gold (match) answer (match) recall prec. fscore
Gene expression 619 (400) 497 (400) 64.62 80.48 71.68
Transcription 101 (26) 100 (26) 25.74 26.00 25.87
Protein catabolism 14 (10) 15 (10) 71.43 66.67 68.97
Localization 99 (34) 39 (34) 34.34 87.18 49.28
=[SIMPLE ALL]= 833 (470) 651 (470) 56.42 72.20 63.34
Binding 333 (74) 264 (74) 22.22 28.03 24.79
Protein modification 1 (0) 0 (0) 0.00 0.00 0.00
Phosphorylation 160 (119) 168 (119) 74.38 70.83 72.56
Ubiquitination 30 (0) 0 (0) 0.00 0.00 0.00
Acetylation 0 (0) 0 (0) 0.00 0.00 0.00
Deacetylation 0 (0) 0 (0) 0.00 0.00 0.00
=[PROT-MOD ALL]= 191 (119) 168 (119) 62.30 70.83 66.30
Regulation 288 (23) 84 (23) 7.99 27.38 12.37
Positive regulation 1130 (129) 444 (129) 11.42 29.05 16.39
Negative regulation 526 (54) 166 (54) 10.27 32.53 15.61
=[REGULATION ALL]= 1944 (206) 694 (206) 10.60 29.68 15.62
==[EVENT TOTAL]== 3301 (869) 1777 (869) 26.33 48.90 34.23
Table 2: Results on the test data, measured using ?strict equality?.
We have added a number of heuristics to boost
multi-argument frames. Multiplying the probabil-
ity of a frame by its cubed length (giving two-
argument slots 9 times higher probability), and
giving Cause-slots 50% higher scores globally led
to best results.
We mainly trained and evaluated using the
?strict equality? evaluation criteria as our refer-
ence. The results on the development data are
shown in table 1. With more relaxed equality def-
initions, the results were always a few percentage
points better. Our results in the official test run are
shown in table 2. In sum, our submitted system
has good performance for simple events, bad per-
formance for Binding events, and a bias towards
precision due to a syntactic-based filtering step.
6 Conclusions and Future work
Our participation in the 2013 BioNLP shared task
was a useful opportunity to revise components of
the OntoGene pipeline and begin the implemen-
tation of a novel event generator. Due to lack of
time, it was not completed in time for the official
submission. We will continue its development and
use the BioNLP datasets.
Acknowledgments
This research is partially funded by the
Swiss National Science Foundation (grant
105315 130558/1).
References
[Clematide and Rinaldi2012] Simon Clematide and
Fabio Rinaldi. 2012. Ranking relations between
diseases, drugs and genes for a curation task.
Journal of Biomedical Semantics, 3(Suppl 3):S5.
[Clematide et al2011] Simon Clematide, Fabio Ri-
naldi, and Gerold Schneider. 2011. Ontogene at
calbc ii and some thoughts on the need of document-
wide harmonization. In Proceedings of the CALBC
II workshop, EBI, Cambridge, UK, 16-18 March.
[Daume? III2004] Hal Daume? III. 2004. Notes on
CG and LM-BFGS optimization of logistic regres-
sion. Paper available at http://pub.hal3.
name#daume04cg-bfgs, implementation avail-
able at http://hal3.name/megam/, August.
[Ekbal et al2013] Asif Ekbal, Sriparna Saha, and
Sachin Girdhar. 2013. Evolutionary approach for
classifier ensemble: An application to bio-molecular
event extraction. In Ajith Abraham and Sabu M
Thampi, editors, Intelligent Informatics, volume 182
of Advances in Intelligent Systems and Computing,
pages 9?15. Springer Berlin Heidelberg.
119
[Grover et al2000] Claire Grover, Colin Matheson, An-
drei Mikheev, and Marc Moens. 2000. Lt ttt - a flex-
ible tokenisation tool. In Proceedings of Second In-
ternational Conference on Language Resources and
Evaluation (LREC 2000).
[Kaljurand et al2009] Kaarel Kaljurand, Gerold
Schneider, and Fabio Rinaldi. 2009. UZurich in the
BioNLP 2009 Shared Task. In Proceedings of the
BioNLP workshop, Boulder, Colorado.
[Kim et al2013] Jin-Dong Kim, Yue Wang, and Ya-
mamoto Yasunori. 2013. The genia event extraction
shared task, 2013 edition - overview. In Proceedings
of BioNLP Shared Task 2013 Workshop, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
[Minnen et al2001] Guido Minnen, John Carroll, and
Darren Pearce. 2001. Applied morphological pro-
cessing of English. Natural Language Engineering,
7(3):207?223.
[Rinaldi et al2008] Fabio Rinaldi, Thomas Kappeler,
Kaarel Kaljurand, Gerold Schneider, Manfred Klen-
ner, Simon Clematide, Michael Hess, Jean-Marc
von Allmen, Pierre Parisot, Martin Romacker, and
Therese Vachon. 2008. OntoGene in BioCreative
II. Genome Biology, 9(Suppl 2):S13.
[Rinaldi et al2010] Fabio Rinaldi, Gerold Schneider,
Kaarel Kaljurand, Simon Clematide, Therese Va-
chon, and Martin Romacker. 2010. OntoGene in
BioCreative II.5. IEEE/ACM Transactions on Com-
putational Biology and Bioinformatics, 7(3):472?
480.
[Rinaldi et al2012a] Fabio Rinaldi, Simon Clematide,
Yael Garten, Michelle Whirl-Carrillo, Li Gong,
Joan M. Hebert, Katrin Sangkuhl, Caroline F. Thorn,
Teri E. Klein, and Russ B. Altman. 2012a. Using
ODIN for a PharmGKB re-validation experiment.
Database: The Journal of Biological Databases and
Curation.
[Rinaldi et al2012b] Fabio Rinaldi, Gerold Schneider,
and Simon Clematide. 2012b. Relation mining ex-
periments in the pharmacogenomics domain. Jour-
nal of Biomedical Informatics, 45(5):851?861.
[Rinaldi et al2013] Fabio Rinaldi, Simon Clematide,
Simon Hafner, Gerold Schneider, Gintare
Grigonyte, Martin Romacker, and Therese Va-
chon. 2013. Using the ontogene pipeline for
the triage task of biocreative 2012. The Journal
of Biological Databases and Curation, Oxford
Journals.
[Schneider2008] Gerold Schneider. 2008. Hy-
brid Long-Distance Functional Dependency Pars-
ing. Doctoral Thesis, Institute of Computational
Linguistics, University of Zurich.
[Tuggener et al2011] D Tuggener, M Klenner,
G Schneider, S Clematide, and F Rinaldi. 2011. An
incremental model for the coreference resolution
task of bionlp 2011. In BioNLP 2011, pages 151?
152. Association for Computational Linguistics
(ACL), June.
120
