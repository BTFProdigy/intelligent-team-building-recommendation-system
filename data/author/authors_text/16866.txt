Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860?865,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
___________________  
*Corresponding author 
Cross-lingual Opinion Analysis via Negative Transfer Detection  
Lin Gui1,2, Ruifeng Xu1*, Qin Lu2, Jun Xu1, Jian Xu2, Bin Liu1, Xiaolong Wang1 
1Key Laboratory of Network Oriented Intelligent Computation, Shenzhen Graduate School, 
Harbin Institute of Technology, Shenzhen 518055 
2Department Of Computing, the Hong Kong Polytechnic University 
guilin.nlp@gmail.com, xuruifeng@hitsz.edu.cn, csluqin@comp.polyu.edu.hk, xujun@hitsz.edu.cn, 
csjxu@comp.polyu.edu.hk,{bliu,wangxl}@insun.hit.edu.cn 
 
Abstract 
Transfer learning has been used in opin-
ion analysis to make use of available lan-
guage resources for other resource scarce 
languages. However, the cumulative 
class noise in transfer learning adversely 
affects performance when more training 
data is used. In this paper, we propose a 
novel method in transductive transfer 
learning to identify noises through the 
detection of negative transfers. Evalua-
tion on NLP&CC 2013 cross-lingual 
opinion analysis dataset shows that our 
approach outperforms the state-of-the-art 
systems. More significantly, our system 
shows a monotonic increase trend in per-
formance improvement when more train-
ing data are used.  
1 Introduction 
Mining opinions from text by identifying their 
positive and negative polarities is an important 
task and supervised learning methods have been 
quite successful. However, supervised methods 
require labeled samples for modeling and the 
lack of sufficient training data is the performance 
bottle-neck in opinion analysis especially for re-
source scarce languages. To solve this problem, 
the transfer leaning method (Arnold et al, 2007) 
have been used to make use of samples from a 
resource rich source language to a resource 
scarce target language, also known as cross lan-
guage opinion analysis (CLOA). 
In transductive transfer learning (TTL) where 
the source language has labeled data and the tar-
get language has only unlabeled data, an algo-
rithm needs to select samples from the unlabeled 
target language as the training data and assign 
them with class labels using some estimated con-
fidence. These labeled samples in the target lan-
guage, referred to as the transferred samples, also 
have a probability of being misclassified. During 
training iterations, the misclassification introduc-
es class noise which accumulates, resulting in a 
so called negative transfer that affects the classi-
fication performance.  
In this paper, we propose a novel method 
aimed at reducing class noise for TTL in CLOA. 
The basic idea is to utilize transferred samples 
with high quality to identify those negative trans-
fers and remove them as class noise to reduce 
noise accumulation in future training iterations. 
Evaluations on NLP&CC 2013 CLOA evalua-
tion data set show that our algorithm achieves the 
best result, outperforming the current state-of-
the-art systems. More significantly, our system 
shows a monotonic increasing trend in perfor-
mance when more training data are used beating 
the performance degradation curse of most trans-
fer learning methods when training data reaches 
certain size. 
The rest of the paper is organized as follows. 
Section 2 introduces related works in transfer 
learning, cross lingual opinion analysis, and class 
noise detection technology. Section 3 presents 
our algorithm. Section 4 gives performance eval-
uation. Section 5 concludes this paper. 
2 Related works 
TTL has been widely used before the formal 
concept and definition of TTL was given in (Ar-
nold, 2007). Wan introduced the co-training 
method into cross-lingual opinion analysis (Wan, 
2009; Zhou et al, 2011), and Aue et al intro-
duced transfer learning into cross domain analy-
sis (Aue, 2005) which solves similar problems. 
In this paper, we will use the terms source lan-
guage and target language to refer to all cross 
lingual/domain analysis. 
Traditionally, transfer learning methods focus 
on how to estimate the confidence score of trans-
ferred samples in the target language or domain 
(Blitzer et al 2006, Huang et al, 2007; Sugiya-
ma et al, 2008, Chen et al 2011, Lu et al, 2011). 
In some tasks, researchers utilize NLP tools such 
as alignment to reduce the bias towards that of 
860
 the source language in transfer learning (Meng et 
al., 2012). However, detecting misclassification 
in transferred samples (referred to as class noise) 
and reducing negative transfers are still an unre-
solved problem. 
There are two basic methods for class noise 
detection in machine learning. The first is the 
classification based method (Brodley and Friedl, 
1999; Zhu et al 2003; Zhu 2004; Sluban et al, 
2010) and the second is the graph based method 
(Zighed et al 2002; Muhlenbach et al 2004; 
Jiang and Zhou, 2004). Class noise detection can 
also be applied to semi-supervised learning be-
cause noise can accumulate in iterations too. Li 
employed Zighed?s cut edge weight statistic 
method in self-training (Li and Zhou, 2005) and 
co-training (Li and Zhou, 2011). Chao used Li?s 
method in tri-training (Chao et al 2008). (Fuku-
moto et al 2013) used the support vectors to de-
tect class noise in semi-supervised learning.  
In TTL, however, training and testing samples 
cannot be assumed to have the same distributions. 
Thus, noise detection methods used in semi-
supervised learning are not directly suited in 
TTL. Y. Cheng has tried to use semi-supervised 
method (Jiang and Zhou, 2004) in transfer learn-
ing (Cheng and Li, 2009). His experiment 
showed that their approach would work when the 
source domain and the target domain share simi-
lar distributions. How to reduce negative trans-
fers is still a problem in transfer learning. 
3 Our Approach 
In order to reduce negative transfers, we pro-
pose to incorporate class noise detection into 
TTL. The basic idea is to first select high quality 
labeled samples after certain iterations as indica-
tor to detect class noise in transferred samples. 
We then remove noisy samples that cause nega-
tive transfers from the current accumulated train-
ing set to retain an improved set of training data 
for the remainder of the training phase. This neg-
ative sample reduction process can be repeated 
several times during transfer learning. Two ques-
tions must be answered in this approach: (1) how 
to measure the quality of transferred samples, 
and (2) how to utilize high quality labeled sam-
ples to detect class noise in training data. 
3.1 Estimating Testing Error 
To determine the quality of the transferred 
samples that are added iteratively in the learning 
process, we cannot use training error to estimate 
true error because the training data and the test-
ing data have different distributions. In this work, 
we employ the Probably Approximately Correct 
(PAC) learning theory to estimate the error 
boundary. According to the PAC learning theory, 
the least error boundary ? is determined by the 
size of the training set m and the class noise rate 
?, bound by the following relation: 
  ?   (   )                      ( ) 
In TTL, m increases linearly, yet ? is multi-
plied in each iteration. This means the signifi-
cance of m to performance is higher at the begin-
ning of transfer learning and gradually slows 
down in later iterations. On the contrary, the in-
fluence of class noise increases. That is why per-
formance improves initially and gradually falls to 
negative transfer when noise accumulation out-
performs the learned information as shown in 
Fig.1. In TTL, transferred samples in both the 
training data and test data have the same distribu-
tion. This implies that we can apply the PAC 
theory to analyze the error boundary of the ma-
chine learning model using transferred data. 
 
Figure 1 Negative transfer in the learning process 
According to PAC theorem with an assumed 
fixed probability ? (Angluin and Laird, 1988), 
the least error boundary ? is given by:   
  ?   (   ? )  ( (   ) )       ( ) 
where N is a constant decided by the hypothesis 
space.  In any iteration during TTL, the hypothe-
sis space is the same and the probability ? is 
fixed. Thus the least error boundary is deter-
mined by the size of the transferred sample m 
and the class noise of transferred samples ?. Ac-
cording to (2), we apply a manifold assumption 
based method to estimate ?. Let T be the number 
of iterations to serve as one period. We then es-
timate the least error boundary before and after 
each T to measure the quality of transferred sam-
ples during each T. If the least error boundary is 
reduced, it means that transferred samples used 
in this period are of high quality and can improve 
the performance. Otherwise, the transfer learning 
algorithm should stop.  
861
 3.2 Estimating Class Noise 
For formula (2) to work, we need to know the 
class noise rate ? to calculate the error boundary. 
Obviously, we cannot use conditional probabili-
ties from the training data in the source language 
to estimate the noise rate ? of the transferred 
samples because the distribution of source lan-
guage is different from that of target language. 
Consider a KNN graph on the transferred 
samples using any similarity metric, for example, 
cosine similarity, for any two connected vertex 
(     )and (     ) in the graph from samples to 
classes, the edge weight is given by: 
       (     )                         ( ) 
Furthermore, a sign function for the two vertices 
(     )and (     ), is defined as: 
    {
          
          
                   ( ) 
According to the manifold assumption, the 
conditional probability  (  |  ) can be approxi-
mated by the frequency of  (     ) which is 
equal to  (     ). In opinion annotations, the 
agreement of two annotators is often no larger 
than 0.8. This means that for the best cases 
 (     )=0.2. Hence     follows a Bernoulli 
distribution with p=0.2 for the best cases in 
manual annotations.  
Let      (     )  be the vertices that are 
connected to the     vertex, the statistical magni-
tude of the     vertex can be defined as: 
   ?                                 ( )  
where j refers to the     vertex that is connected 
to the     vertex.  
From the theory of cut edge statics, we know 
that the expectation of    is: 
    (     )  ?                  ( )  
And the variance of    is: 
  
   (     ) (     )  ?    
 
 ( )  
By the Center Limit Theorem (CLT),    fol-
lows the normal distribution: 
(     )
  
  (   )                    ( )  
To detect the noise rate of a sample (     ) , 
we can use (8) as the null hypothesis to test the 
significant level. Let    denotes probability of 
the correct classification for a transferred sample. 
   should follow a normal distribution,  
   
 
?    
?  
 
(    )
 
   
   
  
           ( )  
Note that experiments (Li and Zhou, 2011; 
Cheng and Li, 2009; Brodley and Friedl, 1999) 
have shown that     is related to the error rate of 
the example (     ), but it does not reflect the 
ground-truth probability in statistics. Hence we 
assume the class noise rate of example (     ) is: 
                              (  ) 
 We take the general significant level of 0.05 
to reject the null hypothesis. It means that if    of 
(     ) is larger than 0.95, the sample will be 
considered as a class noisy sample. Furthermore, 
   can be used to estimate the average class noise 
rate of a transferred samples in (2). 
In our proposed approach, we establish the 
quality estimate period T to conduct class noise 
detection to estimate the class noise rate of trans-
ferred samples. Based on the average class noise 
we can get the least error boundary so as to tell if 
an added sample is of high quality. If the newly 
added samples are of high quality, they can be 
used to detect class noise in transferred training 
data. Otherwise, transfer learning should stop. 
The flow chart for negative transfer is in Fig.2. 
SLS(labeled)
TLS
(unlabeled)
Classifier
Top k
TS
 period 1
TS
period 2
TS
 period n
KNN 
graph
Estimate ?i and ?n 
?n ? ?n-1?
Output SLS and TS 
(period 1 to n-1)
No
Yes
Del te TS
 ?i? 0.95 
period 1 to n-1
Input 
Input 
T iterations per period
Transfer
process
Negative
transfer
detection
Figure 2 Flow charts of negative transfer detection 
In the above flow chart, SLS and TLS refer to 
the source and target language samples, respec-
tively. TS refers to the transferred samples. Let T 
denote quality estimate period T in terms of itera-
tion numbers. The transfer process select k sam-
ples in each iteration. When one period of trans-
fer process finishes, the negative transfer detec-
tion will estimate the quality by comparing and 
either select the new transferred samples or re-
move class noise accumulated up to this iteration. 
4 Experiment 
4.1 Experiment Setting 
The proposed approach is evaluated on the 
NLP&CC 2013 cross-lingual opinion analysis (in 
862
 short, NLP&CC) dataset 1 . In the training set, 
there are 12,000 labeled English Amazon.com 
products reviews, denoted by Train_ENG, and 
120 labeled Chinese product reviews, denoted as 
Train_CHN, from three categories, DVD, BOOK, 
MUSIC. 94,651 unlabeled Chinese products re-
views from corresponding categories are used as 
the development set, denoted as Dev_CHN. In 
the testing set, there are 12,000 Chinese product 
reviews (shown in Table.1). This dataset is de-
signed to evaluate the CLOA algorithm which 
uses Train_CHN, Train_ENG and Dev_CHN to 
train a classifier for Test_CHN. The performance 
is evaluated by the correct classification accuracy 
for each category in Test_CHN2:  
          
                                  
    
 
where c is either DVD, BOOK or MUSIC. 
Team DVD Book Music 
Train_CHN 40 40 40 
Train_ENG 4000 4000 4000 
Dev_CHN 17814 47071 29677 
Test_CHN 4000 4000 4000 
Table.1 The NLP&CC 2013 CLOA dataset 
In the experiment, the basic transfer learning 
algorithm is co-training. The Chinese word seg-
mentation tool is ICTCLAS (Zhang et al 2003) 
and Google Translator3 is the MT for the source 
language. The monolingual opinion classifier is 
SVMlight4, word unigram/bigram features are em-
ployed. 
4.2 CLOA Experiment Results 
Firstly, we evaluate the baseline systems 
which use the same monolingual opinion classi-
fier with three training dataset including 
Train_CHN, translated Train_ENG and their un-
ion, respectively.  
 DVD Book Music Accuracy 
Train_CHN 0.552 0.513 0.500 0.522 
Train_ENG 0.729 0.733 0.722 0.728 
Train_CHN 
+Train_ENG 
0.737 0.722 0.742 0.734 
Table.2 Baseline performances  
It can be seen that using the same method, the 
classifier trained by Train_CHN are on avergage 
20% worse than the English counter parts.The 
combined use of Train_CHN and translated 
Train_ENG, however, obtained similar 
                                                 
1http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip 
2http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf 
3https://translate.google.com 
4http://svmlight.joachims.org/ 
performance to the English counter parts. This 
means the predominant training comes from the 
English training data. 
In the second set of experiment, we compare  
our proposed approach to the official results in 
NLP&CC 2013 CLOA evaluation and the result 
is given in Table 3. Note that in Table 3, the top 
performer of NLP&CC 2013 CLOA evaluation 
is the HLT-HITSZ system(underscored in the 
table), which used the co-training method in 
transfer learning (Gui et al 2013), proving that 
co-training is quite effective for cross-lingual 
analysis. With the additional negative transfer 
detection, our proposed approach achieves the 
best performance on this dataset outperformed 
the top system (by HLT-HITSZ) by a 2.97% 
which translate to 13.1% error reduction im-
provement to this state-of-the-art system as 
shown in the last row of Table 3.     
Team DVD Book Music Accuracy 
BUAA 0.481 0.498 0.503 0.494 
BISTU 0.647 0.598 0.661 0.635 
HLT-HITSZ 0.777 0.785 0.751 0.771 
THUIR 0.739 0.742 0.733 0.738 
SJTU 0.772 0.724 0.745 0.747 
WHU 0.783 0.770 0.760 0.771 
Our approach 0.816 0.801 0.786 0.801 
Error 
Reduction 
0.152 0.072 0.110 0.131 
Table.3 Performance compares with NLP&CC 
2013 CLOA evaluation results 
To further investigate the effectiveness of our 
method, the third set of experiments evaluate the 
negative transfer detection (NTD) compared to 
co-training (CO) without negative transfer 
detection as shown in Table.4 and Fig.3 Here, we 
use the union of Train_CHN and Train_ENG as 
labeled data and Dev_CHN as unlabeled data to 
be transferred in the learning algorithms. 
 DVD Book Music Mean 
NTD 
Best case 0.816 0.801 0.786 0.801 
Best period 0.809 0.798 0.782 0.796 
Mean 0.805 0.795 0.781 0.794 
CO 
Best case 0.804 0.796 0.783 0.794 
Best period 0.803 0.794 0.781 0.792 
Mean 0.797 0.790 0.775 0.787 
Table.4 CLOA performances 
Taking all categories of data, our proposed 
method improves the overall average precision 
(the best cases) from 79.4% to 80.1% when 
compared to the state of the art system which 
translates to error reduction of 3.40% (p-
value?0.01 in Wilcoxon signed rank test). Alt-
hough the improvement does not seem large, our 
863
  
   
Figure 3 Performance of negative transfer detection vs. co-training 
algorithm shows a different behavior in that it 
can continue to make use of available training 
data to improve the system performance. In other 
words, we do not need to identify the tipping 
point where the performance degradation can 
occur when more training samples are used. Our 
approach has also shown the advantage of stable 
improvement.  
In the most practical tasks, co-training based 
approach has the difficulty to determine when to 
stop the training process because of the negative 
transfer. And thus, there is no sure way to obtain 
the above best average precision. On the contrary, 
the performance of our proposed approach keeps 
stable improvement with more iterations, i.e. our 
approach has a much better chance to ensure the 
best performance. Another experiment is con-
ducted to compare the performance of our pro-
posed transfer learning based approach with su-
pervised learning. Here, the achieved perfor-
mance of 3-folder cross validation are given in 
Table 5. 
 DVD Book Music Average 
Supervised 0.833 0.800 0.801 0.811 
Our approach 0.816 0.801 0.786 0.801 
Table.5 Comparison with supervised learning  
The accuracy of our approach is only 1.0% 
lower than the supervised learning using 2/3 of 
Test_CHN. In the BOOK subset, our approach 
achieves match result. Note that the performance 
gap in different subsets shows positive correla-
tion to the size of Dev_CHN. The more samples 
are given in Dev_CHN, a higher precision is 
achieved even though these samples are unla-
beled. According to the theorem of PAC, we 
know that the accuracy of a classifier training 
from a large enough training set with confined 
class noise rate will approximate the accuracy of 
classifier training from a non-class noise training 
set. This experiment shows that our proposed 
negative transfer detection controls the class 
noise rate in a very limited boundary. Theoreti-
cally speaking, it can catch up with the perfor-
mance of supervised learning if enough unla-
beled samples are available. In fact, such an ad-
vantage is the essence of our proposed approach.  
5 Conclusion 
In this paper, we propose a negative transfer 
detection approach for transfer learning method 
in order to handle cumulative class noise and 
reduce negative transfer in the process of transfer 
learning. The basic idea is to utilize high quality 
samples after transfer learning to detect class 
noise in transferred samples. We take cross lin-
gual opinion analysis as the data set to evaluate 
our method. Experiments show that our proposed 
approach obtains a more stable performance im-
provement by reducing negative transfers. Our 
approach reduced 13.1% errors than the top sys-
tem on the NLP&CC 2013 CLOA evaluation 
dataset. In BOOK category it even achieves bet-
ter result than the supervised learning. Experi-
mental results also show that our approach can 
obtain better performance when the transferred 
samples are added incrementally, which in pre-
vious works would decrease the system perfor-
mance. In future work, we plan to extend this 
method into other language/domain resources to 
identify more transferred samples.  
Acknowledgement 
This research is supported by NSFC 61203378, 
61300112, 61370165, Natural Science Founda-
tion of GuangDong S2013010014475, MOE 
Specialized Research Fund for the Doctoral Pro-
gram of Higher Education 20122302120070,  
Open Projects Program of National Laboratory 
of Pattern Recognition?Shenzhen Foundational 
Research Funding JCYJ20120613152557576, 
JC201005260118A, Shenzhen International Co-
operation Research Funding 
GJHZ20120613110641217 and Hong Kong Pol-
ytechnic University Project code Z0EP. 
DVD Book Music 
864
 Reference 
Angluin, D., Laird, P. 1988. Learning from Noisy 
Examples. Machine Learning, 2(4): 343-370. 
Arnold, A., Nallapati, R., Cohen, W. W. 2007. A 
Comparative Study of Methods for Transductive 
Transfer Learning. In Proc. 7th IEEE ICDM Work-
shops, pages 77-82. 
Aue, A., Gamon, M. 2005. Customizing Sentiment 
Classifiers to New Domains: a Case Study, In Proc. 
of t RANLP. 
Blitzer, J., McDonald, R., Pereira, F. 2006. Domain 
Adaptation with Structural Correspondence Learn-
ing. In Proc. EMNLP, 120-128. 
Brodley, C. E., Friedl, M. A. 1999. Identifying and 
Eliminating Mislabeled Training Instances. Journal 
of Artificial Intelligence Research, 11:131-167. 
Chao, D., Guo, M. Z., Liu, Y.,  Li, H. F. 2008.  Partic-
ipatory Learning based Semi-supervised Classifica-
tion. In Proc. of 4th ICNC, pages 207-216. 
Cheng, Y., Li, Q. Y. 2009. Transfer Learning with 
Data Edit. LNAI, pages 427?434. 
Chen, M., Weinberger, K. Q.,  Blitzer, J. C. 2011.  
Co-Training for Domain Adaptation. In Proc. of 
23th NIPS. 
Fukumoto, F., Suzuki, Y., Matsuyoshi, S. 2013. Text 
Classification from Positive and Unlabeled Data 
using Misclassified Data Correction. In Proc. of 
51st ACL, pages 474-478. 
Gui, L., Xu, R.,  Xu, J., et al 2013. A Mixed Model 
for Cross Lingual Opinion Analysis. In CCIS, 400, 
pages 93-104. 
Huang, J., Smola, A., Gretton, A., Borgwardt, K.M., 
Scholkopf, B. 2007. Correcting Sample Selection 
Bias by Unlabeled Data. In Proc. of 19th NIPS,  
pages 601-608. 
Jiang, Y., Zhou, Z. H. 2004. Editing Training Data for 
kNN Classifiers with Neural Network Ensemble. In 
LNCS, 3173,  pages 356-361. 
Li, M., Zhou, Z. H. 2005. SETRED: Self-Training 
with Editing. In Proc. of PAKDD, pages 611-621. 
Li, M., Zhou, Z. H. 2011.  COTRADE: Confident Co-
Training With Data Editing. IEEE Transactions on 
Systems, Man, and Cybernetics?Part B: Cyber-
netics, 41(6):1612-1627. 
Lu, B., Tang, C. H., Cardie, C., Tsou, B. K. 2011. 
Joint Bilingual Sentiment Classification with Un-
labeled Parallel Corpora. In Proc. of 49th ACL, 
pages 320-330. 
Meng, X. F., Wei, F. R., Liu, X. H., et al 2012. 
Cross-Lingual Mixture Model for Sentiment Clas-
sification. In Proc. of 50th ACL, pages 572-581. 
Muhlenbach, F., Lallich, S., Zighed, D. A. 2004. 
Identifying and Handling Mislabeled Instances.  
Journal of Intelligent Information System, 22(1): 
89-109. 
Pan, S. J., Yang, Q. 2010. A Survey on Transfer 
Learning, IEEE Transactions on Knowledge and 
Data Engineering, 22(10):1345-1360. 
Sindhwani, V., Rosenberg, D. S. 2008. An RKHS for 
Multi-view Learning and Manifold Co-
Regularization. In Proc. of 25th  ICML, pages 976?
983. 
Sluban, B., Gamberger, D., Lavra, N. 2010.  Advanc-
es in Class Noise Detection. In Proc.19th ECAI,  
pages 1105-1106. 
Sugiyama, M.,  Nakajima, S., Kashima, H., Buenau, 
P.V., Kawanabe, M. 2008. Direct Importance Es-
timation with Model Selection and its Application 
to Covariate Shift Adaptation. In Proc. 20th NIPS. 
Wan, X. 2009. Co-Training for Cross-Lingual Senti-
ment Classification, In Proc. of the 47th Annual 
Meeting of the ACL and the 4th IJCNLP of the 
AFNLP,  235?243. 
Zhang, H. P., Yu, H. K., Xiong, D. Y., and Liu., Q. 
2003. HHMM-based Chinese Lexical Analyzer 
ICTCLAS. In 2nd SIGHAN workshop affiliated 
with 41th ACL, pages 184-187. 
 Zhou, X., Wan X., Xiao, J. 2011. Cross-Language 
Opinion Target Extraction in Review Texts. In 
Proc. of IEEE 12th ICDM, pages 1200-1205. 
Zhu, X. Q., Wu, X. D., Chen, Q. J. 2003.  Eliminating 
Class Noise in Large Datasets. In Proc. of 12th 
ICML, pages 920-927. 
Zhu, X. Q. 2004. Cost-guided Class Noise Handling 
for Effective Cost-sensitive Learning In Proc. of 4th  
IEEE ICDM,  pages 297-304. 
Zighed, D. A., Lallich, S., Muhlenbach, F. 2002.  
Separability Index in Supervised Learning. In Proc. 
of PKDD, pages 475-487. 
865
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 107?112,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Incorporating Rule-based and Statistic-based Techniques for 
Coreference Resolution 
 
 
Ruifeng Xu, Jun Xu, Jie Liu, Chengxiang Liu, Chengtian Zou, Lin Gui, Yanzhen 
Zheng, Peng Qu
Human Language Technology Group, Key Laboratory of Network Oriented Intelligent 
Computation, Shenzhen Graduate School, Harbin Institute of Technology, China 
{xuruifeng.hitsz;hit.xujun;lyjxcz;matitalk;chsky.zou;monta3pt;
zhyz.zheng;viphitqp@gmail.com} 
 
 
Abstract 
This paper describes a coreference resolution 
system for CONLL 2012 shared task 
developed by HLT_HITSZ group, which 
incorporates rule-based and statistic-based 
techniques. The system performs coreference 
resolution through the mention pair 
classification and linking. For each detected 
mention pairs in the text, a Decision Tree (DT) 
based binary classifier is applied to determine 
whether they form a coreference. This 
classifier incorporates 51 and 61 selected 
features for English and Chinese, respectively. 
Meanwhile, a rule-based classifier is applied to 
recognize some specific types of coreference, 
especially the ones with long distances. The 
outputs of these two classifiers are merged. 
Next, the recognized coreferences are linked to 
generate the final coreference chain. This 
system is evaluated on English and Chinese 
sides (Closed Track), respectively. It achieves 
0.5861 and 0.6003 F1 score on the 
development data of English and Chinese, 
respectively. As for the test dataset, the 
achieved F1 scores are 0.5749 and 0.6508, 
respectively. This encouraging performance 
shows the effectiveness of our proposed 
coreference resolution system. 
1 Introduction 
Coreference resolution aims to find out the 
different mentions in a document which refer to the 
same entity in reality (Sundheim and Beth, 1995; 
Lang et al 1997; Chinchor and Nancy, 1998;). It is 
a core component in natural language processing 
and information extraction.  Both rule-based 
approach (Lee et al 2011) and statistic-based 
approach (Soon et al, 2001; Ng and Cardie, 2002; 
Bengtson and Roth, 2008; Stoyanov et al, 2009; 
Chen et al 2011) are proposed in coreference 
resolution study. Besides the frequently used 
syntactic and semantic features, the more linguistic 
features are exploited in recent works (Versley, 
2007; Kong et al 2010). 
CoNLL-2012 proposes a shared task, ?Modeling 
multilingual unrestricted coreference in the 
OntoNotes? (Pradhan et al 2012). This is an 
extension of the CoNLL-2011 shared task. The 
task involves automatic anaphoric mention 
detection and coreference resolution across three 
languages including English, Chinese and Arabic. 
HLT_HITSZ group participated in the Closed 
Track evaluation on English and Chinese side. This 
paper presents the framework and techniques of 
HLT_HITSZ system which incorporates both rule-
based and statistic-based techniques. In this system, 
the mentions are firstly identified based on the 
provided syntactic information. The mention pairs 
in the document are fed to a Decision Tree based 
classifier to determine whether they form a 
coreference or not. The rule-based classifiers are 
then applied to recognize some specific types of 
coreference, in particular, the long distance ones. 
Finally, the recognized coreference are linked to 
obtain the final coreference resolution results. This 
system incorporates lexical, syntactical and 
semantic features. Especially for English, WordNet 
is used to provide semantic information of the 
mentions, such as semantic distance and the 
107
category of the mentions and so on. Other than the 
officially provided number and gender data, we 
generated some lexicons from the training dataset 
to obtain the values of some features. This system 
achieves 0.5861 and 0.6003 F1 scores on English 
and Chinese development data, respectively, and 
0.5749 and 0.6508 F1 scores on English and 
Chinese testing data, respectively. The achieved 
encouraging performances show that the proposed 
incorporation of rule-based and statistic-based 
techniques is effective. 
The rest of this report is organized as below. 
Section 2 presents the mention detection. Section 3 
presents the coreference determination and Section 
4 presents the coreference linking. The 
experimental results are given in Section 5 in detail. 
Finally, Section 6 concludes this report. 
2 Mention Detection 
In this stage, the system detects the mentions from 
the text. The pairs of these mentions in one 
document are regarded as the coreference 
candidates. Thus, the high recall is a more 
important target than higher precision for this stage. 
Corresponding to English and Chinese, we adopted 
different detection methods, respectively. 
2.1 Mention Detection - English 
HLT_HITSZ system chooses the marked noun 
phrase (NP), pronouns (PRP) and PRP$ in English 
data as the mentions. The system selects most 
named entities (NE) as the mentions but filter out 
some specific types. Firstly, the NEs which cannot 
be labeled either as NP or NML are filter out 
because there are too cases that the pairs of these 
NEs does not corefer even they are in the same 
form as shown in the training dataset. Second, the 
NEs of ORDINAL, PERCENT and MONEY types 
are filtered because they have very low coreference 
ratio (less than 2%). Furthermore, for the cases that 
NPs overlapping a shorter NP, normally, only the 
longer one are choose. An exception is that if the 
shorter NPs are in parallel structures with the same 
level to construct a longer NP. For example, for a 
NP ?A and B?, ?A?, ?B? and ?A and B? as 
regarded ed as three different mentions.   
2.2 Mention Detection ? Chinese 
HLT_HITSZ system extracts all NPs and PNs as 
the mention candidates. For the NPs have the 
overlaps, we handle them in three ways: 1. For the 
cases that two NPs share the same tail, the longer 
NP is kept and the rest discarded; 2. For cases that 
the longer NP has a NR as its tail, the NPs which 
share the same tail are discarded; 3. In MZ and 
NW folders, they are many mentions nested 
marked as the nested co-referent mentions. The 
system selects the longest NP as mention in this 
stage while the other mention candidates in the 
longest NP will be recalled in the post processing 
stage. 
3 Coreference Determination 
Any pair of two detected mentions in one 
document becomes one coreference candidate. In 
this stage, the classifiers are developed to 
determine whether this pair be a coreference or not. 
During the generation of mention pairs, it is 
observed that linking any two mentions in one 
document as candidates leads to much noises.  The 
statistical observation on the Chinese training 
dataset show that 90% corefered mention pairs are 
in the distance of 10 sentences. Similar results are 
found in the English training dataset while the 
context window is set to 5 sentences. Therefore, in 
this stage, the context windows for generating 
mention pairs as coreference candidates for 
English and Chinese are limited to 5 and 10 
sentences, respectively. 
3.1 The Statistic-based Coreference 
Determination 
The same framework is adopted in the statistical-
based coreference determination for English and 
Chinese, respectively, which is based on a machine 
learning-based statistical classifier and selected 
language-dependent features. Through transfer the 
examples in the training test into feature-valued 
space, the classifier is trained. This binary 
classifier will be applied to determine whether the 
input mention pair be a coreference or not. Here, 
we evaluated three machine learning based 
classifiers including Decision Tree, Support Vector 
Machines and Maximum Entropy on the training 
data while Decision Tree perform the best. Thus, 
DT classifier is selected. Since the annotations on 
the training data from different directory show 
some inconsistence, multiple classifiers 
corresponding to each directory are trained 
individually.  
108
3.1.1 Features - English 
51 features are selected for English coreference 
determination. The features are camped to six 
categories. Some typical features are listed below: 
1. Basic features: 
(1) Syntactic type of the two mentions, 
includes NP, NE, PRP, PRP$. Here, only 
the NPs which do not contain any named 
entities or its head word isn?t a named 
entity are considered as an NP while the 
others are discarded. 
(2) If one mention is a PRP or PRP$, use an 
ID to specify which one it is.  
(3) The sentence distance between two 
mentions. 
(4) Whether one mention is contained by 
another one. 
2. Parsing features: 
(1) Whether two mentions belong to one NP. 
(2) The phrase distance between the two 
mentions.  
(3) The predicted arguments which the two 
mentions belong to. 
3. Named entity related features: 
(1) If both of the two mentions may be 
considered as named entities, whether 
they have the same type. 
(2) If one mention is a common NP or PRP 
and another one can be considered as 
named entity, whether the words of the 
common NP or PRP can be used to refer 
this type of named entity. This knowledge 
is extracted from the training dataset. 
(3) Whether the core words of the two named 
entity type NP match each other.    
4. Features for PRP: 
(1) If both mentions are PRP or PRP$, use an 
ID to show what they are. The PRP$ with 
the same type will be assigned the same 
ID, for example, he, him and his. 
(2) Whether the two mentions has the same 
PRP ID. 
5. Semantic Features: 
(1) Whether the two mentions have the same 
headword. 
(2) Whether the two mentions belong to the 
same type. Here, we use WordNet to get 
three most common sense of each NP and 
compare the type they belong to.  
(3) The semantic distance between two 
mentions. WordNet is used here.  
(4) The natures of the two mentions, including 
number, gender, is human or not, and 
match each other or not. We use WordNet 
and a lexicon extracted from the gender 
and number file here. 
6. Document features: 
(1) How many speakers in this document. 
(2) Whether the mention is the first or the last 
sentence of the document. 
(3) Whether the two mentions are from the 
same speaker. 
3.1.2 Features - Chinese 
There are 61 features adopted in Chinese side. 
Because of the restriction of closed crack, most of 
features use the position and POS information. It is 
mentionable that the ways for calculating the 
features values. For instance, the sentence distance 
is not the real sentence distance in the document. 
For instead, the value is the number of sentences in 
which there are at least one mention between the 
mention pair. This ignores the sentences of only 
modal particles.  
The 61 features are camped into five groups. 
Some example features are listed below. 
1. Basic information: 
(1) The matching degree of two mentions 
(2) The word distance of two mentions 
(3) The sentence distance of two mentions 
2. Parsing information: 
(1) Predicted arguments which the two 
mentions belong to and corresponding 
layers. 
3. POS features 
(1) Whether the mention is NR 
(2) Whether the two mentions are both NR 
and are matched 
4. Semantic features: 
(1) Whether the two mention is related 
(2) Whether the two mentions corefer in the 
history. Since the restriction of closed 
track, we did not use any additional 
semantic resources. Here, we extract the 
co-reference history from the training set 
to obtain some semantic information, such 
as ?NN ??? and ?NN ??? corefered in 
the training data, and they are regarded as 
coreference in the testing data.  
5. Document Features: 
109
(1) Whether the two mentions have the same 
speaker. 
(2) Whether the mention is a human. 
(3) Whether the mention is the first mention in 
the sentence. 
(4) Whether the sentence to which the mention 
belongs to is the first sentence. 
(5) Whether the sentence to which the mention 
belongs to is the second sentence 
(6) Whether the sentence to which the mention 
belongs to is the last sentence 
(7) The number of the speakers in the 
document. 
3.2 The Rule-based Coreference 
Determination 
The rule-based classifier is developed to recognize some 
specific types of coreference and especially, the long 
distance ones.  
3.2.1 Rule-based Classifier - English 
To achieve a high precision, only the mention pairs 
of NE-NE (include NPs those can be considered as 
NE) or NP-NP types with the same string are 
classified here.  
For the NE-NE pair, the classifier identifies their 
NE part from the whole NP, if their strings are the 
same, they are considered as coreference. 
For the NP-NP pair, the pairs satisfy the 
following rules are regarded as coreference. 
(1) The POS of the first word isn?t ?JJR? or ?JJ?. 
(2) If NP has only one word, its POS isn?t ?NNS? 
or ?NNPS?. 
(3) The NP have no word like ?every?, ?every-?, 
?none?, ?no?,  ?any?,  ?some?,  ?each?. 
(4) If the two NP has article, they can?t be both 
?a? or ?an?. 
Additionally, for the PRP mention pairs, only 
?I?, ?me?, ?my? with the same speaker can be 
regarded as coreference. 
3.2.2 Rule-based Classifier - Chinese 
A rule-based classifier is developed to determine 
whether the mention pairs between PNs and 
mentions not PN corefer or not. For instance, the 
mention pairs between the PN ??? which is after a 
comma and the mention which is marked as ARG0 
in the same sentence. In the sentence ?????? 
??  ?  ?  ??  ??  ??  ??  ?  ???, 
because the mention pair between ??????? 
and the first ??? match the mentioned above rule, 
it  is classified as a positive one. The result on the 
development set shows that the rule-based 
classifier brings good improvement. 
4 Coreference Chain Construction  
4.1 Coreference Chain Construction-English 
The evaluation on development data shows that the 
achieved precision of our system is better than 
recall.  Thus, in this stage, we simply link every 
pair of mentions together if there is any links can 
link them together to generate the initial 
coreference chain. After that, the mentions have 
the distance longer than 5 sentences are observed. 
The NE-NE or NP-NP mention pairs between one 
known coreference and an observing mention with 
long distance are classified to determine they are 
corefered or not by using a set of rules. The new 
detected conference will be linked to the initial 
coreference chain.  
4.2 Coreference Chain Construction-Chinese 
The coreference chain construction for Chinese is 
similar to English. Furthermore, as mentioned 
above, in MZ and NW folders, there are many 
mentions nested marked as the nested co-
referenced mentions. In this stage, HLT_HITSZ 
system generates the nested co-reference mentions 
for improving the analysis for these two folders. 
Additionally, the system uses some rules to 
improve the coreference chain construction. We 
find that the trained classifier performs poor in co-
reference resolution related to Pronoun. So, most  
rules adopted here are related to these Pronouns: 
????, ???, ???, ???, ???, ????, ????, 
???. We use these rules to bridge the chain of 
pronouns and the chain of other type. 
Although high precision for NT co-reference 
cases are achieved through string matching, the 
recall is not satisfactory. It partially attributes to 
the fact that the flexible use of Chinese. For 
example, to express the year of 1980, we found 
???????, ??????, ? ?????, ???
? ?, ?1980 ? ?. Similar situation happens for 
month (?, ??) and day (?,?), we conclude 
most situations to several templates to improve the 
rule-based conference resolution. 
110
5 Evaluation Results 
5.1 Dataset 
The status of training dataset, development dataset 
and testing dataset in CoNLL 2012 for English and 
Chinese are given in Table 1 and Table 2, 
respectively. 
 Files Sentence Cluster Coreference
Train 1,940 74,852 35,101 155,292 
Development 222 9,603 4,546 19,156 
Test 222 9,479 n/a n/a 
Table 1. Status of CoNLL 2012 dataset - English 
 
 Files Sentence Cluster Coreference
Train 1,391 36,487 28,257 102,854 
Develop 172 6,083 3,875 14,383 
Test 166 4,472 n/a n/a 
Table 2. Status of CoNLL 2012 dataset - Chinese 
5.2 Evaluation on Mention Detection 
Firstly, the mention detection performance is evaluated. 
The performance achieved on the development dataset 
(Gold/Auto) and test data on English and Chinese are 
given in Table 3 and Table 4, respectively. In which, 
Gold means the development dataset with gold 
manually annotation and Auto means the automatically 
generated annotations.  
 Precision Recall F1 
Develop-Gold 0.8499 0.6716 0.7503 
Develop-Auto 0.8456 0.6256 0.7192 
Test 0.8455 0.6264 0.7196 
Table 3. Performance on Mention Detection - English 
 
 Precision Recall F1 
Develop-Gold 0.7402 0.7360 0.7381 
Develop-Auto 0.6987 0.6429 0.6697 
Test 0.7307 0.7502 0.7403 
Table 4. Performance on Mention Detection - Chinese 
 
Generally speaking, our system achieves acceptable 
mention detection performance, but further 
improvements are desired.  
5.3 Evaluation on Coreference Resolution 
The performance on coreference resolution is next 
evaluated. The achieved performances on the 
development data (Gold/Auto) and test dataset on 
English and Chinese are given in Table 5 and Table 6, 
respectively. It is shown that the OF performance drops 
0.0309(Gold) and 0.0112(Auto) from development 
dataset to test dataset on English, respectively. On the 
contrary, the OF performance increases 0.0096(Gold) 
and 0.0505(Auto) from development dataset to test 
dataset on Chinese, respectively. Compared with the 
performance reported in CoNLL2012 shared task, our 
system achieves a good result, ranked 3rd, on Chinese. 
The results show the effectiveness of our proposed 
system. 
 Precision Recall F1 
MUC 0.7632 0.6455 0.6994 
BCUB 0.7272 0.6797 0.7027 
CEAFE 0.3637 0.4840 0.4154 
OF-Develop-Gold   0.6058 
MUC 0.7571 0.5993 0.6691 
BCUB 0.7483 0.6441 0.6923 
CEAFE 0.3350 0.4865 0.3968 
OF-Develop-Auto   0.5861 
MUC 0.7518 0.5911 0.6618 
BCUB 0.7329 0.6228 0.6734 
CEAFE 0.3264 0.4829 0.3895 
OF-Test   0.5749 
Table 5. Performance on Coreference Resolution ? 
English  
 Precision Recall F1 
MUC 0.6892 0.6655 0.6771
BCUB 0.7547 0.7410 0.7478
CEAFE 0.4876 0.5105 0.4988
OF-Develop-Gold   0.6412
MUC 0.6535 0.5643 0.6056
BCUB 0.7812 0.6809 0.7276
CEAFE 0.4322 0.5101 0.4679
OF-Develop-Auto   0.6003
MUC 0.6928 0.6595 0.6758
BCUB 0.7765 0.7328 0.7540
CEAFE 0.5072 0.5390 0.6253
OF-Test(Gold parses)   0.6508
MUC 
BCUB 
CEAFE 
OF-Test-Predicted-mentions 
(Auto parses) 
0.5502 
0.6839 
0.5040 
0.6147
0.7638
0.4481
0.5807
0.7216
0.4744
0.5922
MUC 
BCUB 
CEAFE 
OF-Test-Gold-mention-
boundaries(Auto parses) 
0.6354 
0.7136 
0.5390 
0.6873
0.7870
0.4907
0.6603
0.7485
0.5137
0.6408
MUC 
BCUB 
CEAFE 
OF-Test-Gold-mentions 
(Auto parses) 
0.6563 
0.6505 
0.7813 
0.9407
0.9123
0.4377
0.7732
0.7595
0.5611
0.6979
Table 6. Performance on Coreference Resolution ? 
Chinese 
111
6 Conclusions 
This paper presents the HLT_HITSZ system for 
CoNLL2012 shared task. Generally speaking, this 
system uses a statistic-based classifier to handle 
short distance coreference resolution and uses a 
rule-based classifier to handle long distance cases. 
The incorporation of rule-based and statistic-based 
techniques is shown effective to improve the 
performance of coreference resolution. In our 
future work, more semantic and knowledge bases 
will be incorporated to improve coreference 
resolution in open track. 
 
Acknowledgement 
This research is supported by HIT.NSFIR.201012 
from Harbin Institute of Technology, China and 
China Postdoctoral Science Foundation No. 
2011M500670. 
References  
B. Baldwin. 1997. CogNIAC: High Precision 
Coreference with Limited Knowledge and Linguistic 
Resources. Proceedings of Workshop on Operational 
Factors in Practical, Robust Anaphora Resolution for 
Unrestricted Texts. 
E. Bengtson, D. Roth. 2008. Understanding the Value of 
Features for Coreference Resolution. Proceedings of 
EMNLP 2008, 294-303. 
M. S. Beth M. 1995. Overview of Results of the MUC-6 
Evaluation. Proceedings of the Sixth Message 
Understanding Conference (MUC-6) 
W. P. Chen, M. Y. Zhang, B. Qin,  2011. Coreference 
Resolution System using Maximum Entropy 
Classifier. Proceedings of CoNLL-2011. 
N. A. Chinchor. 1998. Overview of MUC-7/MET-2. 
Proceedings of the Seventh Message Understanding 
Conference (MUC-7). 
F. Kong, G. D. Zhou, L. H. Qian, Q. M. Zhu. 2010. 
Dependency-driven Anaphoricity Determination for 
Coreference Resolution. Proceedings of COLING 
2010, 599-607  
J. Lang, B. Qin, T. Liu. 2007. Intra-document 
Coreference Resolution: The State of the Art. Journal 
of Chinese Language and Computing , 2007, 17( 4) : 
227-253. 
H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. 
Surdeanu, D. Jurafsky. 2011. Stanford?s Multi-Pass 
Sieve Coreference Resolution System at the CoNLL-
2011 Shared Task. Proceedings of CoNLL-2011. 
V. Ng and C. Cardie. 2002. Improving Machine 
Learning Approaches to Coreference Resolution. 
Proceedings of ACL 2002. 
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A 
Machine Learning Approach to Coreference 
Resolution of Noun Phrases. Computational 
Linguistics, 27(4):521-544 
S. Pradhan and A. Moschitti et al 2012. CoNLL-2012 
Shared Task: Modeling Multilingual Unrestricted 
Coreference in OntoNotes. Proceedings of CoNLL 
2012 
V. Stoyanov, N. Gilbert, C. Cardie, E. Riloff. 2009. 
Conundrums in Noun Phrase Coreference Resolution: 
Making Sense of the State-of-the-Art. Proceeding 
ACL 2009  
Y. Versley. 2007. Antecedent Selection Techniques for 
High-recall Coreference Resolution. Proceedings of 
EMNLP/CoNLL 2007. 
Y. Yang, N. W. Xue, P. Anick. 2011. A Machine 
Learning-Based Coreference Detection System For 
OntoNotes.  Proceedings of CoNLL-2011. 
 
 
112
