A Bootstrapping Method for Extracting Bilingual Text Pairs 
Hiroshi Masuiehil Raymond Flournoy* 
*Fuii Xerox Co., Ltd. 
Corporate Research Center 
430 Sakai, Nakai-machi, Ashigarakami-gun, 
Kanagawa 259-0157, Japan 
{ masuichi, flournoy, kauflnann, 
Stefan Kaufmann * Stanley Peters * 
? Center for the Study of Language and Information 
Stanford University 
210 Panama Street, Stanford, 
CA 94305-4115, U.S.A. 
peters} @csli.stanford.edu 
Abstract 
This paper proposes a method for extracting 
bilingual text pairs from a comparable cor- 
pus. The basic idea of the method is to ap- 
ply bootstrapping to an existing corpus- 
based cross-language information retrieval 
(CLIR) approach. We conducted prelimi- 
nary tests with English and Japanese bilin- 
gual corpora. The bootstrapping method 
led to much better esults for the task of ex- 
tracting translation pairs compared with a 
corpus-based CLIR method without boot- 
strapping, and the extracted translation pairs 
could be useftfl training data for improving 
results of the corpus-based CLIR method. 
1 Introduction 
A parallel corpus is an important resource for 
corpus-based approaches to CLIR. These 
approaches use parallel corpora as statistical 
training data and then retrieve documents writ- 
ten in a language different from that of the query. 
One disadvantage of these approaches i lack of 
resources. Parallel corpora are not always 
readily available and those that are available 
tend to be relatively small or to cover only a 
small number of subjects. 
A bilingual comparable corpus is a set of texts 
in two different languages from the same do- 
main or on the same topic. Unlike a parallel 
corpus it is composed independently in the re- 
spective language text sets. It can be more 
readily obtained from the Internet or CD-ROM 
resources than parallel corpora. Zanettin 
(1998) introduced several available bilingual 
comparable corpora such as news paper articles 
selected by dates and subject codes, medical 
articles from journals and textbooks, and articles 
for tourists from brochures and guides. Zanet- 
tin (1994) also reported that it is highly likely 
that much relevant information can be found 
across languages in a topic-related bilingual 
comparable corpus. In this paper, we propose a 
method for extracting bilingual text pairs which 
share the same information fiom a bilingual 
colnparable corpus, and show the possibility that 
the resulting bilingual text pairs can be useful 
for corpus-based CLIR approaches when we use 
them as training data instead of a parallel corpus. 
Sheridan (1998) also proposed an approach to 
building lnultilingual test collection from com- 
parable corpora consisting of news articles. 
The idea is to reduce the work of manual rele- 
vance judgements by restricting news articles to 
be examined to a couple of days. Disadvan- 
tages to this approach are that it relies on time- 
sensitive texts, texts obtained by this approach 
are constrained to referencing specific events, 
and nontrivial work by hulnans is still necessm'y. 
On the other hand, our goal is to extract bilin- 
gual text pairs automatically from any kind of 
bilingual comparable corpora. 
This paper is organized as follows: Section 
2 introduces the basic idea for extracting 
relevant ext pairs from a bilingual comparable 
corpus. Our method is based on a corpus-based 
CLIR method, so we overview previous corpus- 
based CLIR approaches in Section 3. Section 4 
describes an experimental procedure, the results 
it produced, and an analysis of the results. The 
conclusion is given in Section 5. 
2 The Basic Idea 
As we will describe in Section 3, several CLIR 
approaches that rely on parallel corpora have 
been proposed and lead to successful retrieval 
results. In those approaches, a parallel corpus 
used as training data should be large enough to 
obtain good retrieval results. Although we use 
a CLIR method which relies on a parallel corpus, 
we begin with a very small parallel corpus. We 
retrieve bilingual text pairs from a bilingual 
comparable corpus using the small parallel cor- 
pus as training data. Then we concatenate the 
text pairs to the initial small parallel corpus and 
grow the parallel corpus by iterating the retrieval 
and concatenation processes (Figure 1). 
1066 
I comparable COlptlS I
Figure 1: The bootstrapping method 
This kind of bootstrapping method has a 
problem, however: It is highly sensitive to the 
accuracy of the text pairs obtained in the early 
stages of the iterations. In order to solve this 
problem, we concatenate only a small number of 
the most "reliable" text pairs to the initial paral- 
lel corpus in the early stages, then gradually 
increase the number of the text pairs which are 
concatenated to the initial parallel corpus. We 
will describe the details of the method in Section 
4. 
3 Corpus-based CLIR approaches 
3.1 Previous Researches 
As we mentioned in Section 2, we use a CLIR 
method which relies on a parallel corpus in our 
bootstrapping method. One approach to cor- 
pus-based CLIR is to use the Latent Semantic 
Indexing technique proposed by Fumas et al 
(1988) on a parallel corpus to construct a lan- 
guage illdependent representation el'queries and 
documents (Landauer and Lfltman, 1990). 
Another approach that relies on a parallel corpus 
has been suggested by l)unning and l)avis 
(1993). Their method is based on the vector 
space model and involves the linear trausforula- 
tion of the representation f a query. A parallel 
corpus can also be used to enhance existing 
knowledge-based resources. The resources ark 
used to translate the query and then classical IR 
matching techniques are applied to compute the 
similarity between the trauslated query and 
documents (Hull and Grel'enstette, 1996). 
3.2 hfforlnation Mapping for CLIR 
For our bootstrapping method, we adopted a 
CLIR method which is based on the hfforma- 
tion Mapping approach (Masuichi et al, 
1999). Information Mapping is basically a 
wlriant of the vector space model, and is based 
on an approach first proposed by Schtitze 
(1995). The approach is closely related to 
Latent Semantic Indexing, and the dilTerence 
between these two is discussed in Schfitze and 
Pedersen (1997). Note that our bootstrapping 
method does not depend on any particuhu" 
properties o1' the Information Mapping ap- 
proach, so it could employ other corpus-based 
CLIR methods such as Latent Semantic in- 
dexing. 
Information Mapping begins with a large 
word-by-word matrix. A list of n content- 
bearing words and m w)cabulary words corre- 
spond to the columns and the rows of the 
matrix. The most fiequently appearing n 
words in a training corpus are selected as 
content-bearing words and the most frequently 
appearing m words as vocabulary words. 
Each cell of the matrix holds the nmnber of 
total cooccurrences between a content-bearing 
word and a vocabulary word in the training 
corpus. In this way, an n-dimensional vector 
which represents the word's distributional 
behavior is produced t'or each vocabulary 
word. Then the original n-dimensional vec- 
tor space is converted into a condensed, lower- 
dilnensional, real-valued matrix using Singular 
Value l)ecomposition (SVD) (Berry, 1992). 
The lower-dimensional vector space is called 
word space. A document vector and a query 
vector are calculated by summing the vectors 
corresponding to the vocabulary words in the 
document or the query, and the proximity 
between the two vectors is del'ined as the cosi- 
ne of the angle between them. 
To apply this method to CL1R, we regard 
each translation pair in a training parallel 
corpus of language LI and L2 as a single 
COlnpotmd document and create a word-by- 
word matrix and then a word space. The 
word space represents a hmguage independent 
vector space for vocabulary words in both 1,1 
and L2, and therefore query and document 
vectors in both LI and L2 can be calculated 
and compmed in the salne word space. 
4 Experimental tests and Results 
4.1 Tests with complete-pair corpora 
We used an English-Japanese bilingual patent 
text corpus for our experilnental tests. For 
our first test, we prepared I000 English- 
Japanese patent text pairs as a pseudo bilin- 
gual comparable corpus. For each Japanese 
patent text in the corpus, its English transla- 
tion by humans exists j, so this corpus could be 
regarded as an ideal bilingual comparable 
corpus. We also prepared 100 pairs as an 
initial parallel corpus (a training corpus) to 
create an initial word space. All the patents 
The quality of the translations wtrics greatly from 
word-for-word translations to short sunnnaries. 
1067 
in the two corpora were randomly selected 
from the Japanese patents issued in 1991, and 
the two corpora shared no patent. We used 
only the title and abstract exts and removed 
all other information, such as author, patent ID 
and issue date. Table 1 shows an example of 
an English-Japanese pair in the corpora. All 
characters in the English texts are l-byte char- 
acters and all characters, including alphabeti- 
cal and numerical characters, in the Japanese 
texts are 2-byte, so there is no word which is 
shared by both English and Japanese texts. 
We used all words which appeared in a train- 
ing corpus as vocabulary words, and the most 
frequently appearing 3000 English words as 
content-bearing words and then reduced the 
dimension of the vectors from 3000 to 200 by 
SVD. 
llose liar 'l'ransl~zrring Fertilizer from Fcflilizer Tmlki of Mobile \["arm Machine Abslracl: 
PROlll ,I~M TO l ie  .SOI.VI.~D: To provide a mechanism To arrange a ferlilizer Imnsli~r bose 
from a ferlilizer lallk wilhoul catlsing hindrance Io lhe olher mcchaniSlllS, t}lc. SOI.UTION: 
A fertilizer Irans fer hose 38 Io deliver a f?~lilizer rllll| il fcriilizer lank 31 placed al ~1 side of 
a mobile machine l~dy I Io lhe downslream side of a ferlilizing par128 is laid along the 
oilier circulllli?rellCe of a passage 23 placed ~l\[Ollg die back and a side o1" a drivcl's seal 8 and 
exlending \[ioln Ihe driver's eal 8 Io a working inacbille I I. 
1 ~l l :~  I-/'6.~tzltE~t')x-'ga 1 h',6 ll~llEt~t128 T ~'~IV'IIE~/I~.J~'C'~IIB,I~,--7,.a 8-'2, 
Table 1 : An example of an English- 
Japanese patent pair 
We began with a word space created from 
the 100 English-Japanese translation pairs (the 
initial parallel corpus). Then using the word 
space, we calculated 1000 English patent 
vectors and 1000 Japanese patent vectors 
which correspond to the patent texts in the 
pseudo comparable corpus. Next we extract- 
ed English-Japanese patent pairs which satis- 
fied the simple condition that the English 
patent vector in the pair has the highest prox- 
imity (the biggest cosine) with the Japanese 
patent vector in the pair among the 1000 Ja- 
panese patent vectors, and vice versa (hereaf- 
ter we call these pairs mutual-proximity pairs). 
Note that mutual-proximity pairs are, of 
course, not always correct translation pairs. 
Then we selected the 10 most "reliable" mu- 
tual-proximity pairs, assuming that the higher 
the proximity between the two vectors of a 
mutual-proximity pair, the more reliable the 
mutual-proximity pair is. Finally we con- 
catenated the 10 mutual-proximity pairs to the 
initial 100 translation pairs. This is the first 
stage of our bootstrapping method. 
In the second stage, we created a new word 
space regarding the 110 English-Japanese 
pairs obtained in the first stage as a training 
corpus. Then we selected the 20 most reli- 
able mutual-proximity pairs and concatenated 
them to the initial 100 patent ranslation pairs. 
At the Nth stage, we selected the N* 10 most 
reliable lnutual-proximity pairs. If the num- 
ber of the nmtual-proximity pairs obtained in 
the stage is less than N*I0, all of the mutual- 
proximity pairs were concatenated to the ini- 
tial 100 patent ranslation pairs. 
We repeated this procedure up to the 100th 
stage. At the 100th stage, we obtained 727 
mutual-proximity pairs and 721 pairs out of 
the 727 pairs were correct translation pairs. 
Therefore the recall of the obtained pairs was 
72.1% (721/1000) and the precision was 
99.2% (721/727) (see the column of Testl and 
the row of the "bootstrapping method" of 
Table 2). On the other hand, we obtained 
341 mutual-proximity pairs and 258 pairs out 
of the 341 pairs were correct ranslation pairs 
in the case of the normal Information Mapping 
method which corresponds to the first stage of 
our bootstrapping method. In this case, the 
recall was 25.8% and the precision was 75.7% 
(see the column of Testl and the row of the 
"normal method" of Table 2). 
I 0C, 
8(2 
6(: 
,I(i 
2( 
~ s i / m  (e~) 
~ recall ( ~ ) 
, I . I , I , I , 
20 40 6(1 80 100 
Figure 2: The change of precision and recall 
with complete-pair corpus 
Figure 2 shows the change of the precision 
and the recall through the 100 stages. The 
precision was kept over 93.3% and the recall 
went up gradually. We could successfully 
grow the bilingual text pairs using bootstrap- 
ping. 
l l o rmal  
method 
boot- 
strapping 
method 
Prec 75.7 
Rec 25.8 
Prec 99.2 
Rec 72.1 
75.6 76.6 
26.6 26.9 
99.1 99.7 
74.0 73.0 
78.2 72.8 
25.4 27.1 
98.9 98.7 
71.0 70.6 
Table 2: Results of extracting tests with 
complete-pair corpus 
1068 
We prepared 4 more different sets of 1000 
pairs 1'or pseudo comparable corpora and dif- 
ferent sets of 100 pairs for initial parallel 
corpora, and repeated the same test 4 more 
times. Table 2 shows results of the 5 tests of 
the bootstrapping method and the normal 
Information Mapping method. In each case the 
bootstrapping method could drastically im- 
prove both the precision and the recall. 
We also conducted tests to see if the result- 
ing text pairs obtained at the 100th stage in the 
previous tests are useful for the normal In fer  
marion Mapping method. We prepared an- 
other 1000 English-Japanese patent ranslation 
pairs for each of the 5 previous tests as 
evaluation corpora. No same patents were 
shared between any two of all the corpora. 
We extracted mutual-proximity pairs froln the 
new 1000 English-Japanese pair with the nor- 
mal Information Mapping method, using (1) 
the initial parallel corpus in the previous test, 
(2) the initial parallel corpus + the mutual- 
proximity pairs obtained in the previous test, 
(3) the initial parallel corpus + the 1000 Eng- 
lish-Japanese correct translation pairs in the 
pseudo comparable corpus of the previous test, 
as a training corpus respectively. For exam- 
ple, in Test 1, the number of pairs in the 
refining corpus is 100 for (1), 827 with 6 error 
pairs for (2) and 1100 for (3). 
the Introduction, it is highly likely that a real 
bilingual comparable corpus includes bilingual 
pairs which share the same information, but it 
also includes a lot of irrelevant texts. To 
simulate this, we replaced half of the Japanese 
patent exts in the pseudo comparable corpora 
of the previous tests with different Japanese 
patent texts which were randomly selected. 
Therefore the corpus included 500 English- 
Japanese translation pairs, and 500 English 
patents and 500 Japanese patents which were 
totally irrelevant to each other. 
100 
80 
60 
40 
20 
0 
I I I I 
~f~f l  ~ I , I rcc dll(%) , 
20 40 60 80 100 
Figure 3: The change of precision and recall 
with 50%-error-pair corpus 
lh'cC initial 
pairs Rcc 
inilail + Prcc 
boot- 
stmpping 
pmrs Rec 
initail + Prec 
complete 
pmrs Rcc 77.5 79.3 79.0 77.4 
Table 3: Results of ewduat\]on 
complete-pair corpus 
77.5 77.3 
23.8 29.3 
98.9 98.7 
74.5 75.0 
99.0 99.1 
73.3 
26.1 
98.8 
75.1 75.0 
99.6 98.7 
75.6 75.4 
25.1 25.8 
99. I 99.2 
73.5 
98.7 
78.6 
tests for 
Table 3 shows the results. The results of 
(3) can be considered as the ceilings of the 
precision aud the recall, because we used all 
the correct translation pairs in the pseudo 
comparable corpus. In each case, both the 
precision and the recall of (2) is very close to 
the ceilings, so we think the bilingual text 
pairs obtained by our bootstrapping method is 
useful as a training corpus for the normal 
Information Mapping method. 
4.2 Tests with incomplete-pair corpora 
In the tests described above, we used the ideal 
pseudo COlnparable corpus. As described in 
nolill~|\] 
mclhod 
boor 
stral~ping 
method 
Prcc 55.3 
Rcc 28.4 
l'rcc 82.1 
P, cc 69.8 
50.0 52.8 
26.8 28,0 
81.4 83.8 
70.8 67.4 
46.6 53,5 
25.(} 29.4 
81.0 80,7 
67.4 69.2 
Table 4: Results of extracting tests 
with 50%-error-pair corpus 
l'rec 77.5 77.3 73.3 75.6 initial 
pairs Rec 23.8 29.3 26. I 25. I 
initail + l'rec 96.2 95.7 93.4 93.3 
boot- 
strapping 
pairs Rec 61. I 61.9 59.8 57.4 
initail + Prec 98.4 98.7 97.9 98,0 
complete 
pan's Roe 66.1 70.7 68.6 69.0 71.1 
Table 5: Results of evaluation tests 
for 50%-error-pair corpus 
75.4 
25.8 
95.9 
60.5 
98.9 
Results are shown in Figure 3, Table 4 and 
Table5, which correspond to Figure 2, Table 2 
and Table 3 respectively. 
1069 
I(~) I I I I 
80 
60 
40 
20 
0 
+ recall (%) 
? 
20 40 60 80 100 
Figure 4: The change of precision and recall 
with 80%-error-pair corpus 
llOrlllal 
method 
boot- 
strapping 
method 
l'rcc 23.4 
Rec 27.5 
Prec 52.5 
Rec 58.0 
17.8 20.7 
20.0 24.5 
55.2 50.9 
53.0 55.O 
21.1 27.0 
23.5 30.0 
53.2 53.4 
53.5 50.2 
Table 6: Results of extracting tests 
with 80%-error-pair corpus 
initial 
pairs 
inilail + 
boot- 
strapping 
paws 
initail + 
complete 
pailS 
Table 7: Results of evaluation tests 
(5, for 80 N-error-pair corpus 
Prec 
Rcc 
Prcc 
Rcc 
Prcc 
Rcc 
77.5 
23.8 
82.9 
37.9 
96.1 
54.7 
77.3 73,3 
29.3 26.1 
85.5 81.1 
36.3 35.3 
96.4 96.0 
58.7 55.0 
75.6 
25. I 
83.5 
33.5 
94.0 
55.3 
75.4 
25.8 
85.4 
33.4 
95.7 
53.4 
Figure 4, Table 6 and Table 7 show results in 
the case that we replaced 80% of Japanese pat- 
ent texts with irrelevant Japanese patent exts. 
The results of these tests are not as good as 
the results of tests with the ideal pseudo compa- 
rable corpora. Figure 4 and 6 show, however, 
the bootstrapping method iml?roved both the 
precision and recall of the extracted text pairs as 
compared to the normal method. Figure 5 and 
7 also show that the bilingual text pairs obtained 
by the bootstrapping method are still useful as a 
training corpus for the normal method. 
5 Conclusion 
We proposed a lnethod of extracting bilingual 
text pairs from a comparable corpus. The 
method is based on an existing corpus-based 
CLIR method and uses bootstrapping. Alt- 
hough our research is in the preliminary stage of 
development and tested with artificial corpora 
consisting of English and Japanese patent exts, 
the bootstrapping led to nmch better results for 
the task of extracting translation pairs than the 
results produced by a normal CLIR method, and 
the extracted translation pairs could be useful for 
improving the results of the normal CLIR when 
we used them as a training corpus. 
References 
Berry, M. W. (1992) Large Scale Singular Vahte 
Computations. International Journal of Supercom- 
puter Applications, 6/1, pp. 13-49. 
Dunning T. E. and Davis M. W. (1993) Multi-lingual 
information retrieval. Computational Memoranda 
iri Co~aitive and Computer Science MCCS-93-252, 
New-Mexico State University, Computing Re- 
search Laboratory. 
Furnas, G. W., Dcerwester, S., Dumais, S. T., Lan- 
daucr, T. K., H~rshlnan, R. A., Streeter, L. A. and 
Lochbaum, K. E. (1988) lnfbrmation retrieval us- 
ing a singular value decomj)osition model of latel!t 
semantic structure. In proceedings of die l l th 
ACM International Conference on Research and 
Development in hfformation Retrieval, pp. 465- 
480. 
Hull, D. and Grefenstette, G. (1996) Quelying across 
languages." A dictionaty-bas'ed apRroach to mul- 
tilinettal infomtation retrieval. In Proceedings of 
SIGIR'96, \[~p. 49-57. 
Landauer, T. K. and Littman, L. M. (1990) Fully 
automatic ross-language document retrieval using 
latettt semantic indexittg. In Proceedintzs of lhe 6lli 
Conference of Univcrsi.ly ol' Wterloo Centre for the 
New Oxford English Dictionary and Text Research, 
pp. 31-38. 
Masuichi, H., Flournoy, R., Kaufinann, S. and Peters, 
S. (1999) Query, Translation Method for Cross 
Language h!forthation Retrieval. In Proc'eedinjg8 of 
the "Worksh6p on Machine Translation lor Cross 
Lantzua~ze Inlormation Retrieval, MT Summit VII, 
pp. 30-S4. 
Schfitze, H. (1995) Ambiguity Resohttion in Lcm- 
guage Learning: Computa'tional atzd Cognitive 
9 ) Z Models. \[ hD tlicsis, Stanford University, l)cpart- 
mcnt of Linguistics. 
Schi.itze, H. and Pederscn, J. (1997)A coocur-retlce- 
based thesaurus attd two al~lglications to in- 
.folT~lation retrieval. Informatmn Processing & 
management, 33/3, pp. 307-318. 
Sheridan, P., Ballerini, J. P. and Schfinble, P. (1998) 
Building a large multilingual test,, collection from 
conqmrable news documents. In 'Cross-Lanfam~c 
Information Retrieval", Kluwer Academic PuNis'fi- 
ers, pp. 137-150. 
Zanettin, F. (1994) Parallel Words: Designing a 
Bilingual Database lk~r Translation Actiwties. In 
"Corpora in Language Education and Research: a 
Selection of Papers fi'om TALC 94", Lancaster 
University, UK, pp. 99-111. 
Zanettin, F. (1998) Bilingual comp(Irable con)era 
and the training of translators. In META, XLIII, 
4, Special Issue. The corpus-based ap.proach: a new 
paradigm in translation s'tudics", pp. 616-630. 
1070 
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1133?1141,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Real-time decision detection in multi-party dialogue
Matthew Frampton, Jia Huang, Trung Huu Bui and Stanley Peters
Center for the Study of Language and Information
Stanford University
Stanford, CA, 94305, USA
{frampton|jiahuang|thbui|peters}@stanford.edu
Abstract
We describe a process for automatically
detecting decision-making sub-dialogues
in multi-party, human-human meetings in
real-time. Our basic approach to decision
detection involves distinguishing between
different utterance types based on the roles
that they play in the formulation of a de-
cision. In this paper, we describe how this
approach can be implemented in real-time,
and show that the resulting system?s per-
formance compares well with other detec-
tors, including an off-line version.
1 Introduction
In collaborative and organized work environ-
ments, people share information and make de-
cisions through multi-party conversations, com-
monly referred to as meetings. The demand for
automatic methods that process, understand and
summarize information contained in audio and
video recordings of meetings is growing rapidly,
as evidenced by on-going projects which are fo-
cused on this goal, (Waibel et al, 2003; Janin et
al., 2004). Our research is part of a general effort
to develop a system that can automatically extract
and summarize information such as conversational
topics, action items, and decisions.
This paper concerns the development of a real-
time decision detector ? a system which can de-
tect and summarize decisions as they are made
during a meeting. Such a system could provide
a summary of all of the decisions which have been
made up until the current point in the meeting,
and this is something which we expect will help
users to enjoy more productive meetings. Cer-
tainly, good decision-making relies on access to
relevant information, and decisions made earlier
in a meeting often have a bearing on the current
topic of discussion, and so form part of this rele-
vant information. However, in a long and winding
meeting, participants might not have these earlier
decisions at the forefront of their minds, and so
an accurate and succinct reminder, as provided by
a real-time decision detector, could potentially be
very useful. A record of earlier decisions could
also help users to identify outstanding issues for
discussion, and to therefore make better use of the
remainder of the meeting.
Our approach to decision detection uses an an-
notation scheme which distinguishes between dif-
ferent types of utterance based on the roles which
they play in the decision-making process. Such a
scheme facilitates the detection of decision discus-
sions (Fern?andez et al, 2008), and by indicating
which utterances contain particular types of infor-
mation, it also aids their summarization. To auto-
matically detect decision discussions, we use what
we refer to as hierarchical classification. Here, in-
dependent binary sub-classifiers detect the differ-
ent decision dialogue acts, and then based on the
sub-classifier hypotheses, a super-classifier deter-
mines which dialogue regions are decision discus-
sions. In this paper then, we address the chal-
lenges for applying this approach in real-time, and
produce a system which is able to detect decisions
soon after they are made, (for example within a
minute). We conduct tests and compare this sys-
tem?s performance with other detectors, including
an off-line equivalent.
The remainder of the paper proceeds as follows.
Section 2 describes related work, and Section 3 de-
scribes our annotation scheme for decision discus-
sions, and our experimental data. Next, Section
4 explains the hierarchical classification approach
in more detail, and Section 5 considers how it can
be applied in real-time. Section 6 describes the
experiments in which we test the real-time detec-
tor, and finally, Section 7 presents conclusions and
ideas for future work.
1133
2 Related Work
Decisions are one of the most important meet-
ing outputs. User studies (Lisowska et al, 2004;
Banerjee et al, 2005) have confirmed that meeting
participants consider this to be the case, and Whit-
taker et al (2006) found that the development of
an automatic decision detection component is crit-
ical to the re-use of meeting archives. As a result,
with the new availability of substantial meeting
corpora such as the ISL (Burger et al, 2002), ICSI
(Janin et al, 2004) and AMI (McCowan et al,
2005) Meeting Corpora, recent years have seen an
increasing amount of research on decision-making
dialogue.
This recent research has tackled issues such
as the automatic detection of agreement and dis-
agreement (Hillard et al, 2003; Galley et al,
2004), and of the level of involvement of conver-
sational participants (Wrede and Shriberg, 2003;
Gatica-Perez et al, 2005). In addition, Verbree
et al (2006) created an argumentation scheme in-
tended to support automatic production of argu-
ment structure diagrams from decision-oriented
meeting transcripts. Only very recent research has
specifically investigated the automatic detection of
decisions, namely (Hsueh and Moore, 2007) and
(Fern?andez et al, 2008).
Hsueh and Moore (2007) used the AMI Meeting
Corpus, and attempted to automatically identify
dialogue acts (DAs) in meeting transcripts which
are ?decision-related?. Within any meeting, the
authors decided which DAs were decision-related
based on two different kinds of manually created
summary: the first was an extractive summary of
the whole meeting, and the second, an abstrac-
tive summary of the decisions which were made.
Those DAs in the extractive summary which sup-
port any of the decisions in the abstractive sum-
mary were manually tagged as decision-related.
Hsueh and Moore (2007) then trained a Maxi-
mum Entropy classifier to recognize this single
DA class, using a variety of lexical, prosodic, dia-
logue act and conversational topic features. They
achieved an F-score of 0.35, which gives an indi-
cation of the difficulty of this task.
Unlike Hsueh and Moore (2007), Fern?andez
et al (2008) made an attempt at modelling the
structure of decision-making dialogue. They de-
signed an annotation scheme that takes account of
the different roles which different utterances play
in the decision-making process ? for example,
their scheme distinguishes between decision DAs
(DDAs) which initiate a discussion by raising a
topic/issue, those which propose a resolution, and
those which express agreement for a proposed res-
olution and cause it to be accepted as a decision.
The authors applied the annotation scheme to a
portion of the AMI corpus, and then took what
they refer to as a hierarchical classification ap-
proach in order to automatically identify decision
discussions and their component DAs. Here, one
binary Support Vector Machine (SVM) per DDA
class hypothesized occurrences of that DDA class,
and then based on the hypotheses of these so-
called sub-classifiers, a super-classifier, (a further
SVM), determined which regions of dialogue rep-
resented decision discussions. This approach pro-
duced better results than the kind of ?flat classi-
fication? approach pursued by Hsueh and Moore
(2007) where a single classifier looks for exam-
ples of a single decision-related DA class. Using
manual transcripts, and a variety of lexical, utter-
ance, speaker, DA and prosodic features for the
sub-classifiers, the super-classifier?s F1-score was
0.58 according to a lenient match metric. Note that
(Purver et al, 2007) had previously pursued the
same basic approach as Fern?andez et al (2008) in
order to detect action items.
While both Hsueh and Moore (2007), and
Fern?andez et al (2008) attempted off-line decision
detection, in this paper, we attempt real-time deci-
sion detection. We take the same basic approach
as Fern?andez et al (2008), and make changes to
its implementation so that it can work effectively
in real-time.
3 Data
The AMI corpus (McCowan et al, 2005), is a
freely available corpus of multi-party meetings
containing both audio and video recordings, as
well as a wide range of annotated information
including dialogue acts and topic segmentation.
Conversations are all in English, but participants
can include non-native English speakers. All of
the meetings in our sub-corpus last around 30 min-
utes, and are scenario-driven, wherein four partic-
ipants play different roles in a company?s design
team: project manager, marketing expert, inter-
face designer and industrial designer. The discus-
sions concern how to design a remote control.
We used the off-line version of the Decipher
speech recognition engine (Stolcke et al, 2008) in
1134
order to obtain off-line ASR transcripts for these
17 meetings, and the real-time version, to ob-
tain real-time ASR transcripts. Decipher gener-
ates the transcripts by first producing Word Con-
fusion Networks (WCNs) and then extracting their
best paths. The real-time recognizer generates
?live? transcripts with 5 to 15 seconds of latency
for immediate display. In processing completed
meetings, the off-line system makes seven recog-
nition passes, including acoustic adaptation and
language model rescoring, in about 4.2 times real-
time (on a 4-score 2.6 GHz Opteron server). In
general usage with multi-party dialogue, the word
error rate (WER) for the off-line version of De-
cipher is approximately 23%, and for the real-
time version, approximately 35%
1
. Stolcke et al
(2008) report a WER of 26.9% for the off-line ver-
sion on AMI meetings.
The real-time ASR transcripts for the 17 meet-
ings contain a total of 8440 utterances/dialogue
acts, (around 496 per meeting), and the off-line
ASR transcripts, 7495 utterances/dialogue acts,
(around 441 per meeting).
3.1 Modelling Decision Discussions
We use the same annotation scheme as
(Fern?andez et al, 2008) in order to model
decision-making dialogue. As stated in Section 2,
this scheme distinguishes between a small number
of dialogue act types based on the role which they
perform in the formulation of a decision. Recall
that using this scheme in conjunction with hierar-
chical classification produced better decision de-
tection than a ?flat classification? approach with a
single ?decision-related? DA class. Since it indi-
cates which utterances contain particular types of
information, such a scheme also aids the summa-
rization of decision discussions.
The annotation scheme (see Table 1 for a sum-
mary) is based on the observation that a decision
discussion contains the following main structural
components: (a) a topic or issue requiring resolu-
tion is raised, (b) one or more possible resolutions
are considered, (c) a particular resolution is agreed
upon, that is, it becomes the decision. Hence the
scheme distinguishes between three correspond-
ing decision dialogue act (DDA) classes: Issue (I),
Resolution (R), and Agreement (A). Class R is fur-
ther subdivided into Resolution Proposal (RP) and
1
This information was obtained through personal commu-
nication.
Resolution Restatement (RR). Note that an utter-
ance can be assigned to more than one of these
DDA classes, and that within a decision discus-
sion, more than one utterance may correspond to a
particular DDA class.
Here we use the sample decision discussion
below in 1 in order to provide examples of the
different DDA types. I utterances introduce the
topic of the decision discussion, examples be-
ing ?Are we going to have a backup?? and ?But
would a backup really be necessary?? On the
other hand, R utterances specify the resolution
which is ultimately adopted as the decision. RP
utterances propose this resolution (e.g. ?I think
maybe we could just go for the kinetic energy. . . ?),
while RR utterances close the discussion by con-
firming/summarizing the decision (e.g. ?Okay,
fully kinetic energy?). Finally, A utterances agree
with the proposed resolution, so causing it to be
adopted as the decision, (e.g. ?Yeah?, ?Good?
and ?Okay?.
(1) A: Are we going to have a backup?
Or we do just?
B: But would a backup really be necessary?
A: I think maybe we could just go for the
kinetic energy and be bold and innovative.
C: Yeah.
B: I think? yeah.
A: It could even be one of our selling points.
C: Yeah ?laugh?.
D: Environmentally conscious or something.
A: Yeah.
B: Okay, fully kinetic energy.
D: Good.
2
3.2 Experimental data for real-time decision
detection
Originally, two individuals used the annotation
scheme described above in order to annotate the
manual transcripts of 9 and 10 meetings respec-
tively. The annotators overlapped on two meet-
ings, and their kappa inter-annotator agreement
ranged from 0.63 to 0.73 for the four DDA classes.
The highest agreement was obtained for class RP,
and the lowest for class A. Although these kappa
values are not extremely high, if we used a single,
less homogeneous ?decision-related? DA class
like Hsueh and Moore (2007), then its kappa score
2
This example was extracted from the AMI dialogue
ES2015c and has been modified slightly for presentation pur-
poses.
1135
key DDA class description
I issue utterances introducing the issue or topic under discussion
R resolution utterances containing the resolution adopted as the decision
RP ? proposal ? utterances where the decision is originally proposed
RR ? restatement ? utterances where the decision is confirmed or restated
A agreement utterances explicitly signalling agreement with the decision
Table 1: Set of decision dialogue act (DDA) classes
would probably be significantly lower. The de-
cision discussion annotations used by Hsueh and
Moore (2007) are part of the AMI corpus, and are
for the manual transcriptions. The reader can find
a comparison between these annotations and our
own manual transcript annotations in (Fern?andez
et al, 2008).
After obtaining the new off-line and real-time
ASR transcripts, we transferred the DDA annota-
tions from the manual transcripts. In both sets of
ASR transcripts, each meeting contains on aver-
age around 26 DAs tagged with one or more of the
DDA sub-classes in Table 1. DDAs are thus very
sparse, corresponding to only 5.3% of utterances
in the real-time transcripts, and 6.0% in the off-
line. In the real-time transcripts, Issue utterances
make up less than 1.2% of the total number of ut-
terances in a meeting, while Resolution utterances
are around 1.6%: 1.2% are RP and less than 0.4%
are RR on average. Almost half of DDA utterances
(slightly over 2.6% of all utterances on average)
are tagged as belonging to class Agreement. In the
off-line transcripts, the percentages are fairly sim-
ilar: 1.6% of utterances are Issue DDAs, 2.0% are
RP, 0.5% are RR, and 2.4% are A.
We now move on to describe the hierarchical
classification approach which we use to try to au-
tomatically detect decision sub-dialogues and their
component DDAs.
4 Hierarchical Classification
Hierarchical classification is designed to exploit
the fact that within decision discussions, our
DDAs can be expected to co-occur in particular
types of patterns. It involves two different types of
classifier:
1. Sub-classifier: One independent binary sub-
classifier per DDA class classifies each utter-
ance.
2. Super-classifier: A sliding window shifts
through the meeting one utterance at a time,
and following each shift, a binary super-
classifier determines whether the region of
dialogue within the window is part of a de-
cision discussion.
In our decision detectors, the sub-classifiers run
in parallel in order to reduce processing time.
For each utterance, the sub-classifiers use fea-
tures which are derived from the properties of
that utterance in context. On the other hand,
the super-classifier?s features are the hypothesized
class labels and confidence scores for the utter-
ances within the window. In various experiments,
we have found that a suitable size for the window,
is the average length of a decision discussion in
our data in utterances. The super-classifier also
?corrects? the sub-classifiers. This means that if a
DA is classified as positive by a sub-classifier, but
does not fall within a region classified as part of
a decision discussion by the super-classifier, then
the sub-classifier?s hypothesis is changed to nega-
tive.
We now move on to consider how this basic ap-
proach to decision detection can be implemented
in a real-time system.
5 Design considerations for our real-time
system
A real-time decision detector should detect deci-
sions as soon after they are made as possible. It is
for this reason that we have set our real-time de-
tector to automatically run at frequent and regular
intervals during a meeting. An alternative would
be to give the user (a meeting participant) respon-
sibility for instructing the detector when to run.
However, a user may sometimes leave substantial
gaps between giving run commands. When this
happens, the detector will have to process a large
number of utterances in a single run, and so the
user may wait some time before being presented
with any results. In addition, giving the user re-
sponsibility for instructing the detector when to
1136
Figure 1: Decision discussion regions hypothesized by
consecutive runs overlap (D
1
to D
3
and D
2
to D
4
) and so
are merged.
run means burdening the user with an extra task to
perform during the meeting, and this goes against
the general philosophy behind the system?s devel-
opment. The system is intended to be as unobtru-
sive as possible during the meeting, and to relieve
users of tasks which distract their attention away
from the current discussion (e.g. note-taking), not
to create new tasks, however small.
Obviously, on the first occasion that the detector
runs during a meeting, it can only process ?new?
(previously unprocessed) utterances, but on sub-
sequent runs, it has the option to reprocess some
number of ?old? utterances (utterances which it
has already processed in a previous run). Cer-
tainly, the detector should reprocess some of the
most recent old utterances because it is possible
that a decision discussion straddles these utter-
ances and new utterances. However, the number of
old utterances that are reprocessed should be lim-
ited. If the meeting has lasted a while already, then
the processing of a large portion of the earlier old
utterances is likely to be redundant ? it will sim-
ply produce the same results for these utterances
as the previous run.
The fact that the real-time detector processes re-
cent old utterances means that consecutive runs
can produce hypotheses for decision discussion re-
gions which overlap, or which are duplicates. Fig-
ure 1 gives an example of the former. We deal with
overlapping hypotheses by merging them into one,
so forming a larger single decision discussion re-
gion. Figure 2 gives an example of duplicate hy-
potheses. Here, on run n, the detector hypothe-
sizes decision discussion D
1
to D
2
, and then on
run n+1, since the bounds of this original hypoth-
esis are now wholly contained within the region of
Figure 2: Consecutive runs hypothesize the same decision
discussion region D
1
to D
2
, and so one of the duplicates is
discarded.
old reprocessed utterances, the detector hypothe-
sizes a duplicate. We deal with such cases by dis-
carding the duplicate.
6 Experiments
We conducted various experiments related to real-
time decision detection, our goal being to produce
a system which:
? relative to alternative versions, detects deci-
sion discussions accurately,
? generates results for any portion of dialogue
very soon after that portion of dialogue has
ended.
The current version of our real-time detector is set
to process the same number of old and new utter-
ances on each run. Here, we refer to this value as i,
and hence on each run the system processes a total
of 2i utterances (i old and i new). Another of the
system?s characteristics is that runs take place ev-
ery i utterances, meaning that as we decrease i, the
system provides new results more frequently and
is hence ?more real-time?. One of the things we
investigate here then, is what to set i to in order
to best satisfy the two design goals given above.
Having found this value, we compare the hierar-
chical real-time detector?s performance with alter-
native detectors, these being:
? an off-line detector applied to off-line ASR
transcripts,
? a flat real-time detector,
? an off-line detector applied to the real-time
ASR transcripts.
1137
Lexical unigrams after text normalization
Utterance length in words, duration in
word rate
Speaker speaker ID & AMI speaker role
Context features as above for utterances
u +/- 1. . .u +/- 5
Table 2: Features for decision DA detection
Note that the off-line detectors use hierarchical
classification, and that the flat real-time detec-
tor uses a single binary classifier which treats all
DDAs as members of a single merged DDA class.
6.1 Classifiers and features
All classifiers (sub and super-classifiers) in all de-
tectors are linear-kernel Support Vector Machines
(SVMs), produced using SVMlight (Joachims,
1999). For the sub-classifiers, we are obviously re-
stricted to using features which can be computed
in a very short period of time, and in the experi-
ments here, we use lexical, utterance and speaker
features. These are summarized in Table 2. An
utterance?s lexical features are the words in its
transcription, its utterance features are its dura-
tion, number of words, and word rate (number of
words divided by duration), and its speaker fea-
tures are the speaker?s role (see Section 3) and ID.
We also use lexical features for the previous and
where available, next utterances: the I, RP and RR
sub-classifiers use the lexical features for the pre-
vious/next utterance and the A sub-classifier, those
from the previous/next 5 utterances. These set-
tings produced the best results in preliminary ex-
periments. We do not use DA features because
we lack an automatic DA tagger, nor do we use
prosodic features because (Fern?andez et al, 2008)
was unable to derive any value from them with
SVMs.
6.2 Evaluation
We evaluate each of our decision detectors in 17-
fold cross validations, where in each fold, the de-
tector trains on 16 meetings and then tests on the
remaining one. Evaluation can be made at three
levels:
1. The sub-classifiers? detection of each of the
DDA classes.
2. The sub-classifiers? detection of each of the
DDA classes after correction by the super-
classifier.
Figure 3: The relationship between the number of old/new
utterances processed in a single run, and the super-classifier?s
F1-score. Here the sub-classifiers use only lexical features.
3. The super-classifier?s detection of decision
discussion regions.
For 1 and 2, we use the same lenient-match met-
ric as (Fern?andez et al, 2008; Hsueh and Moore,
2007), which allows a margin of 20 seconds pre-
ceding and following a hypothesized DDA. Note
that here we only give credit for hypotheses based
on a 1-1 mapping with the gold-standard labels.
For 3, we follow (Fern?andez et al, 2008; Purver et
al., 2007) and use a windowed metric that divides
the dialogue into 30-second windows and evalu-
ates on a per window basis.
6.3 Results and analysis
Here, Section 6.3.1 will present results for differ-
ent values of i, the number of old/new utterances
processed in a single run. Section 6.3.2 then com-
pares the performance of the real-time and off-line
systems, (and also real-time systems which use hi-
erarchical vs. flat classification), and Section 6.3.3
presents some feature analysis.
6.3.1 Varying the number of old/new
utterances processed in a run
Figure 3 shows the relationship between i, the set-
ting for the number of old/new utterances pro-
cessed in a single run, and the super-classifier?s
F1-score. Here, the sub-classifiers are using only
lexical features. We can see from the graph that
as i increases to 15, the super-classifier?s F1-score
also increases, but thereafter, it plateaus. Hence
15 is apparently the value which best satisfies the
two design goals given at the start of Section 6.
It should also be noted that 15 is the mean length
of a decision discussion in our data, and so per-
1138
sub-classifiers super
I RP RR A classifier
Re .73 .73 .84 .71 .82
Pr .08 .09 .03 .15 .40
F1 .15 .16 .06 .25 .54
Table 3: Results for the hierarchical real-time
decision detector, using lexical, utterance and
speaker features.
sub-classifiers super
I RP RR A classifier
Re .51 .51 .10 .63 .83
Pr .12 .11 .04 .15 .41
F1 .19 .19 .05 .24 .55
Table 4: Results for the hierarchical off-line de-
cision detector on off-line ASR transcripts, using
lexical, utterance and speaker features.
haps this is a transferable finding. The mean du-
ration of a run when i = 15 is approximately 4
seconds, while the mean duration of 15 utterances
in our data-set is approximately 60 seconds, mean-
ing that for the average case, the detector returns
the results for the current run, long before it is
due to make the next. Significant lee-way is per-
haps necessary here, because the final version of
the real-time detector will include a summariza-
tion component which extracts key phrases from
Issue/Resolution utterances, and its processing can
last some time, even for a single decision.
We should say then, that the system is not
strictly real-time because in general, it detects de-
cisions soon after they are made (for example
within a minute), rather than immediately after. In
the future we intend to modify the system so that
it can run more frequently than once every i ut-
terances. However it is important that runs do not
occur too frequently ? for example, if i = 15 and
the system runs after every utterance, then the ex-
tra processing will cause it to gradually fall further
and further behind the meeting.
6.3.2 Real-time vs. off-line results
Table 3 shows the results achieved by a hierarchi-
cal real-time decision detector whose run settings
are as described above, and whose sub-classifiers
3
use lexical, utterance and speaker features. These
results compare well with those of an equivalent
3
In Tables 3 to 6, sub-classifier results are post-correction
(see Section 6.2).
sub-classifiers super
I RP RR A classifier
Re .50 .51 .09 .63 .83
Pr .11 .11 .03 .14 .41
F1 .19 .18 .05 .23 .55
Table 5: Results for the hierarchical off-line de-
tector on real-time ASR transcripts, using lexical,
utterance and speaker features.
sub-classifiers super
I RP RR A classifier
Re .67 .74 .84 .66 .85
Pr .07 .08 .03 .14 .41
F1 .13 .15 .05 .24 .55
Table 6: Results for the hierarchical real-time de-
cision detector, using lexical features only.
off-line detector, which are shown in Table 4. The
F1-scores for the real-time and off-line decision
super-classifiers are .54 and .55 respectively, and
the difference is not statistically significant. This
may indicate that the hierarchical classification ap-
proach is fairly robust to increasing ASR Word
Error Rates (WERs). Combining the output from
each of the independent sub-classifiers might com-
pensate somewhat for any decreases in their indi-
vidual accuracy, as there was here for the I and RP
sub-classifiers.
The hierarchical real-time detector?s F1-score is
also 10 points higher than a flat classifier (.54 vs.
.44). Hence, while Fern?andez et al (2008) demon-
strated that the hierarchical classification approach
could improve off-line decision detection, we have
demonstrated here that it can also improve real-
time decision detection.
Table 5 shows the results when an off-line
detector is applied to real-time ASR transcripts.
Here, the super-classifier obtains an F1-score of
.55, one point higher than the real-time detector,
but again, the difference is not statistically signifi-
cant.
6.3.3 Feature analysis
We also investigated the contribution of the ut-
terance and speaker features. Table 6 shows the
results for the hierarchical real-time decision de-
tector when its sub-classifiers use only lexical fea-
tures. The sub-classifier F1-scores are all slightly
lower than when utterance and speaker features
are used (see Table 3), and the super-classifier
1139
score is only 1 point different. None of these dif-
ferences are statistically significant.
Since lexical features are important, we used in-
formation gain in order to investigate which words
are predictive of each DDA type. Due to differ-
ences in the transcripts, the predictive words for
the off-line and real-time systems are not the same,
but we can find commonalities, and these com-
monalities make sense given the DDA definitions.
Firstly in Resolution and particularly Issue DAs,
some of the most predictive words could be used
to define discussion topics, and so we might ex-
pect to find them in the meeting agenda. Exam-
ples are ?energy?, and ?color?. Predictive words
for Resolutions also include semantically-related
words which are key in defining the decision (?ki-
netic?,?green?). Additional predictive words for
RPs are the personal pronouns ?I? and ?we?,
and the verbs, ?think? and ?like?, and for RRs,
words which we would associate with summing
up (?consensus?, ?definitely?, and ?okay?). Un-
surprisingly, for Agreements, ?yeah? and ?okay?
both score very highly.
7 Conclusion
(Fern?andez et al, 2008) described an approach
to decision detection in multi-party meetings and
demonstrated how it could work relatively well in
an off-line system. The approach has two defining
characteristics. The first is its use of an annota-
tion scheme which distinguishes between differ-
ent utterance types based on the roles which they
play in the decision-making process. The second
is its use of hierarchical classification, whereby
binary sub-classifiers detect instances of each of
the decision DAs (DDAs), and then based on the
sub-classifier hypotheses, a super-classifier deter-
mines which regions of dialogue are decision dis-
cussions.
In this paper then, we have taken the same ba-
sic approach to decision detection as Fern?andez et
al. (2008), but changed the way in which it is im-
plemented so that it can work effectively in real-
time. Our implementation changes include run-
ning the detector at regular and frequent intervals
during the meeting, and reprocessing recent utter-
ances in case a decision discussion straddles these
and brand new utterances. The fact that the de-
tector reprocesses utterances means that on con-
secutive runs, overlapping and duplicate hypothe-
sized decision discussions are possible. We have
therefore added facilities to merge overlapping hy-
potheses and to remove duplicates.
In general, the resulting system is able to detect
decisions soon after they are made (for example
within a minute), rather than immediately after. It
has performed well in testing, achieving an F1-
score of .54, which is only one point lower than
an equivalent off-line system, and in any case, the
difference was not statistically significant. A flat
real-time detector achieved .44.
In future work, we plan to extend the decision
discussion annotation scheme and try to extract
supporting arguments for decisions. We will also
experiment with using sequential models in order
to try to exploit any sequential ordering patterns in
the occurrence of the DDAs.
Acknowledgements This material is based
upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Con-
tract No. FA8750-07-D-0185/0004, and by the
Department of the Navy Office of Naval Research
(ONR) under Grants No. N00014-05-1-0187 and
N00014-09-1-0106. Any opinions, findings and
conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of DARPA or ONR.
We are grateful to the three anonymous EMNLP
reviewers for their helpful comments and sugges-
tions, and to our partners at SRI International who
provided us with off-line and real-time transcripts
for our meeting data.
References
Satanjeev Banerjee, Carolyn Ros?e, and Alex Rudnicky.
2005. The necessity of a meeting recording and
playback system, and the benefit of topic-level anno-
tations to meeting browsing. In Proceedings of the
10th International Conference on Human-Computer
Interaction.
Susanne Burger, Victoria MacLaren, and Hua Yu.
2002. The ISL Meeting Corpus: The impact of
meeting type on speech style. In Proceedings of the
7th International Conference on Spoken Language
Processing (INTERSPEECH - ICSLP), Denver, Col-
orado.
Raquel Fern?andez, Matthew Frampton, Patrick Ehlen,
Matthew Purver, and Stanley Peters. 2008. Mod-
elling and detecting decisions in multi-party dia-
logue. In Proc. of the 9th SIGdial Workshop on Dis-
course and Dialogue.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
1140
ment and disagreement in conversational speech:
Use of Bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Daniel Gatica-Perez, Ian McCowan, Dong Zhang, and
Samy Bengio. 2005. Detecting group interest level
in meetings. In IEEE International Conference on
Acoustics, Speech, and Signal Processing (ICASSP).
Dustin Hillard, Mari Ostendorf, and Elisabeth
Shriberg. 2003. Detection of agreement vs. dis-
agreement in meetings: Training with unlabeled
data. In Companion Volume of the Proceedings of
HLT-NAACL 2003 - Short Papers, Edmonton, Al-
berta, May.
Pey-Yun Hsueh and Johanna Moore. 2007. Automatic
decision detection in meeting speech. In Proceed-
ings of MLMI 2007, Lecture Notes in Computer Sci-
ence. Springer-Verlag.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip
Dhillon, Jane Edwards, Javier Marc??as-Guarasa,
Nelson Morgan, Barbara Peskin, Elizabeth Shriberg,
Andreas Stolcke, Chuck Wooters, and Britta Wrede.
2004. The ICSI meeting project: Resources and re-
search. In Proceedings of the 2004 ICASSP NIST
Meeting Recognition Workshop.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Sch?olkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods ?
Support Vector Learning. MIT Press.
Agnes Lisowska, Andrei Popescu-Belis, and Susan
Armstrong. 2004. User query analysis for the spec-
ification and evaluation of a dialogue processing and
retrieval system. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Eval-
uation.
Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,
S. Bourban, M. Flynn, M. Guillemot, T. Hain,
J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,
M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of Measuring Behavior, the 5th Inter-
national Conference on Methods and Techniques in
Behavioral Research, Wageningen, Netherlands.
Matthew Purver, John Dowding, John Niekrasz,
Patrick Ehlen, Sharareh Noorbaloochi, and Stanley
Peters. 2007. Detecting and summarizing action
items in multi-party dialogue. In Proceedings of the
8th SIGdial Workshop on Discourse and Dialogue,
Antwerp, Belgium.
Andreas Stolcke, Xavier Anguera, Kofi Boakye,
?
Ozg?ur
C?etin, Adam Janin, Matthew Magimai-Doss, Chuck
Wooters, and Jing Zheng. 2008. The ICSI-SRI
Spring 2007 meeting and lecture recognition system.
In Proc. of CLEAR 2007 and RT2007.
Daan Verbree, Rutger Rienks, and Dirk Heylen. 2006.
First steps towards the automatic construction of
argument-diagrams from real discussions. In Pro-
ceedings of the 1st International Conference on
Computational Models of Argument, volume 144,
pages 183?194. IOS press.
A. Waibel, T. Schultz, M. Bett, M. Denecke, R. Malkin,
I. Rogina, R. Stiefelhagen, and J. Yang. 2003.
SMaRT: The smart meeting room task at ISL. In
ICASSP.
Steve Whittaker, Rachel Laban, and Simon Tucker.
2006. Analysing meeting records: An ethnographic
study and technological implications. In S. Renals
and S. Bengio, editors, Machine Learning for Multi-
modal Interaction: Second International Workshop,
MLMI 2005, Revised Selected Papers, volume 3869
of Lecture Notes in Computer Science, pages 101?
113. Springer.
Britta Wrede and Elizabeth Shriberg. 2003. Spotting
?hot spots? in meetings: Human judgements and
prosodic cues. In Proceedings of the 9th European
Conference on Speech Communication and Technol-
ogy, Geneva, Switzerland.
1141
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 273?281,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Who is ?You?? Combining Linguistic and Gaze Features to Resolve
Second-Person References in Dialogue?
Matthew Frampton1, Raquel Ferna?ndez1, Patrick Ehlen1, Mario Christoudias2,
Trevor Darrell2 and Stanley Peters1
1Center for the Study of Language and Information, Stanford University
{frampton, raquelfr, ehlen, peters}@stanford.edu
2International Computer Science Institute, University of California at Berkeley
cmch@icsi.berkeley.edu, trevor@eecs.berkeley.edu
Abstract
We explore the problem of resolving the
second person English pronoun you in
multi-party dialogue, using a combination
of linguistic and visual features. First, we
distinguish generic and referential uses,
then we classify the referential uses as ei-
ther plural or singular, and finally, for the
latter cases, we identify the addressee. In
our first set of experiments, the linguistic
and visual features are derived from man-
ual transcriptions and annotations, but in
the second set, they are generated through
entirely automatic means. Results show
that a multimodal system is often prefer-
able to a unimodal one.
1 Introduction
The English pronoun you is the second most fre-
quent word in unrestricted conversation (after I
and right before it).1 Despite this, with the ex-
ception of Gupta et al (2007b; 2007a), its re-
solution has received very little attention in the lit-
erature. This is perhaps not surprising since the
vast amount of work on anaphora and reference
resolution has focused on text or discourse - medi-
ums where second-person deixis is perhaps not
as prominent as it is in dialogue. For spoken di-
alogue pronoun resolution modules however, re-
solving you is an essential task that has an impor-
tant impact on the capabilities of dialogue summa-
rization systems.
?We thank the anonymous EACL reviewers, and Surabhi
Gupta, John Niekrasz and David Demirdjian for their com-
ments and technical assistance. This work was supported by
the CALO project (DARPA grant NBCH-D-03-0010).
1See e.g. http://www.kilgarriff.co.uk/BNC_lists/
Besides being important for computational im-
plementations, resolving you is also an interesting
and challenging research problem. As for third
person pronouns such as it, some uses of you are
not strictly referential. These include discourse
marker uses such as you know in example (1), and
generic uses like (2), where you does not refer to
the addressee as it does in (3).
(1) It?s not just, you know, noises like something
hitting.
(2) Often, you need to know specific button se-
quences to get certain functionalities done.
(3) I think it?s good. You?ve done a good review.
However, unlike it, you is ambiguous between sin-
gular and plural interpretations - an issue that is
particularly problematic in multi-party conversa-
tions. While you clearly has a plural referent in
(4), in (3) the number of its referent is ambigu-
ous.2
(4) I don?t know if you guys have any questions.
When an utterance contains a singular referen-
tial you, resolving the you amounts to identifying
the individual to whom the utterance is addressed.
This is trivial in two-person dialogue since the cur-
rent listener is always the addressee, but in conver-
sations with multiple participants, it is a complex
problem where different kinds of linguistic and vi-
sual information play important roles (Jovanovic,
2007). One of the issues we investigate here is
2In contrast, the referential use of the pronoun it (as well
as that of some demonstratives) is ambiguous between NP
interpretations and discourse-deictic ones (Webber, 1991).
273
how this applies to the more concrete problem of
resolving the second person pronoun you.
We approach this issue as a three-step prob-
lem. Using the AMI Meeting Corpus (McCowan
et al, 2005) of multi-party dialogues, we first dis-
criminate between referential and generic uses of
you. Then, within the referential uses, we dis-
tinguish between singular and plural, and finally,
we resolve the singular referential instances by
identifying the intended addressee. We use multi-
modal features: initially, we extract discourse fea-
tures from manual transcriptions and use visual in-
formation derived from manual annotations, but
then we move to a fully automatic approach, us-
ing 1-best transcriptions produced by an automatic
speech recognizer (ASR) and visual features auto-
matically extracted from raw video.
In the next section of this paper, we give a brief
overview of related work. We describe our data in
Section 3, and explain how we extract visual and
linguistic features in Sections 4 and 5 respectively.
Section 6 then presents our experiments with man-
ual transcriptions and annotations, while Section
7, those with automatically extracted information.
We end with conclusions in Section 8.
2 Related Work
2.1 Reference Resolution in Dialogue
Although the vast majority of work on reference
resolution has been with monologic text, some re-
cent research has dealt with the more complex
scenario of spoken dialogue (Strube and Mu?ller,
2003; Byron, 2004; Arstein and Poesio, 2006;
Mu?ller, 2007). There has been work on the iden-
tification of non-referential uses of the pronoun it:
Mu?ller (2006) uses a set of shallow features au-
tomatically extracted from manual transcripts of
two-party dialogue in order to train a rule-based
classifier, and achieves an F-score of 69%.
The only existing work on the resolution of you
that we are aware of is Gupta et al (2007b; 2007a).
In line with our approach, the authors first disam-
biguate between generic and referential you, and
then attempt to resolve the reference of the ref-
erential cases. Generic uses of you account for
47% of their data set, and for the generic vs. ref-
erential disambiguation, they achieve an accuracy
of 84% on two-party conversations and 75% on
multi-party dialogue. For the reference resolution
task, they achieve 47%, which is 10 points over
a baseline that always classifies the next speaker
as the addressee. These results are achieved with-
out visual information, using manual transcripts,
and a combination of surface features and manu-
ally tagged dialogue acts.
2.2 Addressee Detection
Resolving the referential instances of you amounts
to determining the addressee(s) of the utterance
containing the pronoun. Recent years have seen
an increasing amount of research on automatic
addressee detection. Much of this work focuses
on communication between humans and computa-
tional agents (such as robots or ubiquitous com-
puting systems) that interact with users who may
be engaged in other activities, including interac-
tion with other humans. In these situations, it
is important for a system to be able to recognize
when it is being addressed by a user. Bakx et
al. (2003) and Turnhout et al (2005) studied this
issue in the context of mixed human-human and
human-computer interaction using facial orienta-
tion and utterance length as clues for addressee
detection, while Katzenmaier et al (2004) inves-
tigated whether the degree to which a user utter-
ance fits the language model of a conversational
robot can be useful in detecting system-addressed
utterances. This research exploits the fact that hu-
mans tend to speak differently to systems than to
other humans.
Our research is closer to that of Jovanovic
et al (2006a; 2007), who studied addressing in
human-human multi-party dialogue. Jovanovic
and colleagues focus on addressee identification in
face-to-face meetings with four participants. They
use a Bayesian Network classifier trained on sev-
eral multimodal features (including visual features
such as gaze direction, discourse features such as
the speaker and dialogue act of preceding utter-
ances, and utterance features such as lexical clues
and utterance duration). Using a combination of
features from various resources was found to im-
prove performance (the best system achieves an
accuracy of 77% on a portion of the AMI Meeting
Corpus). Although this result is very encouraging,
it is achieved with the use of manually produced
information - in particular, manual transcriptions,
dialogue acts and annotations of visual focus of at-
tention. One of the issues we aim to investigate
here is how automatically extracted multimodal
information can help in detecting the addressee(s)
of you-utterances.
274
Generic Referential Ref Sing. Ref Pl.
49.14% 50.86% 67.92% 32.08%
Table 1: Distribution of you interpretations
3 Data
Our experiments are performed using the AMI
Meeting Corpus (McCowan et al, 2005), a collec-
tion of scenario-driven meetings among four par-
ticipants, manually transcribed and annotated with
several different types of information (including
dialogue acts, topics, visual focus of attention, and
addressee). We use a sub-corpus of 948 utterances
containing you, and these were extracted from 10
different meetings. The you-utterances are anno-
tated as either discourse marker, generic or refer-
ential.
We excluded the discourse marker cases, which
account for only 8% of the data, and of the refer-
ential cases, selected those with an AMI addressee
annotation.3 The addressee of a dialogue act can
be unknown, a single meeting participant, two
participants, or the whole audience (three partici-
pants in the AMI corpus). Since there are very few
instances of two-participant addressee, we distin-
guish only between singular and plural addressees.
The resulting distribution of classes is shown in
Table 1.4
We approach the reference resolution task as a
two-step process, first discriminating between plu-
ral and singular references, and then resolving the
reference of the singular cases. The latter task re-
quires a classification scheme for distinguishing
between the three potential addressees (listeners)
for the given you-utterance.
In their four-way classification scheme,
Gupta et al (2007a) label potential addressees in
terms of the order in which they speak after the
you-utterance. That is, for a given you-utterance,
the potential addressee who speaks next is labeled
1, the potential addressee who speaks after that is
2, and the remaining participant is 3. Label 4 is
used for group addressing. However, this results
in a very skewed class distribution because the
next speaker is the intended addressee 41% of
the time, and 38% of instances are plural - the
3Addressee annotations are not provided for some dia-
logue act types - see (Jovanovic et al, 2006b).
4Note that the percentages of the referential singular and
referential plural are relative to the total of referential in-
stances.
L1 L2 L3
35.17% 30.34% 34.49%
Table 2: Distribution of addressees for singular you
remaining two classes therefore make up a small
percentage of the data.
We were able to obtain a much less skewed class
distribution by identifying the potential addressees
in terms of their position in relation to the current
speaker. The meeting setting includes a rectangu-
lar table with two participants seated at each of
its opposite longer sides. Thus, for a given you-
utterance, we label listeners as either L1, L2 or
L3 depending on whether they are sitting opposite,
diagonally or laterally from the speaker. Table 2
shows the resulting class distribution for our data-
set. Such a labelling scheme is more similar to Jo-
vanovic (2007), where participants are identified
by their seating position.
4 Visual Information
4.1 Features from Manual Annotations
We derived per-utterance visual features from the
Focus Of Attention (FOA) annotations provided
by the AMI corpus. These annotations track meet-
ing participants? head orientation and eye gaze
during a meeting.5 Our first step was to use the
FOA annotations in order to compute what we re-
fer to as Gaze Duration Proportion (GDP) values
for each of the utterances of interest - a measure
similar to the ?Degree of Mean Duration of Gaze?
described by (Takemae et al, 2004). Here a GDP
value denotes the proportion of time in utterance u
for which subject i is looking at target j:
GDPu(i, j) =
?
j
T (i, j)/Tu
were Tu is the length of utterance u in millisec-
onds, and T (i, j), the amount of that time that i
spends looking at j. The gazer i can only refer to
one of the four meeting participants, but the tar-
get j can also refer to the white-board/projector
screen present in the meeting room. For each utter-
ance then, all of the possible values of i and j are
used to construct a matrix of GDP values. From
this matrix, we then construct ?Highest GDP? fea-
tures for each of the meeting participants: such
5A description of the FOA labeling scheme is avail-
able from the AMI Meeting Corpus website http://corpus.
amiproject.org/documentations/guidelines-1/
275
For each participant Pi
? target for whole utterance
? target for first third of utterance
? target for second third of utterance
? target for third third of utterance
? target for -/+ 2 secs from you start time
? ratio 2nd hyp. target / 1st hyp. target
? ratio 3rd hyp. target / 1st hyp. target
? participant in mutual gaze with speaker
Table 3: Visual Features
features record the target with the highest GDP
value and so indicate whom/what the meeting par-
ticipant spent most time looking at during the ut-
terance.
We also generated a number of additional fea-
tures for each individual. These include firstly,
three features which record the candidate ?gazee?
with the highest GDP during each third of the ut-
terance, and which therefore account for gaze tran-
sitions. So as to focus more closely on where par-
ticipants are looking around the time when you
is uttered, another feature records the candidate
with the highest GDP -/+ 2 seconds from the start
time of the you. Two further features give some
indication of the amount of looking around that
the speaker does during an utterance - we hypoth-
esized that participants (especially the speaker)
might look around more in utterances with plu-
ral addressees. The first is the ratio of the sec-
ond highest GDP to the highest, and the second
is the ratio of the third highest to the highest. Fi-
nally, there is a highest GDP mutual gaze feature
for the speaker, indicating with which other indi-
vidual, the speaker spent most time engaged in a
mutual gaze.
Hence this gives a total of 29 features: seven
features for each of the four participants, plus one
mutual gaze feature. They are summarized in Ta-
ble 3. These visual features are different to those
used by Jovanovic (2007) (see Section 2). Jo-
vanovic?s features record the number of times that
each participant looks at each other participant
during the utterance, and in addition, the gaze di-
rection of the current speaker. Hence, they are not
highest GDP values, they do not include a mutual
gaze feature and they do not record whether par-
ticipants look at the white-board/projector screen.
4.2 Automatic Features from Raw Video
To perform automatic visual feature extraction, a
six degree-of-freedom head tracker was run over
each subject?s video sequence for the utterances
containing you. For each utterance, this gave 4 se-
quences, one per subject, of the subject?s 3D head
orientation and location at each video frame along
with 3D head rotational velocities. From these
measurements we computed two types of visual
information: participant gaze and mutual gaze.
The 3D head orientation and location of each
subject along with camera calibration information
was used to compute participant gaze information
for each video frame of each sequence in the form
of a gaze probability matrix. More precisely, cam-
era calibration is first used to estimate the 3D head
orientation and location of all subjects in the same
world coordinate system.
The gaze probability matrix is a 4 ? 5 matrix
where entry i, j stores the probability that subject
i is looking at subject j for each of the four sub-
jects and the last column corresponds to the white-
board/projector screen (i.e., entry i, j where j = 5
is the probability that subject i is looking at the
screen). Gaze probability G(i, j) is defined as
G(i, j) = G0e
??i,j2/?2
where ?i,j is the angular difference between the
gaze of subject i and the direction defined by the
location of subjects i and j. G0 is a normalization
factor such that
?
j G(i, j) = 1 and ? is a user-
defined constant (in our experiments, we chose
? = 15 degrees).
Using the gaze probability matrix, a 4 ? 1 per-
frame mutual gaze vector was computed that for
entry i stores the probability that the speaker and
subject i are looking at one another.
In order to create features equivalent to those
described in Section 4.1, we first collapse the
frame-level probability matrix into a matrix of bi-
nary values. We convert the probability for each
frame into a binary judgement of whether subject
i is looking at target j:
H(i, j) = ?G(i, j)
? is a binary value to evaluate G(i, j) > ?, where
? is a high-pass thresholding value - or ?gaze prob-
ability threshold? (GPT) - between 0 and 1.
Once we have a frame-level matrix of binary
values, for each subject i, we compute GDP val-
ues for the time periods of interest, and in each
case, choose the target with the highest GDP as the
candidate. Hence, we compute a candidate target
for the utterance overall, for each third of the ut-
terance, and for the period -/+ 2 seconds from the
276
you start time, and in addition, we compute a can-
didate participant for mutual gaze with the speaker
for the utterance overall.
We sought to use the GPT threshold which pro-
duces automatic visual features that agree best
with the features derived from the FOA annota-
tions. Hence we experimented with different GPT
values in increments of 0.1, and compared the re-
sulting features to the manual features using the
kappa statistic. A threshold of 0.6 gave the best
kappa scores, which ranged from 20% to 44%.6
5 Linguistic Information
Our set of discourse features is a simplified ver-
sion of those employed by Galley et al (2004) and
Gupta et al (2007a). It contains three main types
(summarized in Table 4):
? Sentential features (1 to 13) encode structural,
durational, lexical and shallow syntactic patterns
of the you-utterance. Feature 13 is extracted us-
ing the AMI ?Named Entity? annotations and in-
dicates whether a particular participant is men-
tioned in the you-utterance. Apart from this fea-
ture, all other sentential features are automatically
extracted, and besides 1, 8, 9, and 10, they are all
binary.
? Backward Looking (BL)/Forward Looking (FL)
features (14 to 22) are mostly extracted from ut-
terance pairs, namely the you-utterance and the
BL/FL (previous/next) utterance by each listener
Li (potential addressee). We also include a few
extra features which are not computed in terms of
utterance pairs. These indicate the number of par-
ticipants that speak during the previous and next 5
utterances, and the BL and FL speaker order. All
of these features are computed automatically.
? Dialogue Act (DA) features (23 to 24) use the
manual AMI dialogue act annotations to represent
the conversational function of the you-utterance
and the BL/FL utterance by each potential ad-
dressee. Along with the sentential feature based
on the AMI Named Entity annotations, these are
the only discourse features which are not com-
puted automatically. 7
6The fact that our gaze estimator is getting any useful
agreement with respect to these annotations is encouraging
and suggests that an improved tracker and/or one that adapts
to the user more effectively could work very well.
7Since we use the manual transcripts of the meetings, the
transcribed words and the segmentation into utterances or di-
alogue acts are of course not given automatically. A fully
automatic approach would involve using ASR output instead
of manual transcriptions? something which we attempt in
(1) # of you pronouns
(2) you (say|said|tell|told| mention(ed)|mean(t)|
sound(ed))
(3) auxiliary you
(4) wh-word you
(5) you guys
(6) if you
(7) you know
(8) # of words in you-utterance
(9) duration of you-utterance
(10) speech rate of you-utterance
(11) 1st person
(12) general case
(13) person Named Entity tag
(14) # of utterances between you- and BL/FL utt.
(15) # of speakers between you- and BL/FL utt.
(16) overlap between you- and BL/FL utt. (binary)
(17) duration of overlap between you- and BL/FL utt.
(18) time separation between you- and BL/FL utt.
(19) ratio of words in you- that are in BL/FL utt.
(20) # of participants that speak during prev. 5 utt.
(21) # of participants that speak during next 5 utt.
(22) speaker order BL/FL
(23) dialogue act of the you-utterance
(24) dialogue act of the BL/FL utterance
Table 4: Discourse Features
6 First Set of Experiments & Results
In this section we report our experiments and re-
sults when using manual transcriptions and anno-
tations. In Section 7 we will present the results
obtained using ASR output and automatically ex-
tracted visual information. All experiments (here
and in the next section) are performed using a
Bayesian Network classifier with 10-fold cross-
validation.8 In each task, we give raw overall ac-
curacy results and then F-scores for each of the
classes. We computed measures of information
gain in order to assess the predictive power of the
various features, and did some experimentation
with Correlation-based Feature Selection (CFS)
(Hall, 2000).
6.1 Generic vs. Referential Uses of You
We first address the task of distinguishing between
generic and referential uses of you.
Baseline. A majority class baseline that classi-
fies all instances of you as referential yields an ac-
curacy of 50.86% (see Table 1).
Results. A summary of the results is given in Ta-
ble 5. Using discourse features only we achieve
an accuracy of 77.77%, while using multimodal
Section 7.
8We use the the BayesNet classifier implemented in the
Weka toolkit http://www.cs.waikato.ac.nz/ml/weka/.
277
Features Acc F1-Gen F1-Ref
Baseline 50.86 0 67.4
Discourse 77.77 78.8 76.6
Visual 60.32 64.2 55.5
MM 79.02 80.2 77.7
Dis w/o FL 78.34 79.1 77.5
MM w/o FL 78.22 79.0 77.4
Dis w/o DA 69.44 71.5 67.0
MM w/o DA 72.75 74.4 70.9
Table 5: Generic vs. referential uses
(MM) yields 79.02%, but this increase is not sta-
tistically significant.
In spite of this, visual features do help to dis-
tinguish between generic and referential uses -
note that the visual features alone are able to beat
the baseline (p < .005). The listeners? gaze is
more predictive than the speaker?s: if listeners
look mostly at the white-board/projector screen in-
stead of another participant, then the you is more
likely to be referential. More will be said on this
in Section 6.2.1 in the analysis of the results for
the singular vs. plural referential task.
We found sentential features of the you-
utterance to be amongst the best predictors, es-
pecially those that refer to surface lexical proper-
ties, such as features 1, 11, 12 and 13 in Table 4.
Dialogue act features provide useful information
as well. As pointed out by Gupta et al (2007b;
2007a), a you pronoun within a question (e.g.
an utterance tagged as elicit-assess or
elicit-inform) is more likely to be referen-
tial. Eliminating information about dialogue acts
(w/o DA) brings down performance (p < .005),
although accuracy remains well above the baseline
(p < .001). Note that the small changes in perfor-
mance when FL information is taken out (w/o FL)
are not statistically significant.
6.2 Reference Resolution
We now turn to the referential instances of you,
which can be resolved by determining the ad-
dressee(s) of the given utterance.
6.2.1 Singular vs. Plural Reference
We start by trying to discriminate singular vs. plu-
ral interpretations. For this, we use a two-way
classification scheme that distinguishes between
individual and group addressing. To our knowl-
edge, this is the first attempt at this task using lin-
guistic information.9
9But see e.g. (Takemae et al, 2004) for an approach that
uses manually extracted visual-only clues with similar aims.
Baseline. A majority class baseline that consid-
ers all instances of you as referring to an individual
addressee gives 67.92% accuracy (see Table 1).
Results. A summary of the results is shown in
Table 6. There is no statistically significant differ-
ence between the baseline and the results obtained
when visual features are used alone (67.92% vs.
66.28%). However, we found that visual informa-
tion did contribute to identifying some instances of
plural addressing, as shown by the F-score for that
class. Furthermore, the visual features helped to
improve results when combined with discourse in-
formation: using multimodal (MM) features pro-
duces higher results than the discourse-only fea-
ture set (p < .005), and increases from 74.24% to
77.05% with CFS.
As in the generic vs. referential task, the white-
board/projector screen value for the listeners? gaze
features seems to have discriminative power -
when listeners? gaze features take this value, it is
often indicative of a plural rather than a singular
you. It seems then, that in our data-set, the speaker
often uses the white-board/projector screen when
addressing the group, and hence draws the listen-
ers? gaze in this direction. We should also note
that the ratio features which we thought might be
useful here (see Section 4.1) did not prove so.
Amongst the most useful discourse features
are those that encode similarity relations between
the you-utterance and an utterance by a potential
addressee. Utterances by individual addressees
tend to be more lexically cohesive with the you-
utterance and so if features such as feature 19 in
Table 4 indicate a low level of lexical similarity,
then this increases the likelihood of plural address-
ing. Sentential features that refer to surface lexical
patterns (features 6, 7, 11 and 12) also contribute
to improved results, as does feature 21 (number of
speakers during the next five utterances) - fewer
speaker changes correlates with plural addressing.
Information about dialogue acts also plays a
role in distinguishing between singular and plu-
ral interpretations. Questions tend to be addressed
to individual participants, while statements show a
stronger correlation with plural addressees. When
no DA features are used (w/o DA), the drop in per-
formance for the multimodal classifier to 71.19%
is statistically significant (p < .05). As for the
generic vs. referential task, FL information does
not have a significant effect on performance.
278
Features Acc F1-Sing. F1-Pl.
Baseline 67.92 80.9 0
Discourse 71.19 78.9 54.6
Visual 66.28 74.8 48.9
MM* 77.05 83.3 63.2
Dis w/o FL 72.13 80.1 53.7
MM w/o FL 72.60 79.7 58.1
Dis w/o DA 68.38 78.5 40.5
MM w/o DA 71.19 78.8 55.3
Table 6: Singular vs. plural reference; * = with Correlation-
based Feature Selection (CFS).
6.2.2 Detection of Individual Addressees
We now turn to resolving the singular referential
uses of you. Here we must detect the individual
addressee of the utterance that contains the pro-
noun.
Baselines. Given the distribution shown in Ta-
ble 2, a majority class baseline yields an accu-
racy of 35.17%. An off-line system that has access
to future context could implement a next-speaker
baseline that always considers the next speaker to
be the intended addressee, so yielding a high raw
accuracy of 71.03%. A previous-speaker base-
line that does not require access to future context
achieves 35% raw accuracy.
Results. Table 7 shows a summary of the re-
sults, and these all outperform the majority class
(MC) and previous-speaker baselines. When all
discourse features are available, adding visual in-
formation does improve performance (74.48% vs.
60.69%, p < .005), and with CFS, this increases
further to 80.34% (p < .005). Using discourse or
visual features alone gives scores that are below
the next-speaker baseline (60.69% and 65.52% vs.
71.03%). Taking all forward-looking (FL) infor-
mation away reduces performance (p < .05), but
the small increase in accuracy caused by taking
away dialogue act information is not statistically
significant.
When we investigated individual feature contri-
bution, we found that the most predictive features
were the FL and backward-looking (BL) speaker
order, and the speaker?s visual features (including
mutual gaze). Whomever the speaker spent most
time looking at or engaged in a mutual gaze with
was more likely to be the addressee. All of the vi-
sual features had some degree of predictive power
apart from the ratio features. Of the other BL/FL
discourse features, features 14, 18 and 19 (see Ta-
ble 4) were more predictive. These indicate that
utterances spoken by the intended addressee are
Features Acc F1-L1 F1-L2 F1-L3
MC baseline 35.17 52.0 0 0
Discourse 60.69 59.1 60.0 62.7
Visual 65.52 69.1 63.5 64.0
MM* 80.34 80.0 82.4 79.0
Dis w/o FL 52.41 50.7 51.8 54.5
MM w/o FL 66.55 68.7 62.7 67.6
Dis w/o DA 61.03 58.5 59.9 64.2
MM w/o DA 73.10 72.4 69.5 72.0
Table 7: Addressee detection for singular references; * =
with Correlation-based Feature Selection (CFS).
often adjacent to the you-utterance and lexically
similar.
7 A Fully Automatic Approach
In this section we describe experiments which
use features derived from ASR transcriptions and
automatically-extracted visual information. We
used SRI?s Decipher (Stolcke et al, 2008)10 in or-
der to generate ASR transcriptions, and applied
the head-tracker described in Section 4.2 to the
relevant portions of video in order to extract the
visual information. Recall that the Named Entity
features (feature 13) and the DA features used in
our previous experiments had been manually an-
notated, and hence are not used here. We again
divide the problem into the same three separate
tasks: we first discriminate between generic and
referential uses of you, then singular vs. plural
referential uses, and finally we resolve the ad-
dressee for singular uses. As before, all exper-
iments are performed using a Bayesian Network
classifier and 10-fold cross validation.
7.1 Results
For each of the three tasks, Figure 7 compares
the accuracy results obtained using the fully-
automatic approach with those reported in Section
6. The figure shows results for the majority class
baselines (MCBs), and with discourse-only (Dis),
and multimodal (MM) feature sets. Note that the
data set for the automatic approach is smaller,
and that the majority class baselines have changed
slightly. This is because of differences in the ut-
terance segmentation, and also because not all of
the video sections around the you utterances were
processed by the head-tracker.
In all three tasks we are able to significantly
outperform the majority class baseline, but the vi-
sual features only produce a significant improve-
10Stolcke et al (2008) report a word error rate of 26.9% on
AMI meetings.
279
Figure 1: Results for the manual and automatic systems; MCB = majority class baseline, Dis = discourse features, MM =
multimodal, * = with Correlation-based Feature Selection (CFS), FL = forward-looking, man = manual, auto = automatic.
ment in the individual addressee resolution task.
For the generic vs. referential task, the discourse
and multimodal classifiers both outperform the
majority class baseline (p < .001), achieving
accuracy scores of 68.71% and 68.48% respec-
tively. In contrast to when using manual transcrip-
tions and annotations (see Section 6.1), removing
forward-looking (FL) information reduces perfor-
mance (p < .05). For the referential singular
vs. plural task, the discourse and multimodal with
CFS classifier improve over the majority class
baseline (p < .05). Multimodal with CFS does
not improve over the discourse classifier - indeed
without feature selection, the addition of visual
features causes a drop in performance (p < .05).
Here, taking away FL information does not cause
a significant reduction in performance. Finally,
in the individual addressee resolution task, the
discourse, visual (60.78%) and multimodal clas-
sifiers all outperform the majority class baseline
(p < .005, p < .001 and p < .001 respec-
tively). Here the addition of visual features causes
the multimodal classifier to outperform the dis-
course classifier in raw accuracy by nearly ten per-
centage points (67.32% vs. 58.17%, p < .05), and
with CFS, the score increases further to 74.51%
(p < .05). Taking away FL information does
cause a significant drop in performance (p < .05).
8 Conclusions
We have investigated the automatic resolution of
the second person English pronoun you in multi-
party dialogue, using a combination of linguistic
and visual features. We conducted a first set of
experiments where our features were derived from
manual transcriptions and annotations, and then a
second set where they were generated by entirely
automatic means. To our knowledge, this is the
first attempt at tackling this problem using auto-
matically extracted multimodal information.
Our experiments showed that visual informa-
tion can be highly predictive in resolving the ad-
dressee of singular referential uses of you. Visual
features significantly improved the performance of
both our manual and automatic systems, and the
latter achieved an encouraging 75% accuracy. We
also found that our visual features had predictive
power for distinguishing between generic and ref-
erential uses of you, and between referential sin-
gulars and plurals. Indeed, for the latter task,
they significantly improved the manual system?s
performance. The listeners? gaze features were
useful here: in our data set it was apparently the
case that the speaker would often use the white-
board/projector screen when addressing the group,
thus drawing the listeners? gaze in this direction.
Future work will involve expanding our data-
set, and investigating new potentially predictive
features. In the slightly longer term, we plan to
integrate the resulting system into a meeting as-
sistant whose purpose is to automatically extract
useful information from multi-party meetings.
280
References
Ron Arstein and Massimo Poesio. 2006. Identifying
reference to abstract objects in dialogue. In Pro-
ceedings of the 10th Workshop on the Semantics and
Pragmatics of Dialogue (Brandial?06), pages 56?
63, Potsdam, Germany.
Ilse Bakx, Koen van Turnhout, and Jacques Terken.
2003. Facial orientation during multi-party inter-
action with information kiosks. In Proceedings of
INTERACT, Zurich, Switzerland.
Donna Byron. 2004. Resolving pronominal refer-
ence to abstract entities. Ph.D. thesis, University
of Rochester, Department of Computer Science.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of Bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Daniel Jurafsky. 2007a. Resolving ?you? in multi-
party dialog. In Proceedings of the 8th SIGdial
Workshop on Discourse and Dialogue, Antwerp,
Belgium, September.
Surabhi Gupta, Matthew Purver, and Daniel Jurafsky.
2007b. Disambiguating between generic and refer-
ential ?you? in dialog. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
Mark Hall. 2000. Correlation-based Feature Selection
for Machine Learning. Ph.D. thesis, University of
Waikato.
Natasa Jovanovic, Rieks op den Akker, and Anton Ni-
jholt. 2006a. Addressee identification in face-to-
face meetings. In Proceedings of the 11th Confer-
ence of the European Chapter of the ACL (EACL),
pages 169?176, Trento, Italy.
Natasa Jovanovic, Rieks op den Akker, and Anton Ni-
jholt. 2006b. A corpus for studying addressing
behaviour in multi-party dialogues. Language Re-
sources and Evaluation, 40(1):5?23. ISSN=1574-
020X.
Natasa Jovanovic. 2007. To Whom It May Concern -
Addressee Identification in Face-to-Face Meetings.
Ph.D. thesis, University of Twente, Enschede, The
Netherlands.
Michael Katzenmaier, Rainer Stiefelhagen, and Tanja
Schultz. 2004. Identifying the addressee in human-
human-robot interactions based on head pose and
speech. In Proceedings of the 6th International
Conference on Multimodal Interfaces, pages 144?
151, State College, Pennsylvania.
Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,
S. Bourban, M. Flynn, M. Guillemot, T. Hain,
J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,
M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of Measuring Behavior, the 5th Inter-
national Conference on Methods and Techniques in
Behavioral Research, Wageningen, Netherlands.
Christoph Mu?ller. 2006. Automatic detection of non-
referential It in spoken multi-party dialog. In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 49?56, Trento, Italy.
Christoph Mu?ller. 2007. Resolving it, this, and that
in unrestricted multi-party dialog. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics, pages 816?823, Prague,
Czech Republic.
Andreas Stolcke, Xavier Anguera, Kofi Boakye, O?zgu?r
C?etin, Adam Janin, Matthew Magimai-Doss, Chuck
Wooters, and Jing Zheng. 2008. The icsi-sri spring
2007 meeting and lecture recognition system. In
Proceedings of CLEAR 2007 and RT2007. Springer
Lecture Notes on Computer Science.
Michael Strube and Christoph Mu?ller. 2003. A ma-
chine learning approach to pronoun resolution in
spoken dialogue. In Proceedings of ACL?03, pages
168?175.
Yoshinao Takemae, Kazuhiro Otsuka, and Naoki
Mukawa. 2004. An analysis of speakers? gaze
behaviour for automatic addressee identification in
multiparty conversation and its application to video
editing. In Proceedings of IEEE Workshop on Robot
and Human Interactive Communication, pages 581?
586.
Koen van Turnhout, Jacques Terken, Ilse Bakx, and
Berry Eggen. 2005. Identifying the intended
addressee in mixed human-humand and human-
computer interaction from non-verbal features. In
Proceedings of ICMI, Trento, Italy.
Bonnie Webber. 1991. Structure and ostension in
the interpretation of discourse deixi. Language and
Cognitive Processes, 6(2):107?135.
281
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 24?25,
Vancouver, October 2005.
A Flexible Conversational Dialog System for MP3 Player
Fuliang Weng
1
 Lawrence Cavedon
2
 Badri Raghunathan
1
 Danilo Mirkovic
2 
Ben Bei
1
Heather Pon-Barry
1
 Harry Bratt
3
 Hua Cheng
2
 Hauke Schmidt
1
 Rohit Mishra
4
 Brian Lathrop
4
Qi Zhang
1
   Tobias Scheideck
1
   Kui Xu
1
    Tess Hand-Bender
1
   Sandra Upson
1
     Stanley Peters
2
Liz Shriberg
3
 Carsten Bergmann
4
Research and Technology Center, Robert Bosch Corp., Palo Alto, California
1
Center for Study of Language and Information, Stanford University, Stanford, California
2
Speech Technology and Research Lab, SRI International, Menlo Park, California
3
Electronics Research Lab, Volkswagen of America, Palo Alto, California
4
{Fuliang.weng,badri.raghunathan,hauke.Schmidt}@rtc.bosch.com
{lcavedon,huac,peters}@csli.Stanford.edu
{harry,ees}@speech.sri.com
{rohit.mishra,carsten.bergmann}@vw.com
1 Abstract
In recent years, an increasing number of new de-
vices have found their way into the cars we drive.
Speech-operated devices in particular provide a
great service to drivers by minimizing distraction,
so that they can keep their hands on the wheel and
their eyes on the road. This presentation will dem-
onstrate our latest development of an in-car dialog
system for an MP3 player designed under a joint
research effort from Bosch RTC, VW ERL, Stan-
ford CSLI, and SRI STAR Lab funded by NIST
ATP [Weng et al2004] with this goal in mind.
This project has developed a number of new tech-
nologies, some of which are already incorporated
in the system.  These include: end-pointing with
prosodic cues, error identification and recovering
strategies, flexible multi-threaded, multi-device
dialog management, and content optimization and
organization strategies. A number of important
language phenomena are also covered in the sys-
tem?s functionality. For instance, one may use
words relying on context, such as ?this,? ?that,? ?it,?
and ?them,? to reference items mentioned in par-
ticular use contexts. Different types of verbal revi-
sion are also permitted by the system, providing a
great convenience to its users. The system supports
multi-threaded dialogs so that users can diverge to
a different topic before the current one is finished
and still come back to the first after the second
topic is done. To lower the cognitive load on the
drivers, the content optimization component orga-
nizes any information given to users based on on-
tological structures, and may also refine users?
queries via various strategies. Domain knowledge
is represented using OWL, a web ontology lan-
guage recommended by W3C, which should
greatly facilitate its portability to new domains.
The spoken dialog system consists of a number of
components (see Fig. 1 for details). Instead of the
hub architecture employed by Communicator pro-
jects [Senef et al 1998], it is developed in Java and
uses a flexible event-based, message-oriented mid-
dleware. This allows for dynamic registration of
new components. Among the component modules
in Figure 1, we use the Nuance speech recognition
engine with class-based ngrams and dynamic
grammars, and the Nuance Vocalizer as the TTS
engine. The Speech Enhancer removes noises and
echo. The Prosody module will provide additional
features to the Natural Language Understanding
(NLU) and Dialogue Manager (DM) modules to
improve their performance.
The NLU module takes a sequence of recognized
words and tags, performs a deep linguistic analysis
with probabilistic models, and produces an XML-
based semantic feature structure representation.
Parallel to the deep analysis, a topic classifier as-
signs top n topics to the utterance, which are used
in the cases where the dialog manager cannot make
24
any sense of the parsed structure. The NLU mod-
ule also supports dynamic updates of the knowl-
edge base.
The CSLI DM module mediates and manages in-
teraction. It uses the dialogue-move approach to
maintain dialogue context, which is then used to
interpret incoming utterances (including fragments
and revisions), resolve NPs, construct salient re-
sponses, track issues, etc. Dialogue states can also
be used to bias SR expectation and improve SR
performance, as has been performed in previous
applications of the DM. Detailed descriptions of
the DM can be found in [Lemon et al2002; Mirk-
ovic & Cavedon 2005].
The Knowledge Manager (KM) controls access to
knowledge base sources (such as domain knowl-
edge and device information) and their updates.
Domain knowledge is structured according to do-
main-dependent ontologies. The current KM
makes use of OWL, a W3C standard, to represent
the ontological relationships between domain enti-
ties. Prot?g? (http://protege.stanford.edu), a do-
main-independent ontology tool, is used to
maintain the ontology offline. In a typical interac-
tion, the DM converts a user?s query into a seman-
tic frame (i.e. a set of semantic constraints) and
sends this to the KM via the content optimizer.
The Content Optimization module acts as an in-
termediary between the dialogue management
module and the knowledge management module
during the query process. It receives semantic
frames from the DM, resolves possible ambigui-
ties, and queries the KM. Depending on the items
in the query result as well as the configurable
properties, the module selects and performs an ap-
propriate optimization strategy.
Early evaluation shows that the system has a
task completion rate of 80% on 11 tasks of MP3
player domain, ranging from playing requests to
music database queries. Porting to a restaurant se-
lection domain is currently under way.
References
Seneff, Stephanie, Ed Hurley, Raymond Lau, Christine Pao,
Philipp Schmid, and Victor Zue, GALAXY-II: A Reference
Architecture for Conversational System Development, In-
ternational Conference on Spoken Language Processing
(ICSLP), Sydney, Australia, December 1998.
Lemon, Oliver, Alex Gruenstein, and Stanley Peters, Collabo-
rative activities and multi-tasking in dialogue systems,
Traitement Automatique des Langues (TAL), 43(2), 2002.
Mirkovic, Danilo, and Lawrence Cavedon, Practical Multi-
Domain, Multi-Device Dialogue Management, Submitted
for publication, April 2005.
Weng, Fuliang, Lawrence Cavedon, Badri Raghunathan, Hua
Cheng, Hauke Schmidt, Danilo Mirkovic, et al, Develop-
ing a conversational dialogue system for cognitively over-
loaded users, International Conference on Spoken
Language Processing (ICSLP), Jeju, Korea, October 2004.
25
NAACL HLT Demonstration Program, pages 23?24,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
A Conversational In-car Dialog System
Baoshi Yan1 Fuliang Weng1 Zhe Feng1 Florin Ratiu2 Madhuri Raya1 Yao Meng1
Sebastian Varges2 Matthew Purver2 Annie Lien1 Tobias Scheideck1 Badri Raghunathan1
Feng Lin1 Rohit Mishra4 Brian Lathrop4 Zhaoxia Zhang4 Harry Bratt3 Stanley Peters2
Research and Technology Center, Robert Bosch LLC, Palo Alto, California1
Center for the Study of Language and Information, Stanford University, Stanford, California2
Speech Technology and Research Lab, SRI International, Menlo Park, California3
Electronics Research Lab, Volkswagen of America, Palo Alto, California4
Abstract
In this demonstration we present a con-
versational dialog system for automobile
drivers. The system provides a voice-
based interface to playing music, finding
restaurants, and navigating while driving.
The design of the system as well as the
new technologies developed will be pre-
sented. Our evaluation showed that the
system is promising, achieving high task
completion rate and good user satisfation.
1 Introduction
As a constant stream of electronic gadgets such as
navigation systems and digital music players en-
ters cars, it threatens driving safety by increasing
driver distraction. According to a 2005 report by
the National Highway Traffic Safety Administration
(NHTSA) (NHTSA, 2005), driver distraction and
inattention from all sources contributed to 20-25%
of police reported crashes. It is therefore impor-
tant to design user interfaces to devices that mini-
mize driver distraction, to which voice-based inter-
faces have been a promising approach as they keep
a driver?s hands on the wheel and eyes on the road.
In this demonstration we present a conversational
dialog system, CHAT, that supports music selection,
restaurant selection, and driving navigation (Weng
et al, 2006). The system is a joint research effort
from Bosch RTC, VWERL, Stanford CSLI, and SRI
STAR Lab funded by NIST ATP. It has reached a
promising level, achieving a task completion rate of
98%, 94%, 97% on playing music, finding restau-
rants, and driving navigation respectively.
Specifically, we plan to present a number of fea-
tures in the CHAT system, including end-pointing
with prosodic cues, robust natural language under-
standing, error identification and recovery strate-
gies, content optimization, full-fledged reponse gen-
eration, flexible multi-threaded, multi-device dialog
management, and support for random events, dy-
namic information, and domain switching.
2 System Descriptions
The spoken dialog system consists of a number of
components (see the figure on the next page). In-
stead of the hub architecture employed by Commu-
nicator projects (Seneff et al, 1998), it is devel-
oped in Java and uses flexible event-based, message-
oriented middleware. This allows for dynamic regis-
tration of new components. Among the component
modules in the figure, we use the Nuance speech
recognition engine with class-based n-grams and
dynamic grammars, and the Nuance Vocalizer as the
TTS engine. The Speech Enhancer removes noises
and echo. The Prosody module will provide addi-
tional features to the Natural Language Understand-
ing (NLU) and Dialog Manager (DM) modules to
improve their performance.
The NLU module takes a sequence of recognized
words and tags, performs a deep linguistic analysis
with probabilistic models, and produces an XML-
based semantic feature structure representation. Par-
allel to the deep analysis, a topic classifier assigns
n-best topics to the utterance, which are used in the
cases where the dialog manager cannot make any
sense of the parsed structure. The NLU module also
supports dynamic updates of the knowledge base.
The DM module mediates and manages interac-
23
tion. It uses an information-state-update approach to
maintain dialog context, which is then used to inter-
pret incoming utterances (including fragments and
revisions), resolve NPs, construct salient responses,
track issues, etc. Dialog states can also be used to
bias SR expectation and improve SR performance,
as has been performed in previous applications of
the DM. Detailed descriptions of the DM can be
found in (Lemon et al, 2002) (Mirkovic and Cave-
don, 2005).
The Knowledge Manager (KM) controls access
to knowledge base sources (such as domain knowl-
edge and device information) and their updates. Do-
main knowledge is structured according to domain-
dependent ontologies. The current KMmakes use of
OWL, a W3C standard, to represent the ontological
relationships between domain entities.
The Content Optimization module acts as an in-
termediary between the dialog management module
and the knowledge management module and con-
trols the amount of content and provides recommen-
dations to user. It receives queries in the form of se-
mantic frames from the DM, resolves possible ambi-
guities, and queries the KM. Depending on the items
in the query result as well as configurable properties,
the module selects and performs an appropriate op-
timization strategy (Pon-Barry et al, 2006).
The Response Generation module takes query re-
sults from the KM or Content Optimizer and gener-
ates natural language sentences as system responses
to user utterances. The query results are converted
into natural language sentences via a bottom-up ap-
proach using a production system. An alignment-
based ranking algorithm is used to select the best
generated sentence.
The system supports random events and dy-
namic external information, for example, the system
prompts users for the next turn when they drive close
to an intersection and dialogs can be carried out in
terms of the current dynamic situation. The user can
also switch among the three different applications
easily by explicitly instructing the system which do-
main to operate in.
3 Acknowledgement
This work is partially supported by the NIST Ad-
vanced Technology Program.
References
Oliver Lemon, Alex Gruenstein, and Stanley Peters.
2002. Collaborative activities and multi-tasking in
dialogue systems. In Traitement Automatique des
Langues (TAL), page 43(2).
Danilo Mirkovic and Lawrence Cavedon. 2005. Prac-
tical Plug-and-Play Dialogue Management. In Pro-
ceedings of the 6th Meeting of the Pacific Associa-
tion for Computational Linguistics (PACLING), page
43(2), Tokyo, Japan.
National Highway Traffic Safety Administration
NHTSA. 2005. NHTSA Vehicle Safety Rulemaking
and Supporting Research Priorities: Calendar Years
2005-2009. January.
Heather Pon-Barry, Fuliang Weng, and Sebastian Varges.
2006. Evaluation of content presentation strategies
for an in-car spoken dialogue system. In Proceedings
of the 9th International Conference on Spoken Lan-
guage Processing (Interspeech/ICSLP), pages 1930?
1933, Pittsburgh, PA, September.
Stephanie Seneff, Ed Hurley, Raymond Lau, Chris-
tine Pao, Philipp Schmid, and Victor Zue. 1998.
GALAXY-II: A Reference Architecture for Conversa-
tional System Development. In International Confer-
ence on Spoken Language Processing (ICSLP), page
43(2), Sydney, Australia, December.
Fuliang Weng, Sebastian Varges, Badri Raghunathan,
Florin Ratiu, Heather Pon-Barry, Brian Lathrop,
Qi Zhang, Tobias Scheideck, Harry Bratt, Kui Xu,
Matthew Purver, Rohit Mishra, Annie Lien, Mad-
huri Raya, Stanley Peters, Yao Meng, Jeff Russel,
Lawrence Cavedon, Liz Shriberg, and Hauke Schmidt.
2006. CHAT: A conversational helper for automo-
tive tasks. In Proceedings of the 9th International
Conference on Spoken Language Processing (Inter-
speech/ICSLP), pages 1061?1064, Pittsburgh, PA,
September.
24
Automated Tutoring Dialogues for Training in Shipboard
Damage Control
John Fry, Matt Ginzton, Stanley Peters, Brady Clark & Heather Pon-Barry
Stanford University
Center for the Study of Language Information
Stanford CA 94305-4115 USA
{fry,mginzton,peters,bzack,ponbarry}@csli.stanford.edu
Abstract
This paper describes an application
of state-of-the-art spoken language
technology (OAA/Gemini/Nuance)
to a new problem domain: engaging
students in automated tutorial dia-
logues in order to evaluate and im-
prove their performance in a train-
ing simulator.
1 Introduction
Shipboard damage control refers to the task of
containing the effects of fire, explosions, hull
breaches, flooding, and other critical events
that can occur aboard Naval vessels. The
high-stakes, high-stress nature of this task, to-
gether with limited opportunities for real-life
training, make damage control an ideal target
for AI-enabled educational technologies like
training simulators and tutoring systems.
This paper describes the spoken dialogue
system we developed for automated critiquing
of student performance on a damage control
training simulator. The simulator is DC-
Train (Bulitko and Wilkins, 1999), an im-
mersive, multimedia training environment for
damage control. DC-Train?s training sce-
narios simulate a mixture of physical phenom-
ena (e.g., fire, flooding) and personnel issues
(e.g., casualties, communications, standard-
ized procedures). Our current tutoring sys-
tem is restricted fire damage scenarios only,
and in particular to the twelve fire scenar-
ios available in DC-Train version 2.5, but
in future versions we plan to support post-
session critiques for all of the damage phe-
nomena that will be modeled by DC-Train
4.0: fire, flooding, missile damage, and wall
or firemain ruptures.
2 Previous Work
Eliciting self-explanation from a student has
been shown to be a highly effective tutoring
method (Chi et al, 1994). For this reason,
a number of automated tutoring systems cur-
rently use NLP techniques to engage students
in reflective dialogues. Three notable exam-
ples are the medical Circsim tutor (Zhou et
al., 1999); the Basic Electricity and Electron-
ics (BE&E) tutor (Rose? et al, 1999); and
the computer literacy AutoTutor (Wiemer-
Hastings et al, 1999).
Our system shares several features with
these three tutoring systems:
A knowledge base Our system encodes
all domain knowledge relevant to supporting
intelligent tutoring feedback into a structure
called an Expert Session Summary (Section
4). These expert summaries encode causal
relationships between events on the ship as
well as the proper and improper responses to
shipboard crises.
Tutoring strategies In our system, as in
those above, the flow of dialogue is controlled
by (essentially) a finite-state transition net-
work (Fig. 1).
An interpretation component In our
system, the student?s speech is recognized and
parsed into logical forms (Section 3). A dia-
logue manager inspects the current dialogue
information state to determine how best to
incorporate each new utterance into the dia-
logue (Lemon et al, 2001).
Prompt
student review
of actions
Correct
student?s
report Prompt for
reflection on
START
END
continue"
"OK, let?s
event N...
Summary
of damage
main points
Review
performance
student?s
Evaluate
reflections
Correct
student?s
"You handled
this one well"
event 1
of damage
Summary
Brief
summary of
session
errors
Figure 1: Post-session dialogue move graph (simplified)
However, an important difference is that
the three systems above are entirely text-
based, whereas ours is a spoken dialogue sys-
tem. Our speech interface offers greater natu-
ralness than keyboard-based input. In this re-
spect, our system is similar to cove (Roberts,
2000), a training simulator for conning Navy
ships that uses speech to interact with the
student. But whereas cove uses short conver-
sational exchanges to coach the student dur-
ing the simulation, our system engages in ex-
tended tutorial dialogues after the simulation
has ended. Besides being more natural, spo-
ken language systems are also better suited to
multimodal interactions (viz., one can point
and click while talking but not while typing).
An additional significant difference between
our system and a number of other automated
tutoring systems is our use of ?deep? process-
ing techniques. While other systems utilize
?shallow? statistical approaches like Latent Se-
mantic Analysis (e.g. AutoTutor), our system
utilizes Gemini, a symbolic grammar. This
approach enables us to provide precise and
reliable meaning representations.
3 Implementation
To facilitate the implementation of multi-
modal, mixed-initiative tutoring interactions,
we decided to implement our system within
the Open Agent Architecture (OAA) (Martin
et al, 1999). OAA is a framework for coor-
dinating multiple asynchronous communicat-
ing processes. The core of OAA is a ?facilita-
tor? which manages message passing between
a number of software agents that specialize
in certain tasks (e.g., speech recognition or
database queries). Our system uses OAA to
coordinate the following five agents:
1. The Gemini NLP system (Dowding et
al., 1993). Gemini uses a single unifi-
cation grammar both for parsing strings
of words into logical forms (LFs) and for
generating sentences from LF inputs.
2. A Nuance speech recognition server,
which converts spoken utterances to
strings of words. The Nuance server re-
lies on a language model, which is com-
piled directly from the Gemini grammar,
ensuring that every recognized utterance
is assigned an LF.
3. The Festival text-to-speech system,
which ?speaks? word strings generated by
Gemini.
4. A Dialogue Manager which coordi-
nates inputs from the user, interprets the
user?s dialogue moves, updates the dia-
logue context, and delivers speech and
graphical outputs to the user.
5. A Critique Planner, described below
in Section 4.
Agents 1-3 are reusable, ?off-the-shelf? dia-
logue system components (apart from the
Gemini grammar, which must be modified for
each application). We implemented agents 4
and 5 in Java specifically for this application.
Variants of this OAA/Gemini/Nuance ar-
chitecture have been deployed successfully in
other dialogue systems, notably SRI?s Com-
mandTalk (Stent et al, 1999) and an un-
Figure 2: Screen shot of post-session tutorial dialogue system
manned helicopter interface developed in our
laboratory (Lemon et al, 2001).
4 Planning the dialogue
Each student session with DC-Train pro-
duces a session transcript, i.e. a time-stamped
record of every event (both computer- and
student-initiated) that occurred during the
simulation. These transcripts serve as the
input to our post-session Critique Planner
(CP).
The CP plans a post-session tutorial di-
alogue in two steps. In the first step, an
Expert Session Summary (ESS) is cre-
ated from the session transcript. The ESS
is a tree whose parent nodes represent dam-
age events and whose leaves represent actions
taken in response to those damage events.
Each student-initiated action in the ESS is
evaluated as to its timeliness and conformance
to damage control doctrine. Actions that the
student should have taken but did not are also
inserted into the ESS and flagged as such.
Each action node in the ESS therefore falls
into one of three classes: (i) correct actions;
(ii) errors of commission (e.g., the student
sets fire containment boundaries incorrectly);
and (iii) errors of omission (e.g., the student
fails to secure permission from the captain be-
fore flooding certain compartments).
Our current tutoring system covers scenar-
ios generated by DC-Train 2.5, which covers
fire scenarios only. Future versions will use
scenarios generated by DC-Train 4.0, which
covers damage control scenarios involving fire,
smoke, flooding, pipe and hull ruptures, and
equipment deactivation. Our current tutor-
ing system is based on an ESS graph that is
generated by an expert model that consists
of an ad-hoc set of firefighting rules. Future
versions will be based on an ESS graph that
is generated by an successor to the Minerva-
DCA expert model (Bulitko and Wilkins,
1999), an extended Petri Net envisionment-
based reasoning system. The new expert
model is designed to produce an ESS graph
during the course of problem solving that con-
tains nodes for all successful and unsuccessful
plan and goal achievement events, along with
an explanation structure for each graph node.
The second step in planning the post-
session tutorial dialogue is to produce a di-
alogue move graph (Fig. 1). This is a di-
rected graph that encodes all possible configu-
rations of dialogue structure and content that
can be handled by the system.
Generating an appropriate dialogue move
graph from an ESS requires pedagogical
knowledge, and in particular a tutoring strat-
egy. The tutoring strategy we adopted is
based on our analysis of videotapes of fifteen
actual DC-Train post-session critiques con-
ducted by instructors at the Navy?s Surface
Warfare Officer?s School in Newport, RI. The
strategy we observed in these critiques, and
implemented in our system, can be outlined
as follows:
1. Summarize the results of the simulation
(e.g., the final condition of the ship).
2. For each major damage event in the ESS:
(a) Ask the student to review his ac-
tions, correcting his recollections as
necessary.
(b) Evaluate the correctness of each stu-
dent action.
(c) If the student committed errors,
ask him how these could have been
avoided, and evaluate the correct-
ness of his responses.
3. Finally, review each type of error that
arose in step (2c).
A screen shot of the tutoring system in
action is shown in Fig. 2. As soon as a
DC-Train simulation ends, the dialogue sys-
tem starts up and the dialogue manager be-
gins traversing the dialogue move graph. As
the dialogue unfolds, a graphical representa-
tion of the ESS is revealed to the student in
piecemeal fashion as depicted in the top right
frame of Fig. 2.
Acknowledgments
This work is supported by the Depart-
ment of the Navy under research grant
N000140010660, a multidisciplinary univer-
sity research initiative on natural language in-
teraction with intelligent tutoring systems.
References
V V. Bulitko and D C. Wilkins. 1999. Automated
instructor assistant for ship damage control. In
Proceedings of AAAI-99, Orlando, FL, July.
M. T. H. Chi, N. de Leeuw, M. Chiu, and C.
LaVancher. 1994. Eliciting self-explanations
improves understanding. Cognitive Science,
18(3):439?477.
J. Dowding, J. Gawron, D. Appelt, J. Bear, L.
Cherny, R. C. Moore, and D. Moran. 1993.
Gemini: A natural language system for spoken-
language understanding. In Proceedings of the
ARPA Workshop on Human Language Technol-
ogy.
O. Lemon, A. Bracy, A. Gruenstein, and S. Pe-
ters. 2001. A multi-modal dialogue system for
human-robot conversation. In Proceedings of
NAACL 2001.
D. Martin, A. Cheyer, and D. Moran. 1999.
The Open Agent Architecture: a framework for
building distributed software systems. Applied
Artificial Intelligence, 13(1-2).
B. Roberts. 2000. Coaching driving skills in
a shiphandling trainer. In Proceedings of the
AAAI Fall Symposium on Building Dialogue
Systems for Tutorial Applications.
C. P. Rose?, B. Di Eugenio, and J. D. Moore. 1999.
A dialogue based tutoring system for basic elec-
tricity and electronics. In S. P. Lajoie and
M. Vivet, editors, Artificial Intelligence in Ed-
ucation (Proceedings of AIED?99), pages 759?
761. IOS Press, Amsterdam.
A. Stent, J. Dowding, J. Gawron, E. O. Bratt, and
R. C. Moore. 1999. The CommandTalk spoken
dialogue system. In Proceedings of ACL ?99,
pages 183?190, College Park, MD.
P. Wiemer-Hastings, K. Wiemer-Hastings, and
A. Graesser. 1999. Improving an intelli-
gent tutor?s comprehension of students with la-
tent semantic analysis. In S. P. Lajoie and
M. Vivet, editors, Artificial Intelligence in Ed-
ucation (Proceedings of AIED?99), pages 535?
542. IOS Press, Amsterdam.
Y. Zhou, R. Freedman, M. Glass, J. A. Michael,
A. A. Rovick, and M. W. Evens. 1999. Deliver-
ing hints in a dialogue-based intelligent tutoring
system. In Proceedings of AAAI-99, Orlando,
FL, July.
Multi-tasking and Collaborative Activities in Dialogue Systems
Oliver Lemon, Alexander Gruenstein, Alexis Battle, and Stanley Peters
Center for the Study of Language and Information
Stanford University, CA 94305
lemon,alexgru,ajbattle,peters@csli.stanford.edu
Abstract
We explain dialogue management tech-
niques for collaborative activities with hu-
mans, involving multiple concurrent tasks.
Conversational context for multiple con-
current activities is represented using a
?Dialogue Move Tree? and an ?Activity
Tree? which support multiple interleaved
threads of dialogue about different activi-
ties and their execution status. We also de-
scribe the incremental message selection,
aggregation, and generation method em-
ployed in the system.
1 Introduction
This paper describes implemented multi-modal dia-
logue systems1 which support collaboration with au-
tonomous devices in their execution of multiple con-
current tasks. We will focus on the particular mod-
elling and processing aspects which allow the sys-
tems to handle dialogues about multiple concurrent
tasks in a coherent and natural manner. Many con-
versations between humans have this property, and
dialogues between humans and semi-autonomous
devices will have this feature in as much as devices
are able to carry out activities concurrently. This
ability to easily interleave communication streams is
a very useful property of conversational interactions.
Humans are adept at carrying out conversations with
1This research was (partially) funded under the Wallenberg
laboratory for research on Information Technology and Au-
tonomous Systems (WITAS) Project, Linko?ping University, by
the Wallenberg Foundation, Sweden.
multiple threads, or topics, and this capability en-
ables fluid and efficient communication, and thus ef-
fective co-ordination of actions (see (Lemon et al,
2002) for a more extensive discussion). We will
show how to endow a dialogue system with some
of these capabilities.
The main issues which we address in this paper
are:
  Representation of dialogue context such that
collaborative activities and multi-tasking are
supported.
  Dialogue management methods such that free
and natural communication over several con-
versational topics is supported.
  Natural generation of messages in multi-
tasking collaborative dialogues.
In Section 2 we discuss the demands of multi-
tasking and collaboration with autonomous devices.
Section 3 covers the robot with which our current
dialogue system interacts, and the architecture of
the dialogue system. In Section 4 we introduce the
?joint activities? and Activity Models which repre-
sent collaborative tasks and handle multi-tasking in
an interface layer between the dialogue system and
autonomous devices. Section 5 presents the dia-
logue modelling and management techniques used
to handle multiple topics and collaborative activi-
ties. Section 6 surveys the message selection, ag-
gregation, and generation component of the system,
in the context of multi-tasking.
     Philadelphia, July 2002, pp. 113-124.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
2 Multi-tasking and Collaboration
A useful dialogue system for interaction with au-
tonomous devices will enable collaboration with hu-
mans in the planning and execution of tasks. Dia-
logue will be used to specify and clarify instructions
and goals for the device, to monitor its progress,
and also to jointly solve problems. Before we deal
with such issues in detail, we note that such devices
also have the following properties which are relevant
from the point of view of dialogue management:
  Devices exist within dynamic environments,
where new objects appear and are available for
discussion. Device sensors may give rise to
new information at any time, and this may need
to be communicated urgently.
  Devices may perform multiple concurrent ac-
tivities which may succeed, fail, become can-
celled, or be revised. These activities can be
topics of conversation.
(Allen et al, 2001) present a taxonomy of dia-
logue systems ranging from ?finite-state script? di-
alogues for simple tasks (such as making a long-
distance call) to the most complex ?agent-based
models? which cover dialogues where different pos-
sibilities, such as future plans, are discussed. Within
this taxonomy, a useful dialogue system for interac-
tion with autonomous devices must be located at or
near the ?agent-based? point since we wish to com-
municate with devices about their possible actions,
their plans, and the tasks they are currently attempt-
ing. For these reasons we built a dialogue manager
that represents (possibly collaborative) activities and
their execution status, and tracks multiple threads of
dialogue about concurrent and planned activities.
For these sorts of reasons it is clear that form-
filling or data-base query style dialogues (e.g. the
CSLU Toolkit, (McTear, 1998)) will not suffice here
(see (Elio and Haddadi, 1999; Allen et al, 2001) for
similar arguments).
3 The WITAS Dialogue System
In our current application, the autonomous system
is the WITAS2 UAV (?unmanned aerial vehicle?) ?
a small robotic helicopter with on-board planning
2See http://www.ida.liu.se/ext/witas
and deliberative systems, and vision capabilities (for
details see e.g. (Doherty et al, 2000)). This robot
helicopter will ultimately be controlled by the dia-
logue system developed at CSLI, though at the mo-
ment we interact with a simulated3 UAV. Mission
goals are provided by a human operator, and an on-
board planning system then responds. While the he-
licopter is airborne, an on-board active vision sys-
tem interprets the scene or focus below to interpret
ongoing events, which may be reported (via NL gen-
eration) to the operator (see Section 6). The robot
can carry out various ?activities? such as flying to a
location, or following a vehicle, or landing. These
activities are specified by the user during dialogue,
or can be initiated by the UAV?s on-board AI. In any
case, a major component of the dialogue, and a way
of maintaining its coherence, is tracking the state of
current or planned activities of the device.
A more interesting and problematic notion is that
of ?joint-activities? between an autonomous system
and a human operator. These are activities which
the autonomous system cannot complete alone, but
which require some human intervention. In our cur-
rent scenarios, the UAV?s vision system is not good
enough to determine whether a particular vehicle is
the one sought-after, and only the human operator
has the authority to determine this, so that human
and robot must collaborate in order to find and track
a vehicle. The dialogue in Figure 2 shows how a
typical interaction works4 (other capabilities, such
as clarification subdialogues, are covered in (Lemon
et al, 2001)). Note here that the user is able to make
explicit queries about the robot?s activities (both cur-
rent and future), that there are concurrent activi-
ties, and that conversational initiative centers around
the joint activities currently being specified and ex-
ecuted.
4 Activity Models
The idea of Activity Modelling in our system is
the vision that dialogue systems can, in generality,
be built for ?devices? which carry out certain well-
3Our UAV simulator uses KIF statements under JTP (the
Java Theorem Prover) to represent and non-montonically up-
date UAV state information.
4The system runs on a laptop computer under Windows
2000. Video footage of the system can be found at http:
//www-csli.stanford.edu/semlab/witas/
Figure 2: A demonstration of the WITAS dialogue system (November 2001)
Multi-modal Utterances Dialogue Move
Operator (O): Our job is to look for a red car Command (Joint Activity)
UAV (U): Ok. I am looking for one. Report (Confirm Activity)
O: Fly here please [+click on map] Command (Deictic)
U: Okay. I will fly to waypoint one Report (Confirm Activity)
U: Now taking off and flying there. Report (Current Activity)
O: Stop that. Go to the tower instead. Command, Revision
U: I have cancelled flying to waypoint one. I will fly to the
tower.
Report (Activity status)
O: What are you doing? Wh-question (Current Activity)
U: I am searching for a red car and flying to the tower Answer (Current Activity)
O: What will you do next? Wh-question (Planned Activity)
U: I have nothing planned. Answer(Planned Activity)
U: I see a red car on main street [display on map, show video
images], Is this the right car?
Report, Yn-question (Activity)
O: Yes, that?s the right car Yn-answer (Positive)
U: Okay. I am following it . Report (Current activity)
facilitator
OAA2
Synthesizer
Generator
Gemini
Parser and 
Recognizer
Speech
Festival
Display
Interactive Map
NL
SR
TTS
DM
GUI
Activities
Model
Interface
       
Dialogue Move Tree (DMT)
Activity Tree (AT)
System Agenda (SA)
Pending List (PL)
Modality Buffer (MB)
ROBOT
Salience List (SL)
Speech
Nuance
DIALOGUE MANAGER
Figure 1: The WITAS dialogue system architecture
defined activities (e.g. switch lights on, record on
channel   , send email  to  , search for vehicle  ),
and that an important part of the dialogue context to
be modelled in such a system is the device?s planned
activities, current activities, and their execution sta-
tus5. We choose to focus on building this class of
dialogue systems because we share with (Allen et
al., 2001), a version of the the Practical Dialogue
5Compare this with the motivation behind the ?Pragmatic
Adapter? idea of (LuperFoy et al, 1998).
Hypothesis:
?The conversational competence required
for practical dialogues, although still com-
plex, is significantly simpler to achieve
than general human conversational com-
petence.?
We also share with (Rich et al, 2001) the idea that
declarative descriptions of the goal decomposition
of activities (COLLAGEN?s ?recipes?, our ?Activ-
ity Models?) are a vital layer of representation, be-
tween a dialogue system and the device with which
it interacts.
In general we assume that a device is capable of
performing some ?atomic? activities or actions (pos-
sibly simultaneously), which are the lowest-level ac-
tions that it can perform. Some devices will only
know how to carry out sequences of atomic activ-
ities, in which case it is the dialogue system?s job
to decompose linguistically specified high-level ac-
tivities (e.g. ?record the film on channel 4 tonight?)
into a sequence of appropriate atomic actions for the
device. In this case the dialogue system is provided
with a declarative ?Activities Model? (see e.g. Fig-
ure 3) for the device which states how high-level
linguistically-specified activities can be decomposed
into sequences of atomic actions. This model con-
tains traditional planning constraints such as precon-
ditions and postconditions of actions. In this way, a
relatively ?stupid? device (i.e. with little or no plan-
ning capabilities) can be made into a more intelli-
gent device when it is dialogue-enabled.
At the other end of the spectrum, more intelli-
gent devices are able to plan their own sequences of
atomic actions, based on some higher level input. In
this case, it is the dialogue system?s role to translate
natural language into constraints (including tempo-
ral constraints) that the device?s planner recognizes.
The device itself then carries out planning, and in-
forms the dialogue manager of the sequence of ac-
tivities that it proposes. Dialogue can then be used
to re-specify constraints, revise activities, and mon-
itor the progress of tasks. We propose that the pro-
cess of decomposing a linguistically specified com-
mand (e.g. ?vacuum in the main bedroom and the
lounge, and before that, the hall?) into an appropri-
ate sequence of constraints for the device?s on-board
planner, is an aspect of ?conversational intelligence?
that can be added to devices by dialogue-enabling
them.
We are developing one representation and reason-
ing scheme to cover this spectrum of cases from de-
vices with no planning capabilities to some more
impressive on-board AI. Both dialogue manager
and robot/device have access to a single ?Activity
Tree? which is a shared representation of current
and planned activities and their execution status, in-
volving temporal and hierarchical ordering (in fact,
one can think of the Activity Tree as a Hierarchical
Task Network for the device). This tree is built top-
down by processing verbal input from the user, and
its nodes are then expanded by the device?s planner
(if it has one). In cases where no planner exists, the
dialogue manager itself expands the whole tree (via
the Activity Model for the device) until only leaves
with atomic actions are left for the device to execute
in sequence. The device reports completion of activ-
ities that it is performing and any errors that occur
for an activity.
Note that because the device and dialogue system
share the same representation of the device?s activ-
ities, they are always properly coordinated. They
also share responsibility for different aspects of con-
structing and managing the whole Activity Tree.
Note also that some activities can themselves be
speech acts, and that this allows us to build collabo-
rative dialogue into the system. For example, in Fig-
ure 3 the ASK-COMPLETE activity is a speech act,
generating a yes-no question to be answered by the
user.
4.1 An example Activity Model
An example LOCATE activity model for the UAV
is shown in Figure 3. It is used when constructing
parts of the activity tree involving commands such
as ?search for?, ?look for? and so on. For instance,
if the user says ?We?re looking for a truck?, that ut-
terance is parsed into a logical form involving the
structure (locate, np[det(a),truck]).
The dialogue manager then accesses the Activity
Model for LOCATE and adds a node to the Activ-
ity Tree describing it. The Activity Model speci-
fies what sub-activities should be invoked, and un-
der what conditions they should be invoked, what
the postconditions of the activity are. Activity Mod-
els are similar to the ?recipes? of (Rich et al, 2001).
For example, in Figure 3 the Activity Model for LO-
CATE states that,
  it uses the camera resource (so that any other
activity using the camera must be suspended,
or a dialogue about resource conflict must be
initiated),
  that the preconditions of the activity are that the
UAV must be airborne, with fuel and engine in-
dicators satisfactory,
  that the whole activity can be skipped if the
UAV is already ?locked-on? to the sought ob-
ject,
  that the postcondition of the activity is that the
UAV is ?locked-on? to the sought object,
  that the activity breaks into three sequen-
tial sub-activities: WATCH-FOR, FOLLOW-OBJ,
and ASK-COMPLETE.
Nodes on the Activity Tree can be either: ac-
tive, complete, failed, suspended, or canceled. Any
change in the state of a node (typically because of
a report from the robot) is placed onto the System
Agenda (see Section 5) for possible verbal report to
the user, via the message selection and generation
module (see Section 6).
Figure 3: A ?Locate? Activity Model for a UAV, exhibiting collaborative dialogue
Locate// locate is "find-by-type", collaborative activity.
// Breaks into subactivities: watch_for, follow, ask_complete.
{ResourcesUsed {camera;} // will be checked for conflicts.
PreConditions //check truth of KIF statements.
{(Status flight inair) (Status engine ok) (Status fuel ok);}
SkipConditions // skip this Activity if KIF condition true.
{(Status locked-on THIS.np);}
PostConditions// assert these KIF statements when completed.
{(Status locked-on THIS.np) ;}
Children SEQ //sequential sub-activities.
{TaskProperties
{command = "watch_for"; // basic robot action ---
np = THIS.np;} // set sensors to search.
TaskProperties
{command = "follow_obj"; //triggers complex activity --
np = THIS.np;} //following a candidate object.
TaskProperties //collaborative speech action:
{command = "ask_complete";//asks user whether this is
np = THIS.np; }}} //object we are looking for.
5 The Dialogue Context Model
Dialogue management falls into two parts ? dialogue
modelling (representation), and dialogue control (al-
gorithm). In this section we focus on the representa-
tional aspects, and section 5.2 surveys the main al-
gorithms. As a representation of conversational con-
text, the dialogue manager uses the following data
structures which make up the dialogue Information
State (IS);
  Dialogue Move Tree (DMT)
  Activity Tree (AT)
  System Agenda (SA)
  Pending List (PL)
  Salience List (SL)
  Modality Buffer (MB)
Figure 4 shows how the Dialogue Move Tree re-
lates to other parts of the dialogue manager as a
whole. The solid arrows represent possible update
functions, and the dashed arrows represent query
functions. For example, the Dialogue Move Tree
can update Salience List, System Agenda, Pend-
ing List, and Activity Tree, while the Activity Tree
can update only the System Agenda and send ex-
ecution requests to the robot, and it can query the
Activity Model (when adding nodes). Likewise, the
Message Generation component queries the System
Agenda and the Pending List, and updates the Dia-
logue Move Tree whenever a synthesized utterance
is produced.
Figure 5 shows an example Information State
logged by the system, displaying the interpretation
of the system?s utterance ?now taking off? as a re-
port about an ongoing ?go to the tower? activity (the
Pending List and System Agenda are empty, and
thus are not shown).
5.1 The Dialogue Move Tree
Dialogue management uses a set of abstract dia-
logue move classes which are domain independent
(e.g. command, activity-query, wh-question, revi-
sion,     ). Any ongoing dialogue constructs a par-
ticular Dialogue Move Tree (DMT) representing the
current state of the conversation, whose nodes are
DIALOGUE
ACTIVITY
MOVE
TREE
AGENDA
SYSTEM TREE
Activities)
(NPs,
(Selection and Aggregation)
SALIENCE
ACTIVITY
LAYER
speech
synthesis
INFORMATION
INDEXICAL
(Active Node List)
MESSAGE 
GENERATION
ACTIVITY
MODEL
DEVICE
LIST
PENDING
LIST
MODALITY
BUFFER
Map Display Inputs
(parsed human speech)
(mouse clicks)
Conversational Move Inputs
Figure 4: Dialogue Manager Architecture (solid arrows denote possible updates, dashed arrows represent
possible queries)
instances of the dialogue move classes, and which
are linked to nodes on the Activity Tree where ap-
propriate, via an activity tag (see below).
Incoming logical forms (LFs) from the pars-
ing process are always tagged with a dialogue
move (see e.g. (Ginzburg et al, 2001)), which pre-
cedes more detailed information about an utter-
ance. For instance the logical form: command([go],
[param-list ([pp-loc(to, arg([np(det([def],the),
[n(tower,sg)])]))])])
corresponds to the utterance ?go to the tower?,
which is flagged as a command.
A slightly more complex example is; re-
port(inform, agent([np([n(uav,sg)])]), compl-
activity([command([take-off])]))
which corresponds to ?I have taken off? ? a re-
port from the UAV about a completed ?taking-off?
activity.
The first problem in dialogue management is
to figure out how these incoming ?Conversational
Moves? relate to the current dialogue context. In
other words, what dialogue moves do they consti-
tute, and how do they relate to previous moves in
the conversation? In particular, given multi-tasking,
to which thread of the conversation does an incom-
ing utterance belong? We use the Dialogue Move
Tree to answer these questions:
1. A DMT is a history or ?message board? of
dialogue contributions, organized by ?thread?,
based on activities.
2. A DMT classifies which incoming utterances
can be interpreted in the current dialogue con-
text, and which cannot be. It thus delimits
a space of possible Information State update
functions.
3. A DMT has an Active Node List which con-
trols the order in which this function space is
searched 6.
4. A DMT classifies how incoming utterances are
to be interpreted in the current dialogue con-
text.
In general, then, we can think of the DMT as
representing a function space of dialogue Informa-
6It also defines an ordering on language models for speech
recognition.
tion State update functions. The details of any par-
ticular update function are determined by the node
type (e.g. command, question) and incoming dia-
logue move type and their contents, as well as the
values of Activity Tag and Agent.
Note that this notion of ?Dialogue Move Tree? is
quite different from previous work on dialogue trees,
in that the DMT does not represent a ?parse? of the
dialogue using a dialogue grammar (e.g. (Ahrenberg
et al, 1990)), but instead represents all the threads
in the dialogue, where a thread is the set of utter-
ances which serve a particular dialogue goal. In the
dialogue grammar approach, new dialogue moves
are attached to a node on the right frontier of the
tree, but in our approach, a new move can attach
to any thread, no matter where it appears in the
tree. This means that the system can flexibly in-
terpret user moves which are not directly related to
the current thread (e.g. a user can ignore a system
question, and give a new command, or ask their
own question). Finite-state representations of dia-
logue games have the restriction that the user is con-
strained by the dialogue state to follow a particular
dialogue path (e.g. state the destination, clarify, state
preferred time,     ). No such restriction exists with
DMTs, where dialogue participants can begin and
discontinue threads at any time.
We discuss this further below.
5.2 Interpretation and State Update
The central algorithm controlling dialogue manage-
ment has two main steps, Attachment, and Process
Node;
1. Attachment: Process incoming input conversa-
tional move   with respect to the current DMT
and Active Node List, and ?attach? a new node

interpreting   to the tree if possible.
2. Process Node: process the new node  , if it
exists, with respect to the current information
state. Perform an Information State update us-
ing the dialogue move type and content of  .
When an update function  exists, its effects de-
pend on the details of the incoming input   (in par-
ticular, to the dialogue move type and the contents
of the logical form) and the DMT node to which it
attaches. The possible attachments can be thought
of as adjacency pairs, and each dialogue move class
contains information about which node types it can
attach. For instance the command node type can at-
tach confirmation, yn-question, wh-question, and re-
port nodes.
Examples of different attachments available in our
current system can be seen in Figure 67. For exam-
ple, the first entry in the table states that a command
node, generated by the user, with activity tag  , is
able to attach any system confirmation move with
the same activity tag, any system yes-no question
with that tag, any system wh- question with that tag,
or any system report with that activity tag. Similarly,
the rows for wh-question nodes state that:
  a wh-question by the system with activity tag 
can attach a user?s wh-answer (if it is a possible
answer for that activity)
  a user?s wh-question can attach a system wh-
answer, and no particular activity need be spec-
ified.
These possible attachments delimit the ways in
which dialogue move trees can grow, and thus clas-
sify the dialogue structures which can be captured in
the current system. As new dialogue move types are
added to the system, this table is being extended to
cover other conversation types (e.g. tutoring (Clark
et al, 2001)).
It is worth noting that the node type created af-
ter attachment may not be the same as the dialogue
move type of the incoming conversational move   .
Depending on the particular node which attaches the
new input, and the move type of that input, the cre-
ated node may be of a different type. For exam-
ple, if a wh-question node attaches an input which is
simply a command, the wh-question node may inter-
pret the input as an answer, and attach a wh-answer.
These interpretation rules are local to the node to
which the input is attached. In this way, the DMT
interprets new input in context, and the pragmatics
of each new input is contextually determined, rather
than completely specified via parsing using conver-
sational move types. Note that Figure 6 does not
state what move type new input is attached as, when
it is attached.
7Where Activity Tags are not specified, attachment does not
depend on sharing of Activity Tags.
In the current system, if the user produces an ut-
terance which can attach to several nodes on the
DMT, only the ?most active? node (as defined by the
Active Node List) will attach the incoming move. It
would be interesting to explore such events as trig-
gers for clarification questions, in future work.
6 Message generation
Since the robot is potentially carrying out multiple
activities at once, a particular problem is how to de-
termine appropriate generation of utterances about
those activities, in a way which does not overload
the user with information, yet which establishes and
maintains appropriate context in a natural way.
Generation for dialogue systems in general is
problematic in that dialogue contributions arise in-
crementally, often in response to another partici-
pant?s utterances. For this reason, generation of
large pieces of text is not appropriate, especially
since the user is able to interrupt the system. Other
differences abound, for example that aggregation
rules must be sensitive to incremental aspects of
message generation.
As well as the general problems of message selec-
tion and aggregation in dialogue systems, this par-
ticular type of application domain presents specific
problems in comparison with, say, travel-planning
dialogue systems ? e.g. (Seneff et al, 1991). An au-
tonomous device will, in general, need to communi-
cate about,
  its perceptions of a changing environment,
  progress towards user-specified goals,
  execution status of activities or tasks,
  its own internal state changes,
  the progress of the dialogue itself.
For these reasons, the message selection and gen-
eration component of such a system needs to be
of wider coverage and more flexible than template-
based approaches, while remaining in real, or near-
real, time (Stent, 1999). As well as this, the system
must potentially be able to deal with a large band-
width stream of communications from the robot,
and so must be able to intelligently filter them for
?relevance? so that the user is not overloaded with
unimportant information, or repetitious utterances.
In general, the system should appear as ?natural? as
possible from the user?s point of view ? using the
same language as the user if possible (?echoing?),
using anaphoric referring expressions where possi-
ble, and aggregating utterances where appropriate.
A ?natural? system should also exhibit ?variability?
in that it can convey the same content in a variety
of ways. A further desirable feature is that the sys-
tem?s generated utterances should be in the cover-
age of the dialogue system?s speech recognizer, so
that system-generated utterances effectively prime
the user to speak in-grammar.
Consequently we attempted to implement the fol-
lowing features in message selection and generation:
relevance filtering; recency filtering; echoing; vari-
ability; aggregation; symmetry; real-time genera-
tion.
Our general method is to take as inputs to the pro-
cess various communicative goals of the system, ex-
pressed as logical forms, and use them to construct a
single new logical form to be input to Gemini?s Se-
mantic Head-Driven Generation algorithm (Shieber
et al, 1990), which produces strings for Festival
speech synthesis. We now describe how to use com-
plex dialogue context to produce natural generation
in multitasking contexts.
6.1 Message selection - filtering
Inputs to the selection and generation module are
?concept? logical forms (LFs) describing the com-
municative goals of the system. These are struc-
tures consisting of context tags (e.g. activity identi-
fier, dialogue move tree node, turn tag) and a con-
tent logical form consisting of a Dialogue Move
(e.g. report, wh-question), a priority tag (e.g. warn
or inform), and some additional content tags (e.g.
for objects referred to). An example input logical
form is, ?report(inform, agent(AgentID), cancel-
activity(ActivityID))?, which corresponds to the re-
port ?I have cancelled flying to the tower? when
AgentID refers to the robot and ActivityID refers to
a ?fly to the tower? task.
Items which the system will consider for genera-
tion are placed (either directly by the robot, or indi-
rectly by the Activity Tree) on the ?System Agenda?
(SA), which is the part of the dialogue Information
State which stores communicative goals of the sys-
tem. Communicative goals may also exist on the
?Pending List? (PL) which is the part of the infor-
mation state which stores questions that the system
has asked, but which the user has not answered, so
that they may be re-raised by the system. Only ques-
tions previously asked by the system can exist on the
Pending List.
Due to multi-tasking, at any time there is a num-
ber of ?Current Activities? which the user and sys-
tem are performing (e.g. fly to the tower, search for
a red car). These activities are topics of conversa-
tion (defining threads of the DMT) represented in
the dialogue information state, and the system?s re-
ports can be generated by them (in which case the
are tagged with that activity label) or can be rele-
vant to an activity in virtue of being about an object
which is in focus because it is involved in that activ-
ity.
Some system reports are more urgent that others
(e.g. ?I am running out of fuel?) and these carry the
label warning. Warnings are always relevant, no
matter what activities are current ? they always pass
the recency and relevance filters.
Echoing (for noun-phrases) is achieved by access-
ing the Salience List whenever generating referential
terms, and using whatever noun-phrase (if any) the
user has previously employed to refer to the object
in question. If the object is top of the salience list,
the generator will select an anaphoric expression.
The end result of our selection and aggregation
module (see section 6.2) is a fully specified logi-
cal form which is to be sent to the Semantic-Head-
Driven Generation component of Gemini (Shieber
et al, 1990). The bi-directionality of Gemini (i.e.
that we use the same grammar for both parsing and
generation) automatically confers a useful ?symme-
try? property on the system ? that it only utters sen-
tences which it can also understand. This means that
the user will not be misled by the system into em-
ploying out-of-vocabulary items, or out-of-grammar
constructions. Another side effect of this is that
the system utterances prime the user to make in-
grammar utterances, thus enhancing co-ordination
between user and system in the dialogues.
6.2 Incremental aggregation
Aggregation combines and compresses utterances to
make them more concise, avoid repetitious language
structure, and make the system?s speech more nat-
ural and understandable. In a dialogue system ag-
gregation should function incrementally because ut-
terances are generated on the fly. In dialogue sys-
tems, when constructing an utterance we often have
no information about the utterances that will follow
it, and thus the best we can do is to compress it
or ?retro-aggregate? it with utterances that preceded
it. Only occasionally does the System Agenda con-
tain enough unsaid utterances to perform reasonable
?pre-aggregation?.
Each dialogue move type (e.g. report, wh-
question) has its own aggregation rules, stored in
the class for that LF type. In each type, rules spec-
ify which other dialogue move types can aggregate
with it, and exactly how aggregation works. The
rules note identical portions of LFs and unify them,
and then combine the non-identical portions appro-
priately.
For example, the LF that represents the phrase ?I
will fly to the tower and I will land at the parking
lot?, will be converted to one representing ?I will fly
to the tower and land at the parking lot? according
to the compression rules. Similarly, ?I will fly to the
tower and fly to the hospital? gets converted to ?I
will fly to the tower and the hospital?.
The ?retro-aggregation? rules result in sequences
of system utterances such as, ?I have cancelled fly-
ing to the school. And the tower. And landing at the
base.?
7 Summary
We explained the dialogue modelling techniques
which we implemented in order to build a real-
time multi-modal conversational interface to an au-
tonomous device. The novel issues tackled by the
system and its dialogue model are that it is able to
manage conversations about multiple tasks and col-
laborative activities in a robust and natural way.
We argued that in the case of dialogues with
devices, a dialogue management mechanism has
to be particularly robust and flexible, especially
in comparison with finite-state or frame-based di-
alogue managers which have been developed for
information-seeking dialogues, such as travel plan-
ning, where topics of conversation are predeter-
mined. Another challenge was that conversations
may have multiple open topics at any one time, and
this complicates utterance interpretation and gener-
ation.
We discussed the dialogue context model and al-
gorithms used to produce a system with the follow-
ing features:
  supports multi-tasking, multiple topics, and
collaboration,
  support of commands, questions, revisions, and
reports, over a dynamic environment,
  multi-modal, mixed-initiative, open-ended dia-
logues,
  echoic and variable message generation, fil-
tered for relevance and recency
  asynchronous, real-time operation.
An video demonstration of the system is avail-
able at www-csli.stanford.edu/semlab/
witas/.
References
Lars Ahrenberg, Arne Jonsson, and Nils Dalhbeck. 1990.
Discourse representation and discourse management
for natural language interfaces. In In Proceedings of
the Second Nordic Conference on Text Comprehension
in Man and machine.
James Allen, Donna Byron, Myroslva Dzikovska, George
Ferguson, Lucian Galescu, and Amanda Stent. 2001.
Toward conversational human-computer interaction.
AI Magazine, 22(4):27?37.
Brady Clark, John Fry, Matt Ginzton, Stanley Pe-
ters, Heather Pon-Barry, and Zachary Thomsen-Gray.
2001. Automated tutoring dialogues for training in
shipboard damage control. In Proceedings of SIGdial
2001.
Patrick Doherty, Go?sta Granlund, Krzystof Kuchcinski,
Erik Sandewall, Klas Nordberg, Erik Skarman, and Jo-
han Wiklund. 2000. The WITAS unmanned aerial
vehicle project. In European Conference on Artificial
Intelligence (ECAI 2000).
Renee Elio and Afsaneh Haddadi. 1999. On abstract
task models and conversation policies. In Workshop
on Specifying and Implementing Conversation Poli-
cies, Autonomous Agents?99, Seattle.
Jonathan Ginzburg, Ivan A. Sag, and Matthew Purver.
2001. Integrating Conversational Move Types in
the Grammar of Conversation. In Bi-Dialog 2001?
Proceedings of the 5th Workshop on Formal Semantics
and Pragmatics of Dialogue, pages 45?56.
Beth-Ann Hockey, Gregory Aist, Jim Hieronymous,
Oliver Lemon, and John Dowding. 2002. Targeted
help: Embedded training and methods for evaluation.
In Proceedings of Intelligent Tutoring Systems (ITS).
(to appear).
Oliver Lemon, Anne Bracy, Alexander Gruenstein, and
Stanley Peters. 2001. Information states in a multi-
modal dialogue system for human-robot conversation.
In Peter Ku?hnlein, Hans Reiser, and Henk Zeevat, edi-
tors, 5th Workshop on Formal Semantics and Pragmat-
ics of Dialogue (Bi-Dialog 2001), pages 57 ? 67.
Oliver Lemon, Alexander Gruenstein, and Stanley Peters.
2002. Collaborative activities and multi-tasking in di-
alogue systems. Traitement Automatique des Langues
(TAL). Special Issue on Dialogue (to appear).
Susann LuperFoy, Dan Loehr, David Duff, Keith Miller,
Florence Reeder, and Lisa Harper. 1998. An architec-
ture for dialogue management, context tracking, and
pragmatic adaptation in spoken dialogue systems. In
COLING-ACL, pages 794 ? 801.
Micheal McTear. 1998. Modelling spoken dialogues
with state transition diagrams: Experiences with the
CSLU toolkit. In Proc 5th International Conference
on Spoken Language Processing.
Charles Rich, Candace Sidner, and Neal Lesh. 2001.
Collagen: applying collaborative discourse theory to
human-computer interaction. AI Magazine, 22(4):15?
25.
S. Seneff, L. Hirschman, and V. W. Zue. 1991. Interac-
tive problem solving and dialogue in the ATIS domain.
In Proceedings of the Fourth DARPA Speech and Nat-
ural Language Workshop. Morgan Kaufmann.
Stuart M. Shieber, Gertjan van Noord, Fernando C. N.
Pereira, and Robert C. Moore. 1990. Semantic-
head-driven generation. Computational Linguistics,
16(1):30?42.
Amanda Stent. 1999. Content planning and generation
in continuous-speech spoken dialog systems. In Pro-
ceedings of KI?99 workshop ?May I Speak Freely??.
Figure 5: A snapshot of an Information State (from the HTML system logs)
Utterance: ??now taking off?? (by System 11/7/01 4:50 PM)
Conversational Move:
report(inform,agent([np([n(uav,sg)])]),curr_activity([command([take_off])]))
Dialogue Move Tree (position on active node list in parens [0 = most active])
* Root (1)
Root
o Command (0)
command([go],[param_list([pp_loc(to,arg([np(det([def],the),[n(tower,
sg)])]))])]) [[dmtask0] current]
+ Report
report(inform,agent([np([n(uav,sg)])]),curr_activity([command
([take_off])]))[]
o Report
report(inform,agent([np([n(uav,sg)])]),confirm_activity([command([go],
[param_list([pp_loc(to,arg([np(det([def],the),[n(tower,sg)],
)]))])])])) [[dmtask0] current]
Activity Tree
* root
o [dmtask0] current
relation = SEQuential
command = go
pp = pp_loc(to,Args)
np = np(det([def],the),[n(tower,sg)])
+ [sim3] current
relation = none
command = take_off
pp = null, np = null
Salience List (least salient -- most salient)
* [np(det([def],the),[n(tower,sg)])] (speech)
* [np(det([def],the),[n(tower,sg)])] (speech)
Figure 6: Attachment in the Dialogue Move Classes
DMT Node Attaches
Node Type Activity
Tag
Speaker Node Type Activity
Tag
Speaker
command t user confirmation, t system
y-n question, t system
wh-question, t system
report t system
confirmation t system
report t system command t user
wh-question t system wh-answer t user
wh-question user wh-answer system
yn-question t system yn-answer t user
revision t user wh-question t system
yn-answer t user confirmation t system
wh-answer user confirmation system
wh-answer system confirmation user
root n/a n/a command, user
question, user
revision user
root n/a n/a report system
Figure 7: Part of the Graphical User Interface, showing a flight plan
Probabilistic Dialogue Modelling
Oliver Lemon
CSLI
Stanford University
lemon@csli.stanford.edu,
Prashant Parikh
IRCS
University of Pennsylvania
pjparikh@aol.com
Stanley Peters
CSLI
Stanford University
peters@csli.stanford.edu
Abstract
We show how Bayesian networks and re-
lated probabilistic methods provide an ef-
ficient way of capturing the complex bal-
ancing of different factors that determine
interpretation and generation in dialogue.
As a case study, we show how a prob-
abilistic approach can be used to model
anaphora resolution in dialogue1 .
1 Introduction
The use of probabilistic and decision-theoretic in-
ference in dialogue modelling and management has
been explored in preliminary fashion by (Pulman,
1996) and (Keizer, 2001). Probabilistic meth-
ods look promising when modelling systems where
there is uncertainty, and simple true/false judge-
ments obscure some of the subtleties of represen-
tation and processing that are required of an accu-
rate model. Dialogue systems are of this nature be-
cause uncertainty is present due to speech recogni-
tion noise, speech-act uncertainty, and so on. Epis-
temic uncertainty is rife in dialogue, and probability
distributions provide a natural model of the ambigu-
ities that thus arise. For these reasons it is natural to
explore probabilistic representations and algorithms
in dialogue management, rather than purely deter-
ministic models. We have experience building deter-
ministic dialogue managers (see e.g. (Lemon et al,
1This research was (partially) funded under the Wallenberg
laboratory for research on Information Technology and Au-
tonomous Systems (WITAS) Project, Linko?ping University, by
the Wallenberg Foundation, Sweden.
2001; Lemon et al, 2002)) which use deterministic
context update rules.
This paper briefly describes our construction of a
Bayes Net modelling dialogue context. We will con-
sider a series of examples of increasing complexity
involving anaphoric resolution in Section 3.1. We
will point out how they are to be resolved intuitively,
and then discuss how our Bayesian net fares. We
will see that many of the best insights of determin-
istic approaches (e.g. in the axiomatic BDI tradition
and in the planning literature) can be preserved, of-
ten in less brittle forms, in a probabilistic setting.
1.1 Probabilistic modelling ideas
Our approach to resolving anaphora (and dialogue
moves) was to generate a probability distribution of
the random variable of interest (e.g. salience of ref-
erent) and then choose the value of the variable cor-
responding to the highest probability as the interpre-
tation (e.g. the referent). This decision has a theo-
retical justification that can be found in a theorem in
(Parikh, 1990) in the context of his game-theoretic
model of language use. The theorem states that un-
der certain conditions (which hold in our context)
the correct interpretation of an utterance is the most
likely one.
2 Interpretation and Generation
The two major aspects of dialogue management are
the interpretation of incoming (user) utterances, and
the timely and appropriate generation of utterances
by the dialogue system. To cover these aspects we
have constructed a Bayes Net as shown in Figure 1.
     Philadelphia, July 2002, pp. 125-128.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
In the implementation of this network we used
CIspace?s Java-based Bayes Net toolkit.2
The conditional probability table for the nodes
representing the dialogue move at time   and
salience list at time   are obviously the core of
the network. These tables are too large to present
in this paper. We constructed them by hand, us-
ing heuristics gained from experience in program-
ming rule-based dialogue systems. In future, the ta-
bles could be learned from data, or could instantiate
continuous-valued functions of rule-based systems.
Salience
List
t?1
System
Utterance
t
User 
Dialogue
Move
t
User
Dialogue
Move
t?1
Dialogue
Move
System
t?1
List
Salience
t
t
Activity
Input 
User 
Logical 
Form t
Figure 1: A Prototype Bayes Net for dialogue man-
agement
3 Anaphora resolution
Several different factors enter into the resolution of
anaphora in a dialogue. How recently a potential
referent was referred to is one important factor, an-
other is the embedding activity within which the
anaphoric reference was made (e.g. the type of verb
phrase in which the referent appears), a third is the
intra-sentential location of the relevant noun phrases
in the preceding dialogue, a fourth is the relative
prominence of a potential referent in the dialogue
situation, and so on. The basic idea is that condi-
2www.cs.ubc.ca/labs/lci/CIspace/
tional probability distributions are generated dynam-
ically in the Bayesian network. When we look at a
distribution corresponding to a node we are inter-
ested in, then the most salient object in the context
will be the one whose value has the highest proba-
bility.
We point out that an obvious deterministic way
to rank different combinations of factors (in an
optimality-theoretic way, for example), and choose
the most salient object based on this ranking, does
not seem to work ? because any potential ranking
of principles (e.g. ?Recency overrides subject place-
ment?) would have to be context-dependent, and this
defeats the idea of a ranking. See the examples in
Figures 5 and 6.
3.1 Examples
Here we work with two basic activities of the
WITAS robot helicopter (see (Lemon et al, 2002))
which our dialogue system interfaces to ? moving
and searching. The helicopter can move to vari-
ous landmarks (e.g. the tower, the church) and way-
points, and can see various objects (e.g. landmarks,
vehicles). Our results use the network in Figure 1. In
reading the tables (the figures appear after the refer-
ences), use the following key:
U=user, S=system, r=red car, g=green car,
w=waypoint, s= search, m=move. All examples
start with an even distribution of 0.2 for all variables
(all objects are equally salient) at the start of each
dialogue.
3.1.1 Activity and recency
We will start with what may be the simplest type
of example of anaphoric reference, in Figure 2. Here
it is intuitively clear that ?it? is intended to pick out
the green car. The contribution of ?see it? is mod-
elled as an observed even distribution over all possi-
ble referents which can be seen (i.e.  and  each
get a 0.5 weight). The conditional probability table
for Salience List at time   is then used to compute the
new probability distribution over the object-activity
pairs ( 	
		
 ). Here we see that the
green car is the most salient after the user?s second
utterance (  ), and that this salience increases after
the utterance ?it?, because  was both the most re-
cent NP, and is also a possible object in the context
of the ?seeing? activity.
In the example in Figure 3, the anaphoric pronoun
?it? should pick out the red car and not the waypoint,
even though the waypoint was referred to more re-
cently. Intuitively, this is because the embedding ac-
tivity of looking for the red car is tied to the pro-
noun, and this fact overrides the most recent refer-
ent. Here, the waypoint is not a possible object in
the ?seeing? activity, whereas the red car has been
introduced as part of that activity. Thus the pronoun
?it? in the user?s final utterance has the effect of rais-
ing the probabilities of all the objects which can be
seen, and this in fact overrides the recency effect of
the utterance of ?waypoint?.
An extended example (not shown) shows how ac-
tivity information can outweigh recency in an inter-
leaved fashion and then that a newly introduced ref-
erent can become the most salient. Having consid-
ered the ways in which activity and recency interact
in determining salience for anaphoric resolution, we
then investigated adding another determining factor
in the model ? the syntactic placement of the refer-
ring expression.
3.1.2 Placement, activity, and recency
Figure 4 shows how subject placement influences
availability for anaphoric reference. Here, the sub-
ject (?red car?) of the user?s second utterance is in-
tuitively the one picked out by the later anaphoric
expression, and not the green car, even though ?the
green car? is the most recent NP. See Figure 4 for our
results, using an extension of the network in Figure
1, where the ?Activity   ? node was enriched to in-
clude syntactic information about the input ? specif-
ically, what referring expressions appear in subject
and object places. Note here that the red car be-
comes the most salient object after the user?s sec-
ond utterance. We model the referential import of
this sentence as an input triple ?     ?to the Activ-
ity   node ? denoting: red car (subject), no activity,
green car (object). The updated table for this node
ensures that objects in subject place are given more
weight than those in object place.
In Figure 5, the subject (?red car?) of the user?s
second utterance is intuitively the one picked out by
the later anaphoric expression, and not the green car,
even though ?the green car? is involved in the ?see-
ing? activity.
In Figure 6 the red car is most salient after the
second utterance, but the waypoint becomes more
salient, even though the red car was in subject po-
sition, because the waypoint is involved in the ac-
tivity of moving, as is the pronoun ?it?, and so is
a better candidate for anaphoric resolution. Com-
bined with Figure 5 this shows that no static ranking
of anaphoric binding principles will cover all situ-
ations, and that a probabilistic approach is useful ?
even as a theoretical model.
Obviously this model could be made more com-
plex with representations for direct and indirect ob-
jects, and so forth, but we leave this for future work.
4 Conclusion
We presented a Bayes Net which we have imple-
mented to deal with dialogue move interpretation
and reference resolution, and gave examples of its
use for weighing a variety of factors (recency, activ-
ity, placement) in anaphoric resolution in particular.
We saw that many of the insights of deterministic ap-
proaches (e.g. the WITAS Project, see (Lemon et al,
2002)) can be preserved, often in less brittle forms,
in a probabilistic setting. We also have unpublished
results for dialogue move classification.
References
Simon Keizer. 2001. A probabilistic approach to dia-
logue act clarification. In Proceedings of Bi-Dialog
2001.
Oliver Lemon, Anne Bracy, Alexander Gruenstein, and
Stanley Peters. 2001. Information states in a multi-
modal dialogue system for human-robot conversation.
In Peter Ku?hnlein, Hans Reiser, and Henk Zeevat, edi-
tors, 5th Workshop on Formal Semantics and Pragmat-
ics of Dialogue (Bi-Dialog 2001), pages 57 ? 67.
Oliver Lemon, Alexander Gruenstein, Alexis Battle, and
Stanley Peters. 2002. Multi-tasking and collabora-
tive activities in dialogue systems. In Proceedings
of 3rd SIGdial Workshop on Discourse and Dialogue,
Philadelphia. (to appear).
Prashant Parikh. 1990. Situations, games, and ambigu-
ity. In R. Cooper, K. Mukai, and J. Perry, editors, Situ-
ation Theory and its Applications I. CSLI Publications.
Prashant Parikh. 2001. The Use of Language. CSLI
Publications, Stanford, CA.
Stephen G. Pulman. 1996. Conversational games, belief
revision and bayesian networks. In 7th Computational
Linguistics in the Netherlands (CLIN) meeting.
Utterance P(rm) P(gm) P(wm) P(rs) P(gs) Salient
U: Search for the red car .22 .06 .06 .6 .06 red car
S: Okay
U: Go to the green car .066 .53 .018 .18 .206 green car
S: Okay
U: Can you see it? .0196 .7002 .0054 .054 .2206 green car
Figure 2: Probability distributions in example: Recency
Utterance P(rm) P(gm) P(wm) P(rs) P(gs) Salient
U: Do you see the red car? .18 .06 .06 .64 .06 red car
S: No
U: Go to the waypoint .054 .018 .718 .192 .008 waypoint
S: Okay
U: Do you see it? .1108 .1036 .2154 .313 .2572 red car
Figure 3: Probability distributions in example: Activity overrides recency
Utterance P(rm) P(gm) P(wm) P(rs) P(gs) Salient
U: Go to the green car .06 .524 .06 .06 .296 green car
S: Okay
U: The red car is beside the
green car
.333 .1899 .018 .333 .1261 red car
S: Okay
U: Can you see it? .1955 .1622 .0054 .3543 .2825 red car
Figure 4: Probability distributions in example: Subject placement overrides recency
Utterance P(rm) P(gm) P(wm) P(rs) P(gs) Salient
U: Search for the green car .06 .296 .06 .06 .524 green car
S: Okay
U: The red car is beside the
green car
.263 .2006 .018 .263 .2554 red car
S: Okay
U: Can you see it? .1796 .1622 .0054 .3282 .3206 red car
Figure 5: Probability distributions in example: Subject placement overrides activity
Utterance P(rm) P(gm) P(wm) P(rs) P(gs) Salient
U: Go to the waypoint .06 .06 .76 .06 .06 waypoint
R: The red car is at the way-
point
.333 .018 .298 .333 .018 red car
U: Did you get to it? .2888 .1582 .3288 .1444 .0913 waypoint
Figure 6: Probability distributions for example: Activity overrides subject placement
Unsupervised Monolingual and Bilingual Word-Sense
Disambiguation of Medical Documents using UMLS
Dominic Widdows, Stanley Peters, Scott Cederberg, Chiu-Ki Chan
Stanford University, California
{dwiddows,peters,cederber,ckchan}@csli.stanford.edu
Diana Steffen
Consultants for Language Technology,
Saarbru?cken, Germany
steffen@clt-st.de
Paul Buitelaar
DFKI, Saarbru?cken, Germany
paulb@dfki.de
Abstract
This paper describes techniques for unsu-
pervised word sense disambiguation of En-
glish and German medical documents us-
ing UMLS. We present both monolingual
techniques which rely only on the structure
of UMLS, and bilingual techniques which
also rely on the availability of parallel cor-
pora. The best results are obtained using
relations between terms given by UMLS,
a method which achieves 74% precision,
66% coverage for English and 79% preci-
sion, 73% coverage for German on evalua-
tion corpora and over 83% coverage over the
whole corpus. The success of this technique
for German shows that a lexical resource
giving relations between concepts used to
index an English document collection can
be used for high quality disambiguation in
another language.
1 Introduction
This paper reports on experiments in monolingual
and multilingual word sense disambiguation (WSD)
in the medical domain using the Unified Medical
Language System (UMLS). The work described was
carried out as part of the MUCHMORE project 1 for
multilingual organisation and retrieval of medical in-
formation, for which WSD is particularly important.
The importance of WSD to multilingual applica-
tions stems from the simple fact that meanings repre-
sented by a single word in one language may be rep-
resented by multiple words in other languages. The
English word drug when referring to medically ther-
apeutic drugs would be translated as medikamente,
1http://muchmore.dfki.de
while it would be rendered as drogen when referring
to a recreationally taken narcotic substance of the
kind that many governments prohibit by law.
The ability to disambiguate is therefore essential
to the task of machine translation ? when translat-
ing from English to Spanish or from English to Ger-
man we would need to make the distinctions men-
tioned above and other similar ones. Even short of
the task of full translation, WSD is crucial to ap-
plications such as cross-lingual information retrieval
(CLIR), since search terms entered in the language
used for querying must be appropriately rendered in
the language used for retrieval. WSD has become a
well-established subfield of natural language process-
ing with its own evaluation standards and SENSE-
VAL competitions (Kilgarriff and Rosenzweig, 2000).
Methods for WSD can effectively be divided into
those that require manually annotated training data
(supervised methods) and those that do not (unsu-
pervised methods) (Ide and Ve?ronis, 1998). In gen-
eral, supervised methods are less scalable than unsu-
pervised methods because they rely on training data
which may be costly and unrealistic to produce, and
even then might be available for only a few ambigu-
ous terms. The goal of our work on disambiguation
in the MUCHMORE project is to enable the correct
semantic annotation of entire document collections
with all terms which are potentially relevant for or-
ganisation, retrieval and summarisation of informa-
tion. Therefore a decision was taken early on in the
project that we should focus on unsupervised meth-
ods, which have the potential to be scaled up enough
to meet our needs.
This paper is arranged as follows. In Section 2 we
describe the lexical resource (UMLS) and the cor-
pora we used for our experiments. We then describe
and evaluate three different methods for disambigua-
tion. The bilingual method (Section 3) takes ad-
vantage of our having a translated corpus, because
knowing the translation of an ambiguous word can
be enough to determine its sense. The collocational
method (Section 4) uses the occurence of a term in a
recognised fixed expression to determine its meaning.
UMLS relation based methods (Section 5) use rela-
tions between terms in UMLS to determine which
sense is being used in a particular instance. Other
techniques used in the MUCHMORE project in-
clude domain-specific sense selection (Buitelaar and
Sacaleanu, 2001), used to select senses appropri-
ate to the medical domain from a general lexical
resource, and instance-based learning, a machine-
learning technique that has been adapted for word-
sense disambiguation (Widdows et al, 2003).
2 Language resources used in these
experiments
2.1 Lexical Resource ? UMLS
The Unified Medical Language System (UMLS) is
a resource that contains linguistic, terminological
and semantic information in the medical domain.2
It is organised in three parts: Specialist Lexi-
con, MetaThesaurus and Semantic Network. The
MetaThesaurus contains concepts from more than
60 standardised medical thesauri, of which for our
purposes we only use the concepts from MeSH (the
Medical Subject Headings thesaurus). This decision
is based on the fact that MeSH is also available in
German. The semantic information that we use in
annotation is the so-called Concept Unique Identifier
(CUI), a code that represents a concept in the UMLS
MetaThesaurus. We consider the possible ?senses? of
a term to be the set of CUI?s which list this term
as a possible realisation. For example, UMLS con-
tains the term trauma as a possible realisation of the
following two concepts:
C0043251 Injuries and Wounds: Wounds
and Injuries: trauma: traumatic disorders:
Traumatic injury:
C0021501 Physical Trauma: Trauma
(Physical): trauma:
Each of these CUI?s is a possible sense of the term
trauma. The term trauma is therefore noted as am-
biguous, since it can be used to express more than
one UMLS concept. The purpose of disambiguation
is to find out which of these possible senses is ac-
tually being used in each particular context where
there term trauma is used.
2UMLS is freely available under license from
the United States National Library of Medicine,
http://www.nlm.nih.gov/research/umls/
CUI?s in UMLS are also interlinked to each other
by a number of relations. These include:
? ?Broader term? which is similar to the hyper-
nymy relation in WordNet (Fellbaum, 1998). In
general, x is a ?broader term? for y if every y is
also a (kind of) x.
? More generally, ?related terms? are listed, where
possible relationships include ?is like?, ?is clini-
cally associated with?.
? Cooccurring concepts, which are pairs of con-
cepts which are linked in some information
source. In particular, two concepts are regarded
as cooccurring if they have both been used to
manually index the same document in MED-
LINE. We will refer to such pairs of concepts
as coindexing concepts.
? Collocations and multiword expressions. For ex-
ample, the term liver transplant is included sep-
arately in UMLS, as well as both the terms liver
and transplant. This information can sometimes
be used for disambiguation.
2.2 The Springer Corpus of Medical
Abstracts
The experiments and implementations of WSD de-
scribed in this paper were all carried out on a par-
allel corpus of English-German medical scientific ab-
stracts obtained from the Springer Link web site.3
The corpus consists approximately of 1 million to-
kens for each language. Abstracts are from 41 medi-
cal journals, each of which constitutes a relatively ho-
mogeneous medical sub-domain (e.g. Neurology, Ra-
diology, etc.). The corpus was automatically marked
up with morphosyntactic and semantic information,
as described by S?pela Vintar et al (2002). In brief,
whenever a token is encountered in the corpus that is
listed as a term in UMLS, the document is annotated
with the CUI under which that term is listed. Ambi-
guity is introduced by this markup process because
the lexical resources often list a particular term as a
possible realisation of more than one concept or CUI,
as with the trauma example above, in which case
the document is annotated with all of these possible
CUI?s.
The number of tokens of UMLS terms included by
this annotation process is given in Table 1. The table
shows how many tokens were found by the annota-
tion process, listed according to how many possible
senses each of these tokens was assigned in UMLS (so
that the number of ambiguous tokens is the number
3http://link.springer.de/
Number of Senses 1 2 3 4
Before Disambiguation
English 223441 31940 3079 56
German 124369 7996 0 0
After Disambiguation
English 252668 5299 568 5
German 131302 1065 0 0
Table 1: The number of tokens of terms that have 1,
2, 3 and 4 possible senses in the Springer corpus
of tokens with more than one possible sense). The
greater number of concepts found in the English cor-
pus reflects the fact that UMLS has greater cover-
age for English than for German, and secondly that
there are many small terms in English which are ex-
pressed by single words which would be expressed
by larger compound terms in German (for exam-
ple knee + joint = kniegelenk). Table 1 also shows
how many tokens of UMLS concepts were in the an-
notated corpus after we applied the disambiguation
process described in Section 5, which proved to be
our most successful method. As can be seen, our
disambiguation methods resolved some 83% of the
ambiguities in the English corpus and 87% of the
ambiguities in the German corpus (we refer to this
proportion as the ?Coverage? of the method). How-
ever, this only measures the number of disambigua-
tion decisions that were made: in order to determine
how many of these decisions were correct, evaluation
corpora were needed.
2.3 Evaluation Corpora
An important aspect of word sense disambiguation is
the evaluation of different methods and parameters.
Unfortunately, there is a lack of test sets for evalu-
ation, specifically for languages other than English
and even more so for specific domains like medicine.
Given that our work focuses on German as well as
English text in the medical domain, we had to de-
velop our own evaluation corpora in order to test our
disambiguation methods.
Because in the MUCHMORE project we devel-
oped an extensive format for linguistic and semantic
annotation (S?pela Vintar et al, 2002) that includes
annotation with UMLS concepts, we could automat-
ically generate lists of all ambiguous UMLS types
(English and German) along with their token fre-
quencies in the corpus. Using these lists we selected a
set of 70 frequent types for English (token frequencies
at least 28, 41 types having token frequencies over
100). For German, we only selected 24 ambiguous
types (token frequencies at least 11, 7 types having
token frequencies over 100) because there are fewer
ambiguous terms in the German annotation (see Ta-
ble 1). We automatically selected instances to be
annotated using a random selection of occurrences if
the token frequency was higher than 100, and using
all occurrences if the token frequency was lower than
100. The level of ambiguity for these UMLS terms is
mostly limited to only 2 senses; only 7 English terms
have 3 senses.
Correct senses of the English tokens in context
were chosen by three medical experts, two native
speakers of German and one of English. The Ger-
man evaluation corpus was annotated by the two
German speakers. Interannotator agreement for in-
dividual terms ranged from very low to very high,
with an average of 65% for German and 51% for En-
glish (where all three annotators agreed). The rea-
sons for this low score are still under investigation.
In some cases, the UMLS definitions were insufficient
to give a clear distinction between concepts, espe-
cially when the concepts came from different origi-
nal thesauri. This allowed the decision of whether
a particular definition gave a meaningful ?sense? to
be more or less subjective. Approximately half of
the disagreements between annotators occured with
terms where interannotator agreement was less than
10%, which is evidence that a significant amount of
the disagreement between annotators was on the type
level rather than the token level. In other cases, it
is possible that there was insufficient contextual in-
formation provided for annotators to agree. If one of
the annotators was unable to choose any of the senses
and declared an instance to be ?unspecified?, this also
counted against interannotator agreement. What-
ever is responsible, our interannotator agreement fell
far short of the 88%-100% achieved in SENSEVAL
(Kilgarriff and Rosenzweig, 2000, ?7), and until this
problem is solved or better datasets are found, this
poor agreement casts doubt on the generality of the
results obtained in this paper.
A ?gold standard? was produced for the German
UMLS evaluation corpus and used to evaluate the
disambiguation of German UMLS concepts. The En-
glish experiments were evaluated on those tokens for
which the annotators agreed. More details and dis-
cussion of the annotation process is available in the
project report (Widdows et al, 2003).
In the rest of this paper we describe the techniques
that used these resources to build systems for word
sense disambiguation, and evaluate their level of suc-
cess.
3 Bilingual Disambiguation
The mapping between word-forms and senses differs
across languages, and for this reason the importance
of word-sense disambiguation has long been recog-
nised for machine translation. By the same token,
pairs of translated documents naturally contain in-
formation for disambiguation. For example, if in a
particular context the English word drugs is trans-
lated into French as drogues rather than medica-
ments, then the English word drug is being used
to mean narcotics rather than medicines. This ob-
servation has been used for some years on varying
scales. Brown et al (1991) pioneered the use of sta-
tistical WSD for translation, building a translation
model from one million sentences in English and
French. Using this model to help with translation
decisions (such as whether prendre should be trans-
lated as take or make), the number of acceptable
translations produced by their system increased by
8%. Gale et al (1992) use parallel translations to
obtain training and testing data for word-sense dis-
ambiguation. Ide (1999) investigates the information
made available by a translation of George Orwell?s
Nineteen Eighty-four into six languages, using this
to analyse the related senses of nine ambiguous En-
glish words into hierarchical clusters.
These applications have all been case studies of a
handful of particularly interesting words. The large
scale of the semantic annotation carried out by the
MUCHMORE project has made it possible to extend
the bilingual disambiguation technique to entire dic-
tionaries and corpora.
To disambiguate an instance of an ambiguous
term, we consulted the translation of the abstract
in which it appeared. We regarded the translated
abstract as disambiguating the ambiguous term if it
met the following two criteria:
? Only one of the CUI?s was assigned to any term
in the translated abstract.
? At least one of the terms to which this CUI
was assigned in the translated abstract was un-
ambiguous (i.e. was not also assigned another
CUI).
3.1 Results for Bilingual Disambiguation
We attempted both to disambiguate terms in the
German abstracts using the corresponding English
abstracts, and to disambiguate terms in the English
abstracts using the corresponding German ones. In
this collection of documents, we were able to disam-
biguate 1802 occurrences of 63 English terms and
1500 occurrences of 43 German terms. Comparing
this with the evaluation corpora gave the results in
Table 2.4
4In all of the results presented in this paper, Precision
is the proportion of decisions made which were correct
Precision Recall Coverage
English 81% 18% 22%
German 66% 22% 33%
Table 2: Results for bilingual disambiguation
As can be seen, the recall and coverage of this
method is not especially good but the precision (at
least for English) is very high. The German results
contain roughly the same proportion of correct deci-
sions as the English, but many more incorrect ones
as well.
Our disambiguation results break down into three
cases:
1. Terms ambiguous in one language that translate
as multiple unambiguous terms in the other lan-
guage; one of the meanings is medical and the
other is not.
2. Terms ambiguous in one language that trans-
late as multiple unambiguous terms in the other
language; both of the terms are medical.
3. Terms that are ambiguous between two mean-
ings that are difficult to distinguish.
One striking aspect of the results was that rel-
atively few terms were disambiguated to different
senses in different occurrences. This phenomenon
was particularly extreme in disambiguating the Ger-
man terms; of the 43 German terms disambiguated,
42 were assigned the same sense every time we were
able to disambiguate them. Only one term, Metas-
tase, was assigned difference senses; 88 times it was
assigned CUI C0027627 (?The spread of cancer from
one part of the body to another ...?, associated with
the English term Metastasis and 6 times it was as-
signed CUI C0036525 ?Used with neoplasms to in-
dicate the secondary location to which the neoplas-
tic process has metastasized?, corresponding to the
English terms metastastic and secondary). Metas-
tase therefore falls into category 2 from above, al-
though the distinction between the two meanings is
relatively subtle.
The first and third categories above account for
the vast majority of cases, in which only one mean-
ing is ever selected. It is easy to see why this would
according to the evaluation corpora, Recall is the pro-
portion of instances in the evaluation corpora for which
a correct decision was made, and Coverage is the propor-
tion of instances in the evaluation corpora for which any
decision was made. It follows that
Recall = Precision ? Coverage.
happen in the first category, and it is what we want
to happen. For instance, the German term Krebse
can refer either to crabs (Crustaceans) or to cancer-
ous growths; it is not surprising that only the latter
meaning turns up in the corpus under consideration
and that we can determine this from the unambigu-
ous English translation cancers.
In English somewhat more terms were disam-
biguated multiple ways: eight terms were assigned
two different senses across their occurrences. All
three types of ambiguity were apparent. For in-
stance, the second type (medical/medical ambiguity)
appeared for the term Aging, which can refer either
to aging people (Alte Menschen) or to the process of
aging itself (Altern); both meanings appeared in our
corpus.
In general, the bilingual method correctly find the
meanings of approximately one fifth of the ambigu-
ous terms, and makes only a few mistakes for English
but many more for German.
4 Collocational disambiguation
By a ?collocation? we mean a fixed expression formed
by a group of words occuring together, such as
blood vessel or New York. (For the purposes of
this paper we only consider contiguous multiword
expressions which are listed in UMLS.) There is a
strong and well-known tendency for words to ex-
press only one sense in a given collocation. This
property of words was first described and quantified
by Yarowsky (1993), and has become known gen-
erally as the ?One Sense Per Collocation? property.
Yarowsky (1995) used the one sense per collocation
property as an essential ingredient for an unsuper-
vised Word-Sense Disambiguation algorithm. For ex-
ample, the collocations plant life and manufacturing
plant are used as ?seed-examples? for the living thing
and building senses of plant, and these examples can
then be used as high-precision training data to per-
form more general high-recall disambiguation.
While Yarowsky?s algorithm is unsupervised (the
algorithm does not need a large collection of anno-
tated training examples), it still needs direct human
intervention to recognise which ambiguous terms are
amenable to this technique, and to choose appropri-
ate ?seed-collocations? for each sense. Thus the algo-
rithm still requires expert human judgments, which
leads to a bottleneck when trying to scale such meth-
ods to provide Word-Sense Disambiguation for a
whole document collection.
A possible method for widening this bottleneck is
to use existing lexical resources to provide seed collo-
cations. The texts of dictionary definitions have been
used as a traditional source of information for disam-
biguation (Lesk, 1986). The richly detailed structure
of UMLS provides a special opportunity to combine
both of these approaches, because many multiword
expressions and collocations are included in UMLS
as separate concepts.
For example, the term pressure has the following
three senses in UMLS, each of which is assigned to a
different semantic type (TUI):
Sense of pressure Semantic Type
Physical pressure Quantitative Concept
(C0033095)
Pressure - action Therapeutic or
(C0460139) Preventive Procedure
Baresthesia, sensation
of pressure (C0234222)
Organ or Tissue Func-
tion
Many other collocations and compounds which in-
clude the word pressure are also of these semantic
types, as summarised in the following table:
Quantitative
Concept
mean pressure, bar pressure,
population pressure
Therapeutic
Procedure
orthostatic pressure, acupres-
sure
Organ or Tissue
Function
arterial pressure, lung pres-
sure, intraocular pressure
This leads to the hypothesis that the term pres-
sure, when used in any of these collocations, is used
with the meaning corresponding to the same seman-
tic type. This allows deductions of the following
form:
Collocation bar pressure, mean pressure
Semantic type Quantitative Concept
Sense of pressure C0033095, physical pressure
Since nearly all English and German multiword
technical medical terms are head-final, it follows that
the a multiword term is usually of the same seman-
tic type as its head, the final word. (So for example,
lung cancer is a kind of cancer, not a kind of lung.)
For English, UMLS 2001 contains over 800,000 multi-
word expressions the last word in which is also a term
in UMLS. Over 350,000 of these expressions have a
last word which on its own, with no other context,
would be regarded as ambiguous (has more that one
CUI in UMLS). Over 50,000 of these multiword ex-
pressions are unambiguous, with a unique semantic
type which is shared by only one of the meanings of
the potentially ambiguous final word. The ambigu-
ity of the final word in such multiword expressions
is thus resolved, providing over 50,000 ?seed colloca-
tions? for use in semantically annotating documents
with disambiguated word senses.
4.1 Results for collocational disambiguation
Unfortunately, results for collocational disambigua-
tion (Table 3) were disappointing compared with the
promising number of seed collocations we expected
to find. Precision was high, but comparatively few
of the collocations suggested by UMLS were found
in the Springer corpus.
Precision Recall Coverage
English 79% 3% 4%
German 82% 1% 1.2%
Table 3: Results for collocational disambiguation
In retrospect, this may not be surprising given that
many of the ?collocations? in UMLS are rather col-
lections of words such as
C0374270 intracoronary percutaneous
placement s single stent transcatheter vessel
which would almost never occur in natural text.
Thus very few of the potential collocations we ex-
tracted from UMLS actually occurred in the Springer
corpus. This scarcity was especially pronounced for
German, because so many terms which are several
words in English are compounded into a single word
in German. For example, the term
C0035330 retinal vessel
does occur in the (English) Springer corpus and con-
tains the ambiguous word vessel, whose ambiguity is
successfully resolved using the collocational method.
However, in German this concept is represented by
the single word
C0035330 Retinagefaesse
and so this ambiguity never arises in the first place.
It should still be remarked that the few decisions
that were made by the collocational method were
very accurate, demonstrating that we can get some
high precision results using this method. It is pos-
sible that recall could be improved by relaxing the
conditions which a multiword expression in UMLS
must satisfy to be used as a seed-collocation.
5 Disambiguation using related
UMLS terms found in the same
context
While the collocational method turned out to give
disappointing recall, it showed that accurate infor-
mation could be extracted directly from the existing
UMLS and used for disambiguation, without extra
human intervention or supervision. What we needed
was advice on how to get more of this high-quality
information out of UMLS, which we still believed to
be a very rich source of information which we were
not yet exploiting fully. Fortunately, no less than 3
additional sources of information for disambiguation
using related terms from UMLS were suggested by a
medical expert.5 The suggestion was that we should
consider terms that were linked by conceptual rela-
tions (as given by the MRREL and MRCXT files
in the UMLS source) and which were noted as coin-
dexing concepts in the same MEDLINE abstract (as
given by the MRCOC file in the UMLS source). For
each separate sense of an ambiguous word, this would
give a set of related concepts, and if examples of any
of these related concepts were found in the corpus
near to one of the ambiguous words, it might indi-
cate that the correct sense of the ambiguous word
was the one related to this particular concept.
This method is effectively one of the many variants
of Lesk?s (1986) original dictionary-based method for
disambiguation, where the words appearing in the
definitions of different senses of ambiguous words are
used to indicate that those senses are being used if
they are observed near the ambiguous word. How-
ever, we gain over purely dictionary-based methods
because the words that occur in dictionary defini-
tions rarely correspond well with those that occur
in text. The information we collected from UMLS
did not suffer from this drawback: the pairs of coin-
dexing concepts from MRCOC were derived precisely
from human judgements that these two concepts
both occured in the same text in MEDLINE.
The disambiguation method proceeds as follows.
For each ambiguous word w, we find its possible
senses {sj(w)}. For each sense sj , find all CUI?s
in MRREL, MRCXT or MRCOC files that are re-
lated to this sense, and call this set {crel(sj)}. Then
for each occurrence of the ambiguous word w in the
corpus we examine the local context to see if a term
t occurs whose sense6 (CUI) is one of the concepts
in {crel(sj)}, and if so take this as positive evidence
that the sense sj is the appropriate one for this con-
text, by increasing the score of sj by 1. In this way,
each sense sj in context gets assigned a score which
measures the number of terms in this context which
are related to this sense. Finally, choose the sense
5Personal communication from Stuart Nelson (instru-
mental in the design of UMLS), at the MUCHMORE
workshop in Croatia, September 2002.
6This fails to take into account that the term t might
itself be ambiguous ? it is possible that results could be
improved still further by allowing for mutual disambigua-
tion of more than one term at once.
with the highest score.
One open question for this algorithm is what re-
gion of text to use as a context-window. We experi-
mented with using sentences, documents and whole
subdomains, where a ?subdomain? was considered to
be all of the abstracts appearing in one of the jour-
nals in the Springer corpus, such as Arthroskopie
or Der Chirurg. Thus our results (for each lan-
guage) vary according to which knowledge sources
were used (Conceptually Related Terms from MR-
REL and MRCXT or coindexing terms from MR-
COC, or a combination), and according to whether
the context-window for recording cooccurence was a
sentence, a document or a subdomain.
5.1 Results for disambiguation based on
related UMLS concepts
The results obtained using this method (Tables 5.1
and 5.1) were excellent, preserving (and in some
cases improving) the high precision of the bilingual
and collocational methods while greatly extending
coverage and recall. The results obtained by using
the coindexing terms for disambiguation were partic-
ularly impressive, which coincides with a long-held
view in the field that terms which are topically re-
lated to a target word can be much richer clues for
disambiguation that terms which are (say) hierarchi-
cally related. We are very fortunate to have such
a wealth of information about the cooccurence of
pairs of concepts through UMLS, which appears to
have provided the benefits of cooccurence data from
a manually annotated training sample without hav-
ing to perform the costly manual annotation.
In particular, for English (Table 5.1), results were
actually better using only coindexing terms rather
than combining this information with hierarchically
related terms: both precision and recall are best
when using only the MRCOC knowledge source. As
we had expected, recall and coverage increased but
precision decreased slightly when using larger con-
texts.
The German results (Table 5.1) were slightly dif-
ferent, and even more successful, with nearly 60% of
the evaluation corpus being correctly disambiguated,
nearly 80% of the decisions being correct. Here, there
was some small gain when combining the knowledge
sources, though the results using only coindexing
terms were almost as good. For the German experi-
ments, using larger contexts resulted in greater recall
and greater precision. This was unexpected ? one
hypothesis is that the sparser coverage of the German
UMLS contributed to less predictable results on the
sentence level.
These results are comparable with some of the bet-
ter SENSEVAL results (Kilgarriff and Rosenzweig,
2000) which used fully supervised methods, though
the comparison may not be accurate because we are
choosing between fewer senses than on avarage in
SENSEVAL, and because of the doubts over our in-
terannotator agreement.
Comparing these results with the number of words
disambiguated in the whole corpus (Table 1), it is
apparent that the average coverage of this method is
actually higher for the whole corpus (over 80%) than
for the words in the evaluation corpus. It is possible
that this reflects the fact the the evaluation corpus
was specifically chosen to include words with ?inter-
esting? ambiguities, which might include words which
are more difficult than average to disambiguate. It is
possible that over the whole corpus, the method ac-
tually works even better than on just the evaluation
corpus.
This technique is quite groundbreaking, because it
shows that a lexical resource derived almost entirely
from English data (MEDLINE indexing terms) could
successfully be used for automatic disambiguation in
a German corpus. (The alignment of documents and
their translations was not even considered for these
experiments so the results do not depend at all on
our having access to a parallel corpus.) This is be-
cause the UMLS relations are defined between con-
cepts rather than between words. Thus if we know
that there is a relationship between two concepts, we
can use that relationship for disambiguation, even if
the original evidence for this relationship was derived
from information in a different language from the
language of the document we are seeking to disam-
biguate. We are assigning the correct senses based
not upon how terms are related in language, but how
medical concepts are related to one another.
It follows that this technique for disambiguation
should be applicable to any language which UMLS
covers, and applicable at very little cost. This pro-
posal should stimulate further research, and not too
far behind, successful practical implementation.
6 Summary and Conclusion
We have described three implementations of unsu-
pervised word-sense disambiguation techniques for
medical documents. The bilingual method relies on
the availability of a translated parallel corpus: the
collocational and relational methods rely solely on
the structure of UMLS, and could therefore be ap-
plied to new collections of medical documents with-
out requiring any new resources. The method of
disambiguation using relations between terms given
by UMLS was by far the most successful method,
achieving 74% precision, 66% coverage for English
ENGLISH Related terms Related terms Coindexing terms Combined
RESULTS (MRREL) (MRCXT) (MRCOC) (majority voting)
Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov.
Sentence 50 14 28 60 9 15 78 32 41 74 32 43
Document 48 24 50 63 22 35 74 46 62 72 45 63
Subdomain 51 33 65 64 38 59 74 49 66 71 49 69
Table 4: Results for disambiguation based on UMLS relations (English)
GERMAN Related terms Related terms Coindexing terms Combined
RESULTS (MRREL) (MRCXT) (MRCOC) (majority voting)
Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov.
Sentence 64 24 38 75 11 15 76 29 38 77 31 40
Document 68 43 63 75 27 36 79 52 66 79 53 67
Subdomain 70 51 73 74 52 70 79 58 73 79 58 73
Table 5: Results for disambiguation based on UMLS relations (German)
and 79% precision, 73% coverage for German on the
evaluation corpora, and achieving over 80% coverage
overall. This result for German is particularly en-
couraging, because is shows that a lexical resource
giving relations between concepts in one language
can be used for high quality disambiguation in an-
other language.
Acknowledgments
This research was supported in part by the Re-
search Collaboration between the NTT Communi-
cation Science Laboratories, Nippon Telegraph and
Telephone Corporation and CSLI, Stanford Univer-
sity, and by EC/NSF grant IST-1999-11438 for the
MUCHMORE project.
We would like to thank the National Library of
Medicine for providing the UMLS, and in particular
Stuart Nelson for his advice and guidance.
References
P. Brown, S. de la Pietra, V. de la Pietra, and R Mer-
cer. 1991. Word sense disambiguation using sta-
tistical methods. In ACL 29, pages 264?270.
Paul Buitelaar and Bogdan Sacaleanu. 2001. Rank-
ing and selecting synsets by domain relevance. In
Proceedings of WordNet and Other Lexical Re-
sources, NAACL 2001 Workshop, Pittsburgh, PA,
June.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press, Cam-
bridge MA.
W. Gale, K. Church, and D. Yarowsky. 1992. A
method for disambiguating word senses in a large
corpus. Computers and the Humanities, 26:415?
439.
Nancy Ide and Jean Ve?ronis. 1998. Introduction
to the special issue on word sense disambiguation:
The state of the art. Computational Linguistics,
24(1):1?40, March.
Nancy Ide. 1999. Parallel translations and
sense discriminators. In Proceedings of the ACL
SIGLEX workshop on Standardizing Lexical Re-
sources, pages 52?61.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and results for english senseval. Computers and
the Humanities, 34(1-2):15?48, April.
M. E. Lesk. 1986. Automated sense disambiguation
using machine-readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the SIGDOC conference. ACM.
S?pela Vintar, Paul Buitelaar, Ba?rbel Ripplinger,
Bogdan Sacaleanu, Diana Raileanu, and Detlef
Prescher. 2002. An efficient and flexible format
for linguistic and semantic annotation. In Third
International Language Resources and Evaluation
Conference, Las Palmas, Spain.
Dominic Widdows, Diana Steffen, Scott Ceder-
berg, Chiu-Ki Chan, Paul Buitelaar, and Bog-
dan Sacaleanu. 2003. Methods for word-sense
disambiguation. Technical report, MUCHMORE
project report.
David Yarowsky. 1993. One sense per collocation.
In ARPA Human Language Technology Workshop,
pages 266?271, Princeton, NJ.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics, pages
189?196.
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 156?163,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Modelling and Detecting Decisions in Multi-party Dialogue
Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen, Matthew Purver, and Stanley Peters
Center for the Study of Language and Information
Stanford University
{raquel|frampton|ehlen|mpurver|peters}@stanford.edu
Abstract
We describe a process for automatically de-
tecting decision-making sub-dialogues in tran-
scripts of multi-party, human-human meet-
ings. Extending our previous work on ac-
tion item identification, we propose a struc-
tured approach that takes into account the dif-
ferent roles utterances play in the decision-
making process. We show that this structured
approach outperforms the accuracy achieved
by existing decision detection systems based
on flat annotations, while enabling the extrac-
tion of more fine-grained information that can
be used for summarization and reporting.
1 Introduction
In collaborative and organized work environments,
people share information and make decisions exten-
sively through multi-party conversations, usually in
the form of meetings. When audio or video record-
ings are made of these meetings, it would be valu-
able to extract important information, such as the
decisions that were made and the trains of reason-
ing that led to those decisions. Such a capability
would allow work groups to keep track of courses
of action that were shelved or rejected, and could al-
low new team members to get quickly up to speed.
Thanks to the recent availability of substantial meet-
ing corpora?such as the ISL (Burger et al, 2002),
ICSI (Janin et al, 2004), and AMI (McCowan et
al., 2005) Meeting Corpora?current research on the
structure of decision-making dialogue and its use for
automatic decision detection has helped to bring this
vision closer to reality (Verbree et al, 2006; Hsueh
and Moore, 2007b).
Our aim here is to further that research by ap-
plying a simple notion of dialogue structure to the
task of automatically detecting decisions in multi-
party dialogue. A central hypothesis underlying our
approach is that this task is best addressed by tak-
ing into account the roles that different utterances
play in the decision-making process. Our claim is
that this approach facilitates both the detection of
regions of discourse where decisions are discussed
and adopted, and also the identification of important
aspects of the decision discussions themselves, thus
opening the way to better and more concise report-
ing.
In the next section, we describe prior work on re-
lated efforts, including our own work on action item
detection (Purver et al, 2007). Sections 3 and 4 then
present our decision annotation scheme, which dis-
tinguishes several types of decision-related dialogue
acts (DAs), and the corpus used as data (in this study
a section of the AMI Meeting Corpus). Next, in Sec-
tion 5, we describe our experimental methodology,
including the basic conception of our classification
approach, the features we used in classification, and
our evaluation metrics. Section 6 then presents our
results, obtained with a hierarchical classifier that
first trains individual sub-classifiers to detect the dif-
ferent types of decision DAs, and then uses a super-
classifier to detect decision regions on the basis of
patterns of these DAs, achieving an F-score of 58%.
Finally, Section 7 presents some conclusions and di-
rections for future work.
2 Related Work
Recent years have seen an increasing interest in re-
search on decision-making dialogue. To a great
extent, this is due to the fact that decisions have
156
been shown to be a key aspect of meeting speech.
User studies (Lisowska et al, 2004; Banerjee et al,
2005) have shown that participants regard decisions
as one of the most important outputs of a meeting,
while Whittaker et al (2006) found that the develop-
ment of an automatic decision detection component
is critical to the re-use of meeting archives. Identify-
ing decision-making regions in meeting transcripts
can thus be expected to support development of a
wide range of applications, such as automatic meet-
ing assistants that process, understand, summarize
and report the output of meetings; meeting tracking
systems that assist in implementing decisions; and
group decision support systems that, for instance,
help in constructing group memory (Romano and
Nunamaker, 2001; Post et al, 2004; Voss et al,
2007).
Previously researchers have focused on the in-
teractive aspects of argumentative and decision-
making dialogue, tackling issues such as the detec-
tion of agreement and disagreement and the level
of emotional involvement of conversational partic-
ipants (Hillard et al, 2003; Wrede and Shriberg,
2003; Galley et al, 2004; Gatica-Perez et al, 2005).
From a perhaps more formal perspective, Verbree et
al. (2006) have created an argumentation scheme in-
tended to support automatic production of argument
structure diagrams from decision-oriented meeting
transcripts. Only Hsueh and Moore (2007a; 2007b),
however, have specifically investigated the auto-
matic detection of decisions.
Using the AMI Meeting Corpus, Hsueh and
Moore (2007b) attempt to identify the dialogue acts
(DAs) in a meeting transcript that are ?decision-
related?. The authors define these DAs on the ba-
sis of two kinds of manually created summaries: an
extractive summary of the whole meeting, and an
abstractive summary of the decisions made in the
meeting. Those DAs in the extractive summary that
support any of the decisions in the abstractive sum-
mary are then manually tagged as decision-related
DAs. They trained a Maximum Entropy classifier
to recognize this single DA class, using a variety of
lexical, prosodic, dialogue act and topical features.
The F-score they achieved was 0.35, which gives a
good indication of the difficulty of this task.
In our previous work (Purver et al, 2007), we at-
tempted to detect a particular kind of decision com-
mon in meetings, namely action items?public com-
mitments to perform a given task. In contrast to
the approach adopted by Hsueh and Moore (2007b),
we proposed a hierarchical approach where indi-
vidual classifiers were trained to detect distinct ac-
tion item-related DA classes (task description, time-
frame, ownership and agreement) followed by a
super-classifier trained on the hypothesized class la-
bels and confidence scores from the individual clas-
sifiers that would detect clusters of multiple classes.
We showed that this structured approach produced
better classification accuracy (around 0.39 F-score
on the task of detecting action item regions) than a
flat-classifier baseline trained on a single action item
DA class (around 0.35 F-score).
In this paper we extend this approach to the more
general task of detecting decisions, hypothesizing
that?as with action items?the dialogue acts in-
volved in decision-making dialogue form a rather
heterogeneous set, whose members co-occur in par-
ticular kinds of patterns, and that exploiting this
richer structure can facilitate their detection.
3 Decision Dialogue Acts
We are interested in identifying the main conver-
sational units in a decision-making process. We ex-
pect that identifying these units will help in detect-
ing regions of dialogue where decisions are made
(decision sub-dialogues), while also contributing to
identification and extraction of specific decision-
related bits of information.
Decision-making dialogue can be complex, often
involving detailed discussions with complicated ar-
gumentative structure (Verbree et al, 2006). Deci-
sion sub-dialogues can thus include a great deal of
information that is potentially worth extracting. For
instance, we may be interested in knowing what a
decision is about, what alternative proposals were
considered during the decision process, what argu-
ments were given for and against each of them, and
last but not least, what the final resolution was.
Extracting these and other potential decision com-
ponents is a challenging task, which we do not in-
tend to fully address in this paper. This initial study
concentrates on three main components we believe
constitute the backbone of decision sub-dialogues.
A typical decision sub-dialogue consists of three
main components that often unfold in sequence. (a)
157
key DDA class description
I issue utterances introducing the issue or topic under discussion
R resolution utterances containing the decision that is adopted
RP ? proposal ? utterances where the decision adopted is proposed
RR ? restatement ? utterances where the decision adopted is confirmed or restated
A agreement utterances explicitly signalling agreement with the decision made
Table 1: Set of decision dialogue act (DDA) classes
A topic or issue that requires some sort of conclu-
sion is initially raised. (b) One or more proposals are
considered. And (c) once some sort of agreement is
reached upon a particular resolution, a decision is
adopted.
Dialogue act taxonomies often include tags
that can be decision-related. For instance, the
DAMSL taxonomy (Core and Allen, 1997) in-
cludes the tags agreement and commit, as well
as a tag open-option for utterances that ?sug-
gest a course of action?. Similarly, the AMI
DA scheme1 incorporates tags like suggest,
elicit-offer-or-suggestion and assess.
These tags are however very general and do not cap-
ture the distinction between decisions and more gen-
eral suggestions and commitments.2 We therefore
devised a decision annotation scheme that classifies
utterances according to the role they play in the pro-
cess of formulating and agreeing on a decision. Our
scheme distinguishes among three main decision di-
alogue act (DDA) classes: issue (I), resolution (R),
and agreement (A). Class R is further subdivided into
resolution proposal (RP) and resolution restatement
(RR). A summary of the classes is given in Table 1.
Annotation of the issue class includes any utter-
ances that introduce the topic of the decision discus-
sion. For instance, in example (1) below, the utter-
ances ?Are we going to have a backup?? and ?But
would a backup really be necessary?? are tagged as
I. The classes RP and RR are used to annotate those
utterances that specify the resolution adopted?i.e.
the decision made. Annotation with the class RP
includes any utterances where the resolution is ini-
1A full description of the AMI Meeting Corpus DA scheme
is available at http://mmm.idiap.ch/private/ami/
annotation/dialogue acts manual 1.0.pdf, after
free registration.
2Although they can of course be used to aid the identification
process?see Section 5.3.
tially proposed (like the utterance ?I think maybe we
could just go for the kinetic energy. . . ?). Sometimes
decision discussions include utterances that sum up
the resolution adopted, like the utterance ?Okay,
fully kinetic energy? in (1). This kind of utterance
is tagged with the class RR. Finally, the agreement
class includes any utterances in which participants
agree with the (proposed) resolution, like the utter-
ances ?Yeah? and ?Good? as well as ?Okay? in di-
alogue (1).
(1) A: Are we going to have a backup?
Or we do just?
B: But would a backup really be necessary?
A: I think maybe we could just go for the
kinetic energy and be bold and innovative.
C: Yeah.
B: I think? yeah.
A: It could even be one of our selling points.
C: Yeah ?laugh?.
D: Environmentally conscious or something.
A: Yeah.
B: Okay, fully kinetic energy.
D: Good.3
Note that an utterance can be assigned to more
than one of these classes. For instance, the utter-
ance ?Okay, fully kinetic energy? is annotated both
as RR and A. Similarly, each decision sub-dialogue
may contain more than one utterance corresponding
to each class, as we saw above for issue. While
we do not a priori require each of these classes to
be present for a set of utterances to be considered
a decision sub-dialogue, all annotated decision sub-
dialogues in our corpus include the classes I, RP and
A. The annotation process and results are described
in detail in the next section.
3This example was extracted from the AMI dialogue
ES2015c and has been modified slightly for presentation pur-
poses.
158
4 Data: Corpus & Annotation
In this study, we use 17 meetings from the AMI
Meeting Corpus (McCowan et al, 2005), a pub-
licly available corpus of multi-party meetings con-
taining both audio recordings and manual transcrip-
tions, as well as a wide range of annotated infor-
mation including dialogue acts and topic segmenta-
tion. Conversations are all in English, but they can
include native and non-native English speakers. All
meetings in our sub-corpus are driven by an elicita-
tion scenario, wherein four participants play the role
of project manager, marketing expert, interface de-
signer, and industrial designer in a company?s de-
sign team. The overall sub-corpus makes up a total
of 15,680 utterances/dialogue acts (approximately
920 per meeting). Each meeting lasts around 30
minutes.
Two authors annotated 9 and 10 dialogues each,
overlapping on two dialogues. Inter-annotator
agreement on these two dialogues was similar to
(Purver et al, 2007), with kappa values ranging
from 0.63 to 0.73 for the four DDA classes. The
highest agreement was obtained for class RP and the
lowest for class A.4
On average, each meeting contains around 40
DAs tagged with one or more of the DDA sub-
classes in Table 1. DDAs are thus very sparse, cor-
responding to only 4.3% of utterances. When we
look at the individual DDA sub-classes this is even
more pronounced. Utterances tagged as issue make
up less than 0.9% of utterances in a meeting, while
utterances annotated as resolution make up around
1.4%?1% corresponding to RP and less than 0.4%
to RR on average. Almost half of DDA utterances
(slightly over 2% of all utterances on average) are
tagged as belonging to class agreement.
We compared our annotations with the annota-
tions of Hsueh and Moore (2007b) for the 17 meet-
ings of our sub-corpus. The overall number of ut-
terances annotated as decision-related is similar in
the two studies: 40 vs. 30 utterances per meeting on
average, respectively. However, the overlap of the
annotations is very small leading to negative kappa
scores. As shown in Figure 1, only 12.22% of ut-
4The annotation guidelines we used are available on-
line at http://godel.stanford.edu/twiki/bin/
view/Calo/CaloDecisionDiscussionSchema

  









  !"
##Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 235?243,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Extracting decisions from multi-party dialogue using directed graphical
models and semantic similarity
Trung H. Bui1, Matthew Frampton1, John Dowding2, and Stanley Peters1
1Center for the Study of Language and Information, Stanford University
{thbui|frampton|peters}@stanford.edu
2University of California/Santa Cruz
jdowding@ucsc.edu
Abstract
We use directed graphical models (DGMs)
to automatically detect decision discus-
sions in multi-party dialogue. Our ap-
proach distinguishes between different di-
alogue act (DA) types based on their role
in the formulation of a decision. DGMs
enable us to model dependencies, includ-
ing sequential ones. We summarize deci-
sions by extracting suitable phrases from
DAs that concern the issue under discus-
sion and its resolution. Here we use a
semantic-similarity metric to improve re-
sults on both manual and ASR transcripts.
1 Introduction
In work environments, people share information
and make decisions in multi-party conversations
known as meetings. The demand for systems that
can automatically process, understand and sum-
marize information contained in audio and video
recordings of meetings is growing rapidly. Our
own research, and that of other contemporary
projects (Janin et al, 2004), aim at meeting this
demand.
At present, we are focusing on the automatic
detection and summarization of decision discus-
sions. Our approach for detecting decision dis-
cussions involves distinguishing between differ-
ent dialogue act (DA) types based on their role
in the decision-making process. Two of these
types are DAs which describe the Issue under dis-
cussion, and DAs which describe its Resolution.
To summarize a decision discussion, we identify
words and phrases in the Issue and Resolution
DAs, which can be used to produce a concise, de-
scriptive summary.
This paper describes new experiments in both
detecting and summarizing decision discussions.
In the detection stage, we investigate the use of
Directed Graphical Models (DGMs). DGMs are
attractive because they can be used to model se-
quence and dependencies between predictor vari-
ables. In the summarization stage, we attempt to
improve phrase selection with a new feature that
measures the level of semantic similarity between
candidate Issue phrases and Resolution utterances,
and vice-versa. The feature is generated by a
semantic-similarity metric which uses WordNet as
a knowledge source. The motivation is that ordi-
narily, the Issue and Resolution components in a
decision summary should be semantically similar.
The paper proceeds as follows. Firstly, Sec-
tion 2 describes related work, and Section 3, our
data-set and annotation scheme for decision dis-
cussions. Section 4 then reports our decision de-
tection experiments using DGMs, and Section 5,
the summarization experiments. Finally, Section
6 draws conclusions and proposes ideas for future
work.
2 Related Work
User studies (Banerjee et al, 2005) have con-
firmed that meeting participants consider deci-
sions to be one of the most important meeting
outputs, and (Whittaker et al, 2006) found that
the development of an automatic decision detec-
tion component is critical to the re-use of meet-
ing archives. With the new availability of substan-
tial meeting corpora such as the AMI corpus (Mc-
Cowan et al, 2005), recent years have therefore
seen an increasing amount of research on decision-
making dialog. This research has tackled issues
such as the automatic detection of agreement and
disagreement (Galley et al, 2004), and of the
235
level of involvement of conversational participants
(Gatica-Perez et al, 2005). In addition, (Verbree
et al, 2006) created an argumentation scheme in-
tended to support automatic production of argu-
ment structure diagrams from decision-oriented
meeting transcripts. As yet, there has been rela-
tively little work which specifically addresses the
automatic detection and summarization of deci-
sions.
Decision discussion detection: (Hsueh and
Moore, 2007) used the AMI Meeting Corpus, and
attempted to automatically identify DAs in meet-
ing transcripts which are ?decision-related?. For
each meeting, two manually created summaries
were used to judge which DAs were decision-
related: an extractive summary of the whole meet-
ing, and an abstractive summary of its decisions.
Those DAs in the extractive summary which sup-
port any of the decisions in the abstractive sum-
mary were manually tagged as decision-related.
(Hsueh and Moore, 2007) then trained a Maxi-
mum Entropy classifier to recognize this single
DA class, using a variety of lexical, prosodic, DA
and conversational topic features. They achieved
an F-score of 0.35.
Unlike (Hsueh and Moore, 2007), (Ferna?ndez et
al., 2008b) made an attempt at modelling the struc-
ture of decision-making dialogue. The authors de-
signed an annotation scheme that takes account of
the different roles which utterances can play in the
decision-making process?for example it distin-
guishes between DDAs (decision DAs) which ini-
tiate a discussion by raising an issue, those which
propose a resolution, and those which express
agreement for a proposed resolution. The authors
annotated a portion of the AMI corpus, and then
applied what they refer to as ?hierarchical classi-
fication?. Here, one sub-classifier per DDA class
hypothesizes occurrences of that DDA class, and
then based on these hypotheses, a super-classifier
determines which regions of dialogue are deci-
sion discussions. All of the classifiers, (sub and
super), were linear kernel binary Support Vec-
tor Machines (SVMs). Results were better than
those obtained with (Hsueh and Moore, 2007)?s
approach?the F1-score for detecting decision dis-
cussions in manual transcripts was .58 vs. .50.
Note that (Purver et al, 2007) had previously pur-
sued the same basic approach as (Ferna?ndez et al,
2008b) in order to detect action items.
In this paper, we build on the promising results
of (Ferna?ndez et al, 2008b), by using Directed
Graphical Models (DGMs) in place of SVMs.
DGMs are attractive because they provide a natu-
ral framework for modelling sequence and depen-
dencies between variables including the DDAs.
We are especially interested in whether DGMs
better exploit non-lexical features. (Ferna?ndez et
al., 2008b) obtained much more value from lexi-
cal than non-lexical features (and indeed no value
at all from prosodic features), but lexical features
have disadvantages. In particular, they can be do-
main specific, increase the size of the feature space
dramatically, and deteriorate more than other fea-
tures in quality when ASR is poor.
Decision summarization: Recent years have
seen research on spoken dialogue summarization
(e.g. (Zechner, 2002)). Most has attempted to gen-
erate summaries of full dialogues, but some very
recent research has focused on specific dialogue
events, namely action items (Purver et al, 2007),
and decisions (Ferna?ndez et al, 2008a).
(Ferna?ndez et al, 2008a) used the DDA an-
notation scheme mentioned above, and began by
extracting the DDAs which raise issues or pro-
vide accepted resolutions. Only manual tran-
scripts were used and the DDAs were extracted
by hand rather than automatically. The next step
was to parse each DDA with a general rule-based
parser (Dowding et al, 1993), producing multi-
ple short fragments rather than one full utterance
parse. Then, for each DDA, an SVM regression
model used various features (including parse, se-
mantic and lexical features) to select the fragment
which was most likely to appear in a gold-standard
extractive decision summary. The entire manual
utterance transcriptions were used as the baseline,
and although the SVM?s precision was high, it was
not enough to offset the baseline?s perfect recall,
and so its F-score was lower. The ?Oracle?, which
always chooses the fragment with the highest F1-
score produced very good results. This motivates
deeper investigation into how to improve the frag-
ment/parse selection phase, and so we assess the
usefulness of a semantic-similarity feature for the
SVM. We conduct experiments with ASR as well
as manual transcripts.
3 Data
For the experiments reported in this study, we used
17 meetings from the AMI Meeting Corpus (Mc-
Cowan et al, 2005), a freely available corpus of
236
multi-party meetings with both audio and video
recordings, and a wide range of annotated in-
formation including DAs and topic segmentation.
Conversations are in English, but some partici-
pants are non-native English speakers. The meet-
ings last around 30 minutes each, and are scenario-
driven, wherein four participants play different
roles in a company?s design team: project man-
ager, marketing expert, interface designer and in-
dustrial designer.
3.1 Modelling Decision Discussions
We use the same annotation scheme as (Ferna?ndez
et al, 2008b) to model decision-making dialogue.
As stated in Section 2, this scheme distinguishes
between a small number of DA types based on the
role which they perform in the formulation of a de-
cision. Apart from improving the initial detection
of decision discussions (Ferna?ndez et al, 2008b),
such a scheme also aids their subsequent summa-
rization, because it indicates which utterances con-
tain particular types of information.
The annotation scheme is based on the observa-
tion that a decision discussion contains the follow-
ing main structural components: (a) a topic or is-
sue requiring resolution is raised, (b) one or more
possible resolutions are considered, (c) a particular
resolution is agreed upon and so becomes the de-
cision. Hence the scheme distinguishes between
three main decision dialogue act (DDA) classes:
issue (I), resolution (R), and agreement (A). Class
R is further subdivided into resolution proposal
(RP) and resolution restatement (RR). I utterances
introduce the topic of the decision discussion, ex-
amples being ?Are we going to have a backup??
and ?But would a backup really be necessary??
in Dialogue 1. On the other hand, R utterances
specify the resolution which is ultimately adopted
as the decision. RP utterances propose this reso-
lution (e.g. ?I think maybe we could just go for
the kinetic energy. . . ?), while RR utterances close
the discussion by confirming/summarizing the de-
cision (e.g. ?Okay, fully kinetic energy?) . Finally,
A utterances agree with the proposed resolution,
signalling that it is adopted as the decision, (e.g.
?Yeah?, ?Good? and ?Okay?). Note that an utter-
ance can be assigned to more than one DDA class,
and within a decision discussion, more than one
utterance can be assigned to the same DDA class.
We use both manual and ASR one-best tran-
scripts1 in the experiments described here. DDA
annotations were first made on the manual tran-
scripts, and then transferred onto the ASR tran-
scripts. Inter-annotator agreement was satisfac-
tory, with kappa values ranging from .63 to .73 for
the four DDA classes. Due to different segmen-
tation, the manual and ASR transcripts contain a
total of 15,680 and 8,357 utterances respectively,
and on average, 40 and 33 DDAs per meeting.
Hence DDAs are slightly less sparse in the ASR
transcripts: for all DDAs, 6.7% vs. 4.3% of the to-
tal number of utterances, for I, 1.6% vs. 0.9%, for
RP, 2% vs. 1%, for RR, 0.5% vs. 0.4%, and for A,
2.6% vs. 2%.
(1) A: Are we going to have a backup? Or we do
just?
B: But would a backup really be necessary?
A: I think maybe we could just go for the
kinetic energy and be bold and innovative.
C: Yeah.
B: I think? yeah.
A: It could even be one of our selling points.
C: Yeah ?laugh?.
D: Environmentally conscious or something.
A: Yeah.
B: Okay, fully kinetic energy.
D: Good.2
4 Decision Discussion Detection using
Directed Graphical Models
A directed graphical model (DGM) M, (see Mur-
phy (2002)), is a directed acyclic graph consisting
of nodes which represent random variables, arcs
which represent dependencies among these vari-
ables, and a probability distribution P over the
variables. Let X = {X1, X2, ..., Xn} be a set of
random variables that are associated with nodes in
a DGM and Pa(Xi) be parents of Xi. The proba-
bility distribution of the model M satisfies:
P (X1, X2, ..., Xn) =
n?
i=1
(P (Xi)|Pa(Xi))
When a DGM is used as a classifier, the goal is to
correctly infer the value of the class node Xc ? X
given a vector of values for the observed node(s)
1We used SRI?s Decipher for which (Stolcke et al, 2008)
reports a word error rate of 26.9% on AMI meetings.
2This example was extracted from the AMI dialogue
ES2015c and has been modified slightly for presentation pur-
poses.
237
Xo ? X\Xc. This is done by using M to find the
value of Xc which gives the highest conditional
probability P (Xc|Xo).
To detect each individual DDA class, we ex-
amined the four simple DGMs in Figure 1 (see
Appendix). The DDA node is binary where
value 1 indicates the presence of a DDA and 0
its absence. The evidence node (E) is a multi-
dimensional vector of observed values of non-
lexical features. These include utterance features
(UTT) such as length in words, duration in mil-
liseconds, position within the meeting (as percent-
age of elapsed time), manually annotated dialogue
act (DA) features3 such as inform, assess, suggest,
and prosodic features (PROS) such as energy and
pitch. These features are the same as the non-
lexical features used by Ferna?ndez et al (2008b).
The hidden component node (C) represents the
distribution of observable evidence E as a single
Gaussian in the -sim models, and a mixture in the
-mix models. For the -mix models, the number
of Gaussian components is hand-tuned during the
training phase.
More complex models are constructed from the
four simple models in Figure 1 to allow for depen-
dencies between different DDAs. For example, the
model in Figure 2 (see Appendix) generalizes Fig-
ure 1c with arcs connecting the DDA classes based
on analysis of the annotated AMI data.
4.1 Experiments
The DGM classifiers in Figures 1 and 2 were im-
plemented in Matlab using the BNT software4.
Since the current BNT version does not sup-
port multiple time series training for fully observ-
able Dynamic Bayesian Networks (DBNs), we ex-
tended the software for training models using this
structure (e.g., Figure 1c and Figure 2).
A DGM classifier is considered to have hy-
pothesized a DDA if the marginal probability of
its DDA node is above a hand-tuned threshold.
We tested the DGMs on manual and ASR tran-
scripts in a 17-fold cross-validation, and evaluated
their performance on both a per-utterance basis,
and also with the same lenient-match metric as
Ferna?ndez et al (2008b). This allows a margin
of 20 seconds preceding and following a hypoth-
esized DDA, and so we refer to it as the 40 sec-
ond metric. In addition, we hypothesized decision
3We use the AMI DA annotations. These are only avail-
able for manual transcripts.
4http://www.cs.ubc.ca/?murphyk/Software/BNT/bnt.html
discussion regions using the DGM output and the
following two simple rules:
? A decision discussion region begins with an
Issue DDA.
? A decision discussion region contains at least
one Issue DDA and one Resolution DDA.
To evaluate the accuracy of these hypothesized re-
gions, like Ferna?ndez et al (2008b), we divided
the dialogue into 30-second windows and evalu-
ated on a per window basis.
4.2 Results
Tables 1 and 2 show the F1-scores for each
DGM when using the best feature sets (I:
UTT+DA+PROS, RP: UTT+DA, RR: UTT, A:
UTT+DA). The BN-mix model gives the highest
F1-score for A on both evaluation metrics, and the
DBN-mix model, the highest for I, RP, and RR,
but there are no statistically significant differences
between any of the alternative DGMs.
Classifier I RP RR A
BN-mix .09 .09 .04 .19
DBN-mix .16 .14 .05 .17
BN-sim .12 .09 .04 .17
DBN-sim .15 .11 .04 .16
Table 1: F1-score (per utterance) of the DGMs us-
ing the best combination of non-lexical features.
Classifier I RP RR A
BN-mix .19 .24 .07 .38
DBN-mix .27 .24 .07 .32
BN-sim .23 .22 .06 .36
DBN-sim .25 .22 .06 .31
Table 2: F1-score (40 seconds) of the DGMs using
the best combination of non-lexical features.
To determine whether modeling dependencies
between DDAs improves performance, we exper-
imented with the DGMs that are generalized from
the DBN-sim (Figure 2) and DBN-mix models.
The F1-scores did not improve for I, RP, and RR,
while for A, the DGM generalized from DBN-sim
gave a .03 improvement according to the 40 sec-
onds metric, but this was not statistically signifi-
cant.
For each DDA, Table 3 compares the results of
the best DGM and the hierarchical SVM classi-
fication method of Ferna?ndez et al (2008b) (see
238
Section 2). The DGM performs better for all
DDAs on both evaluation metrics (p < 0.005).
Note that while prosodic features proved useless
to SVM classifiers (Ferna?ndez et al (2008b)), with
DGMs, they have some predictive power.
Per utterance 40 seconds
Classifier DDA Pr Re F1 Pr Re F1
SVM I .03 .62 .05 .04 .89 .08
DGM .11 .28 .16 .20 .44 .27
SVM RP .03 .60 .07 .05 .90 .10
DGM .09 .35 .14 .16 .57 .24
SVM RR .01 .49 .02 .01 .80 .03
DGM .02 .42 .05 .04 .58 .07
SVM A .05 .70 .10 .07 .90 .13
DGM .13 .31 .19 .29 .55 .38
Table 3: Performance of the DGM classifier vs.
the SVM classifier. Both use the best combination
of non-lexical features.
We also generated results without DA features.
Here, the best F1-scores for I, RP, and A degrade
between .07 and .09 (p < 0.05), but they are still
higher than the equivalent SVM results with DA
features. Since (Ferna?ndez et al, 2008b) report
that lexical features are the most useful for the
SVM classifiers, it will be interesting to see how
well the DGMs perform when they use lexical as
well as non-lexical features.
Detecting DDAs in ASR transcripts: Table 4
compares the DGM F1-scores when using ASR
one-best and manual transcripts. The DGMs per-
form well on ASR output. For I and RP, the results
on ASR are actually higher, perhaps because the
DDAs are less sparse. In the absence of DA fea-
tures, prosodic features improve the performance
for A in both sources.
UTT UTT+PROS
I RP RR A I RP RR A
ASR .20 .21 .06 .24 .16 .24 .07 .28
Man .18 .17 .07 .27 .16 .15 .05 .30
Table 4: F1-scores (40 seconds) computed using
ASR one-best vs. manual transcriptions.
Detecting decision discussion regions: Table 5
shows that according to the 30-second window
metric, rule-based classification with DGM output
compares well with hierarchical SVM classifica-
tion (Ferna?ndez et al, 2008b). In fact, even when
the latter uses lexical as well as non-lexical fea-
tures, its F1-score is still about the same as the
DGM-based classifier. Our future work will in-
volve dispensing with the rule-based approach and
designing a DGM which can detect decision dis-
cussion regions.
Classifier Pr Re F1
SVM .35 .88 .50
DGM .39 .93 .55
Table 5: Results in detecting decision discussion
regions for the SVM super-classifier and rule-
based DGM classifier, both using the best com-
bination of non-lexical features.
5 Decision Summarization
We now turn to the task of extracting useful
phrases for summarization. Since a summary of a
decision discussion should minimally contain the
issue under discussion, and its resolution, we leave
Agreement (A) utterances aside, and concentrate
on extracting phrases from Issues (I) and Resolu-
tions (R).
Our basic approach is the same taken in
(Ferna?ndez et al, 2008a): The WCN5 of each I
and R utterance is parsed by the Gemini parser
(Dowding et al, 1993) to produce multiple short
fragments, and then an SVM regression model
uses certain features in order to select the parse
that is most likely to match a gold-standard extrac-
tive summary. Our work is new in two respects:
summarizing from ASR output in addition to man-
ual transcriptions, and using a semantic-similarity
feature in the SVM. This new feature is generated
using Ted Pedersen?s semantic-similarity package
(Pedersen, 2002), and is motivated by the fact that
ordinarily the Issue summary should be semanti-
cally similar to the Resolution and vice versa.
The next section describes the lexical resources
used by Gemini, and Section 5.2, the metric for
calculating semantic similarity.
5.1 Open-Domain Semantic Parser
Since human-human spoken dialogue, especially
after being processed by an imperfect recognizer,
is likely to be highly ungrammatical, we have de-
veloped a semantic parser that only attempts to
find basic predicate-argument structures of the ma-
jor phrase types (S, VP, NP, and PP) and has access
to a broad-coverage lexicon. To build a broad-
coverage lexicon, we used publicly available lex-
ical resources for English, including COMLEX,
5When using manual transcripts, we create ?dummy
WCNs?: WCNs with a single path.
239
VerbNet, WordNet, and NOMLEX.
COMLEX provides detailed syntactic informa-
tion for the 40k most common words of En-
glish, and VerbNet, detailed semantic information
for verbs, including verb class, verb frames, the-
matic roles, mappings of syntactic position to the-
matic roles, and selection restrictions on thematic
role fillers. From WordNet we extracted another
15K nouns and the semantic class information for
all nouns. These semantic classes were hand-
aligned to the selectional classes used in Verb-
Net, based on the upper ontology of EuroWord-
Net. NOMLEX provides syntactic information for
event nominalizations, and information for map-
ping the noun arguments to the corresponding verb
syntactic positions.
These resources were combined and converted
to the Prolog-based format used in the Gemini
framework, which includes a fast bottom-up ro-
bust parser in which syntactic and semantic in-
formation is applied interleaved. Gemini can
compute parse probabilities on the context-free
skeleton of the grammar. In the experiments de-
scribed here these parse probabilities are trained
on Switchboard tree-bank data.
5.2 Semantic Similarity Metric: Normalized
Path Length
Ted Pedersen?s semantic similarity package (Ped-
ersen, 2002) can be used to apply a number of
different metrics that use WordNet as a knowl-
edge base. The metric used here, Normalized Path
Length (Leacock and Chodorow, 1998), defines
the semantic similarity sim between words w1 and
w2 as:
simc1,c2 = ? log
len(c1, c2)
2?D (1)
where c1 and c2 are concepts corresponding to w1
and w2, len(c1, c2) is the length of the shortest
path between them, and D is the maximum depth
of the taxonomy.
5.3 Experiments
Data: For the manual transcripts in our sub-
corpus, the average length in words of I and R ut-
terances is 12.2 and 11.9 respectively, and for the
ASR, 22.4 and 18.1. To provide a gold-standard,
phrases from I and R utterances in the man-
ual transcriptions were annotated as summary-
worthy. The aim was to select those phrases
which should appear in an extractive summary, or
could be the basis of a generated abstractive sum-
mary. As a general guideline, we tried to select
the phrase(s) which describe the issue/resolution
as succinctly as possible. This does not include
phrases which express the speaker?s attitude to-
wards the issue/resolution. Dialogue 2 is an exam-
ple where square brackets indicate which phrases
were selected as summary-worthy.
(2) A:(I) So we we?re looking at [sliders for both
volume and channel change]
B:(R)I was thinking kind of [just for the
volume]
Regression models: We use SVMlight
(Joachims, 1999) to learn separate SVM re-
gression models for Issues and Resolutions.
These rank the Gemini parses for each utterance
according to their likelihood of matching the
gold-standard summary. The top-ranked parse
is then entered into the automatically-generated
decision summary.
Features: We train the regression models with
various types of feature (see Table 6), including
properties of the WCN paths, parse, semantic and
lexical features. As lexical features are likely to be
more domain-specific, and they dramatically in-
crease size of the feature space, we prefer to avoid
them if possible.
To generate the semantic-similarity feature for
an I/R parse, we compute its semantic similarity
with the full transcripts of each of the R/I utter-
ances within the same decision discussion. The
feature?s value is then equal to the greatest of the
resulting semantic-similarity scores. Since Ted
Pedersen?s package operates on the noun portion
of WordNet, we must first extract all of the nouns
in the parse/utterance transcription. Next, we form
all of the possible pairs containing one noun from
the parse, and one from the utterance transcrip-
tion. Then we compute the semantic similarity
for each pair, and take their sum to be the level
of semantic similarity between the parse and the
utterance transcription. We experimented with av-
eraging rather than summing these scores, but the
resulting semantic-similarity feature was less pre-
dictive.
Evaluation: The models are evaluated in 10-
fold cross-validations using the same metric as
(Ferna?ndez et al, 2008a): Recall is the total pro-
portion of the gold-standard extractive summary
240
WCN phrase length (WCN arcs)
start/end point (absolute & percentage)
Parse parse probability
phrase type (S/VP/NP/PP)
Semantic main verb VerbNet class
head noun WordNet synset
Sem-sim Normalized Path Length
Lexical main verb, head noun
Table 6: Features for parse fragment ranking
Issue Resolution
Re Pr F1 Re Pr F1
Baseline 1.0 .50 .67 1.0 .60 .75
Oracle .77 .96 .85 .74 .99 .84
WCN,parse,sem .63 .69 .66 .61 .66 .64
+ sem-sim .65 .71 .68 .64 .69 .67
+ lexical .65 .67 .66 .65 .70 .67
Table 7: Parse ranking results for I & R Utterances
using manual transcriptions.
covered by the selected parse; precision is the to-
tal proportion of the chosen parse which overlaps
with the gold-standard summary. The baseline is
the entire transcription, and we also compare to
an ?oracle? that always chooses a parse with the
highest F1-score. Note that we use the extractive
summaries from the manual transcriptions as the
gold-standard for the evaluation of the results ob-
tained with ASR.
Results and analysis: Results with manual tran-
scriptions are shown in Table 7, and those with
ASR, in Table 8. In all cases, when starting with
a feature set containing WCN, parse and seman-
tic features, the F1-score is improved by adding
the semantic-similarity feature. For Issues, the F1-
score improves from .66 to .68 with manual tran-
scripts, and from .30 to .32 with ASR. The im-
provements for Resolutions are highly significant:
with manual transcripts, the F1 score increases
from .64 to .67 (p < 0.005), and with ASR, from
.33 to .37 (p < 0.005). Note that the further addi-
tion of lexical features only produces a significant
improvement in the case of I summarization with
ASR.
Compared to the full transcript baseline, we
achieve higher F1-scores for Issues?.68 vs. .67
with manual transcriptions, and .35 vs. .31 with
ASR?but slightly lower for Resolutions. There
remains a fairly large gap between our best scores
and their corresponding oracles (especially with
ASR), and so there may still be potential for sub-
stantial improvement.
Issue Resolution
Re Pr F1 Re Pr F1
Baseline .77 .20 .31 .80 .27 .40
Oracle .61 .87 .72 .59 .91 .72
WCN,parse,sem .28 .33 .30 .31 .35 .33
+ sem-sim .30 .34 .32 .35 .38 .36
+ lexical .35 .35 .35 .34 .39 .37
Table 8: Parse ranking results for I & R Utterances
using ASR.
6 Conclusions and Future Work
This paper has presented work on the detec-
tion and summarization of decision discussions
in multi-party dialogue. In the detection experi-
ments, we investigated the use of directed graph-
ical models (DGMs), and found that when us-
ing non-lexical features, the DGMs outperform
the hierarchical SVM classification method of
Ferna?ndez et al (2008b). The F1-score for the
four DDA classes increased between .04 and .19
(p < .005), and for identifying decision discus-
sion regions, by .05. This is encouraging because
lexical features have disadvantages?for example
they can be domain specific and greatly increase
the feature space. In addition, modelling the de-
pendencies between the DDA classes increased
performance for Agreement utterances, and the
DGMs were robust to ASR.
In the summarization experiments, we sum-
marized decision discussions by extracting key
words/phrases from their Issue (I) and Resolu-
tion (R) utterances. Each utterance?s Word Confu-
sion Network was parsed with an open-domain se-
mantic parser, thus producing multiple candidate
phrases, and then an SVM regression model se-
lected one of these phrases to enter into the sum-
mary. The experiments here investigated the use-
fulness of a new SVM feature which measures the
level of semantic similarity between candidate I
parses and R utterances, and vice-versa. This fea-
ture was generated with a semantic-similarity met-
ric which uses WordNet as a knowledge source.
It was found to improve performance with both
manual transcripts and ASR, and for R summa-
rization, the improvements were highly significant
(p < .005).
In future work, we plan to integrate lexical fea-
tures into our DGMs by using a switching Dy-
namic Bayesian Network similar to that reported
in (Ji and Bilmes, 2005). We also plan to extend
the decision discussion annotation scheme so that
we can try to automatically extract supporting ar-
241
guments for decisions.
Acknowledgements This material is based
upon work supported by the Defense Advanced
Research Projects Agency (DARPA) under Con-
tract No. FA8750-07-D-0185/0004, and by the
Department of the Navy Office of Naval Research
(ONR) under Grants No. N00014-05-1-0187 and
N00014-09-1-0106. Any opinions, findings and
conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of DARPA or ONR.
References
Satanjeev Banerjee, Carolyn Rose?, and Alex Rudnicky.
2005. The necessity of a meeting recording and
playback system, and the benefit of topic-level anno-
tations to meeting browsing. In Proceedings of the
10th International Conference on Human-Computer
Interaction.
John Dowding, Jean Mark Gawron, Doug Appelt, John
Bear, Lynn Cherny, Robert Moore, and Douglas
Moran. 1993. GEMINI: a natural language system
for spoken-language understanding. In Proceedings
of the 31st Annual Meeting of the Association for
Computational Linguistics (ACL).
Raquel Ferna?ndez, Matthew Frampton, John Dowding,
Anish Adukuzhiyil, Patrick Ehlen, and Stanley Pe-
ters. 2008a. Identifying relevant phrases to summa-
rize decisions in spoken meetings. In Proceedings
of Interspeech.
Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen,
Matthew Purver, and Stanley Peters. 2008b. Mod-
elling and detecting decisions in multi-party dia-
logue. In Proceedings of the 9th SIGdial Workshop
on Discourse and Dialogue.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of Bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Daniel Gatica-Perez, Ian McCowan, Dong Zhang, and
Samy Bengio. 2005. Detecting group interest level
in meetings. In Proceedings of ICASSP.
Pey-Yun Hsueh and Johanna Moore. 2007. Automatic
decision detection in meeting speech. In Proceed-
ings of MLMI 2007, Lecture Notes in Computer Sci-
ence. Springer-Verlag.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip
Dhillon, Jane Edwards, Javier Marc??as-Guarasa,
Nelson Morgan, Barbara Peskin, Elizabeth Shriberg,
Andreas Stolcke, Chuck Wooters, and Britta Wrede.
2004. The ICSI meeting project: Resources and re-
search. In Proceedings of the 2004 ICASSP NIST
Meeting Recognition Workshop.
Gang Ji and Jeff Bilmes. 2005. Dialog act tagging
using graphical models. In Proceedings of ICASSP.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods ?
Support Vector Learning. MIT Press.
Claudia Leacock and Martin Chodorow, 1998. Word-
Net: An Electronic Lexical Database, chapter Com-
bining local context and WordNet similarity for
word sense identification. University of Chicago
Press.
Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,
S. Bourban, M. Flynn, M. Guillemot, T. Hain,
J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,
M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of Measuring Behavior, the 5th Inter-
national Conference on Methods and Techniques in
Behavioral Research, Wageningen, Netherlands.
Kevin Murphy. 2002. Dynamic Bayesian Networks:
Representation, Inference and Learning. Ph.D. the-
sis, University of California Berkeley.
Ted Pedersen. 2002. Semantic similarity package.
http:/www.d.umn.edu/ tpederse/similarity.
Matthew Purver, John Dowding, John Niekrasz,
Patrick Ehlen, Sharareh Noorbaloochi, and Stanley
Peters. 2007. Detecting and summarizing action
items in multi-party dialogue. In Proceedings of the
8th SIGdial Workshop on Discourse and Dialogue,
Antwerp, Belgium.
Andreas Stolcke, Xavier Anguera, Kofi Boakye, O?zgu?r
C?etin, Adam Janin, Matthew Magimai-Doss, Chuck
Wooters, and Jing Zheng. 2008. The ICSI-SRI
spring 2007 meeting and lecture recognition system.
In Proceedings of CLEAR 2007 and RT2007.
Daan Verbree, Rutger Rienks, and Dirk Heylen. 2006.
First steps towards the automatic construction of
argument-diagrams from real discussions. In Pro-
ceedings of the 1st International Conference on
Computational Models of Argument, volume 144,
pages 183?194. IOS press.
Steve Whittaker, Rachel Laban, and Simon Tucker.
2006. Analysing meeting records: An ethnographic
study and technological implications. In S. Renals
and S. Bengio, editors, Machine Learning for Multi-
modal Interaction: Second International Workshop,
MLMI 2005, Revised Selected Papers, volume 3869
of Lecture Notes in Computer Science, pages 101?
113. Springer.
Klaus Zechner. 2002. Automatic summarization of
open-domain multiparty dialogues in diverse genres.
Computational Linguistics, 28(4):447?485.
242
Appendix
DDA
E
a) BN-sim
DDA
E
b) BN-mix
C
DDA
time t-1 time t
E
DDA
E
c) DBN-sim
DDA
time t-1 time t
E
DDA
E
d) DBN-mix
CC
Figure 1: Simple DGMs for individual decision
detection. During training, the shaded nodes are
hidden, and the clear nodes are observable.
A
time t-1 time t
E
A
E
I I
RP RP
RR RR
Figure 2: A DGM that takes the dependencies be-
tween decisions into account.
243
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 306?309,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Cascaded Lexicalised Classifiers for Second-Person Reference Resolution
Matthew Purver
Department of Computer Science
Queen Mary University of London
London E1 4NS, UK
mpurver@dcs.qmul.ac.uk
Raquel Ferna?ndez
ILLC
University of Amsterdam
1098 XH Amsterdam, Netherlands
raquel.fernandez@uva.nl
Matthew Frampton and Stanley Peters
CSLI
Stanford University
Stanford, CA 94305, USA
frampton,peters@csli.stanford.edu
Abstract
This paper examines the resolution of the
second person English pronoun you in
multi-party dialogue. Following previous
work, we attempt to classify instances as
generic or referential, and in the latter case
identify the singular or plural addressee.
We show that accuracy and robustness can
be improved by use of simple lexical fea-
tures, capturing the intuition that different
uses and addressees are associated with
different vocabularies; and we show that
there is an advantage to treating referen-
tiality and addressee identification as sep-
arate (but connected) problems.
1 Introduction
Resolving second-person references in dialogue is
far from trivial. Firstly, there is the referentiality
problem: while we generally conceive of the word
you1 as a deictic addressee-referring pronoun, it
is often used in non-referential ways, including as
a discourse marker (1) and with a generic sense
(2). Secondly, there is the reference problem: in
addressee-referring cases, we need to know who
the addressee is. In two-person dialogue, this is
not so difficult; but in multi-party dialogue, the ad-
dressee could in principle be any one of the other
participants (3), or any group of more than one (4):
(1) It?s not just, you know, noises like something
hitting.
(2) Often, you need to know specific button
sequences to get certain functionalities done.
(3) I think it?s good. You?ve done a good review.
(4) I don?t know if you guys have any questions.
1We include your, yours, yourself, yourselves.
This paper extends previous work (Gupta et al,
2007; Frampton et al, 2009) in attempting to au-
tomatically treat both problems: detecting refer-
ential uses, and resolving their (addressee) refer-
ence. We find that accuracy can be improved by
the use of lexical features; we also give the first
results for treating both problems simultaneously,
and find that there is an advantage to treating them
as separate (but connected) problems via cascaded
classifiers, rather than as a single joint problem.
2 Related Work
Gupta et al (2007) examined the referentiality
problem, distinguishing generic from referential
uses in multi-party dialogue; they found that 47%
of uses were generic and achieved a classification
accuracy of 75%, using various discourse features
and discriminative classifiers (support vector ma-
chines and conditional random fields). They at-
tempted the reference-resolution problem, using
only discourse (non-visual) features, but accuracy
was low (47%).
Addressee identification in general (i.e. in-
dependent of the presence of you) has been ap-
proached in various ways. Traum (2004) gives
a rule-based algorithm based on discourse struc-
ture; van Turnhout et al (2005) used facial ori-
entation as well as utterance features; and more
recently Jovanovic (2006; 2007) combined dis-
course and gaze direction features using Bayesian
networks, achieving 77% accuracy on a portion of
the AMI Meeting Corpus (McCowan et al, 2005)
of 4-person dialogues.
In recent work, therefore, Frampton et al
(2009) extended Gupta et al?s method to in-
clude multi-modal features including gaze direc-
tion, again using Bayesian networks on the AMI
corpus. This gave a small improvement on the ref-
306
erentiality problem (achieving 79% accuracy), and
a large improvement on the reference-resolution
task (77% accuracy distinguishing singular uses
from plural, and 80% resolving singular individ-
ual addressee reference).
However, they treated the two tasks in isola-
tion, and also broke the addressee-reference prob-
lem into two separate sub-tasks (singular vs. plu-
ral reference, and singular addressee reference). A
full computational you-resolution module would
need to treat all tasks (either simultaneously as one
joint classification problem, or as a cascaded se-
quence) ? with inaccuracy at one task necessar-
ily affecting performance at another ? and we ex-
amine this here. In addition, we examine the ef-
fect of lexical features, following a similar insight
to Katzenmaier et al (2004); they used language
modelling to help distinguish between user- and
robot-directed utterances, as people use different
language for the two ? we expect that the same is
true for human participants.
3 Method
We used Frampton et al (2009)?s AMI corpus
data: 948 ?you?-containing utterances, manu-
ally annotated for referentiality and accompanied
by the AMI corpus? original addressee annota-
tion. The very small number of two-person ad-
dressee cases were joined with the three-person
(i.e. all non-speaker) cases to form a single ?plu-
ral? class. 49% of cases are generic; 32% of
referential cases are plural, and the rest are ap-
proximately evenly distributed between the singu-
lar participants. While Frampton et al (2009) la-
belled singular reference by physical location rel-
ative to the speaker (giving a 3-way classification
problem), our lexical features are more suited to
detecting actual participant identity ? we there-
fore recast the singular reference task as a 4-way
classification problem and re-calculate their per-
formance figures (giving very similar accuracies).
Discourse Features We use Frampton et al
(2009)?s discourse features. These include sim-
ple durational and lexical/phrasal features (includ-
ing mention of participant names); AMI dialogue
act features; and features expressing the simi-
larity between the current utterance and previ-
ous/following utterances by other participants. As
dialogue act features are notoriously hard to tag
automatically, and ?forward-looking? information
about following utterances may be unavailable in
an on-line system, we examine the effect of leav-
ing these out below.
Visual Features Again we used Frampton et al
(2009)?s features, extracted from the AMI corpus
manual focus-of-attention annotations which track
head orientiation and eye gaze. Features include
the target of gaze (any participant or the meet-
ing whiteboard/projector screen) during each ut-
terance, and information about mutual gaze be-
tween participants. These features may also not
always be available (meeting rooms may not al-
ways have cameras), so we investigate the effect
of their absence below.
Lexical Features The AMI Corpus simulates a
set of scenario-driven business meetings, with par-
ticipants performing a design task (the design of
a remote control). Participants are given specific
roles to play, for example that of project manager,
designer or marketing expert. It therefore seems
possible that utterances directed towards particular
individuals will involve the use of different vocab-
ularies reflecting their expertise. Different words
or phrases may also be associated with generic
and referential discussion, and extracting these au-
tomatically may give benefits over attempting to
capture them using manually-defined features. To
exploit this, we therefore added the use of lexical
features: one feature for each distinct word or n-
gram seen more than once in the corpus. Although
such features may be corpus- or domain-specific,
they are easy to extract given a transcript.
4 Results and Discussion
4.1 Individual Tasks
We first examine the effect of lexical features on
the individual tasks, using 10-way cross-validation
and comparing performance with Frampton et al
(2009). Table 1 shows the results for the referen-
tiality task in terms of overall accuracy and per-
class F1-scores; ?MC Baseline? is the majority-
class baseline; results labelled ?EACL? are Framp-
ton et al (2009)?s figures, and are presented for
all features and for reduced feature sets which
might be more realistic in various situations: ?-V?
removes visual features; ?-VFD? removes visual
features, forward-looking discourse features and
dialogue-act tag features.
As can be seen, adding lexical features
(?+words? adds single word features, ?+3grams?
adds n-gram features of lengths 1-3) improves the
307
Features Acc Fgen Fref
MC Baseline 50.9 0 67.4
EACL 79.0 80.2 77.7
EACL -VFD 73.7 74.1 73.2
+words 85.3 85.7 84.9
+3grams 87.5 87.4 87.5
+3grams -VFD 87.2 86.9 87.6
3grams only 85.9 85.2 86.4
Table 1: Generic vs. referential uses
Features Acc Fsing Fplur
MC Baseline 67.9 80.9 0
EACL 77.1 83.3 63.2
EACL -VFD 71.4 81.5 37.1
+words 83.1 87.8 72.5
+3grams 85.9 90.0 76.6
+3grams -VFD 87.1 91.0 77.6
3grams only 86.9 90.8 77.0
Table 2: Singular vs. plural reference.
performance significantly ? accuracy is improved
by 8.5% absolute above the best EACL results,
which is a 40% reduction in error. Robustness to
removal of potentially problematic features is also
improved: removing all visual, forward-looking
and dialogue act features makes little difference.
In fact, using only lexical n-gram features, while
reducing accuracy by 2.6%, still performs better
than the best EACL classifier.
Table 2 shows the equivalent results for the
singular-plural reference distinction task; in this
experiment, we used a correlation-based fea-
ture selection method, following Frampton et al
(2009). Again, performance is improved, this time
giving a 8.8% absolute accuracy improvement, or
38% error reduction; robustness to removing vi-
sual and dialogue act features is also very good,
even improving performance.
For the individual reference task (again using
feature selection), we give a further ?NS baseline?
of taking the next speaker; note that this performs
rather well, but requires forward-looking informa-
tion so should not be compared to ?-F? results.
Results are again improved (Table 3), but the im-
provement is smaller: a 1.4% absolute accuracy
improvement (7% error reduction); we conclude
from this that visual information is most impor-
tant for this part of the task. Robustness to feature
unavailability still shows some improvement: ex-
Features Acc FP1 FP2 FP3 FP4
MC baseline 30.7 0 0 0 47.0
NS baseline 70.7 71.6 71.1 72.7 68.2
EACL 80.3 82.8 79.7 75.9 81.4
EACL -V 73.8 79.2 70.7 74.1 71.4
EACL -VFD 56.6 58.9 55.5 64.0 47.3
+words 81.4 83.9 79.7 79.3 81.8
+3grams 81.7 83.9 80.3 79.3 82.5
+3grams -V 74.8 81.3 71.7 75.2 71.4
+3grams -VFD 60.7 66.3 55.9 66.2 53.0
3grams only 60.7 63.1 58.1 52.9 63.4
3grams +NS 74.5 76.7 73.8 75.0 72.7
Table 3: Singular addressee detection.
cluding all visual, forward-looking and dialogue-
act features has less effect than on the EACL sys-
tem (60.7% vs. 56.6% accuracy), and a system
using only n-grams and the next speaker identity
gives a respectable 74.5%.
Feature Analysis We examined the contribu-
tion of particular lexical features using Informa-
tion Gain methods. For the referentiality task, we
found that generic uses of you were more likely
to appear in utterances containing words related to
the main meeting topic, such as button, channel,
or volume (properties of the to-be-designed remote
control). In contrast, words related to meeting
management, such as presentation, email, project
and meeting itself, were predictive of referential
uses. The presence of first person pronouns and
discourse and politeness markers such as okay,
please and thank you was also indicative of refer-
entiality, as were n-grams capturing interrogative
structures (e.g. do you).
For the plural/singular distinction, we found
that the plural first person pronoun we correlated
with plural references of you. Other predictive n-
grams for this task were you mean and you know,
which were indicative of singular and plural refer-
ences, respectively. Finally, for the individual ref-
erence task, useful lexical features included par-
ticipant names, and items related to their roles.
For instance, the n-grams sales, to sell and make
money correlated with utterances addressed to the
?marketing expert?, while utterances containing
speech recognition and technical were addressed
to the ?industrial designer?.
Discussion The best F-score of the three sub-
tasks is for the generic/referential distinction; the
308
Features Acc Fgen Fplur FP1 FP2 FP3 FP4
MC baseline 49.1 65.9 0 0 0 0 0
EACL 58.3 73.3 24.3 57.6 57.0 36.0 51.1
+3grams 60.9 74.8 42.0 57.7 52.2 35.6 50.2
3grams only 67.5 84.8 61.6 39.1 39.3 30.6 38.6
Cascade +3grams 78.1 87.4 59.1 64.1 76.4 75.0 82.6
Table 4: Combined task: generic vs. plural vs. singular addressee.
worst is for the detection of plural reference (Fplur
in Table 2). This is not surprising: humans find the
former task easy to annotate ? Gupta et al (2007)
report good inter-annotator agreement (? = 0.84)
? but the latter hard. In their analysis of the AMI
addressee annotations, Reidsma et al (2008) ob-
serve that most confusions amongst annotators are
between the group-addressing label and the labels
for individuals; whereas if annotators agree that an
utterance is addressed to an individual, they also
reach high agreement on that addressee?s identity.
4.2 Combined Task
We next combined the individual tasks into one
combined task; for each you instance, a 6-way
classification as generic, group-referring or refer-
ring to one of the 4 participants. This was at-
tempted both as a single classification exercise us-
ing a single Bayesian network; and as a cascaded
pipeline of the three individual tasks; see Table 4.
Both used correlation-based feature selection.
For the single joint classifier, n-grams again im-
prove performance over the EACL features. Using
only n-grams gives a significant improvement, per-
haps due to the reduction in the size of the feature
space on this larger problem. Accuracy is reason-
able (67.5%), but while F-scores are good for the
generic class (above 80%), others are low.
However, use of three cascaded classifiers
improves performance to 78% and gives large
per-class F-score improvements, exploiting
the higher accuracy of the first two stages
(generic/referential, singular/plural), and the fact
that different features are good for different tasks.
5 Conclusions
We have shown that the use of simple lexical fea-
tures can improve performance and robustness for
all aspects of second-person pronoun resolution:
referentiality detection and reference identifica-
tion. An overall 6-way classifier is feasible, and
cascading individual classifiers can help. Future
plans include testing on ASR transcripts, and in-
vestigating different classification techniques for
the joint task.
References
M. Frampton, R. Ferna?ndez, P. Ehlen, M. Christoudias,
T. Darrell, and S. Peters. 2009. Who is ?you?? com-
bining linguistic and gaze features to resolve second-
person references in dialogue. In Proceedings of the
12th Conference of the EACL.
S. Gupta, J. Niekrasz, M. Purver, and D. Jurafsky.
2007. Resolving ?you? in multi-party dialog. In
Proceedings of the 8th SIGdial Workshop on Dis-
course and Dialogue.
N. Jovanovic, R. op den Akker, and A. Nijholt. 2006.
Addressee identification in face-to-face meetings. In
Proceedings of the 11th Conference of the EACL.
N. Jovanovic. 2007. To Whom It May Concern -
Addressee Identification in Face-to-Face Meetings.
Ph.D. thesis, University of Twente, The Netherlands.
M. Katzenmaier, R. Stiefelhagen, and T. Schultz. 2004.
Identifying the addressee in human-human-robot in-
teractions based on head pose and speech. In Pro-
ceedings of the 6th International Conference on
Multimodal Interfaces.
I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-
ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, M. Kronenthal, G. Lathoud, M. Lin-
coln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of the 5th International Conference on
Methods and Techniques in Behavioral Research.
D. Reidsma, D. Heylen, and R. op den Akker. 2008.
On the contextual analysis of agreement scores. In
Proceedings of the LREC Workshop on Multimodal
Corpora.
D. Traum. 2004. Issues in multi-party dialogues. In
F. Dignum, editor, Advances in Agent Communica-
tion, pages 201?211. Springer-Verlag.
K. van Turnhout, J. Terken, I. Bakx, and B. Eggen.
2005. Identifying the intended addressee in mixed
human-humand and human-computer interaction
from non-verbal features. In Proceedings of ICMI.
309
Proceedings of the ACL 2010 Conference Short Papers, pages 307?312,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Decision detection using hierarchical graphical models
Trung H. Bui
CSLI
Stanford University
Stanford, CA 94305, USA
thbui@stanford.edu
Stanley Peters
CSLI
Stanford University
Stanford, CA 94305, USA
peters@csli.stanford.edu
Abstract
We investigate hierarchical graphical
models (HGMs) for automatically detect-
ing decisions in multi-party discussions.
Several types of dialogue act (DA) are
distinguished on the basis of their roles in
formulating decisions. HGMs enable us
to model dependencies between observed
features of discussions, decision DAs, and
subdialogues that result in a decision. For
the task of detecting decision regions, an
HGM classifier was found to outperform
non-hierarchical graphical models and
support vector machines, raising the
F1-score to 0.80 from 0.55.
1 Introduction
In work environments, people share information
and make decisions in multi-party conversations
known as meetings. The demand for systems that
can automatically process information contained
in audio and video recordings of meetings is grow-
ing rapidly. Our own research, and that of other
contemporary projects (Janin et al, 2004) aim at
meeting this demand.
We are currently investigating the automatic de-
tection of decision discussions. Our approach in-
volves distinguishing between different dialogue
act (DA) types based on their role in the decision-
making process. These DA types are called De-
cision Dialogue Acts (DDAs). Groups of DDAs
combine to form a decision region.
Recent work (Bui et al, 2009) showed that
Directed Graphical Models (DGMs) outperform
other machine learning techniques such as Sup-
port Vector Machines (SVMs) for detecting in-
dividual DDAs. However, the proposed mod-
els, which were non-hierarchical, did not signifi-
cantly improve identification of decision regions.
This paper tests whether giving DGMs hierarchi-
cal structure (making them HGMs) can improve
their performance at this task compared with non-
hierarchical DGMs.
We proceed as follows. Section 2 discusses re-
lated work, and section 3 our data set and anno-
tation scheme for decision discussions. Section
4 summarizes previous decision detection exper-
iments using DGMs. Section 5 presents the HGM
approach, and section 6 describes our HGM exper-
iments. Finally, section 7 draws conclusions and
presents ideas for future work.
2 Related work
User studies (Banerjee et al, 2005) have con-
firmed that meeting participants consider deci-
sions to be one of the most important meeting
outputs, and Whittaker et al (2006) found that
the development of an automatic decision de-
tection component is critical for re-using meet-
ing archives. With the new availability of sub-
stantial meeting corpora such as the AMI cor-
pus (McCowan et al, 2005), recent years have
seen an increasing amount of research on decision-
making dialogue. This research has tackled is-
sues such as the automatic detection of agreement
and disagreement (Galley et al, 2004), and of
the level of involvement of conversational partic-
ipants (Gatica-Perez et al, 2005). Recent work
on automatic detection of decisions has been con-
ducted by Hsueh and Moore (2007), Ferna?ndez et
al. (2008), and Bui et al (2009).
Ferna?ndez et al (2008) proposed an approach
to modeling the structure of decision-making di-
alogue. These authors designed an annotation
scheme that takes account of the different roles
that utterances can play in the decision-making
process?for example it distinguishes between
DDAs that initiate a decision discussion by rais-
ing an issue, those that propose a resolution of the
issue, and those that express agreement to a pro-
posed resolution. The authors annotated a por-
tion of the AMI corpus, and then applied what
307
they refer to as ?hierarchical classification.? Here,
one sub-classifier per DDA class hypothesizes oc-
currences of that type of DDA and then, based
on these hypotheses, a super-classifier determines
which regions of dialogue are decision discus-
sions. All of the classifiers, (sub and super), were
linear kernel binary SVMs. Results were bet-
ter than those obtained with (Hsueh and Moore,
2007)?s approach?the F1-score for detecting de-
cision discussions in manual transcripts was 0.58
vs. 0.50. Purver et al (2007) had earlier detected
action items with the approach Ferna?ndez et al
(2008) extended to decisions.
Bui et al (2009) built on the promising results
of (Ferna?ndez et al, 2008), by employing DGMs
in place of SVMs. DGMs are attractive because
they provide a natural framework for modeling se-
quence and dependencies between variables, in-
cluding the DDAs. Bui et al (2009) were espe-
cially interested in whether DGMs better exploit
non-lexical features. Ferna?ndez et al (2008) ob-
tained much more value from lexical than non-
lexical features (and indeed no value at all from
prosodic features), but lexical features have limi-
tations. In particular, they can be domain specific,
increase the size of the feature space dramatically,
and deteriorate more in quality than other features
when automatic speech recognition (ASR) is poor.
More detail about decision detection using DGMs
will be presented in section 4.
Beyond decision detection, DGMs are used for
labeling and segmenting sequences of observa-
tions in many different fields?including bioin-
formatics, ASR, Natural Language Processing
(NLP), and information extraction. In particular,
Dynamic Bayesian Networks (DBNs) are a pop-
ular model for probabilistic sequence modeling
because they exploit structure in the problem to
compactly represent distributions over multi-state
and observation variables. Hidden Markov Mod-
els (HMMs), a special case of DBNs, are a classi-
cal method for important NLP applications such
as unsupervised part-of-speech tagging (Gael et
al., 2009) and grammar induction (Johnson et al,
2007) as well as for ASR. More complex DBNs
have been used for applications such as DA recog-
nition (Crook et al, 2009) and activity recogni-
tion (Bui et al, 2002).
Undirected graphical models (UGMs) are also
valuable for building probabilistic models for seg-
menting and labeling sequence data. Conditional
Random Fields (CRFs), a simple UGM case, can
avoid the label bias problem (Lafferty et al, 2001)
and outperform maximum entropy Markov mod-
els and HMMs.
However, the graphical models used in these
applications are mainly non-hierarchical, includ-
ing those in Bui et al (2009). Only Sutton et al
(2007) proposed a three-level HGM (in the form of
a dynamic CRF) for the joint noun phrase chunk-
ing and part of speech labeling problem; they
showed that this model performs better than a non-
hierarchical counterpart.
3 Data
For the experiments reported in this study, we
used 17 meetings from the AMI Meeting Corpus1,
a freely available corpus of multi-party meetings
with both audio and video recordings, and a wide
range of annotated information including DAs and
topic segmentation. The meetings last around 30
minutes each, and are scenario-driven, wherein
four participants play different roles in a com-
pany?s design team: project manager, marketing
expert, interface designer and industrial designer.
We use the same annotation scheme as
Ferna?ndez et al (2008) to model decision-making
dialogue. As stated in section 2, this scheme dis-
tinguishes between a small number of DA types
based on the role which they perform in the for-
mulation of a decision. Besides improving the de-
tection of decision discussions (Ferna?ndez et al,
2008), such a scheme also aids in summarization
of them, because it indicates which utterances pro-
vide particular types of information.
The annotation scheme is based on the observa-
tion that a decision discussion typically contains
the following main structural components: (a) A
topic or issue requiring resolution is raised; (b)
One or more possible resolutions are considered;
(c) A particular resolution is agreed upon, and so
adopted as the decision. Hence the scheme dis-
tinguishes between three main DDA classes: issue
(I), resolution (R), and agreement (A). Class R is
further subdivided into resolution proposal (RP)
and resolution restatement (RR). I utterances in-
troduce the topic of the decision discussion, ex-
amples being ?Are we going to have a backup??
and ?But would a backup really be necessary?? in
Table 1. In comparison, R utterances specify the
resolution which is ultimately adopted as the deci-
1http://corpus.amiproject.org/
308
(1) A: Are we going to have a backup? Or we do
just?
B: But would a backup really be necessary?
A: I think maybe we could just go for the
kinetic energy and be bold and innovative.
C: Yeah.
B: I think? yeah.
A: It could even be one of our selling points.
C: Yeah ?laugh?.
D: Environmentally conscious or something.
A: Yeah.
B: Okay, fully kinetic energy.
D: Good.
Table 1: An excerpt from the AMI dialogue
ES2015c. It has been modified slightly for pre-
sentation purposes.
sion. RP utterances propose this resolution (e.g. ?I
think maybe we could just go for the kinetic energy
. . . ?), while RR utterances close the discussion by
confirming/summarizing the decision (e.g. ?Okay,
fully kinetic energy?). Finally, A utterances agree
with the proposed resolution, signaling that it is
adopted as the decision, (e.g. ?Yeah?, ?Good? and
?Okay?). Unsurprisingly, an utterance may be as-
signed to more than one DDA class; and within a
decision discussion, more than one utterance can
be assigned to the same DDA class.
We use manual transcripts in the experiments
described here. Inter-annotator agreement was sat-
isfactory, with kappa values ranging from .63 to
.73 for the four DDA classes. The manual tran-
scripts contain a total of 15,680 utterances, and on
average 40 DDAs per meeting. DDAs are sparse
in the transcripts: for all DDAs, 6.7% of the total-
ity of utterances; for I,1.6%; for RP, 2%; for RR,
0.5%; and for A, 2.6%. In all, 3753 utterances (i.e.,
23.9%) are tagged as decision-related utterances,
and on average there are 221 decision-related ut-
terances per meeting.
4 Prior Work on Decision Detection
using Graphical Models
To detect each individual DDA class, Bui et al
(2009) examined the four simple DGMs shown
in Fig. 1. The DDA node is binary valued, with
value 1 indicating the presence of a DDA and 0
its absence. The evidence node (E) is a multi-
dimensional vector of observed values of non-
lexical features. These include utterance features
(UTT) such as length in words2, duration in mil-
liseconds, position within the meeting (as percent-
age of elapsed time), manually annotated dialogue
act (DA) features3 such as inform, assess, suggest,
and prosodic features (PROS) such as energy and
pitch. These features are the same as the non-
lexical features used by Ferna?ndez et al (2008).
The hidden component node (C) in the -mix mod-
els represents the distribution of observable evi-
dence E as a mixture of Gaussian distributions.
The number of Gaussian components was hand-
tuned during the training phase.
DDA
E
a) BN-sim
DDA
E
b) BN-mix
C
DDA
time t-1 time t
E
DDA
E
c) DBN-sim
DDA
time t-1 time t
E
DDA
E
d) DBN-mix
CC
Figure 1: Simple DGMs for individual decision
dialogue act detection. The clear nodes are hidden,
and the shaded nodes are observable.
More complex models were constructed from
the four simple models in Fig. 1 to allow for de-
pendencies between different DDAs. For exam-
ple, the model in Fig. 2 generalizes Fig. 1c with
arcs connecting the DDA classes based on analy-
sis of the annotated AMI data.
Atime t-1 time t
E E
I RP RR AI RP RR
Figure 2: A DGM that takes the dependencies be-
tween decision dialogue acts into account.
Decision discussion regions were identified us-
ing the DGM output and the following two simple
rules: (1) A decision discussion region begins with
an Issue DDA; (2) A decision discussion region
contains at least one Issue DDA and one Resolu-
tion DDA.
2This feature is a manual count of lexical tokens; but word
count was extracted automatically from ASR output by Bui
et al (2009). We plan experiments to determine how much
using ASR output degrades detection of decision regions.
3The authors used the AMI DA annotations.
309
The authors conducted experiments using the
AMI corpus and found that when using non-
lexical features, the DGMs outperform the hierar-
chical SVM classification method of (Ferna?ndez et
al., 2008). The F1-score for the four DDA classes
increased between 0.04 and 0.19 (p < 0.005),
and for identifying decision discussion regions, by
0.05 (p > 0.05).
5 Hierarchical graphical models
Although the results just discussed showed graph-
ical models are better than SVMs for detecting de-
cision dialogue acts (Bui et al, 2009), two-level
graphical models like those shown in Figs. 1 and 2
cannot exploit dependencies between high-level
discourse items such as decision discussions and
DDAs; and the ?superclassifier? rule (Bui et al,
2009) used for detecting decision regions did not
significantly improve the F1-score for decisions.
We thus investigate whether HGMs (structured
as three or more levels) are superior for discov-
ering the structure and learning the parameters
of decision recognition. Our approach composes
graphical models to increase hierarchy with an ad-
ditional level above or below previous ones, or in-
serts a new level such as for discourse topics into
the interior of a given model.
Fig. 3 shows a simple structure for three-level
HGMs. The top level corresponds to high-level
discourse regions such as decision discussions.
The segmentation into these regions is represented
in terms of a random variable (at each DR node)
that takes on discrete values: {positive, negative}
(the utterance belongs to a decision region or not)
or {begin, middle, end, outside} (indicating the
position of the utterance relative to a decision dis-
cussion region). The middle level corresponds to
mid-level discourse items such as issues, resolu-
tion proposals, resolution restatements, and agree-
ments. These classes (C1, C2, ..., Cn nodes) are
represented as a collection of random variables,
each corresponding to an individual mid-level ut-
terance class. For example, the middle level of the
three-level HGM Fig. 3 could be the top-level of
the two-level DGM in Fig. 2, each middle level
node containing random variables for the DDA
classes I, RP, RR, and A. The bottom level cor-
responds to vectors of observed features as before,
e.g. lexical, utterance, and prosodic features.
CnC
Cn
C
DR DR
C1
E ELevel 1
Level 2
Level 3
current utterance next utterance
C1
Figure 3: A simple structure of a three-level
HGM: DRs are high-level discourse regions;
C1, C2, ..., Cn are mid-level utterance classes; and
Es are vectors of observed features.
6 Experiments
The HGM classifier in Figure 3 was implemented
in Matlab using the BNT software4. The classifier
hypothesizes that an utterance belongs to a deci-
sion region if the marginal probability of the ut-
terance?s DR node is above a hand-tuned thresh-
old. The threshold is selected using the ROC curve
analysis5 to obtain the highest F1-score. To evalu-
ate the accuracy of hypothesized decision regions,
we divided the dialogue into 30-second windows
and evaluated on a per window basis.
The best model structure was selected by com-
paring the performance of various handcrafted
structures. For example, the model in Fig. 4b out-
performs the one in Fig. 4a. Fig. 4b explicitly
models the dependency between the decision re-
gions and the observed features.
I RP RR A
DR
E
I RP RR A
DR
E
a) b)
Figure 4: Three-level HGMs for recognition of de-
cisions. This illustrates the choice of the structure
for each time slice of the HGM sequence models.
Table 2 shows the results of 17-fold cross-
validation for the hierarchical SVM classifica-
tion (Ferna?ndez et al, 2008), rule-based classifi-
cation with DGM output (Bui et al, 2009), and
our HGM classification using the best combina-
tion of non-lexical features. All three methods
4http://www.cs.ubc.ca/?murphyk/Software/BNT/bnt.html
5http://en.wikipedia.org/wiki/Receiver operating characteristic
310
were implemented by us using exactly the same
data and 17-fold cross-validation. The features
were selected based on the best combination of
non-lexical features for each method. The HGM
classifier outperforms both its SVM and DGM
counterparts (p < 0.0001)6. In fact, even when the
SVM uses lexical as well as non-lexical features,
its F1-score is still lower than the HGM classifier.
Classifier Pr Re F1
SVM 0.35 0.88 0.50
DGM 0.39 0.93 0.55
HGM 0.69 0.96 0.80
Table 2: Results for detection of decision dis-
cussion regions by the SVM super-classifier,
rule-based DGM classifier, and HGM clas-
sifier, each using its best combination of
non-lexical features: SVM (UTT+DA), DGM
(UTT+DA+PROS), HGM (UTT+DA).
In contrast with the hierarchical SVM and rule-
based DGM methods, the HGM method identifies
decision-related utterances by exploiting not just
DDAs but also direct dependencies between deci-
sion regions and UTT, DA, and PROS features. As
mentioned in the second paragraph of this section,
explicitly modeling the dependency between deci-
sion regions and observable features helps to im-
prove detection of decision regions. Furthermore,
a three-level HGM can straightforwardly model
the composition of each high-level decision region
as a sequence of mid-level DDA utterances. While
the hierarchical SVM method can also take depen-
dency between successive utterances into account,
it has no principled way to associate this depen-
dency with more extended decision regions. In
addition, this dependency is only meaningful for
lexical features (Ferna?ndez et al, 2008).
The HGM result presented in Table 2 was
computed using the three-level DBN model (see
Fig. 4b) using the combination of UTT and DA
features. Without DA features, the F1-score de-
grades from 0.8 to 0.78. However, this difference
is not statistically significant (i.e., p > 0.5).
7 Conclusions and Future Work
To detect decision discussions in multi-party dia-
logue, we investigated HGMs as an extension of
6We used the paired t test for computing statistical signif-
icance. http://www.graphpad.com/quickcalcs/ttest1.cfm
the DGMs studied in (Bui et al, 2009). When
using non-lexical features, HGMs outperform the
non-hierarchical DGMs of (Bui et al, 2009) and
also the hierarchical SVM classification method
of Ferna?ndez et al (2008). The F1-score for
identifying decision discussion regions increased
to 0.80 from 0.55 and 0.50 respectively (p <
0.0001).
In future work, we plan to (a) investigate cas-
caded learning methods (Sutton et al, 2007) to
improve the detection of DDAs further by using
detected decision regions and (b) extend HGMs
beyond three levels in order to integrate useful se-
mantic information such as topic structure.
Acknowledgments
The research reported in this paper was spon-
sored by the Department of the Navy, Office of
Naval Research, under grants number N00014-
09-1-0106 and N00014-09-1-0122. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the Office of
Naval Research.
References
Satanjeev Banerjee, Carolyn Rose?, and Alex Rudnicky.
2005. The necessity of a meeting recording and
playback system, and the benefit of topic-level anno-
tations to meeting browsing. In Proceedings of the
10th International Conference on Human-Computer
Interaction.
H. H. Bui, S. Venkatesh, and G. West. 2002. Pol-
icy recognition in the abstract hidden markov model.
Journal of Artificial Intelligence Research, 17:451?
499.
Trung Huu Bui, Matthew Frampton, John Dowding,
and Stanley Peters. 2009. Extracting decisions from
multi-party dialogue using directed graphical mod-
els and semantic similarity. In Proceedings of the
10th Annual SIGDIAL Meeting on Discourse and
Dialogue (SIGdial09).
Nigel Crook, Ramon Granell, and Stephen Pulman.
2009. Unsupervised classification of dialogue acts
using a dirichlet process mixture model. In Pro-
ceedings of SIGDIAL 2009: the 10th Annual Meet-
ing of the Special Interest Group in Discourse and
Dialogue, pages 341?348.
Raquel Ferna?ndez, Matthew Frampton, Patrick Ehlen,
Matthew Purver, and Stanley Peters. 2008. Mod-
elling and detecting decisions in multi-party dia-
logue. In Proceedings of the 9th SIGdial Workshop
on Discourse and Dialogue.
311
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 678?687.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of Bayesian networks to model pragmatic de-
pendencies. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Daniel Gatica-Perez, Ian McCowan, Dong Zhang, and
Samy Bengio. 2005. Detecting group interest level
in meetings. In Proceedings of ICASSP.
Pey-Yun Hsueh and Johanna Moore. 2007. Automatic
decision detection in meeting speech. In Proceed-
ings of MLMI 2007, Lecture Notes in Computer Sci-
ence. Springer-Verlag.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip
Dhillon, Jane Edwards, Javier Marc??as-Guarasa,
Nelson Morgan, Barbara Peskin, Elizabeth Shriberg,
Andreas Stolcke, Chuck Wooters, and Britta Wrede.
2004. The ICSI meeting project: Resources and re-
search. In Proceedings of the 2004 ICASSP NIST
Meeting Recognition Workshop.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proceedings of
Human Language Technologies 2007: The Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 139?146,
Rochester, New York, April. Association for Com-
putational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, pages 282?
289. Morgan Kaufmann.
Iain McCowan, Jean Carletta, W. Kraaij, S. Ashby,
S. Bourban, M. Flynn, M. Guillemot, T. Hain,
J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud,
M. Lincoln, A. Lisowska, W. Post, D. Reidsma, and
P. Wellner. 2005. The AMI Meeting Corpus. In
Proceedings of Measuring Behavior, the 5th Inter-
national Conference on Methods and Techniques in
Behavioral Research, Wageningen, Netherlands.
Matthew Purver, John Dowding, John Niekrasz,
Patrick Ehlen, Sharareh Noorbaloochi, and Stanley
Peters. 2007. Detecting and summarizing action
items in multi-party dialogue. In Proceedings of the
8th SIGdial Workshop on Discourse and Dialogue,
Antwerp, Belgium.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. Journal of Machine
Learning Research, 8:693?723.
Steve Whittaker, Rachel Laban, and Simon Tucker.
2006. Analysing meeting records: An ethnographic
study and technological implications. In S. Renals
and S. Bengio, editors, Machine Learning for Multi-
modal Interaction: Second International Workshop,
MLMI 2005, Revised Selected Papers, volume 3869
of Lecture Notes in Computer Science, pages 101?
113. Springer.
312
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 253?256,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Detection of time-pressure induced stress in speech via acoustic indicators ?
Matthew Frampton, Sandeep Sripada, Ricardo Augusto Hoffmann Bion and Stanley Peters
Center for the Study of Language and Information
Stanford University, Stanford, CA, 94305 USA
{frampton@,ssandeep@,ricardoh@,peters@csli.}stanford.edu
Abstract
We use automatically extracted acoustic
features to detect speech which is gener-
ated under stress, achieving 76.24% accu-
racy with a binary logistic regression. Our
data are task-oriented human-human dia-
logues in which a time-limit is unexpect-
edly introduced partway through. Anal-
ysis suggests that we can detect approxi-
mately when this event occurs. We also
consider the importance of normalizing
the acoustic features by speaker, and de-
tecting stress in new speakers.
1 Introduction
The term stressed speech can refer to speech
generated under psychological stress (Sigmund et
al., 2007). Stress alters an individual?s mental
and physiological state, which then affects their
speech. The ability to identify stressed speech
would be very valuable to Spoken Dialogue Sys-
tems (SDSs), especially in ?stressful? applications
such as search-and-rescue robots. Speech recog-
nizers are usually trained on normal speech, and
so can struggle badly on other speech. Tech-
niques exist for making ASR robust to noise/stress
(Hansen and Patil, 2007), but knowing when to ap-
ply them will in general require the ability to de-
tect stressed speech. This ability is clearly also
needed when the user?s stress level should affect
how the SDS responds. An SDS should some-
times generate stressed speech itself?for exam-
ple, to impart a sense of urgency on the user.
This paper investigates spectral-based acoustic
indicators of stress in human-human, task-oriented
?The research reported in this paper was sponsored by the
Department of the Navy, Office of Naval Research, under
grant number N00014-017-1-1049. Any opinions, findings
and conclusions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reflect the
views of the Office of Naval Research.
dialogues in which stress is induced in the lat-
ter stages by the unexpected introduction of time-
pressure. Unlike previous studies, we detect stress
in whole utterances in the raw audio, which is
more realistic for applications. We also consider
the importance of normalizing the features, and
detection of both the introduction of the stressor,
and stress in new speakers.
2 Related work
Stressors and clip sizes: The stressors in pre-
vious studies include logical problems, images of
human bodies with skin diseases/severe accident
injuries (Tolkmitt and Scherer, 1986), loss of con-
trol of a helicopter (Protopapas and Liberman,
2001), university examinations (Sigmund et al,
2007), and an increasingly difficult air controller
simulation and verbal quiz (Scherer et al, 2008).
Sigmund et al (2007) detect stress in approxi-
mately 2000 voiced segments of 5 vowels. Tolk-
mitt and Scherer (1986), Protopapas and Liberman
(2001) and Scherer et al (2008) detect stress in
whole utterances, but these are respectively, read
from a card, quiz answers, and with verbal content
removed. Studies on the Speech under Simulated
and Actual Stress (SUSAS) corpus (Hansen and
Bou-Ghazale, 1997) detect stress in words. These
include (Hansen, 1996; Zhou, 1999; Hansen and
Womack, 1996; Zhou, 2001; Casale et al, 2007).
The SUSAS corpus contains aircraft communica-
tion words from a common highly confusable vo-
cabulary set of 35, and they are divided into differ-
ent speaking styles.
Acoustic cues: The most widely investigated
acoustic cues relate to fundamental frequency (F0,
also called pitch), formant frequencies and spec-
tral composition e.g. (Tolkmitt and Scherer, 1986;
Hansen, 1996; Zhou, 1999; Protopapas and Liber-
man, 2001; Sigmund et al, 2007; Scherer et
al., 2008). Mel-Frequency Cepstral Coefficients
253
Category Examples
F0-related Median, mean, minimum, time of minimum as % thr? clip, max, time of max as % thr? clip,
range (max-min), standard deviation, mean absolute slope,
mean slope without octave jumps, number of voiced frames.
Intensity-related Median, mean, minimum, time of minimum as % thr? clip, max, time of max as % thr? clip,
range (max-min), standard deviation.
Formant-related Mean, minimum, time of minimum as % through clip, max, time of max as % through clip,
(for F1-F3) range (max-min).
Spectral tilt-related Mean, minimum, maximum, range (max-min).
Table 1: The acoustic features which are extracted from the audio clips using Praat (Boersma and Weenink, 2010).
(MFCCs)1 and Teager Energy Operator (TEO)2
(Kaiser, 1990) based features have also been con-
sidered e.g. (Hansen and Womack, 1996; Zhou,
2001; Casale et al, 2007).
Features of all these types have proved useful
in detecting stressed speech. The classification
methods employed are various, including a tradi-
tional binary hypothesis detection-theory method
(Zhou, 1999) and neural networks (Hansen and
Womack, 1996; Scherer et al, 2008), while Casale
et al (2007) used genetic algorithms for feature
selection. Of the two more recent studies which
detected stress in whole utterances, Protopapas
and Lieberman found that mean and maximum F0
within an utterance correlate highly with subject
stress ratings, and Scherer et al?s neural network
outperformed a human baseline. Note that find-
ings/results in these and other previous studies are
not directly comparable with our own, because we
detect stress in whole utterances in raw audio.
3 Data
The original data (Eberhard et al, 2010) are 4
task-oriented dialogues between 2 native English-
speaking participants. Hence there are 8 speakers
in total (7 male, 1 female), and the dialogues con-
tain 263, 172, 228 and 210 utterances respectively.
During a dialogue, the participants (the direc-
tor and member) are on a floor with corridors and
rooms that contain various colored boxes. The di-
rector stays in one room, and gives task instruc-
tions via walkie-talkie to the member, providing
directions with a map which is partially complete
and accurate for box locations. The tasks are lo-
cating boxes which are unmarked on the map, and
transferring blocks between and retrieving speci-
fied boxes. Initial instructions do not mention a
1MFCCs model the human auditory system?s nonlinear
filtering in measuring spectral band energies.
2The TEO is a nonlinear operator which uses mechanical
and physical considerations to extract the signal energy.
time limit, but at the end of the 7th minute, the di-
rector is given a timer and told there are 3 minutes
to complete the current tasks, plus one new task.
We use the Nuance speech recognizer (V. 9.0)
to end-point each dialogue?s audio signal, and the
resulting clips are mostly 1 to 3 seconds. In
preliminary experiments (not reported), denoising
seemed to remove acoustic information which is
indicative of stress. Hence we use raw audio.
Stressed speech: For present purposes, we as-
sume that all speech after the introduction of the
time limit is stressed. Hence 448 of the 663 au-
dio clips in our experimental data are unstressed,
and 215 are stressed. In future we plan to use
the Amazon Mechanical Turk to obtain perceived
stress ratings on a scale with more gradations.
4 Experiments
Acoustic features: We use Praat (Boersma and
Weenink, 2010) to compute F0, intensity, formant
and spectral tilt-related features for each clip (Ta-
ble 1). F0 (pitch) corresponds to the rate of vocal
cord vibration in Hertz (Hz), and Intensity, to the
sound?s loudness in decibels (dB), (derived from
the amplitude or increase in air pressure). A for-
mant is a concentration of acoustic energy around
a particular frequency in the speech wave. There
are several, each corresponding to a resonance in
the vocal tract, and we consider the lowest three
(F1-F3). Spectral tilt measures the difference in
energy between the 1st and 2nd formants, and so
estimates the degree to which energy at the funda-
mental dominates in the glottal source waveform.
Comparing different normalization methods:
We evaluate binary logistic regression models with
10-fold cross-validation, and try the following 4
methods for normalizing each clip?s acoustic fea-
tures according to its speaker.
? Maximum normalization: Due to the possi-
bility of outliers, we divide each feature value
254
Normalization % Accuracy US %correct S %correct MCB
Maximum normalization 74.4 (74.25) 86.67 (85.05) 48.5 (54.3) 67.8 (67.1)
Z-score 73.5 (73.78) 84.89 (84.12) 49.53 (52.7) 67.8 (67.1)
US Average 75.61 (76.24) 86.63 (86.2) 53.5 (55.9) 67.8 (67.1)
S Average 75.31 (75) 84.67 (84.375) 55.6 (55.9) 67.8 (67.1)
No normalization 68.52 (70.45) 84.34 (82.8) 37.4 (45.2) 67.8 (67.1)
Table 2: Binary logistic regression 10-fold cross validation with different feature normalization approaches: Scores within
brackets are when the female speaker data is removed; S = Stressed, US = Unstressed, MCB = Majority Class Baseline.
by the 95th percentile value for that feature,
rather than the maximum.
? Z-score: Using the mean and standard devi-
ation for each feature, the feature vector is
converted to Z-scores3.
? Unstressed (US) average: Each feature is
normalized by its mean value in the un-
stressed region.
? Stressed (S) average: Each feature is normal-
ized by its mean value in the stressed region.
Table 2 shows the results. All those gener-
ated with feature normalization are significantly
better (p < 0.005) than the majority class base-
line (MCB), (i.e. classifying all utterances as un-
stressed). Without normalization, the overall accu-
racy drops about 5?6%, and the stressed speech
class about 11?18%. Different normalization
methods do not produce very different results,
but US average gives the best overall accuracy
(75.61%). When we remove the female speaker,
this increases to 76.24%, and feature normaliza-
tion remains important.
We also tested our assumption that the speech
before and after the introduction of time-pressure
is unstressed and stressed respectively, by check-
ing that they really are different. As before, we
considered 7 minutes unstressed, and 3 stressed,
and used US average normalization. However we
now assigned different minutes to the unstressed
and stressed categories: first we swapped the 6th
and 8th, then also the 5th and 9th, and then also the
7th and 10th. As a result, classification accuracy
dropped, (to 75%, then 68.71%, then 67.66%),
which supports our assumption.
Feature contribution analysis: Table 3 shows
the US average normalized features with informa-
tion gain greater than zero. Intensity and pitch
features are ranked most predictive (i.e. maximum
3A Z-score indicates the number of standard deviations
between an observation and the mean.
and mean intensity, and mean and median pitch),
but Spectral tilt mean and a couple of formant fea-
tures are also predictive. In general, higher values
for the most predictive pitch and intensity features
(e.g. Intensity max and Pitch mean) seem to indi-
cate stress. An interaction term for Intensity max
and Pitch mean caused a significant improvement
in the fit of the model?the ?2 value (or change in
the -2 Log likelihood) was 4.952 (p < 0.05).
Feature Info. Gain
Intensity max .101
Pitch mean .099
Intensity mean .099
Pitch median .088
Pitch max .059
Intensity min .046
Spectral tilt mean .042
Pitch min .041
F1 min .038
Intensity range .034
Intensity std. dev. .033
F3 range .033
Intensity median .031
Table 3: Unstressed average normalized features ranked by
information gain.
Detecting the introduction of the stressor:
Figure 1 shows the percentage of audio clips in
each minute that were classified as stressed. As we
would hope, there is a dramatic increase from the
7th to the 8th minute (around 20% to over 50%).
Such an increase could be used to detect the intro-
duction of the stressor, time-pressure.
Detecting stress in new speakers: To detect
stressed speech in new speakers, we evaluate the
logistic regression with an 8-fold cross-validation,
in each fold training on 7 speakers, and testing on
the other. We apply US average normalization, ini-
tially with the average values for the new speaker?s
unstressed speech, and then with the average val-
ues in unstressed speech across all ?seen? speakers
(speakers in the training set). Evaluation scores
(Table 4) are now lower, especially for the lat-
ter approach, but the former remains significantly
255
Figure 1: The percentage of clips in each minute of the
dialogues which our classifier marks as stressed, (note that
time-pressure is introduced at the end of minute 7).
better than the MCB. Since the female speaker?s
stress class F-score is 0, we tried normalizing the
7 male speakers based on only seen male data,
and then average accuracy for a male rose from
67.09% to 68.02% (not statistically significant).
Spkr % Accuracy F-unstress F-stress
1 62.5 (62.2) .77 (.74) .07 (0.34)
2 75 (75) .84 (.84) .09 (0.44)
3 57.6 (72.9) .62 (.81) .49 (0.5)
4 71.73 (74) .84 (.83) 0 (0.43)
5 71.62 (73.0) .77 (.77) .62 (0.68)
6 77.6 (80.4) .86 (.88) .51 (0.52)
7 64.36 (65.6) .74 (.77) .4 (0.35)
8 60.97 (71.7) .67 (.8) .49 (0.46)
Av. 67.67 (71.9) .76 (.80) .33 (0.46)
Table 4: Predicting stress in new speakers: New speaker
features are normalized based on unstressed speech for all
speakers in training set (unbracketed) and on their own un-
stressed speech (bracketed). Speaker 4 is the female.
5 Conclusion
For detecting stressed speech, we demonstrated
the importance of normalizing acoustic features by
speaker, and achieved 76.24% classification accu-
racy with a binary logistic regression model. The
most indicative features were maximum and mean
intensity within an utterance, and mean and me-
dian pitch. After the introduction of time-pressure,
the percentage of clips classified as stressed in-
creased dramatically, showing that it is possible
to detect approximately when this event occurs.
We also attempted to detect stressed speech in new
speakers, and as expected, results were poorer.
In future work we plan to expand our data-set
with more dialogues, and test accuracy for detect-
ing the introduction of the stressor. We want to use
MFCCs and TEO features, and also non-acoustic
features such as disfluency features. As mentioned
previously, we also hope to move beyond binary
classification, by acquiring perceived stress ratings
on a scale with more gradations.
References
P. Boersma and D. Weenink. 2010. Praat: doing pho-
netics by computer (version 5.1.29). Available from
http://www.praat.org/. [Computer program].
S. Casale, A. Russo, and S. Serrano. 2007. Multistyle classi-
fication of speech under stress using feature subset selec-
tion based on genetic algorithms. Speech Communication,
49:801?810.
K. Eberhard, H. Nicholson, S. Kubler, S. Gundersen, and M.
Scheutz. 2010. The Indiana ?Cooperative Remote Search
Task? (CReST) Corpus. In Proc. of LREC.
J.H.L. Hansen and S. Bou-Ghazale. 1997. Getting started
with SUSAS: a Speech Under Simulated and Actual Stress
database. In Eurospeech-97: International Conference on
Speech Communication and Technology.
J. Hansen and S. Patil, 2007. Speaker Classification I: Fun-
damentals, Features, and Methods, chapter Speech Under
Stress: Analysis, Modeling and Recognition, pages 108?
137. Springer-Verlag, Berlin, Heidelberg.
J. Hansen and B.Womack. 1996. Feature analysis and neural
network based classification of speech under stress. IEEE
Transactions on Speech & Audio Processing, 4(4):307?
313.
J. Hansen. 1996. Analysis and compensation of speech
under stress and noise for environmental robustness in
speech recognition. Speech Communications, Special Is-
sue on Speech Under Stress, 20(2):151?170.
J.F. Kaiser. 1990. On a simple algorithm to calculate the
energy of a signal. In Proc. of ICASSP.
A. Protopapas and P. Liberman. 2001. Fundamental fre-
quency of phonation and perceived emotional stress. Jour-
nal of the Acoustical Society of America, 101(4):2267?
2277.
S. Scherer, H. Hofmann, M. Lampmann, M. Pfeil, S. Rhinow,
F. Schwenker, and G. Palm. 2008. Emotion recognition
from speech: Stress experiment. In Proc. of LREC.
M. Sigmund, A. Prokes, and Z. Brabec. 2007. Statistical
analysis of glottal pulses in speech under psychological
stress. In Proc. of the 16th European Signal Processing
Conference.
F. J. Tolkmitt and K. R. Scherer. 1986. Effect of experi-
mentally induced stress on vocal parameters. Journal of
Experimental Psychology, 12(3):302?313.
G. Zhou. 1999. Nonlinear speech analysis and acoustic
model adaptation with applications to stress classification
and speech recognition. Ph.D. thesis, Duke University.
G. Zhou. 2001. Nonlinear feature based classification of
speech under stress. IEEE Transactions on Speech & Au-
dio Processing, 9:201?216.
256
