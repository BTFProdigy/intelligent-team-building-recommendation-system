Event-based Information Extraction for the biomedical domain: the Caderige project 
 
Erick Alphonse**, Sophie Aubin*, Philippe Bessi?res**, Gilles Bisson****, Thierry Hamon*, 
Sandrine Lagarrigue***, Adeline Nazarenko*, Alain-Pierre Manine**, Claire N?dellec**, 
Mohamed Ould Abdel Vetah**, Thierry Poibeau*, Davy Weissenbacher* 
 
*Laboratoire d?Informatique de Paris-Nord  
CNRS UMR 7030 
Av. J.B. Cl?ment 93430 F-Villetaneuse 
{firstname.lastname}@lipn.univ-paris13.fr 
**Laboratoire Math?matique, Informatique et G?nome (MIG), 
INRA,  
Domaine de Vilvert, 78352 F-Jouy-en-Josas 
{firstname.lastname}@jouy.inra.fr 
***Laboratoire de G?n?tique Animale,  
INRA-ENSAR 
Route de Saint Brieuc, 35042 Rennes Cedex 
lagarrig@roazhon.inra.fr 
****Laboratoire Leibniz ? UMR CNRS 5522  
46 Avenue F?lix Viallet - 38031 F-Grenoble Cedex 
Gilles.Bisson@imag.fr 
 
 Abstract  
This paper gives an overview of the 
Caderige project. This project involves 
teams from different areas (biology, 
machine learning, natural language 
processing) in order to develop high-
level analysis tools for extracting 
structured information from biological 
bibliographical databases, especially 
Medline. The paper gives an overview 
of the approach and compares it to the 
state of the art.  
1 Introduction 
Developments in biology and biomedicine are 
reported in large bibliographical databases 
either focused on a specific species (e.g. 
Flybase, specialized on Drosophilia 
Menogaster) or not (e.g. Medline). This type 
of  information sources is crucial for biologists 
but there is a lack of tools to explore them and 
extract relevant information. While recent 
named entity recognition tools have gained a 
certain success on these domains, event-based 
Information Extraction (IE) is still a challenge.  
The Caderige project aims at designing and 
integrating Natural Language Processing 
(NLP) and Machine Learning (ML) techniques 
to explore, analyze and extract targeted 
information in biological textual databases. We 
promote a corpus-based approach focusing on 
text pre-analysis and normalization: it is 
intended to drain out the linguistic variation 
dimension, as most as possible. Actually, the 
MUC (1995) conferences have demonstrated 
that extraction is more efficient when 
performed on normalized texts. The extraction 
patterns are thus easier to acquire or learn, 
more abstract and easier to maintain 
Beyond extraction patterns, it is also possible 
to acquire from the corpus, via ML methods, a 
part of the knowledge necessary for text 
normalization as shown here.  
This paper gives an overview of current 
research activities and achievements of the 
Caderige project. The paper first presents our 
approach and compares it with the one 
developed in the framework of a similar 
project called Genia (Collier et al 1999). We 
then propose an account of Caderige 
techniques on various filtering and 
normalization tasks, namely, sentence filtering, 
resolution of named entity synonymy, 
syntactic parsing, and ontology learning. 
Finally, we show how extraction patterns can 
be learned from normalized and annotated 
documents, all applied to biological texts.  
2 Description of our approach 
In this section, we give some details about the 
motivations and choices of implementation. 
We then briefly compare our approach with the 
one of the Genia project. 
43
2.1 Project organization 
The Caderige project is a multi disciplinary 
French research project on the automatic 
mining of textual data from the biomedical 
domain and is mainly exploratory orientated. It 
involved biology teams (INRA), computer 
science teams (LIPN, INRA and Leibniz-
IMAG) and NLP teams (LIPN) as major 
partners, plus LRI and INRIA from 2000 to 
2003. 
2.2 Project motivations 
Biologists can search bibliographic databases 
via the Internet, using keyword queries that 
retrieve a large superset of relevant papers. 
Alternatively, they can navigate through 
hyperlinks between genome databanks and 
referenced papers. To extract the requisite 
knowledge from the retrieved papers, they 
must identify the relevant abstracts or 
paragraphs. Such manual processing is time 
consuming and repetitive, because of the 
bibliography size, the relevant data sparseness, 
and the database continuous updating. From 
the Medline database, the focused query 
?Bacillus subtilis and transcription? which 
returned 2,209 abstracts in 2002, retrieves 
2,693 of them today. We chose this example 
because Bacillus subtilis is a model bacterium 
and transcription is a central phenomenon in 
functional genomics involved in genic 
interaction, a popular IE problem. 
GerE stimulates cotD transcription and 
inhibits cotA transcription in vitro by 
sigma K RNA polymerase, as expected from 
in vivo studies, and, unexpectedly, 
profoundly inhibits in vitro 
transcription of the gene (sigK) that 
encode sigma K. 
Figure 1: A sentence describing a genic interaction 
Once relevant abstracts have been retrieved, 
templates should be filled by hand since there 
is no available IE tool operational in genomics  
Type: positive 
Agent: GerE 
 
Interaction 
Target: transcription of the 
gene sigK 
Figure 2: A template describing a genic 
interaction. 
Still, applying IE ? la MUC to genomics and 
more generally to biology is not an easy task 
because IE systems require deep analysis 
methods to locate relevant fragments. As 
shown in the example in Figures 1 and 2, 
retrieving that GerE is the agent of the 
inhibition of the transcription of the gene sigK 
requires at least syntactic dependency analysis 
and coordination processing. In most of the 
genomics IE tasks (function, localization, 
homology) the methods should then combine 
the semantic-conceptual analysis of text 
understanding methods with IE through pattern 
matching. 
2.3 Comparison with the Genia project 
Our approach is very close to the one of the 
Genia project (Collier et al, 1999). Both 
projects rely on precise high-level linguistic 
analysis to be able to perform IE. The kind of 
information being searched is similar, 
concerning mainly gene and protein interaction 
as most of the research in this domain. The 
Genia corpus (Ohtae et al 2001) is not 
specialized on a specific species whereas ours 
is based on Bacillus Subtilis.  
Both projects develop annotation tools and 
Document Type Definition (DTD), which are, 
for the most part, compatible. The aim here is 
to build training corpus to which various 
techniques of NLP and ML are applied in 
order to acquire efficient event-based 
extraction patterns. The choice of ML and 
NLP methods differs but their aim is similar to 
our: normalizing text with predicate-arguments 
structures for learning better patterns. For 
example, Genia uses a combination of parsers 
to finally perform an HPSG-like analysis. The 
Caderige syntactic analysis is based on the 
specialization of the Link Parser (Sleator and 
Temperley, 1993 see section 4) to the 
biological domain.  
 In the following two sections, we detail our 
text filtering and normalization methods. 
Filtering aims at pruning the irrelevant part of 
the corpus while normalization aims at 
building an abstract representation of the 
relevant text. Section 4 is devoted to the 
acquisition of extraction patterns from the 
filtered and normalized text. 
3 Text filtering 
IR and text filtering are a prerequisite step to 
IE, as IE methods (including normalization and 
learning) cannot be applied to large and 
irrelevant corpora (they are not robust enough 
and they are computationally expensive). IR 
here is done through Medline interface by 
keyword queries for filtering the appropriate 
44
document subset. Then, text filtering, reduces 
the variability of textual data with the 
following assumptions: 
? desired information is local to sentences ; 
? relevant sentences contain at least two gene 
names. 
These hypotheses may lead to miss some genic 
interactions, but we assume that information 
redundancy is such that at least one instance of 
each interaction is contained into a single 
sentence in the corpus. The documents 
retrieved are thus segmented into sentences 
and the sentences with at least two gene names 
are selected. 
To identify the only relevant sentences among 
thoses,  classical supervised ML methods have 
been applied to a Bacillus Subtilis corpus in 
which relevant and irrelevant sentences had 
been annotated by a biological expert. Among 
SVMs, Na?ve Bayes (NB) methods, Neural 
Networks, decision trees (Marcotte et al, 
2001;  Nedellec et al, 2001), (Nedellec et al 
2001) demonstrates that  simple NB methods 
coupled with feature selection seem to perform 
well by yielding around 85 % precision and 
recall. Moreover, our first experiments show 
that the linguistic-based representation changes 
such as the use of lemmatization, terminology 
and named entities, do not lead to significant 
improvements. The relevant sentences filtered 
at this step are then used as input of the next 
tasks, normalization and IE. 
4 Normalization 
This section briefly presents three text 
normalization tasks: normalization of entity 
names, normalization of relations between text 
elements through syntactic dependency parsing 
and semantic labeling. The normalization 
process, by providing an abstract 
representation of the sentences, allows the 
identification of regularities that simplify the 
acquisition or learning of pattern rules. 
4.1 Entity names normalization 
Named Entity recognition is a critical point in 
biological text analysis, and a lot of work was 
previously done to detect gene names in text 
(Proux and al., 1998), (Fukuda and al., 1998). 
So, in Caderige, we do not develop any 
original NE extraction tool. We focus on a less 
studied problem that is synonyms recognition.  
Beyond typographical variations and 
abbreviations, biological entities often have 
several different names. Synonymy of gene 
names is a well-known problem, partly due to 
the huge amount of data manipulated (43.238 
references registered in Flybase for 
Drosophilia Melanogaster for example). Genes 
are often given a temporary name by a 
biologist. This name is then changed according 
to information on the concerned gene: for 
example SYGP-ORF50 is a gene name 
temporarily attributed by a sequencing project 
to the PMD1 yeast gene. We have shown that, 
in addition to available data in genomic 
database (GenBank, SwissProt,?), it is 
possible to acquire many synonymy relations 
with good precision through text analysis. By 
focusing on synonymy trigger phrases such as 
"also called" or "formerly", we can extract text 
fragments of that type :  gene trigger gene. 
However, the triggers themselves are subject to 
variation and the arguments of the synonymy 
relation must be precisely identified. We have 
shown that it is possible to define patterns to 
recognize synonymy expressions. These 
patterns have been trained on a representative 
set of sentences from Medline and then tested 
on a new corpus made of 106 sentences 
containing the keyword formerly. Results on 
the test corpus are the following: 97.5% 
precision, 75% recall. We chose to have a high 
precision since the acquired information must 
be valid for further acquisition steps 
(Weissenbacher, 2004).  
The approach that has been developed is very 
modular since abstract patterns like gene 
trigger gene (the trigger being a linguistic 
marker or a simple punctuation) can be 
instantiated by various linguistic items. A 
score can be computed for each instantiation of 
the pattern, during a learning phase on a large 
representative corpus. The use of a reduced 
tagged corpus and of a large untagged corpus 
justify the use of semi-supervised learning 
techniques.  
4.2  Sentence parsing 
The extraction of structured information from 
texts requires precise sentence parsing tools 
that exhibit relevant relation between domain 
entities. Contrary to (Akane et al 2001), we 
chose a partial parsing approach: the analysis 
is focused on relevant parts of texts and, from 
these chunks, on specific relations. Several 
reasons motivate this choice: among others, the 
fact that relevant information generally appears 
in predefined syntactic patterns and, moreover, 
45
the fact that we want to learn domain 
knowledge ontologies from specific syntactic 
relations (Faure and Nedellec, 2000 ; Bisson et 
al. 2000). 
First experiments have been done on several 
shallow parsers. It appeared that constituent 
based parsers are efficient to segment the text 
in syntactic phrases but fail to extract relevant 
functional relationships betweens phrases. 
Dependency grammars are more adequate 
since they try to establish links between heads 
of syntactic phrases. In addition, as described 
in Schneider (1998), dependency grammars are 
looser on word order, which is an advantage 
when working on  a domain specific language.  
Two dependency-based syntactic parsers have 
been tested (Aubin 2003): a hybrid commercial 
parser (henceforth HCP) that combines 
constituent and dependency analysis, and a 
pure dependency analyzer: the Link Parser.   
Prasad and Sarkar (2000) promote a twofold 
evaluation for parsers: on the one hand the use 
of a representative corpus and, on the other 
hand, the use of specific manually elaborated 
sentences. The idea is to evaluate analyzers on 
real data (corpus evaluation) and then to check 
the performance on specific syntactic 
phenomena. In this experiment, we chose to 
have only one corpus, made of sentences 
selected from the Medline corpus depending 
on their syntactic particularity. This strategy 
ensures representative results on real data. 
A set of syntactic relations was then selected 
and manually evaluated. This led to the results 
presented for major relations only in table 1. 
For each analyzer and relation, we compute a 
recall and precision score (recall = # relevant 
found relations / # relations to be found; 
precision = # relevant found relations / # 
relations found by the system).  
The Link Parser generally obtains better results 
than HCP. One reason is that a major 
particularity of our corpus (Medline abstracts) 
is that sentences are often (very) long (27 
words on average) and contain several clauses. 
The dependency analyzer is more accurate to 
identify relevant relationships between 
headwords whereas the constituent parser is 
lost in the sentence complexity. We finally 
opted for the Link Parser. Another advantage 
of the Link Parser is the possibility to modify 
its set of rules (see next subsection). The Link 
parser is currently used in INRA to extract 
syntactic relationships from texts in order to 
learn domain ontologies on the basis of a 
distributional analysis (Harris 1951, Faure and 
N?dellec, 1999).  
4.3 Recycling a general parser for biology 
During the evaluation tests, we noticed that 
some changes had to be applied either to the 
parser or to the text itself to improve the 
syntactic analysis of our biomedical corpus. 
The corpus needs to be preprocessed: sentence 
segmentation, named entities and terms 
recognition are thus performed using generic 
modules tuned for the biology domain
1
. Term 
recognition allows the removing of numerous 
structure ambiguities, which clearly benefits 
the parsing quality and execution time.  
                                                     
1
 A term analyser is currently being built at LIPN 
using existing term resources like Gene Ontology 
(see Hamon and Aubin, 2004). 
  Link Parser HCP 
Rel nbRel relOK R. RelTot P. RelOK R RelTot P. 
Subject 
18 13 0.72 19 0.68 14 0.78 20 0.65 
Object 
18 16 0.89 17 0.94 9 0.5 13 0.69 
Prep 
48 25 0.52 55 0.45 20 0.42 49 0.41 
V-GP1 
14 13 0.93 15 0.87 9 0.64 23 0.39 
O-GP 
16 7 0.43 12 0.58 12 0.75 28 0.43 
NofN 
16 13 0.81 15 0.87 14 0.87 26 0.54 
VtoV 
10 9 0.9 9 1 7 0.7 7 1 
VcooV 
10 8 0.8 9 0.89 6 0.6 6 1 
NcooN 
10 8 0.7 10 0.8 4 0.4 6 0.67 
nV-Adj 
10 8 0.8 9 0.89 0 0 0 1 
PaSim 
18 17 0.94 18 0.94 17 0.94 22 0.77 
PaRel 
12 11 0.92 11 1 8 0.67 11 0.73 
Table 1: Evaluation of two parsers on various syntactic relations 
Relations meaning: subject = subject-verb, Object = verb-object, Prep = prepositional phrase, V-GP = verb-prep. 
phrase, O-GP = Object- prep. phrase, NofN = Noun of noun, VtoV = Verb to Verb, VcooV = Verb coord. Verb, 
NcooN = Noun coord. Noun, nV-Adj = not + Verb or adjective, PaSim = passive form, PaRel = passive relative 
46
Concerning the Link Parser, we have manually 
introduced new rules and lexicon to allow the 
parsing of syntactic structures specific to the 
domain. For instance, the Latin-derived Noun 
Adjective phrase "Bacillus subtilis" has a 
structure inverse to the canonical English noun 
phrase (Adjective Noun). Another major task 
was to loosen the rules constraints because 
Medline abstracts are written by biologists 
who express themselves in sometimes broken 
English. A typical error is the omission of the 
determinant before some nouns that require 
one. We finally added words unknown to the 
original parser. 
4.4 Semantic labelling 
Asium software is used to semi-automatically 
acquire relevant semantic categories by 
distributional semantic analysis of parsed 
corpus. These categories contribute to text 
normalization at two levels, disambiguating 
syntactic parsing and typing entities and 
actions for IE. Asium is based on an original 
ascendant hierarchical clustering method that 
builds a hierarchy of semantic classes from the 
syntactic dependencies parsed in the training 
corpus. Manual validation is required in order 
to distinguish between different meanings 
expressed by identical syntactic structures. 
5 Extraction pattern learning 
Extraction pattern learning requires a training 
corpus from which the relevant and 
discriminant regularities can be automatically 
identified. This relies on two processes: text 
normalization that is domain-oriented but not 
task-oriented (as described in previous 
sections), and task-oriented annotation by the 
expert of the task.  
5.1 Annotation procedure 
The Caderige annotation language is based on 
XML and a specific DTD (Document Type 
Definition that can be used to annotate both 
prokaryote and eukaryote organisms by 50 
tags with up to 8 attributes. Such a precision is 
required for learning feasibility and extraction 
efficiency. Practically, each annotation aims at 
highlighting the set of words in the sentence 
describing: 
? Agents (A): the entities activating or 
controlling the interaction 
? Targets (T): the entities that are produced 
or controlled 
? Interaction (I): the kind of control 
performed during the interaction 
? Confidence (C): the confidence level in this 
interaction. 
The annotation of ?A low level of GerE 
activated transcription of CotD by GerE RNA 
polymerase in vitro ...? is given below. The 
attributes associated to the tag <GENIC-
INTERACTION> express the fact that the 
interaction is a transcriptional activation and 
that it is certain. The other tags (<IF>, 
<AF1>, ?) mark  the agent (AF1 and AF2), the 
target (TF1) and the interaction (IF). 
 
<GENIC-INTERACTION 
 id=?1?  
 type=?transcriptional?  
 assertion=?exist?  
 regulation=?activate?  
 uncertainty=?certain?  
 self-contained=?yes?  
 text-clarity=?good?> 
  <IF>A<I> low level </I>of</IF>     
  <AF1><A1  
     type=protein  
        role=modulate  
        direct=yes> GerE 
  </A1></AF1>,  
  <IF><I>activated</I> transcription  
      of</IF>    
     <TF1><T1 type=protein> CotD </T1>           
         </TF1> by   
     <AF2><A2  
           type=protein  
         role=required> 
       GerE RNA polymerase 
   </A2></AF2>,  
   <CF>but<C>in vitro</C></CF> 
</GENIC-INTERACTION> 
5.2 The annotation editor2 
Annotations cannot be processed in text form 
by biologists. The annotation framework 
developed by Caderige provide a general XML 
editor with a graphic interface for creating, 
checking and revising annotated documents. 
For instance, it displays the text with graphic 
attributes as defined in the editor XML style 
sheet, it allows to add the tags without strong 
constraint on the insertion order and it 
automatically performs some checking. 
The editor interface is composed of four main 
parts (see Figure 3). The editable text zone for 
annotation, the list of XML tags that can be 
used at a given time, the attributes zone to edit 
the values of the selected tag, and the XML 
                                                     
2
 Contact one of the authors if you are interested to 
use this annotation tool in a research project 
47
code currently generated. In the text zone, the 
above sentence is displayed as follows: 
A low level of GerE activated 
transcription of CotD by GerE RNA 
polymerase but in vitro 
This editor is currently used by some of the 
Caderige project partners and at SIB (Swiss 
Institute of BioInformatics) with another DTD, 
in the framework of the European BioMint 
project. Several corpora on various species 
have been annotated using this tool, mainly by 
biologists from INRA.  
5.3 Learning 
The vast majority of approaches relies on 
hand-written pattern rules that are based on 
shallow representations of the sentences (e.g. 
Ono et al, 2001). In Caderige, the deep 
analysis methods increase the complexity of 
the sentence representation, and thus of the IE 
patterns. ML techniques appear therefore very 
appealing to automate the process of rule 
acquisition (Freitag, 1998; Califf et al, 1998; 
Craven et al, 1999).  
Learning IE rules is seen as a discrimination  
task, where the concept to learn is a n-ary 
relation between arguments which correspond 
to the template fields. For example, the 
template in figure 2 can be filled by learning a 
ternary relation genic-interaction(X,Y,Z), 
where X,Y and Z are the type, the agent and 
the target of the interaction. The learning 
algorithm is provided with a set of positive and 
negative examples built from the sentences 
annotated and normalized. We use the 
relational learning algorithm, Propal (Alphonse 
et al, 2000). The appeal of using a relational 
method for this task is that it can naturally 
represent the relational structure of the 
syntactic dependencies in the normalized 
sentences and the background knowledge if 
needed, such as for instance semantic relations.  
For instance, the IE rules learned by Propal 
extract, from the following sentence :"In this 
mutant, expression of the spoIIG gene, whose 
transcription depends on both sigA and the 
phosphorylated Spo0A protein, Spo0AP, a 
major transcription factor during early stages 
of sporulation, was greatly reduced at 43 
degrees C.", successfully extract the two 
relations genic-interaction(positive, sigA, 
spoIIG) and genic-interaction(positive, 
Spo0AP, spoIIG). As preliminary experiments, 
we selected a subset of sentences as learning 
dataset, similar to this one. The performance of 
the learner evaluated by ten-fold cross-
validation is 69?6.5% of recall and 86?3.2% 
of precision. This result is encouraging, 
showing that the normalization process 
provides a good representation for learning IE 
rules with both high recall and high precision. 
6 Conclusion 
We have presented in this paper some results 
from the Caderige project. Two major issues 
are the development of a specific annotation 
editor for domain specialists and a set of 
machine learning and linguistic processing 
tools tuned for the biomedical domain.  
Current developments focus on the use of 
learning methods in the extraction process. 
These methods are introduced at different 
levels in the system architecture. A first use is 
Figure 3: the Caderige annotation editor 
48
the acquisition of domain knowledge to 
enhance the extraction phase. A second use 
concerns a dynamic adaptation of existing 
modules during the analysis according to 
specific features in a text or to specific text 
genres.  
7 References 
E. Agichtein and H. Yu (2003). Extracting 
synonymous gene and protein terms from 
biological literature. Bioinformatics, vol. 19 
Suppl.1, Oxford Press. 
E. Alphonse and C. Rouveirol (2000). Lazy 
propositionalisation for Relational  
Learning. In 14th European Conference on 
Artificial Intelligence (ECAI?00, W. Horn ed.), 
Berlin, pp. 256-260.  
S. Aubin (2003). ?valuation comparative de deux 
analyseurs produisant des relations syntaxiques. 
In workshop TALN and multilinguism. Batz-sur-
Mer. 
Y. Akane, Y. Tateisi, Y. Miyao and J. Tsujii. 
(2001). Event extraction from biomedical papers 
using a full parser. In Proceedings of the sixth 
Pacific Symposium on Biocomputing (PSB 2001). 
Hawaii, U.S.A.. pp. 408-419.  
G. Bisson, C. Nedellec, L. Ca?amero 2000. 
Designing clustering methods for ontology 
building: The Mo?K workbench. In Proceedings 
of Ontology Learning workshop (ECAI 2000), 
Berlin, 22 ao?t 2000.  
M. E. Califf, 1998. Relational Learning Techniques 
for Natural Language Extraction. Ph.D. 
Disseration, Computer Science Department, 
University of Texas, Austin, TX. AI Technical 
Report 98-276. 
N. Collier, Hyun Seok Park, Norihiro Ogata, Yuka 
Tateisi, Chikashi Nobata, Takeshi Sekimizu, 
Hisao Imai and Jun'ichi Tsujii. (1999). The 
GENIA project: corpus-based knowledge 
acquisition and information extraction from 
genome research papers. In Proceedings of the 
European Association for Computational 
Linguistics (EACL 1999). 
M. Craven et al, 1999. Constructing Biological 
Knowledge Bases by Extracting Information 
from Text Sources. ISMB 1999: 77-86 
D. Faure and C. Nedellec (1999). Knowledge 
acquisition of predicate argument structures from 
technical texts using Machine Learning: the 
system ASIUM. In EKAW'99, pp. 329-334, 
Springer-Verlag.  
D. Freitag, 1998, Multistrategy learning for 
information extraction. In Proceedings of the 
Fifteenth International Conference on Machine 
Learning, 161-169. Madison, WI: Morgan 
Kaufmann 
T. Hamon and S. Aubin (2004). Evaluating 
terminological resource coverage for relevant 
sentence selection and semantic class building. 
LIPN internal report. 
K. Fukuda, T. Tsunoda, A. Tamura, T. Takagi 
(1998). Toward information extraction : 
identifying protein names from biological papers. 
Proceedings of the Pacific Symposium of 
Biocomputing, pp. 707-718. 
Z. Harris (1951). Methods in Structural Linguistics. 
Chicago. University of Chicago Press.  
E.M. Marcotte, I. Xenarios I., and D. Eisenberg 
(2001). Mining litterature for protein-protein 
interactions. In Bioinformatics, vo. 17 n? 4, 
pp. 359-363. 
MUC (1995) Proceeding of the 6
th
 Message 
understanding Conference. Morgan Kaufmann. 
Palo Alto.  
C. N?dellec, M. Ould Abdel Vetah and P. Bessi?res 
(2001). Sentence Filtering for Information 
Extraction in Genomics: A Classification 
Problem. In Proceedings of the International 
Conference on Practical Knowledge Discovery in 
Databases (PKDD?2001), pp. 326?338. Springer 
Verlag, LNAI 2167, Freiburg. 
T. Ohta, Yuka Tateisi, Jin-Dong Kim, Hideki Mima 
and Jun'ichi Tsujii. (2001). Ontology Based 
Corpus Annotation and Tools. In Proceedings of 
the 12th Genome Informatics 2001. pp. 469--470. 
T. Ono, H. Hishigaki, A. Tanigami and T. Takagi 
(2001). Automated extraction of information on 
protein-protein interactions from the biological  
literature. Bioinformatics. vol 17, n? 2, pp. 155-
161, Oxford Press. 
B. Prasad and A. Sarkar (2000) Comparing Test-
suite based evaluation and Corpus-based 
evaluation of a wide-coverage grammar for 
English. In Using Evaluation within Human 
Language Technology. LREC. Athens.  
D. Proux, F. Rechenmann, L. Julliard, V. Pillet, B. 
Jacq (1998). Detecting gene symbols and names 
in biological texts : a first step toward pertinent 
information extraction. In Genome Informatics, 
vol. 9, pp. 72-80. 
G. Schneider (1998). A Linguistic Comparison of 
Constituency, Dependency and Link Grammar. 
PhD thesis, Institut f?r Informatik der Universit?t 
Z?rich, Switzerland. 
D. Sleator and D. Temperley (1993). Parsing 
English with a Link Grammar. In Third 
International Workshop on Parsing 
Technologies. Tilburg. Netherlands. 
D. Weissenbacher (2004). La relation de 
synonymie en g?nomique. In Recital conference. 
Fes. 
49
Proceedings of the Workshop on BioNLP, pages 89?96,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring graph structure for detection of reliability zones within synonym
resources: Experiment with the Gene Ontology
Thierry Hamon
LIPN ? UMR 7030
Universite? Paris 13 ? CNRS
99 av. J-B Cle?ment
F-93430 Villetaneuse, France
thierry.hamon@lipn.univ-paris13.fr
Natalia Grabar
Centre de Recherche des Cordeliers
Universite? Paris Descartes, UMR S 872
INSERM, U872
HEGP AP-HP, 20 rue Leblanc
Paris, France
natalia.grabar@spim.jussieu.fr
Abstract
Computing the semantic similarity between
terms relies on existence and usage of seman-
tic resources. However, these resources, often
composed of equivalent units, or synonyms,
must be first analyzed and weighted in or-
der to define within them the reliability zones
where the semantic cohesiveness is stronger.
We propose an original method for acquisition
of elementary synonyms based on exploitation
of structured terminologies, analysis of syn-
tactic structure of complex (multi-unit) terms
and their compositionality. The acquired syn-
onyms are then profiled thanks to endogenous
lexical and linguistic indicators (other types
of relations, lexical inclusions, productivity),
which are automatically inferred within the
same terminologies. Additionally, synonymy
relations are observed within graph, and its
structure is analyzed. Particularly, we ex-
plore the usefulness of the graph theory no-
tions such as connected component, clique,
density, bridge, articulation vertex, and cen-
trality of vertices.
1 Introduction
In various tasks and applications of natural language
processing and of biomedical informatics (i.e., query
expansions, information retrieval, text mining, infor-
mation extraction or terminology matching), it is im-
portant to be able to decide whether two terms (i.e.,
acetone anabolism and acetone biosynthesis, repli-
cation of mitochondrial DNA and mtDNA replica-
tion) convey the same or different meaning. This is
particularly important for deciphering and comput-
ing semantic similarity between words and terms.
Lexicon of specific resources (synonym, morpho-
logical or orthographic variants) can be used for de-
tection of semantic similarity. However, depend-
ing on languages and domains, such resources are
not equally well described. Morphological descrip-
tion is the most complete for both general (Bur-
nage, 1990; Hathout et al, 2001) and biomedical
(NLM, 2007; Schulz et al, 1999; Zweigenbaum
et al, 2003) languages. But the situation is not as
successful at the semantic level: little synonym re-
sources can be found. If WordNet (Fellbaum, 1998)
proposes general language synonym relations for
English, the corresponding resources for other lan-
guages are not freely available. Moreover, the ini-
tiative for fitting WordNet to the biomedical area
(Smith and Fellbaum, 2004) seems to have been
abandoned, although there is a huge need for this
kind of resources.
In our previous work, we proposed to use the ex-
isting biomedical terminologies (i.e., Gene Ontology
(Gene Ontology Consortium, 2001), Snomed (Co?te?
et al, 1997), UMLS (NLM, 2007)), wich provide
complex terms, and to acquire from them lexical re-
sources of synonyms. Indeed, the use of complex
biomedical terms seems to be less suitable and gen-
eralizable as compared to lexical resources (Poprat
et al, 2008). Within the biological area, we pro-
posed to exploit the Gene Ontology (GO), and more
specifically to exploit compositional structure of its
terms (Hamon and Grabar, 2008). However, with
the acquisition of synonymy we faced two prob-
lems: (1) contextual character of these relations
(Cruse, 1986), i.e., two terms or words are con-
sidered as synonyms if they can occur within the
89
same context, which makes this relation more or
less broad depending on the usage; (2) ability of
automatic tools to detect and characterize these re-
lations, i.e., two terms or words taken out of their
context can convey different relations than the one
expected. Because we aim at acquiring synonymy
resources which could be used by various applica-
tions and on various corpora, we need to profile them
and possibly to detect the reliability zones. We pro-
posed to do this profiling through lexical and lin-
guistic indicators generated within the same termi-
nology (Grabar et al, 2008), such as productivity,
cooccurence with other types of relations (is-a,
part-of) and with lexical inclusion. These indi-
cators on reliability zones will be used for defining
the synonymity degree of terms and for preparing
the validation of the acquired synonym resources. In
the current work, we continue profiling the acquired
synonyms, but rely on the form of the graph built
from pairs of synonyms. We exploit for this some
notions of the graph theory (Diestel, 2005). In the
following of this paper, we first present our mate-
rial (sec. 2) and methods (sec. 3), we then present
and discuss results (sec. 4) and conclude with some
perspectives (sec. 5).
2 Material
We use the Gene Ontology (GO) as the original re-
source from which synonym lexicon (or elementary
synonym relations) are induced. The goal of the GO
is to produce a structured, common, controlled vo-
cabulary for describing the roles of genes and their
products in any organism. GO terms convey three
types of biological meanings: biological processes,
molecular functions and cellular components. Terms
are structured through four types of relationships:
subsumption is-a, meronymy part-of, syn-
onymy and regulates. The version, we used
in the current work, was downloaded in February
20081. It provides 26,057 concepts and their 79,994
terms. When we create pairs of terms, which we ex-
ploit with our methods, we obtain 260,399 is-a,
29,573 part-of and 459,834 synonymy relations.
There are very few regulates relations, therefore
we don?t exploit them in our work.
1Our previous work has been performed with an anterior
version of the GO.
3 Methods
GO terms present compositional structure, like
within the concept GO:0009073, where composi-
tionality can be observed through the substitution of
one of the components (underlined):
aromatic amino acid family biosynthesis
aromatic amino acid family anabolism
aromatic amino acid family formation
aromatic amino acid family synthesis
Compositionality of the GO terms has been ex-
ploited previously, for instance (Verspoor et al,
2003) propose to derive simple graphs from relations
between complex GO terms, (Mungall, 2004) ex-
ploits the compositionality as a mean for consistency
checking of the GO, (Ogren et al, 2005) use it for
enriching the GO with missing synonym terms. We
propose to exploit the compositionality for induction
of synonym lexical resources (i.e., biosynthesis, an-
abolism, formation, synthesis in the given example).
While the cited works are based on the string match-
ing within GO terms, our approach aims at exploit-
ing the syntactic analysis of terms, which makes it
independent from the graphical form of the analyzed
terms (like examples on fig. 1). Our method has sev-
eral steps: linguistic preprocessing of the GO terms
(sec. 3.1), induction of elementary semantic lexi-
con (sec. 3.2), and then the profiling the synonymy
lexicon through the lexical and linguistic indicators
(sec. 3.3), and through the analysis of connected
components built from the induced synonym pairs
(sec. 3.4). Steps 3.1 to 3.3 have been already de-
scribed in our previous work: we mention here the
main notions for the sake of clarity.
3.1 Preprocessing the GO terms: Ogmios NLP
platform
The aim of terminology preprocessing step is to
provide syntactic analysis of terms for computing
their syntactic dependency relations. We use the
Ogmios platform2 and perform: segmentation into
words and sentences; POS-tagging and lemmatiza-
tion (Schmid, 1994); and syntactic analysis3. Syn-
tactic dependencies between term components are
2http://search.cpan.org/?thhamon/Alvis-NLPPlatform/
3http://search.cpan.org/?thhamon/Lingua-YaTeA/
90
component
expansion headcomponent
replicationmtDNA
component
expansionhead
component
mitochondrial DNAreplication (of)
Figure 1: Parsing tree of the terms replication of mitochondrial DNA and mtDNA replication.
computed according to assigned POS tags and shal-
low parsing rules. Each term is considered as a syn-
tactic binary tree composed of two elements: head
component and expansion component. For instance,
replication is the head component of the two terms
analyzed on figure 1.
3.2 Acquiring the elementary semantic
relations
The notion of compositionality assumes that the
meaning of a complex expression is fully deter-
mined by its syntactic structure, the meaning of its
parts and the composition function (Partee, 1984).
On the basis of syntactically analysed terms, we ap-
ply a set of compositional rules: if the meaning M
of two complex terms A rel B and A? rel B, where
A is its head and B its expansion components, is
given as following:
M(A rel B) = f(M(A),M(B),M(rel))
M(A? rel B) = f(M(A?),M(B),M(rel))
for a given composition function f , if A rel B and
A? rel B are complex synonym terms and if B com-
ponents are identical (such as acetone within ace-
tone catabolism and acetone breakdown), then the
synonymy relation between components A and A?
{catabolism, breakdown} can be induced. The mod-
ification is also accepted on expansion component
B: from terms replication of mitochondrial DNA
and mtDNA replication (fig. 1), we can induce syn-
onymy between mitochondrial DNA and mtDNA.
Finally, the modification is also accepted for both
components A rel B and A? rel B?, such as in
nicotinamide adenine dinucleotide catabolism and
NAD breakdown, where one pair, i.e. {catabolism,
breakdown}, can be known from previously pro-
cessed synonyms and allow to induce the new pair
{nicotinamide adenine dinucleotide, NAD}. The
method is recursive and each induced elementary
synonym relation can then be propagated in order
to induce new elementary relations, which allows to
generate a more exhaustive lexicon of synonyms.
This method is not specific to the synonymy. As
it works at the syntactic level of terms, it there-
fore can be applied to other relationships: relation-
ship between elementary terms is inherited from
the relationship between complex terms. If we ex-
ploit complex terms related with part-of rela-
tions and if the compositionality rules can be ap-
plied, then we can induce elementary part-of re-
lations. For instance, complex terms cerebral cor-
tex development GO:0021987 and cerebral cortex
regionalization GO:0021796 have a part-of re-
lation between them, and we can induce the elemen-
tary part-of relation between their components
development and regionalization. Similarly, on the
basis of two GO terms that have is-a relation be-
tween them, cell activation GO:0001775 and astro-
cyte activation GO:0048143, we can induce the ele-
mentary is-a relation between cell and astrocyte.
3.3 Exploiting lexical and linguistic indicators
Several endogenously generated indicators are used
for profiling the induced lexicon of synonyms:
? Elementary is-a relations;
? Elementary part-of relations;
? Lexical inclusion: terms within each induced
synonymy pair are controlled for the lexical in-
clusion. If the test is positive, like in {DNA
binding, binding}, this would suggest that the
analyzed terms may convey a hierarchical rela-
tion: indeed, lexical subsumption marks often a
hierarchical subsumption (Kleiber and Tamba,
1990), which can be either is-a or part-of
relations;
? Productivity: number of originalGO pairs from
which this elementary relation is inferred. For
instance, synonymy relations {binding, DNA
91
(a) Connected component of synonyms (b) Clique of synonyms
Figure 2: Connected components formed with pairs of elementary synonym relations.
binding} and {cell, lymphocyte} are inferred
from only one original pair of GO synonyms,
while the pair {T-cell, T-lymphocyte} is sup-
ported by eight original GO synonym pairs.
Factors that would weaken synonymy relations and
make them less reliable are their co-occurrence with
lexical inclusions, is-a or part-of relations, and
their low productivity.
3.4 Exploiting the graph theory notions
Pairs of induced synonyms are observed through the
connected components they form: lexical entries are
nodes or vertices and relations between them are
edges or paths. For instance, connected component
2(a) contains four pairs of synonyms: {membrane
lumen, envelope lumen}, {membrane lumen, in-
termembrane space}, {envelope lumen, intermem-
brane space} and {intermembrane space, IMS}. On
each edge, we projected information associated with
the relation corresponding to this edge. For instance,
{membrane lumen, intermembrane space} relation
is labelled as synonymy SY N and shows 2 as pro-
ductivity value (it has been acquired from two origi-
nal pairs of synonyms within GO). If other relation-
ships (INCL, PAR, HIER) are associated to a
given synonymy relation, they are also indicated to-
gether with their productivity.
As a matter of fact, figure 2 presents two typical
examples of connected components we can obtain
(in these examples, both of them have four nodes):
? Connected component (fig. 2(a)) is a graph in
which any two vertices are connected to each
other by edges. Connected components have
not orphan vertices, which would remain not
connected to any other vertex.
? Clique, also called block (fig. 2(b)) is a par-
ticular case of connected components: clique
is a maximally connected component. In such
graphs, all the vertices are interconnected be-
tween them.
We propose to exploit four more notions of the graph
theory, which we assume can be useful for further
profiling of the acquired synonymy relations:
? Density of a connected component is the ra-
tio between the number of its edges and the
number of edges of the corresponding clique.
For instance, the connected component on fig-
ure 2(a) has 4 edges while the corresponding
clique would have 6 edges. In that respect,
this connected component has the dentisty of
0.67. Besides, the clique on figure 2(b) shows
the maximum density (i.e., 1). (For all the fig-
92
ures, we indicate their density, together with the
number of vertices and edges).
? Bridge is defined as an edge which re-
moval would increase the number of con-
nected components. For instance, within con-
nected component 2(a), removing the edge
{intermembrane space, IMS} would lead to the
creation of two new connected components:
(1) single-vertex component IMS, and (2) con-
nected component with three vertices inter-
membrane space, membrane lumen and enve-
lope lumen. Consequently articulation vertices
are defined as vertices which removal would in-
crease the number of connected components.
At figure 2(a), the articulation vertex is inter-
membrane space.
? The centrality of a vertex is defined as the num-
ber of shortest paths passing through it. For in-
stance, on figure 2(a), intermembrane space?s
centrality is 4, while the centrality of other ver-
tices is null.
4 Results and Discussion
4.1 Acquiring the elementary synonymy
relations and their lexical and linguistic
profiling
79 994 GO terms have been fully analyzed through
the Ogmios platform. Compositional rules (sec. 3.2)
have been applied and allowed to induce 9,085 se-
mantic relations among which: 3,019 synonyms,
3,243 is-a and 1,205 part-of. 876 lexical in-
clusions have discovered within all these elementary
pairs. 2,533 synonymy pairs are free of the lexical
profiling indicators. However, 486 synonymy rela-
tions (16%) cooccur with other relations, and the de-
tails of this cooccurrence is showed in table 1. We
can observe for instance that 142 synonym pairs are
also labelled as is-a relations, and 34 as part-of
relations. Productivity of the induced synonyms is
between 1 and 422 original complex GO terms.
Connected component on figure 3 illustrates
coocurrence of synonymy relations with other types
of relations: the pair {import, ion import} shows
synonym and inclusion relations; the pair {import,
uptake} shows synonym and hierarchical relations,
both acquired on seven original pairs of GO terms.
Figure 3: Connected component where synonymy rela-
tions cooccur with other relations.
Synonymy and other relations Number
syno ? is-a 142
syno ? par 34
syno ? incl 309
syno ? par ? is-a 14
syno ? incl ? is-a \ par 40
syno ? incl ? par \ is-a 2
syno ? incl ? is-a ? par 1
Table 1: Number of synonymy relations which cooccur
with other relations (is-a, part-of and lexical inclu-
sions incl).
4.2 Analysing the induced synonym pairs
through the graph theory
3,019 induced synonym pairs have been grouped
into 1,018 connected components. These compo-
nents contain 2 to 69 nodes, related among them
by 1 to 132 edges. Analyses of the connected
components have been performed with Perl pack-
age Graph and additionnal Perl scripts. Among
the studied connected components, we have 914
cliques composed of 2 (n=708), 3 (n=66), 4 (n=88),
5 (n=44) or 6 (n=8) nodes. The remaining 104
connected components are less dense with edges.
The density of the connected components is between
93
Figure 4: Connected component with three bridges: {ion homeostasis, homeostasis}, {homeostasis, regulation} and
{cell cycle control, regulation}.
0.0467 and 1 (in case of cliques). Among the 104
connected components, which are not cliques, we
detected 249 bridges: 0 to 35 depending on con-
nected components. In order to propose a general
approach exploiting graph theory notions for syn-
onym profiling we analyse the structure of three rep-
resentative connected components.
Density of the connected component 2(a) is 0.67.
It contains one bridge: {intermembrane space,
IMS}. This edge corresponds to the acronym and its
expanded form, which can cause its contextual char-
acter. Moreover, intermembrane space is the central
node of this connected component.
Connected component 3 (density=0.38) contains
two bridges {uptake, recycling} and {salvage, cy-
cling}, and three articulation vertices uptake, re-
cicling and salvage with the measures of central-
ity 16, 18 and 10 respectively. Indeed, the major-
ity of shortest paths pass by uptake and recicling
nodes. Otherwise, edges around the salvage ver-
tex are weakened because of the cooccurrence of
synonymy and hierarchical relations. As we have
already noticed, the edge {import, uptake} shows
the cooccurrence of synonymy and hierarchical re-
lations, but its productivity is rather high (seven for
each relation), which stregthens this edge.
Finally, connected component 4 (density=0.33)
contains three bridges {ion homeostasis, homeosta-
sis}, {homeostasis, regulation} and {cell cycle con-
trol, regulation} and three articulation vertices: reg-
ulation, cell cycle control and homeostasis with the
measures of centrality 52, 37 and 16 respectively.
The bridge {ion homeostasis, homeostasis} is weak-
ened by the cooccurrence of synonymy, hierarchi-
cal and lexical inclusion relations. Otherwise, other
edges seem to convey non ambiguous synonymy.
94
From the analyzed examples, we can see that the
graph theory may have several implications on pro-
filing of synonyms. However, these implications
must still be formalized and, possibly, expressed as
a single reliability indicator, alone or combined with
the lexical and linguistic clues.
First, within a connected component, with a given
number of nodes, higher the number of edges, higher
will be its density and closer it will be to a clique
(fig. 2(b)). Consequently, within a clique, the se-
mantic cohesion is more strong. Indeed, in these
cases, terms are far more strongly related between
them. But when the density value decreases the se-
mantic cohesiveness of connected components de-
creases as well. In other words, density is an indi-
cation on the semantic cohesiveness between terms
within connected components. As for bridges, we
assume that they indicate breaking points within
connected components, such as {cell cycle control,
regulation} within figure 4. The weak character
of these points can increased when the synonymy
relation co-occurs with other relationships (is-a,
part-of, lexical inclusion). Consequently, re-
moval of bridges can create connected components
with higher density and therefore with stronger syn-
onymy relations. Finally, the centrality of vertices
measure may be useful for identification of poly-
semic words or terms.
The connected components analysis can also in-
dicate the missing relations. For instance, if a con-
nected component, which is not a clique, has no
bridges but its density is not maximal, this would
indicate that it misses some correct synonymy rela-
tions which can be easily induced.
5 Conclusion and Perspectives
In this paper, we propose an original method for
inducing synonym lexicon from structured termi-
nologies. This method exploits the compositional-
ity principle and three rules based on syntactic de-
pendency analysis of terms. More specifically, we
explore various indicators for profiling the acquired
synonym relations, which is motivated by the fact
that synonymy is a contextual relation and its va-
lidity and universality is not guaranteed. We as-
sume the semantic cohesiveness of synonymy rela-
tions should be qualified and quantified. Thus, we
propose several indicators for profiling the inferred
synonymy relations and for detecting possible weak
and strong points. First, lexical and linguistic clues
are generated endogenously within the same termi-
nology: other types of elementary semantic relations
(is-a and part-of), lexical inclusions and pro-
ductivity of the acquired semantic relations. Then,
more specifically, this work is dedicated to explor-
ing of the usefulness of notions of the graph the-
ory. We propose to study the form and specificities
of connected components formed by synonymy re-
lations. We exploited the following notions from the
graph theory: distinction between connected com-
ponents and cliques, their density, bridges and artic-
ulation vertices within connected components, and
the centrality of their vertices. We observed that the
lexical indicators as well as connected components
characteristics are helpful for profiling the acquired
synonymy relations. These clues are intended to be
used for preparing the validation of this lexicon by
experts and also for its weighting in order to con-
trol and guarantee the specificity of lexicon during
its use by automatic tools.
Currently, we study separately the endogeneous
lexical indicators, and the characteristics of the con-
nected components. However, in the future, these
two types of clues should be combined. For this,
these indicators should be modelized in order to pro-
vide a weight of each edge. This weight can be
used for profiling of connected component through
the detection of strong and weak points. Notice
that the current version of the Graph package can-
not take into account this additional information on
edges and should be modified. Another perspective
is the better exploitation of the Gene Ontology and
taking into account the nature of synonymy relations
as they are labelled by thier creators: exact, broad,
narrow or related. Additionnally, for a more precise
profiling, the four relationships of GO (synonymy,
is-a, part-of and regulates) can be cross-
validated, while currently, we perform the validation
of synonymy relations through is-a and part-of
(and other indicators). We plan also to use the in-
duced relations and propagate them through corpora
and discover some of the missing synonyms (Hole
and Srinivasan, 2000). In this way, applying the
same compositionality principle, we can enrich and
extend the Gene Ontology: new synonyms of GO
95
terms and even other relations between GO terms
and terms from corpora can be detected. As noticed,
this method can be applied to other terminologies
and languages as far as structured terminological re-
sources and NLP tools exist. For instance, within
the context of search of clinical documents, we suc-
cessfully tested this method on the French part of the
UMLS (Grabar et al, 2009). From a more ontolog-
ical perspective, our method can be used for consis-
tency checking of a terminologies, like in (Mungall,
2004). Moreover, as this method performs syntactic
analysis of terms and their decomposition into se-
mantically independent components, it can be used
for the transformation of a pre-coordinated terminol-
ogy into a post-coordinated one.
References
G. Burnage. 1990. CELEX - A Guide for Users. Centre
for Lexical Information, University of Nijmegen.
Roger A. Co?te?, Louise Brochu, and Lyne Cabana, 1997.
SNOMED Internationale ? Re?pertoire d?anatomie
pathologique. Secre?tariat francophone international
de nomenclature me?dicale, Sherbrooke, Que?bec.
David A. Cruse. 1986. Lexical Semantics. Cambridge
University Press, Cambridge.
Reinhard Diestel. 2005. Graph Theory. Springer-Verlag
Heidelberg, New-York.
Christian Fellbaum. 1998. A semantic network of en-
glish: the mother of all WordNets. Computers and Hu-
manities. EuroWordNet: a multilingual database with
lexical semantic network, 32(2-3):209?220.
Gene Ontology Consortium. 2001. Creating the
Gene Ontology resource: design and implementation.
Genome Research, 11:1425?1433.
Natalia Grabar, Marie-Christine Jaulent, and Thierry Ha-
mon. 2008. Combination of endogenous clues for
profiling inferred semantic relations: experiments with
gene ontology. In JAMIA (AMIA 2008), pages 252?6,
Washington, USA.
Natalia Grabar, Paul-Christophe Varoutas, Philippe
Rizand, Alain Livartowski, and Thierry Hamon. 2009.
Automatic acquisition of synonym ressources and as-
sessment of their impact on the enhanced search in
ehrs. Methods of Information in Medicine, 48(2):149?
154. PMID 19283312.
Thierry Hamon and Natalia Grabar. 2008. Acquisition of
elementary synonym relations from biological struc-
tured terminology. In Computational Linguistics and
Intelligent Text Processing (5th International Confer-
ence on NLP, 2006), number 4919 in LNCS, pages 40?
51. Springer.
Nabil Hathout, Fiammetta Namer, and Georgette Dal.
2001. An experimental constructional database: the
MorTAL project. In P. Boucher, editor, Morphology
book. Cascadilla Press, Cambridge, MA.
WT Hole and S Srinivasan. 2000. Discovering missed
synonymy in a large concept-oriented metathesaurus.
In AMIA 2000, pages 354?8.
Georges Kleiber and Ire`ne Tamba. 1990. L?hyperonymie
revisite?e : inclusion et hie?rarchie. Langages, 98:7?
32, juin. L?hyponymie et l?hyperonymie (dir. Marie-
Franc?oise Mortureux).
CJ Mungall. 2004. Obol: integrating language and
meaning in bio-ontologies. Comparative and Func-
tional Genomics, 5(6-7):509?520.
NLM, 2007. UMLS Knowledge Sources Manual. Na-
tional Library of Medicine, Bethesda, Maryland.
www.nlm.nih.gov/research/umls/.
PV Ogren, KB Cohen, and L Hunter. 2005. Implica-
tions of compositionality in the Gene Ontology for its
curation and usage. In Pacific Symposium of Biocom-
puting, pages 174?185.
Barbara H Partee, 1984. Compositionality. F Landman
and F Veltman.
Michael Poprat, Elena Beisswanger, and Udo Hahn.
2008. Building a biowordnet using wordnet data struc-
tures and wordnet?s software infrastructure - a failure
story. In ACL 2008 workshop ?Software Engineering,
Testing, and Quality Assurance for Natural Language
Processing?, pages 31?9.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Stefan Schulz, Martin Romacker, Pius Franz, Albrecht
Zaiss, Rdiger Klar, and Udo Hahn. 1999. Towards a
multilingual morpheme thesaurus for medical free-text
retrieval. In Medical Informatics in Europe (MIE).
Barry Smith and Christian Fellbaum. 2004. Medical
wordnet: a new methodology for the construction and
validation of information. In Proc of 20th CoLing,
pages 371?382, Geneva, Switzerland.
Cornelia M Verspoor, Cliff Joslyn, and George J Papcun.
2003. The gene ontology as a source of lexical seman-
tic knowledge for a biological natural language pro-
cessing application. In SIGIR workshop on Text Anal-
ysis and Search for Bioinformatics, pages 51?56.
Pierre Zweigenbaum, Robert Baud, Anita Burgun, Fi-
ammetta Namer, E?ric Jarrousse, Natalia Grabar,
Patrick Ruch, Franck Le Duff, Benot Thirion, and
Ste?fan Darmoni. 2003. Towards a Unified Medical
Lexicon for French. In Medical Informatics in Europe
(MIE).
96
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 20?28,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Semantic distance and terminology structuring methods for the
detection of semantically close terms
Marie Dupuch
CNRS UMR 8163 STL
Universite? Lille 1&3
59653 Villeneuve d?Ascq, France
dupuchm@hotmail.fr
Lae?titia Dupuch
Universite? Toulouse III Paul Sabatier
France
laetitia1dupuch@hotmail.com
Thierry Hamon
LIM&BIO (EA3969) UFR SMBH
Universite? Paris 13, France
thierry.hamon@univ-paris13.fr
Natalia Grabar
CNRS UMR 8163 STL
Universite? Lille 1&3
59653 Villeneuve d?Ascq, France
natalia.grabar@univ-lille3.fr
Abstract
The identification of semantically similar lin-
guistic expressions despite their formal differ-
ence is an important task within NLP appli-
cations (information retrieval and extraction,
terminology structuring...) We propose to de-
tect the semantic relatedness between biomed-
ical terms from the pharmacovigilance area.
Two approaches are exploited: semantic dis-
tance within structured resources and termi-
nology structuring methods applied to a raw
list of terms. We compare these methods and
study their complementarity. The results are
evaluated against the reference pharmacovigi-
lance data and manually by an expert.
1 Introduction
When an automatic system is able to identify that
different linguistic expressions convey the same or
similar meanings, this is a positive point for several
applications. For instance, when documents refer-
ring to muscle pain or cephalgia are searched, in-
formation retrieval system can also take advantage
of the synonyms, like muscle ache or headache, to
return more relevant documents and in this way to
increase the recall. This is also a great advantage
for systems designed for instance for text mining,
terminology structuring and alignment, or for more
specific tasks such as pharmacovigilance.
The pharmacovigilance area covers the identifi-
cation of adverse drug reactions (ADRs) in order
to improve the vigilance on the health products.
Pharmacovigilance reports are traditionally encoded
with normalised terms from the dedicated termi-
nologies, such as MedDRA (Medical Dictionary for
Drug Regulatory Activities) (Brown et al, 1999).
MedDRA is a relatively fine-grained terminology
with nearly 90,000 terms. This means that a given
pharmacovigilance report can be coded with dif-
ferent terms which have close meaning (Fescharek
et al, 2004), like muscle pain and muscle ache or
headache and cephalgia: although formally differ-
ent the terms from these pairs have the same mean-
ing. The difficulty is then to detect their semantic
closeness. Indeed, if this semantic information is
available, reports from the phramacovigilance data-
banks and mentionning similar adverse events can
be aggregated: the safety signal is intensified and
the safety regulation process is improved.
In order to aggregate the pharmacovigilance re-
ports, several types of semantic information from
MedDRA are used: (1) different hierarchical levels
of MedDRA between the five levels available; (2)
the SMQs (Standardized MedDRA Queries) which
group together terms associated to a given medical
condition such as Acute renal failure, Angioedema
or Embolic and thrombotic events; and (3) specific
resources (Bousquet et al, 2005; Iavindrasana et al,
2006; Alecu et al, 2008; Jaulent and Alecu, 2009).
The SMQs are defined by groups of experts through
a long and meticulous work consisting of the man-
ual study of the MedDRA structure and of the anal-
ysis of the scientific literature (CIOMS, 2004). 84
SMQs have been created so far. They become the
gold standard data of the pharmacovigilance area.
However, the SMQs currently suffer from the lack of
exhausitivity (Pearson et al, 2009): the set of SMQs
is not exhaustive because this is an ongoing work.
We assume that automatic approaches can be ex-
20
ploited to systematize and accelerate the process of
recruiting the semantically related MedDRA terms
and to build the SMQs. We propose to exploit two
approaches: methods dedicated to the terminology
structuring and semantic distance approaches. We
compare and combine the generated results. For the
evaluation, we compare the results with the existing
SMQs and also analyse them manually with an ex-
pert. Our work is different from previous work be-
cause we exploit the whole set of the available Med-
DRA terms, we apply several methods to cluster the
terms and we perform several types of evaluation.
2 Material
We exploit two kinds of material: material issued
from MedDRA and specific to the pharmacovigi-
lance area (sections 2.1 and 2.3), and linguistic re-
sources issued from general and biomedical lan-
guages (section 2.2). The MedDRA terms are struc-
tured into five hierarchical levels: SOC (System Or-
gan Class) terms belong to the first and the high-
est level, while LLT (Lowest Level Terms) terms be-
long to the fifth and the lowest level. Terms from
the fourth level PT (Preferred Terms) are usually ex-
ploited for the coding of the pharmacovigilance re-
ports. They are also used for the creation of SMQs.
A given PT term may belong to several SMQs.
2.1 Ontology ontoEIM
ontoEIM is an ontology of ADRs (Alecu et al,
2008) created through the projection of MedDRA
to SNOMED CT (Stearns et al, 2001). This projec-
tion is performed thanks to the UMLS (NLM, 2011),
where an important number of terminologies are al-
ready merged and aligned, among which MedDRA
and SNOMED CT. The current rate of alignment of
the PT MedDRA terms with SNOMED CT is weak
(version 2011): 51.3% (7,629 terms). Projection of
MedDRA to SNOMED CT allows to improve the
representation of the MedDRA terms:
? the structure of the MedDRA terms is parallel
to that of SNOMED CT, which makes it more
fine-grained (Alecu et al, 2008). The num-
ber of hierarchical levels within the ontoEIM
reaches 14, instead of five levels in MedDRA;
? the MedDRA terms receive formal defini-
tions: semantic primitives which decompose
the meaning. MedDRA terms can be described
along up to four axes from SNOMED CT, ex-
emplified here through the term Arsenical ker-
atosis: (1) Morphology (type of abnormal-
ity): Squamous cell neoplasm; (2) Topogra-
phy (anatomical localization): Skin structure;
(3) Causality (agent or cause of the abnormal-
ity): Arsenic AND OR arsenic compound; and
(4) Expression (manifestation of the abnormal-
ity): Abnormal keratinization. The formal def-
initions are not complete. For instance, only
12 terms receive formal definitions along these
four axes and 435 along three axes. This is due
to the incomplete alignment of the MedDRA
terms and to the fact these four elements are
not relevant for every term (their absence is not
always problematic).
2.2 Linguistic resources
Linguistic resources provide three kinds of pairs
of synonym words: (1) Medical synonyms ex-
tracted from the UMLS 2011AA (n=228,542) and
then cleaned up (n=73,093); (2) Medical syn-
onyms acquired from three biomedical terminolo-
gies thanks to the exploitation of their composition-
ality (Grabar and Hamon, 2010) (n=28,691); (3)
Synonyms from the general language provided by
WordNet (Fellbaum, 1998) (n=45,782). Among
the pairs of words recorded in these resources, we
can find {accord, concordance}, {aceperone, ac-
etabutone}, {adenazole, tocladesine}, {adrenaline,
epinephrine} or {bleeding, hemorrhage}. The last
two pairs are provided by medical and general re-
sources. However, the pair {accord, concordance}
is provided only by medical resources.
2.3 Standardized MedDRA Queries
We exploit 84 SMQs as reference data. Among these
SMQs, we distinguish 20 SMQs which are struc-
tured hierarchically. We also exploit 92 sub-SMQs,
which compose these 20 hierarchical SMQs.
3 Methods
Our method consists into four main steps (figure 1):
(1) computing of the semantic distance and similar-
ity between the MedDRA terms and their cluster-
ing (section 3.1), (2) the application of the termi-
nology structuring methods to acquire semantic re-
21
Abdominal abscess
Abdominal cavity
T
Abscess morphology
M
Pharyngal abscess
M
Neck structure
T
...
...
...
...
...
...
...
...
...
...
...
...
...
1o
o
o6
5
o 2
o 3
o
7
4o
POS?tagging
Syntactic analysis
Detection ofhierarchical relations
Detection of
synonymy relations
ontoEIM resource (Zhong et al 2002)(Leacock & Chodorow, 1998)
Computing of the semantic distance Clustering of MedDRA terms
and similarity
Term structuring
Lexical inclusion
Synoterm + resources
Genia taggerOgmios platformYaTeA
Pre?processing
Me
rgin
g of
 the
 clu
ster
s
Clustering within directed graphs
EvaluationRadiusHAC (with the R project)
SMQsHierarchical SMQs
sub?SMQs
Semantic distance and similarity approaches
Terminology structuring approach
(Rada et al 1989)
Strongly connected components
Faster
Faster
Raw list ofMedDRAterms
Figure 1: General schema of the experiment composed of four steps: (1) semantic distance approaches, (2) terminology
structuring approaches, (3) their combination and (4) their evaluation
lations between MedDRA terms and their cluster-
ing (section 3.2), (3) the merging of these two sets
of clusters (section 3.3), (4) the evaluation of the
merged clusters (section 3.4). We exploit Perl lan-
guage, R1 project and several NLP tools.
3.1 Semantic distance approach
The semantic distance and similarity approach is ap-
plied to the 7,629 PT MedDRA terms and their for-
mal definitions from ontoEIM. The two main steps
are: computing the distance or similarity (section
3.1.1) and clustering of terms (section 3.1.2).
3.1.1 Computing the semantic distance
Because we work with a tree-structured resource,
we exploit edge-based algorithms to compute the
distance or similarity between two terms t1 and t2:
two semantic distances (Rada (Rada et al, 1989)
and Zhong (Zhong et al, 2002)) and one seman-
tic similarity (Leacock and Chodorow, 1998). In
the following, we call them semantic distance al-
gorithms. For each algorithm, three paths may be
exploited: between the MedDRA terms but also be-
tween the elements of their formal definitions on
two axes (morphology M and topography T often
involved in diagnostics (Spackman and Campbell,
1http://www.r-project.org
1998)). For the illustration, let?s consider two Med-
DRA terms, Abdominal abscess and Pharyngeal ab-
scess defined as follows:
? Abdominal abscess: M = Abscess morphology,
T = Abdominal cavity structure
? Pharyngeal abscess: M = Abscess morphol-
ogy, T = Neck structure
The shortest paths sp are computed between these
two MedDRA terms and between their formal defi-
nitions, whose hierarchical structure is also inherited
from SNOMED CT. The weight of edges is set to 1
because all the relations are of the same kind (hier-
archical), and the value of each shortest path corre-
sponds to the sum of the weights of all its edges. The
semantic distance sd are then exploited to compute
the unique distance between the ADR terms from
MedDRA:
?
i?{ADR,M,T}
Wi ? sdi(t1, t2)
?
i?{ADR,M,T}
Wi
, where the
three axes {ADR,M, T} respectively correspond
to terms meaning the ADR, axis Morphology M
and axis Topography T ; t1 and t2 are two ADR
terms; Wi is the coefficient associated with each
of the three axes; and sdi is the semantic distance
computed on a given axis. We carry out several ex-
22
head
component componentexpansion headcomponent componentexpansion
pain muscle ache muscle
Figure 2: Syntactically analyzed terms (muscle pain and muscle ache) into their head and expansion components
periments. Semi-matrices 7629*7629 with semantic
distance between the terms are built.
3.1.2 Clustering of terms
An unsupervised creation of clusters is applied to
the semi-matrices. We exploit two approaches:
? R radius approach: every MedDRA term is
considered a possible center of a cluster and its
closest terms are clustered with it. The thresh-
olds tested correspond to the following inter-
vals: 2 and 3 for Rada, [0; 5.059] for LCH and
[0; 0.49] for Zhong. The intersection of these
clusters is not empty.
? HAC hierarchical ascendant classification is
performed through the R Project tools (hclust
function). Iteratively, this function chooses the
best centers for terms and builds the hierar-
chy of terms by progressively clustering those
which are closest to these centers. Then the
unique cluster with all the terms is split up.
Several splitting values between 100 and 7,000
are tested. These clusters are exclusive.
Clusters created with the radius approach are
merged in order to eliminate smaller clusters in-
cluded in bigger clusters and in order to aggregate
clusters which have an important intersection be-
tween them. For the intersection, we test several in-
tersection values within the interval [10; 90], which
means that two compared clusters may have between
10% and 90% of common terms.
3.2 Terminology structuring approach
The terminology structuring methods are applied to
a raw list of 18,209 MedDRA PTs. They allow
the detection of semantic relations between these
terms. The POS-tagging is done with Genia tag-
ger (Tsuruoka et al, 2005) and the syntactic analy-
sis with the YATEA parser (Aubin and Hamon, 2006).
Three kinds of methods are applied for the acquisi-
tion of synonymy and hierarchical relations: lexical
inclusions (section 3.2.1), morpho-syntactic variants
(section 3.2.2) and compositionality (section 3.2.3).
The terms are then clustered (section 3.2.4).
3.2.1 Lexical inclusion and hierarchy
The lexical inclusion hypothesis (Kleiber and
Tamba, 1990), which states that when a given term
is lexically included at the head syntactic position
in another term there is a semantic subsumption be-
tween them, allows to identify hierarchical relations
between terms. For instance, on figure 2, the short
term pain is the hierarchical parent and the long term
muscle pain is its hierarchical child because pain is
the syntactic head of muscle pain. The lexical inclu-
sions are computed on POS-tagged and syntactically
analyzed terms. We compute two kinds of lexical in-
clusions:
? syntactic dependencies on minimal syntactic
heads: the parent term corresponds to the short-
est lexical form of the syntactic head. For in-
stance, within the term kaolin cephalin clotting
time, the minimal head is time;
? syntactic dependencies on maximal syntactic
heads: the parent term is the most complete lex-
ical form of the syntactic head. Within the same
term kaolin cephalin clotting time, the maximal
head is cephalin clotting time.
Parent and child terms have to be MedDRA terms.
3.2.2 Morpho-syntactic variants
We exploit Faster (Jacquemin, 1996) for the in-
dentification of morpho-syntactic variants between
the PT terms. This tool applies several transforma-
tion rules, such as insertion (cardiac disease/cardiac
valve disease), morphological derivation (artery
restenosis/arterial restenosis) or permutation (aorta
coarctation/coarctation of the aorta). Each transfor-
mation rule is associated with hierarchical or syn-
onymy relations: the insertion introduces a hierar-
chical relation (cardiac valve disease is more spe-
cific than cardiac disease), while the permutation in-
troduces a synonymy relation. When several trans-
formations are involved, the detected relations may
23
be ambiguous: gland abscess and abscess of sali-
vary gland combines permutation (synonymy) and
insertion (hierarchy) rules. In such cases the hierar-
chical relation prevails.
3.2.3 Compositionality and synonymy
The synonymy relations are acquired in two ways.
First, the synonymy relation is established between
two simple MedDRA terms if this relation is pro-
vided by the linguitistic resources. Second, the
identification of synonym relations between com-
plex terms relies on the semantic compositionality
(Partee, 1984). Hence, two complex terms are con-
sidered synonyms if at least one of their compo-
nents at the same syntactic position (head or ex-
pansion) are synonyms. For instance, on figure 2,
given the synonymy relation between the two words
pain and ache, the terms muscle pain and muscle
ache are also identified as synonyms (Hamon and
Nazarenko, 2001). Three transformation rules are
applied: on the head component (figure 2), on the
expansion component and on both of them. We per-
form several experiments: each medical synonymy
resource is first used individually and then in com-
bination with WordNet.
3.2.4 Clustering of terms
The sets of terms related through the lexical in-
clusions are considered as directed graphs: the terms
are the nodes of the graph while the hierarchical re-
lations are the directed edges. We partition these di-
rected graphs and identify clusters of terms which
could correspond to or be part of the SMQs. Among
connected components and strongly connected com-
ponents, we choose to generate the strongly con-
nected components: they allow an intersection be-
tween clusters which means that a given term may
belong to several clusters (this is also the case with
the SMQs). Thus, within the directed graphs G we
have to identify the maximal sub-graphs H of G
where for each pair {x, y} of the nodes from H ,
there exists a directed edge from x to y (or from y to
x). To improve the coverage of the obtained clusters,
we also add the synonyms: if a term has a synonymy
relation with the term from a cluster then this term
is also included in this cluster. From a graph theory
point of view, the initial graph is augmented with
two edges going from and to the synonyms.
Methods and relationships #relations
Hierarchical relations
Maximal syntactic head 3,366
Minimal syntactic head 3,816
Morpho-syntactic variants 743
Medical synonyms
3 biomedical terminologies 1,879
UMLS/Filtered UMLS 190
Morpho-syntactic variants 100
Medical synonyms and WordNet
3 biomedical terminologies 1,939
UMLS/Filtered UMLS 227
Table 1: Hierarchical and synonymy relations generated
by terminology structuring methods
3.3 Merging of clusters from two approaches
We merge the clusters generated by the two ap-
proaches. The merging is performed on the inter-
section between the clusters. As previously, we test
intersection values within the interval [10; 90].
3.4 Evaluation
We give judgments on: (1) the correctness of the
generated relations, (2) their relevance according to
the reference data, (3) their relevance according to
the manual evaluation by an expert. The evaluation
is performed with three measures: precision P (per-
centage of the relevant terms clustered divided by
the total number of the clustered terms), recall R
(percentage of the relevant terms clustered divided
by the number of terms in the corresponding SMQ)
and F-measure F1. The association between the
SMQs and the clusters relies on the best F1.
4 Results
Semantic relations acquired with terminology struc-
turing are indicated in table 1. There is a small
difference between relations acquired through maxi-
mal and minimal syntactic heads, although the influ-
ence of medical resources for the acquisition of syn-
onymy varies according to the resources. WordNet
slightly increases the number of synonyms. Faster
generates a large set of hierarchical and synonymy
relations. MedDRA terms have also been processed
with semantic distance and clustered. The best
thresholds with the radius clustering are 2 for Rada,
24
Approach Hierarchical SMQs SMQs and sub-SMQs
#clusters interval mean #clusters interval mean
Semantic distance 2,667 [2; 1,206] 73 2,931 [2; 546] 17
Structuring (hierarchical) 690 [1; 134] 3.69 748 [1; 117] 3.43
Structuring (hierarchical+synonymy) 690 [1; 136] 4.11 748 [1; 119] 3.82
Merging (hierarchical) 2,732 [1; 1,220] 72.40 2,998 [1; 563] 24.44
Merging (hierarchical+synonymy) 2,732 [1; 1,269] 75.94 2,998 [1; 594] 26.03
Table 2: Number of clusters and their size (the interval and the mean number of terms per cluster) for individual
approaches and for their merging computed for hierarchical SMQs and also for SMQs and sub-SMQs
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60  70  80
precision
recallf?measure
(a) Semantic distance
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60  70  80
precision
recallf?measure
(b) Terminology structuring
Figure 3: Results (precision, recall and F-measure) for semantic distance and terminology structuring approaches
4.10 for LCH and 0 for Zhong. With the HAC, the
best results are obtained with 300 classes (number of
terms per class is within the interval [1; 98], mean
number of terms per class is 25.34). Our results
show that the best parameters for the semantic dis-
tance are the Rada distance, radius approach and no
formal definitions, while the best parameters for the
terminology structuring are maximal syntactic head
with hierarchical relations by Faster augmented with
synonyms. For the merging of the clusters we apply
50% intersection for hierarchical SMQs and 80% in-
tersection for SMQs and sub-SMQs. We exploit and
discuss these results. The percentage of the Med-
DRA terms involved by the terminology structur-
ing is the 32% with hierarchical relations, it reaches
40% when the synonymy is also considered. With
semantic distance, all the terms from ontoEIM (51%
of the MedDRA) are used.
Table 2 provides information on clusters: num-
ber of clusters, number of terms per cluster (their
interval and the mean number of terms per cluster).
In table 2, we first indicate the results for the indi-
vidual approaches, and then when the merging of
the approaches is performed. We observe that the
merging has a positive effect on the number and the
size of clusters: data generated by the individual ap-
proaches (and by synonymy) are complementary.
4.1 Correctness of the semantic relations
A manual analysis of the generated hierarchical re-
lations indicates that these relations are always cor-
rect: the constraint involved through the syntac-
tic analysis guarantees correct propositions. Nev-
ertheless, we observed a small number of syntac-
tic ambiguities. They appear within 144 pairs (5%)
with maximal syntactic heads and correspond to
pairs like: {anticonvulsant drug level, drug level},
{blood smear test, smear test}, {eye movement dis-
order, movement disorder}. Thus, within the first
25
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60  70  80
precision
recallf?measure
Figure 4: Results (precision, recall and F-measure) ob-
tained when the two approaches are merged
pair, there is an ambiguity on drug as two de-
pendencies seem possible: {anticonvulsant drug
level, drug level} as proposed by the system and
{anticonvulsant drug level, level}. But whatever the
syntactic analysis performed, the semantic relations
are correct.
4.2 Relevance of the generated clusters
Figures 3 and 4 provide quantitative evaluation of
the clusters: semantic distance (figure 3(a)), termi-
nology structuring (figure 3(b)), merging of these
two sets (figure 4). On figure 3, we can observe
that there is a great variability among the SMQs and
the two approaches. The positive result is that these
approaches are indeed complementary: their merg-
ing slightly increases performance. An analysis of
the clusters generated with terminology structuring
shows that: (1) hierarchical relations form the basis
of the clusters: they correspond to 96% of the in-
volved terms and show 69% precision. Only three
clusters do not contain hierarchical relations; (2)
Faster relations are involved in 50% of clusters and
show precision between 75 and 85%; (3) one third
of the clusters contains synonymy relations, which
precision varies between 55 and 69%; (4) relations
acquired with the UMLS resources are involved in
14% of clusters while their precision is only 38%.
We also performed a detailed qualitative analysis
of several SMQs and clusters with an expert. Table 3
presents the analysis for three SMQs: Angioedema,
Embolic and thrombotic events, arterial and Haemo-
dynamic oedema, effusions and fluid overload. It
indicates the number of terms in the SMQ and in
the corresponding clusters clu, as well as the num-
ber of common terms between them com and the
performance (precision P , recall R and F-measure
F ) when computed against the reference data Ref-
erence and also after the analysis performed by the
expert After expertise. The results obtained with
the two approaches are indicated: semantic dis-
tance sd and terminology structuring struc, as well
as their merging merg. In the colums Reference,
we can observe that the best F-measure values are
obtained with the terminology structuring method
for the SMQ Haemodynamic oedema, effusions and
fluid overload (F=45) and with the semantic distance
for the SMQ Embolic and thrombotic events, arte-
rial (F=32). The merging of the two methods sys-
tematically improves the results: in the given exam-
ples, for all three SMQs.
A detailed analysis of the generated noise indi-
cates that across the SMQs we have similar situa-
tions: we generate false positives (terms non rele-
vant for the medical conditions, such as Pulmonary
oedema, Gestational oedema, Spinal cord oedema
for the SMQ Angioedema), but also the SMQs may
contain non relevant terms or may miss relevant
terms (thus, Testicular oedema, Injection site ur-
ticaria, Bronchial eodema are missing in the SMQ
Angioedema). The expert evaluation (columns Af-
ter expertise in table 3) attempts to analyse also the
quality of the SMQs. The corrected performance
of the clusters is improved in several points, which
indicates that automatic approaches may provide a
useful basis for the creation of SMQs.
5 Discussion
Despite the incompleteness of the ontoEIM re-
source, the semantic distance approach is quite ef-
ficient and provides the core terms for the building
of the SMQs. Among the several algorithms tested,
the most simple algorithm (Rada et al, 1989), which
exploits the shortest path, leads to the best results,
while the additional information on the hierarchi-
cal depth exploited by other algorithms appears non
useful. The clustering method which allows the gen-
eration of non-disjoint clusters is the most efficient
as MedDRA terms may belong to several SMQs.
26
Number of terms Reference After expertise
SMQs SMQ clu com P R F P R F
Angioedemasd 52 32 13 40 25 30 43 26 33
Angioedemastruc 52 31 19 61 36 45 61 36 45
Angioedemamerg 52 33 21 63 42 50 71 48 57
Embolic and thrombotic events...sd 132 159 48 30 36 32 32 39 35.2
Embolic and thrombotic events...struc 132 13 12 92 9 16 92 9 16
Embolic and thrombotic events...merg 132 130 49 38 37 37.5 47 46 46.5
Haemodynamic oedema, effusions...sd 36 22 7 32 20 24 54 33 41
Haemodynamic oedema, effusions...struc 36 31 13 42 36 39 84 72 78
Haemodynamic oedema, effusions...merg 36 35 16 46 44 45 86 83 84.5
Table 3: Comparison between the two approaches (semantic distance sd and terminology structuring struc) and the
merging of the two approaches merg for three SMQs: Angioedema, Embolic and thrombotic events, arterial and
Haemodynamic oedema, effusions and fluid overload
Traditionnal classification methods, which produce
disjoint clusters, are less efficient for this task.
It has been surprising to observe that the contri-
bution of the generated hierarchical relations is so
important (table 1) and that these relations appear to
be so often correct for the creation of SMQs. In-
deed, because PT terms belong to the same hierar-
chical level of MedDRA, they should be hierarchi-
cally equivalent between them. In reality, within a
cluster, we can find several hierarchical levels of the
PT terms. This means that the hierarchical structure
of MedDRA could be more fine-grained and that in-
termediate hierarchical levels could be created. As
for the generated synonymy relations, their number
is low and they contribute in a lesser way to the
building of the clusters: this means that the PTs are
semantically differentiated between them.
Finally, the merging of these two approaches is
beneficial for the generation of clusters: the per-
formance is improved, although slightly. The two
approaches provide indeed complementary results.
The low recall and F-measure are due to the material
and methods exploited: ontoEIM contains only 51%
of the MedDRA terms to be processed while the ex-
ploited terminology structuring methods are not able
to detect more common features between the terms.
The difference between the results obtained
against the reference data and after the expert eval-
uation (table 3) show that the reference data are not
very precise. In previous work, it has already been
observed that some important PT terms can be miss-
ing in the SMQs (Pearson et al, 2009). With the
proposed automatic methods we could find some of
these terms. It has been also demonstrated that the
SMQs are over-inclusive (Mozzicato, 2007; Pear-
son et al, 2009). In the proposed analysis of the
SMQs, we have also found terms which have too
large meaning and which should not be included in
the SMQs.
6 Conclusion and Perspectives
We have applied two different approaches to the
clustering of pharmacovigilance terms with simi-
lar or close meaning. We performed a comparison
of the results obtained with these two approaches
and analysed their complementarity. Several experi-
ments have been carried out in order to test different
parameters which may influence the performance of
the methods. Although the automatic creation of the
SMQs is a difficult task, our results seem to indi-
cate that the automatic methods may be used as a
basis for the creation of new SMQs. The precision
of the clusters is often satisfactory, while their merg-
ing leads to the improvement of their completeness.
These approaches generate complementary data and
their combination provides more performant results.
Future studies will lead to the identification of
other parameters which influence the quality of clus-
ters and also other factors which may be exploited
for the merging of clusters. More robust distances
and clustering methods will also be used in future
work, as well as approaches for a better acquisi-
27
tion and evaluation of the hierarchical structure of
SMQs. We plan also to design corpora-based meth-
ods which may also to increase the recall of the re-
sults. We will perform an exhaustive analysis of the
nature of semantic relations which can be observed
within the SMQs and propose other methods to fur-
ther improve the coverage of the clusters. Different
filters will be tested to remove the true false posi-
tive relations between terms. The results will also
be evaluation by several experts, which will allow to
assess the inter-expert variation and its influence on
the results. Besides, the obtained clusters will also
be evaluated through their impact on the pharma-
covigilance tasks and through the exploring of the
pharmacovigilance databases.
References
I Alecu, C Bousquet, and MC Jaulent. 2008. A case
report: using snomed ct for grouping adverse drug re-
actions terms. BMC Med Inform Decis Mak, 8(1):4?4.
S Aubin and T Hamon. 2006. Improving term extrac-
tion with terminological resources. In FinTAL 2006,
number 4139 in LNAI, pages 380?387. Springer.
C Bousquet, C Henegar, A Lillo-Le Loue?t, P Degoulet,
and MC Jaulent. 2005. Implementation of auto-
mated signal generation in pharmacovigilance using a
knowledge-based approach. Int J Med Inform, 74(7-
8):563?71.
EG Brown, L Wood, and S Wood. 1999. The medical
dictionary for regulatory activities (MedDRA). Drug
Saf., 20(2):109?17.
CIOMS. 2004. Development and rational use of stan-
dardised MedDRA queries (SMQs): Retrieving ad-
verse drug reactions with MedDRA. Technical report,
CIOMS.
C Fellbaum. 1998. A semantic network of english: the
mother of all WordNets. Computers and Humanities.
EuroWordNet: a multilingual database with lexical se-
mantic network, 32(2-3):209?220.
R Fescharek, J Ku?bler, U Elsasser, M Frank, and
P Gu?thlein. 2004. Medical dictionary for regulatory
activities (MedDRA): Data retrieval and presentation.
Int J Pharm Med, 18(5):259?269.
N Grabar and T Hamon. 2010. Exploitation of linguis-
tic indicators for automatic weighting of synonyms
induced within three biomedical terminologies. In
MEDINFO 2010, pages 1015?9.
T Hamon and A Nazarenko. 2001. Detection of syn-
onymy links between terms: experiment and results.
In Recent Advances in Computational Terminology,
pages 185?208. John Benjamins.
J Iavindrasana, C Bousquet, P Degoulet, and MC Jaulent.
2006. Clustering WHO-ART terms using semantic
distance and machine algorithms. In AMIA Annu Symp
Proc, pages 369?73.
Christian Jacquemin. 1996. A symbolic and surgical ac-
quisition of terms through variation. In S. Wermter,
E. Riloff, and G. Scheler, editors, Connectionist, Sta-
tistical and Symbolic Approaches to Learning for Nat-
ural Language Processing, pages 425?438, Springer.
MC Jaulent and I Alecu. 2009. Evaluation of an ontolog-
ical resource for pharmacovigilance. In Stud Health
Technol Inform, pages 522?6.
G Kleiber and I Tamba. 1990. L?hyperonymie revisite?e :
inclusion et hie?rarchie. Langages, 98:7?32, juin.
C Leacock and M Chodorow, 1998. Combining local
context and WordNet similarity for word sense iden-
tification, chapter 4, pages 305?332.
P Mozzicato. 2007. Standardised MedDRA queries:
their role in signal detection. Drug Saf, 30(7):617?9.
NLM, 2011. UMLS Knowledge Sources Manual. Na-
tional Library of Medicine, Bethesda, Maryland.
www.nlm.nih.gov/research/umls/.
Barbara H. Partee. 1984. Compositionality. In F. Land-
man and F. Veltman, editors, Varieties of formal se-
mantics. Foris, Dordrecht.
RK Pearson, M Hauben, DI Goldsmith, AL Gould,
D Madigan, DJ O?Hara, SJ Reisinger, and
AM Hochberg. 2009. Influence of the Med-
DRA hierarchy on pharmacovigilance data mining
results. Int J Med Inform, 78(12):97?103.
R Rada, H Mili, E Bicknell, and M Blettner. 1989. De-
velopment and application of a metric on semantic
nets. IEEE Transactions on systems, man and cyber-
netics, 19(1):17?30.
K Spackman and K Campbell. 1998. Composi-
tional concept representation using SNOMED: To-
wards further convergence of clinical terminologies.
In Journal of American Medical Informatics Associ-
ation (JAMIA), pages 740?744.
MQ Stearns, C Price, KA Spackman, and AY Wang.
2001. SNOMED clinical terms: overview of the de-
velopment process and project status. In AMIA, pages
662?666.
Y Tsuruoka, Y Tateishi, JD Kim, T Ohta, J McNaught,
S Ananiadou, and J Tsujii. 2005. Developing a ro-
bust part-of-speech tagger for biomedical text. LNCS,
3746:382?392.
J Zhong, H Zhu, J Li, and Y Yu. 2002. Concep-
tual graph matching for semantic search. In 10th
International Conference on Conceptual Structures,
ICCS2002, LNCS 2393, Springer Verlag, pages 92?
106.
28
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 109?117,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Combining Compositionality and Pagerank for the Identification of
Semantic Relations between Biomedical Words
Thierry Hamon
LIM&BIO UFR SMBH
Universite? Paris 13, France
thierry.hamon@univ-paris13.fr
Christopher Engstro?m
Division of Applied Mathematics
Ma?lardalen University
Va?stera?s, Sweden
Mounira Manser
LIM&BIO UFR SMBH
Universite? Paris 13, France
Zina Badji and Natalia Grabar
CNRS UMR 8163 STL
Universite? Lille 1&3
59653 Villeneuve d?Ascq, France
natalia.grabar@univ-lille3.fr
Sergei Silvestrov
Division of Applied Mathematics
Ma?lardalen University
Va?stera?s, Sweden
Abstract
The acquisition of semantic resources and re-
lations is an important task for several appli-
cations, such as query expansion, information
retrieval and extraction, machine translation.
However, their validity should also be com-
puted and indicated, especially for automatic
systems and applications. We exploit the com-
positionality based methods for the acquisi-
tion of synonymy relations and of indicators
of these synonyms. We then apply pager-
ank-derived algorithm to the obtained seman-
tic graph in order to filter out the acquired syn-
onyms. Evaluation performed with two inde-
pendent experts indicates that the quality of
synonyms is systematically improved by 10 to
15% after their filtering.
1 Introduction
Natural languages have extremely rich means to ex-
press or to hide semantic relations: these can be
more or less explicit. Nevertheless, the semantic
relations are important to various NLP tasks within
general or specialized languages (i.e., query expan-
sions, information retrieval and extraction, text min-
ing or machine translation) and their deciphering
must be tackled by automatic approaches. We fo-
cus in this work on synonymy relations. Thus, it
is important to be able to decide whether two terms
(i.e., anabolism and acetone anabolism, acetone an-
abolism and acetone biosynthesis, replication of mi-
tochondrial DNA and mtDNA replication) convey
the same, close or different meanings. According to
the ability of an automatic system to decipher such
relations, the answers of the system will be more or
less exhaustive. Several solutions may be exploited
when deciphering the synonymy relations:
1. Exploitation of the existing resources in which
the synonyms are already encoded. However,
in the biomedical domain, such resources are
not well described. If the morphological de-
scription is the most complete (NLM, 2007;
Schulz et al, 1999; Zweigenbaum et al, 2003),
little or no freely available synonym resources
can be found, while the existing terminologies
often lack the synonyms.
2. Exploitation and adaptation of the existing
methods (Grefenstette, 1994; Hamon et al,
1998; Jacquemin et al, 1997; Shimizu et al,
2008; Wang and Hirst, 2011).
3. Proposition of new methods specifically
adapted to the processed data.
Due to the lack of resources, we propose to ex-
ploit the solutions 2 and 3. In either of these situ-
ations, the question arises about the robustness and
the validity of the acquired relations. For instance,
(Hamon and Grabar, 2008) face two problems: (1)
contextual character of synonymy relations (Cruse,
1986), i.e., two words are considered as synonyms
if they can occur within the same context, which
makes this relation more or less broad depending on
the usage; (2) ability of automatic tools to detect and
characterize these relations, i.e., two words taken out
of their context can convey different relations than
the one expected. Our objective is to assess the relia-
bility of synonymy resources. We propose to weight
and to filter the synonym relations with the pager-
ank-derived algorithm (Brin and Page, 1998). When
109
head
componentcomponentexpansion
storagelipid
componentexpansionheadcomponent
retention lipids(of)
Figure 1: Parsing tree of the terms lipid storage and re-
tention of lipids
processing textual data, this algorithm has been pre-
viously applied in different contexts such as seman-
tic disambiguation (Mihalcea et al, 2004; Sinha and
Mihalcea, 2007; Agirre and Soroa, 2009), summa-
rization (Fernandez et al, 2009) and, more recently,
for the identification of synonyms (Sinha and Mi-
halcea, 2011). This last work takes into account the
usage of a given word in corpora and its known syn-
onyms from lexical resources. Other related works
propose also the exploitation of the random walk al-
gorithm for the detection of semantic relatedness of
words (Gaume, 2006; Hughes and Ramage, 2007)
and of documents (Hassan et al, 2007). Our work
is different from the previous work in several ways:
(1) the acquisition of synonymy is done on resources
provided by a specialized domain; (2) the pager-
ank algorithm is exploited for the filtering of seman-
tic relations generated with linguistically-based ap-
proaches; (3) the pagerank algorithm is adapted to
the small size of the processed data.
In the following of this paper, we present first the
material (section 2), then the method we propose
(section 3). We then describe the experiments per-
formed and the results (section 4), as well as their
evaluation and discussion (section 5). Finally, we
conclude and indicate some perspectives (section 6).
2 Material
We use the Gene Ontology (GO) as the original re-
source from which synonym lexicon (or elementary
synonym relations) are induced. The goal of the GO
is to produce a structured vocabulary for describing
the roles of genes and their products in any organ-
ism. GO terms are structured with four types of re-
lations: subsumption is-a, meronymy part-of,
synonymy and regulates. The version used in
the current work is issued from the UMLS 2011AA.
It provides 54,453 concepts and their 94,161 terms.
The generated pairs of terms have 119,430 is-a
and 101,254 synonymy relations.
3 Methods
Our method has several steps: preprocessing of GO
terms (section 3.1), induction of elementary syn-
onyms (section 3.2) and their characterization with
lexical and linguistic indicators (section 3.3), anal-
ysis of the synonymy graph, its weighting thanks to
the pagerank algorithm and its filtering (section 3.4).
We also perform an evaluation of the generated and
filtered synonymy relations (section 3.5).
In the following, we call original synonyms those
synonyms which are provided by GO, and we call
elementary synonyms those synonyms which are in-
duced by the compositionality based approach.
3.1 Preprocessing the GO terms: Ogmios NLP
platform
The aim of terminology preprocessing step is to
provide syntactic analysis of terms for computing
their syntactic dependency relations. We use the
Ogmios platform1 and perform: segmentation into
words and sentences; POS-tagging and lemmatiza-
tion (Tsuruoka et al, 2005); and syntactic analysis2.
Syntactic dependencies between term components
are computed according to assigned POS tags and
shallow parsing rules. Each term is considered as
a syntactic binary tree composed of two elements:
head component and expansion component. For in-
stance, lipid is the head component of the two terms
analyzed on figure 1.
3.2 Compositionality based induction of
synonyms
GO terms present compositional structure (Verspoor
et al, 2003; Mungall, 2004; Ogren et al, 2005). In
the example below (concept GO:0009073) the com-
positionality can be observed through the substitu-
tion of one of the components (underlined):
aromatic amino acid family biosynthesis
aromatic amino acid family anabolism
aromatic amino acid family formation
aromatic amino acid family synthesis
We propose to exploit the compositionality for in-
duction of synonym resources (i.e., biosynthesis, an-
abolism, formation, synthesis in the given example).
1http://search.cpan.org/?thhamon/Alvis-NLPPlatform/
2http://search.cpan.org/?thhamon/Lingua-YaTeA/
110
While the cited works are based on the string match-
ing, our approach exploits their syntactic analysis,
which makes it independent on their surface graphi-
cal form (like examples on figure 1).
Compositionality assumes that the meaning of a
complex expression is fully determined by its syn-
tactic structure, the meaning of its parts and the com-
position function (Partee, 1984). This assumption is
very often true in specialized langages, which are
known to be compositional. On the basis of syntac-
tically analysed terms, we apply a set of composi-
tional rules: if the meaningM of two complex terms
A rel B and A? rel B, where A is its head and B its
expansion components, is given as following:
M(A rel B) = f(M(A),M(B),M(rel))
M(A? rel B) = f(M(A?),M(B),M(rel))
for a given composition function f , if A rel B and
A? rel B are complex synonym terms and if B com-
ponents are identical (such as acetone within ace-
tone catabolism and acetone breakdown), then the
synonymy relation between components A and A?
{catabolism, breakdown} can be induced. The mod-
ification is also accepted on expansion component
B: from terms replication of mitochondrial DNA
and mtDNA replication (fig. 1), we can induce syn-
onymy between mitochondrial DNA and mtDNA.
Finally, the modification is also accepted for both
components A rel B and A? rel B?, such as in
nicotinamide adenine dinucleotide catabolism and
NAD breakdown, where one pair, i.e. {catabolism,
breakdown}, can be known from previously pro-
cessed synonyms and allow to induce the new
pair {nicotinamide adenine dinucleotide, NAD}. It
should noticed that rel depends on the original re-
lations: if the original terms are synonyms then the
elementary terms are also synonyms, if the original
terms are hierarchically related then the elementary
terms are also hierarchically related, etc.
3.3 Lexically-based profiling of the induced
elementary synonyms
In order to test and improve the quality of the in-
duced synonymy relations, we confront these syn-
onyms with approaches which allow to acquire the
hyperonymy relations. All these resources are endo-
geneously acquired from the same terminology GO:
? Each induced pair of synonyms is controlled
for the lexical inclusion (Kleiber and Tamba,
1990; Bodenreider et al, 2001). If the test is
positive, like in the pair {DNA binding, bind-
ing} this would suggest that this pair may con-
vey a hierarchical relation. Indeed, it has been
observed that lexical subsumption marks often
a hierarchical subsumption. Thus, in the pair
{DNA binding, binding}, binding is the hierar-
chical parent of DNA binding, while DNA bind-
ing has a more specific meaning than binding.
One can assume that the cooccurrence of syn-
onymy with the lexical subsumption makes the
synonymy less reliable;
? The same compositional method, as described
in the previous section, is applied to original
GO term pairs related through is-a relations.
In this way, we can also infer is-a elemen-
tary relations. Thus, if a pair of induced syn-
onyms is also induced through is-a relations,
i.e. {binding, DNA binding}, this also makes
the synonymy relations less reliable.
In summary, an induced synonymy relation is con-
sidered to be less reliable when it cooccurs with
a lexical inclusion or with is-a relation. For in-
stance, several edges from figure 2 present the cooc-
currence of synonymy relations with the is-a rela-
tions (such as, {holding, retention}, {retention, stor-
age} or {retention, sequestering}).
3.4 Pagerank-derived filtering of the induced
elementary synonyms
The induced semantic relations can be represented
as graphs where the nodes correspond to words and
the edges to one or more relations between given two
words. An example of what it can look like can be
seen on figure 2: the induced synonymy relations
may indeed cooccur with non-synonymy relations,
like the hierarchical relations is-a. We propose to
use a pagerank approach (Brin and Page, 1998) in
order to separate a given graph of synonym relations
into subsets (or groups) within which all the words
are considered as synonyms with each other but not
with any other word outside their subset. In order
not to influence the results by the varying size of
the graphs, we exploit a non-normalized version of
pagerank (Engstro?m, 2011). Thus, given the usual
111
storage sequestering
holding
retention
sequestration
syn(3), is?a(1)
syn(1), is?a(3)syn(1), is?a(2)
syn(2) syn(2)
syn(2)
Figure 2: An example of graph generated thanks to the
induced semantic relations: pairs related with synonymy
relations syn may also be related with non-synonymy
relations (like hierarchical relation is-a)
normalized version P (1)Si of pagerank:
Definition 1 P (1)S for system S is defined as the
eigenvector with eigenvalue one to the matrix
M = c(A+ gvT )T + (1? c)v1T
where g is a n ? 1 vector with zeros for nodes with
outgoing nodes and 1 for all dangling nodes, 0 <
c < 1, A is the linkmatrix with sum of every row
equal to one, v is a non-negative weightvector with
sum one.
As we mentioned, with the processed data we
have to use the non-normalized version of pagerank:
Definition 2 P (2)S for system S is defined as:
P (2)S =
P (1)S ||V ||1
d
,with d = 1?
?
cATP (1)S
where V is the part of a global weightvector corre-
sponding to the system S. We let V be the one vector
such that all words are weighted equally.
Looking at the example from figure 2, we start
from any node and then randomly either stop by a
probability c or choose (possibly weighted by edge-
weights) a new node by the probability 1 ? c from
any of those linked to the chosen node. The page-
rank of a node can then be seen as the sum of the
probabilities of all paths to the node in question
(starting in every node once including itself).
Usually A is a two-dimensional matrix in which
the sum of every row is equal to one and all non-
zero elements are equal between them. In order to
use different types of relations and different weights
on these relations we calculate cA. Given B, where
B contains the weights of different edges and their
type, we calculate A as:
Ai,j = (Bi,j,SY N/(Bi,j,OTHER + 1))/ni
where ni is the total number of edges connected to
node i. We treat all relations as symmetric relations
for the filtering algorithm when creating B. While
some relations aren?t symmetric it seems reasonable
to assume they affect the likelihood of synonyms in
both directions. We also do not distinguish non-
synonym relations among them. However, we try
a few variations on how to weight A such as assign-
ing different weights to synonym and non-synonym
relations or using a logarithmic scale to decrease the
effect of very different weights in B.
Further to the weighting, the rows of A do not
necessarily sum to one. We propose then not to
choose a specific value for c, but to threshold the
sum of every row in cA to 0.95. This means that for
most of the rows we set crow = 1/
?
Arow ? 0.95,
but for rows with a low sum we don?t increase the
strength of the links but rather keep them as they
are (crow = 1). Choosing the threshold can be
seen as choosing c in the ordinary pagerank formu-
lation. A low threshold means that only the immedi-
ate surrounding of a node may impact its pagerank,
while a high threshold means that distant nodes may
also have an impact. Higher threshold is also use-
ful to separate the pagerank of nodes and to make
slower the convergence when calculating the pager-
ank. When the sum of all rows is less than one and
all non-zero elements are positive we can guarantee
that the pagerank algorithm converges (Bryan and
Leise, 2006). We also use the Power Method modi-
fied for the non-normalized version of pagerank (En-
gstro?m, 2011). On the basis of these elements, we
apply the following algorithm for segmenting the
graph into groups of nodes:
1. Calculate weighted linkmatrix;
2. Calculate pagerank from uniform weightvector
vi;
112
3. Select the node with the highest pagerank;
4. Calculate pagerank from non-uniform
weightvector (zero vector with a single 1
for the selected node);
5. Nodes with P (2) > cutoff are selected as syn-
onyms with selected node and each other;
6. Remove the found synonym nodes from the
graph;
7. If the graph is non empty, restart from step 1;
8. Otherwise end: words belonging to the same
group are considered as synonyms.
We present the application of the algorithm on
the example from figure 2 using the cutoff =
1.5. We start by calculating the weights on the
links (weighted linkmatrix). For instance, given
the relation from storage to retention we have:
Ai,j = (Bi,j,SY N/(Bi,j,OTHER + 1))/ni =
(1/(2 + 1))/3 = 1/9. After computing the
weights for all the relations and thresholding the
sum of rows to 0.95, when the sum of weights
out of a node is larger than 0.95, we obtain fig-
ure 3. This gives the pagerank from uniform vec-
tor [4.8590, 7.7182, 16.4029, 16.1573, 15.4152], in
which we select the node storage with the highest
pagerank. Pagerank from non-uniform weightvec-
tor is then [0.5490, 1.0970, 4.7875, 4.0467, 3.9079],
in which we select the nodes with rank larger than
cutoff = 1.5 (storage, sequestration, sequestering)
as synonyms. After removing these nodes, we re-
calculate the weight matrix and repeate the algo-
rithm: the two remaining nodes are found to belong
to the same group. We then terminate the algorithm.
3.5 Evaluation protocol
The evaluation is performed against the manually
validated synonymy relations. This validation has
been done by two independent experts with the
background in biology. They were asked to vali-
date the induced synonyms acquired as the step 3.2
of the method. The inter-expert Cohen?s kappa is
0.75. On the basis of this evaluation, we compute
the precision: percentage of relations which allow to
correctly group terms within the connected compo-
nents and the groups. We compute two kinds of pre-
cision (Sebastiani, 2002): micro-precision which is
the classical conception of this measure obtained at
3: storage 4: sequestering
1: holding
2: retention
5: sequestration
0.060.07
0.5
0.44
0.480.48
0.45
0.11
0.45
0.08
0.95
0.44
Figure 3: Example from figure 2 with weighted links
the level of the relations, and macro-precision which
corresponds to the mean of the precisions obtained
at the level of connected components or groups. The
evaluation is done with the induced synonyms and
also after their filtering with the pagerank-derived
algorithm. This last evaluation leads to a better ob-
servation of the efficiency of the pagerank algorithm.
4 Experiments and Results
The GO terms have been fully processed with the
NLP tools (POS-tagging and syntactic analysis) in
order to prepare the next step, during which the ele-
mentary relations and the indicators are acquired.
4.1 Application of the lexical NLP methods
We applied the NLP method to the GO terms.
The application of the compositionality approach to
original synonymy and hierarchical relations gen-
erated 3,707 and 10,068 elementary relations, syn-
onymous and hierarchical respectivelly. Depend-
ing on the syntactic structure of the original terms,
the synonymy relations are induced between simple
or complex terms, but also between their abbrevi-
ated and full forms, between the morpho-syntactic
variants, etc. Very few of these synonyms exist
within GO or within the WordNet resource (Fell-
baum, 1998). We also detected 1,608 lexical in-
clusions. The lexical inclusions and the is-a re-
lations are preserved only if they cooccur with in-
113
duced synonymy relations. All these relations are
then grouped into connected components (figure 2):
the synonymy relations correspond to edges, term
components correspond to nodes, while the infor-
mation on is-a relations and on lexical inclusions
appears as reliability indicators of the synonymy
edges. A total of 2,017 connected components are
generated. The biggest connected component con-
tains 140 nodes and 183 edges. At this step, the con-
nected components are evaluated against the refer-
ence data: we compute the precision.
4.2 Filtering of the induced synonyms with the
pagerank-derived algorithm
We the apply the pagerank-derived algorithm to the
induced synonyms, but also to the combinations of
these synonyms with is-a relations and/or with
lexical inclusions. The objective is then to filter the
induced synonyms and to improve their reliability.
We perform seven experiments, in which the syn-
onymy and the indicators may receive the same im-
portance or may be weighted:
1. syn: only the elementary synonymy relations
are considered;
2. syn-isa: combination of synonymy and hierar-
chical is-a relations;
3. syn-incl: combination of synonymy relations
with lexical inclusions;
4. syn-isa-incl: combination of synonymy and hi-
erarchical relations with lexical inclusions;
5. syn-isa(535): combination of synonymy rela-
tions with lexical inclusions, using different
weights: (Ai,j = 5Bi,j,SY N/(3Bi,j,OTHER +
5))/ni;
6. syn-isa(353): combination of synonymy rela-
tions with lexical inclusions, using different
weights: (Ai,j = 3Bi,j,SY N/(5Bi,j,OTHER +
3))/ni.
7. syn-isa(log): combination of synonymy rela-
tions with lexical inclusions, using logarithmic
weights: (Ai,j = ((1/ln(2))ln(Bi,j,SY N +
1)/((1/ln(2))ln(Bi,j,OTHER + 2)))/ni.
According to the method described in section 3.4,
the connected components of the synonymy rela-
tions obtained in section 3.2 are segmented again
into one or more smaller and more homogeneous
groups. The number of groups varies between 745
and 1,798 across the experiments. Moreover, around
25% of the synonymy relations may be removed by
pagerank. These connected components and groups
can also be evaluated against the reference data and
we can compute the precision.
5 Evaluation and Discussion
The evaluation has been done by two indepen-
dent experts, with the Cohen?s kappa inter-expert
agreement 0.75. We exploit the reference data of
the two experts separately (we distinguish expert1
and expert2) and in common. We also distinguish
macro-precision and micro-precision. Finally, the
precision is first evaluated after the induction step
with the NLP methods, and then after the process-
ing of the acquired synonymy relations through the
pagerank-derived algorithm and their filtering.
For the weighting of the non-synonymy and syn-
onymy relations, we tested and applied several coef-
ficients: 5, 3 and 5 in experiment 5 (syn-isa535); 3,
5 and 3 in experiment 6 (syn-isa353), etc. Different
weights have been tested ranging from 1 to 7, as well
as the log variations. On the whole, these variations
have no significant impact on the results. But then, it
is very important to respect the dependence among
these coefficients and not to set them randomly.
The filtering of the synonymy relations has to con-
trol two factors: (1) the first is related to the fact
that the removed relations are to be true negatives
and that among them there should be no or a small
number of correct relations; while (2) the second is
related to the fact that the remaining relations are to
be true positives and that among them there should
be no or a small number of wrong relations.
Figure 4: Impact of the cutoff values on the filtering of
synonymy relations
114
miP_Exp1 maP_Exp1 miP_Exp2 maP_Exp2
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 1  2  3  4  5  6  7
before Pagerank
maP_Exp1 and maP_Exp2before Pagerank
miP_Exp1
miP_Exp2 before Pagerank
(a) Connected components with terms
miP_Exp1 maP_Exp1 miP_Exp2 maP_Exp2
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 1  2  3  4  5  6  7
before Pagerank
maP_Exp1 and maP_Exp2before Pagerank
miP_Exp1
miP_Exp2 before Pagerank
(b) Groups of terms
Figure 5: Evaluation of the results in terms of micro-precision miP and of macro-precision maP for connected
components and for groups of terms (performed according to the reference data provided by two experts)
On figure 4, we present the impact of the cut-
off values on the selection and filtering of the syn-
onyms. Like with other parameters, we have tested
several values between 0.5 and 4. This figure illus-
trates the distribution of the correctly removed rela-
tions. The cutoff values have an important impact
on the results: we can observe that the optimal cut-
off values are set between 1.5 and 2 because they
allow to remove the highest number of the wrong
relations. We have set the cutoff value to 1.5. The
choice of cutoff is an important factor for the defi-
nition of the amount of the links that are to be re-
moved: the higher the cutoff the higher the number
of clusters. On the data processed in this work, the
cutoff value has been defined experimentally thanks
to the observation of the processed data. For the gen-
eralization of this method to new unknown but sim-
ilar linguistic data (new terminology, new langage,
new domain...), the cutoff will be either set in order
to remove a certain predefined number of links or
will be defined from a typical sample of the data.
Contrary to the cutoff values, the choice of thresh-
old doesn?t greatly impact the results, although us-
ing a lower threshold makes it harder to choose a
good cutoff values since the ranking of different
nodes will be closer to each other.
As for the analysis of the precision and of the
relations which are correctly kept within the con-
nected components, let?s observe figure 5. On this
figure, we present the evaluation results performed
within the connected components with induced syn-
onyms (figure 5(a)) and within the groups of filtered
synonyms (figure 5(b)). On the y-axis we indicate
the precision values, and on the x-axis, we indicate
the different experiments performed as mentioned
above: 1 in which only synonyms are exploited, 2
in which synonyms are combined with hierarchical
is-a relations, 3 in which synonyms are combined
with lexical inclusions, etc. Horizontal lines corre-
spond to the precision obtained before the applica-
tion of the pagerank: they remain the same whatever
the experiment. These lines correspond to three ref-
erence data provided by the expert1, the expert2 and
by their common data. As for the points, they indi-
cate the precision obtained further to the pagerank:
it varies according to experiments and experts. On
the basis of figure 5, we can observe that:
? the difference between the expert evaluations is
very low (0.02);
? the pagerank allows to increase the precision
(between 0.10 and 0.15 for micro-precision,
while macro-precision varies by 0.05);
? the consideration of synonymy alone provides
performant results;
? the consideration of is-a relations improves
the results but lexical inclusions decrease them;
? the increased weight of some of the quality in-
dicators has no effect on the evaluation;
? macro-precision is superior to micro-precision
because our data contain mainly small groups,
115
while the few large connected components have
a very low precision;
? there is but a small difference between con-
nected components (figure 5(a)) and groups
(figure 5(b));
? the consideration of is-a relations and of lex-
ical inclusions provides the best precision but
the amount of the remaining synonyms is then
the lowest. As we explained, it is important
to keep the highest number of the correct re-
lations, although when a lot of relations is re-
moved, it is logical to obtain a higher precision.
This means that the combination of is-a re-
lations and of lexical inclusions is not suitable
because it removes too much of synonyms.
In relation with this last observation, is should be
noted that the balance between the removed and the
remaining relations is a subtle parameter.
The obtained results indicate that the pagerank is
indeed useful for the filtering of synonyms, although
the parameters exploited by this algorithm must be
defined accurately. Thus, it appears that synonymy
alone may be sufficient for this filtering. When the
quality indicators are considered, is-a relations are
suitable for this filtering because very often they pro-
pose true hierarchical relations. However, the lex-
ical inclusions have a negative effect of the filter-
ing. We assume this is due to the fact that the lexical
inclusions are ambiguous: they may convey hierar-
chical relations but also equivalence relations (Har-
alambous and Lavagnino, 2011). Indeed, contextu-
ally some terms may be shortened or may be subject
to an elision while their meaning is not impacted.
Currently, the pagerank is limited by the fact that
it is applied to a relatively small set of data while
it is designed to process very large data. Then, it
can be interesting to enrich the model and to be able
to take into account other quality indicators, such as
frequencies, productivity or other semantic relations
proposed within GO (part-of and regulates).
Moreover, we can also give a lesser weight to some
indicators (such as lexical inclusions) with penal-
ties and keep the strong weight for other indicators.
In the current model of the pagerank, we thresh-
old rows to < 0.95. However, we assume that the
algorithm may have problems with very large and
very connected graphs: the pagerank may spread
out in the graph too much and possibly allow the
first words with the highest pagerank to make groups
with only one word. This can be corrected if an addi-
tional calculation is added and when the group con-
tains only one word at step 5.
6 Conclusion and Perspectives
We propose an original approach for inducing syn-
onyms from terminologies and for their filtering.
The methods exploit the NLP methods, composi-
tionality principle and pagerank-derived algorithm.
This work is motivated by the fact that synonymy
is a contextual relation and its validity and univer-
sality are not guaranteed. We assume the seman-
tic cohesiveness of synonymy relations should be
qualified and quantified. The compositionality and
NLP methods allow to acquire endogeneously the
synonymy relations and the quality indicators, while
the pagerank-derived algorithm leads to the filtering
of the acquired synonyms. Its functionning is based
upon the synonymy relations and also upon the ac-
quired indicators (is-a relations and lexical inclu-
sions). It appears that the synonymy relations alone
provide good clues for their filtering. The is-a re-
lations are also fruitful, while the use of the lexical
inclusions appears not to be suitable.
In the future, we plan to add and test other indi-
cators. Other experiments will also be done with the
pagerank approach. For instance, it will be inter-
esting to propose a model which takes into account
that, within a cluster, words may be synonym with
some cluster words but not with all the words of the
cluster. This method can be adapted for the process-
ing of corpora and also applied to terms from other
terminologies. The acquired and filtered synonymy
relations will be exploited within the NLP applica-
tions in order to test the efficiency of these resources
and also the usefulness and efficiency of their filter-
ing. Moreover, the compositionality approach can
be adapted and exploited for the paraphrasing of the
biomedical terms and for the improvement of their
understanding by non expert people.
References
E Agirre and A Soroa. 2009. Personalizing PageRank
for word sense disambiguation. In EACL 2009, pages
33?41, Athens, Greece, March.
116
O Bodenreider, A Burgun, and TC Rindflesch. 2001.
Lexically-suggested hyponymic relations among med-
ical terms and their representation in the UMLS. In
URI INIST CNRS, editor, Terminologie et Intelligence
artificielle (TIA), pages 11?21, Nancy.
S Brin and L Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks
and ISDN Systems, 30(1?7):107?117.
K Bryan and T Leise. 2006. The $25, 000, 000, 000
eigenvector: the linear algebra behind google. SIAM
Rev., 48(3):569?581.
David A. Cruse. 1986. Lexical Semantics. Cambridge
University Press, Cambridge.
C Engstro?m. 2011. Pagerank as a solution to a lin-
ear system, pagerank in changing systems and non-
normalized versions of pagerank. Master?s thesis,
Mathematics, Centre for Mathematical sciences, Lund
University. LUTFMA-3220-2011.
C Fellbaum. 1998. A semantic network of english: the
mother of all WordNets. Computers and Humanities.
EuroWordNet: a multilingual database with lexical se-
mantic network, 32(2-3):209?220.
S Fernandez, E SanJuan, and JM Torres-Moreno. 2009.
Re?sume?s de texte par extraction de phrases, algo-
rithmes de graphe et e?nergie textuelle. In Socie?te?
Francophone de Classification, pages 101?104.
B Gaume. 2006. Cartographier la forme du sens dans les
petits mondes lexicaux. In JADT.
G Grefenstette. 1994. Explorations in automatic the-
saurus discovery. Kluwer Academic Publishers.
T Hamon and N Grabar. 2008. Acquisition of elementary
synonym relations from biological structured termi-
nology. In Computational Linguistics and Intelligent
Text Processing (5th International Conference on NLP,
2006), number 4919 in LNCS, pages 40?51. Springer.
T Hamon, A Nazarenko, and C Gros. 1998. A step
towards the detection of semantic variants of terms
in technical documents. In COLING-ACL?98, pages
498?504.
Y Haralambous and E Lavagnino. 2011. La re?duction de
termes complexes dans les langues de spc?ialite?. TAL,
52(1):37?68.
S Hassan, R Mihalcea, and C Banea. 2007. Random-
walk term weighting for improved text classification.
In ICSC, pages 242?249.
T Hughes and D Ramage. 2007. Lexical semantic relat-
edness with random graph walks. In EMNLP-CoNLL,
pages 581?589. Association for Computational Lin-
guistics.
C Jacquemin, JL Klavans, and E Tzoukerman. 1997.
Expansion of multi-word terms for indexing and re-
trieval using morphology and syntax. In ACL/EACL
97), pages 24?31, Barcelona, Spain.
G Kleiber and I Tamba. 1990. L?hyperonymie revisite?e :
inclusion et hie?rarchie. Langages, 98:7?32, juin.
R Mihalcea, P Tarau, and E Figa. 2004. Pagerank on se-
mantic networks, with application to word sense dis-
ambiguation. In COLING, pages 1126?1132.
CJ Mungall. 2004. Obol: integrating language and
meaning in bio-ontologies. Comparative and Func-
tional Genomics, 5(6-7):509?520.
NLM, 2007. UMLS Knowledge Sources Manual. Na-
tional Library of Medicine, Bethesda, Maryland.
www.nlm.nih.gov/research/umls/.
PV Ogren, KB Cohen, and L Hunter. 2005. Implica-
tions of compositionality in the Gene Ontology for its
curation and usage. In Pacific Symposium of Biocom-
puting, pages 174?185.
BH Partee, 1984. Compositionality. F Landman and F
Veltman.
S Schulz, M Romacker, P Franz, A Zaiss, R Klar, and
U Hahn. 1999. Towards a multilingual morpheme
thesaurus for medical free-text retrieval. In Medical
Informatics in Europe (MIE), pages 891?4.
F Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1?47.
N Shimizu, M Hagiwara, Y Ogawa, K Toyama, and
H Nakagawa. 2008. Metric learning for synonym ac-
quisition. In COLING, pages 793?800.
R Sinha and R Mihalcea. 2007. Unsupervised graph-
based word sense disambiguation using measures of
word semantic similarity. In IEEE International Con-
ference on Semantic Computing (ICSC 2007), pages
363?369.
RS Sinha and RF Mihalcea. 2011. Using centrality algo-
rithms on directed graphs for synonym expansion. In
FLAIRS.
Y Tsuruoka, Y Tateishi, JD Kim, T Ohta, J McNaught,
S Ananiadou, and J Tsujii. 2005. Developing a ro-
bust part-of-speech tagger for biomedical text. LNCS,
3746:382?392.
CM Verspoor, C Joslyn, and GJ Papcun. 2003. The Gene
Ontology as a source of lexical semantic knowledge
for a biological natural language processing applica-
tion. In SIGIR workshop on Text Analysis and Search
for Bioinformatics, pages 51?56.
T Wang and G Hirst. 2011. Exploring patterns in dictio-
nary definitions for synonym extraction. Natural Lan-
guage Engineering, 17.
P Zweigenbaum, R Baud, A Burgun, F Namer, E? Jar-
rousse, N Grabar, P Ruch, F Le Duff, B Thirion, and
S Darmoni. 2003. Towards a Unified Medical Lexicon
for French. In Medical Informatics in Europe (MIE),
pages 415?20.
117
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 99?103,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
BioNLP Shared Task 2013: Supporting Resources
Pontus Stenetorp 1 Wiktoria Golik 2 Thierry Hamon 3
Donald C. Comeau 4 Rezarta Islamaj Dog?an 4 Haibin Liu 4 W. John Wilbur 4
1 National Institute of Informatics, Tokyo, Japan
2 French National Institute for Agricultural Research (INRA), Jouy-en-Josas, France
3 University Paris 13, Paris, France
4 National Center for Biotechnology Information, National Library of Medicine,
National Institutes of Health, Bethesda, MD, USA
pontus@nii.ac.jp wiktoria.golik@jouy.inra.fr thierry.hamon@univ-paris13.fr
{comeau,islamaj,liuh11,wilbur}@ncbi.nlm.nih.gov
Abstract
This paper describes the technical con-
tribution of the supporting resources pro-
vided for the BioNLP Shared Task 2013.
Following the tradition of the previous
two BioNLP Shared Task events, the task
organisers and several external groups
sought to make system development easier
for the task participants by providing auto-
matically generated analyses using a vari-
ety of automated tools. Providing analy-
ses created by different tools that address
the same task also enables extrinsic evalu-
ation of the tools through the evaluation of
their contributions to the event extraction
task. Such evaluation can improve under-
standing of the applicability and benefits
of specific tools and representations. The
supporting resources described in this pa-
per will continue to be publicly available
from the shared task homepage
http://2013.bionlp-st.org/
1 Introduction
The BioNLP Shared Task (ST), first organised in
2009, is an ongoing series of events focusing on
novel challenges in biomedical domain informa-
tion extraction. In the first BioNLP ST, the or-
ganisers provided the participants with automat-
ically generated syntactic analyses from a variety
of Natural Language Processing (NLP) tools (Kim
et al, 2009) and similar syntactic analyses have
since then been a key component of the best per-
forming systems participating in the shared tasks.
This initial work was followed up by a similar ef-
fort in the second event in the series (Kim et al,
2011), extended by the inclusion of software tools
and contributions from the broader BioNLP com-
munity in addition to task organisers (Stenetorp et
al., 2011).
Although no formal study was carried out to es-
timate the extent to which the participants utilised
the supporting resources in these previous events,
we note that six participating groups mention us-
ing the supporting resources in published descrip-
tions of their methods (Emadzadeh et al, 2011;
McClosky et al, 2011; McGrath et al, 2011;
Nguyen and Tsuruoka, 2011; Bjo?rne et al, 2012;
Vlachos and Craven, 2012). These resources have
been available also after the original tasks, and
several subsequent studies have also built on the
resources. Van Landeghem et al (2012) applied a
visualisation tool that was made available as a part
of the supporting resources, Vlachos (2012) em-
ployed the syntactic parses in a follow-up study
on event extraction, Van Landeghem et al (2013)
used the parsing pipeline created to produce the
syntactic analyses, and Stenetorp et al (2012) pre-
sented a study of the compatibility of two different
representations for negation and speculation anno-
tation included in the data.
These research contributions and the overall
positive reception of the supporting resources
prompted us to continue to provide supporting re-
sources for the BioNLP Shared Task 2013. This
paper presents the details of this technical contri-
bution.
2 Organisation
Following the practice established in the
BioNLP ST 2011, the organisers issued an
open call for supporting resources, welcoming
contributions relevant to the task from all authors
of NLP tools. In the call it was mentioned that
points such as availability for research purposes,
support for well-established formats and access
99
Name Annotations Availability
BioC Lemmas and syntactic constituents Source
BioYaTeA Terms, lemmas, part-of-speech and syntactic constituencies Source
Cocoa Entities Web API
Table 1: Summary of tools/analyses provided by external groups.
to technical documentation would considered
favourable (but not required) and each supporting
resource provider was asked to write a brief
description of their tools and how they could
potentially be applied to aid other systems in the
event extraction task. This call was answered
by three research groups that offered to provide
a variety of semantic and syntactic analyses.
These analyses were provided to the shared
task participants along with additional syntactic
analyses created by the organisers.
However, some of the supporting resource
providers were also participants in the main event
extraction tasks, and giving them advance access
to the annotated texts for the purpose of creating
the contributed analyses could have given those
groups an advantage over others. To address this
issue, the texts were made publicly available one
week prior to the release of the annotations for
each set of texts. During this week, the supporting
analysis providers annotated the texts using their
automated tools and then handed the analyses to
the shared task organisers, who made them avail-
able to the task participants via the shared task
homepage.
3 Analyses by External Groups
This section describes the tools that were applied
to create supporting resources by the three exter-
nal groups. These contributions are summarised in
Table 1.
BioC Don Comeau, Rezarta Islamaj, Haibin
Liu and John Wilbur of the National Center for
Biotechnology Information provided the output of
the shallow parser MedPost (Smith et al, 2004)
and the BioLemmatizer tool (Liu et al, 2012),
supplied in the BioC XML format1 for annota-
tion interchange (Comeau et al, 2013). The BioC
format address the problem of interoperability be-
tween different tools and platforms by providing a
unified format for use by various tools. Both Med-
Post and BioLemmatizer are specifically designed
1http://bioc.sourceforge.net/
for biomedical texts. The former annotates parts-
of-speech and performs sentence splitting and to-
kenisation, while the latter performs lemmatisa-
tion. In order to make it easier for participants
to get started with the BioC XML format, the
providers also supplied example code for parsing
the format in both the Java and C++ programming
languages.
BioYaTeA Wiktoria Golik of the French Na-
tional Institute for Agricultural Research (INRA)
and Thierry Hamon of University Paris 13 pro-
vided analyses created by BioYaTeA2 (Golik et
al., 2013). BioYaTeA is a modified version of the
YaTeA term extraction tool (Aubin and Hamon,
2006) adapted to the biomedical domain. Working
on a noun-phrase level, BioYaTeA provides anno-
tations such as lemmas, parts-of-speech, and con-
stituent analysis. The output formats used were a
simple tabular format as well as BioYaTeA-XML,
an XML representation specific to the tool.
Cocoa S. V. Ramanan of RelAgent Private Ltd
provided the output of the Compact cover anno-
tator (Cocoa) for biological noun phrases.3 Co-
coa provides noun phrase-level entity annotations
for over 20 different semantic categories such as
macromolecules, chemicals, proteins and organ-
isms. These annotations were made available for
the annotated texts for the shared task along with
the opportunity for the participants to use the Co-
coa web API to annotate any text they may con-
sider beneficial for their system. The data format
used by Cocoa is a subset of the standoff format
used for the shared task entity annotations, and it
should thus be easy to integrate into existing event
extraction systems.
4 Analyses by Task Organisers
This section describes the syntactic parsers ap-
plied by the task organisers and the pre-processing
2http://search.cpan.org/?bibliome/
Lingua-BioYaTeA/
3http://npjoint.com/
100
Name Model Availability
Enju Biomedical Binary
Stanford Combination Binary, Source
McCCJ Biomedical Source
Table 2: Parsers used for the syntactic analyses.
and format conversions applied to their output.
The applied parsers are listed in Table 2.
4.1 Syntactic Parsers
Enju Enju (Miyao and Tsujii, 2008) is a deep
parser based on the Head-Driven Phrase Struc-
ture Grammar (HPSG) formalism. Enju analyses
its input in terms of phrase structure trees with
predicate-argument structure links, represented in
a specialised XML-format. To make the analyses
of the parser more accessible to participants, we
converted its output into the Penn Treebank (PTB)
format using tools included with the parser. The
use of the PTB format also allow for its output to
be exchanged freely for that of the other two syn-
tactic parsers and facilitates further conversions
into dependency representations.
McCCJ The BLLIP Parser (Charniak and John-
son, 2005), also variously known as the Charniak
parser, the Charniak-Johnson parser, or the Brown
reranking parser, has been applied in numerous
biomedical domain NLP efforts, frequently using
the self-trained biomedical model of McClosky
(2010) (i.e. the McClosky-Charniak-Johnson or
McCCJ parser). The BLLIP Parser is a con-
stituency (phrase structure) parser and the applied
model produces PTB analyses as its native out-
put. These analyses were made available to par-
ticipants without modification.
Stanford The Stanford Parser (Klein and Man-
ning, 2002) is a widely used publicly available
syntactic parser. As for the Enju and BLLIP
parsers, a model trained on a dataset incorporating
biomedical domain annotations is available also
for the Stanford parser. Like the BLLIP parser,
the Stanford parser is constituency-based and pro-
duces PTB analyses, which were provided to task
participants. The Stanford tools additionally in-
corporate methods for automatic conversion from
this format to other representations, discussed fur-
ther below.
4.2 Pre-processing and Conversions
To create the syntactic analyses from the Enju,
BLLIP and Stanford Parser systems, we first ap-
plied a uniform set of pre-processing steps in order
to normalise over differences in e.g. tokenisation
and thus ensure that the task participants can eas-
ily swap the output of one system for another. This
pre-processing was identical to that applied in the
BioNLP 2011 Shared Task, and included sentence
splitting of the annotated texts using the Genia
Sentence Splitter,4 the application of a set of post-
processing heuristics to correct frequently occur-
ring sentence splitting errors, and Genia Treebank-
like tokenisation (Tateisi et al, 2004) using a to-
kenisation script created by the shared task organ-
isers. 5
Since several studies have indicated that repre-
sentations of syntax and aspects of syntactic de-
pendency formalism differ in their applicability to
support information extraction tasks (Buyko and
Hahn, 2010; Miwa et al, 2010; Quirk et al, 2011),
we further converted the output of each of the
parsers from the PTB representation into three
other representations: CoNNL-X, Stanford De-
pendencies and Stanford Collapsed Dependencies.
For the CoNLL-X format we employed the con-
version tool of Johansson and Nugues (2007), and
for the two Stanford Dependency variants we used
the converter provided with the Stanford CoreNLP
tools (de Marneffe et al, 2006). These analyses
were provided to participants in the output for-
mats created by the respective tools, i.e. the TAB-
separated column-oriented format CoNLL and the
custom text-based format of the Stanford Depen-
dencies.
5 Results and Discussion
Just like in previous years the supporting resources
were well-received by the shared task participants
and as many as five participating teams mentioned
utilising the supporting resources in their initial
submissions (at the time of writing, the camera-
ready versions were not yet available). This level
of usage of the supporting resources by the partici-
pants is thus comparable to what was observed for
the 2011 shared task.
Following in the tradition of the 2011 support-
4https://github.com/ninjin/geniass
5https://github.com/ninjin/bionlp_
st_2013_supporting/blob/master/tls/
GTB-tokenize.pl
101
ing resources, to aim for reproducibility, the pro-
cessing pipeline containing pre/post-processing
and conversion scripts for all the syntactic parses
has been made publicly available under an open
licence.6 The repository containing the pipeline
also contains detailed instructions on how to re-
produce the output and how it can potentially be
applied to other texts.
Given the experience of the organisers in
analysing medium-sized corpora with a variety of
syntactic parsers, many applied repeatedly over
several years, we are also happy to report that the
robustness of several publicly available parsers has
recently improved noticeably. Random crashes,
corrupt outputs and similar failures appear to be
transitioning from being expected to rare occur-
rences.
In this paper, we have introduced the supporting
resources provided for the BioNLP 2013 Shared
Task by the task organisers and external groups.
These resources included both syntactic and se-
mantic annotations and were provided to allow the
participants to focus on the various novel chal-
lenges of constructing event extraction systems by
minimizing the need for each group to separately
perform standard processing steps such as syntac-
tic analysis.
Acknowledgements
We would like to give special thanks to Richard
Johansson for providing and allowing us to dis-
tribute an improved and updated version of his for-
mat conversion tool.7 We would also like to ex-
press our appreciation to the broader NLP com-
munity for their continued efforts to improve the
availability of both code and data, thus enabling
other researchers to stand on the shoulders of gi-
ants.
This work was partially supported by the
Quaero programme funded by OSEO (the French
agency for innovation). The research of Donald
C. Comeau, Rezarta Islamaj Dog?an, Haibin Liu
and W. John Wilbur was supported by the Intra-
mural Research Program of the National Institutes
of Health (NIH), National Library of Medicine
(NLM).
6https://github.com/ninjin/bionlp_st_
2013_supporting
7https://github.com/ninjin/
pennconverter
References
Sophie Aubin and Thierry Hamon. 2006. Improving
term extraction with terminological resources. In
Advances in Natural Language Processing, pages
380?387. Springer.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012.
University of Turku in the BioNLP?11 Shared Task.
BMC Bioinformatics, 13(Suppl 11):S4.
Ekaterina Buyko and Udo Hahn. 2010. Evaluating
the Impact of Alternative Dependency Graph Encod-
ings on Solving Event Extraction Tasks. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 982?992,
Cambridge, MA, October.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Donald C. Comeau, Rezarta Islamaj Dog?an, Paolo Ci-
ccarese, Kevin Bretonnel Cohen, Martin Krallinger,
Florian Leitner, Zhiyong Lu, Yifan Peng, Fabio Ri-
naldi, Manabu Torii, Alfonso Valencia, Karin Ver-
spoor, Thomas C. Wiegers, Cathy H. Wu, and
W. John Wilbur. 2013. BioC: A minimalist ap-
proach to interoperability for biomedical text pro-
cessing. submitted.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. Double layered learning for bio-
logical event extraction from text. In Proceedings
of the BioNLP Shared Task 2011 Workshop, pages
153?154. Association for Computational Linguis-
tics.
Wiktoria Golik, Robert Bossy, Zorana Ratkovic, and
Claire Ne?dellec. 2013. Improving Term Extraction
with Linguistic Analysis in the Biomedical Domain.
In Special Issue of the journal Research in Comput-
ing Science, Samos, Greece, March. 14th Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proc. of the 16th Nordic Conference
on Computational Linguistics (NODALIDA), pages
105?112.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop Com-
panion Volume for Shared Task, pages 1?9, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
102
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of Genia Event
Task in BioNLP Shared Task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop.
Dan Klein and Christopher D Manning. 2002. Fast ex-
act inference with a factored model for natural lan-
guage parsing. Advances in neural information pro-
cessing systems, 15(2003):3?10.
Haibin Liu, Tom Christiansen, William Baumgartner,
and Karin Verspoor. 2012. BioLemmatizer: a
lemmatization tool for morphological processing of
biomedical text. Journal of Biomedical Semantics,
3(1):3.
David McClosky, Mihai Surdeanu, and Christopher D
Manning. 2011. Event Extraction as Dependency
Parsing for BioNLP 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 41?45.
Association for Computational Linguistics.
David McClosky. 2010. Any domain parsing: Auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Brown University.
Liam R McGrath, Kelly Domico, Courtney D Cor-
ley, and Bobbie-Jo Webb-Robertson. 2011. Com-
plex biological event extraction from full text us-
ing signatures of linguistic and semantic features.
In Proceedings of the BioNLP Shared Task 2011
Workshop, pages 130?137. Association for Compu-
tational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating Dependency Rep-
resentations for Event Extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 779?787,
Beijing, China, August.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Nhung TH Nguyen and Yoshimasa Tsuruoka. 2011.
Extracting bacteria biotopes with semi-supervised
named entity recognition and coreference resolution.
In Proceedings of the BioNLP Shared Task 2011
Workshop, pages 94?101. Association for Compu-
tational Linguistics.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. MSR-NLP Entry in
BioNLP Shared Task 2011. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 155?
163, Portland, Oregon, USA, June.
Larry Smith, Thomas Rindflesch, and W. John Wilbur.
2004. MedPost: a part-of-speech tagger for bio
medical text. Bioinformatics, 20(14):2320?2321.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo,
Tomoko Ohta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011. BioNLP Shared Task 2011: Supporting Re-
sources. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 112?120, Portland, Oregon,
USA, June.
Pontus Stenetorp, Sampo Pyysalo, Tomoko Ohta,
Sophia Ananiadou, and Jun?ichi Tsujii. 2012.
Bridging the gap between scope-based and event-
based negation/speculation annotations: a bridge not
too far. In Proceedings of the Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics, pages 47?56. Association for Computa-
tional Linguistics.
Y Tateisi, T Ohta, and J Tsujii. 2004. Annotation of
predicate-argument structure on molecular biology
text. Proceedings of the Workshop on the 1st In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-04).
Sofie Van Landeghem, Kai Hakala, Samuel Ro?nnqvist,
Tapio Salakoski, Yves Van de Peer, and Filip Gin-
ter. 2012. Exploring biomolecular literature with
EVEX: connecting genes through events, homology,
and indirect associations. Advances in Bioinformat-
ics, 2012.
Sofie Van Landeghem, Jari Bjo?rne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, et al 2013. Large-scale event extrac-
tion from literature with multi-level gene normaliza-
tion. PloS one, 8(4):e55814.
Andreas Vlachos and Mark Craven. 2012. Biomedical
event extraction from abstracts and full papers using
search-based structured prediction. BMC Bioinfor-
matics, 13(Suppl 11):S5.
Andreas Vlachos. 2012. An investigation of imita-
tion learning algorithms for structured prediction. In
Workshop on Reinforcement Learning, page 143.
103
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 90?95,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Reducing VSM data sparseness by generalizing contexts:
application to health text mining
Amandine P
?
erinet
INSERM, U1142, LIMICS, Paris, France
Sorbonne Universit?es, UPMC Univ Paris 06, Paris, France
Universit?e Paris 13, Sorbonne Paris Cit?e, Villetaneuse, France
amandine.perinet@edu.univ-paris13.fr
Thierry Hamon
LIMSI-CNRS, Orsay, France
Universit?e Paris 13, Sorbonne Paris Cit?e
Villetaneuse, France
hamon@limsi.fr
Abstract
Vector Space Models are limited with low
frequency words due to few available con-
texts and data sparseness. To tackle this
problem, we generalize contexts by inte-
grating semantic relations acquired with
linguistic approaches. We use three meth-
ods that acquire hypernymy relations on a
EHR corpus. Context Generalization ob-
tains the best results when performed with
hypernyms, the quality of the relations be-
ing more important than the quantity.
1 Introduction
Distributional Analysis (DA) (Harris, 1954; Firth,
1957) computes a similarity between target words
from the contexts shared by those two words.
This hypothesis is applied with geometric meth-
ods, such as the Vector Space Model (VSM) (Tur-
ney and Pantel, 2010). The advantage of the VSM
is that the similarity of word meaning can be easily
quantified by measuring their distance in the vec-
tor space, or the cosine of the angle between them
(Mitchell and Lapata, 2010). On the other hand,
a major inconvenience is data sparseness within
the matrix that represents the vector space (Tur-
ney and Pantel, 2010). The data sparseness prob-
lem is the consequence of the word distribution in
a corpus (Baroni et al., 2009): in any corpus, most
of the words have a very low frequency and ap-
pear only a few times. Thus, those words have a
limited set of contexts and similarity is difficult to
catch. Thus, methods based on DA perform better
when more information is available (Weeds and
Weir, 2005; van der Plas, 2008) and are efficient
with large corpora of general language. But with
specialized texts, as EHR texts that are usually of
smaller size, reducing data sparseness is a major
issue and methods need to be adapted.
Semantic grouping of contexts should decrease
their diversity, and thus increase the frequency of
the remaining generalized contexts. We assume
that generalizing contexts may influence the distri-
butional context frequencies. Information for gen-
eralization can be issued from existing resources
or can be computed by linguistic approaches. In
this paper, we propose to use semantic relations
acquired by relation acquisition methods to group
words in contexts. We define a method that
switches words in DA contexts for their hierar-
chical parent or morphosyntactic variant that have
been computed on the corpus with linguistic ap-
proaches before applying the VSM method.
In the following, we first present the related
work, then our method and we finally describe the
different experiments we led. The results obtained
on the EHR corpus are then evaluated in terms of
precision and MAP, and analyzed.
2 Related work
Our approach relates with works that influence
distributional contexts to improve the performance
of VSMs. Some of them intend to change the
way to consider contexts; Broda et al. (2009) do
not use the raw context frequency in DA, but they
first rank contexts according to their frequency,
and take the rank into account. Other models
use statistical language models to determine the
most likely substitutes to represent the contexts
(Baskaya et al., 2013). They assign probabilities to
arbitrary sequences of words that are then used to
create word pairs to feed a co-occurrence model,
before performing a clustered algorithm (Yuret,
2012). The limit of such methods is that their per-
formance is proportional to vocabulary size and re-
quires the availability of training data.
Influence on contexts may also be performed by
embedding additional semantic information. The
semantic relations may be issued from an exist-
ing resource or automatically computed. With
a method based on bootstrapping, Zhitomirsky-
Geffet and Dagan (2009) modify the weights of
90
the elements in contexts relying on the seman-
tic neighbors found with a distributional similar-
ity measure. Based on this work, Ferret (2013)
uses a set of examples selected from an origi-
nal distributional thesaurus to train a supervised
classifier. This classifier is then applied for
reranking the neighbors of the thesaurus selec-
tion. Within Vector Space Model, Tsatsaronis and
Panagiotopoulou (2009) use a word thesaurus to
interpret the orthogonality of terms and measure
semantic relatedness.
With the same purpose of solving the problem of
data sparseness, other methods are based on di-
mensionality reduction, such as Latent Semantic
Analysis (LSA) in (Pad?o and Lapata, 2007) or
Non-negative Matrix Factorization (NMF) (Zheng
et al., 2011). Matrix decomposition techniques are
usually applied to reduce the dimensionality of the
original matrix, thereby rendering it more infor-
mative (Mitchell and Lapata, 2010).
Our approach differs from the aforementioned
ones in that we add semantic information in con-
texts to reduce the number of contexts and to in-
crease their frequency. Contrary to these latter ap-
proaches, we do not reduce the contexts by remov-
ing information but by generalyzing information
and integrating extra semantic knowledge.
3 VSM and context generalization
The contexts in which occurs a target word have
associated frequencies which may be used to form
probability estimates. The goal of our method is
to influence the distributional context frequencies
by generalizing contexts.
Step 1: target and context definition During
this step, we define targets and contexts, with dif-
ferent constraints for their extraction. To adapt
our method to specialized texts, we identify terms
(specific terminological entities that denote an
event) with a term extractor (Y
A
T
E
A (Aubin and
Hamon, 2006)). Target words are both nouns
and terms (T). Their distributional contexts corre-
spond to a graphical window of n number of words
around the targets (Wilks et al., 1990; Sch?utze,
1998; Lund and Burgess, 1996). We consider two
different window sizes defined in section 4.
Linguistic approaches During the generaliza-
tion process, we use three existing linguistic ap-
proaches: two that acquire hypernymy relations
and one to get morphosyntactic variants. Lexico-
syntactic Patterns (LSP) acquire hypernymy re-
lations. We use the patterns defined by (Hearst,
1992). Lexical Inclusion (LI) acquires hypernymy
relations and uses the syntactic analysis of the
terms. Based on the hypothesis that if a term is
lexically included in another, generally there is a
hypernymy relation between the two terms (kid-
ney transplant - cadaveric kidney transplant) (Bo-
denreider et al., 2001). Terminological Variation
(TV) acquires both hypernyms and synonyms. TV
uses rules that define a morpho-syntactic transfor-
mation, mainly the insertion (blood transfusion -
blood cell transfusion (Jacquemin, 1996).
Step 2: context generalization Once targets
and contexts are defined, we generalize contexts
with the relations acquired by the three linguis-
tic approaches we mentioned. To integrate the
relations in contexts, we replace words in con-
text by their hypernym or morphosyntactic variant.
We define two rules: (1) if the context matches
with one hypernym, context is replaced by this
hypernym. (2) if the context matches with sev-
eral hypernyms or variants, we take the hypernym
or variant frequency into account, and choose the
most frequent hypernym/variant. The generaliza-
tion step is individually or sequentially performed
when several relation sets are available.
Step 3: computation of semantic similarity
After the generalization step, similarity between
target words is computed. As we previously de-
crease diversity in contexts, we choose a mea-
sure that favors words appearing in similar con-
texts. We use the Jaccard Index (Grefenstette,
1994) which normalizes the number of contexts
shared by two words by the total number of con-
texts of those two words.
Parameter: thresholds The huge number of re-
lations we obtain after computing similarity be-
tween targets leads us to remove the supposed
wrong relations with three thresholds: (i) number
of shared lemmatized contexts (2 for a large win-
dow, 1 for a small window) ; (ii) number of the
lemmatized contexts (2 for a large window, 1 for
a small window) ; (iii) number of the lemmatized
targets (3 for both window sizes). For each pa-
rameter, the threshold is automatically computed,
according to the corpus, as the mean of the values
of parameters on the corpus. And we experiment
two thresholds on similarity score we empirically
defined : sim > 0.001 and sim > 0.0005.
91
4 Experiments
In this section, we present the material we use for
the experiments and evaluation, and the distribu-
tional parameter values of the VSM automatically
determined from the data. We then describe the
generalization sets we experiment and the evalua-
tion measures we used for evaluation.
4.1 Corpus
We use the collection of anonymous clinical En-
glish texts provided by the 2012 i2b2/VA chal-
lenge (Sun et al., 2013).
The corpus is pre-processed within the Ogmios
platform (Hamon et al., 2007). We perform mor-
phosyntactic tagging and lemmatization with Tree
Tagger (Schmid, 1994), and term extraction with
Y
A
T
E
A (Aubin and Hamon, 2006).
4.2 Distributional parameters
We consider two window sizes: a large window
of 21 words (? 10 words, centered on the tar-
get, henceforth W21) and a narrow one of 5 words
(? 2 words, centered on the target, W5).
The window size influences on the type, the
volume and the quality of the acquired relations.
Generally, the smaller windows allow to acquire
more relevant contexts for a target, but increase
the data sparseness problem (Rapp, 2003). They
give better results for classical types of relations
(eg. synonymy), whereas larger windows are more
appropriate for domain relations (eg. colloca-
tions)(Sahlgren, 2006; Peirsman et al., 2008).
4.3 Generalizing distributional contexts
We define several sets of context generalization.
We experiment in step 2 different ways of gener-
alizing contexts. We use as a baseline the VSM
without any generalization in the contexts (VS-
Monly), and compare the generalization sets to it.
Regarding context generalization, we first ex-
ploit the relations acquired from only one linguis-
tic approach. We apply the method described
at the section 3 (step 2) by separately using the
three different sets of relations automatically ac-
quired. Distributional contexts are replaced by
their hypernym acquired with lexico-syntactic pat-
terns (VSM/LSP) and lexical inclusion (VSM/LI),
and by their morphosyntactic variants acquired
with terminological variation (VSM/TV). Then,
we replace contexts with relations acquired by two
approaches (TV then LI, LSP then TV, etc.). This
generalization is done sequentially: we generalize
all the contexts with the relations acquired by one
method (e.g. LI), and then with the relations ac-
quired by another method (e.g. TV). And finally,
similarly to what we perform with two methods,
we experiment the generalization of contexts by
relations acquired with the three different linguis-
tic approaches (e.g. LSP then LI then TV). We
experiment all the possible combinations. With
both the single and multiple generalization, we
aim at evaluating the contribution of each method
but also the impact of the order of the methods.
4.4 Evaluation
In order to evaluate the quality of the acquired re-
lations, we compare our relations to the 53,203
UMLS relations between terms occurring in our
EHR corpus. We perform the evaluation with
the Mean Average Precision (MAP) (Buckley and
Voorhees, 2005) and the macro-precision com-
puted for each target word: semantic neighbors
found in the resource by the total semantic neigh-
bors acquired by our method. We consider three
sets of neighbors: precision after examining 1
(P@1), 5 (P@5) and 10 (P@10) neighbors.
5 Results and discussion
Best results are obtained with a large window of
21 words, with a precision P@1 of 0.243 against
0.032 for a 5 word window, both for VSMonly,
with a threshold of 0.001. Thus, a high thresh-
old on the similarity score is not always relevant.
We observe on this corpus that the generalization
with the several linguistic approaches does not im-
prove the results. For instance, VSM/LI obtains
0.250 of P@1 with a > 0.001 threshold, and this
precision is the same with VSM/LI+TV and with
VSM/LI+LSP. This is an interesting behavior, dif-
ferent from what have been observed so far on
more general French corpora that contains cook-
ing recipes (P?erinet and Hamon, 2013).
We discuss here the results we obtain for terms,
for the two thresholds on the similarity score: a
low and a higher thresholds, with relations with a
similarity above 0.0005 and above 0.001. We ob-
serve that with a higher threshold, the precision is
higher, with a P@1 of 0.243 against 0.187 for the
lower threshold (when considering VSMonly). As
for the number of relations acquired, with a lower
threshold we obtain more relations (3,936 rela-
tions acquired for the baseline) than with a higher
92
threshold (326 relations for the baseline).
We evaluate precison after examining three
groups of neighbors. The best results are ob-
tained with P@1, and in most cases, precision de-
creases when we consider more neighbors: the
more neighbors we consider, the lower precision
is. For a 0.001 threshold, the generalized experi-
ment sets obtain a higher precision than VSMonly,
in any case. While for a 0005 threshold, the use of
LI to generalize contexts decreases the precision.
We also observe that when considering generali-
sation with TV or LSP only, or their combination,
the P@10 is slightly better than P@5.
The MAP values are higher when the thresold
on the similarity measure is low, with 0.446 for
VSM/LI against 0.089 with the > 0.001 thresh-
old. It means that some correct relations are not
well ranked with the similarity score, but are still
present. We observe that the MAP values are
always higher with the generalization sets than
with the baseline with both thresholds: 0.089 for
VSM/LI, 0.446 for VSM/LI+LSP, etc.
Comparison of the experimental sets When
considering the relations found in the UMLS, we
observe that the generalization with LSP brings
the same relations that the baseline VSMonly plus
22 relations, the generalization with TV brings 16
more relations that VSMonly, and finally that the
generalization with LI decreases the number of re-
lations acquired. When the generalization of the
contexts is performed with LI, only with LI or with
LI combined to another method, it decreases the
number of relations acquired as well as the num-
ber of relations found in the resource. On the con-
trary, generalizing contexts with LSP increases the
number of relations acquired as well as the num-
ber of relations found in the UMLS resource. We
obtain the highest number of relations when gener-
alizing contexts with LSP, with 454 relations, and
the highest precision with 0.273 for P@1.
Comparing those results with the relations ac-
quired with the linguistic approaches on the EHR
corpus shows a correlation between the quality of
the relations acquired with the generalized sets and
the relations used for generalization. Indeed, LI
gives the highest number of relations with 14,437
relations, then TV gives 631 relations, and fi-
nally LSP acquires only three relations: pancre-
atic complication - necrosis, pancreatic complica-
tion - abscess, gentle laxative - milk of magnesia.
With these relations, if the second term (eg.
necrosis) is found in the context, it is replaced by
the first term (eg. pancreatic complication). These
three relations used for generalization give better
results in terms of precision that the many relations
given by the two other approaches. We could de-
duce that the number of relations may not be as im-
portant as their quality when they are used for gen-
eralization. But when the LSP are used after TV or
LI, they do not improve the results. From this ob-
servation, we make the hypothesis that these sec-
ond terms may have already been replaced during
the generalization with LI or TV. To confirm or re-
ject this hypothesis, we look closer to the relations
acquired with TV and LI. In TV, we find no rela-
tion including any of these second terms. On the
contrary, with LI, we found the relation milk - milk
of magnesia that inhibits one of the three relations
acquired with the LSP.
We deduce that even if the quality of the re-
lations used for generalization is more important
than their number, the number of relations still
matters. If generalization is first performed with
a great number of relations, then a small number
of relations used for generalization is not enough
and does not improve the results.
6 Conclusion and perspectives
In this work, we face the problem of data sparese-
ness of distributional methods. This problem espe-
cially arises from specialized corpora which have
a smaller size and in which words and terms have
lower frequencies.
To achieve this goal, we propose to generalize
distributional contexts with hypernyms and vari-
ants acquired by three existing approaches. We fo-
cus on the acquistion of relations between terms.
We experimented several generalization sets, us-
ing one, two or the three methods sequentially
to replace words in context by their hypernym or
variant. Evaluation of the method has been per-
formed on an EHR English text collection. Gen-
eralization obtains the best results when realized
with hypernyms. The quality of the relations mat-
ters much more than their number: few but good
relations used to generalize contexts give better re-
sults than many relations of poorer quality. For
future work, we plan to use for generalization re-
lations issued from different distributional and ter-
minological resources. Finally, we will intend to
combine the methods before normalization.
93
References
Sophie Aubin and Thierry Hamon. 2006. Improving
term extraction with terminological resources. In
Advances in Natural Language Processing, number
4139 in LNAI, pages 380?387. Springer.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web:
a collection of very large linguistically processed
web-crawled corpora. Language Resources and
Evaluation, 43(3):209?226.
Osman Baskaya, Enis Sert, Volkan Cirik, and Deniz
Yuret. 2013. Ai-ku: Using substitute vectors and
co-occurrence modeling for word sense induction
and disambiguation. In Proceedings of SemEval -
2013, pages 300?306, Atlanta, Georgia, USA. As-
sociation for Computational Linguistics.
Olivier Bodenreider, Anita Burgun, and Thomas Rind-
flesch. 2001. Lexically-suggested hyponymic rela-
tions among medical terms and their representation
in the umls. In TIA 2001, pages 11?21.
Bartosz Broda, Maciej Piasecki, and Stan Szpakowicz.
2009. Rank-based transformation in measuring se-
mantic relatedness. In Yong Gao and Nathalie Jap-
kowicz, editors, Canadian Conference on AI, vol-
ume 5549, pages 187?190. Springer.
Chris Buckley and Ellen Voorhees. 2005. Retrieval
system evaluation. In Ellen Voorhees and Donna
Harman, editors, TREC: Experiment and Evaluation
in Information Retrieval, chapter 3. MIT Press.
Olivier Ferret. 2013. S?election non supervis?ee de
relations s?emantiques pour am?eliorer un th?esaurus
distributionnel. In TALN 2013, pages 48?61, Les
Sables d?Olonne, France.
J.R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in linguistic analysis, pages 1?32.
Gregory Grefenstette. 1994. Corpus-derived first, sec-
ond and third-order word affinities. In Sixth Euralex
International Congress, pages 279?290.
T. Hamon, A. Nazarenko, T. Poibeau, S. Aubin, and
J. Derivi`ere. 2007. A robust linguistic platform for
efficient and domain specific web content analysis.
In RIAO 2007, Pittsburgh, USA.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In International
Conference on Computational Linguistics, pages
539?545, Nantes, France.
Christian Jacquemin. 1996. A symbolic and surgi-
cal acquisition of terms through variation. In CoRR,
pages 425?438.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
mentation, and Computers, 28:203?208.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Comput. Linguist., 33(2):161?199.
Yves Peirsman, Heylen Kris, and Geeraerts Dirk.
2008. Size matters. tight and loose context def-
initions in english word space models. In ESS-
LLI Workshop on Distributional Lexical Semantics,
Hamburg, Germany.
Amandine P?erinet and Thierry Hamon. 2013. Hybrid
acquisition of semantic relations based on context
normalization in distributional analysis. In Proceed-
ings of TIA 2013, pages 113?120, Paris, France.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In MT Summit?2003,
pages 315?322.
Magnus Sahlgren. 2006. The Word-Space Model:
Using Distributional Analysis to Represent Syntag-
matic and Paradigmatic Relations between Words
in High-Dimensional Vector Spaces. Ph.D. thesis,
Stockholm University, Stockholm, Sweden.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In New Methods in Language
Processing, pages 44?49, Manchester, UK.
Hinrich Sch?utze. 1998. Automatic word sense dis-
crimination. Comput. Linguist., 24(1):97?123.
Weiyi Sun, Anna Rumshisky, and
?
Ozlem Uzuner.
2013. Evaluating temporal relations in clinical text:
2012 i2b2 challenge. JAMIA, 20(5):806?813.
George Tsatsaronis and Vicky Panagiotopoulou. 2009.
A generalized vector space model for text retrieval
based on semantic relatedness. In EACL 2009,
pages 70?78, Stroudsburg, PA, USA.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. JAIR, 37:141?188.
Lonneke van der Plas. 2008. Automatic lexico-
semantic acquisition for question answering. Th`ese
de doctorat, University of Groningen, Groningen.
Julie Weeds and David Weir. 2005. Co-occurrence
retrieval: A flexible framework for lexical distribu-
tional similarity. Comput. Linguist., 31(4):439?475.
Yorick A. Wilks, Dan, James E. Mcdonald, Tony
Plate, and Brian M. Slator. 1990. Providing ma-
chine tractable dictionary tools. Journal of Machine
Translation, 2.
94
Deniz Yuret. 2012. Fastsubs: An efficient and exact
procedure for finding the most likely lexical substi-
tutes based on an n-gram language model. IEEE
Signal Process. Lett., 19(11):725?728.
Wenbin Zheng, Yuntao Qian, and Hong Tang. 2011.
Dimensionality reduction with category information
fusion and non-negative matrix factorization for text
categorization. In Hepu Deng, Duoqian Miao, Jing-
sheng Lei, and Fu Lee Wang, editors, AICI, volume
7004 of LNCS, pages 505?512.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009.
Bootstrapping distributional feature vector quality.
Comput. Linguist., 35(3):435?461.
95
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 101?105,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Tuning HeidelTime for identifying time expressions in clinical texts in
English and French
Thierry Hamon
LIMSI-CNRS, BP133, Orsay
Universit?e Paris 13
Sorbonne Paris Cit?e, France
hamon@limsi.fr
Natalia Grabar
CNRS UMR 8163 STL
Universit?e Lille 3
59653 Villeneuve d?Ascq, France
natalia.grabar@univ-lille3.fr
Abstract
We present work on tuning the Heideltime
system for identifying time expressions in
clinical texts in English and French lan-
guages. The main amount of the method
is related to the enrichment and adap-
tation of linguistic resources to identify
Timex3 clinical expressions and to nor-
malize them. The test of the adapted ver-
sions have been done on the i2b2/VA 2012
corpus for English and a collection of clin-
ical texts for French, which have been an-
notated for the purpose of this study. We
achieve a 0.8500 F-measure on the recog-
nition and normalization of temporal ex-
pressions in English, and up to 0.9431 in
French. Future work will allow to improve
and consolidate the results.
1 Introduction
Working with unstructured narrative texts is very
demanding on automatic methods to access, for-
malize and organize the information contained in
these documents. The first step is the indexing of
the documents in order to detect basic facts which
will allow more sophisticated treatments (e.g., in-
formation extraction, question/answering, visual-
ization, or textual entailment). We are mostly in-
terested in indexing of documents from the med-
ical field. We distinguish two kinds of indexing:
conceptual and contextual.
Conceptual indexing consists in finding out the
mentions of notions, terms or concepts contained
in documents. It is traditionally done thanks to
the exploitation of terminological resources, such
as MeSH (NLM, 2001), SNOMED International
(C?ot?e et al., 1993), SNOMED CT (Wang et al.,
2002), etc. The process is dedicated to the recog-
nition of these terms and of their variants in doc-
uments (Nadkarni et al., 2001; Mercer and Di
Marco, 2004; Bashyam and Taira, 2006; Schulz
and Hahn, 2000; Davis et al., 2006).
The purpose of contextual indexing is to go fur-
ther and to provide a more fine-grained annota-
tion of documents. For this, additional informa-
tion may be searched in documents, such as polar-
ity, certainty, aspect or temporality related to the
concepts. If conceptual indexing extracts and pro-
vides factual information, contextual indexing is
aimed to describe these facts with more details.
For instance, when processing clinical records, the
medical facts related to a given patient can be aug-
mented with the associated contextual informa-
tion, such as in these examples:
(1) Patient has the stomach aches.
(2) Patient denies the stomach aches.
(3) After taking this medication, patient
started to have the stomach aches.
(4) Two weeks ago, patient experienced the
stomach aches.
(5) In January 2014, patient experienced the
stomach aches.
In example (1), the information is purely fac-
tual, while it is negated in example (2). Example
(3) conveys also aspectual information (the med-
ical problem has started). In examples (4) and
(5), medical events are positioned in the time: rel-
ative (two weeks ago) and absolute (in January
2014). We can see that the medical history of pa-
tient can become more precise and detailed thanks
to such contextual information. In this way, fac-
tual information related to the stomach aches of
patient may receive these additional descriptions
which make each occurrence different and non-
redundant. Notice that the previous I2B2 contests
1
addressed the information extraction tasks related
to different kinds of contextual information.
1
https://www.i2b2.org/NLP
101
Temporality has become an important research
field in the NLP topics and several challenges ad-
dressed this taks: ACE (ACE challenge, 2004),
SemEval (Verhagen et al., 2007; Verhagen et al.,
2010; UzZaman et al., 2013), I2B2 2012 (Sun
et al., 2013). We propose to continue working
on the extraction of temporal information related
to medical events. This kind of study relies on
several important tasks when processing the nar-
rative documents : identification and normaliza-
tion of linguistic expressions that are indicative of
the temporality (Verhagen et al., 2007; Chang and
Manning, 2012; Str?otgen and Gertz, 2012; Kessler
et al., 2012), and their modelization and chain-
ing (Batal et al., 2009; Moskovitch and Shahar,
2009; Pustejovsky et al., 2010; Sun et al., 2013;
Grouin et al., 2013). The identification of tempo-
ral expressions provides basic knowledge for other
tasks processing the temporality information. The
existing available automatic systems such as Hei-
delTime (Str?otgen and Gertz, 2012) or SUTIME
(Chang and Manning, 2012) exploit rule-based
approaches, which makes them adaptable to new
data and areas. During a preliminary study, we
tested several such systems for identification of
temporal relations and found that HeidelTime has
the best combination of performance and adapt-
ability. We propose to exploit this automatic sys-
tems, to adapt and to test it on the medical clinical
documents in two languages (English and French).
In the following of this study, we introduce
the corpora (Section 2) and methods (Section 3).
We then describe and discuss the obtained results
(Section 4.2) and conclude (Section 5).
2 Material
Corpora composed of training and test sets are the
main material we work with. The corpora are in
two languages, English and French, and has com-
parable sizes. All the processed corpora are de-
identified. Corpora in English are built within the
I2B2 2012 challenge (Sun et al., 2013). The train-
ing corpus consists of 190 clinical records and the
test corpus of 120 records. The reference data con-
tain annotations of temporal expressions accord-
ing to the Timex3s guidelines: date, duration, fre-
quency and time (Pustejovsky et al., 2010). Cor-
pora in French are built on purpose of this study.
The clinical documents are issued from a French
hospital. The training corpus consists of 182 clin-
ical records and the test corpus of 120 records. 25
documents from the test set are annotated to pro-
vide the reference data for evaluation.
3 Method
HeidelTime is a cross-domain temporal tagger that
extracts temporal expressions from documents and
normalizes them according to the Timex3 anno-
tation standard, which is part of the markup lan-
guage TimeML (Pustejovsky et al., 2010). This
is a rule-based system. Because the source code
and the resources (patterns, normalization infor-
mation, and rules) are strictly separated, it is pos-
sible to develop and implement resources for ad-
ditional languages and areas using HeidelTime?s
rule syntax. HeidelTime is provided with modules
for processing documents in several languages,
e.g. French (Moriceau and Tannier, 2014). In En-
glish, several versions of the system exist, such as
general-language English and scientific English.
HeidelTime uses different normalization strate-
gies depending on the domain of the documents
that are to be processed: news, narratives (e.g.
Wikipedia articles), colloquial (e.g. SMS, tweets),
and scientific (e.g. biomedical studies). The news
strategy allows to fix the document creation date.
This date is important for computing and normal-
izing the relative dates, such as two weeks ago
or 5 days later, for which the reference point in
time is necessary: if the document creation date is
2012/03/24, two weeks ago becomes 2012/03/10.
Our method consists of three steps: tuning Hei-
delTime to clinical data in English and French
(Section 3.1), evaluation of the results (Section
3.2), and exploitation of the computed data for the
visualization of the medical events (Section 3.3).
3.1 Tuning HeidelTime
While HeidelTime proposes a good coverage of
the temporal expressions used in general language
documents, it needs to be adapted to specialized
areas. We propose to tune this tool to the medi-
cal domain documents. The tuning is done in two
languages (English and French). Tuning involves
three aspects:
1. The most important adaptation needed is re-
lated to the enrichment and encoding of lin-
guistic expressions specific to medical and
especially clinical temporal expressions, such
as post-operative day #, b.i.d. meaning twice
a day, day of life, etc.
102
2. The admission date is considered as the refer-
ence or starting point for computing relative
dates, such as 2 days later. For the identi-
fication of the admission date, specific pre-
processing step is applied in order to detect it
within the documents;
3. Additional normalizations of the temporal
expressions are done for normalizing the
durations in approximate numerical values
rather than in the undefined ?X?-value; and
for external computation for some durations
and frequencies due to limitations in Heidel-
Time?s internal arithmetic processor.
3.2 Evaluating the results
HeidelTime is tuned on the training set. It is evalu-
ated on the test set. The results generated are eval-
uated against the reference data with:
? precision P: percentage of the relevant tem-
poral expressions extracted divided by the to-
tal number of the temporal expressions ex-
tracted;
? recallR: percentage of the relevant temporal
expressions extracted divided by the number
of the expected temporal expressions;
? APR: the arithmetic average of the precision
and recall values
P+R
2
;
? F-measure F : the harmonic mean of the pre-
cision and recall values
P?R
P+R
.
3.3 Exploiting the results
In order to judge about the usefulness of the tem-
poral information extracted, we exploit it to build
the timeline. For this, the medical events are asso-
ciated with normalized and absolute temporal in-
formation. This temporal information is then used
to order and visualize the medical events.
4 Experiments and Results
4.1 Experiments
The experiments performed are the following.
Data in English and French are processed. Data in
two languages are processed by available versions
of HeidelTime: two existing versions (general lan-
guage and scientific language) and the medical
version created thanks to the work performed in
this study. Results obtained are evaluated against
the reference data.
4.2 Results
We added several new rules to HeidelTime (164
in English and 47 in French) to adapt the recog-
nition of temporal expressions in medical docu-
ments. Some cases are difficult to annotate. For
instance, it is complicated to decide whether some
expressions are concerned with dates or durations.
The utterance like 2 years ago (il y a 2 ans) is
considered to indicate the date. The utterance like
since 2010 (depuis 2010) is considered to indicate
the duration, although it can be remarked that the
beginning of the duration interval marks the begin-
ning of the process and its date. Another complex
situation appears with the relative dates:
? as already mentioned, date like 2 years ago
(il y a 2 ans) are to be normalized according
to the reference time point;
? a more complex situation appears with ex-
pressions like the day of the surgery (le jour
de l?op?eration) or at the end of the treatment
by antiobiotics (`a la fin de l?antibiothrapie),
for which it is necessary first to make the ref-
erence in time of the other medical event be-
fore being able to define the date in question.
In Table 1, we present the evaluation results for
English. On the training corpus, with the general
language version and the scientific version of Hei-
delTime, we obtain F-measure around 0.66: preci-
sion (0.77 to 0.79) is higher than recall (0.56). The
values of F-measure and APR are identical. The
version we adapted to the medical language pro-
vides better results for all the evaluation measures
used: F-measure becomes then 0.84, with preci-
sion up to 0.85 and recall 0.84. This is a good im-
provement of the automatic tool which indicates
that specialized areas, such as medical area, use
indeed specific lexicon and constructions. Inter-
estingly, on the test corpus, the results decrease
for the general language and scientific versions
of HeidelTime, but increase for the medical ver-
sion of HeidelTime, with F-measure 0.85. During
the I2B2 competition, the maximal F-measure ob-
tained was 0.91. With F-measure 0.84, our system
was ranked 10/14 on the English data. Currently,
we improve these previous results.
In Table 2, we present the results obtained on
the French test corpus (26 documents). Two ver-
sions of HeidelTime are applied: general lan-
guage, that is already available, and medical, that
has been developed in the presented work. We can
103
Versions of HeidelTime Training Test
P R APR F P R APR F
general language 0.7745 0.5676 0.6551 0.6551 0.8000 0.5473 0.6499 0.6499
scientific 0.7877 0.5676 0.6598 0.6598 0.8018 0.5445 0.6486 0.6486
medical 0.8478 0.8381 0.8429 0.8429 0.8533 0.8467 0.8500 0.8500
Table 1: Results obtained on training and test sets in English.
1990 1995 2000 2005 2010
n
e
g
a
t
i
v
e
m
a
m
m
o
g
r
a
m
b
i
l
a
t
e
r
a
l
b
r
e
a
s
t
m
a
s
s
e
s
l
e
f
t
s
i
m
p
l
e
m
a
s
t
e
c
t
o
m
y
c
h
e
s
t
w
a
l
l
n
o
d
u
l
e
s
d
e
c
r
e
a
s
e
i
n
t
h
e
p
r
e
t
r
a
c
h
e
a
l
n
o
d
e
s
t
h
i
o
t
e
p
a
,
V
e
l
b
a
n
,
M
e
t
h
o
t
r
e
x
a
t
e
c
o
m
p
l
a
i
n
t
s
o
f
f
e
v
e
r
Figure 1: Visualization of temporal data.
Versions of Test
HeidelTime P R F
general language 0.9030 0.9341 0.9183
medical 0.9504 0.9341 0.9422
Table 2: Results obtained on test set in French.
observe that the adapted version suits better the
content of clinical documents and improves the F-
measure values by 3 points, reaching up to 0.94.
The main limitation of the system is due to
the incomplete coverage of the linguistic expres-
sions (e.g. au cours de, mensuel (during, monthly)).
Among the current false positives, we can find ra-
tios (2/10 is considered as date, while it means lab
results), polysemous expressions (Juillet in rue du
14 Juillet (14 Juillet street)), and segmentation errors
(few days detected instead of the next few days).
These limitations will be fixed in the future work.
In Figure 1, we propose a visualization of the
temporal data, which makes use of the temporal
information extracted. In this way, the medical
events can be ordered thanks to their temporal an-
chors, which becomes a very useful information
presentation in clinical practice (Hsu et al., 2012).
The visualization of unspecified expressions (e.g.
later, sooner) is being studied. Although it seems
that such expressions often occur with more spe-
cific expressions (e.g. later that day).
5 Conclusion
HeidelTime, an existing tool for extracting
and normalizing temporal information, has been
adapted to the medical area documents in two
languages (English and French). It is evaluated
against the reference data, which indicates that
its tuning to medical documents is efficient: we
reach F-measure 0.85 in English and up to 0.94
in French. More complete data in French are be-
ing annotated, which will allow to perform a more
complete evaluation of the tuned version. We plan
to make the tuned version of HeidelTime freely
available. Automatically extracted temporal infor-
mation can be exploited for the visualization of the
clinical data related to patients. Besides, these data
can be combined with other kinds of contextual in-
formation (polarity, uncertainty) to provide a more
exhaustive picture of medical history of patients.
Acknowledgments
This work is partially performed under the grant
ANR/DGA Tecsan (ANR-11-TECS-012). The au-
thors are thankful to the CHU de Bordeaux for
making available the clinical documents.
104
References
ACE challenge. 2004. The ACE 2004 eval-
uation plan. evaluation of the recogni-
tion of ace entities, ace relations and ace
events. Technical report, ACE challenge.
http://www.itl.nist.gov/iad/mig/tests/ace/2004.
V Bashyam and Ricky K Taira. 2006. Indexing
anatomical phrases in neuro-radiology reports to the
UMLS 2005aa. In AMIA, pages 26?30.
Iyad Batal, Lucia Sacchi, Riccardo Bellazzi, and Milos
Hauskrecht. 2009. A temporal abstraction frame-
work for classifying clinical temporal data. In AMIA
Annu Symp Proc. 2009, pages 29?33.
Angel X. Chang and Christopher D. Manning. 2012.
SUTIME: A library for recognizing and normalizing
time expressions. In LREC, pages 3735?3740.
Roger A. C?ot?e, D. J. Rothwell, J. L. Palotay, R. S.
Beckett, and Louise Brochu. 1993. The Sys-
tematised Nomenclature of Human and Veterinary
Medicine: SNOMED International. College of
American Pathologists, Northfield.
Neil Davis, Henk Harlema, Rob Gaizauskas, Yikun
Guo, Moustafa Ghanem, Tom Barnwell, Yike Guo,
and Jon Ratcliffe. 2006. Three approaches to GO-
tagging biomedical abstracts. In Udo Hahn and
Michael Poprat, editors, SMBM, pages 21 ? 28, Jena,
Germany.
Cyril Grouin, Natalia Grabar, Thierry Hamon, Sophie
Rosset, Xavier Tannier, and Pierre Zweigenbaum.
2013. Hybrid approaches to represent the clini-
cal patient?s timeline. J Am Med Inform Assoc,
20(5):820?7.
William Hsu, Ricky K Taira, Suzie El-Saden,
Hooshang Kangarloo, and Alex AT Bui. 2012.
Context-based electronic health recond: toward pa-
tient specific healthcare. IEEE Transactions on
information technology in biomedicine, 16(2):228?
234.
Remy Kessler, Xavier Tannier, Caroline Hagge,
Vronique Moriceau, and Andr Bittar. 2012. Find-
ing salient dates for building thematic timelines. In
50th Annual Meeting of the Association for Compu-
tational Linguistics, pages 730?739.
Robert E Mercer and Chrysanne Di Marco. 2004. A
design methodology for a biomedical literature in-
dexing tool using the rhetoric of science. In HLT-
NAACL 2004, Workshop Biolink, pages 77?84.
Vronique Moriceau and Xavier Tannier. 2014. French
resources for extraction and normalization of tempo-
ral expressions with heideltime. In LREC.
Robert Moskovitch and Yuval Shahar. 2009. Medical
temporal-knowledge discovery via temporal abstrac-
tion. In AMIA Annu Symp Proc, pages 452?456.
P Nadkarni, R Chen, and C Brandt. 2001. Umls con-
cept indexing for production databases: a feasibility
study. J Am Med Inform Assoc, 8(1):80?91.
National Library of Medicine, Bethesda, Mary-
land, 2001. Medical Subject Headings.
www.nlm.nih.gov/mesh/meshhome.html.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Lau-
rent Romary. 2010. ISO-TimeML: An interna-
tional standard for semantic annotation. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Ste-
lios Piperidis, Mike Rosner, and Daniel Tapias,
editors, Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
Stefan Schulz and Udo Hahn. 2000. Morpheme-
based, cross-lingual indexing for medical document
retrieval. Int J Med Inform, 58-59:87?99.
Jannik Str?otgen and Michael Gertz. 2012. Temporal
tagging on different domains: Challenges, strate-
gies, and gold standards. In Proceedings of the
Eigth International Conference on Language Re-
sources and Evaluation (LREC?12), pages 3746?
3753. ELRA.
Weiyi Sun, Anna Rumshisky, and
?
Ozlem Uzuner.
2013. Evaluating temporal relations in clinical text:
2012 i2b2 challenge. JAMIA, 20(5):806?813.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, James Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval tempo-
ral relation identification. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 75?80, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57?
62, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
AY Wang, JH Sable, and KA Spackman. 2002. The
snomed clinical terms development process: refine-
ment and analysis of content. In AMIA, pages 845?
9.
105
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 11?20,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Automatic diagnosis of understanding of medical words
Natalia Grabar
CNRS UMR 8163 STL
Universit?e Lille 3
59653 Villeneuve d?Ascq, France
natalia.grabar@univ-lille3.fr
Thierry Hamon
LIMSI-CNRS, BP133, Orsay
Universit?e Paris 13
Sorbonne Paris Cit?e, France
hamon@limsi.fr
Dany Amiot
CNRS UMR 8163 STL
Universit?e Lille 3
59653 Villeneuve d?Ascq, France
dany.amiot@univ-lille3.fr
Abstract
Within the medical field, very specialized
terms are commonly used, while their un-
derstanding by laymen is not always suc-
cessful. We propose to study the under-
standability of medical words by laymen.
Three annotators are involved in the cre-
ation of the reference data used for training
and testing. The features of the words may
be linguistic (i.e., number of characters,
syllables, number of morphological bases
and affixes) and extra-linguistic (i.e., their
presence in a reference lexicon, frequency
on a search engine). The automatic cate-
gorization results show between 0.806 and
0.947 F-measure values. It appears that
several features and their combinations are
relevant for the analysis of understandabil-
ity (i.e., syntactic categories, presence in
reference lexica, frequency on the general
search engine, final substring).
1 Introduction
The medical field has deeply penetrated our daily
life, which may be due to personal or family
health condition, watching TV and radio broad-
casts, reading novels and journals. Nevertheless,
the availability of this kind of information does not
guarantee its correct understanding, especially by
laymen, such as patients. The medical field has in-
deed a specific terminology (e.g., abdominoplasty,
hepatic, dermabrasion or hepatoduodenostomy)
commonly used by medical professionals. This
fact has been highlighted in several studies dedi-
cated for instance to the understanding of pharma-
ceutical labels (Patel et al., 2002), of information
provided by websites (Rudd et al., 1999; Berland
et al., 2001; McCray, 2005; Oregon Evidence-
based Practice Center, 2008), and more generally
the understanding between patients and medical
doctors (AMA, 1999; McCray, 2005; Jucks and
Bromme, 2007; Tran et al., 2009).
We propose to study the understanding of words
used in the medical field, which is the first step to-
wards the simplification of texts. Indeed, before
the simplification can be performed, it is neces-
sary to know which textual units may show under-
standing difficulty and should be simplified. We
work with data in French, such as provided by
an existing medical terminology. In the remain-
der, we present first some related work, especially
from specialized fields (section 2). We then intro-
duce the linguistic data (section 4) and methodol-
ogy (section 5) we propose to test. We present and
discuss the results (section 6), and conclude with
some directions for future work (section 7).
2 Studying the understanding of words
The understanding (of words) may be seen as a
scale going from I can understand to I cannot un-
derstand, and containing one or more intermediate
positions (i.e., I am not sure, I have seen it be-
fore but do not remember the meaning, I do not
know but can interpret). Notice that it is also re-
lated to the ability to provide correct explanation
and use of words. As we explain later, we con-
sider words out of context and use a three-position
scale. More generally, understanding is a complex
notion closely linked to several other notions stud-
ied in different research fields. For instance, lex-
ical complexity is studied in linguistics and gives
clues on lexical processes involved, that may im-
pact the word understanding (section 2.1). Work
in psycholinguistics is often oriented on study of
word opacity and the mental processes involved in
their understanding (Jarema et al., 1999; Libben et
al., 2003). Readability provides a set of methods
to compute and quantify the understandability of
words (section 2.3). The specificity of words to
specialized areas is another way to capture their
understandability (section 2.2). Finally, lexical
11
simplification aims at providing simpler words to
be used in a given context (section 2.3).
2.1 Linguistics
In linguistics, the question is closely related to lex-
ical complexity and compoundings. It has been
indeed observed that at least five factors, linguis-
tic and extra-linguistic, may be involved in the se-
mantic complexity of the compounds. One factor
is related to the knowledge of the components of
the complex words. Formal (how the words, such
as a?erenchyme, can be segmented) and seman-
tic (how the words can be understood and used)
points of view can be distinguished. A second
factor is that complexity is also due to the vari-
ety of morphological patterns and relations among
the components. For instance, ?erythrocyte (erythro-
cyte) and ovocyte (ovocyte) instantiate the [N1N2]
pattern in which N2 (cyte) can be seen as a con-
stant element (Booij, 2010), although the relations
between N1 and N2 are not of the same type in
these two compounds: in ?erythrocyte, N1 ?erythr(o)
denotes a property of N2 (color), while in ovo-
cyte, N1 ovo (egg) corresponds to a specific de-
velopment stage of female cells. Another factor
appears when some components are polysemous,
within a given field (i.e., medical field) or across
the fields. For instance, a?er(o) does not always
convey the same meaning: in a?eroc`ele, a?er- de-
notes ?air? (tumefaction (c`ele) formed by an air in-
filtration), but not in a?erasth?enie, which refers to
an asthenia (psychic disorder) observable among
jet pilots. Yet another factor may be due to the dif-
ference in the order of components: according to
whether the compounding is standard (in French,
the main semantic element is then on the left, such
as in pneu neige (snow tyre), which is fundamen-
tally a pneu (tyre)) or neoclassical (in French, the
main semantic element is then on the right, such as
?erythrocyte, which is a kind of cyte cell / corpuscle
with red color). It is indeed complicated for a user
without medical training to correctly interpret a
word that he does not know and for which he can-
not reuse the existing standard compounding pat-
terns. This difficulty is common to all Roman lan-
guages (Iacobini, 2003), but not to Germanic lan-
guages (L?udeling et al., 2002). Closely related is
the fact that with neoclassical compounds, a given
component may change its place according to the
global semantics of the compounds, such as path-
in pathology, polyneuropathe, cardiopathy. Fi-
nally, the formal similarity between some deriva-
tion processes (such as the derivation in -oide, like
in lipoid) and neoclassical compounding (such as
-ase in lipase), which apply completely different
interpretation patterns (Iacobini, 1997; Amiot and
Dal, 2005), can also make the understanding more
difficult.
2.2 Terminology
In the terminology field, the automatic identifica-
tion of difficulty of terms and words remains im-
plicit, while this notion is fundamental in termi-
nology (W?uster, 1981; Cabr?e and Estop`a, 2002;
Cabr?e, 2000). The specificity of terms to a given
field is usually studied. The notion of understand-
ability can be derived from it. Such studies can
be used for filtering the terms extracted from spe-
cialized corpora (Korkontzelos et al., 2008). The
features exploited include for instance the pres-
ence and the specificity of pivot words (Drouin
and Langlais, 2006), the neighborhood of the term
in corpus or the diversity of its components com-
puted with statistical measures such as C-Value or
PageRank (Daille, 1995; Frantzi et al., 1997; May-
nard and Ananiadou, 2000). Another possibility is
to check whether lexical units occur within refer-
ence terminologies and, if they do, they are con-
sidered to convey specialized meaning (Elhadad
and Sutaria, 2007).
2.3 NLP studies
The application of the readability measures is an-
other way to evaluate the complexity of words and
terms. Among these measures, it is possible to dis-
tinguish classical readability measures and com-
putational readability measures (Franc?ois, 2011).
Classical measures usually rely on number of let-
ters and/or of syllables a word contains and on
linear regression models (Flesch, 1948; Gunning,
1973), while computational readability measures
may involve vector models and a great variabil-
ity of features, among which the following have
been used to process the biomedical documents
and words: combination of classical readability
formulas with medical terminologies (Kokkinakis
and Toporowska Gronostaj, 2006); n-grams of
characters (Poprat et al., 2006), manually (Zheng
et al., 2002) or automatically (Borst et al., 2008)
defined weight of terms, stylistic (Grabar et al.,
2007) or discursive (Goeuriot et al., 2007) fea-
tures, lexicon (Miller et al., 2007), morphologi-
cal features (Chmielik and Grabar, 2011), combi-
12
Categories A1 (%) A2 (%) A3 (%) Unanimity (%) Majority (%)
1. I can understand 8,099 (28) 8,625 (29) 7,529 (25) 5,960 (26) 7,655 (27)
2. I am not sure 1,895 (6) 1,062 (4) 1,431 (5) 61 (0.3) 597 (2)
3. I cannot understand 19,647 (66) 19,954 (67) 20,681 (70) 16,904 (73.7) 20,511 (71)
Total annotations 29,641 29,641 29,641 22,925 28,763
Table 1: Number (and percentage) of words assigned to reference categories by three annotators (A1, A2
and A3), and in the derived datasets unanimity and majority.
nations of different features (Wang, 2006; Zeng-
Treiler et al., 2007; Leroy et al., 2008).
Specific task has been dedicated to the lexi-
cal simplification within the SemEval challenge in
2012
1
. Given a short input text and a target word
in English, and given several English substitutes
for the target word that fit the context, the goal
was to rank these substitutes according to how
?simple? they are (Specia et al., 2012). The par-
ticipants applied rule-based and/or machine learn-
ing systems. Combinations of various features
have been used: lexicon from spoken corpus
and Wikipedia, Google n-grams, WordNet (Sinha,
2012); word length, number of syllables, latent se-
mantic analysis, mutual information and word fre-
quency (Jauhar and Specia, 2012); Wikipedia fre-
quency, word length, n-grams of characters and of
words, random indexing and syntactic complexity
of documents (Johannsen et al., 2012); n-grams
and frequency from Wikipedia, Google n-grams
(Ligozat et al., 2012); WordNet and word fre-
quency (Amoia and Romanelli, 2012).
3 Aims of the present study
We propose to investigate how the understandabil-
ity of French medical words can be diagnosed with
NLP methods. We rely on the reference annota-
tions performed by French speakers without medi-
cal training, which we associate with patients. The
experiments performed rely on machine learning
algorithms and a set of 24 features. The medical
words studied are provided by an existing medical
terminology.
4 Linguistic data and their preparation
The linguistic data are obtained from the medical
terminology Snomed International (C?ot?e, 1996).
This terminology?s aim is to describe the whole
medical field. It contains 151,104 medical terms
structured into eleven semantic axes such as dis-
1
http://www.cs.york.ac.uk/semeval-2012/
orders and abnormalities, procedures, chemical
products, living organisms, anatomy, social sta-
tus, etc. We keep here five axes related to the
main medical notions (disorders, abnormalities,
procedures, functions, anatomy). The objective
is not to consider axes such as chemical products
(trisulfure d?hydrog`ene (hydrogen sulfide)) and living
organisms (Sapromyces, Acholeplasma laidlawii)
that group very specific terms hardly known by
laymen. The 104,649 selected terms are tokenized
and segmented into words (or tokens) to ob-
tain 29,641 unique words: trisulfure d?hydrog`ene
gives three words (trisulfure, de, hydrog`ene).
This dataset contains compounds (abdominoplas-
tie (abdominoplasty), dermabrasion (dermabrasion)),
constructed (cardiaque (cardiac), acineux (acinic),
lipo??de (lipoid)) and simple (acn?e (acne), fragment
(fragment)) words. These data are annotated by
three speakers 25-40 year-old, without medical
training, but with linguistic background. We ex-
pect the annotators to represent the average knowl-
edge of medical words amongst the population as
a whole. The annotators are presented with a list of
terms and asked to assign each word to one of the
three categories: (1) I can understand the word;
(2) I am not sure about the meaning of the word;
(3) I cannot understand the word. The assumption
is that the words, which are not understandable by
the annotators, are also difficult to understand by
patients. These manual annotations correspond to
the reference data (Table 1).
5 Methodology
The proposed method has two aspects: gener-
ation of the features associated to the analyzed
words and a machine learning system. The main
research question is whether the NLP methods
can distinguish between understandable and non-
understandable medical words and whether they
can diagnose these two categories.
13
5.1 Generation of the features
We exploit 24 linguistic and extra-linguistic fea-
tures related to general and specialized languages.
The features are computed automatically, and can
be grouped into ten classes:
Syntactic categories. Syntactic categories and
lemmas are computed by TreeTagger (Schmid,
1994) and then checked by Flemm (Namer, 2000).
The syntactic categories are assigned to words
within the context of their terms. If a given word
receives more than one category, the most fre-
quent one is kept as feature. Among the main
categories we find for instance nouns, adjectives,
proper names, verbs and abbreviations.
Presence of words in reference lexica. We ex-
ploit two reference lexica of the French language:
TLFi
2
and lexique.org
3
. TLFi is a dictionary of the
French language covering XIX and XX centuries.
It contains almost 100,000 entries. lexique.org is a
lexicon created for psycholinguistic experiments.
It contains over 135,000 entries, among which in-
flectional forms of verbs, adjectives and nouns. It
contains almost 35,000 lemmas.
Frequency of words through a non specialized
search engine. For each word, we query the
Google search engine in order to know its fre-
quency attested on the web.
Frequency of words in the medical terminology.
We also compute the frequency of words in the
medical terminology Snomed International.
Number and types of semantic categories asso-
ciated to words. We exploit the information on the
semantic categories of Snomed International.
Length of words in number of their characters
and syllables. For each word, we compute the
number of its characters and syllables.
Number of bases and affixes. Each lemma
is analyzed by the morphological analyzer D?erif
(Namer and Zweigenbaum, 2004), adapted to the
treatment of medical words. It performs the de-
composition of lemmas into bases and affixes
known in its database and it provides also seman-
tic explanation of the analyzed lexemes. We ex-
ploit the morphological decomposition informa-
tion (number of affixes and bases).
Initial and final substrings of the words. We
compute the initial and final substrings of differ-
ent length, from three to five characters.
2
http://www.atilf.fr/
3
http://www.lexique.org/
Number and percentage of consonants, vowels
and other characters. We compute the number and
the percentage of consonants, vowels and other
characters (i.e., hyphen, apostrophe, comas).
Classical readability scores. We apply two clas-
sical readability measures: Flesch (Flesch, 1948)
and its variant Flesch-Kincaid (Kincaid et al.,
1975). Such measures are typically used for eval-
uating the difficulty level of a text. They exploit
surface characteristics of words (number of char-
acters and/or syllables) and normalize these values
with specifically designed coefficients.
5.2 Machine learning system
The machine learning algorithms are used to study
whether they can distinguish between words un-
derstandable and non-understandable by laymen
and to study the importance of various features for
the task. The functioning of machine learning al-
gorithms is based on a set of positive and nega-
tive examples of the data to be processed, which
have to be described with suitable features such
as those presented above. The algorithms can then
detect the regularities within the training dataset to
generate a model, and apply the generated model
to process new unseen data. We apply various al-
gorithms available within the WEKA (Witten and
Frank, 2005) platform.
The annotations provided by the three annota-
tors constitute our reference data. We use on the
whole five reference datasets (Table 1): 3 sets of
separate annotations provided by the three anno-
tators (29,641 words each); 1 unanimity set, on
which all the annotators agree (n=22,925); 1 ma-
jority set, for which we can compute the major-
ity agreement (n=28,763). By definition, the two
last datasets should present a better coherence and
less annotation ambiguity because some ambigui-
ties have been resolved by unanimity or by major-
ity vote.
5.3 Evaluation
The inter-annotator agreement is computed with
the Cohen?s Kappa (Cohen, 1960), applied to pairs
of annotators, which values are then leveraged to
obtain the unique average value; and Fleiss? Kappa
(Fleiss and Cohen, 1973), suitable for processing
data provided by more than two annotators. The
interpretation of the scores are for instance (Landis
and Koch, 1977): substantial agreement between
0.61 and 0.80, almost perfect agreement between
0.81 and 1.00.
14
With machine learning, we perform a ten-fold
cross-validation, which means that the evaluation
test is performed ten times on different randomly
generated test sets (1/10 of the whole dataset),
while the remaining 9/10 of the whole dataset is
used for training the algorithm and creating the
model. In this way, each word is used during the
test step. The success of the applied algorithms is
evaluated with three classical measures: R recall,
P precision and F F-measure. In the perspective
of our work, these measures allow evaluating the
suitability of the methodology to the distinction
between understandable and non-understandable
words and the relevance of the chosen features.
The baseline corresponds to the assignment of
words to the biggest category, e.g., I cannot under-
stand, which represents 66 to 74%, according to
datasets. We can also compute the gain, which is
the effective improvement of performance P given
the baseline BL (Rittman, 2008):
P?BL
1?BL
.
6 Automatic analysis of
understandability of medical words:
Results and Discussion
We address the following aspects: annotations
(inter-annotator agreement, assignment of words
to three categories), quantitative results provided
by the machine learning algorithms, impact of the
individual features on the distinction between cat-
egories, and usefulness of the method.
6.1 Annotations and inter-annotator
agreement
The time needed for performing the manual ref-
erence annotations depends on annotators and
ranges from 3 to 6 weeks. The annotation results
presented in Table 1 indicate that the annotators
1 and 2 often provide similar results on their un-
derstanding of the medical words, while for the
third annotator the task appears to be more difficult
as he indicates globally a higher number of non-
understandable words. The non-understandable
words are the most frequent for all annotators and
cover 66 to 70% of the whole dataset. The inter-
annotator agreement shows substantial agreement:
Fleiss? Kappa 0.735 and Cohen?s Kappa 0.736.
This is a very good result, especially when work-
ing with linguistic data for which the agreement is
usually difficult to obtain.
The evolution of annotations per category (Fig-
ure 1), such as provided by the annotators, can dis-
 0
 5000
 10000
 15000
 20000
 0  5000  10000  15000  20000  25000
Nu
mb
er i
n e
ach
 ca
teg
ory
Words
I cannot understand
I can understand
I am not sure
A1A2A3
Figure 1: Evolution of the annotations within the
reference data.
tinguish easily between the three categories: (1)
the most frequently chosen category is I cannot
understand and it grows rapidly with new words;
(2) the next most frequently chosen category is I
can understand, although it grows more slowly;
(3) the third category, which gathers the words on
which the annotators show some hesitation, is very
small. Given the proximity between the lines in
each category, we can conclude that the annota-
tors have similar difficulties in understanding the
words from the dataset.
6.2 Quantitative results obtained with
machine learning
P R F
J48 0.876 0.889 0.881
RandomForest 0.880 0.892 0.884
REPTree 0.874 0.890 0.879
DecisionTable 0.872 0.891 0.880
LMT 0.876 0.895 0.884
SMO 0.858 0.876 0.867
Table 2: Performance obtained on the majority
dataset with various algorithms.
We tested several machine learning algorithms
to discover which of them are the most suitable
to the task at hand. In Table 2, with results com-
puted on the majority dataset, we can observe that
the algorithms provide with similar performance
(between 0.85 and 0.90 P and R). In the remain-
ing of the paper, we present results obtained with
J48 (Quinlan, 1993). Table 3 shows P , R and
F values for the five datasets: three annotators,
majority and unanimity datasets. We can observe
15
that, among the three annotators, it is easier to
reproduce the annotations of the third annotator:
we gain then 0.040 with F comparing to the two
other annotators. The results become even better
with the majority dataset (F=0.881), and reach F
up to 0.947 on the unanimity dataset. As we ex-
pected, these two last datasets present less annota-
tion ambiguity. The best categorization results are
observed with I can understand and I cannot un-
derstand categories, while the I am not sure cate-
gory is poorly managed by machine learning algo-
rithms. Because this category is very small, the av-
erage performance obtained on all three categories
remains high.
A1 A2 A3 Una. Maj.
P 0.794 0.809 0.834 0.946 0.876
R 0.825 0.826 0.862 0.949 0.889
F 0.806 0.814 0.845 0.947 0.881
Table 3: J48 performance obtained on five datasets
(A1, A2, A3, unanimity and majority).
In Table 4, we indicate the gain obtained by J48
compared to baseline: it ranges from 0.13 to 0.20,
which is a good improvement, despite the cate-
gory I am not sure that is difficult to discriminate.
We also indicate the accuracy obtained on these
datasets.
A1 A2 A3 Una. Maj.
BL 0.66 0.67 0.70 0.74 0.71
F 0.806 0.814 0.845 0.947 0.881
gain 0.14 0.13 0.14 0.20 0.16
Acc. 0.825 0.826 0.862 0.948 0.889
Table 4: Gain obtained for F by J48 on five
datasets (A1, A2, A3, unanimity and majority).
6.3 Impact of individual features on
understandability of medical words
To observe the impact of individual features, we
did several iterations of experiments during which
we incrementally increased the set of features: we
started with one feature and then, at each iteration,
we added one new feature, up to the 24 features
available. We tried several random orders. The
test presented here is done again on the majority
dataset. Figures 2 present the results obtained in
terms of P , R and F . Globally, we can observe
that some features show positive impact while oth-
ers show negative or null impact:
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 5  10  15  20
Per
for
ma
nce
Feature subsets
POS?tag
initial substrings
final substrings
word length reference lexica
web frequency
PrecisionRecallF?measure
Figure 2: Impact of individual features.
? with the syntactic categories (POS-tags)
alone we obtain P and R between 0.65 and
0.7. The performance is then close to the
baseline performance. Often, proper names
and abbreviations are associated with the
non-understandable words. There is no dif-
ference between TreeTagger alone and the
combination of TreeTagger with Flemm;
? the initial and final substrings have positive
impact. Among the final substrings, those
with three and four characters (ie, -omie of
-tomie (meaning cut), -phie of -rraphie (mean-
ing stitch), -?emie (meaning blood)) show posi-
tive impact, but substrings with five charac-
ters have negative impact and the previously
gained improvement is lost. We may con-
clude that the five-character long final sub-
strings may be too specific;
? the length of words in characters have neg-
ative impact on the categorization results.
There seems to be no strong link between this
feature and the understanding of words: short
and long words may be experienced as both
understandable or not by annotators;
? the presence of words in the reference lexica
(TLFI and lexique.org) is beneficial to both
precision and recall. We assume these lexica
may represent common lexical competence
of French speakers. For this reason, words
that are present in these lexica, are also easier
to understand;
? the frequencies of words computed through
a general search engine are beneficial.
16
Words with higher frequencies are often as-
sociated with a better understanding, al-
though the frequency range depends on the
words. For instance, coccyx (coccyx) or drain
(drain) show high frequencies (1,800,000 and
175,000,000, respectively) and they belong
indeed to the I can understand category.
Words like colique (diarrhea) or clitoridien
(clitoral) show lower frequencies (807,000 and
9,821, respectively), although they belong to
the same category. On contrary, other words
with quite high frequencies, like coagulase
(coagulase), clivage (cleavage) or douve (fluke)
(655,000, 1,350,000 and 1,030,000, respec-
tively) are not understood by the annotators.
According to these experiments, our results point
out that, among the most efficient features, we can
find syntactic categories, presence of words in the
reference lexica, frequencies of words on Google
and three- and four-character end substring. In
comparison to the existing studies, such as those
presented during the SemEval challenge (Specia
et al., 2012), we propose to exploit a more com-
plete set of features, several of which rely on the
NLP methods (e.g., syntactic tagging, morpholog-
ical analysis). Especially the syntactic tagging ap-
pears to be salient for the task. In comparison to
work done on general language data (Gala et al.,
2013), our experiment shows better results (be-
tween 0.825 and 0.948 accuracy against 0.62 ac-
curacy in the cited work), which indicates that spe-
cialized domains have indeed very specific words.
Additional tests should be performed to obtain a
more detailed impact of the features.
6.4 Usefulness of the method
We applied the proposed method to words from
discharge summaries. The documents are pre-
processed according to the same protocol and the
words are assigned the same features as previ-
ously (section 5). The model learned on the una-
nimity set is applied. The results are shown in
Figure 3. Among the words categorized as non-
understandable (in red and underlined), we find:
? abbreviations (NIHSS, OAP, NaCl, VNI);
? technical medical terms (hypoesth?esie
(hypoesthesia), par?esie (paresia), throm-
bolyse (thrombolysis), iatrog`ene (iatrogenic),
oxyg?enoth?erapie (oxygen therapy), d?esaturation
(desaturation));
Figure 3: Detection of non-understandable words
within discharge summaries.
? medication names (CALCIPARINE);
In the example from Figure 3, three types of errors
can be distinguished when common words are cat-
egorized as non-understandable:
? inflected forms of words (suites (conse-
quences), cardiologiques (cardiological));
? constructed forms of words (thrombolys?e
(with thrombolysis));
? hyphenated words (post-r?eanimation (post
emergency medical service)).
Notice that in other processed documents, other
errors occur. For instance, misspelled words and
words that miss accented characters (probleme
instead of probl`eme (problem), realise instead of
r?ealis?e (done), particularite instead particularit?e
(particularity)) are problematic. Another type of er-
rors may occur when technical words (e.g. pro-
lapsus (prolapsus), paroxysme (paroxysm), tricuspide
(tricuspid)) are considered as understandable.
Besides, only isolated words are currently pro-
cessed, which is the limitation of the current
method. Still, consideration of complex medi-
cal terms, that convey more complex medical no-
tions, should also be done. Such terms may indeed
change the understanding of words, as in these ex-
amples: AVC isch?emique (ischemic CVA (cerebrovas-
cular accident)), embolie pulmonaire basale droite
(right basal pulmonary embolism), d?esaturation `a 83 %
17
(desaturation at 83%), anticoagulation curative (cu-
rative anticoagulation). In the same way, numerical
values may also arise misunderstanding of medi-
cal information. Processing of these additional as-
pects (inflected and constructed forms of words,
hyphenated or misspelled words, complex terms
composed with several words and numerical val-
ues) is part of the future work.
6.5 Limitations of the current study
We proposed several experiments for analyzing
the understandability of medical words. We tried
to analyze these data from different points of view
to get a more complete picture. Still, there are
some limitations. These are mainly related to the
linguistic data and to their preparation.
The whole set of the analyzed words is large:
almost 30,000 entries. We assume it is possi-
ble that annotations provided may show some
intra-annotator inconsistencies due for instance to
the tiredness and instability of the annotators (for
instance, when a given unknown morphological
components is seen again and again, the meaning
of this component may be deduced by the anno-
tator). Nevertheless, in our daily life, we are also
confronted to the medical language (our personal
health or health of family or friend, TV and ra-
dio broadcast, various readings of newspapers and
novels) and then, it is possible that the new med-
ical notions may be learned during the annotation
period of the words, which lasted up to four weeks.
Nevertheless, the advantage of the data we have
built is that the whole set is completely annotated
by each annotator.
When computing the features of the words, we
have favored those, which are computed at the
word level. In the future work, it may be interest-
ing to take into account features computed at the
level of morphological components or of complex
terms. The main question will be to decide how
such features can be combined all together.
The annotators involved in the study have a
training in linguistics, although their relation with
the medical field is poor: they have no specific
health problems and no expertise in medical ter-
minology. We expect they may represent the av-
erage level of patients with moderate health lit-
eracy. Nevertheless, the observed results may re-
main specific to the category of young people with
linguistic training. Additional experiments are re-
quired to study this aspect better.
7 Conclusion and Future research
We proposed a study of words from the medi-
cal field, which are manually annotated as under-
standable, non-understandable and possibly un-
derstandable to laymen. The proposed approach
is based on machine learning and a set with 24
features. Among the features, which appear to be
salient for the diagnosis of understandable words,
we find for instance the presence of words in the
reference lexica, their syntactic categories, their fi-
nal substring, and their frequencies on the web.
Several features and their combinations can be dis-
tinguished, which shows that the understandability
of words is a complex notion, which involves sev-
eral linguistic and extra-linguistic criteria.
The avenue for future research includes for in-
stance the exploitation of corpora, while currently
we use features computed out of context. We
assume indeed that corpora may provide addi-
tional relevant information (semantic or statistical)
for the task aimed in this study. Additional as-
pects related to the processing of documents (in-
flected and constructed forms of words, hyphen-
ated or misspelled words, complex terms com-
posed with several words and numerical values) is
another perspective. Besides, the classical read-
ability measures exploited have been developed
for the processing of English language. Working
with French-language data, we should use mea-
sures, which are adapted to this language (Kandel
and Moles, 1958; Henry, 1975). In addition, we
can also explore various perspectives, which ap-
pear from the current limitations, such as comput-
ing and using features computed at different levels
(morphological components, words and complex
terms), applying other classical readability mea-
sures adapted to the French language, and adding
new reference annotations provided by laymen
from other social-professional categories.
Acknowledgments
This work is performed under the grant
ANR/DGA Tecsan (ANR-11-TECS-012) and
the support of MESHS (COMETE project). The
authors are thankful to the CHU de Bordeaux for
making available the clinical documents.
References
AMA. 1999. Health literacy: report of the council
on scientific affairs. Ad hoc committee on health lit-
18
eracy for the council on scientific affairs, American
Medical Association. JAMA, 281(6):552?7.
D Amiot and G Dal. 2005. Integrating combining
forms into a lexeme-based morphology. In Mediter-
ranean Morphology Meeting (MMM5), pages 323?
336.
M Amoia and M Romanelli. 2012. Sb: mmsystem -
using decompositional semantics for lexical simpli-
fication. In *SEM 2012, pages 482?486, Montr?eal,
Canada, 7-8 June. Association for Computational
Linguistics.
GK Berland, MN Elliott, LS Morales, JI Algazy,
RL Kravitz, MS Broder, DE Kanouse, JA Munoz,
JA Puyol, M Lara, KE Watkins, H Yang, and
EA McGlynn. 2001. Health information on the in-
ternet. accessibility, quality, and readability in en-
glish ans spanish. JAMA, 285(20):2612?2621.
Geert Booij. 2010. Construction Morphology. Oxford
University Press, Oxford.
A Borst, A Gaudinat, C Boyer, and N Grabar. 2008.
Lexically based distinction of readability levels of
health documents. In MIE 2008. Poster.
MT Cabr?e and R Estop`a. 2002. On the units of spe-
cialised meaning uses in professional com- muni-
cation. In International Network for Terminology,
pages 217?237.
TM Cabr?e. 2000. Terminologie et linguistique: la
thorie des portes. Terminologies nouvelles, 21:10?
15.
J Chmielik and N Grabar. 2011. D?etection de la
sp?ecialisation scientifique et technique des docu-
ments biom?edicaux gr?ace aux informations mor-
phologiques. TAL, 51(2):151?179.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
RA C?ot?e, 1996. R?epertoire d?anatomopathologie de la
SNOMED internationale, v3.4. Universit?e de Sher-
brooke, Sherbrooke, Qu?ebec.
B Daille. 1995. Rep?erage et extraction de terminologie
par une approche mixte statistique et linguistique.
Traitement Automatique des Langues (T.A.L.), 36(1-
2):101?118.
P Drouin and P Langlais. 2006. valuation du potentiel
terminologique de candidats termes. In JADT, pages
379?388.
N Elhadad and K Sutaria. 2007. Mining a lexicon
of technical terms and lay equivalents. In BioNLP,
pages 49?56.
JL Fleiss and J Cohen. 1973. The equivalence of
weighted kappa and the intraclass correlation coef-
ficient as measures of reliability. Educational and
Psychological Measurement, 33:613?619.
R Flesch. 1948. A new readability yardstick. Journal
of Applied Psychology, 23:221?233.
T Franc?ois. 2011. Les apports du traitements automa-
tique du langage la lisibilit du franais langue tran-
gre. Phd thesis, Universit Catholique de Louvain,
Louvain.
KT Frantzi, S Ananiadou, and J Tsujii. 1997. Auto-
matic term recognition using contextual clues. In
MULSAIC IJCAI, pages 73?79.
N Gala, T Franc?ois, and C Fairon. 2013. Towards a
french lexicon with difficulty measures: NLP help-
ing to bridge the gap between traditional dictionaries
and specialized lexicons. In eLEX-2013.
L Goeuriot, N Grabar, and B Daille. 2007. Car-
act?erisation des discours scientifique et vulgaris?e en
franc?ais, japonais et russe. In TALN, pages 93?102.
N Grabar, S Krivine, and MC Jaulent. 2007. Classifi-
cation of health webpages as expert and non expert
with a reduced set of cross-language features. In
AMIA, pages 284?288.
R Gunning. 1973. The art of clear writing. McGraw
Hill, New York, NY.
G Henry. 1975. Comment mesurer la lisibilit. Labor,
Bruxelles.
C Iacobini. 1997. Distinguishing derivational pre-
fixes from initial combining forms. In First mediter-
ranean conference of morphology, Mytilene, Island
of Lesbos, Greece, septembre.
C Iacobini, 2003. Composizione con elementi neoclas-
sici, pages 69?96.
Gonia Jarema, Cline Busson, Rossitza Nikolova,
Kyrana Tsapkini, and Gary Libben. 1999. Process-
ing compounds: A cross-linguistic study. Brain and
Language, 68(1-2):362?369.
SK Jauhar and L Specia. 2012. Uow-shef: Sim-
plex ? lexical simplicity ranking based on contextual
and psycholinguistic features. In *SEM 2012, pages
477?481, Montr?eal, Canada, 7-8 June. Association
for Computational Linguistics.
A Johannsen, H Mart??nez, S Klerke, and A S?gaard.
2012. Emnlp@cph: Is frequency all there is to sim-
plicity? In *SEM 2012, pages 408?412, Montr?eal,
Canada, 7-8 June. Association for Computational
Linguistics.
R Jucks and R Bromme. 2007. Choice of words
in doctor-patient communication: an analysis of
health-related internet sites. Health Commun,
21(3):267?77.
L Kandel and A Moles. 1958. Application de lindice
de flesch la langue franaise. Cahiers tudes de
Radio-Tlvision, 19:253?274.
19
JP Kincaid, RP Jr Fishburne, RL Rogers, and
BS Chissom. 1975. Derivation of new readabil-
ity formulas (automated readability index, fog count
and flesch reading ease formula) for navy enlisted
personnel. Technical report, Naval Technical Train-
ing, U. S. Naval Air Station, Memphis, TN.
D Kokkinakis and M Toporowska Gronostaj. 2006.
Comparing lay and professional language in cardio-
vascular disorders corpora. In Australia Pham T.,
James Cook University, editor, WSEAS Transactions
on BIOLOGY and BIOMEDICINE, pages 429?437.
I Korkontzelos, IP Klapaftis, and S Manandhar. 2008.
Reviewing and evaluating automatic term recogni-
tion techniques. In GoTAL, pages 248?259.
JR Landis and GG Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159?174.
G Leroy, S Helmreich, J Cowie, T Miller, and
W Zheng. 2008. Evaluating online health informa-
tion: Beyond readability formulas. In AMIA 2008,
pages 394?8.
Gary Libben, Martha Gibson, Yeo Bom Yoon, and Do-
miniek Sandra. 2003. Compound fracture: The role
of semantic transparency and morphological head-
edness. Brain and Language, 84(1):50?64.
AL Ligozat, C Grouin, A Garcia-Fernandez, and
D Bernhard. 2012. Annlor: A na??ve notation-
system for lexical outputs ranking. In *SEM 2012,
pages 487?492.
A L?udeling, T Schmidt, and S Kiokpasoglou. 2002.
Neoclassical word formation in german. Yearbook
of Morphology, pages 253?283.
D Maynard and S Ananiadou. 2000. Identifying terms
by their family and friends. In Proceedings of COL-
ING 2000, pages 530?536, Saarbrucken, Germany.
A McCray. 2005. Promoting health literacy. J of Am
Med Infor Ass, 12:152?163.
T Miller, G Leroy, S Chatterjee, J Fan, and B Thoms.
2007. A classifier to evaluate language specificity of
medical documents. In HICSS, pages 134?140.
Fiammetta Namer and Pierre Zweigenbaum. 2004.
Acquiring meaning for French medical terminology:
contribution of morphosemantics. In Annual Sym-
posium of the American Medical Informatics Asso-
ciation (AMIA), San-Francisco.
F Namer. 2000. FLEMM : un analyseur flexionnel du
franc?ais `a base de r`egles. Traitement automatique
des langues (TAL), 41(2):523?547.
Oregon Evidence-based Practice Center. 2008. Bar-
riers and drivers of health information technology
use for the elderly, chronically ill, and underserved.
Technical report, Agency for healthcare research and
quality.
V Patel, T Branch, and J Arocha. 2002. Errors in inter-
preting quantities as procedures : The case of phar-
maceutical labels. International journal of medical
informatics, 65(3):193?211.
M Poprat, K Mark?o, and U Hahn. 2006. A lan-
guage classifier that automatically divides medical
documents for experts and health care consumers.
In MIE 2006 - Proceedings of the XX International
Congress of the European Federation for Medical
Informatics, pages 503?508, Maastricht.
JR Quinlan. 1993. C4.5 Programs for Machine Learn-
ing. Morgan Kaufmann, San Mateo, CA.
R Rittman. 2008. Automatic discrimination of genres.
VDM, Saarbrucken, Germany.
R Rudd, B Moeykens, and T Colton, 1999. Annual
Review of Adult Learning and Literacy, page ch 5.
H Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Inter-
national Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
R Sinha. 2012. Unt-simprank: Systems for lexical
simplification ranking. In *SEM 2012, pages 493?
496, Montr?eal, Canada, 7-8 June. Association for
Computational Linguistics.
L Specia, SK Jauhar, and R Mihalcea. 2012. Semeval-
2012 task 1: English lexical simplification. In *SEM
2012, pages 347?355.
TM Tran, H Chekroud, P Thiery, and A Julienne. 2009.
Internet et soins : un tiers invisible dans la relation
m?edecine/patient ? Ethica Clinica, 53:34?43.
Y Wang. 2006. Automatic recognition of text diffi-
culty from consumers health information. In IEEE,
editor, Computer-Based Medical Systems, pages
131?136.
I.H. Witten and E. Frank. 2005. Data mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco.
Eugen W?uster. 1981. L?tude scientifique gnrale de la
terminologie, zone frontalire entre la linguistique, la
logique, l?ontologie, l?informatique et les sciences
des choses. In G. Rondeau et H. Felber, editor,
Textes choisis de terminologie, volume I. Fonde-
ments thoriques de la terminologie, pages 55?114.
GISTERM, Universit de Laval, Qubec. sous la di-
rection de V.I. Siforov.
Q Zeng-Treiler, H Kim, S Goryachev, A Keselman,
L Slaugther, and CA Smith. 2007. Text charac-
teristics of clinical reports and their implications for
the readability of personal health records. In MED-
INFO, pages 1117?1121, Brisbane, Australia.
W Zheng, E Milios, and C Watters. 2002. Filtering
for medical news items using a machine learning ap-
proach. In AMIA, pages 949?53.
20
Proceedings of the 4th International Workshop on Computational Terminology, pages 1?10,
Dublin, Ireland, August 23 2014.
Generalising and normalising distributional contexts
to reduce data sparsity: application to medical corpora
Amandine P
?
erinet
Universit?e Paris 13, Sorbonne Paris Cit?e
Villetaneuse, France
amandine.perinet@edu.univ-paris13.fr
Thierry Hamon
LIMSI-CNRS, Orsay, France
Universit?e Paris 13, Sorbonne Paris Cit?e
Villetaneuse, France
hamon@limsi.fr
Abstract
Vector space models implement the distributional hypothesis. They are based on the repetition
of information occurring in the contexts of words to associate. However, these models suffer
from a high number of dimensions and data sparsity in the matrix of contextual vectors. This
is a major issue with specialised corpora that are of much smaller size and with much lower
context frequencies. We tackle the problem of data sparsity on specialised texts and we propose
a method that allows to make the matrix denser, by generalising and normalising distributional
contexts. Generalisation gives better results with the Jaccard index, narrow sliding windows and
relations of lexical inclusion. On the other hand, normalisation has no positive effect on the
relation extraction, with any combination of distributional parameters.
1 Introduction
Distributional Analysis (DA) assumes that words occurring in a similar context tend to be semantically
close (Harris, 1954; Firth, 1957). This hypothesis is usually applied through vector space models (VSM)
where vectors represent the contextual information and distributional statistical data (Sahlgren, 2006).
Each target word in a text is represented as a point defined according to its distributional properties in
the text (Turney and Pantel, 2010; Lund and Burgess, 1996). Thus, the semantic similarity between
two words is defined as a closeness in an n-dimension space, where each dimension corresponds to
some potential shared contexts. The VSMs easily quantify the semantic similarity between two words
by measuring the distance between the two corresponding vectors within this space, or the cosine of
their angle. On the other hand, besides the high number of dimensions required (for example, Sahlgren
(2006) uses VSMs with up to several millions of dimensions), VSMs also suffer from data sparseness
within the matrix representing the vector space (Chatterjee and Mohan, 2008): many elements are equal
to zero because only few contexts are associated to a target word. This disadvantage is partly due to word
distribution in corpora: whatever the corpus size, most words have low frequencies and a very limited set
of contexts compared to the number of words in the corpora. These last two elements make the similarity
between two words hard to compute. Hence, methods based on the distributional hypothesis show better
results when much information is available and especially with general corpora, usually of great size
(Weeds and Weir, 2005; van der Plas, 2008). But the reduction of data sparseness is still an important
aspect with general corpora. It is as well a major issue when working with specialised corpora. Indeed,
these corpora are characterised by smaller sizes, and with frequencies and a number of different contexts
especially lower. We focus here on this last point. We propose a rule-based method that aims at reducing
context diversity by generalising contexts. The frequency of the obtained distributional contexts is then
increased and, consequently, data sparseness and the dimensions of the vector space model are reduced.
We present here a generalisation of the distributional contexts thanks to semantic relations acquired on
corpora. The parameters of the distributional method are tuned to specialised corpora, especially in
integrating those generalised contexts.
We first present a state of the art on data sparsity reduction within distributional methods. Then
we describe the proposed context generalisation and normalisation method as well as the experiments
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
we performed to evaluate its impact on specialised corpora. Results are evaluated and analysed with
precision, R-precision and MAP.
2 Related work
Reducing data sparsity is a main issue in distributional analysis. The existing methods aim at influencing
the selection of useful contexts or at integrating semantic information to modify context distribution.
Thus, contrary to the common usage, Broda et al. (2009) propose to weight contexts by first ranking
contexts according to their frequency, and then take the rank into account to weight contexts. Other ap-
proaches rely on statistical language models to determine the most likely substitutes to represent contexts
(Baskaya et al., 2013). These models assign probabilities to arbitrary sequences of words based on their
co-occurrence frequencies in a training corpora (Yuret, 2012). These substitutes and their probabilities
are then used to create word pairs to feed a co-occurrence model and to cluster the word list. The limit of
such methods is their performance which depends on vocabulary size and requires an increasing amount
of training data. Influence on contexts may also be done by incorporating additional semantic infor-
mation: it has been shown that such information used to modify the standard distributional method can
improve its performance (Tsatsaronis and Panagiotopoulou, 2009). This semantic information, in partic-
ular semantic relations, may be automatically computed or issued from an existing resource. Thus, with
a bootstrap method, Zhitomirsky-Geffet and Dagan (2009) modify the context weights with the seman-
tic neighbours proposed by a distributional similarity measure. Based on this latter work, Ferret (2013)
addresses the problem of low frequency words. To better consider this information, a set of positive and
negative examples are selected with an unsupervised classifier. A supervised classifier is then applied for
re-ranking the semantic neighbours. The method allows to improve the quality of the similarity relation
between nouns with low or mid frequency.
The sparseness problem may also be tackled from the algorithmic point of view by limiting the di-
mensions of the context matrix, especially by smoothing it in order to reduce the number of vector
compounds (Turney and Pantel, 2010). Thus, Latent Semantic Analysis (LSA) (Landauer and Dumais,
1997; Pad?o and Lapata, 2007) implements a factorization method of matrices by Singular Value Decom-
position (SVD). The original data in the context matrix is abstracted in independant linear components,
that allow to reduce noise and to highlight the main elements. Besides reducing the computational cost,
dimension reduction significantly improves precision in LSA applications. For instance, the use of the
SVD to compute word similarity allows to obtain scores equivalent to human scores in a TOEFL test
with multiple choice questions of synonymy (Landauer and Dumais, 1997). As for low frequency, the
SVD is a way to counterbalance the lack of data (Vozalis and Margaritis, 2003). Also, some methods, as
the non-negative matrix factorization (Lee and Seung, 1999), allow to better model word frequency. But,
when it comes to the acquisition of semantic relations, performances do not seem better than the ones
obtained with the LSA (Turney and Pantel, 2010; Utsumi, 2010). Furthermore, the dimension reduction
makes easier to treat context vectors, but it does not solve the initial issue of building a huge co-occurence
matrix. Random Indexing (RI) (Kanerva et al., 2000) may be considered as a solution to this problem,
as it incrementally builds the context matrix according to an index vector of the target word randomly
generated, as well as reducing the matrix dimension. RI and LSA have similar performance when iden-
tifying synonyms in a similar way than the TOEFL test (Karlgren and Sahlgren, 2001). Recently, the
selection of the best contexts combined with a normalisation of their weights allows to improve the qual-
ity of a SVD reduced matrix (Polajnar and Clark, 2014). In the context of definition retrieval and phrase
similarity computation, their impact depends on the compositional semantics operators used.
As above work, we aim at incorporating semantic information within distributional contexts, but by
reducing the number of contexts and increasing their frequency. Contrary to SVD based methods that
limit the contexts by removing information, here we both generalise and normalise contexts through the
integration of additional semantic knowledge computed from our corpora.
2
3 Material
In this section, we present the corpus we use. We also describe the approaches used to acquire the
semantic relations integrated in our method for context generalisation/normalisation.
Corpora To evaluate our approach, we use the Menelas corpus (Zweigenbaum, 1994). It consists in a
medical text collection, in French, on the topic of coronary diseases. The corpus contains 84,839 words.
It has been analysed through the Ogmios platform (Hamon et al., 2007). The linguistic analysis includes
a morphosyntactic tagging and a lemmatisation of the corpus, with TreeTagger (Schmid, 1994), and a
term extraction with Y
A
T
E
A (Aubin and Hamon, 2006). This last step allows to identify terminological
entities (both single word units, for eg. artery, and complex terms (i.e. multi-word expressions), for eg.
coronary disease, that denote the domain concepts).
Semantic relations acquisition Our generalisation and normalisation method of distributional con-
texts is based on semantic relations acquired from the entire corpus. We use several classical approches
that allow to acquire semantic relations between terms. For context generalisation, we use lexico-
syntactic patterns, lexical inclusion and terminological variation rules. Context normalisation is based
on a rule-based synonymy acquisition.
? Lexico-syntactic patterns (LSP) We use the patterns defined by (Morin and Jacquemin, 2004) to
detect 98 hypernymy relations between simple or complex terms, for instance: {some |several
etc.} SN: LIST or {other}? SN such as LIST. The relations acquired with such pat-
terns are usually relevant but the pattern coverage remains low.
? Lexical Inclusion (LI) This approach is based on the hypothesis that the lexical inclusion of a term
(ex: infarctus in another (infarctus du myocarde (myocardial infarction) convey a hypernymy rela-
tion between those terms (Grabar and Zweigenbaum, 2003). We constrain the method by exploiting
the term syntactic analysis provided by Y
A
T
E
A. We obtain 7,187 relations between the complex term
and its head. This approach is known to acquire relations with high precision.
? Terminological variation (TV) Terminological variant acquisition method proposed by
(Jacquemin, 2001) exploits morphosyntactic transformation rules, as the insertion or the permu-
tation, (chirurgie coronarienne (coronary surgery) / chirurgie de revascularisation coronarienne
(Coronary revascularisation surgery)) to identify semantic relations between terms.
The terminological variation rules, essentially the insertion on our French corpus, allow to acquire
171 hypernymy relations.
? Semantic compositionality (Syn) For context normalisation, we use 168 synonymy relations
acquired with the method defined in (Hamon et al., 1998). Based on the semantic compositionality
principle, a synonymy relation is inferred between complex terms, if at least one of their component
are synonyms (infection de blessure (wound infection) and septicit de blessure (wound sepsis)).
4 Distributional context generalisation and normalisation
A solution to the problem of data sparsity on specialised corpora or smaller size corpora consists in in-
creasing the density of the context matrix by disregarding superficial variations of contexts that are not
strongly statistically significant or that result from the noise of the distributional method. Thus, we gen-
eralise (conceptual abstraction) and normalise (abstraction of minor lexical variations) contexts using the
semantic information extracted from our corpus. In that respect, we use semantic relations automatically
acquired with standard methods on specialised corpora. After a brief description of the distributional
analysis we performed on specialised corpora, we present the distributional context generalisation and
normalisation.
3
4.1 Distributional method
We focus on the extraction of relations between nouns, tokens tagged as nouns by TreeTagger, and
terms, specific terminological entities extracted during the linguistic analysis of the corpus by Y
A
T
E
A(see
section 3). These semantic relations are crucial in specialised language. Nouns and terms are our targets.
The distributional contexts of these targets correspond to adjectives, nouns, verbs and terms co-occurring
with the target within a sliding window. A context is for us one element (a word or MWE), and it
corresponds to one dimension in the vector space. For both targets and contexts, we consider the lemmas.
Pre?processing
Corpus
Target & contextdefinition
Relations
Lexical inclusion (LI)
Terminological variation (TV)
Lexico?syntactic patterns (LSP)
Semantic
compositionality(SC)
on the target word and on the similarityThresholds on the shared contexts,
Jaccard index and cosine measure
Computation of
semantic similarity
Nouns + TermsGraphical windows   (21 & 5 words)
step 1 step 2
Vector Space Model
Linguistic approaches 
step 3
 
Term extractionPOS tagging & Lemmatisation
a. for generalisation b. for normalisation
Context
and normalisationgeneralisation
Figure 1: Process of distributional analysis
The overview of the distributional method is given in figure 1. The context generalisation and nor-
malisation step takes place after the distributional context definition (see section 4.2). After extracting,
generalising or normalising contexts, we compute a similarity score between each pair of target words,
considering their shared contexts. We use the Jaccard index, recognised as suitable to specialised corpora
(Grefenstette, 1994). The Jaccard index normalises the number of contexts shared by two words, w
m
and w
n
, by the total number of contexts of those two words, ctxt(w). We also use the cosine of the
context vector. In order to grant more or less importance to the information in contexts, we use these
two measures combined with a weight function. With Jaccard, we use the relative frequency, that allows
to consider the importance of a target, compared to the total number of contexts for the target. And
with cosine we use Mutual Information (MI) (Fano, 1963). While these scores quantify the similarity
between target words, it is necessary to apply thresholds to limit the proposed number of distributional
relations and discard potentially noisy relations. We also intend to make the context matrix denser by
applying thresholds on three distributional parameters: the number of shared contexts, the frequency of
shared contexts and the frequency of target words. For each parameter, a threshold is automatically com-
puted as the mean of the values taken by each parameter on the whole corpus. During the experiments
(section 5), we test the impact of these thresholds on the results.
4.2 Rules of generalisation and normalisation of distributional contexts
Context generalisation and normalisation comes after the context definition step. It aims at reducing
context sparsity and increasing the number of context occurrences. Generalisation and normalisation
rules are separately applied and exploit additional semantic relations acquired on the corpus.
Rules of generalisation We generalise contexts with semantic relations automatically acquired from
the corpus with hypernymy relations proposed by lexico-syntactic patterns and lexical inclusion (see
section 3). While terminological variation approach does not propose semantically typed relations, the
insertion rule is the only one used to acquire variants and it can be considered that the obtained rela-
tions are hypernymy relations. The hypernym and the hyponym are identified from the number of words
present in each term: the shortest term then corresponds to the hypernym (l?esion significative (signifi-
cant lesion)), and the longest term is the hyponym (l?esion coronaire significative (significant coronary
lesion)).
4
We have then for each word ctxt
i
(w) in the context of the target word w three sets of hypernymy
relations H
s
(ctxt
i
(w)) = {H
1
, . . . ,H
n
} : H
PLS
, H
IL
and H
V T
, with a hypernym set that may be
empty. We define two substitution rules that allow to generalise contexts. Thus, for each word ctxt
i
(w)
in the context of a target word w, we apply one of the following rules:
1. if |H
S
(ctxt
i
(w))| = 1, then ctxt
i
(w) := H
1
, i.e. if the word in context corresponds to only one
hypernym (H
1
), acquired by one or several methods S, the word is replaced by this hypernym.
For example, if lexical inclusion provides the relations restriction / restriction du d?ebit coronaire
(restriction of coronary output), restriction du d?ebit coronaire is replaced by restriction.
2. if |H
S
(ctxt
i
(w))| > 1, ctxt
i
(w) = argmax
|H
i
|
(H
S
(ctxt
i
(w))), i.e. if context corresponds to
several hypernyms acquired by one or several methods S, we take into consideration the hypernym
frequency |H
1
|, . . . , |H
n
| in the corpus, and we select the hypernym with the highest frequency. For
example, for the word art`ere coronaire (coronary artery) in context, the lexico-syntactic patterns
provide the following hypernyms: veine (vein), art`ere (artery), and vaisseau (vessel), the one that
is the most frequent is chosen and replaces art`ere coronaire in context.
4.2.1 Rules of normalisation
The normalisation rule aims at reducing semantic variation with automatically acquired synonymy rela-
tions. These relations are first organised in clusters of synonyms and a cluster representative is chosen:
given the relations proposed by the acquisition method (section 3), the cluster representative corresponds
to the most frequent word in the cluster. We have then for each target word ctxt
i
(w) in the context of the
word w, a synonym cluster S
s
(R) = {S
1
, . . . , S
n
, R}, with its representative R. We define one context
normalisation rule applied for each word ctxt
i
(w) in the context of a word w to substitute the context
word by the representative of the cluster it belongs: if ?R|ctxt
i
(w) ? S
S
(R), then ctxt
i
(w) := R
5 Experiments and evaluation
5.1 Experiments
We performed several series of experiments on the Menelas corpus to evaluate the impact of both gen-
eralisation and normalisation rules on the quality of the acquired relations. Our baseline is the VSM
without context substitution (VSMonly). First, we automatically compute the thresholds on the distribu-
tional parameters from the baseline (see section 4.1). The values of the thresholds are listed in table 1.
We experiment two sliding window sizes ; a small window allows to detect classical types of relations
(synonymy, meronymy, hypernymy, etc.) but increases the data sparseness problem. On the other hand,
a large window provides more general relations, a contextual proximity.
Parameters 21 word window 5 word window
Similarity score
Jaccard: sim > 0, 000999 Jaccard: sim > 0, 000999
Cosine: sim > 0.9699 Cosine: sim > 0.9699
Number of contexts 2 1
Context frequency 3 2
Target frequency 3 3
Table 1: Definition of the threshold values on distributional parameters and on the similarity score ac-
cording to the window width (21 and 5 words) and similarity measures (Jaccard index and Cosine)
We perform separately the experiments regarding generalisation and normalisation rules. With gen-
eralisation rules, in order to grasp the contribution of each linguistic method (see section 3), we define
a set of experiments where context generalisation is performed using the hypernymy relations proposed
individually by each method. The context generalisation rules ctxt
i
(w) are then applied separately us-
ing the sets H
LSP
(ctxt
i
(w)) (VSM/LSP), H
LI
(ctxt
i
(w)) (VSM/LI) and H
TV
(ctxt
i
(w)) (VSM/TV).
Then, sequentially, we apply generalisation rules by using the sets of hypernymy relations proposed
by two linguistic approaches (H
LSP
(ctxt
i
(w)) then H
LI
(ctxt
i
(w)) ? VSM/LSP+LI, H
TV
(ctxt
i
(w))
5
then H
LSP
(ctxt
i
(w)) ? VSM/TV+LSP, etc.). All contexts are generalised following the relations pro-
posed by one of the sets.Likewise, we combine the three sets of relations (for instance, H
LSP
(ctxt
i
(w))
then H
LI
(ctxt
i
(w)) then H
TV
(ctxt
i
(w)) ? VSM/LSP+LI+TV). By combining the hypernymy relation
sources in several ways, we evaluate the complementarity of the approaches used for context general-
isation. We also study the impact of the order of these methods in the generalisation sequence. All
the hypernymy relations independently of the method used for their acquisition. We consider the set
H(ctxt
i
(w)) = H
LSP
(ctxt
i
(w)) ?H
LI
(ctxt
i
(w)) ?H
TV
(ctxt
i
(w)) ? VSM/ALL3. With context nor-
malisation, we consider only one set of experiment, with normalisation of contexts (VSM/Syn).
All the experiments have been performed on both window sizes: 5 words (? 2 words, centered on the
target) and 21 words (? 10 words, centered on the target). Indeed the window size influences the number
and quality, but also the type of the relations acquired with distributional analysis. In general, a small
window size (5 words) allows to have a highest number of relevant contexts for a given target word, but
leads to more data sparsity than with a larger window (Rapp, 2003). Furthermore, the results obtained
with small size windows are of greatest quality, especially for classical relations (synonymy, antonymy,
hypernymy, meronymy, etc.), whereas larger windows are more adapted to the identification of domain
specific relations (Sahlgren, 2006; Peirsman et al., 2008).
5.2 Evaluation
As usual to evaluate distributional methods, the obtained relations are considered as semantic neighbour
sets associated to target words, and the quality of the neighbour sets is measured by comparing them to
semantic relations issued from existing resources (Curran, 2004; Ferret, 2010). Thus, we compare the
semantic relations acquired by our approach with the 1,735,419 relations in the French part of the UMLS
metathesaurus
1
. The resource contains hypernyms, synonyms, co-hyponyms, meronyms and domain
relations.
We used classical measures to evaluate the quality of our results: macro-precision (Sebastiani, 2002),
the mean of the average precisions (MAP) (Buckley and Voorhees, 2005) and R-precision.
Macro-precision equally considers all target words whatever the number of semantic neighbours and
provides a comprehensive quality of the results by computing the mean of the precision of each neighbour
set. We consider one size of neighbour set for each target word: precision after examining 1 (P@1)
semantic neighbour, the neighbour ranked first by its similarity score. Alternatively, we use R-precision
that individually defines the size of the neighbour sets to examine as the number of correct neighbours
expected for the corresponding target word (Buckley and Voorhees, 2005). To compute R-precision, we
compare our results not to all the relations from the French part of UMLS, but to reference sets built
from this resource, for each experiment. Thus, we have as many references as experiments. The mean
of average precisions (MAP) is obtained taking in consideration the not interpolated precision of the
semantic neighbours given their rank. It reflects the ranking quality and evaluates the relevance of the
similarity measure used. Thus, the MAP favours the similarity measure that ranks all the correct semantic
neighbours on top of the list. Reciprocally, adding noisy semantic neighbours at the end of the list does
not discriminate against the method.
6 Results and discussion
In this section, we present and discuss the results we obtain, first with the 21 word window and then with
the 5 word window. For both sizes, we present the number of relations acquired (Acq. Rel), the number
of relations found in the UMLS resource (Rel. UMLS), and the results in terms of MAP, R-precision and
precision to 1 (P@1). Before discussing our results, we briefly present some results of similar work.
Results in existing similar work In order to understand better our results, we first provide some results
obtained in similar work. Keep in mind that these results are not obtained with the same corpus. Indeed, a
major problem is that the comparisons with reference resources are often given for large copora and very
frequent words (Curran and Moens, 2002), or for different tasks than our task. An effective comparison
1
http://www.nlm.nih.gov/research/umls/
6
is then difficult. We first present results obtained on a similar task, but with general copora, and then
results obtained on similar documents (i.e. specialised medical texts) but in a different tasks. Despite
this limit, we can still quote for comparison the values obtained by Ferret (2011) for the evaluation of
semantic neighbours extraction on English general copora (of 380 million words). The parameters of his
VSM are a small sliding window of 3 words (? 1 word centered on the target), the cosine measure and
mutual information. In his work, Ferret (2011) considers three sets of target words according to their
frequency. As in the Menelas corpus, the highest frequency of a target word is 270 and that only a few
frequencies are above 100, we may consider the set of low frequency words, that occur less than 100
times. The highest values he obtains for those target words are a MAP of 0.03, a P@1 of 0.026 and an
R-precision of 0.02. For more frequent words, occurring between 100 and 1,000 times, the values are
higher: a MAP of 0.125, a P@1 of 0.209 and an R-precision of 0.104.
For a comparison with specialised texts, we can look at Moen et al. (2014)?s work on document
similarity between care episodes in a retrieval system. The matrix is then a term-document matrix, and
not a term-context one, and the task is different. We do not know exactly how the comparison is effective,
but they obtain for their best system a MAP of 0.326 and a precision at 10 neighbours of 0.515.
Acquired Rel. Rel. in UMLS MAP R-precision P@1
JACC COS JACC COS JACC COS JACC COS JACC COS
VSMonly
(baseline)
406 9,154 4 46 0.406 0.105 0.250 0.000 0.250 0.000
VSM/TV 472 5,322 8 24 0.280 0.149 0.143 0.053 0.143 0.053
VSM/LI 324 2,844 4 18 0.454 0.232 0.250 0.167 0.250 0.200
VSM/LSP 398 4,684 6 18 0.219 0.154 0.000 0.071 0.000 0.071
VSM/TV+LI 324 2,844 4 18 0.454 0.232 0.250 0.167 0.250 0.200
VSM/TV+LSP 398 4,678 6 18 0.219 0.149 0.000 0.071 0.000 0.071
VSM/LSP+LI 336 2,748 4 14 0.454 0.263 0.250 0.208 0.250 0.250
VSM/ALL3 336 2,982 4 16 0.414 0.259 0.250 0.192 0.250 0.231
VSM/Syn 474 5,282 8 24 0.280 0.157 0.143 0.053 0.143 0.053
Table 2: Results obtained with the Jaccard index and Cosine measure for a 21 word window
Large window For the large window, we present the results obtained with Jaccard and Cosine. We do
not present all the generalisation sets because adding more relations (more methods) in the generalisation
process does not change the results: once we generalise with two methods, the results get stable. We first
observe a different behaviour according to the similarity measure in terms of relations acquired: Cosine
allow to acquire many more relations than Jaccard, and as well more relations acquired with Cosine
are found in the UMLS. However results are in general much better with Jaccard. Quite similar results
are observed between both similarity measures when generalisation is perfomed with all three linguistic
methods at a time (VSM/ALL3). Using the three methods at the same time for generalisation does not
provide better results. Indeed, it even decreases the number of relations found in the UMLS. As for
generalisation/normalisation, it allows to decrease the number of relations acquired by two when Cosine
is used, that is good because the number of relations acquired with Cosine is extremely high, but it also
divides the number of relations found in the UMLS by two.
When terminological variation is individually used, it always improves the quality of the results in
terms of MAP, precision and R-precision for Cosine, whereas it always decreases the quality with Jac-
card. The normalisation with synonyms with both similarity measures behaves similarly to generalisation
with terminological variation: they both get the best recall.
With Jaccard, generalisation with lexical inclusion improves the MAP results, that means that the
relations are better ranked. Lexical inclusion improves the results when used individually or within a
combination. Lexico syntactic patterns with a large window have little (with Cosine) or negative impact
on the results. Lexical inclusion allows to increase the MAP, R-precision and precision values when used
after the lexico syntactic patterns with Cosine.
Finally, we can conclude that with a large window, the use of Jaccard and generalisation with lexical
inclusion improves the quality of the relations acquired. But the recall is also really low. With Cosine,
the recall is higher and generalisation with LI is also the best choice.
7
Acquired Rel. Rel. in UMLS MAP R-precision P@1
JACC COS JACC COS JACC COS JACC COS JACC COS
VSMonly
(baseline)
1,882 16,178 6 60 0.502 0.118 0.333 0.054 0.333 0.048
VSM/TV 2,258 13,804 16 56 0.276 0.110 0.143 0.051 0.143 0.051
VSM/LI 976 6,172 2 38 0.536 0.132 0.500 0.067 0.500 0.067
VSM/LSP 2,112 12,656 16 50 0.187 0.106 0.071 0.057 0.071 0.057
VSM/TV+LI 976 6,172 2 38 0.536 0.132 0.500 0.067 0.500 0.067
VSM/TV+LSP 2,066 12,338 16 50 0.191 0.106 0.071 0.057 0.071 0.057
VSM/LSP+LI 934 5,996 4 38 0.378 0.135 0.250 0.067 0.250 0.067
VSM/ALL3 1,002 6,540 4 38 0.379 0.131 0.250 0.067 0.250 0.067
VSM/Syn 2,292 14,022 16 56 0.273 0.110 0.143 0.051 0.143 0.051
Table 3: Results obtained for a 5 word window? with thresholds on the distributional parameters
Narrow window With the 5 word window, the observations and results also differ according to the
similarity measure used. The best results are also obtained with the Jaccard Index and the behaviour of
both similarity measures is similar to the one observed with a large window: generalisation with lexical
inclusion reduces by two the number of relations acquired but also the number of relations found in
the UMLS. In order to better understand the behaviour of generalisation with lexical inclusion, and to
improve the results in terms of recall without decreasing precision, manual evaluation is required. The
results obtained with Cosine are lower than with a large window.
The choice of the similarity measure is a difficult choice and is linked to the other distributional
parameters of the VSM. We can deduce that Jaccard with small corpora allows to get a better precision
than Cosine, and obtains better results with a narrow window. Generalisation with lexical inclusion
appears to be the best generalisation for both measures, and normalisation with synonymy relations does
not improve the results.
But when LI is combined with relations acquired with lexico-syntactic patterns, its contribution de-
creases the results with the Jaccard index, and on the contrary improves the results with the Cosine.
The order of the methods also matters and differs according to the similarity measure: with Jaccard, the
generalisation with relations acquired with lexical inclusion before lexico-syntactic patterns has a lower
precision than the inverse combination (i.e. VSM/LSP+LI).
7 Conclusion
In this paper, we address the reduction of data sparsity in matrices of context vectors used to imple-
ment the distributional analysis. We proposed to generalise and normalise the distributional contexts
with synonymy and hypernymy relations acquired from our corpus. Words in contexts are considered as
hyponyms and are replaced by hypernyms identified on the corpus, or are considered as members of a
synonym set, and normalised with the cluster representative of this set. We performed some experiments
on a French medical corpus combining several parameters. Even if the evaluation of distributional meth-
ods is difficult, we compare the results to the semantic relations proposed by the French UMLS. Several
evaluation measures have been used to evaluate the impact of context generalisation and normalisation
on distributional analysis. The analysis of the results show that when the size of the window that allow
to produce distributional contexts is small and when the Jaccard index is used, it is better to generalise
contexts with relations acquired with lexical inclusion. However, when the window is large, generalisa-
tion with lexical inclusion with the use of Jaccard index also improves the results. Normalisation seems
to have no positive effect on relation extraction, with any combination of distributional parameters.
Beside a manual analysis of the relations and of the impact of the process of generalisation and normal-
isation on manipulated data, these results open several perspectives. The hypernymy relations we used
have been separately exploited. But these relations could be considered as a sketch towards a taxonomy
and we plan to adapt the context generalisation method in order to consider this network of relations
acquired from corpora. Furthemore, all the relations acquired from corpora may be noisy. We plan to use
other sources of relations as the ones contained in terminologies. It could then be possible to evaluate the
impact of generalisation and of the relations when their terminological type is known. Finally, we plan to
8
compare our method with two other dimension reduction methods, such as Random Indexing and LSA.
References
S. Aubin and T. Hamon. 2006. Improving term extraction with terminological resources. In Advances in Natural
Language Processing, number 4139 in LNAI, pages 380?387. Springer.
O. Baskaya, E. Sert, V. Cirik, and D. Yuret. 2013. Ai-ku: Using substitute vectors and co-occurrence modeling
for word sense induction and disambiguation. In Proc. of SemEval - 2013, pages 300?306, Atlanta, USA. ACL.
B. Broda, M. Piasecki, and S. Szpakowicz. 2009. Rank-based transformation in measuring semantic relatedness.
In Yong Gao and Nathalie Japkowicz, editors, Canadian Conference on AI, volume 5549, pages 187?190.
Springer.
C. Buckley and E. Voorhees. 2005. Retrieval system evaluation. In Ellen Voorhees and Donna Harman, editors,
TREC: Experiment and Evaluation in Information Retrieval, chapter 3. MIT Press.
N. Chatterjee and S. Mohan. 2008. Discovering word senses from text using random indexing. In Proc. of the
9th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing?08, pages
299?310, Berlin, Heidelberg. Springer-Verlag.
J.R. Curran and M. Moens. 2002. Improvements in automatic thesaurus extraction. In Workshop on Unsupervised
lexical acquisition, volume 9, pages 59?66, Morristown, NJ, USA. Association for Computational Linguistics.
J. R. Curran. 2004. From distributional to semantic similarity. Ph.D. thesis, Institute for Communicating and
Collaborative Systems School of Informatics University of Edinburgh.
R. Fano. 1963. Transmission of Information: A Statistical Theory of Communications. The MIT Press, Cam-
bridge, MA.
O. Ferret. 2010. Similarit smantique et extraction de synonymes partir de corpus. In TALN 2010, Montral.
O. Ferret. 2011. Utiliser l?amorage pour am?eliorer une mesure de similarit?e s?emantique. In Mathieu Lafourcade
and Violaine Prince, editors, TALN 2011, volume 1, pages 155?160, Montpellier, France, juillet.
Olivier Ferret. 2013. S?election non supervis?ee de relations s?emantiques pour am?eliorer un th?esaurus distribution-
nel. In TALN 2013, pages 48?61, Les Sables d?Olonne, France.
J.R. Firth. 1957. A synopsis of linguistic theory 1930-1955. Studies in linguistic analysis, pages 1?32.
N. Grabar and P. Zweigenbaum. 2003. Lexically-based terminology structuring. In Terminology, volume 10,
pages 23?54.
G. Grefenstette. 1994. Corpus-derived first, second and third-order word affinities. In Sixth Euralex International
Congress, pages 279?290.
T. Hamon, A. Nazarenko, and C. Gros. 1998. A step towards the detection of semantic variants of terms in
technical documents. In International Conference on Computational Linguistics (COLING-ACL?98), pages
498?504, Universit?e de Montr?eal, Quebec, Canada.
T. Hamon, A. Nazarenko, T. Poibeau, S. Aubin, and J. Derivi`ere. 2007. A robust linguistic platform for efficient
and domain specific web content analysis. In RIAO 2007, Pittsburgh, USA.
Z. Harris. 1954. Distributional structure. Word, 10(23):146?162.
C. Jacquemin. 2001. Spotting and discovering terms through natural language processing. The MIT Press.
P. Kanerva, J. Kristofersson, and A. Holst. 2000. Random indexing of text samples for latent semantic analysis.
In L.R. Gleitman and A.K. Josh, editors, Proceedings of the 22nd Annual Conference of the Cognitive Science
Society, volume 1036, Erlbaum, New Jersey.
J. Karlgren and M. Sahlgren. 2001. From words to understanding. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages 294?308. Foundations of Real-World Intelligence.
T.K. Landauer and S.T. Dumais. 1997. A solution to plato?s problem: The latent semantic analysis theory of acqui-
sition, induction, and representation of knowledge. Psychological Review; Psychological Review, 104(2):211.
9
D.D. Lee and H.S. Seung. 1999. Learning the parts of objects by nonnegative matrix factorization. Nature,
401:788?791.
K. Lund and C. Burgess. 1996. Producing high-dimensional semantic spaces from lexical co-occurrence. Behav-
ior Research Methods, Instrumentation, and Computers, 28:203?208.
H. Moen, E. Marsi, F. Ginter, L.-M. Murtola, T. Salakoski, and S. Salanter?a. 2014. Care episode retrieval. In
Proc. of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi), pages 116?
124, Gothenburg, Sweden. ACL.
E. Morin and C. Jacquemin. 2004. Automatic Acquisition and Expansion of Hypernym Links. Computers and
the Humanities, 38(4):363?396.
S. Pad?o and M. Lapata. 2007. Dependency-based construction of semantic space models. Comput. Linguist.,
33(2):161?199.
Y. Peirsman, H. Kris, and G. Dirk. 2008. Size matters. tight and loose context definitions in english word space
models. In ESSLLI Workshop on Distributional Lexical Semantics, Hamburg, Germany.
T. Polajnar and S. Clark. 2014. Improving distributional semantic vectors through context selection and normali-
sation. In Proceedings of EACL 2014. To appear.
R. Rapp. 2003. Word sense discovery based on sense descriptor dissimilarity. In MT Summit?2003, pages 315?
322.
M. Sahlgren. 2006. The Word-Space Model: Using Distributional Analysis to Represent Syntagmatic and Paradig-
matic Relations between Words in High-Dimensional Vector Spaces. Ph.D. thesis, Stockholm Univ., Sweden.
H. Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In New Methods in Language Pro-
cessing, pages 44?49, Manchester, UK.
F. Sebastiani. 2002. Machine learning in automated text categorization. ACM Comput. Surv., 34(1):1?47.
G. Tsatsaronis and V. Panagiotopoulou. 2009. A generalized vector space model for text retrieval based on
semantic relatedness. In EACL 2009, pages 70?78, Stroudsburg, PA, USA. Association for Computational
Linguistics.
P.D. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of
artificial intelligence research, 37:141?188.
A. Utsumi. 2010. Evaluating the performance of nonnegative matrix factorization for constructing semantic
spaces: Comparison to latent semantic analysis. In Proceedings of SMC, pages 2893?2900. IEEE.
L. van der Plas. 2008. Automatic lexico-semantic acquisition for question answering. Th`ese de doctorat, Univer-
sity of Groningen, Groningen.
E. Vozalis and K. G. Margaritis. 2003. Analysis of recommender systems? algorithms. In The 6th Hellenic
European Conference on Computer Mathematics & its Applications (HERCMA), Athens, Greece.
J. Weeds and D. Weir. 2005. Co-occurrence retrieval: A flexible framework for lexical distributional similarity.
Comput. Linguist., 31(4):439?475.
D. Yuret. 2012. Fastsubs: An efficient and exact procedure for finding the most likely lexical substitutes based on
an n-gram language model. IEEE Signal Process. Lett., 19(11):725?728.
M. Zhitomirsky-Geffet and I. Dagan. 2009. Bootstrapping distributional feature vector quality. Comput. Linguist.,
35(3):435?461.
P. Zweigenbaum. 1994. Menelas: an access system for medical records using natural language. Computer
Methods and Programs in Biomedicine, 45.
10
Proceedings of the 4th International Workshop on Computational Terminology, pages 94?103,
Dublin, Ireland, August 23 2014.
Unsupervised method for the acquisition of general language paraphrases
for medical compounds
Natalia Grabar
CNRS UMR 8163 STL
Universit?e Lille 3
59653 Villeneuve d?Ascq, France
natalia.grabar@univ-lille3.fr
Thierry Hamon
LIMSI-CNRS, BP133, Orsay
Universit?e Paris 13
Sorbonne Paris Cit?e, France
hamon@limsi.fr
Abstract
Medical information is widespread in modern society (e.g. scientific research, medical blogs,
clinical documents, TV and radio broadcast, novels). Moreover, everybody?s life may be con-
cerned with medical problems. However, the medical field conveys very specific and often
opaque notions (e.g., myocardial infarction, cholecystectomy, abdominal strangulated hernia,
galactose urine), that are difficult to understand by lay people. We propose an automatic method
based on the morphological analysis of terms and on text mining for finding the paraphrases of
technical terms. Analysis of the results and their evaluation indicate that we can find correct
paraphrases for 343 terms. Depending on the semantics of the terms, error rate of the extractions
ranges between 0 and 59%. This kind of resources is useful for several Natural Language Pro-
cessing applications (i.e., information extraction, text simplification, question and answering).
1 Background
Medical and health information is widespread in the modern society in light of pressing health concerns
and of maintaining of healthy lifestyles. Besides, it is also available through modern media: scientific
research, articles, medical blogs and fora, clinical documents, TV and radio broadcast, novels, discussion
fora, epidemiological alerts, etc. Still, availability of medical and health information does not guaran-
tee its easy and correct understanding by lay people. The medical field conveys indeed very technical
notions, such as in example (1).
(1) myocardial infarction, cholecystectomy, erythredema polyneuropathy, acromegaly, galactosemia
Although technical, these notions are nevertheless important for patients (AMA, 1999; McCray, 2005;
Eysenbach, 2007; Oregon Evidence-based Practice Center, 2008). It has been shown that in several
situations such notions cannot be correctly understood by patients: the steps needed for the medication
preparing and use (Patel et al., 2002); the instructions on drugs from patient package inserts, and the
information delivered in informed consensus and health brochures: it appears that among the 2,600
patients recruited in two hospitals, 26% to 60% cannot manage information available in these sources
(Williams et al., 1995); health information in different languages (English, Spanish, French) provided in
websites created for patients require high reading level (Berland et al., 2001; Hargrave et al., 2003; Kusec,
2004) and remains difficult to manage by patients, which can be negative for the communication between
patients and medical professionals, and the healthcare process (Tran et al., 2009). This situation sets the
context of our work. Our objective is to propose method for the automatic acquisition of paraphrases for
technical medical notions. More particularly, we propose to concentrate on terms and their words that
show neoclassical compounding word formation (Booij, 2010; Iacobini, 1997; Amiot and Dal, 2005),
such as in the example (1). Such words often involve Latin and Greek roots or bases, which makes them
more difficult to understand, as such words must be decomposed first (see examples (2) and (3)). To our
knowledge, this kind of approach has not been applied for the acquisition of laymen paraphrases.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
94
(2) myocardial is formed with Latin myo (muscle) and Greek cardia (heart)
(3) cholecystectomy is formed with Greek chole (bile), Latin cystis (bladder), and Greek ectomy (surgical
removal)
Our work is related to the following research topics:
? Readability. The readability studies the ease in which text can be understood. Two kinds of readabil-
ity measures are distinguished: classical and computational (Franc?ois, 2011). Classical measures
are usually based on number of characters and/or syllables in words, sentences or documents and on
linear regression models (Flesch, 1948; Gunning, 1973; Dubay, 2004). Computational measures,
that are more recent, can involve vectorial models and a great variety of descriptors. These de-
scriptors, usually specific to the texts processed, are for instance: combination of classical measures
with medical terminologies (Kokkinakis and Toporowska Gronostaj, 2006); n-grams of characters
(Poprat et al., 2006); discursive descriptors (Goeuriot et al., 2007); lexicon (Miller et al., 2007);
morphological descriptors (Chmielik and Grabar, 2011); combination of various descriptors (Wang,
2006; Zeng-Treiler et al., 2007; Leroy et al., 2008; Franc?ois and Fairon, 2013).
? Lexical simplification. The lexical simplification helps to make text easier to understand. Lexical
simplification of texts in English has been addressed during the SemEval 2012 challenge
a
. Given
a short input text and a target word in English, and given several English substitutes for the tar-
get word that fit the context, the goal was to rank these substitutes according to how simple they
are (Specia et al., 2012). Several clues have been applied: lexicon extracted from oral corpus and
Wikipedia, Google n-grams, WordNet (Sinha, 2012); word length, number of syllables, mutual in-
formation and frequency of words (Jauhar and Specia, 2012); frequency in Wikipedia, word length,
n-grams of characters and of words, syntactic complexity of documents (Johannsen et al., 2012);
n-grams, frequency in Wikipedia, n-Google grams (Ligozat et al., 2012); WordNet and word fre-
quency (Amoia and Romanelli, 2012).
? Dedicated resources. The building of resources suitable for performing the simplification is an-
other related research topics. Such resources are mainly two-fold lexica in which specialized and
non-specialized vocabularies are aligned (in examples (4) to (6), the technical terms are followed
by their non-technical equivalents). The first initiative of the kind appeared with the collaborative
effort Consumer Health Vocabulary (Zeng and Tse, 2006) (examples in (4)). One of the meth-
ods was applied to the most frequently occurring medical queries aligned to the UMLS (Unified
Medical Language System) concepts (Lindberg et al., 1993). Another work exploited a small cor-
pus and several statistical association measures for building aligned lexicon with technical terms
from the UMLS and their lay equivalents (Elhadad and Sutaria, 2007). Similar work in other lan-
guages followed. In French, researchers proposed methods for the acquisition of syntactic variation
(Del?eger and Zweigenbaum, 2008; Cartoni and Del?eger, 2011) from comparable specialized and
non-specialized corpora, that led to the detection of verb/noun variations (examples in (5)) and a
larger set of syntactic variations (examples in (6)). Besides, research on the acquisition of termi-
nological variation (Hahn et al., 2001), synonymy (Fern?andez-Silva et al., 2011) and paraphrasing
(Max et al., 2012) is also relevant to outline the topics.
(4) {myocardial infarction, heart attack}, {abortion, termination of pregnancy}, {acrodynia, pink
disease}
(5) {consommation r?eguli`ere, consommer de fac?on r?eguli`ere} (regular use), {g?ene `a la lecture,
emp?eche de lire} (reading difficulty), {?evolution de l?affection, la maladie ?evolue} (evolution of the
condition)
(6) {retard de cicatrisation, retarder la cicatrisation} (delay the healing), {apports caloriques, apport
en calories} (calorie supply), {calculer les doses, doses sont calcul?ees} (calculate the dose), {efficacit?e
est renforc?ee, renforcer son efficacit?e} (improve the efficiency)
a
http://www.cs.york.ac.uk/semeval-2012/
95
Our work is closely related to the building of resources dedicated to the lexical simplification. Our
objective is to propose method for paraphrasing the technical medical terms (i.e. medical compounds) in
expressions that are easier to understand by lay people. This aspect is seldom addressed: we can observe
that only some examples in (4) are concerned with the paraphrasing of technical and compound terms
(myocardial infarction, acrodynia). We work with the French data. Contrary to previous work, we do
not use comparable corpora with technical and non-technical texts. Instead, we exploit terms from an
existing medical terminology and corpora built from social media sources. We assume that this kind
of corpora may provide lay people equivalents for technical terms. We also rely on the morphological
analysis of technical terms. The expected result is to obtain pairs like {myocardial, heart muscle} or
{cholecystectomy, removal of gall bladder}. In the following, we start with the presentation of the
resources used (section 2), we present then the steps of the methodology (section 3). We describe and
discuss the obtained results (section 4) and conclude with some directions for future work (section 5).
2 Resources
2.1 Medical terms
The material processed is issued from the French part of the UMLS. It provides syntactically simple terms
that contain one word only (acrodynia), and syntactically complex terms that contain more than one word
(myocardial infarction). Syntactically complex terms are segmented in words. Each term is associated
to semantic types. When a given word receives more than one semantic type, a manual post-processing
allows to disambiguate it: each word is assigned to one semantic type only. Among the semantic types
available, we consider the three most common in the medical practice to which the lay people are the
most exposed: Anatomy (616 words): describe human body anatomy (e.g. abdominopelvic); Disorders
(2,283 words): describe medical problems and their signs (e.g. infarction, diabetes); Procedures (1,271
words): describe procedures which may be performed by medical staff to detect or cure disorders (e.g.
cholecystectomy). In what follows, word and term can be exchangeable and mean either the graphical
unit provided by the segmentation, or the medical notion.
2.2 Corpora
Wiki LesDiab DiabDoct HT Dos
Number of pages/threads 17,525 6,939 387,435 67,652 8,319
Number of articles/messages 17,525 1,438 22,431 12,588 1,124
Number of words 4,326,880 624,571 35,059,868 6,788,361 836,520
Table 1: Size of the corpora exploited.
We use several corpora collected from the social media sources (their sizes are indicated in Table 1):
1. Wiki contains French Wikipedia articles downloaded in February 2014, of which we keep those that
are categorized under the medical category Portail de la m?edecine;
2. LesDiab is collected from the discussion forum Les diab?etiques
b
posted between June and July
2013. It is dedicated to diabetes;
3. DiabDoct is collected in June 2011 from the discussion forum Diab`ete of Doctissimo
c
4. HT is collected in May 2013 from the discussion forum Hypertension of Doctissimo
d
5. Dos is collected in May 2013 from the discussion forum Douleurs de dos (backache) of Doctissimo
e
b
http://www.lesdiabetiques.com/modules.php?name=Forums
c
http://forum.doctissimo.fr/sante/diabete/liste sujet-1.htm
d
http://forum.doctissimo.fr/sante/hypertension-problemes-cardiaques/liste sujet-1.htm
e
http://forum.doctissimo.fr/sante/douleur-dos/liste sujet-1.htm
96
The Wiki corpus contains encyclopaedic information on several medical notions from Wikipedia. Thanks
to the collaborative writing of the articles, these contain mostly correct information about the topics
concerned. Other corpora are collected from the dedicated fora (e.g. diabetes or backache). We assume
that people involved in these discussions may show low, middle or high degree of knowledge about the
disorders and related notions. We expect that all our corpora are written in a simple style and that they
contain paraphrases of technical terms. From Table 1, we can observe that the corpora vary in size.
3 Methodology for the automatic acquisition of paraphrases for medical compounds
The methodology is designed for analyzing the neoclassical medical compounds and for searching their
non-technical paraphrases in corpora. In our approach, the paraphrases may occur alone, such as heart
muscle, without being accompanied by their technical compounds (myocarde). In this case, we need first
to acquire the knowledge needed for their automatic detection. We propose to rely on the morphological
analysis of terms. The method is composed of four main steps: the processing of terms, the processing of
corpora, the extraction of layman paraphrases for technical terms, and the evaluation of the extractions.
3.1 The processing of medical terms
To reach the morphological information on terms we apply three specific processing:
1. Morpho-syntactic tagging and lemmatization of terms. The terms are morpho-syntactically tagged
and lemmatized with TreeTagger for French (Schmid, 1994). The morpho-syntactic tagging is
done in context of the terms. If a given word receives more than one tag, the most frequent one is
kept. At this step, we obtain term lemmas with their part-of-speech tags, such as in example (7).
(7) myocardique/A (myocardial/A), chol?ecystectomie/N (cholecystectomy/N), polyneuropathie/N
(polyneuropathy/N), acrom?egalie/N (acromegaly/N), galactos?emie/N (galactosemia/N)
2. Morphological analysis. The lemmas are then morphologically analyzed with D?eriF (Namer,
2009). This tool performs the analysis of lemmas in order to detect their morphological structure,
to decompose them into their components (bases and affixes), and to semantically analyze their
structure. We give some examples of the morphological analysis in (8).
(8) myocardique/A: [[[myo N*] [carde N*] NOM] ique ADJ]
chol?ecystectomie/N: [[chol?ecysto N*] [ectomie N*] NOM]
polyneuropathie/N: [poly [[neur N*] [pathie N*] NOM] NOM]
acrom?egalie/N: [[acr N*] [m?egal N*] ie NOM]
galactos?emie/N: [[galactose NOM] [?em N*] ie NOM]
The computed bases and affixes are associated with syntactic categories (NOM, ADJ, V). When a
given base is suppletive (does not exist in modern French but was borrowed from Latin or Greek
languages), D?eriF assigns the most probable category (e.g. N* for nouns, A* for adjectives). For
instance, the analysis of myocardique/A indicates that this word contains the suppletive noun bases
myo N* (muscle) and carde N* (heart), and the affix -ique/ADJ. We can observe that some bases can
be decomposed further (e.g. galactose in galact (milk) and ose (sugars), cholecystectomy in chole (bile)
and cystis (bladder)). The words that contain more than one base are considered to be compounds
and are processed in the further steps of the method.
3. Association of morphological components with French words. The bases are ?translated? with words
from modern French. We use for this resource built in previous work (Zweigenbaum and Grabar,
2003; Namer, 2003) (see some examples in (9)).
(9) myocardique/A: myo=muscle (muscle), carde=coeur (heart)
chol?ecystectomie/N: chol?ecysto=v?esicule biliaire (gall bladder), ectomie=ablation (removal)
97
polyneuropathie/N: poly=nombreux (several), neuro=nerf (nerve), pathie=maladie (disorder)
acrom?egalie/N: acr=extr?emit?e (extremity), m?egal=grandeur (size)
galactos?emie/N: galactose=galactose (galactose), ?em=sang (blood)
Some words can remain technical (e.g., galactose, v?esicule biliaire), while other components totally
lose their technical meaning (e.g. m?egal=grandeur (size), poly=nombreux (several)).
3.2 The processing of corpora
The corpora are first segmented in words and sentences. Then, we also perform morpho-syntactic tagging
and lemmatization with TreeTagger for French.
3.3 The extraction of layman paraphrases corresponding to technical terms
French words corresponding to the morphological decomposition of terms (examples in (9)) are projected
on corpora in order to extract sentences and their segments which can provide the layman paraphrases for
the corresponding technical terms. Sentences that contain the translated French words are extracted as
candidates for proposing the paraphrases. Additionally, the segments delimited by these words are also
extracted. We consider the co-occurrence of the words issued from the morphological decomposition in
a sliding graphical window of n words. In the experiments presented, the window size n is fixed to 10
words. Smaller or larger windows show less performance.
(10) Les causes de tachycardie ventriculaire sont superposables `a celles des extrasystoles ventric-
ulaires: infarctus du myocarde, insuffisance cardiaque, hypertrophie du muscle du coeur et
prolapsus de la valve mitrale.
The sentence in (10) contains words muscle and coeur, underlined in the example, that correspond to
the morphological components of myocardique (see examples in (9)). For this reason, this sentence is
extracted, as well as the segment delimited by these two words muscle du coeur (heart muscle).
3.4 The evaluation
The objective of the evaluation is to assess whether the proposed method is valid for the acquisition
of paraphrases for technical medical terms. The obtained results are evaluated manually by a com-
puter scientist with no training in biomedicine, but with background in computational linguistics and
morphology. We analyze the candidates for paraphrases from several points of view: Are the French
words corresponding to the components extracted correctly? Do these French words provide valid can-
didates for paraphrases? How easy are these paraphrases to be understood by laymen or by non-experts
in medicine? During the evaluation related to the second point (Do these French words provide valid
candidates for paraphrases?), we distinguish four situations:
1. the extraction is correct: e.g. myocardique paraphrased in muscle du coeur (heart muscle);
2. the extraction suffers from the incorrect morphological decomposition or from the wrong ?trans-
lation? in French: e.g. p?erianal is ?translated? in autour (around) and an (meaning year as it is). The
?translation? of this last word an is not correct and should be anus (anus) instead. Because of the
wrong ?translation?, we collect a lot of incorrect segments like autour de 30 ans (around 30 years);
3. the extraction should be post-processed but contains the correct paraphrase: e.g. spondylarthrose,
?translated? in vert`ebre (vertebra) and arthrose (arthrosis), is paraphrased in arthrose que l?on ne voyait
pas sur la vert`ebre (arthrosis that was not seen on the vertebra), while the correct paraphrase from this
segment should be arthrose sur la vert`ebre (arthrosis on the vertebra);
4. the extraction is wrong and can provide no useful information.
This evaluation allows to estimate precision of the results in three versions: strong precision P
strong
(only
the correct extractions are considered (extractions from 1)); weak precision P
weak
(correct extractions
and extractions that need post-processing are considered (extractions from 1 and 3)); rate of incorrect
extractions %
incorrect
(the percentage of the incorrect extractions is computed (extractions from 4)).
98
4 Results and Discussion
4.1 The morphological analysis of terms
We generate the morphological analysis for 218 single words from the anatomy semantic type, 1,789 dis-
order words and 1,023 procedure words: over 70% of words are morphologically analyzed. Among these
words, we observe compounds (myocardique) and words formed with affixes (e.g. r?eadaptation derived
from adaptation, derived in its turn from adapter). The remaining words may be simple (e.g. abc`es
(abscess), l`epre (leprosy), cicatrice (scar)) or contain bases and affixes that are not managed by D
?erif (e.g.
pneumostrongylose (pneumostrongylosis), lagophtalmie (lagophthalmos), n?ecatorose (necatorosis)). Among the
generated decompositions by D?erif, we can find some cases with ambiguous decomposition that occur
when medical terms can be decomposed in several possible ways, among which only one is semantically
correct. For instance, posturographie (posturography) is decomposed into: [post [[uro N*] [graphie N*]
NOM] NOM], which may be glossed as control during the period which follows the therapy done on
the urinary system. From the formal point of view, such decomposition is very possible, although it is
weak semantically. For the term posturographie, the right decomposition is: [[posturo N*] [graphie
N*] NOM], which is related to the definition of the optimal body position when walking or sitting. As
indicated above, some terms (e.g. p?erianal) can be incorrectly ?translated? in French.
4.2 The preprocessing of corpora
Our main difficulty at this step is related to the processing of forum messages and to their segmentation
into sentences. In addition to possible and frequent spelling and grammatical errors, forum messages
have also a very specific punctuation, which may be missing or convey personal feelings and emotions.
This seriously impedes the possibility to provide the correct segmentation in sentences, and means that,
because of the missing punctuation, the mapping of decomposed terms with corpora may be done with
bigger text segments in which the semantic relations between the mapped components may be weak or
non-existent, and provide incorrect extractions. We plan to combine the current method with the syntactic
analysis in order to ensure that stronger syntactic and semantic relations exist between the components.
4.3 The extraction of paraphrases and their evaluation
We present the results on extraction of sentences and paraphrases from the corpora processed. In Table
2, for the three semantic types of terms (anatomy ana., disorders dis., and procedures pro.) from each
corpus, we indicate the following information: the number of different sentences extracted (sentences),
yje number of different terms (uniq. terms), the number of correct paraphrases (correct), the number
of paraphrases that are possibly correct (pos. correct), the number of paraphrases which morphological
analysis and ?translation? should be improved (morph. ana.), and the number of incorrect paraphrases
(incorrect). The last three lines indicate the precision values: strong precision (P
strong
), weak precision
(P
weak
) and incorrect extractions (%
incorrect
).
Number of Wiki LesDiab DiabDoct HT Dos
ana dis pro ana dis pro ana dis pro ana dis pro ana dis pro
sentences 1238 4003 999 15 71 10 721 2901 564 246 1233 678 42 708 30
uniq. terms 93 382 154 7 30 5 35 204 48 29 133 42 13 44 13
correct 469 1571 364 3 32 4 227 1189 67 114 637 38 12 466 13
pos. correct 270 868 93 3 7 - 40 332 5 10 85 9 3 98 2
morph. ana. 41 155 323 1 2 6 100 3 394 22 - 591 2 1 12
incorrect 462 1424 220 8 30 - 354 1 98 100 511 40 25 135 3
P
strong
38 39 36 20 45 40 32 40 12 46 52 6 29 66 43
P
weak
60 61 46 40 55 40 37 52 13 50 59 7 36 80 50
%
incorrect
40 39 54 53 42 0 49 47 17 41 41 41 59 20 10
Table 2: Results on the paraphrases extracted and evaluated.
99
From the data presented in Table 2, we can propose several observations: (1) the Wiki corpus, that is
not the largest in our dataset, provides the largest number of extractions (sentences and unique terms);
(2) among the three semantic types (anatomy, disorders and procedures), the number of paraphrases ex-
tracted for disorders is the largest in all corpora; (3) the largest set of paraphrases, that suffer from the
incorrect morphological decomposition or ?translation?, is obtained for the procedure terms. According
to these observations, P
strong
ranges between 20 to 46% for anatomy, 39 and 66% for disorders, and 6 to
43 for procedures. The P
weak
values, that takes into account the paraphrases that need post-processing,
show the increase by 0 to 28% by comparison with the P
strong
values. The %
incorrect
values indicate
that anatomy terms show the largest rate (40 to 59%) of incorrect paraphrases: it is possible that the
anatomy terms present the lowest rate of compositionality. The incorrect paraphrases are between 20
and 47 among the disorder terms, and between 0 to 54 among the procedure terms. The syntactic anal-
ysis may help to improve the current results. On the whole, the proposed method allows to extract the
paraphrases for 722 different terms from the corpora processed. Within the evaluated set of extractions,
these paraphrases are correct for 273 terms; while 343 terms are provided with correct paraphrases and
paraphrases that need to be post-processed. Most of the extracted paraphrases are noun phrases, and, at
a lesser extent, verb phrases. We present some examples of the correct paraphrases extracted:
- dorsalgie (dorsalgia): douleur dans le dos (pain in the back)
- my?elocyte (myelocyte): cellules dans la moelle osseuse (cells of the bone marrow)
- lombalgie (lombalgia): douleurs dans les reins (pain in kidney)
- gastralgie (gastralgia): douleurs `a l?estomac (stomach pain)
- desmorrhexie (desmorrhexia): rupture des ligaments (ligamentous rupture)
- h?epatite (hepatitis): inflammation du foie (liver inflammation)
We can find several types of paraphrases that suffer from incorrect decomposition or ?translation?:
? syringomy?elie (syringomyelia) is currently ?translated? in moelle (marrow or spinal cord) and canal (canal).
This term means a disorder in which a cyst or cavity forms within the spinal cord. We assume that
a more correct ?translation? of this term should be: moelle (marrow or spinal cord) and cavit?e (cavity);
? sous-dural is ?translated? in sous (sub) and dur (hard). The term is related to specific space in brain
that can be opened by the separation of the arachnoid mater from the dura mater. Concerning its
?translation?, we assume that dure-m`ere (dura mater) should be used instead of dur (hard). Besides,
the names of anatomical locations often remains difficult to understand. We assume that even when
terms are decomposed and ?translated? correctly, the paraphrases for such terms may be not suitable
for laymen: other types of explanations (e.g. schemes or pictures) should be used instead;
? hyper?emie (hyperaemia) is ?translated? in hyper and sang (blood). The term means the increase of
blood flow to different tissues in the body. This term is not fully compositional because the notion
of tissues is absent, while necessary for its understanding. The proposed extractions for this term
mainly come from corpora related to diabetes, in which hyper and hypo are often used in relation
with the hyperglycemia or hypoglycemia. This means that hyper should be ?translated? with other
words, such as increase or elevated;
? h?et?erotopie is translated in autre (another) and endroit (place). The term means the displacement of an
organ from its normal position and that [an organ] is found in another place than the one expected.
This term brings no correct candidates for paraphrases because: it is not fully compositional and its
?translation? provides very common words widely used in the corpora.
Among the incorrect extractions we can find: (1) more terms with non-compositional semantics (such
as ost?eodermie (osteoderm), causalgie (causalgia), ad?eno??de (adenoid), or xanthochromie (xanthochromia)) for
which the extracted paraphrases capture only part of the meaning; and (2) extractions that must be con-
troled by the syntactic analysis (e.g. petite boule de peau qui a sortie entre l?ongle et... (small skinball
that appeared between the nail and...) for micronychie (micronychia)) to make them more grammatical. Para-
phrases extracted from the Wiki corpus cover larger range of medical terms, while those extracted from
100
fora dedicated to a given medical topics are redundant. On the whole, we can consider that the currently
proposed method allows extracting interesting candidates as the paraphrases of technical terms, that are
indeed much easier to understand than the technical terms by themselves.
If we compare the obtained results with those presented in previous work, we can observe that:
? we extract paraphrases for larger number of terms: 343 terms with correct and possibly correct
paraphrases (722 terms with paraphrases in total) in our work against a total of 65 and 82 in (Del?eger
and Zweigenbaum, 2008), 109 in (Cartoni and Del?eger, 2011), and 152 in (Elhadad and Sutaria,
2007). In our work, the terms may receive more than one paraphrase;
? the precision values we obtain are comparable with those indicated in previous work: 67% and 60%
in (Del?eger and Zweigenbaum, 2008), 66% in (Cartoni and Del?eger, 2011), and 58% in (Elhadad
and Sutaria, 2007);
? in the cited work, the content of the corpora is explored but no reference is done to the set of terms
expected to be found. Because we work with a termset, we can compute the recall. If we consider the
terms that can be analyzed morphologically (3,030 terms), and for which we can find the paraphrases
with the proposed method, the recall value is close to 10% with the correct paraphrases (299 terms),
and to 24% with all the paraphrases extracted (722 terms). Yet, it is not sure that all of the terms, that
have been analyzed morphologically, can be provided with paraphrases in the corpora processed.
Besides, we should not forget that the nature of compounds and the decomposition of terms into com-
ponents also mean that specific semantic relations exist between these components (Namer and Zweigen-
baum, 2004; Booij, 2010). These are inherent to the syntactic constructions extracted. The characteristics
of these relations will be described and modeled in future work.
5 Conclusions and Future work
We propose to exploit social media texts in order to detect paraphrases for technical medical terms,
concentrating particularly on neoclassical compounds (e.g., myocardial, cholecystectomy, galactose,
acromegaly). The work is done in French. The method relies on the morphological analysis of terms,
on the ?translation? of the components of terms in modern French words (e.g. {card, heart}), and on
the projection of these words on corpora. The method allows extracting correct and possibly correct
paraphrases for up to 343 technical terms. For covering larger set of terms, additional corpora must be
treated. The extracted paraphrases are easier to understand than the original technical terms. Moreover,
the semantic relations among the components, although non explicated, are conveyed by the paraphrases.
We can consider that the method proves to be efficient and promising for the creation of lexicon suit-
able for the simplification of medical texts. Besides, the purpose of the method is to cover neoclassical
compound terms that are usually non treated with automatic approaches, as they do not present clear
formal similarity with their paraphrases. One of the difficulties we have currently is related to the lack of
constrains on the extracted segments. In future work, we plan to apply the syntactic analysis for parsing
the extracted sentences. Another possibility is to compute the probability for a given paraphrase to be
correct, which can rely for instance on frequency of the extracted paraphrases, on their syntactic struc-
ture, etc. In order to make the extraction of paraphrases more exhaustive, we will apply the method to
other corpora and we will use additional resources (synonyms, associative resources) for performing the
approximate mapping of paraphrases. In future work, we will take into account syntactically complex
terms and not only simple words. The very objective of our work is to exploit and test the resource
created for the simplification of medical texts.
Acknowledgments
The authors acknowledge the support of the Universit?e Paris 13 (project BQR Bonus Quality Research,
2011), the support of the MESHS Lille projet
?
Emergent CoMeTe, and the support of the French Agence
Nationale de la Recherche (ANR) and the DGA, under the Tecsan grant ANR-11-TECS-012.
101
References
AMA. 1999. Health literacy: report of the council on scientific affairs. Ad hoc committee on health literacy for
the council on scientific affairs, American Medical Association. JAMA, 281(6):552?7.
D Amiot and G Dal. 2005. Integrating combining forms into a lexeme-based morphology. In Mediterranean
Morphology Meeting (MMM5), pages 323?336.
M Amoia and M Romanelli. 2012. Sb: mmsystem - using decompositional semantics for lexical simplification.
In *SEM 2012, pages 482?486, Montr?eal, Canada, 7-8 June. Association for Computational Linguistics.
GK Berland, MN Elliott, LS Morales, JI Algazy, RL Kravitz, MS Broder, DE Kanouse, JA Munoz, JA Puyol,
M Lara, KE Watkins, H Yang, and EA McGlynn. 2001. Health information on the internet. accessibility,
quality, and readability in english ans spanish. JAMA, 285(20):2612?2621.
Geert Booij. 2010. Construction Morphology. Oxford University Press, Oxford.
B Cartoni and L Del?eger. 2011. Dcouverte de patrons paraphrastiques en corpus comparable: une approche base
sur les n-grammes. In TALN.
J Chmielik and N Grabar. 2011. D?etection de la sp?ecialisation scientifique et technique des documents
biom?edicaux gr?ace aux informations morphologiques. TAL, 51(2):151?179.
L Del?eger and P Zweigenbaum. 2008. Paraphrase acquisition from comparable medical corpora of specialized
and lay texts. In AMIA 2008, pages 146?50.
William H. Dubay. 2004. The principles of readability. Impact Information. Available at
http://almacenplantillasweb.es/wp-content/uploads/2009/11/The-Principles-of-Readability.pdf.
N Elhadad and K Sutaria. 2007. Mining a lexicon of technical terms and lay equivalents. In BioNLP, pages 49?56.
Gunther Eysenbach. 2007. Poverty, human development, and the role of ehealth. J Med Internet Res, 9(4):e34.
S Fern?andez-Silva, J Freixa, and MT Cabr?e. 2011. A proposed method for analysing the dynamics of cognition
through term variation. Terminology, 17(1):49?73.
R Flesch. 1948. A new readability yardstick. Journal of Applied Psychology, 23:221?233.
T Franc?ois and C Fairon. 2013. Les apports du TAL `a la lisibilit?e du franc?ais langue ?etrang`ere. TAL, 54(1):171?
202.
T Franc?ois. 2011. Les apports du traitements automatique du langage la lisibilit du franais langue trangre. Phd
thesis, Universit Catholique de Louvain, Louvain.
L Goeuriot, N Grabar, and B Daille. 2007. Caract?erisation des discours scientifique et vulgaris?e en franc?ais,
japonais et russe. In TALN, pages 93?102.
R Gunning. 1973. The art of clear writing. McGraw Hill, New York, NY.
Udo Hahn, Martin Honeck, Michael Piotrowsky, and Stefan Schulz. 2001. Subword segmentation - leveling out
morphological variations for medical document retrieval. In AMIA, 229-33.
Darren Hargrave, Ute Bartels, Loretta Lau, Carlos Esquembre, and
?
Eric Bouffet. 2003. ?evaluation de la qualit?e
de l?information m?edicale francophone accessible au public sur internet : application aux tumeurs c?er?ebrales de
l?enfant. Bulletin du Cancer, 90(7):650?5.
C Iacobini. 1997. Distinguishing derivational prefixes from initial combining forms. In First mediterranean
conference of morphology, Mytilene, Island of Lesbos, Greece, septembre.
SK Jauhar and L Specia. 2012. Uow-shef: Simplex ? lexical simplicity ranking based on contextual and psycholin-
guistic features. In *SEM 2012, pages 477?481, Montr?eal, Canada, 7-8 June. Association for Computational
Linguistics.
A Johannsen, H Mart??nez, S Klerke, and A S?gaard. 2012. Emnlp@cph: Is frequency all there is to simplicity?
In *SEM 2012, pages 408?412, Montr?eal, Canada, 7-8 June. Association for Computational Linguistics.
D Kokkinakis and M Toporowska Gronostaj. 2006. Comparing lay and professional language in cardiovascular
disorders corpora. In Australia Pham T., James Cook University, editor, WSEAS Transactions on BIOLOGY
and BIOMEDICINE, pages 429?437.
102
Sanja Kusec. 2004. Les sites web relatifs au diab`ete, sont-ils lisibles ? Dib`ete et soci?et?e, 49(3):46?48.
G Leroy, S Helmreich, J Cowie, T Miller, and W Zheng. 2008. Evaluating online health information: Beyond
readability formulas. In AMIA 2008, pages 394?8.
AL Ligozat, C Grouin, A Garcia-Fernandez, and D Bernhard. 2012. Annlor: A na??ve notation-system for lexical
outputs ranking. In *SEM 2012, pages 487?492.
DA Lindberg, BL Humphreys, and AT McCray. 1993. The unified medical language system. Methods Inf Med,
32(4):281?291.
Aur?elien Max, Houda Bouamor, and Anne Vilnat. 2012. Generalizing sub-sentential paraphrase acquisition across
original signal type of text pairs. In EMNLP, pages 721?31.
A McCray. 2005. Promoting health literacy. J of Am Med Infor Ass, 12:152?163.
T Miller, G Leroy, S Chatterjee, J Fan, and B Thoms. 2007. A classifier to evaluate language specificity of medical
documents. In HICSS, pages 134?140.
Fiammetta Namer and Pierre Zweigenbaum. 2004. Acquiring meaning for French medical terminology: contri-
bution of morphosemantics. In Annual Symposium of the American Medical Informatics Association (AMIA),
San-Francisco.
F Namer. 2003. Automatiser l?analyse morpho-s?emantique non affixale: le syst`eme D?eriF. Cahiers de Gram-
maire, 28:31?48.
F Namer. 2009. Morphologie, Lexique et TAL : l?analyseur D?eriF. TIC et Sciences cognitives. Hermes Sciences
Publishing, London.
Oregon Evidence-based Practice Center. 2008. Barriers and drivers of health information technology use for the
elderly, chronically ill, and underserved. Technical report, Agency for healthcare research and quality.
V Patel, T Branch, and J Arocha. 2002. Errors in interpreting quantities as procedures : The case of pharmaceutical
labels. International journal of medical informatics, 65(3):193?211.
M Poprat, K Mark?o, and U Hahn. 2006. A language classifier that automatically divides medical documents
for experts and health care consumers. In MIE 2006 - Proceedings of the XX International Congress of the
European Federation for Medical Informatics, pages 503?508, Maastricht.
H Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In ICNMLP, pages 44?49, Manchester,
UK.
R Sinha. 2012. Unt-simprank: Systems for lexical simplification ranking. In *SEM 2012, pages 493?496,
Montr?eal, Canada, 7-8 June. Association for Computational Linguistics.
L Specia, SK Jauhar, and R Mihalcea. 2012. Semeval-2012 task 1: English lexical simplification. In *SEM 2012,
pages 347?355.
TM Tran, H Chekroud, P Thiery, and A Julienne. 2009. Internet et soins : un tiers invisible dans la relation
m?edecine/patient ? Ethica Clinica, 53:34?43.
Y Wang. 2006. Automatic recognition of text difficulty from consumers health information. In IEEE, editor,
Computer-Based Medical Systems, pages 131?136.
MV Williams, RM Parker, DW Baker, NS Parikh, K Pitkin, WC Coates, and JR Nurss. 1995. Inadequate func-
tional health literacy among patients at two public hospitals. JAMA, 274(21):1677?82.
QT Zeng and T Tse. 2006. Exploring and developing consumer health vocabularies. JAMIA, 13:24?29.
Q Zeng-Treiler, H Kim, S Goryachev, A Keselman, L Slaugther, and CA Smith. 2007. Text characteristics of
clinical reports and their implications for the readability of personal health records. In MEDINFO, pages 1117?
1121, Brisbane, Australia.
Pierre Zweigenbaum and Natalia Grabar. 2003. Corpus-based associations provide additional morphological
variants to medical terminologies. In AMIA.
103
