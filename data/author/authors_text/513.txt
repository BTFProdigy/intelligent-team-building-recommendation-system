Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 280?287,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Learning with Annotation Noise
Eyal Beigman
Olin Business School
Washington University in St. Louis
beigman@wustl.edu
Beata Beigman Klebanov
Kellogg School of Management
Northwestern University
beata@northwestern.edu
Abstract
It is usually assumed that the kind of noise
existing in annotated data is random clas-
sification noise. Yet there is evidence
that differences between annotators are not
always random attention slips but could
result from different biases towards the
classification categories, at least for the
harder-to-decide cases. Under an annota-
tion generation model that takes this into
account, there is a hazard that some of the
training instances are actually hard cases
with unreliable annotations. We show
that these are relatively unproblematic for
an algorithm operating under the 0-1 loss
model, whereas for the commonly used
voted perceptron algorithm, hard training
cases could result in incorrect prediction
on the uncontroversial cases at test time.
1 Introduction
It is assumed, often tacitly, that the kind of
noise existing in human-annotated datasets used in
computational linguistics is random classification
noise (Kearns, 1993; Angluin and Laird, 1988),
resulting from annotator attention slips randomly
distributed across instances. For example, Os-
borne (2002) evaluates noise tolerance of shallow
parsers, with random classification noise taken to
be ?crudely approximating annotation errors.? It
has been shown, both theoretically and empiri-
cally, that this type of noise is tolerated well by
the commonly used machine learning algorithms
(Cohen, 1997; Blum et al, 1996; Osborne, 2002;
Reidsma and Carletta, 2008).
Yet this might be overly optimistic. Reidsma
and op den Akker (2008) show that apparent dif-
ferences between annotators are not random slips
of attention but rather result from different biases
annotators might have towards the classification
categories. When training data comes from one
annotator and test data from another, the first an-
notator?s biases are sometimes systematic enough
for a machine learner to pick them up, with detri-
mental results for the algorithm?s performance on
the test data. A small subset of doubly anno-
tated data (for inter-annotator agreement check)
and large chunks of singly annotated data (for
training algorithms) is not uncommon in compu-
tational linguistics datasets; such a setup is prone
to problems if annotators are differently biased.1
Annotator bias is consistent with a number of
noise models. For example, it could be that an
annotator?s bias is exercised on each and every in-
stance, making his preferred category likelier for
any instance than in another person?s annotations.
Another possibility, recently explored by Beigman
Klebanov and Beigman (2009), is that some items
are really quite clear-cut for an annotator with any
bias, belonging squarely within one particular ca-
tegory. However, some instances ? termed hard
cases therein ? are harder to decide upon, and this
is where various preferences and biases come into
play. In a metaphor annotation study reported by
Beigman Klebanov et al (2008), certain markups
received overwhelming annotator support when
people were asked to validate annotations after a
certain time delay. Other instances saw opinions
split; moreover, Beigman Klebanov et al (2008)
observed cases where people retracted their own
earlier annotations.
To start accounting for such annotator behavior,
Beigman Klebanov and Beigman (2009) proposed
a model where instances are either easy, and then
all annotators agree on them, or hard, and then
each annotator flips his or her own coin to de-
1The different biases might not amount to much in the
small doubly annotated subset, resulting in acceptable inter-
annotator agreement; yet when enacted throughout a large
number of instances they can be detrimental from a machine
learner?s perspective.
280
cide on a label (each annotator can have a different
?coin? reflecting his or her biases). For annota-
tions generated under such a model, there is a dan-
ger of hard instances posing as easy ? an observed
agreement between annotators being a result of all
coins coming up heads by chance. They therefore
define the expected proportion of hard instances in
agreed items as annotation noise. They provide
an example from the literature where an annota-
tion noise rate of about 15% is likely.
The question addressed in this article is: How
problematic is learning from training data with an-
notation noise? Specifically, we are interested in
estimating the degree to which performance on
easy instances at test time can be hurt by the pre-
sence of hard instances in training data.
Definition 1 The hard case bias, ? , is the portion
of easy instances in the test data that are misclas-
sified as a result of hard instances in the training
data.
This article proceeds as follows. First, we show
that a machine learner operating under a 0-1 loss
minimization principle could sustain a hard case
bias of ?( 1?
N
) in the worst case. Thus, while an-
notation noise is hazardous for small datasets, it is
better tolerated in larger ones. However, 0-1 loss
minimization is computationally intractable for
large datasets (Feldman et al, 2006; Guruswami
and Raghavendra, 2006); substitute loss functions
are often used in practice. While their tolerance to
random classification noise is as good as for 0-1
loss, their tolerance to annotation noise is worse.
For example, the perceptron family of algorithms
handle random classification noise well (Cohen,
1997). We show in section 3.4 that the widely
used Freund and Schapire (1999) voted percep-
tron algorithm could face a constant hard case bias
when confronted with annotation noise in training
data, irrespective of the size of the dataset. Finally,
we discuss the implications of our findings for the
practice of annotation studies and for data utiliza-
tion in machine learning.
2 0-1 Loss
Let a sample be a sequence x1, . . . , xN drawn uni-
formly from the d-dimensional discrete cube Id =
{?1, 1}d with corresponding labels y1, . . . , yN ?
{?1, 1}. Suppose further that the learning al-
gorithm operates by finding a hyperplane (w,?),
w ? Rd, ? ? R, that minimizes the empirical er-
rorL(w,?) =
?
j=1...N [yj?sgn(
?
i=1...d x
i
jw
i?
?)]2. Let there be H hard cases, such that the an-
notation noise is ? = HN .
2
Theorem 1 In the worst case configuration of in-
stances a hard case bias of ? = ?( 1?
N
) cannot be
ruled out with constant confidence.
Idea of the proof : We prove by explicit con-
struction of an adversarial case. Suppose there is
a plane that perfectly separates the easy instances.
The ?(N) hard instances will be concentrated in
a band parallel to the separating plane, that is
near enough to the plane so as to trap only about
?(
?
N) easy instances between the plane and the
band (see figure 1 for an illustration). For a ran-
dom labeling of the hard instances, the central
limit theorem shows there is positive probability
that there would be an imbalance between +1 and
?1 labels in favor of ?1s on the scale of
?
N ,
which, with appropriate constants, would lead to
the movement of the empirically minimal separa-
tion plane to the right of the hard case band, mis-
classifying the trapped easy cases.
Proof : Let v = v(x) =
?
i=1...d x
i denote the
sum of the coordinates of an instance in Id and
take ?e =
?
d ? F?1(
?
? ? 2?
d
2 + 12) and ?h =?
d ? F?1(? +
?
? ? 2?
d
2 + 12), where F (t) is the
cumulative distribution function of the normal dis-
tribution. Suppose further that instances xj such
that ?e < vj < ?h are all and only hard instances;
their labels are coinflips. All other instances are
easy, and labeled y = y(x) = sgn(v). In this case,
the hyperplane 1?
d
(1 . . . 1) is the true separation
plane for the easy instances, with ? = 0. Figure 1
shows this configuration.
According to the central limit theorem, for d,N
large, the distribution of v is well approximated by
N (0,
?
d). If N = c1 ? 2d, for some 0 < c1 < 4,
the second application of the central limit the-
orem ensures that, with high probability, about
?N = c1?2d items would fall between ?e and ?h
(all hard), and
?
? ? 2?
d
2N = c1
?
?2d would fall
between 0 and ?e (all easy, all labeled +1).
Let Z be the sum of labels of the hard cases,
Z =
?
i=1...H yi. Applying the central limit the-
orem a third time, for large N , Z will, with a
high probability, be distributed approximately as
2In Beigman Klebanov and Beigman (2009), annotation
noise is defined as percentage of hard instances in the agreed
annotations; this implies noise measurement on multiply an-
notated material. When there is just one annotator, no dis-
tinction between easy vs hard instances can be made; in this
sense, all hard instances are posing as easy.
281
0 ?e ?h
Figure 1: The adversarial case for 0-1 loss.
Squares correspond to easy instances, circles ? to
hard ones. Filled squares and circles are labeled
?1, empty ones are labeled +1.
N (0,
?
?N). This implies that a value as low as
?2? cannot be ruled out with high (say 95%) con-
fidence. Thus, an imbalance of up to 2
?
?N , or of
2
?
c1?2d, in favor of ?1s is possible.
There are between 0 and ?h about 2
?
c1
?
?2d
more?1 hard instances than +1 hard instances, as
opposed to c1
?
?2d easy instances that are all +1.
As long as c1 < 2
?
c1, i.e. c1 < 4, the empirically
minimal threshold would move to ?h, resulting in
a hard case bias of ? =
?
?
?
c12d
(1??)?c12d
= ?( 1?
N
).
To see that this is the worst case scenario, we
note that 0-1 loss sustained on ?(N) hard cases
is the order of magnitude of the possible imba-
lance between ?1 and +1 random labels, which
is ?(
?
N). For hard case loss to outweigh the loss
on the misclassified easy instances, there cannot
be more than ?(
?
N) of the latter 2
Note that the proof requires that N = ?(2d)
namely, that asymptotically the sample includes
a fixed portion of the instances. If the sample is
asymptotically smaller, then ?e will have to be ad-
justed such that ?e =
?
d ? F?1(?( 1?
N
) + 12).
According to theorem 1, for a 10K dataset with
15% hard case rate, a hard case bias of about 1%
cannot be ruled out with 95% confidence.
Theorem 1 suggests that annotation noise as
defined here is qualitatively different from more
malicious types of noise analyzed in the agnostic
learning framework (Kearns and Li, 1988; Haus-
sler, 1992; Kearns et al, 1994), where an adver-
sary can not only choose the placement of the hard
cases, but also their labels. In worst case, the 0-1
loss model would sustain a constant rate of error
due to malicious noise, whereas annotation noise
is tolerated quite well in large datasets.
3 Voted Perceptron
Freund and Schapire (1999) describe the voted
perceptron. This algorithm and its many vari-
ants are widely used in the computational lin-
guistics community (Collins, 2002a; Collins and
Duffy, 2002; Collins, 2002b; Collins and Roark,
2004; Henderson and Titov, 2005; Viola and
Narasimhan, 2005; Cohen et al, 2004; Carreras
et al, 2005; Shen and Joshi, 2005; Ciaramita and
Johnson, 2003). In this section, we show that the
voted perceptron can be vulnerable to annotation
noise. The algorithm is shown below.
Algorithm 1 Voted Perceptron
Training
Input: a labeled training set (x1, y1), . . . , (xN , yN )
Output: a list of perceptrons w1, . . . , wN
Initialize: t? 0; w1 ? 0; ?1 ? 0
for t = 1 . . . N do
y?t ? sign(?wt, xt?+ ?t)
wt+1 ? wt + yt?y?t2 ? xt
?t+1 ? ?t + yt?y?t2 ? ?wt, xt?
end for
Forecasting
Input: a list of perceptrons w1, . . . , wN
an unlabeled instance x
Output: A forecasted label y
y? ?
PN
t=1 sign(?wt, xt?+ ?t)
y ? sign(y?)
The voted perceptron algorithm is a refinement
of the perceptron algorithm (Rosenblatt, 1962;
Minsky and Papert, 1969). Perceptron is a dy-
namic algorithm; starting with an initial hyper-
plane w0, it passes repeatedly through the labeled
sample. Whenever an instance is misclassified
by wt, the hyperplane is modified to adapt to the
instance. The algorithm terminates once it has
passed through the sample without making any
classification mistakes. The algorithm terminates
iff the sample can be separated by a hyperplane,
and in this case the algorithm finds a separating
hyperplane. Novikoff (1962) gives a bound on the
number of iterations the algorithm goes through
before termination, when the sample is separable
by a margin.
282
The perceptron algorithm is vulnerable to noise,
as even a little noise could make the sample in-
separable. In this case the algorithm would cycle
indefinitely never meeting termination conditions,
wt would obtain values within a certain dynamic
range but would not converge. In such setting,
imposing a stopping time would be equivalent to
drawing a random vector from the dynamic range.
Freund and Schapire (1999) extend the percep-
tron to inseparable samples with their voted per-
ceptron algorithm and give theoretical generaliza-
tion bounds for its performance. The basic idea
underlying the algorithm is that if the dynamic
range of the perceptron is not too large then wt
would classify most instances correctly most of
the time (for most values of t). Thus, for a sample
x1, . . . , xN the new algorithm would keep track
of w0, . . . , wN , and for an unlabeled instance x it
would forecast the classification most prominent
amongst these hyperplanes.
The bounds given by Freund and Schapire
(1999) depend on the hinge loss of the dataset. In
section 3.2 we construct a difficult setting for this
algorithm. To prove that voted perceptron would
suffer from a constant hard case bias in this set-
ting using the exact dynamics of the perceptron is
beyond the scope of this article. Instead, in sec-
tion 3.3 we provide a lower bound on the hinge
loss for a simplified model of the perceptron algo-
rithm dynamics, which we argue would be a good
approximation to the true dynamics in the setting
we constructed. For this simplified model, we
show that the hinge loss is large, and the bounds
in Freund and Schapire (1999) cannot rule out a
constant level of error regardless of the size of the
dataset. In section 3.4 we study the dynamics of
the model and prove that ? = ?(1) for the adver-
sarial setting.
3.1 Hinge Loss
Definition 2 The hinge loss of a labeled instance
(x, y) with respect to hyperplane (w,?) and mar-
gin ? > 0 is given by ? = ?(?, ?) = max(0, ? ?
y ? (?w, x? ? ?)).
? measures the distance of an instance from
being classified correctly with a ?margin. Figure 2
shows examples of hinge loss for various data
points.
Theorem 2 (Freund and Schapire (1999))
After one pass on the sample, the probability
that the voted perceptron algorithm does not
? ?? ??? ?
Figure 2: Hinge loss ? for various data points in-
curred by the separator with margin ?.
predict correctly the label of a test instance
xN+1 is bounded by 2N+1EN+1
[
d+D
?
]2
where
D = D(w,?, ?) =
?
?N
i=1 ?
2
i .
This result is used to explain the convergence of
weighted or voted perceptron algorithms (Collins,
2002a). It is useful as long as the expected value of
D is not too large. We show that in an adversarial
setting of the annotation noise D is large, hence
these bounds are trivial.
3.2 Adversarial Annotation Noise
Let a sample be a sequence x1, . . . , xN drawn uni-
formly from Id with y1, . . . , yN ? {?1, 1}. Easy
cases are labeled y = y(x) = sgn(v) as before,
with v = v(x) =
?
i=1...d x
i. The true separation
plane for the easy instances is w? = 1?
d
(1 . . . 1),
?? = 0. Suppose hard cases are those where
v(x) > c1
?
d, where c1 is chosen so that the
hard instances account for ?N of all instances.3
Figure 3 shows this setting.
3.3 Lower Bound on Hinge Loss
In the simplified case, we assume that the algo-
rithm starts training with the hyperplane w0 =
w? = 1?
d
(1 . . . 1), and keeps it throughout the
training, only updating ?. In reality, each hard in-
stance can be decomposed into a component that is
parallel to w?, and a component that is orthogonal
to it. The expected contribution of the orthogonal
3See the proof of 0-1 case for a similar construction using
the central limit theorem.
283
0 c1?d
Figure 3: An adversarial case of annotation noise
for the voted perceptron algorithm.
component to the algorithm?s update will be posi-
tive due to the systematic positioning of the hard
cases, while the contributions of the parallel com-
ponents are expected to cancel out due to the sym-
metry of the hard cases around the main diagonal
that is orthogonal to w?. Thus, while wt will not
necessarily parallel w?, it will be close to parallel
for most t > 0. The simplified case is thus a good
approximation of the real case, and the bound we
obtain is expected to hold for the real case as well.
For any initial value ?0 < 0 all misclassified in-
stances are labeled ?1 and classified as +1, hence
the update will increase ?0, and reach 0 soon
enough. We can therefore assume that ?t ? 0
for any t > t0 where t0  N .
Lemma 3 For any t > t0, there exist ? =
?(?, T ) > 0 such that E(?2) ? ? ? ?.
Proof : For ? ? 0 there are two main sources
of hinge loss: easy +1 instances that are clas-
sified as ?1, and hard -1 instances classified as
+1. These correspond to the two components of
the following sum (the inequality is due to disre-
garding the loss incurred by a correct classification
with too wide a margin):
E(?2) ?
[?]?
l=0
1
2d
(
d
l
)
(
?
?
d
?
l
?
d
+ ?)2
+
1
2
d?
l=c1
?
d
1
2d
(
d
l
)
(
l
?
d
?
?
?
d
+ ?)2
Let 0 < T < c1 be a parameter. For ? > T
?
d,
misclassified easy instances dominate the loss:
E(?2) ?
[?]?
l=0
1
2d
(
d
l
)
(
?
?
d
?
l
?
d
+ ?)2
?
[T
?
d]?
l=0
1
2d
(
d
l
)
(
T
?
d
?
d
?
l
?
d
+ ?)2
?
T
?
d?
l=0
1
2d
(
d
l
)
(T ?
l
?
d
+ ?)2
?
1
?
2pi
? T
0
(T + ? ? t)2e?t
2/2dt = HT (?)
The last inequality follows from a normal ap-
proximation of the binomial distribution (see, for
example, Feller (1968)).
For 0 ? ? ? T
?
d, misclassified hard cases
dominate:
E(?2) ?
1
2
d?
l=c1
?
d
1
2d
(
d
l
)
(
l
?
d
?
?
?
d
+ ?)2
?
1
2
d?
l=c1
?
d
1
2d
(
d
l
)
(
l
?
d
?
T
?
d
?
d
+ ?)2
?
1
2
?
1
?
2pi
? ?
??1(?)
(t? T + ?)2e?t
2/2dt
= H?(?)
where ??1(?) is the inverse of the normal distri-
bution density.
Thus E(?2) ? min{HT (?),H?(?)}, and
there exists ? = ?(?, T ) > 0 such that
min{HT (?),H?(?)} ? ? ? ? 2
Corollary 4 The bound in theorem 2 does not
converge to zero for large N .
We recall that Freund and Schapire (1999) bound
is proportional to D2 =
?N
i=1 ?
2
i . It follows from
lemma 3 that D2 = ?(N), hence the bound is in-
effective.
3.4 Lower Bound on ? for Voted Perceptron
Under Simplified Dynamics
Corollary 4 does not give an estimate on the hard
case bias. Indeed, it could be that wt = w? for
almost every t. There would still be significant
hinge in this case, but the hard case bias for the
voted forecast would be zero. To assess the hard
case bias we need a model of perceptron dyna-
mics that would account for the history of hyper-
planesw0, . . . , wN the perceptron goes through on
284
a sample x1, . . . , xN . The key simplification in
our model is assuming that wt parallels w? for all
t, hence the next hyperplane depends only on the
offset ?t. This is a one dimensional Markov ran-
dom walk governed by the distribution
P(?t+1??t = r|?t) = P(x|
yt ? y?t
2
??w?, x? = r)
In general ?d ? ?t ? d but as mentioned before
lemma 3, we may assume ?t > 0.
Lemma 5 There exists c > 0 such that with a high
probability ?t > c ?
?
d for most 0 ? t ? N .
Proof : Let c0 = F?1(
?
2 +
1
2); c1 = F
?1(1??).
We designate the intervals I0 = [0, c0 ?
?
d]; I1 =
[c0 ?
?
d, c1 ?
?
d] and I2 = [c1 ?
?
d, d] and define
Ai = {x : v(x) ? Ii} for i = 0, 1, 2. Note that the
constants c0 and c1 are chosen so that P(A0) =
?
2
and P(A2) = ?. It follows from the construction
in section 3.2 that A0 and A1 are easy instances
and A2 are hard. Given a sample x1, . . . , xN , a
misclassification of xt ? A0 by ?t could only hap-
pen when an easy +1 instance is classified as ?1.
Thus the algorithm would shift ?t to the left by
no more than |vt ? ?t| since vt = ?w?, xt?. This
shows that ?t ? I0 implies ?t+1 ? I0. In the
same manner, it is easy to verify that if ?t ? Ij
and xt ? Ak then ?t+1 ? Ik, unless j = 0 and
k = 1, in which case ?t+1 ? I0 because xt ? A1
would be classified correctly by ?t ? I0.
We construct a Markov chain with three states
a0 = 0, a1 = c0 ?
?
d and a2 = c1 ?
?
d governed
by the following transition distribution:
?
?
?
?
1? ?2 0
?
2
?
2 1? ?
?
2
?
2
1
2 ?
3?
2
1
2 + ?
?
?
?
?
Let Xt be the state at time t. The principal eigen-
vector of the transition matrix (13 ,
1
3 ,
1
3) gives the
stationary probability distribution of Xt. Thus
Xt ? {a1, a2} with probability 23 . Since the tran-
sition distribution of Xt mirrors that of ?t, and
since aj are at the leftmost borders of Ij , respec-
tively, it follows that Xt ? ?t for all t, thus
Xt ? {a1, a2} implies ?t ? I1?I2. It follows that
?t > c0 ?
?
d with probability 23 , and the lemma
follows from the law of large numbers 2
Corollary 6 With high probability ? = ?(1).
Proof : Lemma 5 shows that for a sample
x1, . . . , xN with high probability ?t is most of
the time to the right of c ?
?
d. Consequently
for any x in the band 0 ? v ? c ?
?
d we get
sign(?w?, x?+?t) = ?1 for most t hence by defi-
nition, the voted perceptron would classify such
an instance as ?1, although it is in fact a +1 easy
instance. Since there are ?(N) misclassified easy
instances, ? = ?(1) 2
4 Discussion
In this article we show that training with annota-
tion noise can be detrimental for test-time results
on easy, uncontroversial instances; we termed this
phenomenon hard case bias. Although under
the 0-1 loss model annotation noise can be tole-
rated for larger datasets (theorem 1), minimizing
such loss becomes intractable for larger datasets.
Freund and Schapire (1999) voted perceptron al-
gorithm and its variants are widely used in compu-
tational linguistics practice; our results show that
it could suffer a constant rate of hard case bias ir-
respective of the size of the dataset (section 3.4).
How can hard case bias be reduced? One pos-
sibility is removing as many hard cases as one
can not only from the test data, as suggested in
Beigman Klebanov and Beigman (2009), but from
the training data as well. Adding the second an-
notator is expected to detect about half the hard
cases, as they would surface as disagreements be-
tween the annotators. Subsequently, a machine
learner can be told to ignore those cases during
training, reducing the risk of hard case bias. While
this is certainly a daunting task, it is possible that
for annotation studies that do not require expert
annotators and extensive annotator training, the
newly available access to a large pool of inexpen-
sive annotators, such as the Amazon Mechanical
Turk scheme (Snow et al, 2008),4 or embedding
the task in an online game played by volunteers
(Poesio et al, 2008; von Ahn, 2006) could provide
some solutions.
Reidsma and op den Akker (2008) suggest a
different option. When non-overlapping parts of
the dataset are annotated by different annotators,
each classifier can be trained to reflect the opinion
(albeit biased) of a specific annotator, using dif-
ferent parts of the datasets. Such ?subjective ma-
chines? can be applied to a new set of data; an
item that causes disagreement between classifiers
is then extrapolated to be a case of potential dis-
agreement between the humans they replicate, i.e.
4http://aws.amazon.com/mturk/
285
a hard case. Our results suggest that, regardless
of the success of such an extrapolation scheme in
detecting hard cases, it could erroneously invali-
date easy cases: Each classifier would presumably
suffer from a certain hard case bias, i.e. classify
incorrectly things that are in fact uncontroversial
for any human annotator. If each such classifier
has a different hard case bias, some inter-classifier
disagreements would occur on easy cases. De-
pending on the distribution of those easy cases in
the feature space, this could invalidate valuable
cases. If the situation depicted in figure 1 corre-
sponds to the pattern learned by one of the clas-
sifiers, it would lead to marking the easy cases
closest to the real separation boundary (those be-
tween 0 and ?e) as hard, and hence unsuitable for
learning, eliminating the most informative mate-
rial from the training data.
Reidsma and Carletta (2008) recently showed
by simulation that different types of annotator
behavior have different impact on the outcomes of
machine learning from the annotated data. Our re-
sults provide a theoretical analysis that points in
the same direction: While random classification
noise is tolerable, other types of noise ? such as
annotation noise handled here ? are more proble-
matic. It is therefore important to develop models
of annotator behavior and of the resulting imper-
fections of the annotated datasets, in order to di-
agnose the potential learning problem and suggest
mitigation strategies.
References
Dana Angluin and Philip Laird. 1988. Learning from
Noisy Examples. Machine Learning, 2(4):343?370.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From Annotator Agreement to Noise Models. Com-
putational Linguistics, accepted for publication.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing Disagreements. In
COLING 2008 Workshop on Human Judgments in
Computational Linguistics, pages 2?7, Manchester,
UK.
Avrim Blum, Alan Frieze, Ravi Kannan, and Santosh
Vempala. 1996. A Polynomial-Time Algorithm for
Learning Noisy Linear Threshold Functions. In Pro-
ceedings of the 37th Annual IEEE Symposium on
Foundations of Computer Science, pages 330?338,
Burlington, Vermont, USA.
Xavier Carreras, Llu?is Ma`rquez, and Jorge Castro.
2005. Filtering-Ranking Perceptron Learning for
Partial Parsing. Machine Learning, 60(1):41?71.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense Tagging of Unknown Nouns in WordNet.
In Proceedings of the Empirical Methods in Natural
Language Processing Conference, pages 168?175,
Sapporo, Japan.
William Cohen, Vitor Carvalho, and Tom Mitchell.
2004. Learning to Classify Email into ?Speech
Acts?. In Proceedings of the Empirical Methods
in Natural Language Processing Conference, pages
309?316, Barcelona, Spain.
Edith Cohen. 1997. Learning Noisy Perceptrons by
a Perceptron in Polynomial Time. In Proceedings
of the 38th Annual Symposium on Foundations of
Computer Science, pages 514?523, Miami Beach,
Florida, USA.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In
Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 263?370,
Philadelphia, USA.
Michael Collins and Brian Roark. 2004. Incremen-
tal Parsing with the Perceptron Algorithm. In Pro-
ceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, pages 111?118,
Barcelona, Spain.
Michael Collins. 2002a. Discriminative Training
Methods for Hidden Markov Hodels: Theory and
Experiments with Perceptron Algorithms. In Pro-
ceedings of the Empirical Methods in Natural Lan-
guage Processing Conference, pages 1?8, Philadel-
phia, USA.
Michael Collins. 2002b. Ranking Algorithms for
Named Entity Extraction: Boosting and the Voted
Perceptron. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 489?496, Philadelphia, USA.
Vitaly Feldman, Parikshit Gopalan, Subhash Khot, and
Ashok Ponnuswami. 2006. New Results for Learn-
ing Noisy Parities and Halfspaces. In Proceedings
of the 47th Annual IEEE Symposium on Foundations
of Computer Science, pages 563?574, Los Alamitos,
CA, USA.
William Feller. 1968. An Introduction to Probability
Theory and Its Application, volume 1. Wiley, New
York, 3rd edition.
Yoav Freund and Robert Schapire. 1999. Large Mar-
gin Classification Using the Perceptron Algorithm.
Machine Learning, 37(3):277?296.
Venkatesan Guruswami and Prasad Raghavendra.
2006. Hardness of Learning Halfspaces with Noise.
In Proceedings of the 47th Annual IEEE Symposium
on Foundations of Computer Science, pages 543?
552, Los Alamitos, CA, USA.
286
David Haussler. 1992. Decision Theoretic General-
izations of the PAC Model for Neural Net and other
Learning Applications. Information and Computa-
tion, 100(1):78?150.
James Henderson and Ivan Titov. 2005. Data-Defined
Kernels for Parse Reranking Derived from Proba-
bilistic Models. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 181?188, Ann Arbor, Michigan, USA.
Michael Kearns and Ming Li. 1988. Learning in the
Presence of Malicious Errors. In Proceedings of the
20th Annual ACM symposium on Theory of Comput-
ing, pages 267?280, Chicago, USA.
Michael Kearns, Robert Schapire, and Linda Sellie.
1994. Toward Efficient Agnostic Learning. Ma-
chine Learning, 17(2):115?141.
Michael Kearns. 1993. Efficient Noise-Tolerant
Learning from Statistical Queries. In Proceedings
of the 25th Annual ACM Symposium on Theory of
Computing, pages 392?401, San Diego, CA, USA.
Marvin Minsky and Seymour Papert. 1969. Percep-
trons: An Introduction to Computational Geometry.
MIT Press, Cambridge, Mass.
A. B. Novikoff. 1962. On convergence proofs on per-
ceptrons. Symposium on the Mathematical Theory
of Automata, 12:615?622.
Miles Osborne. 2002. Shallow Parsing Using Noisy
and Non-Stationary Training Material. Journal of
Machine Learning Research, 2:695?719.
Massimo Poesio, Udo Kruschwitz, and Chamberlain
Jon. 2008. ANAWIKI: Creating Anaphorically An-
notated Resources through Web Cooperation. In
Proceedings of the 6th International Language Re-
sources and Evaluation Conference, Marrakech,
Morocco.
Dennis Reidsma and Jean Carletta. 2008. Reliability
measurement without limit. Computational Linguis-
tics, 34(3):319?326.
Dennis Reidsma and Rieks op den Akker. 2008. Ex-
ploiting Subjective Annotations. In COLING 2008
Workshop on Human Judgments in Computational
Linguistics, pages 8?16, Manchester, UK.
Frank Rosenblatt. 1962. Principles of Neurodynamics:
Perceptrons and the Theory of Brain Mechanisms.
Spartan Books, Washington, D.C.
Libin Shen and Aravind Joshi. 2005. Incremen-
tal LTAG Parsing. In Proceedings of the Human
Language Technology Conference and Empirical
Methods in Natural Language Processing Confer-
ence, pages 811?818, Vancouver, British Columbia,
Canada.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and Fast ? But is it
Good? Evaluating Non-Expert Annotations for Nat-
ural Language Tasks. In Proceedings of the Empir-
ical Methods in Natural Language Processing Con-
ference, pages 254?263, Honolulu, Hawaii.
Paul Viola and Mukund Narasimhan. 2005. Learning
to Extract Information from Semi-Structured Text
Using a Discriminative Context Free Grammar. In
Proceedings of the 28th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval, pages 330?337, Salvador,
Brazil.
Luis von Ahn. 2006. Games with a purpose. Com-
puter, 39(6):92?94.
287
Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 1?8,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Discourse Topics and Metaphors
Beata Beigman Klebanov
Northwestern University
beata@northwestern.edu
Eyal Beigman
Washington University in St. Louis
beigman@wustl.edu
Daniel Diermeier
Northwestern University
d-diermeier@northwestern.edu
Abstract
Using metaphor-annotated material that is
sufficiently representative of the topical
composition of a similar-length document in
a large background corpus, we show that
words expressing a discourse-wide topic of
discussion are less likely to be metaphorical
than other words in a document. Our
results suggest that to harvest metaphors more
effectively, one is advised to consider words
that do not represent a discourse topic.
Traditionally, metaphor detectors use the
observation that a metaphorically used item creates
a local incongruity because there is a violation
of a selectional restriction, such as providing a
non-vehicle object to the verb derail in Protesters
derailed the conference. Current state of art
in metaphor detection therefore tends to be
?localistic? ? the distributional profile of the target
word in its immediate grammatical or collocational
context in a background corpus or a database
like WordNet is used to determine metaphoricity
(Mason, 2004; Krishnakumaran and Zhu, 2007;
Birke and Sarkar, 2006; Gedigian et al, 2006; Fass,
1991).
However, some theories of metaphor postulate
certain features of metaphors that connect it to the
surrounding text beyond the small grammatical or
proximal locality. For example, for Kittay (1987)
metaphor is a discourse phenomenon; although
the minimal metaphoric unit is a clause, often
much larger chunks of text constitute a metaphor.
Consider, for example, the TRAIN metaphor in the
following excerpt from a Sunday Times article on
20 September 1992:
Thatcher warned EC leaders to stop their
endless round of summits and take notice
of their own people. ?There is a fear that
the European train will thunder forward,
laden with its customary cargo of gravy,
towards a destination neither wished for
nor understood by electorates. But the
train can be stopped,? she said.
In the example above, the quotation is not in itself
a metaphor, as there is no indication that something
other than the actual train is being discussed (and
so no local incongruities exist). Only when situated
in the context prepared by the first sentence (and
indeed the rest of the article), the train imagery
becomes a metaphor.
According to Kittay, a metaphor occurs when
a semantic field is used to discuss a different
content domain. The theory therefore predicts that a
metaphorically used semantic domain would be off-
topic in the given document.
Although a single document can have singular,
idiosyncratic topics, it is likelier to discuss a mix of
topics that are typical of the discourse of which it is
part. We therefore derive the following hypothesis:
Words in a given document that represent a common
topic of discussion in a corpus of relevant documents
would be predominantly non-metaphorical. That is,
a smaller share of metaphorically used words in a
document would fall in such topical words than the
share of topical words in the document.
We test this hypothesis in the current article.
1
Using a large background corpus, we estimate
the topical composition of the target documents
(section 1) that were annotated for metaphors
(section 2). We then report the results of the
experiment (section 3) that strongly support the
hypothesis, and discuss the findings (section 4). The
concluding section provides a summary and outlines
the significance of the results for the practice of
metaphor detection.
1 Topic identification
1.1 EUI corpus
Our aimwas to create a large corpus of British media
discourse regarding the emerging European Union
institutions, with both Euro-phile and Euro-sceptic
camps represented. Our corpus consists of 12,814
articles drawn from three British newspapers: The
Guardian (34%), The Times (38%), and The
Independent (28%), dating from 1990 to 2000.
We used LexisNexis Academic1 to search for the
Subject index term European Union Institutions
(henceforth, EUI).2 After results are retrieved, we
further narrow them down to only documents on the
subject European Union Institutions in the detailed
subject index of the retrieved results.3,4
1.2 Identification of discourse topics
We converted all 12,858 documents5 (henceforth,
EUI+M corpus) into plain text format and removed
1http://academic.lexisnexis.com/online-services/academic-
features.aspx
2In LexisNexis subject index hierarchy: Government
and Public Administration/International Organization and
Bodies/International Governmental Organizations/European
Union Institutions.
3In the initial search, an article that scores 72% on the
subject would be retrieved, but it would not be classified as
being on this subject, and so would not be included in the final
dataset. Articles in the final dataset tend to score about 90% on
the subject, according to LexisNexis index.
4There is a gap in LexisNexis? index coverage of The
Times during 1996-7 and of The Independent during 2000. To
avoid under-representation of the newspaper and of the relevant
years in the sample, we added articles returned for the search
SECTION(Home news) AND (European Union OR Brussels)
on The Times 01/1996 through 04/1998, and SECTION(News
AND NOT Foreign) AND (European Union OR Brussels) on
The Independent throughout 2000.
512,814 EUI corpus plus 44 documents annotated for
metaphors, to be described in section 2.
words from a list of 153 common function words.
We then constructed an indexing vocabulary V that
included all and only words that (a) contained only
letters; and (b) appeared at least 6 times in the
collection. All documents were indexed using this
21,046 word vocabulary. We will designate all the
indexed words in document i as Di.
To identify the main discourse topics in
the EUI+M corpus, we submitted the indexed
documents to an unsupervised clustering method
Latent Dirichlet Allocation (Blei et al, 2003)
(henceforth, LDA).6 The designation of the clusters
as topics is supported by findings reported in Blei
et al (2003) that the clusters contain information
relevant for topic discrimination. Additionally,
Chanen and Patrick (2007) show that LDA achieves
significant correlations with humans on a topic
characterization task, where humans produced not
just a topic classification but also identified phrases
they believed were indicative of each class.
Using the default settings of LDA
implementation,7 we analyzed the corpus into
100 topics. Table 1 exemplifies some of the
emergent topics.
1.3 Topical words in a text
LDA is a generative model of text. According to its
outlook, every text is about a small (typically 5-7)
number of topics, and each indexed word in the text
belongs to one of these topics. However, in many
cases, the relationship between the word and the
topic is quite tentative, as the word is not particularly
likely given the topic. We therefore use parameter k
to control topic assignments ? we only take LDA?s
assignment of word to topic if the word is in the
top k most likely words for that topic. For k=25,
about 15% of in-vocabulary words in a document
are assigned to a topic; for k=400, about half the
in-vocabulary words are assigned to some topic. We
designate byTki all indexed words in document i that
are assigned to some topic for the given value of k.
The ratio |Tki ||Di| describes the proportion of discoursetopical words in the indexed words for the given
document.
6No stemming was performed.
7downloaded from http://www.cs.princeton.edu/?blei/lda-c/
2
Table 1: Examples of topics identified by LDA in the
EUI+M corpus. All words are taken from top 25 most
likely words given the topic. We boldface one word per
cluster, that could provide, in our view, an appropriate
label for the cluster.
foreign nato military war russian defence soviet
piece un kosovo sanctions bosnia moscow
rail tunnel transport train pounds channel eurostar
ferry trains passengers services paris eurotunnel
countries europe enlargement new membership
members eastern conference reform voting summit
commission foreign join poland negotiations
parliament mep party socialist strasbourg christian
vote leader labour conservative right political green
democrat elections epp
television commission satellite tv broadcasting
tickets film broadcasters bbc programmes media
industry channel public directive
court article justice member directive treaty
question provisions case law regulation judgment
interpretation rules order proceedings
social workers employment working hours
jobs week employers legislation unions
employees chapter rights health minimum
bank central euro monetary rates currency
interest bundesbank markets economic exchange
finance inflation dollar german
players football clubs uefa league fifa game cup
fishing fish fishermen fisheries quota vessels
boats waters sea fleet
racism racist ethnic xenophobia black minorities
jury discrimination white relations
drugs patent research human companies genetic
scientists health medical biotechnology disease
children parents punishment school rights family
childcare corporal education law father mother
controls immigration border asylum checks
passport police citizens crime europol
energy nuclear emissions oil electricity gas
environment carbon tax pollution fuel global cut
commission fraud commissioners brussels report
allegations officials inquiry meps corruption
mismanagement staff santer
2 Metaphor annotation
Ideally, we should have sampled a small sub-corpus
from the EUI corpus for metaphor annotation;
however, the choice of the data for annotation
predated the construction of the EUI corpus.
Our interest being in the way metaphors used
in public discourse help shape attitudes towards
a complex, ongoing and fateful political reality,
we came across Musolff?s (2000) work on the
British discourse on the European integration
process throughout the 1990s. Working in the
corpus linguistics tradition, Musolff (2000) studied
a number of metaphors recurrent in this discourse,
making available a selection of materials he used,
marked with the metaphors.8
One caveat to directly using the database is the
lack of clarity regarding the metaphor annotation
procedure. In particular, the author does not
report how many people participated, or any inter-
annotator agreement figures. We therefore chose
4 out of Musolff?s list of source domains, took
all articles corresponding to them (128 documents),
along with 23 articles from other source domains,
and submitted them to a group of 8 undergraduate
annotators, on top of Musolff?s original markup that
is treated as another annotator.
Annotators received the following instructions,
reflecting our focus on the persuasive use of
metaphor, as part of an argument:
Generally speaking, a metaphor is a
linguistic expression whereby something
is compared to something else that it is
clearly literally not, in order to make a
point. Thus, in Tony Blair?s famous ?I
haven?t got a reverse gear?, Tony Blair
is compared to a car in order to stress
his unwillingness/inability to retract his
statements or actions. We would say in
this case that a metaphor from a VEHICLE
domain is used. In this study we will
consider metaphors from 4 domains.
For the 4 chosen domains we provided the
following descriptions, along with 2 examples for
each:
8available from http://www.dur.ac.uk/andreas.musolff/Arcindex.htm
3
AUTHORITY Metaphors that have to do with
discipline and authority, like school, religion,
royalty, asylum, prison, etc.
LOVE Metaphors from love/romance and family.
BUILD Metaphors that have to do with building
(the process) and houses and other buildings or
constructions, their parts and uses.
VEHICLE Metaphors that have to do with land-
borne vehicles, their parts, operation and
maintenance.
People were instructed to mark every paragraph
where a metaphor from a given domain occurs. They
were also asked to provide a comment that briefly
summarizes the ground for their decision, saying
what is being compared to what.9
Table 2 shows the inter-annotator agreement
figures.
Table 2: Inter-annotator agreement, measured on 2364
paragraphs (151 documents).11
Source Domain of Metaphor ?
LOVE 0.66
VEHICLE 0.66
AUTHORITY 0.39
BUILD 0.43
LOVE and VEHICLE are close to acceptable
reliability, with the other two types scoring low.
In order to understand the nature of disagreements,
we submitted the annotated materials plus some
random annotations to 7 out of the original 8 people
for validation, 4-8 weeks after they completed
the annotations, asking them to accept or reject
9In the topics vs metaphors experiment, we test the
hypothesis on words rather than paragraphs. For metaphors
from a pre-specified domain, such as VEHICLE or LOVE, it
was usually clear which words in the paragraph belong to the
domain and are used metaphorically. People?s comments often
explicitly used words from the paragraph, or made it otherwise
clear through their description. For OpenMeta phase (please see
below), where people were asked to mark metaphors from any
source domain, they were also asked to single out the words in
the paragraph that witness the metaphor, and these are the words
used in the current experiment.
11These are results for binary classification for each metaphor
type rather than a multiclass classification, since some articles
have more than one type and some have none.
metaphor markups. We found that metaphors
initially marked by at least 4 people (out of 9) were
accepted as valid by people who did not initially
mark them in 91% of the cases, on average across
the metaphor types. These are thus uncontroversial
cases, with the missing annotations likely due to
attention slips rather than to genuine differences of
opinion. Metaphors initially marked by 1-3 people
were more controversial, with the average validation
rate of 41% (Beigman Klebanov et al, 2008).
Evidently, some of the metaphors are clearer-
cut than others, yet even the more difficult cases
got non-negligible support at validation time from
people who did not initially mark them. We
therefore decided to regard the whole of the
annotated data as valid for the purpose of the current
research. Our focus is on finding metaphors (recall),
and less on making sure all candidate metaphors are
acceptable to all annotators; it suffices to know that
even the minority opinion often finds support.
In the second stage of the research, we expanded
the repertoire of the metaphor types to include
additional source domains, mainly from Musolff?s
list. The dataset has so far been subjected to
non-expert annotations by a group of the total of
15 undergraduate students. Metaphors from the
source domains of VEHICLE, LOVE, BUILDING,
AUTHORITY, WAR, SHOW, SCHOOL, RELIGION,
MEDICINE were annotated by different subsets of
the students.
The outcome of the second stage of the project is
not sufficient for addressing the issue of discourse
topics vs metaphors, however, as there are instances
of metaphors in the text that do not fall into any
of the source domains singled out by Musolff as
recurrent ones in the discourse under consideration.
We are now at an early stage of the third phrase
we call OpenMeta, where annotators are asked to
mark all metaphors they can detect, not confining
themselves to a given list of source domains.
Only annotators who participated in the previous,
type-constrained, version of the task participate in
OpenMeta project. So far, we have 44 documents
annotated by 3 people for open-domain metaphors.
This subset features as full a coverage of all
metaphors used in the documents as we were able
to obtain so far, and it is going to serve as test data
for the topics vs metaphors hypothesis.
4
Our test set is thus biased towards recurrent
metaphorical domains (those named by Musolff),
and towards metaphors that are relatively salient
to a naive reader, from recurrent or other source
domains. Metaphors marked in the test data are
those afforded a high degree of rhetorical presence
in the discourse ? either quantitatively, because
they are repeated and elaborated, or qualitatively,
because they are striking enough to arrest the
naive reader?s attention. According to the Presence
Theory in rhetoric (Perelman and Olbrechts-Tyteca,
1969; Gross and Dearin, 2003; Atkinson et al,
2008), elements afforded high presence are key to
the rhetorical design of the argument. These are
not so much metaphors we live by without even
noticing, such as those often studied in Conceptual
Metaphor literature, like VALUE AS SIZE or TIME
AS SPACE; these are metaphors that are clearly a
matter of the author?s conscious choice, closest in
the current theorizing to Steen?s (2008) notion of
deliberate metaphors.
2.1 Pseudo sampling
The annotated data is not really a sample of the
corpus. In fact, it is not known to us exactly how the
documents were chosen; although all 44 metaphor
annotated documents are from the newspapers and
dates participating in the EUI corpus, only 20% are
actually in the EUI corpus. How can we establish
that there is a fit between the EUI collection and
the annotated texts? We check how well discourse
topics cover the documents, in the corpus and in
the annotated material. Specifically, for a fixed
k, is there a difference in the |Tki ||Di| for annotateddocuments as opposed to the corpus at large? Using
a random sample of 50 documents from EUI corpus,
a 2-tailed t-test yielded p < 0.05, for all k, the
trend being towards a better coverage of the EUI
documents than of the metaphor annotated ones.
We hypothesized that this was due to the large
discrepancy in the lengths of the texts: An average
text in the EUI sample is 432 words long, whereas
the metaphor annotated texts are 775 words long on
average, with the shortest having 343 words. Shorter
texts tend to be less elaborate and more ?to the
point?, with a higher percentage of topical words.
To neutralize the effect of length on topical
coverage, we chose from the EUI sample only
documents that were at least 343 words long,
resulting in 31 documents. Comparing those to the
44 metaphor annotated documents, we found p >
0.37 for every k, i.e. the annotated documents are
indistinguishable in topical coverage from similar-
length documents in the EUI corpus.
3 Experiment
3.1 Summary of notation
V All and only non-stop words containing only
letters that appeared in at least 6 documents in
the collection.
Di All words in document i that are in V.
Tki All words in document i that are in V and are
in the top k words for some topic active in
document i according to LDA output.
Mi All words in document i that are in V and are
marked as metaphors in this document.
3.2 Hypothesis
We hypothesize that words in a given document
that are high-ranking representatives of a common
topic of discussion in a relevant corpus are less
likely to be metaphorical than other words in the
document. That is, such words would contain a
smaller proportion of metaphors than their share in
text. Using the definitions above: For an average
document i and any k, |Tki ||Di| >
|Mi?Tki |
|Mi| .
3.3 Results
As we hypothesized, metaphors are under-
represented in topically used words. Thus, for
k=25, about 15% of the indexed words in the
document are deemed topical, containing about
3% of the metaphorically used indexed words
in that document. For k=400, about 53% of the
indexed words are topical, capturing only 22% of
the metaphors.
4 Discussion
4.1 Metaphors from salient domains
A number of domains singled out by Musolff (2000)
as being recurrent metaphors in the corpus, such
5
0.000.100.20
0.300.400.50
0.60
25 50 100 150 200 250 300 350 400k
Figure 1: As hypothesized, |Tki ||Di| , shown in circles, is
larger than |Mi?Tki ||Mi| , shown in squares, for various k.
as VEHICLE or LOVE, are also things people care
about politically, hence they also correspond to
recurrent topics of discussion (see clusters titled
transport and childcare in table 1). It has been
shown experimentally that the subject?s in-depth
familiarity with the source domain is necessary
for the metaphor to work as intended ? see for
example Gentner and Gentner (1983) work on using
water flow metaphors for electricity. Our results
suggest that participants in political discourse draw
on domains not only familiar in general, but indeed
highly salient in the specific discourse itself.
As a consequence, an extended metaphor from a
discourse-topical domain can be easily mistaken by
the topic detection software for a topical use of the
relevant items. Consider, for example, an extract
from a 19 December 1991 article in Times:
Denis Healey, former Labour Chancellor
of the Exchequer, urged the primeminister
to stop playing Tory party politics with
the negotiations over Europe and drew an
image of Mr Major as a driver. He said:
?I understand that if you are driving a car
and sitting behind you is a lady with a
handbag and a man with fangs, you may
feel it wiser to drive in the slow lane. My
own advice is that he should pull into a
lay-by, turf the others out and then hand
the wheel over to firmer and safer hands.?
LDA considered {drive driving} to belong to
the topic that deals with safety and road accidents,
including in its 200 most likely words {crash
died accidents pedestrians traffic safety cars maps
motorists}, although additional metaphorically used
items from the same semantic domain, such as
lane and wheel, were not among the top 200
representatives of this topic.
It is an intriguing direction for future research
to compare the topical and metaphorical uses of
such domains, in order to determine which aspects
loom large indeed, being both matters of literal
concern and prolific generators of metaphors, and
how these are manipulated for persuasive effects.
The example above suggests that in the British EU-
related discourse in 1990s safety of driving is both
a topic-of-discussion (?Cyclists and pedestrians are
more vulnerable on British roads than anywhere else
in the European Union?, proclaims The Times on 18
February 2000) and a metaphorical axis, stressing
the importance of care and control, the hallmark
of the Euro-sceptic stance towards the European
integration process.
4.2 Topical metaphors
Putting aside topic detector?s mistakes on extended
metaphors from certain domains such as discussed
in the previous section, what do metaphors in the
topical vocabulary look like? The last topic shown
in table 1 has to do with criticism towards EU
bureaucracy, reflecting extensive discussions in the
British media in the late 1990s of alleged corruption
and mismanagement in the European Commission.
Together with the words cited in the table, this topic
lists root as one of its 300 most likely words.
This word shows up as a metaphor in 3 of our test
documents. In two of them it is used precisely in the
context projected by the topic:
In limpid language, whose meaning no
bureaucrat can twist, these four wise
men and one wise woman delivered, to
their great credit, a coruscating indictment
not just of individual commissioners, but
of the entire management and corporate
culture of the European Commission.
They have made an incontestable case, in
Tony Blair?s words, for ?root and branch
reform?.
6
Here, root is used in the root and branch idiom
suggesting a complete change, a reform, which
comes as part of a bundle with severe criticism.
Yet the figurative nature of this expression as a
metaphor from PLANT domain is apparent to naive
readers, making it an instance of imagery routinely
going together with criticism in this corpus. A
related metaphorical sense of root is attested in
similar contexts in the corpus, further explaining its
connection to the topic:
Not unless they insist on credible systems
to hold commissioners and bureaucrats to
account. And not unless they appoint
a new team with a brief not just to
root out malpractices but to shut down
entire programmes, such as tourism and
humanitarian aid, which the Commission
is incompetent to manage and which
should never have been added to its ever-
expanding empire.
A bloodied European Commission looks
likely to cling on to power today after
an eleventh-hour threat to quit by its
President, Jacques Santer, called the bluff
of the European Parliament ... All
week MEPs had been talking up the
?nuclear option? of sacking the full
Commission body over a burgeoning
fraud and nepotism scandal that dates
from 1995 ... Early 1997: Finnish
Commissioner Erkki Liikanen announces
plan to root out nepotism in Commission
and improve financial controls.
In the third document with root metaphor, root
is used in a different environment, and is not
considered topical by LDA:
For at the root of this conflict lies the
German denial that unemployment has
anything to do with cyclical fluctuations
in the economy.
Our quantitative results show that cases such
as root are more an exception than a rule. Yet,
from the perspective of the argumentative use of
metaphors, such cases are instructive of the way
certain metaphors get ?attached? to certain topics of
discussion. In this case, the majority of mentions
of root in this critical context come from Tony
Blair?s expression that was cited and referenced
widely enough to acquire a statistical association
with the discussion of the Commission?s failings
in the corpus. Indeed, the political significance of
Blair?s successful appropriation of the issue was not
lost on the media:
Tony Blair has swiftly positioned himself
as the champion of ?root and branch?
reform. Not to be outdone, William Hague
unveiled a ?10-point plan? for reform
of the Commission, no doubt drawing
on his extensive McKinsey management
expertise.
In future work, we plan to look closely at the
topical metaphors, as they potentially represent
outcomes of leadership battles fought in the media,
and can thus have political consequences.
5 Conclusion
Using metaphor-annotated material that is
sufficiently representative of the topical composition
of a similar-length document in a large background
corpus, we showed that words expressing a
discourse-wide topic of discussion are less likely to
be metaphorical than other words in a document.
This is, to our knowledge, the first quantitative
demonstration of the connection between
metaphoricity of a given word and its role in the
relevant background discourse. It complements the
traditionally ?localistic? outlook on metaphors that
is based on the observation that a metaphorically
used item creates a local incongruity because there
is a violation of a selectional restrictions between
verbs and their arguments (Fass, 1991; Mason,
2004; Gedigian et al, 2006; Birke and Sarkar, 2006)
or in the adjective-noun pairs (Krishnakumaran and
Zhu, 2007). Global discourse-level information
can potentially be used to focus metaphor detectors
operating at the local level on items with higher
metaphoric potential.
Reining and Lo?nneker-Rodman (2007) use
minimal topical information to focus their search
for metaphors. Working with a French-language
7
corpus discussing European politics, Reining and
Lo?nneker-Rodman (2007) proposed harvesting
salient collocates of the lemma Europe, that
represents the main topic of discussion and is
thus hypothesized to be the main target domain
of metaphors in this corpus. Indeed, numerous
instances of metaphors were collected using a
4-word window around the lemma in their corpus.
Our work can be understood as developing a
more nuanced approach to finding the likely target
domains in the corpus ? those words that represent
a topic of discussion rather than the means to
discuss a topic. Thus, it is not just Europe per se
that is the target, but, more specifically, aspects
such as monetary integration, employment, energy,
immigration, transportation, and defense, among
others. Our results suggest that to harvest deliberate
metaphors more effectively, one is advised to
consider words that do not represent a discourse
topic.
References
Nathan Atkinson, David Kaufer, and Suguru Ishizaki.
2008. Presence and Global Presence in Genres of Self-
Presentation: A Framework for Comparative Analysis.
Rhetoric Society Quarterly, 38(3):1?27.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing Disagreements. In COL-
ING 2008 Workshop on Human Judgments in Compu-
tational Linguistics, pages 2?7, Manchester, UK.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of nonlit-
eral language. In Proceedings of EACL, pages 329?
336.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet Allocation. Journal of Machine Learn-
ing Resarch, 3:993?1022.
Ari Chanen and Jon Patrick. 2007. Measuring correla-
tion between linguists judgments and Latent Dirichlet
Allocation topics. In Proceedings of the Australasian
Language Technology workshop, pages 13?20, Mel-
bourne, Australia.
Dan Fass. 1991. Met*: A method for discriminating
metonymy and metaphor by computer. Computational
Linguistics, 17(1):49?90.
Matt Gedigian, John Bryant, Srinivas Narayanan, and
Branimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of NAACL Workshop on Scalable Natural
Language Understanding, pages 41?48.
Deidre Gentner and Donald Gentner. 1983. Flowing wa-
ters or teeming crowds: Mental models of electricity.
In D. Gentner and A. Stevens, editors, Mental models.
Hillsdale, NJ: Lawrence Erlbaum.
Alan Gross and Ray Dearin. 2003. Chaim Perelman.
Albany: SUNY Press.
Eva Feder Kittay. 1987. Metaphor: Its cognitive force
and linguistic structure. Oxford: Calderon Press.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of the Workshop on Computational
Approaches to Figurative Language, pages 13?20,
Rochester, New York.
Zachary J. Mason. 2004. CorMet: A computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Andreas Musolff. 2000. Mirror images of Eu-
rope: Metaphors in the public debate about
Europe in Britain and Germany. Mu?nchen:
Iudicium. Annotated data is available at
http://www.dur.ac.uk/andreas.musolff/Arcindex.htm.
Chaim Perelman and Lucie Olbrechts-Tyteca. 1969. The
New Rhetoric: A Treatise on Argumentation. Wilkin-
son, J. and Weaver, P. (trans). Notre Dame, IN: Uni-
versity of Notre Dame Press.
Astrid Reining and Birte Lo?nneker-Rodman. 2007.
Corpus-driven metaphor harvesting. In Proceedings of
the Workshop on Computational Approaches to Figu-
rative Language, pages 5?12, Rochester, New York.
Gerard Steen. 2008. The Paradox of Metaphor: Why
We Need a Three-Dimensional Model of Metaphor.
Metaphor and Symbol, 23(4):213?241.
8
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 13?16,
New York, June 2006. c?2006 Association for Computational Linguistics
Measuring Semantic Relatedness Using People and WordNet
Beata Beigman Klebanov
School of Computer Science and Engineering
The Hebrew University, Jerusalem, Israel
beata@cs.huji.ac.il
Abstract
In this paper, we (1) propose a new dataset
for testing the degree of relatedness be-
tween pairs of words; (2) propose a new
WordNet-based measure of relatedness, and
evaluate it on the new dataset.
1 Introduction
Estimating the degree of semantic relatedness be-
tween words in a text is deemed important in
numerous applications: word-sense disambigua-
tion (Banerjee and Pedersen, 2003), story segmen-
tation (Stokes et al, 2004), error correction (Hirst
and Budanitsky, 2005), summarization (Barzilay and
Elhadad, 1997; Gurevych and Strube, 2004).
Furthermore, Budanitsky and Hirst (2006) noted
that various applications tend to pick the same mea-
sures of relatedness, which suggests a certain com-
monality in what is required from such a measure by
the different applications. It thus seems worthwhile
to develop such measures intrinsically, before putting
them to application-based utility tests.
The most popular, by-now-standard testbed is
Rubenstein and Goodenough?s (1965) list of 65 noun
pairs, ranked by similarity of meaning. A 30-pair
subset (henceforth, MC) passed a number of repli-
cations (Miller and Charles, 1991; Resnik, 1995), and
is thus highly reliable.
Rubenstein and Goodenough (1965) view simi-
larity of meaning as degree of synonymy. Researchers
have long recognized, however, that synonymy is only
one kind of semantic affinity between words in a
text (Halliday and Hasan, 1976), and expressed a
wish for a dataset for testing a more general notion
of semantic relatedness.1
1?. . . similarity of meaning is not the same thing as
semantic relatedness. However, there is at present no
large dataset of human judgments of semantic related-
This paper proposes and explores a new related-
ness dataset. In sections 2-3, we briefly introduce
the experiment by Beigman Klebanov and Shamir
(henceforth, BS), and use the data to induce related-
ness scores. In section 4, we propose a new WordNet-
based measure of relatedness, and use it to explore
the new dataset. We show that it usually does bet-
ter than competing WordNet-based measures (sec-
tion 5). We discuss future directions in section 6.
2 Data
Aiming at reader-based exploration of lexical cohe-
sion in texts, Beigman Klebanov and Shamir con-
ducted an experiment with 22 students, each reading
10 texts: 3 news stories, 4 journalistic and 3 fiction
pieces (Beigman Klebanov and Shamir, 2006). Peo-
ple were instructed to read the text first, and then
go over a separately attached list of words in order
of their appearance in the text, and ask themselves,
for every newly mentioned concept, ?which previ-
ously mentioned concepts help the easy accommoda-
tion of the current concept into the evolving story,
if indeed it is easily accommodated, based on the
common knowledge as perceived by the annotator?
(Beigman Klebanov and Shamir, 2005); this preced-
ing helper concept is called an anchor. People were
asked to mark all anchoring relations they could find.
The rendering of relatedness between two concepts
is not tied to any specific lexical relation, but rather
to common-sense knowledge, which has to do with
?knowledge of kinds, of associations, of typical sit-
uations, and even typical utterances?.2 The phe-
nomenon is thus clearly construed as much broader
than degree-of-synonymy.
Beigman Klebanov and Shamir (2006) provide re-
liability estimation of the experimental data using
ness? (Hirst and Budanitsky, 2005); ?To our knowledge,
no datasets are available for validating the results of se-
mantic relatedness metric? (Gurevych, 2005).
2according to Hirst (2000), cited in the guidelines
13
statistical analysis and a validation experiment, iden-
tifying reliably anchored items with their strong an-
chors, and reliably un-anchored items. Such analysis
provides high-validity data for classification; how-
ever, much of the data regarding intermediate de-
grees of relatedness is left out.
3 Relatedness Scores
Our idea is to induce scores for pairs of anchored
items with their anchors (henceforth, AApairs)
using the cumulative annotations by 20 people.3
Thus, an AApair written by all 20 people scores 20,
and that written by just one person scores 1. The
scores would correspond to the perceived relatedness
of the pair of concepts in the given text.
In Beigman Klebanov and Shamir?s (2006) core
classification data, no distinctions are retained be-
tween pairs marked by 19 or 13 people. Now we
are interested in the relative relatedness, so it is im-
portant to handle cases where the BS data might
under-rate a pair. One such case are multi-word
items; we remove AApairs with suspect multi-word
elements.4 Further, we retain only pairs that belong
to open-class parts of speech (henceforth, POS), as
functional categories contribute little to the lexical
texture (Halliday and Hasan, 1976). The Size col-
umn of table 1 shows the number of AApairs for
each BS text, after the aforementioned exclusions.
The induced scores correspond to cumulative
judgements of a group of people. How well do they
represent the people?s ideas? One way to measure
group homogeneity is leave-one-out estimation, as
done by Resnik (1995) for MC data, attaining the
high average correlation of r = 0.88. In the current
case, however, every specific person made a binary
decision, whereas a group is represented by scores 1
to 20; such difference in granularity is problematic
for correlation or rank order analysis.
Another way to measure group homogeneity is to
split it into subgroups and compare scores emerging
from the different subgroups. We know from
Beigman Klebanov and Shamir?s (2006) analysis that
it is not the case that the 20-subject group clusters
into subgroups that systematically produced differ-
ent patterns of answers. This leads us to expect rel-
ative lack of sensitivity to the exact splits into sub-
groups.
To validate this reasoning, we performed 100 ran-
dom choices of two 9-subject4 groups, calculated the
scores induced by the two groups, and computed
3Two subjects were revealed as outliers and their data
was removed (Beigman Klebanov and Shamir, 2006).
4See Beigman Klebanov (2006) for details.
Pearson correlation between the two lists. Thus, for
every BS text, we have a distribution of 100 coeffi-
cients, which is approximately normal. Estimations
of ? and ? of these distributions are ? = .69 ? .82
(av. 0.75), ? = .02? .03 for the different BS texts.
To summarize: although the homogeneity is lower
than for MC data, we observe good average inter-
group correlations with little deviation across the 100
splits. We now turn to discussion of a relatedness
measure, which we will evaluate using the data.
4 Gic: WordNet-based Measure
Measures using WordNet taxonomy are state-of-
the-art in capturing semantic similarity, attaining
r=.85 ?.89 correlations with the MC dataset (Jiang
and Conrath, 1997; Budanitsky and Hirst, 2006).
However, they fall short of measuring relatedness,
as, operating within a single-POS taxonomy, they
cannot meaningfully compare kill to death. This is
a major limitation with respect to BS data, where
only about 40% of pairs are nominal, and less than
10% are verbal. We develop a WordNet-based mea-
sure that would allow cross-POS comparisons, using
glosses in addition to the taxonomy.
One family of WordNet measures are methods
based on estimation of information content (hence-
forth, IC) of concepts, as proposed in (Resnik, 1995).
Resnik?s key idea in corpus-based information con-
tent induction using a taxonomy is to count every
appearance of a concept as mentions of all its hy-
pernyms as well. This way, artifact#n#1, although
rarely mentioned explicitly, receives high frequency
and low IC value. We will count a concept?s men-
tion towards all its hypernyms AND all words5 that
appear in its own and its hypernyms? glosses. Analo-
gously to artifact, we expect properties mentioned in
glosses of more general concepts to be less informa-
tive, as those pertain to more things (ex., visible,
a property of anything that is-a physical object).
The details of the algorithm for information con-
tent induction from taxonomy and gloss information
(ICGT ) are given in appendix A.
To estimate the semantic affinity between two
senses A and B, we average the ICGT values of the
3 words with the highest ICGT in the overlap of A?s
and B?s expanded glosses (the expansion follows the
algorithm in appendix A).6
5We induce IC values on (POS-tagged base
form) words rather than senses. Ongoing gloss
sense-tagging projects like eXtended WordNet
(http://xwn.hlt.utdallas.edu/links.html) would allow
sense-based calculation in the future.
6The number 3 is empirically-based; the idea is to
counter-balance (a) the effect of an accidental match of a
14
Data Size Gic BP Data Size Gic BP
BS-1 1007 .29 .19 BS-6 536 .24 .19
BS-2 776 .37 .16 BS-7 917 .22 .10
BS-3 1015 .22 .09 BS-8 529 .24 .12
BS-4 512 .34 .39 BS-9 509 .31 .16
BS-5 1020 .25 .11 BS10 417 .36 .19
Table 1: Dataset sizes and correlations of Gic, BP
with human ratings. r > 0.16 is significant at
p < .05; r > .23 is significant at p < .01. Average
correlation (AvBS) is r=.28 (Gic), r=.17 (BP).
If A? (the word of which A is a sense) appears
in the expanded gloss of B, we take the maximum
between the ICGT (A?) and the value returned by
the 3-smoothed calculation. To compare two words,
we take the maximum value returned by pairwise
comparisons of their WordNet senses.7
The performance of this measure is shown under
Gic in table 1. Gic manages robust but weak corre-
lations, never reaching the r = .40 threshold.
5 Related Work
We compare Gic to another WordNet-based measure
that can handle cross-POS comparisons, proposed
by Banerjee and Pedersen (2003). To compare word
senses A and B, the algorithm compares not only
their glosses, but also glosses of items standing in
various WordNet relations with A and B. For ex-
ample, it compares the gloss of A?s meronym to that
of B?s hyponym. We use the default configuration
of the measure in WordNet::Similarity-0.12 package
(Pedersen et al, 2004), and, with a single exception,
the measure performed below Gic; see BP in table 1.
As mentioned before, taxonomy-based similarity
measures cannot fully handle BS data. Table 2 uses
nominal-only subsets of BS data and the MC nominal
similarity dataset to show that (a) state-of-the-art
WordNet-based similarity measure JC8 (Jiang and
Conrath, 1997; Budanitsky and Hirst, 2006) does
very poorly on the relatedness data, suggesting that
nominal similarity and relatedness are rather differ-
ent things; (b) Gic does better on average, and is
more robust; (c) Gic yields on MC to gain perfor-
mance on BS, whereas BP is no more inclined to-
single word which is relatively rarely used in glosses; (b)
the multitude of low-IC items in many of the overlaps
that tend to downplay the impact of the few higher-IC
members of the overlap.
7To speed the processing up, we use first 5 WordNet
senses of each item for results reported here.
8See formula in appendix B. We use (Pedersen et
al., 2004) implementation with a minor alteration ? see
Beigman Klebanov (2006).
wards relatedness than JC.
Data Gic BP JC Data Gic BP JC
BS-1 .38 .18 .21 BS-6 .25 .16 .22
BS-2 .53 .18 .37 BS-7 .23 .10 .04
BS-3 .21 .04 .01 BS-8 .32 .10 .00
BS-4 .28 .38 .33 BS-9 .24 .17 .27
BS-5 .12 .07 .16 BS10 .41 .25 .25
AvBS .30 .16 .19 MC .78 .80 .86
Table 2: MC and nominal-only subsets of BS: corre-
lations of various measures with the human ratings.
Table 3 illustrates the relatedness vs. similarity
distinction. Whereas, taxonomically speaking, son
is more similar to man, as reflected in JC scores,
people marked family and mother as much stronger
anchors for son in BS-2; Gic follows suit.
AApair Human Gic JC
son ? man 2 0.355 22.3
son ? family 13 0.375 16.9
son ? mother 16 0.370 20.1
Table 3: Relatendess vs. similarity
6 Conclusion and Future Work
We proposed a dataset of relatedness judgements
that differs from the existing ones9 in (1) size ?
about 7000 items, as opposed to up to 350 in existing
datasets; (2) cross-POS data, as opposed to purely
nominal or verbal; (3) a broad approach to semantic
relatedness, not focussing on any particular relation,
but grounding it in the reader?s (idea of) common
knowledge; this as opposed to synonymy-based simi-
larity prevalent in existing databases.
We explored the new data with WordNet-based
measures, showing that (1) the data is different in
character from a standard similarity dataset, and
very challenging for state-of-the-art methods; (2) the
proposed novel WordNet-based measure of related-
ness usually outperforms its competitor, as well as
a state-of-the-art similarity measure when the latter
applies.
In future work, we plan to explore distributional
methods for modeling relatedness, as well as the
use of text-based information to improve correlations
with the human data, as judgments are situated in
specific textual contexts.
9Though most widely used, MC is not the only avail-
able dataset; we will address other datasets in a subse-
quent paper.
15
References
Satanjeev Banerjee and Ted Pedersen. 2003. Ex-
tended gloss overlaps as a measure of semantic
relatedness. In Proceedings of IJCAI.
Regina Barzilay and Michael Elhadad. 1997. Using
lexical chains for text summarization. In Pro-
ceedings of ACL Intelligent Scalable Text Summa-
rization Workshop.
Beata Beigman Klebanov and Eli Shamir. 2005.
Guidelines for annotation of concept mention pat-
terns. Technical Report 2005-8, Leibniz Center for
Research in Computer Science, The Hebrew Uni-
versity of Jerusalem, Israel.
Beata Beigman Klebanov and Eli Shamir. 2006.
Reader-based exploration of lexical cohesion. To
appear in Language Resources and Evaluation.
Springer, Netherlands.
Beata Beigman Klebanov. 2006. Using people and
WordNet to measure semantic relatedness. Tech-
nical Report 2006-17, Leibniz Center for Research
in Computer Science, The Hebrew University of
Jerusalem, Israel.
Alexander Budanitsky and Graeme Hirst. 2006.
Evaluating WordNet-based measures of semantic
distance. Computational Linguistics, 32(1):13?47.
Iryna Gurevych and Michael Strube. 2004. Semantic
similarity applied to spoken dialogue summariza-
tion. In Proceedings of COLING.
Iryna Gurevych. 2005. Using the structure of a con-
ceptual network in computing semantic related-
ness. In Proceedings of IJCNLP.
M.A.K. Halliday and Ruqaiya Hasan. 1976. Cohe-
sion in English. Longman Group Ltd.
Graeme Hirst and Alexander Budanitsky. 2005.
Correcting real-word spelling errors by restoring
lexical cohesion. Natural Language Engineering,
11(1):87?111.
Graeme Hirst. 2000. Context as a spurious concept.
In Proceedings of CICLING.
Jay Jiang and David Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxon-
omy. In Proceedings on International Conference
on Research in Computational Linguistics.
George Miller and Walter Charles. 1991. Contex-
tual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
Ted Pedersen, Siddharth Patwardhan, and Jason
Michelizzi. 2004. WordNet::Similarity-measuring
the relatedness of concepts. In Proceedings of
NAACL.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In
Proceedings of IJCAI.
Herbert Rubenstein and John Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
Nicola Stokes, Joe Carthy, and Alan F. Smeaton.
2004. SeLeCT: A lexical cohesion based news story
segmentation system. Journal of AI Communica-
tions, 17(1):3?12.
A Gloss&Taxonomy IC (ICGT )
We refer to POS-tagged base form items as ?words?
throughout this section. For every word-sense W in
WordNet database for a given POS:
1. Collect all content words from the gloss of W ,
excluding examples, including W ? - the POS-
tagged word of which W is a sense.
2. If W is part of a taxonomy, expand its gloss,
without repetitions, with words appearing in
the glosses of all its super-ordinate concepts,
up to the top of the hierarchy. Thus, the ex-
panded gloss for airplane#n#1 would contain
words from the glosses of the relevant senses of
aircraft , vehicle, transport, etc.
3. Add W ?s sense count to all words in its ex-
panded gloss.10
Each POS database induces its own counts on each
word that appeared in the gloss of at least one of its
members. When merging the data from the differ-
ent POS, we scale the aggregated counts, such that
they correspond to the proportion of the given word
in the POS database where it was the least informa-
tive. The standard log-frequency calculation trans-
forms these counts into taxonomy-and-gloss based in-
formation content (ICGT ) values.
B JC measure of similarity
In the formula, IC is taxonomy-only based informa-
tion content, as in (Resnik, 1995), LS is the lowest
common subsumer of the two concepts in the Word-
Net hierarchy, and Max is the maximum distance11
between any two concepts.
JC(c1, c2) = Max?(IC(c1)+IC(c2)?2?IC(LS(c1, c2))
To make JC scores comparable to Gic?s [0,1] range,
the score can be divided by Max. Normalization has
no effect on correlations.
10We do add-1-smoothing on WordNet sense counts.
11This is about 26 for WordNet-2.0 nominal hierar-
chy with add-1-smoothed SemCor database; see Beigman
Klebanov (2006) for details.
16
Proceedings of the ACL Student Research Workshop, pages 55?60,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
Using Readers to Identify Lexical Cohesive Structures in Texts
Beata Beigman Klebanov
School of Computer Science and Engineering
The Hebrew University of Jerusalem
Jerusalem, 91904, Israel
beata@cs.huji.ac.il
Abstract
This paper describes a reader-based exper-
iment on lexical cohesion, detailing the
task given to readers and the analysis of
the experimental data. We conclude with
discussion of the usefulness of the data in
future research on lexical cohesion.
1 Introduction
The quest for finding what it is that makes an ordered
list of linguistic forms into a text that is fluently read-
able by people dates back at least to Halliday and
Hasan?s (1976) seminal work on textual cohesion.
They identified a number of cohesive constructions:
repetition (using the same words, or via repeated
reference, substitution and ellipsis), conjunction and
lexical cohesion.
Some of those structures - for example, cohesion
achieved through repeated reference - have been
subjected to reader based tests, often while trying to
produce gold standard data for testing computational
models, a task requiring sufficient inter-annotator
agreement (Hirschman et al, 1998; Mitkov et al,
2000; Poesio and Vieira, 1998).
Experimental investigation of lexical cohesion is
an emerging enterprise (Morris and Hirst, 2005) to
which the current study contributes. We present our
version of the question to the reader to which lexi-
cal cohesion patterns are an answer (section 2), de-
scribe an experiment on 22 readers using this ques-
tion (section 3), and analyze the experimental data
(section 4).
2 From Lexical Cohesion to Anchoring
Cohesive ties between items in a text draw on the
resources of a language to build up the text?s unity
(Halliday and Hasan, 1976). Lexical cohesive ties
draw on the lexicon, i.e. word meanings.
Sometimes the relation between the members of
a tie is easy to identify, like near-synonymy (dis-
ease/illness), complementarity (boy/girl), whole-to-
part (box/lid), but the bulk of lexical cohesive tex-
ture is created by relations that are difficult to clas-
sify (Morris and Hirst, 2004). Halliday and Hasan
(1976) exemplify those with pairs like dig/garden,
ill/doctor, laugh/joke, which are reminiscent of the
idea of scripts (Schank and Abelson, 1977) or
schemata (Rumelhart, 1984): certain things are ex-
pected in certain situations, the paradigm example
being menu, tables, waiters and food in a restaurant.
However, texts sometimes start with descriptions
of situations where many possible scripts could ap-
ply. Consider a text starting with Mother died to-
day.1 What are the generated expectations? A de-
scription of an accident that led to the death, or of
a long illness? A story about what happened to the
rest of the family afterwards? Or emotional reac-
tion of the speaker - like the sense of loneliness in
the world? Or something more ?technical? - about
the funeral, or the will? Or something about the
mother?s last wish and its fulfillment? Many direc-
tions are easily thinkable at this point.
We suggest that rather than generating predic-
tions, scripts/schemata could provide a basis for
abduction. Once any ?normal? direction is ac-
1the opening sentence of A. Camus? The Stranger
55
tually taken up by the following text, there is a
connection back to whatever makes this a normal
direction, according to the reader?s commonsense
knowledge (possibly coached in terms of scripts or
schemata). Thus, had the text developed the ill-
ness line, one would have known that it can be
best explained-by/blamed-upon/abduced-to the pre-
viously mentioned lethal outcome. We say in this
case that illness is anchored by died, and mark it
illness  died; we aim to elicit such anchoring rela-
tions from the readers.
3 Experimental Design
We chose 10 texts for the experiment: 3 news ar-
ticles, 4 items of journalistic writing, and 3 fiction
pieces. All news and one fiction story were taken in
full; others were cut at a meaningful break to stay
within 1000 word limit. The texts were in English -
original language for all but two texts.
Our subjects were 22 students at the Hebrew Uni-
versity of Jerusalem, Israel; 19 undergraduates and
3 graduates, all aged 21-29 years, studying various
subjects - Engineering, Cognitive Science, Biology,
History, Linguistics, Psychology, etc. Three of the
participants named English their mother tongue; the
rest claimed very high proficiency in English. Peo-
ple were paid for participation.
All participants were first asked to read the guide-
lines that contained an extensive example of an an-
notation done by us on a 4-paragraph text (a small
extract is shown in table 1), and short paragraphs
highlighting various issues, like the possibility of
multiple anchors per item (see table 1) and of multi-
word anchors (Scientific or American alone do not
anchor editor, but taken together they do).
In addition, the guidelines stressed the importance
of separation between general and personal knowl-
edge, and between general and instantial relations.
For the latter case, an example was given of a story
about children who went out in a boat with their fa-
ther who was an experienced sailor, with an explana-
tion that whereas father children and sailor boat
are based on general commonsense knowedge, the
connection between sailor and father is not some-
thing general but is created in the particular case be-
cause the two descriptions apply to the same person;
people were asked not to mark such relations.
Afterwards, the participants performed a trial an-
notation on a short news story, after which meetings
in small groups were held for them to bring up any
questions and comments2.
The Federal Aviation Administration underestimated
the number of aircraft flying over the Pantex Weapons Plant
outside Amarillo, Texas, where much of the nation?s surplus
plutonium is stored, according to computerized studies
under way by the Energy Department.
the whereamarillo texas outside
federal much
aviation nation federal
administration federal surplus
underestimated plutoniumweapons
numberunderestimated is
of stored surplus
aircraftaviation according
flyingaircraft aviation to
overflying computerized
pantex studiesunderestimated
weapons under
plant way
outside by
amarillo energyplutonium
texas federal departmentadministration
Table 1: Example Annotation from the Guidelines
(extract). x    c d  means each of c and d is an
anchor for x.
The experiment then started. For each of the 10
texts, each person was given the text to read, and
a separate wordlist on which to write down annota-
tions. The wordlist contained words from the text,
in their appearance order, excluding verbatim and
inflectional repetitions3 . People were instructed to
read the text first, and then go through the wordlist
and ask themselves, for every item on the list, which
previously mentioned items help the easy accommo-
dation of this concept into the evolving story, if in-
deed it is easily accommodated, based on the com-
monsense knowledge as it is perceived by the anno-
tator. People were encouraged to use a dictionary if
they were not sure about some nuance of meaning.
Wordlist length per text ranged from 175 to 339
items; annotation of one text took a person 70 min-
2The guidelines and all the correspondence with the partici-
pants is archived and can be provided upon request.
3The exclusion was done mainly to keep the lists to reason-
able length while including as many newly mentioned items as
possible. We conjectured that repetitions are usually anchored
by the previous mention; this assumption is a simplification,
since sometimes the same form is used in a somewhat different
sense and may get anchored separately from the previous use of
this form. This issue needs further experimental investigation.
56
utes on average (each annotator was timed on two
texts; every text was timed for 2-4 annotators).
4 Analysis of Experimental Data
Most of the existing research in computational lin-
guistics that uses human annotators is within the
framework of classification, where an annotator de-
cides, for every test item, on an appropriate tag out
of the pre-specified set of tags (Poesio and Vieira,
1998; Webber and Byron, 2004; Hearst, 1997; Mar-
cus et al, 1993).
Although our task is not that of classification, we
start from a classification sub-task, and use agree-
ment figures to guide subsequent analysis. We use
the by now standard   statistic (Di Eugenio and
Glass, 2004; Carletta, 1996; Marcu et al, 1999;
Webber and Byron, 2004) to quantify the degree of
above-chance agreement between multiple annota-
tors, and the  statistic for analysis of sources of
unreliability (Krippendorff, 1980). The formulas for
the two statistics are given in appendix A.
4.1 Classification Sub-Task
Classifying items into anchored/unanchored can be
viewed as a sub-task of our experiment: before writ-
ing any particular item as an anchor, the annotator
asked himself whether the concept at hand is easy
to accommodate at all. Getting reliable data on this
task is therefore a pre-condition for asking any ques-
tions about the anchors. Agreement on this task av-
erages     

for the 10 texts. These reliability
figures do not reach the      area which is the
accepted threshold for deciding that annotators were
working under similar enough internalized theories4
of the phenomenon; however, the figures are high
enough to suggest considerable overlaps.
Seeking more detailed insight into the degree of
similarity of the annotators? ideas of the task, we
follow the procedure described in (Krippendorff,
1980) to find outliers. We calculate the category-
by-category co-markup matrix 	 for all annotators5 ;
then for all but one annotators, and by subtraction
find the portion that is due to this one annotator.
We then regard the data as two-annotator data (one
4whatever annotators think the phenomenon is after having
read the guidelines
5See formula 7 in appendix A.
vs. everybody else), and calculate agreement coef-
ficients. We rank annotators (1 to 22) according to
the degree of agreement with the rest, separately for
each text, and average over the texts to obtain the
conformity rank of an annotator. The lower the rank,
the less compliant the annotator.
Annotators? conformity ranks cluster into 3
groups described in table 2. The two members of
group A are consistent outliers - their average rank
for the 10 texts is below 2. The second group (B)
is, on average, in the bottom half of the annota-
tors with respect to agreement with the common,
whereas members of group C display relatively high
conformity.
Gr Size Ranks Agr. within group ( )
A 2 1.7 - 1.9 0.55
B 9 5.8 - 10.4 0.41
C 11 13.6 - 18.3 0.54
Table 2: Groups of annotators, by conformity ranks.
It is possible that annotators in groups A, B and C
have alternative interpretations of the guidelines, but
our idea of the ?common? (and thus the conformity
ranks) is dominated by the largest group, C. Within-
group agreement rates shown in table 2 suggest that
two annotators in group A do indeed have an alter-
native understanding of the task, being much better
correlated between each other than with the rest.
The figures for the other two groups could sup-
port two scenarios: (1) each group settled on a dif-
ferent theory of the phenomenon, where group C is
in better agreement on its version that group B on
its own; (2) people in groups B and C have basically
the same theory, but members of C are more sys-
tematic in carrying it through. It is crucial for our
analysis to tell those apart - in the case of multiple
stable interpretations it is difficult to talk about the
anchoring phenomenon; in the core-periphery case,
there is hope to identify the core emerging from 20
out of 22 annotations.
Let us call the set of majority opinions on a list of
items an interpretation of the group, and let us call
the average majority percentage consistency. Thus,
if all decisions of a 9 member group were almost
unanimous, the consistency of the group is 8/9 =
89%, whereas if every time there was a one vote
57
edge to the winning decision, the consistency was
5/9=56%. The more consistent the interpretation
given by a group, the higher its agreement coeffi-
cient.
If groups B and C have different interpretations,
adding a person p from group C to group B would
usually not improve the consistency of the target
group (B), since p is likely to represent majority
opinion of a group with a different interpretation.
On the other hand, if the two groups settled on
basically the same interpretation, the difference in
ranks reflects difference in consistency. Then mov-
ing p from C to B would usually improve the con-
sistency in B, since, coming from a more consistent
group, p?s agreement with the interpretation is ex-
pected to be better than that of an average member
of group B, so the addition strengthens the majority
opinion in B6.
We performed this analysis on groups A and C
with respect to group B. Adding members of group
A to group B improved the agreement in group B
only for 1 out of the 10 texts. Thus, the relation-
ship between the two groups seems to be that of dif-
ferent interpretations. Adding members of group C
to group B resulted in improvement in agreement in
at least 7 out of 10 texts for every added member.
Thus, the difference between groups B and C is that
of consistency, not of interpretation; we may now
search for the well-agreed-upon core of this inter-
pretation. We exclude members of group A from
subsequent analysis; the remaining group of 20 an-
notators exhibits an average agreement of      
on anchored/unanchored classification.
4.2 Finding the Common Core
The next step is finding a reliably classified subset of
the data. We start with the most agreed upon items -
those classified as anchored or non-anchored by all
the 20 people, then by 19, 18, etc., testing, for ev-
ery such inclusion, that the chances of taking in in-
stances of chance agreement are small enough. This
means performing a statistical hypothesis test: with
how much confidence can we reject the hypothesis
6Experiments with synthetic data confirm this analysis: with
20 annotations split into 2 sets of sizes 9 and 11, it is possible
to get an overall agreement of about     either with 75%
and 90% consistency on the same interpretation, or with 90%
and 95% consistency on two interpretations with induced (i.e.
non-random) overlap of just 20%.
that certain agreement level7 is due to chance. Con-
fidence level of      is achieved including items
marked by at least 13 out of 20 people and items
unanimously left unmarked.8
The next step is identifying trustworthy anchors
for the reliably anchored items. We calculated av-
erage anchor strength for every text: the number of
people who wrote the same anchor for a given item,
averaged on all reliably anchored items in a text. Av-
erage anchor strength ranges between 5 and 7 in dif-
ferent texts. Taking only strong anchors (anchors of
at least the average strength), we retain about 25%
of all anchors assigned to anchored items in the reli-
able subset. In total, there are 1261 pairs of reliably
anchored items with their strong anchors, between
54 and 205 per text.
Strength cut-off is a heuristic procedure; some of
those anchors were marked by as few as 6 or 7 out
of 20 people, so it is not clear whether they can be
trusted as embodiments of the core of the anchoring
phenomenon in the analyzed texts. Consequently, an
anchor validation procedure is needed.
4.3 Validating the Common Core
We observe that although people were asked to mark
all anchors for every item they thought was an-
chored, they actually produced only 1.86 anchors
per anchored item. Thus, people were most con-
cerned with finding an anchor, i.e. making sure that
something they think is easily accommodatable is
given at least one preceding item to blame for that;
they were less diligent in marking up all such items.
This is also understandable processing-wise; after a
scrupulous read of the text, coming up with one or
two anchors can be done from memory, only occa-
sionally going back to the text; putting down all an-
chors would require systematic scanning of the pre-
vious stretch of text for every item on the list; the
latter task is hardly doable in 70 minutes.
7A random variable ranging between 0 and 20 says how
many ?random? people marked an item as anchored. We model
?random? versions of annotators by taking the proportions 	 

of items marked as anchored by annotator  in the whole of the
dataset, and assuming that for every word, the person was toss-
ing a coin with P(heads) = 	 
, independently for every word.
8Confidence level of 	    allows augmenting the set
of reliably unanchored items with those marked by 1 or 2 peo-
ple, retaining the same cutoff for anchoredness. This cut covers
more than 60% of the data, and contains 1504 items, 538 of
which are anchored.
58
Having in mind the difficulty of producing an ex-
haustive list of anchors for every item, we conducted
a follow-up experiment to see whether people would
accept anchors when those are presented to them, as
opposed to generating ones. We used 6 out of the
10 texts and 17 out of 20 annotators for the follow-
up experiment. Each person did 3 text, each texts
received 7-9 annotations of this kind.
For each text, the reader was presented with the
same list of words as in the first part, only now each
word was accompanied by a list of anchors. For each
item, every anchor generated by at least one person
was included; the order of the anchors had no corre-
spondence with the number of people who generated
it. A small number of items also received a random
anchor ? a randomly chosen word from the preced-
ing part of the wordlist. The task was crossing over
anchors that the person does not agree with.
Ideally, i.e. if lack of markup is merely a dif-
ference in attention but not in judgment, all non-
random anchors should be accepted. To see the dis-
tance of the actual results from this scenario, we cal-
culate the total mass of votes as number of anchored-
anchor pairs times number of people, and check
how many are accept votes. For all non-random
pairs, 62% were accept votes; for the core annota-
tions (pairs of reliably anchored items with strong
anchors) 94% were accept votes, texts ranging be-
tween 90% and 96%; for pairs with a random an-
chor, only 15% were accept votes. Thus, agreement
based analysis of anchor generation data allowed us
to identify a highly valid portion of the annotations.
5 Conclusion
This paper presented a reader-based experiment on
finding lexical cohesive patterns in texts. As it often
happens with tasks related to semantics/pragmatics
(Poesio and Vieira, 1998; Morris and Hirst, 2005),
the inter-reader agreement levels did not reach the
accepted reliability thresholds. We showed, how-
ever, that statistical analysis of the data, in conjunc-
tion with a subsequent validation experiment, allow
identification of a reliably annotated core of the phe-
nomenon.
The core data may now be used in various ways.
First, it can seed psycholinguistic experimentation
of lexical cohesion: are anchored items processed
quicker than unanchored ones? When asked to re-
call the content of a text, would people remember
prolific anchors of this text? Such experiments will
further our understanding of the nature of text-reader
interaction and help improve applications like text
generation and summarization.
Second, it can serve as a minimal test data for
computational models of lexical cohesion: any good
model should at least get the core part right. Much
of the existing applied research on lexical cohesion
uses WordNet-based (Miller, 1990) lexical chains to
identify the cohesive texture for a larger text pro-
cessing application (Barzilay and Elhadad, 1997;
Stokes et al, 2004; Moldovan and Novischi, 2002;
Al-Halimi and Kazman, 1998). We can now subject
these putative chains to a direct test; in fact, this is
the immediate future research direction.
In addition, analysis techniques discussed in the
paper ? separating interpretation disagreement from
difference in consistency, using statistical hypoth-
esis testing to find reliable parts of the annota-
tions and validating them experimentally ? may be
applied to data resulting from other kinds of ex-
ploratory experiments to gain insights about the phe-
nomena at hand.
Acknowledgment
I would like to thank Prof. Eli Shamir for guidance
and numerous discussions.
References
Reem Al-Halimi and Rick Kazman. 1998. Temporal in-
dexing through lexical chaining. In C. Fellbaum, ed-
itor, WordNet: An Electronic Lexical Database, pages
333?351. MIT Press, Cambridge, MA.
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In Proceedings
of the ACL Intelligent Scalable Text Summarization
Workshop, pages 86?90.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Linguis-
tics, 22(2):249?254.
Barbara Di Eugenio and Michael Glass. 2004. The kappa
statistic: a second look. Computational Linguistics,
30(1):95?101.
M.A.K. Halliday and Ruqaiya Hasan. 1976. Cohesion in
English. Longman Group Ltd.
59
Marti Hearst. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Lynette Hirschman, Patricia Robinson, John D. Burger,
and Marc Vilain. 1998. Automating coreference:
The role of annotated training data. CoRR, cmp-
lg/9803001.
Klaus Krippendorff. 1980. Content Analysis. Sage Pub-
lications.
Daniel Marcu, Estibaliz Amorrortu, and Magdalena
Romera. 1999. Experiments in constructing a corpus
of discourse trees. In Proceedings of ACL?99 Work-
shop on Standards and Tools for Discourse Tagging,
pages 48?57.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313 ? 330.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?312.
Ruslan Mitkov, Richard Evans, Constantin Orasan,
Catalina Barbu, Lisa Jones, and Violeta Sotirova.
2000. Coreference and anaphora: developing anno-
tating tools, annotated resources and annotation strate-
gies. In Proceedings of the Discourse Anaphora and
Anaphora Resolution Colloquium (DAARC?2000),
pages 49?58.
Dan Moldovan and Adrian Novischi. 2002. Lexical
chains for question answering. In Proceedings of
COLING 2002.
Jane Morris and Graeme Hirst. 2004. Non-classical lexi-
cal semantic relations. In Proceedings of HLT-NAACL
Workshop on Computational Lexical Semantics.
Jane Morris and Graeme Hirst. 2005. The subjectivity
of lexical cohesion in text. In James C. Chanahan,
Yan Qu, and Janyce Wiebe, editors, Computing atti-
tude and affect in text. Springer, Dodrecht, The Nether-
lands.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
David E. Rumelhart. 1984. Understanding under-
standing. In J. Flood, editor, Understanding Reading
Comprehension, pages 1?20. Delaware: International
Reading Association.
Roger Schank and Robert Abelson. 1977. Scripts, plans,
goals, and understanding: An inquiry into human
knowledge structures. Hillsdale, NJ: Lawrence Erl-
baum.
Sidney Siegel and John N. Castellan. 1988. Nonpara-
metric statistics for the behavioral sciences. McGraw
Hill, Boston, MA.
Nicola Stokes, Joe Carthy, and Alan F. Smeaton. 2004.
Select: A lexical cohesion based news story segmenta-
tion system. Journal of AI Communications, 17(1):3?
12.
Bonny Webber and Donna Byron, editors. 2004. Pro-
ceedings of the ACL-2004 Workshop on Discourse An-
notation, Barcelona, Spain, July.
A Measures of Agreement
Let   be the number of items to be classified; 
- the number of categories to classify into;  - the
number of raters;  is the number of annotators
who assigned the i-th item to j-th category. We
use Siegel and Castellan?s (1988) version of  ; al-
though it assumes similar distributions of categories
across coders in that it uses the average to estimate
the expected agreement (see equation 2), the cur-
rent experiment employs 22 coders, so averaging is a
much better justified enterprise than in studies with
very few coders (2-4), typical in discourse annota-
tion work (Di Eugenio and Glass, 2004). The calcu-
lation of the  statistic follows (Krippendorff, 1980).
The   Statistic
  
  	 
   	


   	
(1)
  	



  
 
 
    
 
 
  
(2)
  	


  





	


 


  


 



	 (3)
The  Statistic

   Squibs
From Annotator Agreement to Noise Models
Beata Beigman Klebanov?
Northwestern University
Eyal Beigman??
Northwestern University
This article discusses the transition from annotated data to a gold standard, that is, a subset
that is sufficiently noise-free with high confidence. Unless appropriately reinterpreted, agreement
coefficients do not indicate the quality of the data set as a benchmarking resource: High overall
agreement is neither sufficient nor necessary to distill some amount of highly reliable data from
the annotated material. A mathematical framework is developed that allows estimation of the
noise level of the agreed subset of annotated data, which helps promote cautious benchmarking.
1. Introduction
By and large, the reason a computational linguist engages in an annotation project is to
build a reliable data set for the eventual testing, and possibly training, of an algorithm
performing the task. Hence, the crucial question regarding the annotated data set is
whether it is good for benchmarking.
For classification tasks, the current practice is to infer this information from the
value of an inter-annotator agreement coefficient such as the ? statistic (Cohen 1960;
Siegel and Castellan 1988; Carletta 1996). If agreement is high, the whole of the data set
is good for training and testing; the remaining disagreements are typically adjudicated
by an expert (Snyder and Palmer 2004; Palmer, Kingsbury, and Gildea 2005; Girju,
Badulescu, andMoldovan 2006) or through discussion (Litman, Hirschberg, and Swerts
2006), or, in case of more than two annotators, the majority label is chosen (Vieira and
Poesio 2000).1 There are some studies where cases of disagreement were removed from
test data (Markert and Nissim 2002; Dagan, Glickman, and Magnini 2006). If agreement
is low, the whole data set is discarded as unreliable. The threshold of acceptability seems
to have stabilized around ? = 0.67 (Carletta 1996; Di Eugenio and Glass 2004).
There is little understanding, however, of exactly how and how well the value of ?
reflects the quality of the data for benchmarking purposes. We develop a model of an-
notation generation that allows estimation of the level of noise in a specially constructed
gold standard. A gold standard with a noise figure supports cautious benchmarking,
? Kellogg School of Management, Northwestern University, Evanston, IL, beata@northwestern.edu.
?? Kellogg School of Management, Northwestern University, Evanston, IL, e-beigman@northwestern.edu.
1 In many studies, the procedure for handling disagreements is not clearly specified. For example, Gildea
and Jurafsky (2002) mention a ?consistency check?; in Lapata (2002), two annotators attained ? = 0.78 on
200 test instances, but it is not clear how cases of disagreements were settled.
Submission received: 30 June 2008; revised submission received: 3 December 2008; accepted for publication:
26 January 2009.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 4
by requiring that the performance of an algorithm be better than baseline by more than
that which can be attributed to noise. Articulating an annotation generation model also
allows us to shed light on the information ? can contribute to benchmarking.
2. Annotation Noise
We are interested in finding out which parts of the annotated data are sufficiently
reliable. This question presupposes a division of instances into two types: reliable
and unreliable, or, as we shall call them, easy and hard, under the assumption that
items that are easy are reliably annotated, whereas items that are hard display con-
fusion and disagreement. The plausibility of separation into easy and hard instances
is supported by researchers conducting annotation projects: ?With many judgments
that characterize natural language, one would expect that there are clear cases as well
as borderline cases that are more difficult to judge? (Wiebe, Wilson, and Cardie 2005,
page 200).
This suggests a model of annotation generation with latent variables for types, thus,
for every instance i, there is a variable li with values E (easy) and H (hard). Let n be the
number of instances, k the number of annotators, and Xij the classification of the ith in-
stance by the jth annotator. An annotation generationmodel assigns a functional form to
the joint distribution conditioned on the latent variable P(Xi1, . . . ,Xik|li). Similar models
have been studied in biometrics (Aickin 1990; Hui and Zhou 1998; Albert, McShane,
and Shih 2001; Albert and Dodd 2004). The main assumption is that, conditioned on the
type, annotators agree on easy instances and independently flip a coin on hard ones.
The joint distribution satisfies:
P(Xi1= ...=Xik|li=E)=1; P(Xi1=b1, ...,Xik=bk|li=H)=
?k
j=1
P(Xij=bj|li=H)
We want to take only easy instances into the gold standard, so that it contains
only settled, trustworthy judgments.2 The problem is that the fact of being easy or
hard is not directly observable, but has to be inferred from the observed annotations.
In particular, some of the observed agreements will in fact be hard instances, since
coin-flips could occasionally come out all-heads or all-tails. Our objective is to estimate,
with a given degree of confidence (?), the proportion ? of hard instances in the agreed
annotations, based on the number of observed disagreements. The value of ? is the level
of annotation noise in the gold standard comprising agreed annotations.
Let p be the probability that the annotators agree on a hard instance in a binary
classification task:
p=P(Xi1= ...=Xik|li=H)=
?k
j=1
P(Xij=0|li=H)+
?k
j=1
P(Xij=1|li=H)
Denote by Ad the event that there are d disagreed instances; these are hard, and are
assumed to be labeled by coin-flips. Let Bh be the event that there are overall h hard
2 On the status of hard instances, see Section 5.1.
496
Beigman Klebanov and Beigman From Annotator Agreement to Noise Models
instances; some of these may be unobserved as they surface as random agreements. We
note that P(Ad|Bh) =
(
h
d
)
? (1? p)d ? ph?d for d ? h, hence:
P(Bh|Ad) =
P(Ad ? Bh)
P(Ad)
=
P(Ad|Bh) ? P(Bh)
?n
i=d P(Ad|Bi) ? P(Bi)
=
(
h
d
)
? ph?d ? P(Bh)
?n
i=d
(
i
d
)
? pi?d ? P(Bi)
Let X be a random variable designating the number of coin-flips. It follows that
P(X > t|Ad) =
?n
i=t+1
(
i
d
)
? pi?d ? P(Bi)
?n
i=d
(
i
d
)
? pi?d ? P(Bi)
(1)
Let t0 be the smallest integer for which P(X > t0|Ad) < 1? ?. Given d observed dis-
agreements, we estimate the noise level of the agreed subset of the annotations as at
most ? = t0?dn?d , with confidence ?.
3. Relation to ? Statistic
3.1 The Case of High ? with Two Annotators
Suppose 1,000 instances have been annotated by two people, such that 900 are instances
of agreement. Both in the 900 agreed instances and in the 100 disagreed ones, the
categories were estimated to be equiprobable for both annotators.3 In this case p = 0.5,
? = 0.8,4 which is usually taken to be an indicator of sufficiently agreeable guidelines,
and, by implication, of a high quality data set. Our candidate gold standard is the 900
instances of agreement. What is its 95% confidence noise rate?We find, using ourmodel,
that with more than 5% probability up to 125 agreements are due to coin-flipping, hence
? = 13.8%.5 This scenario is not hypothetical. In Poesio and Vieira (1998) Experiment 1,
the classification of definite descriptions into Anaphoric-or-Associative versus Unfa-
miliar has n = 992, d = 121, p = 0.47, which, with 95% confidence, yields ? = 15%.
Let us reverse the question: For a two-annotator project with 1,000 instances, how
many disagreements could we tolerate, so that the agreed part is 95% noise-free with
95% confidence? Only 33 disagreements, corresponding to ? = 0.93. In practice, this
means that a two-annotator project of this size is unlikely to produce a high-quality
gold standard, the high ? notwithstanding.
3.2 The Case of Low ? with Five Annotators
Suppose now 1,000 instances are annotated by five people, with 660 agreements. With
categories equiprobable in both hard and easy instances, p = 0.0625. The exact value
of ? depends on the distribution of votes in the 340 disagreed cases, from ? = 0.73
when all disagreements are split 4-to-1, to ? = 0.52 when all disagreements are split
3-to-2. Assuming disagreements are coin-flips, the most likely measurement would be
about ? = 0.637, where the 340 observed coin-flips yielded the most likely pattern.6 This
value of ? is considered low, yet the 660 agreed items make a gold standard within the
3 We estimate P(Xij=1|li=H) by the proportion of disagreed instances that annotator j put in category 1.
4 For calculating ?, we use the version shown in Equation (2).
5 In all our calculations P(B1) = ...=P(Bn ), that is, a priori, any number of hard instances is equiprobable.
6 That is, there are twice as many 3-to-2 cases than 4-to-1, corresponding to
(
5
3
)
as opposed to
(
5
4
)
.
497
Computational Linguistics Volume 35, Number 4
noise rate of ? = 5% with 95% confidence, according to our model. Hence it is possible
for the overall annotation to have low-ish ?, but the agreement of all five annotators,
if observed sufficiently frequently, is reliable, and can be used to build a clean gold
standard.
3.3 Interpreting the ? Statistic in the Annotation Generation Model
The ? statistic is defined as ? = PA?PE1?PE where PA is the observed agreement and PE is the
agreement expected by chance, calculated from the marginals. We use the Siegel and
Castellan (1988) version, referred to as K in Artstein and Poesio (2008):
PE =
m
?
j=1
p2j ; pj =
?n
i=1 aij
nk
; PA =
1
n
n
?
i=1
PAi ; PAi =
?m
j=1
(aij
2
)
(
k
2
) (2)
where n is the number of items; m is the number of categories; k is the number of anno-
tators; and aij is the number of annotators who assigned the ith item to the jth category.
Suppose there are h hard instances and e easy ones, and m = 2. Suppose further
that all annotators flip the same coin on hard instances, and that the distribution of the
categories in easy and hard instances is the same and is given by q1, . . . , qm. Then the
probability for chance agreement between two annotators is q =
?m
j=1 q
2
j , of which PE is
an estimator. Agreement on a particular instance PAi is measured by the proportion
of agreeing pairs of annotators out of all such pairs, and PA is an estimator of the
expected agreement across all instances. Our model assumes perfect agreement on easy
instances and agreement with probability q on hard ones, so we expect to see e+q?h
agreed instances, hence PA is an estimator of
e+qh
e+h . Putting these together, ?=
PA?PE
1?PE
is an estimator of
e+qh
e+h
?q
1?q =
e
e+h , the proportion of easy instances.
7 In fact, Aickin (1990)
shows that ? is very close to this ratio when themarginal distribution over the categories
is uniform, with a more substantial divergence for skewed category distributions.8
The correspondence between ? and the proportion of easy instances makes it clear
why ? is not a sufficient indicator of data quality for benchmarking. For when ? = 0.8,
20% of the data are hard cases. Using all data, especially for testing, is thus potentially
hazardous, and the crucial question is: Can we zero in on the easy instances effectively,
without admitting much noise? This is exactly the question answered by the model.
When the distribution of categories is the same in easy and hard instances and
uniform, ? can be used to address this question as well. Recall that in the two-annotator
case in Section 3.1, ? = 0.8, that is, 80% of instances are estimated to be easy. Because
easy cases are a subset of agreed ones in our model, 800 of the agreed 900 instances are
easy, giving an estimate of 11% noise in the gold standard. Requiring 95% confidence in
noise estimation, we found ? = 13.8%, using our model. Similarly, in the five-annotator
7 The proportion of easy cases is positive, whereas the estimator ? can be negative with non-negligible
probability when e = O(
?
h).
8 In Aickin (1990), category distribution on easy cases is derived from that in the hard cases. The closer the
categories are to uniform distribution in the hard cases, the closer their distribution in hard cases is to
that in easy cases. For example, if the categories are distributed uniformly in hard cases, they are also so
distributed in the easy ones. If the categories are distributed ( 13 ,
2
3 ) in the hard cases, they are distributed
( 15 ,
4
5 ) in the easy cases. For this reason, in Aickin?s model, it is not possible to distinguish between
category imbalance (many more 0s than 1s) and differences in category distributions in easy and hard
cases. His simulations show that in cases of category imbalance (which imply, in his model, differences in
category distributions in easy and hard cases), ? tends to underestimate the proportion of easy instances.
498
Beigman Klebanov and Beigman From Annotator Agreement to Noise Models
scenario in Section 3.2, ? = 0.637 tells us that about 637 out of 1,000 instances are easy;
they are captured quite precisely by the 660 agreements, yielding a noise estimate of
3.5%, again somewhat lower than the high confidence one we gave using the model.
4. Training and Testing in the Presence of Annotation Noise
We discuss two uses of a gold standard within the benchmarking enterprise. The data
could be used for testing, and, if there is enough of it and after an appropriate partition,
for training as well. We consider each case separately in the following sections.
4.1 Testing with Annotation Noise
The two questions one wants to answer using the data are: Howwell does an algorithm
capture the phenomenon? For any two algorithms, which one is better? Consider the
algorithm comparison situation. Suppose we have a gold standard with L items of
which up to R are noise (?=RL ). Two algorithms might differ in performance on the easy
cases, the hard ones, or both. Because we cannot distinguish between easy and hard
instances in the gold standard, we are unable to attribute the difference in performance
correctly. Moreover, as the annotations of the hard instances are random coin-flips, there
is an expected difference in performance that is a result of pure chance.
Suppose two algorithms perform equally well on easy instances; their performance
on the hard ones is as good as agreement-by-coin-flipping would allow. Thus, the
difference in the number of ?correct? answers on hard instances for algorithms A and
B is a random variable S satisfying S =
?R
i=1 Xi where X1, . . . ,XR are independent and
identically distributed random variables which obtain values?1 (A ?right?, B ?wrong?)
and 1 (A ?wrong?, B ?right?) with probability 14 and 0 with probability
1
2 , thus ?S = 0;
?S =
?
R
2 . By Chebyshev?s inequality Pr(|S| > k?) ?
1
k2
: that is, the chance difference
between the algorithms will be within 4.5? with 95% probability.9 In our example, L =
900 and R = 125, hence a difference of up to 35 ?correct? answers (3.9% of the gold
standard) can be attributed to chance.10
This example shows that even if getting a clean data set is not feasible, it is impor-
tant to report the noise rate of the data set that has been produced. This would allow
calibrating the benchmarking procedure by requiring the difference between the two
competing algorithms to be larger than the chance difference scale.
Some perils of testing on noisy data were discussed in a recent article in this journal
by Reidsma and Carletta (2008). They showed that a machine-learning classifier is
sensitive to the type of noise in the data. Specifically, if the noise is in the form of
category over-use (an annotator disproportionately favors a certain category), when
algorithm performance is measured against the noisy data, accuracy estimates are often
inflated relative to performance on the real data, uncorrupted by noise (see Figure 3(b)
therein). This is because ?when the observed data is used to test performance, some of
9 For large R, normal approximation can be used with the tighter 2? bound for 95% confidence.
10 We note that because the difference attributable to coin-flipping is O(
?
?
L ), and assuming noise rate
is constant, the scale of chance difference diminishes with larger data sets (see also footnote 9).
The issue is more important when dealing with small-to-moderate data sets. However, even for
a 130K test set (Sections 22?24 of the Wall Street Journal corpus, standardly used as a test set in
POS-tagging benchmarks), it is useful to know the estimated noise rate, as it is not clear that all
reported improvements in performance would come out significant. For example, Shen, Satta, and
Joshi (2007) summarize performance of five previously published and three newly reported algorithms,
all between 97.10% and 97.33%.
499
Computational Linguistics Volume 35, Number 4
the samples match not because the classifier gets the label right, but because it overuses
the same label as the human coder? (Reidsma and Carletta 2008, page 232). On the
other hand, if disagreements are random classification noise (the label of any instance
can be flipped with a certain probability), a performance estimate based on observed
data would often be lower than performance on the real data, because the noise that
corrupted it was ignored by the classifier (see Figure 2(d) therein).
Reidsma and Carletta (2008) suggest that the community develops methods to
investigate the patterns of disagreements between annotators to gain insight into the po-
tential of incorrect performance estimation. Althoughwe agree on the general point that
human agreements and disagreements should bear directly on the practice of estimating
the performance of an algorithm, we focus on improving the quality of performance
estimation. We suggest (1) mitigating the effect of annotation noise on performance
estimation by using the least noisy part of the data set for testing, that is, a gold standard
with agreed items; (2) providing an estimate of the level of noise in the gold standard,
which can be used to gauge the divergence between the estimate of performance using
the gold standard from the real performance figure on the easy instances (i.e., on noise-
free data), similarly to the algorithm comparison scenario provided herein.
4.2 Learning with Annotation Noise
The problem with noise in the training data is the potential for misclassification of easy
instances in the test data as a result of hard instances in the training data, the problem
we call hard case bias.
Learning in the presence of noise is an active research area in machine learning.
However, annotation noise is different from existing well-understood noise models.
Specifically, random classification noise, where each instance has the same probability of
having its label flipped, is known to be tolerable in supervised learning (Blum et al 1996;
Cohen 1997; Reidsma and Carletta 2008). In annotation noise, coin-flipping is confined
to hard instances, which should not be assumed to be uniformly distributed across the
feature space. Indeed, there is reason to believe that they form clusters; certain feature
combinations tend to give rise to hard instances. The finding reported by Reidsma and
op den Akker (2008) that a classifier trained on data from one annotator tended to agree
much better with test data from the same annotator than with that of another annotator
exemplifies a situation where observed hard cases (i.e., cases where the annotators
disagree) constitute a pattern in the feature space that a classifier picks up.
In a separate article, we establish a number of properties of learning under anno-
tation noise (Beigman and Beigman Klebanov 2009). We show that the 0-1 loss model
may be vulnerable to annotation noise for small data sets, but becomes increasingly
robust the larger the data set, with worst-case hard case bias of ?( 1?
n
). We also show
that learning with the popular voted-perceptron algorithm (Freund and Schapire 1999)
could suffer a constant rate of hard case bias irrespective of the size of the data set.
5. Discussion
5.1 The Status of Hard Instances
We suggested that only the easy instances should be taken into the gold standard. This
is not to say that hard cases should be eliminated from the researcher?s attention; we
merely argue that they should not be used for testing algorithms for benchmarking
500
Beigman Klebanov and Beigman From Annotator Agreement to Noise Models
purposes. Hard cases are interesting for theory development, because this is where the
theory might have a difficulty, but they do not allow for a fair comparison, as their
correct label cannot be determined under the current theory. The agreed data embodies
the well-articulated parts of the theory, which are ready for deployment as a gold
standard for machine learning. Once the theory is improved to a stage where some of
the previously hard cases receive an unproblematic treatment, those items can be added
to the data set, which can make the task more challenging for the machine. Linguistic
theories-in-the-making can have limited coverage; they do not immediately attain the
status of medical conditions, for example, where there presumably exists a true label
even for the hardest-to-diagnose cases.11
5.2 Plausibility of the Model
Beyond the separation into easy and hard instances, our model prescribes certain an-
notator behavior for each type. In our work on metaphor, we observed that certain
metaphor markups were retracted by their authors, when asked after 4?8 weeks to
revisit the annotations (Beigman Klebanov, Beigman, and Diermeier 2008). These were
apparently hard cases, with people resolving their doubts inconsistently on the two
occasions; coin-flipping is a reasonable first-cut model for such cases. The model also
accommodates category over-use bias (Di Eugenio and Glass 2004; Artstein and Poesio
2008; Reidsma and Carletta 2008), as P(Xij=bj|li=H) may vary across annotators.
Still, this model is clearly a simplification. For example, it is possible that there
is more than one degree of hardness, and annotator behavior changes accordingly.
Another extension is modeling imperfect annotators, allowed to commit random errors
on easy cases; this extension would be needed if a large number of annotators is used.
Such extensions, as well as methods for estimating these more complex models,
should clearly be put on the community?s research agenda. The main contribution
of the simple model is in outlining the trajectory from agreement to gold standard
with a noise estimate, and indicating the potential benefit of the latter to data uti-
lization (low overall agreement does not preclude the existence of a reliable subset)
and to prudent benchmarking. Furthermore, the simple model helps us improve the
understanding of the information provided by the ? statistic, and to appreciate its
limitations. It also allows us to see the benefit of adding annotators, as discussed in the
next section.
5.3 Adding Annotators
If we want the test data to be able to detect small advances in machines? handling of
the task, we need to produce gold standards with low noise levels. The level of noise
in agreed data depends on two parameters: (a) the number of agreed items, and (b) the
probability of chance agreement between annotators. Although the first is not under
the researcher?s control once the data set is chosen, the second is, by changing the
number of annotators. Obviously, the more annotators are required to agree, the lower
p will be, and the smaller the number of agreements that can be attributed to coin-
flipping. If indeed 800 out of 1,000 items are easy, agreement between two annotators
can only detect them with up to 13.8% noise. Adding a third annotator means p = 0.25.
11 As one of the anonymous reviewers pointed out, some medical conditions, such as autism, are also only
partially understood.
501
Computational Linguistics Volume 35, Number 4
We are most likely to observe 850 agreed instances, which would not contain more
than 7.7% noise, with 95% confidence. Effectively, we got rid of about half the random
agreements.
Acknowledgments
We thank Eli Shamir and Bei Yu for reading
earlier drafts of this article, as well as the
editor and the anonymous reviewers for
comments that helped us improve the
article significantly.
References
Aickin, Mikel. 1990. Maximum likelihood
estimation of agreement in the constant
predictive probability model, and its
relation to Cohen?s kappa. Biometrics,
46(2):293?302.
Albert, Paul and Lori Dodd. 2004. A
cautionary note on the robustness of latent
class models for estimating diagnostic
error without a gold standard. Biometrics,
60(2):427?435.
Albert, Paul, Lisa McShane, and Joanna Shih.
2001. Latent class modeling approaches
for assessing diagnostic error without a
gold standard: With applications to p53
immunohistochemical assays in bladder
tumors. Biometrics, 57(2):610?619.
Artstein, Ron and Massimo Poesio. 2008.
Inter-coder agreement for computational
linguistics. Computational Linguistics,
34(4):555?596.
Beigman, Eyal and Beata Beigman Klebanov.
2009. Learning with annotation noise. In
Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics,
Singapore.
Beigman Klebanov, Beata, Eyal Beigman,
and Daniel Diermeier. 2008. Analyzing
disagreements. In COLING 2008 Workshop
on Human Judgments in Computational
Linguistics, pages 2?7, Manchester.
Blum, Avrim, Alan Frieze, Ravi Kannan,
and Santosh Vempala. 1996. A
polynomial-time algorithm for learning
noisy linear threshold functions. In
Proceedings of the 37th Annual IEEE
Symposium on Foundations of Computer
Science, pages 330?338, Burlington, VT.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249?254.
Cohen, Edith. 1997. Learning noisy
perceptrons by a perceptron in polynomial
time. In Proceedings of the 38th Annual
Symposium on Foundations of Computer
Science, pages 514?523, Miami Beach, FL.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20(1):37?46.
Dagan, Ido, Oren Glickman, and Bernardo
Magnini. 2006. The PASCAL recognising
textual entailment challenge. In The
PASCAL Recognising Textual Entailment
Challenge, Springer, Berlin, pages 177?190.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95?101.
Freund, Y. and R. E. Schapire. 1999. Large
margin classification using the perceptron
algorithm.Machine Learning, 37(3):277?296.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Girju, Roxana, Adriana Badulescu, and Dan
Moldovan. 2006. Automatic discovery
of part-whole relations. Computational
Linguistics, 32(1):83?135.
Hui, Siu and Xiao Zhou. 1998. Evaluation of
diagnostic tests without gold standards.
Statistical Methods in Medical Research,
7(4):354?370.
Lapata, Maria. 2002. The disambiguation of
nominalizations. Computational Linguistics,
28(3):357?388.
Litman, Diane, Julia Hirschberg, and Marc
Swerts. 2006. Characterizing and
predicting corrections in spoken dialogue
systems. Computational Linguistics,
32(3):417?438.
Markert, Katja and Malvina Nissim. 2002.
Metonymy resolution as a classification
task. In Proceedings of the Empirical Methods
in Natural Language Processing Conference,
pages 204?213, Philadelphia, PA.
Palmer, Martha, Paul Kingsbury, and Daniel
Gildea. 2005. The proposition bank: An
annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Poesio, Massimo and Renata Vieira. 1998.
A corpus-based investigation of definite
description use. Computational Linguistics,
24(2):183?216.
Reidsma, Dennis and Jean Carletta. 2008.
Reliability measurement without limit.
Computational Linguistics, 34(3):319?326.
Reidsma, Dennis and Rieks op den Akker.
2008. Exploiting subjective annotations.
In COLING 2008 Workshop on Human
Judgments in Computational Linguistics,
pages 8?16, Manchester.
502
Beigman Klebanov and Beigman From Annotator Agreement to Noise Models
Shen, Libin, Giorgio Satta, and Aravind
Joshi. 2007. Guided learning for
bidirectional sequence classification. In
Proceedings of the 45th Annual Meeting
of the Association of Computational
Linguistics, pages 760?767, Prague.
Siegel, Sidney and N. John Castellan Jr.
1988. Nonparametric Statistics for the
Behavioral Sciences. McGraw-Hill,
2nd edition.
Snyder, Benjamin and Martha Palmer. 2004.
The English all-words task. In Senseval-3:
3rd International Workshop on the Evaluation
of Systems for the Semantic Analysis of Text,
pages 41?43, Barcelona.
Vieira, Renata and Massimo Poesio. 2000. An
empirically based system for processing
definite descriptions. Computational
Linguistics, 26(4):539?593.
Wiebe, Janyce, Teresa Wilson, and Claire
Cardie. 2005. Annotating expressions of
opinions and emotions in language.
Language Resources and Evaluation,
39(2):165?210.
503

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 438?446,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Some Empirical Evidence for Annotation Noise in a Benchmarked Dataset
Beata Beigman Klebanov
Kellogg School of Management
Northwestern University
beata@northwestern.edu
Eyal Beigman
Washington University in St. Louis
beigman@wustl.edu
Abstract
A number of recent articles in computational
linguistics venues called for a closer exami-
nation of the type of noise present in anno-
tated datasets used for benchmarking (Rei-
dsma and Carletta, 2008; Beigman Klebanov
and Beigman, 2009). In particular, Beigman
Klebanov and Beigman articulated a type of
noise they call annotation noise and showed
that in worst case such noise can severely
degrade the generalization ability of a linear
classifier (Beigman and Beigman Klebanov,
2009). In this paper, we provide quantita-
tive empirical evidence for the existence of
this type of noise in a recently benchmarked
dataset. The proposed methodology can be
used to zero in on unreliable instances, facili-
tating generation of cleaner gold standards for
benchmarking.
1 Introduction
Traditionally, studies in computational linguistics
use few trained annotators. Lately this might be
changing, as inexpensive annotators are available in
large numbers through projects like Amazon Me-
chanical Turk or through online games where an-
notations are produced as a by-product (Poesio et
al., 2008; von Ahn, 2006), and, at least for certain
tasks, the quality of multiple non-expert annotations
is close to that of a small number of experts (Snow
et al, 2008; Callison-Burch, 2009).
Apart from the reduced costs, mass annotation is
a promising way to get detailed information about
the dataset, such as the level of difficulty of the dif-
ference instances. Such information is important
both from the linguistic and from the machine learn-
ing perspective, as the existence of a group of in-
stances difficult enough to look like they have been
labeled by random guesses can in the worst case
induce the machine learner training on the dataset
to misclassify a constant proportion of easy, non-
controversial instances, as well as produce incor-
rect comparative results in a benchmarking setting
(Beigman Klebanov and Beigman, 2009; Beigman
and Beigman Klebanov, 2009) .
In this article, we employ annotation generation
models to estimate the types of instances in a multi-
ply annotated dataset for a binary classification task.
We provide the first quantitative empirical demon-
stration, to our knowledge, of the existence of what
Beigman Klebanov and Beigman (2009) call ?anno-
tation noise? in a benchmarked dataset, that is, for
a case where instances cannot be plausibly assigned
to just two classes, and where instances in the third
class can be plausibly described as having been an-
notated by flips of a nearly fair coin. The ability to
identify such instances helps improve the gold stan-
dard by eliminating them, and allows further empiri-
cal investigation of their impact on machine learning
for the task in question.
2 Generative models of annotation
We present a graphical model for the generation of
annotations. The basic idea is that there are different
types of instances that induce different responses
from annotators. Each instance may have a true la-
bel of ?0? or ?1?, however, the researcher?s access
to it is mediated by annotators who are guessing the
true label by flipping a coin, where the bias of the
coin depends on the type of the instance. The bias
of the coin essentially models the difficulty of label-
438
ing the instance; coins biased close to 0 and 1 cor-
respond to instances that are easy to classify; a fair
coin represents instances that are very difficult if not
impossible to classify correctly with the given pool
of annotators. The model presented in Beigman Kle-
banov and Beigman (2009) is a special case with 3
types (A,B,C) where pA=0, pC=1 (easy cases), and
0<pB<1 represents the hard cases, the harder the
closer pB is to 0.5. Models used here are a type of la-
tent class models (McCutcheon, 1987) widely used
in the Biometrics community (Espeland and Handel-
man, 1989; Yang and Becker, 1997; Albert et al,
2001; Albert and Dodd, 2004).
The goal of modeling is to determine whether
more than two types of instances need to be postu-
lated, to estimate how difficult each type is, and to
identify the troublemaking instances.
The graphical model is presented in figure 1. We
assume the dataset of size N is a mixture of k dif-
ferent types of instances. The proportion of types is
given by ? = (?1, . . . , ?k), and coin biases for each
type are given by p = (p1, . . . , pk). Each instance is
annotated by n i.i.d coinflips, and random variable
x ? {0, . . . , n} counts the number of ?1?s in the n
annotations given to an instance. Each instance be-
longs to a type t ? {1, ..., k}, characterized by a coin
with the probability pt of annotating with the label
?1?. Conditioned on t, the number of ?1?s in n an-
notations has a binomial distribution with parameter
pt: Pr(x = j|t) =
(n
j
)
pjt (1? pt)
n?j .
x t ? N 
x t ? 
N a ? 
p 
p 
x a ? N 
p 
Figure 1: A graphical model of annotation generation.
The probability of observing j ?1?s out of n an-
notations for an instance given ? and p is therefore
Pr(x = j|?, p) =
?k
t=1 Pr(t|?) ? Pr(x = j|t) =
=
(n
j
)?k
t=1 ?tp
j
t (1 ? pt)
n?j . The annotations are
thus generated by a superposition of k binomials.
3 Data
3.1 Recognizing Textual Entailment -1
For the experiments reported here we use the 800
item test data of the first Recognizing Textual Entail-
ment benchmark (RTE-1) from Dagan et al (2006).
This task drew a lot of attention in the community,
with a series of benchmarks in 2005-2007.
The task is defined as follows: ?... textual entail-
ment is defined as a directional relationship between
pairs of text expressions, denoted by T - the entail-
ing ?Text?, and H - the entailed ?Hypothesis?. We
say that T entails H if the meaning of H can be in-
ferred from the meaning of T, as would typically be
interpreted by people. This somewhat informal defi-
nition is based on (and assumes) common human
understanding of language as well as common back-
ground knowledge? (Dagan et al, 2006). Further
guidelines included an instruction to disregard tense
differences, to accept cases where the inference is
?very probable (but not completely certain)? and to
avoid cases where the inference ?has some positive
probability that is not clearly very high.? An exam-
ple of a true entailment is the pair T-H: (T) Cavern
Club sessions paid the Beatles ?15 evenings and ?5
lunchtime. (H) The Beatles perform at Cavern.
Although annotated by a small number of experts
for the benchmark, the RTE-1 dataset has been later
transferred to a mass annotation framework by Snow
et al (2008), who submitted simplified guidelines
to the Amazon Mechanical Turk workplace (hence-
forth, AMT), collected 10 annotations per item from
the total of 164 annotators, and showed that major-
ity vote by Turkers agreed with expert annotation in
89.7% of the cases. We call the Snow et al (2008)
Turker annotations SRTE dataset, and use it in sec-
tion 6. The instructions, followed by two examples,
read: ?Please state whether the second sentence (the
Hypothesis) is implied by the information in the first
sentence (the Text), i.e., please state whether the Hy-
pothesis can be determined to be true given that the
Text is true. Assume that you do not know anything
about the situation except what the Text itself says.
Also, note that every part of the Hypothesis must be
implied by the Text in order for it to be true.? The
guidelines for Turkers are somewhat different from
the original, not mentioning the issue of highly prob-
able though not certain inference or a special treat-
439
ment of tense mismatch between H and T, as well as
discouraging reliance on background knowledge.
Using Snow et al (2008) instructions, we col-
lected 20 annotations for each of the 800 items
through AMT from the total of 441 annotators. Each
annotator did the minimum of 2 items, and was
paid $0.01 for 2 items, for the total annotator cost
of $80. We used only annotators with prior AMT
approval rate of at least 95%, that is, only people
whose performance in previous tasks on AMT was
almost always approved by the requester of the task.
Our design is thus somewhat different from Snow et
al. (2008), as we paid more and selected annotators
with a stake in their AMT reputation.
3.2 Preparing the data for model fitting
We collected the annotations in two separate batches
of 10 annotations per item, using the same set of in-
structions, incentives, and examples. We hypothe-
sized that controlling for these elements, we would
get two random samples from the same distribution
of Turkers, and hence will have two samples to make
sure a model fitted on one sample generalized to
the other. It turned out, however, that a 3-Binomial
model with a good fit on one of the samples was re-
jected with high probability for the other.1 Thus, on
the one hand, the variations between annotators in
each sample were not as high as to preclude a model
that captures only instance variability from fitting
well; on the other hand, evidently, the two samples
did not come from the same annotator distribution,
but differed systematically due to factors we did not
control for.2 In order for our models not to inherit a
systematic bias of any of the two samples, we mixed
the two samples, and constructed two sets, BRTEa
and BRTEb, each with 10 annotations per item, by
randomly splitting the 20 answers per item into two
groups, allowing the same annotator to contribute
to different groups on different instances. Indeed,
after the randomization, a model fitted for BRTEa
produced excellent generalization on BRTEb, as we
will see in section 4.2.
1For details of the model fitting procedure, see section 4.
2Such factors could be the hour and day of assignment, as
the composition of AMT?s global 24/7 workforce could differ
systematically by day and hour.
4 Fitting a model to BRTE data
Using the model template presented in section 2, we
successively attempt to fit a model with k = 2, 3, . . .
until a model with a good fit is found or no degrees
of freedom are left. For a given k, we fit the pa-
rameters ? and p using non-linear least squares trust-
region method as implemented in the default version
of MATLAB?s lsqnonlin function. We then use ?2 to
measure goodness of fit; a model that cannot be re-
jected with 95% confidence (p>0.05) would be con-
sidered a good fit. In all cases N=800, n=10, as we
use 10 annotations for each instance.
4.1 Mixture of 2 Binomials
Suppose k=2, with types t0 and t1. The best fit yields
p0=0.237, p1=0.867, ?0=431800 , ?1=1-?0. The model
(shown in figure 2) is a poor fit, with ?2=73.66 well
above the critical value of 14.07 for df=7, p=0.05.3
02040
6080100
120140160
0 1 2 3 4 5 6 7 8 9 10Number of label "1" annotations
Number of instan
ces ObservedPredicted
Figure 2: Fitting the model B1+B2 to BRTEa data. B1?
B(10,0.237) on 431 instances, B2? B(10,0.867) on 369
instances. The point (x,y) means that there are y in-
stances given label ?1? in exactly x out of 10 annotations.
4.2 Model M: Mixture of 3 Binomials
Suppose now k=3. The best fitting model
M=B1+B2+B3 is specified in figure 3; M fits the
data very well. Assuming B1 and B3 reflect items
3For degrees of freedom, we take the number of datapoints
being fitted (11), take one degree of freedom off for knowing in
advance the total number of instances, and take off additional 3
degrees of freedom for estimating p0, p1, and ?0 from the data.
We are therefore left with 7 degrees of freedom in this case.
440
with uncontroversial labels ?0? and ?1?, respec-
tively, the model suggests that detecting ?0? (no tex-
tual entailment) is somewhat more difficult for non-
experts than detecting ?1? (there is textual entail-
ment) in this dataset, with the rate of incorrect pre-
dictions of about 20% and 10%, respectively.4 The
model also predicts that 159800 ? 20% of the data are
difficult cases, with annotators flipping a close-to-a-
fair coin (p=0.5487).
02040
6080100
120140
0 1 2 3 4 5 6 7 8 9 10
ObservedB1B2B3Predicted
Figure 3: Fitting the model M=B1+B2+B3 to BRTEa
data. B1? B(10,0.1978) on 343 instances, B2?
B(10,0.5487) on 159 instances, B3? B(10,0.8942) on
298 instances. The binomials are shown in grey lines.
The model M fits with ?2=5.091; for df=5, this corre-
sponds to p=0.4.
We use the dataset BRTEb to test the model de-
veloped on BRTEa. The model fits with ?2=13.13,
which, for df=10,5 corresponds to p=0.2154.
We therefore conclude that, after eliminating sys-
tematic differences between annotators, we were un-
able to fit a model with two types of instances,
whereas a model with three types of instances pro-
vides a good fit both for the dataset on which it is
estimated and for a new dataset. This constitutes
empirical evidence for the existence of a group of
instances with near-random labels in this recently
4We note that any conclusions from the model hold for the
particular 800 item dataset in question, and not for the task of
recognizing textual entailment in general, as the dataset is not
necessarily a representative sample. In fact, we know from Da-
gan et al (2006) that these 800 items are not a random sam-
ple, but rather what remained after some 400 instances were re-
moved due to disagreements between expert annotators or due
to the judgment of one of organizers of the RTE-1 challenge.
5No parameters are fitted using the BRTEb data.
benchmarked dataset, at least for our pool of more
than 400 non-expert annotators.
5 Could annotator heterogeneity provide
an alternative explanation?
In the previous section, we established that instance
heterogeneity can explain the observations. We
might however ask whether a different model could
provide a similarly fitting explanation. Specifically,
heterogeneity among annotators has been seen as a
major source of noise in the aggregate data and there
are several works attempting to separate high qual-
ity annotators from low quality ones (Raykar et al,
2009; Donmez et al, 2009; Sheng et al, 2008; Car-
penter, 2008). Could we explain the observed beha-
vior with a model with only two types of instances
that allows for annotator heterogeneity?
In this section we construct such a model. We
show that this model entails an instance distribu-
tion that is a superposition of two normal distribu-
tions. We subsequently show that the best fitting
two-Gaussian model does not provide a good fit.
We use a generation model similar to those in
(Raykar et al, 2009; Carpenter, 2008) but with
weaker parametric assumptions. The graphical
model is given in figure 4.
x t ? N 
x t ? 
N a ? 
p 
p 
x 
? 
N 
p 
t ? 
Figure 4: Annotation generation model with annotator
heterogeneity.
We assume there are two types of instances t ?
{0, 1} with the proportions ? = (?0, ?1). The 2n
probabilities p = (pt1, . . . , ptn) for t = 0, 1 cor-
respond to coins drawn independently from some
distribution with parameter ? = (?1, . . . , ?n). We
make no assumption on the functional form apart
from a positive probability to draw a value between
0 and 1, this in particular is true for the beta distribu-
tion used in (Raykar et al, 2009; Carpenter, 2008).
As before, the number of ?1?s attributed to an in-
stance of type t is a random variable x, determined
441
by independent flips of the n coins that correspond
to the value of t. The marginal distribution of x is:
Pr(x = j|?, ?) =
=
?
t=0,1
Pr(t|?)
?
[0,1]n
Pr(pt|?)?Pr(x = j|pt, t, ?)dpt
=
?
t=0,1
?t
?
[0,1]n
Pr(pt|?)
?
?
?
|S|=j
?
i?S
pti
?
i6?S
(1? pti)
?
? dpt
Let x1, . . . , xN be the random variables correspond-
ing to the number of ?1?s attributed to instances
1, . . . , N . W.l.g we assume instances 1, . . . , N ? are
all of type t0 (N ? = ?0 ? N ) and the rest of type t1.
Since 0 ? xj ? n it follows that E(xj),Var(xj) <
? for j = 1, . . . , N . If for each instance the coin-
flips are independent, we can think of this as a two
step process where we first draw the coins and then
flip them. Thus, x1, . . . , xN ? are i.i.d and the cen-
tral limit theorem implies that the average number
of ?1?s on t0 instances, namely the random variable
y0 = 1N ?
?N ?
j=1 xj has an approximately normal dis-
tribution.6 Making the same argument for the distri-
bution of y1 for instances of type t1, it follows that
the number of ?1?s attributed to an instance of any
type y = y0 + y1 would have a distribution that is a
superposition of two Gaussians.
The best least-squares fit of all two-Gaussian
models to BRTEa data is produced by G=N1+N2,
N1? N (2.22, 1.73) on 418 instances, N2?
N (9.07,1.41) on 382 instances; G is shown in
figure 5. G fits with ?2=36.77, much above the crit-
ical value ?2=11.07 for df=5, p=0.05. We can thus
rule out annotator heterogeneity as the only expla-
nation of the observed pattern of responses.
6 Testing M on SRTE data
We further test M on the annotations collected by
Snow et al (2008) for the same 800 item dataset.
While the instructions and the task were identical in
BRTEa, BRTEb, and BRTE datasets, and in all cases
6It can be shown that y0 ? N (?, ?) for ? = n ? EDist(?)(p)
and ? =
p
VarDist(?)(p) ? n, using the expectation and variance
of the coin parameter for type t0 instances. For example, for a
beta distribution with parameters ? and ? these would be ? =
?
?+? n and ? =
q
??
?+? n.
02040
6080100
120
0 1 2 3 4 5 6 7 8 9 10
ObservedPredicted
Figure 5: Model G?s fit to BRTEa data, G= N1+N2, a
mixture of two Gaussians.
each item was given 10 annotations, the incentive
design was different (see section 3).
Figure 6 shows that model M=B1+B2+B3 does
not fit well, as SRTE dataset exhibits a rather diffe-
rent distribution from both BRTE datasets. In par-
ticular, it is clear that had a model been fitted on
SRTE data, the coin flipping probabilities for the
clear types, B1 and B3, would have to be moved
towards 0.5; that is, an average annotator in SRTE
dataset had worse ability to detect clear 0s and clear
1s than an average BRTE annotator. We note that
BRTEa and BRTEb agreed with expert annotation
in 92.5% and 90.8% of the instances, respectively,
both better than 89.7% in SRTE.7 Since we offered
somewhat better incentives in BRTE, it is tempting
to attribute the observed better quality of BRTE an-
notations to the improved incentives, although it is
possible that some other uncontrolled AMT-related
factor is responsible for the difference between the
datasets, just as we found for our original two col-
lected samples (see section 3.2).
Supposing the main source of misfit is difference
in incentives, we conjecture that the difference be-
tween the 441 BRTE annotators and the 164 SRTE
ones is due to the existence in SRTE of unmotivated,
or ?lazy? annotators, that is, people who flipped the
same coin on every instance, no matter what type.
Our hypothesis is that once an annotator is diligent
(and motivated) enough to pay attention to the data,
her annotations can be described by model M, but
some annotators are not sufficiently diligent.
7Turker annotations were aggregated using majority vote, as
in Snow et al (2008) section 4.3.
442
0204060
80100120140
0 1 2 3 4 5 6 7 8 9 10
BRTEaBRTEbObserved (SRTE)Predicted by M
Figure 6: Model M?s fit to SRTE data. BRTEa and
BRTEb are shown in grey lines.
In this model we assume there are three types
of instances as before, and two types of annotators
a ? {D,L}, for Diligent and Lazy, with their pro-
portions in the population ? = (?D, ?L). The corre-
sponding graphical model is shown in figure 7.
x t ? N 
x 
t ? N 
a ? 
p 
p 
x 
? 
N 
p 
t ? 
x 
t ? N 
a ? p 
c 
Figure 7: Annotation generation with diligent and lazy
annotators.
We assume that diligent annotators flip coins cor-
responding to the types of instances, whereas lazy
annotators always flip the same coin pL.
Let nD and nL=n?nD be the number of diligent
and lazy annotations given to a certain instance, thus
Pr(nD=r|?)=
(n
r
)
?rD?
n?r
L , and the probability of ob-
serving j label ?1? annotations for an instance of
type t is given by:
Pr(x = j|t, ?, p) =
n?
r=1
[(
n
r
)
?rD?
n?r
L ?
?
[
?
(j1,j2)?S
(
r
j1
)
pj1t (1? pt)
r?j1 ?
?
(
n? r
j2
)
pj2L (1? pL)
n?r?j2
]]
where S={(j1, j2):j1+j2=j; j1?r;j2?n-r}. Finally,
Pr(x=j|?, ?, p)=
?k
t=1 ?t Pr(x=j|t, ?, p).
We assume that model M provides the values for
? and p for all diligent annotators, and estimate ?
and pL, the proportion of the lazy annotators and
the coin they flip. The best fitting model yields
?=(0.79,0.21), and pL=0.74, predicting that about
one-fifth of SRTE annotators are lazy.8 This model
fits with ?2=14.63, which is below the critical level
of ?2=15.51 for df=8,p=0.05, hence a hypothesis
that model M behavior for the diligent annotators
and flipping a coin with bias 0.74 for the lazy ones
generated the SRTE data cannot be rejected with
high confidence. We note that Carpenter (2008) ar-
rived at a similar conclusion ? that there are quite
a few annotators making random guesses in SRTE
dataset ? by means of jointly estimating annotator
accuracies.
7 Discussion
To summarize our findings: With systematic dif-
ferences between annotators smoothed out, there
is evidence that non-expert annotators performing
RTE task on RTE-1 test data tend to flip a close-
to-fair coin on about 20% of instances, according
to the best fitting model.9 This constitutes, to our
knowledge, the first empirical evidence for the ex-
istence of the kind of noise termed annotation noise
in Beigman Klebanov and Beigman (2009). Given
Beigman Klebanov and Beigman (2009) warning
against annotation noise in test data and their find-
ing in Beigman and Beigman Klebanov (2009) that
annotation noise in training data can potentially dev-
astate a linear classifier learning from the data, the
immediate usefulness of our result is that instances
of this difficult type can be identified, removed from
the dataset before further benchmarking, and pos-
8A more precise statement is that there are about one-fifth
lazy potential annotators in the SRTE pool for any given item.
It is possible that the length of stay of an annotator in the pool is
not independent of her diligence; for example, Callison-Burch
(2009) found in his AMT experiments with tasks related to ma-
chine translation that lazy annotators tended to stay longer and
do more annotations.
9Beigman Klebanov and Beigman (2009) discuss the con-
nection between noise models and inter-annotator agreement.
443
sibly used in a controlled fashion for subsequent
studies of the impact of annotation noise on specific
learning algorithms and feature spaces for this task.
The current literature on generating benchmark-
ing data from AMT annotations overwhelmingly
considers annotator heterogeneity as the source of
observed discrepancies, with instances falling into
two classes only. Our results suggest that, at least in
RTE data, instance heterogeneity cannot be ignored.
It also transpired that small variations in incen-
tives (as between SRTE and BRTE), and even un-
known factors possibly related to differences in the
composition of AMT?s workforce can lead to sys-
tematic differences in the resulting annotator pools,
which results in annotations that are described by
models with somewhat different parameter values.
This can potentially limit the usefulness of our main
finding, because it is not clear how reliable the iden-
tification of hard cases is using any particular group
of Turkers. While this is a valid concern in general,
we show in section 7.1 that many items consistently
found to be hard by different groups of Turkers war-
rant at least an additional examination, as they often
represent borderline cases of highly or not-so-highly
probable inferences, corruption of meaning by un-
grammaticality, or difficulties related to the treat-
ment of time references and background knowledge.
Finally, our findings seem to be at odds with the
fact that the 800 items analyzed here were left af-
ter all items on which two experts disagreed and all
items that looked controversial to the arbiter were
removed (see section 3). One potential explanation
is that things that are hard for Turkers are not nec-
essarily hard for experts. Yet it is possible that two
or three annotators, graduate students or faculty in
computational linguistics, are an especially homoge-
nous and small pool of people to base gold standard
annotations of the way things are ?typically inter-
preted by people? upon. Furthermore, there is some
evidence from additional expert re-annotations of
this dataset that some controversies remain; we dis-
cuss relation to expert annotations in section 7.2.
7.1 Hard cases
We examine some of the instances that in all likeli-
hood belong to the difficult type, according to Turk-
ers. We focus on items that received between 4 and
7 class ?1? annotations in SRTE and in each of our
two datasets (before randomization).
(1) T: Saudi Arabia, the biggest oil producer in
the world, was once a supporter of Osama bin
Laden and his associates who led attacks against
the United States. H: Saudi Arabia is the
world?s biggest oil exporter.
(2) T: Seiler was reported missing March 27 and
was found four days later in a marsh near her
campus apartment. H: Abducted Audrey Seiler
found four days after missing.
(3) T: The spokesman for the rescue authorities,
Linart Ohlin, said that the accident took place
between 01:00 and dawn today, Friday (00:00
GMT) in a disco behind the theatre, where ?hun-
dreds? of young people were present. H: The
fire happened in the early hours of Friday morn-
ing, and hundreds of young people were present.
(4) T: William Leonard Jennings sobbed loudly as
was charged with killing his 3-year-old son,
Stephen, who was last seen alive on Dec.12,
1962. H: William Leonard Jennings killed his
3-year-old son, Stephen.
Labeling of examples 1-4 seems to hinge on the
assessment of the likelihood of an alternative expla-
nation. Thus, it is possible that the biggest producer
of oil is not the biggest exporter, because, for ex-
ample, its internal consumption is much higher than
in the second-biggest producer. In 2, abduction is
a possible cause for being missing, but how rela-
tively probable is it? Similarly, fire is a kind of ac-
cident, but can we infer that there was fire from a
report about an accident? In 4, could the man have
sobbed because on top of loosing his son he was
also being falsely accused of having killed him? Ex-
perts marked all five as true entailments, while many
Turkers had reservations.
(5) T: Bush returned to the White House late Satur-
day while his running mate was off campaigning
in the West. H: Bush left the White House.
(6) T: De la Cruz?s family said he had gone to Saudi
Arabia a year ago to work as a driver after a long
period of unemployment. H: De la Cruz was
unemployed.
(7) T: Measurements by ground-based instruments
around the world have shown a decrease of up
to 10 percent in sunlight from the late 1950s to
the early 1990s. H: The world is about 10 per
cent darker than half a century ago.
444
In examples 5-7 time seems to be an issue. If Bush
returned to White House, he must have left it before-
hand, but does this count as entailment, or is the hy-
pothesis referencing a time concurrent with the text,
in which case T and H are in contradiction? In 6,
can H be seen as referring to some time more than a
year ago? In 7, if the hypothesis is taken to be stated
in mid- or late-2000s, the time of annotation, half
a century ago would reach to late 1950s, but it is
possible that further substantial reduction occurred
between early 1990s mentioned in the text and mid
2000s, amounting to much more than 10%. Experts
labeled example 5 as false, 6 and 7 as true.
(8) T: On 2 February 1990, at the opening of Parlia-
ment, he declared that apartheid had failed and
that the bans on political parties, including the
ANC, were to be lifted. H: Apartheid in South
Africa was abolished in 1990.
(9) T: Kennedy had just won California?s Demo-
cratic presidential primary when Sirhan shot
him in Los Angeles on June 5, 1968. H: Sirhan
killed Kennedy.
Labeling examples 8 and 9 (both true according to
the experts) requires knowledge about South African
and American politics, respectively. Was the ban on
ANC the only or the most important manifestation
of apartheid? Was abolishing apartheid merely an
issue of declaring that it failed? In 9, killing is a po-
tential but not necessary outcome of shooting, so de-
tails of Robert Kennedy?s case need to be known to
the annotator to render the case-specific judgment.
(10) T: The version for the PC has essentially the
same packaging as those for the big game con-
soles, but players have been complaining that
it offers significantly less versatility when it
comes to swinging through New York. H: Play-
ers have been complaining that it sells signifi-
cantly less versatility when it comes to swinging
through New York.
(11) T: During his trip to the Middle East that took
three days, Clinton made the first visit by an
American president to the Palestinian Territories
and participated in a three-way meeting with Is-
raeli Prime Minister Benjamin Netanyahu and
Palestinian President Yasser Arafat. H: During
his trip to the east of the Middle which lasted
three days, the Clinton to first visit to Ameri-
can President to the occupied Palestinian terri-
tories and participated in meeting tripartite co-
operation with Israeli Prime Minister Benjamin
Netanyahu and Palestinian President, Yasser
Arafat.
(12) T: The ISM non-manufacturing index rose to
64.8 in July from 59.9 in June. H: The non-
manufacturing index of the ISM raised 64.8 in
July from 59.9 in June.
(13) T: Henryk Wieniawski, a Polish-born musician,
was known for his special preference for resur-
recting neglected or lost works for the violin. H:
Henryk Wieniawski was born in Polish.
Examples 10-13 were labeled as false by experts,
possibly betraying over-sensitivity to the failings of
language technology. Sells is not an ideal substitu-
tion for offers, but in a certain sense versatility is
sold as part of a product. In 11-13, some Turkers
felt the hypothesis is not too bad a rendition of the
text or of its part, while experts seemed to hold MT
to a higher standard.
7.2 Turkers vs experts
Model M puts 159 items in the difficult type B2.
While M is the best fitting model, it is possible to
find a model that still fits with p>0.05 but places
a smaller number of items in B2, in order to ob-
tain a conservative estimate on the number of dif-
ficult cases. The model with B1? B(10, 0.21) on
373 items, B2? B(10,0.563) on 110 items, B3?
B(10,0.89) on 327 items still produces a fit with
p>0.05, but going down to 100 instances in B2
makes it impossible to find a good fit with a 3 type
model. There are therefore about 110 difficult cases
by a conservative estimate. Assuming there remain
110 hard cases in the 800 item dataset for which
even experts flip a fair coin, we expect about 55
disagreements between the 800 item gold standard
from RTE-1 and a replication by a new expert, or
an agreement of 745800=93% on average. This estimate
is consistent with reports of 91% to 96% replication
accuracy for the expert annotations on various sub-
sets of the data by different groups of experts (see
section 2.3 in Dagan et al (2006)).
Acknowledgments
We would like to thank the anonymous reviewers of
this and the previous draft for helping us improve the
paper significantly. We also thank Amar Cheema for
his advice on AMT.
445
References
Paul Albert and Lori Dodd. 2004. A Cautionary Note on
the Robustness of Latent Class Models for Estimating
Diagnostic Error without a Gold Standard. Biometrics,
60(2):427?435.
Paul Albert, Lisa McShane, Joanna Shih, and The U.S.
National Cancer Institute Bladder Tumor Marker Net-
work. 2001. Latent Class Modeling Approaches for
Assessing Diagnostic Error without a Gold Standard:
With Applications to p53 Immunohistochemical As-
says in Bladder Tumors. Biometrics, 57(2):610?619.
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with Annotation Noise. In Proceedings of
the 47th Annual Meeting of the Association for Com-
putational Linguistics, pages 280?287, Singapore.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From Annotator Agreement to Noise Models. Ac-
cepted to Computational Linguistics.
Chris Callison-Burch. 2009. Fast, Cheap, and Cre-
ative: Evaluating Translation Quality Using Amazon?s
Mechanical Turk. In Proceedings of the Empirical
Methods in Natural Language Processing Conference,
pages 286?295, Singapore.
Bob Carpenter. 2008. Multilevel Bayesian Mod-
els of Categorical Data Annotation. Unpub-
lished manuscript, last accessed 28 July 2009
at lingpipe.files.wordpress.com/2009/01/anno-bayes-
entities-09.pdf.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In J. Quin?onero Candela, I. Dagan,
B. Magnini, and F. d?Alche?-Buc, editors, Machine
Learning Challenges, pages 177?190. Springer.
Pinar Donmez, Jaime Carbonell, and Jeff Schneider.
2009. Efficiently Learning and Accuracy of Labeling
Sources for Selective Sampling. In Proceedings of the
15th International Conference on Knowledge Discov-
ery and Data Mining, pages 259?268, Paris, France.
Mark Espeland and Stanley Handelman. 1989. Using
Class Models to Characterize and Assess Relative Er-
ror in Discrete Measurements. Biometrics, 45(2):587?
599.
Allan McCutcheon. 1987. Latent Class Analysis. New-
bury Park, CA, USA: Sage.
Massimo Poesio, Udo Kruschwitz, and Jon Chamberlain.
2008. ANAWIKI: Creating Anaphorically Annotated
Resources through Web Cooperation. In Proceedings
of the 6th International Conference on Language Re-
sources and Evaluation, Marrakech, Morocco.
Vikas Raykar, Shipeng Yu, Linda Zhao, Anna Jerebko,
Charles Florin, Gerardo Hermosillo Valadez, Luca Bo-
goni, and Linda Moy. 2009. Supervised Learning
from Multiple Experts: Whom to Trust when Every-
one Lies a Bit. In Proceedings of the 26th Annual In-
ternational Conference on Machine Learning, pages
889?896, Montreal, Canada.
Dennis Reidsma and Jean Carletta. 2008. Reliability
Measurement without Limits. Computational Linguis-
tics, 34(3):319?326.
Victor Sheng, Foster Provost, and Panagiotis Ipeirotis.
2008. Get Another Label? Improving Data Quality
and Data Mining Using Multiple, Noisy Labelers. In
Proceedings of the 14th International Conference on
Knowledge Discovery and Data Mining, pages 614?
622, Las Vegas, Nevada, USA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and Fast - But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. In Proceedings of the Empirical Methods
in Natural Language Processing Conference, pages
254?263, Honolulu, Hawaii.
Luis von Ahn. 2006. Games with a Purpose. Computer,
39(6):92?94.
Ilsoon Yang and Mark Becker. 1997. Latent Vari-
able Modeling of Diagnostic Accuracy. Biometrics,
53(3):948?958.
446
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 698?709,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Game-Theoretic Model of Metaphorical Bargaining
Beata Beigman Klebanov
Kellogg School of Management
Northwestern University
beata@northwestern.edu
Eyal Beigman
Washington University in St. Louis
beigman@wustl.edu
Abstract
We present a game-theoretic model of bar-
gaining over a metaphor in the context of
political communication, find its equilib-
rium, and use it to rationalize observed
linguistic behavior. We argue that game
theory is well suited for modeling dis-
course as a dynamic resulting from a num-
ber of conflicting pressures, and suggest
applications of interest to computational
linguists.
1 Introduction
A 13 Dec 1992 article in The Times starts thus:
The European train chugged out of the station
last night; for most of the day it looked as if it
might be stalled there for some time. It managed
to pull away at around 10:30 pm only after the
Spanish prime minister, Felipe Gonzalez, forced
the passengers in the first class carriages into a
last minute whip round to sweeten the trip for the
European Community?s poor four: Spain, Portu-
gal, Greece and Ireland.
The fat controller, Helmut Kohl, beamed with
satisfaction as the deal was done. The elegantly-
suited Francois Mitterrand was equally satisfied.
But nobody was as pleased as John Major, sta-
tionmaster for the UK presidency, for whom the
agreement marked a scarce high point in a bat-
tered premiership.
The departure had actually been delayed by
seven months by Danes on the line. Just when
that problem was solved, there was the volu-
ble outbreak, orchestrated by Spain, from the
poor four passengers demanding that they should
travel free and be given spending money, too.
The coupling of the carriages may not be reli-
ably secure but the pan-European express is in
motion. That few seem to agree the destination
suggests that future arguments are inevitable at
every set of points. Next stop: Copenhagen.
Apart from an entertaining read, the extended
metaphor provides an elaborate conceptual cor-
respondence between a familiar domain of train
journeys and the unfolding process of European
integration. Carriages are likened to nation states;
passengers to their peoples; treaties to stations;
politicians to responsible rail company employees.
In a compact form, the metaphor gives expres-
sion to both the small and the large scale of the
process. It provides for the recent history: Den-
mark?s failure to ratify the 1992 Maastricht treaty
until opt-outs were negotiated later that year is
compared to dissenters sabotaging the journey by
laying on the tracks (Danes on the line); nego-
tiations over the Cohesion Fund that would pro-
vide less developed regions with financial aid to
help them comply with convergence criteria are
likened to second class carriages with poor pas-
sengers for whom the journey had to be subsi-
dized. At a more general level, the European in-
tegration is a purposeful movement towards some
destination according to a worked out plan, get-
ting safely through negotiation and implementa-
tion from one treaty to another, as a train moving
on its rails through subsequent stations, with each
nation being separate yet tied with everyone else.
Numerous inferences regarding speed, timetables,
stations, passengers, different classes of tickets,
temporary obstacles on the tracks, and so on can
be made by the reader based on the knowledge of
train journeys, giving him or her a feeling of an en-
hanced understanding1 of the highly complex pro-
cess of European integration.
So apt was the metaphor that political fights
were waged over its details (Musolff, 2000). Wor-
ries about destination were given an eloquent ex-
pression by Margaret Thatcher (Sunday Times, 20
Sept 1992):
She warned EC leaders to stop their endless
round of summits and take notice of their own
people. ?There is a fear that the European train
will thunder forward, laden with its customary
cargo of gravy, towards a destination neither
wished for nor understood by electorates. But
the train can be stopped,? she said.
1More on enhanced understanding in sections 3.2 and 4.2.
698
The metaphor proved flexible enough for fur-
ther elaboration. John Major, a Conservative PM
of Britain, spoke on June 1st, 1994 about his vi-
sion of the decision making at the EU level, say-
ing that he had never believed that Europe must
act as one on every issue, and advocating ?a sensi-
ble new approach, varying when it needs to, multi-
track, multi-speed, multi-layered.? He attempted
to turn a largely negative Conservative take on the
European train (see Thatcher above) into a tenable
positive vision ? each nation-carriage is now pre-
sumably a rather autonomous entity, waiting on a
side track for the right locomotive, in a huge yet
smoothly operating railroad system.
Major?s political opponents offered their
counter-frames. In both cases, the imagery of
a large transportation system was taken up, yet
turned around to suggest that ?multi, for every-
one? amounts to Britain being in ?the slow lane,?
and a different image was suggested that makes
the negative evaluation of Britain?s opt-outs
more poignant ? a football metaphor, where
relegation to the second division is a sign of a
weak performance, and a school metaphor, where
Britain is portrayed as an under-achiever:
John Cunningham, Labour He has admitted that his Go-
vernment would let Britain fall behind in Europe. He
is apparently willing to offer voluntary relegation to the
second division in Europe, and he isn?t even prepared to
put up a fight. I believe that in any two-speed Europe,
Britain must be up with those in the fast lane. Clearly
Mr Major does not.
Paddy Ashdown, Liberal Democrat Are you really saying
that the best that Britain can hope for under your leader-
ship is ... the slow lane of a two-speed Europe? Most
people in this country will want to aim higher, and will
reject your view of a ?drop-out? Britain.
The pro-European camp rallied around the
?Britain in the slow lane? version as a critical
stance towards the government?s European policy.
Of the alternative metaphors, the school metaphor
has some traction in the Euro discourse, where the
European (mainly German) financial officers are
compared to school authorities, and governments
struggling to meet the strict convergence criteria to
enter the Euro are compared to pupils that barely
make the grade with Britain as a ?drop-out? who
gave up even trying (Musolff, 2000).
The fact that European policy is being commu-
nicated and negotiated via a metaphor is not sur-
prising; after all, ?there is always someone willing
to help us think by providing us with a metaphor
that accords with HIS views.?2 From the point of
view of the dynamics of political discourse, the
puzzle is rather the apparent tendency of politi-
cians to be compelled by the rival?s metaphori-
cal framework. Thatcher tries to turn the train
metaphor used by the pro-EU camp around. Yet,
assuming metaphors are matters of choice, why
should Thatcher feel constrained by her rival?s
choice, why doesn?t she ignore it and merely sug-
gest a new metaphor of her own design? As the
evidence above suggests, this is not Thatcher?s
idiosyncrasy, as Major and his rivals acted simi-
larly. Can this dynamic be explained?
In this article, we use the explanatory frame-
work of game theory, seeking to rationalize the ob-
served behavior by designing a game that would
produce, at equilibrium, the observed dynamics.
Specifically, we formalize the notion that the price
of ?locking? the public into a metaphorical frame
of reference is that a politician is coerced into stay-
ing within the metaphor as well, even if he or she
is at the receiving end of a rival?s rhetorical move.
Since the use of game theory is not common in
computational linguistics, we first explain its main
attributes, justify our decision to make use of it,
and draw connections to research questions that
can benefit from its application (section 2). Next,
we design the game of bargaining over a metaphor,
and find its equilibrium (section 3), followed by a
discussion (section 4).
2 Game-Theoretic models
The basic construct is that of a game, that is,
a model of participants in an interaction (called
?players?), their goals (or ?utilities?) and allow-
able moves. Different moves yield different util-
ities for a player; it is assumed that each player
would pick a strategy that maximizes her utility.
The observable is the actual sequence of moves;
importantly, these are assumed to be the optimal
outcome (an equilibrium) of the relevant game. A
popular notion of equilibrium is Nash equilibrium
(Nash, 1950). For extensive form games (the type
employed in this paper), the notion of subgame
perfect equilibirum is typically used, denoting a
Nash equilibrium that would remain such if the
players start from any stage of the evolving game
(Selten (1975; 1965)).
The task of a game theorist is to reverse-
engineer the model for which the observed se-
2Capitalization in the original, Bolinger (1980, p. 146).
699
quence of actions is an equilibrium. The resulting
model is thereby able to rationalize the observed
behavior as a naturally emerging dynamics be-
tween agents maximizing certain utility functions.
In economics, game-theoretic models are used to
explain price change, organization of production,
and market failures (Mas-Colell et al, 1995; von
Neumann and Morgenstern, 1944); in biology ?
the operation of natural selection processes (Ax-
elrod and Hamilton, 1981; Maynard Smith and
Price, 1973); in social sciences ? political institu-
tions, collective action, and conflict (Greif, 2006;
Schelling, 1997; North, 1990). In recent appli-
cations in linguistics, pragmatic phenoma such as
implicatures are rendered as an equilibrium out-
come of a communication game (Ja?ger and Ebert,
2008; van Rooij, 2008; Ross, 2007; van Rooij and
Schulz, 2004; Parikh, 2001; Glazer and Rubin-
stein, 2001; Dekker and van Rooy, 2000).
Computing equilibria is simple for some games
and quite evolved for others. For example, com-
puting the equilibrium of a zero-sum game is equi-
valent to LP optimization (Luce and Raiffa, 1957);
an equilibrium of general bimatrix games can be
found using a pivoting algorithm (von Stengel,
2007; Lemke and Howson, 1964). Interesting
connections have been pointed out between game
theory and machine learning: Freund and Schapire
(1996) present both online learning and boosting
as a repeated zero-sum game; Shalev-Shwartz and
Singer (2006) show similarly that loss minimiza-
tion in online learning is akin to an equilibrium
path in a repeated game.
While game theoretic models are not much uti-
lized in computational linguistics, they are quite
attractive to tackle some of the problems com-
putational linguists are interested in. For exam-
ple, generation of referring expressions (Paraboni
et al, 2007; Gardent et al, 2004; Siddharthan
and Copestake, 2004; Dale and Reiter, 1995) can
be rendered as a communication game with util-
ity functions that reflect pressures to use shorter
expressions while avoiding excessive ambiguity
(Clark and Parikh, 2007), with corpora anno-
tated for entity mentions informing the design
of a model. Generally, computational linguis-
tics research produces algorithms to detect enti-
ties of various kinds, be it topics, named entities,
metaphors, moves in a multi-party conversations,
or syntactic constructions in large corpora; such
primary data can be used to trace developments
not only in chronological terms (Gruhl et al, 2004;
Allan, 2002), but in strategic terms, i.e. in terms
that reflect agendas of the actors, such as political
agendas in legislatures (Quinn et al, 2006) or ac-
tivist forums (Greene and Resnik, 2009), research
agendas in group meetings (Morgan et al, 2001),
or social agendas in speed-dates (Jurafsky et al,
2009). Game theoretical models are well suited
for modeling dynamics that emerge under multi-
ple, possibly conflicting constraints, as we exem-
plify in this article.
3 The model
We extend Rubinstein (1982) model of negotia-
tion through offers and counter-offers between two
players with a public benefit constraint.
The model consists of (1) two players repre-
senting the opposing sides, (2) a set of frames
X?Rn compact and convex, (3) preference re-
lations described by continuous utility func-
tions U1, U2:X?R+, (4) a sequence of frames
X0?X1 . . .?2X that can be suggested to the pub-
lic, and (5) a sequence of public preferences over
frames inXt for t=0, 1, 2, . . . described by a public
utility function Upt .
The game proceeds as follows. Initially the
frame is F0=X . In odd rounds player 1 appeals to
the public with a frame A1t?Xt|Ft , Xt|Ft={A?Xt :
A?Ft}, player 2 counters with a frame A2t?Xt|Ft .
The public chooses one of the frames based on
Upt (A
i
t) with ties broken in 1?s favor. The ac-
cepted frame becomes the current frame for the
next round Ft+1. In even rounds the parts of play-
ers 1 and 2 are reversed.
A finite sequence F0, . . . , Ft?1 gives the his-
tory of the bargaining process up to t. A
strategy ?i of player i is a function specify-
ing for any history h={F0, . . . , Ft?1} the move
player i makes at time t, namely the frame Ait
she chooses to address the public. A sequence
F0, F1, F2, F3, . . . describes a path the bargaining
process can take, leading to an outcome ??t=0Ft.
The players? utility for an outcome is given by
Ui=limt??
?
Ft
Ui(x)d?Ft for i=1, 2 where ?Ft is
a probability measure on Ft. If ??t=0Ft={x} the
utility is the point utility of x otherwise it is the
expected utility on the intersection set.
3.1 Player utility
For a given issue under discussion, such as Eu-
ropean integration process, we order the possible
700
states of the world along a single dimension that
spans the policy variations proposed by the diffe-
rent players (politicians). Politics of a single issue
are routinely modeled as lying on a single dimen-
sion.3 In the British context, various configura-
tions of the unfolding European reality are situated
along the line between high degree of integration
and complete separatism; Liberal Democrats are
the most pro-European party, while United King-
dom Independence Party are at the far-right end of
the scale, preferring British withdrawal from the
EU. The two major parties, Labour and Conserva-
tives (Tories), prefer intermediate left-leaning and
right-leaning positions, respectively. A schematic
description is shown in figure 1.
??????
???
???
??????
???
???
LibDem? Labour? Tories? UKIP?
????????????????????
? that is unfolding too fast 
? but it is possible to regulate the speed 
? in which case we?ll go slower than others 
??? ???
??????
Figure 1: Preferences on pro-anti Europe axis.
The utilities of the different players can in this
case be described as continuous single-peaked
functions over an interval.4 Thus X=[0, 1], and
the utility functions Ui(x)=?(||x? vi||) for vi?X
where ? is a monotonically strictly decreasing
function and || || is Euclidean distance.
3.2 Public utility
We note the difference between two types of util-
ities: The utility of the players is over outcomes,
the utility of the public is over sets of outcomes
(frames). The latter does not represent a utility the
public has for one outcome or another, but rather a
utility it has for an enhanced understanding. Thus,
the public?s utility from a frame is a function of
the information content of the proposed frame re-
lative to the current frame, i.e. the relative en-
tropy of the two sets.5 Formally, if the accepted
3Indeed, Poole and Rosenthal (1997) argue that no more
than two dimensions are needed to account for voting patterns
on all issues in the US Congress.
4Single-peakedness is a common assumption in position
modeling in political science (Downs, 1957).
5The notion that new beliefs are refinements of existing
ones is current in contemporary theorizing about formation
and change of beliefs, evaluations, and preferences. An up-
date based on the latest available information is consistent
with memory-based theories; in our model, in the equilib-
rium, the current frame contains information about the path-
so-far, thus early stages of the bargaining processes are in
some sense integrated into the current frame, compatible with
the rival, online model of belief formation. See Druckman
and Luria (2000) for a review of the relevant literature.
frame at time t is Ft then for any Borel set A?Ft
the public utility for A is Upt (A)=?(Entt(A))
where Entt(A)=??t(A) log ?t(A) for a continu-
ous probability measure ?t on Ft and ? is a con-
tinuous, monotone ascending function; for A 6?Ft,
Upt (A)=0. We take ?t to be the relative length of
the segment ?t(A)=
|A|
|Ft|
, hence the entropy maxi-
mizing subsegments are of length |Ft|2 .
3.3 Game dynamics
At every point in the game, a certain set of the
states-of-affairs is being deemed sufficiently pro-
bable by the public to require consideration. Sup-
pose that initially any state of affairs within the in-
terval [0, 1] is assigned a uniform probability and
thus merits public attention. Each in her turn, the
players propose to the public to concentrate on
a subset of the currently considered states of af-
fairs, arguing that those are the likelier ones to ob-
tain, hence merit further attention. The metaphor
used to deliver the proposal describes the newly
proposed subset in a way that makes those states-
of-affairs that are in it aligned with the metaphor,
whereas all other states are left out of the proposed
metaphorical frame. As the game proceeds, the
pub ic attention is concentrated on successively
smaller sets of eventualities, and these are given
a more and more detailed metaphoric description,
providing the educational gratification of increa-
singly knowing better and better what is going on.
At each step, each player strives to provide maxi-
mum public gratification while leading the public
to focus on the frame (i.e. subset of states of af-
fairs) that best meets the player?s preferences.6
Figure 2 sketches the frame negotiation through
train metaphor, from some point in time when the
general train metaphor got established, through
Thatcher?s flashing out the issue of excessive
speed and unclear direction, Major?s multi-track
corrective, and reply of his opponents on the left.
The final frame has all those states of affairs that
fit the extended metaphor ? everyone is acting
within the same broad system of rules, with Britain
and perhaps others sometimes wanting to negoti-
ate special, more gradual procedures, which would
leave Britain less tightly integrated into the com-
6We note that in our model every utterance has an impact
on the public for which the player bears the consequences and
is therefore a (costly) strategic move in the game. This is dif-
ferent from models of cheap talk such as Aumann (1990),
Lewis (1969) where communication is devoid of strategic
moves and is used primarily as a coordination device.
701
munity than some other European partners.Integration is likea train journey?? that isunfolding too fast? but it is possible toregulate the speed? in which case we?ll goslower than others
Figure 2: Bargaining over train metaphor.
3.4 The equilibrium
A pair of strategies (?1, ?2) is a Nash equilibrium
if there is no deviation strategy ? such that (?, ?2)
leads to an outcome with higher utility for player 1
than outcome of (?1, ?2) and the same for player
2. A subgame are all the possible moves following
a history h={F0, . . . , Ft}, in our case it is equi-
valent to a game with an initial frame Ft and the
corresponding utilities. A sub-strategy is that part
of the original strategy that is a strategy on the
subgame. A pair of strategies is a subgame per-
fect equilibrium if, for any subgame, their sub-
strategies are a Nash equilibrium.
Theorem 1 In the frame bargaining game with
single-peaked preferences
1. There exists a canonical subgame perfect
equilibrium path F0, F1, F2, . . . such that
??t=0Ft={x}.
2. For any subgame perfect equilibrium path
F ?0, F
?
1, F
?
2, . . . there exists T such that
??t=0F
?
t=?
T
t=0Ft.
The theorem states that the outcome of the bar-
gaining will always be a frame on the canoni-
cal path. The rivals would suggest more specific
frames either until convergence or until a situation
where any further specification would produce a
frame that ?misses their point,? so-to-speak, by re-
moving too much of the favorable outcome space
for both players. Figure 3 shows a situation where
parties could decide to stall on the current frame:
If player 1 has to choose between retaining F0, or
playing F1 which would result in the rival?s play-
ing F2, player 1 might choose to remain in F0 if
the utility of any outcome of the subgame starting
from F2 is lower than that of F0, as long as player
1 believes that player 2 would reason similarly.
F0 
F2 
F1 
Player 1 Player 2 
??? ???
Figure 3: Stalled bargaining.
The idea of the proof is to construct a pair of
strategies where each side attempts to pull the pub-
licly accepted frame in the direction of its peak
utility point. We show, assuming the peak of the
first mover is to the left of peak of the second, that
any deviation of the first mover would enable the
second to shift the public frame more to the right,
to an outcome of lower utility to the first mover.
The full details of the proof of part 1 are given in
the appendix; part 2 is proved in an accompanying
technical report.
The equilibrium exhibits the following prop-
erties: (a) a first mover?s advantage ? for any
player, the outcome would be closer to her peak
point if she moves first than if she moves second;
(b) a centrist?s advantage ? if a player moves first
and her peak is closer to the middle of the initial
frame, she can derive a higher utility from the out-
come than if her peak were further from the mid-
dle. Please see appendix for justifications.
4 Discussion
4.1 Political communication
This article studies some properties of frame bar-
gaining through metaphor in political communi-
cation, where rival politicians choose how to ela-
borate the current metaphor to educate the pub-
lic about the ongoing situation in a way most con-
sistent with their political preferences. Modeling
the public preferences as highest relative entropy
subset of possible states-of-affairs, we show that
strategic choices by the politicians lead to a sub-
game perfect equilibrium where the less politically
extreme player who moves first is at an advantage.
In a democracy, such player would typically be
the government, as the bulk of voters do not by
definition vote for extreme views, and since the
government is the agent that brings about changes
in the current states of affairs, and is thus the first
and most prepared to explain them to the public.
Indeed, Entman?s model of frame activation in po-
litical discourse is hierarchical, with the govern-
702
ment (administration) being the topmost frame-
activator, and opposition and media elites typi-
cally reacting to the administration?s frame (Ent-
man, 2003).
4.2 Metaphor in political communication
The role of metaphor in communication has long
been a subject of interest, with views ranging from
an ornament that beautifies the argument in the
ancient rhetorical traditions, to the contemporary
views of conceptual metaphor as permeating every
aspect of life (Lakoff and Johnson, 1980).
In political communication specifically,
metaphor has long been known as a framing
device. Framing can be defined as ?selecting
and highlighting some facets of events or issues,
and making connections among them in order to
promote a particular interpretation, evaluation,
or solution? (Entman, 2003). Metaphors are
notorious for allowing subliminal framing, where
the metaphor seems so natural that the aspects
of the phenomenon in question that do not align
with the metaphor are seamlessly concealed.
For example, WAR AS A COMPETITIVE GAME
metaphor emphasizes the glory of winning and the
shame of defeat, but hides the death-and-suffering
aspect of the war, which makes sports metaphors
a strategic choice when wishing to arouse a
pro-war sentiment in the audience (Lakoff, 1991).
Such subliminal framing can often be effectively
contested by merely exposing the frame.
Our examples show a different use of metaphor.
Far from being subliminal or covert, the details of
the metaphor, its implications, and the evaluation
promoted by any given version are an important
tool in the public discussion of a complex politi-
cal issue. The function of metaphorical framing
here resembles a pedagogical one, where render-
ing an abstract theory in physics (such as electri-
city) in concrete commonsensical terms (such as
water flow) is an effective strategy to enhance the
students? understanding of the former (Gentner
and Gentner, 1983). The measure of success for a
given version of the frame is its ability to sway the
public in the evaluative direction envisioned by the
author by providing sufficient educational benefit,
so-to-speak, that is, convincingly rendering a good
portion of a complex reality in accessible terms.
Once a frame is found that provides extensive
education benefit, such as the EUROPEAN INTE-
GRATION AS TRAIN JOURNEY above, a politi-
cian?s attempt to debunk a metaphor as inappropri-
ate risk public antagonism, as this would be akin
to taking the benefit of enhanced understanding
away. Thus, rather than contesting the validity of
the metaphoric frame, politicians strive to find a
way to turn the metaphor around, i.e. accept the
general framework, but focus on a previously un-
explored aspect that would lead to a different eva-
luative tilt. Our results show that being the first
to use an effective metaphor that manages to lock
the public in its framework is a strategic advantage
as the need to communicate with the same public
would compel the rival to take up the metaphor
of your choice. To our knowledge, this is the first
explanation of the use of extended metaphor in po-
litical communication on a complex issue in terms
of the agendas of the rival parties and the chang-
ing disposition of the public being addressed. It
is an open question whether similar ?locking in?
of the public can be attained by non-metaphorical
means, and whether the ensuing dynamics would
be similar.
4.3 Social dynamics
This article contributes to the growing literature on
modeling social linguistic behavior, like debates
(Somasundaran and Wiebe, 2009), dating (Juraf-
sky et al, 2009; Ranganath et al, 2009), colla-
borative authoring and editing in wikis (Leuf and
Cunningham, 2001) such as Wikipedia (Vuong et
al., 2008; Kittur et al, 2007; Vie?gas et al, 2004).
The latter literature in particular sees the social ac-
tivity as an unfolding process, for example, detec-
ting the onset and resolution of a controversy over
the content of a Wikipedia article through track-
ing article talk7 and deletion-and-reversion pat-
terns. Somewhat similarly to the metaphor debate
discussed in this article, Vie?gas et al (2004) note
first-mover advantage in Wikipedia authoring, that
is, the first version gives the tone for the subse-
quent edits and has its parts survive for relatively
many editing cycles. Finding out how the ini-
tial contribution constrains and guides subsequent
edits of the content of aWikipedia article and what
kind of argumentative strategies are employed in
persuading others to retain one?s contribution is an
interesting direction for future research.
A number of recent studies of the linguistic as-
pects of social processes are construed as if the
7a page separate from the main article that is devoted to
the discussion of the edits
703
events are taking place all-at-once ? there is no
differentiation between early and later stages of a
debate in Somasundaran and Wiebe (2009) or ini-
tial and subsequent speed-dates for the same sub-
ject in Jurafsky et al (2009). Yet adopting a dy-
namic perspective stands to reason in such cases.
For example, Somasundaran and Wiebe (2009)
built a system for recognizing stance in an online
debate (such as pro-iPhone or pro-Blackberry on
http://www.covinceme.net). They noticed that the
task was complicated by concessions ? acknow-
ledgments of some virtues of the competitor be-
fore stating own preference. This is quite possi-
bly an instance of debate dynamics whereby as the
debate evolves certain common ground emerges
between the sides and the focus of the debate
changes from the initial stage of elucidating which
features are better in which product to a stage
where the ?facts? are settled and acknowledged by
both sides and the debate moves to evaluation of
the relative importance of those features.
As another example, consider the construction
of statistical models of various emotional and per-
sonality traits based on a corpus of speed dates
such as Jurafsky et al (2009). Take the trait of
intelligence. In their experiment with speed-dates,
Fisman et al (2006) found that males tend to dis-
prefer females they perceive as more intelligent or
ambitious than themselves. Consequently, an in-
telligent female might choose to act less intelligent
in later rounds of speed dating if she has not so far
met a sufficiently intelligent male, assuming she
prefers a less-intelligent male to no match at all.
Better sensitivity to the dynamics of social pro-
cesses underlying the observed linguistic commu-
nication will we believe result in increased inte-
rest in game-theoretic models, as these are espe-
cially well suited to handle cases where the sides
have certain goals and adapt their moves based on
the current situations, the other side?s move, and
possibly other considerations, such as the need to
address effectively a wider audience, beyond the
specific interlocutors. A game theoretic explana-
tion advances the understanding of the process be-
ing modeled, and hence of the applicability, and
the potential adaptation, of statistical models de-
veloped on a certain dataset to situations that dif-
fer somewhat from the original data: For exam-
ple, a corpus with more rounds of speed-dates
per participant might suddenly make females seem
smarter, or a debate with a longer history would
feature more, and perhaps more elaborate, conces-
sions.
5 Empirical challenges
We suggested that models of dynamics such as
the one presented in this article be built over data
where entities of interest are clearly identified.
This article is based on chapters 1 and 2 of the
book by Musolff (2000) which itself is informed
by a corpus-linguistic analysis of metaphor in me-
dia discourse in Britain and Germany. We now
discuss the state of affairs in empirical approaches
to detecting metaphors.
5.1 Metaphors in NLP
Metaphors received increasing attention from
computational linguistics community in the last
two decades. The tasks that have been ad-
dressed are explication of the reasoning behind
the metaphor (Barnden et al, 2002; Narayanan,
1999; Hobbs, 1992); detection of conventional
metaphors between two specific domains (Mason,
2004); classification of words, phrases or sen-
tences as metaphoric or non-metaphoric (Krishna-
kumaran and Zhu, 2007; Birke and Sarkar, 2006;
Gedigian et al, 2006; Fass, 1991).
We are not aware of research on automatic
methods specifically geared to recognition of ex-
tended metaphors. Indeed, most computational
work cited above concentrates on the detection of
a local incongruity due to a violation of selectional
restrictions when the verb or one of its arguments
is used metaphorically (as in Protesters derailed
the conference). Extended metaphors are expected
to be difficult for such approaches, since many of
the clauses are completely situated in the source
domain and hence no local incongruities exist (see
examples on the first page of this article).
5.2 Data collection
Supervised approaches to metaphor detection need
to rely on annotated data. While metaphors are
ubiquitous in language, an annotation project that
seeks to narrow the scope of relevant metaphors
down to metaphors from a particular source do-
main (such as train journeys) that describe a par-
ticular target domain (such as European integra-
tion) and are uttered by certain entities (such as
senior UK politicians) face the problem of spar-
sity of the relevant data in the larger discourse: A
random sample of the size amenable to human an-
704
notation is unlikely to capture in sufficient detail
material pertaining to the one metaphor of interest.
To increase the likelihood of finding mentions
of the source domain, a lexicon of words from
the source domain can be used to select docu-
ments (Hardie et al, 2007; Gedigian et al, 2006).
Another approach is metaphor ?harvesting? ?
hypothesizing that metaphors of interest would oc-
cur in close proximity to lexical items representing
the target domain of the metaphor, such as the 4
word window around the lemma Europe used in
Reining and Lo?nneker-Rodman (2007).
5.3 Data annotation
A further challenge is producing reliable anno-
tations. Pragglejaz (2007) propose a methodo-
logy for testing metaphoricity of a word in dis-
course and report ?=0.56-0.70 agreement for a
group of six highly expert annotators. Beigman
Klebanov et al (2008) report ?=0.66 for detec-
ting paragraphs containing metaphors from the
source domains LOVE and VEHICLE with mul-
tiple non-expert annotators, though other source
domains that often feature highly conventiona-
lized metaphors (like structure or foundation from
BUILDLING domain) or are more abstract and dif-
ficult to delimit (such as AUTHORITY) present a
more challenging annotation task.
5.4 Measuring metaphors
A fully empirical basis for the kind of model pre-
sented in this paper would also involve defining
a metric on metaphors that would allow measu-
ring the frame chosen by the given version of the
metaphor relatively to other such frames ? that is,
quantifying which part of the ?integration is a train
journey? metaphor is covered by those states of af-
fairs that also fit Thatcher?s critical rendition.
6 Conclusion
This article addressed a specific communicative
setting (rival politicians trying to ?sell? to the pub-
lic their versions of the unfolding realities and ne-
cessary policies) and a specific linguistic tool (an
extended metaphor), showing that the particular
use made of metaphor in such setting can be ratio-
nalized based on the characteristics of the setting.
Various questions now arise. Given the cen-
tral role played by the public gratification con-
straint in our model, would conversational situa-
tions without the need to persuade the public, such
as meetings of small groups of peers or phone con-
versations between friends, tend less to the use of
extended metaphor? Conversely, does the use of
extended metaphor in other settings testify to the
existence of presumed onlookers who need to be
?captured? in a particular version of reality ? as
in pedagogic or poetic context?
Considerations of the participants? agendas and
their impact on the ensuing dynamics of the ex-
change would we believe lead to further interest in
game theoretic models when addressing complex
social dynamics in situations like collaborative
authoring, debates, or dating, and will augment
the existing mostly statistical approaches with a
broader picture of the relevant communication.
A Proof of Existence of a Subgame
Perfect Equilibrium
For a segment [a, b] and a?v1<v2?b let
U1(x)=?(||x ? v1||) and U2(x)=?(||x ? v2||)
be utility functions with peaks v1 and v2, re-
spectively. For a history h={F0, . . . , Ft} where
Ft=[lt, rt], let ??1(h), player 1?s move, be de-
fined as choosing Ft+1=[lt+1, rt+1] such that
|Ft+1|=
|Ft|
2 , and rt+1 is as close as possible to
v1. ??2 sets lt+1 with respect to v2 in a symmet-
ric fashion. Since Ft shrinks by half every round,
limt?? lt=limt?? rt=x?, converging to a point.
We now show (??1, ?
?
2) is an equilibrium by show-
ing that neither player has a profitable deviation.
Notice that after the first round the subgame is
identical to the initial game with F1 replacing F0,
and the roles of players reversed. Player 2 had no
influence on the choice of F1, hence she has a pro-
fitable deviation iff she has a profitable deviation
on the continuation subgame where she is the first
mover. It thus suffices to show that the first mover
(player 1) has no profitable deviations to establish
that (??1, ?
?
2) is an equilibrium.
Since by definition ??2 always chooses an en-
tropy maximizing segment, for player 1 to choose
a non-entropy maximizing segment (more or less
than half the length) amounts to yielding the round
to player 2, which is equivalent in terms of the re-
sulting accepted frame to a situation where player
1 chooses an entropy maximizing segment ? the
same one chosen by player 2. Thus we need to
consider only deviations with entropy maximizing
frames.
Step 1: Suppose ??1 is a strategy of player 1 and
let F ?0, F
?
1, F
?
2, . . . be the sequence of frames on
705
the path corresponding to the pair (??1, ?
?
2). Let
t0 be the first move deviating from the equilibrium
path, namely Ft0 6=F
?
t0 . We first show that Ft0?1
could not be (a) completely to the left of v1 or (b)
completely to the right of v2. Suppose (a) holds.
Then by definition rt0?2=rt0?1<v1, and, induc-
tively, r0=rt0?1<v1; this contradicts r0=1 that fol-
lows from F0=[0, 1]. Possibility (b) is similarly
refuted. Therefore, the only two cases for Ft0?1
with respect to v1 are depicted in figure 4. Note
that this implies v1?x??v2.
??? ???
Case 2: 
Case 1: Ft0?1
Ft0?1
rt0
Figure 4: Two cases of current frame location.
Step 2: In case 1, ??1 will choose frames of type
[lt, v1] for any t?t0, and ??2 will do the same on
any history in the continuation game, hence the
outcome will eventually be v1. As this is player 1?s
peak utility point, she has no profitable deviation.
Step 3: In case 2, Ft0 is the leftmost entropy
maximizing subsegment of Ft0?1 and the devia-
tion F ?t0 can only be a shift to the right namely
r?t0?rt0 . If player 2 could choose [v2, rt0+1] given
rt0 , she can still choose the same frame given r
?
t0 ,
so the outcome would be v2 and F ?t0 was not pro-
fitable. If player 2 could not choose [v2, rt0+1]
given rt0 , implying that x
?<v2, but as a result of
the deviation can now choose [v2, r?t0+1], imply-
ing that the outcome would be v2, clearly player
1 has not benefited from the deviation since U1
is descending right of v1. If player 2 still cannot
choose [v2, r?t0+1] after the deviation, she would
choose the rightmost entropy maximizing segment
with l?t0+1?lt0+1. If this still allows player 1 to
do [l?t0+2, v1] and hence to lead to v1 as the out-
come, it was possible in [lt0+2, v1] as well, so no
profit is gained by having deviated. Otherwise,
r?t0+2?rt0+2.
Step 3 can be repeated ad infinitum to show
that r?t?rt unless for some history h the de-
viation enables ?2(h)=[v2, r?t]. In the former
case we get limt?? r?t=x
??x?=limt?? rt where
??t=1F
?
t={x
?}. Since r?t and rt are to the right
of v1 and U1 is descending right of v1 it fol-
lows that U1(x?)?U1(x?). In the latter case
x??v2. Since Ft is never strictly to the right of v2,
x?=limt?? lt?v2?x?, therefore U1(x?)?U1(x?).
In either case the deviation ??1 cannot result in a
better outcome for player 1. This finishes the proof
that (??1, ?
?
2) is a Nash equilibrium.
Notice that (??1, ?
?
2) prescribe sub-strategies on
any subgame that are themselves Nash equilibria
for the subgames, hence (??1, ?
?
2) is a subgame per-
fect equilibrium 2
First Mover?s Advantage: The proof of step
3 shows that having the left boundary of the cur-
rent frame further to the right cannot yield a bet-
ter outcome for player 1. Yet, if player 1?s first
turn comes after that of player 2, she will start
with a current frame with the left boundary further
to the right than the initial frame before player 2
moved, since moving the left boundary is player
2?s equilibrium strategy. Hence a player would
never achieve a better outcome starting second if
both players are playing the canonical strategy.
Centrist?s Advantage: Let M be the middle of
F0. Consider a more extreme version of player 1
? player 1#. Suppose w.l.g. v#1 <v1?M . In case
v#1 <v1<v2, for all utilities u of the outcome of
dynamics vs player 2, if player 1# could attain u,
player 1 could attain u or more; the reverse is not
true, for example when |v#1 ? lt|<
|Ft|
2 ?|v1 ? lt|
and player 1 (or 1#) is moving first. In case
v2<v
#
1 <v1, if player 1 (or 1
#) moves first, she
is able to force her peak point as the outcome. If
v#1 <v2<v1, player 1 can force v1 as the outcome,
whereas player 1# would not necessarily be able
to force v#1 , as player 2 would pull the outcome
towards v2. Hence a first moving centrist is never
worse off, and often better off, than a first moving
extremist.
References
James Allan, editor. 2002. Topic Detection and Track-
ing: Event-Based Information Organization. Nor-
well, MA:Kluwer Academic Publishers.
Robert Aumann. 1990. Nash Equilibria are not Self-
Enforcing. In Jean J. Gabszewicz, Jean-Francois
Richard, and Laurence A. Wolsey, editors, Eco-
nomic Decision-Making: Games, Econometrics and
Optimisation, pages 201?206. Amsterdam: Elsevier.
Robert Axelrod and William D. Hamilton. 1981. The
evolution of cooperation. Science, 211(4489):1390?
1396.
John A. Barnden, Sheila R. Glasbey, Mark G. Lee, and
Alan M. Wallington. 2002. Reasoning in metaphor
706
understanding: The ATT-Meta approach and sys-
tem. In Proceedings of COLING, pages 121?128.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing disagreements. In
Ron Artstein, Gemma Boleda, Frank Keller, and
Sabine Schulte im Walde, editors, Proceedings of
COLING Workshop on Human Judgments in Com-
putational Linguistics, pages 2?7, Manchester, UK,
August. International Committee on Computational
Linguistics.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of non-
literal language. In Proceedings of EACL, pages
329?336.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Resarch, 3:993?1022.
Dwight Bolinger. 1980. Language ? The Loaded
Weapon. London: Longman.
Robin Clark and Prashant Parikh. 2007. Game Theory
and Discourse Anaphora. Journal of Logic, Lan-
guage and Information, 16:265?282.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18:233?263.
Paul Dekker and Robert van Rooy. 2000. Bi-
directional optimality theory: An application of
game theory. Journal of Semantics, 17(3):217?242.
Anthony Downs. 1957. An economic theory of politi-
cal action in a democracy. The Journal of Political
Economy, 65(2):135?150.
James Druckman and Arthur Luria. 2000. Preference
formation. Annual Review of Political Science, 2:1?
24.
Robert M. Entman. 2003. Cascading activation: Con-
testing the White House?s frame after 9/11. Political
Communication, 20:415?432.
Dan Fass. 1991. Met*: a method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49?90.
Raymond Fisman, Sheena Iyengar, Emir Kamenica,
and Itamar Simonson. 2006. Gender Differences
in Mate Selection: Evidence from a Speed Dat-
ing Experiment. Quarterly Journal of Economics,
121(2):673?697.
Yoav Freund and Robert E. Schapire. 1996. Game
theory, on-line prediction, and boosting. In Pro-
ceedings of the annual conference on Computational
Learning Theory, pages 325?332, Desenzano del
Garda, Italy, June -July.
Claire Gardent, Hlne Manue?lian, Kristina Striegnitz,
and Marilisa Amoia. 2004. Generating Definite De-
scriptions: Non-Incrementality, Inference and Data.
In Thomas Pechmann and Christopher Habel, ed-
itors, Multidisciplinary Approaches to Language
Production. Mouton de Gruyter.
Matt Gedigian, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of NAACL Workshop on Scalable Natural
Language Understanding, pages 41?48.
Deidre Gentner and Donald Gentner. 1983. Flowing
waters or teeming crowds: Mental models of electri-
city. In D. Gentner and A. Stevens, editors, Mental
models. Hillsdale, NJ: Lawrence Erlbaum.
Jacob Glazer and Ariel Rubinstein. 2001. Debates and
decisions: On a rationale of argumentation rules.
Games and Economic Behavior, 36(2):158?173.
Stephan Greene and Philip Resnik. 2009. More than
Words: Syntactic Packaging and Implicit Sentiment.
In Proceedings of NAACL, pages 503?511, Boulder,
CO, June.
Avner Greif. 2006. Institutions and the path to the
modern economy: Lessons from medieval trade.
Cambridge University Press.
Daniel Gruhl, R. Guha, David Liben-Nowell, and An-
drew Tomkins. 2004. Information diffusion through
blogspace. In Proceedings of the 13th international
conference on World Wide Web, pages 491?501.
Andrew Hardie, Veronika Koller, Paul Rayson, and
Elena Semino. 2007. Exploiting a semantic anno-
tation tool for metaphor analysis. In Proceedings
of the Corpus Linguistics Conference, Birmingham,
UK, Julyt.
Jerry Hobbs. 1992. Metaphor and abduction. In An-
drew Ortony, Jon Slack, and Oliviero Stock, editors,
Communication from an Artificial Intelligence Per-
spective: Theoretical and Applied Issues, pages 35?
58. Springer Verlag.
Gerhard Ja?ger and Christian Ebert. 2008. Prag-
matic Rationalizability. In Proceedings of the 13th
annual meeting of Gesellschaft fur Semantik, Sinn
und Bedeutung, pages 1?15, Stuttgart, Germany,
September-October.
Dan Jurafsky, Rajesh Ranganath, and Dan McFarland.
2009. Extracting social meaning: Identifying inter-
actional style in spoken conversation. In Proceed-
ings of NAACL, pages 638?646, Boulder, CO, June.
Aniket Kittur, Bongwon Suh, Bryan A. Pendleton, and
Ed H. Chi. 2007. He says, she says: Conflict and co-
ordination in Wikipedia. In CHI-07: Proceedings of
the SIGCHI conference on Human Factors in Com-
puting Systems, pages 453?462, San Jose, CA, USA.
707
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of NAACL Workshop on Computa-
tional Approaches to Figurative Language, pages
13?20.
George Lakoff and Mark Johnson. 1980. Metaphors
We Live By. Chicago University Press.
George Lakoff. 1991. Metaphor and war: The
metaphor system used to justify war in the Gulf.
Peace Research, 23:25?32.
Carlton E. Lemke and Joseph T. Howson. 1964.
Equilibrium Points of Bimatrix Games. Journal of
the Society for Industrial and Applied Mathematics,
12(2):413?423.
Bo Leuf and Ward Cunningham. 2001. The Wiki way:
quick collaboration on the Web. Boston: Addison-
Wesley.
David Lewis. 1969. Convention. Cambridge, MA:
Harvard University Press.
Robert D. Luce and Howard Raiffa. 1957. Games and
decisions. New York: John Wiley and Sons.
AndreuMas-Colell, Michael D.Whinston, and Jerry R.
Green. 1995. Microeconomic theory. Oxford Uni-
versity Press.
Zachary J. Mason. 2004. CorMet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
John Maynard Smith and George R. Price. 1973. The
logic of animal conflict. Nature, 246(5427):15?18.
Nelson Morgan, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Adam Janin, Thilo Pfau, Elizabeth
Shriberg, and Andreas Stolcke. 2001. The Meeting
Project at ICSI. In Proceedings of the HLT, pages
246?252, San Diego, CA.
Andreas Musolff. 2000. Mirror images of Europe:
Metaphors in the public debate about Europe in
Britain and Germany. Mu?nchen: Iudicium.
Srini Narayanan. 1999. Moving right along: A
computational model of metaphoric reasoning about
events. In Proceedings of AAAI, pages 121?128.
John F. Nash. 1950. Equilibrium points in n-person
games. Proceedings of the National Academy of Sci-
ences, 36(1):48?49.
Douglass C. North. 1990. Institutions, institutional
change, and economic performance. Cambridge
University Press.
Ivandr Paraboni, Kees van Deemter, and Judith Mas-
thoff. 2007. Generating Referring Expressions:
Making Referents Easy to Identify. Computational
Lingusitics, 33(2):229?254.
Prashant Parikh. 2001. The Use of Language. Stan-
ford: CSLI Publications.
Keith T. Poole and Howard Rosenthal. 1997.
Congress: A Political-Economic History of Roll Call
Voting. Oxford University Press.
Group Pragglejaz. 2007. MIP: A Method for Iden-
tifying Metaphorically Used Words in Discourse.
Metaphor and Symbol, 22(1):1?39.
Kevin M. Quinn, Burt L. Monroe, Michael Colaresi,
Michael H. Crespin, and Dragomir R. Radev. 2006.
An automated method of topic-coding legislative
speech over time with application to the 105th-108th
U.S. Senate. Unpublished Manuscript.
Rajesh Ranganath, Dan Jurafsky, and Dan McFarland.
2009. It?s not you, it?s me: Detecting flirting and
its misperception in speed-dates. In Proceedings of
EMNLP, pages 334?342, Singapore, August.
Astrid Reining and Birte Lo?nneker-Rodman. 2007.
Corpus-driven metaphor harvesting. In Proceedings
of the Workshop on Computational Approaches to
Figurative Language, pages 5?12, Rochester, New
York.
Ian Ross. 2007. Situations and Solution Concepts
in Game-Theoretic Approaches to Pragmatics. In
Ahti-Veikko Pietarinen, editor, Game Theory and
Linguistic Meaning, pages 135?147. Oxford, UK:
Elsevier Ltd.
Ariel Rubinstein. 1982. Perfect equilibrium in a bar-
gaining model. Econometrica, 50(1):97?109.
Thomas C. Schelling. 1997. The strategy of conflict.
Harvard University Press.
Reinhard Selten. 1965. Spieltheoretische behand-
lung eines oligopolmodells mit nachfragetra?gheit.
Zeitschrift fu?r die Gesamte Staatswissenschaft,
12:301?324.
Reinhard Selten. 1975. Re-examination of the Per-
fectness Concept for Equilibrium Points in Exten-
sive Form Games. International Journal of Game
Theory, 4:25?55.
Shai Shalev-Shwartz and Yoram Singer. 2006. Convex
Repeated Games and Fenchel Duality. In Proceed-
ings of NIPS, pages 1265?1272.
Advaith Siddharthan and Ann Copestake. 2004. Gen-
erating referring expressions in open domains. In
Proceedings of the ACL, pages 407?414, Barcelona,
Spain, July.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing Stances in Online Debates. In Proceedings
of the ACL, pages 226?234.
Robert van Rooij and Katrin Schulz. 2004. Exhaustive
Interpretation of Complex Sentences. Journal of
Logic, Language and Information, 13(4):491?519.
708
Robert van Rooij. 2008. Games and Quantity
implicatures. Journal of Economic Methodology,
15(3):261?274.
Fernanda B. Vie?gas, Martin Wattenberg, and Kushal
Dave. 2004. Studying cooperation and conflict be-
tween authors with history flow visualizations. In
CHI-04: Proceedings of the SIGCHI conference on
Human Factors in Computing Systems, pages 575?
582, Vienna, Austria.
John von Neumann and Oskar Morgenstern. 1944.
Theory of games and economic behavior. Princeton
University Press.
Bernhard von Stengel. 2007. Equilibrium computa-
tion for two-player games in strategic and extensive
form. In Noam Nisan, Tim Roughgarden, Eva Tar-
dos, and Vijay Vazirani, editors, Algorithmic Game
Theory, pages 53?78. Cambridge University Press.
Ba-Quy Vuong, Ee-Peng Lim, Aixin Sun, Minh-Tam
Le, and Hady Wirawan Lauw. 2008. On ranking
controversies in Wikipedia: models and evaluation.
In Proceedings of the international conference on
Web Search and Web Data Mining, pages 171?182,
Palo Alto, CA, USA.
709
Proceedings of the ACL 2010 Conference Short Papers, pages 253?257,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Vocabulary Choice as an Indicator of Perspective
Beata Beigman Klebanov, Eyal Beigman, Daniel Diermeier
Northwestern University and Washington University in St. Louis
beata,d-diermeier@northwestern.edu, beigman@wustl.edu
Abstract
We establish the following characteris-
tics of the task of perspective classifi-
cation: (a) using term frequencies in a
document does not improve classification
achieved with absence/presence features;
(b) for datasets allowing the relevant com-
parisons, a small number of top features is
found to be as effective as the full feature
set and indispensable for the best achieved
performance, testifying to the existence
of perspective-specific keywords. We re-
late our findings to research on word fre-
quency distributions and to discourse ana-
lytic studies of perspective.
1 Introduction
We address the task of perspective classification.
Apart from the spatial sense not considered here,
perspective can refer to an agent?s role (doctor vs
patient in a dialogue), or understood as ?a par-
ticular way of thinking about something, espe-
cially one that is influenced by one?s beliefs or
experiences,? stressing the manifestation of one?s
broader perspective in some specific issue, or ?the
state of one?s ideas, the facts known to one, etc.,
in having a meaningful interrelationship,? stress-
ing the meaningful connectedness of one?s stances
and pronouncements on possibly different issues.1
Accordingly, one can talk about, say, opinion
on a particular proposed legislation on abortion
within pro-choice or pro-life perspectives; in this
case, perspective essentially boils down to opi-
nion in a particular debate. Holding the issue con-
stant but relaxing the requirement of a debate on a
specific document, we can consider writings from
pro- and con- perspective, in, for example, the
death penalty controversy over a course of a period
of time. Relaxing the issue specificity somewhat,
1Google English Dictionary, Dictionary.com
one can talk about perspectives of people on two
sides of a conflict; this is not opposition or sup-
port for any particular proposal, but ideas about
a highly related cluster of issues, such as Israeli
and Palestinian perspectives on the conflict in all
its manifestations. Zooming out even further, one
can talk about perspectives due to certain life con-
tingencies, such as being born and raised in a par-
ticular culture, region, religion, or political tradi-
tion, such perspectives manifesting themselves in
certain patterns of discourse on a wide variety of
issues, for example, views on political issues in the
Middle East from Arab vs Western observers.
In this article, we consider perspective at all
the four levels of abstraction. We apply the same
types of models to all, in order to discover any
common properties of perspective classification.
We contrast it with text categorization and with
opinion classification by employing models rou-
tinely used for such tasks. Specifically, we con-
sider models that use term frequencies as features
(usually found to be superior for text categoriza-
tion) and models that use term absence/presence
(usually found to be superior for opinion classi-
fication). We motivate our hypothesis that pre-
sence/absence features would be as good as or
better than frequencies, and test it experimentally.
Secondly, we investigate the question of feature
redundancy often observed in text categorization.
2 Vocabulary Selection
A line of inquiry going back at least to Zipf strives
to characterize word frequency distributions in
texts and corpora; see Baayen (2001) for a sur-
vey. One of the findings in this literature is that
a multinomial (called ?urn model? by Baayen)
is not a good model for word frequency distri-
butions. Among the many proposed remedies
(Baayen, 2001; Jansche, 2003; Baroni and Evert,
2007; Bhat and Sproat, 2009), we would like to
draw attention to the following insight articulated
253
most clearly in Jansche (2003). Estimation is im-
proved if texts are construed as being generated by
two processes, one choosing which words would
appear at all in the text, and then, for words that
have been chosen to appear, how many times they
would in fact appear. Jansche (2003) describes a
two-stage generation process: (1) Toss a z-biased
coin; if it comes up heads, generate 0; if it comes
up tails, (2) generate according to F (?), where
F (?) is a negative binomial distribution and z is a
parameter controlling the extent of zero-inflation.
The postulation of two separate processes is
effective for predicting word frequencies, but is
there any meaning to the two processes? The first
process of deciding on the vocabulary, or word
types, for the text ? what is its function? Jansche
(2003) suggests that the zero-inflation component
takes care of the multitude of vocabulary words
that are not ?on topic? for the given text, including
taboo words, technical jargon, proper names. This
implies that words that are chosen to appear are
all ?on topic?. Indeed, text segmentation studies
show that tracing recurrence of words in a text
permits topical segmentation (Hearst, 1997; Hoey,
1991). Yet, if a person compares abortion to infan-
ticide ? are we content with describing this word
as being merely ?on topic,? that is, having a certain
probability of occurrence once the topic of abor-
tion comes up? In fact, it is only likely to occur
if the speaker holds a pro-life perspective, while a
pro-choicer would avoid this term.
We therefore hypothesize that the choice of vo-
cabulary is not only a matter of topic but also
of perspective, while word recurrence has mainly
to do with the topical composition of the text.
Therefore, tracing word frequencies is not going to
be effective for perspective classification beyond
noting the mere presence/absence of words, dif-
ferently from the findings in text categorization,
where frequency-based features usually do better
than boolean features for sufficiently large voca-
bulary sizes (McCallum and Nigam, 1998).
3 Data
Partial Birth Abortion (PBA) debates: We use
transcripts of the debates on Partial Birth Abor-
tion Ban Act on the floors of the US House and
Senate in 104-108 Congresses (1995-2003). Simi-
lar legislation was proposed multiple times, passed
the legislatures, and, after having initially been ve-
toed by President Clinton, was signed into law
by President Bush in 2003. We use data from
278 legislators, with 669 speeches in all. We
take only one speech per speaker per year; since
many serve multiple years, each speaker is repre-
sented with 1 to 5 speeches. We perform 10-fold
cross-validation splitting by speakers, so that all
speeches by the same speaker are assigned to the
same fold and testing is always inter-speaker.
When deriving the label for perspective, it is im-
portant to differentiate between a particular leg-
islation and a pro-choice / pro-life perspective.
A pro-choice person might still support the bill:
?I am pro-choice, but believe late-term abortions
are wrong. Abortion is a very personal decision
and a woman?s right to choose whether to ter-
minate a pregnancy subject to the restrictions of
Roe v. Wade must be protected. In my judgment,
however, the use of this particular procedure can-
not be justified.? (Rep. Shays, R-CT, 2003). To
avoid inconsistency between vote and perspective,
we use data from pro-choice and pro-life non-
governmental organizations, NARAL and NRLC,
that track legislators? votes on abortion-related
bills, showing the percentage of times a legislator
supported the side the organization deems consis-
tent with its perspective. We removed 22 legisla-
tors with a mixed record, that is, those who gave
20-60% support to one of the positions.2
Death Penalty (DP) blogs: We use University
of Maryland Death Penalty Corpus (Greene and
Resnik, 2009) of 1085 texts from a number of pro-
and anti-death penalty websites. We report 4-fold
cross-validation (DP-4) using the folds in Greene
and Resnik (2009), where training and testing data
come from different websites for each of the sides,
as well as 10-fold cross-validation performance on
the entire corpus, irrespective of the site.3
Bitter Lemons (BL): We use the GUEST part
of the BitterLemons corpus (Lin et al, 2006), con-
taining 296 articles published in 2001-2005 on
http://www.bitterlemons.org by more than 200 dif-
ferent Israeli and Palestinian writers on issues re-
lated to the conflict.
Bitter Lemons International (BL-I): We col-
lected 150 documents each by a different per-
2Ratings are from: http://www.OnTheIssues.org/. We fur-
ther excluded data from Rep. James Moran, D-VA, as he
changed his vote over the years. For legislators rated by nei-
ther NRLC nor NARAL, we assumed the vote aligns with the
perspective.
3The 10-fold setting yields almost perfect performance
likely due to site-specific features beyond perspective per se,
hence we do not use this setting in subsequent experiments.
254
son from either Arab or Western perspectives
on Middle Eastern affairs in 2003-2009 from
http://www.bitterlemons-international.org/. The
writers and interviewees on this site are usually
former diplomats or government officials, aca-
demics, journalists, media and political analysts.4
The specific issues cover a broad spectrum, includ-
ing public life, politics, wars and conflicts, educa-
tion, trade relations in and between countries like
Lebanon, Jordan, Iraq, Egypt, Yemen, Morocco,
Saudi Arabia, as well as their relations with the
US and members of the European Union.
3.1 Pre-processing
We are interested in perspective manifestations
using common English vocabulary. To avoid the
possibility that artifacts such as names of senators
or states drive the classification, we use as features
words that contain only lowercase letters, possibly
hyphenated. No stemming is performed, and no
stopwords are excluded.5
Table 1: Summary of corpora
Data #Docs #Features # CV folds
PBA 669 9.8 K 10
BL 296 10 K 10
BL-I 150 9 K 10
DP 1085 25 K 4
4 Models
For generative models, we use two versions
of Naive Bayes models termed multi-variate
Bernoulli (here, NB-BOOL) and multinomial (here,
NB-COUNT), respectively, in McCallum and
Nigam (1998) study of event models for text cate-
gorization. The first records presence/absence of a
word in a text, while the second records the num-
ber of occurrences. McCallum and Nigam (1998)
found NB-COUNT to do better than NB-BOOL for
sufficiently large vocabulary sizes for text catego-
rization by topic. For discriminative models, we
use linear SVM, with presence-absence, norma-
lized frequency, and tfidf feature weighting. Both
types of models are commonly used for text clas-
sification tasks. For example, Lin et al (2006) use
4We excluded Israeli, Turkish, Iranian, Pakistani writers
as not clearly representing either perspective.
5We additionally removed words containing support, op-
pos, sustain, overrid from the PBA data, in order not to in-
flate the performance on perspective classification due to the
explicit reference to the upcoming vote.
NB-COUNT and SVM-NORMF for perspective clas-
sification; Pang et al (2002) consider most and
Yu et al (2008) all of the above for related tasks
of movie review and political party classification.
We use SVMlight (Joachims, 1999) for SVM and
WEKA toolkit (Witten and Frank, 2005; Hall et
al., 2009) for both version of Naive Bayes. Param-
eter optimization for all SVMmodels is performed
using grid search on the training data separately
for each partition into train and test data.6
5 Results
Table 2 summarizes the cross-validation results for
the four datasets discussed above. Notably, the
SVM-BOOL model is either the best or not signif-
icantly different from the best performing model,
although the competitors use more detailed textual
information, namely, the count of each word?s ap-
pearance in the text, either raw (NB-COUNT), nor-
malized (SVM-NORMF), or combined with docu-
ment frequency (SVM-TFIDF).
Table 2: Classification accuracy. Scores sig-
nificantly different from the best performance
(p2t<0.05 on paired t-test) are given an asterisk.
Data NB SVM
BOOL COUNT BOOL NORMF TFIDF
PBA *0.93 0.96 0.96 0.96 0.97
DP-4 0.82 0.82 0.83 0.82 0.727
DP-10 *0.88 *0.93 0.98 *0.97 *0.97
BL 0.89 0.88 0.89 0.86 0.84
BL-I 0.68 0.66 0.73 0.65 0.65
We conclude that there is no evidence for the
relevance of the frequency composition of the
text for perspective classification, for all levels of
venue- and topic-control, from the tightest (PBA
debates) to the loosest (Western vs Arab authors
on Middle Eastern affairs). This result is a clear
indication that perspective classification is quite
different from text categorization by topic, where
count-based features usually perform better than
boolean features. On the other hand, we have not
6Parameter c controlling the trade-off between errors
on training data and margin is optimized for all datasets,
with the grid c = {10?6, 10?5, . . . , 105}. On the DP
data parameter j controlling penalties for misclassification
of positive and negative cases is optimized as well (j =
{10?2, 10?1, . . . , 102}), since datasets are unbalanced (for
example, there is a fold with 27%-73% split).
7Here SVM-TFIDF is doing somewhat better than SVM-
BOOL on one of the folds and much worse on two other folds;
paired t-test with just 4 pairs of observations does not detect
a significant difference.
255
observed that boolean features are reliably better
than count-based features, as reported for the sen-
timent classification task in the movie review do-
main (Pang et al, 2002).
We note the low performance on BL-I, which
could testify to a low degree of lexical consolida-
tion in the Arab vs Western perspectives (more on
this below). It is also possible that the small size of
BL-I leads to overfitting and low accuracies. How-
ever, PBA subset with only 151 items (only 2002
and 2003 speeches) is still 96% classifiable, so size
alone does not explain low BL-I performance.
6 Consolidation of perspective
We explore feature redundancy in perspective
classification.We first investigate retention of only
N best features, then elimination thereof. As a
proxy of feature quality, we use the weight as-
signed to the feature by the SVM-BOOL model
based on the training data. Thus, to get the per-
formance with N best features, we take the N2
highest and lowest weight features, for the posi-
tive and negative classes, respectively, and retrain
SVM-BOOL with these features only.8
Table 3: Consolidation of perspective. Nbest
shows the smallest N and its proportion out of
all features for which the performance of SVM-
BOOL with only the best N features is not sig-
nificantly inferior (p1t>0.1) to that of the full
feature set. No-Nbest shows the largest num-
ber N for which a model without N best fea-
tures is not significantly inferior to the full model.
N={50, 100, 150, . . . , 1000}; for DP and BL-I, ad-
ditionally N={1050, 1100, ..., 1500}; for PBA, ad-
ditionally N={10, 20, 30, 40}.
Data Nbest No-Nbest
N % N %
PBA 250 2.6% 10 <1%
BL 500 4.9% 100 <1%
DP 100 <1% 1250 5.2%
BL-I 200 2.2% 950 11%
We observe that it is generally sufficient to use
a small percentage of the available words to ob-
tain the same classification accuracy as with the
full feature set, even in high-accuracy cases such
as PBA and BL. The effectiveness of a small
subset of features is consistent with the observa-
tion in the discourse analysis studies that rivals
8We experimented with the mutual information based fea-
ture selection as well, with generally worse results.
in long-lasting controversies tend to consolidate
their vocabulary and signal their perspective with
certain stigma words and banner words, that is,
specific keywords used by a discourse commu-
nity to implicate adversaries and to create sym-
pathy with own perspective, respectively (Teubert,
2001). Thus, in abortion debates, using infanti-
cide as a synonym for abortion is a pro-life stigma.
Note that this does not mean the rest of the fea-
tures are not informative for classification, only
that they are redundant with respect to a small per-
centage of top weight features.
When N best features are eliminated, perfor-
mance goes down significantly with even smaller
N for PBA and BL datasets. Thus, top features
are not only effective, they are also crucial for ac-
curate classification, as their discrimination capa-
city is not replicated by any of the other vocabu-
lary words. This finding is consistent with Lin
and Hauptmann (2006) study of perspective vs
topic classification: While topical differences be-
tween two corpora are manifested in difference in
distributions of great many words, they observed
little perspective-based variation in distributions
of most words, apart from certain words that are
preferentially used by adherents of one or the other
perspective on the given topic.
For DP and BL-I datasets, the results seem
to suggest perspectives with more diffused key-
word distribution (No-NBest figures are higher).
We note, however, that feature redundancy exper-
iments are confounded in these cases by either a
low power of the paired t-test with only 4 pairs
(DP) or by a high variance in performance among
the 10 folds (BL-I), both of which lead to nume-
rically large discrepancy in performance that is not
deemed significant, making it easy to ?match? the
full set performance with small-N best features as
well as without large-N best features. Better com-
parisons are needed in order to verify the hypo-
thesis of low consolidation.
In future work, we plan to experiment with ad-
ditional features. For example, Greene and Resnik
(2009) reported higher classification accuracies
for the DP-4 data using syntactic frames in which
a selected group of words appeared, rather than
mere presence/absence of the words. Another di-
rection is exploring words as members of seman-
tic fields ? while word use might be insufficiently
consistent within a perspective, selection of a se-
mantic domain might show better consistency.
256
References
Herald Baayen. 2001. Word frequency distributions.
Dordrecht: Kluwer.
Marco Baroni and Stefan Evert. 2007. Words
and Echoes: Assessing and Mitigating the Non-
Randomness Problem in Word Frequency Distribu-
tion Modeling. In Proceedings of the ACL, pages
904?911, Prague, Czech Republic.
Suma Bhat and Richard Sproat. 2009. Knowing the
Unseen: Estimating Vocabulary Size over Unseen
Samples. In Proceedings of the ACL, pages 109?
117, Suntec, Singapore, August.
Stephan Greene and Philip Resnik. 2009. More
than Words: Syntactic Packaging and Implicit Sen-
timent. In Proceedings of HLT-NAACL, pages 503?
511, Boulder, CO, June.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringe, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1).
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64.
Michael Hoey. 1991. Patterns of Lexis in Text. Oxford
University Press.
Martin Jansche. 2003. Parametric Models of Linguis-
tic Count Data. In Proceedings of the ACL, pages
288?295, Sapporo, Japan, July.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press.
Wei-Hao Lin and Alexander Hauptmann. 2006. Are
these documents written from different perspec-
tives? A test of different perspectives based on sta-
tistical distribution divergence. In Proceedings of
the ACL, pages 1057?1064, Morristown, NJ, USA.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? Identifying perspectives at the document and
sentence levels. In Proceedings of CoNLL, pages
109?116, Morristown, NJ, USA.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text clas-
sification. In Proceedings of AAAI-98 Workshop
on Learning for Text Categorization, pages 41?48,
Madison, WI, July.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In Proceedings of
EMNLP, Philadelphia, PA, July.
Wolfgang Teubert. 2001. A Province of a Federal
Superstate, Ruled by an Unelected Bureaucracy ?
Keywords of the Euro-Sceptic Discourse in Britain.
In Andreas Musolff, Colin Good, Petra Points, and
Ruth Wittlinger, editors, Attitudes towards Europe:
Language in the unification process, pages 45?86.
Ashgate Publishing Ltd, Hants, England.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, 2 edition.
Bei Yu, Stefan Kaufmann, and Daniel Diermeier.
2008. Classifying party affiliation from political
speech. Journal of Information Technology and Pol-
itics, 5(1):33?48.
257
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1148?1158,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Word Association Profiles and their Use for Automated Scoring of Essays
Beata Beigman Klebanov and Michael Flor
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541
{bbeigmanklebanov,mflor}@ets.org
Abstract
We describe a new representation of the
content vocabulary of a text we call word
association profile that captures the pro-
portions of highly associated, mildly asso-
ciated, unassociated, and dis-associated
pairs of words that co-exist in the given
text. We illustrate the shape of the dis-
tirbution and observe variation with genre
and target audience. We present a study
of the relationship between quality of writ-
ing and word association profiles. For a
set of essays written by college graduates
on a number of general topics, we show
that the higher scoring essays tend to have
higher percentages of both highly asso-
ciated and dis-associated pairs, and lower
percentages of mildly associated pairs of
words. Finally, we use word association
profiles to improve a system for automated
scoring of essays.
1 Introduction
The vast majority of contemporary research that
investigates statistical properties of language deals
with characterizing words by extracting infor-
mation about their behavior from large corpora.
Thus, co-occurrence of words in n-word windows,
syntactic structures, sentences, paragraphs, and
even whole documents is captured in vector-space
models built from text corpora (Turney and Pan-
tel, 2010; Basili and Pennacchiotti, 2010; Erk and
Pado?, 2008; Mitchell and Lapata, 2008; Bullinaria
and Levy, 2007; Jones and Mewhort, 2007; Pado
and Lapata, 2007; Lin, 1998; Landauer and Du-
mais, 1997; Lund and Burgess, 1996; Salton et al,
1975). However, little is known about typical pro-
files of texts in terms of co-occurrence behavior
of their words. Some information can be inferred
from the success of statistical techniques in pre-
dicting certain structures in text. For example, the
fact that a text segmentation algorithm that uses
information about patterns of word co-occurrences
can detect sub-topic shifts in a text (Riedl and Bie-
mann, 2012; Misra et al, 2009; Eisenstein and
Barzilay, 2008) tells us that texts contain some
proportion of more highly associated word pairs
(those in subsequent sentences within the same
topical unit) and of less highly associated pairs
(those in sentences from different topical units).1
Yet, does each text have a different distribution
of highly associated, mildly associated, unassoci-
ated, and dis-associated pairs of words, or do texts
tend to strike a similar balance of these? What
are the proportions of the different levels of asso-
ciation, how much variation there exists, and are
there systematic differences between various kinds
of texts? We present research that makes a first
step in addressing these questions.
From the applied perspective, our interest is in
quantifying differences between well-written and
poorly written essays, for the purposes of auto-
mated scoring of essays. We therefore concentrate
on essay data for the main experiments reported in
this paper, although some additional corpora will
be used for illustration purposes.
The paper is organized as follows. Section 2
presents our methodology for building word as-
sociation profiles for texts. Section 3 illustrates
the profiles for three corpora from different gen-
res. Section 4.2 presents our study of the relation-
ship between writing quality and patterns of word
associations, with section 4.5 showing the results
of adding a feature based on word association pro-
file to a state-of-art essay scoring system. Related
work is reviewed is section 5.
1Note that the classical approach to topical segmentation
of texts, TextTiling (Hearst, 1997), uses only word repeti-
tions. The cited approaches use topic models that are in turn
estimated using word co-occurrence.
1148
2 Methodology
In order to describe the word association profile
of a text, three decisions need to be made. The
first decision is how to quantify the extent of co-
occurrence between two words; we will use point-
wise mutual information (PMI) estimated from a
large and diverse corpus of texts. The second is
which pairs of words in a text to consider when
building a profile for the text; we opted for all pairs
of content word types occurring in a text, irrespec-
tive of the distance between them. We consider
word types, not tokens; no lemmatization is per-
formed. The third decision is how to represent the
co-occurrence profiles; we use a histogram where
each bin represents the proportion of word pairs in
the given interval of PMI values. The rest of the
section gives more detail about these decisions.
To obtain comprehensive information about
typical co-occurrence behavior of words of
English, we build a first-order co-occurrence
word-space model (Turney and Pantel, 2010; Ba-
roni and Lenci, 2010). The model was generated
from a corpus of texts of about 2.5 billion words,
counting co-occurrence in a paragraph,2 using no
distance coefficients (Bullinaria and Levy, 2007).
About 2 billion words come from the Gigaword
2003 corpus (Graff and Cieri, 2003). Additional
500 million words come from an in-house corpus
containing popular science and fiction texts. Oc-
currence counts of 2.1 million word types and of
1,279 million word type pairs are efficiently com-
pressed using the TrendStream technology (Flor,
2013), resulting in a database file of 4.7GB. Trend-
Stream is a trie-based architecture for storage, re-
trieval, and updating of very large word n-gram
datasets. We store pairwise word associations as
bigrams; since associations are unordered, only
one of the orders in actually stored in the database.
There is an extensive literature on the use of
word-association measures for NLP, especially for
detection of collocations (Pecina, 2010; Evert,
2008; Futagi et al, 2008). The use of point-
wise mutual information with word-space models
is noted in (Zhang et al, 2012; Baroni and Lenci,
2010; Mitchell and Lapata, 2008; Turney, 2001).
Point-wise mutual information is defined as fol-
lows (Church and Hanks, 1990):
2In all texts, we use human-marked paragraphs, indicated
either by a new line or by an xml markup.
PMI(x, y) = log2
P (x, y)
P (x)P (y) (1)
Differently from Church and Hanks (1990), we
disregard word order when computing P (x, y).
All probabilities are estimated using frequencies.
We define WAPT ? a word association pro-
file of a text T ? as the distribution of PMI(x, y)
for all pairs of content3 word types (x, y) ?T.
All pairs of word types for which the associations
database returned a null value (the pair has never
been observed in the same paragraph) are ex-
cluded from the calculation. For our main dataset
(described later as setA, section 4.1), the average
percentage of non-null values per text is 92%.
To represent the WAP of a text, we use a 60-bin
histogram spanning all PMI values. The lowest
bin (shown in Figures 1 and 2 as PMI = ?5) con-
tains pairs with PMI??5; the topmost bin (shown
in Figures 1 and 2 as PMI = 4.83) contains pairs
with PMI> 4.67, while the rest of the bins contain
word pairs (x, y) with ?5 <PMI(x, y) ? 4.67.
Each bin in the histogram (apart from the top and
the bottom ones) corresponds to a PMI interval
of 0.167. We chose a relatively fine-grained bin-
ning and performed no optimization for grid selec-
tion; for more sophisticated gridding approaches
to study non-linear relationships in the data, see
Reshef et al (2011).
We will say that a text A is tighter than text
B if the WAP of A is shifted towards the higher
end of PMI values relative to text B. The intuition
behind the terminology is that texts with higher
proportions of highly associated pairs are likelier
to be more focused, dealing with a small num-
ber of topics at greater length, as opposed to texts
that bring various different themes into the text to
various extents. Thus, the text ?The dog barked
and wagged its tail? is much tighter than the text
?Green ideas sleep furiously?, with all the six con-
tent word pairs scoring above PMI=5.5 in the first
and below PMI=2.2 in the second.4
3 Illustration: The shape of the
distribution
For a first illustration, we use a corpus of 5,904
essays written as part of a standardized graduate
3We part-of-speech tag a text using OpenNLP tagger
(http://opennlp.apache.org) and only take into account com-
mon and proper nouns, verbs, adjectives, and adverbs.
4We omitted colorless from the second example, as color-
less is actually highly associated with green (PMI=4.36).
1149
school admission test (a full descrption of these
data is given in section 4.1, under setA p1-p6). For
each essay, we compute the WAP and represent it
using the 60-bin histogram. For each bin in the
histogram, we compute its average value over the
5,904 essays; additionally, we compute the 15th
and 85th percentiles for each bin, so that the band
between them contains values observed for 70%
of the texts. The series with the solid thick (blue)
line in Figure 1 shows the distribution of the ave-
rage percentage of word type pairs per bin (essays-
av); the dotted lines above and below show the
band capturing the middle 70% of the distribution
(essays-15 and essays-85).
We observe that the shape of the WAP is very
stable across essays, and the variation around the
average is quite limited.
Next, consider the thin solid (green) line with
asterisk-shaped markers in Figure 1 that plots a
similarly-binned histogram for the normal distri-
bution with ?=0.90 and ?=0.66. We note that
for values below PMI=2.17, the normal curve is
within or almost within the 70% band for the essay
data. The divergence occurs at the right tail with
PMI>2.17, that covers, on average, about 8% of
the pairs (5.6% and 10.4% for the 15th and 85th
percentiles, respectively).
To get an idea about possible variation in the
distribution, we consider two additional corpora
from different genres. We use a corpus of Wall
Street Journal 1987 articles from the TIPSTER
collection.5 We picked articles of 250 to 700
words in length, in order to keep the length of texts
comparable to the essay data, while varying the
genre; 770 such articles were found. The dashed
(orange) line in Figure 1 shows the distribution of
average values for the WSJ collection (wsj-av).
We observe that the shape of the distribution is
similar to that of essay data, although WSJ articles
tend to be less tight, on average, since the distribu-
tion in PMI<2.17 area in the WSJ data is shifted
to the left relative to essays. Yet, the picture at the
right tail is remarkably similar to that of the es-
says, with 9% of word pairs, on average, having
PMI>2.17.
The second additional corpus contains 140 lite-
rary texts written or adapted for readers in grades
3 and 4 in US schools (Sheehan et al, 2008).
In terms of length, these texts fall into the same
range as the other corpora, averaging 507 words.
5LDC93T3A in LDC catalogue
The average WAP for these texts is shown with
a thin solid (purple) line with circular markers
in Figure 1 (Grades 3-4). These texts are much
tighter than texts in the other two collections, as
the distribution is shifted to the right. The right
tail, with PMI>2.17, holds 19% of all word pairs
in these texts ? more than twice the proportion
in essays written by college graduates or in texts
from the WSJ.
It is instructive to check whether the over-use
of highly associated pairs is felt during reading.
These texts strike an adult reader as overly ex-
plicit, taking the space to state things that an adult
reader would readily infer or assume. For exam-
ple, consider the following opening paragraph:
?Grandma Rose gave Daniel a recorder.
A recorder is a musical instrument.
Daniel learned to play by blowing on the
recorder. It didn?t take lots of air. It
didn?t take big hands to hold since it was
pocket-sized. His fingers covered the
toneholes just fine. Soon Daniel played
entire songs. His mother loved to lis-
ten. Sometimes she hummed along with
Daniel?s recorder.?
The second and the third sentences state things
that for an adult reader would be too obvious
to need mention. In fact, these sentences al-
most seem like training sentences ? the kind of
sentences from which the associations between
recorder and musical instrument, play, blowing
can be learned. According to Hoey?s theory of
lexical priming (Hoey, 2005), one of the main
functions of schooling is to imbue children with
the societally sanctioned word associations.
To conclude the illustration, we observe that
there are some broad similarities between the dif-
ferent copora in terms of the distribution of pairs
of word types. Thus, texts seem to be mainly made
of pairs of weakly associated words ? about half
the pairs of word types lie between PMI of 0.5
and 1.5, in all the examined collections (52% for
essays, 44% for each of WSJ and young reader
corpora). The percentages of pairs at the low and
the high ends of PMI differ with genre ? writing
for children favors the higher end, while typical
Wall Street Journal writing favors the low end,
relatively to a corpus of essays on general topics
written by college graduates.
These observations are necessarily very tenta-
tive, as only a few corpora were examined. Still,
1150
681012
e?of?pairs?of?word?types
es
sa
ys
?av
es
sa
ys
?1
5
es
sa
ys
?8
5
ws
j?a
v
N(
0.9
0,0
.66
)
Gr
ad
es
?3?
4
024
?5
?4
?3
?2
?1
0
1
2
3
4
5
Percentag
PM
I
Figure 1: WAP histograms for three corpora, shown with smooth lines instead of bars for readability.
Average for essays (a thick solid blue line), average for WSJ articles (a dashed orange line); average for
Grades 3-4 corpus (a thin solid purple line with round markers). Normal distribution is shown with a thin
solid green line with asterisk markers. Middle 70% of essays fall between the dotted lines.
we believe the illustration is suggestive, in that
there is both constancy in writing for a similar pur-
pose (observe the limited variation around the ave-
rage that captures 70% of the essays) and variation
with genre and target audience. In what follows,
we will explore more thoroughly the information
provided by word association profiles regarding
the quality of writing.
4 Application to Essay Scoring
Texts written for a test and scored by relevant pro-
fessionals is a setting where variation in text qua-
lity is expected. In this section, we report our ex-
periments with using WAPs to explore the varia-
tion in quality as quantified by essay scores. We
first describe the data (section 4.1), then show the
patterns of relationships between essay scores and
word association profiles (section 4.2). Finally,
we report on an experiment where we significantly
improve the performance of a very competitive,
state-of-art system for automated scoring of es-
says, using a feature derived from WAP.
4.1 Data
We consider two collections of essays written as
responses in an analytical writing section of a
high-stakes standardized test for graduate school
admission; the time limit for essay composition
was 45 minutes. Essays were written in response
to a prompt (essay question). A prompt is usually a
general statement, and the test-taker is asked to de-
velop an argument supporting or refuting the state-
ment. Example prompts are: ?High-speed elec-
tronic communications media, such as electronic
mail and television, tend to prevent meaningful
and thoughtful communication? and ?In the age of
television, reading books is not as important as it
once was. People can learn as much by watching
television as they can by reading books.?
The first collection (henceforth, setA) contains
8,899 essays written in response to nine different
prompts, about 1,000 per prompt;6 the per-prompt
subsets will be termed setA-p1 through setA-p9.
Each essay in setA was scored by 1 to 4 human
raters on a scale of 1 to 6; the majority of essays re-
ceived 2 human scores. We use the average of the
available human scores as the gold-standard score
for the essay. Most essays thereby receive an inte-
ger score,7 so the ranking of the essays is coarse.
From this set, p1-p6 were used for feature selec-
tion, data visualization, and estimation of the re-
gression models (training), while sets p7-p9 were
reserved for a blind test.
The second collection (henceforth, setB) con-
6While we sampled exactly 1,000 essays per prompt, we
removed empty responses, resulting in 975 to 1,000 essays
per sample.
7as the two raters agree most of the time
1151
tains 400 essays, with 200 essays written on each
of two prompts given as examples above (setB-p1
and setB-p2). In an experimental study by Attali
et al (2013), each essay was scored by 16 profes-
sional raters on a scale of 1 to 6, allowing plus and
minus scores as well, quantified as 0.33 ? thus, a
score of 4- is rendered as 3.67. This fine-grained
scale resulted in higher mean pairwise inter-rater
correlations than the traditional integer-only scale
(r=0.79 vs around r=0.70 for the operational sco-
ring). We use the average of 16 raters as the final
grade for each essay. This dataset provides a very
fine-grained ranking of the essays, with almost no
two essays getting exactly the same score.
Rounded setA p1-p9 setB
Score av min max p1 p2
1 .01 .00 .01 ? ?
2 .05 .04 .06 .03 .03
3 .25 .20 .29 .30 .28
4 .44 .42 .47 .54 .55
5 .21 .16 .24 .13 .14
6 .04 .02 .07 .01 .02
Table 1: Score distribution in the essay data. For
the sake of presentation in this table, all scores
were rounded to integer scores, so a score of 3.33
was counted as 3, and a score of 3.5 was counted
as 4. A cell with the value of .13 (row titled 5
and column titled SetB p1) means that 13% of
the essays in setB-p1 received scores that round
to 5. For setA, average, minimum, and maximum
values across the nine prompts are shown.
Table 1 shows the distribution of rounded scores
in both collections. Average essay scores are be-
tween 3.74 to 3.98 across the different prompts
from both collections. The use of 16 raters seems
to have moved the rounded scores towards the
middle; however, the relative ranking of the essays
is much more delicate in setB than in setA.
4.2 Essay Score vs WAP
We calculated correlations between essay score
and the proportion of word pairs in each of the 60
bins of the WAP histogram, separately for each of
the prompts p1-p6 in setA. For a sample of 1,000
instances, a correlation of r=0.065 is significant at
p = 0.05. Figure 2 plots the correlations.
First, we observe that, perhaps contrary to ex-
pectation, the proportion of the highest values of
PMI (the area to the right of PMI=4 in Figure 2)
does not yield a consistent correlation with essay
scores. Thus, inasmuch as highest PMI values
tend to capture multi-word expressions (South and
Africa; Merill and Lynch), morphological vari-
ants (bids and bidding), or synonyms (mergers
and takeovers), their proportion in word type pairs
does not seem to give a clear signal regarding the
quality of writing.8
In contrast, the area of moderately high PMI
values (from PMI=2.5 to PMI=3.67 in Figure 2)
produces a very consistent picture, with only two
points out of 48 in that interval9 lacking signif-
icant positive correlation with essay score (p2 at
PMI=3.17 and p5 at PMI=3).
Next, observe the consistent negative correla-
tions between essay score and the proportion of
word pairs in bins PMI=0.833 through PMI=1.5.
Here again, out of the 30 data points correspond-
ing to these values, only 3 failed to reach statistical
significance, although the trend there is still nega-
tive.
Finally, there is a trend towards a positive cor-
relation between essay scores and the proportion
of mildly negative PMI values (-2<PMI<0), that
is, better essays tend to use more pairs of dis-
associated words, although this trend is not as
clear-cut as the one on the right-hand side of the
distribution.
Assuming that a higher proportion of high PMI
pairs corresponds to more topic development and
that a higher proportion of negative PMIs corre-
ponds to more creative use of language (in that
pairs are chosen that do not generally tend to ap-
pear together), it seems that the better essays are
both more topical and more creative than the lower
scoring ones. In what follows, we check whether
the information about essay quality provided by
WAP can be used to improve essay scoring.
8It is also possible that some of the instances with very
high PMI are pairs that contain low frequency words for
which the database predicts a spuriously high PMI based on a
single (and a-typical) co-occurrence that happens to repeat in
an essay ? similar to the Schwartz eschews example in (Man-
ning and Schu?tze, 1999, Table 5.16, p. 181). On the one
hand, we do not expect such pairs to occur in any systematic
pattern, so they could obscure an otherwise more systematic
pattern in the high PMI bins. On the other hand, we do not
expect to see many such pairs, simply because a repetition
of an a-typical event is likely to be very rare. We thank an
anonymous reviewer for suggesting this direction, and leave
a more detailed examination of the pairs in the highest-PMI
bins to future work.
9There are 8 bins of width of 0.167 in the given interval,
with 6 datapoints per bin.
1152
?0
.100.10.20.3
?5
?4
?3
?2
?1
0
1
2
3
4
5
relation?with?Essay?Score
p1 p2 p3 p4 p5 p6
?0
.4
?0
.3
?0
.2
Pearson?Cor
PM
I
Figure 2: Correlations with essay score for various bins of the WAP histogram. P1 to P6 correspond to
the first 6 prompts in SetA.
4.3 Baseline
As a baseline, we use e-rater (Attali and Burstein,
2006), a state-of-art essay scoring system deve-
loped at Educational Testing Service.10 E-rater
computes more than 100 micro-features, which are
aggregated into macro-features aligned with spe-
cific aspects of the writing construct. The system
incorporates macro-features measuring grammar,
usage, mechanics, style, organization and develop-
ment, lexical complexity, and vocabulary usage.
Table 2 gives examples of micro-features covered
by the different macro-features.
E-rater models are built using linear regression
on large samples of test-taker essays. We use a
generic e-rater model built at Educational Testing
Service using essays across a variety of writing
prompts, with no connection to the current project
and its authors. This model obtains Pearson corre-
lations of r=0.8324-0.8721 with the human scores
on setA, and the staggering r=0.9191 and r=0.9146
with the human scores on setB-p1 and setB-p2,
respectively. This is a very competitive baseline,
as e-rater features explain more than 70% of the
variation in essay scores on a relatively coarse
scale (setA) and more than 80% of the variation
in scores on a fine-grained scale (setB).
10http://www.ets.org/erater/about/
Macro- Example Micro-Features
Feature
Grammar, agreement errors
Usage, and verb formation errors
Mechanics missing punctuation
Style passive
very long or short sentences
excessive repetition
Organization use of discourse elements:
and thesis, support, conclusion
Development
Lexical average word frequency
Complexity average word length
Vocabulary similarity to vocabulary in
high- vs low-scoring essays
Table 2: Features used in e-rater (Attali and
Burstein, 2006).
4.4 Adding WAP
We define HAT ? high associative tight-
ness ? as the percentage of word type pairs
with 2.33<PMI?3.67 (bins PMI=2.5 through
PMI=3.67). This range correponds to the longest
sequence of adjacent bins in the PMI>0 area that
had a positive correlation with essay score in the
setA-p1 set. The HAT feature attains significant
1153
(at p = 0.05) correlations with essay scores,
r=0.11 to r=0.27 for the prompts in setA, and
r=0.22 and r=0.21 for the two prompts in setB. We
note that the HAT feature is not correlated with es-
say length. Essay length is not used as a feature in
e-rater models, but it typically correlates strongly
with the human essay score (at about r=0.70 in our
data), as well as with the score provided by e-rater
(at about r=0.80).
We also explored a feature that captured the
area with the negative correlations identified in
section 4.2. This feature did not succeed in im-
proving the performance over the baseline on setA
p1-p6; we tentatively conclude that information
contained in that feature, i.e. the proprotion of
mildly associated vocabulary in an essay, is indi-
rectly captured by another feature or group of fea-
tures already present in e-rater. Likewise, a feature
that calculates the average PMI for all pairs of con-
tent word types in the text failed to produce an im-
provement over the baseline for setA p1-p6. The
reason for this can be observed in Figure 2: The
higher-scoring essays having more of both the low
and the high PMI pairs leads to about the same
average PMI as for the lower-scoring essays that
have a higher concentration of values closer to the
average PMI.
4.5 Evaluation
To evaluate the usefulness of WAP in improving
automated scoring of essays, we estimate a lin-
ear regression model using the human score as a
dependent variable (label) and e-rater score and
the HAT as the two independent variables (fea-
tures). The correlations between the two inde-
pendent variables (e-rater and HAT) are between
r=0.11 and r=0.24 on the prompts in setA and
setB.
We estimate a regression model on each of
setA-pi, i ? {1, .., 6}, and evaluate them on each
of setA-pj, j ? {7, .., 9}, and compare the perfor-
mance with that of e-rater alone on setA-pj. Note
that e-rater itself is not trained on any of the data
in setA and setB; we use the same e-rater model
for all evaluations, a generic model that was pre-
trained on a large number of essays across diffe-
rent prompts. For setB, we estimate the regression
model on setB-p1 and test on setB-p2, and vice
versa.
Table 3 shows the evaluation results. The HAT
feature leads to a statistically significant improve-
Train Test E-rater E-rater+HAT t
on Test on Test
setA
p1 p7 0.84043 0.84021 -0.371
p2 p7 0.84043 0.84045 0.408
p3 p7 0.84043 0.83999 -0.597
p4 p7 0.84043 0.84044 0.411
p5 p7 0.84043 0.84028 -0.280
p6 p7 0.84043 0.83926 -1.080
p1 p8 0.83244 0.83316 1.688
p2 p8 0.83244 0.83250 2.234
p3 p8 0.83244 0.83327 1.530
p4 p8 0.83244 0.83250 2.237
p5 p8 0.83244 0.83311 1.752
p6 p8 0.83244 0.83339 1.191
p1 p9 0.86370 0.86612 4.282
p2 p9 0.86370 0.86389 5.205
p3 p9 0.86370 0.86659 4.016
p4 p9 0.86370 0.86388 5.209
p5 p9 0.86370 0.86591 4.390
p6 p9 0.86370 0.86730 3.448
setB
p1 p2 0.9146 0.9178 0.983
p2 p1 0.9191 0.9242 2.690
Table 3: Performance of baseline model (e-rater)
and models where e-rater was augmented with
HAT, a feature based on the word association
profile. Performance is measured using Pearson
correlation with essay score. We use Wilcoxon
Signed-Ranked test for matched pairs, and report
the sum of signed ranks (W), the number of ranks
(n), and the p value. E-rater+HAT is significantly
better than e-rater alone, W=138, n=20, p<0.05.
We also measure significance of the improvement
for each row individually, using McNemar?s test
for significance of difference in same-sample cor-
relations (McNemar, 1955, p.148); we report the
t value for each test. For values of t > 1.645,
we can reject the hypothesis that e-rater+HAT is
not better than e-rater alone with 95% confidence.
Significant improvements are underlined.
1154
ment in the performance of automated scoring.
An improvement is observed for 14 out of the 18
evaluations for setA, as well as for both evalua-
tions for setB.11 Moreover, the largest relative im-
provement of 0.55%, from 0.9191 to 0.9242, was
observed for the setting with the highest baseline
performance, suggesting that the HAT feature is
still effective even after the delicate ranking of
the essays revealed an exceptionally strong perfor-
mance of e-rater.
5 Related Work
Most of the attention in the computational linguis-
tics research that deals with analysis of the lexis
of texts has so far been paid to what in our terms
would be the very high end of the word associa-
tion profile. Thus, following Halliday and Hasan
(1976), Hoey (1991), and Morris and Hirst (1991),
the notion of lexical cohesion has been used to
capture repetitions of words and occurrence of
words with related meanings in a text. Lexically
cohesive words are traced through the text, for-
ming lexical chains or graphs, and these repre-
sentations are used in a variety of applications,
such as segmentation, keyword extraction, sum-
marization, sentiment analysis, temporal indexing,
hypelink generation, error correction (Guinaudeau
et al, 2012; Marathe and Hirst, 2010; Ercan and
Cicekli, 2007; Devitt and Ahmad, 2007; Hirst
and Budanitsky, 2005; Inkpen and De?silets, 2005;
Gurevych and Strube, 2004; Stokes et al, 2004;
Silber and McCoy, 2002; Green, 1998; Al-Halimi
and Kazman, 1998; Barzilay and Elhadad, 1997).
To our knowledge, lexical cohesion has not so far
been used for automated scoring of essays. Our
results suggest that this direction is promising, as
merely the proportion of highly associated word
pairs is already contributing a clear signal regar-
ding essay quality; it is possible that additional
information can be derived from richer represen-
tations common in the lexical cohesion literature.
Aspects related to the distribution of words in
essays have been studied in relation to essay sco-
ring. One line of work focuses on assessing co-
herence of essays. Foltz et al (1998) use Latent
11We also performed a cross-validation test on setA p1-
p6, where we estimated a regression model on setA-pi and
evaluate it on setA-pj, for all i, j ? {1, .., 6}, i 6= j, and
compared the performance with that of e-rater alone on setA-
pj, yielding 30 different train-test combinations. The results
were similar to those of the blind test presented here, with e-
rater+HAT significantly improving upon e-rater alone, using
Wilcoxon test, W=374, n=29, p<0.05.
Semantic Analysis to model the smoothness of
transitions between adjacent segments of an essay.
Higgins et al (2004) compare sentences from cer-
tain discourse segments in an essay to determine
their semantic similarity, such as comparing the-
sis statements to conclusions or thesis statements
to essay prompts. Additional approaches include
evaluation of coherence based on repeated refe-
rence to entities (Burstein et al, 2010; Barzilay
and Lapata, 2008; Miltsakaki and Kukich, 2004).
Our approach is different in that it does not mea-
sure the flow of the text, that is, the sequencing
and repetition of the words, but rather assesses the
choice of vocabulary as a whole.
Topic models have been proposed as a tech-
nique for capturing clusters of related words that
tend to occur in the same documents in a given
collection. A text is modeled as being composed
of a small number of topics, and words in the text
are generated conditioned on the selected topics
(Gruber et al, 2007; Blei et al, 2003). Since
(a) topics encapsulate clusters of highly associated
words, and (b) topics for a given text are modeled
as being chosen independently from each other,
we expect a negative correlation between the num-
ber of topics in a document and the tightness of the
word association profile of the text.
An alternative representation of word associ-
ation profile would be a weighted graph, where
the weights correspond to pairwise associations
between words. Thus, for longer texts, graph
analysis techniques would be applicable. Steyvers
and Tenenbaum (2005) analyze the graphs in-
duced from large repositories like WordNet or
databases of free associations, and find them to be
scale-free and small-world; it is an open question
whether word association graphs induced from
book-length texts would exhibit similar properties.
In the theoretical tradition, our work is closest in
spirit to Michael Hoey?s theory of lexical priming
(Hoey, 2005), positing that users of language inter-
nalize patterns of occurrence and non-occurrence
of words not only with other words, but also in cer-
tain positions in a text, in certain syntactic environ-
ments, and in certain evaluative contexts, and use
these when creating their own texts. We believe
that word association profiles reflect the artwork
that goes into using those internalized associations
between words when creating a text, achieving the
right mix of strong and weak, positive and nega-
tive associations.
1155
6 Conclusion
In this paper, we described a new representation
of the content vocabulary of a text we call word
association profile that captures the proportions
of highly associated, mildly associated, unassoci-
ated, and dis-associated pairs of words selected to
co-exist in the given text by its author. We ob-
served that the shape of the distribution is quite
stable across various texts, with about half the
pairs having a mild association; the allocation of
pairs to the higher and the lower levels of associa-
tion does vary across genres and target audiences.
We further presented a study of the relationship
between quality of writing and word association
profiles. For a dataset of essays written by college
graduates on a number of general topics in a stan-
dardized test for graduate school admission and
scored by professional raters, we showed that the
higher scoring essays tend to have higher percen-
tages of both highly associated and dis-associated
pairs, and lower percentagese of mildly associated
pairs of words. We hypothesize that this pattern
is consistent with the better essays demonstrating
both a better topic development (hence the higher
percentage of highly related pairs) and a more cre-
ative use of language resources, as manifested in a
higher percentage of word pairs that generally do
not tend to appear together.
Finally, we demonstrated that the information
provided by word association profiles leads to a
significant improvement in a highly competitive,
state-of-art essay scoring system that already mea-
sures various aspects of writing quality.
In future work, we intend to investigate in more
detail the contribution of various kinds of words to
word association profiles, as well as pursue appli-
cation to evaluation of text complexity.
References
Reem Al-Halimi and Rick Kazman. 1998. Temporal
indexing through lexical chaining. In C. Fellbaum,
editor, WordNet: An Electronic Lexical Database,
pages 333?351. Cambridge, MA: MIT Press.
Yigal Attali and Jill Burstein. 2006. Automated Essay
Scoring With e-rater R?V.2. Journal of Technology,
Learning, and Assessment, 4(3).
Yigal Attali, Will Lewis, and Michael Steier. 2013.
Scoring with the computer: Alternative procedures
for improving reliability of holistic essay scoring.
Language Testing, 30(1):125?141.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673?721.
Regina Barzilay and Michael Elhadad. 1997. Using
lexical chains for text summarization. In Proceed-
ings of ACL Intelligent Scalable Text Summarization
Workshop.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Roberto Basili and Marco Pennacchiotti. 2010. Dis-
tributional lexical semantics: Toward uniform rep-
resentation paradigms for advanced acquisition and
processing tasks. Natural Language Engineering,
16(4):347?358.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510?526.
Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.
Using entity-based features to model coherence in
student essays. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 681?684, Los Angeles, California,
June. Association for Computational Linguistics.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16(1):22?29.
Ann Devitt and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 984?991, Prague, Czech Republic,
June. Association for Computational Linguistics.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ?08, pages 334?
343, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Gonenc Ercan and Ilyas Cicekli. 2007. Using lexical
chains for keyword extraction. Information Process-
ing & Management, 43(6):1705?1714.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
897?906, Honolulu, Hawaii, October. Association
for Computational Linguistics.
1156
Stefan Evert. 2008. Corpora and collocations. In
A. Lu?deling and M. Kyto?, editors, Corpus Linguis-
tics: An International Handbook. Berlin: Mouton de
Gruyter.
Michael Flor. 2013. A fast and flexible architecture for
very large word n-gram datasets. Natural Language
Engineering, 19(1):61?93.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
latent semantic analysis. Discourse Processes,
25(2):285?307.
Yoko Futagi, Paul Deane, Martin Chodorow, and Joel
Tetreault. 2008. A computational approach to de-
tecting collocation errors in the writing of non-native
speakers of English. Computer Assisted Language
Learning, 21(4):353?367.
David Graff and Christopher Cieri. 2003. English Gi-
gaword LDC2003T05. Linguistic Data Consortium,
Philadelphia.
Stephen Green. 1998. Automated link generation: Can
we do better than term repetition? Computer Net-
works, 30:75?84.
Amit Gruber, Yair Weiss, and Michal Rosen-Zvi.
2007. Hidden topic markov models. Journal of
Machine Learning Research - Proceedings Track,
2:163?170.
Camille Guinaudeau, Guillaume Gravier, and Pascale
Se?billot. 2012. Enhancing lexical cohesion measure
with confidence measures, semantic relations and
language model interpolation for multimedia spoken
content topic segmentation. Computer Speech and
Language, 26(2):90?104.
Iryna Gurevych and Michael Strube. 2004. Seman-
tic similarity applied to spoken dialogue summariza-
tion. In Proceedings of Coling 2004, pages 764?
770, Geneva, Switzerland, August. COLING.
Michael A.K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Marti Hearst. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of
coherence in student essays. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings, pages 185?192, Boston,
Massachusetts, USA, May. Association for Compu-
tational Linguistics.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lexi-
cal cohesion. Natural Language Engineering,
11(1):87?111.
Michael Hoey. 1991. Patterns of Lexis in Text. Oxford
University Press.
Michael Hoey. 2005. Lexical Priming. Routledge.
Diana Inkpen and Alain De?silets. 2005. Semantic
similarity for detecting recognition errors in auto-
matic speech transcripts. In Proceedings of Empir-
ical Methods in Natural Language Processing Con-
ference, pages 49?56, Vancouver, British Columbia,
Canada, October. Association for Computational
Linguistics.
Michael Jones and Douglas Mewhort. 2007. Repre-
senting word meaning and order information in a
composite holographic lexicon. Psychological Re-
view, 114(1):1?37.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to Plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of ACL, pages
768?774, Montreal, Canada.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments & Computers, 28:203?208.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Meghana Marathe and Graeme Hirst. 2010. Lexical
Chains Using Distributional Measures of Concept
Distance. In Proceedings of 11th International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics (CICLING), pages 291?302, Iasi,
Romania, March.
Quinn McNemar. 1955. Psychological Statistics. New
York: J. Wiley and Sons, 2nd edition.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring sys-
tems. Natural Language Engineering, 10(1):25?55.
Hemant Misra, Franc?ois Yvon, Joemon M. Jose, and
Olivier Cappe. 2009. Text segmentation via topic
modeling: an analytical study. In Proceedings of
the 18th ACM conference on Information and know-
ledge management, CIKM ?09, pages 1553?1556,
New York, NY, USA. ACM.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics, pages 236?244, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion, the thesaurus, and the structure of text. Com-
putational linguistics, 17(1):21?48.
1157
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Pavel Pecina. 2010. Lexical association measures
and collocation extraction. Language Resources and
Evaluation, 44:137?158.
David Reshef, Yakir Reshef, Hilary Finucane, Sharon
Grossman, Gilean McVean, Peter Turnbaugh, Eric
Lander, Michael Mitzenmacher, and Pardis Sabeti.
2011. Detecting novel associations in large data
sets. Science, 334(6062):1518?1524.
Martin Riedl and Chris Biemann. 2012. How text seg-
mentation algorithms gain from topic models. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 553?557, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
Gerard Salton, Andrew Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613?620.
Kathy Sheehan, Irene Kostin, and Yoko Futagi. 2008.
When do standard approaches for measuring vo-
cabulary difficulty, syntactic complexity and refer-
ential cohesion yield biased estimates of text diffi-
culty? In Proceedings of the Cognitive Science So-
ciety, pages 1978?1983, Washington, DC, July.
Gregory Silber and Kathleen McCoy. 2002. Efficiently
computed lexical chains as an intermediate represen-
tation for automatic text summarization. Computa-
tional Linguistics, 28(4):487?496.
Mark Steyvers and Joshua B. Tenenbaum. 2005. The
Large-Scale Structure of Semantic Networks: Sta-
tistical Analyses and a Model of Semantic Growth.
Cognitive Science, 29:41?78.
Nicola Stokes, Joe Carthy, and Alan F. Smeaton. 2004.
Select: A lexical cohesion based news story seg-
mentation system. Journal of AI Communications,
17(1):3?12.
Peter Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Articial Intelligence Research,
37:141?188.
Peter D. Turney. 2001. Mining the Web for Syno-
nyms: PMI-IR versus LSA on TOEFL. In European
Conference on Machine Learning, pages 491?502,
Freiburg, Germany, September.
Ziqi Zhang, Anna Gentile, and Fabio Ciravegna. 2012.
Recent advances in methods of lexical semantic re-
latedness ? a survey. Natural Language Engineer-
ing, FirstView:1?69.
1158
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 247?252,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Content Importance Models for Scoring Writing From Sources
Beata Beigman Klebanov Nitin Madnani Jill Burstein Swapna Somasundaran
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541
{bbeigmanklebanov,nmadnani,jburstein,ssomasundaran}@ets.org
Abstract
Selection of information from external
sources is an important skill assessed in
educational measurement. We address an
integrative summarization task used in an
assessment of English proficiency for non-
native speakers applying to higher educa-
tion institutions in the USA. We evaluate a
variety of content importance models that
help predict which parts of the source ma-
terial should be selected by the test-taker
in order to succeed on this task.
1 Introduction
Selection and integration of information from ex-
ternal sources is an important academic and life
skill, mentioned as a critical competency in the
Common Core State Standards for English Lan-
guage Arts/Literacy: College-ready students will
be able to ?gather relevant information from mul-
tiple print and digital sources, assess the credibi-
lity and accuracy of each source, and integrate the
information while avoiding plagiarism.?
1
Accordingly, large-scale assessments of writing
incorporate tasks that test this skill. One such test
requires test-takers to read a passage, then to lis-
ten to a lecture discussing the same topic from
a different point of view, and to summarize the
points made in the lecture, explaining how they
cast doubt on points made in the reading. The qua-
lity of the information selected from the lecture is
emphasized in excerpts from the scoring rubric for
this test (below); essays are scored on a 1-5 scale:
Score 5 successfully selects the important infor-
mation from the lecture and coherently and
accurately presents this information in rela-
tion to the relevant information presented in
the reading.
1
http://www.corestandards.org/
ELA-Literacy/CCRA/W.
Score 4 is generally good in selecting the impor-
tant information from the lecture ..., but it
may have a minor omission.
Score 3 contains some important information
from the lecture ..., but it may omit one major
key point.
Score 2 contains some relevant information from
the lecture ... The response significantly
omits or misrepresents important points.
Score 1 provides little or no meaningful or rele-
vant coherent content from the lecture.
The ultimate goal of our project is to improve
automated scoring of such essays by taking into
account the extent to which a response integrates
important information from the lecture. This pa-
per reports on the first step aimed at automatically
assigning importance scores to parts of the lecture.
The next step ? developing an essay scoring sys-
tem using content importance models along with
other features of writing quality, will be addressed
in future work. A simple essay scoring mechanism
will be used for evaluation purposes in this paper,
as described in the next section.
2 Design of Experiment
In evaluations of summarization algorithms, it is
common practice to derive the gold standard con-
tent importance scores from human summaries, as
done, for example, in the pyramid method, where
the importance of a content element corresponds
to the number of reference human summaries that
make use of it (Nenkova and Passonneau, 2004).
Selection of the appropriate content plays a cru-
cial role in attaining a high score for the essays
we consider here, as suggested by the quotes from
the scoring rubric in ?1, as well as by a corpus
study by Plakans and Gebril (2013). We therefore
observe that high-scoring essays can be thought
247
of as high-quality human summaries of the lec-
ture, albeit containing, in addition, references to
the reading material and language that contrasts
the different viewpoints, making them a somewhat
noisy gold standard. On the other hand, since low-
scoring essays contain deficient summaries of the
lecture, our setup allows for a richer evaluation
than typical in studies using gold standard human
data only, in that a good model should not only
agree with the gold standard human summaries
but should also disagree with sub-standard human
summaries. We therefore use correlation with es-
say score to evaluate content importance models.
The evaluation will proceed as follows. Every
essay E is responding to a test prompt that con-
tains a lecture L and a reading R. We identify the
essay?s overlap with the lecture:
O(E,L) = {x|x ? L, x ? E} (1)
where the exact definition of x, that is, what is
taken to be a single unit of information, will be
one of the parameters to be studied. The essay is
then assigned the following score by the content
importance model M :
S
M
(E) =
?
x?O(E,L)
w
M
(x)? C(x,E)
n
E
(2)
where w
M
(x) is the importance weight as-
signed by model M to item x in the lecture,
C(x,E) is the count of tokens in E that realize
the information unit x, and n
E
is the number of
tokens in the essay. In this paper, the distinction
between x and C is that between type and token
count of instances of that type.
2
This simple sco-
ring mechanism quantifies the rate of usage of im-
portant information per token in the essay. Finally,
we calculate the correlation of scores assigned to
essays by model M with scores assigned to the
same essays by human graders.
This design ensures that once x is fixed, all the
content importance models are evaluated within
the same scoring scheme, so any differences in the
correlations can be attributed to the differences in
the weights assigned by the importance models.
2
In the future, we intend to explore more complex rea-
lization functions, allowing paraphrase, skip n-grams (as in
ROUGE (Lin, 2004)), and other approximate matches, such
as misspellings and inflectional variants.
3 Content Importance Models
Our setting can be thought of as a special kind
of summarization task. Test-takers are required
to summarize the lecture while referencing the
reading, making this a hybrid of single- and multi-
document summarization, where one source is
treated as primary and the other as secondary.
We therefore consider models of content impor-
tance that had been found useful in the summariza-
tion literature, as well as additional models that
utilize a special feature of our scenario: We have
hundreds of essays of varying quality responding
to any given prompt, as opposed to a typical news
summarization scenario where a small number of
high quality human summaries are available for a
given article. A sample of these essays can be used
when developing a content importance model.
We define the following importance models.
For all definitions, x is a unit of information
in the lecture; C(x, t) is the number of tokens in
text t that realize x; n
L
and n
R
are the number of
tokens in the lecture and the reading, respectively.
3
Na??ve: w(x) = 1. This is a simple overlap model.
Prob: w(x) =
C(x,L)
n
L
, an MLE estimate of
the probability that x appears in the lecture.
Those x that appear more are more important.
Position: w(x) =
FP (x)
n
L
, where FP (x) is the
offset of the first occurrence of x in the lec-
ture. The offset corresponds to the token?s
serial number in the text, 1 through n
L
.
LectVsRead: w(x) =
C(x,L)
n
L
?
C(x,R)
n
R
, that is, the
difference in the probabilities of occurrence
of x in the lecture and in the reading passage
that accompanies the lecture. This model at-
tempts to capture the contrastive aspect of
importance ? the content that is unique to
the lecture is more important than the content
that is shared by the lecture and the reading.
The following two models capitalize on evi-
dence of use of information in better and worse es-
says. For estimating these models, we sample, for
each prompt, a development set of 750 essays re-
sponding to the prompt (that is, addressing a given
pair of lecture and reading stimuli). Out of these,
we take, for each prompt, all essays at score points
3
Prob, Position, and LectVsRead models normalize by
n
R
and n
L
to enable comparison of essays responding to dif-
ferent lecture + reading stimuli (prompts).
248
4 and 5 (EGood) and all essays at score points 1
and 2 (EBad). These data do not overlap with the
experimental data described in section 4. In both
definitions below, e is an essay.
Good: w(x) =
|{e?EGood|x?e}|
|EGood|
. An x is more im-
portant if more good essays use it. Hong and
Nenkova (2014) showed that a variant of this
measure used on pairs of articles and their ab-
stracts from the New York Times effectively
identified words that typically go into sum-
maries, across topics. In contrast, our mea-
surements are prompt-specific.
GoodVsBad: w(x) =
|{e?EGood|x?e}|
|EGood|
?
|{e?EBad|x?e}|
|EBad|
. An x is more important if
good essays use it more than bad essays.
To our knowledge, this measure has not
been used in the summarization literature,
probably because a large sample of human
summaries of varying quality is typically not
available.
4 Data
We use 116 prompts drawn from an assessment of
English proficiency for non-native speakers. Each
prompt contains a lecture and a reading passage.
For each prompt, we sample about 750 essays.
Each essay has an operational score provided by
a human grader. Table 1 shows the distribution of
essay scores; mean score is 3. Text transcripts of
the lectures were used.
Score 1 2 3 4 5
Proportion 0.13 0.18 0.35 0.25 0.09
Table 1: Distribution of essay scores.
5 Results
Independently from the content importance
models, we address the effect of the granularity of
the unit of information. Intuitively, since all the
materials for a given prompt deal with the same
topic, we expect large unigram overlaps between
lecture and reading, and between good and bad
essays, whereas n-grams with larger n can be
more distinctive. On the other hand, larger n lead
to misses, where an information unit would fail
to be identified in an essay due to a paraphrase,
thus impairing the ability of the scoring function
to use the content importance model effectively.
We therefore evaluate each content importance
model for different granularities of the content
unit x: n-grams for n = 1, 2, 3, 4. Table 2 shows
the correlations with essay scores.
Content Pearson?s r
Importance
Model n=1 n=2 n=3 n=4
Na??ve 0.24 0.27* 0.24 0.20
Prob 0.04 0.14 0.17 0.14
Position 0.22 0.30* 0.26* 0.20
LectVsRead 0.09 0.25* 0.31* 0.26*
Good 0.07 0.15 0.10 0.07
GoodVsBad 0.54* 0.42* 0.32* 0.21
Table 2: Correlations with essay scores attained by
content models, for various definitions of informa-
tion unit (n-grams with n = 1, 2, 3, 4). Five top
scores are boldfaced. The baseline performance
is shown in underlined italics. Correlations that
are significantly better (p < 0.05) than the na??ve
n = 1 model are marked with an asterisk. We
use McNemar (1955, p. 148) test for significance
of difference between same-sample correlations.
N = 85, 252 for all correlations.
6 Discussion
The Na??ve model with n = 1 can be considered a
baseline, corresponding to unweighted word over-
lap between the lecture and the essay. This model
attains a significant positive correlation with essay
score (r = 0.24), suggesting that, in general, bet-
ter writers use more material from the lecture.
Our next observation is that the Prob and Good
models do not improve over the baseline, that is,
their weighting schemes generally assign higher
weights to the wrong units. We believe the rea-
son for this is that the most highly used n-grams,
in the lecture and in the essays, correspond to ge-
neral topical and functional elements. The impor-
tance of these elements is discounted in the more
effective Position, LectVsRead, and GoodVsBad
models, highlighting subtler aspects of the lecture.
Next, let us consider the granularity of the units
of information. We observe that 4-grams are in-
ferior to trigrams for all models, suggesting that
data sparsity is becoming a problem for matching
4-word sequences. For models that assign weight
based on one or two sources (lecture, or lecture
and reading) ? Na??ve, Position, LectVsRead ? un-
igram models are generally ineffective, while bi-
249
gram and trigram models significantly outperform
the baseline. We interpret this as suggesting that
it is certain particular, detailed aspects of the top-
ical concepts that constitute the important nuggets
in the lecture; these are usually realized by multi-
word sequences.
The GoodVsBad models show a different pat-
tern, obtaining the best performance with a uni-
gram version. These models are sensitive to data
sparsity not only when matching essays to the
lecture (this problem is common to all models)
but also during model building. Recall that the
weights in a GoodVsBad model are estimated
based on differential use in samples of good and
bad essays. The estimation of use-in-a-corpus is
more accurate for smaller n, because longer n-
grams are more susceptible to paraphrasing, which
leads to under-estimation of use. Assuming that
paraphrasing behavior of good and bad writers is
not the same ? in fact, there is corpus evidence
that better writers paraphrase more (Burstein et
al., 2012) ? the resulting inaccuracies might im-
pact the estimation of differential use in a sys-
tematic manner, making the n > 1 models less
effective than the unigrams. Given that (a) the
GoodVsBad bigram model is the second best over-
all in spite of the shortcomings of the estimation
process, and (b) that the bigram models worked
better than unigram models for all the other con-
tent importance models, the GoodVsBad bigram
model could probably be improved significantly
by using a more flexible information realization
mechanism.
To illustrate the information assigned high im-
portance by different models, consider a lec-
ture discussing advantages of fish farming. The
top-scoring Good bigrams are topical expressions
(fish farming), functional bigrams around fish and
farming,
4
aspects of content dealt with at length
in the lecture (wild fish, commercial fishing), bi-
grams referencing some of the claims ? fish con-
taining less fat and being used for fish meal. In
addition, this model picks out some sequences of
function words and punctuation (of the, are not,
?, and?, ?, the?) that suggest that better essays
tend to give more detail (hence have more com-
plex noun phrases and coordinated constructions)
and to draw contrast.
For the bigram GoodVsBad model, the topi-
cal bigram fish farming is not in the top 20 bi-
4
such as that fish, of fish, farming is, ?, fish?
grams. Although some bigrams are shared with
the Good model, the GoodVsBad model selects
additional details about the claims, such as the
contrast between inedible fish and edible fish that
is eaten by humans, as well as reference to chemi-
cals used in farming and to the claim that wild fish
are already endangered by other practices.
The most important bigrams according to the
LectVsRead model include functional bigrams
around fish and farming, functional sequences
(that the, is a), as well as commercial fishing and
edible fish. Also selected are functional bigrams
around consumption and species, hinting, indi-
rectly, at the edibility differences between species.
Finally, this model selects almost all bigrams in
the reading passage makes, the reading makes
claims that and the reading says. While distin-
guishing the lecture from the reading, these do not
capture topic-relevant content of the lecture.
The GoodVsBad unigram model selects poul-
try, endangered, edible, chemicals among its top 6
unigrams,
5
effectively touching upon the connec-
tion with other farm-raised foods (poultry, chemi-
cals), with wild fish (endangered) and with human
benefit (edible) that are made in the lecture.
7 Related work
Modern essay scoring systems are complex and
cover various aspects of the writing construct,
such as grammar, organization, vocabulary (Sher-
mis and Burstein, 2013). The quality of content
is often addressed by features that quantify the
similarity between the vocabulary used in an es-
say and reference essays from given score points
(Attali and Burstein, 2006; Foltz et al, 2013; At-
tali, 2011). For example, Attali (2011) proposed a
measure of differential use of words in higher and
lower scoring essays defined similarly to Good-
VsBad, without, however, considering the source
text at all. Such features can be thought of as con-
tent quality features, as they implicitly assume that
writers of better essays use better content. How-
ever, there are various kinds of better content, only
one of them being selection of important informa-
tion from the source; other elements of content
originate with the writer, such as examples, dis-
course markers, evaluations, introduction and con-
clusion, etc. Our approach allows focusing on a
particular aspect of content quality, namely, selec-
tion of appropriate materials from the source.
5
the other two being fishing and used.
250
Our results are related to the findings of Gure-
vich and Deane (2007) who studied the difference
between the reading and the lecture in their im-
pact on essay scores for this test. Using data from
a single prompt, they showed that the difference
between the essay?s average cosine similarity to
the reading and its average cosine similarity to the
lecture is predictive of the score for non-native
speakers of English, thus using a model similar
to LectVsRead, although they took all lecture,
reading, and essay words into account, in contrast
to our model that looks only at n-grams that ap-
pear in the lecture. Our study shows that the ef-
fectiveness of lecture-reading contrast models for
essay scoring generalizes to a large set of prompts.
Similarly, Evanini et al (2013) found that over-
lap with material that is unique to the lecture (not
shared with the reading) was predictive of scores
in a spoken source-based question answering task.
In the vast literature on summarization, our
work is closest to Hong and Nenkova (2014) who
studied models of word importance for multi-
document summarization of news. The Prob, Po-
sition, and Good models are inspired by their
findings of the effectiveness of similar models in
their setting. We found that, in our setting, Prob
and Good models performed worse than assigning
a uniform weight to all words. We note, however,
that models from Hong and Nenkova (2014) are
not strictly comparable, since their word proba-
bility models were calculated after stopword ex-
clusion, and their model that inspired our Good
model was defined somewhat differently and val-
idated using content words only. The defini-
tion of our Position model and its use in the es-
say scoring function S (equation 2) correspond to
Hong and Nenkova (2014) average first location
model for scoring summaries. Differently from
their findings, this model is not effective for sin-
gle words in our setting. Position models over n-
grams with n > 1 are effective, but their predic-
tion is in the opposite direction of that found for
the news data ? the more important materials tend
to appear later in the lecture, as indicated by the
positive r between average first position and essay
score. These findings underscore the importance
of paying attention to the genre of the source ma-
terial when developing summarization systems.
Our summarization task incorporates elements
of contrastive opinion summarization (Paul et al,
2010; Kim and Zhai, 2009), since the lecture and
the reading sometimes interpret the same facts in
a positive or negative light (for example, the fact
that chemicals are used in fish farms is negative
if compared to wild fish, but not so if compared
to other farm-raised foods like poultry). Relation-
ships between aspect and sentiment (Brody and
Elhadad, 2010; Lazaridou et al, 2013) are also
relevant, since aspects of the same fact are em-
phasized with different evaluations (the quantity
vs the variety of species that go into fish meal for
farmed fish). We hypothesize that units participat-
ing in sentiment and aspect contrasts are of higher
importance; this is a direction for future work.
8 Conclusion
In this paper, we addressed the task of automati-
cally assigning importance scores to parts of a lec-
ture that is to be summarized as part of an English
language proficiency test. We investigated the op-
timal units of information to which importance
should be assigned, as well as a variety of impor-
tance scoring models, drawing on the news sum-
marization and essay scoring literature.
We found that bigrams and trigrams were ge-
nerally more effective than unigrams and 4-grams
across importance models, with some exceptions.
We also found that the most effective impor-
tance models are those that equate importance
of an n-gram with its preferential use in higher-
scoring essays than in lower-scoring ones, above
and beyond merely looking at the n-grams used in
good essays. This demonstrates the utility of using
not only gold, high-quality human summaries, but
also sub-standard ones when developing content
importance models.
Additional importance criteria that are intrinsic
to the lecture, as well as those that capture contrast
with a different source discussing the same topic,
were also found to be reasonably effective. Since
different importance models often select different
items as most important, we intend to investigate
complementarity of the different models.
Finally, our results highlight that the effective-
ness of an importance model depends on the genre
of the source text. Thus, while a first sentence
baseline is very competitive in news summariza-
tion, we found that important information tends
not to be located in the opening sentences in our
data (these tend to provide general, introductory
information), but appears later on, when more de-
tailed, specific claims are put forward.
251
References
Yigal Attali and Jill Burstein. 2006. Automated Essay
Scoring With e-rater
R
?V.2. Journal of Technology,
Learning, and Assessment, 4(3).
Yigal Attali. 2011. A Differential Word Use Measure
for Content Analysis in Automated Essay Scoring.
ETS Research Report, RR-11-36.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 804?812, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jill Burstein, Michael Flor, Joel Tetreault, Nitin Mad-
nani, and Steven Holtzman. 2012. Examining Lin-
guistic Characteristics of Paraphrase in Test-Taker
Summaries. ETS Research Report, RR-12-18.
Keelan Evanini, Shasha Xie, and Klaus Zechner. 2013.
Prompt-based content scoring for automated spoken
language assessment. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 157?162, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Peter Foltz, Lynn Streeter, Karen Lochbaum, and
Thomas Landauer. 2013. Implementation and Ap-
plication of the Intelligent Essay Assessor. In Mark
Shermis and Jill Burstein, editors, Handbook of au-
tomated essay evaluation: Current applications and
new directions, pages 68?88. New York: Routh-
ledge.
Olga Gurevich and Paul Deane. 2007. Document
similarity measures to distinguish native vs. non-
native essay writers. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Companion Volume, Short Papers, pages
49?52, Rochester, New York, April. Association for
Computational Linguistics.
Kai Hong and Ani Nenkova. 2014. Improving
the estimation of word importance for news multi-
document summarization. In The Conference of the
European Chapter of the Association for Computa-
tional Linguistics, Gottenberg, Sweden, April. As-
sociation for Computational Linguistics.
Hyun Duk Kim and ChengXiang Zhai. 2009. Gener-
ating comparative summaries of contradictory opin-
ions in text. In Proceedings of the 18th ACM Confer-
ence on Information and Knowledge Management,
CIKM ?09, pages 385?394, New York, NY, USA.
ACM.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1630?1639, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Proceedings of
ACL workshop: Text summarization branches out,
pages 74?81, Barcelona, Spain, July. Association for
Computational Linguistics.
Quinn McNemar. 1955. Psychological Statistics. New
York: J. Wiley and Sons, 2nd edition.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Human Language Technologies
2004: The Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 145?152, Boston, Massachusetts, USA, May
2 - May 7. Association for Computational Linguis-
tics.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 66?76, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Lia Plakans and Atta Gebril. 2013. Using multiple
texts in an integrated writing assessment: Source
text use as a predictor of score. Journal of Second
Language Writing, 22:217?230.
Mark Shermis and Jill Burstein, editors. 2013. Hand-
book of Automated Essay Evaluation: Current Ap-
plications and Future Directions. New York: Rout-
ledge.
252
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 390?396,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Difficult Cases: From Data to Learning, and Back
Beata Beigman Klebanov
?
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541
bbeigmanklebanov@ets.org
Eyal Beigman
?
Liquidnet Holdings Inc.
498 Seventh Avenue
New York, NY 10018
e.beigman@gmail.com
Abstract
This article contributes to the ongoing dis-
cussion in the computational linguistics
community regarding instances that are
difficult to annotate reliably. Is it worth-
while to identify those? What informa-
tion can be inferred from them regarding
the nature of the task? What should be
done with them when building supervised
machine learning systems? We address
these questions in the context of a sub-
jective semantic task. In this setting, we
show that the presence of such instances
in training data misleads a machine learner
into misclassifying clear-cut cases. We
also show that considering machine lear-
ning outcomes with and without the diffi-
cult cases, it is possible to identify specific
weaknesses of the problem representation.
1 Introduction
The problem of cases that are difficult for anno-
tation received recent attention from both the the-
oretical and the applied perspectives. Such items
might receive contradictory labels, without a clear
way of settling the disagreement. Beigman and
Beigman Klebanov (2009) showed theoretically
that hard cases ? items with unreliable annota-
tions ? can lead to unfair benchmarking results
when found in test data, and, in worst case, to a
degradation in a machi74ne learner?s performance
on easy, uncontroversial instances if found in the
training data. Schwartz et al (2011) provided an
empirical demonstration that the presence of such
difficult cases in dependency parsing evaluations
1
The work presented in this paper was done when the first
author was a post-doctoral fellow at Northwestern University,
Evanston, IL and the second author was a visiting assistant
professor at Washington University, St. Louis, MO.
leads to unstable benchmarking results, as diffe-
rent gold standards might provide conflicting an-
notations for such items. Reidsma and Carletta
(2008) demonstrated by simulation that systema-
tic disagreements between annotators negatively
impact generalization ability of classifiers built
using data from different annotators. Oosten et
al. (2011) showed that judgments of readability
of the same texts by different groups of experts
are sufficiently systematically different to hamper
cross-expert generalization of readability classi-
fiers trained on annotations from different groups.
Rehbein and Ruppenhofer (2011) discuss the ne-
gative impact of systematic simulated annotation
inconsistencies on active learning performance on
a word-sense disambiguation task.
In this paper, we address the task of classify-
ing words in a text as semantically new or old.
Using multiple annotators, we empirically identify
instances that show substantial disagreement be-
tween annotators. We then discuss those both from
the linguistic perspective, identifying some char-
acteristics of such cases, and from the perspec-
tive of machine learning, showing that the pres-
ence of difficult cases in the training data misleads
the machine learner on easy, clear-cut cases ? a
phenomenon termed hard case bias in Beigman
and Beigman Klebanov (2009). The main con-
tribution of this paper is in providing additional
empricial evidence in support of the argument put
forward in the literature regarding the need to pay
attention to problematic, disagreeable instances in
annotated data ? not only from the linguistic per-
spective, but also from a machine learning one.
2 Data
The task considered here is that of classifying first
occurrences of words in a text as semantically old
or new. One of goals of the project is to inves-
tigate the relationship between various kinds of
non-novelty in text, and, in particular, the rela-
390
tionship between semantic non-novelty (conceptu-
alized as semantic association with some preced-
ing word in the text), the information structure in
terms of given and new information, and the cog-
nitive status of discourse entities (Postolache et al,
2005; Birner and Ward, 1998; Gundel et al, 1993;
Prince, 1981). If an annotator identified an asso-
ciative tie from the target word back to some other
word in the text, the target word is thereby classi-
fied as semantically old (class 1, or positive); if no
ties were identified, it is classified as new (class 0,
or negative).
For the project, annotations were collected for
10 texts of various genres, where annotators were
asked, for every first appearance of a word in a
text, to point out previous words in the text that
are semantically or associatively related to it. All
data was annotated by 22 undergraduate and grad-
uate students in various disciplines who were re-
cruited for the task. During outlier analysis, data
from two annotators was excluded from considera-
tion, while 20 annotations were retained. This task
is fairly subjective, with inter-annotator agreement
?=0.45 (Beigman Klebanov and Shamir, 2006).
Table 1 shows the number and proportion of in-
stances that received the ?semantically old? (1) la-
bel from i annotators, for 0? i ? 20. The first col-
umn shows the number of annotators who gave the
label ?semantically old? (1). Column 2 shows the
number and proportion of instances that received
the label 1 from the number of annotators shown in
column 1. Column 3 shows the split into item dif-
ficulty groups. We note that while about 20% of
the instances received a unanimous 0 annotation
and about 12% of the instances received just one 1
label out of 20 annotators, the remaining instances
are spread out across various values of i. Reasons
for this spread include intrinsic difficulty of some
of the items, as well as attention slips. Since anno-
tators need to consider the whole of the preceding
text when annotating a given word, maintaining
focus is a challenge, especially for words that first
appear late in the text.
Our interest being in difficult, disagreeable
cases, we group the instances into 5 bands accor-
ding to the observed level of disagreement and
the tendency in the majority of the annotations.
Thus, items with at most two label 1 annotations
are clearly semantically new, while those with at
least 17 (out of 20) are clearly semantically old.
The groups Hard 0 and Hard 1 contain instances
# 1s # instances group
(proportion)
0 476 (.20) Easy 0
1 271 (.12) (.40)
2 191 (.08)
3 131 (.06) Hard 0
4 106 (.05) (.25)
5 76 (.03)
6 95 (.04)
7 85 (.04)
8 78 (.03)
9 60 (.03) Very
10 70 (.03) Hard
11 60 (.03) (.08)
12 57 (.02) Hard 1
13 63 (.03) (.13)
14 68 (.03)
15 49 (.02)
16 65 (.03)
17 60 (.03) Easy 1
18 72 (.03) (.14)
19 94 (.04)
20 99 (.04)
Table 1: Sizes of subsets by levels of agreement.
with at least a 60% majority classification, while
the middle class ? Very Hard ? contains instances
for which it does not appear possible to even iden-
tify the overall tendency.
In what follows, we investigate the learnabi-
lity of the classification of semantic novelty from
various combinations of easy, hard, and very hard
data.
3 Experimental Setup
3.1 Training Partitions
The objective of the study is to determine the use-
fulness of instances of various types in the training
data for semantic novelty classification. In parti-
cular, in light of Beigman and Beigman Klebanov
(2009), we want to check whether the presence of
less reliable data (hard cases) in the training set
adversely impacts performance on the highly reli-
able data (easy cases). We therefore test separately
on easy and hard cases.
We ran 25 rounds of the following experiment.
All easy cases are randomly split 80% (train) and
20% (test), all hard cases are split into train and
test sets in the same proportions. Then various
391
parts of the training data are used to train the 5 sys-
tems described in Table 2. We build models using
easy data; hard data; easy and hard data; easy,
hard, and very hard data; easy data and a weighted
sample of the hard data. The labels for very hard
data were assigned by flipping a fair coin.
System Easy Hard Very Hard
E +
H +
E+H + +
E+H+VH + + +
E+H
100
w
+ sample
1
Table 2: The 5 training regimes used in the experi-
ment, according to the parts of the data utilized for
training.
3.2 Machine Learning
We use linear Support Vector Machines classifier
as implemented in SVMLight (Joachims, 1999).
Apart from being a popular and powerful ma-
chine learning method, linear SVM is one of the
family of classifiers analyzed in Beigman and
Beigman Klebanov (2009), where they are theo-
retically shown to be vulnerable to hard case bias
in the worst case.
To represent the instances, we use two features
that capture semantic relatedness between words.
One feature uses Latent Semantic Analysis (Deer-
wester et al, 1990) trained on the Wall Street Jour-
nal articles to quantify the distributional similarity
of two words, the other uses an algorithm based
on WordNet (Miller, 1990) to calculate seman-
tic relatedness, combining information from both
the hierarchy and the glosses (Beigman Klebanov,
2006). For each word, we calculate LSA (Word-
Net) relatedness score for this word with each pre-
ceding word in the text, and report the highest pair-
wise score as the LSA (WordNet) feature value for
the given word. The values of the features can
be thought of as quantifying the strength of the
evidence for semantic non-newness that could be
obtained via a distributional or a dictionary-based
method.
1
The weight corresponds to the number of people who
marked the item as 1, for hard cases. We take a weighted
sample of 100 hard cases.
4 Results
We calculate the accuracy of every system sepa-
rately on the easy and hard test data. Table 3 shows
the results.
Train Test-E Test-H
Acc Rank Acc Rank
E 0.781 1 0.643 2
E+H 0.764 2 0.654 1
E+H+VH 0.761 2 0.650 1,2
H 0.620 3 0.626 3
E+H
100
w
0.779 1 0.645 2
Table 3: Accuracy and ranking for semantic no-
velty classification for systems built using various
training data and tested on easy (Test-E) and hard
(Test-H) cases. Systems with insignificant differ-
ences in performance (paired t-test, n=25, p>0.05)
are given the same rank.
We observe first the performance of the system
trained solely on hard cases (H in Table 3). This
system shows the worst performance, both on the
easy test and on the hard test. In fact, this system
failed to learn anything about the positive class in
24 out of the 25 runs, classifying all cases as nega-
tive. It is thus safe to conclude that in the feature
space used here the supervision signal in the hard
cases is too weak to guide learning.
The system trained solely on easy cases (E in
Table 3) significantly outperforms H both on the
easy and on the hard test. That is, easy cases are
more informative about the classification of hard
cases than the hard cases themselves. This shows
that at least some hard cases pattern similarly to
the easy ones in the feature space; SVM failed to
single them out when trained on hard cases alone,
but they are learnable from the easy data.
The system that trained on all cases ? both easy
and hard ? attains the best performance on hard
cases but yields to E on the easy test (Test-E). This
demonstrates what Beigman and Beigman Kle-
banov (2009) called hard case bias ? degradation
in test performance on easy cases due to hard cases
in the training data. The negative effect of using
hard cases in training data can be mitigated if we
only use a small sample of them (system E+H
100
w
);
yet neither this nor other schemes we tried of
selectively incorporating hard cases into training
data produced an improvement over E when tested
on easy cases (Test-E).
392
5 Discussion
5.1 Beyond worst case
Beigman and Beigman Klebanov (2009) per-
formed a theoretical analysis showing that hard
cases could lead to hard case bias where hard cases
have completely un-informative labels, with pro-
bability of p=0.5 for either label. These corre-
spond to very hard cases in our setting. According
to Table 3, it is indeed the case that adding the
very hard cases hurts performance, but not signif-
icantly so ? compare results for E+H vs E+H+VH
systems.
Our results suggest that un-informative labels
are not necessary for the hard case bias to sur-
face. The instances grouped under Hard 1 have
the probability of p=0.66 for class 1 and the in-
stances grouped under Hard 0 have the probabi-
lity of p=0.71 for class 0. Thus, while the labels
are somewhat informative, it is apparently the case
that the hard instances are distributed sufficiently
differently in the feature space from the easy cases
with the same label to produce a hard case bias.
Inspecting the distribution of hard cases (Fig-
ure 1), we note that hard cases do not follow
the worst case pattern analyzed in Beigman and
Beigman Klebanov (2009), where they were con-
centrated in an area of the feature space that was
removed far from the separation plane, a malig-
nant but arguably unlikely scenario (Dligach et al,
2010). Here, hard cases are spread both close and
far from the plane, yet their distribution is suffi-
ciently different from that of the easy cases to pro-
duce hard case bias during learning.Hard cases
00.10.2
0.30.40.5
0.60.70.8
0 0.2 0.4 0.6 0.8 1LSA score
WordNet score  
                
Easy Separator Hard "-"Easy+Hard Separator Hard "+"
Figure 1: Hard cases with separators learned from
easy and easy+hard training data.
5.2 The nature of hard cases
Figure 1 plots the hard instances in the two-
dimensional feature space: Latent Semantic Anal-
ysis score is shown on x-axis, and WordNet-based
score is shown on the y-axis. The red lines show
the linear separator induced when the system is
trained on easy cases only (system E in Table 3),
whereas the green line shows the separator in-
duced when the system is trained on both easy and
hard cases (system E+H).
It is apparent from the figure that the difference
in the distributions of the easy and the hard cases
lead to a lower threshold for LSA score when
WordNet score is zero and a higher threshold of
WordNet score when LSA score is zero in hard
vs easy cases. That is, the system exposed to hard
cases learned to trust LSA more and to trust Word-
Net less when determining that an instance is se-
mantically old than a system that saw only easy
cases at train time.
The tendency to trust WordNet less yields an
improvement in precision (92.1% for system E+H
on Test-E class 1 data vs 84% for system E on
Test-E class 1 data), which comes at a cost of a
drop in recall (42.2% vs 53.3%) on easy positive
cases. This suggests that high WordNet scores that
are not supported by distributional evidence are a
source of Hard 0 cases that made the system more
cautious when relying on WordNet scores.
The pattern of low LSA score and high Word-
Net score often obtains for rare senses of words:
Distributional evidence typically points away from
these senses, but they can be recovered through
dictionary definitions (glosses) in WordNet.
An example of hard 0 case involves a homony-
mous rare sense. Deck is used in the observation
deck sense in one of the texts. However, it was
found to be highly related to buy by WordNet-
based measure through the notion of illegal ? buy
in the sense of bribe and deck in the sense of a
packet of illegal drugs. This is clearly a spuri-
ous connection that makes deck appear semanti-
cally associated with preceding material, whereas
annotators largely perceived it as new.
Exposure to such cases at training time leads the
system to forgo handling rare senses that lack dis-
tributional evidence, thus leading to misclassifica-
tion of easy positive cases that exhibit a similar
pattern. Thus, stall and market are both used in the
sales outlet sense in one of the text. They come out
highly related by WordNet measure; yet in the 68
393
instances of stall in the training data for LSA the
homonymous verbal usage predominates. Simi-
larly, partner is overwhelmingly used in the busi-
ness partner sense in the WSJ data, hence wife and
partner come out distributionally unrelated, while
the WordNet based measure successfully recovers
these connections.
Our features, while rich enough to diagnose
a rare sense (low LSA score and high WordNet
score), do not provide information regarding the
appropriateness of the rare sense in context. Short
of full scale word sense disambiguation, we expe-
rimented with the idea of taking the second highest
pairwise score as the value of the WordNet fea-
ture, under the assumption that an appropriate rare
sense is likely to be related to multiple words in
the preceding text, while a spurious rare sense is
less likely to be accidentally related to more than
one preceding word. We failed to improve per-
formance, however; it is thus left for future work
to enrich the representation of the problem so that
cases with inappropriate rare senses can be differ-
entiated from the appropriate ones. In the context
of the current article, the identification of a parti-
cular weakness in the representation is an added
value of the analysis of the machine learning per-
formance with and without the difficult cases.
6 Related Work
Reliability of annotation is a concern widely
discussed in the computational linguistics litera-
ture (Bayerl and Paul, 2011; Beigman Klebanov
and Beigman, 2009; Artstein and Poesio, 2008;
Craggs and McGee Wood, 2005; Di Eugenio and
Glass, 2004; Carletta, 1996). Ensuring high re-
liability is not always feasible, however; the ad-
vent of crowdsourcing brought about interest in
algorithms for recovering from noisy annotations:
Snow et al (2008), Passonneau and Carpenter
(2013) and Raykar et al (2010) discuss methods
for improving over annotator majority vote when
estimating the ground truth from multiple noisy
annotations.
A situation where learning from a small num-
ber of carefully chosen examples leads to a better
performance in classifiers is discussed in the ac-
tive learning literature (Schohn and Cohn, 2000;
Cebron and Berthold, 2009; Nguyen and Smeul-
ders, 2004; Tong and Koller, 2001). Recent work
in the proactive active learning and multi-expert
active learning paradigms incorporates considera-
tions of item difficulty and annotator expertise into
an active learning scheme (Wallace et al, 2011;
Donmez and Carbonell, 2008).
In information retrieval, one line of work con-
cerns the design of evaluation schemes that reflect
different levels of document relevance to a given
query (Kanoulas and Aslam, 2009; Sakai, 2007;
Kek?al?ainen, 2005; Sormunen, 2002; Voorhees,
2001; J?arvelin and Kek?al?ainen, 2000; Voorhees,
2000). J?arvelin and Kek?al?ainen (2000) consider,
for example, a tiered evaluation scheme, where
precision and recall are reported separately for ev-
ery level of relevance, which is quite analogous
to the idea of testing separately on easy and hard
cases as employed here. The graded notion of
relevance addressed in the information retrieval
research assumes a coding scheme where people
assign documents into one of the relevance tiers
(Kek?al?ainen, 2005; Sormunen, 2002). In our case,
the graded notion of semantic novelty is a possible
explanation for the observed pattern of annotator
responses.
7 Conclusion
This article contributes to the ongoing discussion
in the computational linguistics community re-
garding instances that are difficult to annotate re-
liably ? how to identify those, and what to do
with them once identified. We addressed this is-
sue in the context of a subjective semantic task.
In this setting, we showed that the presence of
difficult instances in training data misleads a ma-
chine learner into misclassifying clear-cut, easy
cases. We also showed that considering machine
learning outcomes with and without the difficult
cases, it is possible to identify specific weaknesses
of the problem representation. Our results align
with the literature suggesting that difficult cases
in training data can be disruptive (Beigman and
Beigman Klebanov, 2009; Schwartz et al, 2011;
Rehbein and Ruppenhofer, 2011; Reidsma and
Carletta, 2008); yet we also show that investigat-
ing their impact on the learning outcomes in some
detail can provide insight about the task at hand.
The main contribution of this paper is there-
fore in providing additional empirical evidence in
support of the argument put forward in the litera-
ture regarding the need to pay attention to prob-
lematic, disagreeable instances in annotated data
? both from the linguistic and from the machine
learning perspectives.
394
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Petra Saskia Bayerl and Karsten Ingmar Paul. 2011.
What determines inter-coder agreement in manual
annotations? a meta-analytic investigation. Comput.
Linguist., 37(4):699?725, December.
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with Annotation Noise. In Proceedings
of the 47th Annual Meeting of the Association for
Computational Linguistics, pages 280?287, Singa-
pore, August.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From Annotator Agreement to Noise Models. Com-
putational Linguistics, 35(4):493?503.
Beata Beigman Klebanov and Eli Shamir. 2006.
Reader-based exploration of lexical cohesion. Lan-
guage Resources and Evaluation, 40(2):109?126.
Beata Beigman Klebanov. 2006. Measuring Seman-
tic Relatedness Using People and WordNet. In Pro-
ceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Short
Papers, pages 13?16, New York City, USA, June.
Association for Computational Linguistics.
Betty Birner and Gregory Ward. 1998. Information
Status and Non-canonical Word Order in English.
Amsterdam/Philadelphia: John Benjamins.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Nicolas Cebron and Michael Berthold. 2009. Active
learning for object classification: From exploration
to exploitation. Data Mining and Knowledge Dis-
covery, 18:283?299.
Richard. Craggs and Mary McGee Wood. 2005. Eval-
uating Discourse and Dialogue Coding Schemes.
Computational Linguistics, 31(3):289?296.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal
of the American Society For Information Science,
41:391?407.
Barbara Di Eugenio and Michael Glass. 2004. The
kappa statistic: a second look. Computational Lin-
guistics, 30(1):95?101.
Dmitriy Dligach, Rodney Nielsen, and Martha Palmer.
2010. To Annotate More Accurately or to Annotate
More. In Proceedings of the 4th Linguistic Annota-
tion Workshop, pages 64?72, Uppsala, Sweden, July.
Pinar Donmez and Jaime G. Carbonell. 2008. Proac-
tive learning: Cost-sensitive active learning with
multiple imperfect oracles. In Proceedings of the
17th ACM Conference on Information and Knowl-
edge Management, CIKM ?08, pages 619?628, New
York, NY, USA. ACM.
Jeanette Gundel, Nancy Hedberg, and Ron Zacharski.
1993. Cognitive status and the form of referring ex-
pressions in discourse. Language, 69:274?307.
Kalervo J?arvelin and Jaana Kek?al?ainen. 2000. IR
Evaluation Methods for Retrieving Highly Relevant
Documents. In Proceedings of the 23th Annual In-
ternational Conference on Research and Develop-
ment in Information Retrieval, pages 41?48, Athens,
Greece, July.
Thorsten Joachims. 1999. Advances in Kernel
Methods - Support Vector Learning. In Bern-
hard Schlkopf, Christopher Burges, and Alexander
Smola, editors, Making large-scale SVM learning
practical, pages 169?184. MIT Press.
Evangelos Kanoulas and Javed Aslam. 2009. Empir-
ical Justification of the Gain and Discount Function
for nDCG . In Proceedings of the 19th ACM Confer-
ence on Information and Knowledge Management,
pages 611?620, Hong Kong, November.
Jaana Kek?al?ainen. 2005. Binary and graded relevance
in IR evaluations ? Comparison of the effects on
ranking of IR systems. Information Processing and
Management, 41:1019?1033.
George Miller. 1990. WordNet: An on-line lexical
database. International Journal of Lexicography,
3(4):235?312.
Hieu Nguyen and Arnold Smeulders. 2004. Ac-
tive Learning Using Pre-clustering. In Proceedings
of 21st International Conference on Machine Lear-
ning, pages 623?630, Banff, Canada, July.
Philip Oosten, Vronique Hoste, and Dries Tanghe.
2011. A posteriori agreement as a quality mea-
sure for readability prediction systems. In Alexan-
der Gelbukh, editor, Computational Linguistics and
Intelligent Text Processing, volume 6609 of Lec-
ture Notes in Computer Science, pages 424?435.
Springer Berlin Heidelberg.
Rebecca J. Passonneau and Bob Carpenter. 2013. The
benefits of a model of annotation. In Proceedings of
the 7th Linguistic Annotation Workshop and Interop-
erability with Discourse, pages 187?195, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Oana Postolache, Ivana Kruijff-Korbayova, and Geert-
Jan Kruijff. 2005. Data-driven approaches for in-
formation structure identification. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 9?16, Vancouver, British
Columbia, Canada, October.
395
Ellen Prince. 1981. Toward a taxonomy of given-new
information. In Peter Cole, editor, Radical Prag-
matics, pages 223?255. Academic Press.
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, and Linda Moy. 2010. Learning from crowds.
J. Mach. Learn. Res., 11:1297?1322, August.
Ines Rehbein and Josef Ruppenhofer. 2011. Evaluat-
ing the impact of coder errors on active learning. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages 43?
51, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Dennis Reidsma and Jean Carletta. 2008. Reliability
Measurement without Limits. Computational Lin-
guistics, 34(3):319?326.
Tetsuya Sakai. 2007. On the reliability of information
retrieval metrics based on graded relevance. Infor-
mation Processing and Management, 43:531?548.
Greg Schohn and David Cohn. 2000. Less is more:
Active Learning with Support Vector Mfachines. In
Proceedings of 17th International Conference on
Machine Learning, pages 839?846, San Francisco,
July.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 663?672, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: Evaluating non-expert annotations for nat-
ural language tasks. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 254?263, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Eero Sormunen. 2002. Liberal relevance criteria of
TREC ? Counting on negligible documents? In
Proceedings of the 25th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval, pages 324?330, Tampere,
Finland, August.
Simon Tong and Daphne Koller. 2001. Support Vec-
tor Machine active learning with applications to text
classification. Journal of Machine Learning Re-
search, 2:45?66.
Ellen Voorhees. 2000. Variations in relevance judge-
ments and the measurement of retrieval effective-
ness. Information Processing and Management,
36:697?716.
Ellen Voorhees. 2001. Evaluation by highly relevant
documents. In Proceedings of the 24th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 74?82,
New Orleans, LA, USA, September.
B. Wallace, K. Small, C. Brodley, and T. Trikalinos,
2011. Who Should Label What? Instance Alloca-
tion in Multiple Expert Active Learning, chapter 16,
pages 176?187.
396
Transactions of the Association for Computational Linguistics, 1 (2013) 99?110. Action Editor: Chris Callison-Burch.
Submitted 12/2012; Published 5/2013. c?2013 Association for Computational Linguistics.
Using Pivot-Based Paraphrasing and Sentiment Profiles to Improve a
Subjectivity Lexicon for Essay Data
Beata Beigman Klebanov, Nitin Madnani, Jill Burstein
Educational Testing Service
660 Rosedale Road, Princeton, NJ 08541, USA
{bbeigmanklebanov,nmadnani,jburstein@ets.org}
Abstract
We demonstrate a method of improving a seed
sentiment lexicon developed on essay data by
using a pivot-based paraphrasing system for
lexical expansion coupled with sentiment pro-
file enrichment using crowdsourcing. Profile
enrichment alone yields up to 15% improve-
ment in the accuracy of the seed lexicon on 3-
way sentence-level sentiment polarity classifi-
cation of essay data. Using lexical expansion
in addition to sentiment profiles provides a
further 7% improvement in performance. Ad-
ditional experiments show that the proposed
method is also effective with other subjectivity
lexicons and in a different domain of applica-
tion (product reviews).
1 Introduction
In almost any sub-field of computational linguistics,
creation of working systems starts with an invest-
ment in manually-generated or manually-annotated
data for computational exploration. In subjectivity
and sentiment analysis, annotation of training and
testing data and construction of subjectivity lexicons
have been the loci of costly labor investment.
Many subjectivity lexicons are mentioned in the
literature. The two large manually-built lexicons
for English ? the General Inquirer (Stone et al,
1966) and the lexicon provided with the Opinion-
Finder distribution (Wiebe and Riloff, 2005) ? are
available for research and education only1 and un-
der GNU GPL license that disallows their incor-
poration into proprietary materials,2 respectively.
1http://www.wjh.harvard.edu/ inquirer/j1 1/manual/
2http://www.gnu.org/copyleft/gpl.html
Those wishing to integrate sentiment analysis into
products, along with those studying subjectivity in
languages other than English, or for specific do-
mains such as finance, or for particular genres
such as MySpace comments, reported construction
of lexicons (Taboada et al, 2011; Loughran and
McDonald, 2011; Thelwall et al, 2010; Rao and
Ravichandran, 2009; Jijkoun and Hofmann, 2009;
Pitel and Grefenstette, 2008; Mihalcea et al, 2007).
In this paper, we address the step of expanding
a small-scale, manually-built subjectivity lexicon (a
seed lexicon, typically for a domain or language
in question) into a much larger but noisier lexi-
con using an automatic procedure. We present
a novel expansion method using a state-of-the-art
paraphrasing system. The expansion yields a 4-fold
increase in lexicon size; yet, the expansion alone
is insufficient in order to improve performance on
sentence-level sentiment polarity classification.
In this paper we test the following hypothesis.
We suggest that the effectiveness of the expansion
is hampered by (1) introduction of opposite-polarity
items, such as introducing resolute as an expansion
of forceful, or remarkable as an expansion of pecu-
liar; (2) introduction of weakly polar, neutral, or am-
biguous words as expansions of polar seed words,
such as generating concern as an expansion of anx-
iety or future as an expansion of aftermath;3 (3) in-
ability to distinguish between stronger or clear-cut
versus weaker or ambiguous sentiment and to make
a differential use of those.
We address items (1) and (2) by enriching the lexi-
con with sentiment profiles (section 3), and propose
3Table 2 and Figure 1 provide support to these assessments.
99
a way of effectively utilizing this information for
the sentence-level sentiment polarity classification
task (sections 5 and 6). Profile-enrichment alone
yields up to 15% increase in performance for the
seed lexicon when using different machine learning
algorithms; paraphraser-based expansion with sen-
timent profiles improves performance by an addi-
tional 7%. Overall, we observe an improvement of
up to 25% in classification accuracy over the seed
lexicon without profiles.
In section 7, we present comparative evaluations,
demonstrating the competitiveness of the expanded
and profile-enriched lexicon, as well as the effective-
ness of the expansion and enrichment paradigm pre-
sented here for different subjectivity lexicons, dif-
ferent lexical expansion methods, and in a different
domain of application (product reviews).
2 Building Subjectivity Lexicons
The goal of our sentiment analysis project is to allow
for the identification of sentiment in sentences that
appear in essay responses to a variety of tasks de-
signed to test English proficiency in both native- and
non-native-speaker populations in a standardized as-
sessment as well as in an instructional settings. In
order to allow for the future use of the sentiment
analyzer in a proprietory product and to ensure its fit
to the test-taker essay domain, we began our work
with the construction of a seed lexicon relying on
our materials (section 2.1). We then used a statisti-
cal paraphrasing system to expand the seed lexicon
(section 2.2).
2.1 Seed Lexicon
In order to inform the process of lexicon construc-
tion, we randomly sampled 5,000 essays from a cor-
pus of about 100,000 essays containing writing sam-
ples across many topics. Essays were responses
to several different writing assignments, including
graduate school entrance exams, non-native English
speaker proficiency exams, and professional licen-
sure exams. Our seed lexicon is a combination of
(1) positive and negative sentiment words manually
selected from a full list of word types in these data,
and (2) words marked in a small-scale annotation of
a sample of sentences from these data for all posi-
tive and negative words. A more detailed descrip-
tion of the construction of seed lexicon can be found
in Beigman Klebanov et al(2012). The seed lexi-
con contains 749 single words, 406 positive and 343
negative.
2.2 Expanded Lexicon
We used a pivot-based lexical and phrasal para-
phrase generation system (Madnani and Dorr, 2013).
The paraphraser implements the pivot-based method
as described by Bannard and Callison-Burch (2005)
with several additional filtering mechanisms to in-
crease the precision of the extracted pairs. The
pivot-based method utilizes the inherent monolin-
gual semantic knowledge from bilingual corpora:
We first identify phrasal correspondences between
English and a given foreign language F , then map
from English to English by following translation
units from English to the other language and back.
For example, if the two English phrases e1 and e2
both correspond to the same foreign phrase f , then
they may be considered to be paraphrases of each
other with the following probability:
p(e1|e2) ? p(e1|f)p(f |e2)
If there are several pivot phrases that link the two
English phrases, then they are all used in computing
the probability:
p(e1|e2) ?
?
f ?
p(e1|f ?)p(f ?|e2)
Seed Expansion Seed Expansion
abuse exploitation costly onerous
accuse reproach dangerous unsafe
anxiety disquiet improve reinforce
conflict crisis invaluable precious
Table 1: Examples of paraphraser expansions.
Some examples of expansions generated by the
paraphraser are shown in Table 1. More details
about this kind of approach can be found in Ban-
nard and Callison-Burch (2005). We use the French-
English parallel corpus (approximately 1.2 million
sentences) from the corpus of European parliamen-
tary proceedings (Koehn, 2005) as the data on which
pivoting is performed to extract the paraphrases.
However, the base paraphrase system is susceptible
100
to large amounts of noise due to the imperfect bilin-
gual word alignments. Therefore, we implement ad-
ditional heuristics in order to minimize the num-
ber of noisy paraphrase pairs (Madnani and Dorr,
2013). For example, one such heuristic filters out
pairs where a function word may have been inferred
as a paraphrase of a content word. For the lexicon
expansion experiment reported here, we use the top
15 single-word paraphrases for every word from the
seed lexicon, excluding morphological variants of
the seed word. This process results in an expanded
lexicon of 2,994 different words, 1,666 positive and
1,761 negative (433 words are in both the positive
and the negative lists). The expanded lexicon in-
cludes the seed lexicon.
3 Inducing sentiment profiles
Let ?w be the sentiment profile of the word w.
?w = (pposw , pnegw , pneuw ) (1)
where ?i?{pos,neg,neu} piw = 1. Thus, a sentiment
profile of a word is essentially a 3-sided coin, cor-
responding to its probability of coming out positive,
negative, and neutral, respectively.
3.1 Estimating sentiment profiles
Our goal is to estimate the profile using outcomes of
multiple trials as follows. For every word, a person
is shown the word and asked whether it is positive,
negative, or neutral. A person?s decision is modeled
as flipping the coin corresponding to the word, and
recording the outcome ? positive, negative, or neu-
tral. We run N=20 such trials for every word in the
expanded lexicon using the CrowdFlower crowd-
sourcing site,4 for a total cost of $800. We use maxi-
mum likelihood estimate of sentiment profile:
p?iw = niw (2)
where niw is the proportion ofN trials on the wordw
that fell in cell i ? {pos, neg, neu}. Table 2 shows
some estimated profiles.
Following Goodman (1965) and Quesenberry and
Hurst (1964), we calculate confidence intervals for
the parameters piw:
(p?iw)? = (B + 2niw ? T )/(2(N +B)) (3)
4www.crowdflower.com
Word p?posw p?neuw p?negw
forceful 0 0.15 0.85
resolute 0.8 0.15 0.05
peculiar 0.05 0.15 0.8
remarkable 1 0 0
anxiety 0 0 1
concern 0.25 0.4 0.35
absurd 0 0 1
laughable 0.5 0.05 0.45
deadly 0 0 1
fateful 0.25 0.45 0.3
consequence 0.05 0.15 0.8
outcome 0.15 0.85 0
Table 2: Examples of estimated sentiment profiles.
Words in gray are expansions generated from words in
the preceding row; note the difference in the profiles.
(p?iw)+ = (B + 2niw + T )/(2(N +B)) (4)
where
T =
?
B[B + 4niw(N ? niw)/N ]) (5)
For confidence ? that all piw, i ? {pos, neg, neu}
are simultaneously within their respective intervals,
the value of B is determined as the upper?/3?100th
percentile of the ?2 distribution with one degree of
freedom. We use ?=0.1, resulting in B=4.55. The
resulting interval is about 0.2 around the estimated
value when p?iw is close to 0.5, and somewhat nar-
rower for p?iw closer to 0 or 1. We will use this infor-
mation when inducing features from the profiles.
3.2 Sentiment distributions of the lexicons
The estimated sentiment profiles per word allow us
to visualize the distributions of the two lexicons. In
Figure 1, we plot the number of entries in the lexi-
con as a function of the difference in positive and
negative parts of the profile, in 0.2-wide bins. Thus,
a word w would be in the second-leftmost bin if
?0.8 < (p?posw ? p?negw ) < ?0.6.
While the expansion process more than doubles
the number of words in the highest bins for both
the positive and the negative polarity, it clearly
introduces a large number of words in the low-
and medium bins into the lexicon. It is in this
sense that the expansion process is noisy; appa-
rently, seed words with clear and strong polarity
101
10
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
2.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
3.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
4.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
5.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
6.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
7.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
8.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
9.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
10.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
11.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
12.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
13.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
14.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
2.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
3.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
4.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
5.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
6.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
7.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
8.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
9.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
10.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
11.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
12.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
13.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
14.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
Figure 1: Sentiment distributions for the seed (left) and
the expanded (right) lexicons.
are often expanded into low intensity, neutral, or
ambiguous ones, as in pairs like absurd/laughable,
deadly/fateful, anxiety/concern shown in Table 2.
4 Related Work
The most popular seed expansion methods discussed
in the literature are based on WordNet (Miller,
1995) or another lexicographic resource, on dis-
tributional similarity with the seeds, or on a mix-
ture thereof (Cruz et al, 2011; Baccianella et al,
2010; Velikovich et al, 2010; Qiu et al, 2009; Mo-
hammad et al, 2009; Esuli and Sebastiani, 2006;
Kim and Hovy, 2004; Andreevskaia and Bergler,
2006; Hu and Liu, 2004; Kanayama and Nasukawa,
2006; Strapparava and Valitutti, 2004; Kamps et al,
2004; Takamura et al, 2005; Turney and Littman,
2003; Hatzivassiloglou and McKeown, 1997). The
paraphrase-based expansion method is in the dis-
tributional similarity camp; we also experimented
with WordNet-based expansion as descibed in sec-
tion 7.2.
The task of assigning sentiment profiles to words
in a sentiment lexicon has been addressed in the lite-
rature. SentiWordNet assigns profiles to all words in
WordNet based on a propagation algorithm from a
small seed set manually annotated by a small num-
ber of judges (Baccianella et al, 2010; Cerini et al,
2007). Andreevskaia and Bergler (2006) use graph
propagation algorithms on WordNet to assign cen-
trality scores in positive and negative categories; a
similar approach based on web-scale co-occurrence
graphs is discussed in Velikovich et al(2010). Thel-
wall et al(2010) manually annotated a set of words
for strength of sentiment and used machine learning
to fine-tune it. Taboada et al(2011) produced an
expert annotation of their lexicon with strength of
sentiment. Subasic and Huettner (2001) manually
built an affect lexicon with intensities. Wiebe and
Riloff (2005) classifed lexicon entries into weakly
and strongly subjective, based on their relative fre-
quency of appearance in subjective versus objective
contexts in a large annotated dataset.
Our sentiment profiles are best thought of as
relatively fine-grained priors for the sentiment ex-
pressed by a given word out-of-context. These re-
flect a mixture of strength of sentiment (p?posgood >
p?posdecent), contextual ambiguity (concern can be in-terpreted as similar to worry or to care, as in ?Her
condition was causing concern? versus ?He showed
genuine concern for her?), and dominance of a po-
lar connotation (abandon is p?neg=1; it has a negative
overtone even if the actual sense is not that of desert
but of vacate, as in ?You must abandon your office?).
To the best of our knowledge, this paper presents
the first attempt to integrate judgements obtained
through crowdsourcing on a large scale into a sen-
timent lexicon, showing the effectiveness of this
lexicon-enrichment procedure for a sentiment clas-
sification task.
5 Using profiles for sentence-level
sentiment polarity classification
To evaluate the usefulness of the lexicons, we use
them to generate features for machine learning sys-
tems, and compare performance on 3-way sentence-
level sentiment polarity classification. To ensure ro-
bustness of the observed trends, we experiment with
a number of machine learning algorithms: SVM
Linear and RBF, Na??ve Bayes, Logistic Regression
(using WEKA (Hall et al, 2009)), and c5.0 Decision
Trees (Quinlan, 1993).5
5.1 Data
We generated the data for training and testing the
machine learning systems as follows. We used our
5available from http://rulequest.com/
102
pool of 100,000 essays to sample a second, non-
overlapping set of 5,000 essays, so that no essay
used for lexicon development appears in this set.
From these essays, we randomly sampled 550 sen-
tences, and submitted them to sentiment polarity an-
notation by two experienced research assistants; 50
double-annotated sentenced showed ?=0.8. TEST
set contains the 43 agreed double-annotated sen-
tences, and additional 238 sampled from the 500
single-annotated sentences, 281 sentence in total.
The category distribution in the TEST set is 46.6%
neutral, 32.4% positive, and 21% negative.
The TRAIN set contains the remaining sentences,
plus positive, negative, and neutral sentences anno-
tated during lexicon development, for the total of
1,631 sentences. The category distribution in TRAIN
is 39% neutral, 35% positive, 26% negative.
5.2 From lexicons to features
Our goal is to evaluate the impact of sentiment pro-
files on sentence-level sentiment polarity classifica-
tion for the seed and the expanded lexicons, while
also looking for the most effective ways to represent
this information for machine learners.
We implement two baseline systems. One pro-
vides the machine learner with the most detailed in-
formation contained in a lexicon: BL-full has 2 fea-
tures for every lexicon word, taking the values (1,0)
for positive match in a sentence, (0,1) ? for negative,
(1,1) for a word in both positive and negative parts
of the lexicon, and (0,0) otherwise.
The second baseline provides the machine learner
with only summary information about the overall
sentiment of the sentence. BL-sum uses only 2 fea-
tures: (1) the total count of positive words in the
sentence; (2) the total count of negative words in the
sentence, according to the given lexicon.
For the sentiment-enriched runs, we construct a
number of representations: Int-full, Int-sum, Int-
bin, and Int-c. Int-full and Int-sum are parallel to
the respective baseline systems. Int-full represents
each lexicon word as 2 features corresponding to the
word?s estimated p?posw and p?negw , providing the most
detailed information to the machine learner. In the
Int-sum condition, we use p?posw and p?negw for every
word to induce 2 features: (1) the sum of positive
probabilities of all words in the sentence; (2) the
sum of negative probabilities for all words in the
sentence, according to the given lexicon.
For Int-bin runs, we use bins of the size of 0.2 ?
half of the maximal confidence interval ? to group
together words with close estimates. We produce
10 features. For positive bins, the 5 features count
the number of words in the sentence that fall in
bini, 1 ? i ? 5, respectively, that is, words with
0.2(i? 1) < p?posw ? 0.2i. Bin 1 also includes words
with p?posw = 0, since these cannot be distinguished
with high confidence from p?posw =0.1. Note that we
do not provide a scale, we merely represent different
ranges with different features. This should allow the
machine learners the flexibility to weight the diffe-
rent bins differently when inducing classifiers.
The Int-c condition represents a coarse-grained
setting. We produce 4 features, two for each pola-
rity: (1) the number of words such that 0 ? p?posw <
0.4; (2) the number of words such that 0.4 ? p?posw ?
1; similarity for the negative polarity.
Table 3 summarizes conditions and features.
Cond. #F Feature Description
BL-full 2|L| (1Lpos?S(w),1Lneg?S(w))
BL-sum 2 f1=|{w : w ? Lpos ? S}|
f2=|{w : w ? Lneg ? S}|
Int-full 2|L| (p?posw , p?negw ) ?w ? A
Int-sum 2 (?w?A p?posw , ?w?A p?negw )
Int-bin 10 f1=|{w ? A : 0 ? p?posw ? 0.2}|
...
f10=|{w ? A : 0.8 < p?negw ? 1}|
Int-c 4 f1=|{w ? A : 0 ? p?posw < 0.4}|
...
f4=|{w ? A : 0.4 ? p?negw ? 1}|
Table 3: Description of conditions. Column 2 shows the
number of features. In column 3: 1 is an indicator func-
tion; L is a lexicon; Lpos is the part of the lexicon con-
taining positive words (same with negatives); S is a sen-
tence for which a feature vector is built; A = L ? S. For
all w ? L ? S in the -full conditions, w is represented
with (0,0).
6 Results
Table 4 shows classification accuracies for 5 ma-
chine learning systems across 6 conditions, for the
seed and the expanded lexicons.
Let BL denote the best-performing baseline (BL-
103
Machine Condition Seed Expanded
Learner
? Majority 0.466 0.466
c5.0 BL-full 0.441 0.498
BL-sum 0.512 0.480
Int-full 0.441 0.498
Int-sum 0.566 0.616
Int-bin 0.587 0.641
Int-c 0.530 0.577
SVM BL-full 0.466 0.466
RBF BL-sum 0.527 0.495
Int-full 0.466 0.466
Int-sum 0.548 0.601
Int-bin 0.573 0.644
Int-c 0.530 0.562
SVM BL-full 0.584 0.566
Linear BL-sum 0.509 0.502
Int-full 0.580 0.609
Int-sum 0.601 0.580
Int-bin 0.573 0.630
Int-c 0.569 0.569
Logistic BL-full 0.545 0.509
Regression BL-sum 0.545 0.509
Int-full 0.534 0.502
Int-sum 0.555 0.584
Int-bin 0.584 0.616
Int-c 0.545 0.577
Na??ve BL-full 0.598 0.584
Bayes BL-sum 0.509 0.473
Int-full 0.598 0.580
Int-sum 0.545 0.605
Int-bin 0.559 0.626
Int-c 0.537 0.601
Table 4: Classification accuracies on TEST set. Majo-
rity baseline corresponds to classifying all sentences as
neutral. The best performance is boldfaced. Let BL
stand for the best-performing baseline (BL-full or BL-
sum) for a combination of machine learner and lexicon.
We use Wilcoxon Signed-Rank test, reporting the num-
ber of signed ranks (N) and the sum of signed ranks (W).
Statistically significant results at p=0.05 are: Int-sum >
BL (N=10, W=43); Int-bin > BL (N=10, W=48); Int-
bin > Int-sum (N=10, W=43); Int-bin > Int-full (N=10,
W=47); Int-sum > Int-full (N=10, W=37); Int-bin > Int-
c (N=10, W=55); Int-sum > Int-c (N=10, W=55); Ex-
panded> Seed under Int condition (includes Int-full, Int-
sum, Int-bin, Int-c) (N=18, W=152, z=3.3). Differences
between Int-full, Int-c, and BL are not significant.
full or BL-sum) for a combination of machine
learner and lexicon. The results show that (1) Int-
bin > Int-sum > BL = Int-c = Int-full; (2) Ex-
panded > Seed under Int condition. All inequalities
are statistically significant at p=0.05 (see caption of
Table 4 for details).
First, both the seed and the expanded lexicons
benefit from profile enrichment, although, as pre-
dicted, the expanded lexicon yields larger gains due
to its more varied profiles: The seed lexicon gains up
to 15% in accuracy (c5.0 BL-sum vs Int-bin), while
the expanded lexicon gains up to 30%, as SVM RBF
scores go up from 0.495 to 0.644.
Second, observe that profiling allows the ex-
panded lexicon to leverage its improved coverage:
While it is inferior to the best baseline run with the
seed lexicon for all systems, it succeeds in impro-
ving the seed lexicon accuracies by 5%-12% across
the different systems for the Int-bin runs. The best
run of the expanded lexicon (Int-bin for SVM RBF)
improves upon the best run of the seed lexicon (Int-
sum for SVM-linear) by 7%, demonstrating the suc-
cess of the paraphraser-based expansion once pro-
files are taken into account. Overall, comparing the
best baseline for the seed lexicon with Int-bin con-
dition of the expanded lexicon, we observe an im-
provement between 5% (0.598 to 0.626 for Na??ve
Bayes) and 25% (0.512 to 0.641 for c5.0), proving
the effectiviness of the paraphrase-based expansion
with profile enrichment paradigm.
Third, representing profiles using 10 bins (Int-bin)
provides a small but consistent improvement over
the summary representation (Int-sum) that sums
positivity and negativity of the sentiment-bearing
words in a sentence, over a coarse-grained represen-
tation (Int-c), as well as over the full-information
representation (Int-full). Even Na??ve Bayes and
SVM linear, known to work well with large feature
sets, show better performance in the Int-bin con-
dition for the expanded lexicon. The results indi-
cate that an intermediate degree of detail ? between
summary-only and coarse-grained representation on
the one hand and full-information representation on
the other ? is the best choice in our setting.
104
7 Comparative Evaluations
In this section, we present comparative evaluations
of the work presented in this paper with respect to
related work. This section shows that the paraphrase
expansion+profile enrichment solution proposed in
this paper is effective for our task beyond off-the-
shelf solutions, and that its effectiveness generalizes
to sentiment analysis in a different domain. We also
show that profile enrichment can be effectively cou-
pled with other methods of lexical expansion, al-
though the paraphraser-based expansion receives a
larger boost in performance from profile enrichment
than the alternative expansion methods we consider.
In section 7.1, we demonstrate that the
paraphrase-based expansion and profile enrich-
ment yield superior performance on our data
relative to state-of-art subjectivity lexicons ? Opin-
ionFinder, General Inquirer, and SentiWordNet.
In section 7.2, we show that profile enrichment
can be effectively coupled with other methods
of lexical expansion, such as a WordNet-based
expansion and an expansion that utilizes Lin?s
distributional thesaurus. However, we find that the
paraphraser-based expansion benefits the most from
profile enrichment, and attains better performance
on our data than the alterantive expansion methods.
In section 7.3, we show that the paraphrase-based
expansion and profile enrichment paradigm is
effective for other subjecitivy lexicons on other
data. We use a dataset of product reviews annotated
for sentence-level positivity and negativity as
new data for evaluation (Hu and Liu, 2004). We
use subsets of OpinionFinder, General Inquirer,
and sentiment lexicon from Hu and Liu (2004).
We demonstrate that paraphrase-based expansion
and profile enrichment improve the accuracy of
sentiment classification of product reviews for
every lexicon and machine learner combination; the
magnitude of improvement is 5% on average.
7.1 Competitiveness of the Expanded Lexicon
Had we been able to use the OpinionFinder or
the General Inquirer lexicons (OFL and GIL) as-
is, how would the results have compared to those
attained using our lexicons? We performed the
baseline runs with both lexicons; OFL accuracies
were 0.544-0.594 across machine learning systems,
GIL?s ? 0.491-0.584 (see GIL column in Table 5).
We also experimented with using the weaksubj
and strongsubj labels in OFL as somewhat parallel
distinctions to the ones presented here (see sec-
tion 4 ? Related Work ? for a more detailed discus-
sion). We used (1,0,0) profile for strong positives,
(0.3,0,0.7) for weak positives, (0,1,0) for strong neg-
atives, and (0,0.3,0.7) for weak negatives, and ran all
the feature representations discussed in section 5.2.
Table 5 column OFL shows the best run for every
machine learning system, across the different feature
representations, and choosing the better performing
run between vanilla OFL and the version enriched
with weak/strong distinctions.
Machine Seed OFL GIL SWN Exp.
Learner BL
c5.0 0.512 0.598 0.491 0.516 0.641
SVM-RBF 0.527 0.594 0.495 0.520 0.644
SVM-lin. 0.584 0.594 0.580 0.569 0.630
Log. Reg. 0.545 0.598 0.541 0.537 0.616
Na??ve B. 0.598 0.573 0.584 0.587 0.626
Table 5: Performance of different lexicons on essay data
using various machine learning systems. For each sys-
tem and lexicon, the best performance across the applica-
ble feature representations from section 5.2 and the vari-
ants (see text) is shown. Seed BL column shows the best
baseline performance of our seed lexicon ? before para-
phraser expansion and profile enrichment were applied.
Exp. column shows the performance of Int-bin feature
representation for the expanded lexicon after profile en-
richment.
Additionally, we experimented with SentiWord-
Net (Baccianella et al, 2010). SentiWordNet is a
resource for opinion mining built on top of Word-
Net, which assigns each synset in WordNet a score
triplet (positive, negative, and objective), indicating
the strength of each of these three properties for the
words in the synset. The SentiWordNet annotations
were automatically generated, starting with a set of
manually labeled synsets. Currently, SentiWordNet
includes an automatic annotation for all the synsets
in WordNet, totaling more than 100,000 words. It
is therefore the largest-scale lexicon with intensity
information that is currently available.
Since SentiWordNet assigns scores to synsets and
since our data is not sense-tagged, we induced Sen-
105
tiWordNet scores in the following ways. We part-
of-speech tagged our train and test data using Stan-
ford tagger (Toutanova et al, 2003). Then, we took
the SentiWordNet scores for the top sense for the
given part-of-speech (SWN-1). In a different vari-
ant, we took a weighted average of the scores for the
different senses, using the weighting algorithm pro-
vided on SentiWordNet website6 (SWN-2). Table 5
column SWN shows the best performance figures
between SWN-1 and SWN-2, across the feature rep-
resentations in section 5.2.
The comparative results in Table 5 clearly show
that while our vanilla seed lexicon performs com-
parably to off-the-shelf lexicons on our data, the
paraphraser-expanded lexicon with sentitment pro-
files outperforms OpinionFinder, General Inquirer,
and SentiWordNet.
7.2 Sentiment Profile Enrichment with Other
Lexical Expansion Methods
We presented a novel lexicon expansion method
using a paraphrasing system. We also experimented
with more standard methods, using WordNet and
distributional similarity (Beigman Klebanov et al,
2012; Esuli and Sebastiani, 2006; Kim and Hovy,
2004; Andreevskaia and Bergler, 2006; Hu and Liu,
2004; Kanayama and Nasukawa, 2006; Strapparava
and Valitutti, 2004; Kamps et al, 2004; Takamura
et al, 2005; Turney and Littman, 2003; Hatzivas-
siloglou and McKeown, 1997). Specifically, we im-
plemented a WordNet (Miller, 1995) based expan-
sion that uses the 3 most frequent synonyms of the
top sense of the seed word (WN-e). We also imple-
mented a method based on distributional similarity:
Using Lin?s proximity-based thesaurus (Lin, 1998)
trained on our in-house essay data as well as on well-
formed newswire texts, we took all words with the
proximity score > 1.80 to any of the seed lexicon
words (Lin-e). Just like the paraphraser lexicon,
both perform worse than the seed lexicon in 9 out
of 10 baseline runs (BL-sum and Bl-full conditions
for the 5 machine learners).
To test the effect of profile enrichment, all words
in WN-e and Lin-e underwent profile estimation as
described in section 3.1, yielding lexicons WN-e-p
and Lin-e-p, respectively. Figure 2 shows the distri-
6http://sentiwordnet.isti.cnr.it/, under ?Sample code.?
1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
2.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
3.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
4.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
5.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
6.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
7.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
8.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
9.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
10.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
11.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
12.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
13.2
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
14.2.1
0
200
400
600
800
1000
1200
?
1.
0
?
0.
5
0.
0
0.
5
1.
0
Figure 2: Sentiment profile distributions for Lin-e-p (left)
and WN-e-p (right) lexicons.
butions. WN-e-p and Lin-e-p exhibit similar trends
to those of the paraphraser. Substituting WN-e-p
for Expanded data in Table 4, we find the same re-
lationships between the different feature sets: Int-
bin>Int-sum>Int-full=BL. For Lin-e-p, Int-sum de-
teriorates: Int-bin>Int-sum=Int-full=BL. For the
20 runs in the Int condition, Paraphraser>WN-e-
p>Lin-e-p.7 Note that this is also the order of lexi-
con sizes: Lin-e is the most conservative expan-
sion (1,907 words), WN-e is the second with 2,527
words, and the lexicon expanded using paraphrasing
is the largest with 2,994 words. Table 6 shows the
performance of Lin-e-p, WN-e-p, and of the Ex-
panded lexicon from Table 4 using the Int-bin fea-
ture representation. The average relative improve-
ments over the best baseline range between 6.6% to
14.6% for the different expansion methods.
Profile induction appears to be a powerful lexicon
clean-up procedure that works especially well with
more aggressive and thus potentially noisier expan-
sions: The machine learners depress low-intensity
and ambiguous expansions, thereby allowing the
effective utilization of the improved coverage of
sentiment-bearing vocabulary.
7.3 Effectiveness of the Paraphrase Expansion
with Profile Enrichment Paradigm in a
Different Domain
In order to check whether the paraphrase-based ex-
pasion and profile enrichment paradigm discussed in
this paper generalizes to other subjectivity lexicons
7All > are signficant at p=0.05 using Wilcoxon test.
106
Machine Seed Lin-e-p WN-e-p Exp.
Learner BL
c5.0 0.512 0.584 0.616 0.641
SVM-RBF 0.527 0.598 0.601 0.644
SVM-lin. 0.584 0.577 0.569 0.630
Log. Reg. 0.545 0.587 0.580 0.616
Na??ve B. 0.598 0.591 0.623 0.626
Av. Gain 0.066 0.085 0.146
Table 6: Performance of WordNet-based, Lin-based, and
Paraphraser-based expansions with profile enrichment in
the Int-bin condition. Seed BL column shows the best
baseline performance of the seed lexicon ? before expan-
sion and profile enrichment were applied. The last line
shows the average relative gain over the best baseline
calculated as AGlex = ?m?M Lexm?SeedBLmSeedBLm , where
M = {c5.0, SVM-RBF, SVM-linear, Logistic Regres-
sion, Na??ve Bayes}, and lex ? {Lin-e-p, Wn-e-p, Exp}.
and domains of application, we experimented with
a product reviews dataset (Hu and Liu, 2004) and
additional lexicons as follows.
7.3.1 Lexicons
We use the OpinionFinder and General Inquirer
lexicons (OFL and GIL) as before, as well as
the lexicon of positive and negative sentiment and
opinion words available along with (Hu and Liu,
2004) product reviews dataset ? HL.8
Since each of these lexicons contains more than
3,000 words, enrichment of the full lexicons with
profiles is beyond the financial scope of our project.
We therefore restrict each of the lexicons to the size
of their overlap with our seed lexicon (see 2.1); the
overlaps have between 415 and 467 words. These re-
stricted lexicons are our initial lexicons for the new
experiment that parallel the role of the seed lexicon
in the experiments on essay data.
For each of the 3 initial lexicons L, L?{OFL,
GIL, HL}, we follow the paraphrase-based expan-
sion as described in section 2.2. This results in about
4.5-fold expansion of each lexicon, the new lexi-
cons L-e, L?{OFL, GIL, HL}, numbering between
2,015 and 2,167 words. Both the initial and the ex-
panded lexicons now undergo profile enrichment as
described in section 3.1, producing lexicons L-p and
8http://www.cs.uic.edu/?liub/FBS/sentiment-
analysis.html#lexicon
L-e-p, L?{OFL, GIL, HL}.
7.3.2 Data
We use the dataset from Hu and Liu (2004)9 that
contains reviews of 5 products from amazon.com:
two digital cameras, a DVD player, an MP3 player,
and a cellular phone. The reviews are annotated at
sentence level with a label that desrcibes the par-
ticular feature that is the subject of the positive or
negative evaluation and the polarity and extent of
the evaluation. For example, the sentence ?The
phone book is very user-friendly and the speaker-
phone is excellent? is labeled as PHONE BOOK[+2],
SPEAKERPHONE[+2], while the sentence ?I am
bored with the silver look? is labeled LOOK[?1]. We
used all sentences that were labeled with a numeri-
cal score for at least one feature, removing a small
number of sentences labeled with both positive and
negative scores for different features.10 We used the
sign of the numerical score to label the sentences as
positive or negative. The resulting dataset consists
of 1,695 sentences, 1,061 positive and 634 nega-
tive; accuracy for a majority baseline on this dataset
is 0.626. Our experiments on this dataset are done
using 5-fold cross-validation.
7.3.3 Results
Table 7 shows classification accuracies for the
product review data using different lexicons and ma-
chine learners. We observe that the combination of
paraphrase-based expansion and profile enrichment
(L-e-p column in the table) resulted in an improved
performance over the initial lexicon (L column in
the table) in all cases, with the average gain of 5%
in accuracy.
Furthermore, the contributions of the expansion
and the profile enrichment are complementary, since
their combination performs better than each in iso-
lation. We note that profile enrichment alone for the
initial lexicon did not yield an improvement. This
can be explained by the fact that the initial lexicons
are highly polar, so profiles provide little additional
information: The percentage of words with p?pos ?
0.8 or p?neg ? 0.8 is 84%, 86% and 91% for GIL,
9http://www.cs.uic.edu/?liub/FBS/sentiment-
analysis.html#datasets, the link under ?Customer Review
Datasets (5 products)?
10such as ?The headset that comes with the phone has good
sound volume but it hurts the ears like you cannot imagine!?
107
Machine Lexicon Variant
Learner L L-p L-e L-e-p
L = OFL?Seed, |L|=467, |L-e|=2,167
c5.0 0.663 0.670 0.691 0.704
SVM-RBF 0.668 0.676 0.693 0.714
SVM-lin. 0.675 0.670 0.688 0.696
Log. Reg. 0.666 0.658 0.693 0.698
Na??ve B. 0.668 0.668 0.686 0.695
L = GIL?Seed, |L|=415,|L-e|=2,015
c5.0 0.644 0.658 0.663 0.686
SVM-RBF 0.650 0.665 0.653 0.683
SVM-lin. 0.665 0.665 0.677 0.681
Log. Reg. 0.664 0.658 0.678 0.694
Na??ve B. 0.669 0.666 0.678 0.703
L = HL?Seed, |L|=434, |L-e|=2,054
c5.0 0.676 0.675 0.689 0.706
SVM-RBF 0.673 0.674 0.700 0.713
SVM-lin. 0.676 0.664 0.703 0.710
Log. Reg. 0.668 0.661 0.703 0.699
Na??ve B. 0.668 0.672 0.697 0.697
Table 7: Accuracies on product review data. For each ma-
chine learner and lexicon, the best baseline performance
is shown as L for the initial lexicon and as L-e for the
paraphrase-expanded lexicon. L-p and L-e-p show the
performance of Int-bin feature set on the profile-enriched
initial and paraphrase-expanded lexicons, respectively.
The three initial lexicons L are OpinionFinder (OFL),
General Inquirer (GIL), and (Hu and Liu, 2004) (HL),
each intersected with our seed lexicon. Sizes of the intial
and expanded lexicons are provided.
OFL, and HL-derived lexicons, respectively. In con-
trast, for the expanded lexicons, these percentages
are 51%, 53%, and 56%; these lexicons benefit from
profile enrichment.
8 Conclusions
We demonstrated a method of improving a seed sen-
timent lexicon by using a pivot-based paraphrasing
system for lexical expansion and sentiment profile
enrichment using crowdsourcing. Profile enrich-
ment alone yielded up to 15% improvement in the
performance of the seed lexicon on the task of 3-
way sentence-level sentiment polarity classification
of test-taker essay data. While the lexical expansion
on its own failed to improve upon the performance
of the seed lexicon, it became much more effective
on top of sentiment profiles, generating a 7% perfor-
mance boost over the best profile-enriched run with
the seed lexicon. Overall, paraphrase-based expan-
sion coupled with profile enrichment yields an up to
25% improvement in accuracy.
Additionally, we showed that our paraphrase-
expanded and profile-enriched lexicon performs
significantly better on our data than off-the-shelf
subjectivity lexicons, namely, Opinion Finder, Gen-
eral Inquirer, and SentiWordNet. Furthermore, our
results suggest that paraphrase-based expansion de-
rives more benefit from profiles than two competing
expansion mechanisms based on WordNet and on
Lin?s distributional thesaurus.
Finally, we demonstrated the effectiveness of the
paraphraser-based expansion with profile enrich-
ment paradigm on a different dataset. We used Hu
and Liu (2004) product review data with sentence-
level sentiment polarity labels. Paraphrase-based
expansion with profile enrichment yielded an im-
proved performance across all lexicons and machine
learning algorithms we tried, with an average im-
provement rate of 5% in classification accuracy.
Recent literature argues that sentiment polarity
is a property of word senses, rather than of words
(Gyamfi et al, 2009; Su and Markert, 2008; Wiebe
and Mihalcea, 2006), although Dragut et al(2012)
successfully operate with ?mostly negative? and
?mostly positive? words based on the polarity distri-
butions of word senses. We plan to address in future
work sense disambiguation for words that have mul-
tiple senses with very different sentiment, such as
stress, as either anxiety (negative) or emphasis (neu-
tral).
References
Alina Andreevskaia and Sabine Bergler. 2006. Mining
WordNet for a fuzzy sentiment: Sentiment tag extrac-
tion of WordNet glosses. In Proceedings of EACL,
pages 209?216, Trento, Italy.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SENTIWORDNET 3.0: An Enhanced
Lexical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of LREC, pages 2200?2204,
Malta.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL, pages 597?604, Ann Arbor, MI.
108
Beata Beigman Klebanov, Jill Burstein, Nitin Madnani,
Adam Faulkner, and Joel Tetreault. 2012. Build-
ing sentiment lexicon(s) from scratch for essay data.
In Proceedings of the 13th International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing), New Delhi, India, March.
S. Cerini, V. Compagnoni, A. Demontis, M. Formentelli,
and G. Gandini. 2007. Micro-WNOp: A gold stan-
dard for the evaluation of automatically compiled lexi-
cal resources for opinion mining. In Andrea Sanso,
editor, Language resources and linguistic theory: Ty-
pology, second language acquisition, pages 200?210.
Franco Angeli Editore, Milano, IT.
Ferm??n L. Cruz, Jose? A. Troyano, F. Javier Ortega, and
Fernando Enr??quez. 2011. Automatic expansion
of feature-level opinion lexicons. In Proceedings of
the 2nd Workshop on Computational Approaches to
Subjectivity and Sentiment Analysis, pages 125?131,
Portland, Oregon, June.
Eduard Dragut, Hong Wang, Clement Yu, Prasad Sistla,
and Weiyi Meng. 2012. Polarity consistency check-
ing for sentiment dictionaries. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
997?1005, Jeju Island, Korea, July. Association for
Computational Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006. Determin-
ing term subjectivity and term orientation for opinion
mining. In Proceedings of EACL, pages 193?200,
Trento, Italy.
Leo A. Goodman. 1965. On Simultaneous Confidence
Intervals for Multinomial Proportions. Technometrics,
7(2):247?254.
Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem
Akkaya. 2009. Integrating knowledge for subjectivity
sense labeling. In Proceedings of NAACL, pages 10?
18, Boulder, CO.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL, pages 174?181, Madrid,
Spain.
Minqing Hu and Bing Liu. 2004. Mining and
summarizing customer reviews. In Proceedings of
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168?177,
Seattle, WA.
Valentin Jijkoun and Katja Hofmann. 2009. Gener-
ating a Non-English Subjectivity Lexicon: Relations
That Matter. In Proceedings of EACL, pages 398?405,
Athens, Greece.
Jaap Kamps, Maarten Marx, Robert Mokken, and
Maarten de Rijke. 2004. Using WordNet to measure
semantic orientation of adjectives. In Proceedings of
LREC, pages 1115?1118, Lisbon, Portugal.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic Lexicon Expansion for Domain-oriented
Sentiment Analysis. In Proceedings of EMNLP, pages
355?363, Syndey, Australia.
Soo-Min Kim and Edward Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of COLING,
pages 1367?1373, Geneva, Switzerland.
Philip Koehn. 2005. EUROPARL: A Parallel corpus for
Statistical Machine Translation. In Proceedings of the
Machine Translation Summit.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of ACL, pages 768?774,
Montreal, Canada.
Tim Loughran and Bill McDonald. 2011. When is a Li-
ability not a Liability? Textual Analysis, Dictionaries,
and 10-Ks. Journal of Finance, 66:35?65.
Nitin Madnani and Bonnie Dorr. 2013. Generating Tar-
geted Paraphrases for Improved Translation. ACM
Transactions on Intelligent Systems and Technology, to
appear.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 2007.
Learning multilingual subjective language via cross-
lingual projections. In Proceedings of ACL, pages
976?983, Prague, Czech Republic.
George Miller. 1995. WordNet: A lexical database.
Communications of the ACM, 38:39?41.
Saif Mohammad, Cody Dunne, and Bonnie Dorr. 2009.
Generating high-coverage semantic orientation lexi-
cons from overtly marked words and a thesaurus. In
Proceedings of EMNLP, pages 599?608, Singapore,
August.
Guillaume Pitel and Gregory Grefenstette. 2008. Semi-
automatic building method for a multidimensional af-
fect dictionary for a new language. In Proceedings of
LREC, Marrakech, Morocco.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international
jont conference on Artifical intelligence, IJCAI?09,
pages 1199?1204.
C. Quesenberry and D. Hurst. 1964. Large sample si-
multaneous confidence intervals for multinomial pro-
portions. Technometrics, 6:191?195.
J. R. Quinlan. 1993. C4.5: Programs for machine lear-
ning. Morgan Kaufmann Publishers.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of EACL, pages 675?682, Athens.
109
Philip Stone, Dexter Dunphy, Marshall Smith, and Daniel
Ogilvie. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
Carlo Strapparava and Alessandro Valitutti. 2004.
WordNet-affect: an affective extension of WordNet.
In Proceedings of LREC, pages 1083?1086, Lisbon,
Portugal.
Fangzhong Su and Katja Markert. 2008. Eliciting
Subjectivity and Polarity Judgements on Word Senses.
In Proceedings of COLING, pages 825?832, Manch-
ester, UK.
P. Subasic and A. Huettner. 2001. Affect analysis of text
using fuzzy semantic typing. IEEE Transactions on
Fuzzy Systems, 9(4).
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-Based
Method for Sentiment Analysis. Computational Lin-
guistics, 37(2):267?307.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientation of words using
spin model. In Proceedings of ACL, pages 133?140,
Ann Arbor, MI.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment strength
detection in short informal text. Journal of the Amer-
ican Society for Information Science and Technology,
61(12):2544?2558.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proceedings of HLT-NAACL, pages 252?259.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21(4):315346.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of
Web-derived polarity lexicons. In Proceedings of
NAACL, pages 777?785, Los Angeles, CA.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of ACL, pages 1065?
1072, Sydney, Australia.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of CICLING (invited pa-
per), pages 486?497, Mexico City.
110
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 2?7
Manchester, August 2008
Analyzing Disagreements
Beata Beigman Klebanov, Eyal Beigman, Daniel Diermeier
Kellogg School of Business
Northwestern University
{beata,e-beigman,d-diermeier}@northwestern.edu
Abstract
We address the problem of distinguishing
between two sources of disagreement in
annotations: genuine subjectivity and slip
of attention. The latter is especially likely
when the classification task has a default
class, as in tasks where annotators need to
find instances of the phenomenon of inter-
est, such as in a metaphor detection task
discussed here. We apply and extend a data
analysis technique proposed by Beigman
Klebanov and Shamir (2006) to first dis-
till reliably deliberate (non-chance) anno-
tations and then to estimate the amount of
attention slips vs genuine disagreement in
the reliably deliberate annotations.
1 Introduction
Classification tasks fall into two broad categories.
Those in the first category proceed by requiring
that every item is explicitly assigned a tag out of
a given set of tags; part-of-speech tagging is an
example (Santorini, 1990).
In the second group of tasks, the annotator is
asked to identify a phenomenon of interest, thus
implicitly classifying items as belonging to the
phenomenon (marked) and not belonging to it (left
unmarked). When the studied phenomenon is ex-
pected to have low incidence, this is a time-saving
strategy, as annotators do not need to bother with
explicitly marking (almost) everything as a non-
phenomenon. A recent example of such a task is
Beigman Klebanov and Shamir (2006), where an-
notators were asked to provide anchors for words
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
deemed anchored in the text (i.e. associatively
connected to a previous item in the text), thus leav-
ing words that did not receive an anchor implic-
itly marked as un-anchored. Psychological exper-
iments where people are asked to respond to the
occurrence of a given phenomenon can also be
viewed as implicit classifications; for example, see
Spiro?s (2007) work on identification of bound-
aries of musical phrases by listeners. The task
of metaphor detection discussed in this paper also
falls under the implicit classification category.
While such a strategy uses annotators? time effi-
ciently, some of the observed disagreements could
be due to an annotator missing an occurrence of
the relevant phenomenon, rather than genuinely
disagreeing on the matter of occurrence.
We show in section 2 that our metaphor
identification task features less-than-perfect inter-
annotator agreement. Section 3 uses Beigman Kle-
banov and Shamir?s (2006) methodology to find
annotations that can be reliably attributed to a de-
liberate decision by at least some of the annotators.
We then discuss the use of validation experiment to
distinguish between slips of attention and genuine
disagreements (sections 4,5).
2 Metaphor Detection Study
For a project studying the use of metaphors in pub-
lic discourse, a dataset of 151 articles from the
British press was subjected to annotation.1 Partic-
ipants were asked to mark paragraphs that contain
occurrences of metaphors from LOVE, VEHICLE,
AUTHORITY and BUILDING domains (hence-
forth, metaphor types).
For example, the following paragraph in 20
September 1992 issue of Sunday Times contains an
1This is part of the data discussed in (Musolff, 2000).
2
extended metaphor from the VEHICLE domain:
Thatcher warned EC leaders to stop their
endless round of summits and take no-
tice of their own people. ?There is a
fear that the European train will thunder
forward, laden with its customary cargo
of gravy, towards a destination nei-
ther wished for nor understood by elec-
torates. But the train can be stopped,?
she said.
The title2 of one of the articles in the 19 Octo-
ber 1999 issue of The Guardian contains a LOVE
metaphor:
Euro-flirting is not only a matter of de-
sire.
The discussion in this paper is based on the out-
put of 9 annotators who performed metaphor iden-
tification (henceforth, production task), and of 7
annotators (out of 9) who took part in the sub-
sequent validation study (henceforth, validation
task). Subjects were not told about validation until
after they finished production on the whole of the
dataset. A time gap of 2 weeks existed between
the end of the production study and the start of
the validation, each of the tasks taking 6 weeks,
in weekly installments of 25 articles each.
For the production task, the annotators were
instructed to mark every paragraph where a
metaphor from the given metaphor type appeared;
the 151-article dataset yields 2364 paragraphs.
This paradigm corresponds to the implicit clas-
sification task discussed earlier, in that only the
positive (metaphor-containing) cases are given an
explicit markup. The incidence of positive cases
is quite low ? VEHICLE, the most ubiquitous
type, featured in 4% of the paragraphs, on average
across annotators.
We note that the appearances of the different
metaphor types are not mutually exclusive, and,
indeed, there is no a-priori reason to suppose
any relationship between them. For example, the
following paragraph from the leading article in
15 November 1995 issue of The Guardian was
marked by some annotators as containing both
LOVE and VEHICLE metaphors:
The first European bank notes - proba-
bly to be called ?euros? - will not be in
2A title is treated as a paragraph in our annotations.
circulation until 2002 judging by yester-
day?s report from the European Mone-
tary Institute. But this doesn?t mean that
monetary union has been delayed be-
yond 1999 because the printing of Euro-
pean bank notes will have been preceded
by a period of three years when na-
tional currencies will have been locked
together in indissoluble monetary matri-
mony [...] Although France looks as if it
might buckle under the strain of meet-
ing the fiscal criteria and in Germany
the SDP is having doubts (though only
about whether the new currency will be
strong enough) the Maastricht train is
still theoretically on the rails. Nobody
has changed the timetable.
We therefore treat the detection of metaphors
from each metaphor type as a separate binary
classification task. Table 1 shows the inter-
annotator agreement for the production task using
the ? statistic (Carletta, 1996; Krippendorff, 1980;
Siegel and Castellan, 1988).
Table 1: Metaphor annotation data (production),
by metaphor type. The third column shows the
percentage of paragraphs (out of 2364) marked as
having a metaphor of the given type, on average
across 9 annotators.
Type ? marked
VEHICLE 0.66 4.0%
LOVE 0.66 2.5%
AUTHORITY 0.39 2.7%
BUILD 0.43 1.7%
Clearly, it is not the case that the whole of the
dataset was reliably annotated, even for the better-
agreed-upon metaphor types like VEHICLE and
LOVE. Hence, additional procedures are needed
to distill reliable annotations. We apply Beigman
Klebanov and Shamir?s (2006) statistical tech-
nique to find a subset of the data that is sufficiently
reliable, and later corroborate the statistical analy-
sis through the validation task.
3 Reliably Deliberate Annotations
In Beigman Klebanov and Shamir (2006), 22
subjects performed the anchoring annotation; the
overall inter-annotator agreement was ?=0.45.
3
Thus, some of the data was clearly unreliable, as
in our metaphor detection task, but the possibility
existed that some other part was in fact annotated
sufficiently reliably.
Beigman Klebanov and Shamir?s (2006) analy-
sis proceeded thus: Suppose each of the 20 anno-
tators3 (i = 1...20) was flipping a coin with the
probability of heads p
i
equal to the proportion of
?anchored? markups in annotator i?s data. What
is the level of agreement for which this scenario is
sufficiently improbable? For their data, the random
anchoring hypothesis could be rejected with 99%
confidence for cases marked by at least 13 people.
Items featuring at least this level of agreement can
be considered, with high probability, as deliber-
ately annotated as ?anchored?, as at least some of
those who marked them were not flipping a coin.
Following the procedure in Beigman Klebanov
and Shamir (2006), we wish to determine a re-
liably deliberate subset of our metaphor annota-
tions. We induce 9 random pseudo-annotators
from the 9 actual ones, each marking paragraphs
at random as containing a metaphor of a given
type or not. Pseudo-annotator i flips a coin
with p(heads) = p
i
, which is the proportion of
metaphor markups by the i?th annotator for the
most common metaphor type (VEHICLE).
Assuming each annotator flips her coin, we cal-
culate the probability of 3 or more coins coming up
heads simultaneously;4 this probability is 0.0045.
Thus, with 99.5% confidence, a metaphor markup
by at least 3 people is not a result of coinflip, at
least for some of the annotators. We note, how-
ever, that 99.5% confidence is insufficient for our
case: It allows for random highly agreed markup
in 0.5% of the instances. Given that only up to
4% of the instances have positive markups, this
would yield a high percentage of random items
in the positive instances. The probability of 4 or
more pseudo-annotators having their coins come
up heads simultaneously is below 0.0003; we con-
sider this sufficient confidence for our case, and
regard metaphor markups produced by at least 4
people as reliably deliberate.
Note that we cannot find a similar threshold for
no-metaphor annotations, as a lack of metaphor
3Two people were excluded as outliers.
4In Beigman Klebanov and Shamir (2006), a normal ap-
proximation is used to handle collective decision making by
20 pseudo-annotators. In the current case, 9 annotators is a
sufficiently small number to allow an exact probability calcu-
lation over the 512 possibilities.
annotation could happen by chance with a high
probability (p = 0.69). In view of the potential use
of the dataset for evaluating metaphor detection
algorithms, a putative metaphor suggested by the
algorithm cannot be rejected based on the lack of
metaphor annotation in the data. A complementary
procedure would be needed, for example, collect-
ing human judgments for the putative metaphors
separately.
4 Attention Slips vs Genuine
Disagreements
Deliberate annotation does not guarantee agree-
ment. It remained the case that some of the reliably
deliberate data in Beigman Klebanov and Shamir
(2006) was actually produced by only some of the
original subjects. Indeed, some of the deliberately
marked metaphors were annotated by only 4 out
of the 9 participants. For cases where the posi-
tive annotations were produced deliberately, what
is the status of negative annotations accorded to
the same items? Were these mere attention slips,
or genuine differences of opinion? Note that this
question cannot be meaningfully posed regarding
the parts of annotations for which the hypothesis
of random positive marking could not be rejected
with sufficiently high probability, since, obviously,
apparent disagreements there could be simply a re-
sult of different coinflip outcomes.
Beigman Klebanov and Shamir (2006) hypoth-
esized that dissenting annotations of the reliable
pairs would be cases of attention slips, rather than
genuine differences of opinion. In other words,
while there was no initial agreement, these items
were potentially agreeable. To test the hypothe-
sis, they devised a validation experiment, where
subjects were presented with all pairs marked by
at least one annotator, plus some random pairs,
and were asked to cross out things they disagree
with. The reasoning was as follows: If attention
slip was the cause for a dissenting negative anno-
tation, when the subject is asked about the relevant
item, i.e. it is explicitly brought to her attention,
she would accept it, whereas if a case is that of
a genuine disagreement, she would reject it. To
control for the possibility that people just accept
everything so that not to be dissonant with others,
some random annotations were also included.
The results reported by Beigman Klebanov and
Shamir (2006) largely bore out the hypothesis.
First, people did not tend to accept everything,
4
as only 15% of judgments of random annota-
tions and only 62% of judgments on all human-
generated annotations were ?accept? judgments.
However, 94% of judgments of the reliable anno-
tations were ?accept? judgments. Hence, the rate
of genuine disagreement on the reliably deliberate
part of Beigman Klebanov and Shamir?s (2006)
data turned out to be quite low.
We are interested in estimating the degree of
genuine disagreements in metaphor production.
Using Beigman Klebanov and Shamir?s method-
ology, we collected all paragraphs marked as con-
taining a metaphor of a given type by at least one
of the 9 annotators, plus added random markups.
This data was submitted to 7 subjects for valida-
tion.
Table 2: Percentage of ?Accept? validations for
random (Rand) and human (Hum) metaphor pro-
duction data, as well as for the partition of the
human data into reliably deliberate (Rel) and unre-
liable (URel) subsets. For each subset, the number
of data instances covered by the subset is shown.
Subscripts indicate metaphor type: (V)EHICLE,
(L)OVE, (A)UTHORITY, (B)UILD. The bottom
line shows the average over metaphor types.
Subset # Acc Subset # Acc
Rand
V
94 5% Hum
V
194 73%
Rand
L
56 6% Hum
L
137 64%
Rand
A
62 12% Hum
A
258 51%
Rand
B
40 1% Hum
B
126 68%
Rand 252 6% Hum 715 62%
Subset # Acc Subset # Acc
URel
V
92 49% Rel
V
102 94%
URel
L
81 43% Rel
L
56 95%
URel
A
218 42% Rel
A
40 96%
URel
B
86 55% Rel
B
40 96%
URel 477 46% Rel 238 95%
Table 2 reports the percentage of ?accept? votes
for random and human metaphor production data,
as well as for reliably deliberate and unreliable
subsets of the human data. As in Beigman Kle-
banov and Shamir?s case, the validation experi-
ment clearly distinguishes between random, hu-
man in general, and reliably deliberate subsets, and
puts the estimated degree of genuine disagreement
in metaphor identification at 5% on average, with
little variation across the metaphor types. That
is, given that, with high probability, at least some
humans deliberately identified a paragraph as con-
taining a metaphor, the chance for its rejection is
about 5%. The rest of observed production dis-
agreements, for the reliably deliberate subset, are
remedied at validation time, thus probably consti-
tuting attention slips during production. The reli-
ably deliberate subset contains 33% (238/715) of
all human-generated data.
5 Separating self and others
One potential confounder in the above analysis
is conflation of self-consistency with affirmation
of someone else?s annotations. It is possible that
many of the validation-time ?accept? votes are
cases of people accepting their own earlier annota-
tion; the proportion of such cases is expected to in-
crease the more people marked the metaphor dur-
ing production. Therefore, to get a more precise
estimate of the degree of genuine disagreement,
we control for self-affirmation, and calculate the
proportion of ?accept? validations in cases where
the person did not mark the metaphor during pro-
duction. Specifically, if X of the 7 people who par-
ticipated in both production and validation marked
the metaphor at production,5 we check the split of
the remaining 7-X votes during validation. Table 3
presents average other-affirmation rates for the re-
liably deliberate and unreliable human produced
data. Note that only 184 out of the 238 deliberately
reliable cases can be used, as the remaining 54 are
cases where all 7 annotators produced the markup,
so there is no disagreement.
Table 3: Percentage of ?Accept? validations for re-
liably deliberate (Rel) and unreliable (URel) sub-
sets of the metaphor production data, given that the
subject himself did NOT produce the metaphor.
Subset # Acc Subset # Acc
URel
V
92 44% Rel
V
78 90%
URel
L
81 39% Rel
L
38 92%
URel
A
218 35% Rel
A
30 91%
URel
B
86 53% Rel
B
38 91%
URel 477 41% Rel 184 91%
5The actual total of the production annotations could be up
to X+2, as there were 2 more annotators in production than
in validation.
5
According to the table, 91% cases of disagree-
ments in the reliably deliberate data are remedied
at validation time. That is, given that, with high
probability, at least some human deliberately iden-
tified a paragraph as containing a metaphor, the
chance for its rejection by a human who initially
apparently disagreed with the annotation is only
about 9%.
Finally, validation data allows an investigation
of the stability of people?s judgments by calculat-
ing self-rejection rates, i.e. estimating the prob-
ability of rejecting during validation an instance
that the same annotator marked as containing a
metaphor during production. Table 4 shows the
results.
Table 4: Percentage of ?Reject? validations for re-
liably deliberate (Rel) and unreliable (URel) sub-
sets of the metaphor production data, given that the
subject himself produced the annotation.
Subset # Rej Subset # Rej
URel
V
72 25% Rel
V
102 4%
URel
L
55 26% Rel
L
56 5%
URel
A
198 22% Rel
A
40 2%
URel
B
60 23% Rel
B
40 2%
URel 3856 23% Rel 238 4%
For the reliably deliberate data, i.e. cases where
at least 4 people produced the markup, the average
self-rejection rate is 4%. This low figure further
supports the designation of the reliably deliberate
subset as such, i.e. containing stable annotations,
as in 96% of the cases a person who produced the
markup is likely to re-affirm it when asked again,
even after a substantial time delay.7
For the ?unreliable? data, i.e. cases where only
one or two people marked the metaphor during
production, the average self-rejection rate is 23%.
Self-rejection means either that the initial positive
markup was a mistake, or that it is difficult for the
annotator to make up his mind about the annota-
tion of the item. In any case, high self-rejection
6Note that only 385 of the 477 items in the unreliable data
could be used for the calculation. The remaining items were
not produced by any of the 7 people who participated in both
production and validation, but only by one or both of the 2
additional production-task annotators.
7The time difference between production and validation
per article ranged between 4 and 8 weeks, due to differences
in the order in which the different subjects were given the
articles.
rate means that the relevant production annotations
cannot be trusted to contain a settled judgment that
could be then agreed or disagreed with by other an-
notators, or indeed replicated by a computational
model.
We consider self-rejected cases potential indica-
tors of a difficulty on the annotator?s part to de-
cide on the correct markup. We plan a more de-
tailed investigation of the materials to see whether
these cases exhibit any interesting common prop-
erties that could help characterize the difficulties in
metaphor identification task.
6 Conclusion
In this article, we showed an application of
Beigman Klebanov and Shamir?s (2006) method-
ology for analyzing annotation data to metaphor
identification annotations. The approach allowed
establishing an agreement threshold beyond which
the annotations are reliably deliberate, in the sense
that, with high probability, at least some of the
annotators who detected a metaphor were not flip-
ping a coin. This threshold is agreement of 4 out
of 9 annotators, for 99.9% reliability.
To investigate the nature of disagreements in the
reliably deliberate subset, we followed Beigman
Klebanov and Shamir (2006) in conducting a val-
idation study, where subjects were asked to ac-
cept or reject markups produced during the ini-
tial annotation study, as well as some random
annotations. Sharpening the methodology some-
what, we showed that in 91% of reliably deliber-
ate cases where an annotator did not produce the
markup himself, he accepted it during validation.
Hence, the bulk of the initial disagreements were
amended during validation, with the residual 9%
being likely locations for genuine difference of
opinion.
Further analysis of validation data revealed that
the reliably deliberate subset features low self-
rejection rates, meaning that people are consis-
tent with their own production. This was not the
case for the subset deemed unreliable during sta-
tistical analysis, where a 23% self-rejection rate
was observed. We hypothesize that some of these
would be hard-to-decide cases with respect to the
metaphor identification task, and hence warrant a
closer look in order to characterize annotator diffi-
culties with the task.
6
7 Acknowledgment
We would like to thank an anonymous reviewer
for a thorough and insightful review that helped as
improve this article significantly.
References
Beigman Klebanov, Beata and Eli Shamir. 2006.Reader-based exploration of lexical cohesion. Lan-
guage Resources and Evaluation, 40(2):109?126.
Carletta, Jean. 1996. Assessing agreement on clas-
sification tasks: the kappa statistic. Computational
Linguistics, 22(2):249?254.
Krippendorff, Klaus. 1980. Content Analysis. SagePublications.
Musolff, Andreas. 2000. Mirror images of Europe:
Metaphors in the public debate about Europe in
Britain and Germany. Mu?nchen: Iudicium.
Santorini, Beatrice. 1990. Part-of-speechtagging guidelines for the Penn Tree-
bank project (3rd revision, 2nd printing).ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz.
Siegel, Sidney and John Castellan. 1988. Nonparamet-
ric Statistics for the Behavioral Sciences. McGraw-
Hill Book Company.
Spiro, Neta. 2007. What contributes to the percep-
tion of musical phrases in Western classical music?Ph.D. thesis, University of Amsterdam, The Nether-
lands.
7
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 63?72,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Measuring the Use of Factual Information in Test-Taker Essays
Beata Beigman Klebanov
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
bbeigmanklebanov@ets.org
Derrick Higgins
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541, USA
dhiggins@ets.org
Abstract
We describe a study aimed at measuring the
use of factual information in test-taker essays
and assessing its effectiveness for predicting
essay scores. We found medium correlations
with the proposed measures, that remained
significant after the effect of essay length was
factored out. The correlations did not dif-
fer substantionally between a simple, rela-
tively robust measure vs a more sophisticated
measure with better construct validity. Impli-
cations for development of automated essay
scoring systems are discussed.
1 Introduction
Automated scoring of essays deals with various as-
pects of writing, such as grammar, usage, mecha-
nics, as well as organization and content (Attali
and Burstein, 2006). For assessment of content,
the focus is traditionally on topical appropriateness
of the vocabulary (Attali and Burstein, 2006; Lan-
dauer et al, 2003; Louis and Higgins, 2010; Chen
et al, 2010; De and Kopparapu, 2011; Higgins et
al., 2006; Ishioka and Kameda, 2006; Kakkonen et
al., 2005; Kakkonen and Sutinen, 2004; Lemaire
and Dessus, 2001; Rose? et al, 2003; Larkey, 1998),
although recently other aspects, such as detection
of sentiment or figurative language, have started to
attract attention (Beigman Klebanov et al, 2012;
Chang et al, 2006).
The nature of factual information used in an es-
say has not so far been addressed, to our knowledge;
yet a misleading premise, insufficient factual basis,
or an example that flies in the face of the reader?s
knowledge clearly detract from an essay?s quality.
This paper presents a study on assessing the use
of factual knowledge in argumentative essays on ge-
neral topics written for a graduate school entrance
exam. We propose a definition of fact, and an opera-
tionalization thereof. We find that the proposed mea-
sure has positive medium-strength correlation with
essay grade, which remains significant after the im-
pact of essay length is factored out. In order to
quantify which aspects of the measure drive the ob-
served correlations, we gradually relax the measure-
ment procedure, down to a simple and robust proxy
measure. Surprisingly, we find that the correlations
do not change throughout the relaxation process. We
discuss the findings in the context of validity vs re-
liability of measurement, and point out implications
for automated essay scoring.
2 What is a Fact?
To help articulate the notion of fact, we use the fol-
lowing definition from a seminal text in argumenta-
tion theory: ?... in the context of argumentation, the
notion of fact is uniquely characterized by the idea
that is held of agreements of a certain type relating
to certain data, those which refer to an objective rea-
lity, and, in Poincare?s words, designate essentially
?what is common to several thinking beings, and
could be common to all? (Perelman and Olbrechts-
Tyteca, 1969, 67). Factuality is thus a matter of se-
lecting certain kinds of data and securing a certain
type of agreement over those data.
Of the different statements that refer to objec-
tive reality, the term facts is used to ?designate ob-
63
jects of precise, limited agreement? (Perelman and
Olbrechts-Tyteca, 1969, 69). These are contrasted
with presumptions ? statements connected to what
is normal and likely (ibid.). We suggest that the dis-
tinctions in the scope of the required agreement can
be related to the referential device used in a state-
ment: If the reference is more rigid (Kripke, 1980),
that is, less prone to change in time and to inde-
terminacy of the boundaries, the scope of the ne-
cessary agreement is likely to be more precise and
limited. With proper names prototypically being the
most rigid designators, we will focus our efforts on
statements about named entities.1
Perhaps the simplest model of the universal au-
dience is an encyclopedia ? a body of knowledge
that is verified by experts, and is, therefore, ?com-
mon to several thinking beings, and could be com-
mon to all? by virtue of the authority of the experts
and the wide availability of the resource. However,
many facts known to various groups of people that
could be known to all are absent from any encyclo-
pedia. The knowledge contained in the WWW at
large, reaching not only statements explicitly con-
tributed to an encyclopedia but also those made by
people on their blogs ? is perhaps as close as it gets
to a working model of the universal audience.
Recent developments in Open Information Ex-
traction make it possible to tap into this vast know-
ledge resource. Indeed, fact-checking is one of the
applications the developers of OpenIE have in mind
for their emergent technology (Etzioni et al, 2008).
3 Open Information Extraction
Traditionally, the goal of an information extrac-
tion system is automated population of structured
databases of events or concepts of interest and their
properties by analyzing large corpora of text (Chin-
chor et al, 1993; Onyshkevych, 1993; Grishman and
Sundheim, 1995; Ravichandran and Hovy, 2002;
Agichtein and Gravano, 2000; Davidov and Rap-
poport, 2009).
1For example, Barack Obama picks out precisely one per-
son, and the same one in 2010 as it did in 1990. In contrast, the
current US president picks out different people every 4-8 years.
For indeteminacy of boundaries, consider a statement like US
officials are wealthy. To determine its truth, one must first se-
cure agreement on acceptable referents of US officials.
In contrast, the recently proposed Open Informa-
tion Extraction paradigm aims to detect related pairs
of entities without knowing in advance what kinds of
relations exist between entities in the source data and
without any seeding (Banko and Etzioni, 2008). The
possibility of such extraction in English is attributed
by the authors to a small number of syntactic pat-
terns that realize binary relations between entities.
In particular, they found that almost 40% of such re-
lations are realized by the argument-verb-argument
pattern (henceforth, AVA) (see Table 1 in Banko and
Etzioni (2008)).
The TextRunner system (Banko and Etzioni,
2008) is trained using a CRF classifier on S-V-O
tuples from a parsed corpus as positive examples,
and tuples that violate phrasal structure as negative
ones. The examples are described using features
that do not require parsing or semantic role labe-
ling. Features include part-of-speech tags, regular
expressions (detecting capitalization, punctuation,
etc.), context words belonging to closed classes, and
conjunctions of features occurring in adjacent posi-
tions within six words of the current word.
TextRunner achieves P=0.94, R=0.65, and F-
Score=0.77 on the AVA pattern (Banko and Etzioni,
2008). We note that all relations in the test sen-
tences involve a predicate connecting two named en-
tities, or a named entity and a date.2 The authors
kindly made available to us for research purposes a
database of about 2 bln AVA extractions produced
by TextRunner; this database was used in the expe-
riments reported below.
4 Data
We randomly sampled essays written on 10 diffe-
rent prompts, 200 essays per prompt. Essays are
graded on the scale of 1-6; the distribution of grades
is shown in table 1.
Grade 1 2 3 4 5 6
% 0.6 4.9 23.5 42.6 23.8 4.7
Table 1: The distribution of grades for 2,000 essays.
2http://www.cs.washington.edu/research/knowitall/hlt-
naacl08-data.txt
64
5 Building Queries from Essays
We define a query as a 3-tuple <NE,?,NP>,3 where
NE is a named entity and NP is a noun phrase from
the same or neighboring sentence in a test-taker es-
say (the selection process is described in section
5.2). We use the pattern of predicate matches against
the TextRunner database to assess the degree and the
equivocality of the connection between NE and NP.
5.1 Named Entities in Test-Taker Essay
We use the Stanford Named Entity Recognizer
(Finkel et al, 2005) that tags named entities as peo-
ple, locations, organizations, and miscellaneous. We
annotated a sample of 90 essays for named entities;
the sample yielded 442 tokens, which we classified
as shown in Table 2. The Enamex classes (people,
locations, organizations) account for 58% of all the
entities in the sample. The recognizer?s recall of
people and locations is excellent (though they are
not always classified correctly ? see caption of Ta-
ble 2), although test-taker essays feature additional
entity types that are not detected as well.
Category Recall Examples
Location 0.98 Iraq, USA
Person 0.96 George W. Bush, Freud
Org. 0.87 Guggenheim Foundation
Gov. 0.79 No Child Left Behind
Awards 0.79 Nobel Prize
Events 0.68 Civil War, World War I
Sci & Tech 0.59 GPS, Windows 3.11
Art 0.44 Beowulf, Little Women
Table 2: Recall of the Stanford NER by category. Note
that an entity is counted as recalled as long as it is iden-
tified as belonging to any NE category, even if it is mis-
classified. For example, Freud is tagged as location, but
we count it towards the recall of people.
In terms of precision, we observed that the tagger
made few clear mistakes, such as tagging sentence-
initial adverbs and their mis-spelled versions as
named entities (Eventhough, Afterall). The bulk of
3We do not attempt matching the predicate, as (1) in many
cases there is no clearly lexicalized predicate (see the discussion
of single step patterns in section 5.2) and (2) adding a predicate
field would make matches against the database sparser (see sec-
tion 6.1).
the 96 items over-generated by the tagger are in the
?grey area? ? while we haven?t marked them, they
are not clearly mistakes. A common case are names
of national and religious groups, such as Muslim
or Turkish, or capitalizations of otherwise common
nouns for emphasis and elevation, such as Arts or
Masters. Given our objective to ground the queries
in items with specific referents, these are less sui-
table. If all such cases are counted as mistakes, the
tagger?s precision is 82%.
5.2 Selection of NPs
We employ a grammar-based approach for selecting
NPs. We use the Stanford dependency parser (de
Marneffe et al, 2006; Klein and Manning, 2003) to
determine dependency relations.
In order to find out which dependency paths con-
nect between named entities and clearly related NPs
in essays, we manually marked concepts related to
95 NEs in 10 randomly sampled essays. We marked
210 query-able concepts in total. The resulting 210
dependency paths were classified according to the
direction of the movement.
Out of the 210 paths, 51 (24%) contain a single
upward or downard step, that is, are cases where
the NE is the head of the constituent in which the
NP is embedded, or the other way around. Some
examples are shown in Figure 1. Note that the pre-
dicate connecting NE and NP is not lexicalized, but
the existence of connection is signaled by the close-
knit grammatical pattern.
The most prolific family of paths starts with an
upward step, followed by a sequences of 1-4 down-
wards steps; 71 (34%) of all paths are of this type.
Most typically, the first upward move connects the
NE to the predicate of which it is an argument, and,
down from there, to either the head of another argu-
ment (??) or to an argument?s head?s modifier (???).
These are explicit relations, where the relation is
typically lexicalized by the predicate.
We expand the context of extraction beyond a sin-
gle sentence only for NEs classified as PERSON. We
apply a gazetteer of private names by gender from
US Census 2010 to expand a NE of a given gen-
der with the appropriate personal pronouns; a word
that is a part of the original name (only surname, for
4NE=Kroemer; NP=Heterojunction Bipolar Transitor
65
? a Nobel Prize in a science field
? Chaucer, in the 14 century, ...
? the prestige of the Nobel Prize
? Kidman?s talent
?? Kroemer received the Nobel Prize
??? Kroemer received the Nobel Prize for his work
on the Heterojunction Bipolar Transitor4
Figure 1: Examples of dependency paths used for query
construction.
example), is also considered an anaphor and a can-
didate for expansion. We expand the context of the
PERSON entity as long as the subsequent sentence
uses any of the anaphors for the name. This way, we
hope to capture an extended discussion of a named
entity and construct queries around its anaphoric
mentions just as we do around the regular, NE men-
tion. A name that is not predominantly male or fe-
male is not expanded with personal pronouns. Ta-
ble 3 shows the distribution of queries automatically
generated from the sample of 2,000 essays.
? 2,817 15.9%
? 798 4.5%
?? 813 4.6%
?? 372 2.1%
?? 4,940 27.8%
??? 2,691 15.1%
???? 1,568 8.8%
??? 3,772 21.2%
total 17,771 100%
Table 3: Distribution of queries by path type.
6 Matching and Filtering Queries
6.1 Relaxation for improved matching
To estimate the coverage of the fact repository with
respect to the queries extracted from essays, we sub-
mit each query to the TextRunner repository in the
<NE,?,NP> format and record the number of times
the repository returned any matches at all. The per-
centage of matched queries is 21%. To increase the
chances of finding a match, we process the NP to re-
move determiners and pre-modifiers of the head that
are very frequent words, such as removing a very
from a very beautiful photograph.
Additionally, we produce three variants of the NP.
The first, NP1, contains only the sequence of nouns
ending with the head noun; in the example, NP1
would be photograph. The second variant, NP2,
contains only the word that is rarest in the whole
of NP. All capitalized words are given the lowest
frequency of 1. Thus, if any of the NP words are
capitalized, the NP2 would either contain an out of
vocabulary word to the left of the first capitalized
word, or the leftmost capitalized word. This means
that names would typically be split such that only the
first name is taken. For example, the NP the author
Orhan Phamuk would generate NP2 Orhan. When
no capitalized words exist, we take the rarest one,
thus a NP category 3 hurricane would yield NP2
hurricane. The third variant only applies to NPs
with capitalized parts, and takes the rightmost capi-
talized word in the query. Thus, the NP the actress
Nicole Kidman would yield NP3 Kidman.
Applying these procedures to every NP inflates
the number of actual queries posed to the TextRun-
ner repository by almost two-fold (31,211 instead of
17,771), while yielding a 50% increase in the num-
ber of cases where at least one variant of the original
query had at least one match against the repository
(from 21% to 35%).
6.2 Match-specific filters
In order to zero in on matches that correpond to fac-
tual statements and indeed pertain to the queried ar-
guments, we implement a number of filters.
Predicate filters
We filter out modal and hedged predicates, using
lists of relevant markers. We remove predicates like
might turn out to be or possibly attended, as well as
future tense predicates (marked with will).
Argument filters
For matches that passed the predicate filters, we
check the arguments. Let mARG be the actual
string that matched ARG (ARG ?{NE,NP}). Let
EC (Essay Context) refer to source sentence(s) in
66
the essay.5 We filter out the following matches:
? Capitalized words follow ARG in mARG that
are not in EC;
? >1 capitalized or rare words precede ARG in
mARG that are not in EC and not honorifics;
? mARG is longer than 8 words;
? More than 3 words follow ARG in mARG.
The filters target cases where mARG is more spe-
cific than ARG, and so the connection to ARG might
be tenuous, such as ARG=Harriet Beecher Stowe,
mARG = Harriet Beecher Stowe Center.
6.3 Filters based on overall pattern of matches
6.3.1 Negation filter
For all matches for a given query that passed the
filters in section 6.2, we tally positive vs negative
predicates.6 If the ratio of negative to positive is
above a threshold (we use 0.1), we consider the
query an unsuitable candidate for being ?potentially
common to all,? and therefore do not credit the au-
thor with having mentioned a fact.
This criterion of potential acceptance by a uni-
versal audience fails a query such as <Barack
Obama,?,US citizen>, based on the following pat-
tern of matches:
Count Predicate
10 is not
4 is
2 was always
1 is really
1 isn?t
1 was not
In a similar fashion, an essay writer?s statement
that ?The beating of Rodney King in Los Angeles
... made for tense race relations? is not quite in ac-
cord with the 16 hits garnered by the statement ?The
Los Angeles riots were not caused by the Rodney
King verdict,? against other hits with predicates like
erupted after, occurred after, resulted from, were
sparked by, followed.
5A single sentence, unless anaphor-based expansion was
carried out; see section 5.2.
6We use a list of negation markers to detect those.
Somewhat more subtly, the connection between
Albert Einstein and atomic bomb, articulated as ?For
example, Albert Einstein?s accidental development
of the atomic bomb has created a belligerent tech-
nological front? by a test-taker, is opposed by 6 hits
with the predicate did not build against matches with
predicates such as paved the way to, led indirectly
to, helped in, created the theory of. The conflicting
accounts seem to reflect a lack of consensus on the
degree of Einstein?s responsibility.
The cases above clearly demonstrate the implica-
tions of the argumentative notion of facts used in
our project. Facts are statements that the audience is
prepared to accept without further justification, dif-
ferently from arguments, and even from presump-
tions (statements about what is normal and likely),
for which, as Perelman and Olbrechts-Tyteca (1969)
observe, ?additional justification is beneficial for
strengthening the audience?s adherence.? Certainly
in the Obama case and possibly in others, a different
notion of factuality, for example, a notion that em-
phasizes availability of legally acceptable suppor-
ting evidence, would have led to a different result.
Yet, in an ongoing instance of argumentation, the
mere need to resort to such a proof is already a sign
that the audience is not prepared to accept a state-
ment as a fact.
6.4 Additional filters
We also implemented a number of filters aimed at
detecting excessive diversity in the matches, which
could suggest that there is no clear and systema-
tic relation between the NE and the NP. The filters
are conjunctions of thresholds operating over mea-
sures such as purity of matches (percentage of exact
matches in NE or NP), degree of overlap of non-pure
matches with the context of the query in the essay,
clustering of the predicates (recurrence of the same
predicates across matches), general frequencies of
NE and NP.
7 Evaluation
7.1 Manual check of queries
A manual check of a small subset of queries was ini-
tially intended as an interim evaluation of the query
construction process, to see how often the produced
queries are deficient candidates for later verification.
67
However, we also decided to include a human fact-
check of the queries that were found to be verifiable,
to see the kinds of factual mistakes made in essays.
A research assistant was asked to classify 500
queries into Wrong (the NE and NP are not
related in the essay), Trivial (almost any NE
could be substituted, as in <WWI,?, Historians>),
Subjective (<T.S.Eliot,?,the most frightening poet
of all time>), VC ? verifiable and correct, VI ? veri-
fiable and incorrect. Table 4 shows the distribution.
W T S VC VI
18% 13% 13% 54% 2%
Table 4: The distribution of query types for 500 queries.
Queries classified as Wrong (18%) mostly cor-
respond to parser mistakes. Trivial and Subjective
queries, while not attributing to the author connec-
tions that she has not made, are of questionable value
as far as fact-checking goes. Perhaps the most sur-
prising figure is the meager amount of verifiable and
incorrect queries. Examples of relevant statements
from essays include (NE and NP are boldfaced):
? For example, Paul Gaugin who was a sucess-
ful business man, with a respectable wife and
family, suddenly gave in to the calling of the
arts and left his life. (He was a failing busi-
nessman immediately before leaving family.)
? For example, in Jane Austin?s Little Women,
she portrays the image of a lovely family and
the wonders of womenhood. (The book is by
Louisa May Alcott.)
? This occurrence can be seen with the Rod-
ney King problem in California during the late
1980?s. (The Rodney King incident occurred
on March 3, 1991).
? We see the philosophers Aristotle, Plato,
Socrates and their practical writings of the
political problems and issues of the day.
(Socrates is not known to have left writings.)
First, we observe that factual mistakes are rare.
Furthermore, they seem to pertain to one in a series
of related facts, most of which are correct and testify
to the author?s substantial knowledge about the mat-
ter ? consider Paul Gaugin?s biography or the con-
tents of ?Little Women? in the examples above. It
is therefore unclear how detrimental the occasional
factual ?glitches? are to the quality of the essay.
8 Application to Essay Scoring
We show Pearson correlations between human
scores given to essays and a number of characte-
ristics derived from the work described here, as well
as the partial correlations when the effect of essay
length is factored out. We calculated both the cor-
relations using raw numbers and on a logarithmic
scale, with the latter generally producing higher cor-
realtions. Therefore, we are reporting the correla-
tions between grade and the logarithm of the rele-
vant characteristic. The characteristics are:
#NE The number of NE tokens in an essay.
#Queries The number of queries generated by the
system from the given essay (as described in
section 5.2).
#Matched Queries The number of queries for
which a match was found in the TextRunner
database. If the original query or any of its ex-
pansion variants (see section 6.1) had matches,
the query contributes a count of 1.
#Filtered Matches The number of queries that
passed the filters introduced in section 6. If the
original query or any of its expansion variants
passed the filters, the query contributes a count
of 1.
Table 5 shows the results. First, we find that all
correlations are significant at p=0.05, as well as the
partial correlations exluding the effect of length for 7
out of 10 prompts. All correlations are positive, that
is, the more factual information a writer employs in
an essay, the higher the grade ? beyond the oft re-
ported correlations between the grade and the length
of an essay (Powers, 2005).
Second, we notice that all characteristics ? from
the number of named entities to the number of fil-
tered matches ? produce similar correlation figures.
Third, there are large differences between average
numbers of named entities per essay across prompts.
68
Prompt NE Pearson Corr. with Grade Partial Corr. Removing Length
#NE #Q #Mat. # Filt. #NE #Q #Mat. # Filt.
P1 280 0.144 0.154 0.182 0.185 0.006 0.019 0.058 0.076
P2 406 0.265 0.259 0.274 0.225 0.039 0.053 0.072 0.069
P3 452 0.245 0.225 0.188 0.203 0.049 0.033 0.009 0.051
P4 658 0.327 0.302 0.335 0.327 0.165 0.159 0.177 0.160
P5 704 0.470 0.477 0.473 0.471 0.287 0.294 0.304 0.305
P6 750 0.429 0.415 0.388 0.373 0.271 0.242 0.244 0.257
P7 785 0.470 0.463 0.479 0.469 0.302 0.302 0.341 0.326
P8 838 0.423 0.390 0.406 0.363 0.264 0.228 0.266 0.225
P9 919 0.398 0.445 0.426 0.393 0.158 0.209 0.233 0.219
P10 986 0.455 0.438 0.375 0.336 0.261 0.257 0.170 0.175
AV. 678 0.363 0.357 0.353 0.335 0.180 0.180 0.187 0.186
Table 5: Pearson correlation and partial correlation removing the effect of length between a number of characteristics
(all on a log scale) and the grade. The second column shows the total number of identified named entities in the
200-essay sample from the given prompt. The prompts are sorted by the second column.
Generally, the higher the number, the better the num-
ber of named entities in the essay predicts its grade
(the more NEs the higher the grade). This suggests
that the use of named entities might be relatively
irrelevant for some prompts, and much more rele-
vant for others. For example, prompt P10 reads
?The arts (painting, music, literature, etc.) reveal
the otherwise hidden ideas and impulses of a soci-
ety,? thus practically inviting exemplification using
specific works of art or art movements, while suc-
cess with prompt P1 ? ?The human mind will al-
ways be superior to machines because machines are
only tools of human minds? ? is apparently not as
dependent on named entity based exemplification.
Excluding prompts with smaller than average total
number of named entities (<678), the correlations
average 0.40-0.44 across the various characteristics,
with partial correlations averaging 0.25-0.26.
9 Discussion and Conclusion
9.1 Summary of the main result
In this article, we proposed a way to measure the
use of factual information in text-taker essays. We
demonstrated that the use of factual information is
indicative of essay quality, observing positive corre-
lations between the count of instances of fact-use in
essays and the grade of the essay, beyond what can
be attributed to a correlation between the total num-
ber of words in an essay and the grade.
9.2 What is driving the correlations?
We also investigated which of the components of
the fact-use measure were responsible for the ob-
served correlations. Specifically, we considered (a)
the number instances of fact-use that were verified
against a database of human-produced assertions,
filtered for controversy and excessive diversity; (b)
the number of instances of fact-use that were verified
against the database, without subsequent filtering;
(c) the number of instances of fact-use identified in
an essay (without checking against the database); (d)
the number of named entities used in an essay (with-
out constructing queries around the entity). These
steps correspond to a gradual relaxation of the full
fact-checking procedure all the way to a proxy mea-
sure that counts the number of named entities.
We observed similar correlations throughout the
relaxation procedure. We therefore conclude that the
number of named entities is the driving force behind
the correlations, with no observed effect of the query
construction and verification procedures.7 This re-
sult could be explained by two factors.
First, a manual check of 500 queries showed that
factual mistakes are rare ? only 2% of the queries
corresponded to factually incorrect statements. Fur-
thermore, mistakes were often accompanied by the
7While the trend is in the direction of an increase in Pearson
correlations from (a) to (d), the differences are not statistically
significant.
69
test-taker?s use of additional facts about the same en-
tity which were correct; this might alleviate the im-
pact of a mistake in the eyes of a grader.
Second, the query verification procedure applied
to only about 35% of the queries ? those for which
at least one match was found in the database, that
is, 65% of the queries could not be assessed using
the database of 2 bln extractions. The verification
procedure is thus much less robust than the proce-
dure for detecting named entities, which performs at
above >80% recall and precision.
9.3 Implications for automated scoring
Our results suggest that essays on a general topic
written by adults for a high-stakes exam contain
few incorrect facts, so the potential for a full fact-
checking system to improve correlations with grades
beyond merely detecting the potential for a factual
statement using a named entity recognizer is not
large. While a measure based on the number of
?verified? facts found in an essay demonstrated a
significant correlation with human scores beyond
the contribution of essay length, a simpler measure
based only on the number of named entities in the
essay demonstrated a similar relationship with hu-
man scores.
Given the similarity in the two features? empiri-
cal usefulness, it would seem that the feature that
counts the number of named entities in an essay is a
better candidate, due to its simplicity and robustness.
However, there is another perspective from which a
feature based only on the number of named entities
in an essay may be less suitable for use in scoring:
the perspective of construct validity, the degree to
which a test (or, in this case, a scoring system) ac-
tually measures what it purports to. As mentioned
above, the number of named entities in an essay is,
at best, a proxy measure,8 roughly indicative of the
referencing of factual statements in support of an ar-
gument within an essay. Because the measure itself
is not directly sensitive to how named entities are
used in the essay, though, even entities with no con-
nection to the essay topic would tend to contribute
to the score, and the measure is therefore vulnerable
to manipulation by test-takers.
8For a discussion of proxes vs trins in essay grading, see
(Page and Petersen, 1995).
An obvious strategy to exploit this scoring mecha-
nism would be to simply include more named enti-
ties in an essay, either interspersing them randomly
throughout the text, or including them in long lists of
examples to illustrate a single point. Such a blatant
approach could potentially be detected by the use of
a filter or advisory (Higgins et al, 2006; Landauer
et al, 2003) designed to identify anomalous writing
strategies. However, there could be more subtle ap-
proaches to exploiting such a feature. For example,
it is possible that test-takers might be inclined to in-
crease their use of named entities by adducing more
facts in support of an argument, and would go be-
yond the comfort zone of their actual factual know-
ledge, thus making more factual mistakes. Test gam-
ing strategies have been recognized as a threat to au-
tomated scoring systems for some time (Powers et
al., 2001), and there is evidence based on test tak-
ers? own self-reported behavior that this threat is real
(Powers, 2011). This is one major reason why large-
scale operational testing programs (such as GRE or
TOEFL) use automated essay scoring only in com-
bination with human ratings. In sum, the degree to
which a linguistic feature is predictive of human es-
say scores is not the only criterion for evaluation; the
washback effects of using the feature (on writing be-
havior and on instruction) must also be considered.
The second finding of this study is that the ef-
fectiveness of fact-checking for essay assessment is
compromised by the limited coverage of the wealth
of factual statements made by essay writers, with
only 35% of queries garnering any hits at all in a
large general-purpose database of assertions. It is
possible, however, that OpenIE technology can be
used to collect more focused repositories on specific
topics, such as the history of the American Civil
War, which could be used to assess responses to
tasks related to that particular subject matter. This
is one of the directions of our future research.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the 5th ACM conference on Digital
Libraries, pages 85?94. ACM.
Yigal Attali and Jill Burstein. 2006. Automated Es-
70
say Scoring With e-rater R?V.2. Journal of Technology,
Learning, and Assessment, 4(3).
Michele Banko and Oren Etzioni. 2008. The Tradeoffs
Between Open and Traditional Relation Extraction. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 28?36,
Columbus, OH, June. Association for Computational
Linguistics.
Beata Beigman Klebanov, Jill Burstein, Nitin Madnani,
Adam Faulkner, and Joel Tetreault. 2012. Building
Subjectivity Lexicon(s) From Scratch For Essay Data.
In Proceedings of CICLING, New Delhi, India.
Tao-Hsing Chang, Chia-Hoang Lee, and Yu-Ming
Chang. 2006. Enhancing Automatic Chinese Es-
say Scoring System from Figures-of-Speech. In Pro-
ceedings of the 20th Pacific Asia Conference on Lan-
guage, Information and Computation, pages 28?34.
Yen-Yu Chen, Chien-Liang Liu, Chia-Hoang Lee, and
Tao-Hsing Chang. 2010. An Unsupervised Auto-
mated Essay Scoring System. IEEE Transactions on
Intelligent Systems, 25(5):61?67.
Nancy Chinchor, Lynette Hirschman, and David Lewis.
1993. Evaluating Message Understanding Systems:
An Analysis of the Third Message Understanding
Conference (MUC-3). Computational Linguistics,
19(3):409?449.
Dmitry Davidov and Ari Rappoport. 2009. Geo-mining:
Discovery of Road and Transport Networks Using Di-
rectional Patterns. In Proceedings of EMNLP, pages
267?275.
Arijit De and Sunil Kopparapu. 2011. An unsupervised
approach to automated selection of good essays. In
Recent Advances in Intelligent Computational Systems
(RAICS), 2011 IEEE, pages 662 ?666.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating Typed De-
pendency Parses from Phrase Structure Parses. In Pro-
ceedings of LREC, pages 449?454, Genoa, Italy, May.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel Weld. 2008. Open information extraction from
the web. Commun. ACM, 51(12):68?74.
Jenny Finkel, Trond Grenager, and Christopher Manning.
2005. Incorporating Non-local Information into In-
formation Extraction Systems by Gibbs Sampling. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 363?370,
Ann Arbor, MI, June. Association for Computational
Linguistics.
Ralph Grishman and Beth Sundheim. 1995. Design of
the MUC-6 evaluation. In Proceedings of MUC, pages
1?11.
Derrick Higgins, Jill Burstein, and Yigal Attali. 2006.
Identifying off-topic student essays without topic-
specific training data. Natural Language Engineering,
12(2):145?159.
Tsunenori Ishioka and Masayuki Kameda. 2006. Auto-
mated Japanese Essay Scoring System based on Arti-
cles Written by Experts. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 233?240, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Tuomo Kakkonen and Erkki Sutinen. 2004. Automatic
assessment of the content of essays based on course
materials. In Proceedings of the International Confer-
ence on Information Technology: Research and Edu-
cation, pages 126?130, London, UK.
Tuomo Kakkonen, Niko Myller, Jari Timonen, and Erkki
Sutinen. 2005. Automatic Essay Grading with Prob-
abilistic Latent Semantic Analysis. In Proceedings of
the Second Workshop on Building Educational Appli-
cations Using NLP, pages 29?36, Ann Arbor, Michi-
gan, June. Association for Computational Linguistics.
Dan Klein and Christopher Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
Saul Kripke. 1980. Naming and Necessity. Harvard Uni-
versity Press.
Thomas Landauer, Darrell Laham, and Peter Foltz. 2003.
Automated scoring and annotation of essays with the
Intelligent Essay Assessor. In Mark Shermis and Jill
Burstein, editors, Automated essay scoring: A cross-
disciplinary perspective, pages 87?112. Lawrence Erl-
baum Associates, Mahwah, New Jersey.
Leah Larkey. 1998. Automatic essay grading using text
categorization techniques. In Proceedings of SIGIR,
pages 90?95, Melbourne, AU.
Beno??t Lemaire and Philippe Dessus. 2001. A System to
Assess the Semantic Content of Student Essays. Jour-
nal of Educational Computing Research, 24:305?320.
Annie Louis and Derrick Higgins. 2010. Off-topic essay
detection using short prompt texts. In Proceedings of
the NAACL HLT 2010 Fifth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 92?95, Los Angeles, California, June. Associ-
ation for Computational Linguistics.
Boyan Onyshkevych. 1993. Template design for infor-
mation extraction. In Proceedings of MUC, pages 19?
23.
Ellis Page and Nancy Petersen. 1995. The computer
moves into essay grading: Updating the ancient test.
Phi Delta Kappan, 76:561?565.
Cha??m Perelman and Lucie Olbrechts-Tyteca. 1969. The
New Rhetoric: A Treatise on Argumentation. Notre
71
Dame, Indiana: University of Notre Dame Press.
Translated by John Wilkinson and Purcell Weaver
from French original published in 1958.
Donald Powers, Jill Burstein, Martin Chodorow, Mary
Fowles, and Karen Kukich. 2001. Stumping
E-Rater: Challenging the Validity of Automated
Essay Scoring. ETS research report RR-01-03,
http://www.ets.org/research/policy research reports/rr-
01-03.
Donald Powers. 2005. ?Wordiness?: A selective review
of its influence, and suggestions for investigating
its relevance in tests requiring extended written
responses. ETS research memorandum RM-04-08,
http://www.ets.org/research/policy research reports/rm-
04-08.
Donald Powers. 2011. Scoring the TOEFL
Independent Essay Automatically: Re-
actions of Test Takers and Test Score
Users. ETS research manuscript RM-11-34,
http://www.ets.org/research/policy research reports/rm-
11-34.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a Question Answering System.
In Proceedings of ACL, pages 41?47.
Carolyn Rose?, Antonio Roqueand, Dumisizwe Bhembe,
and Kurt VanLehn. 2003. A hybrid text classifica-
tion approach for analysis of student essays. In Pro-
ceedings of the Second Workshop on Building Educa-
tional Applications Using NLP, pages 29?36.
72
Proceedings of the First Workshop on Metaphor in NLP, pages 11?20,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Argumentation-Relevant Metaphors in Test-Taker Essays
Beata Beigman Klebanov and Michael Flor
Educational Testing Service
{bbeigmanklebanov,mflor}@ets.org
Abstract
This article discusses metaphor annotation in
a corpus of argumentative essays written by
test-takers during a standardized examination
for graduate school admission. The quality of
argumentation being the focus of the project,
we developed a metaphor annotation proto-
col that targets metaphors that are relevant
for the writer?s arguments. The reliability of
the protocol is ?=0.58, on a set of 116 es-
says (the total of about 30K content-word to-
kens). We found a moderate-to-strong correla-
tion (r=0.51-0.57) between the percentage of
metaphorically used words in an essay and the
writing quality score. We also describe en-
couraging findings regarding the potential of
metaphor identification to contribute to auto-
mated scoring of essays.
1 Introduction
The goal of our project is to automatically score
the quality of argumentation in essays written for
a standardized graduate school admission exam.
Metaphors being important argumentative devices,
we report on annotating data for potential training
and testing of metaphor detection software that
would eventually be used for automated scoring of
essays.
Metaphors of various kinds can be relevant to ar-
gumentation. Some metaphors create vivid images
and function as examples or as organizing ideas be-
hind a series of examples. These are akin to pictures
that are worth a thousand words, and are highly po-
tent rhetorical devices. Metaphors of a less artistic
crafting ? more conventionalized ones, metaphors
that we ?live by? according to Lakoff and John-
son?s (1980) famous tenet ? subtly organize our
thinking and language production in culturally co-
herent ways.
For an example of a vivid metaphor that helps or-
ganize the essay, consider an essay on the relation-
ship between arts and government funding thereof
(see example 1). The author?s image of a piece of
art as a slippery object that escapes its captor?s grip
as a parallel to the relationship between an artist and
his or her patron/financier is a powerful image that
provides a framework for the author?s examples (in
the preceding paragraph, Chaucer is discussed as
a clever and subversive writer for his patron) and
elaborations (means of ?slippage?, like veiled ima-
gery, multiple meanings, etc).
(1) Great artistic productions, thus, tend to
rise above the money that bought them, to
bite, as it were, the hand that fed them.
This is not always so, of course. But
the point is that great art is too slippery
to be held in the grip of a governing
power. Through veiled imagery, multiple
meanings, and carefully guarded language,
a poem can both powerfully criticize a ruler
and not blow its cover.
For an example of a conventional metaphor, con-
sider the metaphor of construction/building. The
connotation of foundations is something essential,
old, solid, and lying deep, something that, once laid,
remains available for new construction for a long pe-
riod of time. It is often used to explain emergence
11
of things ? the existence of foundations (or support,
or basis) is contrasted with the (presumed) idea of
appearance out of nothing. Certain topics of discus-
sion are particularly amenable for arguments from
construction-upon-foundation. For example, con-
sider an essay question ?Originality does not mean
thinking something that was never thought before;
it means putting old ideas together in new ways,?
where an explanation of the emergence of something
is required. Examples 2-6 show excerpts from es-
says answering this prompt that employ the founda-
tion metaphor.
(2) The foundation of the United States was
also based on a series of older ideas into
which the fathers of our nation breathed
new life.
(3) History is a progressive passing on of ideas,
a process of building on the foundations laid
by the previous generations. New ideas can-
not stand if they are without support from
the past.
(4) New discoveries and ideas are also original
for some time, but eventually they become
the older, accepted pieces that are the build-
ing blocks for originality.
(5) Original thinking can include old ideas
which almost always are a basis for
continued thought leading to new ideas.
(6) Humans are born of their ancestors, thrive
from their intelligence, and are free to build
on the intellectual foundations laid.
The two types of metaphors exemplified above
have different argumentative roles. The first orga-
nizes a segment of an essay around it, firstly by
imposing itself on the reader?s mind (a property
rhetoricians call presence (Perelman and Olbrechts-
Tyteca, 1969; Gross and Dearin, 2003; Atkinson
et al, 2008)), secondly by helping select support-
ing ideas or examples that are congruent with the
parts of the target domain that are highlighted by the
metaphor (this property is termed framing (Lakoff,
1991; Entman, 2003)), such as the idea of evasive-
ness purported by the ART AS A SLIPPERY OB-
JECT metaphor that is taken up both in the preceding
Chaucer example and in an elaboration.
By contrast, metaphors ?we live by? without even
noticing, such as TIME IS MONEY or IDEAS ARE
BUILDINGS, are not usually accorded much reader
attention; they are processed by using the conven-
tional connotation of the word as if it were an
additional sense of that word, without invoking a
comparison between two domains (for processing
by categorization see (Bowdle and Gentner, 2005;
Glucksbeg and Haught, 2006)). Thus, the word
foundation is unlikely to elicit an image of a con-
struction site, but rather will directly invoke the con-
cept of something essential and primary. It is un-
clear to what extent such highly conventionalized
metaphors that are not deliberately construed as
metaphors have the framing property beyond fram-
ing induced by any lexical choice ? that of stress-
ing the chosen over the un-chosen alternative (Bil-
lig, 1996). Therefore, the fact that an essay writer
used a conventional metaphor is not in itself a mark
of rhetorical sophistication; it is possible, however,
that, if certain metaphorical source domains are par-
ticularly apt for the given target domain (as the do-
main of construction to discuss emergence), using
the metaphor is akin to choosing a solid though not
particularly original argument.
Our interest being in metaphors that play a role
in argumentation, we attempted to devise an annota-
tion protocol that would be specifically geared to-
wards identification of such metaphors. In what
follows, we review the literature on approaches to
annotating metaphors in a given discourse (sec-
tion 2), we describe the protocol and the annotation
procedure (section 3), report inter-annotator agree-
ment (section 4), quantify the relationship between
metaphorical density (percentage of metaphorically
used words in an essay) and essay quality as mea-
sured by essay score, as well as estimate the poten-
tial usefulness of metaphor detection for automated
scoring of essays (section 5.2).
2 Related Work
Much of the contemporary work on metaphor in psy-
chological and computational veins is inspired by
Lakoff and Johnson?s (1980) research on concep-
tual metaphor. Early work in this tradition concen-
trated on mapping the various conceptual metaphors
in use in a particular culture (Lakoff and Johnson,
12
1980; Lakoff and Ko?vecses, 1987; Ko?vecses, 2002).
Examples for various conceptual mappings are col-
lected, resulting in the Master Metaphor List (Lakoff
et al, 1991), showing common metaphorical map-
pings and their instances of use. For example, the
LIFE IS A JOURNEY conceptual metaphor that maps
the source domain of JOURNEY to the target domain
of LIFE is used in expressions such as:
? He just sails through life.
? He is headed for great things.
? If this doesn?t work, I?ll just try a different
route.
? She?ll cross that bridge when she comes to it.
? We?ve come a long way.
While exemplifying the extent of metaphoricity of
everyday English, such a list is not directly appli-
cable to annotating metaphors in discourse, due to
the limited coverage of the expressions pertaining to
each conceptual metaphor, as well as of the concep-
tual metaphors themselves.
Studies of discourse metaphor conducted in
the Critical Discourse Analysis tradition (Musolff,
2000; Charteris-Black, 2005) analyze a particular
discourse for its employment of metaphors. For
example, an extensive database of metaphors in
British and German newspaper discourse on Euro-
pean integration in the 1990s was compiled by Mu-
solff (2000); the author did not make it clear how
materials for annotation were selected.
A systematic but not comprehensive approach to
creating a metaphor-rich dataset is to pre-select ma-
terials using linguistic clues (Goatly, 1997) for the
presence of metaphor, such as utterly or so to speak.
Shutova and Teufel (2010) report precision statis-
tics for using different clues to detect metaphoric
sentences; expressions such as literally, utterly, and
completely indicate a metaphorical context in more
than 25% of cases of their use in the British National
Corpus. Such cues can aid in pre-selecting data for
annotation so as to increase the proportion of mate-
rials with metaphors beyond a random sample.
Another approach is to decide on the source do-
mains of interest in advance, use a dictionary or
thesaurus to detect words belonging to the domain,
and annotate them for metaphoricity (Stefanowitsch,
2006; Martin, 2006; Gedigan et al, 2006). Gedi-
gan et al (2006) found that more than 90% of
verbs belonging to MOTION and CURE domains in
a Wall Street Journal corpus were used metaphori-
cally. Fixing the source domain is potentially appro-
priate if common metaphorically used domains in a
given discourse have already been identified, as in
(Koller et al, 2008; Beigman Klebanov et al, 2008).
A complementary approach is to fix the target
domain, and do metaphor ?harvesting? in a win-
dow around words belonging to the target domain.
For example, Reining and Lo?neker-Rodman (2007)
chose the lemma Europe to represent the target do-
main in the discourse on European integration. They
extracted small windows around each occurrence of
Europe in the corpus, and manually annotated them
for metaphoricity. This is potentially applicable to
analyzing essays, because the main target domain of
the discourse is usually given in the prompt, such
as art, originality. The strength of this method is
its ability to focus on metaphors with argumentative
potential, because the target domain, which is the
topic of the essay, is directly involved. The weak-
ness is the possibility of missing metaphors because
they are not immediately adjacent to a string from
the target domain.
The Metaphor Identification Procedure (MIP) is
a protocol for exhaustive metaphoricity annota-
tion proposed by the Pragglejaz group (Pragglejaz,
2007). The annotator classifies every word in a
document (including prepositions) as metaphorical
if it has ?a more basic contemporary meaning? in
other contexts than the one it has in the current con-
text. Basic meanings are explained to be ?more con-
crete, related to bodily action, more precise, and his-
torically older.? The authors ? all highly qualified
linguists who have a long history of research collab-
oration on the subject of metaphor ? attained a kappa
of 0.72 for 6 annotators for one text of 668 words
and 0.62 for another text of 676 words. Shutova and
Teufel (2010) used the protocol to annotate content
verbs only, yielding kappa of 0.64 for 3 volunteer
annotators with some linguistic background, on a set
of sentences containing 142 verbs sampled from the
British National Corpus. It is an open question how
well educated lay people can agree on an exhaustive
metaphor annotation of a text.
13
We note that the procedure is geared towards con-
ceptual metaphors at large, not necessarily argumen-
tative ones, in that the protocol does not consider the
writer?s purpose in using the metaphor. For example,
the noun forms in ?All one needs to use high-speed
forms of communication is a computer or television
and an internet cable? is a metaphor according to
the MIP procedure, because the basic meaning ?a
shape of something? is more concrete/physical than
the contextual meaning ?a type of something,? so a
physical categorization by shape stands for a more
abstract categorization into types. This metaphor
could have an argumentative purport; for instance,
if the types in question were actually very blurred
and difficult to tell apart, by calling them forms (and,
by implications, shapes), they are framed as being
more clearly and easily separable than they actually
are. However, since the ease of categorization of
high-speed electronic communication into types is
not part of the author?s argument, the argumentative
relevance of this metaphor is doubtful.
3 Annotation Protocol
In the present study, annotators were given the fol-
lowing guidelines:
Generally speaking, a metaphor is a lin-
guistic expression whereby something is
compared to something else that it is
clearly literally not, in order to make a
point. Thus, in Tony Blair?s famous ?I
haven?t got a reverse gear,? Tony Blair is
compared to a car in order to stress his
unwillingness/inability to retract his state-
ments or actions. We would say in this
case that a metaphor from a vehicle do-
main is used.
. . . [more examples] . . .
The first task in our study of metaphor
in essays is to read essays and underline
words you think are used metaphorically.
Think about the point that is being made
by the metaphor, and write it down. Note
that a metaphor might be expressed by the
author or attributed to someone else. Note
also that the same metaphor can be taken
up in multiple places in a text.
During training, two annotators were instructed
to apply the guidelines to 6 top-scoring essays an-
swering a prompt about the role of art in society.
After they finished, sessions were held where the
annotators and one of the authors of this paper dis-
cussed the annotations, including explication of the
role played by the metaphor in the essay. A sum-
mary document that presents a detailed consensus
annotation of 3 of the essays was circulated to the
annotators. An example of an annotation is shown
below (metaphors are boldfaced in the text and ex-
plained underneath):
F. Scott Fitzgerald wrote, ?There is a dark
night in every man?s soul where it is
always 2 o?clock in the morning.? His
words are a profound statement of human
nature. Within society, we operate under a
variety of social disguises. Some of these
masks become so second nature that we
find ourselves unable to take them off.
(1) Dark night, 2 o?clock in the morning:
True emotions are not accessible (at 2
o?clock a person is usually asleep and un-
aware of what is going on) and frighten-
ing to handle on one?s own (scary to walk
at night alone); people need mediation to
help accessibility, and also company to al-
leviate the fear. Art provides both. This
metaphor puts forward the two main argu-
ments: accessibility and sharing.
(2) Masks, take off, disguises: could
be referring to the domain of the-
ater/performance. Makes the point that
what people do in real life to themselves
is superficially similar to what art (the-
ater) does to performers ? hiding their true
identity. In the theater, the hiding is tem-
porary and completely reversible at will,
there is really no such thing as inability to
take off the mask. The socially-inflicted
hiding is not necessarily under the per-
son?s control, differently from a theatrical
mask. Supports and extends the accessi-
bility argument: not just lack of courage
or will, but lack of control to access the
true selves.
14
The actual annotation then commenced, on a sam-
ple of essays answering a different question (the
data will be described in section 3.1). Annotators
were instructed to mark metaphors in the text using a
graphical interface that was specially developed for
the project. The guidelines for the actual annotation
are shown below:
During training, you practiced careful
reading while paying attention to non-
literal language and saw how metaphors
work in their context. At the annota-
tion stage, you are not asked to expli-
citly interpret the metaphor and identify
its argumentative contribution (or rather,
its attempted argumentative contribution),
only to mark metaphors, trusting your in-
tuition that you could try to interpret the
metaphor in context if needed.
Note that we have not provided formal defini-
tions of what a literal sense is in order to not inter-
fere with intuitive judgments of metaphoricity (dif-
ferently from Pragglejaz (2007), for example, who
provide definition of a basic sense). Neither have
we set up an explicit classification task, whereby an-
notators are required to classify every single word in
the text as a metaphor or a non-metaphor (again, dif-
ferently from Pragglejaz (2007)); in our task, anno-
tators were instructed to mark metaphors while they
read. This is in the spirit of Steen?s (2008) notion of
deliberate metaphors ? words and phrases that the
writer actually meant to produce as a metaphor, as
opposed to cases where the writer did not have a
choice, such as using in for an expression like in
time, due to the pervasiveness of the time-as-space
metaphor. Note, however, that Steen?s notion is
writer-based; since we have no access to the writers
of the essays, we side with an educated lay reader
and his or her perception of a metaphorical use.
The annotators were instructed to give the author
the benefit of the doubt and *not* to assume that a
common metaphor is necessarily unintenional:
When deciding whether to attribute to the
author the intention of making a point
using a metaphor, please be as liberal as
you can and give the author the benefit
of the doubt. Specifically, if something is
a rather common metaphor that still hap-
pens to fit nicely into the argument the au-
thor is making, we assume that the author
intended it that way.
To clarify what kinds of metaphors are excluded
by our guidelines, we explained as follows:
In contrast, consider cases where an ex-
pression might be perhaps formally clas-
sified as a metaphor, but the literal sense
cannot be seen as relevant to the author?s
argument. For example, consider the fol-
lowing sentence from Essay 2 from our
training material: ?Seeing the beauty of
nature or hearing a moving piece of music
may drive one to perhaps try to replicate
that beauty in a style of one?s own.? Look
at the italicized word ? the preposition in.
According to some theories of metaphor,
that would constitute a metaphorical use:
Literally, in means inside some container;
since style is not literally a container, the
use of in here is non-literal. Suppose now
that the non-literal interpretation invites
the reader to see style as a container. A
container might have more or less room,
can be full or empty, can be rigid or flex-
ible, can contain items of the same or dif-
ferent sorts ? these are some potential im-
ages that go with viewing something as a
container, yet none of them seems to be
relevant to whatever the author is saying
about style, that is, that it is unique (one?s
own) and yet the result is not quite original
(replication).
The two annotators who participated in the task
hold BA degrees in Linguistics, but have no back-
ground in metaphor theory. They were surprised and
bemused by an example like in style, commenting
that it would never have occurred to them to mark it
as a metaphor. In general, the thrust of this proto-
col is to identify metaphorical expressions that are
noticeable and support the author?s argumentative
moves; yet, we targeted a reasonable timeline for
completing the task, with about 30 minutes per text,
therefore we did not require a detailed analysis of
the marked metaphors as done during training.
15
3.1 Data
Annotation was performed on 116 essays written on
the following topic: ?High-speed electronic commu-
nications media, such as electronic mail and tele-
vision, tend to prevent meaningful and thought-
ful communication.? Test-takers are instructed to
present their perspective on the issue, using rele-
vant reasons and/or examples to support their views.
Test-takers are given 45 minutes to compose an es-
say. The essays were sampled from the dataset an-
alyzed in Attali et al (2013), with oversampling
of longer essays. In the Attali et al (2013) study,
each essay was scored for the overall quality of En-
glish argumentative composition; thus, to receive the
maximum score, an essay should present a cogent,
well-articulated analysis of the complexities of the
issue and convey meaning skillfully. Each essay was
scored by 16 professional raters on a scale of 1 to 6,
allowing plus and minus scores as well, quantified
as 0.33 ? thus, a score of 4- is rendered as 3.67. This
fine-grained scale resulted in a high mean pairwise
inter-rater correlation (r=0.79). We use the average
of 16 raters as the final score for each essay. This
dataset provides a fine-grained ranking of the essays,
with almost no two essays getting exactly the same
score.
For the 116 essays, the mean length was 478
words (min: 159, max: 793, std: 142); mean score:
3.82 (min: 1.81, max: 5.77, std: 0.73). Table 1
shows the distribution of essay scores.
Score Number Proportion
of Essays of Essays
2 4 0.034
3 33 0.284
4 59 0.509
5 19 0.164
6 1 0.009
Table 1: Score distribution in the essay data. The first
column shows the rounded score. For the sake of pre-
sentation in this table, all scores were rounded to integer
scores, so a score of 3.33 was counted as 3, and a score
of 3.5 was counted as 4.
4 Inter-Annotator Agreement and Parts of
Speech
The inter-annotator agreement on the total of 55,473
word tokens was ?=0.575. In this section, we inves-
tigate the relationship between part of speech and
metaphor use, as well as part of speech and inter-
annotator agreement.
For this discussion, words that appear in the
prompt (essay topic) are excluded from all sets. Fur-
thermore, we concentrate on content words only (as
identified by the OpenNLP tagger1). Table 2 shows
the split of the content-word annotations by part
of speech, as well as the reliability figures. We
report information for each of the two annotators
separately, as well as for the union of their anno-
tations. We report the union as we hypothesize that
a substantial proportion of apparent disagreements
between annotators are attention slips rather than
substantive disagreements; this phenomenon was at-
tested in a previous study (Beigman Klebanov et al,
2008).
POS Count A1 A2 A1
?
A2 ?
All 55,473 2,802 2,591 3,788 0.575
Cont. 29,207 2,380 2,251 3,211 0.580
Noun 12,119 1,033 869 1,305 0.596
Adj 4,181 253 239 356 0.525
Verb 9,561 1,007 1,039 1,422 0.563
Adv 3,346 87 104 128 0.650
Table 2: Reliability by part of speech. The column Count
shows the total number of words in the given part of
speech across the 116 essays. Columns A1 and A2 show
the number of items marked as metaphors by annotators
1 and 2, respectively, while Column A1
?
A2 shows num-
bers of items in the union of the two annotations. The
second row presents the overall figure for content words.
Nouns constitute 41.5% of all content words; they
are 43.4% of all content-word metaphors for anno-
tator 1, 38.6% for annotator 2, and 40.6% for the
union of the two annotations. Nouns are therefore
represented in the metaphor annotated data in their
general distribution proportions. Of all nouns, 7%-
8.5% are identified as metaphors by a single annota-
tor, while 10.8% of the nouns are metaphors in the
union annotation.
1http://opennlp.apache.org/index.html
16
Verbs are 32.7% of all content words; they are
42.3% of all content-word metaphors for annotator
1, 46.2% for annotator 2, and 44.3% in the union.
Verbs are therefore over-represented in the metaphor
annotated data relative to their general distribution
proportions. Of all verbs, 10.5%-10.9% are identi-
fied as metaphors by a single annotator, while 14.9%
are metaphors in the union annotation.
Adjectives are 14.3% of all content words; they
are 10.6% of all content-word metaphors for anno-
tator 1, 10.6% for annotator 2, and 11.1% in the
union. Adjectives are therefore somewhat under-
represented in the metaphor annotated data with re-
spect to their general distribution. About 6% of ad-
jectives are identified as metaphors in individual an-
notations, and 8.5% in the union annotation.
Adverbs are 11.5% of all content words; they are
3.7% of all content-word metaphors for annotator 1
and 4.6% for annotator 2, and 4% in the union. Ad-
verbs are heavily under-represented in the metaphor
annotated data with respect to their general distri-
bution. Of all non-prompt adverbs, about 3-4% are
identified as metaphors.
The data clearly points towards the propensity of
verbs towards metaphoricity, relative to words from
other parts of speech. This is in line with reports in
the literature that identify verbs as central carriers of
metaphorical vehicles: Cameron (2003) found that
about 50% of metaphors in educational discourse are
realized by verbs, beyond their distributional propor-
tion; this finding prompted Shutova et al (2013) to
concentrate exclusively on verbs.
According to Goatly (1997), parts of speech dif-
fer in the kinds of metaphors they realize in terms of
the recognizability of the metaphorical use as such.
Nouns are more recognizable as metaphors than
other word classes for the following two reasons:
(1) Since nouns are referring expressions, they re-
veal very strongly the clashes between conventional
and unconventional reference; (2) Since nouns of-
ten refer to vivid, imaginable entities, they are more
easliy recognized than metaphors of other parts of
speech. Moreover, morphological derivation away
from nouns ? for example, by affixation ? leads to
more lexicalized and less noticeable metaphors than
the original nouns.
Goatly?s predictions seem to be reflected in inter-
annotator agreement figures for nouns versus adjec-
tives and verbs, with nouns yielding higher reliabi-
lity of identification than verbs and adjectives, with
the latter two categories having more cases where
only one but not both of the annotators noticed a
metaphorical use. Since adverbs are the most distant
from nouns in terms of processes of morphological
derivation, one would expect them to be less eas-
ily noticeable, yet in our annotation adverbs are the
most reliably classified category.
Inspecting the metaphorically used adverbs, we
find that a small number of adverbs cover the bulk
of the volume: together (11), closer (11), away (10),
back (8) account for 46% of the adverbs marked by
annotator 1 in our dataset. Almost all cases of to-
gether come from a use in the phrasal verb bring
together (8 cases), in expressions like ?bringing the
world together into one cyberspace without borders?
or ?electronic mail could bring people closer to-
gether? or ?bringing society together.? In fact, 6 of
the 11 cases of closer are part of the construction
bring closer together, and the other cases have simi-
lar uses like ?our conversations are more meaningful
because we are closer through the internet.?
Interestingly, the metaphorical uses of away also
come from phrasal constructions that are used for
arguing precisely the opposite point ? that cyber-
communications drive people away from each other:
?email, instant messaging, and television support a
shift away from throughful communication,? ?mass
media and communications drive people away from
one another,? ?by typing a message ... you can easily
get away from the conversation.?
It seems that the adverbs marked for meta-
phoricity in our data tend to be (a) part of phrasal
constructions, and (b) part of a commonly made ar-
gument for or against electronic communication ?
that it (metaphorically) brings people together, or
(metaphorically) drives them apart by making the
actual togetherness (co-location) unnecessary for
communication. The adverbs are therefore not of the
derivationally complex kind Goatly has in mind, and
their noticeability might be enhanced by being part
of a common argumentative move in the examined
materials, especially since the annotators were in-
structed to look out for metaphors that support the
writer?s argument.
17
5 Metaphor and Content Scoring
In order to assess the potential of metaphor detec-
tion to contribute to essay scoring, we performed
two tests: correlation with essay scores and a regres-
sion analysis in order to check whether metaphor use
contributes information that is beyond what is cap-
tured by a state-of-art essay scoring system.
As a metaphor-derived feature, we calculated
metaphorical density, that is, the percentage of
metaphorically used words in an essay: All words
marked as metaphors in an essay were counted (con-
tent or other), and the total was divided by essay
length.
5.1 E-rater
As a reference system, we use e-rater (Attali and
Burstein, 2006), a state-of-art essay scoring system
developed at Educational Testing Service.2 E-rater
computes more than 100 micro-features, which are
aggregated into macro-features aligned with specific
aspects of the writing construct. The system in-
corporates macro-features measuring grammar, us-
age, mechanics, style, organization and develop-
ment, lexical complexity, and vocabulary usage. Ta-
ble 3 gives examples of micro-features covered by
the different macro-features.
Macro-Feature Example Micro-Features
Grammar, agreement errors
Usage, and verb formation errors
Mechanics missing punctuation
Style passive, very long or very short
sentences, excessive repetition
Organization use of discourse elements:
and thesis, support, conclusion
Development
Lexical average word frequency
Complexity average word length
Vocabulary similarity to vocabulary in
high- vs low-scoring essays
Table 3: Features used in e-rater (Attali and Burstein,
2006).
E-rater models are built using linear regression on
large samples of test-taker essays. We use an e-rater
model built at Educational Testing Service using
2http://www.ets.org/erater/about/
a large number of essays across different prompts,
with no connection to the current project and its
authors. This model obtains Pearson correlations
of r=0.935 with the human scores. The excellent
performance of the system leaves little room for
improvement; yet, none of the features in e-rater
specifically targets the use of figurative language, so
it is interesting to see the extent to which metaphor
use could help explain additional variance.
5.2 Results
We found that metaphorical density attains correla-
tion of r=0.507 with essay score using annotations
of annotator 1, r=0.556 for annotator 2, and r=0.570
using the union of the two annotators. It is clearly
the case that better essays tend to have higher pro-
portions of metaphors.
We ran a regression analysis with essay score as
the dependent variable and e-rater raw score and
metaphor density in the union annotation as two
independent variables. The correlation with essay
score improved from 0.935 using e-rater alone to
0.937 using the regression equation (the adjusted R2
of the model improved from 0.874 to 0.876). While
the contribution of metaphor feature is not statisti-
cally significant for the size of our dataset (n=116,
p=0.07), we are cautiously optimistic that metaphor
detection can make a contribution to essay scoring
when the process is automated and a larger-scale
evaluation can be performed.
6 Conclusion
This article discusses annotation of metaphors in
a corpus of argumentative essays written by test-
takers during a standardized examination for grad-
uate school admission. The quality of argumenta-
tion being the focus of the project, we developed a
metaphor annotation protocol that targets metaphors
that are relevant for the writer?s arguments. The
reliability of the protocol is ?=0.58, on a set of 116
essays (a total of about 30K content word tokens).
We found a moderate-to-strong correlation
(r=0.51-0.57) between the density of metaphors
in an essay (percentage of metaphorically used
words) and the writing quality score as provided by
professional essay raters.
As the annotation protocol is operationally effi-
18
cient (30 minutes per essay of about 500 words),
moderately reliable (?=0.58), and uses annotators
that do not possess specialized knowledge and
training in metaphor theory, we believe it is fea-
sible to annotate a large set of essays for the pur-
pose of building a supervised machine learning sys-
tem for detection of metaphors in test-taker essays.
The observed correlations of metaphor use with es-
say score, as well as the fact that metaphor use is
not captured by state-of-art essay scoring systems,
point towards the potential usefulness of a metaphor
detection system for essay scoring.
References
Nathan Atkinson, David Kaufer, and Suguru Ishizaki.
2008. Presence and Global Presence in Genres of Self-
Presentation: A Framework for Comparative Analysis.
Rhetoric Society Quarterly, 38(3):1?27.
Yigal Attali and Jill Burstein. 2006. Automated Es-
say Scoring With e-rater R?V.2. Journal of Technology,
Learning, and Assessment, 4(3).
Yigal Attali, Will Lewis, and Michael Steier. 2013. Scor-
ing with the computer: Alternative procedures for im-
proving reliability of holistic essay scoring. Language
Testing, 30(1):125?141.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing disagreements. In COL-
ING 2008 workshop on Human Judgments in Compu-
tational Linguistics, pages 2?7, Manchester, UK.
Michael Billig. 1996. Arguing and Thinking: A Rhetor-
ical Approach to Social Psychology. Cambridge Uni-
versity Press, Cambridge.
Brian Bowdle and Dedre Gentner. 2005. The career of
metaphor. Psychological Review, 112(1):193?216.
Lynne Cameron. 2003. Metaphor in Educational Dis-
course. Continuum, London.
Jonathan Charteris-Black. 2005. Politicians and
rhetoric: The persuasive power of metaphors. Pal-
grave MacMillan, Houndmills, UK and New York.
Robert Entman. 2003. Cascading activation: Contesting
the white houses frame after 9/11. Political Communi-
cation, 20:415?432.
Matt Gedigan, John Bryant, Srini Narayanan, and Bran-
imir Ciric. 2006. Catching metaphors. In PProceed-
ings of the 3rd Workshop on Scalable Natural Lan-
guage Understanding, pages 41?48, New York.
Sam Glucksbeg and Catrinel Haught. 2006. On the rela-
tion between metaphor and simile: When comparison
fails. Mind and Language, 21(3):360?378.
Andrew Goatly. 1997. The Language of Metaphors.
Routledge, London.
Alan Gross and Ray Dearin. 2003. Chaim Perelman.
Albany: SUNY Press.
Zoltan Ko?vecses. 2002. Metaphor: A Practical Intro-
duction. Oxford University Press.
Veronika Koller, Andrew Hardie, Paul Rayson, and Elena
Semino. 2008. Using a semantic annotation tool for
the analysis of metaphor in discourse. Metaphorik.de,
15:141?160.
George Lakoff and Mark Johnson. 1980. Metaphors we
live by. University of Chicago Press, Chicago.
George Lakoff and Zoltan Ko?vecses. 1987. Metaphors
of anger in japanese. In D. Holland and N. Quinn, edi-
tors, Cultural Models in Language and Thought. Cam-
bridge: Cambridge University Press.
George Lakoff, Jane Espenson, Adele Goldberg,
and Alan Schwartz. 1991. Master Metaphor
List, Second Draft Copy. Cognitive Linguisics
Group, Univeristy of California, Berkeley:
http://araw.mede.uic.edu/?alansz/metaphor/
METAPHORLIST.pdf.
George Lakoff. 1991. Metaphor and war: The metaphor
system used to justify war in the gulf. Peace Research,
23:25?32.
James Martin. 2006. A corpus-based analysis of context
effects on metaphor comprehension. In Anatol Ste-
fanowitsch and Stefan Gries, editors, Corpus-Based
Approaches to Metaphor and Metonymy. Berlin: Mou-
ton de Gruyter.
Andreas Musolff. 2000. Mirror images of Eu-
rope: Metaphors in the public debate about
Europe in Britain and Germany. Mu?nchen:
Iudicium. Annotated data is available at
http://www.dur.ac.uk/andreas.musolff/Arcindex.htm.
Cha??m Perelman and Lucie Olbrechts-Tyteca. 1969. The
New Rhetoric: A Treatise on Argumentation. Notre
Dame, Indiana: University of Notre Dame Press.
Translated by John Wilkinson and Purcell Weaver
from French original published in 1958.
Group Pragglejaz. 2007. MIP: A Method for Identifying
Metaphorically Used Words in Discourse. Metaphor
and Symbol, 22(1):1?39.
Astrid Reining and Birte Lo?neker-Rodman. 2007.
Corpus-driven metaphor harvesting. In Proceedings of
the Workshop on Computational Approaches to Figu-
rative Language, pages 5?12, Rochester, New York.
Ekaterina Shutova and Simone Teufel. 2010. Metaphor
Corpus Annotated for Source-Target Domain Map-
pings. In Proceedings of LREC, Valetta, Malta.
Ekaterina Shutova, Simone Teufel, and Anna Korhonen.
2013. Statistical metaphor processing. Computational
Linguistics, 39(1).
Gerard Steen. 2008. The Paradox of Metaphor: Why
We Need a Three-Dimensional Model of Metaphor.
Metaphor and Symbol, 23(4):213?241.
19
Anatol Stefanowitsch. 2006. Corpus-based approaches
to metaphor and metonymy. In Anatol Stefanow-
itsch and Stefan Gries, editors, Corpus-Based Ap-
proaches to Metaphor and Metonymy. Berlin: Mouton
de Gruyter.
20
Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 29?38,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Lexical Tightness and Text Complexity
Michael Flor Beata Beigman Klebanov Kathleen M. Sheehan
Educational Testing Service
Princeton, NJ, 08541, USA
{mflor,bbeigmanklebanov,ksheehan}@ets.org
Abstract
We present a computational notion of Lexical 
Tightness that measures global cohesion of con-
tent words in a text. Lexical tightness represents 
the degree to which a text tends to use words 
that are highly inter-associated in the language. 
We demonstrate the utility of this measure for 
estimating text complexity as measured by US 
school grade level designations of texts. Lexical 
tightness strongly correlates with grade level in 
a collection of expertly rated reading materials. 
Lexical  tightness  captures  aspects  of  prose 
complexity that are not covered by classic read-
ability indexes, especially for literary texts. We 
also present initial findings on the utility of this 
measure for automated estimation of complex-
ity for poetry.
1 Introduction
Adequate estimation of text complexity has a long 
and rich history.  Various readability metrics have 
been designed in the last 100 years (DuBay, 2004). 
Recent work on computational  estimation of text 
complexity for school- and college-level texts in-
cludes (Vajjala and Meurers 2012; Graesser et al, 
2011;  Sheehan et  al.,  2010;  Petersen  and Osten-
dorf, 2009; Heilman et al, 2006). Several commer-
cial  systems were recently evaluated in the Race 
To The Top competition (Nelson et al,  2012) in 
relation to the US Common Core State Standards 
for instruction (CCSSI, 2010). 
A variety of factors influence text  complexity, 
including vocabulary, sentence structure, academic 
orientation,  narrativity,  cohesion,  etc.  (Hiebert, 
2011)  and  corresponding  features  are  utilized  in 
automated  systems  of  complexity  evaluation
(Vajjala and Meurers, 2012; Graesser et al, 2011; 
Sheehan et al, 2010).
We focus on text complexity levels expressed as 
US school grade level equivalents1. Our interest is 
in  quantifying  the  differences  among  texts  (es-
say-length  reading  passages)  at  different  grade 
levels, for the purposes of automatically evaluating 
text complexity.  The work described in this paper 
is part of an ongoing project that investigates novel 
features indicative of text complexity.
The paper is organized as follows. Section 2.1 
presents our methodology for building word asso-
ciation profiles  for  texts.  Section 2.2 defines  the 
measure of lexical tightness (LT). Section 2.3 de-
scribes the datasets used in this study. Sections 3.1 
and  3.2  present  our  study  of  the  relationship 
between LT and text complexity.  Section 3.3 de-
scribes application to poetry. Section 3.4 evaluates 
an improved measure (LTR). Section 4 reviews re-
lated work.
2 Methodology
2.1 Word-Association Profile
We define WAPT ? a word association profile of a 
text T ? as the distribution of association values for 
all pairs of content words of text T, where the asso-
ciation values are estimated from a very large cor-
pus of texts. In this work, WAP is purely illustrat-
ive, and sets the stage for lexical tightness.
1 For age equivalents of grade levels see 
http://en.wikipedia.org/wiki/Educational_stage 
29
There exists an extensive literature on the use of 
word-association measures for NLP, especially for 
detection  of  collocations  (Pecina,  2010;  Evert, 
2008).  The  use  of  pointwise  mutual  information 
(PMI) with word-space models is noted in (Zhang 
et al, 2012; Baroni and Lenci, 2010; Mitchell and 
Lapata, 2008; Turney, 2001). We begin with PMI, 
and provide a modified measure in later sections.
To obtain comprehensive information about co-
occurrence behavior of words in English, we build 
a  first-order  co-occurrence  word-space  model 
(Turney  and  Pantel,  2010;  Baroni  and  Lenci, 
2010). The model was generated from a corpus of 
texts  of  about  2.5  billion  word  tokens,  counting 
non-directed co-occurrence in  a  paragraph,  using 
no  distance  coefficients  (Bullinaria  and  Levy, 
2007). About 2 billion word tokens come from the 
Gigaword  2003  corpus  (Graff  and  Cieri,  2003). 
Additional 500 million word tokens come from an 
in-house corpus containing texts from the genres of 
fiction and popular science. The matrix of 2.1x2.1 
million  word  types  and  their  co-occurrence  fre-
quencies, as well as single-word frequencies, is ef-
ficiently compressed using the TrendStream tech-
nology (Flor, 2013), resulting in a database file of 
4.7GB.  The  same  toolkit  allows  fast  retrieval  of 
word  probabilities  and  statistical  associations  for 
pairs of words.2 
In this study we use all content word tokens of a 
text.  We use the OpenNLP tagger3 to POS-tag a 
text and only take into account nouns, verbs, ad-
jective and adverbs.  We further  apply a stop-list 
(see Appendix A) to filter out auxiliary verbs.
To illustrate why WAP is an interesting notion, 
consider  this  toy  example:  The  texts  ?The  dog 
barked and wagged its tail? vs. ?Green ideas sleep  
furiously?. Their matrices of pairwise word associ-
ations are presented in Table 1. For the first text, 
all  the  six  content  word  pairs  score  above 
PMI=5.5.  On  the  other  hand,  for  ?Green  ideas 
sleep  furiously?,  all  the  six  content  word  pairs 
score below PMI=2.2. The first text puts together 
words that often go together in English, and this 
might be one of the reasons it seems easier to un-
derstand than the second text.
We use histograms to illustrate word-association 
profiles  for  real  texts,  containing  hundreds  of 
2 The distributional word-space model includes counts for 2.1 
million words and 1279 million word pairs (types). Associ-
ation measures are computed on the fly. 
3 http://opennlp.apache.org  
words.  For  a 60-bin histrogram spanning all  ob-
tained PMI values,  the  lowest  bin contains  pairs 
with PMI??5, the highest bin contains pairs with 
PMI>4.83, while the rest of the bins contain word 
pairs  (a,b)  with  -5<PMI(a,b)?4.83.  Figure  1 
presents  WAP  histograms  for  two  real  text 
samples, one for grade level 3 (age 8-9) and one 
for grade level 11 (age 16-17). We observe that the 
shape of distribution is normal-like. The distribu-
tion of GL3 text is shifted to the right ? it contains 
more highly associated word-pairs than the text of 
GL11.  In  a  separate  study  we  investigated  the 
properties of WAP distribution (Beigman-Kleban-
ov and Flor,  2013).  The normal-like  shape turns 
out to be stable across a variety of texts.
The dog barked and wagged its tail:
dog barked wagged tail
dog 7.02 7.64 5.57
barked 9.18 5.95
wagged 9.45
tail
Green ideas sleep furiously:
green ideas sleep furiously
green 0.44 1.47 2.05
ideas 1.01 0.94
sleep 2.18
furiously
Table 1. Word association matrices (PMI values) for 
two illustrative examples.
-5 -4 -3 -2 -1 0 1 2 3 4 5
0
1
2
3
4
5
6
7
8
9
10
TextGL11 TextGL3 PMI
Pe
rce
nta
ge
 of
 pa
irs
 of
 wo
rd 
tok
en
s
Figure  1.  Word  Association  Profiles  for  two  sample 
texts,  showing 60-bin histograms with smoothed lines 
instead of bars. The last bin of the histogram contains 
all pairs with PMI>4.83, hence the uptick at PMI=5.
30
2.2 Lexical Tightness
In this section we consider how to derive a single 
measure to represent each text for further analyses. 
Given the stable  normal-like  shape of  WAP,  we 
use average (mean) value per text for further in-
vestigations. We experimented with several associ-
ation measures.
Point-wise mutual information is defined as fol-
lows (Church and Hanks, 1990): 
PMI = log2 p ?a ,b ?p ?a? p ?b?
Normalized PMI (Bouma, 2009):
NPMI = 2 2( , )log log ( , )( ) ( )
p a b p a bp a p b
? ?
?? ?? ?
Unlike the standard PMI (Manning and Sch?tze, 
1999), NPMI has the property that its values are 
mostly constrained in the range {-1,1}, it is less in-
fluenced by rare extreme values, which is conveni-
ent  for  summing  values  over  multiple  pairs  of 
words.  Additional  experiments  on  our  data  have 
shown that ignoring negative NPMI values4.  works 
best.  Thus,  we  define  Positive  Normalized  PMI 
(PNPMI) for a pair of words  a and b as follows:
PNPMI(a,b) 
=  NPMI(a,b)  if NPMI(a,b)>0
=  0  if NPMI(a,b)?0
or if database has no data for 
co-occurrence of a and b.5
We define Lexical Tightness (LT) of a text as 
the mean value of PNPMI for all pairs of content-
word tokens in a text. Thus, if a text has N words, 
and after filtering we remain with K content words, 
the total number of pairs is K*(K-1)/2. 
Lexical tightness represents the degree to which 
a text tends to use words that are highly inter-asso-
ciated in the language. We conjecture that lexically 
tight texts (with higher values of LT) are easier to 
read  and  would  thus  correspond  to  lower  grade 
levels.
4 Ignoring negative values is described by Bullinaria and Levy 
(2007), also Mohammad and Hirst (2006).
5In our text collection, the average percentage of word-pairs 
not found in database is 5.5% per text.
2.3 Datasets
Our data consists of two sets of passages. The first 
set consists of 1012 passages (636K words) ? read-
ing materials that were used in various tests in state 
and national assessment  frameworks in the USA. 
Part of this set is taken from Sheehan et al (2007) 
(from testing programs and US state departments 
of education), and part was taken from the Standar-
dized State Test Passages set of the Race To The 
Top (RTT)  competition  (Nelson et  al.,  2012).  A 
distinguishing feature of this dataset is that the ex-
act grade level specification was available for each 
text. Table 2 provides the breakdown by grade and 
genre.  Text length in this set ranged between 27 
and 2848 words, with average 629 words. Average 
text length in the literary subset was 689 words and 
in the informational subset 560 words.
Grade
Level
Genre TotalInf Lit Other
1 2 4 1 7
2 2 4 3 9
3 49 63 10 122
4 54 77 8 139
5 47 48 15 110
6 44 43 6 93
7 39 61 6 106
8 73 66 19 158
9 25 25 3 53
10 29 52 2 83
11 18 25 0 43
12 47 20 22 89
Total 429 488 95 1012
Table 2. Counts of texts by grade level and genre, set #1 
Grade
Band GL
Genre TotalInf Lit Other
2?3 2.5 6 10 4 20
4?5 4.5 16 10 4 30
6?8 7 12 16 13 41
9?10 9.5 12 10 17 39
11+ ' 11.5 8 10 20 38
Total 54 56 58 168
Table  3. Counts of texts by grade band and genre, for 
dataset #2. GL specifies our grade level designation.
The second dataset comprises 168 texts (80.8K 
word  tokens)  from Appendix  B of  the  Common 
Core State Standards (CCSSI, 2010)6, not includ-
6 www.corestandards.org/assets/Appendix_B.pdf 
31
ing  poetry  items.  Exact  grade  level  designations 
are not  available for this set,  rather the texts are 
classified into grade bands, as established by ex-
pert  instructors  (Nelson  et  al.,  2012).  Table  3 
provides the breakdown by grade and genre. Text 
length  in  this  set  ranged  between  99  and  2073 
words,  with  average  481  words.  Average  text 
length in the literary subset was 455 words and in 
the informational subset 373 words.
Our  collection  is  not  very  large  in  terms  of 
typical datasets used in NLP research. However, it 
has two unique facets: grading and genres. Rather 
than having grade-ranges, set #1 has exact grade 
designations  for each text.  Moreover,  these  were 
rated by educational experts and used in state and 
nationwide testing programs. 
Previous research has emphasized the importan-
ce of genre effects for predicting readability and 
complexity (Sheehan et al, 2008) and for text ad-
aptation (Fountas and Pinnell, 2001). For all texts 
in our collection, genre designations (information-
al, literary, or 'other') were provided by expert hu-
man  judges  (we  used  the  designations  that  were 
prepared for the RTT competition,  Nelson et  al., 
2012). The 'other' category included texts that were 
somewhere in between literary and informational 
(e.g. biographies), as well as speeches, schedules, 
and manuals.
3 Results 
3.1 Lexical Tightness and Grade Level
Correlations of lexical tightness with grade level 
are shown in Table 4, for sets 1 and 2, the com-
bined set and for literary and informational subsets.
Our first finding is that lexical tightness has con-
siderable  and  statistically  significant  correlation 
with grade level, in each dataset, in the combined 
dataset  and  for  the  specific  subsets.  Notably the 
correlation  between  lexical  tightness  and  grade 
level is negative. Texts of higher grade levels are 
lexically less tight, as predicted.  
Although in these datasets grade level is mode-
rately correlated with text length, lexical tightness 
remains  considerably and significantly correlated 
with grade level even after removing the influence 
of correlations with text length.
Our second finding is that lexical tightness has a 
stronger correlation with grade level for the subset 
of literary texts (r=-0.610) than for informational 
texts (r=-0.499) in set #1. A similar pattern exists 
for set #2.
Figure 2 shows the average LT for each grade 
level,  for  texts  of  set  #1.  As the grade level  in-
creases,  average lexical tightness values decrease 
consistently, especially for informational and liter-
ary  texts.  There  are  two  'outliers'.  Informational 
texts for grade 12 show a sudden increase in lexic-
al tightness. Also, for genre 'other', grades 9,10,11 
are underepresented (see Table 2).
Subset N Correlation GL&length
Correlation 
GL&LT
Partial 
Correlation 
GL&LT
  Set #1
All 1012 0.362 -0.546 -0.472
Inf 429 0.396 -0.499 -0.404
Lit 488 0.408 -0.610 -0.549
  Set #2 (Common Core)
All 168 0.360 -0.441 -0.373
Inf 54 0.406 -0.313 -0.347
Lit 56 0.251 -0.546 -0.505
  Combined set
All 1180 0.339 -0.528 -0.462
Inf 483 0.386 -0.472 -0.369
Lit 544 0.374 -0.601 -0.545
Table  4.  Correlations  of  grade  level  (GL)  with  text 
length  and  lexical  tightness  (LT).  Partial  correlation 
GL&LT  controls  for  text  length.  All  correlations  are 
significant with p<0.04.
Figure 3 shows the average LT for each grade 
band, for texts of set #2. Here as well, decrease of 
lexical tightness is evident with increase of grade 
3 4 5 6 7 8 9 10 11 12
0.040
0.045
0.050
0.055
0.060
0.065
0.070
Lexical Tightness by Grade Level 
Inf Lit other
Grade Level
Le
xic
al 
Tig
htn
es
s
Figure 2. Lexical tighness by grade level and genre, 
for texts of grades 3-12 in dataset #1.
32
level. In this small set, informational texts show a 
relatively  smooth  decrease  of  LT,  while  literary 
texts  show a  sharp  decrease  of  LT in  transition 
from grade band 4-5 (4.5) to grade band 6-8 (7). 
Texts labelled as 'other' genre in set #2 are gener-
ally less 'tight' than literary or informational. Also 
for 'other' genre, bands 7-8, 9-10 and 11-12 have 
equal lexical tighness.
3.2 Grade Level and Readability Indexes
We have also calculated readability indexes for 
each passage in sets 1 and 2. We used well known 
readability formulae: Flesch-Kincaid Grade Level 
(FKGL: Kincaid et al, 1975), Flesch Reading Ease 
(FRE:  Flesch,  1948),  Gunning-Fog  Index  (GFI: 
Gunning, 19527), Coleman Liau Index (CLI: Cole-
man and Liau, 1975) and Automated Readability 
Index (ARI: Senter and Smith, 1967). All of them 
are based on measuring the length of words (in let-
ters  or  syllables)  and  length  of  sentences  (mean 
number  of  words).  For  our  collection,  we  also 
computed the average sentence length (avgSL, as 
word count),  average word frequency8 (avgWF ? 
over all  words),  and average word frequency for 
only  content  words  (avgWFCW).  Results  are 
shown in Table 5. 
Word frequency has quite low correlation with 
grade  level  in  both  datasets.  Readability  indexes 
7 Using the modern formula, as referenced at http://en.wikipe-
dia.org/wiki/Fog_Index 
8 For word frequency we use the unigrams data from the 
Google Web1T collection (Brants and Franz, 2006).
have a strong and consistent correlation with grade 
level.  For  dataset  #1,  readability  indexes  have 
much stronger correlation with grade level for in-
formational  texts  (|r| between  0.7  and  0.81)  as 
compared  to  literary  texts  (|r| between 0.53  and 
0.68), and a similar pattern is seen for dataset #2, 
with overall lower correlation.
The correlation of Flesch-Kincaid (FKGL) val-
ues with LT are  r=-0.444 for set #1,  r=-0.499 for 
the informational subset and  r=-0.541 for literary 
subset. The correlation is r=-0.182 in set #2. 
All Inf Lit
                  Set #1
N (texts): 1012 429 488
FKGL 0.705 0.807 0.673
FRE -0.658 -0.797 -0.629
GFI 0.701 0.810 0.673
CLI 0.537 0.722 0.537
ARI 0.670 0.784 0.653
avgSL 0.667 0.705 0.630
avgWF 0.205 0.128 0.249
avgWFCW 0.039 -0.039 0.095
                    Set #2 (Common Core)
N (texts): 168 54 56
FKGL 0.487 0.670 0.312
FRE9 -0.503 -0.586 -0.398
GFI 0.493 0.622 0.356
CLI 0.430 0.457 0.440
ARI 0.458 0.658 0.298
avgSL 0.407 0.701 0.203
avgWF 0.100 0.234 -0.109
avgWFCW 0.156 -0.053 -0.038
Table 5. Correlations of grade level with readability 
formulae and word frequency. All correlations apart 
from the italicized ones are significant with p<0.05. 
Abbreviations are explained in the text.
3.3 Lexical Tightness and Readability Indexes
To  evaluate  the  usefulness  of  LT  in  predicting 
grade level of passages, we estimate, using dataset 
#1, a linear regression model where the grade level 
is a dependent variable and Flesch-Kincaid score 
and lexical tightness are the two independent vari-
ables (features). First, we checked whether regres-
sion model improves over FKGL in the training set 
(#1). Then, we tested the regression model estim-
ated on 1012 texts of set #1, on 168 texts of set #2.
The  results  of  the  regression  model  on  1012 
texts  of  set  #1  (R2=0.565,  F(2,1009)=655.85, 
9 Flesch Reading Ease formula is inversely related to grade 
level, hence the negative correlations.
2.5 4.5 7 9.5 11.5
0.040
0.045
0.050
0.055
0.060
0.065
0.070
Lexical Tightness by Grade Level 
Inf Lit other
Grade Level
Le
xic
al 
Tig
htn
es
s
Figure 3. Lexical tighness by grade band and genre, 
for texts in dataset #2 (CommonCore).
33
p<0.0001)  indicate  that  the  amount  of  explained 
variance in the grade levels, as measured by the ad-
justed R2 of the model, improved from 0.497 (with 
FKGL alone,  multiple  r=0.705)  to  0.564 (FKGL 
with LT, r=0.752), that is an absolute improvement 
of 6.7%, and a relative improvement of 13.5%.
A separate regression model  was estimated on 
the  informational  texts  of  dataset  #1.  The  result 
(R2=0.664, F(2,426)=420.3, p<0.0001) reveals that 
adjusted  R2 of  the  model  improved  from  0.651 
(with FKGL alone, r=0.807) to 0.663 (FKGL with 
LT,  r=0.815).  Similarly,  a  regression  model  was 
estimated on the literary texts of set #1. The result 
(R2=0.522, F(2,485)=264.6, p<0.0001) reveals that 
adjusted R2 of the model improved from .453 (with 
FKGL alone,  r=0.673) to 0.520 (FKGL with LT, 
r=0.722). We observe that Flesch-Kincaid formula 
works well on informational texts, better than on 
literary  texts;  while  lexical  tightness  correlates 
with grade level in the literary texts better than it 
does in the informational texts. Thus, for informa-
tional texts, adding LT to FKGL provides a small 
(1.2%) but statistically significant improvement for 
predicting  GL.  For  literary  texts,  LT  provides  a 
considerable  improvement  (explaining  additional 
6.3% in the variance).
We use the regression model (FKGL & LT) es-
timated on the 1012 texts of set #1 and test it on 
168 texts of set #2. In dataset #2, FKGL alone cor-
relates with grade level with  r=0.487, and the es-
timated regression equation achieves correlation of 
r=0.574 (the difference between correlation coeffi-
cients  is  statistically  significant10,  p<0.001).  The 
amount of explained variance rises from 23.7% to 
33%,  an  almost  10%  improvement  in  absolute 
scores, and 39% relative improvement over FKGL 
readability index alone.
3.4 Analyzing Poetry
Since poetry is often included in school curricula, 
automated estimation of poem complexity can be 
useful. Poetry is notoriously hard to analyze com-
putationally. Many poems do not adhere to stand-
ard  punctuation  conventions,  have  peculiar  sen-
tence structure  (if  sentence boundaries are  indic-
ated at all). However, poems can be tackled with 
bag-of-words approaches. 
We have collected 66 poems from Appendix B 
of  the  Common  Core  State  Standards  (CCSSI, 
10Non-independent correlations test, McNemar (1955), p.148.
2010). Just as other materials from that source, the 
poems  are  classified  into  grade  bands,  as  estab-
lished by expert instructors. Table 6 provides the 
breakdown by grade band. Text length in this set 
ranges between 21 and 1100 words, the average is 
182, total word count is 12,030.
Grade Band GL N (texts)
K-1 1 12
2?3 2.5 15
4?5 4.5 9
6?8 7 11
9?10 9.5 7
11+ ' 11.5 12
Total 66
Table 6. Counts of poems by grade band, 
from Common Core Appendix B. 
GL specifies our grade level designation.
We computed lexical tightness for all 66 poems 
using the same procedure as for the two larger text 
collections. For computing correlations, texts from 
each grade band where assigned grade level as lis-
ted in Table 6. For the poetry dataset, LT has rather 
low  correlation  with  grade  level,  r=-0.271 
(p<0.002).  Text  length  correlation  with  GL  is 
r=0.218  (p<0.04).  Correlation  of  LT  and  text 
length is  r=-0.261 (p<0.02). Partial correlation of 
LT and GL, controlling for text length, is r=-0.227 
and only almost significant (p=0.069). In this data-
set,  the  correlation  of  Flesch-Kincaid  index 
(FKGL) with GL is r=0.291 (p<0.003) and Flesch 
Reading Ease (FRE)  has  a  stronger  correlation,  
r=-0.335 (p<0.003).
On examining some of the poems, we noted that 
the LT measure does not assign enough importance 
to recurrence of words within a text. For example, 
PNPMI(voice,  voice)  is  0.208,  while  the  ceiling 
value is 1.0. We modify the LT measure in the fol-
lowing way. Revised Association Score (RAS) for 
two words a and b:
=1.0   if a=b (token repetition)
RAS(a,b) =0.9  if a and b are inflectional variants of same lemma
= PNPMI(a,b)  otherwise
Revised Lexical Tightness (LTR) for  a text  is 
average of RAS scores for all accepted word pairs 
in the text (same filtering as before).
34
For the set of 66 poems, LTR moderately correl-
ates with grade level r=-0.353 (p<0.002). LTR cor-
relates  with  text  length  r=0.28  (p<0.02).  Partial 
correlation  of  LTR and  GL,  controlling  for  text 
length,  is  r=-0.312 (p<0.012).  This  suggests  that 
the revised measure captures some aspect of com-
plexity of the poems. 
We  re-estimated  the  regression  model,  using 
FRE readability and LTR, on all 1012 texts of set 
#1. We then applied this model  for prediction of 
grade levels  in  the  set  of  66  poems.  The model 
achieves  a  solid  correlation  with  grade  level, 
r=0.447 (p<0.0001). 
3.5 Revisiting Prose
We revisit the analysis of our two main datasets, 
set #1 and #2, using the revised lexical tightness 
measure  LTR.  Table  7  presents  correlations  of 
grade level with LT and LTR measures. Evidently, 
in each case LTR achieves better correlations. 
Subset N Correlation GL&LT
Correlation 
GL&LTR
  Set #1
All 1012 -0.546 -0.605
Inf 429 -0.499 -0.561
Lit 488 -0.610 -0.659
  Set #2 (Common Core)
All 168 -0.441 -0.492
Inf 54 -0.310 -0.336
Lit 56 -0.546 -0.662
  Combined set
All 1180 -0.528 -0.587
Inf 483 -0.472 -0.531
Lit 544 -0.601 -0.655
Table 7. Pearson correlations of grade level (GL) with 
lexical tightness (LT) and revised lexical tightness 
(LTR). All correlations are significant with p<0.04.
We re-estimated a linear regression model using 
the grade level as a dependent variable and Flesch-
Kincaid score (FKGL) and LTR as the two inde-
pendent variables. The results of regression model 
on  1012  texts  of  dataset  #1,  R2=0.583, 
F(2,1009)=706.07,  p<0.0001,  indicate  that  the 
amount of explained variance in the grade levels, 
as measured by the adjusted R2 of the model, im-
proved from 0.497 (with FKGL alone, r=0.705) to 
0.582 (FKGL with LTR, r=0.764), that is absolute 
improvement of 8.5%. For comparison, the regres-
sion model  with LT explained 0.564 of the vari-
ance, with 6.7% improvement over FKGL alone.
We re-estimated separate regression models for 
informational and literary subsets of set #1. For in-
formational  texts,  the  model  has  R2=0.667, 
F(2,426)=426.8,  p<0.0001,  R2 improving  from 
0.651 (with FKGL alone,  r=0.807) to adjusted R2 
0.666  (FKGL  with  LTR,  r=0.817).  Regression 
model with LT brought an improvement of 1.2%, 
the model with LTR provides 1.5%.
A regression model was estimated on the literary 
texts  of  dataset  #1.  The  result  (R2=0.560, 
F(2,485)=308.5, p<0.0001) reveals that adjusted R2 
of the  model  rose from .453 (with FKGL alone, 
r=0.673) to 0.558 (FKGL with LT,  r=0.748), that 
is 10.5% absolute improvement.  For comparison, 
LT brought 6.3% improvement. As with the origin-
al LT measure, LTR provides the bulk of improve-
ment for evaluation of literary texts.
The  regression  model  (FKGL  with  LTR), 
estimated on all 1012 texts of set #1, is tested on 
168  texts  of  set  #2.  In  set  #2,  FKGL  alone 
correlates with grade level with  r=0.487, and the 
prediction formula achieves correlation of r=0.585 
(the difference between correlation coefficients is 
statistically significant,  p<0.001).  The amount  of 
explained variance rises from 23.7% to 34.3%, that 
is 10.6% absolute improvement. Even better result 
of predicting grade level in set #2 is achieved using 
a  regression  model  of  Flesch  Readability  Ease 
(FRE) and LTR, estimated on all 1012 texts of set 
#1.  This  model  achieves  correlation  of r=0.616 
(p<0.0001) on the 168 texts of set #2, explaining 
37.9% of the variance. 
For  complexity  estimation,  in  both  proze  and 
poetry, LTR is more effective than simple LT.
4 Related Work 
Traditional readability formulae use a small num-
ber of surface features,  such as the average sen-
tence length (a proxy for syntactic complexity) and 
the average word length in syllables or characters 
(a  proxy to  vocabulary difficulty).  Such features 
are considered linguistically shallow, but they are 
surprisingly  effective  and  are  still  widely  used 
(DuBay, 2004;  ?tajner et al, 2012). The formulae 
or their features are incorporated in modern read-
ability classification systems (Vajjala and Meurers, 
2012;  Sheehan et  al.,  2010;  Petersen  and Osten-
dorf, 2009).
Developments  in  computational  linguistics  en-
abled inclusion of multiple features for capturing 
35
various  manifestations  of  text-related  readability. 
Peterson and Ostendorf (2009) compute a variety 
of features: vocabulary/lexical (including the clas-
sic 'syllables per word'), parse features, including 
average parse-tree height, noun-phrase count, verb-
phrase  count  and  average  count  of  subordinated 
clauses. They use machine learning to train classi-
fiers  for  direct  prediction of  grade level.  Vajjala 
and  Meurers  (2012)  also  use  machine  learning, 
with a wide variety of features, including classic 
features,  parse  features,  and  features  motivated 
from studies on second language acquisition, such 
as Lexical  Density and Type-Token Ratio.  Word 
frequency and its derivations, such as proportion of 
rare words, are utilized in many models of com-
plexity (Graesser et al, 2011; Sheehan et al 2010; 
Stenner et al, 2006; Collins-Thompson and Callan, 
2004).
Inspired by psycholinguistic research, two sys-
tems have explicitly set to measure textual cohe-
sion for estimations of readability and complexity: 
Coh-Metrix  (Graesser  et  al.,  2011)  and  Sour-
ceRater (Sheehan et al, 2010). One notion of cohe-
sion involved in those two systems is lexical cohe-
sion ? the amount of lexically/semantically related 
words in a text. Some amount of local lexical cohe-
sion can be measured via stem overlap of adjacent 
sentences, with averaging of such metric per text 
(McNamara et al, 2010). However, Sheehan et al 
(submitted) demonstrated that such measure is not 
well correlated with grade levels.
Perhaps closest to our present study is work re-
ported in Foltz et al (1998) and McNamara et al 
(2010). These studies used Latent Semantic Ana-
lysis,  which  reflects  second  order  co-occurrence 
associative relations, to characterize levels of lex-
ical similarity for pairs of adjacent sentences with-
in  paragraphs,  and  for  all  possible  pairs  of  sen-
tences  within  paragraphs.  McNamara  et  al.  have 
shown success in distinguishing lower and higher 
cohesion versions of the same text,  but  have not 
shown  whether  that  approach  systematically  ap-
plies for different texts and across grade levels.
Our study is a first demonstration that a measure 
of  lexical  cohesion  based  on  word-associations, 
and computed globally for the whole text, is an in-
dicative  feature  that  varies  systematically  across 
grade levels.
In the theoretical tradition, our work is closest in 
spirit to Michael Hoey?s theory of lexical priming 
(Hoey, 2005, 1991), positing that users of language 
internalize patterns of word co-occurrence and use 
them in reading, as well as when creating their own 
texts. We suggest that such patterns become richer 
with age and education, beginning with the most 
tight patterns at early age.
5 Conclusions 
In  this  paper  we  defined  a  novel  computational 
measure, lexical tightness. It represents the degree 
to which a text tends to use words that are highly 
inter-associated  in  the  language.  We  interpret 
lexical tightness as a measure of intra-text global 
cohesion.
This  study  presented  the  relationship  between 
lexical  tightness  and  text  complexity,  using  two 
datasets of reading materials (1180 texts in total), 
with  expert-assigned  grade  levels.  Lexical  tight-
ness has a significant correlation with grade levels: 
about  -0.6  overall.  The  correlation  is  negative: 
texts for lower grades are lexically tight, they use a 
higher  proportion  of  mildly  and  strongly  inter-
associated words; texts for higher grades are less 
tight, they use a lesser amount of inter-associated 
words.  The  correlation  of  lexical  tightness  with 
grade level is stronger for texts of the literary genre 
(fiction and stories) than for text belonging to in-
formational genre (expositional).
While lexical tightness is moderately correlated 
with  readability  indexes,  it  also  captures  some 
aspects of prose complexity that are not covered by 
classic  readability  indexes,  especially for  literary 
texts.  Regression analyses  on a  training set  have 
shown  that  lexical  tightness  adds  between  6.7% 
and 8.5% of explained grade level variance on top 
of  the  best  readability  formula.  The  utility  of 
lexical  tightness  was  confirmed  by  testing  the 
regression formula on a held out set of texts. 
Lexical  tightness  is  also moderately correlated 
with grade level (-0.353) in a small set of poems. 
In the same set,  Flesch Reading Ease readability 
formula  correlates  with  grade  level  at  -0.335.  A 
regression  model  using  that  formula  and  lexical 
tightness achieves correlation of  0.447 with grade 
level.  Thus we have shown that  lexical  tightness 
has good potential for analysis of poetry.
In future work, we intend to a) evaluate on lar-
ger datasets, and b) integrate lexical tightness with 
other  features  used  for  estimation  of  readability. 
We also intend to use this or a related measure for 
evaluation of writing quality.
36
References 
Baroni M. and Lenci A. 2010. Distributional Memory: 
A General Framework for Corpus-Based Semantics. 
Computational Linguistics, 36(4):673-721.
Beigman-Klebanov  B.  and  Flor  M.  2013.  Word 
Association  Profiles  and  their  Use  for  Automated 
Scoring of Essays. To appear in  Proceedings of the  
51th  Annual  Meeting  of  the  Association  for  
Computational Linguistics, ACL 2013.
Bouma  G.  2009.  Normalized  (Pointwise)  Mutual 
Information in Collocation Extraction. In:  Chiarcos, 
Eckart  de  Castilho  &  Stede  (eds), From  Form  to  
Meaning:  Processing  Texts  Automatically,  
Proceedings of the Biennial GSCL Conference 2009, 
31?40, Gunter Narr Verlag: T?bingen.
Brants T. and Franz A. 2006. ?Web 1T 5-gram Version 
1?.  LDC2006T13.  Linguistic  Data  Consortium, 
Philadelphia, PA.
Bullinaria  J.  and  Levy  J.  2007.  Extracting  semantic 
representations from word co-occurrence statistics: A 
computational  study.  Behavior  Research  Methods, 
39:510?526.
Church K. and Hanks P. 1990. Word association norms, 
mutual information and lexicography. Computational  
Linguistics, 16(1):22?29.
Coleman,  M.  and  Liau,  T.  L.  1975.  A  computer 
readability  formula  designed  for  machine  scoring. 
Journal of Applied Psychology, 60:283-284.
Collins-Thompson K. and Callan J. 2004. A language 
modeling approach  to  predicting reading  difficulty. 
Proceedings of HLT / NAACL 2004, Boston, USA.
Common Core State Standards Initiative (CCSSI) 2010. 
Common core state standards for English language 
arts & literacy in history/social studies, science and 
technical subjects. Washington, DC: CCSSO & 
National Governors Association. 
http://www.corestandards.org/ELA-Literacy    
DuBay W.H. 2004. The principles of readability. Impact 
Information:  Costa Mesa,  CA.  http://www.impact-
information.com/impactinfo/readability02.pdf    
Evert S. 2008. Corpora and collocations. In A. L?deling 
and  M.  Kyt?  (eds.),  Corpus  Linguistics:  An  
International  Handbook,  article  58.  Mouton  de 
Gruyter: Berlin.
Flesch R. 1948. A new readability yardstick. Journal of  
Applied Psychology, 32:221-233.
Flor M. 2013. A fast and flexible architecture for very 
large word n-gram datasets. Natural Language 
Engineering, 19(1):61-93.
Foltz P.W., Kintsch W., and Landauer T.K. 1998. The 
measurement of textual coherence with Latent 
Semantic Analysis. Discourse Processes, 25:285-
307.
Fountas I. and Pinnell G.S. 2001. Guiding Readers and 
Writers, Grades 3?6. Heinemann, Portsmouth, NH.
Graesser, A.C., McNamara, D.S., and Kulikowich, J.M. 
Coh-Metrix: Providing Multilevel Analyses of Text 
Characteristics.  Educational  Researcher,  40(5): 
223?234.
Graff,  D.  and  Cieri,  C.  2003.  English  Gigaword.  
LDC2003T05.  Linguistic  Data  Consortium, 
Philadelphia, PA.
Gunning  R.  1952.  The  technique  of  clear  writing. 
McGraw-Hill: New York.
Heilman,  M.,  Collins-Thompson,  K.,  Callan,  J.  and 
Eskenazi,  M.  2006.  Classroom  success  of  an 
intelligent  tutoring  system  for  lexical  practice  and 
reading comprehension. In  Proceedings of the Ninth  
International  Conference  on  Spoken  Language  
Processing, Pittsburgh, PA.
Hiebert,  E.H.  2011.  Using  multiple  sources  of  
information in establishing text complexity. Reading 
Research Report 11.03. TextProject Inc., Santa Cruz, 
CA.
Hoey  M.  1991.  Patterns  of  Lexis  in  Text.  Oxford 
University Press.
Hoey M. 2005. Lexical Priming: A new theory of words  
and language. Routledge, London.
Kincaid  J.P.,  Fishburne  R.P.  Jr,  Rogers  R.L.,  and 
Chissom B.S.  1975.  Derivation  of  new readability  
formulas   for  Navy  enlisted  personnel.  Research 
Branch  Report  8-75,  Millington,  TN:  Naval 
Technical  Training,  U.S.  Naval  Air  Station, 
Memphis, TN.
Manning,  C.  and  Sch?tze  H.  1999.  Foundations  of  
Statistical Natural Language Processing. MIT Press, 
Cambridge, MA.
McNamara,  D.S.,  Louwerse,  M.M.,  McCarthy,  P.M. 
and  Graesser  A.C.  2010.  Coh-metrix:  Capturing 
linguistic features of cohesion.  Discourse Processes, 
47:292-330.
McNemar,  Q.  1955.  Psychological  Statistics.  New 
York, John Wiley & Sons.
Mitchell J. and Lapata M.  2008. Vector-based models 
of semantic composition. In Proceedings of the 46th 
Annual  Meeting  of  the  Association  for  
Computational Linguistics, 236?244, Columbus, OH.
Mohammad  S.  and  Hirst  G.  2006.  Distributional 
Measures  of  Concept-Distance:  A  Task-oriented 
Evaluation. In  Proceedings of the 2006 Conference  
on  Empirical  Methods  in  Natural  Language  
Processing (EMNLP 2006), 35?43.
Nelson  J.,  Perfetti  C.,  Liben  D.,  and Liben  M. 2012. 
Measures of Text Difficulty: Testing their Predictive 
Value  for  Grade  Levels  and  Student  Performance. 
Student  Achievement  Partners.  Available  from 
http://www.ccsso.org/Documents/2012/Measures
%20ofText%20Difficulty_final.2012.pd  f   
Pecina  P.  2010.  Lexical  association  measures  and 
collocation  extraction.  Language  Resources  & 
Evaluation, 44:137?158.
37
Petersen  S.E.  and  Ostendorf  M.  2009.  A  machine 
learning  approach  to  reading  level  assessment. 
Computer Speech and Language, 23: 89?109.
Senter  R.J.  and  Smith  E.A.  1967.  Automated 
Readability Index. Report AMRL-TR-6620. Wright-
Patterson Air Force Base, USA.
Sheehan K.M.,  Kostin I.,  Napolitano D.,  and Flor  M. 
TextEvaluator:  Helping  Teachers  and  Test 
Developers  Select  Texts for  Use in Instruction and 
Assessment.  Submitted  to  The  Elementary  School  
Journal (Special Issue: Text Complexity).  
Sheehan K.M., Kostin I., Futagi Y., and Flor M. 2010. 
Generating automated text complexity classifications 
that  are  aligned  with  targeted  text  complexity 
standards. (ETS RR-10-28). ETS, Princeton, NJ.
Sheehan K.M., Kostin I., and Futagi Y. 2008. When do 
standard  approaches  for  measuring  vocabulary 
difficulty,  syntactic  complexity  and  referential 
cohesion yield biased estimates of text difficulty? In 
B.C.  Love,  K.  McRae,  &  V.M.  Sloutsky  (eds.), 
Proceedings  of  the 30th Annual  Conference  of  the  
Cognitive Science Society, Washington DC.
Sheehan  K.M.,  Kostin  I.,  and  Futagi  Y.  2007. 
SourceFinder:  A  construct-driven  approach  for 
locating  appropriately  targeted  reading 
comprehension  source  texts.  In  Proceedings  of  the  
2007  workshop  of  the  International  Speech  
Communication Association,  Special  Interest  Group 
on Speech and Language Technology in Education, 
Farmington, PA.
?tajner S., Evans R., Or?san C., and Mitkov R. 2012. 
What  Can  Readability  Measures  Really  Tell  Us 
About Text Complexity? In proceedings of workshop 
on   Natural  Language  Processing  for  Improving  
Textual Accessibility (NLP4ITA 2012), 14-22.
Stenner A.J.,  Burdick H., Sanford E., and Burdick D. 
2006.  How  accurate  are  Lexile  text  measures? 
Journal of Applied Measurement, 7(3):307-322.
Turney  P.D.  2001.  Mining  the  Web  for  Synonyms: 
PMI-IR versus  LSA on TOEFL.  In  proceedings  of 
European  Conference  on  Machine  Learning,  491?
502, Freiburg, Germany.
Turney  P.D.  and  Pantel  P.  2010.  From Frequency  to 
Meaning:  Vector  Space  Models  of  Semantics. 
Journal  of  Artificial  Intelligence  Research,  37:141-
188.
Vajjala  S.  and  Meurers  D.  2012.  On  Improving  the 
Accuracy of Readability Classification using Insights 
from Second Language Acquisition. In  proceedings 
of  The 7th Workshop on the Innovative Use of NLP  
for  Building  Educational  Applications,  (BEA-7), 
163?173, ACL.
Zhang  Z.,  Gentile  A.L.,  Ciravegna  F.  2012.  Recent 
advances in methods of lexical semantic relatedness 
?  a  survey.  Natural  Language  Engineering,  DOI: 
http://dx.doi.org/10.1017/S1351324912000125   
Appendix A
The list of stopwords utilized in this study:
a, an, the, at, as, by, for, from, in, on, of, off, up,  
to, out, over, if, then, than, with, have, had, has,  
can,  could,  do,  did,  does,  be,  am,  are,  is,  was,  
were, would, will,  it,  this,  that,  no, not,  yes, but,  
all,  and,  or,  any,  so,  every,  we,  us,  you,  also,  s
Note that most of these words would be excluded 
by POS filtering. However, the full  stop list  was 
applied anyway.
38
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 27?32,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Associative Texture is Lost in Translation
Beata Beigman Klebanov and Michael Flor
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541
{bbeigmanklebanov,mflor}@ets.org
Abstract
We present a suggestive finding regarding
the loss of associative texture in the pro-
cess of machine translation, using com-
parisons between (a) original and back-
translated texts, (b) reference and system
translations, and (c) better and worse MT
systems. We represent the amount of as-
sociation in a text using word association
profile ? a distribution of pointwise mu-
tual information between all pairs of con-
tent word types in a text. We use the av-
erage of the distribution, which we term
lexical tightness, as a single measure of
the amount of association in a text. We
show that the lexical tightness of human-
composed texts is higher than that of the
machine translated materials; human ref-
erences are tighter than machine trans-
lations, and better MT systems produce
lexically tighter translations. While the
phenomenon of the loss of associative tex-
ture has been theoretically predicted by
translation scholars, we present a measure
capable of quantifying the extent of this
phenomenon.
1 Introduction
While most current approaches to machine trans-
lation concentrate on single sentences, there is
emerging interest in phenomena that go beyond a
single sentence and pertain to the whole text being
translated. For example, Wong and Kit (2012)
demonstrated that repetition of content words is
a predictor of translation quality, with poorer
translations failing to repeat words appropriately.
Gong et al (2011) and Tiedemann (2010) present
caching of translations from earlier sections of a
document to facilitate the translation of its later
sections.
In scholarship that deals with properties of hu-
man translation of literary texts, translation is of-
ten rendered as a process that tends to deform
the original, and a number of particular aspects
of deformation have been identified. Specifically,
Berman (2000) discusses the problem of quantita-
tive impoverishment thus:
This refers to a lexical loss. Every
work in prose presents a certain pro-
liferation of signifiers and signifying
chains. Great novelist prose is ?abun-
dant.? These signifiers can be described
as unfixed, especially as a signified may
have a multiplicity of signifiers. For
the signified visage (face) Arlt employs
semblante, rosto and cara without jus-
tifying a particular choice in a particu-
lar sentence. The essential thing is that
visage is marked as an important real-
ity in his work by the use of three sig-
nifiers. The translation that does not re-
spect this multiplicity renders the ?vis-
age? of an unrecognizable work. There
is a loss, then, since the translation con-
tains fewer signifiers than the original.?1
While Berman?s remarks refer to literary trans-
lation, recent work demonstrates its relevance for
machine translation, showing that MT systems
tend to under-use linguistic devices that are com-
monly used for repeated reference, such as super-
ordinates or meronyms, although the pattern with
synonyms and near-synonyms was not clear cut
(Wong and Kit, 2012). Studying a complemen-
tary phenomenon of translation of same-lemma
lexical items in the source document into a target
language, Carpuat and Simard (2012) found that
when MT systems produce different target lan-
guage translations, they are stylistically, syntac-
tically, or semantically inadequate in most cases
1italics in the original
27
(see upper panel of Table 5 therein), that is, diver-
sifying the signifiers appropriately is a challeng-
ing task. For recent work on biasing SMT systems
towards consistent translations of repeated words,
see Ture et al (2012) and Xiao et al (2011).
Moving beyond single signifieds, or concepts,
Berman faults translations for ?the destruction of
underlying networks of signification?, whereby
groups of related words are translated without
preserving the relatedness in the target language.
While these might be unavoidable in any trans-
lation, we show below that machine translation
specifically indeed suffers from such a loss (sec-
tion 3) and that machine translation suffers from it
more than the human translations (section 4).
2 Methodology
We define WAPT ? a word association profile
of a text T ? as the distribution of PMI(x, y) for
all pairs of content2 word types (x, y) ?T.3 We es-
timate PMIs using same-paragraph co-occurrence
counts from a large and diverse corpus of about 2.5
billion words: 2 billion words come from the Gi-
gaword 2003 corpus (Graff and Cieri, 2003); an
additional 500 million words come from an in-
house corpus containing popular science and fic-
tion texts. We further define LTT ? the lexical
tightness of a text T ? as the average value of the
word association profile. All pairs of words in T
for which the corpus had no co-occurrence data
are excluded from the calculations. We note that
the database has very good coverage with respect
to the datasets in sections 3-5, with 94%-96%
of pairs on average having co-occurrence counts
in the database. A more detailed exposition of
the notion of a word association profile, includ-
ing measurements on a number of corpora, can be
found in Beigman Klebanov and Flor (2013).
Our prediction is that translated texts would be
less lexically tight than originals, and that better
translations ? either human or machine ? would be
tighter than worse translations, incurring a smaller
amount of association loss.
3 Experiment 1: Back-translation
For the experiment, we selected 20 editorials on
the topic of baseball from the New York Times
2We part-of-speech tag a text using OpenNLP tagger
(http://opennlp.apache.org) and only take into account com-
mon and proper nouns, verbs, adjectives, and adverbs.
3PMI = Pointwise Mutual Information
Annotated Corpus.4 The selected articles had
baseball annotated as their sole topic, and ranged
from 250 to 750 words in length. We expect
these articles to contain a large group of words
that reflects vocabulary that is commonly used in
discussing baseball and no other systematic sub-
topics. All articles were translated into French,
Spanish, Arabic, and Swedish, and then translated
back to English, using the Google automatic trans-
lation service. Our goal is to observe the effect of
the two layers of translation (out of English and
back) on the lexical tightness of the resulting texts.
Since baseball is not a topic that is commonly
discussed in the European languages or in Ara-
bic, this is a case where culturally foreign material
needs to be rendered in a host (or target) language.
This is exactly the kind of situation where we ex-
pect deformation to occur ? the material is either
altered so that is feels more ?native? in the host
language (domestication) or its foreigness is pre-
served (foreignization) in that the material lacks
associative support in the host language (Venuti,
1995). In the first case, the translation might be
associatively adequate in the host language, but,
being altered, it would produce less culturally pre-
cise result when translated back into English. In
the second case, the result of translating out of En-
glish might already be associatively impoverished
by the standards of the host language.
The italicized phrases in the previous paragraph
underscore the theoretical and practical difficulty
in diagnozing domestication or foreignization in
translating out of English ? an associative model
for each of the host languages will be needed,
as well as some benchmark of the lexical tight-
ness of native texts written on the given topic
against which translations from English could be
judged. While the technique of back-translation
cannot identify the exact path of association loss
? through domestication or foreignization ? it can
help establish that association loss has occurred
in at least one or both of the translation processes
involved, since the original native English version
provides a natural benchmark against which the
resulting back-translations can be measured.
To make the phenomenon of association loss
more concrete, consider the following sentence:
Original Dave Magadan, the hard-hitting rookie
third baseman groomed to replace Knight,
has been hospitalized.
4LDC2008T19 in LDC catalogue
28
Arabic Dave Magadan, the stern rookie 3 base-
man groomed to replace Knight, is in the hos-
pital.5
Spanish Dave Magadan, the strong rookie third
baseman who managed to replace Knight,
has been hospitalized.
French Dave Magadan, the hitting third rookie
player prepared to replace Knight, was hos-
pitalized.
Swedish Dave Magadan, powerful rookie third
baseman groomed to replace Knight, has
been hospitalized.
Observe the translations of the phrase ?hard-
hitting rookie third baseman.? While substituting
strong and powerful for hard-hitting might seem
acceptable semantically, these terms are not asso-
ciated with the other baseball terms in the text,
whereas hitting is highly associated with them:6
Table 1 shows PMI scores for each of hitting,
stern, strong, powerful with the baseball terms
rookie and baseman. The French translation got
the hitting, but substituted the more generic term
player instead of the baseball-specific baseman.
As the bottom panel of Table 1 makes clear, while
player is associated with other baseball terms, the
associations are lower than those of baseman.
rookie baseman hitting
hitting 3.54 5.29
stern 0.35 -1.60
strong 0.54 -0.08
powerful -0.62 -0.63
player 3.95 2.73
baseman 5.11 5.29
Table 1: PMI associations of words introduced in
back-translations with baseball terms rookie, base-
man, and hitting.
Table 2 shows the average lexical tightness
values across 20 texts for the original version as
well as for the back translated versions. The origi-
nal version is statistically significantly tighter than
each of the back translated versions, using 4 ap-
plications of t-test for correlated samples, n=20,
p<0.05 in each case.
5We corrected the syntax of all back-translations while
preserving the content-word vocabulary choices.
6Our tokenizer splits words on hyphens, therefore exam-
ples are shown for hitting rather than for hard-hitting. The
point still holds, since hitting is a baseball term on its own.
Version Av. Std. Min. Max.
LT LT LT LT
Original .953 .092 .832 1.144
Via Arabic .875 .093 .747 1.104
Via Spanish .909 .081 .801 1.069
Via French .912 .087 .786 1.123
Via Swedish .931 .099 .796 1.131
Table 2: Average lexical tightness (Av. LT) for the
original vs back translated versions, on 20 base-
ball texts from the New York Times. Standard de-
viation, minimum, and maximum values are also
shown.
4 Experiment 2: Reference vs Machine
Translation
We use a part of the dataset used in the NIST Open
MT 2008 Evaluation.7 Our set contains transla-
tions of 120 news and web articles from Arabic to
English. For each document, there are 4 human
reference translations and 17 machine translations
by various systems that participated in the bench-
mark. Table 3 shows the average and standard de-
viation of lexical tightness values across the 120
texts for each of the four reference translations,
each of the 17 MT systems, as well as an average
across the four reference translations, and an aver-
age across the 17 MT systems. Each of the 17 MT
systems is statistically significantly less tight than
the average reference human translation (17 appli-
cations of the t-test for correlated samples, n=120,
p<0.05); 12 of the 17 MT systems are statistically
significantly less tight than the least tight human
reference (reference translation #3) at p<0.05; the
average system translation is statistically signifi-
cantly less tight that the average human translation
at p<0.05.
To exemplify a large gap in associative texture
between reference and machine translations, con-
sider the following extracts.8 As the raw MT ver-
sion (MT-raw) is barely readable, we provide a
version where words are re-arranged for readabil-
ity (MT-read), preserving most of the vocabulary.
Since lexical tightness operates on content word
types, adding or removing repetitions and function
words does not impact the calculation, so we re-
moved or inserted those for the sake of readability
7LDC2010T01
8The first paragraph of arb-WL-1-154489-
7725312#Arabic#system21#c.xml vs arb-WL-1-154489-
7725312#Arabic#reference 1#r.xml.
29
Translation Av. Std. Min. Max.
LT LT LT LT
Ref. 1 .873 .140 .590 1.447
Ref. 2 .851 .124 .636 1.256
Ref. 3 .838 .121 .657 1.177
Ref. 4 .865 .131 .639 1.429
Av. Ref. .857 .124 .641 1.317
MT 1 .814 .110 .670 1.113
MT 2 .824 .109 .565 1.089
MT 3 .818 .113 .607 1.137
MT 4 .836 .116 .615 1.144
MT 5 .803 .097 .590 1.067
MT 6 .824 .116 .574 1.173
MT 7 .819 .115 .576 1.162
MT 8 .810 .104 .606 1.157
MT 9 .827 .114 .546 1.181
MT 10 .827 .122 .569 1.169
MT 11 .814 .116 .606 1.131
MT 12 .826 .112 .607 1.119
MT 13 .823 .115 .619 1.116
MT 14 .826 .115 .630 1.147
MT 15 .820 .107 .655 1.124
MT 16 .827 .112 .593 1.147
MT 17 .835 .117 .642 1.169
Av. MT .822 .107 .623 1.106
Table 3: Average lexical tightness (Av. LT) for
the reference vs machine translations, on the NIST
Open MT 2008 Evaluation Arabic to English cor-
pus. Standard deviation, minimum, and maximum
values across the 120 texts are also shown.
in the MT-read version.
MT-raw vision came to me on dream in view of
her dream: Arab state to travel to and group
of friends on my mission and travel quickly
I was with one of the girls seem close to the
remaining more than I was happy and you?re
raised ended === known now
MT-read A vision came to me in a dream. I was
to travel quickly to an Arab state with a group
of friends on a mission. I was with one of
the girls who seemed close to the remaining
ones. I was happy and you are raised. It
ended. It is known now.
Ref A Dream. My sister came to tell me about a
dream she had while she slept. She was say-
ing: I saw you preparing to travel to an Arab
country, myself and a group of girlfriends.
You were sent on a scholarship abroad, and
you were preparing to travel quickly. You
were with one of the girls, who appeared to
be closer to you than the others, and I was
happy and excited because you were travel-
ing. The end. I now know !
The use of vision instead of dream, state in-
stead of country, friends instead of girlfriends,
mission instead of scholarship, raised instead of
excited, along with the complete disapperance
of slept, sister, preparing, abroad, all contribute
to a dramatic loss of associative texture in the
MT version. Highly associated pairs like dream-
slept, tell-saying, girlfriends-girls, travel-abroad,
sister-girls, happy-excited, travel-traveling are all
missed in the machine translation, while the newly
introduced word raised is quite unrelated to the
rest of the vocabulary in the extract.
5 Experiment 3: Quality of Machine
Translation
5.1 System-Level Comparison
In this experiment, we address the following ques-
tion: Is it the case that when a worse MT system A
and a better MT system B translate the same set of
materials, B tends to provide more lexically tight
translations?
To address this question, we use the Metrics-
MATR 2008 development set (Przybocki et al,
2009) from NIST Open MT 2006 evaluation.
Eight MT systems were used to translate 25 news
articles from Arabic to English, and humans pro-
vided scores for translation adequacy on a 1-7
scale. We calculated the average lexical tightness
over 25 texts for each of the eigth MT systems, as
well as the average translation score for each of the
systems. We note that human scores are available
per text segments (roughly equivalent to a sen-
tence, 249 segments in total for 25 texts), rather
than for whole texts. We first derive a human score
for the whole text for a given system by averaging
the scores of the system?s translations of the differ-
ent segments of the text. We then derive a human
score for an MT system by averaging the scores of
its translations of the 25 texts. We found that the
average adequacy score of a system is statistically
significantly positively correlated with the average
lexical tightness that the system?s translations ex-
hibit: r=0.630, n=8, df = 6, p<0.05.
30
5.2 Translation-Level Comparison
The same data could be used to answer the ques-
tion: Is it the case that better translations are
lexically tighter? Experiment 2 demonstrated that
human reference translations are tighter than ma-
chine translations; does the same relationship hold
for better vs worse machine translations? To ad-
dress this question, 25 x 8 = 200 instances of (sys-
tem, text) pairs can be used, where each has a
human score for translation adequacy and a lexi-
cal tightness value. Human scores and lexical
tightness of a translated text are significantly pos-
itively correlated, r=0.178, n=200, p<0.05. Note,
however, that this analysis is counfounded by the
variation in lexical tightness that exists between
texts: As standard deviations and ranges in Ta-
bles 2 and 3 make clear, original human texts, as
well as reference human translation for different
texts, vary in their lexical tightness. Therefore, a
lower lexical tightness value can be expected for
certain texts even for adequate translations, while
for other texts low values of lexical tightness sig-
nal a low quality translation. System-level anal-
ysis as presented in section 5.1 avoids this con-
founding, since all systems translated the same set
of texts, therefore average tightness values per sys-
tem are directly comparable.
6 Discussion and Conclusion
We presented a suggestive finding regarding the
loss of associative texture in the process of ma-
chine translation, using comparisons between (a)
original and back-translated texts, (b) reference
and system translations, (c) better and worse ma-
chine translations. We represented the amount of
association in a text using word association pro-
file ? a distribution of point wise mutual infor-
mation between all pairs of content word types
in a text. We used the average of the distribu-
tion, which we term lexical tightness ? as a sin-
gle measure of the amount of association in a text.
We showed that the lexical tightness of human-
composed texts is higher than that of the machine
translated materials. While the phenomenon of the
loss of associative texture has been theoretically
predicted by translation scholars, lexical tightness
is a computational measure capable of quantifying
the extent of this phenomenon.
Our work complements that of Wong and
Kit (2012) in demonstrating the potential utility
of discourse-level phenomena to assess machine
translations. First, we note that our findings are
orthogonal to the main finding in Wong and Kit
(2012) regarding loss of cohesion through insuffi-
cient word repetition, since our measure looks at
pairs of word types, hence disregards repetitions.
Second, the notion of pairwise word association
generalizes the notion of lexical cohesive devices
by looking not only at repeated reference with dif-
ferent lexical items or at words standing in cer-
tain semantic relations to each other, but at the
whole of the lexical network of the text. Third, dif-
ferently from the cohesion measure proposed by
Wong and Kit (2012), the lexical tightness mea-
sure does not depend on lexicographic resources
such as WordNet that do not exist in many lan-
guages.
References
Beata Beigman Klebanov and Michael Flor. 2013.
Word Association Profiles and their Use for Auto-
mated Scoring of Essays. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics, Sofia, Bulgaria, August.
Antoine Berman. 2000. Translation and the Trials of
the Foreign (translated from 1985 French original by
L. Venuti). In Lawrence Venuti, editor, The Trans-
lation Studies Reader, pages 276?289. New York:
Routledge.
Marine Carpuat and Michel Simard. 2012. The Trou-
ble with SMT Consistency. In Proceedings of the
7th Workshop on Statistical Machine Translation,
pages 442?449, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 909?919, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
David Graff and Christopher Cieri. 2003. English Gi-
gaword LDC2003T05. Linguistic Data Consortium,
Philadelphia.
Mark Przybocki, Kay Peterson, and Sebastien Bron-
sart. 2009. 2008 NIST metrics for machine transla-
tion (MetricsMATR08) development data.
Jo?rg Tiedemann. 2010. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In Proceedings of the 2010
Workshop on Domain Adaptation for Natural Lan-
guage Processing, pages 8?15, Uppsala, Sweden,
July. Association for Computational Linguistics.
31
Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 417?426, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
Lawrence Venuti. 1995. The Translator?s Invisibi-
ilty: A History of Translation. London & New York:
Routledge.
Billy Tak-Ming Wong and Chunyu Kit. 2012. Extend-
ing machine translation evaluation metrics with lexi-
cal cohesion to document level. In EMNLP-CoNLL,
pages 1060?1068.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in
machine translation. In Proceedings of the Machine
Translation Summit XIII.
32
Proceedings of the First Workshop on Argumentation Mining, pages 69?78,
Baltimore, Maryland USA, June 26, 2014. c?2014 Association for Computational Linguistics
Applying Argumentation Schemes for Essay Scoring    Yi Song   Michael Heilman   Beata Beigman Klebanov   Paul Deane Educational Testing Service Princeton, NJ, USA  {ysong, mheilman, bbeigmanklebanov, pdeane}@ets.org    Abstract 
Under the framework of the argumentation scheme theory (Walton, 1996), we developed annotation protocols for an argumentative writing task to support identification and classification of the arguments being made in essays. Each annotation protocol defined ar-gumentation schemes (i.e., reasoning pat-terns) in a given writing prompt and listed questions to help evaluate an argument based on these schemes, to make the argument structure in a text explicit and classifiable. We report findings based on an annotation of 600 essays. Most annotation categories were applied reliably by human annotators, and some categories significantly contributed to essay score. An NLP system to identify sen-tences containing scheme-relevant critical questions was developed based on the human annotations.   1. Introduction In this paper, we analyze the structure of argu-ments as a first step in analyzing their quality.  Argument structure plays a critical role in identi-fying relevant arguments based on their content, so it seems reasonable to focus first on identify-ing characteristic patterns of argumentation and the ways in which such arguments are typically developed when they are explicitly stated. It is worthwhile to classify the arguments in a text and to identify their structure when they are ex-tended to include whole text segments (Walton, 1996; Walton, Reed, and Macagno, 2008), but it is not clear how far human annotation can go in analyzing argument structure.  An analysis of the effectiveness and full com-plexity of argument structure is different than the identification of generic elements that might compose an argument, such as claims (e.g., a thesis sentence), main reasons (e.g., supporting topic sentences), evidence (e.g., elaborating 
segments), and other components, such as the introduction and conclusion (Burstein, Kukich, Wolff, Lu, Chodorow, Braden-Harder, & Harris, 1998; Burstein, Marcu, and Knight, 2003; Pendar & Cotos, 2008). In contrast, here we focus on analyzing specific types of arguments, what the literature terms argumentation schemes (Walton, 1996). Argumentation schemes include schemat-ic content and take into account a pattern of pos-sible argumentation moves in a larger persuasive dialog. Understanding these argumentation schemes is important for understanding the logic behind an argument. Critical questions associat-ed with a particular argumentation scheme pro-vide a normative standard that can be used to evaluate the relevance of an argument?s justifica-tory structure (van Eemeren and Grootendorst, 1992; Walton, 1996; Walton et al., 2008).   We aimed to lay foundations for the automat-ed analysis of argumentation schemes, such as the identification and classification of the argu-ments in an essay. Specifically, we developed annotation protocols for writing prompts in an argument analysis task from a graduate school admissions test. The task was designed to assess how well a student analyzes someone else?s ar-gument, which is provided by the prompt.  The student must critically evaluate the logical soundness of the given argument. The annotation categories were designed to map student re-sponses to the scheme-relevant critical questions. We examined whether this approach provides a useful framework for describing argumentation and whether human annotators can apply it relia-bly and consistently. Furthermore, we have be-gun work on automating the annotation process by developing a system to predict whether sen-tences contain scheme-relevant critical questions. 2. Theoretical Framework As Nussbaum (2011) notes, there have been crit-ical advances in the study of informal argument, 
69
  
which takes place within a social context involv-ing dialog among people with different beliefs, most notably the development of theories that provide relatively rich schemata for classifying informal arguments, such as Walton (1996).  An argumentation scheme is defined as ?a more or less conventionalized way of represent-ing the relation between what is stated in the ar-gument and what is stated in the standpoint? (van Eemeren and Grootendorst, 1992, p. 96). It is a strategic pattern of argumentation linking prem-ises to a conclusion and illustrating how the con-clusion is derived from the premises. This ?in-ternal structure? of argumentation reflects justifi-catory standards that can be used to help evaluate the reasonableness of an argument (van Eemeren and Grootendorst, 2004). Argumentation schemes should be distinguished from the kinds of structures postulated in Mann and Thompson?s (1988) Rhetorical Structure Theory (RST) be-cause they focus on relations inherent in the meaning of the argument, regardless of whether they are explicitly realized in the discourse. Consider, for instance, argument from conse-quences, which applies when the primary claim argues for or against a proposed policy (i.e., course of action) by citing positive or negative consequences that would follow if the policy were adopted (Walton, 1996). Elaborations of an argument from consequences are designed to defend against possible objections. For instance, an opponent could claim that the claimed conse-quences are not probable; or that they are not desirable; or that they are less important than other, undesirable consequences. Thus a sophis-ticated writer, in elaborating an argument from consequences, may provide information to rein-force the idea that the argued consequences are probable, desirable, and more important than any possible undesired effects. These moves corre-spond to what the literature calls critical ques-tions, which function as a standard for evaluating the reasonableness of an argument based on its argumentation schemes (Walton, 1996). Walton and his colleagues (2008) analyzed over 60 argumentation schemes, and identified critical questions associated with certain schemes as the logical moves in argumentative discourse. The range of possible moves is quite large, espe-cially when people use multiple schemes. There have been several efforts to annotate corpora with argumentation scheme information to sup-port future machine learning efforts (Mochales and Ieven, 2009; Palau and Moens, 2009; Rienks, Heylen, and Van der Weijden, 2005; 
Verbree, Rienks, and Heylen, 2006), to support argument representation (Atkinson, Bench-Capon, and McBurney, 2006; Rahwan, Banihashemi, Reed, Walton, and Abdallah, 2010), and to teach argumentative writing (Fer-retti, Lewis, and Andrews-Weckerly, 2009; Nussbaum and Schraw, 2007; Nussbaum and Edwards, 2011; Song and Ferretti, 2013). In ad-dition, Feng and Hirsh (2011) used the argumen-tation schemes to reconstruct the implicit parts (i.e., unstated assumptions) of the argument structure. In many previous studies, the data sets on argumentation schemes were relatively small and the inter-rater agreement was not measured.  We are particularly interested in exploring the relationship between the use of scheme-relevant critical questions and essay quality, as measured by holistic essay scores. The difference between an expert and a novice is that the expert knows which critical questions should be asked when the dynamic of the argument requires them, while the novice misses the essential moves to ask critical questions that help evaluate if the argument is valid or reasonable. Often, students presume information and fail to ask questions that would reveal potential fallacies. For exam-ple, they might use quotations from books, ar-guments from TV programs, or opinions posted online without evaluating whether the infor-mation is adequately supported by evidence. Critically evaluating arguments is considered an important skill in college and graduate school. For example, a widely accepted graduate admis-sions test has a task to assess students? critical thinking and analytical writing skills. In this ar-gument analysis task, students should demon-strate skills in critiquing other people?s argu-ments, such as identifying unwarranted assump-tions or discussing what specific evidence is need to support the argument. They must com-municate their evaluation of the arguments clear-ly to the audience. To accomplish this task suc-cessfully, students need to evaluate the argu-ments against appropriate criteria. Therefore, their essays could be analyzed using an annota-tion approach based on the theory of argumenta-tion schemes and critical questions.  Our research questions were as follows:   1. Can this scheme-based annotation approach be applied consistently by annotators to a corpus of argumentative essays? 2. Do annotation categories based on the theo-ry of argumentation schemes contribute 
70
  
significantly to the prediction of essay scores? 3. Can we use NLP techniques to train an au-tomated classifier for distinguishing sen-tences that raise critical questions from sen-tences that contain no critical questions? 3  Development of Annotation Protocols Although Walton?s argumentation schemes pro-vided a good framework for analyzing argu-ments, it was challenging to apply them in some cases of argument essays because various inter-pretations could be made on some argument structures. For instance, people were often con-fused with argument from consequences, argu-ment from correlation to cause, and argument from cause to effect because all these three types of arguments indicate a causal relationship. While it is good that Walton tried to identify var-iations of a causal relationship, a side effect is that some schemes are not so distinguishable from each other, especially for someone who is not an expert in logic. This ambiguity makes it difficult to apply his theory directly to annota-tion. Thus, we modified Walton?s schemes and created new schemes when necessary to achieve exclusive annotation categories and capture the features in the argument analysis task. In this paper, we illustrate our annotation pro-tocols on a policy argument because over half of the argument analysis prompts for the assess-ment we are working with deal with policy is-sues (i.e., issues involve the possibility of putting a practice into place). Here, we use the ?Patriot Car? prompt as an example.   The following appeared in a memo-randum from the new president of the Patriot car manufacturing company.   "In the past, the body styles of Patriot cars have been old-fashioned, and our cars have not sold as well as have our competitors' cars. But now, since many regions in this country report rapid in-creases in the numbers of newly licensed drivers, we should be able to increase our share of the market by selling cars to this growing population. Thus, we should discontinue our oldest models and con-centrate instead on manufacturing sporty cars. We can also improve the success of our marketing campaigns by switching our advertising to the Youth Advertising 
agency, which has successfully promoted the country's leading soft drink."  Test takers are asked to analyze the reasoning in the argument, consider any assumptions, and discuss how well any evidence that is mentioned supports the conclusion. The prompt states that the new president of the Patriot car manufacturing company pointed out a problem that the body styles of Patriot cars have been old-fashioned and their cars have not sold as well as their competitors? cars. The president proposed a plan to discontinue their oldest mod-els and to concentrate on manufacturing sporty cars. He believed that this plan will lead to an increase in their market share (i.e., the goal). This is a policy issue because it involves whether the plan of discontinuing oldest car models and manufacturing sporty cars should be put into place. This prompt shows a typical pattern of many argument analysis prompts about policy issues: (1) a problem is stated; (2) a plan is pro-posed; and (3) a desirable goal will be achieved if the plan is implemented. Thus, we created a policy scheme that includes these three major components (i.e., problem, plan, and goal), and a causal relationship that bridges the plan to the goal in the policy scheme. Therefore, a causal scheme appears in a policy argument to represent the causal relationship from the proposed plan to the goal. This part is different from Walton?s analysis. He uses the argument from conse-quences scheme for policy arguments, but it cre-ated confusions when applying it to annotation, especially when students unconsciously use the word ?cause? to introduce a potential conse-quence that follows a policy. In addition, our causal scheme combines the argument from cor-relation to cause scheme and the argument from cause to effect scheme specified by Walton.  Accordingly, we revised or re-arranged some of the critical questions in Walton?s theory. For example, challenges to arguments that use a poli-cy scheme fall into the following six categories: (a) problem; (b) goal; (c) plan implementation; (d) plan definition; (e) side effect; and (f) alterna-tive plan. When someone writes that the presi-dent should re-evaluate whether this is really a problem, it matches the question in the ?prob-lem? category; when someone questions if there  is an alternative plan that could also help achieve the goal and is better than the plan proposed by the president, it should be categorized as a chal-lenge in ?alternative plan.? We call these ?specif-ic questions? because they are attached to a par-
71
  
ticular prompt. In other words, specific questions are content dependent. Each category also in-cludes one or more ?general questions? that can be asked for any argument using the same argu-mentation scheme, and in this case, it is the poli-cy scheme.  We have developed annotation protocols for various argumentation schemes. Table 1 includes part of the annotation protocols (i.e., scheme, category, and general critical questions) for three argumentation schemes: the policy argument scheme, the causal argument scheme, and the argument from a sample scheme. This study fo-cuses on these three argumentation schemes and 16 associated categories.  4  Application of the Annotation Ap-proach This section focuses on applying the annotation approach and the following research question: Can this scheme-based annotation approach be applied consistently by raters to a corpus of ar-gumentative essays?  4.1  Annotation Rules 
The first step of the annotation is reading the en-tire essay. It is important to understand the writ-er?s major arguments and the organization of the essay. Next, the annotator will identify and high-light any text segment (e.g., paragraph, sentence, or clause) that addresses a critical question. Usu-ally, the minimal text segment is at the sentence-level, but it could be the case that the selection is at the phrase-level when a sentence includes multiple points that match more than one critical question. Thirdly, for a highlighted unit, the an-notator will choose a topic, a category, and a se-cond topic, if applicable. Only one category label can be assigned to each selected text unit. ?Generic? information will not be selected or assigned an annotation label. Generic infor-mation includes restatements of the text in the prompt, general statements that do not address any specific questions, rhetoric attacks, and irrel-evant information. Note that this notion of gener-ic information is related to ?shell language,? as described by Madnani et al (2012).  However, our definition here focuses more closely on sen-tences that do not raise critical questions.  Sur-face errors (e.g., grammar and spelling) can be 
Scheme Category Critical Question 
Policy 
Problem Is this really a problem? Is the problem well-defined? Goal How desirable is this goal? Are there specific conflicting goals we do not wish to sacrifice? Plan Implementation Is it practically possible to carry out this plan?  Plan Definition Is the plan well defined? Side Effects Are there negative side effects that should be taken into account if we carry out our plan? Alternative plan Are there better alternatives that could achieve the goal? 
Causal 
Causal Mechanism Is there really a correlation? Is the correlation merely a coincidence (invalid causal relationship)? Are there alternative causal factors? Causal Efficacy Is the causal mechanism strong enough to produce the desired effects? Applicability Does this causal mechanism apply? Intervening Factors Are there intervening factors that could undermine the causal mechanism? 
Sample 
Significance Are the patterns we see in the sample clear-cut enough (and in the right direction) to support the desired inference? Representativeness Is there any reason to think that this sample might not be representative of the group about which we wish to make an inference? Stability Is there any reason to think this pattern will be stable across all the circumstances about which we wish to make an inference?  Sample Size Is there any reason to think that the sample may not be large enough and reliable enough to support the inference we wish to draw? Validity Is the sample measured in a way that will give valid information on the population attributes about which we wish to make inferences?  Alternatives Are there external considerations that could invalidate the claims? Table 1: Annotation protocols for three types of argumentation schemes 
72
  
ignored if they do not prevent people from un-derstanding the meaning of the essay. Here is an example of annotated text.  As stated by the president, there is a rap-id increase in the number of newly li-censed drivers which would be a market-able target.  [However, there was no con-crete evidence that these newly licensed drivers favored sporty cars over other model types.]Causal Applicability [On a similar note, there was no anecdotal evidence demonstrating that lack of sales was con-tributed to the old-fashion body styles of the Patriot cars.]Causal Mechanism [There could be numerous other factors contrib-uting to their lack of sales:  prices are not competitive, safety ratings are not as high, features are not as appealing.  The best way to tackle this problem is to send out researches and surveys to get the opinions of consumers.]Causal Mechanism 4.2  Annotation Tool The annotation interface includes the following elements: 1. the original writing prompt; 2. topics that the prompt addresses; 3. categories associated with critical questions relevant to that type of argument; 4. general critical questions that can be used across prompts that possess the same argu-mentation scheme; and 5. specific critical questions for this particular prompt.   The annotators highlight text segments to be an-notated and then clicked a button to choose a topic (e.g., body style versus advertising agency in the Patriot Car prompt) and a category to iden-tify which critical questions were addressed.  4.3  Data and Annotation Procedures In this section, we report our annotation on two selected argument analysis prompts in an as-sessment for graduate school admissions. The actual prompts are not included here because they may be used in future tests. Both prompts deal with policy issues and are involved in causal reasoning, but the second prompt also has a sam-ple scheme (see Table 1). For each prompt, we randomly selected 300 essays to annotate. These essays were written between 2008 and 2010.  
Four annotators with linguistics backgrounds who were not co-authors of the paper received training on the annotation approach. Training focused on the application to specific prompts because each prompt had a specific annotation protocol that covers the argumentation schemes and how they relate to the prompt?s topics. The first author delivered the training sessions, and helped resolve differences of opinion during practice annotation rounds. After training and practice, the annotators annotated 20 pilot essays for a selected prompt to test their agreement. This pilot stage gave us another chance to find and clarify any confusion about the annotation categories. After that, the annotators worked on the sampled set of 300 essays, and these annota-tions were then used for analyses. For each prompt, 40 essays were randomly selected, and all 4 annotators annotated these 40 essays to check the inter-annotator agreement.  For the experiments described later that involve the mul-tiply-annotated set, we used the annotations from the annotator who seemed most consistent. 4.4  Inter-Annotator Agreement To compute human-human agreement, we auto-matically split the essays into sentences.  For each sentence, we computed the annotations that overlapped with at least part of the tence.  Then, for each category, we computed human-human agreement across all sentences about whether that category should be marked or not.  We also created a ?Generic? label, as dis-cussed in section 4.1, for sentences that were not marked by any of the other labels. We computed two inter-annotator agreement statistics. Our primary statistic is Cohen?s kappa between pairs of raters. Four annotators generat-ed 6 pairs of kappa values, and in this report we only report the average kappa value for each an-notation category. As an alternative statistic, we computed Krippendorff?s alpha, a chance-corrected statistic for calculating the inter-annotator agreement between multiple coders (four annotators in our case), which is similar to multi kappa (Krippendorff, 1980). Table 2 shows the kappa and alpha values for each annotation category, excluding those that were rare. To identify rare categories, we aver-aged the numbers of sentences annotated under a category among four annotators, which indicated how many sentences were annotated under this category in 40 essays.  If the number was lower than 10, which means that no more than one sen-tence was annotated in every four essays, then 
73
  
the category was considered rare. Most rare cate-gories had low inter-rater agreement, which is not surprising.  It is not realistic to require anno-tators to always agree about rare categories. From Table 2, we can see that the kappa value and the alpha value on the same category were close. The inter-annotator agreement on the ?ge-neric? category varied little across the two prompts (kappa: 0.572-0.604; alpha: 0.571-0.603), which indicates that the annotators had a fairly good agreement on this category. The an-notators had good agreements on most of the commonly used categories (kappa ranged from 0.549 to 0.848, and alpha ranged from 0.537 to 0.843) except the ?plan definition? under the pol-icy scheme in prompt B (both kappa and alpha values were below 0.400). The major reason for this disagreement is that one annotator marked a significantly higher number of sentences (more than double) for this category than others did.  
 Table 2: Inter-annotator agreement 5  Essay Score and Annotation Features This section explores the second research ques-tion: Do annotation categories based on the theo-ry of argumentation schemes contribute signifi-cantly to the prediction of essay scores?  An-swering this question would tell us whether we capture an important construct of the argument analysis task by recognizing these argumentation features. Specifically, we tested whether these features add predictive value to a model based 
the state-of-the-art e-rater essay scoring system (Burstein, Tetreault, and Madnani, 2013). To explore the relationship between annota-tion categories and essay quality, we ran a multi-ple regression analysis for each prompt. Essay quality was the dependent variable and was measured by a final human score, on a scale from 0 to 6. The independent variables were nine high-level e-rater features and the annotation categories relevant to a prompt (Prompt A: 10 categories; Prompt B 16 categories). The e-rater features were designed to measure different as-pects of writing (grammar, mechanics, style, us-age, word choice, word length, sentence variety, development, and organization). We computed the percentage of sentences that were marked as belonging to each category (i.e., the number of sentences in a category divided by the total num-ber of sentences) to factor out essay length. Note that the generic category was negatively correlated with the essay score in both prompts, since it included responses judged irrelevant to the scheme-relevant critical questions. In other words, the generic responses are the parts of the text that do not present specific critical evalua-tions of the arguments in a given prompt. For the purposes of our evaluation, we used the inverse feature labeled ?all critical questions?: the pro-portion of the text that actually raises some criti-cal question (i.e., is not generic), regardless of scheme. We believe this formulation more trans-parently expresses the underlying mechanism relating the feature to essay quality. For each prompt, we split the 300 essays into two data sets: the training set and the testing set. The testing set had the 40 essays that were anno-tated by all four annotators, and the training set had the remaining 260. We trained three models with stepwise regression on the training set and evaluated them on the testing set:  1. A model that included only the e-rater fea-tures to examine how well the e-rater mod-el works (?baseline?) 2. A model with the baseline features and all the annotation category percentage varia-bles except for the "generic" category vari-able (?baseline + categories?) 3. A model with the baseline features and a feature corresponding to the inverse of the "generic" category (?baseline + all critical questions?).  Table 3 presents the Pearson correlation coef-ficient r values for comparing model predictions 
Prompt Category Kappa Alpha 
Prompt A     Generic 0.572 0.571  Policy : Problem 0.644 0.640  Policy : Side Effects 0.612 0.609  Policy : Alternative Plan 0.665 0.666  Causal : Causal Mechanism 0.680 0.676  Causal : Applicability 0.557 0.555 Prompt B     Generic 0.604 0.603  Policy : Problem 0.848 0.843  Policy : Plan Definition 0.346 0.327  Causal : Causal Mechanism 0.620 0.622  Causal : Applicability 0.767 0.769  Sample : Validity 0.549 0.537 
74
  
to human scores for each of the models. In prompt A, three annotation categories (causal mechanism, applicability, and alternative plan) were selected by the stepwise regression because they significantly contributed to the essay score above the nine e-rater features. This model showed higher test set correlations than the base-line model (? r = .014). The model with the gen-eral argument feature (?all critical questions?) showed a similar increase (? r = .014).   Training Set r Testing Set r Testing Set ? r Prompt A    baseline	 ? .838 .852 --- baseline + specific categories	 ? .852 .866 .014 baseline +  all critical questions	 ? .858 .866 .014  Prompt B    baseline	 ? .818 .761 --- baseline + specific categories	 ? .835 .817 .056 baseline +  all critical questions	 ? .845 .821 .060  Table 3: Performance of essay scoring models with and without argumentation features  Similar observations apply to prompt B. The causal mechanism category added prediction significantly above e-rater with an increase (? r = .056). The model containing the general argu-ment feature (?all critical questions?) performed slightly better (? r = .060). These results suggest that annotation catego-ries based on argumentation schemes contribute additional useful information about essay quality to a strong baseline essay scoring model.  In the next section, we report on preliminary experi-ments testing whether these annotations can be automated, which would almost certainly be nec-essary for practical applications. 6  Argumentation Schemes NLP System We developed an NLP system for automatically identifying the presence of scheme-relevant criti-cal questions in essays, and we evaluated this system with annotated data from the two selected argument prompts. This addresses the third re-search question: Can we use NLP techniques to train an automated classifier for distinguishing 
sentences that raise critical questions from sen-tences that contain no critical questions? 6.1  Modeling In this initial development of the NLP system, we focused on the task of predicting whether a sentence raises any critical questions or none (i.e., generic vs. nongeneric). As such, the task was binary classification at the level of the sen-tence. The system we developed uses the SKLL tool1 to fit L2-penalized logistic regression mod-els with the following features:  ? Word n-grams: Binary indicators for the presence of contiguous subsequences of n words in the sentence. The value of n ranged from 1 to 3. These features had value 1 if a particular n-gram was present in a sentence and 0 otherwise. ? word n-grams of the previous and next sen-tences: These are analogous to the word n-gram features for the current sentence. ? sentence length bins: Binary indicators for whether the sentence is longer than 2t word tokens, where t  ranges from 1 to 10. ? sentence position: The sentence number di-vided by the number of sentences in text. ? part of speech tags: Binary indicators for the presence of words with various parts of speech, as predicted by NLTK 2.0.4. ? prompt overlap: Three features based on lex-ical overlap between the sentence and the prompt for the essay: a) the Jaccard similari-ty between the sets of word n-grams in the sentence and prompt (n = 1, 2, 3), b) the Jac-card similarity between the sets of word uni-grams (i.e., just n = 1) in the sentence and prompt, and c) the Jaccard similarity be-tween the sets of ?content? word unigrams in the sentence and prompt (for this, content words were defined as word tokens that con-tained only numbers and letters and did not appear in NLTK?s English stopword list). 6.2  Experiments For these experiments, we used the training and testing sets described in Section 5. We trained models on the training data for each prompt in-dividually and on the combination of the training data for both prompts. To measure generalization across prompts, we tested these models on the testing data for each prompt and on the combina-                                                1 https://github.com/EducationalTestingService/skll 
75
  
tion of the testing data for the two prompts. We evaluated performance in terms of unweighted Cohen?s kappa. The results are in Table 4.  Training Testing Kappa combined combined .438 Prompt A  .350 Prompt B  .346 combined Prompt A .379 Prompt A  .410 Prompt B  .217 combined Prompt B .498 Prompt A  .285 Prompt B  .478  Table 4: Performance of the NLP Model  The model trained on data from both prompts performed relatively well compared to the other models.  For the testing data for prompt B, the combined model outperformed the model trained on just data from prompt B.  However, the prompt-specific model for prompt A slightly outperformed the combined model on the testing data for prompt A. Although the performance of models trained with data from one prompt and tested with data from another prompt did not perform as well, there is evidence of some generalization across prompts. The model trained on data from prompt B and tested on data from prompt A had kappa = 0.217; the model trained on data from prompt A and tested on data from prompt B had kappa = 0.285. Of course, these human-machine agree-ment values were somewhat lower than human-human agreement values (0.572 and 0.604, re-spectively), leaving substantial room for im-provement in future work. We also examined the most strongly weighted features in the combined model.  We observed that multiple hedge words (e.g., ?perhaps?, ?may?) had positive weights, which associated with the ?generic? class.  We also observed that words related to argumentation (e.g., ?conclu-sions?, ?questions?) had negative weights, which associated them with the nongeneric class, as one would expect.  One issue of concern is that some words related to the specific topics discussed in the prompts received high weights as well, which may limit generalizability.  
7  Conclusion Our research focused on identification and classi-fication of argumentation schemes in argumenta-tive text. We developed annotation protocols that capture various argumentation schemes. The an-notation categories corresponded to scheme-relevant critical questions, and for text segments that do not contain any critical questions, we as-signed a ?generic? category. In this paper, we reported the results based on an annotation of a large pool of student essays (both high-quality and low-quality essays). Results showed that most of the common annotation categories (e.g. causal mechanism, alternative plan) can be ap-plied reliably by the four annotators. However, the annotation work is labor-intensive. People need to receive sufficient train-ing to apply the approach consistently. They must not only identify meaningful chunks of tex-tual information but also assign the right annota-tion category label for the selected text. Despite these complexities, it is a worthwhile investiga-tion. Developing a systematic classification of argument structures not only plays a critical role in this project, but also has a potential contribu-tion to other assessments on argumentation skills aligned with the Common Core State Standards. This work would help improve the current auto-mated scoring techniques for argumentative es-says because this annotation approach takes into account the argument structure and its content.  We ran regression analyses and found that manual annotations grounded in the argumenta-tion schemes theory predict essay quality. Our data showed that features based on manual ar-gument scheme annotations significantly con-tributed to models of essay scores for both prompts. This is probably because our approach focused on the core of argumentation, rather than surface or word-level features (e.g., mechanics, grammar, usage, style, essay organization, and vocabulary) examined by the baseline model. Furthermore, we have implemented an auto-mated system for predicting the human annota-tions. This system focused only on predicting whether or not a sentence raises any critical questions (i.e., generic vs. nongeneric). In the future, we plan to test whether features based on automated annotations make contributions to essay scoring models that are similar to the con-tributions of manual annotations.  We also plan to work on detecting specific critical questions and adding additional features, such as features from Feng and Hirst (2011). 
76
  
Acknowledgements  We would like to thank Keelan Evanini, Jill Burstein, Aoife Cahill, and the anonymous re-viewers of this paper for their helpful comments.  We would also like to thank Michael Flor for helping set up the annotation interface, and Melissa Lopez, Matthew Mulholland, Patrick Houghton, and Laura Ridolfi for annotating the data. References Katie Atkinson, Trevor Bench-Capon, and Peter McBurney. 2006. Computational representation of practical argument. Synthese, 152: 157-206. Burstein, Jill, Karen Kukich, Susanne Wolff, Chi Lu, Martin Chodorow, Lisa Braden-Harder, and Mary Dee Harris. 1998. "Automated scoring using a hybrid feature identification technique." In Pro-ceedings of the 17th international conference on Computational linguistics-Volume 1, pp. 206-210. Association for Computational Linguistics. Jill Burstein, Daniel Marcu, and Kevin Knight. 2003. Finding the WRITE stuff: Automatic identification of discourse structure in student essays. IEEE Transactions on Intelligent Systems, 18(1): 32-39.  Jill Burstein, Joel Tetreault, and Nitin Madnani. 2013. The e-rater automated essay scoring system. In Sermis, M. D. and Burstein, J. (eds.), Handbook of Automated Essay Evaluation: Current Applications and New Directions (pp. 55-67). New York: Routledge.  Vanessa W. Feng and Graeme Hirst. 2011.  Classify-ing arguments by scheme.  Proceedings of the 49th Annual Meeting of the Association for Computa-tional Linguistics, Portland, OR.   Ralph P. Ferretti, William E. Lewis, and Scott An-drews-Weckerly. 2009. Do goals affect the struc-ture of students? argumentative writing strategies? Journal of Educational Psychology, 101: 577-589. Klaus Krippendorff. 1980. Content Analysis: An In-troduction to its Methodology. Beverly Hills, CA : Sage Publications.Mann, William C., and Sandra A. Thompson. 1988. "Rhetorical structure theory: Toward a functional theory of text organization." Text 8(3): 243-281. Nitin Madnani, Michael Heilman, Joel Tetreault, and Martin Chodorow.  2012.  Identifying High Level Organizational Elements in Argumentative Dis-course. Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.  (pp. 20-28).  Association for Com-putational Linguistics. 
Raquel Mochales and Asgje Ieven. 2009. Creating an argumentation corpus: do theories apply to real ar-guments?: a case study on the legal argumentation of the ECHR. In ICAIL ?09: Proceedings of the 12th International Conference on Artificial Intelli-gence and Law.  Michael Nussbaum. 2011. Argumentation, dialogue theory, and probability modeling: alternative frameworks for argumentation research in educa-tion. Educational Psychologist, 46: 84-106.  Nussbaum, E. M. and Edwards, O.V. (2011). Critical questions and argument stratagems: A framework for enhancing and analyzing students? reasoning practices. Journal of the Learning Sciences, 20, 443-488. Palau, R.M. and Moens, M. F. 2009. Automatic ar-gument detection and its role in law and the seman-tic web. In Proceedings of the 2009 conference on law, ontologies and the semantic web. IOS Press, Amsterdam, The Netherlands.Pendar, Nick, and Elena Cotos. 2008. "Automatic identification of discourse moves in scientific article introductions." In Proceedings of the Third Workshop on Innova-tive Use of NLP for Building Educational Applica-tions, pp. 62-70. Association for Computational Linguistics. Rahwan, I., Banihashemi, B., Reed, C. Walton, D., and Abdallah, S. (2010). Representing and classi-fying arguments on the semantic web. The Knowledge Engineering Review. Rienks, R., Heylen, D., and Van der Weijden, E. 2005. Argument diagramming of meeting conver-sations. In A. Vinciarelli, J. Odobez (Ed.), Pro-ceedings of Multimodal Multiparty Meeting Pro-cessing, Workshop at the 7th International Confer-ence on Multimodal Interfaces (pp. 85?92). Trento, Italy.  Yi Song and Ralph P. Ferretti. 2013. Teaching critical questions about argumentation through the revising process: Effects of strategy instruction on college students? argumentative essays. Reading and Writ-ing: An Interdisciplinary Journal, 26(1): 67-90. Stephen E. Toulmin. 1958. The uses of argument. Cambridge University Press, Cambridge, UK. Frans H. van Eemeren and Rob Grootendorst. 1992. Argumentation, communication, and fallacies: A pragma-dialectical perspective. Mahwah, NJ: Erl-baum. Frans H. van Eemeren and Rob Grootendorst. 2004. A systematic theory of argumentation: A pragma-dialectical approach. Cambridge, UK: Cambridge University Press. Verbree, D., Rienks, H., and Heylen, D. (2006). First Steps Towards the Automatic Construction of Ar-gument-Diagrams from Real Discussions. In Pro-
77
  
ceedings of the 2006 conference on Computational Models of Argument: Proceedings of COMMA 2006. IOS Press, Amsterdam, The Netherlands. Douglas N. Walton. 1996. Argumentation schemes for presumptive reasoning. Mahwah, NJ: Lawrence Erlbaum. Douglas N. Walton, Chris Reed, and Fabrizio Macagno. 2008. Argumentation schemes. New York, NY: Cambridge University Press. 
78
Proceedings of the Second Workshop on Metaphor in NLP, pages 11?17,
Baltimore, MD, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Different Texts, Same Metaphors: Unigrams and Beyond
Beata Beigman Klebanov, Chee Wee Leong, Michael Heilman, Michael Flor
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541
{bbeigmanklebanov,cleong,mheilman,mflor}@ets.org
Abstract
Current approaches to supervised learning
of metaphor tend to use sophisticated fea-
tures and restrict their attention to con-
structions and contexts where these fea-
tures apply. In this paper, we describe the
development of a supervised learning sys-
tem to classify all content words in a run-
ning text as either being used metaphori-
cally or not. We start by examining the
performance of a simple unigram baseline
that achieves surprisingly good results for
some of the datasets. We then show how
the recall of the system can be improved
over this strong baseline.
1 Introduction
Current approaches to supervised learning of
metaphor tend to (a) use sophisticated features
based on theories of metaphor, (b) apply to cer-
tain selected constructions, like adj-noun or verb-
object pairs, and (c) concentrate on metaphors
of certain kind, such as metaphors about gover-
nance or about the mind. In this paper, we de-
scribe the development of a supervised machine
learning system to classify all content words in a
running text as either being used metaphorically
or not ? a task not yet addressed in the literature,
to our knowledge. This approach would enable,
for example, quantification of the extent to which
a given text uses metaphor, or the extent to which
two different texts use similar metaphors. Both of
these questions are important in our target appli-
cation ? scoring texts (in our case, essays written
for a test) for various aspects of effective use of
language, one of them being the use of metaphor.
We start by examining the performance of a
simple unigram baseline that achieves surprisingly
good results for some of the datasets. We then
show how the recall of the system can be improved
over this strong baseline.
2 Data
We use two datasets that feature full text anno-
tations of metaphors: A set of essays written for
a large-scale assessment of college graduates and
the VUAmsterdam corpus (Steen et al., 2010),
1
containing articles from four genres sampled from
the BNC. Table 1 shows the sizes of the six sets,
as well as the proportion of metaphors in them; the
following sections explain their composition.
Data #Texts #NVAR #metaphors
tokens (%)
News 49 18,519 3,405 (18%)
Fiction 11 17,836 2,497 (14%)
Academic 12 29,469 3,689 (13%)
Conversation 18 15,667 1,149 ( 7%)
Essay Set A 85 21,838 2,368 (11%)
Essay Set B 79 22,662 2,745 (12%)
Table 1: Datasets used in this study. NVAR =
Nouns, Verbs, Adjectives, Adverbs, as tagged by
the Stanford POS tagger (Toutanova et al., 2003).
2.1 VUAmsterdam Data
The dataset consists of 117 fragments sampled
across four genres: Academic, News, Conversa-
tion, and Fiction. Each genre is represented by ap-
proximately the same number of tokens, although
the number of texts differs greatly, where the news
archive has the largest number of texts.
We randomly sampled 23% of the texts from
each genre to set aside for a blind test to be carried
out at a later date with a more advanced system;
the current experiments are performed using cross-
validation on the remaining 90 fragments: 10-fold
on News, 9-fold on Conversation, 11 on Fiction,
and 12 on Academic. All instances from the same
text were always placed in the same fold.
1
http://www2.let.vu.nl/oz/metaphorlab/metcor/search/index.html
11
The data is annotated using MIP-VU proce-
dure. It is based on the MIP procedure (Prag-
glejaz, 2007), extending it to handle metaphori-
city through reference (such as marking did as a
metaphor in As the weather broke up, so did their
friendship) and allow for explicit coding of diffi-
cult cases where a group of annotators could not
arrive at a consensus. The tagset is rich and is
organized hierarchically, detecting various types
of metaphors, words that flag the presense of
metaphors, etc. In this paper, we consider only the
top-level partition, labeling all content words with
the tag ?function=mrw? (metaphor-related word)
as metaphors, while all other content words are la-
beled as non-metaphors.
2
2.2 Essay Data
The dataset consists of 224 essays written for a
high-stakes large-scale assessment of analytical
writing taken by college graduates aspiring to en-
ter a graduate school in the United States. Out of
these, 80 were set aside for future experiments and
not used for this paper. Of the remaining essays,
85 essays discuss the statement ?High-speed elec-
tronic communications media, such as electronic
mail and television, tend to prevent meaningful
and thoughtful communication? (Set A), and 79
discuss the statement ?In the age of television,
reading books is not as important as it once was.
People can learn as much by watching television
as they can by reading books.? (Set B). Multiple
essays on the same topic is a unique feature of this
dataset, allowing the examination of the effect of
topic on performance, by comparing performance
in within-topic and across-topic settings.
The essays were annotated using a protocol
that prefers a reader?s intuition over a formal de-
finition, and emphasizes the connection between
metaphor and the arguments that are put forward
by the writer. The protocol is presented in detail
in Beigman Klebanov and Flor (2013). All essays
were doubly annotated. The reliability is ? = 0.58
for Set A and ? = 0.56 for Set B. We merge the two
annotations (union), following the observation in
a previous study Beigman Klebanov et al. (2008)
that attention slips play a large role in accounting
for observed disagreements.
We will report results for 10-fold cross-
validation on each of sets A and B, as well as
2
We note that this top-level partition was used for many
of the analyses discussed in (Steen et al., 2010).
across prompts, where the machine learner would
be trained on Set A and tested on Set B and vice
versa.
3 Supervised Learning of Metaphor
For this study, we consider each content-word to-
ken in a text as an instance to be classified as a
metaphor or non-metaphor. We use the logistic
regression classifier in the SKLL package (Blan-
chard et al., 2013), which is based on scikit-learn
(Pedregosa et al., 2011), optimizing for F
1
score
(class ?metaphor?). We consider the following
features for metaphor detection.
? Unigrams (U): All content words from the
relevant training data are used as features,
without stemming or lemmatization.
? Part-of-Speech (P): We use Stanford POS
tagger 3.3.0 and the full Penn Treebank tagset
for content words (tags starting with A, N, V,
and J), removing the auxiliaries have, be, do.
? Concreteness (C): We use Brysbaert et al.
(2013) database of concreteness ratings for
about 40,000 English words. The mean ra-
tings, ranging 1-5, are binned in 0.25 incre-
ments; each bin is used as a binary feature.
? Topic models (T): We use Latent Dirich-
let Allocation (Blei et al., 2003) to derive
a 100-topic model from the NYT corpus
years 2003?2007 (Sandhaus, 2008) to rep-
resent common topics of public discussion.
The NYT data was lemmatized using NLTK
(Bird, 2006). We used the gensim toolkit
(
?
Reh?u?rek and Sojka, 2010) for building the
models, with default parameters. The score
assigned to an instance w on a topic t is
log
P (w|t)
P (w)
where P (w) were estimated from
the Gigaword corpus (Parker et al., 2009).
These features are based on the hypothesis
that certain topics are likelier to be used as
source domains for metaphors than others.
4 Results
For each dataset, we present the results for the
unigram model (baseline) and the results for the
full model containing all the features. For cross-
validation results, all words from the same text
were always placed in the same fold, to ensure that
we are evaluating generalization across texts.
12
M Unigram UPCT
Data F P R F P R F
Set A .20 .72 .43 .53 .70 .47 .56
Set B .22 .79 .54 .64 .76 .60 .67
B-A .20 .58 .45 .50 .56 .50 .53
A-B .22 .71 .28 .40 .72 .35 .47
News .31 .62 .38 .47 .61 .43 .51
Fiction .25 .54 .23 .32 .54 .24 .33
Acad. .23 .51 .20 .27 .50 .22 .28
Conv. .14 .39 .14 .21 .36 .15 .21
Table 2: Summary of performance, in terms of
precision, recall, and F
1
. Set A, B, and VUAm-
sterdam: cross-validation. B-A and A-B: Training
on B and testing on A, and vice versa, respectively.
Column M: F
1
of a pseudo-system that classifies
all words as metaphors.
4.1 Performance of the Baseline Model
First, we observe the strong performance of the
unigram baseline for the cross-validation within
sets A and B (rows 1 and 2 in Table 2). For a
new essay, about half its metaphors will have been
observed in a sample of a few dozen essays on the
same topic; these words are also consistently used
as metaphors, as precision is above 70%. Once the
same-topic assumption is relaxed down to related
topics, the sharing of metaphor is reduced (com-
pare rows 1 vs 3 and 2 vs 4), but still substantial.
Moving to VUAmsterdam data, we observe that
the performance of the unigram model on the
News partition is comparable to its performance in
the cross-prompt scenario in the essay data (com-
pare row 5 to rows 3-4 in Table 2), suggesting that
the News fragments tend to discuss a set of related
topics and exhibit substantial sharing of metaphors
across texts.
The performance of the unigram model is much
lower for the other VUAmsterdam partitions, al-
though it is still non-trivial, as evidenced by its
consistent improvement over a pseudo-baseline
that classifies all words as metaphor, attaining
100% recall (shown in column M in Table 2). The
weaker performance could be due to highly diver-
gent topics between texts in each of the partitions.
It is also possible that the number of different
texts in these partitions is insufficient for covering
the metaphors that are common in these kinds of
texts ? recall that these partitions have small num-
bers of long texts, whereas the News partition has
a larger number of short texts (see Table 1).
4.2 Beyond Baseline
The addition of topic model, POS, and concrete-
ness features produces a significant increase in
recall across all evaluations (p < 0.01), using
McNemar?s test of the significance of differ-
ences between correlated proportions (McNemar,
1947). Even for Conversations, where recall
improvement is the smallest and F
1
score does
not improve, the UPCT model recovers all 161
metaphors found by the unigrams plus 14 addi-
tional metaphors, yielding a significant result on
the correlated test.
We next investigate the relative contribution of
the different types of features in the UPCT model
by ablating each type and observing the effect on
performance. Table 3 shows ablation results for
essay and News data, where substantial improve-
ments over the unigram baseline were produced.
We observe, as expected, that the unigram fea-
tures contributed the most, as removing them re-
sults in the most dramatic drop in performance,
although the combination of concreteness, POS,
and topic models recovers about one-fourth of
metaphors with over 50% precision, showing non-
trivial performance on essay data.
The second most effective feature set for essay
data are the topic models ? they are responsible for
most of the recall gain obtained by the UPCT mo-
del. For example, one of the topics with a positive
weight in essays in set B deals with visual ima-
gery, its top 5 most likely words in the NYT being
picture, image, photograph, camera, photo. This
topic is often used metaphorically, with words
like superficial, picture, framed, reflective, mirror,
capture, vivid, distorted, exposure, scenes, face,
background that were all observed as metaphors in
Set B. In the News data, a topic that deals with hur-
ricane Katrina received a positive weight, as words
of suffering and recovery from distaster are often
used metaphorically when discussing other things:
starved, severed, awash, damaged, relief, victim,
distress, hits, swept, bounce, response, recovering,
suffering.
The part-of-speech features help improve recall
across all datasets in Table 3, while concreteness
features are effective only for some of the sets.
5 Discussion: Metaphor & Word Sense
The classical ?one sense per discourse? finding of
Gale et al. (1992) that words keep their senses
within the same text 98% of the time suggests that
13
Set A cross-val. Set B cross-val. Train B : Test A Train A : Test B News
P R F P R F P R F P R F P R F
M .11 1.0 .20 .12 1.0 .22 .11 1.0 .20 .12 1.0 .22 .18 1.0 .31
U .72 .43 .53 .79 .54 .64 .58 .45 .50 .71 .28 .40 .62 .38 .47
UPCT .70 .47 .56 .76 .60 .67 .56 .50 .53 .72 .35 .47 .61 .43 .51
? U .58 .21 .31 .63 .28 .38 .44 .21 .29 .59 .18 .27 .55 .23 .32
? P .71 .46 .56 .76 .58 .66 .57 .48 .52 .70 .33 .45 .61 .41 .49
? C .70 .46 .55 .77 .58 .66 .56 .50 .53 .71 .34 .46 .61 .43 .50
? T .71 .43 .53 .78 .55 .65 .57 .45 .51 .71 .29 .41 .62 .41 .49
Table 3: Ablation evaluations. Model M is a pseudo-system that classifies all instances as metaphors.
if a word is used as a metaphor once in a text, it is
very likely to be a metaphor if it is used again in
the same text. Indeed, this is the reason for putting
all words from the same text in the same fold in
cross-validations, as training and testing on diffe-
rent parts of the same text would produce inflated
estimates of metaphor classification performance.
Koeling et al. (2005) extend the notion of dis-
course beyond a single text to a domain, such as
articles on Finance, Sports, and a general BNC
domain. For a set of words that each have at
least one Finance and one Sports sense and not
more than 12 senses in total, guessing the pre-
dominant sense in Finance and Sports yielded 77%
and 76% precision, respectively. Our results with
the unigram model show that guessing ?metaphor?
based on a sufficient proportion of previously ob-
served metaphorical uses in the given domain
yields about 76% precision for essays on the same
topic. Thus, metaphoricity distinctions in same-
topic essays behave similarly to sense distinctions
for polysemous words with a predominant sense
in the Finance and Sports articles, keeping to their
domain-specific predominant sense
3
4
of the time.
Note that a domain-specific predominant sense
may or may not be the same as the most frequent
sense overall; similarly, a word?s tendency to be
used metaphorically might be domain specific or
general. The results for the BNC at large are likely
to reflect general rather than domain-specific sense
distributions. According to Koeling et al. (2005),
guessing the predominant sense in the BNC yields
51% precision; our finding for BNC News is 62%
precision for the unigram model. The difference
could be due to the mixing of the BNC genres in
Koeling et al. (2005), given the lower precision of
metaphoricity prediction in non-news (Table 2).
In all, our results suggest that the pattern of
metaphorical and non-metaphorical use is in line
with that of dominant word-sense for more and
less topically restricted domains.
6 Related Work
The extent to which different texts use similar
metaphors was addressed by Pasanek and Scul-
ley (2008) for corpora written by the same author.
They studied metaphors of mind in the oeuvre
of 7 authors, including John Milton and William
Shakespeare. They created a set of metaphori-
cal and non-metaphorical references to the mind
using excerpts from various texts written by these
authors. Using cross-validation with unigram
features for each of the authors separately, they
present very high accuracies (85%-94%), suggest-
ing that authors are highly self-consistent in the
metaphors of mind they select. They also find
good generalizations between some pairs of au-
thors, due to borrowing or literary allusion.
Studies using political texts, such as speeches
by politicians or news articles discussing politi-
cally important events, documented repeated use
of words from certain source domains, such as
rejuvenation in Tony Blair?s speeches (Charteris-
Black, 2005) or railroad metaphors in articles dis-
cussing political integration of Europe (Musolff,
2000). Our results regarding settings with substan-
tial topical consistency second these observations.
According to the Conceptual Metaphor theory
(Lakoff and Johnson, 1980), we expect certain ba-
sic metaphors to be highly ubiquitous in any cor-
pus of texts, such as TIME IS SPACE or UP IS
GOOD. To the extent that these metaphors are
realized through frequent content words, we ex-
pect some cross-text generalization power for a
unigram model. Perhaps the share of these basic
metaphors in all metaphors in a text is reflected
most faithfully in the peformance of the unigram
model on the non-News partitions of the VUAms-
14
terdam data, where topical sharing is minimal.
Approaches to metaphor detection are often ei-
ther rule-based or unsupervised (Martin, 1990;
Fass, 1991; Shutova et al., 2010; Shutova and
Sun, 2013; Li et al., 2013), although supervised
approaches have recently been attempted with the
advent of relatively large collections of metaphor-
annotated materials (Mohler et al., 2013; Hovy et
al., 2013; Pasanek and Sculley, 2008; Gedigan
et al., 2006). These approaches are difficult to
compare to our results, as these typically are not
whole texts but excerpts, and only certain kinds of
metaphors are annotated, such as metaphors about
governance or about the mind, or only words be-
longing to certain syntactic or semantic class are
annotated, such as verbs
3
or motion words only.
Concreteness as a predictor of metaphoricity
was discussed in Turney et al. (2011) in the context
of concrete adjectives modifying abstract nouns.
The POS features are inspired by the discussion
of the preference and aversion of various POS
towards metaphoricity in Goatly (1997). Heintz
et al. (2013) use LDA topics built on Wikipedia
along with manually constructed seed lists for po-
tential source and target topics in the broad tar-
get domain of governance, in order to identify
sentences using lexica from both source and tar-
get domains as potentially containing metaphors.
Bethard et al. (2009) use LDA topics built on BNC
as features for classifying metaphorical and non-
metaphorical uses of 9 words in 450 sentences that
use these words, modeling metaphorical vs non-
metaphorical contexts for these words. In both
cases, LDA is used to capture the topical compo-
sition of a sentence; in contrast, we use LDA to
capture the tendency of words belonging to a topic
to be used metaphorically in a given discourse.
Dunn (2013) compared algorithms based on
various theories of metaphor on VUAmsterdam
data. The evaluations were done at sentence level,
where a sentence is metaphorical if it contains at
least one metaphorically used word. In this ac-
counting, the distribution is almost a mirror-image
of our setting, as 84% of sentences in News were
labeled as metaphorical, whereas 18% of content
words are tagged as such. The News partition was
very difficult for the systems examined in Dunn
(2013) ? three of the four systems failed to pre-
dict any non-metaphorical sentences, and the one
system that did so suffered from a low recall of
3
as in Shutova and Teufel (2010)
metaphors, 20%. Dunn (2013) shows that the
different systems he compared had relatively low
agreement (? < 0.3); he interprets this finding as
suggesting that the different theories underlying
the models capture different aspects of metapho-
ricity and therefore detect different metaphors. It
is therefore likely that features derived from the
various models would fruitfully complement each
other in a supervised learning setting; our findings
suggest that the simplest building block ? that of
a unigram model ? should not be ignored in such
experiments.
7 Conclusions
We address supervised learning of metaphoricity
of words of any content part of speech in a running
text. To our knowledge, this task has not yet been
studied in the literature. We experimented with a
simple unigram model that was surprisingly suc-
cessful for some of the datasets, and showed how
its recall can be further improved using topic mo-
dels, POS, and concreteness features.
The generally solid performance of the unigram
features suggests that these features should not be
neglected when trying to predict metaphors in a
supervised learning paradigm. Inasmuch as me-
taphoricity classification is similar to a coarse-
grained word sense disambiguation, a unigram
model can be thought of as a crude predominant
sense model for WSD, and is the more effective
the more topically homogeneous the data.
By evaluating models with LDA-based topic
features in addition to unigrams, we showed that
topical homogeneity can be exploited beyond uni-
grams. In topically homogeneous data, certain
topics commonly discussed in the public sphere
might not be addressed, yet their general fa-
miliarity avails them as sources for metaphors.
For essays on communication, topics like sports
and architecture are unlikely to be discussed; yet
metaphors from these domains can be used, such
as leveling of the playing field through cheap and
fast communications or buildling bridges across
cultures through the internet.
In future work, we intend to add features that
capture the relationship between the current word
and its immediate context, as well as add essays
from additional prompts to build a more topically
diverse set for exploration of cross-topic generali-
zation of our models for essay data.
15
References
Beata Beigman Klebanov and Michael Flor. 2013.
Argumentation-relevant metaphors in test-taker es-
says. In Proceedings of the First Workshop on
Metaphor in NLP, pages 11?20, Atlanta, Georgia,
June. Association for Computational Linguistics.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing disagreements. In
COLING 2008 workshop on Human Judgments in
Computational Linguistics, pages 2?7, Manchester,
UK.
Steven Bethard, Vicky Tzuyin Lai, and James Martin.
2009. Topic model analysis of metaphor frequency
for psycholinguistic stimuli. In Proceedings of the
Workshop on Computational Approaches to Linguis-
tic Creativity, CALC ?09, pages 9?16, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Steven Bird. 2006. NLTK: The natural language
toolkit. In Proceedings of the ACL, Interactive Pre-
sentations, pages 69?72.
Daniel Blanchard, Michael Heilman,
and Nitin Madnani. 2013. SciKit-
Learn Laboratory. GitHub repository,
https://github.com/EducationalTestingService/skll.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2013. Concreteness ratings for 40 thou-
sand generally known english word lemmas. Behav-
ior Research Methods, pages 1?8.
Jonathan Charteris-Black. 2005. Politicians and
rhetoric: The persuasive power of metaphors. Pal-
grave MacMillan, Houndmills, UK and New York.
Jonathan Dunn. 2013. What metaphor identification
systems can tell us about metaphor-in-language. In
Proceedings of the First Workshop on Metaphor in
NLP, pages 1?10, Atlanta, Georgia, June. Associa-
tion for Computational Linguistics.
Dan Fass. 1991. Met*: A method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49?90.
William Gale, Kenneth Church, and David Yarowsky.
1992. One sense per discourse. In Proceedings of
the Speech and Natural Language Workshop, pages
233?237.
Matt Gedigan, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of the 3rd Workshop on Scalable Natural
Language Understanding, pages 41?48, New York.
Andrew Goatly. 1997. The Language of Metaphors.
Routledge, London.
Ilana Heintz, Ryan Gabbard, Mahesh Srivastava, Dave
Barner, Donald Black, Majorie Friedman, and Ralph
Weischedel. 2013. Automatic Extraction of Lin-
guistic Metaphors with LDA Topic Modeling. In
Proceedings of the First Workshop on Metaphor in
NLP, pages 58?66, Atlanta, Georgia, June. Associa-
tion for Computational Linguistics.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. In Pro-
ceedings of the First Workshop on Metaphor in NLP,
pages 52?57, Atlanta, GA. Association for Compu-
tational Linguistics.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of HLT-
EMNLP, pages 419?426, Vancouver, Canada. Asso-
ciation for Computational Linguistics.
George Lakoff and Mark Johnson. 1980. Metaphors
we live by. University of Chicago Press, Chicago.
Hongsong Li, Kenny Q. Zhu, and Haixun Wang. 2013.
Data-driven metaphor recognition and explanation.
Transactions of the ACL, 1:379?390.
James Martin. 1990. A computational model of
metaphor interpretation. Academic Press Profes-
sional, Inc., San Diego, CA, USA.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153?157.
Michael Mohler, David Bracewell, Marc Tomlinson,
and David Hinote. 2013. Semantic signatures for
example-based linguistic metaphor detection. In
Proceedings of the First Workshop on Metaphor in
NLP, pages 27?35, Atlanta, GA. Association for
Computational Linguistics.
Andreas Musolff. 2000. Mirror images of Eu-
rope: Metaphors in the public debate about
Europe in Britain and Germany. Mu?nchen:
Iudicium. Annotated data is available at
http://www.dur.ac.uk/andreas.musolff/Arcindex.htm.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth
Edition LDC2009T13. Linguistic Data Consortium,
Philadelphia.
Bradley Pasanek and D. Sculley. 2008. Mining mil-
lions of metaphors. Literary and Linguistic Com-
puting, 23(3):345?360.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
16
Group Pragglejaz. 2007. MIP: A Method for Iden-
tifying Metaphorically Used Words in Discourse.
Metaphor and Symbol, 22(1):1?39.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45?
50, Valletta, Malta, May. ELRA.
Evan Sandhaus. 2008. The New York Times Anno-
tated Corpus. LDC Catalog No: LDC2008T19.
Ekaterina Shutova and Lin Sun. 2013. Unsu-
pervised metaphor identification using hierarchical
graph factorization clustering. In Proceedings of
HLT-NAACL, pages 978?988.
Ekaterina Shutova and Simone Teufel. 2010.
Metaphor corpus annotated for source - target do-
main mappings. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC?10), pages 3255?
3261, Valletta, Malta, May. European Language Re-
sources Association (ELRA).
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of the 23rd International
Conference on Computational Linguistics (COL-
ING), pages 1002?1010.
Gerard Steen, Aletta Dorst, Berenike Herrmann, Anna
Kaal, Tina Krennmayr, and Trijntje Pasma. 2010. A
Method for Linguistic Metaphor Identification. Am-
sterdam: John Benjamins.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of NAACL, pages 252?259.
Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-
hen. 2011. Literal and metaphorical sense identi-
fication through concrete and abstract context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 680?
690, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
17
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 35?45,
Dublin, Ireland, August 23, 2014.
ETS Lexical Associations System for the COGALEX-4 Shared Task
Michael Flor
Educational Testing Service
Rosedale Road
Princeton, NJ, 08541, USA
mflor@ets.org
Beata Beigman Klebanov
Educational Testing Service
Rosedale Road
Princeton, NJ, 08541, USA
bbeigmanklebanov@ets.org
Abstract
We  present  an  automated  system  that  computes  multi-cue  associations  and  generates 
associated-word suggestions, using lexical co-occurrence data from a large corpus of English 
texts.  The system performs expansion of cue words to  their  inflectional  variants,  retrieves 
candidate words from corpus data, finds maximal associations between candidates and cues, 
computes an aggregate score for each candidate, and outputs an n-best list of candidates. We 
present experiments using several measures of statistical association, two methods of score 
aggregation, ablation of resources and applying additional filters on retrieved candidates. The 
system  achieves  18.6%  precision  on  the  COGALEX-4  shared  task  data.  Results  with 
additional evaluation methods are presented. We also describe an annotation experiment which 
suggests  that  the  shared  task  may  underestimate  the  appropriateness  of  candidate  words 
produced by the corpus-based system.
1 Introduction
The COGALEX-4 shared task is a multi-cue association task: finding a target word that is associated  
with a set of cue words. The task is motivated, for example, by a tip-of-the-tongue search application,  
as described by the organizers: ?Suppose, we were looking for a word expressing the following ideas: 
'superior dark coffee made of beans from Arabia', but could not remember the intended word 'mocha'. 
Since people always remember something concerning the elusive word, it would be nice to have a 
system accepting this kind of input, to propose then a number of candidates for the target word. Given  
the above example,  we might  enter  'dark',  'coffee',  'beans',  and 'Arabia',  and the system would be  
supposed  to  come  up  with  one  or  several  associated  words  such  as  'mocha',  'espresso',  or  
'cappuccino'.?
The data for  the  shared task were sampled  from the Edinburgh Associative Thesaurus (EAT - 
http://www.eat.rl.ac.uk).  For  each  of  about  8,000  stimulus  words,  the  EAT lists  the  associations 
(words) provided by human respondents, sorted according to the number of respondents who provided 
the  respective  word.  Generally,  when  more  people  provided  the  same  response,  the  underlying  
association is considered to be stronger (Kiss et al., 1973). For the COGALEX-4 shared task, the cues 
were the five strongest responses to an unknown stimulus word, and the task was to recover (guess)  
the stimulus word (henceforth, target word). The data for the task consisted of a training set of 2000 
items (for which target words were provided), and a test set of 2000 items. The origin of the data was 
not  disclosed  before  or  during  the system development  and evaluation phases  of  the  shared  task 
competition.
The ETS entry consisted of a system that uses corpus-based distributional information about pairs  
of words in English. No use was made of human association data (EAT or other), nor of any other  
information such as the order of importance of the cue words, or any special preference for the British 
spelling often used in the EAT.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
35
2 The ETS system for computing multi-cue association
Our system is defined by the following components.
1. Corpus from which the distributional information about word pairs is learned, 
along with preprocessing steps (database generation).
2. The kind of distributional information collected from the corpus (collocation & co-occurrence).
3. A measure of association between two words.
4. An algorithm for generating candidate associates using the resources above.
5. An algorithm for scoring candidate associates.
2.1 Corpus
Our corpus is composed of two sources. One part is the English Gigaword 2003 corpus (Graff and 
Cieri, 2003), with 1.7 billion tokens. The second part is an ETS in-house corpus containing texts from 
the genres of fiction and popular science (Sheehan et al., 2006), with about 430 million tokens.
2.2 Types of distributional information
From this combined corpus we have built two specific lexical resources. One resource is a bigram 
repository,  which  stores  counts  for  sequences  of  two  words.  The  other  resource  is  a  first-order
co-occurrence word-space model (Turney and Pantel, 2010), also known as a Distributional Semantic 
Model (DSM) (Baroni and Lenci, 2010). In our implementation of DSM, we counted non-directed co-
occurrence  of  tokens  in  a  paragraph,  using  no  distance  coefficients  (Bullinaria  and Levy,  2007). 
Counts for 2.1 million word-form types, and the sparse matrix of their co-occurrences, are efficiently  
compressed using the TrendStream toolkit (Flor, 2013), resulting in a database file of 4.7GB. 
The same toolkit supports both n-grams and DSM repositories, and allows fast retrieval of word 
probabilities and statistical associations for pairs of words.1 It also supports retrieval of co-occurrence 
vectors. When generating these two resources, we used no lemmatization and no stoplist. All tokens  
were converted to lowercase. All punctuation was retained and counted as tokens. The only significant  
filtering was applied to numbers: all digit-based numbers (e.g. 5, 2.1) were converted to the symbol '#'  
and counted as such. Tokenization was performed by an internal module of the TrendStream toolkit.
The lexical resources described above were not generated for the COGALEX-4 shared task. Rather, 
those are general-purpose large-scale lexical resources that we have used in previous research, for a  
variety of NLP tasks. This is an important aspect, as our intention was to find out how well those 
general resources would perform on this novel task. Our bigrams repository is actually part of a 5-
gram language  model  that  is  used  for  context-aware  spelling  correction.  The  algorithms  for  that 
application are described by Flor (2012). The DSM has been used for spelling correction (Flor, 2012),  
for essay scoring (Beigman Klebanov and Flor, 2013a), for readability estimation (Flor and Beigman  
Klebanov,  in  press;  Flor  et  al.,  2013),  as  well  as  for  a  study on  quality  of  machine  translation  
(Beigman Klebanov and Flor, 2013b).
2.3 Measures of association
For the shared task, we used three measures of word association.
Pointwise Mutual Information (Church & Hanks, 1990):
PMI ?a ,b?=log 2 P ?a ,b?P ?a ?P ?b?
Normalized Pointwise Mutual Information (Bouma, 2009):
NPMI ?a , b?=?log2 P ?a ,b?P ?a?P ?b ? ?/ ??log2 P ?a ,b??
1 The TrendStream toolkit provides compression and storage for large-scale n-gram models, and for large-scale co-occurrence 
matrices. In all cases, actual counts are stored and values for statistical association measures are computed on the fly during 
data retrieval.
36
Simplified log-Likelihood (Evert, 2008):
SLL?a ,b?=2?P ?a ,b??log P ?a ,b ?P ?a ?P ?b??P ?a ,b??P ?a ?P ?b?
P(a,b) signifies probability of joint  co-occurrence.  For bigrams,  that  is  joint  co-occurrence in a 
specific sequential order (e.g. AB vs. BA) ; for DSM data the co-occurrence is order-independent.
2.4 Procedure for generating candidate multi-cue associates
Our general procedure for generating target candidates is as follows. For each of the five cue words,  
candidate targets are generated separately, from the corpus-based resources:
1. From the DSM (generally associated words)
2. Left words from bigrams (words that, in the corpus, appeared immediately to the left of the cue)
3. Right words from bigrams (words that appeared immediately to the right of the cue)
Retrieved lists of candidates can be quite large, with hundreds and even thousands of different 
neighbors. One specific filter implemented at this stage was that only word-forms (alphabetic strings) 
were allowed, and any punctuation or '#' strings were filtered out.
Since  our  resources  are  not  lemmatized,  we  extended  the  candidate  retrieval  procedure  by 
expanding the cue words to their inflectional variants. This provides richer information about semantic 
association. We used an in-house morphological analyzer/generator. Inflectional expansions were not  
constrained for part of speech or word sense. For example, given the cue set {1:letters 2:meaning 
3:sentences 4:book 5:speech} (from the training set of the shared task, target: 'words'), after expansion 
the set  of  cues is   {1:letters,  lettered,  letter,  lettering 2:meaning,  means,  mean,  meant,  meanings  
3:sentences,  sentence,  sentenced,  sentencing 4:book,  books,  booking,  booked 5:speech,  speeches}. 
The vector of right neighbors for the cue 'letters', brings such words as {sent, from, between, written,  
came,  addressed,  ...}.  The vector  of  left  neighbors  for  same  cue word brings  such candidates  as 
{write, send, love, capital, review, ...}. From the DSM, the vector of co-occurrence may bring some of 
the same words (but with different values of association), as well as words that do not generally occur  
immediately before or after the cue word, e.g. {time, people, word, now,?}. 
Next,  we  apply  filtering  that  ensures  the  minimal  requirement  for  multi-word  association  ?  a 
candidate must be related to all cues. The candidate must appear (at least once) on the list of words 
generated from each cue family. A candidate word that does not meet this requirement is filtered out.2
2.5 Scoring of candidate associates
Scoring of candidate associate-words is a two-stage process. First, for each candidate, we look for the 
strongest association value it has with each of the five cue families. Then, the five strongest values are 
combined into an aggregated score.
For a given cue family, several instances of the same candidate associate might be retrieved, with 
various values of association score (from DSM and n-grams, and also for each specific inflectional  
form of the cue). We pick the highest score, siding with the source that provides the strongest evidence 
of connection between the cue and the candidate associate. The maximal association value is stored as 
the best score for this candidate with the given cue family. We note that since the same measure of  
association is used, the scores from the different sources are numerically comparable. 3 For example, 
when  PMI is  used  as  the  association  measure,  the  following values  were  obtained  for  candidate  
'capital'  with  cue  family  'letters,  lettered,  letter,  lettering'  (expanded  from 'letters').  General  co-
occurrence (DSM): capital & letters: 0.477, capital & letter: 0.074, etc.; left bigrams: capital letters: 
5.268, capital letter: 2.474, etc. The strongest association here is the bigram 'capital letters', and the 
value 5.268 is the best association of the candidate 'capital' with this cue family. 
Next, for each candidate we compute an aggregate score that represents its overall association with  
all five cues. In current study, we experimented with two forms of aggregation: 1) sum of best scores  
2 This is 'baseline' filtering, applied in all experiments. Experiments with additional filtering are described in section 4.2.
3 In any single experimental run we consistently use the same measure of association (no mixing of different formulae).
37
(SBS), and 2) product (multiplication) of ranks (MR). Sum of best scores is simply the sum of best 
association scores that a candidate has with each of the five cues (families). To produce a final ranked  
list of candidate targets, candidates are sorted by their aggregate sum value (better candidates have  
higher values). Multiplication of ranks has been proposed as an aggregation procedure by Rapp (2014,  
2008). In this procedure, all candidates are sorted by their association scores with each of the five cues  
(families) separately, and five rank values are registered for each candidate. The five rank values are  
then multiplied to produce the final aggregate score. All candidates are then sorted by the aggregate  
score, and in such ranking better candidates have lower aggregate scores. Multiplication of ranks is  
computationally more intensive than sum of scores ? for a given set of candidate words from five cues, 
multiplication  of  ranks  requires  six  calls  for  sorting,  while  aggregation  via  sum-of-best-scores 
performs sorting only once.
Finally, all candidates are sorted by their aggregate score and top N are outputted for the calculation 
of precision@N, to be described below.
3 Results
Our system ran with several different configuration settings, using various association measures and 
score aggregation procedures. Under any given configuration, the system produces, for each item (i.e.  
a set of five cue words), a ranked list of candidates. According to the rules of the shared task, official  
results are computed by selecting the single best candidate for the item as the suggested target word. If  
the  suggested  word  strictly  matches  the  gold-standard  word  (ignoring  upper/lower  case),  it  is 
considered a match. If the two strings differ even slightly, it is considered a mismatch. The reported 
result is precision (percent matches) over the test set of 2000 items. 
With strict-matching, our best result for the test-set was precision of 18.6% (372 correctly suggested 
targets). This was obtained by using NPMI as the association measure, product of ranks as the score  
aggregation procedure, and with filtering of candidates using a stoplist and a frequency filter.4
The shared task was described as multi-cue association for finding a sought-after 'missing' word, a  
situation  not  unlike  a  tip-of-the-tongue  phenomenon.  In  such  situation,  a  person  looking  for  an 
associated word,  might  find it  useful  if the system returns not  just  one highest-ranked suggestion 
(which would often be a miss), but a list of several top-ranked suggestions ? the target word might be  
somewhere on such list5. Thus, we also present our results in terms of precision for n-best suggestions 
? i.e. in how many cases the target word was among the top n returned by the system, with n ranging 
from 1 up to 25. 
A similar consideration applies to inflectional variants. A person looking for a word associated with 
a set of cue words, might be satisfied when a system returns either a base-form or an inflected variant  
of the target word. Thus, we report our results both in terms of strict matches to gold-standard targets  
and under a condition of 'inflections-allowed'.6 On the test set, our best result for precision@1, with 
inflections allowed, is 24.35% (487 matching suggestions).
First, we present our baseline results. Figure 1 presents the results of our system for the training set 
of  2000  items,  using  the  NPMI  association  measure.  Panel  1A  presents  data  obtained  using 
aggregation via  sum-of-best-scores  (SBS).  Panel  1B presents  data  obtained  using  aggregation  via 
multiplication of ranks (MR). Figure 2 presents similar breakdown for results of the test set. Both sets  
of results are quite similar. Thus, we restrict our attention to just the results of the test set. 7
4 We initially submitted a result of 14.95% strict-match precision@1 (see Figure 2A). This was improved to 16.1% (Figure 
2B), and with additional filters ? to 18.6% (see section 4.2).
5 A list of n-best suggestions is standard approach for presenting candidate corrections for misspellings (Flor, 2013; Mitton, 
2008). Also, precision ?at n documents? is a well known evaluation approach in information retrieval (Manning et al., 2008). 
A recent use of n-best suggestions in an interactive NLP system is illustrated by Madnani and Cahill (2014).
6 Each target word form, both in the training set and the test set, was automatically expanded to all its inflectional variants,  
using our morphological analyzer/generator. In our evaluations, a candidate target is considered a 'hit' if it matches the 
gold-standard target or one of its inflectional variants.
7 We did not use the training set for any training or parameter tuning. We used it to select the optimal association measures 
for this task ? we also experimented with t-score, weighted PMI and conditional probability, but PMI and NPMI performed 
much better than others. 
38
1 3 5 7 9 11 13 15 17 19 21 23 25
0
10
20
30
40
50
60
Training-set:  NPMI with SBS aggregation
+Inflections
Strict n-best
Pre
cis
ion
 %
A  
1 3 5 7 9 11 13 15 17 19 21 23 25
0
10
20
30
40
50
60
Training-set:  NPMI with MR aggregation
+Inflections
Strict n-bes t
Pre
cis
ion
 %
B
Figure 1. System performance on the training-set (percent correct out of 2000 items), for various 
values of n. Panel A: using sum-of-best-scores aggregation; Panel B: using multiplication-of-ranks 
aggregation. 'Strict': evaluation uses strict matching to gold-standard target, '+Inflections': inflectional 
variants are allowed in matching to gold-standard target.
1 3 5 7 9 11 13 15 17 19 21 23 25
0
10
20
30
40
50
60
Test-set:  NPMI with SBS aggregation
+Inflections
Strict n-best
Pre
cis
ion
 %
A  
1 3 5 7 9 11 13 15 17 19 21 23 25
0
10
20
30
40
50
60
Test-set:  NPMI with MR  aggregation
+Inflections
Strict n-best
Pre
cis
ion
 %
B
Figure 2. System performance on the test-set (percent correct out of 2000 items). 
We found,  as expected,  that performance improves when the target is sought among the  n-best 
candidates produced by the system. With NPMI and MR aggregation, strict-match precision improves 
from 16.1% for  precision@1 to  30.3% for  precision@5,  37% for  precision@10,  and  46.9% for  
precision@25 (Figure 2B).
Another expected result is that performance is better when matching of targets allows inflectional  
variants. This is clearly seen on the charts, as the difference between the two lines. With NPMI and  
MR aggregation, precision@1 improves from 16.1% to 21.45%, precision@5 improves from 30.3% to 
36.3%, and precision@25 improves from 46.9% to 54%, Similar improvement is observed when using 
aggregation via sum-of-best-scores.
Our third finding is that multiplication of ranks achieves slightly better results than sum-of-best-
scores  (Figure  2,  panel  B  vs.  panel  A).  For  precision@1 with  strict  matches,  using  NPMI,  MR 
achieves  16.1% and  with  inflectional  variants  21.45%,  while  SBS  achieves  14.95% and  20.25% 
respectively.  For  precision@10,  MR  achieves  37%  (43.55%),  while  SBS  achieves  36%  (42%). 
Notably, MR is consistently superior to SBS for all values of n-best, from 1 to 25, under both strict or 
inflections-allowed matching, with both NPMI and PMI (see Figure 3). However, the advantage is 
consistently rather small ? about 1-1.5%. Since MR is computationally more intensive, SBS emerges 
as a viable alternative. 
We have  also  conducted  experiments  with  three  different  measures  of  association.  Results  are 
presented in Figure 3. With MR aggregation, NPMI achieves better results than the PMI measure.  
Both measures clearly outperform the Simplified log-Likelihood. Similar  results are obtained with 
SBS aggregation. For each association measure, allowing inflections provides better results than strict 
matching to gold-standard targets.
39
1 3 5 7 9 11 13 15 17 19 21 23 25
0
5
10
15
20
25
30
35
40
45
50
55 Test-set:  SBS aggregation
NPMI+inf NPMI+strict PMI+inf
PMI+strict SLL+inf SLL+strict
n-bes t
Pre
cis
ion
 %
A
 
1 3 5 7 9 11 13 15 17 19 21 23 25
0
5
10
15
20
25
30
35
40
45
50
55 Test-set:  MR aggregation
NPMI+inf NPMI+strict PMI+inf
PMI+strict SLL+inf SLL+strict
n-bes t
Pre
cis
ion
 %
B
Figure 3. System performance on the test-set (2000 items) with three different association measures. 
Panel A: using sum-of-best-scores aggregation; Panel B: using multiplication-of-ranks aggregation. 
Legend: PMI: pointwise mutual information, NPMI: Normalized PMI, SLL: simplified log-likelihood, 
'Strict': evaluation uses strict matching to gold-standard target, '+Inf': inflectional variants are allowed 
in matching to gold-standard target.
4 Additional studies
In  several  additional  experiments  we  looked  at  the  contribution  of  different  factors  to  overall 
performance.  We  tried  several  variations  of  resource  combination  and  also  tested  filtering  of 
candidates by frequency and by using a list of stopwords. 
4.1 Ablation experiments
We investigated how the restriction of resources impacts the performance on this task. Specifically we 
restricted  the  resources  as  follows.  In  one  condition  we  used  only  the  bigrams  data,  retrieving 
candidates only from the vectors of left co-occurring words (immediate preceding words) of each cue  
word (condition NL ? n-grams left). A similar restriction is when candidates are retrieved only from 
right (immediate successor) words (condition NR ? n-grams right). A third condition still uses only 
bigrams, but admits candidates from both left and right vectors (condition NL+NR). Under the fourth 
condition (DSM), n-grams data is not used at all, only the DSM resource is used. In the fifth and sixth 
conditions we combine candidates from DSM with n-gram candidates (left or right vectors only ? 
respectively). The seventh condition is our standard ? candidates from DSM and both left and right 
neighbors from bigrams are admitted. For those experiments, we used NPMI association measure with 
MR aggregation, and included inflections in evaluation. The results are presented in Figure 4.
Using  only  right-hand  associates  (typical  textual  successors  of  cue  words)  provides  very  low 
performance (precision@1 is 2.95%). Using only left-hand associates (typical textual predecessors of 
cue words) provides slightly better performance (precision@1 is 4.5%). However, it is notable that  
there are some items in the EAT data where all cues are strong bigrams with the target, e.g. {orange,  
fruit, lemon, apple, tomato} with target 'juice'.  Combining these two resources (condition NL+NR) 
provides much better performance: precision@1 is 8.5%. Using just the DSM, the system achieves  
10.5% precision@1, which may seem rather close to the combined NL+NR 8.5%. However, with 
DSM, for   n-best  lists  precision rises  quite  sharply (e.g.  24.35% for  precision@5),  while  for  the 
NL+NR setting precision tends to be under 17% for all values of n up to 25. 
Since our DSM and bigrams resources are built on the same corpus of text, for any given set of cues  
the DSM produces all the candidates that the bigrams resource does (but with different association 
values) and a lot of other candidates. However, results for DSM+NR and DSM+NL settings (which 
are better than DSM alone) indicate that association values from bigrams contribute substantially to 
overall  performance.  The  best  result  in  this  experiment  is  achieved  by  a  setting  that  combines  
candidates (and association values) from all three resources, indicating further that associations from 
sequential word combinations (bigrams) provide a substantial contribution to performance in this task.
40
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
0
5
10
15
20
25
30
35
40
45
50
55 Test-set: studies with resource variation
ALL
DSM+NL
DSM+NR
DSM
NL+NR
NL
NR
n-bes t
Pre
cis
ion
 %
Figure 4. System performance on the test-set (2000 items), with various resource restrictions. 
All runs used NPMI association measure and MR aggregation. Evaluation allowed inflections.
NL/NR ? left/right neighbors from bigrams.
4.2 Applying filters on retrieved candidates
We also experimented with applying some filters on the retrieved candidates for each item. One of the 
obvious filters to use is to filter out stopwords. For general tip-of-the-tongue search cases, common  
stopwords are  rarely useful  as  target  words;  thus  presenting stopwords as  candidates  makes  little  
sense.  We used a list  of  87 very common English stopwords,  including the articles  {the,  a,  an}, 
common prepositions, pronouns, wh-question words, etc. However, since the data of the shared task 
comes from EAT, common stopwords are actually targets in some cases in that collection. Therefore, 
we used the following strategy. For a given item, if at least one of the five cue words is a stopword,  
then we assume that the target might also be a stopword, and so we do not use the stoplist to filter  
candidates for this item. However, if none of the cues is a stopword, we do apply filtering ? any  
retrieved candidate word is filtered out if it is on the stoplist. An additional filter, applied with the  
stoplist, was defined as follows: if a candidate word is strictly identical to one of the cue words, the  
candidate is filtered out (to allow for potentially more suitable candidates).8 
The other  filter  considers frequency of  words.  The PMI measure  is  known to overestimate  the 
strength of pair association when one of the words is a low-frequency word (Manning & Sch?tze,  
1999).  Normalized  PMI is  also  sensitive  to  this  aspect,  although less  than PMI.  Thus,  we  use  a 
frequency filter to drop some candidate words. For technical reasons, it was easier for us to apply a  
cutoff on the joint frequency of a candidate and a cue word. We used a cutoff value of 10 ? a candidate 
is dropped if corpus data indicates it co-occurs with the cue words fewer than 10 times in the corpus 
data.
We applied the stoplist filter, the frequency filter and a combination of those two filters, always  
using NPMI as our association measure, aggregating scores via multiplication-of-ranks, and allowing 
inflections in evaluation. No ablation of resources was applied. The results are presented in Figure 5.  
The baseline condition is when neither of the two filters is applied. The frequency filter with cutoff=10 
provides a very small improvement for precision@1, and for higher values of best-n it actually hurts 
performance.  Application  of  a  stoplist  provides  a  very  slight  improvement  of  performance.  The 
combination of a stoplist and frequency cutoff=10 provides a sizable improvement of performance 
(precision@1 is  24.35% vs.  baseline 21.45%, and precision@10 is  44.55% vs.  baseline 43.55%). 
However, for n-best lists of size 15 and above, performance without filters is slightly better than with 
those filters.  For the shared task (using strict matching ? no inflections),  our best result  is 18.6% 
precision@1 with two filters (16.1% without filters).
8 Cases when a candidate word is identical to one of the cues do occur when associate candidates are harvested from corpus 
data. Such candidates have little utility for a missing-word-search task. Notably, however, the training-set for the shared 
task did have one item where the target word was identical to one of the cues: Yeah ~ Yeah no Yes Beatles Oh.
41
Given that the gold-standard targets in the shared task are original stimulus words form the EAT 
collection, we can use a special restriction ? restrict the candidates to just the EAT stimuli word-list  
(Rapp,  2014).  Notably,  this  is  a  very  specific  restriction,  suited  to  the  specific  dataset,  and  not  
applicable to the general case of multi-cue associations or tip-of-the-tongue word searches. We used 
the list of 7913 single-word stimuli from EAT as a filter in our system ? generated candidates that  
were  not  on  this  list  were  dropped  from consideration.  The  results  (Figure  5)  indicate  that  this  
restriction  (EATvocab)  provides  a  substantial  improvement  over  the  baseline  condition.  For 
precison@1,  using  EATvocab  (24.55%)  is  comparable  to  using  a  stoplist+cutoff10  (24.35%).  
However, for larger n-best lists, EATvocab filter provides substantially better performance. 
1 3 5 7 9 11 13 15 17 19 21 23 25
20
25
30
35
40
45
50
55
60
65
Test-set: filtering experiments 
EAT vocab
Stoplist+C10
Stoplist
C10
Baseline
n-bes t
Pre
cis
ion
 %
Condition Precision@1 Precision@10
EAT Vocabulary 24.55% 52.00%
Stoplist & Cutoff10 24.35% 44.55%
Stoplist 22.15% 43.85%
Cutoff10 21.70% 42.50%
Baseline (no filters) 21.45% 43.55%
Figure 5. System performance on the test set with different filtering conditions. All runs use NPMI as?
sociation and MR aggregation. Inflections allowed in evaluation. C10: frequency cutoff=10.
5 Small-scale evaluation using direct human judgments
Inspecting results from training-set data, we observed a number of cases where the system produced 
very plausible targets which however were struck down as incorrect (not matching the gold-standard).  
For example, for the cue set {music, piano, play, player, instrument} the gold-standard target was 
'accordion'. But why not 'violin' or 'trombone'? To provide a more in-depth evaluation of the results, 
we sampled 180 items at random from the test set, along with the candidate targets produced by our 
system,9 and submitted those to evaluation by two research assistants. For each item, evaluators were  
given the five cue words and the best candidate target generated by the system. They were told that the 
word is supposed to be a common associate of the five cues, and asked to indicate, for each item,  
whether the candidate was (a) Just Right; or (b) OK; or (c) Inadequate; (a,b,c are on ordinal scale).
Out of the 180 items, 80 were judged by both annotators. Table 1 presents the agreement matrix  
between the two annotators. Agreement on the 3 classes was kappa=0.49. If  Just Right and  OK are 
collapsed, the agreement is kappa=0.60. The discrepancy is largely due to a substantial number of  
instances that one annotator judged OK and the other ? Just Right.
Inadequate OK Just Right TOTAL
Inadequate 17 6 1 24
OK 6 25 10 41
Just Right 0 3 12 15
TOTAL 23 34 23 80
Table 1. Inter-annotator agreement matrix for a subset of items from the test-set.
9 Using all resources, NPMI association measure, MR aggregation, and with the general stoplist filter.
42
We note that one annotator commented on a difficulty making a decision in a number of cases  
where the cues are a list of mostly adjectives or possessives, and the target produced by the system is  
an  adverb.  For  example,  the  cue  set  {busy,  house,  vacant,  engaged,  empty}  with  the  proposed 
candidate  target  'currently';  the  cue  set  {food,  thirsty,  tired,  empty,  starving}  with  the  proposed 
candidate  'perpetually';  the  cue  set  {fat,  short,  build,  thick,  built}  with  the  proposed  candidate 
'slightly'; the cue set {mine, yours, his, is, theirs} with the proposed target 'rightfully'. This annotator 
felt that these responses were OK, while the other annotator rejected them. 
We merged the two annotations to provide a single annotation for the full set of 180 items by taking 
one annotator's judgment on single-annotated cases and taking the lower of the two judgments for the 
double annotated disagreed cases (thus, OK and Inadequate are merged to Inadequate; Just Right and 
OK are merged to OK). We next compare these annotations to the EAT gold standard. Table 2 shows 
the confusion matrix between the ?gold label? from EAT and our annotation. We observe that the 
totals for Just Right and EAT-match are almost identical (43 vs 42); however, only 17 items were both 
Just Right and EAT-matches. There were 24 EAT matches that were judged as OK by the annotators 
(presumably,  these  did  not  quite  create  the  ?just  right?  impression  for  at  least  one  annotator).  
Examples include: the cue set {beer, tea, storm, ale, bear} with the proposed correct target 'brewing' 
(one annotator commented that the relationship with ?bear? was unclear); the cue set {exam, match,  
tube, try, cricket} with the proposed correct target 'test' (one annotator commented that the relationship 
with 'cricket' was unclear); the cue set {school, secondary, first, education, alcohol} with the proposed 
correct target 'primary' (one annotator commented that the relationship with 'alcohol'  was unclear). 
These  results  might  reflect  cultural  differences  between  original  EAT  respondents  (British 
undergraduates circa year 1970) and present-day American young adults who, e.g. might not know 
much  about  cricket.  Another  possibility  is  that  in  the  EAT  collection,  the  5 th cue  sometimes 
corresponds to a very weak associate provided by just a single respondent out of 100, as in brewing-
bear and  primary-alcohol cases.  Interestingly,  the  weak  cues  did  not  confuse  the  system,  but 
replicability of the human judgments for such cases is doubtful.
Just Right OK Inadequate Total
EAT match 17 24 1 42
EAT mismatch 26 58 54 138
Total 43 82 55 180
Table 2. Annotated data vs. gold-standard matches for a set of 180 items.
There were also 26 instances that were judged as Just Right yet were not EAT-matches. Three of 
these were derivationally related, like 'build' (EAT target) vs 'buildings'  (proposed) for the cue set 
{house, up, construct, destroy, bricks}, the others were 'dwell' vs 'dwellings', 'collector' vs 'collecting'. 
In the rest of the cases, the generated candidates seemed as good as, or better, than the EAT words.  
For example, the cue set {ships, boat, sea, ship, ocean} had 'liners' as the EAT target, whereas the 
system proposed 'cruise'. For the cue set {natural, animal, nature, birds, fear}, the gold-standard EAT 
target is 'instinct', whereas the system proposed 'predatory'. For the cue set {sound, speak, sing, noise,  
speech} the gold-standard EAT target is 'voice', while the system produced 'louder'. For the cue set 
{music, band, noise, club, folk} the target was 'jazz', whereas the system proposed 'dance'. For the cue 
set  {violin,  music,  orchestra,  bow,  instrument}  the  target  was  'cello',  while  the  system produced 
'stringed'. Furthermore, in as many as 58 cases (32%) the response produced by the system did not  
match the target from EAT, but was OK-ed by the annotators. Some examples include: the cue set  
{fool, loaf, idiot, lout, lazy} with proposed candidate 'ignorant'; the cue set {hard, problems, work,  
hardship,  trouble}  with  proposed  candidate  'economic';  {interesting,  intriguing,  amazing,  book,  
exciting}  with  proposed  candidate  'discoveries';  {lazy,  chair,  about,  lying,  sitting}  with  proposed 
candidate 'motionless'. In all, if the system were evaluated by counting Just Right and OK annotations 
as correct, the precison@1 would have been (43+82)/180 = 69%. The estimation of performance based 
on gold-standard EAT data for this set is 42/180 = 23%, exactly one-third of what annotators found to  
be reasonable responses. This suggests that evaluation of multi-cued retrieval on targets from EAT 
rejects many good semantic associates, and thus might be considered too harsh.
43
6 Conclusions
This  paper  presented  an  automated  system  that  computes  multi-cue  associations  and  generates  
associated-word suggestions, using lexical co-occurrence data from a large corpus of English texts.  
The system uses pre-existing resources ? a large  n-ngram database and a large word-co-occurrence 
database, which have been previously used for a range of different NLP tasks. The system performs 
expansion of cue words to their inflectional variants, retrieves candidate words from corpus data, finds  
maximal associations between candidates and cues, and then computes an aggregate score for each 
candidate. The collection of candidates is then sorted and an n-best list is presented as output. In the 
paper we presented experiments using various measures of statistical association and two methods of  
score  aggregation.  We  also  experimented  with  limiting  the  lexical  resources,  and  with  applying 
additional filters on retrieved candidates. 
For test-set evaluation, the shared task requires strict-matches to gold-standard targets. Our system,  
in optimal configuration, was correct in 372 of 2000 cases, that is precision of 18.6%. We have also 
suggested a more lenient evaluation, where a candidate target is also considered correct if it is an 
inflectional  variant  of  the gold-standard word.  When inflections are allowed,  our system achieves 
precision of 24.35%. Performance improves dramatically when evaluation considers in how many 
cases the gold-standard target (or its inflectional variants) are found among the  n-best suggestions 
provided by the system. For example, with a list of 10-best suggestions, precision rises to 45%, and to 
54% with a list of 25-best. Using an n-best list of suggestions makes sense for applications like tip-of-
the-tongue situation. 
We note that the specific data set used in COGALEX-4 shared task, i.e. the Edinburgh Associative 
Thesaurus, might be sub-optimal for evaluation of multi-cue associative search. With the EAT dataset, 
the gold-standard words were the original stimuli from EAT, and the cue words were the associated  
words that were most frequently produced by respondents in the original EAT experiment (Kiss et al., 
1973). Rapp (2014) has argued that corpus-based computation of reverse-associations is a reasonable 
test  case  for  multi-cued word  search.  However,  Rapp also  notes  that  in  many cases,  suggestions  
provided by a corpus-based system are quite reasonable, but are not correct for the EAT dataset. We  
have conducted pilot human annotation on a small subset of the test-set ? judging how reasonable the 
top suggestion of our system is in general, and not whether it matched EAT targets. In this experiment,  
69% of the system's  first  responses were judged acceptable by humans,  while only 23% matched  
targets.  This  provides  a  quantitative  confirmation  that  EAT-based  evaluation  underestimates  the 
quality of results produced by a corpus-based multi-cue association system. 
The use of data from EAT hints at the following direction for future research. In the original EAT 
data, the first cue is actually the strongest associate of the target word (original stimulus), while other 
cues  are  much  weaker  associates.  In  our  current  implementation,  we  treated  all  cues  as  equally 
important.  Future  research  may  include  consideration  for  relative  importance  or  relevance  of  the 
different cues. In potential applications, like the tip-of-the-tongue word search, a user may be able to 
specify which cues are more relevant than others.   
Acknowledgments
Special thanks to Melissa Lopez and Matthew Mulholland for their help with the evaluation study. We 
also thank Mo Zhang, Paul Deane and Keelan Evanini at ETS, and three anonymous COGALEX re?
viewers, for comments on earlier drafts of this paper.
References
Marko Baroni and Allesandro Lenci. 2010. Distributional Memory: A General Framework for Corpus-Based 
Semantics. Computational Linguistics, 36(4), 673-721
Beata Beigman Klebanov and Michael Flor.  2013a.  Word Association Profiles and their Use for Automated 
Scoring  of  Essays. In  Proceedings  of  the  51st  Annual  Meeting  of  the  Association  for  Computational 
Linguistics, pages 1148?1158, Sofia, Bulgaria. 
Beata Beigman Klebanov and Michael Flor. 2013b. Associative Texture Is Lost In Translation. In Proceedings 
of the Workshop on Discourse in Machine Translation (DiscoMT),  pages 27?32. ACL 2013 Conference, 
Sofia, Bulgaria.
44
Gerlof  Bouma.  2009.  Normalized  (Pointwise)  Mutual  Information  in  Collocation  Extraction. In:  Chiarcos, 
Eckart de Castilho & Stede (eds), From Form to Meaning: Processing Texts Automatically, Proceedings of  
the Biennial GSCL Conference 2009, T?bingen, Gunter Narr Verlag, p. 31?40.
John A. Bullinaria  and Joseph P. Levy.  2007. Extracting semantic representations from word co-occurrence 
statistics: A computational study. Behavior Research Methods, 39:510?526.
Kenneth  Church  and  Patrick  Hanks.  1990.  Word  association  norms,  mutual  information  and  lexicography.  
Computational Linguistics, 16(1), 22?29.
David Graff and Christopher Cieri. 2003. English Gigaword. LDC2003T05. Philadelphia, PA, USA: Linguistic 
Data Consortium.
Stefan  Evert.  2008.  Corpora  and  collocations.  In  A.  L?deling  and  M. Kyt?  (eds.),  Corpus  Linguistics:  An 
International Handbook, Mouton de Gruyter: Berlin.
Michael Flor. 2013. A fast and flexible architecture for very large word n-gram datasets.  Natural Language 
Engineering, 19(1), 61-93.
Michael  Flor.  2012.  Four  types  of  context  for  automatic  spelling  correction.  Traitement  Automatique  des  
Langues (TAL),  53:3  (Special  Issue:  Managing  noise  in  the  signal:  error  handling  in  natural  language 
processing), 61-99. 
Michael  Flor  and  Beata  Beigman  Klebanov.  (in  press)  Associative  Lexical  Cohesion  as  a  factor  in  Text  
Complexity. Accepted for publication in the International Journal of Applied Linguistics.
Michael  Flor,  Beata  Beigman  Klebanov  and  Kathleen  M.  Sheehan.  2013.  Lexical  Tightness  and  Text 
Complexity.  In  Proceedings of the 2th Workshop of Natural  Language Processing for Improving Textual 
Accessibility (NLP4ITA), p.29?38. NAACL 2013 Conference, Atlanta, Georgia.
G.R. Kiss, C. Armstrong, R. Milroy  and J. Piper. 1973. An associative thesaurus of English and its computer  
analysis.  In Aitken, A.J., Bailey, R.W. and Hamilton-Smith, N. (Eds.), The Computer and Literary Studies.  
Edinburgh: University Press.
Nitin Madnani and Aoife Cahill. 2014. An Explicit Feedback System for Preposition Errors based on Wikipedia  
Revisions. To appear in Proceedings of the 9th Workshop on Innovative Use of NLP for Building Educational 
applications (BEA-9). ACL 2014 Conference, Baltimore, MD.
Christopher  D.  Manning,  Prabhakar  Raghavan,  and  Hinrich  Sch?tze.  2008.  Introduction  to  Information 
Retrieval. Cambridge University Press.
Christopher D. Manning, and Hinrich Sch?tze. 1999. Foundations of Statistical Natural Language Processing, 
1999, Cambridge, Massachusetts, USA: MIT Press.
Roger  Mitton.  2008.  Ordering  the  suggestions  of  a  spellchecker  without  using  context.  Natural  Language 
Engineering, 15(2), 173?192.
Reinhard Rapp. 2014. Corpus-Based Computation of Reverse-Associations. Proceedings of LREC.
Reinhard Rapp. 2008. The computation of associative responses to multiword stimuli. In  Proceedings of the 
Workshop on Cognitive Aspects of the Lexicon (COGALEX) at COLING-2008, p.102?109. Manchester, UK
Kathleen  M.  Sheehan,  Irene  Kostin,  Yoko  Futagi,  Ramin  Hemat  and  Daniel  Zuckerman.  2006.  Inside 
SourceFinder: Predicting the Acceptability Status of Candidate Reading-Comprehension Source Documents. 
ETS research report RR-06-24. Educational Testing Service: Princeton, NJ.
Peter  Turney  and  Patrick  Pantel.  2010.  From Frequency  to  Meaning:  Vector  Space  Models  of  Semantics.  
Journal of Artificial Intelligence Research, 37, 141-188.
45
