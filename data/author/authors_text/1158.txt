Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 610?619,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Decomposability of Translation Metrics
for Improved Evaluation and Efficient Algorithms
David Chiang and Steve DeNeefe
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292 USA
{chiang,sdeneefe}@isi.edu
Yee Seng Chan and Hwee Tou Ng
Department of Computer Science
National University of Singapore
Law Link
Singapore 117590
{chanys,nght}@comp.nus.edu.sg
Abstract
B??? is the de facto standard for evaluation
and development of statistical machine trans-
lation systems. We describe three real-world
situations involving comparisons between dif-
ferent versions of the same systems where one
can obtain improvements in B??? scores that
are questionable or even absurd. These situ-
ations arise because B??? lacks the property
of decomposability, a property which is also
computationally convenient for various appli-
cations. We propose a very conservative modi-
fication to B??? and a cross between B??? and
word error rate that address these issues while
improving correlation with human judgments.
1 Introduction
B??? (Papineni et al, 2002) was one of the first au-
tomatic evaluation metrics for machine translation
(MT), and despite being challenged by a number
of alternative metrics (Melamed et al, 2003; Baner-
jee and Lavie, 2005; Snover et al, 2006; Chan and
Ng, 2008), it remains the standard in the statistical
MT literature. Callison-Burch et al (2006) have sub-
jected B??? to a searching criticism, with two real-
world case studies of significant failures of corre-
lation between B??? and human adequacy/fluency
judgments. Both cases involve comparisons between
statistical MT systems and other translation meth-
ods (human post-editing and a rule-based MT sys-
tem), and they recommend that the use of B??? be
restricted to comparisons between related systems or
different versions of the same systems. In B????s de-
fense, comparisons between different versions of the
same system were exactly what B??? was designed
for.
However, we show that even in such situations,
difficulties with B??? can arise. We illustrate three
ways that properties of B??? can be exploited to
yield improvements that are questionable or even
absurd. All of these scenarios arose in actual prac-
tice and involve comparisons between different ver-
sions of the same statistical MT systems. They can
be traced to the fact that B??? is not decomposable
at the sentence level: that is, it lacks the property
that improving a sentence in a test set leads to an
increase in overall score, and degrading a sentence
leads to a decrease in the overall score. This prop-
erty is not only intuitive, but also computationally
convenient for various applications such as transla-
tion reranking and discriminative training. We pro-
pose a minimal modification to B??? that reduces
its nondecomposability, as well as a cross between
B??? and word error rate (WER) that is decompos-
able down to the subsentential level (in a sense to be
made more precise below). Both metrics correct the
observed problems and correlate with human judg-
ments better than B???.
2 The B??? metric
Let gk(w) be the multiset of all k-grams of a sentence
w. We are given a sequence of candidate translations
c to be scored against a set of sequences of reference
translations, {r j} = r1, . . . , rR:
c = c1, c2, c3, . . . , cN
r1 = r11, r
1
2, r
1
3, . . . , r
1
N
...
rR = rR1 , r
R
2 , r
R
3 , . . . , r
R
N
610
Then the B??? score of c is defined to be
B???(c, {r j}) =
4?
k=1
prk(c, {r
j})
1
4 ? bp(c, {r j}) (1)
where1
prk(c, {r
j}) =
?
i
????gk(ci) ?
?
j gk(r
j
i )
????
?
i |gk(ci)|
(2)
is the k-gram precision of c with respect to {r j}, and
bp(c, r), known as the brevity penalty, is defined as
follows. Let ?(x) = exp(1 ? 1/x). In the case of a
single reference r,
bp(c, r) = ?
(
min
{
1,
?
i |ci|
?
i |ri|
})
(3)
In the multiple-reference case, the length |ri| is re-
placed with an effective reference length, which can
be calculated in several ways.
? In the original definition (Papineni et al, 2002),
it is the length of the reference sentence whose
length is closest to the test sentence.
? In the NIST definition, it is the length of the
shortest reference sentence.
? A third possibility would be to take the average
length of the reference sentences.
The purpose of the brevity penalty is to prevent
a system from generating very short but precise
translations, and the definition of effective reference
length impacts how strong the penalty is. The NIST
definition is the most tolerant of short translations
and becomes more tolerant with more reference sen-
tences. The original definition is less tolerant but
has the counterintuitive property that decreasing the
length of a test sentence can eliminate the brevity
penalty. Using the average reference length seems
attractive but has the counterintuitive property that
1We use the following definitions about multisets: if X is a
multiset, let #X(a) be the number of times a occurs in X. Then:
|X| ?
?
a
#X(a)
#X?Y (a) ? min{#X(a), #Y (a)}
#X?Y (a) ? max{#X(a), #Y (a)}
an exact match with one of the references may not
get a 100% score. Throughout this paper we use the
NIST definition, as it is currently the definition most
used in the literature and in evaluations.
The brevity penalty can also be seen as a stand-
in for recall. The fraction
?
i |ci |?
i |ri |
in the definition of
the brevity penalty (3) indeed resembles a weak re-
call score in which every guessed item counts as a
match. However, with recall, the per-sentence score
|ci |
|ri |
would never exceed unity, but with the brevity
penalty, it can. This means that if a system generates
a long translation for one sentence, it can generate
a short translation for another sentence without fac-
ing a penalty. This is a serious weakness in the B???
metric, as we demonstrate below using three scenar-
ios, encountered in actual practice.
3 Exploiting the B??? metric
3.1 The sign test
We are aware of two methods that have been pro-
posed for significance testing with B???: bootstrap
resampling (Koehn, 2004b; Zhang et al, 2004) and
the sign test (Collins et al, 2005). In bootstrap re-
sampling, we sample with replacement from the test
set to synthesize a large number of test sets, and
then we compare the performance of two systems on
those synthetic test sets to see whether one is better
95% (or 99%) of the time. But Collins et al (2005)
note that it is not clear whether the conditions re-
quired by bootstrap resampling are met in the case of
B???, and recommend the sign test instead. Suppose
we want to determine whether a set of outputs c from
a test system is better or worse than a set of baseline
outputs b. The sign test requires a function f (bi, ci)
that indicates whether ci is a better, worse, or same-
quality translation relative to bi. However, because
B??? is not defined on single sentences, Collins et
al. use an approximation: for each i, form a compos-
ite set of outputs b? = {b1, . . . , bi?1, ci, bi+1, . . . , bN},
and compare the B??? scores of b and b?.
The goodness of this approximation depends on
to what extent the comparison between b and b? is
dependent only on bi and ci, and independent of the
other sentences. However, B??? scores are highly
context-dependent: for example, if the sentences in
b are on average  words longer than the reference
sentences, then ci can be as short as (N ? 1) words
611
shorter than ri without incurring the brevity penalty.
Moreover, since the ci are substituted in one at a
time, we can do this for all of the ci. Hence, c could
have a disastrously low B??? score (because of the
brevity penalty) yet be found by the sign test to be
significantly better than the baseline.
We have encountered this situation in practice:
two versions of the same system with B??? scores of
29.6 (length ratio 1.02) and 29.3 (length ratio 0.97),
where the sign test finds the second system to be sig-
nificantly better than the first (and the first system
significantly better than the second). Clearly, in or-
der for a significance test to be sensible, it should not
contradict the observed scores, and should certainly
not contradict itself. In the rest of this paper, except
where indicated, all significance tests are performed
using bootstrap resampling.
3.2 Genre-specific training
For several years, much statistical MT research has
focused on translating newswire documents. One
likely reason is that the DARPA TIDES program
used newswire documents for evaluation for several
years. But more recent evaluations have included
other genres such as weblogs and conversation. The
conventional wisdom has been that if one uses a
single statistical translation system to translate text
from several different genres, it may perform poorly,
and it is better to use several systems optimized sep-
arately for each genre.
However, if our task is to translate documents
from multiple known genres, but they are evaluated
together, the B??? metric allows us to use that fact
to our advantage. To understand how, notice that
our system has an optimal number of words that it
should generate for the entire corpus: too few and it
will be penalized by B????s brevity penalty, and too
many increases the risk of additional non-matching
k-grams. But these words can be distributed among
the sentences (and genres) in any way we like. In-
stead of translating sentences from each genre with
the best genre-specific systems possible, we can
generate longer outputs for the genre we have more
confidence in, while generating shorter outputs for
the harder genre. This strategy will have mediocre
performance on each individual genre (according to
both intuition and B???), yet will receive a higher
B??? score on the combined test set than the com-
bined systems optimized for each genre.
In fact, knowing which sentence is in which genre
is not even always necessary. In one recent task,
we translated documents from two different genres,
without knowing the genre of any given sentence.
The easier genre, newswire, also tended to have
shorter reference sentences (relative to the source
sentences) than the harder genre, weblogs. For ex-
ample, in one dataset, the newswire reference sets
had between 1.3 and 1.37 English words per Ara-
bic word, but the weblog reference set had 1.52 En-
glish words per Arabic word. Thus, a system that
is uniformly verbose across both genres will appor-
tion more of its output to newswire than to weblogs,
serendipitously leading to a higher score. This phe-
nomenon has subsequently been observed by Och
(2008) as well.
We trained three Arabic-English syntax-based
statistical MT systems (Galley et al, 2004; Galley
et al, 2006) using max-B??? training (Och, 2003):
one on a newswire development set, one on a we-
blog development set, and one on a combined devel-
opment set containing documents from both genres.
We then translated a new mixed-genre test set in two
ways: (1) each document with its appropriate genre-
specific system, and (2) all documents with the sys-
tem trained on the combined (mixed-genre) devel-
opment set. In Table 3, we report the results of both
approaches on the entire test dataset as well as the
portion of the test dataset in each genre, for both the
genre-specific and mixed-genre trainings.
The genre-specific systems each outperform the
mixed system on their own genre as expected, but
when the same results are combined, the mixed sys-
tem?s output is a full B??? point higher than the com-
bination of the genre-specific systems. This is be-
cause the mixed system produces outputs that have
about 1.35 English words per Arabic word on av-
erage: longer than the shortest newswire references,
but shorter than the weblog references. The mixed
system does worse on each genre but better on the
combined test set, whereas, according to intuition,
a system that does worse on the two subsets should
also do worse on the combined test set.
3.3 Word deletion
A third way to take advantage of the B??? metric
is to permit an MT system to delete arbitrary words
612
in the input sentence. We can do this by introduc-
ing new phrases or rules into the system that match
words in the input sentence but generate no output;
to these rules we attach a feature whose weight is
tuned during max-B??? training. Such rules have
been in use for some time but were only recently
discussed by Li et al (2008).
When we add word-deletion rules to our MT sys-
tem, we find that the B??? increases significantly
(Table 6, line 2). Figure 1 shows some examples
of deletion in Chinese-English translation. The first
sentence has a proper name,?<[[/maigesaisai
?Magsaysay?, which has been mistokenized into four
tokens. The baseline system attempts to translate the
first two phonetic characters as ?wheat Georgia,?
whereas the other system simply deletes them. On
the other hand, the second sentence shows how word
deletion can sacrifice adequacy for the sake of flu-
ency, and the third sentence shows that sometimes
word deletion removes words that could have been
translated well (as seen in the baseline translation).
Does B??? reward word deletion fairly? We note
two reasons why word deletion might be desirable.
First, some function words should truly be deleted:
for example, the Chinese particle?/de and Chinese
measure words often have no counterpart in English
(Li et al, 2008). Second, even content word deletion
might be helpful if it allows a more fluent translation
to be assembled from the remnants. We observe that
in the above experiment, word deletion caused the
absolute number of k-gram matches, and not just k-
gram precision, to increase for all 1 ? k ? 4.
Human evaluation is needed to conclusively de-
termine whether B??? rewards deletion fairly. But to
control for these potentially positive effects of dele-
tion, we tested a sentence-deletion system, which
is the same as the word-deletion system but con-
strained to delete all of the words in a sentence or
none of them. This system (Table 6, line 3) deleted
8?10% of its input and yielded a B??? score with
no significant decrease (p ? 0.05) from the base-
line system?s. Given that our model treats sentences
independently, so that it cannot move information
from one sentence to another, we claim that dele-
tion of nearly 10% of the input is a grave translation
deficiency, yet B??? is insensitive to it.
What does this tell us about word deletion? While
acknowledging that some word deletions can im-
prove translation quality, we suggest in addition that
because word deletion provides a way for the system
to translate the test set selectively, a behavior which
we have shown that B??? is insensitive to, part of
the score increase due to word deletion is likely an
artifact of B???.
4 Other metrics
Are other metrics susceptible to the same problems
as the B??? metric? In this section we examine sev-
eral other popular metrics for these problems, pro-
pose two of our own, and discuss some desirable
characteristics for any new MT evaluation metric.
4.1 Previous metrics
We ran a suite of other metrics on the above problem
cases to see whether they were affected. In none of
these cases did we repeat minimum-error-rate train-
ing; all these systems were trained using max-B???.
The metrics we tested were:
? METEOR (Banerjee and Lavie, 2005), version
0.6, using the exact, Porter-stemmer, andWord-
Net synonmy stages, and the optimized param-
eters ? = 0.81, ? = 0.83, ? = 0.28 as reported
in (Lavie and Agarwal, 2007).
? GTM (Melamed et al, 2003), version 1.4, with
default settings, except e = 1.2, following the
WMT 2007 shared task (Callison-Burch et al,
2007).
? M??S?? (Chan and Ng, 2008), more specifi-
cally M??S??n, which skips the dependency re-
lations.
On the sign test (Table 2), all metrics found sig-
nificant differences consistent with the difference in
score between the two systems. The problem related
to genre-specific training does not seem to affect the
other metrics (see Table 4), but they still manifest
the unintuitive result that genre-specific training is
sometimes worse than mixed-genre training. Finally,
all metrics but GTM disfavored both word deletion
and sentence deletion (Table 7).
4.2 Strict brevity penalty
A very conservative way of modifying the B??? met-
ric to combat the effects described above is to im-
613
(a) source 9]Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1002?1010,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Word Sense Disambiguation Using OntoNotes:
An Empirical Study
Zhi Zhong and Hwee Tou Ng and Yee Seng Chan
Department of Computer Science
National University of Singapore
Law Link, Singapore 117590
{zhongzhi, nght, chanys}@comp.nus.edu.sg
Abstract
The accuracy of current word sense disam-
biguation (WSD) systems is affected by the
fine-grained sense inventory of WordNet as
well as a lack of training examples. Using the
WSD examples provided through OntoNotes,
we conduct the first large-scale WSD evalua-
tion involving hundreds of word types and tens
of thousands of sense-tagged examples, while
adopting a coarse-grained sense inventory. We
show that though WSD systems trained with a
large number of examples can obtain a high
level of accuracy, they nevertheless suffer a
substantial drop in accuracy when applied to
a different domain. To address this issue, we
propose combining a domain adaptation tech-
nique using feature augmentation with active
learning. Our results show that this approach
is effective in reducing the annotation effort
required to adapt a WSD system to a new do-
main. Finally, we propose that one can maxi-
mize the dual benefits of reducing the annota-
tion effort while ensuring an increase in WSD
accuracy, by only performing active learning
on the set of most frequently occurring word
types.
1 Introduction
In language, many words have multiple meanings.
The process of identifying the correct meaning, or
sense of a word in context, is known as word sense
disambiguation (WSD). WSD is one of the funda-
mental problems in natural language processing and
is important for applications such as machine trans-
lation (MT) (Chan et al, 2007a; Carpuat and Wu,
2007), information retrieval (IR), etc.
WSD is typically viewed as a classification prob-
lem where each ambiguous word is assigned a sense
label (from a pre-defined sense inventory) during the
disambiguation process. In current WSD research,
WordNet (Miller, 1990) is usually used as the sense
inventory. WordNet, however, adopts a very fine
level of sense granularity, thus restricting the accu-
racy of WSD systems. Also, current state-of-the-art
WSD systems are based on supervised learning and
face a general lack of training data.
To provide a standardized test-bed for evalua-
tion of WSD systems, a series of evaluation exer-
cises called SENSEVAL were held. In the English
all-words task of SENSEVAL-2 and SENSEVAL-
3 (Palmer et al, 2001; Snyder and Palmer, 2004),
no training data was provided and systems must tag
all the content words (noun, verb, adjective, and
adverb) in running English texts with their correct
WordNet senses. In SENSEVAL-2, the best per-
forming system (Mihalcea and Moldovan, 2001) in
the English all-words task achieved an accuracy of
69.0%, while in SENSEVAL-3, the best perform-
ing system (Decadt et al, 2004) achieved an accu-
racy of 65.2%. In SemEval-2007, which was the
most recent SENSEVAL evaluation, a similar En-
glish all-words task was held, where systems had to
provide the correct WordNet sense tag for all the
verbs and head words of their arguments in run-
ning English texts. For this task, the best perform-
ing system (Tratz et al, 2007) achieved an accuracy
of 59.1%. Results of these evaluations showed that
state-of-the-art English all-words WSD systems per-
formed with an accuracy of 60%?70%, using the
fine-grained sense inventory of WordNet.
The low level of performance by these state-of-
the-art WSD systems is a cause for concern, since
WSD is supposed to be an enabling technology
to be incorporated as a module into applications
1002
such as MT and IR. As mentioned earlier, one of
the major reasons for the low performance is that
these evaluation exercises adopted WordNet as the
reference sense inventory, which is often too fine-
grained. As an indication of this, inter-annotator
agreement (ITA) reported for manual sense-tagging
on these SENSEVAL English all-words datasets is
typically in the mid-70s. To address this issue, a
coarse-grained English all-words task (Navigli et al,
2007) was conducted during SemEval-2007. This
task used a coarse-grained version of WordNet and
reported an ITA of around 90%. We note that the
best performing system (Chan et al, 2007b) of this
task achieved a relatively high accuracy of 82.5%,
highlighting the importance of having an appropri-
ate level of sense granularity.
Another issue faced by current WSD systems is
the lack of training data. We note that the top per-
forming systems mentioned in the previous para-
graphs are all based on supervised learning. With
this approach, however, one would need to obtain
a corpus where each ambiguous word occurrence is
manually annotated with the correct sense, to serve
as training data. Since it is time consuming to per-
form sense annotation of word occurrences, only a
handful of sense-tagged corpora are publicly avail-
able. Among the existing sense-tagged corpora, the
SEMCOR corpus (Miller et al, 1994) is one of the
most widely used. In SEMCOR, content words have
been manually tagged with WordNet senses. Cur-
rent supervised WSD systems (which include all
the top-performing systems in the English all-words
task) usually rely on this relatively small manually
annotated corpus for training examples, and this has
inevitably affected the accuracy and scalability of
current WSD systems.
Related to the problem of a lack of training data
for WSD, there is also a lack of test data. Having
a large amount of test data for evaluation is impor-
tant to ensure the robustness and scalability of WSD
systems. Due to the expensive process of manual
sense-tagging, the SENSEVAL English all-words
task evaluations were conducted on relatively small
sets of evaluation data. For instance, the evaluation
data of SENSEVAL-2 and SENSEVAL-3 English
all-words task consists of 2,473 and 2,041 test exam-
ples respectively. In SemEval-2007, the fine-grained
English all-words task consists of only 465 test ex-
amples, while the SemEval-2007 coarse-grained En-
glish all-words task consists of 2,269 test examples.
Hence, it is necessary to address the issues of
sense granularity, and the lack of both training and
test data. To this end, a recent large-scale anno-
tation effort called the OntoNotes project (Hovy et
al., 2006) was started. Building on the annotations
from the Wall Street Journal (WSJ) portion of the
Penn Treebank (Marcus et al, 1993), the project
added several new layers of semantic annotations,
such as coreference information, word senses, etc.
In its first release (LDC2007T21) through the Lin-
guistic Data Consortium (LDC), the project man-
ually sense-tagged more than 40,000 examples be-
longing to hundreds of noun and verb types with an
ITA of 90%, based on a coarse-grained sense inven-
tory, where each word has an average of only 3.2
senses. Thus, besides providing WSD examples that
were sense-tagged with a high ITA, the project also
addressed the previously discussed issues of a lack
of training and test data.
In this paper, we use the sense-tagged data pro-
vided by the OntoNotes project to investigate the
accuracy achievable by current WSD systems when
adopting a coarse-grained sense inventory. Through
our experiments, we then highlight that domain
adaptation for WSD is an important issue as it sub-
stantially affects the performance of a state-of-the-
art WSD system which is trained on SEMCOR but
evaluated on sense-tagged examples in OntoNotes.
To address this issue, we then show that by com-
bining a domain adaptation technique using feature
augmentation with active learning, one only needs
to annotate a small amount of in-domain examples
to obtain a substantial improvement in the accuracy
of the WSD system which is previously trained on
out-of-domain examples.
The contributions of this paper are as follows.
To our knowledge, this is the first large-scale WSD
evaluation conducted that involves hundreds of word
types and tens of thousands of sense-tagged exam-
ples, and that is based on a coarse-grained sense in-
ventory. The present study also highlights the practi-
cal significance of domain adaptation in word sense
disambiguation in the context of a large-scale empir-
ical evaluation, and proposes an effective method to
address the domain adaptation problem.
In the next section, we give a brief description of
1003
our WSD system. In Section 3, we describe exper-
iments where we conduct both training and evalu-
ation using data from OntoNotes. In Section 4, we
investigate the WSD performance when we train our
system on examples that are gathered from a differ-
ent domain as compared to the OntoNotes evalua-
tion data. In Section 5, we perform domain adapta-
tion experiments using a recently introduced feature
augmentation technique. In Section 6, we investi-
gate the use of active learning to reduce the annota-
tion effort required to adapt our WSD system to the
domain of the OntoNotes data, before concluding in
Section 7.
2 The WSD System
For the experiments reported in this paper, we fol-
low the supervised learning approach of (Lee and
Ng, 2002), by training an individual classifier for
each word using the knowledge sources of local col-
locations, parts-of-speech (POS), and surrounding
words.
For local collocations, we use 11 features:
C?1,?1, C1,1, C?2,?2, C2,2, C?2,?1, C?1,1, C1,2,
C?3,?1, C?2,1, C?1,2, and C1,3, where Ci,j refers to
the ordered sequence of tokens in the local context
of an ambiguous word w. Offsets i and j denote the
starting and ending position (relative to w) of the se-
quence, where a negative (positive) offset refers to a
token to its left (right). For parts-of-speech, we use
7 features: P?3, P?2, P?1, P0, P1, P2, P3, where
P0 is the POS of w, and P?i (Pi) is the POS of the
ith token to the left (right) of w. For surrounding
words, we consider all unigrams (single words) in
the surrounding context of w. These words can be in
a different sentence from w. For our experiments re-
ported in this paper, we use support vector machines
(SVM) as our learning algorithm, which was shown
to achieve good WSD performance in (Lee and Ng,
2002; Chan et al, 2007b).
3 Training and Evaluating on OntoNotes
The annotated data of OntoNotes is drawn from the
Wall Street Journal (WSJ) portion of the Penn Tree-
bank corpus, divided into sections 00-24. These
WSJ documents have been widely used in various
NLP tasks such as syntactic parsing (Collins, 1999)
and semantic role labeling (SRL) (Carreras and Mar-
Section No. of No. of word tokens
word types Individual Cumulative
02 248 425 425
03 79 107 532
04 186 389 921
05 287 625 1546
06 224 446 1992
07 270 549 2541
08 177 301 2842
09 308 677 3519
10 648 3048 6567
11 724 4071 10638
12 740 4296 14934
13 749 4577 19511
14 710 3900 23411
15 748 4768 28179
16 306 576 28755
17 219 398 29153
18 266 566 29719
19 219 389 30108
20 288 536 30644
21 262 470 31114
23 685 3755 -
Table 1: Size of the sense-tagged data in the various WSJ
sections.
quez, 2005). In these tasks, the practice is to use
documents from WSJ sections 02-21 as training data
and WSJ section 23 as test data. Hence for our ex-
periments reported in this paper, we follow this con-
vention and use the annotated instances from WSJ
sections 02-21 as our training data, and instances in
WSJ section 23 as our test data.
As mentioned in Section 1, the OntoNotes data
provided WSD examples for a large number of
nouns and verbs, which are sense-tagged accord-
ing to a coarse-grained sense inventory. In Table 1,
we show the amount of sense-tagged data available
from OntoNotes, across the various WSJ sections.1
In the table, for each WSJ section, we list the num-
ber of word types, the number of sense-tagged ex-
amples, and the cumulative count on the number of
1We removed erroneous examples which were simply
tagged with ?XXX? as sense-tag, or tagged with senses that were
not found in the sense-inventory provided. Also, since we will
be comparing against training on SEMCOR later (which was
tagged using WordNet senses), we removed examples tagged
with OntoNotes senses which were not mapped to WordNet
senses. On the whole, about 7% of the original OntoNotes ex-
amples were removed as a result.
1004
sense-tagged examples. From the table, we see that
sections 02-21, which will be used as training data
in our experiments, contain a total of slightly over
31,000 sense-tagged examples.
Using examples from sections 02-21 as training
data, we trained our WSD system and evaluated on
the examples from section 23. In our experiments,
if a word type in section 23 has no training exam-
ples from sections 02-21, we randomly select an
OntoNotes sense as the answer. Using these ex-
perimental settings, our WSD system achieved an
accuracy of 89.1%. We note that this accuracy is
much higher than the 60%?70% accuracies achieved
by state-of-the-art English all-words WSD systems
which are trained using the fine-grained sense inven-
tory of WordNet. Hence, this highlights the impor-
tance of having an appropriate level of sense granu-
larity.
Besides training on the entire set of examples
from sections 02-21, we also investigated the per-
formance achievable from training on various sub-
sections of the data and show these results as ?ON?
in Figure 1. From the figure, we see that WSD accu-
racy increases as we add more training examples.
The fact that current state-of-the-art WSD sys-
tems are able to achieve a high level of perfor-
mance is important, as this means that WSD systems
will potentially be more usable for inclusion in end-
applications. For instance, the high level of perfor-
mance by syntactic parsers allows it to be used as an
enabling technology in various NLP tasks. Here, we
note that the 89.1% WSD accuracy we obtained is
comparable to state-of-the-art syntactic parsing ac-
curacies, such as the 91.0% performance by the sta-
tistical parser of Charniak and Johnson (2005).
4 Building WSD Systems with
Out-of-Domain Data
Although our WSD system had achieved a high
accuracy of 89.1%, this was achieved by train-
ing on a large amount (about 31,000) of manually
sense annotated examples from sections 02-21 of the
OntoNotes data. Further, all these training data and
test data are gathered from the same domain of WSJ.
In reality, however, since manual sense annotation is
time consuming, it is not feasible to collect such a
large amount of manually sense-tagged data for ev-
ery domain of interest. Hence, in this section, we in-
vestigate the performance of our WSD system when
it is trained on out-of-domain data.
In the English all-words task of the previous SEN-
SEVAL evaluations (SENSEVAL-2, SENSEVAL-
3, SemEval-2007), the best performing English
all-words task systems with the highest WSD ac-
curacy were trained on SEMCOR (Mihalcea and
Moldovan, 2001; Decadt et al, 2004; Chan et al,
2007b). Hence, we similarly trained our WSD sys-
tem on SEMCOR and evaluated on section 23 of the
OntoNotes corpus. For those word types in section
23 which do not have training examples from SEM-
COR, we randomly chose an OntoNotes sense as
the answer. In training on SEMCOR, we have also
ensured that there is a domain difference between
our training and test data. This is because while
the OntoNotes data was gathered from WSJ, which
contains mainly business related news, the SEMCOR
corpus is the sense-tagged portion of the Brown Cor-
pus (BC), which is a mixture of several genres such
as scientific texts, fictions, etc.
Evaluating on the section 23 test data, our WSD
system achieved only 76.2% accuracy. Compared to
the 89.1% accuracy achievable when we had trained
on examples from sections 02-21, this is a substan-
tially lower and disappointing drop of performance
and motivates the need for domain adaptation.
The need for domain adaptation is a general and
important issue for many NLP tasks (Daume III and
Marcu, 2006). For instance, SRL systems are usu-
ally trained and evaluated on data drawn from the
WSJ. In the CoNLL-2005 shared task on SRL (Car-
reras and Marquez, 2005), however, a task of train-
ing and evaluating systems on different domains was
included. For that task, systems that were trained on
the PropBank corpus (Palmer et al, 2005) (which
was gathered from the WSJ), suffered a 10% drop
in accuracy when evaluated on test data drawn from
BC, as compared to the performance achievable
when evaluated on data drawn from WSJ. More re-
cently, CoNLL-2007 included a shared task on de-
pendency parsing (Nivre et al, 2007). In this task,
systems that were trained on Penn Treebank (drawn
from WSJ), but evaluated on data drawn from a
different domain (such as chemical abstracts and
parent-child dialogues) showed a similar drop in per-
formance. For research involving training and eval-
1005
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
02 02-03
02-04
02-05
02-06
02-07
02-08
02-09
02-10
02-12
02-14
02-21
W
SD
 a
cc
ur
ac
y 
(%
)
Section number
WSD Accuracies on Section 23
59
.2
76
.8
77
.5
60
.5
77
.1
77
.5
64
.4
77
.1
77
.6
73
.3
78
.9 80
.3
76
.8
79
.3 80
.9
80
.2
79
.9 8
2.
1
80
.5
80
.5 8
2.
6
81
.6
80
.8 83
.1 8
5.
8
83
.3 85
.6 87
.5
86
.1 87
.6 88
.3
87
.2 88
.7
89
.1
87
.9 88
.9
ON
SC+ON
SC+ON Augment
Figure 1: WSD accuracies evaluated on section 23, using SEMCOR and different OntoNotes sections as training
data. ON: only OntoNotes as training data. SC+ON: SEMCOR and OntoNotes as training data, SC+ON Augment:
Combining SEMCOR and OntoNotes via the Augment domain adaptation technique.
uating WSD systems on data drawn from different
domains, several prior research efforts (Escudero et
al., 2000; Martinez and Agirre, 2000) observed a
similar drop in performance of about 10% when a
WSD system that was trained on the BC part of the
DSO corpus was evaluated on the WSJ part of the
corpus, and vice versa.
In the rest of this paper, we perform domain adap-
tation experiments for WSD, focusing on domain
adaptation methods that use in-domain annotated
data. In particular, we use a feature augmentation
technique recently introduced by Daume III (2007),
and active learning (Lewis and Gale, 1994) to per-
form domain adaptation of WSD systems.
5 Combining In-Domain and
Out-of-Domain Data for Training
In this section, we will first introduce the AUGMENT
technique of Daume III (2007), before showing the
performance of our WSD system with and without
using this technique.
5.1 The AUGMENT technique for Domain
Adaptation
The AUGMENT technique introduced by Daume III
(2007) is a simple yet very effective approach to per-
forming domain adaptation. This technique is appli-
cable when one has access to training data from the
source domain and a small amount of training data
from the target domain.
The technique essentially augments the feature
space of an instance. Assuming x is an instance and
its original feature vector is ?(x), the augmented
feature vector for instance x is
??(x) =
{
< ?(x),?(x),0 > if x ? Ds
< ?(x),0,?(x) > if x ? Dt
,
where 0 is a zero vector of size |?(x)|, Ds and
Dt are the sets of instances from the source and
target domains respectively. We see that the tech-
nique essentially treats the first part of the aug-
mented feature space as holding general features that
are not meant to be differentiated between different
1006
domains. Then, different parts of the augmented fea-
ture space are reserved for holding source domain
specific, or target domain specific features. Despite
its relative simplicity, this AUGMENT technique has
been shown to outperform other domain adaptation
techniques on various tasks such as named entity
recognition, part-of-speech tagging, etc.
5.2 Experimental Results
As mentioned in Section 4, training our WSD sys-
tem on SEMCOR examples gave a relatively low ac-
curacy of 76.2%, as compared to the 89.1% accuracy
obtained from training on the OntoNotes section 02-
21 examples. Assuming we have access to some in-
domain training data, then a simple method to poten-
tially obtain better accuracies is to train on both the
out-of-domain and in-domain examples. To investi-
gate this, we combined the SEMCOR examples with
various amounts of OntoNotes examples to train our
WSD system and show the resulting ?SC+ON? ac-
curacies obtained in Figure 1. We also performed
another set of experiments, where instead of simply
combining the SEMCOR and OntoNotes examples,
we applied the AUGMENT technique when combin-
ing these examples, treating SEMCOR examples as
out-of-domain (source domain) data and OntoNotes
examples as in-domain (target domain) data. We
similarly show the resulting accuracies as ?SC+ON
Augment? in Figure 1.
Comparing the ?SC+ON? and ?SC+ON Aug-
ment? accuracies in Figure 1, we see that the AUG-
MENT technique always helps to improve the ac-
curacy of our WSD system. Further, notice from
the first few sets of results in the figure that when
we have access to limited in-domain training exam-
ples from OntoNotes, incorporating additional out-
of-domain training data from SEMCOR (either using
the strategies ?SC+ON? or ?SC+ON Augment?)
achieves better accuracies than ?ON?. Significance
tests using one-tailed paired t-test reveal that these
accuracy improvements are statistically significant
at the level of significance 0.01 (all significance tests
in the rest of this paper use the same level of signif-
icance 0.01). These results validate the contribution
of the SemCor examples. This trend continues till
the result for sections 02-06.
The right half of Figure 1 shows the accuracy
trend of the various strategies, in the unlikely event
DS ? the set of SEMCOR training examples
DA? the set of OntoNotes sections 02-21 examples
DT ? empty
while DA 6= ?
pmin ??
??WSD system trained on DS and DT using AUGMENT
technique
for each d ? DA do
bs? word sense prediction for d using ?
p? confidence of prediction bs
if p < pmin then
pmin? p, dmin ? d
end
end
DA? DA ? {dmin}
provide correct sense s for dmin and add dmin to DT
end
Figure 2: The active learning algorithm.
that we have access to a large amount of in-domain
training examples. Although we observe that in
this scenario, ?ON? performs better than ?SC+ON?,
?SC+ON Augment? continues to perform better
than ?ON? (where the improvement is statistically
significant) till the result for sections 02-09. Beyond
that, as we add more OntoNotes examples, signif-
icance testing reveals that the ?SC+ON Augment?
and ?ON? strategies give comparable performance.
This means that the ?SC+ON Augment? strategy,
besides giving good performance when one has few
in-domain examples, does continue to perform well
even when one has a large number of in-domain ex-
amples.
6 Active Learning with AUGMENT
Technique
So far in this paper, we have seen that when we have
access to some in-domain examples, a good strategy
is to combine the out-of-domain and in-domain ex-
amples via the AUGMENT technique. This suggests
that when one wishes to apply a WSD system to a
new domain of interest, it is worth the effort to an-
notate a small number of examples gathered from
the new domain. However, instead of randomly se-
lecting in-domain examples to annotate, we could
use active learning (Lewis and Gale, 1994) to help
select in-domain examples to annotate. By doing
so, we could minimize the manual annotation effort
needed.
1007
WSD Accuracies on Section 23
76
78
80
82
84
86
88
90
SemCor 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34
Iteration Number
W
SD
 A
cc
ur
ac
y 
(%
)
50 100 150 200
300 400 500 all
Figure 3: Results of applying active learning with the AUGMENT technique on different number of word types. Each
curve represents the adaptation process of applying active learning on a certain number of most frequently occurring
word types.
In WSD, several prior research efforts have suc-
cessfully used active learning to reduce the annota-
tion effort required (Zhu and Hovy, 2007; Chan and
Ng, 2007; Chen et al, 2006; Fujii et al, 1998). With
the exception of (Chan and Ng, 2007) which tried
to adapt a WSD system trained on the BC part of
the DSO corpus to the WSJ part of the DSO corpus,
the other researchers simply applied active learning
to reduce the annotation effort required and did not
deal with the issue of adapting a WSD system to a
new domain. Also, these prior research efforts only
experimented with a few word types. In contrast, we
perform active learning experiments on the hundreds
of word types in the OntoNotes data, with the aim of
adapting our WSD system trained on SEMCOR to
the WSJ domain represented by the OntoNotes data.
For our active learning experiments, we use the
uncertainty sampling strategy (Lewis and Gale,
1994), as shown in Figure 2. For our experiments,
the SEMCOR examples will be our initial set of
training examples, while the OntoNotes examples
from sections 02-21 will be used as our pool of
adaptation examples, from which we will select ex-
amples to annotate via active learning. Also, since
we have found that the AUGMENT technique is use-
ful in increasing WSD accuracy, we will apply the
AUGMENT technique during each iteration of active
learning to combine the SEMCOR examples and the
selected adaptation examples.
As shown in Figure 2, we train an initial WSD
system using only the set DS of SEMCOR exam-
ples. We then apply our WSD system on the set DA
of OntoNotes adaptation examples. The example in
DA which is predicted with the lowest confidence
will be removed from DA and added to the set DT
of in-domain examples that have been selected via
active learning thus far. We then use the AUGMENT
technique to combine the set of examples in DS and
DT to train a new WSD system, which is then ap-
plied again on the set DA of remaining adaptation
examples, and this active learning process continues
until we have used up all the adaptation examples.
Note that because we are using OntoNotes sections
02-21 (which have already been sense-tagged be-
forehand) as our adaptation data, the annotation of
the selected example during each active learning it-
eration is simply simulated by referring to its tagged
sense.
6.1 Experimental Results
As mentioned earlier, we use the examples in
OntoNotes sections 02-21 as our adaptation exam-
1008
ples during active learning. Hence, we perform
active learning experiments on all the word types
that have sense-tagged examples from OntoNotes
sections 02-21, and show the evaluation results on
OntoNotes section 23 as the topmost ?all? curve in
Figure 3. Since our aim is to reduce the human an-
notation effort required in adapting a WSD system
to a new domain, we may not want to perform active
learning on all the word types in practice. Instead,
we can maximize the benefits by performing active
learning only on the more frequently occurring word
types. Hence, in Figure 3, we also show via var-
ious curves the results of applying active learning
only to various sets of word types, according to their
frequency, or number of sense-tagged examples in
OntoNotes sections 02-21. Note that the various ac-
curacy curves in Figure 3 are plotted in terms of
evaluation accuracies over all the test examples in
OntoNotes section 23, hence they are directly com-
parable to the results reported thus far in this pa-
per. Also, since the accuracies for the various curves
stabilize after 35 active learning iterations, we only
show the results of the first 35 iterations.
From Figure 3, we note that by performing ac-
tive learning on the set of 150 most frequently oc-
curring word types, we are able to achieve a WSD
accuracy of 82.6% after 10 active learning iterations.
Note that in Section 4, we mentioned that training
only on the out-of-domain SEMCOR examples gave
an accuracy of 76.2%. Hence, we have gained an
accuracy improvement of 6.4% (82.6% ? 76.2%)
by just using 1,500 in-domain OntoNotes examples.
Compared with the 12.9% (89.1% ? 76.2%) im-
provement in accuracy achieved by using all 31,114
OntoNotes sections 02-21 examples, we have ob-
tained half of this maximum increase in accuracy, by
requiring only about 5% (1,500/31,114) of the total
number of sense-tagged examples. Based on these
results, we propose that when there is a need to apply
a previously trained WSD system to a different do-
main, one can apply the AUGMENT technique with
active learning on the most frequent word types, to
greatly reduce the annotation effort required while
obtaining a substantial improvement in accuracy.
7 Conclusion
Using the WSD examples made available through
OntoNotes, which are sense-tagged according to a
coarse-grained sense inventory, we show that our
WSD system is able to achieve a high accuracy
of 89.1% when we train and evaluate on these ex-
amples. However, when we apply a WSD system
that is trained on SEMCOR, we suffer a substan-
tial drop in accuracy, highlighting the need to per-
form domain adaptation. We show that by com-
bining the AUGMENT domain adaptation technique
with active learning, we are able to effectively re-
duce the amount of annotation effort required for do-
main adaptation.
References
M. Carpuat and D. Wu. 2007. Improving Statistical Ma-
chine Translation Using Word Sense Disambiguation.
In Proc. of EMNLP-CoNLL07, pages 61?72.
X. Carreras and L. Marquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proc. of CoNLL-2005, pages 152?164.
Y. S. Chan and H. T. Ng. 2007. Domain Adaptation with
Active Learning for Word Sense Disambiguation. In
Proc. of ACL07, pages 49?56.
Y. S. Chan, H. T. Ng, and D. Chiang. 2007a. Word Sense
Disambiguation Improves Statistical Machine Transla-
tion. In Proc. of ACL07, pages 33?40.
Y. S. Chan, H. T. Ng, and Z. Zhong. 2007b. NUS-PT: Ex-
ploiting Parallel Texts for Word Sense Disambiguation
in the English All-Words Tasks. In Proc. of SemEval-
2007, pages 253?256.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
In Proc. of ACL05, pages 173?180.
J. Y. Chen, A. Schein, L. Ungar, and M. Palmer. 2006.
An Empirical Study of the Behavior of Active Learn-
ing for Word Sense Disambiguation. In Proc. of
HLT/NAACL06, pages 120?127.
M. Collins. 1999. Head-Driven Statistical Model for
Natural Language Parsing. PhD dissertation, Univer-
sity of Pennsylvania.
H. Daume III and D. Marcu. 2006. Domain Adaptation
for Statistical Classifiers. Journal of Artificial Intelli-
gence Research, 26:101?126.
H. Daume III. 2007. Frustratingly Easy Domain Adap-
tation. In Proc. of ACL07, pages 256?263.
B. Decadt, V. Hoste, and W. Daelemans. 2004. GAMBL,
Genetic Algorithm Optimization of Memory-Based
WSD. In Proc. of SENSEVAL-3, pages 108?112.
1009
G. Escudero, L. Marquez, and G. Riagu. 2000. An
Empirical Study of the Domain Dependence of Super-
vised Word Sense Disambiguation Systems. In Proc.
of EMNLP/VLC00, pages 172?180.
A. Fujii, K. Inui, T. Tokunaga, and H. Tanaka. 1998. Se-
lective Sampling for Example-based Word Sense Dis-
ambiguation. Computational Linguistics, 24(4).
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% solution.
In Proc. of HLT-NAACL06, pages 57?60.
Y. K. Lee and H. T. Ng. 2002. An Empirical Evaluation
of Knowledge Sources and Learning Algorithms for
Word Sense Disambiguation. In Proc. of EMNLP02,
pages 41?48.
D. D. Lewis and W. A. Gale. 1994. A Sequential Al-
gorithm for Training Text Classifiers. In Proc. of SI-
GIR94.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
D. Martinez and E. Agirre. 2000. One Sense per
Collocation and Genre/Topic Variations. In Proc. of
EMNLP/VLC00, pages 207?215.
R. Mihalcea and D. Moldovan. 2001. Pattern Learning
and Active Feature Selection for Word Sense Disam-
biguation. In Proc. of SENSEVAL-2, pages 127?130.
G. A. Miller, M. Chodorow, S. Landes, C. Leacock, and
R. G. Thomas. 1994. Using a Semantic Concordance
for Sense Identification. In Proc. of ARPA Human
Language Technology Workshop, pages 240?243.
G. A. Miller. 1990. WordNet: An On-line Lexi-
cal Database. International Journal of Lexicography,
3(4):235?312.
R. Navigli, K. C. Litkowski, and O. Hargraves. 2007.
SemEval-2007 Task 07: Coarse-Grained English All-
Words Task. In Proc. of SemEval-2007, pages 30?35.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing. In Proc. of
EMNLP-CoNLL07, pages 915?932.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and H. T.
Dang. 2001. English Tasks: All-Words and Verb Lex-
ical Sample. In Proc. of SENSEVAL-2, pages 21?24.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71?105.
B. Snyder and M. Palmer. 2004. The English All-Words
Task. In Proc. of SENSEVAL-3, pages 41?43.
S. Tratz, A. Sanfilippo, M. Gregory, A. Chappell,
C. Posse, and P. Whitney. 2007. PNNL: A Supervised
Maximum Entropy Approach to Word Sense Disam-
biguation. In Proc. of SemEval-2007, pages 264?267.
J. B. Zhu and E. Hovy. 2007. Active Learning for Word
Sense Disambiguation with Methods for Addressing
the Class Imbalance Problem. In Proc. of EMNLP-
CoNLL07, pages 783?790.
1010
Exploiting Parallel Texts for Word Sense Disambiguation:  
An Empirical Study 
Hwee Tou Ng 
Bin Wang 
Yee Seng Chan 
Department of Computer Science 
National University of Singapore 
3 Science Drive 2, Singapore 117543 
{nght, wangbin, chanys}@comp.nus.edu.sg 
 
Abstract 
A central problem of word sense disam-
biguation (WSD) is the lack of manually 
sense-tagged data required for supervised 
learning. In this paper, we evaluate an ap-
proach to automatically acquire sense-
tagged training data from English-Chinese 
parallel corpora, which are then used for 
disambiguating the nouns in the 
SENSEVAL-2 English lexical sample 
task. Our investigation reveals that this 
method of acquiring sense-tagged data is 
promising. On a subset of the most diffi-
cult SENSEVAL-2 nouns, the accuracy 
difference between the two approaches is 
only 14.0%, and the difference could nar-
row further to 6.5% if we disregard the 
advantage that manually sense-tagged 
data have in their sense coverage. Our 
analysis also highlights the importance of 
the issue of domain dependence in evalu-
ating WSD programs. 
1 Introduction 
The task of word sense disambiguation (WSD) is 
to determine the correct meaning, or sense of a 
word in context. It is a fundamental problem in 
natural language processing (NLP), and the ability 
to disambiguate word sense accurately is important 
for applications like machine translation, informa-
tion retrieval, etc. 
Corpus-based, supervised machine learning 
methods have been used to tackle the WSD task, 
just like the other NLP tasks. Among the various 
approaches to WSD, the supervised learning ap-
proach is the most successful to date. In this ap-
proach, we first collect a corpus in which each 
occurrence of an ambiguous word w has been 
manually annotated with the correct sense, accord-
ing to some existing sense inventory in a diction-
ary. This annotated corpus then serves as the 
training material for a learning algorithm. After 
training, a model is automatically learned and it is 
used to assign the correct sense to any previously 
unseen occurrence of w in a new context. 
While the supervised learning approach has 
been successful, it has the drawback of requiring 
manually sense-tagged data. This problem is par-
ticular severe for WSD, since sense-tagged data 
must be collected separately for each word in a 
language. 
One source to look for potential training data 
for WSD is parallel texts, as proposed by Resnik 
and Yarowsky (1997). Given a word-aligned paral-
lel corpus, the different translations in a target lan-
guage serve as the ?sense-tags? of an ambiguous 
word in the source language. For example, some 
possible Chinese translations of the English noun 
channel are listed in Table 1. To illustrate, if the 
sense of an occurrence of the noun channel is ?a 
path over which electrical signals can pass?, then 
this occurrence can be translated as ???? in Chi-
nese. 
WordNet 
1.7 sense id 
Lumped 
sense id 
Chinese translations WordNet 1.7 English sense descriptions 
1 1 ?? A path over which electrical signals can pass 
2 2 ?? ?? ??? A passage for water  
3 3 ? A long narrow furrow 
4 4 ?? A relatively narrow body of water  
5 5 ?? A means of communication or access 
6 6 ?? A bodily passage or tube 
7 1 ?? A television station and its programs 
 
Table 1: WordNet 1.7 English sense descriptions, the actual lumped senses, and Chinese translations 
of the noun channel used in our implemented approach 
 
 
Parallel corpora Size of English texts (in 
million words (MB)) 
Size of Chinese texts (in 
million characters (MB)) 
Hong Kong News 5.9 (39.4) 10.7 (22.8) 
Hong Kong Laws 7.0 (39.8) 14.0 (22.6) 
Hong Kong Hansards 11.9 (54.2) 18.0 (32.4) 
English translation of Chinese Treebank 0.1 (0.7) 0.2 (0.4) 
Xinhua News 3.6 (22.9) 7.0 (17.0) 
Sinorama 3.2 (19.8) 5.3 (10.2) 
Total 31.7 (176.8) 55.2 (105.4) 
 
Table 2: Size of English-Chinese parallel corpora 
 
This approach of getting sense-tagged corpus 
also addresses two related issues in WSD. Firstly, 
what constitutes a valid sense distinction carries 
much subjectivity. Different dictionaries define a 
different sense inventory. By tying sense distinc-
tion to the different translations in a target lan-
guage, this introduces a ?data-oriented? view to 
sense distinction and serves to add an element of 
objectivity to sense definition. Secondly, WSD has 
been criticized as addressing an isolated problem 
without being grounded to any real application. By 
defining sense distinction in terms of different tar-
get translations, the outcome of word sense disam-
biguation of a source language word is the 
selection of a target word, which directly corre-
sponds to word selection in machine translation.  
While this use of parallel corpus for word sense 
disambiguation seems appealing, several practical 
issues arise in its implementation: 
(i) What is the size of the parallel corpus 
needed in order for this approach to be able to dis-
ambiguate a source language word accurately? 
(ii) While we can obtain large parallel corpora 
in the long run, to have them manually word-
aligned would be too time-consuming and would 
defeat the original purpose of getting a sense-
tagged corpus without manual annotation. How-
ever, are current word alignment algorithms accu-
rate enough for our purpose? 
(iii) Ultimately, using a state-of-the-art super-
vised WSD program, what is its disambiguation 
accuracy when it is trained on a ?sense-tagged? 
corpus obtained via parallel text alignment, com-
pared with training on a manually sense-tagged 
corpus? 
Much research remains to be done to investi-
gate all of the above issues. The lack of large-scale 
parallel corpora no doubt has impeded progress in 
this direction, although attempts have been made to 
mine parallel corpora from the Web (Resnik, 
1999). 
However, large-scale, good-quality parallel 
corpora have recently become available. For ex-
ample, six English-Chinese parallel corpora are 
now available from Linguistic Data Consortium. 
These parallel corpora are listed in Table 2, with a 
combined size of 280 MB. In this paper, we ad-
dress the above issues and report our findings, ex-
ploiting the English-Chinese parallel corpora in 
Table 2 for word sense disambiguation. We evalu-
ated our approach on all the nouns in the English 
lexical sample task of SENSEVAL-2 (Edmonds 
and Cotton, 2001; Kilgarriff 2001), which used the 
WordNet 1.7 sense inventory (Miller, 1990). While 
our approach has only been tested on English and 
Chinese, it is completely general and applicable to 
other language pairs. 
2 
2.1 
2.2 
2.3 
2.4 
Approach 
Our approach of exploiting parallel texts for word 
sense disambiguation consists of four steps: (1) 
parallel text alignment (2) manual selection of tar-
get translations (3) training of WSD classifier (4) 
WSD of words in new contexts. 
Parallel Text Alignment 
In this step, parallel texts are first sentence-aligned 
and then word-aligned. Various alignment algo-
rithms (Melamed 2001; Och and Ney 2000) have 
been developed in the past. For the six bilingual 
corpora that we used, they already come with sen-
tences pre-aligned, either manually when the cor-
pora were prepared or automatically by sentence-
alignment programs. After sentence alignment, the 
English texts are tokenized so that a punctuation 
symbol is separated from its preceding word. For 
the Chinese texts, we performed word segmenta-
tion, so that Chinese characters are segmented into 
words. The resulting parallel texts are then input to 
the GIZA++ software (Och and Ney 2000) for 
word alignment. 
In the output of GIZA++, each English word 
token is aligned to some Chinese word token. The 
alignment result contains much noise, especially 
for words with low frequency counts. 
Manual Selection of Target Translations 
In this step, we will decide on the sense classes of 
an English word w that are relevant to translating w 
into Chinese. We will illustrate with the noun 
channel, which is one of the nouns evaluated in the 
English lexical sample task of SENSEVAL-2. We 
rely on two sources to decide on the sense classes 
of w: 
(i) The sense definitions in WordNet 1.7, which 
lists seven senses for the noun channel. Two 
senses are lumped together if they are translated in 
the same way in Chinese. For example, sense 1 and 
7 of channel are both translated as ???? in Chi-
nese, so these two senses are lumped together. 
(ii) From the word alignment output of 
GIZA++, we select those occurrences of the noun 
channel which have been aligned to one of the 
Chinese translations chosen (as listed in Table 1). 
These occurrences of the noun channel in the Eng-
lish side of the parallel texts are considered to have 
been disambiguated and ?sense-tagged? by the ap-
propriate Chinese translations. Each such occur-
rence of channel together with the 3-sentence 
context in English surrounding channel then forms 
a training example for a supervised WSD program 
in the next step. 
The average time taken to perform manual se-
lection of target translations for one SENSEVAL-2 
English noun is less than 15 minutes. This is a rela-
tively short time, especially when compared to the 
effort that we would otherwise need to spend to 
perform manual sense-tagging of training exam-
ples. This step could also be potentially automated 
if we have a suitable bilingual translation lexicon. 
Training of WSD Classifier 
Much research has been done on the best super-
vised learning approach for WSD (Florian and 
Yarowsky, 2002; Lee and Ng, 2002; Mihalcea and 
Moldovan, 2001; Yarowsky et al, 2001). In this 
paper, we used the WSD program reported in (Lee 
and Ng, 2002). In particular, our method made use 
of the knowledge sources of part-of-speech, sur-
rounding words, and local collocations. We used 
na?ve Bayes as the learning algorithm. Our previ-
ous research demonstrated that such an approach 
leads to a state-of-the-art WSD program with good 
performance. 
WSD of Words in New Contexts 
Given an occurrence of w in a new context, we 
then used the na?ve Bayes classifier to determine 
the most probable sense of w. 
noun No. of 
senses 
before 
lumping 
No. of 
senses 
after 
lumping 
M1 P1 P1-
Baseline 
M2 M3 P2 P2- 
Baseline 
child 4 1 - - - - - - - 
detention 2 1 - - - - - - - 
feeling 6 1 - - - - - - - 
holiday 2 1 - - - - - - - 
lady 3 1 - - - - - - - 
material 5 1 - - - - - - - 
yew 2 1 - - - - - - - 
bar 13 13 0.619 0.529 0.500 - - - - 
bum 4 3 0.850 0.850 0.850 - - - - 
chair 4 4 0.887 0.895 0.887 - - - - 
day 10 6 0.921 0.907 0.906 - - - - 
dyke 2 2 0.893 0.893 0.893 - - - - 
fatigue 4 3 0.875 0.875 0.875 - - - - 
hearth 3 2 0.906 0.844 0.844 - - - - 
mouth 8 4 0.877 0.811 0.846 - - - - 
nation 4 3 0.806 0.806 0.806 - - - - 
nature 5 3 0.733 0.756 0.522 - - - - 
post 8 7 0.517 0.431 0.431 - - - - 
restraint 6 3 0.932 0.864 0.864 - - - - 
sense 5 4 0.698 0.684 0.453 - - - - 
stress 5 3 0.921 0.921 0.921 - - - - 
art 4 3 0.722 0.494 0.424 0.678 0.562 0.504 0.424 
authority 7 5 0.879 0.753 0.538 0.802 0.800 0.709 0.538 
channel 7 6 0.735 0.487 0.441 0.715 0.715 0.526 0.441 
church 3 3 0.758 0.582 0.573 0.691 0.629 0.609 0.572 
circuit 6 5 0.792 0.457 0.434 0.683 0.438 0.446 0.438 
facility 5 3 0.875 0.764 0.750 0.874 0.893 0.754 0.750 
grip 7 7 0.700 0.540 0.560 0.655 0.574 0.546 0.556 
spade 3 3 0.806 0.677 0.677 0.790 0.677 0.677 0.677 
 
Table 3: List of 29 SENSEVAL-2 nouns, their number of senses, and various accuracy figures 
3 An Empirical Study 
We evaluated our approach to word sense disam-
biguation on all the 29 nouns in the English lexical 
sample task of SENSEVAL-2 (Edmonds and Cot-
ton, 2001; Kilgarriff 2001). The list of 29 nouns is 
given in Table 3. The second column of Table 3 
lists the number of senses of each noun as given in 
the WordNet 1.7 sense inventory (Miller, 1990). 
We first lump together two senses s1 and s2 of a 
noun if s1 and s2 are translated into the same Chi-
nese word. The number of senses of each noun 
after sense lumping is given in column 3 of Table 
3. For the 7 nouns that are lumped into one sense 
(i.e., they are all translated into one Chinese word), 
we do not perform WSD on these words. The aver-
age number of senses before and after sense lump-
ing is 5.07 and 3.52 respectively. 
After sense lumping, we trained a WSD classi-
fier for each noun w, by using the lumped senses in 
the manually sense-tagged training data for w pro-
vided by the SENSEVAL-2 organizers. We then 
tested the WSD classifier on the official 
SENSEVAL-2 test data (but with lumped senses) 
for w. The test accuracy (based on fine-grained 
scoring of SENSEVAL-2) of each noun obtained is 
listed in the column labeled M1 in Table 3. 
We then used our approach of parallel text 
alignment described in the last section to obtain the 
training examples from the English side of the par-
allel texts. Due to the memory size limitation of 
our machine, we were not able to align all six par-
allel corpora of 280MB in one alignment run of 
GIZA++. For two of the corpora, Hong Kong Han-
sards and Xinhua News, we gathered all English 
sentences containing the 29 SENSEVAL-2 noun 
occurrences (and their sentence-aligned Chinese 
sentence counterparts). This subset, together with 
the complete corpora of Hong Kong News, Hong 
Kong Laws, English translation of Chinese Tree-
bank, and Sinorama, is then given to GIZA++ to 
perform one word alignment run. It took about 40 
hours on our 2.4 GHz machine with 2 GB memory 
to perform this alignment. 
After word alignment, each 3-sentence context 
in English containing an occurrence of the noun w 
that is aligned to a selected Chinese translation 
then forms a training example. For each 
SENSEVAL-2 noun w, we then collected training 
examples from the English side of the parallel texts 
using the same number of training examples for 
each sense of w that are present in the manually 
sense-tagged SENSEVAL-2 official training cor-
pus (lumped-sense version). If there are insuffi-
cient training examples for some sense of w from 
the parallel texts, then we just used as many paral-
lel text training examples as we could find for that 
sense. We chose the same number of training ex-
amples for each sense as the official training data 
so that we can do a fair comparison between the 
accuracy of the parallel text alignment approach 
versus the manual sense-tagging approach. 
After training a WSD classifier for w with such 
parallel text examples, we then evaluated the WSD 
classifier on the same official SENSEVAL-2 test 
set (with lumped senses). The test accuracy of each 
noun obtained by training on such parallel text 
training examples (averaged over 10 trials) is listed 
in the column labeled P1 in Table 3. 
The baseline accuracy for each noun is also 
listed in the column labeled ?P1-Baseline? in Table 
3. The baseline accuracy corresponds to always 
picking the most frequently occurring sense in the 
training data. 
Ideally, we would hope M1 and P1 to be close 
in value, since this would imply that WSD based 
on training examples collected from the parallel 
text alignment approach performs as well as manu-
ally sense-tagged training examples. Comparing 
the M1 and P1 figures, we observed that there is a 
set of nouns for which they are relatively close. 
These nouns are: bar, bum, chair, day, dyke, fa-
tigue, hearth, mouth, nation, nature, post, re-
straint, sense, stress. This set of nouns is relatively 
easy to disambiguate, since using the most-
frequently-occurring-sense baseline would have 
done well for most of these nouns. 
The parallel text alignment approach works 
well for nature and sense, among these nouns. For 
nature, the parallel text alignment approach gives 
better accuracy, and for sense the accuracy differ-
ence is only 0.014 (while there is a relatively large 
difference of 0.231 between P1 and P1-Baseline of 
sense). This demonstrates that the parallel text 
alignment approach to acquiring training examples 
can yield good results. 
For the remaining nouns (art, authority, chan-
nel, church, circuit, facility, grip, spade), the 
accuracy difference between M1 and P1 is at least 
0.10. Henceforth, we shall refer to this set of 8 
nouns as ?difficult? nouns. We will give an analy-
sis of the reason for the accuracy difference be-
tween M1 and P1 in the next section. 
4 
4.1 
Analysis 
Sense-Tag Accuracy of Parallel Text 
Training Examples 
To see why there is still a difference between the 
accuracy of the two approaches, we first examined 
the quality of the training examples obtained 
through parallel text alignment. If the automati-
cally acquired training examples are noisy, then 
this could account for the lower P1 score. 
The word alignment output of GIZA++ con-
tains much noise in general (especially for the low 
frequency words). However, note that in our ap-
proach, we only select the English word occur-
rences that align to our manually selected Chinese 
translations. Hence, while the complete set of word 
alignment output contains much noise, the subset 
of word occurrences chosen may still have high 
quality sense tags. 
Our manual inspection reveals that the annota-
tion errors introduced by parallel text alignment 
can be attributed to the following sources: 
(i) Wrong sentence alignment: Due to errone-
ous sentence segmentation or sentence alignment, 
the correct Chinese word that an English word w 
should align to is not present in its Chinese sen-
tence counterpart. In this case, word alignment will 
align the wrong Chinese word to w. 
(ii) Presence of multiple Chinese translation 
candidates: Sometimes, multiple and distinct Chi-
nese translations appear in the aligned Chinese 
sentence. For example, for an English occurrence 
channel, both ???? (sense 1 translation) and ??
?? (sense 5 translation) happen to appear in the 
aligned Chinese sentence. In this case, word 
alignment may erroneously align the wrong Chi-
nese translation to channel. 
(iii) Truly ambiguous word: Sometimes, a word 
is truly ambiguous in a particular context, and dif-
ferent translators may translate it differently. For 
example, in the phrase ?the church meeting?, 
church could be the physical building sense (?
? ), or the institution sense (?? ). In manual 
sense tagging done in SENSEVAL-2, it is possible 
to assign two sense tags to church in this case, but 
in the parallel text setting, a particular translator 
will translate it in one of the two ways (?? or ?
?), and hence the sense tag found by parallel text 
alignment is only one of the two sense tags. 
By manually examining a subset of about 1,000 
examples, we estimate that the sense-tag error rate 
of training examples (tagged with lumped senses) 
obtained by our parallel text alignment approach is 
less than 1%, which compares favorably with the 
quality of manually sense tagged corpus prepared 
in SENSEVAL-2 (Kilgarriff, 2001). 
4.2 Domain Dependence and Insufficient 
Sense Coverage 
While it is encouraging to find out that the par-
allel text sense tags are of high quality, we are still 
left with the task of explaining the difference be-
tween M1 and P1 for the set of difficult nouns. Our 
further investigation reveals that the accuracy dif-
ference between M1 and P1 is due to the following 
two reasons: domain dependence and insufficient 
sense coverage. 
Domain Dependence The accuracy figure of 
M1 for each noun is obtained by training a WSD 
classifier on the manually sense-tagged training 
data (with lumped senses) provided by 
SENSEVAL-2 organizers, and testing on the cor-
responding official test data (also with lumped 
senses), both of which come from similar domains. 
In contrast, the P1 score of each noun is obtained 
by training the WSD classifier on a mixture of six 
parallel corpora, and tested on the official 
SENSEVAL-2 test set, and hence the training and 
test data come from dissimilar domains in this 
case. 
Moreover, from the ?docsrc? field (which re-
cords the document id that each training or test 
example originates) of the official SENSEVAL-2 
training and test examples, we realized that there 
are many cases when some of the examples from a 
document are used as training examples, while the 
rest of the examples from the same document are 
used as test examples. In general, such a practice 
results in higher test accuracy, since the test exam-
ples would look a lot closer to the training exam-
ples in this case. 
To address this issue, we took the official 
SENSEVAL-2 training and test examples of each 
noun w and combined them together. We then ran-
domly split the data into a new training and a new 
test set such that no training and test examples 
come from the same document. The number of 
training examples in each sense in such a new 
training set is the same as that in the official train-
ing data set of w. 
A WSD classifier was then trained on this new 
training set, and tested on this new test set. We 
conducted 10 random trials, each time splitting into 
a different training and test set but ensuring that 
the number of training examples in each sense (and 
thus the sense distribution) follows the official 
training set of w. We report the average accuracy 
of the 10 trials. The accuracy figures for the set of 
difficult nouns thus obtained are listed in the col-
umn labeled M2 in Table 3. 
We observed that M2 is always lower in value 
compared to M1 for all difficult nouns. This sug-
gests that the effect of training and test examples 
coming from the same document has inflated the 
accuracy figures of SENSEVAL-2 nouns. 
Next, we randomly selected 10 sets of training 
examples from the parallel corpora, such that the 
number of training examples in each sense fol-
lowed the official training set of w. (When there 
were insufficient training examples for a sense, we 
just used as many as we could find from the paral-
lel corpora.) In each trial, after training a WSD 
classifier on the selected parallel text examples, we 
tested the classifier on the same test set (from 
SENSEVAL-2 provided data) used in that trial that 
generated the M2 score. The accuracy figures thus 
obtained for all the difficult nouns are listed in the 
column labeled P2 in Table 3. 
Insufficient Sense Coverage We observed that 
there are situations when we have insufficient 
training examples in the parallel corpora for some 
of the senses of some nouns. For instance, no oc-
currences of sense 5 of the noun circuit (racing 
circuit, a racetrack for automobile races) could be 
found in the parallel corpora. To ensure a fairer 
comparison, for each of the 10-trial manually 
sense-tagged training data that gave rise to the ac-
curacy figure M2 of a noun w, we extracted a new 
subset of 10-trial (manually sense-tagged) training 
data by ensuring adherence to the number of train-
ing examples found for each sense of w in the cor-
responding parallel text training set that gave rise 
to the accuracy figure P2 for w. The accuracy fig-
ures thus obtained for the difficult nouns are listed 
in the column labeled M3 in Table 3. M3 thus gave 
the accuracy of training on manually sense-tagged 
data but restricted to the number of training exam-
ples found in each sense from parallel corpora. 
4.3 
5 
6 
Discussion 
The difference between the accuracy figures of 
M2 and P2 averaged over the set of all difficult 
nouns is 0.140. This is smaller than the difference 
of 0.189 between the accuracy figures of M1 and 
P1 averaged over the set of all difficult nouns. This 
confirms our hypothesis that eliminating the possi-
bility that training and test examples come from 
the same document would result in a fairer com-
parison. 
In addition, the difference between the accuracy 
figures of M3 and P2 averaged over the set of all 
difficult nouns is 0.065. That is, eliminating the 
advantage that manually sense-tagged data have in 
their sense coverage would reduce the performance 
gap between the two approaches from 0.140 to 
0.065. Notice that this reduction is particularly sig-
nificant for the noun circuit. For this noun, the par-
allel corpora do not have enough training examples 
for sense 4 and sense 5 of circuit, and these two 
senses constitute approximately 23% in each of the 
10-trial test set. 
We believe that the remaining difference of 
0.065 between the two approaches could be attrib-
uted to the fact that the training and test examples 
of the manually sense-tagged corpus, while not 
coming from the same document, are however still 
drawn from the same general domain. To illustrate, 
we consider the noun channel where the difference 
between M3 and P2 is the largest. For channel, it 
turns out that a substantial number of the training 
and test examples contain the collocation ?Channel 
tunnel? or ?Channel Tunnel?. On average, about 
9.8 training examples and 6.2 test examples con-
tain this collocation. This alone would have ac-
counted for 0.088 of the accuracy difference 
between the two approaches. 
That domain dependence is an important issue 
affecting the performance of WSD programs has 
been pointed out by (Escudero et al, 2000). Our 
work confirms the importance of domain depend-
ence in WSD. 
As to the problem of insufficient sense cover-
age, with the steady increase and availability of 
parallel corpora, we believe that getting sufficient 
sense coverage from larger parallel corpora should 
not be a problem in the near future for most of the 
commonly occurring words in a language. 
Related Work 
Brown et al (1991) is the first to have explored 
statistical methods in word sense disambiguation in 
the context of machine translation. However, they 
only looked at assigning at most two senses to a 
word, and their method only asked a single ques-
tion about a single word of context. Li and Li 
(2002) investigated a bilingual bootstrapping tech-
nique, which differs from the method we imple-
mented here. Their method also does not require a 
parallel corpus. 
The research of (Chugur et al, 2002) dealt with 
sense distinctions across multiple languages. Ide et 
al. (2002) investigated word sense distinctions us-
ing parallel corpora. Resnik and Yarowsky (2000) 
considered word sense disambiguation using mul-
tiple languages. Our present work can be similarly 
extended beyond bilingual corpora to multilingual 
corpora. 
The research most similar to ours is the work of 
Diab and Resnik (2002). However, they used ma-
chine translated parallel corpus instead of human 
translated parallel corpus. In addition, they used an 
unsupervised method of noun group disambigua-
tion, and evaluated on the English all-words task. 
Conclusion 
In this paper, we reported an empirical study to 
evaluate an approach of automatically acquiring 
sense-tagged training data from English-Chinese 
parallel corpora, which were then used for disam-
biguating the nouns in the SENSEVAL-2 English 
lexical sample task. Our investigation reveals that 
this method of acquiring sense-tagged data is pro-
mising and provides an alternative to manual sense 
tagging. 
 
Acknowledgements 
 
This research is partially supported by a research 
grant R252-000-125-112 from National University 
of Singapore Academic Research Fund. 
References 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1991. Word-
sense disambiguation using statistical methods. In 
Proceedings of the 29th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 264-
270. 
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002. 
Polysemy and sense proximity in the Senseval-2 test 
suite. In Proceedings of the ACL SIGLEX Workshop 
on Word Sense Disambiguation: Recent Successes 
and Future Directions, pages 32-39. 
Mona Diab and Philip Resnik. 2002. An unsupervised 
method for word sense tagging using parallel cor-
pora. In Proceedings of the 40th Annual Meeting of 
the Association for Computational Linguistics, pages 
255-262. 
Philip Edmonds and Scott Cotton. 2001. SENSEVAL-2: 
Overview. In Proceedings of the Second Interna-
tional Workshop on Evaluating Word Sense 
Disambiguation Systems (SENSEVAL-2), pages 1-5. 
Gerard Escudero, Lluis Marquez, and German Rigau. 
2000. An empirical study of the domain dependence 
of supervised word sense disambiguation systems. In 
Proceedings of the Joint SIGDAT Conference on 
Empirical Methods in Natural Language Processing 
and Very Large Corpora, pages 172-180. 
Radu Florian and David Yarowsky. 2002. Modeling 
consensus: Classifier combination for word sense 
disambiguation. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language 
Processing, pages 25-32. 
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. Sense 
discrimination with parallel corpora. In Proceedings 
of the ACL SIGLEX Workshop on Word Sense Dis-
ambiguation: Recent Successes and Future Direc-
tions, pages 54-60. 
Adam Kilgarriff. 2001. English lexical sample task de-
scription. In Proceedings of the Second International 
Workshop on Evaluating Word Sense Disambigua-
tion Systems (SENSEVAL-2), pages 17-20. 
Yoong Keok Lee and Hwee Tou Ng. 2002. An empiri-
cal evaluation of knowledge sources and learning al-
gorithms for word sense disambiguation. In 
Proceedings of the 2002 Conference on Empirical 
Methods in Natural Language Processing, pages 41-
48. 
Cong Li and Hang Li. 2002. Word translation disam-
biguation using bilingual bootstrapping. In Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics, pages 343-351. 
I. Dan Melamed. 2001. Empirical Methods for Exploit-
ing Parallel Texts. MIT Press, Cambridge. 
Rada F. Mihalcea and Dan I. Moldovan. 2001. Pattern 
learning and active feature selection for word sense 
disambiguation. In Proceedings of the Second Inter-
national Workshop on Evaluating Word Sense Dis-
ambiguation Systems (SENSEVAL-2), pages 127-130. 
George A. Miller. (Ed.) 1990. WordNet: An on-line 
lexical database. International Journal of Lexicogra-
phy, 3(4):235-312. 
Franz Josef Och and Hermann Ney. 2000. Improved 
statistical alignment models. In Proceedings of the 
38th Annual Meeting of the Association for Computa-
tional Linguistics, pages 440-447. 
Philip Resnik. 1999. Mining the Web for bilingual text. 
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 527-
534. 
Philip Resnik and David Yarowsky. 1997. A perspec-
tive on word sense disambiguation methods and their 
evaluation. In Proceedings of the ACL SIGLEX 
Workshop on Tagging Text with Lexical Semantics: 
Why, What, and How?, pages 79-86. 
Philip Resnik and David Yarowsky. 2000. Distinguish-
ing systems and distinguishing senses: New evalua-
tion methods for word sense disambiguation. Natural 
Language Engineering, 5(2):113-133. 
David Yarowsky, Silviu Cucerzan, Radu Florian, 
Charles Schafer, and Richard Wicentowski. 2001. 
The Johns Hopkins SENSEVAL2 system descrip-
tions. In Proceedings of the Second International 
Workshop on Evaluating Word Sense Disambigua-
tion Systems (SENSEVAL-2), pages 163-166. 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 89?96,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Estimating Class Priors in Domain Adaptation
for Word Sense Disambiguation
Yee Seng Chan and Hwee Tou Ng
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
 
chanys,nght  @comp.nus.edu.sg
Abstract
Instances of a word drawn from different
domains may have different sense priors
(the proportions of the different senses of
a word). This in turn affects the accuracy
of word sense disambiguation (WSD) sys-
tems trained and applied on different do-
mains. This paper presents a method to
estimate the sense priors of words drawn
from a new domain, and highlights the im-
portance of using well calibrated probabil-
ities when performing these estimations.
By using well calibrated probabilities, we
are able to estimate the sense priors effec-
tively to achieve significant improvements
in WSD accuracy.
1 Introduction
Many words have multiple meanings, and the pro-
cess of identifying the correct meaning, or sense
of a word in context, is known as word sense
disambiguation (WSD). Among the various ap-
proaches to WSD, corpus-based supervised ma-
chine learning methods have been the most suc-
cessful to date. With this approach, one would
need to obtain a corpus in which each ambiguous
word has been manually annotated with the correct
sense, to serve as training data.
However, supervised WSD systems faced an
important issue of domain dependence when using
such a corpus-based approach. To investigate this,
Escudero et al (2000) conducted experiments
using the DSO corpus, which contains sentences
drawn from two different corpora, namely Brown
Corpus (BC) and Wall Street Journal (WSJ). They
found that training a WSD system on one part (BC
or WSJ) of the DSO corpus and applying it to the
other part can result in an accuracy drop of 12%
to 19%. One reason for this is the difference in
sense priors (i.e., the proportions of the different
senses of a word) between BC and WSJ. For in-
stance, the noun interest has these 6 senses in the
DSO corpus: sense 1, 2, 3, 4, 5, and 8. In the BC
part of the DSO corpus, these senses occur with
the proportions: 34%, 9%, 16%, 14%, 12%, and
15%. However, in the WSJ part of the DSO cor-
pus, the proportions are different: 13%, 4%, 3%,
56%, 22%, and 2%. When the authors assumed
they knew the sense priors of each word in BC and
WSJ, and adjusted these two datasets such that the
proportions of the different senses of each word
were the same between BC and WSJ, accuracy im-
proved by 9%. In another work, Agirre and Mar-
tinez (2004) trained a WSD system on data which
was automatically gathered from the Internet. The
authors reported a 14% improvement in accuracy
if they have an accurate estimate of the sense pri-
ors in the evaluation data and sampled their train-
ing data according to these sense priors. The work
of these researchers showed that when the domain
of the training data differs from the domain of the
data on which the system is applied, there will be
a decrease in WSD accuracy.
To build WSD systems that are portable across
different domains, estimation of the sense priors
(i.e., determining the proportions of the differ-
ent senses of a word) occurring in a text corpus
drawn from a domain is important. McCarthy et
al. (2004) provided a partial solution by describing
a method to predict the predominant sense, or the
most frequent sense, of a word in a corpus. Using
the noun interest as an example, their method will
try to predict that sense 1 is the predominant sense
in the BC part of the DSO corpus, while sense 4
is the predominant sense in the WSJ part of the
89
corpus.
In our recent work (Chan and Ng, 2005b), we
directly addressed the problem by applying ma-
chine learning methods to automatically estimate
the sense priors in the target domain. For instance,
given the noun interest and the WSJ part of the
DSO corpus, we attempt to estimate the propor-
tion of each sense of interest occurring in WSJ and
showed that these estimates help to improve WSD
accuracy. In our work, we used naive Bayes as
the training algorithm to provide posterior proba-
bilities, or class membership estimates, for the in-
stances in the target domain. These probabilities
were then used by the machine learning methods
to estimate the sense priors of each word in the
target domain.
However, it is known that the posterior proba-
bilities assigned by naive Bayes are not reliable, or
not well calibrated (Domingos and Pazzani, 1996).
These probabilities are typically too extreme, of-
ten being very near 0 or 1. Since these probabil-
ities are used in estimating the sense priors, it is
important that they are well calibrated.
In this paper, we explore the estimation of sense
priors by first calibrating the probabilities from
naive Bayes. We also propose using probabilities
from another algorithm (logistic regression, which
already gives well calibrated probabilities) to esti-
mate the sense priors. We show that by using well
calibrated probabilities, we can estimate the sense
priors more effectively. Using these estimates im-
proves WSD accuracy and we achieve results that
are significantly better than using our earlier ap-
proach described in (Chan and Ng, 2005b).
In the following section, we describe the algo-
rithm to estimate the sense priors. Then, we de-
scribe the notion of being well calibrated and dis-
cuss why using well calibrated probabilities helps
in estimating the sense priors. Next, we describe
an algorithm to calibrate the probability estimates
from naive Bayes. Then, we discuss the corpora
and the set of words we use for our experiments
before presenting our experimental results. Next,
we propose using the well calibrated probabilities
of logistic regression to estimate the sense priors,
and perform significance tests to compare our var-
ious results before concluding.
2 Estimation of Priors
To estimate the sense priors, or a priori proba-
bilities of the different senses in a new dataset,
we used a confusion matrix algorithm (Vucetic
and Obradovic, 2001) and an EM based algorithm
(Saerens et al, 2002) in (Chan and Ng, 2005b).
Our results in (Chan and Ng, 2005b) indicate that
the EM based algorithm is effective in estimat-
ing the sense priors and achieves greater improve-
ments in WSD accuracy compared to the confu-
sion matrix algorithm. Hence, to estimate the
sense priors in our current work, we use the EM
based algorithm, which we describe in this sec-
tion.
2.1 EM Based Algorithm
Most of this section is based on (Saerens et al,
2002). Assume we have a set of labeled data D  
with n classes and a set of N independent instances
  	  
 
from a new data set. The likelihood
of these N instances can be defined as:

    
 



 

 






 


Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 33?40,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Word Sense Disambiguation Improves Statistical Machine Translation
Yee Seng Chan and Hwee Tou Ng
Department of Computer Science
National University of Singapore
3 Science Drive 2
Singapore 117543
{chanys, nght}@comp.nus.edu.sg
David Chiang
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292, USA
chiang@isi.edu
Abstract
Recent research presents conflicting evi-
dence on whether word sense disambigua-
tion (WSD) systems can help to improve the
performance of statistical machine transla-
tion (MT) systems. In this paper, we suc-
cessfully integrate a state-of-the-art WSD
system into a state-of-the-art hierarchical
phrase-based MT system, Hiero. We show
for the first time that integrating a WSD sys-
tem improves the performance of a state-of-
the-art statistical MT system on an actual
translation task. Furthermore, the improve-
ment is statistically significant.
1 Introduction
Many words have multiple meanings, depending on
the context in which they are used. Word sense dis-
ambiguation (WSD) is the task of determining the
correct meaning or sense of a word in context. WSD
is regarded as an important research problem and is
assumed to be helpful for applications such as ma-
chine translation (MT) and information retrieval.
In translation, different senses of a word w in a
source language may have different translations in a
target language, depending on the particular mean-
ing of w in context. Hence, the assumption is that
in resolving sense ambiguity, a WSD system will be
able to help an MT system to determine the correct
translation for an ambiguous word. To determine the
correct sense of a word, WSD systems typically use
a wide array of features that are not limited to the lo-
cal context of w, and some of these features may not
be used by state-of-the-art statistical MT systems.
To perform translation, state-of-the-art MT sys-
tems use a statistical phrase-based approach (Marcu
and Wong, 2002; Koehn et al, 2003; Och and
Ney, 2004) by treating phrases as the basic units
of translation. In this approach, a phrase can be
any sequence of consecutive words and is not nec-
essarily linguistically meaningful. Capitalizing on
the strength of the phrase-based approach, Chiang
(2005) introduced a hierarchical phrase-based sta-
tistical MT system, Hiero, which achieves signifi-
cantly better translation performance than Pharaoh
(Koehn, 2004a), which is a state-of-the-art phrase-
based statistical MT system.
Recently, some researchers investigated whether
performing WSD will help to improve the perfor-
mance of an MT system. Carpuat and Wu (2005)
integrated the translation predictions from a Chinese
WSD system (Carpuat et al, 2004) into a Chinese-
English word-based statistical MT system using the
ISI ReWrite decoder (Germann, 2003). Though they
acknowledged that directly using English transla-
tions as word senses would be ideal, they instead
predicted the HowNet sense of a word and then used
the English gloss of the HowNet sense as the WSD
model?s predicted translation. They did not incor-
porate their WSD model or its predictions into their
translation model; rather, they used the WSD pre-
dictions either to constrain the options available to
their decoder, or to postedit the output of their de-
coder. They reported the negative result that WSD
decreased the performance of MT based on their ex-
periments.
In another work (Vickrey et al, 2005), the WSD
problem was recast as a word translation task. The
33
translation choices for a word w were defined as the
set of words or phrases aligned to w, as gathered
from a word-aligned parallel corpus. The authors
showed that they were able to improve their model?s
accuracy on two simplified translation tasks: word
translation and blank-filling.
Recently, Cabezas and Resnik (2005) experi-
mented with incorporating WSD translations into
Pharaoh, a state-of-the-art phrase-based MT sys-
tem (Koehn et al, 2003). Their WSD system pro-
vided additional translations to the phrase table of
Pharaoh, which fired a new model feature, so that
the decoder could weigh the additional alternative
translations against its own. However, they could
not automatically tune the weight of this feature in
the same way as the others. They obtained a rela-
tively small improvement, and no statistical signifi-
cance test was reported to determine if the improve-
ment was statistically significant.
Note that the experiments in (Carpuat and Wu,
2005) did not use a state-of-the-art MT system,
while the experiments in (Vickrey et al, 2005) were
not done using a full-fledged MT system and the
evaluation was not on how well each source sentence
was translated as a whole. The relatively small im-
provement reported by Cabezas and Resnik (2005)
without a statistical significance test appears to be
inconclusive. Considering the conflicting results re-
ported by prior work, it is not clear whether a WSD
system can help to improve the performance of a
state-of-the-art statistical MT system.
In this paper, we successfully integrate a state-
of-the-art WSD system into the state-of-the-art hi-
erarchical phrase-based MT system, Hiero (Chiang,
2005). The integration is accomplished by introduc-
ing two additional features into the MT model which
operate on the existing rules of the grammar, with-
out introducing competing rules. These features are
treated, both in feature-weight tuning and in decod-
ing, on the same footing as the rest of the model,
allowing it to weigh the WSD model predictions
against other pieces of evidence so as to optimize
translation accuracy (as measured by BLEU). The
contribution of our work lies in showing for the first
time that integrating a WSD system significantly im-
proves the performance of a state-of-the-art statisti-
cal MT system on an actual translation task.
In the next section, we describe our WSD system.
Then, in Section 3, we describe the Hiero MT sys-
tem and introduce the two new features used to inte-
grate the WSD system into Hiero. In Section 4, we
describe the training data used by the WSD system.
In Section 5, we describe how the WSD translations
provided are used by the decoder of the MT system.
In Section 6 and 7, we present and analyze our ex-
perimental results, before concluding in Section 8.
2 Word Sense Disambiguation
Prior research has shown that using Support Vector
Machines (SVM) as the learning algorithm for WSD
achieves good results (Lee and Ng, 2002). For our
experiments, we use the SVM implementation of
(Chang and Lin, 2001) as it is able to work on multi-
class problems to output the classification probabil-
ity for each class.
Our implemented WSD classifier uses the knowl-
edge sources of local collocations, parts-of-speech
(POS), and surrounding words, following the suc-
cessful approach of (Lee and Ng, 2002). For local
collocations, we use 3 features, w?1w+1, w?1, and
w+1, where w?1 (w+1) is the token immediately to
the left (right) of the current ambiguous word oc-
currence w. For parts-of-speech, we use 3 features,
P?1, P0, and P+1, where P0 is the POS of w, and
P?1 (P+1) is the POS of w?1 (w+1). For surround-
ing words, we consider all unigrams (single words)
in the surrounding context of w. These unigrams can
be in a different sentence from w. We perform fea-
ture selection on surrounding words by including a
unigram only if it occurs 3 or more times in some
sense of w in the training data.
To measure the accuracy of our WSD classifier,
we evaluate it on the test data of SENSEVAL-3 Chi-
nese lexical-sample task. We obtain accuracy that
compares favorably to the best participating system
in the task (Carpuat et al, 2004).
3 Hiero
Hiero (Chiang, 2005) is a hierarchical phrase-based
model for statistical machine translation, based on
weighted synchronous context-free grammar (CFG)
(Lewis and Stearns, 1968). A synchronous CFG
consists of rewrite rules such as the following:
X ? ??, ?? (1)
34
where X is a non-terminal symbol, ? (?) is a string
of terminal and non-terminal symbols in the source
(target) language, and there is a one-to-one corre-
spondence between the non-terminals in ? and ? in-
dicated by co-indexation. Hence, ? and ? always
have the same number of non-terminal symbols. For
instance, we could have the following grammar rule:
X ? ???t X 1 , go to X 1 every month to? (2)
where boxed indices represent the correspondences
between non-terminal symbols.
Hiero extracts the synchronous CFG rules auto-
matically from a word-aligned parallel corpus. To
translate a source sentence, the goal is to find its
most probable derivation using the extracted gram-
mar rules. Hiero uses a general log-linear model
(Och and Ney, 2002) where the weight of a deriva-
tion D for a particular source sentence and its trans-
lation is
w(D) =
?
i
?i(D)?i (3)
where ?i is a feature function and ?i is the weight for
feature ?i. To ensure efficient decoding, the ?i are
subject to certain locality restrictions. Essentially,
they should be defined as products of functions de-
fined on isolated synchronous CGF rules; however,
it is possible to extend the domain of locality of
the features somewhat. A n-gram language model
adds a dependence on (n?1) neighboring target-side
words (Wu, 1996; Chiang, 2007), making decoding
much more difficult but still polynomial; in this pa-
per, we add features that depend on the neighboring
source-side words, which does not affect decoding
complexity at all because the source string is fixed.
In principle we could add features that depend on
arbitrary source-side context.
3.1 New Features in Hiero for WSD
To incorporate WSD into Hiero, we use the trans-
lations proposed by the WSD system to help Hiero
obtain a better or more probable derivation during
the translation of each source sentence. To achieve
this, when a grammar rule R is considered during
decoding, and we recognize that some of the ter-
minal symbols (words) in ? are also chosen by the
WSD system as translations for some terminal sym-
bols (words) in ?, we compute the following fea-
tures:
? Pwsd(t | s) gives the contextual probability of
the WSD classifier choosing t as a translation
for s, where t (s) is some substring of terminal
symbols in ? (?). Because this probability only
applies to some rules, and we don?t want to pe-
nalize those rules, we must add another feature,
? Ptywsd = exp(?|t|), where t is the translation
chosen by the WSD system. This feature, with
a negative weight, rewards rules that use trans-
lations suggested by the WSD module.
Note that we can take the negative logarithm of
the rule/derivation weights and think of them as
costs rather than probabilities.
4 Gathering Training Examples for WSD
Our experiments were for Chinese to English trans-
lation. Hence, in the context of our work, a syn-
chronous CFG grammar rule X ? ??, ?? gathered
by Hiero consists of a Chinese portion ? and a cor-
responding English portion ?, where each portion is
a sequence of words and non-terminal symbols.
Our WSD classifier suggests a list of English
phrases (where each phrase consists of one or more
English words) with associated contextual probabil-
ities as possible translations for each particular Chi-
nese phrase. In general, the Chinese phrase may
consist of k Chinese words, where k = 1, 2, 3, . . ..
However, we limit k to 1 or 2 for experiments re-
ported in this paper. Future work can explore en-
larging k.
Whenever Hiero is about to extract a grammar
rule where its Chinese portion is a phrase of one or
two Chinese words with no non-terminal symbols,
we note the location (sentence and token offset) in
the Chinese half of the parallel corpus from which
the Chinese portion of the rule is extracted. The ac-
tual sentence in the corpus containing the Chinese
phrase, and the one sentence before and the one sen-
tence after that actual sentence, will serve as the con-
text for one training example for the Chinese phrase,
with the corresponding English phrase of the gram-
mar rule as its translation. Hence, unlike traditional
WSD where the sense classes are tied to a specific
sense inventory, our ?senses? here consist of the En-
glish phrases extracted as translations for each Chi-
nese phrase. Since the extracted training data may
35
be noisy, for each Chinese phrase, we remove En-
glish translations that occur only once. Furthermore,
we only attempt WSD classification for those Chi-
nese phrases with at least 10 training examples.
Using the WSD classifier described in Section 2,
we classified the words in each Chinese source sen-
tence to be translated. We first performed WSD on
all single Chinese words which are either noun, verb,
or adjective. Next, we classified the Chinese phrases
consisting of 2 consecutive Chinese words by simply
treating the phrase as a single unit. When perform-
ing classification, we give as output the set of En-
glish translations with associated context-dependent
probabilities, which are the probabilities of a Chi-
nese word (phrase) translating into each English
phrase, depending on the context of the Chinese
word (phrase). After WSD, the ith word ci in every
Chinese sentence may have up to 3 sets of associ-
ated translations provided by the WSD system: a set
of translations for ci as a single word, a second set
of translations for ci?1ci considered as a single unit,
and a third set of translations for cici+1 considered
as a single unit.
5 Incorporating WSD during Decoding
The following tasks are done for each rule that is
considered during decoding:
? identify Chinese words to suggest translations
for
? match suggested translations against the En-
glish side of the rule
? compute features for the rule
The WSD system is able to predict translations
only for a subset of Chinese words or phrases.
Hence, we must first identify which parts of the
Chinese side of the rule have suggested translations
available. Here, we consider substrings of length up
to two, and we give priority to longer substrings.
Next, we want to know, for each Chinese sub-
string considered, whether the WSD system sup-
ports the Chinese-English translation represented by
the rule. If the rule is finally chosen as part of the
best derivation for translating the Chinese sentence,
then all the words in the English side of the rule will
appear in the translated English sentence. Hence,
we need to match the translations suggested by the
WSD system against the English side of the rule. It
is for these matching rules that the WSD features
will apply.
The translations proposed by the WSD system
may be more than one word long. In order for a
proposed translation to match the rule, we require
two conditions. First, the proposed translation must
be a substring of the English side of the rule. For
example, the proposed translation ?every to? would
not match the chunk ?every month to?. Second, the
match must contain at least one aligned Chinese-
English word pair, but we do not make any other
requirements about the alignment of the other Chi-
nese or English words.1 If there are multiple possi-
ble matches, we choose the longest proposed trans-
lation; in the case of a tie, we choose the proposed
translation with the highest score according to the
WSD model.
Define a chunk of a rule to be a maximal sub-
string of terminal symbols on the English side of the
rule. For example, in Rule (2), the chunks would be
?go to? and ?every month to?. Whenever we find
a matching WSD translation, we mark the whole
chunk on the English side as consumed.
Finally, we compute the feature values for the
rule. The feature Pwsd(t | s) is the sum of the costs
(according to the WSD model) of all the matched
translations, and the feature Ptywsd is the sum of
the lengths of all the matched translations.
Figure 1 shows the pseudocode for the rule scor-
ing algorithm in more detail, particularly with re-
gards to resolving conflicts between overlapping
matches. To illustrate the algorithm given in Figure
1, consider Rule (2). Hereafter, we will use symbols
to represent the Chinese and English words in the
rule: c1, c2, and c3 will represent the Chinese words
???, ???, and ?t? respectively. Similarly, e1, e2,
e3, e4, and e5 will represent the English words go,
to, every, month, and to respectively. Hence, Rule
(2) has two chunks: e1e2 and e3e4e5. When the rule
is extracted from the parallel corpus, it has these
alignments between the words of its Chinese and
English portion: {c1?e3,c2?e4,c3?e1,c3?e2,c3?e5},
which means that c1 is aligned to e3, c2 is aligned to
1In order to check this requirement, we extended Hiero to
make word alignment information available to the decoder.
36
Input: rule R considered during decoding with its own associated costR
Lc = list of symbols in Chinese portion of R
WSDcost = 0
i = 1
while i ? len(Lc):
ci = ith symbol in Lc
if ci is a Chinese word (i.e., not a non-terminal symbol):
seenChunk = ? // seenChunk is a global variable and is passed by reference to matchWSD
if (ci is not the last symbol in Lc) and (ci+1 is a terminal symbol): then ci+1=(i+1)th symbol in Lc, else ci+1 = NULL
if (ci+1!=NULL) and (ci, ci+1) as a single unit has WSD translations:
WSDc = set of WSD translations for (ci, ci+1) as a single unit with context-dependent probabilities
WSDcost = WSDcost + matchWSD(ci, WSDc, seenChunk)
WSDcost = WSDcost + matchWSD(ci+1, WSDc, seenChunk)
i = i + 1
else:
WSDc = set of WSD translations for ci with context-dependent probabilities
WSDcost = WSDcost + matchWSD(ci, WSDc, seenChunk)
i = i + 1
costR = costR + WSDcost
matchWSD(c, WSDc, seenChunk):
// seenChunk is the set of chunks of R already examined for possible matching WSD translations
cost = 0
ChunkSet = set of chunks in R aligned to c
for chunkj in ChunkSet:
if chunkj not in seenChunk:
seenChunk = seenChunk ? { chunkj }
Echunkj = set of English words in chunkj aligned to c
Candidatewsd = ?
for wsdk in WSDc:
if (wsdk is sub-sequence of chunkj) and (wsdk contains at least one word in Echunkj )
Candidatewsd = Candidatewsd ? { wsdk }
wsdbest = best matching translation in Candidatewsd against chunkj
cost = cost + costByWSDfeatures(wsdbest) // costByWSDfeatures sums up the cost of the two WSD features
return cost
Figure 1: WSD translations affecting the cost of a rule R considered during decoding.
e4, and c3 is aligned to e1, e2, and e5. Although all
words are aligned here, in general for a rule, some of
its Chinese or English words may not be associated
with any alignments.
In our experiment, c1c2 as a phrase has a list of
translations proposed by the WSD system, includ-
ing the English phrase ?every month?. matchWSD
will first be invoked for c1, which is aligned to only
one chunk e3e4e5 via its alignment with e3. Since
?every month? is a sub-sequence of the chunk and
also contains the word e3 (?every?), it is noted as
a candidate translation. Later, it is determined that
the most number of words any candidate translation
has is two words. Since among all the 2-word candi-
date translations, the translation ?every month? has
the highest translation probability as assigned by the
WSD classifier, it is chosen as the best matching
translation for the chunk. matchWSD is then invoked
for c2, which is aligned to only one chunk e3e4e5.
However, since this chunk has already been exam-
ined by c1 with which it is considered as a phrase, no
further matching is done for c2. Next, matchWSD is
invoked for c3, which is aligned to both chunks of R.
The English phrases ?go to? and ?to? are among the
list of translations proposed by the WSD system for
c3, and they are eventually chosen as the best match-
ing translations for the chunks e1e2 and e3e4e5, re-
spectively.
6 Experiments
As mentioned, our experiments were on Chinese to
English translation. Similar to (Chiang, 2005), we
trained the Hiero system on the FBIS corpus, used
the NIST MT 2002 evaluation test set as our devel-
opment set to tune the feature weights, and the NIST
MT 2003 evaluation test set as our test data. Using
37
System BLEU-4 Individual n-gram precisions
1 2 3 4
Hiero 29.73 74.73 40.14 21.83 11.93
Hiero+WSD 30.30 74.82 40.40 22.45 12.42
Table 1: BLEU scores
Features
System Plm(e) P (?|?) P (?|?) Pw(?|?) Pw(?|?) Ptyphr Glue Ptyword Pwsd(t|s) Ptywsd
Hiero 0.2337 0.0882 0.1666 0.0393 0.1357 0.0665 ?0.0582 ?0.4806 - -
Hiero+WSD 0.1937 0.0770 0.1124 0.0487 0.0380 0.0988 ?0.0305 ?0.1747 0.1051 ?0.1611
Table 2: Weights for each feature obtained by MERT training. The first eight features are those used by
Hiero in (Chiang, 2005).
the English portion of the FBIS corpus and the Xin-
hua portion of the Gigaword corpus, we trained a tri-
gram language model using the SRI Language Mod-
elling Toolkit (Stolcke, 2002). Following (Chiang,
2005), we used the version 11a NIST BLEU script
with its default settings to calculate the BLEU scores
(Papineni et al, 2002) based on case-insensitive n-
gram matching, where n is up to 4.
First, we performed word alignment on the FBIS
parallel corpus using GIZA++ (Och and Ney, 2000)
in both directions. The word alignments of both
directions are then combined into a single set of
alignments using the ?diag-and? method of Koehn
et al (2003). Based on these alignments, syn-
chronous CFG rules are then extracted from the cor-
pus. While Hiero is extracting grammar rules, we
gathered WSD training data by following the proce-
dure described in section 4.
6.1 Hiero Results
Using the MT 2002 test set, we ran the minimum-
error rate training (MERT) (Och, 2003) with the
decoder to tune the weights for each feature. The
weights obtained are shown in the row Hiero of
Table 2. Using these weights, we run Hiero?s de-
coder to perform the actual translation of the MT
2003 test sentences and obtained a BLEU score of
29.73, as shown in the row Hiero of Table 1. This is
higher than the score of 28.77 reported in (Chiang,
2005), perhaps due to differences in word segmenta-
tion, etc. Note that comparing with the MT systems
used in (Carpuat and Wu, 2005) and (Cabezas and
Resnik, 2005), the Hiero system we are using rep-
resents a much stronger baseline MT system upon
which the WSD system must improve.
6.2 Hiero+WSD Results
We then added the WSD features of Section 3.1 into
Hiero and reran the experiment. The weights ob-
tained by MERT are shown in the row Hiero+WSD
of Table 2. We note that a negative weight is learnt
for Ptywsd. This means that in general, the model
prefers grammar rules having chunks that matches
WSD translations. This matches our intuition. Us-
ing the weights obtained, we translated the test sen-
tences and obtained a BLEU score of 30.30, as
shown in the row Hiero+WSD of Table 1. The im-
provement of 0.57 is statistically significant at p <
0.05 using the sign-test as described by Collins et al
(2005), with 374 (+1), 318 (?1) and 227 (0). Us-
ing the bootstrap-sampling test described in (Koehn,
2004b), the improvement is statistically significant
at p < 0.05. Though the improvement is modest, it is
statistically significant and this positive result is im-
portant in view of the negative findings in (Carpuat
and Wu, 2005) that WSD does not help MT. Fur-
thermore, note that Hiero+WSD has higher n-gram
precisions than Hiero.
7 Analysis
Ideally, the WSD system should be suggesting high-
quality translations which are frequently part of the
reference sentences. To determine this, we note the
set of grammar rules used in the best derivation for
translating each test sentence. From the rules of each
test sentence, we tabulated the set of translations
proposed by the WSD system and check whether
they are found in the associated reference sentences.
On the entire set of NIST MT 2003 evaluation test
sentences, an average of 10.36 translations proposed
38
No. of All test sentences +1 from Collins sign-test
words in No. of % match No. of % match
WSD translations WSD translations used reference WSD translations used reference
1 7087 77.31 3078 77.68
2 1930 66.11 861 64.92
3 371 43.13 171 48.54
4 124 26.61 52 28.85
Table 3: Number of WSD translations used and proportion that matches against respective reference sen-
tences. WSD translations longer than 4 words are very sparse (less than 10 occurrences) and thus they are
not shown.
by the WSD system were used for each sentence.
When limited to the set of 374 sentences which
were judged by the Collins sign-test to have better
translations from Hiero+WSD than from Hiero, a
higher number (11.14) of proposed translations were
used on average. Further, for the entire set of test
sentences, 73.01% of the proposed translations are
found in the reference sentences. This increased to
a proportion of 73.22% when limited to the set of
374 sentences. These figures show that having more,
and higher-quality proposed translations contributed
to the set of 374 sentences being better translations
than their respective original translations from Hi-
ero. Table 3 gives a detailed breakdown of these
figures according to the number of words in each
proposed translation. For instance, over all the test
sentences, the WSD module gave 7087 translations
of single-word length, and 77.31% of these trans-
lations match their respective reference sentences.
We note that although the proportion of matching 2-
word translations is slightly lower for the set of 374
sentences, the proportion increases for translations
having more words.
After the experiments in Section 6 were com-
pleted, we visually inspected the translation output
of Hiero and Hiero+WSD to categorize the ways in
which integrating WSD contributes to better trans-
lations. The first way in which WSD helps is when
it enables the integrated Hiero+WSD system to out-
put extra appropriate English words. For example,
the translations for the Chinese sentence ?. . .? ?
??q??R?Rz?????
??tZ? are as follows.
? Hiero: . . . or other bad behavior ?, will be more
aid and other concessions.
? Hiero+WSD: . . . or other bad behavior ?, will
be unable to obtain more aid and other conces-
sions.
Here, the Chinese words ??Rz? are not trans-
lated by Hiero at all. By providing the correct trans-
lation of ?unable to obtain? for ?? Rz?, the
translation output of Hiero+WSD is more complete.
A second way in which WSD helps is by correct-
ing a previously incorrect translation. For example,
for the Chinese sentence ?. . .? ? \ ) ? |
??. . . ?, the WSD system helps to correct Hiero?s
original translation by providing the correct transla-
tion of ?all ethnic groups? for the Chinese phrase
???:
? Hiero: . . . , and people of all nationalities
across the country, . . .
? Hiero+WSD: . . . , and people of
all ethnic groups across the country, . . .
We also looked at the set of 318 sentences that
were judged by the Collins sign-test to be worse
translations. We found that in some situations,
Hiero+WSD has provided extra appropriate English
words, but those particular words are not used in the
reference sentences. An interesting example is the
translation of the Chinese sentence ??? i? ?
?8q??R?Rz?????.
? Hiero: Australian foreign minister said that
North Korea bad behavior will be more aid
? Hiero+WSD: Australian foreign minister said
that North Korea bad behavior will be
unable to obtain more aid
This is similar to the example mentioned earlier. In
this case however, those extra English words pro-
vided by Hiero+WSD, though appropriate, do not
39
result in more n-gram matches as the reference sen-
tences used phrases such as ?will not gain?, ?will not
get?, etc. Since the BLEU metric is precision based,
the longer sentence translation by Hiero+WSD gets
a lower BLEU score instead.
8 Conclusion
We have shown that WSD improves the transla-
tion performance of a state-of-the-art hierarchical
phrase-based statistical MT system and this im-
provement is statistically significant. We have also
demonstrated one way to integrate a WSD system
into an MT system without introducing any rules
that compete against existing rules, and where the
feature-weight tuning and decoding place the WSD
system on an equal footing with the other model
components. For future work, an immediate step
would be for the WSD classifier to provide trans-
lations for longer Chinese phrases. Also, different
alternatives could be tried to match the translations
provided by the WSD classifier against the chunks
of rules. Finally, besides our proposed approach of
integrating WSD into statistical MT via the intro-
duction of two new features, we could explore other
alternative ways of integration.
Acknowledgements
Yee Seng Chan is supported by a Singapore Millen-
nium Foundation Scholarship (ref no. SMF-2004-
1076). David Chiang was partially supported un-
der the GALE program of the Defense Advanced
Research Projects Agency, contract HR0011-06-C-
0022.
References
C. Cabezas and P. Resnik. 2005. Using WSD techniques for
lexical selection in statistical machine translation. Technical
report, University of Maryland.
M. Carpuat and D. Wu. 2005. Word sense disambiguation
vs. statistical machine translation. In Proc. of ACL05, pages
387?394.
M. Carpuat, W. Su, and D. Wu. 2004. Augmenting ensemble
classification for word sense disambiguation with a kernel
PCA model. In Proc. of SENSEVAL-3, pages 88?92.
C. C. Chang and C. J. Lin, 2001. LIBSVM: a library for sup-
port vector machines. Software available at http://www.
csie.ntu.edu.tw/?cjlin/libsvm.
D. Chiang. 2005. A hierarchical phrase-based model for sta-
tistical machine translation. In Proc. of ACL05, pages 263?
270.
D. Chiang. 2007. Hierarchical phrase-based translation. To
appear in Computational Linguistics, 33(2).
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause restruc-
turing for statistical machine translation. In Proc. of ACL05,
pages 531?540.
U. Germann. 2003. Greedy decoding for statistical machine
translation in almost linear time. In Proc. of HLT-NAACL03,
pages 72?79.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL03, pages 48?54.
P. Koehn. 2003. Noun Phrase Translation. Ph.D. thesis, Uni-
versity of Southern California.
P. Koehn. 2004a. Pharaoh: A beam search decoder for phrase-
based statistical machine translation models. In Proc. of
AMTA04, pages 115?124.
P. Koehn. 2004b. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP04, pages 388?
395.
Y. K. Lee and H. T. Ng. 2002. An empirical evaluation of
knowledge sources and learning algorithms for word sense
disambiguation. In Proc. of EMNLP02, pages 41?48.
P. M. II Lewis and R. E. Stearns. 1968. Syntax-directed trans-
duction. Journal of the ACM, 15(3):465?488.
D. Marcu and W. Wong. 2002. A phrase-based, joint proba-
bility model for statistical machine translation. In Proc. of
EMNLP02, pages 133?139.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proc. of ACL00, pages 440?447.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proc. of ACL02, pages 295?302.
F. J. Och and H. Ney. 2004. The alignment template approach
to statistical machine translation. Computational Linguis-
tics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of ACL03, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU:
A method for automatic evaluation of machine translation.
In Proc. of ACL02, pages 311?318.
A. Stolcke. 2002. SRILM - an extensible language modeling
toolkit. In Proc. of ICSLP02, pages 901?904.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005.
Word-sense disambiguation for machine translation. In
Proc. of EMNLP05, pages 771?778.
D. Wu. 1996. A polynomial-time algorithm for statistical ma-
chine translation. In Proc. of ACL96, pages 152?158.
40
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 49?56,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Domain Adaptation with Active Learning for Word Sense Disambiguation
Yee Seng Chan and Hwee Tou Ng
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
{chanys, nght}@comp.nus.edu.sg
Abstract
When a word sense disambiguation (WSD)
system is trained on one domain but ap-
plied to a different domain, a drop in ac-
curacy is frequently observed. This high-
lights the importance of domain adaptation
for word sense disambiguation. In this pa-
per, we first show that an active learning ap-
proach can be successfully used to perform
domain adaptation of WSD systems. Then,
by using the predominant sense predicted by
expectation-maximization (EM) and adopt-
ing a count-merging technique, we improve
the effectiveness of the original adaptation
process achieved by the basic active learn-
ing approach.
1 Introduction
In natural language, a word often assumes different
meanings, and the task of determining the correct
meaning, or sense, of a word in different contexts
is known as word sense disambiguation (WSD). To
date, the best performing systems in WSD use a
corpus-based, supervised learning approach. With
this approach, one would need to collect a text cor-
pus, in which each ambiguous word occurrence is
first tagged with its correct sense to serve as training
data.
The reliance of supervised WSD systems on an-
notated corpus raises the important issue of do-
main dependence. To investigate this, Escudero
et al (2000) and Martinez and Agirre (2000) con-
ducted experiments using the DSO corpus, which
contains sentences from two different corpora,
namely Brown Corpus (BC) and Wall Street Jour-
nal (WSJ). They found that training a WSD system
on one part (BC or WSJ) of the DSO corpus, and
applying it to the other, can result in an accuracy
drop of more than 10%, highlighting the need to per-
form domain adaptation of WSD systems to new do-
mains. Escudero et al (2000) pointed out that one
of the reasons for the drop in accuracy is the dif-
ference in sense priors (i.e., the proportions of the
different senses of a word) between BC and WSJ.
When the authors assumed they knew the sense pri-
ors of each word in BC and WSJ, and adjusted these
two datasets such that the proportions of the differ-
ent senses of each word were the same between BC
and WSJ, accuracy improved by 9%.
In this paper, we explore domain adaptation of
WSD systems, by adding training examples from the
new domain as additional training data to a WSD
system. To reduce the effort required to adapt a
WSD system to a new domain, we employ an ac-
tive learning strategy (Lewis and Gale, 1994) to se-
lect examples to annotate from the new domain of
interest. To our knowledge, our work is the first to
use active learning for domain adaptation for WSD.
A similar work is the recent research by Chen et al
(2006), where active learning was used successfully
to reduce the annotation effort for WSD of 5 English
verbs using coarse-grained evaluation. In that work,
the authors only used active learning to reduce the
annotation effort and did not deal with the porting of
a WSD system to a new domain.
Domain adaptation is necessary when the train-
ing and target domains are different. In this paper,
49
we perform domain adaptation for WSD of a set of
nouns using fine-grained evaluation. The contribu-
tion of our work is not only in showing that active
learning can be successfully employed to reduce the
annotation effort required for domain adaptation in
a fine-grained WSD setting. More importantly, our
main focus and contribution is in showing how we
can improve the effectiveness of a basic active learn-
ing approach when it is used for domain adaptation.
In particular, we explore the issue of different sense
priors across different domains. Using the sense
priors estimated by expectation-maximization (EM),
the predominant sense in the new domain is pre-
dicted. Using this predicted predominant sense and
adopting a count-merging technique, we improve the
effectiveness of the adaptation process.
In the next section, we discuss the choice of cor-
pus and nouns used in our experiments. We then
introduce active learning for domain adaptation, fol-
lowed by count-merging. Next, we describe an EM-
based algorithm to estimate the sense priors in the
new domain. Performance of domain adaptation us-
ing active learning and count-merging is then pre-
sented. Next, we show that by using the predom-
inant sense of the target domain as predicted by
the EM-based algorithm, we improve the effective-
ness of the adaptation process. Our empirical results
show that for the set of nouns which have different
predominant senses between the training and target
domains, we are able to reduce the annotation effort
by 71%.
2 Experimental Setting
In this section, we discuss the motivations for choos-
ing the particular corpus and the set of nouns to con-
duct our domain adaptation experiments.
2.1 Choice of Corpus
The DSO corpus (Ng and Lee, 1996) contains
192,800 annotated examples for 121 nouns and 70
verbs, drawn from BC and WSJ. While the BC is
built as a balanced corpus, containing texts in var-
ious categories such as religion, politics, humani-
ties, fiction, etc, the WSJ corpus consists primarily
of business and financial news. Exploiting the dif-
ference in coverage between these two corpora, Es-
cudero et al (2000) separated the DSO corpus into
its BC and WSJ parts to investigate the domain de-
pendence of several WSD algorithms. Following the
setup of (Escudero et al, 2000), we similarly made
use of the DSO corpus to perform our experiments
on domain adaptation.
Among the few currently available manually
sense-annotated corpora for WSD, the SEMCOR
(SC) corpus (Miller et al, 1994) is the most widely
used. SEMCOR is a subset of BC which is sense-
annotated. Since BC is a balanced corpus, and since
performing adaptation from a general corpus to a
more specific corpus is a natural scenario, we focus
on adapting a WSD system trained on BC to WSJ in
this paper. Henceforth, out-of-domain data will re-
fer to BC examples, and in-domain data will refer to
WSJ examples.
2.2 Choice of Nouns
The WordNet Domains resource (Magnini and
Cavaglia, 2000) assigns domain labels to synsets in
WordNet. Since the focus of the WSJ corpus is on
business and financial news, we can make use of
WordNet Domains to select the set of nouns having
at least one synset labeled with a business or finance
related domain label. This is similar to the approach
taken in (Koeling et al, 2005) where they focus on
determining the predominant sense of words in cor-
pora drawn from finance versus sports domains.1
Hence, we select the subset of DSO nouns that have
at least one synset labeled with any of these domain
labels: commerce, enterprise, money, finance, bank-
ing, and economy. This gives a set of 21 nouns:
book, business, center, community, condition, field,
figure, house, interest, land, line, money, need, num-
ber, order, part, power, society, term, use, value.2
For each noun, all the BC examples are used as
out-of-domain training data. One-third of the WSJ
examples for each noun are set aside as evaluation
1Note however that the coverage of the WordNet Domains
resource is not comprehensive, as about 31% of the synsets are
simply labeled with ?factotum?, indicating that the synset does
not belong to a specific domain.
225 nouns have at least one synset labeled with the listed
domain labels. In our experiments, 4 out of these 25 nouns have
an accuracy of more than 90% before adaptation (i.e., training
on just the BC examples) and accuracy improvement is less than
1% after all the available WSJ adaptation examples are added
as additional training data. To obtain a clearer picture of the
adaptation process, we discard these 4 nouns, leaving a set of
21 nouns.
50
Dataset No. of MFS No. of No. of
senses acc. training adaptation
BC WSJ (%) examples examples
21 nouns 6.7 6.8 61.1 310 406
9 nouns 7.9 8.6 65.8 276 416
Table 1: The average number of senses in BC and
WSJ, average MFS accuracy, average number of BC
training, and WSJ adaptation examples per noun.
data, and the rest of the WSJ examples are desig-
nated as in-domain adaptation data. The row 21
nouns in Table 1 shows some information about
these 21 nouns. For instance, these nouns have an
average of 6.7 senses in BC and 6.8 senses in WSJ.
This is slightly higher than the 5.8 senses per verb in
(Chen et al, 2006), where the experiments were con-
ducted using coarse-grained evaluation. Assuming
we have access to an ?oracle? which determines the
predominant sense, or most frequent sense (MFS),
of each noun in our WSJ test data perfectly, and
we assign this most frequent sense to each noun in
the test data, we will have achieved an accuracy of
61.1% as shown in the column MFS accuracy of Ta-
ble 1. Finally, we note that we have an average of
310 BC training examples and 406 WSJ adaptation
examples per noun.
3 Active Learning
For our experiments, we use naive Bayes as the
learning algorithm. The knowledge sources we use
include parts-of-speech, local collocations, and sur-
rounding words. These knowledge sources were ef-
fectively used to build a state-of-the-art WSD pro-
gram in one of our prior work (Lee and Ng, 2002).
In performing WSD with a naive Bayes classifier,
the sense s assigned to an example with features
f1, . . . , fn is chosen so as to maximize:
p(s)
n?
j=1
p(fj |s)
In our domain adaptation study, we start with a
WSD system built using training examples drawn
from BC. We then investigate the utility of adding
additional in-domain training data from WSJ. In the
baseline approach, the additional WSJ examples are
randomly selected. With active learning (Lewis and
Gale, 1994), we use uncertainty sampling as shown
DT ? the set of BC training examples
DA ? the set of untagged WSJ adaptation examples
?? WSD system trained on DT
repeat
pmin ??
for each d ? DA do
bs? word sense prediction for d using ?
p ? confidence of prediction bs
if p < pmin then
pmin ? p, dmin ? d
end
end
DA ? DA ? dmin
provide correct sense s for dmin and add dmin to DT
?? WSD system trained on new DT
end
Figure 1: Active learning
in Figure 1. In each iteration, we train a WSD sys-
tem on the available training data and apply it on the
WSJ adaptation examples. Among these WSJ ex-
amples, the example predicted with the lowest con-
fidence is selected and removed from the adaptation
data. The correct label is then supplied for this ex-
ample and it is added to the training data.
Note that in the experiments reported in this pa-
per, all the adaptation examples are already pre-
annotated before the experiments start, since all
the WSJ adaptation examples come from the DSO
corpus which have already been sense-annotated.
Hence, the annotation of an example needed during
each adaptation iteration is simulated by performing
a lookup without any manual annotation.
4 Count-merging
We also employ a technique known as count-
merging in our domain adaptation study. Count-
merging assigns different weights to different ex-
amples to better reflect their relative importance.
Roark and Bacchiani (2003) showed that weighted
count-merging is a special case of maximum a pos-
teriori (MAP) estimation, and successfully used it
for probabilistic context-free grammar domain adap-
tation (Roark and Bacchiani, 2003) and language
model adaptation (Bacchiani and Roark, 2003).
Count-merging can be regarded as scaling of
counts obtained from different data sets. We let
c? denote the counts from out-of-domain training
data, c? denote the counts from in-domain adapta-
tion data, and p? denote the probability estimate by
51
count-merging. We can scale the out-of-domain and
in-domain counts with different factors, or just use a
single weight parameter ?:
p?(fj |si) = c?(fj , si) + ?c?(fj , si)c?(si) + ?c?(si) (1)
Similarly,
p?(si) = c?(si) + ?c?(si)c?+ ?c? (2)
Obtaining an optimum value for ? is not the focus
of this work. Instead, we are interested to see if as-
signing a higher weight to the in-domain WSJ adap-
tation examples, as compared to the out-of-domain
BC examples, will improve the adaptation process.
Hence, we just use a ? value of 3 in our experiments
involving count-merging.
5 Estimating Sense Priors
In this section, we describe an EM-based algorithm
that was introduced by Saerens et al (2002), which
can be used to estimate the sense priors, or a priori
probabilities of the different senses in a new dataset.
We have recently shown that this algorithm is effec-
tive in estimating the sense priors of a set of nouns
(Chan and Ng, 2005).
Most of this section is based on (Saerens et al,
2002). Assume we have a set of labeled data DL
with n classes and a set of N independent instances
(x1, . . . ,xN ) from a new data set. The likelihood of
these N instances can be defined as:
L(x1, . . . ,xN ) =
N?
k=1
p(xk)
=
N?
k=1
[ n?
i=1
p(xk, ?i)
]
=
N?
k=1
[ n?
i=1
p(xk|?i)p(?i)
]
(3)
Assuming the within-class densities p(xk|?i), i.e.,
the probabilities of observing xk given the class ?i,
do not change from the training set DL to the new
data set, we can define: p(xk|?i) = pL(xk|?i). To
determine the a priori probability estimates p?(?i) of
the new data set that will maximize the likelihood of
(3) with respect to p(?i), we can apply the iterative
procedure of the EM algorithm. In effect, through
maximizing the likelihood of (3), we obtain the a
priori probability estimates as a by-product.
Let us now define some notations. When we ap-
ply a classifier trained on DL on an instance xk
drawn from the new data set DU , we get p?L(?i|xk),
which we define as the probability of instance xk
being classified as class ?i by the classifier trained
on DL. Further, let us define p?L(?i) as the a pri-
ori probability of class ?i in DL. This can be esti-
mated by the class frequency of ?i in DL. We also
define p?(s)(?i) and p?(s)(?i|xk) as estimates of the
new a priori and a posteriori probabilities at step s
of the iterative EM procedure. Assuming we initial-
ize p?(0)(?i) = p?L(?i), then for each instance xk in
DU and each class ?i, the EM algorithm provides
the following iterative steps:
p?(s)(?i|xk) =
p?L(?i|xk) bp
(s)(?i)
bpL(?i)?n
j=1 p?L(?j |xk) bp
(s)(?j)
bpL(?j)
(4)
p?(s+1)(?i) = 1N
N?
k=1
p?(s)(?i|xk) (5)
where Equation (4) represents the expectation E-
step, Equation (5) represents the maximization M-
step, and N represents the number of instances in
DU . Note that the probabilities p?L(?i|xk) and
p?L(?i) in Equation (4) will stay the same through-
out the iterations for each particular instance xk
and class ?i. The new a posteriori probabilities
p?(s)(?i|xk) at step s in Equation (4) are simply the
a posteriori probabilities in the conditions of the la-
beled data, p?L(?i|xk), weighted by the ratio of the
new priors p?(s)(?i) to the old priors p?L(?i). The de-
nominator in Equation (4) is simply a normalizing
factor.
The a posteriori p?(s)(?i|xk) and a priori proba-
bilities p?(s)(?i) are re-estimated sequentially dur-
ing each iteration s for each new instance xk and
each class ?i, until the convergence of the estimated
probabilities p?(s)(?i), which will be our estimated
sense priors. This iterative procedure will increase
the likelihood of (3) at each step.
6 Experimental Results
For each adaptation experiment, we start off with a
classifier built from an initial training set consisting
52
 52
 54
 56
 58
 60
 62
 64
 66
 68
 70
 72
 74
 76
 0  5  10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
W
SD
 A
cc
ur
ac
y (
%)
Percentage of adaptation examples added (%)
a-c
a
r
a-truePrior
Figure 2: Adaptation process for all 21 nouns.
of the BC training examples. At each adaptation iter-
ation, WSJ adaptation examples are selected one at
a time and added to the training set. The adaptation
process continues until all the adaptation examples
are added. Classification accuracies averaged over
3 random trials on the WSJ test examples at each
iteration are calculated. Since the number of WSJ
adaptation examples differs for each of the 21 nouns,
the learning curves we will show in the various fig-
ures are plotted in terms of different percentage of
adaptation examples added, varying from 0 to 100
percent in steps of 1 percent. To obtain these curves,
we first calculate for each noun, the WSD accuracy
when different percentages of adaptation examples
are added. Then, for each percentage, we calculate
the macro-average WSD accuracy over all the nouns
to obtain a single learning curve representing all the
nouns.
6.1 Utility of Active Learning and
Count-merging
In Figure 2, the curve r represents the adaptation
process of the baseline approach, where additional
WSJ examples are randomly selected during each
adaptation iteration. The adaptation process using
active learning is represented by the curve a, while
applying count-merging with active learning is rep-
resented by the curve a-c. Note that random selec-
tion r achieves its highest WSD accuracy after all
the adaptation examples are added. To reach the
same accuracy, the a approach requires the addition
of only 57% of adaptation examples. The a-c ap-
proach is even more effective and requires only 42%
of adaptation examples. This demonstrates the ef-
fectiveness of count-merging in further reducing the
annotation effort, when compared to using only ac-
tive learning. To reach the MFS accuracy of 61.1%
as shown earlier in Table 1, a-c requires just 4% of
the adaptation examples.
To determine the utility of the out-of-domain BC
examples, we have also conducted three active learn-
ing runs using only WSJ adaptation examples. Us-
ing 10%, 20%, and 30% of WSJ adaptation exam-
ples to build a classifier, the accuracy of these runs
is lower than the active learning a curve and paired
t-tests show that the difference is statistically signif-
icant at the level of significance 0.01.
6.2 Using Sense Priors Information
As mentioned in section 1, research in (Escudero et
al., 2000) noted an improvement in accuracy when
they adjusted the BC and WSJ datasets such that
the proportions of the different senses of each word
were the same between BC and WSJ. We can simi-
larly choose BC examples such that the sense priors
in the BC training data adhere to the sense priors in
the WSJ evaluation data. To gauge the effectiveness
of this approach, we first assume that we know the
true sense priors of each noun in the WSJ evalua-
tion data. We then gather BC training examples for
a noun to adhere as much as possible to the sense
priors in WSJ. Assume sense si is the predominant
sense in the WSJ evaluation data, si has a sense prior
of pi in the WSJ data and has ni BC training exam-
ples. Taking ni examples to represent a sense prior
of pi, we proportionally determine the number of BC
examples to gather for other senses s according to
their respective sense priors in WSJ. If there are in-
sufficient training examples in BC for some sense s,
whatever available examples of s are used.
This approach gives an average of 195 BC train-
ing examples for the 21 nouns. With this new set
of training examples, we perform adaptation using
active learning and obtain the a-truePrior curve in
Figure 2. The a-truePrior curve shows that by en-
suring that the sense priors in the BC training data
adhere as much as possible to the sense priors in the
WSJ data, we start off with a higher WSD accuracy.
However, the performance is no different from the a
53
curve after 35% of adaptation examples are added.
A possible reason might be that by strictly adhering
to the sense priors in the WSJ data, we have removed
too many BC training examples, from an average of
310 examples per noun as shown in Table 1, to an
average of 195 examples.
6.3 Using Predominant Sense Information
Research by McCarthy et al (2004) and Koeling et
al. (2005) pointed out that a change of predominant
sense is often indicative of a change in domain. For
example, the predominant sense of the noun interest
in the BC part of the DSO corpus has the meaning
?a sense of concern with and curiosity about some-
one or something?. In the WSJ part of the DSO cor-
pus, the noun interest has a different predominant
sense with the meaning ?a fixed charge for borrow-
ing money?, which is reflective of the business and
finance focus of the WSJ corpus.
Instead of restricting the BC training data to ad-
here strictly to the sense priors in WSJ, another alter-
native is just to ensure that the predominant sense in
BC is the same as that of WSJ. Out of the 21 nouns,
12 nouns have the same predominant sense in both
BC and WSJ. The remaining 9 nouns that have dif-
ferent predominant senses in the BC and WSJ data
are: center, field, figure, interest, line, need, order,
term, value. The row 9 nouns in Table 1 gives some
information for this set of 9 nouns. To gauge the
utility of this approach, we conduct experiments on
these nouns by first assuming that we know the true
predominant sense in the WSJ data. Assume that the
WSJ predominant sense of a noun is si and si has ni
examples in the BC data. We then gather BC exam-
ples for a noun to adhere to this WSJ predominant
sense, by gathering only up to ni BC examples for
each sense of this noun. This approach gives an av-
erage of 190 BC examples for the 9 nouns. This is
higher than an average of 83 BC examples for these
9 nouns if BC examples are selected to follow the
sense priors of WSJ evaluation data as described in
the last subsection 6.2.
For these 9 nouns, the average KL-divergence be-
tween the sense priors of the original BC data and
WSJ evaluation data is 0.81. This drops to 0.51 af-
ter ensuring that the predominant sense in BC is the
same as that of WSJ, confirming that the sense priors
in the newly gathered BC data more closely follow
 44
 46
 48
 50
 52
 54
 56
 58
 60
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 82
 0  5  10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100
W
SD
 A
cc
ur
ac
y (
%)
Percentage of adaptation examples added (%)
a-truePrior
a-truePred
a
Figure 3: Using true predominant sense for the 9
nouns.
the sense priors in WSJ. Using this new set of train-
ing examples, we perform domain adaptation using
active learning to obtain the curve a-truePred in Fig-
ure 3. For comparison, we also plot the curves a
and a-truePrior for this set of 9 nouns in Figure 3.
Results in Figure 3 show that a-truePred starts off
at a higher accuracy and performs consistently bet-
ter than the a curve. In contrast, though a-truePrior
starts at a high accuracy, its performance is lower
than a-truePred and a after 50% of adaptation ex-
amples are added. The approach represented by a-
truePred is a compromise between ensuring that the
sense priors in the training data follow as closely
as possible the sense priors in the evaluation data,
while retaining enough training examples. These re-
sults highlight the importance of striking a balance
between these two goals.
In (McCarthy et al, 2004), a method was pre-
sented to determine the predominant sense of a word
in a corpus. However, in (Chan and Ng, 2005),
we showed that in a supervised setting where one
has access to some annotated training data, the EM-
based method in section 5 estimates the sense priors
more effectively than the method described in (Mc-
Carthy et al, 2004). Hence, we use the EM-based
algorithm to estimate the sense priors in the WSJ
evaluation data for each of the 21 nouns. The sense
with the highest estimated sense prior is taken as the
predominant sense of the noun.
For the set of 12 nouns where the predominant
54
 43 44
 45 46
 47 48
 49 50
 51 52
 53 54
 55 56
 57 58
 59 60
 61 62
 63 64
 65 66
 67 68
 69 70
 71 72
 73 74
 75 76
 77 78
 79 80
 81 82
 0  5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100
W
SD
 Ac
cu
rac
y (%
)
Percentage of adaptation examples added (%)
a-c-estPred
a-truePred
a-estPred
a
r
Figure 4: Using estimated predominant sense for the
9 nouns.
Accuracy % adaptation examples needed
r a a-estPred a-c-estPred
50%: 61.1 8 7 (0.88) 5 (0.63) 4 (0.50)
60%: 64.5 10 9 (0.90) 7 (0.70) 5 (0.50)
70%: 68.0 15 12 (0.80) 9 (0.60) 6 (0.40)
80%: 71.5 23 16 (0.70) 12 (0.52) 9 (0.39)
90%: 74.9 46 24 (0.52) 21 (0.46) 15 (0.33)
100%: 78.4 100 51 (0.51) 38 (0.38) 29 (0.29)
Table 2: Annotation savings and percentage of adap-
tation examples needed to reach various accuracies.
sense remains unchanged between BC and WSJ, the
EM-based algorithm is able to predict that the pre-
dominant sense remains unchanged for all 12 nouns.
Hence, we will focus on the 9 nouns which have
different predominant senses between BC and WSJ
for our remaining adaptation experiments. For these
9 nouns, the EM-based algorithm correctly predicts
the WSJ predominant sense for 6 nouns. Hence, the
algorithm is able to predict the correct predominant
sense for 18 out of 21 nouns overall, representing an
accuracy of 86%.
Figure 4 plots the curve a-estPred, which is simi-
lar to a-truePred, except that the predominant sense
is now estimated by the EM-based algorithm. Em-
ploying count-merging with a-estPred produces the
curve a-c-estPred. For comparison, the curves r, a,
and a-truePred are also plotted. The results show
that a-estPred performs consistently better than a,
and a-c-estPred in turn performs better than a-
estPred. Hence, employing the predicted predom-
inant sense and count-merging, we further improve
the effectiveness of the active learning-based adap-
tation process.
With reference to Figure 4, the WSD accuracies
of the r and a curves before and after adaptation
are 43.7% and 78.4% respectively. Starting from
the mid-point 61.1% accuracy, which represents a
50% accuracy increase from 43.7%, we show in
Table 2 the percentage of adaptation examples re-
quired by the various approaches to reach certain
levels of WSD accuracies. For instance, to reach
the final accuracy of 78.4%, r, a, a-estPred, and a-
c-estPred require the addition of 100%, 51%, 38%,
and 29% adaptation examples respectively. The
numbers in brackets give the ratio of adaptation ex-
amples needed by a, a-estPred, and a-c-estPred ver-
sus random selection r. For instance, to reach a
WSD accuracy of 78.4%, a-c-estPred needs only
29% adaptation examples, representing a ratio of
0.29 and an annotation saving of 71%. Note that this
represents a more effective adaptation process than
the basic active learning a approach, which requires
51% adaptation examples. Hence, besides showing
that active learning can be used to reduce the annota-
tion effort required for domain adaptation, we have
further improved the effectiveness of the adaptation
process by using the predicted predominant sense
of the new domain and adopting the count-merging
technique.
7 Related Work
In applying active learning for domain adapta-
tion, Zhang et al (2003) presented work on sen-
tence boundary detection using generalized Win-
now, while Tur et al (2004) performed language
model adaptation of automatic speech recognition
systems. In both papers, out-of-domain and in-
domain data were simply mixed together without
MAP estimation such as count-merging. For WSD,
Fujii et al (1998) used selective sampling for a
Japanese language WSD system, Chen et al (2006)
used active learning for 5 verbs using coarse-grained
evaluation, and H. T. Dang (2004) employed active
learning for another set of 5 verbs. However, their
work only investigated the use of active learning to
reduce the annotation effort necessary for WSD, but
55
did not deal with the porting of a WSD system to
a different domain. Escudero et al (2000) used the
DSO corpus to highlight the importance of the issue
of domain dependence of WSD systems, but did not
propose methods such as active learning or count-
merging to address the specific problem of how to
perform domain adaptation for WSD.
8 Conclusion
Domain adaptation is important to ensure the gen-
eral applicability of WSD systems across different
domains. In this paper, we have shown that active
learning is effective in reducing the annotation ef-
fort required in porting a WSD system to a new do-
main. Also, we have successfully used an EM-based
algorithm to detect a change in predominant sense
between the training and new domain. With this
information on the predominant sense of the new
domain and incorporating count-merging, we have
shown that we are able to improve the effectiveness
of the original adaptation process achieved by the
basic active learning approach.
Acknowledgement
Yee Seng Chan is supported by a Singapore Millen-
nium Foundation Scholarship (ref no. SMF-2004-
1076).
References
M. Bacchiani and B. Roark. 2003. Unsupervised lan-
guage model adaptation. In Proc. of IEEE ICASSP03.
Y. S. Chan and H. T. Ng. 2005. Word sense disambigua-
tion with distribution estimation. In Proc. of IJCAI05.
J. Chen, A. Schein, L. Ungar, and M. Palmer. 2006.
An empirical study of the behavior of active learn-
ing for word sense disambiguation. In Proc. of
HLT/NAACL06.
H. T. Dang. 2004. Investigations into the Role of Lex-
ical Semantics in Word Sense Disambiguation. PhD
dissertation, University of Pennsylvania.
G. Escudero, L. Marquez, and G. Rigau. 2000. An
empirical study of the domain dependence of super-
vised word sense disambiguation systems. In Proc. of
EMNLP/VLC00.
A. Fujii, K. Inui, T. Tokunaga, and H. Tanaka. 1998.
Selective sampling for example-based word sense dis-
ambiguation. Computational Linguistics, 24(4).
R. Koeling, D. McCarthy, and J. Carroll. 2005. Domain-
specific sense distributions and predominant sense ac-
quisition. In Proc. of Joint HLT-EMNLP05.
Y. K. Lee and H. T. Ng. 2002. An empirical evaluation of
knowledge sources and learning algorithms for word
sense disambiguation. In Proc. of EMNLP02.
D. D. Lewis and W. A. Gale. 1994. A sequential algo-
rithm for training text classifiers. In Proc. of SIGIR94.
B. Magnini and G. Cavaglia. 2000. Integrating subject
field codes into WordNet. In Proc. of LREC-2000.
D. Martinez and E. Agirre. 2000. One sense per
collocation and genre/topic variations. In Proc. of
EMNLP/VLC00.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant word senses in untagged text. In
Proc. of ACL04.
G. A. Miller, M. Chodorow, S. Landes, C. Leacock, and
R. G. Thomas. 1994. Using a semantic concordance
for sense identification. In Proc. of HLT94 Workshop
on Human Language Technology.
H. T. Ng and H. B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense: An
exemplar-based approach. In Proc. of ACL96.
B. Roark and M. Bacchiani. 2003. Supervised and unsu-
pervised PCFG adaptation to novel domains. In Proc.
of HLT-NAACL03.
M. Saerens, P. Latinne, and C. Decaestecker. 2002. Ad-
justing the outputs of a classifier to new a priori prob-
abilities: A simple procedure. Neural Computation,
14(1).
D. H. Tur, G. Tur, M. Rahim, and G. Riccardi. 2004.
Unsupervised and active learning in automatic speech
recognition for call classification. In Proc. of IEEE
ICASSP04.
T. Zhang, F. Damerau, and D. Johnson. 2003. Updat-
ing an NLP system to fit new domains: an empirical
study on the sentence segmentation problem. In Proc.
of CONLL03.
56
Proceedings of ACL-08: HLT, pages 55?62,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
MAXSIM: A Maximum Similarity Metric
for Machine Translation Evaluation
Yee Seng Chan and Hwee Tou Ng
Department of Computer Science
National University of Singapore
Law Link, Singapore 117590
{chanys, nght}@comp.nus.edu.sg
Abstract
We propose an automatic machine translation
(MT) evaluation metric that calculates a sim-
ilarity score (based on precision and recall)
of a pair of sentences. Unlike most metrics,
we compute a similarity score between items
across the two sentences. We then find a maxi-
mum weight matching between the items such
that each item in one sentence is mapped to
at most one item in the other sentence. This
general framework allows us to use arbitrary
similarity functions between items, and to in-
corporate different information in our com-
parison, such as n-grams, dependency rela-
tions, etc. When evaluated on data from the
ACL-07 MT workshop, our proposed metric
achieves higher correlation with human judge-
ments than all 11 automatic MT evaluation
metrics that were evaluated during the work-
shop.
1 Introduction
In recent years, machine translation (MT) research
has made much progress, which includes the in-
troduction of automatic metrics for MT evaluation.
Since human evaluation of MT output is time con-
suming and expensive, having a robust and accurate
automatic MT evaluation metric that correlates well
with human judgement is invaluable.
Among all the automatic MT evaluation metrics,
BLEU (Papineni et al, 2002) is the most widely
used. Although BLEU has played a crucial role in
the progress of MT research, it is becoming evident
that BLEU does not correlate with human judgement
well enough, and suffers from several other deficien-
cies such as the lack of an intuitive interpretation of
its scores.
During the recent ACL-07 workshop on statis-
tical MT (Callison-Burch et al, 2007), a total of
11 automatic MT evaluation metrics were evalu-
ated for correlation with human judgement. The re-
sults show that, as compared to BLEU, several re-
cently proposed metrics such as Semantic-role over-
lap (Gimenez and Marquez, 2007), ParaEval-recall
(Zhou et al, 2006), and METEOR (Banerjee and
Lavie, 2005) achieve higher correlation.
In this paper, we propose a new automatic MT
evaluation metric, MAXSIM, that compares a pair
of system-reference sentences by extracting n-grams
and dependency relations. Recognizing that differ-
ent concepts can be expressed in a variety of ways,
we allow matching across synonyms and also com-
pute a score between two matching items (such as
between two n-grams or between two dependency
relations), which indicates their degree of similarity
with each other.
Having weighted matches between items means
that there could be many possible ways to match, or
link items from a system translation sentence to a
reference translation sentence. To match each sys-
tem item to at most one reference item, we model
the items in the sentence pair as nodes in a bipartite
graph and use the Kuhn-Munkres algorithm (Kuhn,
1955; Munkres, 1957) to find a maximum weight
matching (or alignment) between the items in poly-
nomial time. The weights (from the edges) of the
resulting graph will then be added to determine the
final similarity score between the pair of sentences.
55
Although a maximum weight bipartite graph was
also used in the recent work of (Taskar et al, 2005),
their focus was on learning supervised models for
single word alignment between sentences from a
source and target language.
The contributions of this paper are as fol-
lows. Current metrics (such as BLEU, METEOR,
Semantic-role overlap, ParaEval-recall, etc.) do not
assign different weights to their matches: either two
items match, or they don?t. Also, metrics such
as METEOR determine an alignment between the
items of a sentence pair by using heuristics such
as the least number of matching crosses. In con-
trast, we propose weighting different matches dif-
ferently, and then obtain an optimal set of matches,
or alignments, by using a maximum weight match-
ing framework. We note that this framework is not
used by any of the 11 automatic MT metrics in the
ACL-07 MT workshop. Also, this framework al-
lows for defining arbitrary similarity functions be-
tween two matching items, and we could match arbi-
trary concepts (such as dependency relations) gath-
ered from a sentence pair. In contrast, most other
metrics (notably BLEU) limit themselves to match-
ing based only on the surface form of words. Finally,
when evaluated on the datasets of the recent ACL-
07 MT workshop (Callison-Burch et al, 2007), our
proposed metric achieves higher correlation with hu-
man judgements than all of the 11 automatic MT
evaluation metrics evaluated during the workshop.
In the next section, we describe several existing
metrics. In Section 3, we discuss issues to consider
when designing a metric. In Section 4, we describe
our proposed metric. In Section 5, we present our
experimental results. Finally, we outline future work
in Section 6, before concluding in Section 7.
2 Automatic Evaluation Metrics
In this section, we describe BLEU, and the three
metrics which achieved higher correlation results
than BLEU in the recent ACL-07 MT workshop.
2.1 BLEU
BLEU (Papineni et al, 2002) is essentially a
precision-based metric and is currently the standard
metric for automatic evaluation of MT performance.
To score a system translation, BLEU tabulates the
number of n-gram matches of the system translation
against one or more reference translations. Gener-
ally, more n-gram matches result in a higher BLEU
score.
When determining the matches to calculate pre-
cision, BLEU uses a modified, or clipped n-gram
precision. With this, an n-gram (from both the sys-
tem and reference translation) is considered to be
exhausted or used after participating in a match.
Hence, each system n-gram is ?clipped? by the max-
imum number of times it appears in any reference
translation.
To prevent short system translations from receiv-
ing too high a score and to compensate for its lack
of a recall component, BLEU incorporates a brevity
penalty. This penalizes the score of a system if the
length of its entire translation output is shorter than
the length of the reference text.
2.2 Semantic Roles
(Gimenez and Marquez, 2007) proposed using
deeper linguistic information to evaluate MT per-
formance. For evaluation in the ACL-07 MT work-
shop, the authors used the metric which they termed
as SR-Or-*1. This metric first counts the number
of lexical overlaps SR-Or-t for all the different se-
mantic roles t that are found in the system and ref-
erence translation sentence. A uniform average of
the counts is then taken as the score for the sen-
tence pair. In their work, the different semantic roles
t they considered include the various core and ad-
junct arguments as defined in the PropBank project
(Palmer et al, 2005). For instance, SR-Or-A0 refers
to the number of lexical overlaps between the A0
arguments. To extract semantic roles from a sen-
tence, several processes such as lemmatization, part-
of-speech tagging, base phrase chunking, named en-
tity tagging, and finally semantic role tagging need
to be performed.
2.3 ParaEval
The ParaEval metric (Zhou et al, 2006) uses a
large collection of paraphrases, automatically ex-
tracted from parallel corpora, to evaluate MT per-
formance. To compare a pair of sentences, ParaE-
val first locates paraphrase matches between the two
1Verified through personal communication as this is not ev-
ident in their paper.
56
sentences. Then, unigram matching is performed
on the remaining words that are not matched us-
ing paraphrases. Based on the matches, ParaEval
will then elect to use either unigram precision or un-
igram recall as its score for the sentence pair. In
the ACL-07 MT workshop, ParaEval based on re-
call (ParaEval-recall) achieves good correlation with
human judgements.
2.4 METEOR
Given a pair of strings to compare (a system transla-
tion and a reference translation), METEOR (Baner-
jee and Lavie, 2005) first creates a word alignment
between the two strings. Based on the number of
word or unigram matches and the amount of string
fragmentation represented by the alignment, ME-
TEOR calculates a score for the pair of strings.
In aligning the unigrams, each unigram in one
string is mapped, or linked, to at most one unigram
in the other string. These word alignments are cre-
ated incrementally through a series of stages, where
each stage only adds alignments between unigrams
which have not been matched in previous stages. At
each stage, if there are multiple different alignments,
then the alignment with the most number of map-
pings is selected. If there is a tie, then the alignment
with the least number of unigram mapping crosses
is selected.
The three stages of ?exact?, ?porter stem?, and
?WN synonymy? are usually applied in sequence to
create alignments. The ?exact? stage maps unigrams
if they have the same surface form. The ?porter
stem? stage then considers the remaining unmapped
unigrams and maps them if they are the same af-
ter applying the Porter stemmer. Finally, the ?WN
synonymy? stage considers all remaining unigrams
and maps two unigrams if they are synonyms in the
WordNet sense inventory (Miller, 1990).
Once the final alignment has been produced, un-
igram precision P (number of unigram matches m
divided by the total number of system unigrams)
and unigram recall R (m divided by the total number
of reference unigrams) are calculated and combined
into a single parameterized harmonic mean (Rijsber-
gen, 1979):
Fmean =
P ? R
?P + (1 ? ?)R (1)
To account for longer matches and the amount
of fragmentation represented by the alignment, ME-
TEOR groups the matched unigrams into as few
chunks as possible and imposes a penalty based on
the number of chunks. The METEOR score for a
pair of sentences is:
score =
[
1 ? ?
(
no. of chunks
m
)?
]
Fmean
where ?
(
no. of chunks
m
)?
represents the fragmenta-
tion penalty of the alignment. Note that METEOR
consists of three parameters that need to be opti-
mized based on experimentation: ?, ?, and ?.
3 Metric Design Considerations
We first review some aspects of existing metrics and
highlight issues that should be considered when de-
signing an MT evaluation metric.
? Intuitive interpretation: To compensate for
the lack of recall, BLEU incorporates a brevity
penalty. This, however, prevents an intuitive in-
terpretation of its scores. To address this, stan-
dard measures like precision and recall could
be used, as in some previous research (Baner-
jee and Lavie, 2005; Melamed et al, 2003).
? Allowing for variation: BLEU only counts
exact word matches. Languages, however, of-
ten allow a great deal of variety in vocabulary
and in the ways concepts are expressed. Hence,
using information such as synonyms or depen-
dency relations could potentially address the is-
sue better.
? Matches should be weighted: Current met-
rics either match, or don?t match a pair of
items. We note, however, that matches between
items (such as words, n-grams, etc.) should be
weighted according to their degree of similar-
ity.
4 The Maximum Similarity Metric
We now describe our proposed metric, Maximum
Similarity (MAXSIM), which is based on precision
and recall, allows for synonyms, and weights the
matches found.
57
Given a pair of English sentences to be com-
pared (a system translation against a reference
translation), we perform tokenization2 , lemmati-
zation using WordNet3, and part-of-speech (POS)
tagging with the MXPOST tagger (Ratnaparkhi,
1996). Next, we remove all non-alphanumeric to-
kens. Then, we match the unigrams in the system
translation to the unigrams in the reference transla-
tion. Based on the matches, we calculate the recall
and precision, which we then combine into a single
Fmean unigram score using Equation 1. Similarly,
we also match the bigrams and trigrams of the sen-
tence pair and calculate their corresponding Fmean
scores. To obtain a single similarity score scores
for this sentence pair s, we simply average the three
Fmean scores. Then, to obtain a single similarity
score sim-score for the entire system corpus, we
repeat this process of calculating a scores for each
system-reference sentence pair s, and compute the
average over all |S| sentence pairs:
sim-score = 1|S|
|S|
?
s=1
[
1
N
N
?
n=1
Fmeans,n
]
where in our experiments, we set N=3, representing
calculation of unigram, bigram, and trigram scores.
If we are given access to multiple references, we cal-
culate an individual sim-score between the system
corpus and each reference corpus, and then average
the scores obtained.
4.1 Using N-gram Information
In this subsection, we describe in detail how we
match the n-grams of a system-reference sentence
pair.
Lemma and POS match Representing each n-
gram by its sequence of lemma and POS-tag pairs,
we first try to perform an exact match in both lemma
and POS-tag. In all our n-gram matching, each n-
gram in the system translation can only match at
most one n-gram in the reference translation.
Representing each unigram (lipi) at position i by
its lemma li and POS-tag pi, we count the num-
ber matchuni of system-reference unigram pairs
where both their lemma and POS-tag match. To find
matching pairs, we proceed in a left-to-right fashion
2http://www.cis.upenn.edu/ treebank/tokenizer.sed
3http://wordnet.princeton.edu/man/morph.3WN
r1 r2 r3
00.5
0.750.75
0.75
11
1
s3s2s1
0.5
r1 r2 r3
0.75
11
s3s1 s2
Figure 1: Bipartite matching.
(in both strings). We first compare the first system
unigram to the first reference unigram, then to the
second reference unigram, and so on until we find a
match. If there is a match, we increment matchuni
by 1 and remove this pair of system-reference un-
igrams from further consideration (removed items
will not be matched again subsequently). Then, we
move on to the second system unigram and try to
match it against the reference unigrams, once again
proceeding in a left-to-right fashion. We continue
this process until we reach the last system unigram.
To determine the number matchbi of bi-
gram matches, a system bigram (lsipsi , lsi+1psi+1)
matches a reference bigram (lripri , lri+1pri+1) if
lsi = lri , psi = pri , lsi+1 = lri+1 , and psi+1 = pri+1 .
For trigrams, we similarly determine matchtri by
counting the number of trigram matches.
Lemma match For the remaining set of n-grams
that are not yet matched, we now relax our matching
criteria by allowing a match if their corresponding
lemmas match. That is, a system unigram (lsipsi)
matches a reference unigram (lripri) if lsi = lri .
In the case of bigrams, the matching conditions are
lsi = lri and lsi+1 = lri+1 . The conditions for tri-
grams are similar. Once again, we find matches in a
left-to-right fashion. We add the number of unigram,
bigram, and trigram matches found during this phase
to matchuni, matchbi, and matchtri respectively.
Bipartite graph matching For the remaining n-
grams that are not matched so far, we try to match
them by constructing bipartite graphs. During this
phase, we will construct three bipartite graphs, one
58
each for the remaining set of unigrams, bigrams, and
trigrams.
Using bigrams to illustrate, we construct a
weighted complete bipartite graph, where each edge
e connecting a pair of system-reference bigrams has
a weight w(e), indicating the degree of similarity
between the bigrams connected. Note that, without
loss of generality, if the number of system nodes and
reference nodes (bigrams) are not the same, we can
simply add dummy nodes with connecting edges of
weight 0 to obtain a complete bipartite graph with
equal number of nodes on both sides.
In an n-gram bipartite graph, the similarity score,
or the weight w(e) of the edge e connecting a system
n-gram (ls1ps1 , . . . , lsnpsn) and a reference n-gram
(lr1pr1 , . . . , lrnprn) is calculated as follows:
Si =
I(psi , pri) + Syn(lsi , lri)
2
w(e) = 1n
n
?
i=1
Si
where I(psi , pri) evaluates to 1 if psi = pri , and
0 otherwise. The function Syn(lsi , lri) checks
whether lsi is a synonym of lri . To determine this,
we first obtain the set WNsyn(lsi) of WordNet syn-
onyms for lsi and the set WNsyn(lri) of WordNet
synonyms for lri . Then,
Syn(lsi , lri) =
?
?
?
1, WNsyn(lsi) ? WNsyn(lri)
6= ?
0, otherwise
In gathering the set WNsyn for a word, we gather
all the synonyms for all its senses and do not re-
strict to a particular POS category. Further, if we
are comparing bigrams or trigrams, we impose an
additional condition: Si 6= 0, for 1 ? i ? n, else we
will set w(e) = 0. This captures the intuition that
in matching a system n-gram against a reference n-
gram, where n > 1, we require each system token
to have at least some degree of similarity with the
corresponding reference token.
In the top half of Figure 1, we show an example
of a complete bipartite graph, constructed for a set
of three system bigrams (s1, s2, s3) and three refer-
ence bigrams (r1, r2, r3), and the weight of the con-
necting edge between two bigrams represents their
degree of similarity.
Next, we aim to find a maximum weight match-
ing (or alignment) between the bigrams such that
each system (reference) bigram is connected to ex-
actly one reference (system) bigram. This maxi-
mum weighted bipartite matching problem can be
solved in O(n3) time (where n refers to the number
of nodes, or vertices in the graph) using the Kuhn-
Munkres algorithm (Kuhn, 1955; Munkres, 1957).
The bottom half of Figure 1 shows the resulting
maximum weighted bipartite graph, where the align-
ment represents the maximum weight matching, out
of all possible alignments.
Once we have solved and obtained a maximum
weight matching M for the bigram bipartite graph,
we sum up the weights of the edges to obtain the
weight of the matching M : w(M) =
?
e?M w(e),
and add w(M) to matchbi. From the unigram
and trigram bipartite graphs, we similarly calculate
their respective w(M) and add to the corresponding
matchuni and matchtri.
Based on matchuni, matchbi, and matchtri, we
calculate their corresponding precision P and re-
call R, from which we obtain their respective Fmean
scores via Equation 1. Using bigrams for illustra-
tion, we calculate its P and R as:
P = matchbi
no. of bigrams in system translation
R = matchbi
no. of bigrams in reference translation
4.2 Dependency Relations
Besides matching a pair of system-reference sen-
tences based on the surface form of words, previ-
ous work such as (Gimenez and Marquez, 2007) and
(Rajman and Hartley, 2002) had shown that deeper
linguistic knowledge such as semantic roles and syn-
tax can be usefully exploited.
In the previous subsection, we describe our
method of using bipartite graphs for matching of n-
grams found in a sentence pair. This use of bipartite
graphs, however, is a very general framework to ob-
tain an optimal alignment of the corresponding ?in-
formation items? contained within a sentence pair.
Hence, besides matching based on n-gram strings,
we can also match other ?information items?, such
as dependency relations.
59
Metric Adequacy Fluency Rank Constituent Average
MAXSIMn+d 0.780 0.827 0.875 0.760 0.811
MAXSIMn 0.804 0.845 0.893 0.766 0.827
Semantic-role 0.774 0.839 0.804 0.742 0.790
ParaEval-recall 0.712 0.742 0.769 0.798 0.755
METEOR 0.701 0.719 0.746 0.670 0.709
BLEU 0.690 0.722 0.672 0.603 0.672
Table 1: Overall correlations on the Europarl and News Commentary datasets. The ?Semantic-role overlap? metric
is abbreviated as ?Semantic-role?. Note that each figure above represents 6 translation tasks: the Europarl and News
Commentary datasets each with 3 language pairs (German-English, Spanish-English, French-English).
In our work, we train the MSTParser4 (McDon-
ald et al, 2005) on the Penn Treebank Wall Street
Journal (WSJ) corpus, and use it to extract depen-
dency relations from a sentence. Currently, we fo-
cus on extracting only two relations: subject and
object. For each relation (ch, dp, pa) extracted, we
note the child lemma ch of the relation (often a
noun), the relation type dp (either subject or ob-
ject), and the parent lemma pa of the relation (often
a verb). Then, using the system relations and ref-
erence relations extracted from a system-reference
sentence pair, we similarly construct a bipartite
graph, where each node is a relation (ch, dp, pa).
We define the weight w(e) of an edge e between a
system relation (chs, dps, pas) and a reference rela-
tion (chr, dpr, par) as follows:
Syn(chs, chr) + I(dps, dpr) + Syn(pas, par)
3
where functions I and Syn are defined as in the pre-
vious subsection. Also, w(e) is non-zero only if
dps = dpr. After solving for the maximum weight
matching M , we divide w(M) by the number of sys-
tem relations extracted to obtain a precision score P ,
and divide w(M) by the number of reference rela-
tions extracted to obtain a recall score R. P and R
are then similarly combined into a Fmean score for
the sentence pair. To compute the similarity score
when incorporating dependency relations, we aver-
age the Fmean scores for unigrams, bigrams, tri-
grams, and dependency relations.
5 Results
To evaluate our metric, we conduct experiments on
datasets from the ACL-07 MT workshop and NIST
4Available at: http://sourceforge.net/projects/mstparser
Europarl
Metric Adq Flu Rank Con Avg
MAXSIMn+d 0.749 0.786 0.857 0.651 0.761
MAXSIMn 0.749 0.786 0.857 0.651 0.761
Semantic-role 0.815 0.854 0.759 0.612 0.760
ParaEval-recall 0.701 0.708 0.737 0.772 0.730
METEOR 0.726 0.741 0.770 0.558 0.699
BLEU 0.803 0.822 0.699 0.512 0.709
Table 2: Correlations on the Europarl dataset.
Adq=Adequacy, Flu=Fluency, Con=Constituent, and
Avg=Average.
News Commentary
Metric Adq Flu Rank Con Avg
MAXSIMn+d 0.812 0.869 0.893 0.869 0.861
MAXSIMn 0.860 0.905 0.929 0.881 0.894
Semantic-role 0.734 0.824 0.848 0.871 0.819
ParaEval-recall 0.722 0.777 0.800 0.824 0.781
METEOR 0.677 0.698 0.721 0.782 0.720
BLEU 0.577 0.622 0.646 0.693 0.635
Table 3: Correlations on the News Commentary dataset.
MT 2003 evaluation exercise.
5.1 ACL-07 MT Workshop
The ACL-07 MT workshop evaluated the transla-
tion quality of MT systems on various translation
tasks, and also measured the correlation (with hu-
man judgement) of 11 automatic MT evaluation
metrics. The workshop used a Europarl dataset and a
News Commentary dataset, where each dataset con-
sisted of English sentences (2,000 English sentences
for Europarl and 2,007 English sentences for News
Commentary) and their translations in various lan-
guages. As part of the workshop, correlations of
the automatic metrics were measured for the tasks
60
of translating German, Spanish, and French into En-
glish. Hence, we will similarly measure the correla-
tion of MAXSIM on these tasks.
5.1.1 Evaluation Criteria
For human evaluation of the MT submissions,
four different criteria were used in the workshop:
Adequacy (how much of the original meaning is ex-
pressed in a system translation), Fluency (the trans-
lation?s fluency), Rank (different translations of a
single source sentence are compared and ranked
from best to worst), and Constituent (some con-
stituents from the parse tree of the source sentence
are translated, and human judges have to rank these
translations).
During the workshop, Kappa values measured for
inter- and intra-annotator agreement for rank and
constituent are substantially higher than those for
adequacy and fluency, indicating that rank and con-
stituent are more reliable criteria for MT evaluation.
5.1.2 Correlation Results
We follow the ACL-07 MT workshop process of
converting the raw scores assigned by an automatic
metric to ranks and then using the Spearman?s rank
correlation coefficient to measure correlation.
During the workshop, only three automatic met-
rics (Semantic-role overlap, ParaEval-recall, and
METEOR) achieve higher correlation than BLEU.
We gather the correlation results of these metrics
from the workshop paper (Callison-Burch et al,
2007), and show in Table 1 the overall correlations
of these metrics over the Europarl and News Com-
mentary datasets. In the table, MAXSIMn represents
using only n-gram information (Section 4.1) for our
metric, while MAXSIMn+d represents using both n-
gram and dependency information. We also show
the breakdown of the correlation results into the Eu-
roparl dataset (Table 2) and the News Commentary
dataset (Table 3). In all our results for MAXSIM
in this paper, we follow METEOR and use ?=0.9
(weighing recall more than precision) in our calcu-
lation of Fmean via Equation 1, unless otherwise
stated.
The results in Table 1 show that MAXSIMn and
MAXSIMn+d achieve overall average (over the four
criteria) correlations of 0.827 and 0.811 respec-
tively. Note that these results are substantially
Metric Adq Flu Avg
MAXSIMn+d 0.943 0.886 0.915
MAXSIMn 0.829 0.771 0.800
METEOR (optimized) 1.000 0.943 0.972
METEOR 0.943 0.886 0.915
BLEU 0.657 0.543 0.600
Table 4: Correlations on the NIST MT 2003 dataset.
higher than BLEU, and in particular higher than the
best performing Semantic-role overlap metric in the
ACL-07 MT workshop. Also, Semantic-role over-
lap requires more processing steps (such as base
phrase chunking, named entity tagging, etc.) than
MAXSIM. For future work, we could experiment
with incorporating semantic-role information into
our current framework. We note that the ParaEval-
recall metric achieves higher correlation on the con-
stituent criterion, which might be related to the fact
that both ParaEval-recall and the constituent crite-
rion are based on phrases: ParaEval-recall tries to
match phrases, and the constituent criterion is based
on judging translations of phrases.
5.2 NIST MT 2003 Dataset
We also conduct experiments on the test data
(LDC2006T04) of NIST MT 2003 Chinese-English
translation task. For this dataset, human judgements
are available on adequacy and fluency for six sys-
tem submissions, and there are four English refer-
ence translation texts.
Since implementations of the BLEU and ME-
TEOR metrics are publicly available, we score
the system submissions using BLEU (version 11b
with its default settings), METEOR, and MAXSIM,
showing the resulting correlations in Table 4. For
METEOR, when used with its originally proposed
parameter values of (?=0.9, ?=3.0, ?=0.5), which
the METEOR researchers mentioned were based on
some early experimental work (Banerjee and Lavie,
2005), we obtain an average correlation value of
0.915, as shown in the row ?METEOR?. In the re-
cent work of (Lavie and Agarwal, 2007), the val-
ues of these parameters were tuned to be (?=0.81,
?=0.83, ?=0.28), based on experiments on the NIST
2003 and 2004 Arabic-English evaluation datasets.
When METEOR was run with these new parame-
ter values, it returned an average correlation value of
61
0.972, as shown in the row ?METEOR (optimized)?.
MAXSIM using only n-gram information
(MAXSIMn) gives an average correlation value
of 0.800, while adding dependency information
(MAXSIMn+d) improves the correlation value to
0.915. Note that so far, the parameters of MAXSIM
are not optimized and we simply perform uniform
averaging of the different n-grams and dependency
scores. Under this setting, the correlation achieved
by MAXSIM is comparable to that achieved by
METEOR.
6 Future Work
In our current work, the parameters of MAXSIM are
as yet un-optimized. We found that by setting ?=0.7,
MAXSIMn+d could achieve a correlation of 0.972
on the NIST MT 2003 dataset. Also, we have barely
exploited the potential of weighted similarity match-
ing. Possible future directions include adding se-
mantic role information, using the distance between
item pairs based on the token position within each
sentence as additional weighting consideration, etc.
Also, we have seen that dependency relations help to
improve correlation on the NIST dataset, but not on
the ACL-07 MT workshop datasets. Since the accu-
racy of dependency parsers is not perfect, a possible
future work is to identify when best to incorporate
such syntactic information.
7 Conclusion
In this paper, we present MAXSIM, a new auto-
matic MT evaluation metric that computes a simi-
larity score between corresponding items across a
sentence pair, and uses a bipartite graph to obtain
an optimal matching between item pairs. This gen-
eral framework allows us to use arbitrary similarity
functions between items, and to incorporate differ-
ent information in our comparison. When evaluated
for correlation with human judgements, MAXSIM
achieves superior results when compared to current
automatic MT evaluation metrics.
References
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and/or Summarization, ACL05, pages
65?72.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (meta-) evaluation of machine
translation. In Proceedings of the Second Workshop on
Statistical Machine Translation, ACL07, pages 136?
158.
J. Gimenez and L. Marquez. 2007. Linguistic features
for automatic evaluation of heterogenous MT systems.
In Proceedings of the Second Workshop on Statistical
Machine Translation, ACL07, pages 256?264.
H. W. Kuhn. 1955. The hungarian method for the assign-
ment problem. Naval Research Logistic Quarterly,
2(1):83?97.
A. Lavie and A. Agarwal. 2007. METEOR: An auto-
matic metric for MT evaluation with high levels of cor-
relation with human judgments. In Proceedings of the
Second Workshop on Statistical Machine Translation,
ACL07, pages 228?231.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL05, pages 91?98.
I. D. Melamed, R. Green, and J. P. Turian. 2003. Preci-
sion and recall of machine translation. In Proceedings
of HLT-NAACL03, pages 61?63.
G. A. Miller. 1990. WordNet: An on-line lexi-
cal database. International Journal of Lexicography,
3(4):235?312.
J. Munkres. 1957. Algorithms for the assignment and
transportation problems. Journal of the Society for In-
dustrial and Applied Mathematics, 5(1):32?38.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In Proceedings of ACL02, pages 311?318.
M. Rajman and A. Hartley. 2002. Automatic ranking of
MT systems. In Proceedings of LREC02, pages 1247?
1253.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP96,
pages 133?142.
C. Rijsbergen. 1979. Information Retrieval. Butter-
worths, London, UK, 2nd edition.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
Proceedings of HLT/EMNLP05, pages 73?80.
L. Zhou, C. Y. Lin, and E. Hovy. 2006. Re-evaluating
machine translation results with paraphrase support.
In Proceedings of EMNLP06, pages 77?84.
62
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 54?58,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 11: English Lexical Sample Task
via English-Chinese Parallel Text
Hwee Tou Ng and Yee Seng Chan
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
{nght, chanys}@comp.nus.edu.sg
Abstract
We made use of parallel texts to gather train-
ing and test examples for the English lexi-
cal sample task. Two tracks were organized
for our task. The first track used examples
gathered from an LDC corpus, while the
second track used examples gathered from
a Web corpus. In this paper, we describe
the process of gathering examples from the
parallel corpora, the differences with similar
tasks in previous SENSEVAL evaluations,
and present the results of participating sys-
tems.
1 Introduction
As part of the SemEval-2007 evaluation exercise, we
organized an English lexical sample task for word
sense disambiguation (WSD), where the sense-
annotated examples were semi-automatically gath-
ered from word-aligned English-Chinese parallel
texts. Two tracks were organized for this task, each
gathering data from a different corpus. In this paper,
we describe our motivation for organizing the task,
our task framework, and the results of participants.
Past research has shown that supervised learning
is one of the most successful approaches to WSD.
However, this approach involves the collection of
a large text corpus in which each ambiguous word
has been annotated with the correct sense to serve as
training data. Due to the expensive annotation pro-
cess, only a handful of manually sense-tagged cor-
pora are available.
An effort to alleviate the training data bottle-
neck is the Open Mind Word Expert (OMWE)
project (Chklovski and Mihalcea, 2002) to collect
sense-tagged data from Internet users. Data gath-
ered through the OMWE project were used in the
SENSEVAL-3 English lexical sample task. In that
task, WordNet-1.7.1 was used as the sense inven-
tory for nouns and adjectives, while Wordsmyth1
was used as the sense inventory for verbs.
Another source of potential training data is par-
allel texts. Our past research in (Ng et al, 2003;
Chan and Ng, 2005) has shown that examples gath-
ered from parallel texts are useful for WSD. Briefly,
after manually assigning appropriate Chinese trans-
lations to each sense of an English word, the English
side of a word-aligned parallel text can then serve as
the training data, as they are considered to have been
disambiguated and ?sense-tagged? by the appropri-
ate Chinese translations.
Using the above approach, we gathered the train-
ing and test examples for our task from parallel texts.
Note that our examples are collected without manu-
ally annotating each individual ambiguous word oc-
currence, allowing us to gather our examples in a
much shorter time. This contrasts with the setting of
the English lexical sample task in previous SENSE-
VAL evaluations. In the English lexical sample task
of SENSEVAL-2, the sense tagged data were cre-
ated through manual annotation by trained lexicog-
raphers. In SENSEVAL-3, the data were gathered
through manual sense annotation by Internet users.
In the next section, we describe in more detail
the process of gathering examples from parallel texts
and the two different parallel corpora we used. We
then give a brief description of each of the partici-
1http://www.wordsmyth.net
54
pating systems. In Section 4, we present the results
obtained by the participants, before concluding in
Section 5.
2 Gathering Examples from Parallel
Corpora
To gather examples from parallel corpora, we fol-
lowed the approach in (Ng et al, 2003). Briefly, af-
ter ensuring the corpora were sentence-aligned, we
tokenized the English texts and performed word seg-
mentation on the Chinese texts (Low et al, 2005).
We then made use of the GIZA++ software (Och and
Ney, 2000) to perform word alignment on the paral-
lel corpora. Then, we assigned some possible Chi-
nese translations to each sense of an English word
w. From the word alignment output of GIZA++, we
selected those occurrences of w which were aligned
to one of the Chinese translations chosen. The En-
glish side of these occurrences served as training
data for w, as they were considered to have been dis-
ambiguated and ?sense-tagged? by the appropriate
Chinese translations. The English half of the par-
allel texts (each ambiguous English word and its 3-
sentence context) were used as the training and test
material to set up our English lexical sample task.
Note that in our approach, the sense distinction
is decided by the different Chinese translations as-
signed to each sense of a word. This is thus
similar to the multilingual lexical sample task in
SENSEVAL-3 (Chklovski et al, 2004), except that
our training and test examples are collected with-
out manually annotating each individual ambiguous
word occurrence. The average time needed to assign
Chinese translations for one noun and one adjective
is 20 minutes and 25 minutes respectively. This is
a relatively short time, compared to the effort other-
wise needed to manually sense annotate individual
word occurrences. Also, once the Chinese transla-
tions are assigned, more examples can be automat-
ically gathered as more parallel texts become avail-
able.
We note that frequently occurring words are usu-
ally highly polysemous and hard to disambiguate.
To maximize the benefits of our work, we gathered
training data from parallel texts for a set of most fre-
quently occurring noun and adjective types in the
Brown Corpus. Also, similar to the SENSEVAL-3
Dataset Avg. no. Avg. no. of examples
of senses Training Test
LDC noun 5.2 197.6 98.5
LDC adjective 3.9 125.6 62.9
Web noun 3.5 182.0 91.3
Web adjective 2.8 88.8 44.6
Table 1: Average number of senses, training exam-
ples, and test examples per word.
English lexical sample task, we used WordNet-1.7.1
as our sense inventory.
2.1 LDC Corpus
We have two tracks for this task, each track using a
different corpus. The first corpus is the Chinese En-
glish News Magazine Parallel Text (LDC2005T10),
which is an English-Chinese parallel corpus avail-
able from the Linguistic Data Consortium (LDC).
From this parallel corpus, we gathered examples
for 50 English words (25 nouns and 25 adjectives)
using the method described above. From the gath-
ered examples of each word, we randomly selected
training and test examples, where the number of
training examples is about twice the number of test
examples.
The rows LDC noun and LDC adjective in Table
1 give some statistics about the examples. For in-
stance, each noun has an average of 197.6 training
and 98.5 test examples and these examples repre-
sent an average of 5.2 senses per noun.2 Participants
taking part in this track need to have access to this
LDC corpus in order to access the training and test
material in this track.
2.2 Web Corpus
Since not all interested participants may have access
to the LDC corpus described in the previous sub-
section, the second track of this task makes use of
English-Chinese documents gathered from the URL
pairs given by the STRAND Bilingual Databases.3
STRAND (Resnik and Smith, 2003) is a system that
acquires document pairs in parallel translation auto-
matically from the Web. Using this corpus, we gath-
ered examples for 40 English words (20 nouns and
2Only senses present in the examples are counted.
3http://www.umiacs.umd.edu/?resnik/strand
55
20 adjectives).
The rows Web noun and Web adjective in Table 1
show that we selected an average of 182.0 training
and 91.3 test examples for each noun and these ex-
amples represent an average of 3.5 senses per noun.
We note that the average number of senses per word
for the Web corpus is slightly lower than that of the
LDC corpus.
2.3 Annotation Accuracy
To measure the annotation accuracy of examples
gathered from the LDC corpus, we examined a ran-
dom selection of 100 examples each from 5 nouns
and 5 adjectives. From these 1,000 examples, we
measured a sense annotation accuracy of 84.7%.
These 10 words have an average of 8.6 senses per
word in the WordNet-1.7.1 sense inventory. As de-
scribed in (Ng et al, 2003), when several senses
of an English word are translated by the same Chi-
nese word, we can collapse these senses to obtain a
coarser-grained, lumped sense inventory. If we do
this and measure the sense annotation accuracy with
respect to a coarser-grained, lumped sense inventory,
these 10 words will have an average of 6.5 senses per
word and an annotation accuracy of 94.7%.
For the Web corpus, we similarly examined a ran-
dom selection of 100 examples each from 5 nouns
and 5 adjectives. These 10 words have an average of
6.5 senses per word in WordNet-1.7.1 and the 1,000
examples have an average sense annotation accuracy
of 85.0%. After sense collapsing, annotation ac-
curacy is 95.3% with an average of 4.8 senses per
word.
2.4 Training and Test Data from Different
Documents
In our previous work (Ng et al, 2003), we conducted
experiments on the nouns of SENSEVAL-2 English
lexical sample task. We found that there were cases
where the same document contributed both training
and test examples and this inflated the WSD accu-
racy figures. To avoid this, during our preparation
of the LDC and Web data, we made sure that a doc-
ument contributed only either training or test exam-
ples, but not both.
3 Participating Systems
Three teams participated in the Web corpus track
of our task, with each team employing one system.
There were no participants in the LDC corpus track,
possibly due to the licensing issues involved. All
participating systems employed supervised learning
and only used the training examples provided by us.
3.1 CITYU-HIF
The CITYU-HIF team from the City University of
Hong Kong trained a naive Bayes (NB) classifier
for each target word to be disambiguated, using
knowledge sources such as parts-of-speech (POS) of
neighboring words and single words in the surround-
ing context. They also experimented with using dif-
ferent sets of features for each target word.
3.2 HIT-IR-WSD
The system submitted by the HIT-IR-WSD team
from Harbin Institute of Technology used Support
Vector Machines (SVM) with a linear kernel func-
tion as the learning algorithm. Knowledge sources
used included POS of surrounding words, local col-
locations, single words in the surrounding context,
and syntactic relations.
3.3 PKU
The system submitted by the PKU team from Peking
University used a combination of SVM and maxi-
mum entropy classifiers. Knowledge sources used
included POS of surrounding words, local colloca-
tions, and single words in the surrounding context.
Feature selection was done by ignoring word fea-
tures with certain associated POS tags and by se-
lecting the subset of features based on their entropy
values.
4 Results
As all participating systems gave only one answer
for each test example, recall equals precision and
we will only report micro-average recall on the Web
corpus track in this section.
Table 2 gives the overall results obtained by each
of the systems when evaluated on all the test exam-
ples of the Web corpus. We note that all the par-
ticipants obtained scores which exceed the baseline
heuristic of tagging all test examples with the most
56
System ID Contact author Learning algorithm Score
HIT-IR-WSD Yuhang Guo, <astronaut@ir.hit.edu.cn> SVM 0.819
PKU Peng Jin, <jandp@pku.edu.cn> SVM and maximum entropy 0.815
CITYU-HIF Oi Yee Kwong, <rlolivia@cityu.edu.hk> NB 0.753
MFS ? Most frequent sense baseline 0.689
Table 2: Overall micro-average scores of the participants and the most frequent sense (MFS) baseline.
Noun MFS CITYU-HIF HIT-IR-WSD PKU
age 0.486 0.643 0.743 0.700
area 0.480 0.693 0.773 0.773
body 0.872 0.897 0.910 0.923
change 0.411 0.400 0.578 0.611
director 0.580 0.890 0.960 0.960
experience 0.830 0.830 0.880 0.840
future 0.889 0.889 0.990 0.990
interest 0.308 0.165 0.813 0.780
issue 0.651 0.711 0.892 0.855
life 0.820 0.830 0.860 0.740
material 0.719 0.719 0.781 0.641
need 0.907 0.907 0.918 0.918
performance 0.410 0.570 0.690 0.700
program 0.590 0.590 0.730 0.690
report 0.870 0.840 0.880 0.870
system 0.510 0.700 0.610 0.730
time 0.455 0.673 0.733 0.693
today 0.800 0.750 0.800 0.780
water 0.882 0.921 0.868 0.895
work 0.644 0.743 0.842 0.891
Micro-avg 0.656 0.719 0.813 0.802
Table 3: Micro-average scores of the most frequent
sense baseline and the various participants on each
noun.
frequent sense (MFS) in the training data. This sug-
gests that the Chinese translations assigned to senses
of the ambiguous words are appropriate and provide
sense distinctions which are clear enough for effec-
tive classifiers to be learned.
In Table 3 and Table 4, we show the scores ob-
tained by each system on each of the 20 nouns and
20 adjectives. For comparison purposes, we also
show the corresponding MFS score of each word.
Paired t-test on the results of the top two systems
show no significant difference between them.
5 Conclusion
We organized an English lexical sample task using
examples gathered from parallel texts. Unlike the
English lexical task of previous SENSEVAL evalua-
tions where each example is manually annotated, we
Adjective MFS CITYU-HIF HIT-IR-WSD PKU
ancient 0.778 0.667 0.778 0.741
bad 0.857 0.857 0.905 0.905
common 0.533 0.567 0.533 0.633
early 0.769 0.846 0.769 0.769
educational 0.911 0.911 0.911 0.911
free 0.760 0.792 0.854 0.917
high 0.630 0.926 0.815 0.852
human 0.872 0.987 0.962 0.962
little 0.450 0.750 0.650 0.650
long 0.667 0.690 0.786 0.714
major 0.870 0.902 0.880 0.913
medical 0.738 0.787 0.800 0.725
national 0.267 0.467 0.667 0.700
new 0.441 0.441 0.529 0.559
present 0.875 0.917 0.875 0.875
rare 0.727 0.818 0.727 0.909
serious 0.879 0.879 0.879 0.879
simple 0.795 0.818 0.864 0.864
small 0.714 0.929 0.893 0.929
third 0.888 0.988 0.963 0.963
Micro-avg 0.757 0.823 0.831 0.842
Table 4: Micro-average scores of the most frequent
sense baseline and the various participants on each
adjective.
only need to assign appropriate Chinese translations
to each sense of a word. Once this is done, we auto-
matically gather training and test examples from the
parallel texts. All the participating systems of our
task obtain results that are significantly better than
the most frequent sense baseline.
6 Acknowledgements
Yee Seng Chan is supported by a Singapore Millen-
nium Foundation Scholarship (ref no. SMF-2004-
1076).
References
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling
up word sense disambiguation via parallel texts. In
Proceedings of AAAI05, pages 1037?1042, Pittsburgh,
Pennsylvania, USA.
57
Timothy Chklovski and Rada Mihalcea. 2002. Building
a sense tagged corpus with Open Mind Word Expert.
In Proceedings of ACL02 Workshop on Word Sense
Disambiguation: Recent Successes and Future Direc-
tions, pages 116?122, Philadelphia, USA.
Timothy Chklovski, Rada Mihalcea, Ted Pedersen, and
Amruta Purandare. 2004. The SENSEVAL-3 multi-
lingual English-Hindi lexical sample task. In Proceed-
ings of SENSEVAL-3, pages 5?8, Barcelona, Spain.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A
maximum entropy approach to Chinese word segmen-
tation. In Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing, pages 161?
164, Jeju Island, Korea.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
An empirical study. In Proceedings of ACL03, pages
455?462, Sapporo, Japan.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceeedings of ACL00,
pages 440?447, Hong Kong.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
58
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 253?256,
Prague, June 2007. c?2007 Association for Computational Linguistics
NUS-PT: Exploiting Parallel Texts for
Word Sense Disambiguation in the English All-Words Tasks
Yee Seng Chan and Hwee Tou Ng and Zhi Zhong
Department of Computer Science, National University of Singapore
3 Science Drive 2, Singapore 117543
{chanys, nght, zhongzhi}@comp.nus.edu.sg
Abstract
We participated in the SemEval-2007
coarse-grained English all-words task
and fine-grained English all-words task.
We used a supervised learning approach
with SVM as the learning algorithm. The
knowledge sources used include local col-
locations, parts-of-speech, and surrounding
words. We gathered training examples
from English-Chinese parallel corpora,
SEMCOR, and DSO corpus. While the
fine-grained sense inventory of WordNet
was used to train our system employed for
the fine-grained English all-words task, our
system employed for the coarse-grained
English all-words task was trained with the
coarse-grained sense inventory released by
the task organizers. Our scores (for both
recall and precision) are 0.825 and 0.587
for the coarse-grained English all-words
task and fine-grained English all-words task
respectively. These scores put our systems
in the first place for the coarse-grained
English all-words task1 and the second
place for the fine-grained English all-words
task.
1 Introduction
In this paper, we describe the systems we devel-
oped for the coarse-grained English all-words task
1A system developed by one of the task organizers of the
coarse-grained English all-words task gave the highest over-
all score for the coarse-grained English all-words task, but this
score is not considered part of the official scores.
and fine-grained English all-words task of SemEval-
2007. In the coarse-grained English all-words task,
systems have to perform word sense disambiguation
(WSD) of all content words (noun, adjective, verb,
and adverb) occurring in five documents, using a
coarse-grained version of the WordNet sense inven-
tory. In the fine-grained English all-words task, sys-
tems have to predict the correct sense of verbs and
head nouns of the verb arguments occurring in three
documents, according to the fine-grained sense in-
ventory of WordNet.
Results from previous SENSEVAL English all-
words task have shown that supervised learning
gives the best performance. Further, the best per-
forming system in SENSEVAL-3 English all-words
task (Decadt et al, 2004) used training data gathered
from multiple sources, highlighting the importance
of having a large amount of training data. Hence,
besides gathering examples from the widely used
SEMCOR corpus, we also gathered training exam-
ples from 6 English-Chinese parallel corpora and the
DSO corpus (Ng and Lee, 1996).
We developed 2 separate systems; one for each
task. For both systems, we performed supervised
word sense disambiguation based on the approach
of (Lee and Ng, 2002) and using Support Vector
Machines (SVM) as our learning algorithm. The
knowledge sources used include local collocations,
parts-of-speech (POS), and surrounding words. Our
system employed for the coarse-grained English all-
words task was trained with the coarse-grained sense
inventory released by the task organizers, while our
system employed for the fine-grained English all-
words task was trained with the fine-grained sense
253
inventory of WordNet.
In the next section, we describe the different
sources of training data used. In Section 3, we de-
scribe the knowledge sources used by the learning
algorithm. In Section 4, we present our official eval-
uation results, before concluding in Section 5.
2 Training Corpora
We gathered training examples from parallel cor-
pora, SEMCOR (Miller et al, 1994), and the DSO
corpus. In this section, we describe these corpora
and how examples gathered from them are combined
to form the training data used by our systems. As
these data sources use an earlier version of the Word-
Net sense inventory as compared to the test data of
the two tasks we participated in, we also discuss the
need to map between different versions of WordNet.
2.1 Parallel Text
Research in (Ng et al, 2003; Chan and Ng, 2005)
has shown that examples gathered from parallel texts
are useful for WSD. In this evaluation, we gath-
ered training data from 6 English-Chinese parallel
corpora (Hong Kong Hansards, Hong Kong News,
Hong Kong Laws, Sinorama, Xinhua News, and
English translation of Chinese Treebank), available
from the Linguistic Data Consortium (LDC). To
gather examples from these parallel corpora, we fol-
lowed the approach in (Ng et al, 2003). Briefly, af-
ter ensuring the corpora were sentence-aligned, we
tokenized the English texts and performed word seg-
mentation on the Chinese texts (Low et al, 2005).
We then made use of the GIZA++ software (Och and
Ney, 2000) to perform word alignment on the paral-
lel corpora. Then, we assigned some possible Chi-
nese translations to each sense of an English word
w. From the word alignment output of GIZA++, we
selected those occurrences of w which were aligned
to one of the Chinese translations chosen. The En-
glish side of these occurrences served as training
data for w, as they were considered to have been dis-
ambiguated and ?sense-tagged? by the appropriate
Chinese translations.
We note that frequently occurring words are usu-
ally highly polysemous and hard to disambiguate.
To maximize the benefits of using parallel texts, we
gathered training data from parallel texts for the set
of most frequently occurring noun, adjective, and
verb types in the Brown Corpus (BC). These word
types (730 nouns, 326 adjectives, and 190 verbs)
represent 60% of the noun, adjective, and verb to-
kens in BC.
2.2 SEMCOR
The SEMCOR corpus (Miller et al, 1994) is one
of the few currently available, manually sense-
annotated corpora for WSD. It is widely used by
various systems which participated in the English
all-words task of SENSEVAL-2 and SENSEVAL-3,
including one of the top performing teams (Hoste
et al, 2001; Decadt et al, 2004) which had per-
formed consistently well in both SENSEVAL all-
words tasks. Hence, we also gathered examples
from SEMCOR as part of our training data.
2.3 DSO Corpus
Besides SEMCOR, the DSO corpus (Ng and Lee,
1996) also contains manually annotated examples
for WSD. As part of our training data, we gath-
ered training examples for each of the 70 verb types
present in the DSO corpus.
2.4 Combination of Training Data
Similar to the top performing supervised systems
of previous SENSEVAL all-words tasks, we used
the annotated examples available from the SEMCOR
corpus as part of our training data. In gathering ex-
amples from parallel texts, a maximum of 1,000 ex-
amples were gathered for each of the frequently oc-
curring noun and adjective types, while a maximum
of 500 examples were gathered for each of the fre-
quently occurring verb types. In addition, a max-
imum of 500 examples were gathered for each of
the verb types present in the DSO corpus. For each
word, the examples from the parallel corpora and
DSO corpus were randomly chosen but adhering to
the sense distribution (proportion of each sense) of
that word in the SEMCOR corpus.
2.5 Sense Inventory
The test data of the two SemEval-2007 tasks we par-
ticipated in are based on the WordNet-2.1 sense in-
ventory. However, the examples we gathered from
the parallel texts and the SEMCOR corpus are based
on the WordNet-1.7.1 sense inventory. Hence, there
254
is a need to map these examples from WordNet-1.7.1
to WordNet-2.1 sense inventory. For this, we rely
primarily on the WordNet sense mappings automat-
ically generated by the work of (Daude et al, 2000).
To ensure the accuracy of the mappings, we per-
formed some manual corrections of our own, focus-
ing on the set of most frequently occurring nouns,
adjectives, and verbs. For the verb examples from
the DSO corpus which are based on the WordNet-
1.5 sense inventory, we manually mapped them to
WordNet-2.1 senses.
3 WSD System
Following the approach of (Lee and Ng, 2002), we
train an SVM classifier for each word using the
knowledge sources of local collocations, parts-of-
speech (POS), and surrounding words. We omit the
syntactic relation features for efficiency reasons. For
local collocations, we use 11 features: C?1,?1, C1,1,
C?2,?2, C2,2, C?2,?1, C?1,1, C1,2, C?3,?1, C?2,1,
C?1,2, and C1,3, where Ci,j refers to the ordered
sequence of tokens in the local context of an am-
biguous word w. Offsets i and j denote the starting
and ending position (relative to w) of the sequence,
where a negative (positive) offset refers to a token
to its left (right). For parts-of-speech, we use 7 fea-
tures: P?3, P?2, P?1, P0, P1, P2, P3, where P0 is
the POS of w, and P?i (Pi) is the POS of the ith to-
ken to the left (right) of w. For surrounding words,
we consider all unigrams (single words) in the sur-
rounding context of w. These words can be in a dif-
ferent sentence from w.
4 Evaluation
We participated in two tasks of SemEval-2007: the
coarse-grained English all-words task and the fine-
grained English all-words task. In both tasks, when
there is no training data at all for a particular word,
we tag all test examples of the word with its first
sense in WordNet. Since our systems give exactly
one answer for each test example, recall is the same
as precision. Hence we will just report the micro-
average recall in this section.
4.1 Coarse-Grained English All-Words Task
Our system employed for the coarse-grained En-
glish all-words task was trained with the coarse-
English all-words Training data
task SC+DSO SC+DSO+PT
Coarse-grained 0.817 0.825
Fine-grained 0.578 0.587
Table 1: Scores for the coarse-grained English all-
words task and fine-grained English all-words task,
using different sets of training data. SC+DSO
refers to using examples gathered from SEMCOR
and DSO corpus. Similarly, SC+DSO+PT refers to
using examples gathered from SEMCOR, DSO cor-
pus, and parallel texts.
Doc-ID Recall No. of test instances
d001 0.883 368
d002 0.881 379
d003 0.834 500
d004 0.761 677
d005 0.814 345
Table 2: Score of each individual test document, for
the coarse-grained English all-words task.
grained WordNet-2.1 sense inventory released by
the task organizers. We obtained a score of 0.825
in this task, as shown in Table 1 under the column
SC + DSO + PT . It turns out that among the
16 participants of this task, the system which re-
turned the best score was developed by one of the
task organizers. Since the score of this system is
not considered part of the official scores, our score
puts our system in the first position among the par-
ticipants of this task. For comparison, the WordNet
first sense baseline score as calculated by the task
organizers is 0.789. To gauge the contribution of
parallel text examples, we retrained our system us-
ing only examples gathered from the SEMCOR and
DSO corpus. As shown in Table 1 under the col-
umn SC + DSO, this gives a score of 0.817 when
scored against the answer keys released by the task
organizers. Although adding examples from parallel
texts gives only a modest improvement in the scores,
we note that this improvement is achieved from a
relatively small set of word types which are found
to be frequently occurring in BC. Future work can
explore expanding the set of word types by automat-
ing the process of assigning Chinese translations to
each sense of an English word, with the use of suit-
255
able bilingual lexicons.
As part of the evaluation results, the task organiz-
ers also released the scores of our system on each of
the 5 test documents. We show in Table 2 the score
we obtained for each document, along with the to-
tal number of test instances in each document. We
note that our system obtained a relatively low score
on the fourth document, which is a Wikipedia entry
on computer programming. To determine the rea-
son for the low score, we looked through the list of
test words in that document. We noticed that the
noun program has 20 test instances occurring in that
fourth document. From the answer keys released by
the task organizers, all 20 test instances belong to the
sense of ?a sequence of instructions that a computer
can interpret and execute?, which we do not have
any training examples for. Similarly, we noticed that
another noun programming has 27 test instances oc-
curring in the fourth document which belong to the
sense of ?creating a sequence of instructions to en-
able the computer to do something?, which we do
not have any training examples for. Thus, these two
words alone account for 47 of the errors made by our
system in this task, representing 2.1% of the 2,269
test instances of this task.
4.2 Fine-Grained English All-Words Task
Our system employed for the fine-grained English
all-words task was trained on examples tagged
with fine-grained WordNet-2.1 senses (mapped from
WordNet-1.7.1 senses and 1.5 senses as described
earlier). Unlike the coarse-grained English all-
words task, the correct POS tag and lemma of each
test instance are not given in the fine-grained task.
Hence, we used the POS tag from the mrg parse
files released as part of the test data and performed
lemmatization using WordNet. We obtained a score
of 0.587 in this task, as shown in Table 1. This ranks
our system in second position among the 14 partic-
ipants of this task. If we exclude parallel text ex-
amples and train only on examples gathered from
the SEMCOR and DSO corpus, we obtain a score of
0.578.
5 Conclusion
In this paper, we describe the approach taken by
our systems which participated in the coarse-grained
English all-words task and fine-grained English all-
words task of SemEval-2007. Using training exam-
ples gathered from parallel texts, SEMCOR, and the
DSO corpus, we trained supervised WSD systems
with SVM as the learning algorithm. Evaluation re-
sults show that this approach achieves good perfor-
mance in both tasks.
6 Acknowledgements
Yee Seng Chan is supported by a Singapore Millen-
nium Foundation Scholarship (ref no. SMF-2004-
1076).
References
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up word
sense disambiguation via parallel texts. In Proc. of AAAI05,
pages 1037?1042.
Jordi Daude, Lluis Padro, and German Rigau. 2000. Mapping
WordNets using structural information. In Proc. of ACL00,
pages 504?511.
Bart Decadt, Veronique Hoste, Walter Daelemans, and Antal
van den Bosch. 2004. GAMBL, genetic algorithm opti-
mization of memory-based WSD. In Proc. of SENSEVAL-3,
pages 108?112.
Veronique Hoste, Anne Kool, and Walter Daelemans. 2001.
Classifier optimization and combination in the English all
words task. In Proc. of SENSEVAL-2, pages 83?86.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evalu-
ation of knowledge sources and learning algorithms for word
sense disambiguation. In Proc. of EMNLP02, pages 41?48.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A
maximum entropy approach to Chinese word segmentation.
In Proc. of the Fourth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 161?164.
George A. Miller, Martin Chodorow, Shari Landes, Claudia
Leacock, and Robert G. Thomas. 1994. Using a seman-
tic concordance for sense identification. In Proc. of HLT94
Workshop on Human Language Technology, pages 240?243.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating mul-
tiple knowledge sources to disambiguate word sense: An
exemplar-based approach. In Proc. of ACL96, pages 40?47.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Exploit-
ing parallel texts for word sense disambiguation: An empir-
ical study. In Proc. of ACL03, pages 455?462.
Franz Josef Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proc. of ACL00, pages 440?447.
256
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 152?160,
Beijing, August 2010
Exploiting Background Knowledge for Relation Extraction
Yee Seng Chan and Dan Roth
University of Illinois at Urbana-Champaign
{chanys,danr}@illinois.edu
Abstract
Relation extraction is the task of recog-
nizing semantic relations among entities.
Given a particular sentence supervised ap-
proaches to Relation Extraction employed
feature or kernel functions which usu-
ally have a single sentence in their scope.
The overall aim of this paper is to pro-
pose methods for using knowledge and re-
sources that are external to the target sen-
tence, as a way to improve relation ex-
traction. We demonstrate this by exploit-
ing background knowledge such as rela-
tionships among the target relations, as
well as by considering how target rela-
tions relate to some existing knowledge
resources. Our methods are general and
we suggest that some of them could be ap-
plied to other NLP tasks.
1 Introduction
Relation extraction (RE) is the task of detecting
and characterizing semantic relations expressed
between entities in text. For instance, given the
sentence ?Cone, a Kansas City native, was origi-
nally signed by the Royals and broke into the ma-
jors with the team.?, one of the relations we might
want to extract is the employment relation between
the pair of entity mentions ?Cone? and ?Royals?.
RE is important for many NLP applications such
as building an ontology of entities, biomedical in-
formation extraction, and question answering.
Prior work have employed diverse approaches
towards resolving the task. One approach is to
build supervised RE systems using sentences an-
notated with entity mentions and predefined target
relations. When given a new sentence, the RE sys-
tem has to detect and disambiguate the presence of
any predefined relations that might exist between
each of the mention pairs in the sentence. In build-
ing these systems, researchers used a wide variety
of features (Kambhatla, 2004; Zhou et al, 2005;
Jiang and Zhai, 2007). Some of the common fea-
tures used to analyze the target sentence include
the words appearing in the sentence, their part-of-
speech (POS) tags, the syntactic parse of the sen-
tence, and the dependency path between the pair
of mentions. In a related line of work, researchers
have also proposed various kernel functions based
on different structured representations (e.g. de-
pendency or syntactic tree parses) of the target
sentences (Bunescu and Mooney, 2005; Zhou et
al., 2007; Zelenko et al, 2003; Zhang et al,
2006). Additionally, researchers have tried to au-
tomatically extract examples for supervised learn-
ing from resources such as Wikipedia (Weld et al,
2008) and databases (Mintz et al, 2009), or at-
tempted open information extraction (IE) (Banko
et al, 2007) to extract all possible relations.
In this work, we focus on supervised RE. In
prior work, the feature and kernel functions em-
ployed are usually restricted to being defined on
the various representations (e.g. lexical or struc-
tural) of the target sentences. However, in recog-
nizing relations, humans are not thus constrained
and rely on an abundance of implicit world knowl-
edge or background information. What quantifies
as world or background knowledge is rarely ex-
plored in the RE literature and we do not attempt
to provide complete nor precise definitions in this
paper. However, we show that by considering the
relationship between our relations of interest, as
152
well as how they relate to some existing knowl-
edge resources, we improve the performance of
RE. Specifically, the contributions of this paper
are the following:
? When our relations of interest are clustered
or organized in a hierarchical ontology, we
show how to use this information to improve
performance. By defining appropriate con-
straints between the predictions of relations
at different levels of the hierarchy, we obtain
globally coherent predictions as well as im-
proved performance.
? Coreference is a generic relationship that
might exists among entity mentions and we
show how to exploit this information by as-
suming that co-referring mentions have no
other interesting relations. We capture this
intuition by using coreference information to
constraint the predictions of a RE system.
? When characterizing the relationship be-
tween a pair of mentions, one can use a
large encyclopedia such as Wikipedia to in-
fer more knowledge about the two mentions.
In this work, after probabilistically map-
ping mentions to their respective Wikipedia
pages, we check whether the mentions are
related. Another generic relationship that
might exists between a pair of mentions is
whether they have a parent-child relation and
we use this as additional information.
? The sparsity of features (especially lexical
features) is a common problem for super-
vised systems. In this work, we show that
one can make fruitful use of unlabeled data,
by using word clusters automatically gath-
ered from unlabeled texts as a way of gen-
eralizing the lexical features.
? We combine the various relational predic-
tions and background knowledge through a
global inference procedure, which we for-
malize via an Integer Linear Programming
(ILP) framework as a constraint optimization
problem (Roth and Yih, 2007). This allows
us to easily incorporate various constraints
that encode the background knowledge.
Roth and Yih (2004) develop a relation extrac-
tion approach that exploits constraints among en-
tity types and the relations allowed among them.
We extend this view significantly, within a simi-
lar computational framework, to exploit relations
among target relations, background information
and world knowledge, as a way to improve rela-
tion extraction and make globally coherent predic-
tions.
In the rest of this paper, we first describe the
features used in our basic RE system in Section 2.
We then describe how we make use of background
knowledge in Section 3. In Section 4, we show
our experimental results and perform analysis in
Section 5. In Section 6, we discuss related work,
before concluding in Section 7.
2 Relation Extraction System
In this section, we describe the features used in
our basic relation extraction (RE) system. Given
a pair of mentions m1 and m2 occurring within
the same sentence, the system predicts whether
any of the predefined relation holds between the
two mentions. Since relations are usually asym-
metric in nature, hence in all of our experi-
ments, unless otherwise stated, we distinguish be-
tween the argument ordering of the two mentions.
For instance, we consider m1:emp-org:m2 and
m2:emp-org:m1 to be distinct relation types.
Most of the features used in our system are
based on the work in (Zhou et al, 2005). In this
paper, we propose some new collocation features
inspired by word sense disambiguation (WSD).
We give an overview of the features in Table 1.
Due to space limitations, we only describe the col-
location features and refer the reader to (Zhou et
al., 2005) for the rest of the features.
2.1 Collocation Features
Following (Zhou et al, 2005), we use a single
word to represent the head word of a mention.
Since single words might be ambiguous or poly-
semous, we incorporate local collocation features
which were found to be very useful for WSD.
Given the head word hwm of a mention m, the
collocation feature Ci,j refers to the sequence of
tokens in the immediate context of hwm. The off-
sets i and j denote the position (relative to hwm)
153
Category Feature
Lexical hw of m1
hw of m2
hw of m1, m2
BOW in m1
BOW in m2
single word between m1, m2
BOW in between m1, m2
bigrams in between m1, m2
first word in between m1, m2
last word in between m1, m2
Collocations C?1,?1, C+1,+1
C?2,?1, C?1,+1, C+1,+2
Structural m1-in-m2
m2-in-m1
#mentions between m1, m2
any word between m1, m2
M-lvl M-lvl of m1, m2
and m1, m2 E-maintype
E-type m1, m2 E-subtype
m1, m2 M-lvl and E-maintype
m1, m2 M-lvl and E-subtype
m1, m2 E-subtype and m1-in-m2
m1, m2 E-subtype and m2-in-m1
Dependency path between m1, m2
bag-of dep labels between m1, m2
hw of m1 and dep-parent
hw of m2 and dep-parent
Table 1: Features in the basic RE system. The
abbreviations are as follows. hw: head word, M-
lvl: mention level, E-type: entity type, dep-parent:
the word?s parent in the dependency tree.
of the first and last token of the sequence respec-
tively. For instance, C?1,+1 denotes a sequence of
three tokens, consisting of the single token on the
immediate left of hwm, the token hwm itself, and
the single token on the immediate right of hwm.
For each mention, we extract 5 features: C?1,?1,
C+1,+1, C?2,?1, C?1,+1, and C+1,+2.
3 Using Background Knowledge
Now we describe how we inject additional knowl-
edge into our relation extraction system.
3.1 Hierarchy of Relations
When our relations of interest are arranged in a
hierarchical structure, one should leverage this in-
formation to learn more accurate relation predic-
tors. For instance, assume that our relations are
arranged in a two-level hierarchy and we learn
two classifiers, one for disambiguating between
the first level coarse-grained relations, and an-
other for disambiguating between the second level
fine-grained relations.
Since there are a lot more fine-grained relation
types than coarse-grained relation types, we pro-
pose using the coarse-grained predictions which
should intuitively be more reliable, to improve the
fine-grained predictions. We show how to achieve
this through defining appropriate constraints be-
tween the coarse-grained and fine-grained rela-
tions, which can be enforced through the Con-
strained Conditional Models framework (aka ILP)
(Roth and Yih, 2007; Chang et al, 2008). Due
to space limitations, we refer interested readers
to the papers for more information on the CCM
framework.
By doing this, not only are the predictions of
both classifiers coherent with each other (thus ob-
taining better predictions from both classifiers),
but more importantly, we are effectively using the
(more reliable) predictions of the coarse-grained
classifier to constrain the predictions of the fine-
grained classifier. To the best of our knowledge,
this approach for RE is novel.
In this paper, we work on the NIST Automatic
Content Extraction (ACE) 2004 corpus. ACE de-
fines several coarse-grained relations such as em-
ployment/membership, geo-political entity (GPE)
affiliation, etc. Each coarse-grained relation is
further refined into several fine-grained relations1
and each fine-grained relation has a unique par-
ent coarse-grained relation. For instance, the fine-
grained relations employed as ordinary staff, em-
ployed as an executive, etc. are children relations
of employment/membership.
Let mi and mj denote a pair of mentions i and
j drawn from a document containing N mentions.
Let Ri,j denote a relation between mi and mj , and
let R = {Ri,j}, where 1?i, j?N ; i 6=j denote the
set of relations in the document. Also, we denote
the set of predefined coarse-grained relation types
and fine-grained relation types as LRc and LRf
respectively. Since there could possibly be no re-
lation between a mention pair, we add the null la-
bel to LRc and LRf , allowing our classifiers to
predict null for Ri,j . Finally, for a fine-grained re-
lation type rf , let V(rf) denote its parent coarse-
grained relation type.
1With the exception of the Discourse coarse-grained re-
lation.
154
We learn two classifiers, one for disambiguat-
ing between the coarse-grained relations and one
for disambiguating between the fine-grained rela-
tions. Let ?c and ?f denote the feature weights
learned for predicting coarse-grained and fine-
grained relations respectively. Let pR(rc) =
logPc(rc|mi,mj ; ?c) be the log probability that
relation R is predicted to be of coarse-grained
relation type rc. Similarly, let pR(rf) =
logPf (rf |mi,mj ; ?f ) be the log probability that
relation R is predicted to be of fine-grained re-
lation type rf . Let x?R,rc? be a binary variable
which takes on the value of 1 if relation R is la-
beled with the coarse-grained label rc. Similarly,
let y?R,rf? be a binary variable which takes on the
value of 1 if relation R is labeled with the fine-
grained label rf . Our objective function is then:
max
?
R?R
?
rc?LRc
pR(rc) ? x?R,rc?
+
?
R?R
?
rf?LRf
pR(rf) ? y?R,rf? (1)
subject to the following constraints:
?
rc?LRc
x?R,rc? = 1 ?R ? R (2)
?
rf?LRf
y?R,rf? = 1 ?R ? R (3)
x?R,rc? ? {0, 1} ?R ? R, rc ? LRc (4)
y?R,rf? ? {0, 1} ?R ? R, rf ? LRf (5)
Equations (2) and (3) require that each relation
can only be assigned one coarse-grained label and
one fine-grained label. Equations (4) and (5) indi-
cate that x?R,rc? and y?R,rf? are binary variables.
Two more constraints follow:
x?R,rc? ?
?
{rf?LRf |V(rf)=rc}
y?R,rf?
?R ? R , rc ? LRc (6)
y?R,rf? ? x?R,V(rf)? ?R ? R, rf ? LRf (7)
The logical form of Equation (6) can be written
as: x?R,rc? ? y?R,rf1? ? y?R,rf2? . . . ? y?R,rfn?,
where rf1, rf2, . . . , rfn are (child) fine-grained
relations of the coarse-grained relation rc. This
states that if we assign rc to relation R, then we
must also assign to R a fine-grained relation rf
art: Ei ?{gpe, org, per},
Ej ?{fac, gpe, veh, wea}
emp-org: Ei ?{gpe, org, per},
Ej ?{gpe, org, per}
gpe-aff: Ei ?{gpe, org, per},
Ej ?{gpe, loc}
other-aff: Ei ?{gpe, org, per},
Ej ?{gpe, loc}
per-soc: Ei ?{per}, Ej ?{per}
Table 2: Entity type constraints.
which is a child of rc. The logical form of Equa-
tion (7) can be written as: y?R,rf? ? x?R,V(rf)?.
This captures the inverse relation and states that
if we assign rf to R, then we must also assign to
R the relation type V(rf), which is the parent of
rf . Together, Equations (6) and (7) constrain the
predictions of the coarse-grained and fine-grained
classifiers to be coherent with each other. Finally,
we note that one could automatically translate log-
ical constraints into linear inequalities (Chang et
al., 2008).
This method is general and is applicable to
other NLP tasks where a hierarchy exists, such
as WSD and question answering. For instance,
in WSD, one can predict coarse-grained and fine-
grained senses using suitably defined sense inven-
tories and then perform inference via ILP to obtain
coherent predictions.
3.2 Entity Type Constraints
Each mention in ACE-2004 is annotated with one
of seven coarse-grained entity types: person (per),
organization (org), location (loc), geo-political en-
tity (gpe), facility (fac), vehicle (veh), and weapon
(wea).
Roth and Yih (2007) had shown that entity type
information is useful for constraining the possible
labels that a relation R can assume. For instance,
both mentions involved in a personal/social re-
lation must be of entity type per. In this work,
we gather such information from the ACE-2004
documentation and inject it as constraints (on the
coarse-grained relations) into our system. Due
to space limitations, we do not state the con-
straint equations or objective function here, but
we list the entity type constraints we imposed for
each coarse-grained relation mi-R-mj in Table
155
22, where Ei (Ej) denotes the allowed set of en-
tity types for mention mi (mj). Applying the en-
tity type information improves the predictions of
the coarse-grained classifier and this in turn could
improve the predictions of the fine-grained classi-
fier.
3.3 Using Coreference Information
We can also utilize the coreference relations
among entity mentions. Assuming that we know
mentions mi and mj are coreferent with each
other, then there should be no relation between
them3. Let z?i,j? be a binary variable which takes
on the value of 1 if mentions mi and mj are coref-
erent, and 0 if they are not. When z?i,j?=1, we cap-
ture the above intuition with the following con-
straints:
z?i,j? ? x?Ri,j ,null? (8)
z?i,j? ? y?Ri,j ,null? (9)
which can be written in logical form as: z?i,j? ?
x?Ri,j ,null?, and z?i,j? ? y?Ri,j ,null?. We add the
following to our objective function in Equation
(1):
?
mi,mj?m2
co?i,j? ? z?i,j?+ c?o?i,j? ? (1? z?i,j?) (10)
where m is the set of mentions in a document,
co?i,j? and c?o?i,j? are the log probabilities of pre-
dicting that mi and mj are coreferent and not
coreferent respectively. In this work, we assume
we are given coreference information, which is
available from the ACE annotation.
3.4 Using Knowledge from Wikipedia
We propose two ways of using Wikipedia to
gather features for relation extraction. Wikipedia
is a huge online encyclopedia and mainly contains
articles describing entities or concepts.
The first intuition is that if we are able to cor-
rectly map a pair of mentions mi and mj to their
corresponding Wikipedia article (assuming they
2We do not impose entity type constraints on the coarse-
grained relations disc and phys.
3In this work, we assume that no relations are reflexive.
After the experiments in this paper are performed, we ver-
ified that in the ACE corpus we used, less than 1% of the
relations are reflexive.
are represented in Wikipedia), we could use the
content on their Wikipedia pages to check whether
they are related.
In this work, we use a Wiki system (Rati-
nov et al, 2010) which performs context-sensitive
mapping of mentions to Wikipedia pages. In
their work, the authors first identify phrases or
mentions that could be mapped. The correct
Wikipedia article for each mention is then prob-
abilistically predicted using a combination of fea-
tures based on Wikipedia hyperlink structure, se-
mantic coherence, etc. The authors? own evalua-
tion results indicate that the performance of their
system ranges from 70?80%. When given a pair
of mentions and the system returns the Wikipedia
page for either one of the mentions, we introduce
a feature:
w1(mi,mj) =
?
?
?
1, if Ami(mj)
or Amj (mi)
0, otherwise
where Ami(mj) returns true if the head extent
of mj is found (via simple string matching) in
the predicted Wikipedia article of mi. The in-
terpretation of Amj (mi) is similar. We introduce
a new feature into the RE system by combining
w1(mi,mj) with mi,mj E-maintype (defined as
in Table 1).
The second feature based on Wikipedia is as
follows. It will be useful to check whether there
is any parent-child relationship between two men-
tions. Intuitively, this will be useful for recogniz-
ing several relations such as physical part-whole
(e.g. a city is part of a state), subsidiary (a com-
pany is a child-company of another), citizenship
(a person is a citizen of a country), etc.
Given a pair of mentions mi and mj , we use a
Parent-Child system (Do and Roth, 2010) to pre-
dict whether they have a parent-child relation. To
achieve this, the system first gathers all Wikipedia
articles that are related to mi and mj . It then uses
the words in these pages and the category ontol-
ogy of Wikipedia to make its parent-child predic-
tions, while respecting certain defined constraints.
In this work, we use its prediction as follows:
w2(mi,mj) =
{
1, if parent-child(mi,mj)
0, otherwise
156
Figure 1: An example of Brown word cluster hi-
erarchy from (Koo et al, 2008).
where we combine w2(mi,mj) with mi,mj E-
maintype, introducing this as a new feature into
our RE system.
3.5 Using Word Clusters
An inherent problem faced by supervised systems
is that of data sparseness. To mitigate such is-
sues in the lexical features, we use word clusters
which are automatically generated from unlabeled
texts. In this work, we use the Brown clustering
algorithm (Brown et al, 1992), which has been
shown to improve performance in various NLP
applications such as dependency parsing (Koo et
al., 2008), named entity recognition (Ratinov and
Roth, 2009), and relation extraction (Boschee et
al., 2005). The algorithm performs a hierarchical
clustering of the words and represents them as a
binary tree.
Each word is uniquely identified by its path
from the root and every path is represented with
a bit string. Figure 1 shows an example clustering
where the maximum path length is 3. By using
path prefixes of different lengths, one can obtain
clusterings at different granularity. For instance,
using prefixes of length 2 will put apple and pear
into the same cluster, Apple and IBM into the same
cluster, etc. In our work, we use clusters gener-
ated from New York Times text and simply use a
path prefix of length 10. When Brown clusters are
used in our system, all lexical features consisting
of single words will be duplicated. For instance,
for the feature hw of m1, one new feature which is
the length-10 bit string path representing the orig-
inal lexical head word of m1, will be introduced
and presented to the classifier as a string feature.
4 Experiments
We used the ACE-2004 dataset (catalog
LDC2005T09 from the Linguistic Data Con-
sortium) to conduct our experiments. ACE-2004
defines 7 coarse-grained relations and 23 fine-
grained relations. In all of our experiments,
unless otherwise stated, we explicitly model the
argument order (of the mentions) when asked
to disambiguate the relation between a pair of
mentions. Hence, we built our coarse-grained
classifier with 15 relation labels to disambiguate
between (two for each coarse-grained relation
type and a null label when the two mentions are
not related). Likewise, our fine-grained classifier
has to disambiguate between 47 relation labels.
In the dataset, relations do not cross sentence
boundaries.
For our experiments, we trained regularized av-
eraged perceptrons (Freund and Schapire, 1999),
implemented within the Sparse Network of Win-
now framework (Carlson et al, 1999), one for pre-
dicting the coarse-grained relations and another
for predicting the fine-grained relations. Since the
dataset has no split of training, development, and
test sets, we followed prior work (Jiang and Zhai,
2007) and performed 5-fold cross validation to ob-
tain our performance results. For simplicity, we
used 5 rounds of training and a regularization pa-
rameter of 1.5 for the perceptrons in all our exper-
iments. Finally, we concentrate on the evaluation
of fine-grained relations.
4.1 Performance of the Basic RE system
As a gauge on the performance of our basic rela-
tion extraction system BasicRE using only the fea-
tures described in Section 2, we compare against
the state-of-the-art feature-based RE system of
Jiang and Zhai (2007). However, we note that in
that work, the authors performed their evaluation
using undirected coarse-grained relations. That is,
they do not distinguish on argument order of men-
tions and the classifier has to decide among 8 re-
lation labels (7 coarse-grained relation types and a
null label). Performing 5-fold cross validation on
the news wire (nwire) and broadcast news (bnews)
corpora in the ACE-2004 dataset, they reported a
F-measure of 71.5 using a maximum entropy clas-
sifier4. Evaluating BasicRE on the same setting,
4After they heuristically performed feature selection and
applied the heuristics giving the best evaluation performance,
they obtained a result of 72.9.
157
All nwire 10% of nwire
Features Rec% Pre% F1% Rec% Pre% F1%
BasicRE 49.9 51.0 50.5 33.2 29.0 31.0
+Hier +1.3 +1.3 +1.3 +1.1 +1.2 +1.1
+Hier+relEntC +1.5 +2.0 +1.8 +3.3 +3.5 +3.4
+Coref ? +1.4 +0.7 ?0.1 +1.0 +0.5
+Wiki +0.2 +1.9 +1.0 +1.5 +2.5 +2.0
+Cluster ?0.2 +3.2 +1.4 ?0.7 +3.9 +1.7
+ALL +1.5 +6.7 +3.9 +4.7 +10.2 +7.6
Table 3: BasicRE gives the performance of our basic RE system on predicting fine-grained relations,
obtained by performing 5-fold cross validation on only the news wire corpus of ACE-2004. Each sub-
sequent row +Hier, +Hier+relEntC, +Coref, +Wiki, and +Cluster gives the individual contribution
from using each knowledge. The bottom row +ALL gives the performance improvements from adding
+Hier+relEntC+Coref+Wiki+Cluster. ? indicates no change in score.
we obtained a competitive F-measure of 71.25.
4.2 Experimental Settings for Evaluating
Fine-grained Relations
Two of our knowledge sources, the Wiki system
described in Section 3.4 and the word clusters de-
scribed in Section 3.5, assume inputs of mixed-
cased text. We note that the bnews corpus of
ACE-2004 is entirely in lower-cased text. Hence,
we use only the nwire corpus for our experiments
here, from which we gathered 28,943 relation in-
stances and 2,226 of those have a valid (non-null)
relation6.
We also propose the following experimental
setting. First, since we made use of coreference
information, we made sure that while performing
our experiments, all instances from the same doc-
ument are either all used as training data or all
used as test data. Prior work in RE had not en-
sured this, but we argue that this provides a more
realistic setting. Our own experiments indicate
that this results in a 1-2% lower performance on
fine-grained relations.
Secondly, prior work calculate their perfor-
mance on relation extraction at the level of men-
tions. That is, each mention pair extracted is
scored individually. An issue with this way of
scoring on the ACE corpus is that ACE annota-
5Using 10 rounds of training and a regularization param-
eter of 2.5 improves the result to 72.2. In general, we found
that more rounds of training and a higher regularization value
benefits coarse-grained relation classification, but not fine-
grained relation classification.
6The number of relation instances in the nwire and bnews
corpora are about the same.
tors rarely duplicate a relation link for coreferent
mentions. For instance, assume that mentions mi,
mj , and mk exist in a given sentence, mentions
mi and mj are coreferent, and the annotator es-
tablishes a particular relation type r between mj
and mk. The annotator will not usually duplicate
the same relation r between mi and mk and thus
the label between these two mentions is then null.
We are not suggesting that this is an incorrect ap-
proach, but clearly there is an issue since an im-
portant goal of performing RE is to populate or
build an ontology of entities and establish the re-
lations existing among the entities. Thus, we eval-
uate our performance at the entity-level.7 That is,
given a pair of entities, we establish the set of re-
lation types existing between them, based on their
mention annotations. Then we calculate recall
and precision based on these established relations.
Of course, performing such an evaluation requires
knowledge about the coreference relations and in
this work, we assume we are given this informa-
tion.
4.3 Knowledge-Enriched System
Evaluating our system BasicRE (trained only on
the features described in Section 2) on the nwire
corpus, we obtained a F1 score of 50.5, as shown
in Table 3. Next, we exploited the relation hier-
archy as in Section 3.1 and obtained an improve-
ment of 1.3, as shown in the row +Hier. Next,
we added the entity type constraints of Section
7Our experiments indicate that performing the usual eval-
uation on mentions gives similar performance figures and the
trend in Table 3 stays the same.
158
3.2. Remember that these constraints are imposed
on the coarse-grained relations. Thus, they would
only affect the fine-grained relation predictions if
we also exploit the relation hierarchy. In the ta-
ble, we show that all the background knowledge
helped to improve performance, providing a to-
tal improvement of 3.9 to our basic RE system.
Though the focus of this work is on fine-grained
relations, our approach also improves the perfor-
mance of coarse-grained relation predictions. Ba-
sicRE obtains a F1 score of 65.3 on coarse-grained
relations and exploiting background knowledge
gives a total improvement of 2.9.
5 Analysis
We explore the situation where we have very little
training data. We assume during each cross val-
idation fold, we are given only 10% of the train-
ing data we originally had. Previously, when per-
forming 5-fold cross validation on 2,226 valid re-
lation instances, we had about 1780 as training
instances in each fold. Now, we assume we are
only given about 178 training instances in each
fold. Under this condition, BasicRE gives a F1
score of 31.0 on fine-grained relations. Adding all
the background knowledge gives an improvement
of 7.6 and this represents an error reduction of
39% when measured against the performance dif-
ference of 50.5 (31.0) when we have 1780 train-
ing instances vs. 178 training instances. On
the coarse-grained relations, BasicRE gives a F1
score of 51.1 and exploiting background knowl-
edge gives a total improvement of 5.0.
We also tabulated the list of fine-grained re-
lations that improved by more than 1 F1 score
when we incorporated +Wiki, on the experiment
using all of nwire data: phys:near (physically
near), other-aff:ideology (ideology affiliation),
art:user-or-owner (user or owner of artifact), per-
soc:business (business relationship), phys:part-
whole (physical part-whole), emp-org:subsidiary
(organization subsidiary), and gpe-aff:citizen-or-
resident (citizen or resident). Most of these intu-
itively seemed to be information one would find
being mentioned in an encyclopedia.
6 Related Work
Few prior work has explored using background
knowledge to improve relation extraction perfor-
mance. Zhou et al (2008) took advantage of
the hierarchical ontology of relations by propos-
ing methods customized for the perceptron learn-
ing algorithm and support vector machines. In
contrast, we propose a generic way of using the
relation hierarchy which at the same time, gives
globally coherent predictions and allows for easy
injection of knowledge as constraints. Recently,
Jiang (2009) proposed using features which are
common across all relations. Her method is com-
plementary to our approach, as she does not con-
sider information such as the relatedness between
different relations. On using semantic resources,
Zhou et al (2005) gathered two gazettes, one
containing country names and another containing
words indicating personal relationships. In relat-
ing the tasks of RE and coreference resolution, Ji
et al (2005) used the output of a RE system to
rescore coreference hypotheses. In our work, we
reverse the setting and explore using coreference
to improve RE.
7 Conclusion
In this paper, we proposed a broad range of meth-
ods to inject background knowledge into a rela-
tion extraction system. Some of these methods,
such as exploiting the relation hierarchy, are gen-
eral in nature and could be easily applied to other
NLP tasks. To combine the various relation pre-
dictions and knowledge, we perform global infer-
ence within an ILP framework. Besides allowing
for easy injection of knowledge as constraints, this
ensures globally coherent models and predictions.
Acknowledgements This research was partly
sponsored by Air Force Research Laboratory
(AFRL) under prime contract no. FA8750-09-
C-0181. We thank Ming-Wei Chang and James
Clarke for discussions on this research.
References
Banko, Michele, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
159
Open information extraction from the web. In Pro-
ceedings of IJCAI-07, pages 2670?2676.
Boschee, Elizabeth, Ralph Weischedel, and Alex Za-
manian. 2005. Automatic information extraction.
In Proceedings of the International Conference on
Intelligence Analysis.
Brown, Peter F., Vincent J. Della Pietra, Peter V. deS-
ouza, Jenifer C. Lai, and Robert L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467?479.
Bunescu, Razvan C. and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of HLT/EMNLP-05, pages
724?731.
Carlson, Andrew J., Chad M. Cumby, Jeff L. Rosen,
and Dan Roth. 1999. The SNoW learning archi-
tecture. Technical Report UIUCDCS-R-99-2101,
UIUC Computer Science Department, May.
Chang, Ming-Wei, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with con-
straints. In Proceedings of AAAI-08, pages 1513?
1518.
Do, Quang and Dan Roth. 2010. On-the-
fly constraint-based taxonomic relation identifica-
tion. Technical report, University of Illinois.
http://L2R.cs.uiuc.edu/?danr/Papers/DoRo10.pdf.
Freund, Yoav and Robert E. Schapire. 1999. Large
margin classification using the perceptron algo-
rithm. Machine Learning, 37(3):277?296.
Ji, Heng, David Westbrook, and Ralph Grishman.
2005. Using semantic relations to refine corefer-
ence decisions. In Proceedings of HLT/EMNLP-05,
pages 17?24.
Jiang, Jing and ChengXiang Zhai. 2007. A system-
atic exploration of the feature space for relation ex-
traction. In Proceedings of HLT-NAACL-07, pages
113?120.
Jiang, Jing. 2009. Multi-task transfer learning for
weakly-supervised relation extraction. In Proceed-
ings of ACL-IJCNLP-09, pages 1012?1020.
Kambhatla, Nanda. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for information extraction. In Proceedings
of ACL-04, pages 178?181.
Koo, Terry, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08:HLT, pages 595?603.
Mintz, Mike, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of
ACL-IJCNLP-09, pages 1003?1011.
Ratinov, Lev and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of CoNLL-09, pages 147?155.
Ratinov, Lev, Doug Downey, and Dan
Roth. 2010. Wikification for informa-
tion retrieval. Technical report, Univer-
sity of Illinois. http://L2R.cs.uiuc.edu/?
danr/Papers/RatinovDoRo10.pdf.
Roth, Dan and Wen Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNLL-04, pages
1?8.
Roth, Dan and Wen Tau Yih. 2007. Global infer-
ence for entity and relation identification via a lin-
ear programming formulation. In Getoor, Lise and
Ben Taskar, editors, Introduction to Statistical Rela-
tional Learning. MIT Press.
Weld, Daniel S., Raphael Hoffman, and Fei Wu. 2008.
Using wikipedia to bootstrap open information ex-
traction. ACM SIGMOD Special Issue on Managing
Information Extraction, 37(4):62?68.
Zelenko, Dmitry, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083?1106.
Zhang, Min, Jie Zhang, Jian Su, and GuoDong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of COLING-ACL-06, pages 825?
832.
Zhou, Guodong, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of ACL-05.
Zhou, GuoDong, Min Zhang, DongHong Ji, and
QiaoMing Zhu. 2007. Tree kernel-based re-
lation extraction with context-sensitive structured
parse tree information. In Proceedings of EMNLP-
CoNLL-07, pages 728?736.
Zhou, Guodong, Min Zhang, Dong-Hong Ji, and
Qiaoming Zhu. 2008. Hierarchical learning strat-
egy in semantic relation extraction. Information
Processing & Management, 44(3):1008?1021.
160
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 294?303,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Minimally Supervised Event Causality Identification
Quang Xuan Do Yee Seng Chan Dan Roth
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{quangdo2,chanys,danr}@illinois.edu
Abstract
This paper develops a minimally supervised
approach, based on focused distributional sim-
ilarity methods and discourse connectives,
for identifying of causality relations between
events in context. While it has been shown
that distributional similarity can help identify-
ing causality, we observe that discourse con-
nectives and the particular discourse relation
they evoke in context provide additional in-
formation towards determining causality be-
tween events. We show that combining dis-
course relation predictions and distributional
similarity methods in a global inference pro-
cedure provides additional improvements to-
wards determining event causality.
1 Introduction
An important part of text understanding arises from
understanding the semantics of events described in
the narrative, such as identifying the events that are
mentioned and how they are related semantically.
For instance, when given a sentence ?The police
arrested him because he killed someone.?, humans
understand that there are two events, triggered by
the words ?arrested? and ?killed?, and that there is
a causality relationship between these two events.
Besides being an important component of discourse
understanding, automatically identifying causal re-
lations between events is important for various nat-
ural language processing (NLP) applications such
as question answering, etc. In this work, we auto-
matically detect and extract causal relations between
events in text.
Despite its importance, prior work on event
causality extraction in context in the NLP litera-
ture is relatively sparse. In (Girju, 2003), the au-
thor used noun-verb-noun lexico-syntactic patterns
to learn that ?mosquitoes cause malaria?, where the
cause and effect mentions are nominals and not nec-
essarily event evoking words. In (Sun et al, 2007),
the authors focused on detecting causality between
search query pairs in temporal query logs. (Beamer
and Girju, 2009) tried to detect causal relations be-
tween verbs in a corpus of screen plays, but limited
themselves to consecutive, or adjacent verb pairs.
In (Riaz and Girju, 2010), the authors first cluster
sentences into topic-specific scenarios, and then fo-
cus on building a dataset of causal text spans, where
each span is headed by a verb. Thus, their focus was
not on identifying causal relations between events in
a given text document.
In this paper, given a text document, we first iden-
tify events and their associated arguments. We then
identify causality or relatedness relations between
event pairs. To do this, we develop a minimally su-
pervised approach using focused distributional sim-
ilarity methods, such as co-occurrence counts of
events collected automatically from an unannotated
corpus, to measure and predict existence of causal-
ity relations between event pairs. Then, we build on
the observation that discourse connectives and the
particular discourse relation they evoke in context
provide additional information towards determining
causality between events. For instance, in the ex-
ample sentence provided at the beginning of this
section, the words ?arrested? and ?killed? probably
have a relatively high apriori likelihood of being ca-
294
sually related. However, knowing that the connec-
tive ?because? evokes a contingency discourse re-
lation between the text spans ?The police arrested
him? and ?he killed someone? provides further ev-
idence towards predicting causality. The contribu-
tions of this paper are summarized below:
? Our focus is on identifying causality between
event pairs in context. Since events are of-
ten triggered by either verbs (e.g. ?attack?) or
nouns (e.g. ?explosion?), we allow for detec-
tion of causality between verb-verb, verb-noun,
and noun-noun triggered event pairs. To the
best of our knowledge, this formulation of the
task is novel.
? We developed a minimally supervised ap-
proach for the task using focused distributional
similarity methods that are automatically col-
lected from an unannotated corpus. We show
that our approach achieves better performance
than two approaches: one based on a frequently
used metric that measures association, and an-
other based on the effect-control-dependency
(ECD) metric described in a prior work (Riaz
and Girju, 2010).
? We leverage on the interactions between event
causality prediction and discourse relations
prediction. We combine these knowledge
sources through a global inference procedure,
which we formalize via an Integer Linear Pro-
gramming (ILP) framework as a constraint op-
timization problem (Roth and Yih, 2004). This
allows us to easily define appropriate con-
straints to ensure that the causality and dis-
course predictions are coherent with each other,
thereby improving the performance of causality
identification.
2 Event Causality
In this work, we define an event as an action or oc-
currence that happens with associated participants
or arguments. Formally, we define an event e
as: p(a1, a2, . . . , an), where the predicate p is the
word that triggers the presence of e in text, and
a1, a2, . . . , an are the arguments associated with
e. Examples of predicates could be verbs such as
?attacked?, ?employs?, nouns such as ?explosion?,
?protest?, etc., and examples of the arguments of
?attacked? could be its subject and object nouns.
To measure the causality association between a
pair of events ei and ej (in general, ei and ej
could be extracted from the same or different doc-
uments), we should use information gathered about
their predicates and arguments. A simple approach
would be to directly calculate the pointwise mu-
tual information (PMI)1 between pi(ai1, ai2, . . . , ain)
and pj(aj1, aj2, . . . , ajm). However, this leads to very
sparse counts as the predicate pi with its list of ar-
guments ai1, . . . , ain would rarely co-occur (within
some reasonable context distance) with predicate pj
and its entire list of arguments aj1, . . . , ajm. Hence,
in this work, we measure causality association us-
ing three separate components and focused distribu-
tional similarity methods collected about event pairs
as described in the rest of this section.
2.1 Cause-Effect Association
We measure the causality or cause-effect association
(CEA) between two events ei and ej using the fol-
lowing equation:
CEA(ei, ej) =
spp(ei, ej) + spa(ei, ej) + saa(ei, ej) (1)
where spp measures the association between event
predicates, spa measures the association between the
predicate of an event and the arguments of the other
event, and saa measures the association between
event arguments. In our work, we regard each event
e as being triggered and rooted at a predicate p.
2.1.1 Predicate-Predicate Association
We define spp as follows:
spp(ei, ej) = PMI(pi, pj)?max(ui, uj)
?IDF (pi, pj)?Dist(pi, pj) (2)
which takes into account the PMI between pred-
icates pi and pj of events ei and ej respectively,
as well as various other pieces of information. In
Suppes? Probabilistic theory of Casuality (Suppes,
1970), he highlighted that event e is a possible cause
of event e?, if e? happens more frequently with e than
1PMI is frequently used to measure association between
variables.
295
by itself, i.e. P (e?|e) > P (e?). This can be easily
rewritten as P (e,e?)P (e)P (e?) > 1, similar to the definitionof PMI:
PMI(e, e?) = log P (e, e
?)
P (e)P (e?)
which is only positive when P (e,e?)P (e)P (e?) > 1.
Next, we build on the intuition that event predi-
cates appearing in a large number of documents are
probably not important or discriminative. Thus, we
penalize these predicates when calculating spp by
adopting the inverse document frequency (idf):
IDF (pi, pj) = idf(pi)? idf(pj)? idf(pi, pj),
where idf(p) = log D1+N , D is the total number ofdocuments in the collection and N is the number of
documents that p occurs in.
We also award event pairs that are closer together,
while penalizing event pairs that are further apart in
texts, by incorporating the distance measure of Lea-
cock and Chodorow (1998), which was originally
used to measure similarity between concepts:
Dist(pi, pj) = ?log |sent(p
i)? sent(pj)|+ 1
2? ws ,
where sent(p) gives the sentence number (index) in
which p occurs and ws indicates the window-size
(of sentences) used. If pi and pj are drawn from the
same sentence, the numerator of the above fraction
will return 1. In our work, we set ws to 3 and thus,
if pi occurs in sentence k, the furthest sentence that
pj will be drawn from, is sentence k + 2.
The final component of Equation 2, max(ui, uj),
takes into account whether predicates (events) pi and
pj appear most frequently with each other. ui and uj
are defined as follows:
ui = P (p
i, pj)
maxk[P (pi, pk)]? P (pi, pj) + 
uj = P (p
i, pj)
maxk[P (pk, pj)]? P (pi, pj) +  ,
where we set  = 0.01 to avoid zeros in the denom-
inators. ui will be maximized if there is no other
predicate pk having a higher co-occurrence proba-
bility with pi, i.e. pk = pj . uj is treated similarly.
2.1.2 Predicate-Argument and
Argument-Argument Association
We define spa as follows:
spa(ei, ej) =
1
|Aej |
?
a?Aej
PMI(pi, a)
+ 1|Aei |
?
a?Aei
PMI(pj , a), (3)
where Aei and Aej are the sets of arguments of ei
and ej respectively.
Finally, we define saa as follows:
saa(ei, ej) =
1
|Aei ||Aej |
?
a?Aei
?
a??Aej
PMI(a, a?) (4)
Together, spa and saa provide additional contexts
and robustness (in addition to spp) for measuring the
cause-effect association between events ei and ej .
Our formulation of CEA is inspired by the ECD
metric defined in (Riaz and Girju, 2010):
ECD(a, b) = max(v, w)??log dis(a, b)2?maxDistance , (5)
where
v = P (a, b)P (b)? P (a, b) +  ?
P (a, b)
maxt[P (a, bt)]? P (a, b) + 
w= P (a, b)P (a)? P (a, b) +  ?
P (a, b)
maxt[P (at, b)]? P (a, b) +  ,
where ECD(a,b) measures the causality between two
events a and b (headed by verbs), and the sec-
ond component in the ECD equation is similar to
Dist(pi, pj). In our experiments, we will evaluate
the performance of ECD against our proposed ap-
proach.
So far, our definitions in this section are generic
and allow for any list of event argument types. In
this work, we focus on two argument types: agent
(subject) and patient (object), which are typical core
arguments of any event. We describe how we extract
event predicates and their associated arguments in
the section below.
3 Verbal and Nominal Predicates
We consider that events are not only triggered by
verbs but also by nouns. For a verb (verbal predi-
cate), we extract its subject and object from its as-
sociated dependency parse. On the other hand, since
296
events are also frequently triggered by nominal pred-
icates, it is important to identify an appropriate list
of event triggering nouns. In our work, we gathered
such a list using the following approach:
? We first gather a list of deverbal nouns from the
set of most frequently occurring (in the Giga-
word corpus) 3,000 verbal predicate types. For
each verb type v, we go through all its Word-
Net2 senses and gather all its derivationally re-
lated nouns Nv 3.
? From Nv, we heuristically remove nouns that
are less than three characters in length. We also
remove nouns whose first three characters are
different from the first three characters of v. For
each of the remaining nouns in Nv, we mea-
sured its Levenstein (edit) distance from v and
keep the noun(s) with the minimum distance.
When multiple nouns have the same minimum
distance from v, we keep all of them.
? To further prune the list of nouns, we next re-
moved all nouns ending in ?er?, ?or?, or ?ee?,
as these nouns typically refer to a person, e.g.
?writer?, ?doctor?, ?employee?. We also re-
move nouns that are not hyponyms (children)
of the first WordNet sense of the noun ?event?4.
? Since we are concerned with nouns denoting
events, FrameNet (Ruppenhofer et al, 2010)
(FN) is a good resource for mining such nouns.
FN consists of frames denoting situations and
events. As part of the FN resource, each FN
frame consists of a list of lexical units (mainly
verbs and nouns) representing the semantics of
the frame. Various frame-to-frame relations are
also defined (in particular the inheritance re-
lation). Hence, we gathered all the children
frames of the FN frame ?Event?. From these
children frames, we then gathered all their noun
lexical units (words) and add them to our list of
2http://wordnet.princeton.edu/
3The WordNet resource provides derivational information
on words that are in different syntactic (i.e. part-of-speech) cat-
egories, but having the same root (lemma) form and that are
semantically related.
4The first WordNet sense of the noun ?event? has the mean-
ing: ?something that happens at a given place and time?
nouns. Finally, we also add a few nouns denot-
ing natural disaster from Wikipedia5.
Using the above approach, we gathered a list of
about 2,000 noun types. This current approach is
heuristics based which we intend to improve in the
future, and any such improvements should subse-
quently improve the performance of our causality
identification approach.
Event triggering deverbal nouns could have as-
sociated arguments (for instance, acting as subject,
object of the deverbal noun). To extract these ar-
guments, we followed the approach of (Gurevich
et al, 2008). Briefly, the approach uses linguistic
patterns to extract subjects and objects for deverbal
nouns, using information from dependency parses.
For more details, we refer the reader to (Gurevich et
al., 2008).
4 Discourse and Causality
Discourse connectives are important for relating dif-
ferent text spans, helping us to understand a piece of
text in relation to its context:
[The police arrested him] because [he killed someone].
In the example sentence above, the discourse con-
nective (?because?) and the discourse relation it
evokes (in this case, the Cause relation) allows read-
ers to relate its two associated text spans, ?The po-
lice arrested him? and ?he killed someone?. Also,
notice that the verbs ?arrested? and ?killed?, which
cross the two text spans, are causally related. To
aid in extracting causal relations, we leverage on the
identification of discourse relations to provide addi-
tional contextual information.
To identify discourse relations, we use the Penn
Discourse Treebank (PDTB) (Prasad et al, 2007),
which contains annotations of discourse relations
in context. The annotations are done over the
Wall Street Journal corpus and the PDTB adopts a
predicate-argument view of discourse relations. A
discourse connective (e.g. because) takes two text
spans as its arguments. In the rest of this section,
we briefly describe the discourse relations in PDTB
and highlight how we might leverage them to aid in
determining event causality.
5http://en.wikipedia.org/wiki/Natural disaster
297
Coarse-grained relations Fine-grained relations
Comparison Concession, Contrast, Pragmatic-concession, Pragmatic-contrast
Contingency Cause, Condition, Pragmatic-cause, Pragmatic-condition
Expansion Alternative, Conjunction, Exception, Instantiation, List, Restatement
Temporal Asynchronous, Synchronous
Table 1: Coarse-grained and fine-grained discourse relations.
4.1 Discourse Relations
PDTB contains annotations for four coarse-grained
discourse relation types, as shown in the left column
of Table 1. Each of these are further refined into
several fine-grained discourse relations, as shown in
the right column of the table.6 Next, we briefly de-
scribe these relations, highlighting those that could
potentially help to determine event causality.
Comparison A Comparison discourse relation
between two text spans highlights prominent differ-
ences between the situations described in the text
spans. An example sentence is:
Contrast: [According to the survey, x% of Chinese Inter-
net users prefer Google] whereas [y% prefer Baidu].
According to the PDTB annotation manual
(Prasad et al, 2007), the truth of both spans is in-
dependent of the established discourse relation. This
means that the text spans are not causally related and
thus, the existence of a Comparison relation should
imply that there is no causality relation across the
two text spans.
Contingency A Contingency relation between
two text spans indicates that the situation described
in one text span causally influences the situation in
the other. An example sentence is:
Cause: [The first priority is search and rescue] because
[many people are trapped under the rubble].
Existence of a Contingency relation potentially
implies that there exists at least one causal event
pair crossing the two text spans. The PDTB an-
notation manual states that while the Cause and
Condition discourse relations indicate casual influ-
ence in their text spans, there is no causal in-
fluence in the text spans of the Pragmatic-cause
and Pragmatic-condition relations. For instance,
Pragmatic-condition indicates that one span pro-
6PDTB further refines these fine-grained relations into a fi-
nal third level of relations, but we do not use them in this work.
vides the context in which the description of the sit-
uation in the other span is relevant; for example:
Pragmatic-condition: If [you are thirsty], [there?s beer in
the fridge].
Hence, there is a need to also identify fine-grained
discourse relations.
Expansion Connectives evoking Expansion dis-
course relations expand the discourse, such as by
providing additional information, illustrating alter-
native situations, etc. An example sentence is:
Conjunction: [Over the past decade, x women were
killed] and [y went missing].
Most of the Expansion fine-grained relations (ex-
cept for Conjunction, which could connect arbitrary
pieces of text spans) should not contain causality re-
lations across its text spans.
Temporal These indicate that the situations de-
scribed in the text spans are related temporally. An
example sentence is:
Synchrony: [He was sitting at his home] when [the whole
world started to shake].
Temporal precedence of the (cause) event over the
(effect) event is a necessary, but not sufficient req-
uisite for causality. Hence by itself, Temporal re-
lations are probably not discriminative enough for
determining event causality.
4.2 Discourse Relation Extraction System
Our work follows the approach and features de-
scribed in the state-of-the-art Ruby-based discourse
system of (Lin et al, 2010), to build an in-
house Java-based discourse relation extraction sys-
tem. Our system identifies explicit connectives in
text, predict their discourse relations, as well as their
associated text spans. Similar to (Lin et al, 2010),
we achieved a competitive performance of slightly
over 80% F1-score in identifying fine-grained rela-
tions for explicit connectives. Our system is devel-
oped using the Learning Based Java modeling lan-
298
guage (LBJ) (Rizzolo and Roth, 2010) and will be
made available soon. Due to space constraints, we
refer interested readers to (Lin et al, 2010) for de-
tails on the features, etc.
In the example sentences given thus far in this sec-
tion, all the connectives were explicit, as they appear
in the texts. PDTB also provides annotations for im-
plicit connectives, which we do not use in this work.
Identifying implicit connectives is a harder task and
incorporating these is a possible future work.
5 Joint Inference for Causality Extraction
To exploit the interactions between event pair
causality extraction and discourse relation identifi-
cation, we define appropriate constraints between
them, which can be enforced through the Con-
strained Conditional Models framework (aka ILP for
NLP) (Roth and Yih, 2007; Chang et al, 2008). In
doing this, the predictions of CEA (Section 2.1) and
the discourse system are forced to cohere with each
other. More importantly, this should improve the
performance of using only CEA to extract causal
event pairs. To the best of our knowledge, this ap-
proach for causality extraction is novel.
5.1 CEA & Discourse: Implementation Details
Let E denote the set of event mentions in a docu-
ment. Let EP = {(ei, ej) ? E ? E | ei ? E , ej ?
E , i < j, |sent(ei) ? sent(ej)| ? 2} denote the
set of event mention pairs in the document, where
sent(e) gives the sentence number in which event e
occurs. Note that in this work, we only extract event
pairs that are at most two sentences apart. Next, we
define LER = {?causal?, ?? causal?} to be the set of
event relation labels that an event pair ep ? EP can
be associated with.
Note that the CEA metric as defined in Section 2.1
simply gives a score without it being bounded to be
between 0 and 1.0. However, to use the CEA score
as part of the inference process, we require that it be
bounded and thus can be used as a binary prediction,
that is, predicting an event pair as causal or ?causal.
To enable this, we use a few development documents
to automatically find a threshold CEA score that sep-
arates scores indicating causal vs ?causal. Based
on this threshold, the original CEA scores are then
rescaled to fall within 0 to 1.0. More details on this
are in Section 6.2.
Let C denote the set of connective mentions in a
document. We slightly modify our discourse sys-
tem as follows. We define LDR to be the set of
discourse relations. We initially add all the fine-
grained discourse relations listed in Table 1 to LDR.
In the PDTB corpus, some connective examples are
labeled with just a coarse-grained relation, with-
out further specifying a fine-grained relation. To
accommodate these examples, we add the coarse-
grained relations Comparison, Expansion, and Tem-
poral to LDR. We omit the coarse-grained Con-
tingency relation from LDR, as we want to sepa-
rate Cause and Condition from Pragmatic-cause and
Pragmatic-condition. This discards very few exam-
ples as only a very small number of connective ex-
amples are simply labeled with a Contingency label
without further specifying a fine-grained label. We
then retrained our discourse system to predict labels
in LDR.
5.2 Constraints
We now describe the constraints used to support
joint inference, based on the predictions of the CEA
metric and the discourse classifier. Let sc(dr) be
the probability that connective c is predicated to be
of discourse relation dr, based on the output of our
discourse classifier. Let sep(er) be the CEA pre-
diction score (rescaled to range in [0,1]) that event
pair ep takes on the causal or ?causal label er. Let
x?c,dr? be a binary indicator variable which takes on
the value 1 iff c is labeled with the discourse relation
dr. Similarly, let y?ep,er? be a binary variable which
takes on the value 1 iff ep is labeled as er. We then
define our objective function as follows:
max
[
|LDR|
?
c?C
?
dr?LDR
sc(dr) ? x?c,dr?
+|LER|
?
ep?EP
?
er?LER
sep(er) ? y?ep,er?
]
(6)
subject to the following constraints:
?
dr?LDR
x?c,dr? = 1 ?c ? C (7)
?
er?LER
y?ep,er? = 1 ?ep ? EP (8)
x?c,dr? ? {0, 1} ?c ? C, dr ? LDR (9)
y?ep,er? ? {0, 1} ?ep ? EP, er ? LER(10)
299
Equation (7) requires that each connective c can
only be assigned one discourse relation. Equation
(8) requires that each event pair ep can only be
causal or ?causal. Equations (9) and (10) indicate
that x?c,dr? and y?ep,er? are binary variables.
To capture the relationship between event pair
causality and discourse relations, we use the follow-
ing constraints:
x?c,?Cause?? ?
?
ep?EPc
y?ep,?causal?? (11)
x?c,?Condition?? ?
?
ep?EPc
y?ep,?causal??, (12)
where both equations are defined ?c ? C. EPc is
defined to be the set of event pairs that cross the two
text spans associated with c. For instance, if the first
text span of c contains two event mentions ei, ej ,
and there is one event mention ek in the second text
span of c, then EPc = {(ei, ek), (ej , ek)}. Finally,
the logical form of Equation (11) can be written as:
x?c,?Cause?? ? y?epi,?causal?? ? . . . ? y?epj ,?causal??,
where epi, . . . , epj are elements in EPc. This states
that if we assign the Cause discourse label to c,
then at least one of epi, . . . , epj must be assigned as
causal. The interpretation of Equation (12) is simi-
lar.
We use two more constraints to capture the inter-
actions between event causality and discourse rela-
tions. First, we defined Cep as the set of connectives
c enclosing each event of ep in each of its text spans,
i.e.: one of the text spans of c contain one of the
event in ep, while the other text span of c contain the
other event in ep. Next, based on the discourse rela-
tions in Section 4.1, we propose that when an event
pair ep is judged to be causal, then the connective
c that encloses it should be evoking one of the dis-
course relations in LDRa = {?Cause?, ?Condition?,
?Temporal?, ?Asynchronous?, ?Synchrony?, ?Con-
junction?}. We capture this using the following con-
straint:
y?ep,?causal?? ?
?
dra?LDRa
x?c,dra? ?c ? Cep (13)
The logical form of Equation (13) can be written as:
y?ep,?causal?? ? x?c,?Cause?? ? x?c,?Condition?? . . . ?
x?c,?Conjunction??. This states that if we assign ep as
causal, then we must assign to c one of the labels in
LDRa .
Finally, we propose that for any connectives evok-
ing discourse relations LDRb = {?Comparison?,
?Concession?, ?Contrast?, ?Pragmatic-concession?,
?Pragmatic-contrast?, ?Expansion?, ?Alternative?,
?Exception?, ?Instantiation?, ?List?, ?Restate-
ment?}, any event pair(s) that it encloses should be
?causal. We capture this using the following con-
straint:
x?c,drb? ? y?ep,??causal??
? drb ? LDRb , ep ? EPc, (14)
where the logical form of Equation (14) can be writ-
ten as: x?c,drb? ? y?ep,??causal??.
6 Experiments
6.1 Experimental Settings
To collect the distributional statistics for measuring
CEA as defined in Equation (1), we applied part-
of-speech tagging, lemmatization, and dependency
parsing (Marneffe et al, 2006) on about 760K docu-
ments in the English Gigaword corpus (LDC catalog
number LDC2003T05).
We are not aware of any benchmark corpus for
evaluating event causality extraction in contexts.
Hence, we created an evaluation corpus using the
following process: Using news articles collected
from CNN7 during the first three months of 2010, we
randomly selected 20 articles (documents) as evalu-
ation data, and 5 documents as development data.
Two annotators annotated the documents for
causal event pairs, using two simple notions for
causality: the Cause event should temporally pre-
cede the Effect event, and the Effect event occurs be-
cause the Cause event occurs. However, sometimes
it is debatable whether two events are involved in a
causal relation, or whether they are simply involved
in an uninteresting temporal relation. Hence, we al-
lowed annotations of C to indicate causality, and R
to indicate relatedness (for situations when the exis-
tence of causality is debatable). The annotators will
simply identify and annotate the C or R relations be-
tween predicates of event pairs. Event arguments are
not explicitly annotated, although the annotators are
free to look at the entire document text while mak-
ing their annotation decisions. Finally, they are free
7http://www.cnn.com
300
System Rec% Pre% F1%
PMIpp 26.6 20.8 23.3
ECDpp &PMIpa,aa 40.9 23.5 29.9
CEA 62.2 28.0 38.6
CEA+Discourse 65.1 30.7 41.7
Table 2: Performance of baseline systems and our ap-
proaches on extracting Causal event relations.
System Rec% Pre% F1%
PMIpp 27.8 24.9 26.2
ECDpp &PMIpa,aa 42.4 28.5 34.1
CEA 63.1 33.7 43.9
CEA+Discourse 65.3 36.5 46.9
Table 3: Performance of the systems on extracting Causal
and Related event relations.
to annotate relations between predicates that have
any number of sentences in between and are not re-
stricted to a fixed sentence window-size.
After adjudication, we obtained a total of 492
C+R relation annotations, and 414C relation anno-
tations on the evaluation documents. On the devel-
opment documents, we obtained 92 C+R and 71 C
relation annotations. The annotators overlapped on
10 evaluation documents. On these documents, the
first (second) annotator annotated 215 (199) C + R
relations, agreeing on 166 of these relations. To-
gether, they annotated 248 distinct relations. Us-
ing this number, their agreement ratio would be 0.67
(166/248). The corresponding agreement ratio for
C relations is 0.58. These numbers highlight that
causality identification is a difficult task, as there
could be as many as N2 event pairs in a document
(N is the number of events in the document). We
plan to make this annotated dataset available soon.8
6.2 Evaluation
As mentioned in Section 5.1, to enable translat-
ing (the unbounded) CEA scores into binary causal,
?causal predictions, we need to rescale or calibrate
these scores to range in [0,1]. To do this, we first
rank all the CEA scores of all event pairs in the de-
velopment documents. Most of these event pairs will
be ?causal. Based on the relation annotations in
these development documents, we scanned through
8http://cogcomp.cs.illinois.edu/page/publication view/663
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 5  10  15  20  25  30  35  40
Pr
ec
is
io
n(
%
)
K (number of causality predictions)
Precision(%) on top K event causality predictions
CEA
ECDpp & PMIpa,aa
PMIpp
Figure 1: Precision of the top K causality C predictions.
this ranked list of scores to locate the CEA score
t that gives the highest F1-score (on the develop-
ment documents) when used as a threshold between
causal vs ?causal decisions. We then ranked all
the CEA scores of all event pairs gathered from the
760K Gigaword documents, discretized all scores
higher than t into B bins, and all scores lower than
t into B bins. Together, these 2B bins represent the
range [0,1]. We used B = 500. Thus, consecu-
tive bins represent a difference of 0.001 in calibrated
scores.
To measure the causality between a pair of
events ei and ej , a simple baseline is to calculate
PMI(pi, pj). Using a similar thresholding and cali-
bration process to translate PMI(pi, pj) scores into
binary causality decisions, we obtained a F1 score of
23.1 when measured over the causality C relations,
as shown in the row PMIpp of Table 2.
As mentioned in Section 2.1.2, Riaz and Girju
(2010) proposed the ECD metric to measure
causality between two events. Thus, as a point of
comparison, we replaced spp of Equation (1) with
ECD(a, b) of Equation (5), substituting a = pi and
b = pj . After thresholding and calibrating the scores
of this approach, we obtained a F1-score of 29.7, as
shown in the row ECDpp&PMIpa,aa of Table 2.
Next, we evaluated our proposed CEA approach
and obtained a F1-score of 38.6, as shown in the row
CEA of Table 2. Thus, our proposed approach ob-
tained significantly better performance than the PMI
baseline and the ECD approach. Next, we per-
formed joint inference with the discourse relation
predictions as described in Section 5 and obtained
301
an improved F1-score of 41.7. We note that we ob-
tained improvements in both recall and precision.
This means that with the aid of discourse relations,
we are able to recover more causal relations, as well
as reduce false-positive predictions.
Constraint Equations (11) and (12) help to re-
cover causal relations. For improvements in pre-
cision, as stated in the last paragraph of Section
5.2, identifying other discourse relations such as
?Comparison?, ?Contrast?, etc., provides counter-
evidence to causality. Together with constraint
Equation (14), this helps to eliminate false-positive
event pairs as classified by CEA and contributes
towards CEA+Discourse having a higher precision
than CEA.
The corresponding results for extracting both
causality and relatedness C + R relations are given
in Table 3. For these experiments, the aim was for a
more relaxed evaluation and we simply collapsed C
and R into a single label.
Finally, we also measured the precision of the
top K causality C predictions, showing the preci-
sion trends in Figure 1. As shown, CEA in general
achieves higher precision when compared toPMIpp
and ECDpp&PMIpa,aa. The trends for C+R pre-
dictions are similar.
Thus far, we had included both verbal and nom-
inal predicates in our evaluation. When we repeat
the experiments for ECDpp&PMIpa,aa and CEA
on just verbal predicates, we obtained the respective
F1-scores of 31.8 and 38.3 on causality relations.
The corresponding F1-scores for casuality and relat-
edness relations are 35.7 and 43.3. These absolute
F1-scores are similar to those in Tables 2 and 3, dif-
fering by 1-2%.
7 Analysis
We randomly selected 50 false-positive predictions
and 50 false-negative causality relations to analyze
the mistakes made by CEA.
Among the false-positives (precision errors), the
most frequent error type (56% of the errors) is that
CEA simply assigns a high score to event pairs that
are not causal; more knowledge sources are required
to support better predictions in these cases. The next
largest group of error (22%) involves events contain-
ing pronouns (e.g. ?he?, ?it?) as arguments. Ap-
plying coreference to replace these pronouns with
their canonical entity strings or labeling them with
semantic class information might be useful.
Among the false-negatives (recall errors), 23%
of the errors are due to CEA simply assigning a
low score to causal event pairs and more contex-
tual knowledge seems necessary for better predic-
tions. 19% of the recall errors arises from causal
event pairs involving nominal predicates that are not
in our list of event evoking noun types (described in
Section 3). A related 17% of recall errors involves
nominal predicates without any argument. For these,
less information is available for CEA to make pre-
dictions. The remaining group (15% of errors) in-
volves events containing pronouns as arguments.
8 Related Work
Although prior work in event causality extraction
in context is relatively sparse, there are many prior
works concerning other semantic aspects of event
extraction. Ji and Grishman (2008) extracts event
mentions (belonging to a predefined list of target
event types) and their associated arguments. In other
prior work (Chen et al, 2009; Bejan and Harabagiu,
2010), the authors focused on identifying another
type of event pair semantic relation: event corefer-
ence. Chambers and Jurafsky (2008; 2009) chain
events sharing a common (protagonist) participant.
They defined events as verbs and given an existing
chain of events, they predict the next likely event in-
volving the protagonist. This is different from our
task of detecting causality between arbitrary event
pairs that might or might not share common argu-
ments. Also, we defined events more broadly, as
those that are triggered by either verbs or nouns. Fi-
nally, although our proposed CEA metric has resem-
blance the ECD metric in (Riaz and Girju, 2010), our
task is different from theirs and our work differs in
many aspects. They focused on building a dataset of
causal text spans, whereas we focused on identifying
causal relations between events in a given text doc-
ument. They considered text spans headed by verbs
while we considered events triggered by both verbs
and nouns. Moreover, we combined event causality
prediction and discourse relation prediction through
a global inference procedure to further improve the
performance of event causality prediction.
302
9 Conclusion
In this paper, using general tools such as the depen-
dency and discourse parsers which are not trained
specifically towards our target task, and a minimal
set of development documents for threshold tuning,
we developed a minimally supervised approach to
identify causality relations between events in con-
text. We also showed how to incorporate discourse
relation predictions to aid event causality predictions
through a global inference procedure. There are sev-
eral interesting directions for future work, including
the incorporation of other knowledge sources such
as coreference and semantic class predictions, which
were shown to be potentially important in our er-
ror analysis. We could also use discourse relations
to aid in extracting other semantic relations between
events.
Acknowledgments
The authors thank the anonymous reviewers for their
insightful comments and suggestions. University of
Illinois at Urbana-Champaign gratefully acknowl-
edges the support of Defense Advanced Research
Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract No. FA8750-09-C-0181. The first
author thanks the Vietnam Education Foundation
(VEF) for its sponsorship. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the view of the VEF, DARPA, AFRL,
or the US government.
References
Brandon Beamer and Roxana Girju. 2009. Using a bi-
gram event model to predict causal potential. In CI-
CLING.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010. Un-
supervised event coreference resolution with rich lin-
guistic features. In ACL.
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In ACL-HLT.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In ACL.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with con-
straints. In AAAI.
Zheng Chen, Heng Ji, and Robert Haralick. 2009. A
pairwise event coreference model, feature impact and
evaluation for event coreference resolution. In RANLP
workshop on Events in Emerging Text Types.
Roxana Girju. 2003. Automatic detection of causal re-
lations for question answering. In ACL workshop on
Multilingual Summarization and Question Answering.
Olga Gurevich, Richard Crouch, Tracy Holloway King,
and Valeria de Paiva. 2008. Deverbal nouns in knowl-
edge representation. Journal of Logic and Computa-
tion, 18, June.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through unsupervised cross-document infer-
ence. In ACL.
Claudia Leacock and Martin Chodorow, 1998. Combin-
ing Local Context and WordNet Similarity for Word
Sense Identification. MIT Press.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A pdtb-styled end-to-end discourse parser. Tech-
nical report. http://www.comp.nus.edu.sg/ linzi-
hen/publications/tech2010.pdf.
Marie-catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh,
Alan Lee, Aravind Joshi, Livio Robaldo, and
Bonnie Webber. 2007. The penn discourse tree-
bank 2.0 annotation manual. Technical report.
http://www.seas.upenn.edu/ pdtb/PDTBAPI/pdtb-
annotation-manual.pdf.
Mehwish Riaz and Roxana Girju. 2010. Another look at
causality: Discovering scenario-specific contingency
relationships with no supervision. In ICSC.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In LREC.
Dan Roth and Wen Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In CoNLL.
Dan Roth and Wen Tau Yih. 2007. Global inference for
entity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Scheffczyk.
2010. FrameNet II: Extended Theory and Practice.
http://framenet.icsi.berkeley.edu.
Yizhou Sun, Ning Liu, Kunqing Xie, Shuicheng Yan,
Benyu Zhang, and Zheng Chen. 2007. Causal rela-
tion of queries from temporal logs. In WWW.
Patrick Suppes. 1970. A Probabilistic Theory of Causal-
ity. Amsterdam: North-Holland Publishing Company.
303
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 551?560,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Exploiting Syntactico-Semantic Structures for Relation Extraction
Yee Seng Chan and Dan Roth
University of Illinois at Urbana-Champaign
{chanys,danr}@illinois.edu
Abstract
In this paper, we observe that there exists a
second dimension to the relation extraction
(RE) problem that is orthogonal to the relation
type dimension. We show that most of these
second dimensional structures are relatively
constrained and not difficult to identify. We
propose a novel algorithmic approach to RE
that starts by first identifying these structures
and then, within these, identifying the seman-
tic type of the relation. In the real RE problem
where relation arguments need to be identi-
fied, exploiting these structures also allows re-
ducing pipelined propagated errors. We show
that this RE framework provides significant
improvement in RE performance.
1 Introduction
Relation extraction (RE) has been defined as the task
of identifying a given set of semantic binary rela-
tions in text. For instance, given the span of text
?. . . the Seattle zoo . . . ?, one would like to extract the
relation that ?the Seattle zoo? is located-at ?Seattle?.
RE has been frequently studied over the last few
years as a supervised learning task, learning from
spans of text that are annotated with a set of seman-
tic relations of interest. However, most approaches
to RE have assumed that the relations? arguments
are given as input (Chan and Roth, 2010; Jiang and
Zhai, 2007; Jiang, 2009; Zhou et al, 2005), and
therefore offer only a partial solution to the problem.
Conceptually, this is a rather simple approach as
all spans of texts are treated uniformly and are be-
ing mapped to one of several relation types of in-
terest. However, these approaches to RE require a
large amount of manually annotated training data to
achieve good performance, making it difficult to ex-
pand the set of target relations. Moreover, as we
show, these approaches become brittle when the re-
lations? arguments are not given but rather need to
be identified in the data too.
In this paper we build on the observation that there
exists a second dimension to the relation extraction
problem that is orthogonal to the relation type di-
mension: all relation types are expressed in one of
several constrained syntactico-semantic structures.
As we show, identifying where the text span is on the
syntactico-semantic structure dimension first, can be
leveraged in the RE process to yield improved per-
formance. Moreover, working in the second dimen-
sion provides robustness to the real RE problem, that
of identifying arguments along with the relations be-
tween them.
For example, in ?the Seattle zoo?, the entity men-
tion ?Seattle? modifies the noun ?zoo?. Thus, the
two mentions ?Seattle? and ?the Seattle zoo?, are
involved in what we later call a premodifier rela-
tion, one of several syntactico-semantic structures
we identify in Section 3.
We highlight that all relation types can be ex-
pressed in one of several syntactico-semantic struc-
tures ? Premodifiers, Possessive, Preposition, For-
mulaic and Verbal. As it turns out, most of these
structures are relatively constrained and are not dif-
ficult to identify. This suggests a novel algorith-
mic approach to RE that starts by first identifying
these structures and then, within these, identifying
the semantic type of the relation. Not only does this
approach provide significantly improved RE perfor-
551
mance, it carries with it two additional advantages.
First, leveraging the syntactico-semantic struc-
ture is especially beneficial in the presence of small
amounts of data. Second, and more important, is the
fact that exploiting the syntactico-semantic dimen-
sion provides several new options for dealing with
the full RE problem ? incorporating the argument
identification into the problem. We explore one of
these possibilities, making use of the constrained
structures as a way to aid in the identification of the
relations? arguments. We show that this already pro-
vides significant gain, and discuss other possibilities
that can be explored. The contributions of this paper
are summarized below:
? We highlight that all relation types are ex-
pressed as one of several syntactico-semantic
structures and show that most of these are rela-
tively constrained and not difficult to identify.
Consequently, working first in this structural
dimension can be leveraged in the RE process
to improve performance.
? We show that when one does not have a large
number of training examples, exploiting the
syntactico-semantic structures is crucial for RE
performance.
? We show how to leverage these constrained
structures to improve RE when the relations?
arguments are not given. The constrained struc-
tures allow us to jointly entertain argument can-
didates and relations built with them as argu-
ments. Specifically, we show that considering
argument candidates which otherwise would
have been discarded (provided they exist in
syntactico-semantic structures), we reduce er-
ror propagation along a standard pipeline RE
architecture, and that this joint inference pro-
cess leads to improved RE performance.
In the next section, we describe our relation ex-
traction framework that leverages the syntactico-
semantic structures. We then present these struc-
tures in Section 3. We describe our mention entity
typing system in Section 4 and features for the RE
system in Section 5. We present our RE experiments
in Section 6 and perform analysis in Section 7, be-
fore concluding in Section 8.
S = {premodifier, possessive, preposition, formulaic}
gold mentions in training data Mtrain
Dg = {(mi,mj) ?Mtrain ?Mtrain |
mi in same sentence as mj ? i 6= j ? i < j}
REbase = RE classifier trained on Dg
Ds = ?
for each (mi,mj) ? Dg
do
p = structure inference on (mi,mj) using patterns
if p ? S ? (mi,mj) was annotated with a S structure
Ds = Ds ? (mi,mj)
done
REs = RE classifier trained on Ds
Output: REbase and REs
Figure 1: Training a regular baseline RE classi-
fier REbase and a RE classifier leveraging syntactico-
semantic structures REs.
2 Relation Extraction Framework
In Figure 1, we show the algorithm for training
a typical baseline RE classifier (REbase), and for
training a RE classifier that leverages the syntactico-
semantic structures (REs).
During evaluation and when the gold mentions are
already annotated, we apply REs as follows. When
given a test example mention pair (xi,xj), we per-
form structure inference on it using the patterns de-
scribed in Section 3. If (xi,xj) is identified as hav-
ing any of the four syntactico-semantic structures S,
apply REs to predict the relation label, else apply
REbase.
Next, we show in Figure 2 our joint inference al-
gorithmic framework that leverages the syntactico-
semantic structures for RE, when mentions need to
be predicted. Since the structures are fairly con-
strained, we can use them to consider mention can-
didates that are originally predicted as non men-
tions. As shown in Figure 2, we conservatively in-
clude such mentions when forming mention pairs,
provided their null labels are predicted with a low
probability t1.
1In this work, we arbitrary set t=0.2. After the experiments,
and in our own analysis, we observe that t=0.25 achieves better
performance. Besides using the probability of the 1-best predic-
tion, one could also for instance, use the probability difference
between the first and second best predictions. However, select-
ing an optimal t value is not the main focus of this work.
552
S = {premodifier, possessive, preposition, formulaic}
candidate mentions Mcand
Let Lm = argmax
y
PMET (y|m, ?),m ?Mcand
selected mentions Msel = {m ?Mcand |
Lm 6= null ? PMET (null|m, ?) ? t}
QhasNull = {(mi,mj) ?Msel ?Msel |
mi in same sentence as mj ? i 6= j ? i < j ?
(Lmi 6= null ? Lmj 6= null)}
Let pool of relation predictions R = ?
for each (mi,mj) ? QhasNull
do
p = structure inference on (mi,mj) using patterns
if p ? S
r = relation prediction for (mi,mj) using REs
R = R? r
else if Lmi 6= null ? Lmj 6= null
r = relation prediction for (mi,mj) using REbase
R = R? r
done
Output: R
Figure 2: RE using predicted mentions and patterns. Ab-
breviations: Lm: predicted entity label for mention m us-
ing the mention entity typing (MET) classifier described
in Section 4; PMET : prediction probability according to
the MET classifier; t: used for thresholding.
There is a large body of work in using patterns
to extract relations (Fundel et al, 2007; Greenwood
and Stevenson, 2006; Zhu et al, 2009). However,
these works operate along the first dimension, that
of using patterns to mine for relation type examples.
In contrast, in our RE framework, we apply patterns
to identify the syntactico-semantic structure dimen-
sion first, and leverage this in the RE process. In
(Roth and Yih, 2007), the authors used entity types
to constrain the (first dimensional) relation types al-
lowed among them. In our work, although a few of
our patterns involve semantic type comparison, most
of the patterns are syntactic in nature.
In this work, we performed RE evaluation on the
NIST Automatic Content Extraction (ACE) corpus.
Most prior RE evaluation on ACE data assumed that
mentions are already pre-annotated and given as in-
put (Chan and Roth, 2010; Jiang and Zhai, 2007;
Zhou et al, 2005). An exception is the work of
(Kambhatla, 2004), where the author evaluated on
the ACE-2003 corpus. In that work, the author did
not address the pipelined errors propagated from the
mention identification process.
3 Syntactico-Semantic Structures
In this paper, we performed RE on the ACE-2004
corpus. In ACE-2004 when the annotators tagged a
pair of mentions with a relation, they also specified
the type of syntactico-semantic structure2. ACE-
2004 identified five types of structures: premodi-
fier, possessive, preposition, formulaic, and verbal.
We are unaware of any previous computational ap-
proaches that recognize these structures automati-
cally in text, as we do, and use it in the context of
RE (or any other problem). In (Qian et al, 2008), the
authors reported the recall scores of their RE system
on the various syntactico-semantic structures. But
they do not attempt to recognize nor leverage these
structures.
In this work, we focus on detecting the first four
structures. These four structures cover 80% of the
mention pairs having valid semantic relations (we
give the detailed breakdown in Section 7) and we
show that they are relatively easy to identify using
simple rules or patterns. In this section, we indicate
mentions using square bracket pairs, and use mi and
mj to represent a mention pair. We now describe the
four structures.
Premodifier relations specify the proper adjective
or proper noun premodifier and the following noun
it modifies, e.g.: [the [Seattle] zoo]
Possessive indicates that the first mention is in a
possessive case, e.g.: [[California] ?s Governor]
Preposition indicates that the two mentions are
semantically related via the existence of a preposi-
tion, e.g.: [officials] in [California]
Formulaic The ACE04 annotation guideline3 in-
dicates the annotation of several formulaic relations,
including for example address: [Medford] , [Mas-
sachusetts]
2ACE-2004 termed it as lexical condition. We use the term
syntactico-semantic structure in this paper as the mention pair
exists in specific syntactic structures, and we use rules or pat-
terns that are syntactically and semantically motivated to detect
these structures.
3http://projects.ldc.upenn.edu/ace/docs/EnglishRDCV4-3-
2.PDF
553
Structure type Pattern
Premodifier Basic pattern: [u* [v+] w+] , where u, v, w represent words
Each w is a noun or adjective
If u* is not empty, then u*: JJ+ ? JJ ?and? JJ? ? CD JJ* ? RB DT JJ? ? RB CD JJ ?
DT (RB|JJ|VBG|VBD|VBN|CD)?
Let w1 = first word in w+. w1 6= ??s? and POS tag of w1 6= POS
Let vl = last word in v+. POS tag of vl 6= PRP$ nor WP$
Possessive Basic pattern: [u? [v+] w+] , where u, v, w represent words
Let w1 = first word in w+. If w1 = ??s? ? POS tag of w1 = POS, accept mention pair
Let vl = last word in v+. If POS tag of vl = PRP$ or WP$, accept mention pair
Preposition Basic pattern: [mi] v* [mj], where v represent words
and number of prepositions in the text span v* between them = 0, 1, or 2
If satisfy pattern: IN [mi][mj], accept mention pair
If satisfy pattern: [mi] (IN|TO) [mj], accept mention pair
If all labels in Ld start with ?prep?, accept mention pair
Formulaic If satisfy pattern: [mi] / [mj] ? Ec(mi) = PER ? Ec(mj) = ORG, accept mention pair
If satisfy pattern: [mi][mj]
If Ec(mi) = PER ? Ec(mj) = ORG ? GPE, accept mention pair
Table 1: Rules and patterns for the four syntactico-semantic structures. Regular expression notations: ?*? matches
the preceding element zero or more times; ?+? matches the preceding element one or more times; ??? indicates that
the preceding element is optional; ?|? indicates or. Abbreviations: Ec(m): coarse-grained entity type of mention m;
Ld: labels in dependency path between the headword of two mentions. We use square brackets ?[? and ?]? to denote
mention boundaries. The ?/? in the Formulaic row denotes the occurrence of a lexical ?/? in text.
In this rest of this section, we present the
rules/patterns for detecting the above four
syntactico-semantic structure, giving an overview
of them in Table 1. We plan to release all of the
rules/patterns along with associated code4. Notice
that the patterns are intuitive and mostly syntactic in
nature.
3.1 Premodifier Structures
? We require that one of the mentions completely
include the other mention. Thus, the basic pat-
tern is [u* [v+] w+].
? If u* is not empty, we require that it satisfies
any of the following POS tag sequences: JJ+ ?
JJ and JJ? ? CD JJ*, etc. These are (optional)
POS tag sequences that normally start a valid
noun phrase.
? We use two patterns to differentiate between
premodifier relations and possessive relations,
by checking for the existence of POS tags
PRP$, WP$, POS, and the word ??s?.
4http://cogcomp.cs.illinois.edu/page/publications
3.2 Possessive Structures
? The basic pattern for possessive is similar to
that for premodifier: [u? [v+] w+]
? If the word immediately following v+ is ??s? or
its POS tag is ?POS?, we accept the mention
pair. If the POS tag of the last word in v+ is ei-
ther PRP$ or WP$, we accept the mention pair.
3.3 Preposition Structures
? We first require the two mentions to be non-
overlapping, and check for the existence of
patterns such as ?IN [mi] [mj]? and ?[mi]
(IN|TO) [mj]?.
? If the only dependency labels in the depen-
dency path between the head words of mi and
mj are ?prep? (prepositional modifier), accept
the mention pair.
3.4 Formulaic Structures
? The ACE-2004 annotator guidelines specify
that several relations such as reporter signing
off, addresses, etc. are often specified in stan-
dard structures. We check for the existence of
patterns such as ?[mi] / [mj]?, ?[mi] [mj]?,
554
Category Feature
For every POS of wk and offset from lw
word wk wk and offset from lw
in POS of wk, wk, and offset from lw
mention mi POS of wk, offset from lw, and lw
Bc(wk) and offset from lw
POS of wk, Bc(wk), and offset from lw
POS of wk, offset from lw, and Bc(lw)
Contextual C?1,?1 of mi
C+1,+1 of mi
P?1,?1 of mi
P+1,+1 of mi
NE tags tag of NE, if lw of NE coincides
with lw of mi in the sentence
Syntactic parse-label of parse tree constituent
parse that exactly covers mi
parse-labels of parse tree constituents
covering mi
Table 2: Features used in our mention entity typing
(MET) system. The abbreviations are as follows. lw:
last word in the mention; Bc(w): the brown cluster bit
string representing w; NE: named entity
and whether they satisfy certain semantic entity
type constraints.
4 Mention Extraction System
As part of our experiments, we perform RE using
predicted mentions. We first describe the features
(an overview is given in Table 2) and then describe
how we extract candidate mentions from sentences
during evaluation.
4.1 Mention Extraction Features
Features for every word in the mention For ev-
ery word wk in a mention mi, we extract seven fea-
tures. These are a combination of wk itself, its POS
tag, and its integer offset from the last word (lw) in
the mention. For instance, given the mention ?the
operation room?, the offsets for the three words in
the mention are -2, -1, and 0 respectively. These
features are meant to capture the word and POS tag
sequences in mentions.
We also use word clusters which are automat-
ically generated from unlabeled texts, using the
Brown clustering (Bc) algorithm of (Brown et al,
1992). This algorithm outputs a binary tree where
words are leaves in the tree. Each word (leaf) in the
tree can be represented by its unique path from the
Category Feature
POS POS of single word between m1, m2
hw of mi, mj and P?1,?1 of mi, mj
hw of mi, mj and P?1,?1 of mi, mj
hw of mi, mj and P+1,+1 of mi, mj
hw of mi, mj and P?2,?1 of mi, mj
hw of mi, mj and P?1,+1 of mi, mj
hw of mi, mj and P+1,+2 of mi, mj
Base chunk any base phrase chunk between mi, mj
Table 3: Additional RE features.
root and this path can be represented as a simple bit
string. As part of our features, we use the cluster bit
string representation of wk and lw.
Contextual We extract the word C?1,?1 immedi-
ately before mi, the word C+1,+1 immediately after
mi, and their associated POS tags P .
NE tags We automatically annotate the sentences
with named entity (NE) tags using the named en-
tity tagger of (Ratinov and Roth, 2009). This tagger
annotates proper nouns with the tags PER (person),
ORG (organization), LOC (location), or MISC (mis-
cellaneous). If the lw of mi coincides (actual token
offset) with the lw of any NE annotated by the NE
tagger, we extract the NE tag as a feature.
Syntactic parse We parse the sentences using the
syntactic parser of (Klein and Manning, 2003). We
extract the label of the parse tree constituent (if it ex-
ists) that exactly covers the mention, and also labels
of all constituents that covers the mention.
4.2 Extracting Candidate Mentions
From a sentence, we gather the following as candi-
date mentions: all nouns and possessive pronouns,
all named entities annotated by the the NE tagger
(Ratinov and Roth, 2009), all base noun phrase (NP)
chunks, all chunks satisfying the pattern: NP (PP
NP)+, all NP constituents in the syntactic parse tree,
and from each of these constituents, all substrings
consisting of two or more words, provided the sub-
strings do not start nor end on punctuation marks.
These mention candidates are then fed to our men-
tion entity typing (MET) classifier for type predic-
tion (more details in Section 6.3).
555
5 Relation Extraction System
We build a supervised RE system using sentences
annotated with entity mentions and predefined target
relations. During evaluation, when given a pair of
mentions mi, mj , the system predicts whether any
of the predefined target relation holds between the
mention pair.
Most of our features are based on the work of
(Zhou et al, 2005; Chan and Roth, 2010). Due to
space limitations, we refer the reader to our prior
work (Chan and Roth, 2010) for the lexical, struc-
tural, mention-level, entity type, and dependency
features. Here, we only describe the features that
were not used in that work.
As part of our RE system, we need to extract the
head word (hw) of a mention (m), which we heuris-
tically determine as follows: if m contains a prepo-
sition and a noun preceding the preposition, we use
the noun as the hw. If there is no preposition in m,
we use the last noun in m as the hw.
POS features If there is a single word between the
two mentions, we extract its POS tag. Given the hw
of m, Pi,j refers to the sequence of POS tags in the
immediate context of hw (we exclude the POS tag
of hw). The offsets i and j denote the position (rela-
tive to hw) of the first and last POS tag respectively.
For instance, P?2,?1 denotes the sequence of two
POS tags on the immediate left of hw, and P?1,+1
denotes the POS tag on the immediate left of hw and
the POS tag on the immediate right of hw.
Base phrase chunk We add a boolean feature to
detect whether there is any base phrase chunk in the
text span between the two mentions.
6 Experiments
We use the ACE-2004 dataset (catalog
LDC2005T09 from the Linguistic Data Con-
sortium) to conduct our experiments. Following
prior work, we use the news wire (nwire) and
broadcast news (bnews) corpora of ACE-2004 for
our experiments, which consists of 345 documents.
To build our RE system, we use the LIBLINEAR
(Fan et al, 2008) package, with its default settings
of L2-loss SVM (dual) as the solver, and we use an
epsilon of 0.1. To ensure that this baseline RE sys-
tem based on the features in Section 5 is competi-
tive, we compare against the state-of-the-art feature-
based RE systems of (Jiang and Zhai, 2007) and
(Chan and Roth, 2010). In these works, the au-
thors reported performance on undirected coarse-
grained RE. Performing 5-fold cross validation on
the nwire and bnews corpora, (Jiang and Zhai, 2007)
and (Chan and Roth, 2010) reported F-measures of
71.5 and 71.2, respectively. Using the same evalua-
tion setting, our baseline RE system achieves a com-
petitive 71.4 F-measure.
We build three RE classifiers: binary, coarse, fine.
Lumping all the predefined target relations into a
single label, we build a binary classifier to predict
whether any of the predefined relations exists be-
tween a given mention pair.
In this work, we model the argument order of the
mentions when performing RE, since relations are
usually asymmetric in nature. For instance, we con-
sider mi:EMP-ORG:mj and mj :EMP-ORG:mi to
be distinct relation types. In our experiments, we ex-
tracted a total of 55,520 examples or mention pairs.
Out of these, 4,011 are positive relation examples
annotated with 6 coarse-grained relation types and
22 fine-grained relation types5.
We build a coarse-grained classifier to disam-
biguate between 13 relation labels (two asymmetric
labels for each of the 6 coarse-grained relation types
and a null label). We similarly build a fine-grained
classifier to disambiguate between 45 relation labels.
6.1 Evaluation Method
For our experiments, we adopt the experimental set-
ting in our prior work (Chan and Roth, 2010) of en-
suring that all examples from a single document are
either all used for training, or all used for evaluation.
In that work, we also highlight that ACE anno-
tators rarely duplicate a relation link for coreferent
mentions. For instance, assume mentions mi, mj ,
and mk are in the same sentence, mentions mi and
mj are coreferent, and the annotators tag the men-
tion pair mj , mk with a particular relation r. The
annotators will rarely duplicate the same (implicit)
5We omit a single relation: Discourse (DISC). The ACE-
2004 annotation guidelines states that the DISC relation is es-
tablished only for the purposes of the discourse and does not
reference an official entity relevant to world knowledge. In this
work, we focus on semantically meaningful relations. Further-
more, the DISC relation is dropped in ACE-2005.
556
10 documents 5% of data 80% of data
RE model Rec% Pre% F1% Rec% Pre% F1% Rec% Pre% F1%
Binary 58.0 80.3 67.4 64.4 80.6 71.6 73.2 84.0 78.2
Binary+Patterns 73.1 78.5 75.7 (+8.3) 75.3 80.6 77.9 80.1 84.2 82.1
Coarse 33.5 62.5 43.6 42.4 66.2 51.7 62.1 75.5 68.1
Coarse+Patterns 44.2 59.6 50.8 (+7.2) 51.2 64.2 56.9 68.0 75.4 71.5
Fine 18.1 47.0 26.1 26.3 51.6 34.9 51.6 68.4 58.8
Fine+Patterns 24.8 43.5 31.6 (+5.5) 32.2 48.9 38.9 56.4 67.5 61.5
Table 4: Micro-averaged (across the 5 folds) RE results using gold mentions.
10 documents 5% of data 80% of data
RE model Rec% Pre% F1% Rec% Pre% F1% Rec% Pre% F1%
Binary 32.2 46.6 38.1 35.5 48.9 41.1 40.1 52.7 45.5
Binary+Patterns 46.7 45.9 46.3 (+8.2) 47.6 47.8 47.2 50.2 50.4 50.3
Coarse 18.6 41.1 25.6 22.4 40.9 28.9 32.3 47.5 38.5
Coarse+Patterns 26.8 34.7 30.2 (+4.6) 30.3 37.0 33.3 38.9 42.9 40.8
Fine 10.7 32.2 16.1 14.6 33.4 20.3 26.9 44.3 33.5
Fine+Patterns 15.7 26.3 19.7 (+3.6) 19.4 29.2 23.3 31.7 38.3 34.7
Table 5: Micro-averaged (across the 5 folds) RE results using predicted mentions.
relation r between mi and mk, thus leaving the gold
relation label as null. Whether this is correct or not is
debatable. However, to avoid being penalized when
our RE system actually correctly predicts the label
of an implicit relation, we take the following ap-
proach.
During evaluation, if our system correctly pre-
dicts an implicit label, we simply switch its predic-
tion to the null label. Since the RE recall scores
only take into account non-null relation labels, this
scoring method does not change the recall, but could
marginally increase the precision scores by decreas-
ing the count of RE predictions. In our experi-
ments, we observe that both the usual and our scor-
ing method give very similar RE results and the ex-
perimental trends remain the same. Of course, us-
ing this scoring method requires coreference infor-
mation, which is available in the ACE data.
6.2 RE Evaluation Using Gold Mentions
To perform our experiments, we split the 345 docu-
ments into 5 equal sets. In each of the 5 folds, 4 sets
(276 documents) are reserved for drawing training
examples, while the remaining set (69 documents)
is used as evaluation data. In the experiments de-
scribed in this section, we use the gold mentions
available in the data.
When one only has a small amount of train-
ing data, it is crucial to take advantage of external
knowledge such as the syntactico-semantic struc-
tures. To simulate this setting, in each fold, we ran-
domly selected 10 documents from the fold?s avail-
able training documents (about 3% of the total 345
documents) as training data. We built one binary,
one coarse-grained, and one fine-grained classifier
for each fold.
In Section 2, we described how we trained a base-
line RE classifier (REbase) and a RE classifier using
the syntactico-semantic patterns (REs).
We first apply REbase on each test example men-
tion pair (mi,mj) to obtain the RE baseline results,
showing these in Table 4 under the column ?10 doc-
uments?, and in the rows ?Binary?, ?Coarse?, and
?Fine?. We then applied REs on the test exam-
ples as described in Section 2, showing the results
in the rows ?Binary+Patterns?, ?Coarse+Patterns?,
and ?Fine+Patterns?. The results show that by us-
ing syntactico-semantic structures, we obtain signif-
icant F-measure improvements of 8.3, 7.2, and 5.5
for binary, coarse-grained, and fine-grained relation
predictions respectively.
6.3 RE Evaluation Using Predicted Mentions
Next, we perform our experiments using predicted
mentions. ACE-2004 defines 7 coarse-grained entity
types, each of which are then refined into 43 fine-
557
 0
 1
 2
 3
 4
 5
 6
 7
 8
 5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80
RE
 F
1(%
) Im
pro
ve
me
nt
Proportion (%) of data used for training
Improvement in (gold mentions) RE by using patterns
Binary+Pattern
Coarse+Pattern
Fine+Pattern
Figure 3: Improvement in (gold mention) RE.
grained entity types. Using the ACE data annotated
with mentions and predefined entity types, we build
a fine-grained mention entity typing (MET) clas-
sifier to disambiguate between 44 labels (43 fine-
grained and a null label to indicate not a mention).
To obtain the coarse-grained entity type predictions
from the classifier, we simply check which coarse-
grained type the fine-grained prediction belongs to.
We use the LIBLINEAR package with the same set-
tings as earlier specified for the RE system. In each
fold, we build a MET classifier using all the (276)
training documents in that fold.
We apply REbase on all mention pairs (mi,mj)
where both mi and mj have non null entity type pre-
dictions. We show these baseline results in the Rows
?Binary?, ?Coarse?, and ?Fine? of Table 5.
In Section 2, we described our algorithmic ap-
proach (Figure 2) that takes advantage of the struc-
tures with predicted mentions. We show the results
of this approach in the Rows ?Binary+Patterns?,
?Coarse+Patterns?, and ?Fine+Patterns? of Table
5. The results show that by leveraging syntactico-
semantic structures, we obtain significant F-measure
improvements of 8.2, 4.6, and 3.6 for binary, coarse-
grained, and fine-grained relation predictions re-
spectively.
7 Analysis
We first show statistics regarding the syntactico-
semantic structures. In Section 3, we mentioned
that ACE-2004 identified five types of structures:
premodifier, possessive, preposition, formulaic, and
 0
 1
 2
 3
 4
 5
 6
 7
 8
 5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80
RE
 F
1(%
) Im
pro
ve
me
nt
Proportion (%) of data used for training
Improvement in (predicted mentions) RE by using patterns
Binary+Pattern
Coarse+Pattern
Fine+Pattern
Figure 4: Improvement in (predicted mention) RE.
Pattern type Rec% Pre%
PreMod 86.8 79.7
Poss 94.3 88.3
Prep 94.6 20.0
Formula 85.5 62.2
Table 6: Recall and precision of the patterns.
verbal. On the 4,011 examples that we experimented
on, premodifiers are the most frequent, account-
ing for 30.5% of the examples (or about 1,224 ex-
amples). The occurrence distributions of the other
structures are 18.9% (possessive), 23.9% (preposi-
tion), 7.2% (formulaic), and 19.5% (verbal). Hence,
the four syntactico-semantic structures that we fo-
cused on in this paper account for a large majority
(80%) of the relations.
In Section 6, we note that out of 55,520 men-
tion pairs, only 4,011 exhibit valid relations. Thus,
the proportion of positive relation examples is very
sparse at 7.2%. If we can effectively identify and
discard most of the negative relation examples, it
should improve RE performance, including yielding
training data with a more balanced label distribution.
We now analyze the utility of the patterns. As
shown in Table 6, the patterns are effective in infer-
ring the structure of mention pairs. For instance, ap-
plying the premodifier patterns on the 55,520 men-
tion pairs, we correctly identified 86.8% of the 1,224
premodifier occurrences as premodifiers, while in-
curring a false-positive rate of only about 20%6. We
6Random selection will give a precision of about 2.2%
(1,224 out of 55,520) and thus a false-positive rate of 97.8%
558
note that preposition structures are relatively harder
to identify. Some of the reasons are due to possi-
bly multiple prepositions in between a mention pair,
preposition sense ambiguity, pp-attachment ambigu-
ity, etc. However, in general, we observe that infer-
ring the structures allows us to discard a large por-
tion of the mention pairs which have no valid re-
lation between them. The intuition behind this is
the following: if we infer that there is a syntactico-
semantic structure between a mention pair, then it
is likely that the mention pair exhibits a valid rela-
tion. Conversely, if there is a valid relation between
a mention pair, then it is likely that there exists a
syntactico-semantic structure between the mentions.
Next, we repeat the experiments in Section 6.2
and Section 6.3, while gradually increasing the
amount of training data used for training the RE
classifiers. The detailed results of using 5% and 80%
of all available data are shown in Table 4 and Table
5. Note that these settings are with respect to all 345
documents and thus the 80% setting represents us-
ing all 276 training documents in each fold. We plot
the intermediate results in Figure 3 and Figure 4. We
note that leveraging the structures provides improve-
ments on all experimental settings. Also, intuitively,
the binary predictions benefit the most from lever-
aging the structures. How to further exploit this is a
possible future work.
8 Conclusion
In this paper, we propose a novel algorithmic ap-
proach to RE by exploiting syntactico-semantic
structures. We show that this approach provides
several advantages and improves RE performance.
There are several interesting directions for future
work. There are probably many near misses when
we apply our structure patterns on predicted men-
tions. For instance, for both premodifier and posses-
sive structures, we require that one mention com-
pletely includes the other. Relaxing this might
potentially recover additional valid mention pairs
and improve performance. We could also try to
learn classifiers to automatically identify and disam-
biguate between the different syntactico-semantic
structures. It will also be interesting to feedback the
predictions of the structure patterns to the mention
entity typing classifier and possibly retrain to obtain
a better classifier.
Acknowledgements This research is supported by
the Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
We thank Ming-Wei Chang and Quang Do for
building the mention extraction system.
References
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Yee Seng Chan and Dan Roth. 2010. Exploiting back-
ground knowledge for relation extraction. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING), pages 152?160.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer. 2007.
Relex ? Relation extraction using dependency parse
trees. Bioinformatics, 23(3):365?371.
Mark A. Greenwood and Mark Stevenson. 2006. Im-
proving semi-supervised acquisition of relation extrac-
tion patterns. In Proceedings of the COLING-ACL
Workshop on Information Extraction Beyond The Doc-
ument, pages 29?35.
Jing Jiang and ChengXiang Zhai. 2007. A systematic
exploration of the feature space for relation extraction.
In Proceedings of Human Language Technologies -
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL), pages 113?120.
Jing Jiang. 2009. Multi-task transfer learning for
weakly-supervised relation extraction. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics and International Joint Confer-
ence on Natural Language Processing (ACL-IJCNLP),
pages 1012?1020.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for information extraction. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 178?181.
559
Dan Klein and Christoper D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In The Conference on Advances in Neural
Information Processing Systems (NIPS), pages 3?10.
Longhua Qian, Guodong Zhou, Qiaomin Zhu, and Peide
Qian. 2008. Relation extraction using convolution
tree kernel expanded with entity features. In Pacific
Asia Conference on Language, Information and Com-
putation, pages 415?421.
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Pro-
ceedings of the Annual Conference on Computational
Natural Language Learning (CoNLL), pages 147?155.
Dan Roth and Wen Tau Yih. 2007. Global inference for
entity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation extrac-
tion. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
427?434.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-
Rong Wen. 2009. Statsnowball: a statistical approach
to extracting entity relationships. In The International
World Wide Web Conference, pages 101?110.
560
