Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1090?1099,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Soft-Supervised Learning for Text Classification
Amarnag Subramanya & Jeff Bilmes
Dept. of Electrical Engineering,
University of Washington, Seattle, WA 98195, USA.
{asubram,bilmes}@ee.washington.edu
Abstract
We propose a new graph-based semi-
supervised learning (SSL) algorithm and
demonstrate its application to document
categorization. Each document is represented
by a vertex within a weighted undirected
graph and our proposed framework minimizes
the weighted Kullback-Leibler divergence
between distributions that encode the class
membership probabilities of each vertex. The
proposed objective is convex with guaranteed
convergence using an alternating minimiza-
tion procedure. Further, it generalizes in
a straightforward manner to multi-class
problems. We present results on two stan-
dard tasks, namely Reuters-21578 and
WebKB, showing that the proposed algorithm
significantly outperforms the state-of-the-art.
1 Introduction
Semi-supervised learning (SSL) employs small
amounts of labeled data with relatively large
amounts of unlabeled data to train classifiers. In
many problems, such as speech recognition, doc-
ument classification, and sentiment recognition,
annotating training data is both time-consuming
and tedious, while unlabeled data are easily ob-
tained thus making these problems useful appli-
cations of SSL. Classic examples of SSL algo-
rithms include self-training (Yarowsky, 1995) and
co-training (Blum and Mitchell, 1998). Graph-
based SSL algorithms are an important class of SSL
techniques that have attracted much of attention of
late (Blum and Chawla, 2001; Zhu et al, 2003).
Here one assumes that the data (both labeled and
unlabeled) is embedded within a low-dimensional
manifold expressed by a graph. In other words,
each data sample is represented by a vertex within
a weighted graph with the weights providing a mea-
sure of similarity between vertices.
Most graph-based SSL algorithms fall under one
of two categories ? those that use the graph structure
to spread labels from labeled to unlabeled samples
(Szummer and Jaakkola, 2001; Zhu and Ghahra-
mani, 2002) and those that optimize a loss function
based on smoothness constraints derived from the
graph (Blum and Chawla, 2001; Zhu et al, 2003;
Joachims, 2003; Belkin et al, 2005). Sometimes the
two categories are similar in that they can be shown
to optimize the same underlying objective (Zhu and
Ghahramani, 2002; Zhu et al, 2003). In general
graph-based SSL algorithms are non-parametric and
transductive.1 A learning algorithm is said to be
transductive if it is expected to work only on a closed
data set, where a test set is revealed at the time of
training. In practice, however, transductive learners
can be modified to handle unseen data (Zhu, 2005a;
Sindhwani et al, 2005). A common drawback of
many graph-based SSL algorithms (e.g. (Blum and
Chawla, 2001; Joachims, 2003; Belkin et al, 2005))
is that they assume binary classification tasks and
thus require the use of sub-optimal (and often com-
putationally expensive) approaches such as one vs.
rest to solve multi-class problems, let alne struc-
tured domains such as strings and trees. There are
also issues related to degenerate solutions (all un-
labeled samples classified as belonging to a single
1Excluding Manifold Regularization (Belkin et al, 2005).
1090
class) (Blum and Chawla, 2001; Joachims, 2003;
Zhu and Ghahramani, 2002). For more background
on graph-based and general SSL and their applica-
tions, see (Zhu, 2005a; Chapelle et al, 2007; Blitzer
and Zhu, 2008).
In this paper we propose a new algorithm for
graph-based SSL and use the task of text classifica-
tion to demonstrate its benefits over the current state-
of-the-art. Text classification involves automatically
assigning a given document to a fixed number of se-
mantic categories. Each document may belong to
one, many, or none of the categories. In general,
text classification is a multi-class problem (more
than 2 categories). Training fully-supervised text
classifiers requires large amounts of labeled data
whose annotation can be expensive (Dumais et al,
1998). As a result there has been interest is us-
ing SSL techniques for text classification (Joachims,
1999; Joachims, 2003). However past work in semi-
supervised text classification has relied primarily on
one vs. rest approaches to overcome the inherent
multi-class nature of this problem. We believe such
an approach may be sub-optimal because, disregard-
ing data overlap, the different classifiers have train-
ing procedures that are independent of one other.
In order to address the above drawback we pro-
pose a new framework based on optimizing a loss
function composed of Kullback-Leibler divergence
(KL-divergence) (Cover and Thomas, 1991) terms
between probability distributions defined for each
graph vertex. The use of probability distributions,
rather than fixed integer labels, not only leads to a
straightforward multi-class generalization, but also
allows us to exploit other well-defined functions of
distributions, such as entropy, to improve system
performance and to allow for the measure of uncer-
tainty. For example, with a single integer, at most all
we know is its assignment. With a distribution, we
can continuously move from knowing an assignment
with certainty (i.e., an entropy of zero) to expres-
sions of doubt or multiple valid possibilities (i.e., an
entropy greater than zero). This is particularly use-
ful for document classification as we will see. We
also show how one can use the alternating minimiza-
tion (Csiszar and Tusnady, 1984) algorithm to op-
timize our objective leading to a relatively simple,
fast, easy-to-implement, guaranteed to converge, it-
erative, and closed form update for each iteration.
2 Proposed Graph-Based Learning
Framework
We consider the transductive learning problem, i.e.,
given a training setD = {Dl,Du}, whereDl andDu
are the sets of labeled and unlabeled samples respec-
tively, the task is to infer the labels for the samples
in Du. In other words, Du is the ?test-set.? Here
Dl = {(xi, yi)}li=1, Du = {xi}
l+u
i=l+1, xi ? X (the
input space of the classifier, and corresponds to vec-
tors of features) and yi ? Y (the space of classifier
outputs, and for our case is the space of non-negative
integers). Thus |Y| = 2 yields binary classifica-
tion while |Y| > 2 yields multi-class. We define
n = l + u, the total number of samples in the train-
ing set. Given D, most graph-based SSL algorithms
utilize an undirected weighted graph G = (V,E)
where V = {1, . . . , n} are the data points in D
and E = V ? V are the set of undirected edges
between vertices. We use wij ? W to denote the
weight of the edge between vertices i and j. W is
referred to as the weight (or affinity) matrix of G.
As will be seen shortly, the input features xi effect
the final classification results via W, i.e., the graph.
Thus graph construction is crucial to the success of
any graph-based SSL algorithm. Graph construction
?is more of an art, than science? (Zhu, 2005b) and
is an active research area (Alexandrescu and Kirch-
hoff, 2007). In general the weights are formed as
wij = sim(xi,xj)?(j ? K(i)). Here K(i) is the set
of i?s k-nearest-neighbors (KNN), sim(xi,xj) is a
given measure of similarity between xi and xj , and
?(c) returns a 1 if c is true and 0 otherwise. Getting
the similarity measure right is crucial for the success
of any SSL algorithm as that is what determines the
graph. Note that setting K(i) = |V | = n results
in a fully-connected graph. Some popular similarity
measures include
sim(xi,xj) = e
?
?xi?xj?
2
2
?2 or
sim(xi,xj) = cos(xi,xj) =
?xi,xj?
? xi ?2
2
? xj ?2
2
where ? xi ?2 is the L2 norm, and ?xi,xj? is the
inner product of xi and xj . The first similarity mea-
sure is an RBF kernel applied on the squared Eu-
clidean distance while the second is cosine similar-
ity. In this paper all graphs are constructed using
cosine similarity.
1091
We next introduce our proposed approach. For
every i ? V , we define a probability distribution pi
over the elements of Y. In addition let rj , j = 1 . . . l
be another set of probability distributions again over
the elements of Y (recall, Y is the space of classi-
fier outputs). Here {rj}j represents the labels of the
supervised portion of the training data. If the label
for a given labeled data point consists only of a sin-
gle integer, then the entropy of the corresponding rj
is zero (the probability of that integer will be unity,
with the remaining probabilities being zero). If, on
the other hand, the ?label? for a given labeled data
point consists of a set of integers (e.g., if the object
is a member of multiple classes), then rj is able to
represent this property accordingly (see below). We
emphasize again that both pi and rj are probability
distributions, with rj fixed throughout training. The
goal of learning in this paper is to find the best set
of distributions pi, ?i that attempt to: 1) agree with
the labeled data rj wherever it is available; 2) agree
with each other (when they are close according to a
graph); and 3) be smooth in some way. These cri-
teria are captured in the following new multi-class
SSL optimization procedure:
min
p
C
1
(p), where C
1
(p) =
[
l?
i=1
DKL
(
ri||pi
)
+?
n?
i
?
j
wijDKL
(
pi||pj
)
? ?
n?
i=1
H(pi)
?
?
,
(1)
and where p , (p
1
, . . . , pn) denotes the en-
tire set of distributions to be learned, H(pi) =
?
?
y
pi(y) log pi(y) is the standard Shannon en-
tropy function of pi, DKL(pi||qj) is the KL-
divergence between pi and qj , and ? and ? are hy-
perparameters whose selection we discuss in section
5. The distributions ri are derived from Dl (as men-
tioned above) and this can be done in one of the fol-
lowing ways: (a) if y?i is the single supervised label
for input xi then ri(y) = ?(y = y?i), which means
that ri gives unity probability for y equaling the la-
bel y?i; (b) if y?i = {y?
(1)
i , . . . , y?
(k)
i }, k ? |Y| is a set
of possible outputs for input xi, meaning an object
validly falls into all of the corresponding categories,
we set ri(y) = (1/k)?(y ? y?i) meaning that ri is
uniform over only the possible categories and zero
otherwise; (c) if the labels are somehow provided
in the form of a set of non-negative scores, or even
a probability distribution itself, we just set ri to be
equal to those scores (possibly) normalized to be-
come a valid probability distribution. Among these
three cases, case (b) is particularly relevant to text
classification as a given document many belong to
(and in practice may be labeled as) many classes.
The final classification results, i.e., the final labels
for Du, are then given by y? = argmax
y?Y
pi(y).
We next provide further intuition on our objective
function. SSL on a graph consists of finding a la-
belingDu that is consistent with both the labels pro-
vided in Dl and the geometry of the data induced
by the graph. The first term of C
1
will penalize
the solution pi i ? {1, . . . , l}, when it is far away
from the labeled training data Dl, but it does not in-
sist that pi = ri, as allowing for deviations from ri
can help especially with noisy labels (Bengio et al,
2007) or when the graph is extremely dense in cer-
tain regions. As explained above, our framework al-
lows for the case where supervised training is uncer-
tain or ambiguous. We consider it reasonable to call
our approach soft-supervised learning, generalizing
the notion of semi-supervised learning, since there
is even more of a continuum here between fully su-
pervised and fully unsupervised learning than what
typically exists with SSL. Soft-supervised learning
allows uncertainty to be expressed (via a probability
distribution) about any of the labels individually.
The second term of C
1
penalizes a lack of con-
sistency with the geometry of the data and can be
seen as a graph regularizer. If wij is large, we prefer
a solution in which pi and pj are close in the KL-
divergence sense. While KL-divergence is asym-
metric, given that G is undirected implies W is sym-
metric (wij = wji) and as a result the second term
is inherently symmetric.
The last term encourages each pi to be close to
the uniform distribution if not preferred to the con-
trary by the first two terms. This acts as a guard
against degenerate solutions commonly encountered
in SSL (Blum and Chawla, 2001; Joachims, 2003).
For example, consider the case where part of the
graph is almost completely disconnected from any
labeled vertex (which is possible in the k-nearest
neighbor case). In such situations the third term en-
1092
sures that the nodes in this disconnected region are
encouraged to yield a uniform distribution, validly
expressing the fact that we do not know the labels of
these nodes based on the nature of the graph. More
generally, we conjecture that by maximizing the en-
tropy of each pi, the classifier has a better chance of
producing high entropy results in graph regions of
low confidence (e.g. close to the decision boundary
and/or low density regions). This overcomes a com-
mon drawback of a large number of state-of-the-art
classifiers that tend to be confident even in regions
close to the decision boundary.
We conclude this section by summarizing some of
the features of our proposed framework. It should
be clear that C
1
uses the ?manifold assumption?
for SSL (see chapter 2 in (Chapelle et al, 2007))
? it assumes that the input data can be embed-
ded within a low-dimensional manifold (the graph).
As the objective is defined in terms of probability
distributions over integers rather than just integers
(or to real-valued relaxations of integers (Joachims,
2003; Zhu et al, 2003)), the framework general-
izes in a straightforward manner to multi-class prob-
lems. Further, all the parameters are estimated
jointly (compare to one vs. rest approaches which
involve solving |Y| independent problems). Fur-
thermore, the objective is capable of handling label
training data uncertainty (Pearl, 1990). Of course,
this objective would be useless if it wasn?t possible
to efficiently and easily optimize it on large data sets.
We next describe a method that can do this.
3 Learning with Alternating Minimization
As long as ?, ? ? 0, the objective C
1
(p) is con-
vex. This follows since DKL(pi||pj) is convex in
the pair (pi, pj) (Cover and Thomas, 1991), nega-
tive entropy is convex, and a positive-weighted lin-
ear combination of a set of convex functions is con-
vex. Thus, the problem of minimizing C
1
over the
space of collections of probability distributions (a
convex set) constitutes a convex programming prob-
lem (Bertsekas, 2004). This property is extremely
beneficial since there is a unique global optimum
and there are a variety of methods that can be used
to yield that global optimum. One possible method
might take the derivative of the objective along with
Lagrange multipliers to ensure that we stay within
the space of probability distributions. This method
can sometimes yield a closed form single-step an-
alytical expression for the globally optimum solu-
tion. Unfortunately, however, our problem does not
admit such a closed form solution because the gra-
dient of C
1
(p) with respect to pi(y) is of the form,
k
1
pi(y) log pi(y) + k2pi(y) + k3 (where k1, k2, k3
are fixed constants). Sometimes, optimizing the dual
of the objective can also produce a solution, but un-
fortunately again the dual of our objective also does
not yield a closed form solution. The typical next
step, then, is to resort to iterative techniques such
as gradient descent along with modifications to en-
sure that the solution stays within the set of proba-
bility distributions (the gradient of C
1
alone will not
necessarily point in the direction where p is still a
valid distribution) - one such modification is called
the method of multipliers (MOM). Another solu-
tion would be to use computationally complex (and
complicated) algorithms like interior point methods
(IPM). While all of the above methods (described
in detail in (Bertsekas, 2004)) are feasible ways to
solve our problem, they each have their own draw-
backs. Using MOM, for example, requires the care-
ful tuning of a number of additional parameters such
as learning rates, growth factors, and so on. IPM in-
volves inverting a matrix of the order of the number
of variables and constraints during each iteration.
We instead adopt a different strategy based on al-
ternating minimization (Csiszar and Tusnady, 1984).
This approach has a single additional optimization
parameter (contrasted with MOM), admits a closed
form solution for each iteration not involving any
matrix inversion (contrasted with IPM), and yields
guaranteed convergence to the global optimum. In
order to render our approach amenable to AM, how-
ever, we relax our objective C
1
by defining a new
(third) set of distributions for all training samples qi,
i = 1, . . . , n denoted collectively like the above us-
ing the notation q , (q
1
, . . . , qn). We define a new
objective to be optimized as follows:
min
p,q
C
2
(p, q), where C
2
(p, q) =
[
l?
i=1
DKL
(
ri||qi
)
+?
n?
i=1
?
j?N (i)
w
?
ijDKL
(
pi||qj
)
? ?
n?
i=1
H(pi)
?
?
.
1093
Before going further, the reader may be wondering
at this juncture how might it be desirable for us to
have apparently complicated the objective function
in an attempt to yield a more computationally and
methodologically superior machine learning proce-
dure. This is indeed the case as will be spelled out
below. First, in C
2
we have defined a new weight
matrix [W ?]ij = w?ij of the same size as the original
where W ? = W + ?In, where In is the n? n iden-
tity matrix, and where ? ? 0 is a non-negative con-
stant (this is the optimization related parameter men-
tioned above). This has the effect that w?ii ? wii.
In the original objective C
1
, wii is irrelevant since
DKL(p||p) = 0 for all p, but since there are now two
distributions for each training point, there should be
encouragement for the two to approach each other.
Like C
1
, the first term of C
2
ensures that the la-
beled training data is respected and the last term is
a smoothness regularizer, but these are done via dif-
ferent sets of distributions, q and p respectively ?
this choice is what makes possible the relatively sim-
ple analytical update equations given below. Next,
we see that the two objective functions in fact have
identical solutions when the optimization enforces
the constraint that p and q are equal:
min
(p,q):p=q
C
2
(p, q) = min
p
C
1
(p).
Indeed, as ? gets large, the solutions considered vi-
able are those only where p = q. We thus have that:
lim
???
min
p,q
C
2
(p, q) = min
p
C
1
(p).
Therefore, the two objectives should yield the same
solution as long as ? ? wij for all i, j. A key advan-
tage of this relaxed objective is that it is amenable to
alternating minimization, a method to produce a se-
quence of sets of distributions (pn, qn) as follows:
pn = argmin
p
C
2
(p, qn?1), qn = argmin
q
C
2
(pn, q).
It can be shown (we omit the rather lengthy proof
due to space constraints) that the sequence gener-
ated using the above minimizations converges to the
minimum of C
2
(p, q), i.e.,
lim
n??
C
2
(p(n), q(n)) = inf
p,q
C
2
(p, q),
provided we start with a distribution that is initial-
ized properly q(0)(y) > 0 ? y ? Y. The update
equations for p(n) and q(n) are given by
p
(n)
i (y) =
1
Zi
exp
?
(n?1)
i (y)
?i
,
q
(n)
i (y) =
ri(y)?(i ? l) + ?
?
j w
?
jip
(n)
j (y)
?(i ? l) + ?
?
j w
?
ji
,
where
?i = ? + ?
?
j
w
?
ij ,
?
(n?1)
i (y) = ?? + ?
?
j
w
?
ij(log q
(n?1)
j (y)? 1)
and where Zi is a normalizing constant to ensure pi
is a valid probability distribution. Note that each it-
eration of the proposed framework has a closed form
solution and is relatively simple to implement, even
for very large graphs. Henceforth we refer to the
proposed objective optimized using alternating min-
imization as AM.
4 Connections to Other Approaches
Label propagation (LP) (Zhu and Ghahramani,
2002) is a graph-based SSL algorithms that per-
forms Markov random walks on the graph and has
a straightforward extension to multi-class problems.
The update equations for LP (which also we use for
our LP implementations) may be written as
p
(n)
i (y) =
ri(y)?(i ? l) + ?(i > l)
?
j wijp
(n?1)
j (y)
?(i ? l) + ?(i > l)
?
j wij
Note the similarity to the update equation for q(n)i in
our AM case. It has been shown that the squared-
loss based SSL algorithm (Zhu et al, 2003) and LP
have similar updates (Bengio et al, 2007).
The proposed objective C
1
is similar in spirit to
the squared-loss based objective in (Zhu et al, 2003;
Bengio et al, 2007). Our method, however, differs
in that we are optimizing the KL-divergence over
probability distributions. We show in section 5 that
KL-divergence based loss significantly outperforms
the squared-loss. We believe that this could be due
1094
to the following: 1) squared loss is appropriate un-
der a Gaussian loss model which may not be opti-
mal under many circumstances (e.g. classification);
2) KL-divergence DKL(p||q) is based on a relative
(relative to p) rather than an absolute error; and 3)
under certain natural assumptions, KL-divergence is
asymptotically consistent with respect to the under-
lying probability distributions.
AM is also similar to the spectral graph trans-
ducer (Joachims, 2003) in that they both attempt
to find labellings over the unlabeled data that re-
spect the smoothness constraints of the graph. While
spectral graph transduction is an approximate solu-
tion to a discrete optimization problem (which is NP
hard), AM is an exact solution obtained by optimiz-
ing a convex function over a continuous space. Fur-
ther, while spectral graph transduction assumes bi-
nary classification problems, AM naturally extends
to multi-class situations without loss of convexity.
Entropy Minimization (EnM) (Grandvalet and
Bengio, 2004) uses the entropy of the unlabeled data
as a regularizer while optimizing a parametric loss
function defined over the labeled data. While the
objectives in the case of both AM and EnM make
use of the entropy of the unlabeled data, there are
several important differences: (a) EnM is not graph-
based, (b) EnM is parametric whereas our proposed
approach is non-parametric, and most importantly,
(c) EnM attempts to minimize entropy while the pro-
posed approach aims to maximize entropy. While
this may seem a triviality, it has catastrophic conse-
quences in terms of both the mathematics and mean-
ing. The objective in case of EnM is not convex,
whereas in our case we have a convex formulation
with simple update equations and convergence guar-
antees.
(Wang et al, 2008) is a graph-based SSL al-
gorithm that also employs alternating minimiza-
tion style optimization. However, it is inherently
squared-loss based which our proposed approach
out-performs (see section 5). Further, they do not
provide or state convergence guarantees and one
side of their update approximates an NP-complete
optimization procedure.
The information regularization (IR) (Corduneanu
and Jaakkola, 2003) algorithm also makes use of
a KL-divergence based loss for SSL. Here the in-
put space is divided into regions {Ri} which might
or might not overlap. For a given point xi ? Ri,
IR attempts to minimize the KL-divergence between
pi(yi|xi) and p?Ri(y), the agglomerative distribution
for region Ri. Given a graph, one can define a re-
gion to be a vertex and its neighbor thus making IR
amenable to graph-based SSL. In (Corduneanu and
Jaakkola, 2003), the agglomeration is performed by
a simple averaging (arithmetic mean). While IR sug-
gests (without proof of convergence) the use of al-
ternating minimization for optimization, one of the
steps of the optimization does not admit a closed-
form solution. This is a serious practical drawback
especially in the case of large data sets. (Tsuda,
2005) (hereafter referred to as PD) is an extension of
the IR algorithm to hypergraphs where the agglom-
eration is performed using the geometric mean. This
leads to closed form solutions in both steps of the al-
ternating minimization. There are several important
differences between IR and PD on one side and our
proposed approach: (a) neither IR nor PD use an
entropy regularizer, and (b) the update equation for
one of the steps of the optimization in the case of
PD (equation 13 in (Tsuda, 2005)) is actually a spe-
cial case of our update equation for pi(y) and may
be obtained by setting wij = 1/2. Further, our work
here may be easily extended to hypergraphs.
5 Results
We compare our algorithm (AM) with other
state-of-the-art SSL-based text categorization al-
gorithms, namely, (a) SVM (Joachims, 1999),
(b) Transductive-SVM (TSVM) (Joachims, 1999),
(c) Spectral Graph Transduction (SGT) (Joachims,
2003), and (d) Label Propagation (LP) (Zhu and
Ghahramani, 2002). Note that only SGT and LP
are graph-based algorithms, while SVM is fully-
supervised (i.e., it does not make use of any of the
unlabeled data). We implemented SVM and TSVM
using SVM Light (Joachims, b) and SGT using SGT
Light (Joachims, a). In the case of SVM, TSVM and
SGT we trained |Y| classifiers (one for each class) in
a one vs. rest manner precisely following (Joachims,
2003).
5.1 Reuters-21578
We used the ?ModApte? split of the Reuters-21578
dataset collected from the Reuters newswire in
1095
1987 (Lewis et al, 1987). The corpus has 9,603
training (not to be confused with D) and 3,299 test
documents (which representsDu). Of the 135 poten-
tial topic categories only the 10 most frequent cate-
gories are used (Joachims, 1999). Categories outside
the 10 most frequent were collapsed into one class
and assigned a label ?other?. For each document i
in the training and test sets, we extract features xi in
the following manner: stop-words are removed fol-
lowed by the removal of case and information about
inflection (i.e., stemming) (Porter, 1980). We then
compute TFIDF features for each document (Salton
and Buckley, 1987). All graphs were constructed us-
ing cosine similarity with TFIDF features.
For this task Y = { earn, acq, money, grain,
crude, trade, interest, ship, wheat, corn, average}.
For LP and AM, we use the output space Y? = Y?{
other }. For documents in Dl that are labeled with
multiple categories, we initialize ri to have equal
non-zero probability for each such category. For
example, if document i is annotated as belonging
to classes { acq, grain, wheat}, then ri(acq) =
ri(grain) = ri(wheat) = 1/3.
We created 21 transduction sets by randomly sam-
pling l documents from the training set with the con-
straint that each of 11 categories (top 10 categories
and the class other) are represented at least once in
each set. These samples constitute Dl. All algo-
rithms used the same transduction sets. In the case
of SGT, LP and AM, the first transduction set was
used to tune the hyperparameters which we then held
fixed for all the remaining 20 transduction sets. For
all the graph-based approaches, we ran a search over
K ? {2, 10, 50, 100, 250, 500, 1000, 2000, n} (note
K = n represents a fully connected graph). In addi-
tion, in the case of AM, we set ? = 2 for all exper-
iments, and we ran a search over ? ? {1e?8, 1e?4,
0.01, 0.1, 1, 10, 100} and ? ? {1e?8, 1e?6, 1e?4,
0.01, 0.1}, for SGT the search was over c ? {3000,
3200, 3400, 3800, 5000, 100000} (see (Joachims,
2003)).
We report precision-recall break even point
(PRBEP) results on the 3,299 test documents in Ta-
ble 1. PRBEP has been a popular measure in infor-
mation retrieval (see e.g. (Raghavan et al, 1989)).
It is defined as that value for which precision and
recall are equal. Results for each category in Ta-
ble 1 were obtained by averaging the PRBEP over
Category SVM TSVM SGT LP AM
earn 91.3 95.4 90.4 96.3 97.9
acq 67.8 76.6 91.9 90.8 97.2
money 41.3 60.0 65.6 57.1 73.9
grain 56.2 68.5 43.1 33.6 41.3
crude 40.9 83.6 65.9 74.8 55.5
trade 29.5 34.0 36.0 56.0 47.0
interest 35.6 50.8 50.7 47.9 78.0
ship 32.5 46.3 49.0 26.4 39.6
wheat 47.9 44.4 59.1 58.2 64.3
corn 41.3 33.7 51.2 55.9 68.3
average 48.9 59.3 60.3 59.7 66.3
Table 1: P/R Break Even Points (PRBEP) for the top
10 categories in the Reuters data set with l = 20 and
u = 3299. All results are averages over 20 randomly
generated transduction sets. The last row is the macro-
average over all the categories. Note AM is the proposed
approach.
the 20 transduction sets. The final row ?average?
was obtained by macro-averaging (average of av-
erages). The optimal value of the hyperparame-
ters in case of LP was K = 100; in case of AM,
K = 2000, ? = 1e?4, ? = 1e?2; and in the case
of SGT, K = 100, c = 3400. The results show
that AM outperforms the state-of-the-art on 6 out of
10 categories and is competitive in 3 of the remain-
ing 4 categories. Further it significantly outperforms
all other approaches in case of the macro-averages.
AM is significant over its best competitor SGT at
the 0.0001 level according to the difference of pro-
portions significance test.
Figure 1 shows the variation of ?average? PRBEP
against the number of labeled documents (l). For
each value of l, we tuned the hyperparameters over
the first transduction set and used these values for
all the other 20 sets. Figure 1 also shows error-
bars (? standard deviation) all the experiments. As
expected, the performance of all the approaches
improves with increasing number of labeled docu-
ments. Once again in this case, AM, outperforms
the other approaches for all values of l.
5.2 WebKB Collection
World Wide Knowledge Base (WebKB) is a collec-
tion of 8282 web pages obtained from four academic
1096
0 50 100 150 200 250 300 350 400 450 50045
50
55
60
65
70
75
80
85
Number of Labeled Documents
Ave
rage
 PR
BEP
 
 
AMSGTLPTSVMSVM
Figure 1: Average PRBEP over all classes vs.
number of labeled documents (l) for Reuters data
set
0 100 200 300 400 500 600
20
30
40
50
60
70
80
Number of Labeled Documents
Ave
rage
 PR
BEP
 
 
AMSGTLPTSVMSVM
Figure 2: Average PRBEP over all classes vs.
number of labeled documents (l) for WebKB col-
lection.
domains. The web pages in the WebKB set are la-
beled using two different polychotomies. The first
is according to topic and the second is according to
web domain. In our experiments we only consid-
ered the first polychotomy, which consists of 7 cat-
egories: course, department, faculty, project, staff,
student, and other. Following (Nigam et al, 1998)
we only use documents from categories course, de-
partment, faculty, project which gives 4199 docu-
ments for the four categories. Each of the documents
is in HTML format containing text as well as other
information such as HTML tags, links, etc. We used
both textual and non-textual information to construct
the feature vectors. In this case we did not use ei-
ther stop-word removal or stemming as this has been
found to hurt performance on this task (Nigam et al,
1998). As in the the case of the Reuters data set
we extracted TFIDF features for each document and
constructed the graph using cosine similarity.
As in (Bekkerman et al, 2003), we created four
roughly-equal random partitions of the data set. In
order to obtain Dl, we first randomly choose a split
and then sample l documents from that split. The
other three splits constitute Du. We believe this is
more realistic than sampling the labeled web-pages
from a single university and testing web-pages from
the other universities (Joachims, 1999). This method
of creating transduction sets allows us to better eval-
uate the generalization performance of the various
algorithms. Once again we create 21 transduction
sets and the first set was used to tune the hyperpa-
rameters. Further, we ran a search over the same grid
as used in the case of Reuters. We report precision-
Class SVM TSVM SGT LP AM
course 46.5 43.9 29.9 45.0 67.6
faculty 14.5 31.2 42.9 40.3 42.5
project 15.8 17.2 17.5 27.8 42.3
student 15.0 24.5 56.6 51.8 55.0
average 23.0 29.2 36.8 41.2 51.9
Table 2: P/R Break Even Points (PRBEP) for the WebKB
data set with l = 48 and u = 3148. All results are aver-
ages over 20 randomly generated transduction sets. The
last row is the macro-average over all the classes. AM is
the proposed approach.
recall break even point (PRBEP) results on the 3,148
test documents in Table 2. For this task, we found
that the optimal value of the hyperparameter were:
in the case of LP, K = 1000; in case of AM,
K = 1000, ? = 1e?2, ? = 1e?4; and in case of
SGT, K = 100, c = 3200. Once again, AM is sig-
nificant at the 0.0001 level over its closest competi-
tor LP. Figure 2 shows the variation of PRBEP with
number of labeled documents (l) and was generated
in a similar fashion as in the case of the Reuters data
set.
6 Discussion
We note that LP may be cast into an AM-like frame-
work by using the following sequence of updates,
p
(n)
i (y) = ?(i ? l)ri(y) + ?(i > l)q
(n?1)
i ,
q
(n)
i (y) =
?
j wijp
(n)
i (y)
?
j wij
1097
To compare the behavior of AM and LP, we ap-
plied this form of LP along with AM on a simple
5-node binary-classification SSL graph where two
nodes are labeled (node 1 and 2) and the remaining
nodes are unlabeled (see Figure 3, top). Since this is
binary classification (|Y | = 2), each distribution pi
or qi can be depicted using only a single real num-
ber between 0 and 1 corresponding to the probability
that each vertex is class 2 (yes two). We show how
both LP and AM evolve starting from exactly the
same random starting point q0 (Figure 3, bottom).
For each algorithm, the figure shows that both algo-
rithms clearly converge. Each alternate iteration of
LP is such that the labeled vertices oscillate due to
its clamping back to the labeled distribution, but that
is not the case for AM. We see, moreover, qualitative
differences in the solutions as well ? e.g., AM?s so-
lution for the pendant node 5 is less confident than is
LP?s solution. More empirical comparative analysis
between the two algorithms of this sort will appear
in future work.
We have proposed a new algorithm for semi-
supervised text categorization. Empirical results
show that the proposed approach significantly out-
performs the state-of-the-art. In addition the pro-
posed approach is relatively simple to implement
and has guaranteed convergence properties. While
in this work, we use relatively simple features to
construct the graph, use of more sophisticated fea-
tures and/or similarity measures could lead to further
improved results.
Acknowledgments
This work was supported by ONR MURI grant
N000140510388, by NSF grant IIS-0093430, by
the Companions project (IST programme under EC
grant IST-FP6-034434), and by a Microsoft Re-
search Fellowship.
References
Alexandrescu, A. and Kirchhoff, K. (2007). Data-driven
graph construction for semi-supervised graph-based
learnning in nlp. In Proc. of the Human Language
Technologies Conference (HLT-NAACL).
Bekkerman, R., El-Yaniv, R., Tishby, N., and Winter, Y.
(2003). Distributional word clusters vs. words for text
categorization. J. Mach. Learn. Res., 3:1183?1208.
0.8
0.6 0.2
0.8
0.8
Node 1
Label 1
Node 2
Label 2
Node 3
Unlabeled
Node 4
Unlabeled
Node 5
Unlabeled
1
2
3
4
5 0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
AM iteration (and distribution pair) number
ve
rte
x  (d
ata 
pion
t) nu
mbe
r
 
q
(0)
p
(1)
q
(1)
p
(2)
q
(2)
p
(3)
q
(3)
p
(4)
q
(4)
p
(5)
q
(5)
p
(6)
q
(6)
p
(7)
q
(7)
p
(8)
q
(8)
p
(9)
q
(9)
p
(15)
q
(15)
p
(14)
q
(14)
p
(13)
q
(13)
p
(12)
q
(12)
p
(11)
q
(11)
p
(10)
q
(10)
 
LP iteration (and distribution pair) number
ve
rte
x  (d
ata 
pion
t) nu
mbe
r
 
 
1
2
3
4
5 0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
q
(0)
p
(1)
q
(1)
p
(2)
q
(2)
p
(3)
q
(3)
p
(4)
q
(4)
p
(5)
q
(5)
p
(6)
q
(6)
p
(7)
q
(7)
p
(8)
q
(8)
p
(9)
q
(9)
p
(15)
q
(15)
p
(14)
q
(14)
p
(13)
q
(13)
p
(12)
q
(12)
p
(11)
q
(11)
p
(10)
q
(10)
 
Figure 3: Graph (top), and alternating values of pn, qn
for increasing n for AM and LP.
1098
Belkin, M., Niyogi, P., and Sindhwani, V. (2005). On
manifold regularization. In Proc. of the Conference on
Artificial Intelligence and Statistics (AISTATS).
Bengio, Y., Delalleau, O., and Roux, N. L. (2007). Semi-
Supervised Learning, chapter Label Propogation and
Quadratic Criterion. MIT Press.
Bertsekas, D. (2004). Nonlinear Programming. Athena
Scientific Publishing.
Blitzer, J. and Zhu, J. (2008). ACL 2008 tutorial on
Semi-Supervised learning. http://ssl-acl08.
wikidot.com/.
Blum, A. and Chawla, S. (2001). Learning from labeled
and unlabeled data using graph mincuts. In Proc. 18th
International Conf. on Machine Learning, pages 19?
26. Morgan Kaufmann, San Francisco, CA.
Blum, A. and Mitchell, T. (1998). Combining labeled
and unlabeled data with co-training. In COLT: Pro-
ceedings of the Workshop on Computational Learning
Theory.
Chapelle, O., Scholkopf, B., and Zien, A. (2007). Semi-
Supervised Learning. MIT Press.
Corduneanu, A. and Jaakkola, T. (2003). On informa-
tion regularization. In Uncertainty in Artificial Intelli-
gence.
Cover, T. M. and Thomas, J. A. (1991). Elements of In-
formation Theory. Wiley Series in Telecommunica-
tions. Wiley, New York.
Csiszar, I. and Tusnady, G. (1984). Information Geome-
try and Alternating Minimization Procedures. Statis-
tics and Decisions.
Dumais, S., Platt, J., Heckerman, D., and Sahami, M.
(1998). Inductive learning algorithms and represen-
tations for text categorization. In CIKM ?98: Proceed-
ings of the seventh international conference on Infor-
mation and knowledge management, New York, NY,
USA.
Grandvalet, Y. and Bengio, Y. (2004). Semi-supervised
learning by entropy minimization. In Advances in
Neural Information Processing Systems (NIPS).
Joachims, T. SGT Light. http://sgt.joachims.
org.
Joachims, T. SVM Light. http://svmlight.
joachims.org.
Joachims, T. (1999). Transductive inference for text clas-
sification using support vector machines. In Proc. of
the International Conference on Machine Learning
(ICML).
Joachims, T. (2003). Transductive learning via spectral
graph partitioning. In Proc. of the International Con-
ference on Machine Learning (ICML).
Lewis, D. et al (1987). Reuters-21578. http:
//www.daviddlewis.com/resources/
testcollections/reuters21578.
Nigam, K., McCallum, A., Thrun, S., and Mitchell, T.
(1998). Learning to classify text from labeled and un-
labeled documents. In AAAI ?98/IAAI ?98: Proceed-
ings of the fifteenth national/tenth conference on Arti-
ficial intelligence/Innovative applications of artificial
intelligence, pages 792?799.
Pearl, J. (1990). Jeffrey?s Rule, Passage of Experience
and Neo-Bayesianism in Knowledge Representation
and Defeasible Reasoning. Kluwer Academic Pub-
lishers.
Porter, M. (1980). An algorithm for suffix stripping. Pro-
gram, 14(3):130?137.
Raghavan, V., Bollmann, P., and Jung, G. S. (1989). A
critical investigation of recall and precision as mea-
sures of retrieval system performance. ACM Trans.
Inf. Syst., 7(3):205?229.
Salton, G. and Buckley, C. (1987). Term weighting ap-
proaches in automatic text retrieval. Technical report,
Ithaca, NY, USA.
Sindhwani, V., Niyogi, P., and Belkin, M. (2005). Be-
yond the point cloud: from transductive to semi-
supervised learning. In Proc. of the International Con-
ference on Machine Learning (ICML).
Szummer, M. and Jaakkola, T. (2001). Partially la-
beled classification with Markov random walks. In
Advances in Neural Information Processing Systems,
volume 14.
Tsuda, K. (2005). Propagating distributions on a hyper-
graph by dual information regularization. In Proceed-
ings of the 22nd International Conference on Machine
Learning.
Wang, J., Jebara, T., and Chang, S.-F. (2008). Graph
transduction via alternating minimization. In Proc. of
the International Conference on Machine Learning
(ICML).
Yarowsky, D. (1995). Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics.
Zhu, X. (2005a). Semi-supervised learning literature sur-
vey. Technical Report 1530, Computer Sciences, Uni-
versity of Wisconsin-Madison.
Zhu, X. (2005b). Semi-Supervised Learning with
Graphs. PhD thesis, Carnegie Mellon University.
Zhu, X. and Ghahramani, Z. (2002). Learning from
labeled and unlabeled data with label propagation.
Technical report, Carnegie Mellon University.
Zhu, X., Ghahramani, Z., and Lafferty, J. (2003). Semi-
supervised learning using gaussian fields and har-
monic functions. In Proc. of the International Con-
ference on Machine Learning (ICML).
1099
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 459?466, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Part-of-Speech Tagging using Virtual Evidence and Negative Training
Sheila M. Reynolds and Jeff A. Bilmes
Department of Electrical Engineering
University of Washington
Seattle, WA 98195-2500
{sheila,bilmes}@ee.washington.edu
Abstract
We present a part-of-speech tagger which
introduces two new concepts: virtual evi-
dence in the form of an ?observed child?
node, and negative training data to learn
the conditional probabilities for the ob-
served child. Associated with each word
is a flexible feature-set which can in-
clude binary flags, neighboring words, etc.
The conditional probability of Tag given
Word + Features is implemented using
a factored language-model with back-off
to avoid data sparsity problems. This
model remains within the framework of
Dynamic Bayesian Networks (DBNs) and
is conditionally-structured, but resolves
the label bias problem inherent in the con-
ditional Markov model (CMM).
1 Introduction
A common sequence-labeling task in natural lan-
guage processing involves assigning a part-of-
speech (POS) tag to each word in the input text.
Previous authors have used numerous HMM-based
models (Banko and Moore, 2004; Collins, 2002;
Lee et al, 2000; Thede and Harper, 1999) and
other types of networks including maximum entropy
models (Ratnaparkhi, 1996), conditional Markov
models (Klein and Manning, 2002; McCallum et
al., 2000), conditional random fields (CRF) (Laf-
ferty et al, 2001), and cyclic dependency networks
(Toutanova et al, 2003). All of these models make
use of varying amounts of contextual information.
In this paper, we present a new model which re-
mains within the well understood framework of Dy-
namic Bayesian Networks (DBNs), and we show
that it produces state-of-the-art results when ap-
plied to the POS-tagging task. This new model is
conditionally-structured and, through the use of vir-
tual evidence (Pearl, 1988; Bilmes, 2004), resolves
the explaining-away problems (often described as
label or observation bias) inherent in the CMM.
This paper is organized as follows. In sec-
tion 2 we discuss the differences between a hidden
Markov model (HMM) and the corresponding con-
ditional Markov model (CMM). In section 3 we de-
scribe our observed-child model (OCM), introduc-
ing the notion of virtual evidence, and providing an
information-theoretic foundation for the use of nega-
tive training data. In section 4 we discuss our exper-
iments and results, including a comparison of three
simple first-order models and state-of-the-art results
from our feature-rich second-order OCM.
For clarity, the comparisons and derivations in
sections 2 and 3 are done for first-order models us-
ing a single binary feature. The same ideas are then
generalized to a higher order model with more fea-
tures (including adjacent words).
2 Generative vs. Conditional Models
In this section we discuss the tradeoffs between the
generative hidden Markov model (HMM) and the
conditional Markov model (CMM). For pedagogical
reasons, the figures and equations are for first order
models with a single word-feature.
The HMM shown in Figure 1 includes a single
459
feature (the binary flag isCap) in addition to the
word itself. Each observation, oi = (wi, fi), is a
word-feature pair. Let o = {oi} be the observation
sequence and s = {si} be the associated tag (state)
sequence. The HMM1 factorizes the joint probabil-
ity distribution over these two sequences as:
P (s,o) =
?
i
P (si|si?1)P (wi|si)P (fi|si)
Tag
Word
isCap
Figure 1: First order HMM.
A similar model often used for sequence label-
ing tasks is the conditional Markov model (CMM)
which reverses the arrows between the words and
the tags (Figure 2), and factorizes as:
P (s,o) =
?
i
P (si|si?1, wi, fi)P (wi)P (fi)
Tag
Word
isCap
Figure 2: First order CMM.
Because the words and features are observed, this
model does not require that we compute the proba-
bility of the evidence, P (o), when finding the opti-
mal tag sequence. The tag-sequence s which max-
imizes the joint probability P (s,o) is the same one
that maximizes the conditional probability P (s|o).
The CMM, therefore, does not require that we model
the language, allowing us to focus on modeling the
conditional probability of the tags given the words.
The HMM has its advantages as well, principally
that it is easier to train than the CMM because it
1In this HMM, Word and isCap are independent given Tag,
but this need not be true in general.
factorizes the joint probability into simpler com-
ponents. The tables required for P (si|si?1) and
P (oi|si) are significantly smaller than the one for
P (si|si?1, oi) which may be difficult to estimate due
to either data sparsity or normalization issues. One
potential disadvantage of the HMM is that when it is
trained using a maximum likelihood procedure, it is
not necessarily encouraged to optimally classify tags
due to its generative nature. One solution is to train
the HMM using a discriminative procedure. Another
option is to use entirely different models.
A key disadvantage of the CMM is that it
makes critical statements about independence that
the HMM does not: the converging arrows at each
tag put the parent nodes (the previous tag and the
current observation) into causal competition and as
a result the model states that the previous tag is inde-
pendent of the current observation. In other words,
all states (tags) are independent of future observa-
tions (words). The CMM thus incorporates a strong
directional bias which does not exist in the HMM.
One way to eliminate this bias is to use a CRF
(Lafferty et al, 2001; McCallum, 2003), where fac-
tors over neighboring tags may use features from
anywhere in the observation sequence. The CRF
is discriminative and avoids label/observation bias
by using a model that is constrained only in that
the conditional distribution factorizes over an undi-
rected Markov chain. However, most popular train-
ing procedures for a CRF are time-consuming and
complex processes.
3 Using Virtual Evidence
Our goals in this work are to: 1) keep the discrimi-
native nature of the CMM to the extent possible; 2)
avoid label and observation bias issues; and 3) stay
entirely within the DBN framework where training
is relatively simple. We thus propose a new solu-
tion to the problem, which retains the discrimina-
tive conditional form of ?tag given word? from the
CMM, but avoids label bias by temporally linking
adjacent tags in a new way. Specifically, we employ
virtual evidence in the form of a binary observed
child node, ci, between adjacent tags (Figure 3) or
a windowed sequence of tags. During decoding, this
node will always be observed to be equal to 1 (one).
Intuitively, this binary variable acts as an indicator of
460
Tag
Word
isCap
C
Figure 3: First order observed-child model (OCM)
with the tags connected in pairs.
tag-pair consistency. When the tag pairs are consis-
tent (as they are in real text), we should have a high
conditional probability that ci = 1; and when the
tag pairs are not consistent, the conditional probabil-
ity that ci = 1 should be low. With this conditional
distribution, observing ci = 1 during decoding ex-
presses a preference for consistent tag pairs.
The presence of this observed-child node results
in a term in the factorization of the joint probability
distribution that couples its parents:
P (c, s,o) ?
?
i
P (ci|si?1, si)P (si|wi, fi)
where ci is the observed-child node of tags si?1 and
si, and we omit the probability of the observations,
P (wi, fi) which do not affect the final choice of s.
By the rules of d-separation (Pearl, 1988), the ex-
istence of ci defined in this way means that the par-
ents (the adjacent tags) are not conditionally inde-
pendent given the child. This link between adja-
cent tags through an observed-child node allows for
a probabilistic relationship to exist between the ad-
jacent tags. Thus, future words can influence tags,
which is not true for the CMM. Whether or not a
relationship between tags will actually be learned,
however, will critically depend on how the model is
trained. In a graphical model, it is the lack of an
edge that ensures some form of independence; the
presence of an edge (or a path made up of two or
more edges) does not necessarily ensure the reverse.
3.1 Training
The introduction of virtual evidence into a graph-
ical model requires that careful thought be given
to the training process. If we were to na??vely add
ci = 1 to all samples of the training data, the model
would learn that ci is constant rather than random,
and therefore that it is independent of its parents,
si?1 and si. In other words, this na??vely-trained
model would assume that P (ci = 1|si?1, si) =
1 ? (si?1, si), and when used to tag the sentences
in the test-set (also labeled with ci = 1), it would
maximize this simplified joint probability in which
the relationship between si?1 and si has been lost:
P (c, s,o) ?
?
i
P (si|wi, fi)
In order to induce and thereby have the model
learn the relationship between the adjacent tags si?1
and si, the training has to be modified to include
samples that are labeled with ci = 0. The proba-
bility table P (ci = 1|si?1, si) should favor common
(consistent) tag-pairs with high probabilities, while
discouraging rare tag-pairs with low probabilities.
Although all observations (in both training and
test sets) are labeled with ci = 1, we hypothesize
an alternate set of observations labeled with ci = 0.
This alternate set will be the source of the negative
training data 2. It is a set of nonsensical sentences
with the same distribution over individual tags, i.e.
the same P (si), but in this set adjacent tags are in-
dependent. We denote the total number of training
samples by M . This is divided into positive train-
ing samples, M1, and negative training samples, M0,
with M1+M0 = M . The ratio of the amount of pos-
itive to negative training data should be the same as
the ratio of our prior beliefs about tag-pair consis-
tency, namely the ratio of P (ci = 1) to P (ci = 0).
With no evidence to support that one is more likely
than the other, one option is to use the strategy of
?assuming the least? and use a maximum entropy
prior, setting M0 = M1. More flexibly, we can de-
fine n to be the ratio of the two so that M0 = n ?M1.
Now we derive a method for training the condi-
tional probability table P (ci|si?1, si) in terms of the
pointwise mutual information between the adjacent
tags si?1 and si. We first rewrite the conditional
probability (henceforth abbreviated as p) as:
p = P (ci = 1|si?1, si) =
P (ci = 1, si?1, si)
P (si?1, si)
If the probabilities are maximum likelihood (ML)
estimates derived from counts on the training data,
we can equivalently write:
p = N(ci = 1, si?1, si)N(si?1, si)
2This use of implied negative training data is similar to the
?neighborhood? concept described in (Smith and Eisner, 2005)
461
where N(?) is the count function.
Expanding the denominator into two terms:
p = N(ci = 1, si?1, si)N(ci = 1, si?1, si) + N(ci = 0, si?1, si)
Without any negative training data (labeled with
ci = 0), this ratio would always evaluate to 1, and no
probabilistic relationship between si?1 and si would
be learned.
From the start, we have implicitly postulated a re-
lationship between adjacent tags. We now formally
state two hypotheses: H1 that there is a relationship
between adjacent tags which can be described by
some joint probability distribution P (si?1, si), and
the null hypothesis, H0, that there is no such rela-
tionship, i.e. si?1 and si are independent:
PH1 = P (si?1, si)
PH0 = P (si?1)P (si)
Now we can express the counts as follows:
N(ci = 1, si?1, si) = M1 ? P (si?1, si)
N(ci = 0, si?1, si) = M0 ? P (si?1)P (si)
where M1 is the total number of tokens in the (posi-
tive) training data, and M0 is the total number of to-
kens in the induced negative training data. We sub-
stitute M0 with n ? M1 for the reasons mentioned
earlier, and simplify to obtain:
p = P (si?1, si)P (si?1, si) + nP (si?1)P (si)
which can be simplified to obtain:
p = 1
1 + n
[
P (si?1,si)
P (si?1)P (si)
]
?1
The ratio of probabilities in the denominator is the
ratio used in computing the pointwise mutual infor-
mation between si?1 and si. This ratio, which we
will call ?, is also the likelihood ratio between the
two previously stated hypotheses. Finally, we write
the conditional probability as a function of ?:
P (ci = 1|si?1, si) =
1
1 + n??1 =
?
? + n
where ? = PH1PH0
= P (si?1, si)P (si?1)P (si)
= P (si|si?1)P (si)
The conditional probability, P (ci = 1|si?1, si) is a
mapping g(?) from ? ? [0,?) to p ? [0, 1).
Beginning with (Church and Hanks, 1989), nu-
merous authors have used the pointwise mutual in-
formation between pairs of words to analyze word
co-locations and associations. This ratio tells us
whether si?1 and si co-occur more or less often than
would be expected by chance alone.
Consider, for example, the tags DT (determiner)
and NN (noun), and the four possible ordered tag-
pairs. The probabilities P (si) and P (si|si?1) de-
rived from the training data (see section 4.1), the
likelihood ratio score ?, the conditional probability
p = P (ci = 1|si?1, si), and the occurrence counts
N are shown in Table 1. As expected, the sequence
DT-NN (e.g. the surplus) occurs very often and gets
a high score, while DT-DT (e.g. this a) and NN-
DT (e.g. surplus the) occur infrequently and get low
scores. The sequence NN-NN (e.g. trade surplus)
gets a neutral score (? ? 1) indicating that if the pre-
ceding word is a noun, the likelihood that the current
word is a noun is nearly equal to the likelihood that
any randomly chosen word is a noun.
We present two methods for inducing the negative
training counts that are required to train the condi-
tional probability table for P (ci|si?1, si).
In the first method, we generate ?nonsense? sen-
tences by randomly scrambling each sentence in the
training-set n times, using a uniform distribution
over all possible permutations. This results in n
negative training sentences for each positive training
sentence and therefore M0 = n?M1. Effectively, the
ratio of priors on ci is now:
P (ci = 1)
P (ci = 0)
= M1M0
= 1n
The conditional probability table P (ci|si?1, si) is
si?1-si P (si) P (si|si?1) ? p N
DT-NN 0.129 0.4905 3.80 0.79 37301
NN-NN 0.129 0.1270 0.98 0.49 15571
NN-DT 0.080 0.0071 0.09 0.08 870
DT-DT 0.080 0.0018 0.02 0.02 134
Table 1: Sample likelihood ratio scores (?), proba-
bilities, p (for n = 1), and counts for four tag-pairs.
462
then trained using all n+1 versions of each sentence,
thus inducing the desired dependence between si?1
and si. The method of scrambling sentences n-times
only approximates the theory described above be-
cause it is performed on a sentence-by-sentence ba-
sis rather than across the entire training set. Also, the
resulting negative training data represents only n re-
alizations of a random process, so the total number
of samples may not be large enough to approximate
the underlying distribution.
In the second method, rather than generate the
negative training data in the form of scrambled sen-
tences, we compute the negative-training counts di-
rectly, based on the positive unigram counts and the
hypotheses presented in section 3.1. For example,
the negative bigram counts are a function of the
marginal probability of each tag, P (si):
N(ci = 0, si?1, si) = nM1 ? P (si?1)P (si)
Negative unigram and trigram counts are computed
in a similar fashion, and then the conditional proba-
bility table is derived as a smoothed back-off model
directly from the combined sets of counts.
These two methods are conceptually similar but
may exhibit subtle differences: one is randomizing
at the sentence level while the other operates over
the entire training set and does not have the same
sensitivity to small values of n.
4 Experiments and Results
In this section we describe our experiments and the
results obtained. Sections 4.1 and 4.2 describe the
data sets and features. Section 4.3 presents compar-
isons between several simple models using just the
tags, the words, and a single binary feature for each
word. Section 4.4 presents results from a feature-
rich second-order observed-child model in which
tags are linked in groups of three.
All training of language models is done using the
SRILM toolkit (Stolcke, 2002) with the FLM exten-
sions (Bilmes and Kirchhoff, 2003), and the imple-
mentation and testing of the various graphical mod-
els is carried out with the help of the graphical mod-
els toolkit (GMTK) (Bilmes and Zweig, 2002).
4.1 Data Sets
The data used for these experiments is the Wall
Street Journal data from Penn Treebank III (Mar-
cus et al, 1994). We extracted tagged sentences
from the parse trees and divided the data into train-
ing (sections 0-18), development (sections 19-21),
and test (sections 22-24) sets as in (Toutanova et al,
2003). Except for the final results for the feature-
rich model, all results are on the development set.
4.2 Features
The tagged sentences extracted from the Penn Tree-
bank are pre-processed to generate appropriately-
formatted training data for the SRILM toolkit, as
well as the vocabulary and observation files to be
used during testing.
The pre-processing includes building a dictionary
based on the training data. All words containing
uppercase letters are converted to lowercase before
being written to the dictionary. Words that occur
rarely are excluded from the dictionary and are in-
stead mapped to a single out-of-vocabulary word.
This is based on the idea from (Ratnaparkhi, 1996)
that rare words in the training set are similar to un-
known words in the test set, and can be used to learn
how to tag the unknown words that will be encoun-
tered during testing. In this work, rare words are
those that occurr fewer than 5 times. The dictio-
nary also includes special begin-sentence and end-
sentence words, as well as punctuation marks, re-
sulting in a total of 10,824 words. A list of the 45
tags found in the training data is also created, and
is similarly augmented with special begin-sentence
and end-sentence tags, for a total of 47 distinct tags.
Each word has associated with it a set of features.
During training, these features are used to learn a
smoothed back-off model for P (si|wi, fi) (where fi
is a vector of features associated with word wi).
The following five binary flags, taken from
(Toutanova et al, 2003), are derived from the cur-
rent word wi and used as features :
? is-capitalized (refers to the first letter only);
? has-digits (word contains one or more digits);
? is-hyphenated (word contains ?-?);
? is-all-caps (all letters are capitalized);
? is-conjunction (true if is-all-caps, has-digits,
and is-hyphenated are all true, for example
CFC-12 or F/A-18).
Prefixes and suffixes are also known to be infor-
mative and so we add a prefix-feature and a suffix-
463
feature to our set. Previous work used all possible
prefixes and suffixes ranging in length from 1 to k
characters, with k = 4 (Ratnaparkhi, 1996), and
k = 10 (Toutanova et al, 2003). This method re-
sults in very long lists of thousands of suffixes and
prefixes. In this work, we instead analyzed the rare
words in the training data to generate shorter lists of
informative prefixes and suffixes, with lengths be-
tween 1 and 7 characters. Each prefix/suffix was
scored based on the number of times it appeared
with a particular tag, and all prefixes/suffixes that
scored above 20 (an arbitrarily chosen threshold)
were kept. This process resulted in two separate
lists: one with 377 prefixes, and the other with 704
suffixes. Each word is then assigned a single pre-
fix feature and a single suffix feature from these
lists (which both include an entry for ?unknown?).
When assigning prefix and suffix features to the rare
words (in the training data) or the unknown words
(in the test data), we assume that the longest string is
the most informative. (This may not necessarily be
true: for example, although the suffix ing is certainly
more informative than g, it is less clear whether ulat-
ing would be more or less informative than ing.)
We also include the two adjacent words as fea-
tures of the current word. Our model provides great
flexibility in the choice of features to be included
in the current word?s feature-set. This feature-set is
not limited to binary flags and indeed can include
anything that can be extracted from the observa-
tion sequence in the pre-processing stage. By using
a smoothed back-off model, issues related to data-
sparsity and over-fitting are avoided.
4.3 First Order Model Comparisons
In this section we compare results obtained from
three first-order models: HMM, CMM, and OCM,
using a Na??ve Bayes (NB) model as a baseline. The
Na??ve Bayes model is a zeroth-order model with no
connection between adjacent tags, while the first-
order models connect adjacent tags in pairs. (Note
that the HMM in this case is just a ?temporal? NB
since given the tag, the features are independent.) In
these experiments, the only feature used is the is-
capitalized flag (the most informative of the binary
flags tested). The results are shown in Table 2.
The conditional probability tables (CPTs) for
the CMM and the OCM were generated using the
model token known-w. unk.-w.type accur. accur. accur.
Na??ve Bayes 90.56% 93.83% 43.4%
OCMn=0 90.89% 94.07% 45.2%
CMM 93.23% 95.69% 57.9%
OCMn=1 93.94% 96.39% 58.6%
HMM 94.30% 96.53% 62.3%
OCMn=4 94.42% 96.63% 62.7%
Table 2: Scores for first order models.
factored language model (FLM) extensions to the
SRILM toolkit, wth generalized parallel backoff
and Witten-Bell smoothing. (Modified Kneser-Ney
smoothing could not be applied because some of the
required low-order meta-counts needed by the dis-
count estimator were zero.) The negative training
data for the OCM was generated using the scram-
ble method, with values of n as in the table. When
no negative training data is used (n = 0), the CPT
for the observed-child shows a very weak depen-
dence on the specific tag-pair (si?1, si): the proba-
bility values in the tag-bigram model range only be-
tween 0.89 and 1. This weak dependence results in
performance comparable to that of the Na??ve Bayes
model. That there is any dependence at all is due to
the smoothing since ci = 0 is never observed in the
training data. With negative training data (n = 4),
there is a much stronger dependence on the tag-pair,
and the values for P (ci = 1|si?1, si) range between
0.0002 and 1.
We found experimentally that the OCM reached
peak performance with n = 4 and that for larger n
the performance stayed relatively constant: the vari-
ation for values of n up to 14 was only 0.05%.
4.4 Feature-Rich Second-Order OCM
In this section we describe the results obtained from
a more complex second order OCM with the addi-
tional word features described in section 4.2.
This model is illustrated in Figure 4 which, for
clarity highlights the details only for one (tag,word)
pair. The observed-child node, ci, now has three par-
ents: the tags si?1, si, and si+1. Each tag, si, in
turn has K + 1 parents: the current word, wi, and
a set of K features (shown bundled together). The
model switches between the two feature bundles as
464
model description token known-word unknown-wordaccuracy accuracy accuracy
OCM-I, scramble, n = 4 96.39% 96.87% 89.5%
OCM-I, computed counts, n = 4 96.41% 96.90% 89.3%
OCM-I, computed counts, n = 1 96.41% 96.92% 89.0%
OCM-II, computed counts, n = 1 96.64% 97.12% 89.5%
OCM-II, as above, on test-set 96.77% 97.25% 90.0%
Table 3: Tagging accuracy using the feature-rich 2nd order observed-child model.
illustrated, based on the current word. For known
words, a small set of features is used, while a much
larger set of features is used for unknown words.
This switching increases the speed of the model at
no cost: the additional features increase the tagging
accuracy for unknown words but are redundant for
known words.
This model factorizes the joint probability as:
P (c, s,o) ?
?
i
P (ci|si?1, si, si+1)P (si|wi, fi)
where fi is the appropriate feature bundle for word
wi, depending on whether wi is known or unknown.
ci
si-1 si si+1
wi
known-word features
unknown-word features
Figure 4: Second order OCM with tags connected in
triples and switching sets of word features.
Two sets of experiments were performed using
two models, which we will refer to as OCM-I and
OCM-II. Both of these are second order models
(connecting tags in triples), but with different sets
of features. In model OCM-I, the only feature used
for known words is the is-capitalized flag used in
section 4.3. The unknown words use a total of seven
features: suffix, prefix, and all five of the binary flags
described in section 4.2. Model OCM-II adds the ad-
jacent words (wi?1 and wi+1) to the feature-set for
both known and unknown words.
As seen above, the model factorizes the joint
probability into two conditional probability terms.
Each of these CPTs is implemented as a smoothed,
factored-language back-off model.
The observed-child CPT uses generalized back-
off, combining at run-time the results of backing off
from each of the three parents if the specific tag-
triple is not found in the table. The tag CPT uses
linear backoff, dropping the adjacent words first.
The backoff order for the other features was cho-
sen based on experiments to determine the relative
information content of each feature. This resulted
in the following backoff order: prefix, has-digit, is-
conjunction, is-all-caps, is-hyphenated, suffix, is-
capitalized, word (where the least informative fea-
ture, prefix, is the first feature to be dropped).
Results from these experiments are shown in Ta-
ble 3. Except for the last line, which reports results
on the test set, all results are on the development
set. The first three lines show results obtained from
OCM-I (without adjacent word features). The two
methods of generating negative training data yield
nearly identical results, showing that they are com-
parable. Comparing rows 2 and 3 in the table we see
that the computed-counts method is relatively insen-
sitive to the value of n (for n ? 1).
OCM-II, which uses the adjacent words as fea-
tures for both known and unknown words further
improves overall accuracy, and produces state-of-
the-art results. The token-level accuracy result ob-
tained from the OCM-II model on the development
set (96.64%) can be directly compared to an accu-
racy of 96.57% reported in (Toutanova et al, 2003)
for a cyclic dependency network using similar word
features and the same three tag context.
465
5 Conclusions
In this paper, we have introduced two new concepts
to the problem of part-of-speech tagging: virtual evi-
dence and negative training data. We have moreover
shown that this new model can produce state-of-the-
art results on this NLP task with appropriately cho-
sen features. The model stays entirely within the
mathematically formal language of Bayesian net-
works, and even though it is conditional in nature,
the model does not suffer from label or observa-
tion (or directional) bias. Staying within this frame-
work has other advantages as well, including that
the training procedures remain within the relatively
simple maximum likelihood framework, albeit with
appropriate smoothing. We believe that this model
holds great promise for other NLP tasks as well as in
other applications of machine-learning such as com-
putational biology. In particular the way it factor-
izes the joint probability into a ?horizontal? com-
ponent which connects various nodes to the virtual-
evidence node, and a ?vertical? component (used
here to link a tag to a set of observations), provides
great simplicity, flexibility, and power.
6 Acknowledgements
The authors would like to thank the anonymous re-
viewers for their constructive comments. Sheila
Reynolds is supported by an NDSEG fellowship.
References
Michele Banko and Robert C. Moore. 2004. Part of
Speech Tagging in Context. Proceedings of COLING.
Jeff Bilmes. 2004. On Soft Evidence in Bayesian Net-
works. Tech. Rep. UWEETR-2004-0016, U. Wash-
ington Dept. of Electrical Engineering, 2004.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and generalized parallel backoff. Pro-
ceedings of HLT-NAACL: Short Papers, 4-6.
Jeff Bilmes and Geoffrey Zweig. 2002. The graphi-
cal models toolkit: An open source software system
for speech and time-series processing. Proceedings of
ICASSP, vol4, 3916-3919.
Kenneth W. Church and Patrick Hanks. 1989. Word As-
sociation Norms, Mutual Information, and Lexicogra-
phy. Proceedings of ACL, 76-83.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. Proc. EMNLP.
Dan Klein and Christopher D. Manning. 2002. Condi-
tional Structure versus Conditional Estimation in NLP
Models. Proceedings of EMNLP, 9-16.
John Lafferty, Andrew McCallum and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. Pro-
ceedings of ICML, 282-289.
Sang-Zoo Lee, Jun-ichi Tsujii and Hae-Chang Rim.
2000. Part-of-Speech Tagging Based on Hidden
Markov Model Assuming Joint Independence. Pro-
ceedings of 38th ACL, 263-269.
Mitchell P. Marcus, Beatrice Santorini and Mary A.
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313-330.
Andrew McCallum. 2003. Efficiently Inducing Features
of Conditional Random Fields. Proceedings of UAI.
Andrew McCallum, Dayne Freitag and Fernando Pereira.
2000. Maximum-Entropy Markov Models for Infor-
mation Extraction and Segmentation. Proc. 17th In-
ternational Conf. on Machine Learning, 591-598.
Judea Pearl. 1988. Probabilistic Reasoning in Intelli-
gent Systems: Networks of Plausible Inference. Mor-
gan Kaufmann.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. EMNLP 1, 133-142.
Noah A. Smith and Jason Eisner 2005. Contrastive Es-
timation: Training Log-Linear Models on Unlabeled
Data. Proceedings of ACL.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. Proc. ICASSP, vol 2, 901-904.
Scott M. Thede and Mary P. Harper. 1999. A Second-
Order Hidden Markov Model for Part-of-Speech Tag-
ging. Proceedings of 37th ACL, 175-182.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
Proceedings of HLT-NAACL, 252-259.
466
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 995?1002, Vancouver, October 2005. c?2005 Association for Computational Linguistics
The Vocal Joystick: A Voice-Based Human-Computer Interface for
Individuals with Motor Impairments?
Jeff A. Bilmes?, Xiao Li?, Jonathan Malkin?, Kelley Kilanski?, Richard Wright?,
Katrin Kirchhoff?, Amarnag Subramanya?, Susumu Harada?, James A.
Landay?, Patricia Dowden?, Howard Chizeck?
?Dept. of Electrical Engineering
?Dept. of Computer Science & Eng.
?Dept. of Linguistics
?Dept. of Speech & Hearing Science
University of Washington
Seattle, WA
Abstract
We present a novel voice-based human-
computer interface designed to enable in-
dividuals with motor impairments to use
vocal parameters for continuous control
tasks. Since discrete spoken commands
are ill-suited to such tasks, our interface
exploits a large set of continuous acoustic-
phonetic parameters like pitch, loudness,
vowel quality, etc. Their selection is opti-
mized with respect to automatic recogniz-
ability, communication bandwidth, learn-
ability, suitability, and ease of use. Pa-
rameters are extracted in real time, trans-
formed via adaptation and acceleration,
and converted into continuous control sig-
nals. This paper describes the basic en-
gine, prototype applications (in particu-
lar, voice-based web browsing and a con-
trolled trajectory-following task), and ini-
tial user studies confirming the feasibility
of this technology.
1 Introduction
Many existing human-computer interfaces (e.g.,
mouse and keyboard, touch screens, pen tablets,
etc.) are ill-suited to individuals with motor
impairments. Specialized (and often expensive)
human-computer interfaces that have been devel-
oped specifically for this target group include sip
and puff switches, head mice, eye-gaze devices, chin
joysticks and tongue switches. While many indi-
viduals with motor impairments have complete use
?This material is based on work supported by the National
Science Foundation under grant IIS-0326382.
of their vocal system, these devices make little use
of it. Sip and puff switches, for example, have low
communication bandwidth, making it impossible to
achieve more complex control tasks.
Natural spoken language is often regarded as
the obvious choice for a human-computer inter-
face. However, despite significant research efforts
in automatic speech recognition (ASR) (Huang et
al., 2001), existing ASR systems are still not suf-
ficiently robust to a wide variety of speaking condi-
tions, noise, accented speakers, etc. ASR-based in-
terfaces are therefore often abandoned by users after
a short initial trial period. In addition, natural speech
is optimal for communication between humans but
sub-optimal for manipulating computers, windows-
icons-mouse-pointer (WIMP) interfaces, or other
electro-mechanical devices (such as a prosthetic ro-
botic arm). Standard spoken language commands
are useful for discrete but not for continuous op-
erations. For example, in order to move a cursor
from the bottom-left to the upper-right of a screen,
the user might have to repeatedly utter ?up? and
?right? or ?stop? and ?go? after setting an initial tra-
jectory and rate, which is quite inefficient. For these
reasons, we are developing alternative and reusable
voice-based assistive technology termed the ?Vocal
Joystick? (VJ).
2 The Vocal Joystick
The VJ approach has three main characteristics:
1) Continuous control parameters: Unlike standard
speech recognition, the VJ engine exploits continu-
ous vocal characteristics that go beyond simple se-
quences of discrete speech sounds (such as syllables
or words) and include e.g., pitch, vowel quality, and
loudness, which are then mapped to continuous con-
995
trol parameters.
2) Discrete vocal commands: Unlike natural speech,
the VJ discrete input language is based on a pre-
designed set of sounds. These sounds are selected
with respect to acoustic discriminability (maximiz-
ing recognizer accuracy), pronounceability (reduc-
ing potential vocal strain), mnemonic characteris-
tics (reducing cognitive load), robustness to environ-
mental noise, and application appropriateness.
3) Reusable infrastructure: Our goal is not to create
a single application but to provide a modular library
that can be incorporated by developers into a variety
of applications that can be controlled by voice. The
VJ technology is not meant to replace standard ASR
but to enhance and be compatible with it.
2.1 Vocal Characteristics
Three continuous vocal characteristics are extracted
by the VJ engine: energy, pitch, and vowel qual-
ity, yielding four specifiable continuous degrees of
freedom. The first of these, localized acoustic en-
ergy, is used for voice activity detection. In addi-
tion, it is normalized relative to the current vowel
detected (see Section 3.3), and is used by our cur-
rent VJ-WIMP application (Section 4) to control the
velocity of cursor movements. For example, a loud
voice causes a faster movement than does a quiet
voice. The second parameter, pitch, is also extracted
but is currently not mapped to any control dimension
in the VJ-WIMP application but will be in the future.
The third parameter is vowel quality. Unlike conso-
nants, which are characterized by a greater degree of
constriction in the vocal tract, vowels have much in-
herent signal energy and are therefore well-suited to
environments where both high accuracy and noise-
robustness are crucial. Vowels can be characterized
using a 2-D space parameterized by F1 and F2, the
first and second vocal-tract formants (resonant fre-
quencies). We initially experimented with directly
extracting F1/F2 and using them for directly spec-
ifying 2-D continuous control. While we have not
ruled out the use of F1/F2 in the future, we have
so far found that even the best F1/F2 detection al-
gorithms available are not yet accurate enough for
precise real-time specification of movement. There-
fore, we classify vowels directly and map them onto
the 2-D vowel space characterized by degree of con-
striction (i.e., tongue height) and tongue body posi-
tion (Figure 1). In our VJ-WIMP application, we use
Deg
ree 
of C
ons
trict
ion Front Central Back
High
Mid
Low
Tongue Body Position
[iy ] [ix ] [uw ]
[ey] [ax ] [ow ]
[ae ] [a] [aa ]
Figure 1: Vowel configurations as a function of their
dominant articulatory configurations.
the four corners of this chart to map to the 4 princi-
ple directions of up, down, left, and right as shown
in Figure 2 (note that the two figures are flipped and
rotated with respect to each other). We have four
different VJ systems running: A) a 4-class system
allowing only the specification of the 4 principle di-
rections; B) a 5-class system that also includes the
phone [ax] to act as a carrier when wishing to vary
only pitch and loudness; C) a 8-class system that in-
cludes the four diagonal directions; and D) a 9-class
system that includes all phones and directions. Most
of the discussion in this paper refers to the 4-class
system.
A fourth vocal characteristic is also extracted
by the VJ engine, namely discrete sounds. These
sounds may correspond to button presses as on a
mouse or joystick. The choice of sounds depends
on the application and are chosen according to char-
acteristic 2 above.
3 The VJ Engine
Our system-level design goals are modularity, low
latency, and maximal computational efficiency. For
this reason, we share common signal processing
operations in multiple signal extraction modules,
which yields real-time performance but leaves con-
siderable computational headroom for the back-end
applications being driven by the VJ engine.
Figure 3 shows the VJ engine architecture having
three modules: signal processing, pattern recogni-
tion, and motion control.
3.1 Signal Processing
The goal of the signal processing module is to ex-
tract low-level acoustic features that can be used in
996
[iy ]
[ix ]
[uw ]
[ey]
[ow ]
[ae ]
[a]
[aa ][ax ]
Figure 2: Vowel-direction mapping: vowels corre-
sponding to directions.
AcousticWaveform FeatureExtraction
Features:EnergyNCCFF1/F2MFCC
SignalProcessing
Energy
VowelClassification
PatternRecognition
PitchTracking
Discrete SoundRecognition
MotionParameters:
xy-directions,Speed,Acceleration,
Motion Control
SpaceTransformationMotion
ComputerInterfaceDriver Adaptation
Figure 3: System organization
estimating the vocal characteristics. The features we
use are energy, normalized cross-correlation coeffi-
cients (NCCC), formant estimates, Mel-frequency
cepstral coefficients (MFCCs), and formant esti-
mates. To extract features, the speech signal is PCM
sampled at a rate of Fs =16,000Hz. Energy is mea-
sured on a frame-by-frame basis with a frame size
of 25ms and a frame step of 10ms. Pitch is ex-
tracted with a frame size of 40ms and a frame step of
10ms. Multiple pattern recognition tasks may share
the same acoustic features: for example, energy and
NCCCs are used for pitch tracking, and energy and
MFCCs can be used in vowel classification and dis-
crete sound recognition. Therefore, it is more ef-
ficient to decouple feature extraction from pattern
recognition, as is shown in Figure 3.
3.2 Pattern Recognition
The pattern recognition module uses the acoustic
features to extract desired parameters. The estima-
tion and classification system must simultaneously
perform energy computation (available from the in-
put), pitch tracking, vowel classification, and dis-
crete sound recognition.
Many state-of-the-art pitch trackers are based on
dynamic programming (DP). This, however, often
requires the meticulous design of local DP cost func-
tions. The forms of these cost functions are usu-
ally empirically determined and/or their parameters
are tuned by algorithms such as gradient descent
(D.Talkin, 1995). Since different languages and ap-
plications may follow very different pitch transition
patterns, the cost functions optimized for certain lan-
guages and applications may not be the most appro-
priate for others. Our VJ system utilizes a graphi-
cal model mechanism to automatically optimize the
parameters of these cost functions, and has been
shown to yield state-of-the-art performance (X.Li et
al., 2004; J.Malkin et al, 2005).
For frame-by-frame vowel classification, our de-
sign constraints are the need for extremely low la-
tency and low computational cost. Probability es-
timates for vowel classes thus need to be obtained
as soon as possible after the vowel has been uttered
or after any small change in voice quality has oc-
curred. It is well known that models of vowel clas-
sification that incorporate temporal dynamics such
as hidden Markov models (HMMs) can be quite ac-
curate. However, the frame-by-frame latency re-
quirements of VJ make HMMs unsuitable for vowel
classification since HMMs estimate the likelihood
of a model based on the entire utterance. An alter-
native is to utilize causal ?HMM-filtering?, which
computes likelihoods at every frame based on all
frames seen so far. We have empirically found,
however, that slightly non-causal and quite local-
ized estimates of the vowel category probability
is sufficient to achieve user satisfaction. Specifi-
cally, we obtain probability estimates of the form
p(Vt|Xt?? , . . . , Xt+? ), where V is a vowel class,
and Xt?? , . . . , Xt+? are feature frames within a
length 2? + 1 window of features centered at time
t. After several empirical trials, we decided on
neural networks for vowel classification because of
the availability of efficient discriminative training al-
gorithms and their computational simplicity. Specif-
ically we use a simple 2-layer multi-layer percep-
tron (Bishop, 1995) whose input layer consists of
26 ? 7 = 182 nodes, where 26 is the dimension of
Xt, the MFCC feature vector, and 2? + 1 = 7 is the
997
number of consecutive frames, and that has 50 hid-
den nodes (the numbers 7 and 50 were determined
empirically). The output layer has 4 output nodes
representing 4 vowel probabilities. During training,
the network is optimized to minimize the Kullback-
Leibler (K-L) divergence between the output and the
true label distribution, thus achieving the aforemen-
tioned probabilistic interpretation.
The VJ engine needs not only to detect that the
user is specifying a vowel (for continuous control)
but also a consonant-vowel-consonant (CVC) pat-
tern (for discrete control) quickly and with a low
probability of confusion (a VJ system also uses C
and CV patterns for discrete commands). Requir-
ing an initial consonant will phonetically distinguish
these sounds from the pure vowel segments used
for continuous control ? the VJ system constantly
monitors for changes that indicate the beginning of
one of the discrete control commands. The vowel
within the CV and CVC patterns, moreover, can help
prevent background noise from being mis-classified
as a discrete sound. Lastly, each such pattern cur-
rently requires an ending silence, so that the next
command (a new discrete sound or continuous con-
trol vowel) can be accurately initiated. In all cases, a
simple threshold-based rejection mechanism is used
to reduce false positives.
To recognize the discrete control signals, HMMs
are employed since, as in standard speech recogni-
tion, time warping is necessary to normalize for dif-
ferent signal durations corresponding to the same
class. Specifically, we embed phone HMMs into
?word? (C, CV, or CVC) HMMs. In this way, it
is possible to train phone models using a training
set that covers all possible phones, and then con-
struct an application-specific discrete command vo-
cabulary without retraining by recombining existing
phone HMMs into new word HMMs. Therefore,
each VJ-driven application can have its own appro-
priate discrete sound set.
3.3 Motion Control: Direction and Velocity
The VJ motion control module receives several pat-
tern recognition parameters and processes them to
produce output more appropriate for determining 2-
D movement in the VJ-WIMP application.
Initial experiments suggested that using pitch to
affect cursor velocity (Igarashi and Hughes, 2001)
would be heavily constrained by an individual?s vo-
cal range. Giving priority to a more universal user-
independent VJ system, we instead focused on rela-
tive energy. Our observation that users often became
quiet when trying to move small amounts confirmed
energy as a natural choice. Drastically different in-
trinsic average energy levels for each vowel, how-
ever, meant that comparing all sounds to a global av-
erage energy would create a large vowel-dependent
bias. To overcome this, we distribute the energy per
frame among the different vowels, in proportion to
the probabilities output by the neural network, and
track the average energy for each vowel indepen-
dently. By splitting the power in this way, there is
no effect when probabilities are close to 1, and we
smooth out changes during vowel transitions when
probabilities are more evenly distributed.
There are many possible options for determining
velocity (a vector capturing both direction and speed
magnitude) and ?acceleration? (a function determin-
ing how the control-to-display ratio changes based
on input parameters), and the different schemes have
a large impact on user satisfaction. Unlike a standard
mouse cursor, where the mapping is from 2-D hand
movement to a 2-D screen, the VJ system maps from
vocal-tract articulatory movement to a 2-D screen,
and the transformation is not as straightforward. All
values are for the current time frame t unless indi-
cated otherwise. First, a raw direction value is cal-
culated for each axis j ? {x, y} as
dj =
?
i
pi ? ?vi, ej? (1)
in which pi = p(Vt = i|Xt??,...,t+? ) is the proba-
bility for vowel i at time t, vi is a unit vector in the
direction of vowel i, ej is the unit-length positive di-
rectional basis vector along the j axis, and ?v, e? is
the projection of vector v onto unit vector e. To de-
termine movement speed, we first calculate a scalar
for each axis j as
sj =
?
i
max
[
0, gi
(
pi ? f(
E
?i
)
)]
? |?vi, ej?|
where E is the energy in the current frame, ?i is the
average energy for vowel i, and f(?) and gi(?) are
functions used for energy normalization and percep-
tual scaling (such as logs and/or cube-roots). This
therefore allocates frame energy to direction based
on the vowel probabilities. Lastly, we calculate the
velocity for axis j at the current frame as
Vj = ? ? s
?
j ? exp(?sj). (2)
998
where ? represents the overall system sensitivity and
the other values (? and ?) are warping constants, al-
lowing the user to control the shape of the accelera-
tion curve. Typically only one of ? and ? is nonzero.
Setting both to zero results in constant-speed move-
ment along each axis, while ? = 1 and ? = 0
gives a linear mapping that will scale motion with
energy but have no acceleration. The current user-
independent system uses ? = 0.6, ? = 1.0 and sets
? = 0. Lastly, the final velocity along axis j is Vjdj .
Future publications will report on systematic evalu-
ations of different f(?) and gi(?) functions.
3.4 Motion Control: User Adaptation
Since vowel quality is used for continuous control,
inaccuracies can arise due to speaker variability ow-
ing to different speech loudness levels, vocal tract
lengths, etc. Moreover, a vowel class articulated by
one user might partially overlap in acoustic space
with a different vowel class from another user. This
imposes limitations on a purely user-independent
vowel classifier. Differences in speaker loudness
alone could cause significant unpredictability. To
mitigate these problems, we have designed an adap-
tation procedure where each user is asked to pro-
nounce four pre-defined vowel sounds, each last-
ing 2-3 seconds, at the beginning of a VJ ses-
sion. We have investigated several novel adaptation
strategies utilizing both neural networks and support
vector machines (SVM). The fundamental idea be-
hind them both is that an initial speaker-independent
transformation of the space is learned using train-
ing data, and is represented by the first layer of a
neural network. Adaptation data then is used to
transform various parameters of the classifier (e.g.,
all or sub-portions of the neural network, or the para-
meters of the SVM). Further details of some of these
novel adaptation strategies appear in (X.Li et al,
2005), and the remainder will appear in forthcom-
ing publications. Also, the average energy values of
each vowel for each user are recorded and used to
normalize the speed control rate mentioned above.
Preliminary evaluations on the data so far collected
show very good results, with adaptation reducing the
vowel classification error rate by 18% for the 4-class
case, and 35% for the 8-class case. Moreover, infor-
mal studies have shown that users greatly prefer the
VJ system after adaptation than before.
4 Applications and Videos
Our overall intent is for VJ to interface with a va-
riety of applications, and our primary application
so far has been to drive a standard WIMP interface
with VJ controls, what we call the VJ-WIMP ap-
plication. The current VJ version allows left but-
ton clicks (press and release, using the consonant
[k]) as well as left button toggles (using consonant
[ch]) to allow dragging. Since WIMP interfaces
are so general, this allows us to indirectly control
a plethora of different applications. Video demon-
strations are available at the URL: http://ssli.
ee.washington.edu/vj.
One of our key VJ applications is vocal web
browsing. The video (dated 6/2005) shows exam-
ples of two web browsing tasks, one as an exam-
ple of navigating the New York Times web site, the
other using Google Maps to select and zoom in on a
target area. Section 5 describes a preliminary evalu-
ation on these tasks. We have also started using the
VJ engine to control video games (third video ex-
ample), have interfaced VJ with the Dasher system
(Ward et al, 2000) (we call it the ?Vocal Dasher?),
and have also used VJ for figure drawing.
Several additional direct VJ-applications have
also been developed. Specifically, we have directly
interfaced the VJ system into a simple blocks world
environment, where more precise object movement
is possible than via the mouse driver. Specifically,
this environment can draw arbitrary trajectories, and
can precisely measure user fidelity when moving an
object along a trajectory. Fidelity depends both on
positional accuracy and task duration. One use of
this environment shows the spatial direction corre-
sponding to vocal effort (useful for training, forth
video example). Another shows a simple robotic
arm being controlled by VJ. We plan to use this
environment to perform formal and precise user-
performance studies in future work.
5 Preliminary User Study
We conducted a preliminary user study1 to evaluate
the feasibility of VJ and to obtain feedback regard-
ing specific difficulties in using the VJ-WIMP sys-
tem. While this study is not accurate in that: 1) it
does not yet involve the intended target population
1The user study presented here used an earlier version of VJ
than the current improved one described in the preceding pages.
999
of individuals with motor impairments, and: 2) the
users had only a small amount of time to practice and
become adept at using VJ, the study is still indica-
tive of the VJ approach?s overall viability as a novel
voice-based human-computer interface method. The
study quantitatively compares VJ performance with
a standard desktop mouse, and provides qualitative
measurement of the user?s perception of the system.
5.1 Experiment Setup
We recruited seven participants ranging from age 22
to 26, none of whom had any motor impairment.
Of the seven participants, two were female and five
were male. All of them were graduate students in
Computer Science, although none of them had pre-
viously heard of or used VJ. Four of the participants
were native English speakers; the other three had an
Asian language as their mother tongue.
We used a Dell Inspiron 9100 laptop with a 3.2
GHz Intel Pentium IV processor running the Fedora
Core 2 operating system, with a 1280 x 800 24-bit
color display. The laptop was equipped with an ex-
ternal Microsoft IntelliMouse connected through the
USB port which was used for all of the tasks in-
volving the mouse. A head-mounted Amanda NC-
61 microphone was used as the audio input device,
while the audio feedback from the laptop was output
through the laptop speakers. The Firefox browser
was used for all of the tasks, with the browser screen
maximized such that the only portion of the screen
which was not displaying the contents of the web
page was the top navigation toolbar which was 30
pixels high.
5.2 Quantitative and Qualitative Evaluation
At the beginning of the quantitative evaluation, each
participant was given a brief description of the VJ
operations and was shown a demonstration of the
system by a practiced experimenter. The participant
was then guided through an adaptation process dur-
ing which she/he was asked to pronounce the four
directional vowels (Section 3.4). After adaptation,
the participant was given several minutes to practice
using a simple target clicking application. The quan-
titative portion of our evaluation followed a within-
participant design. We exposed each participant to
two experimental conditions which we refer to as
input modalities: the mouse and the VJ. Each par-
ticipant completed two tasks on each modality, with
one trial per task.
The first task was a link navigation task (Link),
in which the participants were asked to start from a
specific web page and follow a particular set of links
to reach a destination. Before the trial, the experi-
menter demonstrated the specified sequence of links
to the participant by using the mouse and clicking at
the appropriate links. The participant was also pro-
vided with a sheet of paper for their reference that
listed the sequence of links that would lead them to
the target. The web site we used was a Computer
Science Department student guide and the task in-
volved following six links with the space between
each successive link including both horizontal and
vertical components.
The second task was map navigation (Map), in
which the participant was asked to navigate an on-
line map application from a starting view (showing
the entire USA) to get to a view showing a partic-
ular campus. The size of the map was 400x400
pixels, and the set of available navigation controls
surrounding the map included ten discrete zoom
level buttons, eight directional panning arrows, and
a click inside the map causing the map to be centered
and zoomed in by one level. Before the trial, a prac-
ticed experimenter demonstrated how to locate the
campus map starting from the USA view to ensure
they were familiar with the geography.
For each task, the participants performed one trial
using the mouse, and one trial using a 4-class VJ.
The trials were presented to the participants in a
counterbalanced order. We recorded the completion
time for each trial, as well as the number of false
positives (system interprets a click when the user
did not make a click sound), missed recognitions
(the user makes a click sound but the system fails to
recognize it as a click), and user errors (whenever
the user clicks on an incorrect link). The recorded
trial times include the time used by all of the above
errors including recovery time.
After the completion of the quantitative evalu-
ation, the participants were given a questionnaire
which consisted of 14 questions related to the partic-
ipants? perception of their experience using VJ such
as the degree of satisfaction, frustration, and embar-
rassment. The answers were encoded on a 7-point
Likert scale. We also included a space where the
participants could write in any comments, and an in-
1000
010
20
30
40
50
60
70
80
90
100
Link Map
Task type
T
as
k 
co
m
p
le
ti
o
n
 t
im
e 
(s
ec
o
n
d
s)
Mouse
Vocal Joystick
Figure 4: Task complement times
0
2
4
6
8
10
12
14
16
18
20
M,
 K
ore
a
M,
 N
ort
he
as
t
M,
 M
idw
es
t
M,
 N
ort
he
as
t
F, 
Mi
d-A
tla
nti
c
F, 
Ch
ina
M,
 C
hin
a
Participant (Gender, Origin)
N
um
be
r o
f m
is
se
d 
re
co
gn
iti
on
s
Link
Map
Figure 5: Missed recognitions by participant
formal post-experiment interview was performed to
solicit further feedback.
5.3 Results
Figure 4 shows the task completion times for Link
and Map tasks, Figure 5 shows the breakdown of
click errors by individual participants, Figure 6
shows the average number of false positive and
missed recognition errors for each of the tasks.
There was no instance of user error in any trial. Fig-
ure 7 shows the median of the responses to each of
the fourteen questionnaire questions (error bars in
each plot show ? standard error). In our measure-
ment of the task completion times, we considered
the VJ?s recognition error rate as a fixed factor, and
thus did not subtract the time spent during those er-
rors from the task completion time.
There were several other interesting observations
that were made throughout the study. We noticed
that the participants who had the least trouble with
missed recognitions for the clicking sound were ei-
0
1
2
3
4
5
6
7
8
9
10
Link Map
Task type
N
um
be
r o
f e
rr
or
s
False positive
Missed Recognition
Figure 6: Average number of click errors per task
1.0
2.0
3.0
4.0
5.0
6.0
7.0
Ea
sy 
to l
ear
n
Ea
sy 
to u
se
Dif
f icu
lt to
 co
ntr
ol
Fru
stra
ting Fu
n
Tir
ing
Em
bar
ras
sin
g
Intu
itiv
e
Err
or 
pro
ne
Se
lf-c
ons
cio
us
Se
lf-c
ons
cio
usn
ess
 de
cre
ase
d
Vo
we
l so
und
s d
isti
ngu
ish
abl
e
Ma
p h
ard
er t
han
 se
arc
h
Mo
tion
 ma
tch
ed 
inte
ntio
n
Strongly
agree
Strongly
disagree
Figure 7: Questionnaire results
ther female or with an Asian language background,
as shown in Figure 5. Our hypothesis regarding the
better performance by female participants is that the
original click sound was trained on one of our fe-
male researcher?s voice. We plan also in future work
to determine how the characteristics of different na-
tive language speakers influence VJ, and ultimately
to correct for any bias.
All but one user explicitly expressed their confu-
sion in distinguishing between the [ae] and [aa] vow-
els. Four of the seven participants independently
stated that their performance would probably have
been better if they had been able to practice longer,
and did not attribute their perceived suboptimal per-
formance to the quality of the VJ?s recognition sys-
tem. Several participants reported that they felt their
vocal cords were strained due to having to produce a
loud sound in order to get the cursor to move at the
desired speed. We suspect this is due either to ana-
log gain problems or to their adapted voice being too
loud, and therefore the system calibrating the nor-
mal speed to correspond to the loud voice. We have
since removed this problem by adjusting our adapta-
1001
tion strategy to express preference for a quiet voice.
In summary, the results from our study suggest
that users without any prior experience were able
to perform basic mouse based tasks using the Vocal
Joystick system with relative slowdown of four to
nine times compared to a conventional mouse. We
anticipate that future planned improvements in the
algorithms underlying the VJ engine (to improve ac-
curacy, user-independence, adaptation, and speed)
will further increase the VJ system?s viability, and
combined with practice could improve VJ enough so
that it becomes a reasonable alternative compared to
a standard mouse?s performance.
6 Related Work
Related voice-based interface studies include
(Igarashi and Hughes, 2001; Olwal and Feiner,
2005). Igarashi & Hughes presented a system where
non-verbal voice features control a mouse system ?
their system requires a command-like discrete sound
to determine direction before initiating a movement
command, where pitch is used to control veloc-
ity. We have empirically found an energy-based
mapping for velocity (as used in our VJ system)
both more reliable (no pitch-tracking errors) and
intuitive. Olwal & Feiner?s system moves the mouse
only after recognizing entire words. de Mauro?s
?voice mouse? http://www.dii.unisi.it/
?maggini/research/voice mouse.html
focuses on continuous cursor movements similar
to the VJ scenario; however, the voice mouse
only starts moving after the vocalization has been
completed leading to long latencies, and it is not
easily portable to other applications. Lastly, the
commercial dictation program Dragon by ScanSoft
includes MouseGridTM(Dra, 2004) which allows
discrete vocal commands to recursively 9-partition
the screen, thus achieving log-command access to a
particular screen point. A VJ system, by contrast,
uses continuous aspects of the voice, has change
latency (about 60ms) not much greater than reaction
time, and allows the user to make instantaneous
directional change using one?s voice (e.g., a user
can draw a ?U? shape in one breath).
7 Conclusions
We have presented new voice-based assistive tech-
nology for continuous control tasks and have
demonstrated an initial system implementation of
this concept. An initial user study using a group
of individuals from the non-target population con-
firmed the feasibility of this technology. We plan
next to further improve our system by evaluating a
number of novel pattern classification techniques to
increase accuracy and user-independence, and to in-
troduce additional vocal characteristics (possibilities
include vibrato, degree of nasality, rate of change
of any of the above as an independent parameter)
to increase the available simultaneous degrees of
freedom controllable via the voice. Moreover, we
plan to develop algorithms to decouple unintended
user correlations of these parameters, and to further
advance both our adaptation and acceleration algo-
rithms.
References
C. Bishop. 1995. Neural Networks for Pattern Recogni-
tion. Clarendon Press, Oxford.
2004. Dragon naturally speaking, MousegridTM, Scan-
Soft Inc.
D.Talkin. 1995. A robust algorithm for pitch track-
ing (RAPT). In W.B.Kleign and K.K.Paliwal, editors,
Speech Coding and Synthesis, pp. 495?515, Amster-
dam. Elsevier Science.
X. Huang, A. Acero, and H.-W. Hon. 2001. Spoken Lan-
guage Processing: A Guide to Theory, Algorithm, and
System Development. Prentice Hall.
T. Igarashi and J. F. Hughes. 2001. Voice as sound: Us-
ing non-verbal voice input for interactive control. In
ACM UIST 2001, November.
J.Malkin, X.Li, and J.Bilmes. 2005. A graphical model
for formant tracking. In Proc. IEEE Intl. Conf. on
Acoustics, Speech, and Signal Processing.
A. Olwal and S. Feiner. 2005. Interaction techniques us-
ing prosodic feature of speech and audio localization.
In Proceedings of the 10th International Conference
on Intelligent User Interfaces, pp. 284?286.
D. Ward, A. F. Blackwell, and D. C. MacKay. 2000.
Dasher - a data entry interface using continuous ges-
tures and language models. In ACM UIST 2000.
X.Li, J.Malkin, and J.Bilmes. 2004. A graphical model
approach to pitch tracking. In Proc. Int. Conf. on Spo-
ken Language Processing.
X.Li, J.Bilmes, and J.Malkin. 2005. Maximum mar-
gin learning and adaptation of MLP classifers. In 9th
European Conference on Speech Communication and
Technology (Eurospeech?05), Lisbon, Portugal, Sep-
tember.
1002
Factored Language Models and Generalized Parallel Backoff
Jeff A. Bilmes Katrin Kirchhoff
SSLI-LAB, University of Washington, Dept. of Electrical Engineering
{bilmes,katrin}@ssli.ee.washington.edu
Abstract
We introduce factored language models
(FLMs) and generalized parallel backoff
(GPB). An FLM represents words as bundles
of features (e.g., morphological classes, stems,
data-driven clusters, etc.), and induces a prob-
ability model covering sequences of bundles
rather than just words. GPB extends standard
backoff to general conditional probability
tables where variables might be heterogeneous
types, where no obvious natural (temporal)
backoff order exists, and where multiple
dynamic backoff strategies are allowed. These
methodologies were implemented during the
JHU 2002 workshop as extensions to the
SRI language modeling toolkit. This paper
provides initial perplexity results on both
CallHome Arabic and on Penn Treebank Wall
Street Journal articles. Significantly, FLMs
with GPB can produce bigrams with signif-
icantly lower perplexity, sometimes lower
than highly-optimized baseline trigrams. In a
multi-pass speech recognition context, where
bigrams are used to create first-pass bigram
lattices or N-best lists, these results are highly
relevant.
1 Introduction
The art of statistical language modeling (LM) is to create
probability models over words and sentences that trade-
off statistical prediction with parameter variance. The
field is both diverse and intricate (Rosenfeld, 2000; Chen
and Goodman, 1998; Jelinek, 1997; Ney et al, 1994),
with many different forms of LMs including maximum-
entropy, whole-sentence, adaptive and cache-based, to
name a small few. Many models are simply smoothed
conditional probability distributions for a word given its
preceding history, typically the two preceding words.
In this work, we introduce two new methods for lan-
guage modeling: factored language model (FLM) and
generalized parallel backoff (GPB). An FLM considers a
word as a bundle of features, and GPB is a technique that
generalized backoff to arbitrary conditional probability
tables. While these techniques can be considered in iso-
lation, the two methods seem particularly suited to each
other ? in particular, the method of GPB can greatly fa-
cilitate the production of FLMs with better performance.
2 Factored Language Models
In a factored language model, a word is viewed as a vec-
tor of k factors, so that wt ? {f1t , f2t , . . . , fKt }. Fac-
tors can be anything, including morphological classes,
stems, roots, and other such features in highly in-
flected languages (e.g., Arabic, German, Finnish, etc.),
or data-driven word classes or semantic features useful
for sparsely inflected languages (e.g., English). Clearly,
a two-factor FLM generalizes standard class-based lan-
guage models, where one factor is the word class and
the other is words themselves. An FLM is a model over
factors, i.e., p(f1:Kt |f1:Kt?1:t?n), that can be factored as a
product of probabilities of the form p(f |f1, f2, . . . , fN ).
Our task is twofold: 1) find an appropriate set of factors,
and 2) induce an appropriate statistical model over those
factors (i.e., the structure learning problem in graphical
models (Bilmes, 2003; Friedman and Koller, 2001)).
3 Generalized Parallel Backoff
An individual FLM probability model can be seen as a di-
rected graphical model over a set of N + 1 random vari-
ables, with child variable F and N parent variables F1
through FN (if factors are words, then F = Wt and Fi =
Wt?i). Two features make an FLM distinct from a stan-
dard language model: 1) the variables {F, F1, . . . , FN}
can be heterogeneous (e.g., words, word clusters, mor-
phological classes, etc.); and 2) there is no obvious nat-
ural (e.g., temporal) backoff order as in standard word-
based language models. With word-only models, back-
off proceeds by dropping first the oldest word, then the
next oldest, and so on until only the unigram remains. In
p(f |f1, f2, . . . , fN ), however, many of the parent vari-
ables might be the same age. Even if the variables have
differing seniorities, it is not necessarily best to drop the
oldest variable first.
F 1F 2F 3F
F
F 1F 2F F 1F 3F F 2F 3F
F 1F F 3FF 2F
A
B C D
E F G
H
Figure 1: A backoff graph for F with three parent vari-
ables F1, F2, F3. The graph shows all possible single-
step backoff paths, where exactly one variable is dropped
per backoff step. The SRILM-FLM extensions, however,
also support multi-level backoff.
We introduce the notion of a backoff graph (Figure 1)
to depict this issue, which shows the various backoff
paths from the all-parents case (top graph node) to the
unigram (bottom graph node). Many possible backoff
paths could be taken. For example, when all variables
are words, the path A? B? E?H corresponds to tri-
gram with standard oldest-first backoff order. The path
A? D?G?H is a reverse-time backoff model. This
can be seen as a generalization of lattice-based language
modeling (Dupont and Rosenfeld, 1997) where factors
consist of words and hierarchically derived word classes.
In our GPB procedure, either a single distinct path
is chosen for each gram or multiple parallel paths are
used simultaneously. In either case, the set of back-
off path(s) that are chosen are determined dynamically
(at ?run-time?) based on the current values of the vari-
ables. For example, a path might consist of nodes
A? (BCD) ? (EF) ?G where node A backs off in par-
allel to the three nodes BCD, node B backs off to nodes
(EF), C backs off to (E), and D backs off to (F).
This can be seen as a generalization of the standard
backoff equation. In the two parents case, this becomes:
pGBO(f |f1, f2) =
{
dN(f,f1,f2)pML(f |f1, f2) if N(f, f1, f2) > ?
?(f1, f2)g(f, f1, f2) otherwise
where dN(f,f1,f2) is a standard discount (determining
the smoothing method), pML is the maximum likeli-
hood distribution, ?(f1, f2) are backoff weights, and
g(f, f1, f2) is an arbitrary non-negative backoff function
of its three factor arguments. Standard backoff occurs
with g(f, f1, f2) = pBO(f |f1), but the GPB procedures
can be obtained by using different g-functions. For exam-
ple, g(f, f1, f2) = pBO(f |f2) corresponds to a different
backoff path, and parallel backoff is obtained by using an
appropriate g (see below). As long as g is non-negative,
the backoff weights are defined as follows:
?(f1, f2) =
1 ?
?
f:N(f,f1,f2)>?
dN(f,f1,f2)pML(f |f1, f2)
?
f:N(f,f1,f2)<=?
g(f, f1, f2)
This equation is non-standard only in the denominator,
where one may no longer sum over the factors f only
with counts greater than ? . This is because g is not nec-
essarily a distribution (i.e., does not sum to unity). There-
fore, backoff weight computation can indeed be more ex-
pensive for certain g functions, but this appears not to be
prohibitive as demonstrated in the next few sections.
Table 1: CallHome Arabic Results.
LM parents backoff function/path(s) ppl
3-gram w1, w2 - / temporal [2, 1] 173
FLM 3-gram w1, w2,m1, s1 - / [2, 1, 4, 3] 178
GPB-FLM 3-gram w1, w2,m1, s1 g1 / [2, 1, (3, 4), 3, 4] 166
2-gram w1 - / temporal [1] 175
FLM 2-gram w1,m1 - / [2, 1] 173
FLM 2-gram w1,m1, s1 - / [1, 2, 3] 179
GPB-FLM 2-gram w1,m1, s1 g1 / [1, (2, 3), 2, 3] 167
4 SRILM-FLM extensions
During the recent 2002 JHU workshop (Kirchhoff et al,
2003), significant extensions were made to the SRI lan-
guage modeling toolkit (Stolcke, 2002) to support arbi-
trary FLMs and GPB procedures. This uses a graphical-
model like specification language, and where many dif-
ferent backoff functions (19 in total) were implemented.
Other features include: 1) all SRILM smoothing methods
at every node in a backoff graph; 2) graph level skipping;
and 3) up to 32 possible parents (e.g., 33-gram). Two of
the backoff functions are (in the three parents case):
g(f, f1, f2, f3) = pGBO(f |f`1 , f`2)
where
(`1, `2) = argmax
(m1,m2)?{(1,2),(1,3),(2,3)}
pGBO(f |fm1 , fm2)
(call this g1) or alternatively, where
(`1, `2) = argmax
(m1,m2)?{(1,2),(1,3),(2,3)}
N(f, fm1 , fm2 )
|{f : N(f, fm1 , fm2 ) > 0}|
(call this g2) where N() is the count function. Imple-
mented backoff functions include maximum/min (nor-
malized) counts/backoff probabilities, products, sums,
mins, maxs, (weighted) averages, and geometric means.
5 Results
GPB-FLMs were applied to two corpora and their per-
plexity was compared with standard optimized vanilla bi-
and trigram language models. In the following, we con-
sider as a ?bigram? a language model with a temporal
history that includes information from no longer than one
previous time-step into the past. Therefore, if factors are
deterministically derivable from words, a ?bigram? might
include both the previous words and previous factors as
a history. From a decoding state-space perspective, any
such bigram would be relatively cheap.
In CallHome-Arabic, words are accompanied with de-
terministically derived factors: morphological class (M),
Table 2: Penn Treebank WSJ Results.
LM parents Backoff function/path(s) ppl (?std. dev.)
3-gram w1, w2 - / temporal [2, 1] 258(?1.2)
2-gram w1 - / temporal [1] 320(?1.3)
GPB-FLM 2-gram A w1, d1, t1 g2 / [(1, 2, 3), (1, 2), (2, 3), (3, 1), 1, 2, 3] 266(?1.1)
GPB-FLM 2-gram B w1, d1, f1 g2 / [2, 1] 276(?1.3)
GPB-FLM 2-gram C w1, d1, c1 g2/ [1, (2, 3), 2, 3] 275(?1.2)
stems (S), roots (R), and patterns (P). Training data con-
sisted of official training portions of the LDC CallHome
ECA corpus plus the CallHome ECA supplement (100
conversations). For testing we used the official 1996 eval-
uation set. Results are given in Table 1 and show perplex-
ity for: 1) the baseline 3-gram; 2) a FLM 3-gram using
morphs and stems; 3) a GPB-FLM 3-gram using morphs,
stems and backoff function g1; 4) the baseline 2-gram;
5) an FLM 2-gram using morphs; 6) an FLM 2-gram us-
ing morphs and stems; and 7) an GPB-FLM 2-gram using
morphs and stems. Backoff path(s) are depicted by listing
the parent number(s) in backoff order. As can be seen, the
FLM alone might increase perplexity, but the GPB-FLM
decreases it. Also, it is possible to obtain a 2-gram with
lower perplexity than the optimized baseline 3-gram.
The Wall Street Journal (WSJ) data is from the Penn
Treebank 2 tagged (?88-?89) WSJ collection. Word
and POS tag information (Tt) was extracted. The sen-
tence order was randomized to produce 5-fold cross-
validation results using (4/5)/(1/5) training/testing sizes.
Other factors included the use of a simple determinis-
tic tagger obtained by mapping a word to its most fre-
quent tag (Ft), and word classes obtained using SRILM?s
ngram-class tool with 50 (Ct) and 500 (Dt) classes.
Results are given in Table 2. The table shows the baseline
3-gram and 2-gram perplexities, and three GPB-FLMs.
Model A uses the true by-hand tag information from the
Treebank. To simulate conditions during first-pass de-
coding, Model B shows the results using the most fre-
quent tag, and Model C uses only the two data-driven
word classes. As can be seen, the bigram perplexities
are significantly reduced relative to the baseline, almost
matching that of the baseline trigram. Note that none of
these reduced perplexity bigrams were possible without
using one of the novel backoff functions.
6 Discussion
The improved perplexity bigram results mentioned above
should ideally be part of a first-pass recognition step of a
multi-pass speech recognition system. With a bigram, the
decoder search space is not large, so any appreciable LM
perplexity reductions should yield comparable word er-
ror reductions for a fixed set of acoustic scores in a first-
pass. For N-best or lattice generation, the oracle error
should similarly improve. The use of an FLM with GPB
in such a first pass, however, requires a decoder that sup-
ports such language models. Therefore, FLMs with GPB
will be incorporated into GMTK (Bilmes, 2002), a gen-
eral purpose graphical model toolkit for speech recogni-
tion and language processing. The authors thank Dimitra
Vergyri, Andreas Stolcke, and Pat Schone for useful dis-
cussions during the JHU?02 workshop.
References
[Bilmes2002] J. Bilmes. 2002. The GMTK docu-
mentation. http://ssli.ee.washington.edu/
?bilmes/gmtk.
[Bilmes2003] J. A. Bilmes. 2003. Graphical models and au-
tomatic speech recognition. In R. Rosenfeld, M. Osten-
dorf, S. Khudanpur, and M. Johnson, editors, Mathematical
Foundations of Speech and Language Processing. Springer-
Verlag, New York.
[Chen and Goodman1998] S. F. Chen and J. Goodman. 1998.
An empirical study of smoothing techniques for language
modeling. Technical Report Tr-10-98, Center for Research
in Computing Technology, Harvard University, Cambridge,
Massachusetts, August.
[Dupont and Rosenfeld1997] P. Dupont and R. Rosenfeld.
1997. Lattice based language models. Technical Report
CMU-CS-97-173, Carnegie Mellon University, Pittsburgh,
PA 15213, September.
[Friedman and Koller2001] N. Friedman and D. Koller. 2001.
Learning Bayesian networks from data. In NIPS 2001 Tuto-
rial Notes. Neural Information Processing Systems, Vancou-
ver, B.C. Canada.
[Jelinek1997] F. Jelinek. 1997. Statistical Methods for Speech
Recognition. MIT Press.
[Kirchhoff et al2003] K. Kirchhoff et al2003. Novel ap-
proaches to arabic speech recognition: Report from the 2002
johns-hopkins summer workshop. In Proc. IEEE Intl. Conf.
on Acoustics, Speech, and Signal Processing, Hong Kong.
[Ney et al1994] H. Ney, U. Essen, and R. Kneser. 1994. On
structuring probabilistic dependencies in stochastic language
modelling. Computer Speech and Language, 8:1?38.
[Rosenfeld2000] R. Rosenfeld. 2000. Two decades of statistical
language modeling: Where do we go from here? Proceed-
ings of the IEEE, 88(8).
[Stolcke2002] A. Stolcke. 2002. SRILM- an extensible lan-
guage modeling toolkit. In Proc. Int. Conf. on Spoken Lan-
guage Processing, Denver, Colorado, September.
Multi-Speaker Language Modeling
Gang Ji and Jeff Bilmes ?
SSLI Lab, Department of Electrical Engineering
University of Washington
Seattle, WA 98195-2500
{gang,bilmes}@ee.washington.edu
Abstract
In conventional language modeling, the words
from only one speaker at a time are repre-
sented, even for conversational tasks such as
meetings and telephone calls. In a conversa-
tional or meeting setting, however, speakers
can have significant influence on each other.
To recover such un-modeled inter-speaker in-
formation, we introduce an approach for con-
versational language modeling that considers
words from other speakers when predicting
words from the current one. By augmenting a
normal trigram context, our new multi-speaker
language model (MSLM) improves on both
Switchboard and ICSI Meeting Recorder cor-
pora. Using an MSLM and a conditional mu-
tual information based word clustering algo-
rithm, we achieve a 8.9% perplexity reduction
on Switchboard and a 12.2% reduction on the
ICSI Meeting Recorder data.
1 Introduction
Statistical language models (LMs) are used in many ap-
plications such as speech recognition, handwriting recog-
nition, spelling correction, machine translation, and in-
formation retrieval. The goal is to produce a probability
model over a word sequence P (W ) = P (w1, ? ? ? , wT ).
Conventional language models are often based on a fac-
torized form P (W ) ? ?t P (wt|?(ht)), where ht is thehistory for wt and ? is a history mapping.
The case of n-gram language modeling, where
?(ht) = wt?n+1 ? ? ?wt?1, is widely used. Typically,
n = 3, which yields a trigram model. A refinement of
this model is the class-based n-gram where the words are
partitioned into equivalence classes (Brown et al, 1992).
?This work was funded by NSF under grant IIS-0121396.
In general, smoothing techniques are applied to lessen
the curse of dimensionality. Among all methods, mod-
ified Kneser-Ney smoothing (Chen and Goodman, 1998)
is widely used because of its good performance.
Modeling conversational language is a particularly dif-
ficult task. Even though conventional techniques work
well on read or prepared speech, situations such as tele-
phone conversations or multi-person meetings pose great
research challenges due to disfluencies, and odd syn-
tactic/discourse patterns. Other difficulties include false
starts, interruptions, and poor or unrepresented grammar.
Most state-of-the-art language models consider word
streams individually and treat different phrases indepen-
dently. In this work, we introduce multi-speaker lan-
guage modeling (MSLM), which models the effects on
a speaker of words spoken by other speakers partici-
pating in the same conversation or meeting. Our new
model achieves initial perplexity reductions of 6.2% on
Switchboard-I, 5.8% on Switchboard Eval-2003, and
10.3% on ICSI Meeting data. In addition, we devel-
oped a word clustering procedure (based on a standard
approach (Brown et al, 1992)) that optimizes conditional
word clusters. Our class-based MSLMs using our new
algorithm yield improvements of 7.1% on Switchboard-I,
8.9% on Switchboard Eval-2003, and 12.2% on meetings.
A brief outline follows: Section 2 introduces multi-
speaker language modeling. Section 3 provides ini-
tial evaluations on Switchboard and the ICSI Meeting
data. Section 4 presents evaluations using our class-based
multi-speaker language models, and Section 5 concludes.
2 Multi-speaker Language Modeling
In a conversational setting, such as during a meeting or
telephone call, the words spoken by one speaker are af-
fected not only by his or her own previous words but
also by other speakers. Such inter-speaker dependency,
however, is typically ignored in standard n-gram lan-
guage models. In this work, information (i.e., word to-
kens) from other speakers (A) is used to better predict
word tokens of the current speaker (W ). When predict-
ing wt, instead of using P (wt|w0, ? ? ? , wt?1), the form
P (wt|w0, ? ? ? , wt?1; a0, ? ? ? , at) is used. Here at repre-
sents a word spoken by some other speaker with appro-
priate starting time (Section 3). A straight-forward im-
plementation is to extend the normal trigram model as:
P (wt|?(ht)) = P (wt|wt?1, wt?2, at). (1)
Hi
Hi
Diana ? Take care
? Bye
A:
W:
(a)
Saturday
A:
W: Saturday
on Sundayon 0.2s or Saturday0.4s
C5:
C4:
C3:
C1:
C0:
Saturday
Saturday
Saturday 1.1s SaturdayC2:
on Sundayon or Saturday
(b)
Figure 1: Examples of phone conversation (a) and meet-
ing (b). (Frame sizes are not proportional to time scale.)
Figure 1 shows an example from Switchboard (a) and
one from a meeting (b). In (a), only two speakers are in-
volved and the words from the current speaker, W , are
affected by the other speaker, A. At the beginning of a
conversation, the response to ?Hi? is likely to be ?Hi? or
?Hello.? At the end of the phone call, the response to
?Take care? might be ?Bye?, or ?You too?, etc. In (b),
we show a typical meeting conversation. Speaker C2 is
interrupting C3 when C3 says ?Sunday?. Because ?Sun-
day? is a day of the week, there is a high probability that
C2?s response is also a day of the week. In our model, we
only consider two streams at a time, W and A. There-
fore, when considering the probability of C2?s words, it
is reasonable to collapse words from all other speakers
(C0,C1,C3,C4, and C5) into one stream A as shown in
the figure. This makes available to C2 the rest of the
meeting to potentially condition on, although it does not
distinguish between different speakers.
Our model, Equation 1, is different from most lan-
guage modeling systems since our models condition on
both previous words and another potential factor A. Such
a model is easily represented using a factored language
model (FLM), an idea introduced in (Bilmes and Kirch-
hoff, 2003; Kirchhoff et al, 2003), and incorporated into
the SRILM toolkit (Stolcke, 2002). Note that a form of
cross-side modeling was used by BBN (Schwartz, 2004),
where in a multi-pass speech recognition system the out-
put of a first-pass from one speaker is used to prime words
in the language model for the other speaker.
3 Initial Evaluation
We evaluate MSLMs on three corpora: Switchboard-
I, Switchboard Eval-2003, and ICSI Meeting data. In
Switchboard-I, 6.83% of the words are overlapped in
time, where we define w1 and w2 as being overlapped
if s(w1) ? s(w2) < e(w1) or s(w2) ? s(w1) < e(w2),
where s(?) and e(?) are the starting and ending time of a
word.
The ICSI Meeting Recorder corpus (Janin et al, 2003)
consists of a number of meeting conversations with three
or more participants. The data we employed has 32 con-
versations, 35,000 sentences and 307,000 total words,
where 8.5% of the words were overlapped. As mentioned
previously, we collapse the words from all other speakers
into one stream A as a conditioning set for W . The data
consists of all speakers taking their turn being W .
To be used in an FLM, the words in each stream need to
be aligned at discrete time points. Clearly, at should not
come from wt?s future. Therefore, for each wt, we use
the closest previous A word in the past for at such that
s(wt?1) ? s(at) < s(wt). Therefore, each at is used
only once and no constraints are placed on at?s end time.
This is reasonable since one can often predict a speaker?s
word after it starts but before it completes.
We score using the model P (wt|wt?1, wt?2, at).1 Dif-
ferent back-off strategies, including different back-off
paths as well as combination methods (Bilmes and Kirch-
hoff, 2003), were tried and here we present the best re-
sults. The backoff order (for Switchboard-I and Meeting)
first dropped at, then wt?2, wt?1, ending with the uni-
form distribution. For Switchboard eval-2003, we used
a generalized parallel backoff mechanism. In all cases,
modified Kneser-Ney smoothing (Chen and Goodman,
1998) was used at all back-off points.
Results on Switchboard-I and the meeting data em-
ployed 5-fold cross-validation. Training data for Switch-
board eval-2003 consisted of all of Switchboard-I. In
Switchboard eval-2003, hand-transcribed time marks are
unavailable, so A was available only at the beginning of
utterances of W .2 Results (mean perplexities and stan-
dard deviations) are listed in Table 1 (Switchboard-I and
meeting) and the |V | column in Table 3.
Table 1: Perplexities from MSLM on Switchboard-I
(swbd-I) and ICSI Meeting data (mr)
data trigram four-gram mslm reduction
swbd-I 73.2?0.4 73.7?0.4 68.5?0.3 6.2%
mr 87.4?4.6 89.5?4.9 78.4?2.7 10.3%
1In all cases, end of sentence tokens, </s>, were not scored
to avoid artificially small perplexities arising when wt = at =
</s>, since P (</s>|</s>) yields a high probability value.
2Having time-marks, say, via a forced alignment would
likely improve our results.
In Table 1, the first column shows data set names. The
second and third columns show our best baseline trigram
and four-gram perplexities, both of which used interpo-
lation and modified Kneser-Ney at every back-off point.
The trigram outperforms the four-gram. The fourth col-
umn shows the perplexity results with MSLMs and the
last column shows the MSLM?s relative perplexity reduc-
tion over the (better) trigram baseline. This positive re-
duction indicates that for both data sets, the utilization of
additional information from other speakers can better pre-
dict the words of the current speaker. The improvement is
larger in the highly conversational meeting setting since
additional speakers, and thus more interruptions, occur.
3.1 Analysis
It is elucidating at this point to identify when and how
A-words can help predict W -words. We thus computed
the log-probability ratio of P (wt|wt?1, wt?2, at) and the
trigram P (wt|wt?1, wt?2) evaluated on all test set tuples
of form (wt?2, wt?1wt, at). When this ratio is large and
positive, conditioning on at significantly increases the
probability of wt in the context of wt?1 and wt?2. The
opposite is true when the ratio is large and negative. To
ensure the significance of our results, we define ?large?
to mean at least 101.5 ? 32, so that using at makes wt at
least 32 times more (or less) probable. We chose 32 in a
data-driven fashion, to be well above any spurious prob-
ability differences due to smoothing of different models.
At the first word of a phrase spoken by W , there are
a number of cases of A words that significantly increase
the probability of a W word relative to the trigram alone.
This includes (in roughly decreasing order of probabil-
ity) echos (e.g., when A says ?Friday?, W repeats it),
greetings/partings (e.g., a W greeting is likely to follow
an A greeting), paraphrases (e.g., ?crappy? followed by
?ugly?, or ?Indiana? followed by ?Purdue?), is-a relation-
ships (e.g., A saying ?corporation? followed by W saying
?dell?, A-?actor? followed by W -?Swayze?, A-?name?
followed by W -?Patricia?, etc.), and word completions.
On the other hand, some A contexts (e.g., laughter) sig-
nificantly decrease the probability of many W words.
Within a W phrase, other patterns emerge. In
particular, some A words significantly decrease the
probability that W will finish a commonly-used phrase.
For example, in a trigram alone, p(bigger|and, bigger),
p(forth|and, back), and p(easy|and, quick), all have
high probability. When also conditioning on A, some
A words significantly decrease the probability of
finishing such phrases. For example, we find that
p(easy|and, quick, ?uh-hmm?)  p(easy|and, quick).
A similar phenomena occurs for other com-
monly used phrases, but only when A has uttered
words such as ?yeah?, ?good?, ?ok?, ?[laugh-
ter]?, ?huh?, etc. While one possible explanation
of this is just due to decreased counts, we found
that for such phrases p(wt|wt?1, wt?2, at) 
minwt?3?S p4(wt|wt?1, wt?2, wt?3) where p4 is a
four-gram, S = {w : C(wt, wt?1, wt?2, w) > 0}, and
C is the 4-gram word count function for the switchboard
training and test sets. Therefore, our hypothesis is
that when W is in the process of uttering a predictable
phrase and A indicates she knows what W will say, it is
improbable that W will complete that phrase.
The examples above came from Switchboard-I, but we
found similar phenomena in the other corpora.
4 Conditional Probability Clustering
100 500 1000 1500 2000 |V|
68
70
72
74
76
78
80
82
84
86
88
number of classes
pe
rp
lex
ity
mr baselinemrswbd baseline
swbd
Figure 2: Class-based MSLM from MCMI clustering on
Switchboard-I (swbd) and ICSI Meeting (mr) data.
Table 2: Three types of class-based MSLMs on
Switchboard-I (swbd) and ICSI Meeting (mr) corpora
# of swbd mr
classes BROWN MMI MCMI BROWN MMI MCMI
100 68.9?0.3 68.4?0.3 68.2?0.3 78.9?3.0 77.3?2.8 76.8?2.8
500 68.9?0.3 68.3?0.3 67.9?0.3 78.7?3.1 77.1?2.8 76.7?2.8
1000 68.9?0.3 68.2?0.3 67.9?0.3 79.0?3.1 77.2?2.7 76.9?2.8
1500 69.0?0.3 68.2?0.3 68.0?0.3 79.6?3.1 77.4?2.7 77.4?2.7
2000 69.0?0.3 68.3?0.3 68.0?0.3 80.1?3.1 77.6?2.7 77.9?2.7
|V | 68.5?0.3 78.3?2.7
Table 3: Class-based MSLM on Switchboard Eval-2003
size 100 500 1000 1500 2000 |V | 3-gram 4-gram
ppl 65.8 65.5 65.6 65.7 66.1 67.9 72.1 76.3
% reduction 8.6 8.9 8.8 8.7 8.3 5.8 0 -5.8
Class-based language models (Brown et al, 1992;
Whittaker and Woodland, 2003) yield great benefits when
data sparseness abounds. SRILM (Stolcke, 2002) can
produce classes to maximize the mutual information
between the classes I(C(wt);C(wt?1)), as described
in (Brown et al, 1992). More recently, a method for clus-
tering words at different positions was developed (Ya-
mamoto et al, 2001; Gao et al, 2002). Our goal is
to produce classes that improve the scores P (wt|ht) =
P (wt|wt?1, wt?2, C1(at)), what we call class-based
MSLMs. In our case, the vocabulary for A is partitioned
into classes by either maximizing conditional mutual
information (MCMI) I(wt;C(at)|wt?1, wt?2) or just
maximizing mutual information (MMI) I(wt;C(at)).
While such clusterings can perform poorly under low
counts, our results show further consistent improvements.
Our new clustering procedures were implemented into
the SRILM toolkit. When partitioned into smaller
classes, the A-tokens are replaced by their correspond-
ing class IDs. The result is then trained using the same
factored language model as before. The resulting per-
plexities for the MCMI case are presented in Figure 2,
where the horizontal axis shows the number of A-stream
classes (the right-most shows the case before clustering),
and the vertical axis shows average perplexity. In both
data corpora, the average perplexities decrease after ap-
plying class-based MSLMs. For both Switchboard-I and
the meeting data, the best result is achieved using 500
classes (7.1% and 12.2% improvements respectively).
To compare different clustering algorithms, results
with the standard method of (Brown et al, 1992)
(SRILM?s ngram-class) are also reported. All the per-
plexities for these three types of class-based MSLMs are
given in Table 2. For Switchboard-I, ngram-class does
slightly better than without clustering. On the meeting
data, it even does slightly worse than no clustering. Our
MMI method does show a small improvement, and the
perplexities are further (but not significantly) reduced us-
ing our MCMI method (but at the cost of much more
computation during development).
We also show results on Switchboard eval-2003 in Ta-
ble 3. We compare an optimized four-gram, a three-
gram baseline, and various numbers of cluster sizes us-
ing our MCMI method and generalized backoff (Bilmes
and Kirchhoff, 2003), which, (again) with 500 clusters,
achieves an 8.9% relative improvement over the trigram.
5 Discussions and Conclusion
In this paper, novel multi-speaker language modeling
(MSLM) is introduced and evaluated. After simply
adding words from other speakers into a normal trigram
context, the new model shows a reasonable improvement
in perplexity. This model can be further improved when
class-based cross-speaker information is employed. We
also presented two different criteria for this clustering.
The more complex criteria gives similar results to the
simple one, presumably due to data sparseness. Even
though Switchboard and meeting data are different in
terms of topic, speaking style, and speaker number, one
might more robustly learn cross-speaker information by
training on the union of these two data sets.
There are a number of ways to extend this work. First,
our current approach is purely data driven. One can imag-
ine that higher level information (e.g., a dialog or other
speech act) about the other speakers might be particularly
important. Latent semantic analysis of stream A might
also be usefully employed here. Furthermore, more than
one word from stream A can be included in the context
to provide additional predictive ability. With the meet-
ing data, there may be a benefit to controlling for spe-
cific speakers based on their degree of influence. Alterna-
tively, an MSLM might help identify the most influential
speaker in a meeting by determining who most changes
the probability of other speakers? words.
Moreover, the approach clearly suggests that a multi-
speaker decoder in an automatic speech recognition
(ASR) system might be beneficial. Once time marks for
each word are provided in an N -best list, our MSLM
technique can be used for rescoring. Additionally, such
a decoder can easily be specified using graphical mod-
els (Bilmes and Zweig, 2002) in first-pass decodings.
We wish to thank Katrin Kirchhoff and the anonymous
reviewers for useful comments on this work.
References
J. Bilmes and K. Kirchhoff. 2003. Factored language
models and generalized parallel backoff. In Human
Language Technology Conference.
J. Bilmes and G. Zweig. 2002. The graphical models
toolkit: An open source software system for speech
and time-series processing. In Proc. ICASSP, June.
P. Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mer-
cer. 1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal Report TR-10-98, Computer Science Group, Har-
vard University, August.
J. Gao, J. Goodman, G. Cao, and H. Li. 2002. Exploring
asymmetric clustering for statistical langauge model-
ing. In Proc. of ACL, pages 183?190, July.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,
and C. Wooters. 2003. The ICSI Meeting Corpus. In
Proc. ICASSP, April.
K. Kirchhoff, J. Bilmes, S. Das, N. Duta, M. Egan, G. Ji,
F. He, J. Henderson, D. Liu, M. Noamany, P. Schone,
R. Schwartz, and D. Vergyri. 2003. Novel approaches
to arabic speech recognition: Report from the 2002
Johns-Hopkins workshop. In Proc. ICASSP, April.
R. Schwartz. 2004. Personal communication.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, September.
E. Whittaker and P. Woodland. 2003. Language mod-
elling for Russian and English using words and classes.
Computer Speech and Language, pages 87?104.
H. Yamamoto, S. Isogai, and Y. Sagisaka. 2001. Multi-
class composite n-gram language model for spoken
language processing using multiple word clusters. In
Proc. of ACL, pages 531?538.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 280?287,
New York, June 2006. c?2006 Association for Computational Linguistics
Backoff Model Training using Partially Observed Data:
Application to Dialog Act Tagging
Gang Ji and Jeff Bilmes
Department of Electrical Engineering
University of Washington
Seattle, WA 98105-2500
{gang,bilmes}@ee.washington.edu
Abstract
Dialog act (DA) tags are useful for many
applications in natural language process-
ing and automatic speech recognition. In
this work, we introduce hidden backoff
models (HBMs) where a large generalized
backoff model is trained, using an embed-
ded expectation-maximization (EM) pro-
cedure, on data that is partially observed.
We use HBMs as word models condi-
tioned on both DAs and (hidden) DA-
segments. Experimental results on the
ICSI meeting recorder dialog act corpus
show that our procedure can strictly in-
crease likelihood on training data and can
effectively reduce errors on test data. In
the best case, test error can be reduced by
6.1% relative to our baseline, an improve-
ment on previously reported models that
also use prosody. We also compare with
our own prosody-based model, and show
that our HBM is competitive even without
the use of prosody. We have not yet suc-
ceeded, however, in combining the bene-
fits of both prosody and the HBM.
1 Introduction
Discourse patterns in natural conversations and
meetings are well known to provide interesting and
useful information about human conversational be-
havior. They thus attract research from many differ-
ent and beneficial perspectives. Dialog acts (DAs)
(Searle, 1969), which reflect the functions that ut-
terances serve in a discourse, are one type of such
patterns. Detecting and understanding dialog act
patterns can provide benefit to systems such as au-
tomatic speech recognition (ASR) (Stolcke et al,
1998), machine dialog translation (Lee et al, 1998),
and general natural language processing (NLP) (Ju-
rafsky et al, 1997b; He and Young, 2003). DA pat-
tern recognition is an instance of ?tagging.? Many
different techniques have been quite successful in
this endeavor, including hidden Markov models (Ju-
rafsky et al, 1997a; Stolcke et al, 1998), seman-
tic classification trees and polygrams (Mast et al,
1996), maximum entropy models (Ang et al, 2005),
and other language models (Reithinger et al, 1996;
Reithinger and Klesen, 1997). Like other tagging
tasks, DA recognition can also be achieved using
conditional random fields (Lafferty et al, 2001; Sut-
ton et al, 2004) and general discriminative model-
ing on structured outputs (Bartlett et al, 2004). In
many sequential data analysis tasks (speech, lan-
guage, or DNA sequence analysis), standard dy-
namic Bayesian networks (DBNs) (Murphy, 2002)
have shown great flexibility and are widely used. In
(Ji and Bilmes, 2005), for example, an analysis of
DA tagging using DBNs is performed, where the
models avoid label bias by structural changes and
avoid data sparseness by using a generalized back-
off procedures (Bilmes and Kirchhoff, 2003).
Most DA classification procedures assume that
within a sentence of a particular fixed DA type,
there is a fixed word distribution over the entire sen-
tence. Similar to (Ma et al, 2000) (and see cita-
tions therein), we have found, however, that intra-
280
sentence discourse patterns are inherently dynamic.
Moreover, the patterns are specific to each type of
DA, meaning a sentence will go through a DA-
specific sequence of sub-DA phases or ?states.? A
generative description of this phenomena is that a
DA is first chosen, and then words are generated
according to both the DA and to the relative posi-
tion of the word in that sentence. For example, a
?statement? (one type of DA) can consist of a sub-
ject (noun phrase), verb phrase, and object (noun
phrase). This particular sequence might be different
for a different DA (e.g., a ?back-channel?). Our be-
lief is that explicitly modeling these internal states
can help a DA-classification system in conversa-
tional meetings or dialogs.
In this work, we describe an approach that is
motivated by several aspects of the typical DA-
classification procedure. First, it is rare to have sub-
DAs labeled in training data, and indeed this is true
of the corpus (Shriberg et al, 2004) that we use.
Therefore, some form of unsupervised clustering or
pre-shallow-parsing of sub-DAs must be performed.
In such a model, these sub-DAs are essentially un-
known hidden variables that ideally could be trained
with an expectation-maximization (EM) procedure.
Second, when training models of language, it is nec-
essary to employ some form of smoothing method-
ology since otherwise data-sparseness would render
standard maximum-likelihood trained models use-
less. Third, discrete conditional probability distri-
butions formed using backoff models that have been
smoothed (particularly using modified Kneser-Ney
(Chen and Goodman, 1998)) have been extremely
successful in many language modeling tasks. Train-
ing backoff models, however, requires that all data
is observed so that data counts can be formed. In-
deed, our DA-specific word models (implemented
via backoff) will also need to condition on the cur-
rent sub-DA, which at training time is unknown.
We therefore have developed a procedure that al-
lows us to train generalized backoff models (Bilmes
and Kirchhoff, 2003), even when some or all of the
variables involved in the model are hidden. We thus
call our models hidden backoff models (HBMs). Our
method is indeed a form of embedded EM training
(Morgan and Bourlard, 1990), and more generally
is a specific form of EM (Neal and Hinton, 1998).
Our approach is similar to (Ma et al, 2000), except
our underlying language models are backoff-based
and thus retain the benefits of advanced smoothing
methods, and we utilize both a normal and a backoff
EM step as will be seen. We moreover wrap up the
above ideas in the framework of dynamic Bayesian
networks, which are used to represent and train all
of our models.
We evaluate our methods on the ICSI meeting
recorder dialog act (MRDA) (Shriberg et al, 2004)
corpus, and find that our novel hidden backoff model
can significantly improve dialog tagging accuracy.
With a different number of hidden states for each
DA, a relative reduction in tagging error rate as
much as 6.1% can be achieved. Our best HBM result
shows an accuracy that improves on the best known
(to our knowledge) result on this corpora which is
one that uses acoustic prosody as a feature. We have
moreover developed our own prosody model and
while we have not been able to usefully employ both
prosody and the HBM technique together, our HBM
is competitive in this case as well. Furthermore, our
results show the effectiveness of our embedded EM
procedure, as we demonstrate that it increases train-
ing log likelihoods, while simultaneously reducing
error rate.
Section 2 briefly summarizes our baseline DBN-
based models for DA tagging tasks. In Section 3,
we introduce our HBMs. Section 4 contains experi-
mental evaluations on the MRDA corpus and finally
Section 5 concludes.
2 DBN-based Models for Tagging
Dynamic Bayesian networks (DBNs) (Murphy,
2002) are widely used in sequential data analysis
such as automatic speech recognition (ASR) and
DNA sequencing analysis (Durbin et al, 1999). A
hidden Markov model (HMM) for DA tagging as in
(Stolcke et al, 1998) is one such instance.
Figure 1 shows a generative DBN model that will
be taken as our baseline. This DBN shows a pro-
logue (the first time slice of the model), an epilogue
(the last slice), and a chunk that is repeated suffi-
ciently to fit the entire data stream. In this case,
the data stream consists of the words of a meet-
ing conversation, where individuals within the meet-
ing (hopefully) take turns speaking. In our model,
the entire meeting conversation, and all turns of all
281
sentence change
DA <s>
dialog act
word <s>
word
prologue chunk epilogue
Figure 1: Baseline generative DBN for DA tagging.
speakers, are strung together into a single stream
rather than treating each turn in the meeting indi-
vidually. This approach has the benefit that we are
able to integrate a temporal DA-to-DA model (such
as a DA bigram).
In all our models, to simplify we assume that the
sentence change information is known (as is com-
mon with this corpus (Shriberg et al, 2004)). We
next describe Figure 1 in detail. Normally, the sen-
tence change variable is not set, so that we are within
a sentence (or a particular DA). When a sentence
change does not occur, the DA stays the same from
slice to slice. During this time, we use a DA-specific
language model (implemented via a backoff strat-
egy) to score the words within the current DA.
When a sentence change event does occur, a new
DA is predicted based on the DA from the previous
sentence (using a DA bigram). At the beginning of
a sentence, rather than conditioning on the last word
of the previous sentence, we condition on the special
start of sentence <s> token, as shown in the figure
by having a special parent that is used only when
sentence change is true. Lastly, at the very beginning
of a meeting, a special start of DA token is used.
The joint probability under this baseline model is
written as follows:
P (W, D) =
?
k
P (dk|dk?1)
?
?
i
P (wk,i|wk,i?1, dk),
(1)
where W = {wk,i} is the word sequence, D = {dk}
is the DA sequence, dk is the DA of the k-th sen-
tence, and wk,i is the i-th word of the k-th sentence
in the meeting.
Because all variables are observed when training
our baseline, we use the SRILM toolkit (Stolcke,
2002), modified Kneser-Ney smoothing (Chen and
Goodman, 1998), and factored extensions (Bilmes
and Kirchhoff, 2003). In evaluations, the Viterbi al-
gorithm (Viterbi, 1967) can be used to find the best
DA sequence path from the words of the meeting
according to the joint distribution in Equation (1).
3 Hidden Backoff Models
When analyzing discourse patterns, it can be seen
that sentences with different DAs usually have dif-
ferent internal structures. Accordingly, in this work
we do not assume sentences for each dialog act have
the same hidden state patterns. For instance (and as
mentioned above), a statement can consist of a noun
followed by a verb phase.
A problem, however, is that sub-DAs are not an-
notated in our training corpus. While clustering and
annotation of these phrases is already a widely de-
veloped research topic (Pieraccini and Levin, 1991;
Lee et al, 1997; Gildea and Jurafsky, 2002), in our
approach we use an EM algorithm to learn these hid-
den sub-DAs in a data-driven fashion. Pictorially,
we add a layer of hidden states to our baseline DBN
as illustrated in Figure 2.
sentence change
DA <s>
dialog act
word <s>
word
prologue chunk epilogue
hidden state
Figure 2: Hidden backoff model for DA tagging.
Under this model, the joint probability is:
P (W, S, D) =
?
k
P (dk|dk?1)
?
?
i
[P (sk,i|sk,i?1, dk)
? P (wk,i|wk,i?1, sk,i, dk)] ,
(2)
282
where S = {sk,i} is the hidden state sequence, sk,i
is the hidden state at the i-th position of the k-th sen-
tence, and other variables are the same as before.
Similar to our baseline model, the DA bigram
P (dk|dk?1) can be modeled using a backoff bi-
gram. Moreover, if the states {sk,i} are known
during training, the word prediction probability
P (wk,i|wk,i?1, sk,i, dk) can also use backoff and be
trained accordingly. The hidden state sequence is
unknown, however, and thus cannot be used to pro-
duce a standard backoff model. What we desire is
an ability to utilize a backoff model (to mitigate data
sparseness effects) while simultaneously retaining
the state as a hidden (rather than an observed) vari-
able, and also have a procedure that trains the entire
model to improve overall model likelihood.
Expectation-maximization (EM) algorithms are
well-known to be able to train models with hidden
states. Furthermore, standard advanced smoothing
methods such as modified Kneser-Ney smoothing
(Chen and Goodman, 1998) utilize integer counts
(rather than fractional ones), and they moreover
need ?meta? counts (or counts of counts). There-
fore, in order to train this model, we propose an
embedded training algorithm that cycles between a
standard EM training procedure (to train the hidden
state distribution), and a stage where the most likely
hidden states (and their counts and meta counts) are
used externally to train a backoff model. This pro-
cedure can be described in detail as follows:
Input : W ? meeting word sequence
Input : D ? DA sequence
Output : P (sk,i|sk,i?1) - state transition CPT
Output : P (wk,i|wk,i?1, sk,i, dk) - word model
randomly generate a sequence S;1
backoff train P (wk,i|wk,i?1, sk,i, dk);2
while not ?converged? do3
EM train P (sk,i|sk,i?1);4
calculate best S? sequence by Viterbi;5
backoff train P (wk,i|wk,i?1, s?k,i, dk);6
end7
Algorithm 1: Embedded training for HBMs
In the algorithm, the input contains words and
a DA for each sentence in the meeting. The out-
put is the corresponding conditional probability ta-
ble (CPT) for hidden state transitions, and a back-
off model for word prediction. Because we train the
backoff model when some of the variables are hid-
den, we call the result a hidden backoff model. While
we have seen embedded Viterbi training used in the
past for simultaneously training heterogeneous mod-
els (e.g., Markov chains and Neural Networks (Mor-
gan and Bourlard, 1990)), this is the first instance
of training backoff-models that involve hidden vari-
ables that we are aware of.
While embedded Viterbi estimation is not guar-
anteed to have the same convergence (or fixed-point
under convergence) as normal EM (Lember and
Koloydenko, 2004), we find empirically this to be
the case (see examples below). Moreover, our algo-
rithm can easily be modified so that instead of tak-
ing a Viterbi alignment in step 5, we instead use a
set of random samples generated under the current
model. In this case, it can be shown using a law-of-
large numbers argument that having sufficient sam-
ples guarantees the algorithm will converge (we will
investigate this modification in future work).
Of course, when decoding with such a model, a
conventional Viterbi algorithm can still be used to
calculate the best DA sequence.
4 Experimental Results
We evaluated our hidden backoff model on the
ICSI meeting recorder dialog act (MRDA) corpus
(Shriberg et al, 2004). MRDA is a rich data set that
contains 75 natural meetings on different topics with
each meeting involving about 6 participants. DA an-
notations from ICSI were based on a previous ap-
proach in (Jurafsky et al, 1997b) with some adapta-
tion for meetings in a number of ways described in
(Bhagat et al, 2003). Each DA contains a main tag,
several optional special tags and an optional ?disrup-
tion? form. The total number of distinct DAs in the
corpus is as large as 1260. In order to make the prob-
lem comparable to other work (Ang et al, 2005), a
DA tag sub-set is used in our experiments that con-
tains back channels (b), place holders (h), questions
(q), statements (s), and disruptions (x). In our eval-
uations, among the entire 75 conversations, 51 are
used as the training set, 11 are used as the develop-
ment set, 11 are used as test set, and the remaining
3 are not used. For each experiment, we used a ge-
netic algorithm to search for the best factored lan-
guage model structure on the development set and
283
we report the best results.
Our baseline system is the generative model
shown in Figure 1 and uses a backoff implementa-
tion of the word model, and is optimized on the de-
velopment set. We use the SRILM toolkit with ex-
tensions (Bilmes and Kirchhoff, 2003) to train, and
use GMTK (Bilmes and Zweig, 2002) for decoding.
Our baseline system has an error rate of 19.7% on
the test set, which is comparable to other approaches
on the same task (Ang et al, 2005).
4.1 Same number of states for all DAs
To compare against our baseline, we use HBMs in
the model shown in Figure 2. To train, we followed
Algorithm 1 as described before and as is here de-
tailed in Figure 3.
  Initialization:
- randomize states
- train word FLM
             Convergence:
- llh change < 0.2%
- or 10 iterations
- find best state path
- train word FLM
Text Text
3-epoch
EM training
No
- find best state path
- train word FLM
Text Text
5-epoch
EM training
Yes
Figure 3: Embedded training: llh = log likelihood
In this implementation, an upper triangular ma-
trix (with self-transitions along the diagonal) is used
for the hidden state transition probability table so
that sub-DA states only propagate in one direction.
When initializing the hidden state sequence of a DA,
we expanded the states uniformly along the sen-
tence. This initial alignment is then used for HBM
training. In the word models used in our experi-
ments, the backoff path first drops previous words,
then does a parallel backoff to hidden state and DA
using a mean combination strategy.
The HBM thus obtained was then fed into the
main loop of our embedded EM algorithm. The
training was considered to have ?converged? if ei-
ther it exceeded 10 iterations (which never hap-
pened) or the relative log likelihood change was less
than 0.2%. Within each embedded iteration, three
EM epochs were used. After each EM iteration,
a Viterbi alignment was performed thus obtaining
what we expect to be a better hidden state alignment.
This updated alignment, was then used to train a
new HBM. The newly generated model was then fed
back into the embedded training loop until it con-
verged. After the procedure met our convergence
criteria, an additional five EM epochs were carried
out in order to provide a good hidden state transi-
tion probability table. Finally, after Viterbi align-
ment and text generation was performed, the word
HBM was trained from the best state sequence.
To evaluate our hidden backoff model, the Viterbi
algorithm was used to find the best DA sequence ac-
cording to test data, and the tagging error rates were
calculated. In our first experiment, an equal num-
ber of hidden states for all DAs were used in each
model. The effect of this number on the accuracy of
DA tagging is shown in Table 1.
Table 1: HBMs, different numbers of hidden states.
# states error improvement
baseline 19.7% ?
2-state 18.7% 5.1%
3-state 19.5% 1.0%
For the baseline system, the backoff path first
drops dialog act, and for the HBMs, all backoff
paths drop hidden state first and drop DA sec-
ond. From Table 1 we see that with two hidden states
for every DA the system can reduce the tagging error
rate by more than 5% relative. As a comparison, in
(Ang et al, 2005), where conditional maximum en-
tropy models (which are conditionally trained) are
used, the error rate is 18.8% when using both word
and acoustic prosody features, and and 20.5% with-
out prosody. When the number of hidden states in-
creases to 3, the improvement decreases even though
it is still (very slightly) better than the baseline. We
believe the reasons are as follows: First, assuming
different DAs have the same number of hidden states
may not be appropriate. For example, back chan-
nels usually have shorter sentences and are constant
in discourse pattern over a DA. On the other hand,
284
questions and statements typically have longer, and
more complex, discourse structures. Second, even
under the same DA, the structure and inherent length
of sentence can vary. For example, ?yes? can also be
a statement even though it has only one word. There-
fore, one-word statements need completely differ-
ent hidden state patterns than those in subject-verb-
object like statements ? having one monolithic 3-
state model for statements might be inappropriate.
This issue is discussed further in Section 4.4.
4.2 Different states for different DAs
In order to mitigate the first problem described
above, we allow different numbers of hidden states
for each DA. This, however, leads to a combinato-
rial explosion of possibilities if done in a na??ve fash-
ion. Therefore, we attempted only a small number
of combinations based on the statistics of numbers
of words in each DA given in Table 2.
Table 2: Length statistics of different DAs.
DA mean median std p
(b) 1.0423 1 0.2361 0.4954
(h) 1.3145 1 0.7759 0.4660
(q) 6.5032 5 6.3323 0.3377
(s) 8.6011 7 7.8380 0.3013
(x) 1.7201 1 1.1308 0.4257
Table 2 shows the mean and median number of
words per sentence for each DA as well as the stan-
dard deviation. Also, the last column provides the p
value according to fitting the length histogram to a
geometric distribution (1 ? p)np. As we expected,
back channels (b) and place holders (h) tend to have
shorter sentences while questions (q) and statements
(s) have longer ones. From this analysis, we use
fewer states for (b) and (h) and more states for (q)
and (s). For disruptions (x), the standard deviation of
number of words histogram is relatively high com-
pared with (b) and (h), so we also used more hidden
states in this case. In our experimental results below,
we used one state for (b) and (h), and various num-
bers of hidden states for other DAs. Tagging error
rates are shown in Table 3.
From Table 3, we see that using different num-
bers of hidden states for different DAs can produce
better models. Among all the experiments we per-
Table 3: Number of hidden states for different DAs.
b h q s x error improvement
1 1 4 4 1 18.9% 4.1%
1 1 3 3 2 18.9% 4.1%
1 1 2 2 2 18.7% 5.1%
1 1 3 2 2 18.6% 5.6%
1 1 3 2 2 18.5% 6.1%
formed, the best case is given by three states for (q),
two states for (s) and (x), and one state for (b) and
(h). This combination gives 6.1% relative reduction
of error rate from the baseline.
4.3 Effect of embedded EM training
Incorporating backoff smoothing procedures into
Bayesian networks (and hidden variable training in
particular) can show benefits for any data domain
where smoothing is necessary. To understand the
properties of our algorithm a bit better, after each
training iteration using a partially trained model, we
calculated both the log likelihood of the training set
and the tagging error rate of the test data. Figure 4
shows these results using the best configuration from
the previous section (three states for (q), two for
(s)/(x) and one for (b)/(h)). This example is typical
of the convergence we see of Algorithm 1, which
empirically suggests that our procedure may be sim-
ilar to a generalized EM (Neal and Hinton, 1998).
1 2 3 4 5 6 7
18
19
20
21
22
23
24
iterations
er
ro
r r
at
e 
(%)
?1.2
?1.15
?1.1
?1.05
x 106
lo
g 
lik
el
ih
oo
d
llh
baseline
error rate
Figure 4: Embedded EM training performance.
We find that the log likelihood after each EM
training is strictly increasing, suggesting that our
embedded EM algorithm for hidden backoff models
285
is improving the overall joint likelihood of the train-
ing data according to the model. This strict increase
of likelihood combined with the fact that Viterbi
training does not have the same theoretical conver-
gence guarantees as does normal EM indicates that
more detailed theoretical analysis of this algorithm
used with these particular models is desirable.
From the figure we also see that both the log
likelihood and tagging error rate ?converge? af-
ter around four iterations of embedded training.
This quick convergence indicates that our embedded
training procedure is effective. The leveling of the
error rates after several iterations shows that model
over-fitting appears not to be an issue presumably
due to the smoothed embedded backoff models.
4.4 Discussion and Error Analysis
A large portion of our tagging errors are due to con-
fusing the DA of short sentences such as ?yeah?, and
?right?. The sentence, ?yeah? can either be a back
channel or an affirmative statement. There are also
cases where ?yeah?? is a question. These types of
confusions are difficult to remove in the prosody-
less framework but there are several possibilities.
First, we can allow the use of a ?fork and join? tran-
sition matrix, where we fork to each DA-specific
condition (e.g., short or long) and join thereafter.
Alternatively, hidden Markov chain structuring al-
gorithms or context (i.e., conditioning the number
of sub-DAs on the previous DA) might be helpful.
Finding a proper number of hidden states for each
DA is also challenging. In our preliminary work, we
simply explored different combinations using sim-
ple statistics of the data. A systematic procedure
would be more beneficial. In this work, we also
did not perform any hidden state tying within dif-
ferent DAs. In practice, some states in statements
should be able to be beneficially tied with other
states within questions. Our results show that having
three states for all DAs is not as good as two states
for all. But with tying, more states might be more
successfully used.
4.5 Influence of Prosody Cues
It has been shown that prosody cues provide use-
ful information in DA tagging tasks (Shriberg et
al., 1998; Ang et al, 2005). We also incorporated
prosody features in our models. We used ESPS
get f0 based on RAPT algorithm (Talkin, 1995) to
get F0 values. For each speaker, mean and variance
normalization is performed. For each word, a linear
regression is carried on the normalized F0 values.
We quantize the slope values into 20 bins and treat
those as prosody features associated with each word.
After adding the prosody features, the simple gener-
ative model as shown in Figure 5 gives 18.4% error
rate, which is 6.6% improvement over our baseline.
There is no statistical difference between the best
performance of this prosody model and the earlier
best HBM. This implies that the HBM can obtain
as good performance as a prosody-based model but
without using prosody.
sentence change
DA <s>
dialog act
word <s>
word
prologue chunk epilogue
prosody
Figure 5: Generative prosody model for DA tagging.
The next obvious step is to combine an HBM with
the prosody information. Strangely, even after ex-
perimenting with many different models (including
ones where prosody depends on DA; prosody de-
pends on DA and the hidden state; prosody depends
on DA, hidden state, and word; and many varia-
tions thereof), we were unsuccessful in obtaining
a complementary benefit when using both prosody
and an HBM. One hypothesis is that our prosody
features are at the word-level (rather than at the DA
level). Another problem might be the small size of
the MRDA corpus relative to the model complexity.
Yet a third hypothesis is that the errors corrected by
both methods are the same ? indeed, we have ver-
ified that the corrected errors overlap by more than
50%. We plan further investigations in future work.
286
5 Conclusions
In this work, we introduced a training method for
hidden backoff models (HBMs) to solve a problem
in DA tagging where smoothed backoff models in-
volving training-time hidden variables are useful.
We tested this procedure in the context of dynamic
Bayesian networks. Different hidden states were
used to model different positions in a DA. According
to empirical evaluations, our embedded EM algo-
rithm effectively increases log likelihood on training
data and reduces DA tagging error rate on test data.
If different numbers of hidden states are used for dif-
ferent DAs, we find that our prosody-independent
HBM reduces the tagging error rate by 6.1% rela-
tive to the baseline, a result that improves upon pre-
viously reported work that uses prosody, and that is
comparable to our own new result that also incorpo-
rates prosody. We have not yet been able to combine
the benefits of both an HBM and prosody informa-
tion. This material is based upon work supported
by the National Science Foundation under Grant No.
IIS-0121396.
References
J. Ang et al 2005. Automatic dialog act segmentation and
classification in multiparty meetings. In ICASSP.
P. Bartlett et al 2004. Exponentiated gradient algorithms for
large-margin structured classification. In NIPS.
S. Bhagat et al 2003. Labeling guide for dialog act tags in the
meeting recordering meetings. Technical Report 2, Interna-
tional Computer Science Insititute.
J. Bilmes and K. Kirchhoff. 2003. Factored language mod-
els and generalized parallel backoff. In Human Lang. Tech.,
North American Chapter of Asssoc. Comp. Ling., Edmonton,
Alberta, May/June.
J. Bilmes and G. Zweig. 2002. The Graphical Models Toolkit:
An open source software system for speech and time-series
processing. Proc. IEEE Intl. Conf. on Acoustics, Speech, and
Signal Processing.
S. Chen and J. Goodman. 1998. An empirical study of smooth-
ing techniques for language modeling. Technical report,
Computer Science Group, Harvard University.
R. Durbin et al 1999. Biological Sequence Analysis: Prob-
abilistic Models of Proteins and Nucleic Acids. Cambridge
University Press.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245?288.
Y. He and S. Young. 2003. A data-driven spoken language un-
derstanding system. In Proc. IEEE Workshop on Automatic
Speech Recognition and Understanding, pages 583?588.
G. Ji and J. Bilmes. 2005. Dialog act tagging using graphical
models. In Proc. IEEE Intl. Conf. on Acoustics, Speech, and
Signal Processing, Philadelphia, PA, March.
D. Jurafsky et al 1997a. Automatic detection of discourse
structure for speech recognition and understanding. In Proc.
IEEE Workshop on Speech Recognition and Understanding.
D. Jurafsky et al 1997b. Switchboard SWBD-
DAMSL shallow-discourse-function annotation coders man-
ual. Technical Report 97-02, Institute of Cognitive Science,
University of Colorado.
J. Lafferty et al 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In ICML.
K. Lee et al 1997. Restricted representation of phrase struc-
ture grammar for building a tree annotated corpus of Korean.
Natural Language Engineering, 3(2-3):215?230.
H. Lee et al 1998. Speech act analysis model of Korean utter-
ances for automatic dialog translation. J. KISS(B) (Software
and Applications), 25(10):1443?1452.
J. Lember and A. Koloydenko. 2004. Adjusted viterbi training.
a proof of concept. In Submission.
K. Ma et al 2000. Bi-modal sentence structure for language
modeling. Speech Communication, 31(1):51?67.
M. Mast et al 1996. Automatic classification of dialog acts
with semantic classification trees and polygrams. Connec-
tionist, Statistical and Symbolic Approaches to Learning for
Natural Language Processing, pages 217?229.
N. Morgan and H. Bourlard. 1990. Continuous speech recogni-
tion using multilayer perceptrons with hidden Markov mod-
els. In ICASSP, pages 413?416.
K. Murphy. 2002. Dynamic Bayesian Networks, Represen-
tation, Inference, and Learning. Ph.D. thesis, MIT, Dept.
Computer Science.
R. Neal and G. Hinton. 1998. A view of the EM algorithm that
justifies incremental, sparse, and other variants. In Learning
in Graphical Models, pages 355?368. Dordrecht: Kluwer
Academic Publishers.
R. Pieraccini and E. Levin. 1991. Stochastic representation of
semantic structure for speech understanding. In Eurospeech,
volume 2, pages 383?386.
N. Reithinger and M. Klesen. 1997. Dialogue act classification
using language models. In Eurospeech.
N. Reithinger et al 1996. Predicting dialogue acts for a speech-
to-speech translation system. In ICLSP, pages 654?657.
J. Searle. 1969. Speech Acts: An Essay in the Philosophy of
Language. Cambridge University Press.
E. Shriberg et al 1998. Can prosody aid the automatic classi-
fication of dialog acts in conversational speech? Language
and Speech, 41(3?4):439?487.
E. Shriberg et al 2004. The ICSI meeting recorder dialog act
(MRDA) corpus. In Proc. of the 5th SIGdial Workshop on
Discourse and Dialogue, pages 97?100.
A. Stolcke et al 1998. Dialog act modeling for conversa-
tional speech. In Proc. AAAI Spring Symp. on Appl. Machine
Learning to Discourse Processing, pages 98?105.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In ICLSP, volume 2, pages 901?904.
C. Sutton et al 2004. Dynamic conditional random fields: fac-
torized probabilistic models for labeling and segmenting se-
quence data. In ICML.
D. Talkin. 1995. A robust algorithm for pitch tracking (rapt).
In W. B. Kleijn and K.K. Paliwal, editors, Speech Coding
and Synthesis, pages 495?518. Elsevier Science.
A. Viterbi. 1967. Error bounds for convolutional codes and an
asymptotically optimum decoding algorithm. IEEE Trans.
on Information Theory, 13(2):260?269.
287
Proceedings of NAACL HLT 2007, Companion Volume, pages 33?36,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Generalized Graphical Abstractions for Statistical Machine Translation
Karim Filali and Jeff Bilmes?
Departments of Computer Science & Engineering and Electrical Engineering
University of Washington
Seattle, WA 98195, USA
{karim@cs,bilmes@ee}.washington.edu
Abstract
We introduce a novel framework for the
expression, rapid-prototyping, and eval-
uation of statistical machine-translation
(MT) systems using graphical mod-
els. The framework extends dynamic
Bayesian networks with multiple con-
nected different-length streams, switching
variable existence and dependence mech-
anisms, and constraint factors. We have
implemented a new general-purpose MT
training/decoding system in this frame-
work, and have tested this on a variety of
existing MT models (including the 4 IBM
models), and some novel ones as well,
all using Europarl as a test corpus. We
describe the semantics of our representa-
tion, and present preliminary evaluations,
showing that it is possible to prototype
novel MT ideas in a short amount of time.
1 Introduction
We present a unified graphical model framework
based on (Filali and Bilmes, 2006) for statistical ma-
chine translation. Graphical models utilize graphical
descriptions of probabilistic processes, and are capa-
ble of quickly describing a wide variety of different
sets of model assumptions. In our approach, either
phrases or words can be used as the unit of transla-
tion, but as a first step, we have only implemented
word-based models since our main goal is to show
?This material was supported by NSF under Grant No. ISS-
0326276.
the viability of our graphical model representation
and new software system.
There are several important advantages to a uni-
fied probabilistic framework for MT including: (1)
the same codebase can be used for training and de-
coding without having to implement a separate de-
coder for each model; (2) new models can be pro-
totyped quickly; (3) combining models (such as in
a speech-MT system) is easier when they are en-
coded in the same framework; (4) sharing algo-
rithms across different disciplines (e.g., the MT and
the constraint-satisfaction community) is facilitated.
2 Graphical Model Framework
A Graphical Model (GM) represents a factorization
of a family of joint probability distributions over a
set of random variables using a graph. The graph
specifies conditional independence relationships be-
tween the variables, and parameters of the model
are associated with each variable or group thereof.
There are many types of graphical models. For ex-
ample, Bayesian networks use an acyclic directed
graph and their parameters are conditional probabili-
ties of each variable given its parents. Various forms
of GM and their conditional independence proper-
ties are defined in (Lauritzen, 1996).
Our graphical representation, which we call
Multi-dynamic Bayesian Networks (MDBNs) (Filali
and Bilmes, 2006), is a generalization of dynamic
Bayesian networks (DBNs) (Dean and Kanazawa,
1988). DBNs are an appropriate representation for
sequential (for example, temporal) stochastic pro-
cesses, but can be very difficult to apply when de-
pendencies have arbitrary time-span and the exis-
tence of random variables is contingent on the val-
33
ues of certain variables in the network. In (Filali
and Bilmes, 2006), we discuss inference and learn-
ing in MDBNs. Here we focus on representation
and the evaluation of our new implementation and
framework. Below, we summarize key features un-
derlying our framework. In section 3, we explain
how these features apply to a specific MT model.
? Multiple DBNs can be represented along with
rules for how they are interconnected ? the rule de-
scription lengths are fixed (they do not grow with the
length of the DBNs).
? Switching dependencies (Geiger and Hecker-
man, 1996): a variable X is a switching parent of
Y if X influences what type of dependencies Y has
with respect to its other parents, e.g., an alignment
variable in IBM Models 1 and 2 is switching.
? Switching existence: A variable X is ?switch-
ing existence? with respect to variable Y if the value
of X determines whether Y exists. An example is a
fertility variable in IBM Models 3 and above.
? Constraints and aggregation: BN semantics can
encode various types of constraints between groups
of variables (Pearl, 1988). For example, in the con-
struct A ? B ? C where B is observed, B can
constrain A and C to be unequal. We extend those
semantics to support a more efficient evaluation of
constraints under some variable order conditions.
3 GM Representation of IBM MT Models
In this section we present a GM representation for
IBM model 3 (Brown et al, 1993) in fig. 1. Model 3
is intricate enough to showcase some of the features
of our graphical representation but not as complex
as, and thus is easier to describe, than model 4. Our
choice of representing IBM models is not because
we believe they are state of the art MT models?
although they are still widely used in producing
alignments and as features in log-linear models?
but because they provide a good initial testbed for
our architecture.
The topmost random variable (RV), ?, is a hid-
den switching existence variable corresponding to
the length of the English string. The box abutting
? includes all the nodes whose existence depends on
the value of ?. In the figure, ? = 3, thus resulting
in three English words e1, ..., e3, connected using a
second-order Markov chain. To each English word
ei corresponds a conditionally dependent fertility ?i,
which indicates how many times ei is used by words
in the French string. Each ?i in turn grants existence
to a set of RVs under it. Given the fertilities (the fig-
ure depicts the case ?1 = 3, ?2 = 1, ?3 = 0), for
each word ei, ?i French word RVs are granted exis-
tence and are denoted by the tablet ?i1, ?i2, . . . , ?i?i
of ei. The values of ? variables need to match the
actual observed French sequence f1, . . . , fm. This is
represented as a shared constraint between all the f ,
?, and ? variables which have incoming edges into
the observed variable v. v?s conditional probability
table is such that it is one only when the associated
constraint is satisfied. The variable ?i,k is a switch-
ing dependency parent with respect to the constraint
variable v and determines which fj participates in
an equality constraint with ?i,k.
In the null word sub-model, the constraint that
successive permutation variables be ordered is im-
plemented using the observed child w of ?0i and
?0(i+1). The probability of w being unity is one only
when the constraint is satisfied and zero otherwise.
The bottom variable m is a switching existence
node (observed to be 6 in the figure) with cor-
responding French word sequence and alignment
variables. The French sequence participates in the
v constraint described above, while the alignment
variables aj ? {1, . . . , ?}, j ? 1, . . . ,m constrain
the fertilities to take their unique allowable values
(for the given alignment). Alignments also restrict
the domain of permutation variables, pi, using the
constraint variable x. Finally, the domain size of
each aj has to lie in the interval [0, ?] and that is en-
forced by the variable u. The dashed edges connect-
ing the alignment a variables represent an extension
to implement an M3/M-HMM hybrid.1
4 Experiments
We have developed (in C++) a new entirely self-
contained general-purpose MT training/decoding
system based on our framework, of which we pro-
vide a preliminary evaluation in this section. Al-
though the framework is perfectly capable of rep-
resenting phrase-based models, we restrict ourselves
to word-based models to show the viability of graph-
ical models for MT and will consider different trans-
lation units in future work. We perform MT ex-
1We refer to the HMM MT model in (Vogel et al, 1996) as
M-HMM to avoid any confusion.
34
mf1
a1
?0 ?1 ?2 ?3
e1 e2 e3
f2
a2
f3
a3
f4
a4
f5
a5
f6
a6
m?
?01 ?02 ?11 ?12 ?13 ?21
?01 ?02 ?11 ?12 ?13 ?21
u v
x
y
w
?
Figure 1: Unrolled Model 3 graphical model with fertility
assignment ?0 = 2, ?1 = 3, ?2 = 1, ?3 = 0.
periments on a English-French subset of the Eu-
roparl corpus used for the ACL 2005 SMT evalu-
ations (Koehn and Monz, 2005). We train an En-
glish language model on the whole training set us-
ing the SRILM toolkit (Stolcke, 2002) and train
MT models mainly on a 10k sentence pair sub-
set of the ACL training set. We test on the 2000
sentence test set used for the same evaluations.
For comparison, we use the MT training program,
GIZA++ (Och and Ney, 2003), the phrase-base de-
coder, Pharaoh (Koehn et al, 2003), and the word-
based decoder, Rewrite (Germann, 2003).
For inference we use a backtracking depth-first
search inference method with memoization that ex-
tends Value Elimination (Bacchus et al, 2003). The
same inference engine is used for both training and
decoding. As an admissible heuristic for decod-
ing, we compute, for each node V with Conditional
Probability Table c, the largest value of c over all
possible configurations of V and its parents (Filali
and Bilmes, 2006).
Decoder BLEU (%)
500 1000 1500 2000
Rewrite 25.3 22.3 21.7 22.01
Pharaoh 20.4 18.1 17.7 18.05
M-HMM 19.9 16.9 15.6 12.5
Table 1: BLEU scores on first 500, 1000, 1500, and
2000 sentences (ordered from shortest to longest) of
the ACL05 English-French 2000 sentence test set us-
ing a 700k sent train set. The last row is our MDBN
system?s simulation of a M-HMM model.
Table 1 compares MT performance between (1)
Pharaoh (which uses beam search), (2) our system,
and (3) Rewrite (hill-climbing). (1) and (2) make
use of a fixed lexical table2 learned using an M-
HMM model specified using our tool, and neither
uses minimum error rate training. (3) uses Model
4 parameters learned using GIZA++. This compari-
son is informative because Rewrite is a special pur-
pose model 4 decoder and we would expect it to
perform at least as well as decoders not written for
a specific IBM model. Pharaoh is more general in
that it only requires, as input, a lexical table from
any given model.3 Our MDBN system is not tai-
lored for the translation task. Pharaoh was able to
decode the 2000 sentences of the test set in 5000s
on a 3.2GHz machine; Rewrite took 84000s, and we
allotted 400000s for our engine (200s per sentence).
We attribute the difference in speed and BLEU score
between our system and Pharaoh to the fact Value
Elimination searches in a depth-first fashion over
the space of partial configurations of RVs, while
Pharaoh expands partial translation hypotheses in a
best-first search manner. Thus, Pharaoh can take ad-
vantage of knowledge about the MT problem?s hy-
pothesis space while the GM is agnostic with respect
to the structure of the problem?something that is
desirable from our perspective since generality is
a main concern of ours. Moreover, the MDBN?s
heuristic and caching of previously explored sub-
trees have not yet proven able to defray the cost,
associated with depth-first search, of exploring sub-
trees that do not contain any ?good? configurations.
Table 2 shows BLEU scores of different MT mod-
els trained using our system. We decode using
Pharaoh because the above speed difference in its
favor allowed us to run more experiments and fo-
cus on the training aspect of different models. M1,
M2, M-HMM, M3, and M4 are the standard IBM
models. M2d and M-Hd are variants in which
the distortion between the French and English po-
sitions is used instead of the absolute alignment po-
sition. M-Hdd is a second-order M-HMM model
(with distortion). M3H (see fig 1) is a variant of
model 3 that uses first-order dependencies between
alignment variables. M-Hr is another HMM model
that uses the relative distortion between the current
alignment and the previous one. This is similar
to the model implemented by GIZA except we did
2Pharaoh?s phrases are single words only.
3It does, however, use simple hard-coded distortion and fer-
tility models.
35
BLEU(%)
Giza train MDBN train
10k 700k 10k 700k
M1 15.67 18.04 14.53 17.74
M2 15.84 18.52 15.74
M2d NA NA 15.75
M-HMM NA NA 15.87
M-Hd NA NA 15.99 18.05
M-Hdd NA NA 15.55
M-Hr 16.98 19.57 16.04
M3 16.78 19.38 15.32
M3H NA NA 15.67
M4 16.81 19.51 15.00
M4H NA NA 15.20
Table 2: BLEU scores for various models trained
using GM and GIZA (when applicable). All models
are decoded using Pharaoh.
not include the English word class dependency. Fi-
nally, model M4H is a simplified model 4, in which
only distortions within each tablet are modeled but a
Markov dependency is also used between the align-
ment variables.
Table 2 also shows BLEU scores obtained by
training equivalent IBM models using GIZA and
the standard training regimen of initializing higher
models with lower ones (we use the same sched-
ules for our GM training, but only transfer lexical ta-
bles). The main observation is that GIZA-trained M-
HMM, M3 and 4 have about 1% better BLEU scores
than their corresponding MDBN versions. We at-
tribute the difference in M3/4 scores to the fact we
use a Viterbi-like training procedure (i.e., we con-
sider a single configuration of the hidden variables
in EM training) while GIZA uses pegging (Brown et
al., 1993) to sum over a set of likely hidden variable
configurations in EM.
While these preliminary results do not show im-
proved MT performance, nor would we expect them
to since they are on simulated IBM models, we find
very promising the fact that this general-purpose
graphical model-based system produces competitive
MT results on a computationally challenging task.
5 Conclusion and Future Work
We have described a new probabilistic framework
for doing statistical machine translation. We have
focused so far on word-based translation. In fu-
ture work, we intend to implement phrase-based MT
models. We also plan to design better approximate
inference strategies for training highly connected
graphs such as IBM models 3 and 4, and some novel
extensions. We are also working on new best-first
search generalizations of our depth-first search in-
ference to improve decoding time. As there has been
increased interest in end-to-end task such as speech
translation, dialog systems, and multilingual search,
a new challenge is how best to combine the complex
components of these systems into one framework.
We believe that, in addition to the finite-state trans-
ducer approach, a graphical model framework such
as ours would be well suited for this scientific and
engineering endeavor.
References
F. Bacchus, S. Dalmao, and T. Pitassi. 2003. Value elimina-
tion: Bayesian inference via backtracking search. In UAI-03,
pages 20?28, San Francisco, CA. Morgan Kaufmann.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263?311.
T. Dean and K. Kanazawa. 1988. Probabilistic temporal rea-
soning. AAAI, pages 524?528.
Karim Filali and Jeff Bilmes. 2006. Multi-dynamic Bayesian
networks. In Advances in Neural Information Processing
Systems 19. MIT Press, Cambridge, MA.
Dan Geiger and David Heckerman. 1996. Knowledge repre-
sentation and inference in similarity networks and bayesian
multinets. Artif. Intell., 82(1-2):45?74.
Ulrich Germann. 2003. Greedy decoding for statistical Ma-
chine Translation in almost linear time. In HLT-NAACL,
pages 72?79, Edmonton, Canada, May. ACL.
P. Koehn and C. Monz. 2005. Shared task: Statistical machine
translation between European languages. pages 119?124,
Ann Arbor, MI, June. ACL.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In HLT-NAACL, May.
S.L. Lauritzen. 1996. Graphical Models. Oxford Science Pub-
lications.
F. J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19?51.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent Systems:
Networks of Plausible Inference. Morgan Kaufmann.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Proc. Int. Conf. on Spoken Language Processing.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word
alignment in statistical translation. In COLING, pages 836?
841.
36
Proceedings of NAACL HLT 2007, Companion Volume, pages 165?168,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Virtual Evidence for Training Speech Recognizers using Partially Labeled
data
Amarnag Subramanya
Department of Electrical Engineering
University of Washington
Seattle, WA 98195-2500
asubram@u.washington.edu
Jeff Bilmes
Department of Electrical Engineering
University of Washington
Seattle, WA 98195-2500
bilmes@ee.washington.edu
Abstract
Collecting supervised training data for au-
tomatic speech recognition (ASR) sys-
tems is both time consuming and expen-
sive. In this paper we use the notion of vir-
tual evidence in a graphical-model based
system to reduce the amount of supervi-
sory training data required for sequence
learning tasks. We apply this approach to
a TIMIT phone recognition system, and
show that our VE-based training scheme
can, relative to a baseline trained with
the full segmentation, yield similar results
with only 15.3% of the frames labeled
(keeping the number of utterances fixed).
1 Introduction
Current state-of-the-art speech recognizers use thou-
sands of hours of training data, collected from a
large number of speakers with various backgrounds
in order to make the models more robust. It is well
known that one of the simplest ways of improv-
ing the accuracy of a recognizer is to increase the
amount of training data. Moreover, speech recog-
nition systems can benefit from being trained on
hand-transcribed data where all the appropriate word
level segmentations (i.e., the exact time of the word
boundaries) are known. However, with increasing
amounts of raw speech data being made available, it
is both time consuming and expensive to accurately
segment every word for every given sentence. More-
over, for languages for which only a small amount
of training data is available, it can be expensive and
challenging to annotate with precise word transcrip-
tions ? the researcher may have no choice but to use
partially erroneous training data.
There are a number of different ways to label
data used to train a speech recognizer. First, the
most expensive case (from an annotation perspec-
tive) is fully supervised training, where both word
sequences and time segmentations are completely
specified1 . A second case is most commonly used
in speech recognition systems, where only the word
sequences of utterances are given, but their precise
segmentations are unknown. A third case falls un-
der the realm of semi-supervised approaches. As
one possible example, a previously trained recog-
nizer is used to generate transcripts for unlabeled
data, which are then used to re-train the recog-
nizer based on some measure of recognizer confi-
dence (Lamel et al, 2002).
The above cases do not exhaust the set of possible
training scenarios. In this paper, we show how the
notion of virtual evidence (VE) (Pearl, 1988) may
be used to obtain the benefits of data with time seg-
mentations but using only partially labeled data. Our
method lies somewhere between the first and sec-
ond cases above. This general framework has been
successfully applied in the past to the activity recog-
nition domain (Subramanya et al, 2006). Here we
make use of the TIMIT phone recognition task as an
example to show how VE may be used to deal with
partially labeled speech training data. To the best of
our knowledge, this paper presents the first system to
express training uncertainty using VE in the speech
domain.
2 Baseline System
Figure 1 shows two consecutive time slices of a dy-
namic Bayesian network (DBN) designed for con-
1This does not imply that all variables are observed during
training. While the inter-word segmentations are known, the
model is not given information about intra-word segmentations.
165
Transition
Transition
Phone
Phone
Position
State
Phone
State
C   =1 C =1
S
H
A
R
P
OObservation
t?1
t?1
P
H
S t?1
Rt?1
At?1
Ot?1 t
t
t
t
t
t
tt?1
VE observed child
VE applied 
via this CPT
VE applied 
via this CPT
Figure 1: Training Graph.
text independent (CI) phone recognition. All ob-
served variables are shaded, deterministic depen-
dences are depicted using solid black lines, value
specific dependences are shown using a dot-dash
lines, and random dependencies are represented us-
ing dashed lines. In this paper, given any random
variable (rv) X , x denotes a particular value of that
rv, DX is the domain of X (x ? DX ), and |DX |
represents its cardinality.
In the above model, Pt is the rv representing
the phone variable, Ht models the current po-
sition within a phone, St is the state, Ot the
acoustic observations, At and Rt indicate state
and phone transitions respectively. Here, DXt =
DXt?1 , ?t,?X . In our implementation here,
DHt , DAt ? {0, 1, 2}, DRt ? {0, 1}. Also
?{c1, . . . , cn} is an indicator function that turns on
when all the conditions {c1, . . . , cn} are true (i.e.
a conjunction over all the conditions). The distri-
bution for Ht is given by p(ht|ht?1, rt?1, at?1) =
?{ht=0,rt?1=1} + ?{ht=at?1+ht?1,rt?1=0}, which im-plies that we always start a phone with Ht = 0.
We allow skips in each phone model, and At=0,
indicates no transition, At=1 implies you transi-
tion to the next state, At=2 causes a state to skip
(Ht+1 = Ht + 2). As the TIMIT corpus pro-
vides phone level segmentations, Pt is observed dur-
ing training. However, for reasons that will be-
come clear in the next section, we treat Pt as hid-
den but make it the parent of a rv Ct, with, p(ct =
1|pt) = ?lt=pt where lt is obtained from the tran-scriptions (lt ? DPt). The above formulation hasexactly the same effect as making Pt observed and
setting it equal to lt (Bilmes, 2004). Additional de-
tails on other CPTs in this model may be found in
(Bilmes and Bartels, 2005). We provide more de-
tails on the baseline system in section 4.1.
Our main reason for choosing the TIMIT phone
recognition task is that TIMIT includes both se-
quence and segment transcriptions (something rare
t t t1 4 7
p2p1
Labeled Unlabeled
t5t2 t6tt 8t30
Figure 2: Illustration showing our rendition of Vir-
tual Evidence.
for LVCSR corpora such as Switchboard and
Fisher). This means that we can compare against
a model that has been trained fully supervised. It is
also well known that context-dependent (CD) mod-
els outperform CI models for the TIMIT phone
recognition task (Glass et al, 1996). We used
CI models primarily for the rapid experimental
turnaround time and since it still provides a rea-
sonable test-bed for evaluating new ideas. We
do note, however, that our baseline CI system is
competitive with recently published CD systems
(Wang and Fosler-Lussier, 2006), albeit which uses
many fewer components per mixture (see Sec-
tion 4.1).
3 Soft-supervised Learning With VE
Given a joint distribution over n variables
p(x1, . . . , xn), ?evidence? simply means that
one of the variables (w.l.o.g. x1) is known. We
denote this by x?1, so the probability distribution
becomes p(x?1, . . . , xn) (no longer a function of x1).
Any configuration of the variables where x1 6= x?1
is never considered. We can mimic this behavior
by introducing a new virtual child variable c into
the joint distribution that is always observed to be
one (so c = 1), and have c interact only with x1
via the CPT p(c = 1|x1) = ?x1=x?1 . Therefore,
?
x1 p(c = 1, x1, . . . , xn) = p(x?1, . . . , xn). Nowconsider setting p(c = 1|x1) = f(x1), where
f() is an arbitrary non-negative function. With
this, different treatment can be given to different
assignments to x1, but unlike hard evidence, we
are not insisting on only one particular value. This
represents the general notion of VE. In a certain
sense, the notion of VE is similar to the prior
distribution in Bayesian inference, but it is different
in that VE expresses preferences over combinations
of values of random variables whereas a Bayesian
prior expresses preferences over combinations of
model parameter values. For a more information on
VE, see (Bilmes, 2004; Pearl, 1988).
VE can in fact be used when accurate phone level
segmentations are not available. Consider the illus-
tration in Figure 2. As shown, t1 and t4 are the
166
start and end times respectively for phone p1, while
t4 and t7 are the start and end times for phone p2.
When the start and end times for each phone are
given, we have information about the identity of
the phone that produced each and every observation.
The general training scenario in most large vocabu-
lary speech recognition systems, however, does not
have access to these starting/ending times, and they
are trained knowing only the sequence of phone la-
bels (e.g., that p2 follows p1).
Consider a new transcription based on Figure 2,
where we know that p1 ended at some time t3 ? t4
and that p2 started at sometime t5 > t4. In the
region between t3 and t5 we have no information
on the identity of the phone variable for each
acoustic frame, except that it is either p1 or p2. A
similar case occurs at the start of phone p1 and
the end of phone p2. The above information can
be used in our model (Figure 1) in the following
way (here given only for t2 ? t ? t6): p(Ct =
1|pt) = ?{pt=p1,t2?t?t3} + ?{pt=p2,t5?t?t6} +
ft(p1)?{pt=p1,t3?t?t5} + gt(p2)?{pt=p2,t3?t?t5}.
Here ft(p1) and gt(p2) represent our relative beliefs
at time t in whether the value of Pt is either p1
or p2. It is important to highlight that rather than
the absolute values of these functions, it is their
relative values that have an effect on inference
(Bilmes, 2004). There are number of different
ways of choosing these functions. First, we can set
ft(p1) = gt(p2) = ?, ? > 0. This encodes our
uncertainty regarding the identity of the phone in
this region while still forcing it to be either p1 or
p2, and equal preference is given for both (referred
to as ?uniform over two phones?). Alternatively,
other functions could take into account the fact that,
in the frames ?close? to t3, it is more likely to be
p1, whereas in the frames ?close? to t5, it is more
likely to be p2. This can be represented by using
a decreasing function for ft(p1) and an increasing
function for gt(p2) (for example linearly increasing
or decreasing with time).
As more frames are dropped around transitions
(e.g., as t3 ? t2 decreases), we use lesser amounts
of labeled data. In an extreme situation, we can drop
all the labels (t3 < t2) to recover the case where only
sequence and not segment information is available.
Alternatively, we can have t3 = t2 +1, which means
that only one frame is labeled for every phone in an
utterance ? all other frames of a phone are left un-
transcribed. From the perspective of a transcriber,
this simulates the task of going through an utter-
ance and identifying only one frame that belongs to
0 10 20 30 40 50 60 70 80 90 100
52
54
56
58
60
62
64
% of Unused Segmentation Data
Ph
on
e A
cc
ur
ac
y
 
 
Baseline
Uniform over 2 phones
Linear Interpolation
Figure 3: Virtual Evidence Results
each particular phone without having to identify the
phone boundary. In contrast to the task of determin-
ing the phone boundary, identifying one frame per
word unit is much simpler, less prone to error or dis-
agreement, and less costly (Greenberg, 1995).
4 Experimental Results
4.1 Baseline System
We trained a baseline TIMIT phone recognition sys-
tem that made full use of all phone level segmen-
tations (the fully supervised case). To obtain the
acoustic observations, the signal was first preem-
phasized (? = 0.97) and then windowed using a
Hamming window of size 25ms at 100Hz. We then
extracted MFCC?s from these windowed features.
Deltas and double deltas were appended to the above
observation vector. Each phone is modeled using 3
states, and 64 Gaussians per state. We follow the
standard practice of building models for 48 different
phones and then mapping them down to 39 phones
for scoring purposes (Halberstadt and Glass, 1997).
The decoding DBN graph is similar to the training
graph (Figure 1) except that the variable Ct is re-
moved when decoding. We test on the NIST Core
test set (Glass et al, 1996). All results reported in
this paper were obtained by computing the string
edit (Levenshtein) distance between the hypothesis
and the reference. All models in this paper were
implemented using the Graphical Models Toolkit
(GMTK) (Bilmes and Bartels, 2005).
4.2 VE Based Training and Results
We tested various cases of VE-based training by
varying the amount of ?dropped? frame labels on
either side of the transition (the dropped labels be-
came the unlabeled frames of Figure 2). We did this
until there was only one frame left labeled for ev-
ery phone. Moreover, in each of the above cases,
we tested a number of different functions to gener-
167
ate the VE scores (see section 3). The results of our
VE experiments are shown in Figure 3. The curves
were obtained by fitting a cubic spline to the points
shown in the figure. The phone accuracy (PA) of our
baseline system (trained in a fully supervised man-
ner) is 61.4%. If the total number of frames in the
training set is NT , and we drop labels on N frames,
the amount of unused data is given by U = NNT ?100(the x-axis in the figure). Thus U = 0% is the fully
supervised case, whereas U = 100% corresponds
to using only the sequence information. Dropping
the label for one frame on either side of every phone
transition yielded U = 24.5%.
It can be seen that in the case of both ?uniform
over 2 phones? and linear interpolation, the PA ac-
tually improves when we drop a small number (?
5 frames) of frames on either side of the transition.
This seems to suggest that there might be some in-
herent errors in the frame level labels near the phone
transitions. The points on the plot at U=84.7% cor-
respond to using a single labeled frame per phone
in every utterance in the training set (average phone
length in TIMIT is about 7 frames). The PA of the
system using a single label per phone is 60.52%. In
this case, we also used a trapezoidal function defined
as follows: if t = ti were the labeled frames for
phone p1, then ft(p1) = 1, ti ? 1 ? t ? ti + 1, and
a linear interpolation function for the other values
t during the transition to generate the VE weights.
This system yielded a PA of 61.29% (baseline accu-
racy 61.4%). We should highlight that even though
this system used only 15.3% of the labels used by
the baseline, the results were similar! The figure
also shows the PA of the system that used only
the sequence information was about 53% (compare
against baseline accuracy of 61.4%). This lends ev-
idence to the claim that training recognizers using
data with time segmentation information can lead to
improved performance.
Given the procedure we used to drop the frames
around transitions, the single labeled frame for ev-
ery phone is usually located on or around the mid-
point of the phone. This however cannot be guaran-
teed if a transcriber is asked to randomly label one
frame per phone. To simulate such a situation, we
randomly choose one frame to be labeled for every
phone in the utterance. We then trained this system
using the ?uniform over 2 phones? technique and
tested it on the NIST core test set. This experiment
was repeated 10 times, and the PA averaged over the
10 trails was found to be 60.5% (standard deviation
0.402), thus showing the robustness of our technique
even for less carefully labeled data.
5 Discussion
In this paper we have shown how VE can be used
to train a TIMIT phone recognition system using
partially labeled data. The performance of this sys-
tem is not significantly worse than the baseline that
makes use of all the labels. Further, though this
method of data transcription is only slightly more
time consuming that sequence labeling, it yeilds sig-
nificant gains in performance (53% v/s 60.5%). The
results also show that even in the presence of fully
labaled data, allowing for uncertainity at the tran-
sitions during training can be beneficial for ASR
performance. It should however be pointed out
that while phone recognition accuracy is not al-
ways a good predictor of word accuracy, we still
expect that our method will ultimately generalize
to word accuracy as well, assuming we have ac-
cess to a corpus where at least one frame of each
word has been labeled with the word identity. This
work was supported by an ONR MURI grant, No.
N000140510388.
References
[Bilmes and Bartels2005] J. Bilmes and C. Bartels. 2005.
Graphical model architectures for speech recognition. IEEE
Signal Processing Magazine, 22(5):89?100, September.
[Bilmes2004] J. Bilmes. 2004. On soft evidence in Bayesian
networks. Technical Report UWEETR-2004-0016, Univer-
sity of Washington, Dept. of EE.
[Glass et al1996] J. Glass, J. Chang, and M. McCandless. 1996.
A probabilistic framework for feature-based speech recogni-
tion. In Proc. ICSLP ?96, volume 4, Philadelphia, PA.
[Greenberg1995] S Greenberg. 1995. The Switchboard tran-
scription project. Technical report, The Johns Hopkins Uni-
versity (CLSP) Summer Research Workshop.
[Halberstadt and Glass1997] A. K. Halberstadt and J. R. Glass.
1997. Heterogeneous acoustic measurements for phonetic
classification. In Proc. Eurospeech ?97, pages 401?404,
Rhodes, Greece.
[Lamel et al2002] L. Lamel, J. Gauvian, and G. Adda. 2002.
Lightly supervised and unsupervised acoustic model train-
ing. Computer Speech and Language.
[Pearl1988] J. Pearl. 1988. Probabilistic Reasoning in Intel-
ligent Systems: Networks of Plausible Inference. Morgan
Kaufmann Publishers, Inc.
[Subramanya et al2006] A. Subramanya, A. Raj, J. Bilmes, and
D. Fox. 2006. Recognizing activities and spatial context
using wearable sensors. In Proc. of the Conference on Un-
certainty in Articial Intelligence (UAI).
[Wang and Fosler-Lussier2006] Y. Wang and E. Fosler-Lussier.
2006. Integrating phonetic boundary discrimination explic-
ity into HMM systems. In Proc. of the Interspeech.
168
Proceedings of the 43rd Annual Meeting of the ACL, pages 338?345,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Dynamic Bayesian Framework to Model Context and Memory in Edit
Distance Learning: An Application to Pronunciation Classification
Karim Filali and Jeff Bilmes?
Departments of Computer Science & Engineering and Electrical Engineering
University of Washington
Seattle, WA 98195, USA
{karim@cs,bilmes@ee}.washington.edu
Abstract
Sitting at the intersection between statis-
tics and machine learning, Dynamic
Bayesian Networks have been applied
with much success in many domains, such
as speech recognition, vision, and compu-
tational biology. While Natural Language
Processing increasingly relies on statisti-
cal methods, we think they have yet to
use Graphical Models to their full poten-
tial. In this paper, we report on experi-
ments in learning edit distance costs using
Dynamic Bayesian Networks and present
results on a pronunciation classification
task. By exploiting the ability within the
DBN framework to rapidly explore a large
model space, we obtain a 40% reduc-
tion in error rate compared to a previous
transducer-based method of learning edit
distance.
1 Introduction
Edit distance (ED) is a common measure of the sim-
ilarity between two strings. It has a wide range
of applications in classification, natural language
processing, computational biology, and many other
fields. It has been extended in various ways; for
example, to handle simple (Lowrance and Wagner,
1975) or (constrained) block transpositions (Leusch
et al, 2003), and other types of block opera-
tions (Shapira and Storer, 2003); and to measure
similarity between graphs (Myers et al, 2000; Klein,
1998) or automata (Mohri, 2002).
?This material was supported by NSF under Grant No. ISS-
0326276.
Another important development has been the use
of data-driven methods for the automatic learning of
edit costs, such as in (Ristad and Yianilos, 1998) in
the case of string edit distance and in (Neuhaus and
Bunke, 2004) for graph edit distance.
In this paper we revisit the problem of learn-
ing string edit distance costs within the Graphi-
cal Models framework. We apply our method to
a pronunciation classification task and show sig-
nificant improvements over the standard Leven-
shtein distance (Levenshtein, 1966) and a previous
transducer-based learning algorithm.
In section 2, we review a stochastic extension of
the classic string edit distance. We present our DBN-
based edit distance models in section 3 and show re-
sults on a pronunciation classification task in sec-
tion 4. In section 5, we discuss the computational
aspects of using our models. We end with our con-
clusions and future work in section 6.
2 Stochastic Models of Edit Distance
Let sm1 = s1s2...sm be a source string over a source
alphabet A, and m the length of the string. sji is the
substring si...sj and sji is equal to the empty string,
?, when i > j. Likewise, tn1 denotes a target string
over a target alhabet B, and n the length of tn1 .
A source string can be transformed into a target
string through a sequence of edit operations. We
write ?s, t? ((s, t) 6= (?, ?)) to denote an edit opera-
tion in which the symbol s is replaced by t. If s=?
and t 6=?, ?s, t? is an insertion. If s 6=? and t=?, ?s, t?
is a deletion. When s 6= ?, t 6= ? and s 6= t, ?s, t? is a
substitution. In all other cases, ?s, t? is an identity.
The string edit distance, d(sm1 , tn1 ) between sm1
and tn1 is defined as the minimum weighted sum of
338
the number of deletions, insertions, and substitutions
required to transform sm1 into tn1 (Wagner and Fis-
cher, 1974). A O(m ? n) Dynamic Programming
(DP) algorithm exists to compute the ED between
two strings. The algorithm is based on the following
recursion:
d(si1, t
j
1) = min
?
?
?
d(si?11 , t
j
1) + ?(?si, ??),
d(si1, t
j?1
1 ) + ?(??, tj?),
d(si?11 , t
j?1
1 ) + ?(?si, tj?)
?
?
?
with d(?, ?)=0 and ? : {?s, t?|(s, t) 6= (?, ?)}?<+
a cost function. When ? maps non-identity edit op-
erations to unity and identities to zero, string ED is
often referred to as the Levenshtein distance.
To learn the edit distance costs from data, Ristad
and Yianilos (1998) use a generative model (hence-
forth referred to as the RY model) based on a mem-
oryless transducer of string pairs. Below we sum-
marize their main idea and introduce our notation,
which will be useful later on.
We are interested in modeling the joint probability
P (Sm1=s
m
1 , T
n
1=t
n
1 | ?) of observing the source/target
string pair (sm1 , tn1 ) given model parameters ?. Si
(resp. Ti), 1?i?m, is a random variable (RV) as-
sociated with the event of observing a source (resp.
target) symbol at position i.1
To model the edit operations, we introduce a hid-
den RV, Z, that takes values in (A ? ? ? B ? ?) \
{(?, ?)}. Z can be thought of as a random vector
with two components, Z(s) and Z(t).
We can then write the joint probability
P (sm1 , t
n
1 | ?) as
P (sm1 , t
n
1 | ?) =
??
{z`1:v(z
`
1)=<s
m
1 ,t
n
1>, max(m,n)?`?m+n}
P (Z`1=z
`
1, s
m
1 , t
n
1 | ?) (1)
where v(z`1) is the yield of the sequence z`1: the
string pair output by the transducer.
Equation 1 says that the probability of a par-
ticular pair of strings is equal to the sum of the
probabilities of all possible ways to generate the
pair by concatenating the edit operations z1...z`. If
we make the assumption that there is no depen-
dence between edit operations, we call our model
memoryless. P (Z`1, sm1 , tn1 | ?) can then be factored
as ?iP (Zi, sm1 , t
n
1 | ?). In addition, we call the
model context-independent if we can write Q(zi) =
1We follow the convention of using capital letters for ran-
dom variables and lowercase letters for instantiations of random
variables.
P (Zi=zi, sm1 , t
n
1 | ?), 1<i<`, where zi=?z
(s)
i , z
(t)
i ?,
in the form
Q(zi) ?
?
????
????
f ins(tbi) for z
(s)
i = ?; z
(t)
i = tbi
fdel(sai) for z
(s)
i = sai ; z
(t)
i = ?
fsub(sai , tbi) for (z
(s)
i , z
(t)
i )= (sai , tbi)
0 otherwise
(2)
where
?
z Q(z) = 1; ai =
?i?1
j=1 1{z(s)j 6=?}
(resp. bi)
is the index of the source (resp. target) string gen-
erated up to the ith edit operation; and f ins,fdel,and
fsub are functions mapping to [0, 1].2 Context in-
dependence is not to be taken here to mean Zi
does not depend on sai or tbi . It depends on them
through the global context which forces Z`1 to gen-
erate (sm1 , t
n
1 ). The RY model is memoryless and
context-independent (MCI).
Equation 2, also implicitly enforces the consis-
tency constraint that the pair of symbols output,
(z(s)i , z
(t)
i ), agrees with the actual pair of symbols,
(sai , tbi), that needs to be generated at step i in or-
der for the total yield, v(z`1), to equal the string pair.
The RY stochastic model is similar to the one in-
troduced earlier by Bahl and Jelinek (1975). The
difference is that the Bahl model is memoryless
and context-dependent (MCD); the f functions are
now indexed by sai (or tai , or both) such that?
z Qsai (z) = 1 ?sai . In general, context depen-
dence can be extended to include up to the whole
source (and/or target) string, sai?11 , sai , smai+1. Sev-
eral other types of dependence can be exploited as
will be discussed in section 3.
Both the Ristad and the Bahl transducer mod-
els give exponentially smaller probability to longer
strings and edit sequences. Ristad presents an al-
ternate explicit model of the joint probability of
the length of the source and target strings. In this
parametrization the probability of the length of an
edit sequence does not necessarily decrease geomet-
rically. A similar effect can be achieved by modeling
the length of the hidden edit sequence explicitly (see
section 3).
3 DBNs for Learning Edit Distance
Dynamic Bayesian Networks (DBNs), of which
Hidden Markov Models (HMMs) are the most fa-
2By convention, sai = ? for ai > m. Likewise, tbi = ? if
bi > n. f ins(?) = fdel(?) = fsub(?, ?) = 0. This takes care
of the case when we are past the end of a string.
339
mous representative, are well suited for modeling
stochastic temporal processes such as speech and
neural signals. DBNs belong to the larger family of
Graphical Models (GMs). In this paper, we restrict
ourselves to the class of DBNs and use the terms
DBN and GM interchangeably. For an example in
which Markov Random Fields are used to compute
a context-sensitive edit distance see (Wei, 2004).3
There is a large body of literature on DBNs and
algorithms associated with them. To briefly de-
fine a graphical model, it is a way of representing
a (factored) probability distribution using a graph.
Nodes of the graph correspond to random variables;
and edges to dependence relations between the vari-
ables.4 To do inference or parameter learning us-
ing DBNs, various generic exact or approximate
algorithms exist (Lauritzen, 1996; Murphy, 2002;
Bilmes and Bartels, 2003). In this section we start
by introducing a graphical model for the MCI trans-
ducer then present four additional classes of DBN
models: context-dependent, memory (where an edit
operation can depend on past operations), direct
(HMM-like), and length models (in which we ex-
plicitly model the length of the sequence of edits
to avoid the exponential decrease in likelihood of
longer sequences). A few other models are dis-
cussed in section 4.2.
3.1 Memoryless Context-independent Model
Fig. 1 shows a DBN representation of the memo-
ryless context-independent transducer model (sec-
tion 2). The graph represents a template which con-
sists, in general, of three parts: a prologue, a chunk,
and an epilogue. The chunk is repeated as many
times as necessary to model sequences of arbitrary
length. The product of unrolling the template is a
Bayesian Network organized into a given number of
frames. The prologue and the epilogue often differ
from the chunk because they model boundary con-
ditions, such as ensuring that the end of both strings
is reached at or before the last frame.
Associated with each node is a probability func-
tion that maps the node?s parent values to the values
the node can take. We will refer to that function as a
3While the Markov Edit Distance introduced in the paper
takes local statistical dependencies into account, the edit costs
are still fixed and not corpus-driven.
4The concept of d-separation is useful to read independence
relations encoded by the graph (Lauritzen, 1996).
Figure 1: DBN for the memory-less transducer
model. Unshaded nodes are hidden nodes with prob-
abilistic dependencies with respect to their parents.
Nodes with stripes are deterministic hidden nodes,
i.e., they take a unique value for each configuration
of their parents. Filled nodes are observed (they can
be either stochastic or deterministic). The graph
template is divided into three frames. The center
frame is repeated m+n? 2 times to yield a graph
with a total of m+n frames, the maximum number
of edit operations needed to transform sm1 into tn1 .
Outgoing light edges mean the parent is a switch-
ing variable with respect to the child: depending on
the value of the switching RV, the child uses different
CPTs and/or a different parent set.
conditional probability table (CPT).
Common to all the frames in fig. 1, are position
RVs, a and b, which encode the current positions in
the source and target strings resp.; source and target
symbols, s and t; the hidden edit operation, Z; and
consistency nodes sc and tc, which enforce the con-
sistency constraint discussed in section 2. Because
of symmetry we will explain the upper half of the
graph involving the source string unless the target
half is different. We drop subscripts when the frame
number is clear from the context.
In the first frame, a and b are observed to have
value 1, the first position in both strings. a and b
determine the value of the symbols s and t. Z takes
a random value ?z(s), z(t)?. sc has the fixed observed
value 1. The only configurations of its parents, Z
and s, that satisfy P (sc=1|s, z) > 0 are such that
(Z(s) = s) or (Z(s) = ? and Z 6= ??, ??). This is the
consistency constraint in equation 2.
In the following frame, the position RV a2 de-
pends on a1 and Z1. If Z1 is an insertion (i.e.
Z(s)1 = ? : the source symbol in the first frame is
340
not output), then a2 retains the same value as a1;
otherwise a2 is incremented by 1 to point to the next
symbol in the source string.
The end RV is an indicator of when we are past
the end of both source and target strings (a>m and
b > n). end is also a switching parent of Z; when
end = 0, the CPT of Z is the same as described
above: a distribution over edit operations. When
end = 1, Z takes, with probability 1, a fixed value
outside the range of edit operations but consistent
with s and t. This ensures 1) no ?null? state (??, ??)
is required to fill in the value of Z until the end
of the graph is reached; our likelihoods and model
parameters therefore do not become dependent on
the amount of ?null? padding; and 2) no probability
mass is taken from the other states of Z as is the case
with the special termination symbol # in the original
RY model. We found empirically that the use of ei-
ther a null or an end state hurts performance to a
small but significant degree.
In the last frame, two new nodes make their ap-
pearance. send and tend ensure we are at or past
the end of the two strings (the RV end only checks
that we are past the end). That is why send depends
on both a and Z. If a>m, send (observed to be 1) is
1 with probability 1. If a<m, then P (send=1)=0
and the whole sequence Z`1 has zero probability. If
a=m, then send only gets probability greater than
zero if Z is not an insertion. This ensures the last
source symbol is indeed consumed.
Note that we can obtain the equivalent of the to-
tal edit distance cost by using Viterbi inference and
adding a costi variable as a deterministic child of the
random variable Zi : in each frame the cost is equal
to costi?1 plus 0 when Zi is an identity, or plus 1
otherwise.
3.2 Context-dependent Model
Adding context dependence in the DBN framework
is quite natural. In fig. 2, we add edges from si,
sprevi, and snexti to Zi. The sc node is no longer
required because we can enforce the consistency
constraint via the CPT of Z given its parents. snexti
is an RV whose value is set to the symbol at the ai+1
position of the string, i.e., snexti=sai+1. Likewise,
sprevi = sai?1. The Bahl model (1975) uses a de-
pendency on si only. Note that si?1 is not necessar-
ily equal to sai?1. Conditioning on si?1 induces an
Figure 2: Context-dependent model.
indirect dependence on whether there was an inser-
tion in the previous step because si?1 = si might be
correlated with the event Z(s)i?1 = ?.
3.3 Memory Model
Memory models are another easy extension of the
basic model as fig. 3 shows. Depending on whether
the variable Hi?1 linking Zi?1 to Zi is stochastic
or deterministic, there are several models that can
be implemented; for example, a latent factor mem-
ory model when H is stochastic. The cardinality of
H determines how much the information from one
frame to the other is ?summarized.? With a deter-
ministic implementation, we can, for example, spec-
ify the usual P (Zi|Zi?1) memory model when H is
a simple copy of Z or have Zi depend on the type of
edit operation in the previous frame.
Figure 3: Memory model. Depending on the type of
dependency between Zi and Hi, the model can be
latent variable based or it can implement a deter-
ministic dependency on a function of Zi
3.4 Direct Model
The direct model in fig. 4 is patterned on the clas-
sic HMM, where the unrolled length of graph is the
same as the length of the sequence of observations.
The key feature of this model is that we are required
341
to consume a target symbol per frame. To achieve
that, we introduce two RVs, ins, with cardinality
2, and del, with cardinality at most m. The depen-
dency of del on ins is to ensure the two events never
happen concomitantly. At each frame, a is incre-
mented either by the value of del in the case of a
(possibly block) deletion or by zero or one depend-
ing on whether there was an insertion in the previous
frame. An insertion also forces s to take value ?.
Figure 4: Direct model.
In essence the direct model is not very differ-
ent from the context-dependent model in that here
too we learn the conditional probabilities P (ti|si)
(which are implicit in the CD model).
3.5 Length Model
While this model (fig. 5) is more complex than
the previous ones, much of the network structure
is ?control logic? necessary to simulate variable
length-unrolling of the graph template. The key idea
is that we have a new stochastic hidden RV, inclen,
whose value added to that of the RV inilen deter-
mines the number of edit operations we are allowed.
A counter variable, counter is used to keep track
of the frame number and when the required num-
ber is reached, the RV atReqLen is triggered. If at
that point we have just reached the end of one of the
strings while the end of the other one is reached in
this frame or a previous one, then the variable end
is explained (it has positive probability). Otherwise,
the entire sequence of edit operations up to that point
has zero probability.
4 Pronunciation Classification
In pronunciation classification we are given a lexi-
con, which consists of words and their correspond-
ing canonical pronunciations. We are also provided
with surface pronunciations and asked to find the
most likely corresponding words. Formally, for each
Figure 5: Length unrolling model.
surface form, tn1 , we need to find the set of words
W? s.t. W? = argmaxwP (w|tn1 ). There are several
ways we could model the probability P (w|tn1 ). One
way is to assume a generative model whereby a word
w and a surface pronunciation tn1 are related via an
underlying canonical pronunciation sm1 of w and a
stochastic process that explains the transformation
from sm1 to tn1 . This is summarized in equation 3.
C(w) denotes the set of canonical pronunciations of
w.
W? = argmax
w
?
sm1 ?C(w)
P (w|sm1 )P (s
m
1 , t
n
1 ) (3)
If we assume uniform probabilities P (w|sm1 )
(sm1 ?C(w)) and use the max approximation in place
of the sum in eq. 3 our classification rule becomes
W? = {w|S??C(w) 6=?, S?=argmax
sm1
P (sm1 , t
n
1 )} (4)
It is straightforward to create a DBN to model the
joint probability P (w, sm1 , tn1 ) by adding a word RV
and a canonical pronunciation RV on top of any of
the previous models.
There are other pronunciation classification ap-
proaches with various emphases. For example,
Rentzepopoulos and Kokkinakis (1996) use HMMs
to convert phoneme sequences to their most likely
orthographic forms in the absence of a lexicon.
4.1 Data
We use Switchboard data (Godfrey et al, 1992) that
has been hand annotated in the context of the Speech
Transcription Project (STP) described in (Green-
berg et al, 1996). Switchboard consists of spon-
taneous informal conversations recorded over the
342
phone. Because of the informal non-scripted nature
of the speech and the variety of speakers, the cor-
pus presents much variety in word pronunciations,
which can significantly deviate from the prototypical
pronunciations found in a lexicon. Another source
of pronunciation variability is the noise introduced
during the annotation of speech segments. Even
when the phone labels are mostly accurate, the start
and end time information is not as precise and it af-
fects how boundary phones get algned to the word
sequence. As a reference pronunciation dictionary
we use a lexicon of the 2002 Switchboard speech
recognition evaluation. The lexicon contains 40000
entries, but we report results on a reduced dictio-
nary5 with 5000 entries corresponding to only those
words that appear in our train and test sets. Ristad
and Yianilos use a few additional lexicons, some of
which are corpus-derived. We did reproduce their
results on the different types of lexicons.
For testing we randomly divided STP data into
9495 training words (corresponding to 9545 pronun-
ciations) and 912 test words (901 pronunciations).
For the Levenshtein and MCI results only, we per-
formed ten-fold cross validation to verify we did not
pick a non-representative test set. Our models are
implemented using GMTK, a general-purpose DBN
tool originally created to explore different speech
recognition models (Bilmes and Zweig, 2002). As
a sanity check, we also implemented the MCI model
in C following RY?s algorithm.
The error rate is computed by calculating, for each
pronunciation form, the fraction of words that are
correctly hypothesized and averaging over the test
set. For example if the classifier returns five words
for a given pronunciation, and two of the words are
correct, the error rate is 3/5*100%.
Three EM iterations are used for training. Addi-
tional iterations overtrained our models.
4.2 Results
Table 1 summarizes our results using DBN based
models. The basic MCI model does marginally bet-
ter than the Levenshtein edit distance. This is con-
sistent with the finding in RY: their gains come from
the joint learning of the probabilities P (w|sm1 ) and
P (sm1 , t
n
1 ). Specifically, the word model accounts
for much of their gains over the Levenshtein dis-
5Equivalent to the E2 lexicon in RY.
tance. We use uniform priors and the simple classi-
fication rule in eq. 4. We feel it is more compelling
that we are able to significantly improve upon stan-
dard edit distance and the MCI model without using
any lexicon or word model.
Memory Models Performance improves with the
addition of a direct dependence of Zi on Zi?1. The
biggest improvement (27.65% ER) however comes
from conditioning on Z(t)i?1, the target symbol that
is hypothesized in the previous step. There was no
gain when conditioning on the type of edit operation
in the previous frame.
Context Models Interestingly, the exact opposite
from the memory models is happening here when
we condition on the source context (versus condi-
tioning on the target context). Conditioning on si
gets us to 21.70%. With si, si?1 we can further re-
duce the error rate to 20.26%. However, when we
add a third dependency, the error rate worsens to
29.32%, which indicates a number of parameters too
high for the given amount of training data. Backoff,
interpolation, or state clustering might all be appro-
priate strategies here.
Position Models Because in the previous mod-
els, when conditioning on the past, boundary condi-
tions dictate that we use a different CPT in the first
frame, it is fair to wonder whether part of the gain
we witness is due to the implicit dependence on the
source-target string position. The (small) improve-
ment due to conditioning on bi indicates there is such
dependence. Also, the fact that the target position is
more informative than the source one is likely due to
the misalignments we observed in the phonetically
transcribed corpus, whereby the first or last phones
would incorrectly be aligned with the previous or
next word resp. I.e., the model might be learning
to not put much faith in the start and end positions
of the target string, and thus it boosts deletion and
insertion probabilities at those positions. We have
also conditioned on coarser-grained positions (be-
ginning, middle, and end of string) but obtained the
same results as with the fine-grained dependency.
Length Models Modeling length helps to a small
extent when it is added to the MCI and MCD mod-
els. Belying the assumption motivating this model,
we found that the distribution over the RV inclen
(which controls how much the edit sequence extends
343
beyond the length of the source string) is skewed to-
wards small values of inclen. This indicates on that
insertions are rare when the source string is longer
than the target one and vice-versa for deletions.
Direct Model The low error rate obtained by this
model reflects its similarity to the context-dependent
model. From the two sets of results, it is clear that
source string context plays a crucial role in predict-
ing canonical pronunciations from corpus ones. We
would expect additional gains from modeling con-
text dependencies across time here as well.
Model Zi Dependencies % Err rate
Lev none 35.97
Baseline none 35.55
Memory
Zi?1 30.05
editOperationType(Zi?1) 36.16
stochastic binary Hi?1 33.87
Z(s)i?1 29.62
Z(t)i?1 27.65
Context
si 21.70
ti 32.06
si, si?1 20.26
ti, ti?1 28.21
si, si?1, sai+1 29.32
si, sai+1 (sai?1 in last frame) 23.14
si, sai?1 (sai+1 in first frame) 23.15
Position
ai 33.80
bi 31.06
ai, bi 34.17
Mixed bi,si 22.22
Z(t)i?1,si 24.26
Length none 33.56si 20.03
Direct none 23.70
Table 1: DBN based model results summary.
When we combine the best position-dependent
or memory models with the context-dependent one,
the error rate decreases (from 31.31% to 25.25%
when conditioning on bi and si; and from 28.28%
to 25.75% when conditioning on z(t)i?1 and si) but not
to the extent conditioning on si alone decreases error
rate. Not shown in table 1, we also tried several other
models, which although they are able to produce
reasonable alignments (in the sense that the Leven-
shtein distance would result in similar alignments)
between two given strings, they have extremely poor
discriminative ability and result in error rates higher
than 90%. One such example is a model in which
Zi depends on both si and ti. It is easy to see where
the problem lies with this model once one considers
that two very different strings might still get a higher
likelihood than more similar pair because, given s
and t s.t. s 6= t, the probability of identity is obvi-
ously zero and that of insertion or deletion can be
quite high; and when s = t, the probability of in-
sertion (or deletion) is still positive. We observe the
same non-discriminative behavior when we replace,
in the MCI model, Zi with a hidden RV Xi, where
Xi takes as values one of the four edit operations.
5 Computational Considerations
The computational complexity of inference in a
graphical model is related to the state space of the
largest clique (maximal complete subgraph) in the
graph. In general, finding the smallest such clique is
NP-complete (Arnborg et al, 1987).
In the case of the MCI model, however, it is not
difficult to show that the smallest such clique con-
tains all the RVs within a frame and the complex-
ity of doing inference is order O(mn ?max(m,n)).
The reason there is a complexity gap is that the
source and target position variables are indexed by
the frame number and we do not exploit the fact
that even though we arrive at a given source-target
position pair along different edit sequence paths at
different frames, the position pair is really the same
regardless of its frame index. We are investigating
generic ways of exploiting this constraint.
In practice, however, state space pruning can sig-
nificantly reduce the running time of DBN infer-
ence. Ukkonen (1985) reduces the complexity of the
classic edit distance to O(d?max(m,n)), where d is
the edit distance. The intuition there is that, assum-
ing a small edit distance, the most likely alignments
are such that the source position does not diverge too
much from the target position. The same intuition
holds in our case: if the source and the target posi-
tion do not get too far out of sync, then at each step,
only a small fraction of the m ? n possible source-
target position configurations need be considered.
The direct model, for example, is quite fast in
practice because we can restrict the cardinality of the
del RV to a constant c (i.e. we disallow long-span
deletions, which for certain applications is a reason-
able restriction) and make inference linear in n with
a running time constant proportional to c2.
344
6 Conclusion
We have shown how the problem of learning edit
distance costs from data can be modeled quite
naturally using Dynamic Bayesian Networks even
though the problem lacks the temporal or order con-
straints that other problems such as speech recog-
nition exhibit. This gives us confidence that other
important problems such as machine translation can
benefit from a Graphical Models perspective. Ma-
chine translation presents a fresh set of challenges
because of the large combinatorial space of possible
alignments between the source string and the target.
There are several extensions to this work that we
intend to implement or have already obtained pre-
liminary results on. One is simple and block trans-
position. Another natural extension is modeling edit
distance of multiple strings.
It is also evident from the large number of depen-
dency structures that were explored that our learn-
ing algorithm would benefit from a structure learn-
ing procedure. Maximum likelihood optimization
might, however, not be appropriate in this case, as
exemplified by the failure of some models to dis-
criminate between different pronunciations. Dis-
criminative methods have been used with significant
success in training HMMs. Edit distance learning
could benefit from similar methods.
References
S. Arnborg, D. G. Corneil, and A. Proskurowski. 1987.
Complexity of finding embeddings in a k-tree. SIAM
J. Algebraic Discrete Methods, 8(2):277?284.
L. R. Bahl and F. Jelinek. 1975. Decoding for channels
with insertions, deletions, and substitutions with appli-
cations to speech recognition. Trans. on Information
Theory, 21:404?411.
J. Bilmes and C. Bartels. 2003. On triangulating dy-
namic graphical models. In Uncertainty in Artifi-
cial Intelligence: Proceedings of the 19th Conference,
pages 47?56. Morgan Kaufmann.
J. Bilmes and G. Zweig. 2002. The Graphical Models
Toolkit: An open source software system for speech
and time-series processing. Proc. IEEE Intl. Conf. on
Acoustics, Speech, and Signal Processing.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In ICASSP, volume 1, pages
517?520.
S. Greenberg, J. Hollenback, and D. Ellis. 1996. Insights
into spoken language gleaned from phonetic transcrip-
tion of the switchboard corpus. In ICSLP, pages S24?
27.
P. N. Klein. 1998. Computing the edit-distance between
unrooted ordered trees. In Proceedings of 6th Annual
European Symposium, number 1461, pages 91?102.
S.L. Lauritzen. 1996. Graphical Models. Oxford Sci-
ence Publications.
G. Leusch, N. Ueffing, and H. Ney. 2003. A novel
string-to-string distance measure with applications to
machine translation evaluation. In Machine Transla-
tion Summit IX, pages 240?247.
V. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Sov. Phys.
Dokl., 10:707?710.
R. Lowrance and R. A. Wagner. 1975. An extension
to the string-to-string correction problem. J. ACM,
22(2):177?183.
M. Mohri. 2002. Edit-distance of weighted automata.
In CIAA, volume 2608 of Lecture Notes in Computer
Science, pages 1?23. Springer.
K. Murphy. 2002. Dynamic Bayesian Networks: Repre-
sentation, Inference and Learning. Ph.D. thesis, U.C.
Berkeley, Dept. of EECS, CS Division.
R. Myers, R.C. Wison, and E.R. Hancock. 2000.
Bayesian graph edit distance. IEEE Trans. on Pattern
Analysis and Machine Intelligence, 22:628?635.
M. Neuhaus and H. Bunke. 2004. A probabilistic ap-
proach to learning costs for graph edit distance. In
ICPR, volume 3, pages 389?393.
P. A. Rentzepopoulos and G. K. Kokkinakis. 1996. Ef-
ficient multilingual phoneme-to-grapheme conversion
based on hmm. Comput. Linguist., 22(3):351?376.
E. S. Ristad and P. N. Yianilos. 1998. Learning string
edit distance. Trans. on Pattern Recognition and Ma-
chine Intelligence, 20(5):522?532.
D. Shapira and J. A. Storer. 2003. Large edit distance
with multiple block operations. In SPIRE, volume
2857 of Lecture Notes in Computer Science, pages
369?377. Springer.
E. Ukkonen. 1985. Algorithms for approximate string
matching. Inf. Control, 64(1-3):100?118.
R. A. Wagner and M. J. Fischer. 1974. The string-to-
string correction problem. J. ACM, 21(1):168?173.
J. Wei. 2004. Markov edit distance. Trans. on Pattern
Analysis and Machine Intelligence, 26(3):311?321.
345
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 262?270,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Compiling a Massive, Multilingual Dictionary via Probabilistic Inference
Mausam Stephen Soderland Oren Etzioni
Daniel S. Weld Michael Skinner* Jeff Bilmes
University of Washington, Seattle *Google, Seattle
{mausam,soderlan,etzioni,weld,bilmes}@cs.washington.edu mskinner@google.com
Abstract
Can we automatically compose a large set
of Wiktionaries and translation dictionar-
ies to yield a massive, multilingual dic-
tionary whose coverage is substantially
greater than that of any of its constituent
dictionaries?
The composition of multiple translation
dictionaries leads to a transitive inference
problem: if word A translates to word
B which in turn translates to word C,
what is the probability that C is a trans-
lation of A? The paper introduces a
novel algorithm that solves this problem
for 10,000,000 words in more than 1,000
languages. The algorithm yields PANDIC-
TIONARY, a novel multilingual dictionary.
PANDICTIONARY contains more than four
times as many translations than in the
largest Wiktionary at precision 0.90 and
over 200,000,000 pairwise translations in
over 200,000 language pairs at precision
0.8.
1 Introduction and Motivation
In the era of globalization, inter-lingual com-
munication is becoming increasingly important.
Although nearly 7,000 languages are in use to-
day (Gordon, 2005), most language resources are
mono-lingual, or bi-lingual.1 This paper investi-
gates whether Wiktionaries and other translation
dictionaries available over the Web can be auto-
matically composed to yield a massive, multilin-
gual dictionary with superior coverage at compa-
rable precision.
We describe the automatic construction of a
massive multilingual translation dictionary, called
1The English Wiktionary, a lexical resource developed by
volunteers over the Internet is one notable exception that con-
tains translations of English words in about 500 languages.
Figure 1: A fragment of the translation graph for two senses
of the English word ?spring?. Edges labeled ?1? and ?3? are
for spring in the sense of a season, and ?2? and ?4? are for
the flexible coil sense. The graph shows translation entries
from an English dictionary merged with ones from a French
dictionary.
PANDICTIONARY, that could serve as a resource
for translation systems operating over a very
broad set of language pairs. The most immedi-
ate application of PANDICTIONARY is to lexical
translation?the translation of individual words or
simple phrases (e.g., ?sweet potato?). Because
lexical translation does not require aligned cor-
pora as input, it is feasible for a much broader
set of languages than statistical Machine Transla-
tion (SMT). Of course, lexical translation cannot
replace SMT, but it is useful for several applica-
tions including translating search-engine queries,
library classifications, meta-data tags,2 and recent
applications like cross-lingual image search (Et-
zioni et al, 2007), and enhancing multi-lingual
Wikipedias (Adar et al, 2009). Furthermore,
lexical translation is a valuable component in
knowledge-based Machine Translation systems,
e.g., (Bond et al, 2005; Carbonell et al, 2006).
PANDICTIONARY currently contains over 200
million pairwise translations in over 200,000 lan-
guage pairs at precision 0.8. It is constructed from
information harvested from 631 online dictionar-
ies and Wiktionaries. This necessitates match-
2Meta-data tags appear in community Web sites such as
flickr.com and del.icio.us.
262
ing word senses across multiple, independently-
authored dictionaries. Because of the millions of
translations in the dictionaries, a feasible solution
to this sense matching problem has to be scalable;
because sense matches are imperfect and uncer-
tain, the solution has to be probabilistic.
The core contribution of this paper is a princi-
pled method for probabilistic sense matching to in-
fer lexical translations between two languages that
do not share a translation dictionary. For exam-
ple, our algorithm can conclude that Basque word
?udaherri? is a translation of Maori word ?koanga?
in Figure 1. Our contributions are as follows:
1. We describe the design and construction of
PANDICTIONARY?a novel lexical resource
that spans over 200 million pairwise transla-
tions in over 200,000 language pairs at 0.8
precision, a four-fold increase when com-
pared to the union of its input translation dic-
tionaries.
2. We introduce SenseUniformPaths, a scal-
able probabilistic method, based on graph
sampling, for inferring lexical translations,
which finds 3.5 times more inferred transla-
tions at precison 0.9 than the previous best
method.
3. We experimentally contrast PANDIC-
TIONARY with the English Wiktionary and
show that PANDICTIONARY is from 4.5 to
24 times larger depending on the desired
precision.
The remainder of this paper is organized as fol-
lows. Section 2 describes our earlier work on
sense matching (Etzioni et al, 2007). Section 3
describes how the PANDICTIONARY builds on and
improves on their approach. Section 4 reports on
our experimental results. Section 5 considers re-
lated work on lexical translation. The paper con-
cludes in Section 6 with directions for future work.
2 Building a Translation Graph
In previous work (Etzioni et al, 2007) we intro-
duced an approach to sense matching that is based
on translation graphs (see Figure 1 for an exam-
ple). Each vertex v ? V in the graph is an or-
dered pair (w, l) where w is a word in a language
l. Undirected edges in the graph denote transla-
tions between words: an edge e ? E between (w1,
l1) and (w2, l2) represents the belief that w1 and
w2 share at least one word sense.
Construction: The Web hosts a large num-
ber of bilingual dictionaries in different languages
and several Wiktionaries. Bilingual dictionaries
translate words from one language to another, of-
ten without distinguishing the intended sense. For
example, an Indonesian-English dictionary gives
?light? as a translation of the Indonesian word ?en-
teng?, but does not indicate whether this means il-
lumination, light weight, light color, or the action
of lighting fire.
The Wiktionaries (wiktionary.org) are sense-
distinguished, multilingual dictionaries created by
volunteers collaborating over the Web. A transla-
tion graph is constructed by locating these dictio-
naries, parsing them into a common XML format,
and adding the nodes and edges to the graph.
Figure 1 shows a fragment of a translation
graph, which was constructed from two sets of
translations for the word ?spring? from an English
Wiktionary, and two corresponding entries from
a French Wiktionary for ?printemps? (spring sea-
son) and ?ressort? (flexible spring). Translations of
the season ?spring? have edges labeled with sense
ID=1, the flexible coil sense has ID=2, translations
of ?printemps? have ID=3, and so forth.3
For clarity, we show only a few of the actual
vertices and edges; e.g., the figure doesn?t show
the edge (ID=1) between ?udaherri? and ?primav-
era?.
Inference: In our previous system we had
a simple inference procedure over translation
graphs, called TRANSGRAPH, to find translations
beyond those provided by any source dictionary.
TRANSGRAPH searched for paths in the graph be-
tween two vertices and estimated the probability
that the path maintains the same word sense along
all edges in the path, even when the edges come
from different dictionaries. For example, there are
several paths between ?udaherri? and ?koanga? in
Figure 1, but all shift from sense ID 1 to 3. The
probability that the two words are translations is
equivalent to the probability that IDs 1 and 3 rep-
resent the same sense.
TRANSGRAPH used two formulae to estimate
these probabilities. One formula estimates the
probability that two multi-lingual dictionary en-
tries represent the same word sense, based on the
proportion of overlapping translations for the two
entries. For example, most of the translations of
3Sense-distinguished multi-lingual entries give rise to
cliques all of which share a common sense ID.
263
French ?printemps? are also translations of the sea-
son sense of ?spring?. A second formula is based
on triangles in the graph (useful for bilingual dic-
tionaries): a clique of 3 nodes with an edge be-
tween each pair of nodes. In such cases, there is
a high probability that all 3 nodes share a word
sense.
Critique: While TRANSGRAPH was the first
to present a scalable inference method for lexical
translation, it suffers from several drawbacks. Its
formulae operate only on local information: pairs
of senses that are adjacent in the graph or triangles.
It does not incorporate evidence from longer paths
when an explicit triangle is not present. Moreover,
the probabilities from different paths are com-
bined conservatively (either taking the max over
all paths, or using ?noisy or? on paths that are
completely disjoint, except end points), thus lead-
ing to suboptimal precision/recall.
In response to this critique, the next section
presents an inference algorithm, called SenseUni-
formPaths (SP), with substantially improved recall
at equivalent precision.
3 Translation Inference Algorithms
In essence, inference over a translation graph
amounts to transitive sense matching: if word A
translates to word B, which translates in turn to
word C, what is the probability that C is a trans-
lation of A? If B is polysemous then C may not
share a sense with A. For example, in Figure 2(a)
if A is the French word ?ressort? (the flexible-
coil sense of spring) and B is the English word
?spring?, then Slovenian word ?vzmet? may or may
not be a correct translation of ?ressort? depending
on whether the edge (B,C) denotes the flexible-
coil sense of spring, the season sense, or another
sense. Indeed, given only the knowledge of the
path A ? B ? C we cannot claim anything with
certainty regarding A to C.
However, if A, B, and C are on a circuit that
starts at A, passes through B and C and re-
turns to A, there is a high probability that all
nodes on that circuit share a common word sense,
given certain restrictions that we enumerate later.
Where TRANSGRAPH used evidence from circuits
of length 3, we extend this to paths of arbitrary
lengths.
To see how this works, let us begin with the sim-
plest circuit, a triangle of three nodes as shown in
Figure 2(b). We can be quite certain that ?vzmet?
shares the sense of coil with both ?spring? and
?ressort?. Our reasoning is as follows: even
though both ?ressort? and ?spring? are polysemous
they share only one sense. For a triangle to form
we have two choices ? (1) either ?vzmet? means
spring coil, or (2) ?vzmet? means both the spring
season and jurisdiction, but not spring coil. The
latter is possible but such a coincidence is very un-
likely, which is why a triangle is strong evidence
for the three words to share a sense.
As an example of longer paths, our inference
algorithms can conclude that in Figure 2(c), both
?molla? and ?vzmet? have the sense coil, even
though no explicit triangle is present. To show
this, let us define a translation circuit as follows:
Definition 1 A translation circuit from v?1 with
sense s? is a cycle that starts and ends at v?1 with
no repeated vertices (other than v?1 at end points).
Moreover, the path includes an edge between v?1
and another vertex v?2 that also has sense s
?.
All vertices on a translation circuit are mutual
translations with high probability, as in Figure
2(c). The edge from ?spring? indicates that ?vzmet?
means either coil or season, while the edge from
?ressort? indicates that ?molla? means either coil
or jurisdiction. The edge from ?vzmet? to ?molla?
indicates that they share a sense, which will hap-
pen if all nodes share the sense season or if either
?vzmet? has the unlikely combination of coil and
jurisdiction (or ?molla? has coil and season).
We also develop a mathematical model of
sense-assignment to words that lets us formally
prove these insights. For more details on the the-
ory please refer to our extended version. This pa-
per reports on our novel algorithm and experimen-
tal results.
These insights suggest a basic version of our al-
gorithm: ?given two vertices, v?1 and v
?
2 , that share
a sense (say s?) compute all translation circuits
from v?1 in the sense s
?; mark all vertices in the
circuits as translations of the sense s??.
To implement this algorithm we need to decide
whether a vertex lies on a translation circuit, which
is trickier than it seems. Notice that knowing
that v is connected independently to v?1 and v
?
2
doesn?t imply that there exists a translation circuit
through v, because both paths may go through a
common node, thus violating of the definition of
translation circuit. For example, in Figure 2(d) the
Catalan word ?ploma? has paths to both spring and
ressort, but there is no translation circuit through
264
spring
English
ressort
French
vzmet
Slovenian
spring
English
ressort
French
vzmet
Slovenian
spring
English
vzmet
Slovenian
ressort
French
molla
Italian
spring
English
ressort
French
ploma
Catalan
Feder
German
???? 
Russian
spring
English
ressort
French
fj?der
Swedish
penna
Italian
Feder
German
(a)                         (b)                                   (c)                                (d)                     (e)
season
coil
jurisdiction
coil
s* s*
s* s*
s*
? ? ?
? ?
feather
coil
?
?
Figure 2: Snippets of translation graphs illustrating various inference scenarios. The nodes in question mark represent the
nodes in focus for each illustration. For all cases we are trying to infer translations of the flexible coil sense of spring.
it. Hence, it will not be considered a transla-
tion. This example also illustrates potential errors
avoided by our algorithm ? here, German word
?Feder? mean feather and spring coil, but ?ploma?
means feather and not the coil.
An exhaustive search to find translation circuits
would be too slow, so we approximate the solution
by a random walk scheme. We start the random
walk from v?1 (or v
?
2) and choose random edges
without repeating any vertices in the current path.
At each step we check if the current node has an
edge to v?2 (or v
?
1). If it does, then all the ver-
tices in the current path form a translation circuit
and, thus, are valid translations. We repeat this
random walk many times and keep marking the
nodes. In our experiments for each inference task
we performed a total of 2,000 random walks (NR
in pseudo-code) of max circuit length 7. We chose
these parameters based on a development set of 50
inference tasks.
Our first experiments with this basic algorithm
resulted in a much higher recall than TRANS-
GRAPH, albeit, at a significantly lower precision.
A closer examination of the results revealed two
sources of error ? (1) errors in source dictionary
data, and (2) correlated sense shifts in translation
circuits. Below we add two new features to our
algorithm to deal with each of these error sources,
respectively.
3.1 Errors in Source Dictionaries
In practice, source dictionaries contain mistakes
and errors occur in processing the dictionaries to
create the translation graph. Thus, existence of a
single translation circuit is only limited evidence
for a vertex as a translation. We wish to exploit
the insight that more translation circuits constitute
stronger evidence. However, the different circuits
may share some edges, and thus the evidence can-
not be simply the number of translation circuits.
We model the errors in dictionaries by assigning
a probability less than 1.0 to each edge4 (pe in the
4In our experiments we used a flat value of 0.6, chosen by
pseudo-code). We assume that the probability of
an edge being erroneous is independent of the rest
of the graph. Thus, a translation graph with pos-
sible data errors converts into a distribution over
accurate translation graphs.
Under this distribution, we can use the proba-
bility of existence of a translation circuit through a
vertex as the probability that the vertex is a trans-
lation. This value captures our insights, since a
larger number of translation circuits gives a higher
probability value.
We sample different graph topologies from our
given distribution. Some translation circuits will
exist in some of the sampled graphs, but not in
others. This, in turn, means that a given vertex v
will only be on a circuit for a fraction of the sam-
pled graphs. We take the proportion of samples in
which v is on a circuit to be the probability that v
is in the translation set. We refer to this algorithm
as Unpruned SenseUniformPaths (uSP).
3.2 Avoiding Correlated Sense-shifts
The second source of errors are circuits that in-
clude a pair of nodes sharing the same polysemy,
i.e., having the same pair of senses. A circuit
might maintain sense s? until it reaches a node that
has both s? and a distinct si. The next edge may
lead to a node with si, but not s?, causing an ex-
traction error. The path later shifts back to sense
s? at a second node that also has s? and si. An ex-
ample for this is illustrated in Figure 2(e), where
both the German and Swedish words mean feather
and spring coil. Here, Italian ?penna? means only
the feather and not the coil.
Two nodes that share the same two senses oc-
cur frequently in practice. For example, many
languages use the same word for ?heart? (the or-
gan) and center; similarly, it is common for lan-
guages to use the same word for ?silver?, the metal
and the color. These correlations stem from com-
parameter tuning on a development set of 50 inference tasks.
In future we can use different values for different dictionaries
based on our confidence in their accuracy.
265
Figure 3: The set {B, C} has a shared ambiguity - each
node has both sense 1 (from the lower clique) and sense 2
(from the upper clique). A circuit that contains two nodes
from the same ambiguity set with an intervening node not in
that set is likely to create translation errors.
mon metaphor and the shared evolutionary roots
of some languages.
We are able to avoid circuits with this type of
correlated sense-shift by automatically identifying
ambiguity sets, sets of nodes known to share mul-
tiple senses. For instance, in Figure 2(e) ?Feder?
and ?fj?der? form an ambiguity set (shown within
dashed lines), as they both mean feather and coil.
Definition 2 An ambiguity set A is a set of ver-
tices that all share the same two senses. I.e.,
?s1, s2, with s1 6= s2 s.t. ?v ? A, sense(v, s1)?
sense(v, s2), where sense(v, s) denotes that v has
sense s.
To increase the precision of our algorithm we
prune the circuits that contain two nodes in the
same ambiguity set and also have one or more in-
tervening nodes that are not in the ambiguity set.
There is a strong likelihood that the intervening
nodes will represent a translation error.
Ambiguity sets can be detected from the graph
topology as follows. Each clique in the graph rep-
resents a set of vertices that share a common word
sense. When two cliques intersect in two or more
vertices, the intersecting vertices share the word
sense of both cliques. This may either mean that
both cliques represent the same word sense, or that
the intersecting vertices form an ambiguity set. A
large overlap between two cliques makes the for-
mer case more likely; a small overlap makes it
more likely that we have found an ambiguity set.
Figure 3 illustrates one such computation.
All nodes of the clique V1, V2, A,B,C,D share
a word sense, and all nodes of the clique
B,C,E, F,G,H also share a word sense. The set
{B,C} has nodes that have both senses, forming
an ambiguity set. We denote the set of ambiguity
sets by A in the pseudo-code.
Having identified these ambiguity sets, we mod-
ify our random walk scheme by keeping track of
whether we are entering or leaving an ambiguity
set. We prune away all paths that enter the same
ambiguity set twice. We name the resulting algo-
rithm SenseUniformPaths (SP), summarized at a
high level in Algorithm 1.
Comparing Inference Algorithms Our evalua-
tion demonstrated that SP outperforms uSP. Both
these algorithms have significantly higher recall
than TRANSGRAPH algorithm. The detailed re-
sults are presented in Section 4.2. We choose SP
as our inference algorithm for all further research,
in particular to create PANDICTIONARY.
3.3 Compiling PanDictionary
Our goal is to automatically compile PANDIC-
TIONARY, a sense-distinguished lexical transla-
tion resource, where each entry is a distinct word
sense. Associated with each word sense is a list of
translations in multiple languages.
We use Wiktionary senses as the base senses
for PANDICTIONARY. Recall that SP requires two
nodes (v?1 and v
?
2) for inference. We use the Wik-
tionary source word as v?1 and automatically pick
the second word from the set of Wiktionary trans-
lations of that sense by choosing a word that is
well connected, and, which does not appear in
other senses of v?1 (i.e., is expected to share only
one sense with v?1).
We first run SenseUniformPaths to expand the
approximately 50,000 senses in the English Wik-
tionary. We further expand any senses from the
other Wiktionaries that are not yet covered by
PANDICTIONARY, and add these to PANDIC-
TIONARY. This results in the creation of the
world?s largest multilingual, sense-distinguished
translation resource, PANDICTIONARY. It con-
tains a little over 80,000 senses. Its construction
takes about three weeks on a 3.4 GHz processor
with a 2 GB memory.
Algorithm 1 S.P.(G, v?1, v
?
2,A)
1: parameters NG: no. of graph samples, NR: no. of ran-
dom walks, pe: prob. of sampling an edge
2: createNG versions ofG by sampling each edge indepen-
dently with probability pe
3: for all i = 1..NG do
4: for all vertices v : rp[v][i] = 0
5: perform NR random walks starting at v?1 (or v
?
2 ) and
pruning any walk that enters (or exits) an ambiguity
set in A twice. All walks that connect to v?2 (or v
?
1 )
form a translation circuit.
6: for all vertices v do
7: if(v is on a translation circuit) rp[v][i] = 1
8: return
?
i
rp[v][i]
NG
as the prob. that v is a translation
266
4 Empirical Evaluation
In our experiments we investigate three key ques-
tions: (1) which of the three algorithms (TG, uSP
and SP) is superior for translation inference (Sec-
tion 4.2)? (2) how does the coverage of PANDIC-
TIONARY compare with the largest existing mul-
tilingual dictionary, the English Wiktionary (Sec-
tion 4.3)? (3) what is the benefit of inference over
the mere aggregation of 631 dictionaries (Section
4.4)? Additionally, we evaluate the inference algo-
rithm on two other dimensions ? variation with the
degree of polysemy of source word, and variation
with original size of the seed translation set.
4.1 Experimental Methodology
Ideally, we would like to evaluate a random sam-
ple of the more than 1,000 languages represented
in PANDICTIONARY.5 However, a high-quality
evaluation of translation between two languages
requires a person who is fluent in both languages.
Such people are hard to find and may not even
exist for many language pairs (e.g., Basque and
Maori). Thus, our evaluation was guided by our
ability to recruit volunteer evaluators. Since we
are based in an English speaking country we were
able to recruit local volunteers who are fluent in
a range of languages and language families, and
who are also bilingual in English.6
The experiments in Sections 4.2 and 4.3 test
whether translations in a PANDICTIONARY have
accurate word senses. We provided our evalua-
tors with a random sample of translations into their
native language. For each translation we showed
the English source word and gloss of the intended
sense. For example, a Dutch evaluator was shown
the sense ?free (not imprisoned)? together with the
Dutch word ?loslopende?. The instructions were
to mark a word as correct if it could be used to ex-
press the intended sense in a sentence in their na-
tive language. For experiments in Section 4.4 we
tested precision of pairwise translations, by having
informants in several pairs of languages discuss
whether the words in their respective languages
can be used for the same sense.
We use the tags of correct or incorrect to com-
pute the precision: the percentage of correct trans-
5The distribution of words in PANDICTIONARY is highly
non-uniform ranging from 182,988 words in English to 6,154
words in Luxembourgish and 189 words in Tuvalu.
6The languages used was based on the availability of na-
tive speakers. This varied between the different experiments,
which were conducted at different times.
Figure 4: The SenseUniformPaths algorithm (SP) more
than doubles the number of correct translations at precision
0.95, compared to a baseline of translations that can be found
without inference.
lations divided by correct plus incorrect transla-
tions. We then order the translations by probabil-
ity and compute the precision at various probabil-
ity thresholds.
4.2 Comparing Inference Algorithms
Our first evaluation compares our SenseUniform-
Paths (SP) algorithm (before and after pruning)
with TRANSGRAPH on both precision and num-
ber of translations.
To carry out this comparison, we randomly sam-
pled 1,000 senses from English Wiktionary and
ran the three algorithms over them. We evalu-
ated the results on 7 languages ? Chinese, Danish,
German, Hindi, Japanese, Russian, and Turkish.
Each informant tagged 60 random translations in-
ferred by each algorithm, which resulted in 360-
400 tags per algorithm7. The precision over these
was taken as a surrogate for the precision across
all the senses.
We compare the number of translations for each
algorithm at comparable precisions. The baseline
is the set of translations (for these 1000 senses)
found in the source dictionaries without inference,
which has a precision 0.95 (as evaluated by our
informants).8
Our results are shown in Figure 4. At this high
precision, SP more than doubles the number of
baseline translations, finding 5 times as many in-
ferred translations (in black) as TG.
Indeed, both uSP and SP massively outperform
TG. SP is consistently better than uSP, since it
performs better for polysemous words, due to its
pruning based on ambiguity sets. We conclude
7Some translations were marked as ?Don?t know?.
8Our informants tended to underestimate precision, often
marking correct translations in minor senses of a word as in-
correct.
267
0.5
0.6
0.7
0.8
0.9
1
0.0 4.0 8.0 12.0 16.0
Pr
ec
is
io
n
Translations in Millions
PanDictionary
English Wiktionary
Figure 5: Precision vs. coverage curve for PANDIC-
TIONARY. It quadruples the size of the English Wiktionary at
precision 0.90, is more than 8 times larger at precision 0.85
and is almost 24 times the size at precision 0.7.
that SP is the best inference algorithm and employ
it for PANDICTIONARY construction.
4.3 Comparison with English Wiktionary
We now compare the coverage of PANDIC-
TIONARY with the English Wiktionary at varying
levels of precision. The English Wiktionary is the
largest Wiktionary with a total of 403,413 transla-
tions. It is also more reliable than some other Wik-
tionaries in making word sense distinctions. In this
study we use only the subset of PANDICTIONARY
that was computed starting from the English Wik-
tionary senses. Thus, this subsection under-reports
PANDICTIONARY?s coverage.
To evaluate a huge resource such as PANDIC-
TIONARY we recruited native speakers of 14 lan-
guages ? Arabic, Bulgarian, Danish, Dutch, Ger-
man, Hebrew, Hindi, Indonesian, Japanese, Ko-
rean, Spanish, Turkish, Urdu, and Vietnamese. We
randomly sampled 200 translations per language,
which resulted in about 2,500 tags. Figure 5
shows the total number of translations in PANDIC-
TIONARY in senses from the English Wiktionary.
At precision 0.90, PANDICTIONARY has 1.8 mil-
lion translations, 4.5 times as many as the English
Wiktionary.
We also compare the coverage of PANDIC-
TIONARY with that of the English Wiktionary in
terms of languages covered. Table 1 reports, for
each resource, the number of languages that have
a minimum number of distinct words in the re-
source. PANDICTIONARY has 1.4 times as many
languages with at least 1,000 translations at pre-
cision 0.90 and more than twice at precision 0.7.
These observations reaffirm our faith in the pan-
lingual nature of the resource.
PANDICTIONARY?s ability to expand the lists
of translations provided by the EnglishWiktionary
is most pronounced for senses with a small num-
0.75
0.8
0.85
0.9
0.95
1 2 3,4 >4
Pre
cis
ion
Avg precision 0.90
Avg precision 0.85
Polysemy of the English source word
3-4
Figure 6: Variation of precision with the degree of poly-
semy of the source English word. The precision decreases as
polysemy increases, still maintaining reasonably high values.
ber of translations. For example, at precision 0.90,
senses that originally had 3 to 6 translations are in-
creased 5.3 times in size. The increase is 2.2 times
when the original sense size is greater than 20.
For closer analysis we divided the English
source words (v?1) into different bins based on the
number of senses that English Wiktionary lists for
them. Figure 6 plots the variation of precision with
this degree of polysemy. We find that translation
quality decreases as degree of polysemy increases,
but this decline is gradual, which suggests that SP
algorithm is able to hold its ground well in difficult
inference tasks.
4.4 Comparison with All Source Dictionaries
We have shown that PANDICTIONARY has much
broader coverage than the English Wiktionary, but
how much of this increase is due to the inference
algorithm versus the mere aggregation of hundreds
of translation dictionaries in PANDICTIONARY?
Since most bilingual dictionaries are not sense-
distinguished, we ignore the word senses and
count the number of distinct (word1, word2) trans-
lation pairs.
We evaluated the precision of word-word trans-
lations by a collaborative tagging scheme, with
two native speakers of different languages, who
are both bi-lingual in English. For each sug-
gested translation they discussed the various
senses of words in their respective languages
and tag a translation correct if they found some
sense that is shared by both words. For this
study we tagged 7 language pairs: Hindi-Hebrew,
# languages with distinct words
? 1000 ? 100 ? 1
English Wiktionary 49 107 505
PanDictionary (0.90) 67 146 608
PanDictionary (0.85) 75 175 794
PanDictionary (0.70) 107 607 1066
Table 1: PANDICTIONARY covers substantially more lan-
guages than the English Wiktionary.
268
050
100
150
200
250
EW 631D PD(0.9) PD(0.85) PD(0.8)
Inferred transl. Direct transl.
Tra
nsl
ati
on
s(i
n m
illio
ns)
Figure 7: The number of distinct word-word translation
pairs from PANDICTIONARY is several times higher than the
number of translation pairs in the English Wiktionary (EW)
or in all 631 source dictionaries combined (631 D). A major-
ity of PANDICTIONARY translations are inferred by combin-
ing entries from multiple dictionaries.
Japanese-Russian, Chinese-Turkish, Japanese-
German, Chinese-Russian, Bengali-German, and
Hindi-Turkish.
Figure 7 compares the number of word-word
translation pairs in the English Wiktionary (EW),
in all 631 source dictionaries (631 D), and in PAN-
DICTIONARY at precisions 0.90, 0.85, and 0.80.
PANDICTIONARY increases the number of word-
word translations by 73% over the source dictio-
nary translations at precision 0.90 and increases it
by 2.7 times at precision 0.85. PANDICTIONARY
also adds value by identifying the word sense of
the translation, which is not given in most of the
source dictionaries.
5 Related Work
Because we are considering a relatively new prob-
lem (automatically building a panlingual transla-
tion resource) there is little work that is directly re-
lated to our own. The closest research is our previ-
ous work on TRANSGRAPH algorithm (Etzioni et
al., 2007). Our current algorithm outperforms the
previous state of the art by 3.5 times at precision
0.9 (see Figure 4). Moreover, we compile this in a
dictionary format, thus considerably reducing the
response time compared to TRANSGRAPH, which
performed inference at query time.
There has been considerable research on meth-
ods to acquire translation lexicons from either
MRDs (Neff and McCord, 1990; Helmreich et
al., 1993; Copestake et al, 1994) or from par-
allel text (Gale and Church, 1991; Fung, 1995;
Melamed, 1997; Franz et al, 2001), but this has
generally been limited to a small number of lan-
guages. Manually engineered dictionaries such as
EuroWordNet (Vossen, 1998) are also limited to
a relatively small set of languages. There is some
recent work on compiling dictionaries from mono-
lingual corpora, which may scale to several lan-
guage pairs in future (Haghighi et al, 2008).
Little work has been done in combining mul-
tiple dictionaries in a way that maintains word
senses across dictionaries. Gollins and Sanderson
(2001) explored using triangulation between alter-
nate pivot languages in cross-lingual information
retrieval. Their triangulation essentially mixes
together circuits for all word senses, hence, is un-
able to achieve high precision.
Dyvik?s ?semantic mirrors? uses translation
paths to tease apart distinct word senses from
inputs that are not sense-distinguished (Dyvik,
2004). However, its expensive processing and
reliance on parallel corpora would not scale to
large numbers of languages. Earlier (Knight and
Luk, 1994) discovered senses of Spanish words by
matching several English translations to a Word-
Net synset. This approach applies only to specific
kinds of bilingual dictionaries, and also requires a
taxonomy of synsets in the target language.
Random walks, graph sampling and Monte
Carlo simulations are popular in literature, though,
to our knowledge, none have applied these to our
specific problems (Henzinger et al, 1999; Andrieu
et al, 2003; Karger, 1999).
6 Conclusions
We have described the automatic construction of
a unique multilingual translation resource, called
PANDICTIONARY, by performing probabilistic in-
ference over the translation graph. Overall, the
construction process consists of large scale in-
formation extraction over the Web (parsing dic-
tionaries), combining it into a single resource (a
translation graph), and then performing automated
reasoning over the graph (SenseUniformPaths) to
yield a much more extensive and useful knowl-
edge base.
We have shown that PANDICTIONARY has
more coverage than any other existing bilingual
or multilingual dictionary. Even at the high preci-
sion of 0.90, PANDICTIONARY more than quadru-
ples the size of the English Wiktionary, the largest
available multilingual resource today.
We plan to make PANDICTIONARY available
to the research community, and also to the Wik-
tionary community in an effort to bolster their ef-
forts. PANDICTIONARY entries can suggest new
translations for volunteers to add to Wiktionary
entries, particularly if combined with an intelli-
gent editing tool (e.g., (Hoffmann et al, 2009)).
269
Acknowledgments
This research was supported by a gift from the
Utilika Foundation to the Turing Center at Uni-
versity of Washington. We acknowledge Paul
Beame, Nilesh Dalvi, Pedro Domingos, Rohit
Khandekar, Daniel Lowd, Parag, Jonathan Pool,
Hoifung Poon, Vibhor Rastogi, Gyanit Singh for
fruitful discussions and insightful comments on
the research. We thank the language experts who
donated their time and language expertise to eval-
uate our systems. We also thank the anynomous
reviewers of the previous drafts of this paper for
their valuable suggestions in improving the evalu-
ation and presentation.
References
E. Adar, M. Skinner, and D. Weld. 2009. Information
arbitrage in multi-lingual Wikipedia. In Procs. of
Web Search and Data Mining(WSDM 2009).
C. Andrieu, N. De Freitas, A. Doucet, and M. Jor-
dan. 2003. An Introduction to MCMC for Machine
Learning. Machine Learning, 50:5?43.
F. Bond, S. Oepen, M. Siegel, A. Copestake, and
D D. Flickinger. 2005. Open source machine trans-
lation with DELPH-IN. In Open-Source Machine
Translation Workshop at MT Summit X.
J. Carbonell, S. Klein, D. Miller, M. Steinbaum,
T. Grassiany, and J. Frey. 2006. Context-based ma-
chine translation. In AMTA.
A. Copestake, T. Briscoe, P. Vossen, A. Ageno,
I. Castellon, F. Ribas, G. Rigau, H. Rodriquez, and
A. Samiotou. 1994. Acquisition of lexical trans-
lation relations from MRDs. Machine Translation,
3(3?4):183?219.
H. Dyvik. 2004. Translation as semantic mirrors: from
parallel corpus to WordNet. Language and Comput-
ers, 49(1):311?326.
O. Etzioni, K. Reiter, S. Soderland, and M. Sammer.
2007. Lexical translation with application to image
search on the Web. In Machine Translation Summit
XI.
M. Franz, S. McCarly, and W. Zhu. 2001. English-
Chinese information retrieval at IBM. In Proceed-
ings of TREC 2001.
P. Fung. 1995. A pattern matching method for finding
noun and proper noun translations from noisy paral-
lel corpora. In Proceedings of ACL-1995.
W. Gale and K.W. Church. 1991. A Program for
Aligning Sentences in Bilingual Corpora. In Pro-
ceedings of ACL-1991.
T. Gollins and M. Sanderson. 2001. Improving cross
language retrieval with triangulated translation. In
SIGIR.
Raymond G. Gordon, Jr., editor. 2005. Ethnologue:
Languages of the World (Fifteenth Edition). SIL In-
ternational.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In ACL.
S. Helmreich, L. Guthrie, and Y. Wilks. 1993. The
use of machine readable dictionaries in the Pangloss
project. In AAAI Spring Symposium on Building
Lexicons for Machine Translation.
Monika R. Henzinger, Allan Heydon, Michael Mitzen-
macher, and Marc Najork. 1999. Measuring index
quality using random walks on the web. In WWW.
R. Hoffmann, S. Amershi, K. Patel, F. Wu, J. Foga-
rty, and D. S. Weld. 2009. Amplifying commu-
nity content creation with mixed-initiative informa-
tion extraction. In ACM SIGCHI (CHI2009).
D. R. Karger. 1999. A randomized fully polynomial
approximation scheme for the all-terminal network
reliability problem. SIAM Journal of Computation,
29(2):492?514.
K. Knight and S. Luk. 1994. Building a large-scale
knowledge base for machine translation. In AAAI.
I.D. Melamed. 1997. A Word-to-Word Model of
Translational Equivalence. In Proceedings of ACL-
1997 and EACL-1997, pages 490?497.
M. Neff and M. McCord. 1990. Acquiring lexical data
from machine-readable dictionary resources for ma-
chine translation. In 3rd Intl Conference on Theoret-
ical and Methodological Issues in Machine Transla-
tion of Natural Language.
P. Vossen, editor. 1998. EuroWordNet: A multilingual
database with lexical semantic networds. Kluwer
Academic Publishers.
270
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 131?141,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Submodularity for Data Selection in Statistical Machine Translation
Katrin Kirchhoff
Department of Electrical Engineering
University of Washington
Seattle, WA, USA
kk2@u.washington.edu
Jeff Bilmes
Department of Electrical Engineering
University of Washington
Seattle, WA, USA
bilmes@u.washington.edu
Abstract
We introduce submodular optimization
to the problem of training data subset
selection for statistical machine translation
(SMT). By explicitly formulating data
selection as a submodular program, we ob-
tain fast scalable selection algorithms with
mathematical performance guarantees, re-
sulting in a unified framework that clarifies
existing approaches and also makes both
new and many previous approaches easily
accessible. We present a new class of
submodular functions designed specifically
for SMT and evaluate them on two differ-
ent translation tasks. Our results show that
our best submodular method significantly
outperforms several baseline methods,
including the widely-used cross-entropy
based data selection method. In addition,
our approach easily scales to large data sets
and is applicable to other data selection
problems in natural language processing.
1 Introduction
SMT has made significant progress over the last
decade, not least due to the availability of increas-
ingly larger data sets. Large-scale SMT systems
are now routinely trained on millions of sentences
of parallel data, and billions of words of mono-
lingual data for language modeling. Large data
sets are often beneficial, but they do create certain
other problems. First, they place higher demands
on computational resources (storage and compute).
Hence, existing software infrastructure may need
to be adapted and optimized to handle such large
data sets. Second, experimental turn-around time
is increased as well, making it more difficult to
quickly train, fine-tune, and evaluate novel model-
ing approaches. Most importantly, however, SMT
performance does not increase linearly with the
training data size but levels off after a certain point.
This is because the additional training data may be
noisy, irrelevant to the task at hand, or inherently
redundant. Thus, a linear increase in the amount of
training data typically leads to a sublinear increase
in performance, an effect known as diminishing
returns. Several recent papers (Bloodgood and
Callison-Burch, 2010; Turchi et al., 2012a; Turchi
et al., 2012b) have amply demonstrated this effect.
A way to counteract this is to perform data sub-
set selection, i.e., choose a subset of the available
training data to optimize a particular quality cri-
terion. One scheme is to select a subset that ex-
presses as much of the information in the original
data set as possible - i.e., the data set should be
?summarized? by excluding redundant information.
Another scheme, popular in the context of SMT, is
to subselect the original training set to match the
properties of a particular test set.
In this paper, we introduce submodularity for
subselecting SMT training data, a methodology
that follows both of the above schemes.
1
Sub-
modular functions (Fujishige, 2005) are a class
of discrete set functions having the property of di-
minishing returns. They occur naturally in a wide
range of problems in a diverse set of fields includ-
ing economics, game theory, operations research,
circuit theory, and more recently machine learn-
ing. Submodular functions share certain properties
with convexity (e.g., naturalness and mathematical
tractability) although submodularity is still quite
distinct from convexity.
We present a novel class of submodular func-
tions particularly suited for SMT subselection and
evaluate it against state-of-the-art baseline meth-
ods on two different translation tasks, showing that
our method outperforms them significantly in most
cases. While many approaches to SMT data se-
lection have been developed previously (a detailed
overview is provided in Section 3), many of them
are heuristic and do not offer performance guaran-
tees. Certain previous approaches, however, have
1
As far as we know, submodularity has not before been
explicitly utilized for SMT subset selection.
131
inadvertently made use of submodular methods.
This, in addition to our own positive results, pro-
vides strong evidence that submodularity is a natu-
ral and practical framework for data subset selec-
tion in SMT and related fields.
An additional advantage of this framework is
that many submodular programs (e.g., the greedy
procedure reviewed in Section 2) are fast and
scalable to large data sets. By contrast, trying
to solve a submodular problem using, say, an
integer-linear programming (ILP) procedure,
would lead to impenetrable scalability problems.
Initial value f(X) = 2 colors in urn.
Updated value f(X?{v}) = 3 with 
added blue ball.
Initial value f(Y) = 3 colors in urn.
Updated value f(Y?{v}) = 3 with 
added blue ball.
X
Y
v v
Figure 1: f (Y ) measures the number of distinct col-
ors in the set of balls Y , and hence is submodular.
This paper makes several contributions: First, we
present a brief overview of submodular functions
(Section 2) and their potential application to natural
language processing (NLP). Next we review pre-
vious approaches to MT data selection (Section 3)
and analyze them with respect to their submodular
properties. We find that some previous approaches
are submodular in nature although this connection
was not heretofore made explicit. Section 4 details
our new approach. We discuss desirable properties
of an SMT data selection objective and present a
new class of submodular functions tailored towards
this problem. Section 5 presents the data and
systems used for the experiments, and results are
reported in Section 6. Section 7 then concludes.
2 Submodular Functions/Optimization
Submodular functions (Edmonds, 1970; Fujishige,
2005), are widely used in mathematics, economics,
circuit theory (Narayanan, 1997), and operations
research. More recently, they have attracted much
interest in machine learning (e.g., (Narasimhan
and Bilmes, 2004; Kolmogorov and Zabih, 2004;
Krause et al., 2008; Krause and Guestrin, 2011;
Jegelka and Bilmes, 2011; Iyer and Bilmes, 2013)),
where they have been applied to a variety of prob-
lems. In natural language and speech processing,
they have been applied to document summariza-
tion (Lin and Bilmes, 2011; Lin and Bilmes, 2012)
and speech data selection (Wei et al., 2013).
We are given a finite size-n set of objects V (i.e.,
|V |= n). A valuation function f : 2
V
? R
+
is de-
fined that returns a non-negative real value for any
subset X ?V . The function f is said to be submodu-
lar if it satisfies the property of diminishing returns:
namely, for all X ? Y and v /? Y , we must have:
f (X ?{v})? f (X)? f (Y ?{v})? f (Y ). (1)
This means that the incremental value (or gain) of
element v decreases when the context in which v
is considered grows from X to Y ? X . We define
the ?gain? as f (v|X), f (X ?{v})? f (X). Hence,
f is submodular if f (v|X)? f (v|Y ). We note that
a function m : 2
V
? R
+
is said to be modular
if it satisfies the above with equality, meaning
m(v|X) = m(v|Y ) for all X ? Y ? V \ {v}. If m
is modular and m( /0) = 0, it can be written as
m(X) =
?
x?X
m(x) and, moreover, is seen simply
as a n-dimensional vector m ? R
V
.
As an example, suppose we have a set V of balls
and f (X) counts the number of colors present
in any subset X ? V . In Figure 1, |X | = 5 and
f (X) = 2, |Y | = 7 and f (Y ) = 3, and X ? Y .
Adding v (a blue ball) to X has a unity gain
f (v|X) = 1 but since a blue ball exists in Y , we
have f (v|Y ) = 0 < f (v|X) = 1.
Submodularity is a natural model for data subset
selection in SMT. In this case, each v ? V is a
distinct training data sentence and V corresponds
to a training set. An important characteristic of
any good model for this problem is that we wish
to decrease the ?value? of a sentence v ?V based
on how much that sentence has in common with
those sentences, say X , that have already been
chosen. The value f (v|X) of a given sentence
v in a context of previously chosen sentences
X ? V further diminishes as the context grows
Y ? X . When, for example, a sentence?s value is
represented as the value of its set of features (e.g.,
n-grams), it is natural for those features? values to
be discounted based on how much representation
of those features already exists in a previously
chosen subset. This corresponds to submodularity,
which can easily be expressed mathematically by
functions such as Eqn. (4) below.
Not only are submodular functions natural for
SMT subset selection, they can also be optimized
efficiently and scalably such that the result has
mathematical performance guarantees. In the re-
mainder of this paper we will assume that f is not
only submodular, but also non-negative ( f (X)? 0
for all X), and monotone non-decreasing ( f (X)?
f (Y ) for all X ? Y ). Such functions are trivial to
uselessly maximize, since f (V ) is the largest possi-
ble valuation. Typically, however, we wish to have
132
Algorithm 1: The Greedy Algorithm
1 Input: Submodular function f : 2
V
? R
+
,
cost vector m, budget b, finite set V .
2 Output: X
k
where k is the number of
iterations.
3 Set X
0
? /0 ; i? 0 ;
4 while m(X
i
)< b do
5 Choose v
i
as follows:
v
i
?
{
argmax
v?V\X
i
f ({v}|X
i
)
m(v)
}
;
6 X
i+1
? X
i
?{v
i
} ; i? i+1 ;
a valuable subset of bounded and small cost, where
cost is measured based on a modular function m(X).
For example, the cost m(v) of a sentence v ? V
might be its length, so m(X) =
?
x?X
m(x) is a sum
of sentence lengths. This leads to the following
optimization problem:
X
?
? argmax
X?V,m(X)?b
f (X), (2)
where b is a known budget. Solving this problem
exactly is NP-complete (Feige, 1998), and express-
ing it as an ILP procedure renders it impractical for
large data sizes. When f is submodular the cost is
just size (m(X) = |X |), then the simple greedy algo-
rithm (detailed below) will have a worst-case guar-
antee of f (
?
X
?
) ? (1? 1/e) f (X
opt
) ? 0.63 f (X
opt
)
where X
opt
is the optimal and
?
X
?
is the greedy so-
lution (Nemhauser et al., 1978).
This constant factor guarantee has practical im-
portance. First, a constant factor guarantee stays
the same as n grows, so the relative worst-case qual-
ity of the solution is the same for small and for big
problem instances. Second, the worst-case result
is achieved only by very contrived and unrealistic
function instances ? the typical case is almost al-
ways much better. Third, the worst-case guarantee
improves depending on the ?curvature? ? ? [0,1]
of the submodular function (Conforti and Cornue-
jols, 1984). When the submodular function is not
fully curved (? < 1, something true of the func-
tions used in this paper), the worst case guarantee
is better, namely
1
?
(1?e
??
) (e.g., a function f with
? = 0.2 has a worst-case guarantee of 0.91). Lastly,
when the cost m is not just cardinality but an arbi-
trary non-negative modular function, a greedy al-
gorithm has similar guarantees (Sviridenko, 2004),
and a scalable variant has a worst-case guarantee
of 1?1/
?
e (Lin and Bilmes, 2010).
The basic greedy algorithm has a very simple
form. Starting with X ? /0, we repeat the operation
X ? X ? argmax
v?V\X
f (v|X)
m(v)
until the budget is
exceeded (m(X) > b) and then backoff to the
previous iteration (complete details are given in
Algorithm 1). While the algorithm has complexity
O(n
2
), there is an accelerated instance of this
algorithm (Minoux, 1978; Leskovec et al., 2007)
that has empirical computational complexity of
O(n logn) where n = |V |. The greedy algorithm,
therefore, scales practically to very large n.
Recently, still much faster (Wei et al., 2014) and
also parallel distributed (Mirzasoleiman et al.,
2013) greedy procedures have been advanced
offering still better scalability.
There are many submodular functions that
are appropriate for subset selection (Lin and
Bilmes, 2011; Lin and Bilmes, 2012). Some
of them are graph-based, where we are given a
non-negative weighted graph G = (V,E,w) and
w : E?R
+
is a set of edge weights (i.e., w(x,y) is
a non-negative similarity score between sentences
x and y). A submodular function is obtained via
a graph cut function f (X) =
?
x?X ,y?V\X
w(x,y)
or via a monotone truncated graph cut
function f (X) =
?
v?V
min(C
v
(X),?C
v
(V ))
where ? ? (0,1) is a scalar parameter and
C
v
(X) =
?
x?X
w(v,x) is a v-specific modular
function. Alternatively, the class of facility loca-
tion functions f (X) =
?
v?V
max
x?X
w(x,v) have
been widely and successfully used in the field of
operations research, and are also applicable here.
In the worse case, the required graph construc-
tion has a worst-case complexity of O(n
2
). While
sparse graphs can be used, this can be prohibitive
when n = |V | gets large. Another class of sub-
modular functions that does not have this prob-
lem is based on a weighted bipartite graph G =
(V,U,E,w) where V are the left vertices, U are the
right vertices, E ? V ?U is a set of edges, and
w : U?R
+
is a set of non-negative weights on the
vertices U . For X ?V , the bipartite neighborhood
function is defined as:
f (X) = w({u ?U : ?x ? X with (x,u) ? E}) (3)
This function is interesting for NLP applications
since U can be seen as a set of ?features? of the ele-
ments v?V (i.e., if V is a set of sentences, U can be
the collective set of n-grams for multiple values of
n, and f (X) is the weight of the n-grams contained
collectively in sentences X).
2
Given a set X ? V ,
2
To be consistent with standard notation in previous liter-
ature, we overload the use of n in ?n-grams? and the size of
our set ?n = |V |?, even though the two ns have no relationship
with each other.
133
we get value from the features of the elements
x ? X , but we get credit for each feature only one
time ? once a given object x ? X has a given fea-
ture u?U , any additions to X by elements also hav-
ing feature u offer no further credit via that feature.
Another interesting class of submodular func-
tions, allowing additional credit from an element
even when its features already exist in X , are what
we call feature-based submodular functions. They
involve sums of non-decreasing concave functions
applied to modular functions (Stobbe and Krause,
2010) and take the following form:
f (X) =
?
u?U
w
u
?
u
(m
u
(X)) (4)
where w
u
> 0 is a feature weight, m
u
(X) =
?
x?X
m
u
(x) is a non-negative modular function
specific to feature u, m
u
(x) is a relevance score (a
non-negative scalar score indicating the relevance
of feature u in object x), and ?
u
is a u-specific
non-negative non-decreasing concave function.
The gain is f (v|X) =
?
u?U
(
?(m
u
(X ? {v}))?
?(m
u
(X))
)
, and thanks to ?
u
?s concavity, the
term ?(m
u
(X ?{v}))??(m
u
(X)) for each feature
u ?U is decaying as X grows. The rate of decay,
and hence the degree of diminishing returns and
ultimately the measure of redundancy of the
information provided by the feature, is controlled
by the concave function. The rate of decay is
also related to the curvature ? of the submodular
function (c.f. ?2), with more aggressive decay
having higher curvature (and a worse worst-case
guarantee). The decay is a modeling choice that
should be decided based on a given application.
Feature-based functions have the advantage that
they do not require the construction of a pairwise
graph; they have a cost of only O(n|U |), which is
linear in the data size and therefore scalable to
large data set sizes.
We utilize this class for our subset selection ex-
periments described in Section 4, where we use one
global concave function ?
u
= ? for all u ?U . In
this work we chose one particular set of features U .
However, given the large body of research into NLP
feature engineering (Jurafsky and Martin, 2009),
this class is extensible beyond just this set, which
makes it suitable for many other NLP applications.
Before describing our SMT-specific functions in
detail, we review previous work on subset selection
for SMT in the context of submodularity.
3 Previous Approaches
There have been many previous approaches to data
subset selection in SMT. In this section, we show
that some of them in fact correspond to submodular
methods, thus introducing a connection between
submodularity and the practical problem of SMT
data selection. The fact that submodularity is
implicitly and unintentionally used in previous
work suggests that it is natural for this problem.
A currently widely-used data selection method in
SMT (which we also use as a baseline in Section 6)
uses the cross-entropy between two language mod-
els (Moore and Lewis, 2010), one trained on the
test set of interest, and another trained on a large set
of generic or out-of-domain training data. We call
this the cross-entropy method. This method trains
a test-set specific (or in-domain) language model,
LM
in
, and a generic (out-of- or mixed-domain) lan-
guage model, LM
out
. Each sentence x ? V in the
training data is given a probability score with both
language models and then ranked in descending
order based on the log ratio
m
ce
(x) =
1
`(x)
log[Pr(x|LM
in
)/Pr(x|LM
out
)] (5)
where `(x) is the length of sentence x. Finally, the
top N sentences are chosen. In (Axelrod et al.,
2011) this method is extended to take both sides
of the parallel corpus into account rather than just
the source side. The cross-entropy approach values
each sentence individually, without regard to any in-
teraction with already selected sentences. This ap-
proach, therefore, is modular (a special case of sub-
modular) and values a set X via m(X) =
?
x?X
m(x).
Moreover, the thresholding method for choosing
a subset corresponds exactly to the optimization
problem in Eqn. (2) where f ? m and the budget
b is set to the sum of the top N sentence scores.
Thanks to modularity, the problem is no longer NP-
complete, and the threshold method solves Eqn. (2)
exactly. On the other hand, a modular function
does not have the diminishing returns property, and
thus has no chance to represent interaction or re-
dundancy between sentences. The chosen subset,
therefore, might have an enormous overrepresenta-
tion of one aspect of the training data while having
minimal or no representation of another aspect, a
major vulnerability of this approach.
Other methods use information retrieval (Hilde-
brand et al., 2005; L?u et al., 2007) which can also
be described as modular function optimization
(e.g., take the top k scoring sentences). Duplicate
134
sentence removal is easily represented by a feature-
based submodular function, Equation (4), where
there is one sentence-specific feature per sentence
and where ?
u
(m
u
(X)) = min(|X ?{u}|,1) ? once
a sentence is chosen, its contribution is saturated
so any duplicate sentence has a gain of zero. Also,
the unseen n-gram function of (Eck et al., 2005;
Bloodgood and Callison-Burch, 2010) corresponds
to a bipartite neighborhood submodular function,
with a weight function defined based on n-gram
counts. Moreover their functions are optimized
using the greedy algorithm; hence they in fact
have a 1? 1/e guarantee. Other methods have
noted and dealt with the existence of redundancy
in phrase-based systems (Ittycheriah and Roukos,
2007) by limiting the set of phrases ? submodular
optimization inherently removes redundancy. Also,
(Callison-Burch et al., 2005; Lopez, 2007) involve
modular functions but where selection is over
subsets of phrases (rather than sentences as in our
current work) and where multiple selections occur,
each specific to an individual test set sentence
rather than the entire test set.
In the feature-decay method, presented in (Bic?ici,
2011; Bic?ici and Yuret, 2011; Bic?ici, 2013), the
value of a sentence is based on its decomposition
into a set of feature values. As sentences are added
to a set, the feature decay approach in general di-
minishes the value of each feature depending on
how much of that feature has already been covered
by those sentences previously chosen ? the pa-
pers define a set of feature decay functions for this
purpose.
Our analysis of (Bic?ici, 2011; Bic?ici and Yuret,
2011; Bic?ici, 2013), from the perspective of sub-
modularity, has revealed an interesting connection.
The feature decay functions used in these papers
turn out to be derivatives of non-decreasing con-
cave functions. For example, in one case ?
?
(a) =
1/(1+ a) which is the derivative of the concave
function ?(a) = ln(1+a). We are given a constant
initialization w
u
for feature u ?U ? in the papers,
they set either w
u
? 1, or w
u
? log(m(V )/m
u
(V )),
or w
u
? log(m(V )/(1+m
u
(V ))), where m(V ) =
?
u
m
u
(V ), and where m
u
(X) =
?
x?X
m
u
(x) is the
count of feature u within the set of sentences
X ?V . This yields the submodular feature function
f
u
(X) = w
u
?(m
u
(X)). The value of sentence v as
measured by feature u in the context of X is the gain
f
u
(v|X), which is a discrete derivative correspond-
ing to w
u
/(1+m
u
(X ?{v})). An alternative decay
function they define is given as ?
?
(a) = 1/(1+b
a
)
for a base b (they set b? 2) which is the derivative
of the following non-decreasing concave function:
?(a) =
[
1?
1
ln(b)
ln
(
1+ exp
(
?a ln(b)
)
)]
(6)
We note that this function is saturating, meaning
that it quickly reaches its asymptote at its maxi-
mum possible value. We can, once again, define
a function specific for feature u ?U as f
u
(X) =
w
u
?(m
u
(X)) with a gain f
u
(v|X) being a discrete
derivative corresponding to w
u
/(1+b
m
u
(X?{v})
).
The connection between this work and submod-
ularity is not complete, however, without consider-
ing the method used for optimization. In fact, Algo-
rithm 1 of (Bic?ici and Yuret, 2011) is precisely the
accelerated greedy algorithm of (Minoux, 1978)
applied to the submodular function corresponding
to f (X) =
?
u?U
f
u
(X), and Algorithm 1 of (Bic?ici,
2013) is the cost-normalized variant of this greedy
algorithm corresponding to a knapsack constraint
(Sviridenko, 2004). Thus, our analysis shows that
these methods also have a 1? 1/e performance
guarantee and also the O(n logn) empirical com-
plexity mentioned in Section 2. This is an impor-
tant connection, as it furthers the evidence that
submodularity is natural for the problem of SMT
subset selection. This also increases the accessibil-
ity of this method since we may view it as a special
case of Equation (4).
Another class of approaches focuses on active
learning. In (Haffari et al., 2009) a large corpus
of noisy parallel data is created automatically; a
smaller set of samples is then selected from this
set that receive human translations. A combination
of several ?informativeness? scores is computed
on a sentence-level basis, and samples are selected
via hierarchical adaptive sampling (Dasgupta and
Hsu, 2008). In (Mandal et al., 2008) a measure
of disagreement between different MT systems, as
well as an entropy-based criterion are used to select
additional data for annotation. In (Bloodgood and
Callison-Burch, 2010) and (Ambati et al., 2010),
active learning is combined with crowd-sourced an-
notations to produce large, human-translated data
sets that are as informative as possible. In (Cao
and Khudanpur, 2012), samples are selected for
discriminative training of an MT system accord-
ing to a greedy algorithm that tries to maximize
overall quality. These methods address a differ-
ent scenario (data selection for annotation or dis-
criminative training) than the one considered here;
however, we also note that the actual selection tech-
niques employed in these papers do not appear to
be submodular.
135
4 Novel Submodular Functions for SMT
In this section, we design a parameterized class
of submodular functions useful for SMT training
data subset selection. By staying within the realm
of submodularity, we retain the advantages of the
greedy algorithm, its theoretical performance as-
surances, and its scalability properties. At the same
time this opens the door to a general framework for
quickly exploring a much larger class of functions
(with the same desirable properties) than before.
It is important to note that we are using sub-
modularity as a ?model? of the selection process,
and the submodular objective acts as a surrogate
for the actual SMT objective function. Thus, the
mathematical guarantee we have is in terms of the
surrogate objective rather than the true SMT ob-
jective. Evaluating one point of the actual SMT
objective would require the complete training and
testing of an SMT system, so even an algorithm as
efficient as Algorithm 1 would be infeasible, even
on small data. It is therefore important to design a
natural and scalable surrogate objective.
We do not consider the graph-based functions
discussed in Section 2 here since they require a
pairwise similarity matrix over all training sen-
tences and thus have O(n
2
) worst-case complexity.
For large tasks with millions or even billions of
sentences, this eventually becomes impractical.
Instead we focus on feature-based functions of the
type presented in Eqn. (4), where each sentence
is represented as a set of features rather than as a
vertex in a graph. In this function there are four
components to specify: 1) U , the linguistic feature
set; 2) m
u
(x), the relevance scores for each feature
u and sentence x; 3) w
u
, the feature weights; and
4) ? , the concave function (we use one concave
function, so ?
u
= ? for all u ?U).
Feature set: U is the set of n-grams from either
the source language U
src
, or from both the source
and target language U
src
?U
tgt
(see Section 6);
since we are interested in selecting a training set
that matches a given test set, we use the set of n-
grams that occur both in the training set and in
the development/test data (for target features, only
development set features are used). I.e., U
src
=
(U
src
dev
?U
src
test
)?U
src
train
and U
tgt
=U
tgt
dev
?U
tgt
train
.
Relevance scores: A feature u within a sentence
x should be valued based on how salient that fea-
ture is within the ?document? in which it occurs;
here, the ?document? is the set of training sen-
tences. This is a task well suited to TFIDF. As
an alternative to raw feature counts we thus also
consider scores of the form m
u
(x)? tfidf(u,x) =
tf(u,x)? idf
trn
(u), where tf(u,x) and idf
trn
(u) are
defined as usual.
Feature weights: We wish to select those training
samples that contain features occurring frequently
in the test data while avoiding the over-selection
of features that are very frequent in the training
data because those are likely to be translated
correctly anyway. This is similar to the approach
in (Moore and Lewis, 2010) (see Equation (5)),
where a log-probability ratio of in-domain to
out-of-domain language model is utilized. In the
present case, we need a value that is specific to
feature u ?U ; a natural approach is to use the ratio
of counts c
tst
(u)/c
trn
(u) where c
tst
(u) is the raw
count of u in the development/test data, and c
trn
(u)
is its raw count in the training data (note that
c
trn
(u) is never zero due to the way U is defined).
As an additional factor we allow feature length
to have an influence. In general, longer n-grams
might be considered more valuable since they
typically lead to better translations and are more
relevant for BLEU. Thus, we include a reward
term for longer n-grams in the form of ?
|u|
where
? ? 1 and |u| is the length of feature u. This gives
greater weight to longer n-grams when ? > 1.
Concave function: It is imperative to find the right
form of concave function since, as described in Sec-
tion 2, the concave shape determines the degree to
which redundancy and diminishing returns are rep-
resented. Intuitively, when the shape of the concave
function for a feature becomes ?flat? rapidly, that
feature quickly looses its ability to provide addi-
tional value to a candidate subset. Many different
concave functions were tested for ? , including one
of the two functions implicit in (Bic?ici and Yuret,
2011) and derived in Section 3, and a variety of
roots of the form ?(a) = a
?
for 0 < ? < 1. In
Table 2, for example, we find evidence that the
simple square root ?(a) =
?
a performs slightly
better than the log function. The square root is
much less curved and decays much more gradually
than either of the two functions implicit in (Bic?ici
and Yuret, 2011), of which one is a log form and
the other is even more curved and quickly satu-
rates (see ?3). The square root function yields a
less curved submodular function, in the sense of
(Conforti and Cornuejols, 1984), resulting in better
worst-case guarantees. Indeed, Table 1 in (Bic?ici
and Yuret, 2011) corroborates by showing that the
more curved saturating function does worse than
the less curved log function.
Four Components Together: Different instantia-
136
tions of the four components discussed above will
result in different submodular functions of the gen-
eral class defined in Eqn. (4). Particular settings
of these general parameters produce the methods
considered in (Bic?ici and Yuret, 2011), thus mak-
ing that approach easily accessible once the general
submodular framework is set up. As a very special
case, this is also true of the cross-entropy method
(Moore and Lewis, 2010), where |U |= 1, m
u
(x)?
exp(m
ce
(x)) of Equation (5)
3
, w
u
? 1, and ?(a) =
a is the identity function. In Section 6, we specify
the parameter settings used in our experiments.
Task Train Dev Test LM
NIST 189M 48k 49k 2.5B
Europarl 52.8M 57.7k 58.1k 53M
Table 1: Data set sizes (number of source-side
words) for MT tasks. LM = language model data.
5 Data and Systems
We evaluate our approach on the NIST Arabic-
English translation task, using the NIST 2006 set
for development and the NIST 2009 set for eval-
uation. The training data consists of all Modern
Standard Arabic-English parallel LDC corpora per-
mitted in the NIST evaluations (minus the restricted
time periods). Together these sets form a mixed-
domain training set containing relevant in-domain
data similar to the NIST data sets but also less rele-
vant data (e.g., the UN parallel corpora); we thus
expect data selection to work well on this task. Ad-
ditional English language modeling data was drawn
from several other LDC corpora (English Giga-
word, AQUAINT, HARD, ANC/DCI and the Amer-
ican National Corpus). Preprocessing included con-
version of the Arabic data to Buckwalter format,
tokenization, spelling normalization, and morpho-
logical segmentation using MADA (Habash et al.,
2009). Numbers and URLs were replaced with
variables. The English data was tokenized and
lowercased. Postprocessing involved recasing the
translation output, replacing variable names with
their original corresponding tokens, and normal-
izing spelling and stray punctuation marks. The
recasing model is an SMT system without reorder-
ing, trained on parallel cased and lowercased ver-
sions of the training data. The recasing model re-
mains fixed for all experiments and is not retrained
3
Due to modularity, any monotone increasing transforma-
tion from m
ce
(x) to m
u
(x) that ensures m
u
(x)? 0 is equivalent.
for different sizes of the training data. Evalua-
tion follows the NIST guidelines and was done by
computing BLEU scores using the official NIST
evaluation tool mteval-v13a.pl with the ?c flag
for case-sensitive scoring. In addition to the NIST
task we also applied our method to the Europarl
German-English translation task. The training data
comes from the Europarl-v7 collection
4
; the devel-
opment set is the 2006 dev set, and the test set is the
2007 test set. The number of reference translations
is 1. The German data was preprocessed by tok-
enization, lower-casing, splitting noun compounds
and lemmatization to address morphological vari-
ation in German. The English side was tokenized
and lowercased. Evaluation was done by comput-
ing BLEU on the lowercased versions of the data.
Since test and training data for this task come from
largely the same domain we expect the training
data to be less redundant or irrelevant; nevertheless
it will be interesting to see how much different data
selection methods can contribute even to in-domain
translation tasks. The sizes of the various data sets
are shown in Table 1.
All translation systems were trained using the
GIZA++/Moses infrastructure (Koehn et al., 2007).
The translation model is a standard phrase-based
model with a maximum phrase length of 7. Since a
large number of experiments had to be run for this
study, more complex hierarchical or syntax-based
translation models were deliberately excluded in
order to limit the experimental turn-around time
needed for each experiment. The reordering model
is a hierarchical model according to (Galley and
Manning, 2008). The feature weights in the log-
linear function were optimized on the development
set BLEU score using minimum error-rate training.
The language models for the NIST task (5-grams)
were trained on three different data sources (Gi-
gaword, GALE data, and all remaining corpora),
which were then interpolated into a single model.
The interpolation weights were optimized sepa-
rately for the two different genres present in the
NIST task (newswire and web text). All models
used Witten-Bell discounting and interpolation of
higher-order and lower-order models. Language
models remain fixed for all experiments, i.e., the
language model training data is not subselected
since we were interested in the effect of data subset
selection on the translation model only. The lan-
guage model for the Europarl system was a 5-gram
trained on Europarl data only.
4
http://http://www.statmt.org/europarl/
137
Method Data Subset Sizes
10% 20% 30% 40%
Rand 0.3991 (? 0.004) 0.4142 (? 0.003) 0.4205 (? 0.002) 0.4220 (? 0.002)
Xent 0.4235 (? 0.004) 0.4292 (? 0.002) 0.4290 (? 0.003) 0.4292 (? 0.001)
SM-1 0.4309 (? 0.000) 0.4367 (? 0.001) 0.4330 (? 0.004) 0.4351 (? 0.002)
SM-2 0.4330
?
(? 0.001) 0.4395
?
(? 0.003) 0.4333 (? 0.001) 0.4366
?
(? 0.003)
SM-3 0.4313
?
(? 0.002) 0.4338 (? 0.002) 0.4361
?
(? 0.002) 0.4351 (? 0.003)
SM-4 0.4276 (? 0.003) 0.4303 (? 0.002) 0.4324 (? 0.002) 0.4329 (? 0.000)
SM-5 0.4285 (? 0.004) 0.4356 (? 0.002) 0.4333 (? 0.003) 0.4324 (? 0.002)
SM-6 0.4302
?
(? 0.004) 0.4334 (? 0.003) 0.4371
?
(? 0.002) 0.4349 (? 0.003)
SM-7 0.4295 (? 0.002) 0.4374 (? 0.002) 0.4344 (? 0.001) 0.4314 (? 0.0004)
SM-8 0.4304
?
(? 0.002) 0.4323 (? 0.000) 0.4358 (? 0.003) 0.4337 (? 0.001)
100% 0.4257
Table 2: BLEU scores (standard deviations) on the NIST 2009 (Ara-En) test set for random (Rand),
cross-entropy (Xent), and submodular (SM) data selection methods defined in Table 4. 100% = system
using all of the training data. Boldface numbers indicate a statistically significant improvement (p? 0.05)
over the median Xent system. Starred scores are also significantly better than SM-5.
Method Data Subset Sizes
10% 20% 30% 40%
Rand 0.2590 (? 0.003) 0.2652 (? 0.001) 0.2677 (? 0.002) 0.2697 (? 0.001)
Xent 0.2639 (? 0.002) 0.2687 (? 0.002) 0.2704 (? 0.001) 0.2723 (? 0.001)
SM-5 0.2653 (? 0.001) 0.2727 (? 0.000) 0.2697 (? 0.002) 0.2720 (? 0.002)
SM-6 0.2697? (? 0.001) 0.2700 (? 0.002) 0.2740? (? 0.002) 0.2723 (? 0.000)
100% 0.2651
Table 3: BLEU scores (standard deviation) on the Europarl translation task for random (Rand), cross-
entropy (Xent), and submodular (SM) data selection methods. 100% = system using all of the training
data. Boldface numbers indicate a statistically significant improvement (p? 0.05) over the median Xent
system. Starred scores are significantly better than SM-5.
6 Experiments
Function parameters
w(u) ?(a) m
u
(x) U
SM-1
c
tst
(u)
c
trn
(u)
?
|u|
?
a tfidf(u,x) U
src
SM-2
?
c
tst
(u)
c
trn
(u)
?
|u|
?
a tfidf(u,x) U
src
?U
tgt
SM-3
c
tst
(u)
c
trn
(u)
?
|u|
?
a c(u,x) U
src
SM-4 c
tst
(u)
?
a tfidf(u,x) U
src
SM-5 1 ln(1+a) c(u,x) U
src
SM-6
?
c
tst
(u)
c
trn
(u)
?
a tfidf(u,x) U
src
SM-7
c
tst
(u)
c
trn
(u)
?
(a) tfidf(u,x) U
src
?U
tgt
SM-8
c
tst
(u)
c
trn
(u)
ln(1+a) tfidf(u,x) U
src
?U
tgt
Table 4: Different instantiations of the general sub-
modular function in Eq. 4 (? = 1.5 in all cases).
We first trained a baseline system on 100% of
the training data. Different data selection methods
were then used to select subsets of 10%, 20%, 30%
and 40% of the data. While not reported in the
tables, above 40%, the performance slowly drops
to the 100% performance.
The first baseline selection method utilizes ran-
dom data selection, for which 3 different data sets
of the specified size were drawn randomly from
the training data. Individual systems were trained
on all random subsets of the same size, and their
scores were averaged. The second baseline is
the cross-entropy method by (Moore and Lewis,
2010). In-domain language models were trained on
the combined development and test data, and out-
of-domain models were trained on an equivalent
amount of data drawn randomly from the training
set. Sentences were ranked by the function in Eq. 5,
and the top k percent were chosen. The order of the
n-gram models was optimized on the development
set and was found to be 3. Larger model orders
resulted in worse performance, possibly due to the
138
limited size of the data used for their training. Since
this method also involves random data selection,
we report the average BLEU score over 5 different
trials. For the submodular selection method, Ta-
ble 4 shows the different values that were tested for
the four components listed in Section 4. The combi-
nation was optimized on the development set. The
selection algorithm (Alg. 1) runs within a few min-
utes on our complete training set of 189M words.
Results on the NIST 2009 test set are shown in
Table 2. The scores for the submodular systems
are averages over 3 different runs of MERT tuning.
Random data subset selection (Row 1) falls short
of the baseline system using 100% of the training
data. The cross-entropy method (Row 2) surpasses
the performance of the baseline system at about
20% of the data, demonstrating that data subset
selection is a suitable technique for such mixed-
domain translation tasks. The following rows show
results for the various submodular functions shown
in Table 4. Out of these, SM-5 corresponds to the
best approach in (Bic?ici and Yuret, 2011). SM-6
is our own best-performing function, beating the
cross-entropy method by a statistically significant
margin (p? 0.05) under all conditions.
5
SM-6 is
also significantly better than SM-5 in two cases.
Finally, it surpasses the performance of the all-data
system at only 10% of the training data; possibly,
even smaller training data sets could be used
but this option was not investigated. While the
bilingual submodular functions SM-2 and SM-7)
yield an improvement of up to 0.015 BLEU points
on the dev set (not shown in the table), they do not
consistently outperform the monolingual functions
on the test set. Since test set target features cannot
be used in our scenario, bilingual features are
only helpful to the extent that the development set
closely matches the test set. However, target fea-
tures should be quite helpful when selecting data
from an out-of-domain set to match an in-domain
training set (as in e.g. (Axelrod et al., 2011)). We
found no gain from the length reward ?
|u|
.
The Europarl results (Table 3) show a similar
pattern. Although the differences in BLEU scores
are smaller overall (as expected on an in-domain
translation task), data subset selection improves
over the all-data baseline system in this case as
well. The cross-entropy method again outperforms
random data selection. On this task we only tested
our submodular function that worked best on the
5
Statistical significance was measured using the paired
bootstrap resampling test of (Koehn, 2004), applied to the
systems with the median BLEU scores.
NIST task; again we find that it outperforms the
cross-entropy method. In two conditions (10% and
30%) these differences are statistically significant.
10% of the training data suffices to outperform the
all-data system, and up to a full BLEU point can be
gained on this task using 20-30% of the data and a
submodular data selection method.
7 Conclusions
We have introduced submodularity to SMT data
subset selection, generalizing previous approaches
to this problem. Our method has theoretical perfor-
mance guarantees, comes with scalable algorithms,
and significantly improves over current, widely-
used data selection methods on two different trans-
lation tasks. There are many possible extensions
to this work. One strategy would be to extend the
feature set U with features representing different
types of linguistic information - e.g., when using
a syntax-based system it might be advantageous
to select training data that covers the set of syn-
tactic structures seen in the test data. Secondly,
the selected data was test data specific. In some
contexts, it is not possible to train test data spe-
cific systems dynamically; in that case, different
submodular functions could be designed to select
a representative ?summary? of the training data.
Finally, the use of submodular functions for subset
selection is applicable to other data sets that can
be represented as features or as a pairwise similar-
ity graph. Submodularity thus can be applied to a
wide range of problems in NLP beyond machine
translation.
Acknowledgments
This material is based on research sponsored by
Intelligence Advanced Research Projects Activity
(IARPA) under agreement number FA8650-12-2-
7263, and is also supported by the National Science
Foundation under Grant No. (IIS-1162606), and by
a Google, a Microsoft, and an Intel research award.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright notation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies or
endorsements, either expressed or implied, of In-
telligence Advanced Research Projects Activity
(IARPA) or the U.S. Government.
139
References
[Ambati et al.2010] V. Ambati, S. Vogel, and J. Car-
bonell. 2010. Active learning and crowd-sourcing
for machine translation. In Proceedings of LREC,
pages 2169?2174, Valletta, Malta.
[Axelrod et al.2011] A. Axelrod, X. He, and J. Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP, pages 355?
362, Edinburgh, Scotland.
[Bic?ici and Yuret2011] E. Bic?ici and D. Yuret. 2011.
Instance selection for machine translation using fea-
ture decay algorithms. In Proceedings of the 6th
Workshop on Statistical Machine Translation, pages
272?283.
[Bic?ici2013] E. Bic?ici. 2013. Feature decay algorithms
for fast deployment of accurate statistical machine
translation systems. In Proceedings of the 8th Work-
shop on Statistical Machine Translation, pages 78?
84.
[Bic?ici2011] E. Bic?ici. 2011. The Regression Model of
Machine Translation. Ph.D. thesis, KOC? University.
[Bloodgood and Callison-Burch2010] M. Bloodgood
and C. Callison-Burch. 2010. Bucking the trend:
large-scale cost-focused active learning for statis-
tical machine translation. In Proceedings of ACL,
pages 854?864.
[Callison-Burch et al.2005] Chris Callison-Burch,
Colin Bannard, and Josh Schroeder. 2005. Scaling
phrase-based statistical machine translation to larger
corpora and longer phrases. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 255?262. Association for
Computational Linguistics.
[Cao and Khudanpur2012] Y. Cao and S. Khudanpur.
2012. Sample selection for large-scale MT discrimi-
native training. In Proceedings of AMTA.
[Conforti and Cornuejols1984] M. Conforti and G. Cor-
nuejols. 1984. Submodular set functions, matroids
and the greedy algorithm: tight worst-case bounds
and some generalizations of the Rado-Edmonds the-
orem. Discrete Applied Mathematics, 7(3):251?
274.
[Dasgupta and Hsu2008] S. Dasgupta and D. Hsu.
2008. Hierarchical sampling for active learning. In
Proceedings of ICML.
[Eck et al.2005] M. Eck, S. Vogel, and A. Waibel. 2005.
Low cost portability for statistical machine transla-
tion based on n-gram frequency and tf-idf. In Pro-
ceedings of the 10th Machine Translation Summit X,
pages 227?234.
[Edmonds1970] J. Edmonds, 1970. Combinatorial
Structures and their Applications, chapter Submodu-
lar functions, matroids and certain polyhedra, pages
69?87. Gordon and Breach.
[Feige1998] U. Feige. 1998. A threshold of ln n for ap-
proximating set cover. Journal of the ACM (JACM),
45(4):634?652.
[Fujishige2005] S. Fujishige. 2005. Submodular func-
tions and optimization. Annals of Discrete Mathe-
matics, volume 58. Elsevier Science.
[Galley and Manning2008] M. Galley and C. D. Man-
ning. 2008. A simple and effective hierarchical
phrase reordering model. In Proceedings of EMNLP,
pages 847?855.
[Habash et al.2009] N. Habash, O. Rambow, and
R. Roth. 2009. A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Proceed-
ings of the MEDAR conference, pages 102?109.
[Haffari et al.2009] G. Haffari, M. Roy, and A. Sarkar.
2009. Active learning for statistical machine transla-
tion. In Proceedings of HLT, pages 415?423.
[Hildebrand et al.2005] A. Hildebrand, M. Eck, S. Vo-
gel, and A. Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In Proceedings of EAMT,
pages 133?142.
[Ittycheriah and Roukos2007] A. Ittycheriah and
S. Roukos. 2007. Direct translation model 2. In
Proceedings of HLT/NAACL, page 5764.
[Iyer and Bilmes2013] R. Iyer and J. Bilmes. 2013.
Submodular optimization with submodular cover
and submodular knapsack constraints. In Neural In-
formation Processing Society (NIPS), Lake Tahoe,
CA, December.
[Jegelka and Bilmes2011] Stefanie Jegelka and Jeff A.
Bilmes. 2011. Submodularity beyond submodular
energies: coupling edges in graph cuts. In Computer
Vision and Pattern Recognition (CVPR), Colorado
Springs, CO, June.
[Jurafsky and Martin2009] D. Jurafsky and J. H. Mar-
tin. 2009. Speech and Language Processing:
An Introduction to Natural Language Processing,
Speech Recognition, and Computational Linguistics.
Prentice-Hall, 2nd edition.
[Koehn et al.2007] P. Koehn, H. Hoang, A. Birch,
C. Callison-Burch, M. Federico, N. Bertoldi,
B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer,
O. Bojar, A. Constantin, and E. Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of ACL.
[Koehn2004] P. Koehn. 2004. Statistical significance
tests for machine translation evaluation. In Proceed-
ings of EMNLP.
[Kolmogorov and Zabih2004] V. Kolmogorov and
R. Zabih. 2004. What energy functions can
be minimized via graph cuts? IEEE TPAMI,
26(2):147?159.
140
[Krause and Guestrin2011] A. Krause and C. Guestrin.
2011. Submodularity and its applications in opti-
mized information gathering. ACM Transactions on
Intelligent Systems and Technology, 2(4).
[Krause et al.2008] A. Krause, H.B. McMahan,
C. Guestrin, and A. Gupta. 2008. Robust sub-
modular observation selection. Journal of Machine
Learning Research, 9:2761?2801.
[Leskovec et al.2007] J. Leskovec, A. Krause,
C. Guestrin, C. Faloutsos, J. VanBriesen, and
N. Glance. 2007. Cost-effective outbreak detection
in networks. In Proceedings of the 13th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 420?429.
[Lin and Bilmes2010] H. Lin and J. Bilmes. 2010.
Multi-document summarization via budgeted maxi-
mization of submodular functions. In Proceedings
of NAACL-HLT, pages 2761?2801.
[Lin and Bilmes2011] H. Lin and J. Bilmes. 2011. A
class of submodular functions for document summa-
rization. In Proceedings of ACL, pages 510?520.
[Lin and Bilmes2012] H. Lin and J. Bilmes. 2012.
Learning mixtures of submodular shells with appli-
cation to document summarization. In Uncertainty
in Artifical Intelligence (UAI), Catalina Island, USA,
July. AUAI.
[Lopez2007] A. Lopez. 2007. Hierarchical phrase-
based translation with suffix arrays. In EMNLP-
CoNLL, pages 976?985.
[L?u et al.2007] Y. L?u, J. Huang, and Q. Liu. 2007. Im-
proving statistical machine translation performance
by training data selection and optimization. In Pro-
ceedings of EMNLP, pages 343?350.
[Mandal et al.2008] A. Mandal, D. Vergyri, W. Wang,
J. Zheng, A. Stolcke, D. Hakkani-T?ur G. T?ur, and
N.F. Ayan. 2008. Efficient data selection for ma-
chine translation. In Proceedings of the Spoken Lan-
guage Technology Workshop, pages 261?264.
[Minoux1978] M. Minoux. 1978. Accelerated greedy
algorithms for maximizing submodular functions.
In Lecture Notes in Control and Information Sci-
ences, volume 7, pages 234?243.
[Mirzasoleiman et al.2013] B. Mirzasoleiman, A. Kar-
basi, R. Sarkar, and A. Krause. 2013. Distributed
submodular maximization: Identifying representa-
tive elements in massive data. In Neural Information
Processing Systems (NIPS).
[Moore and Lewis2010] R. Moore and W. Lewis. 2010.
Intelligent selection of language model training data.
In Proceedings of the Association for Computational
Linguistics, pages 220?224.
[Narasimhan and Bilmes2004] M. Narasimhan and
J. Bilmes. 2004. PAC-learning bounded tree-width
graphical models. In Uncertainty in Artificial Intel-
ligence: Proceedings of the Twentieth Conference
(UAI-2004). Morgan Kaufmann Publishers, July.
[Narayanan1997] H. Narayanan. 1997. Submodular
functions and electrical networks. Elsevier.
[Nemhauser et al.1978] G.L. Nemhauser, L.A. Wolsey,
and M.L. Fisher. 1978. An analysis of approxi-
mations for maximizing submodular set functions i.
Mathematical Programming, 14:265?294.
[Stobbe and Krause2010] P. Stobbe and A. Krause.
2010. Efficient minimization of decomposable sub-
modular functions. In NIPS.
[Sviridenko2004] M. Sviridenko. 2004. A note on
maximizing a submodular set function subject to a
knapsack constraint. Operations Research Letters,
32(1):41?43.
[Turchi et al.2012a] M. Turchi, T. de Bie, C. Goutte,
and N. Cristianini. 2012a. Learning to translate: A
statistical and computational analysis. Advances in
Artificial Intellligence, 2012:484580:15 pages.
[Turchi et al.2012b] M. Turchi, C. Goutte, and N. Cris-
tianini. 2012b. Learning machine translation from
in-domain and out-of-domain data. In Proceedings
of EAMT, Trento, Italy.
[Wei et al.2013] K. Wei, Y. Liu, K. Kirchhoff, and
J. Bilmes. 2013. Using document summarization
techniques for speech data subset selection. In Pro-
ceedings of NAACL, pages 721?726, Atlanta, Geor-
gia, June.
[Wei et al.2014] K. Wei, R. Iyer, and Jeff Bilmes. 2014.
Fast multi-stage submodular maximization. In Pro-
ceedings of ICML, Beijing, China.
141
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 912?920,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Multi-document Summarization via
Budgeted Maximization of Submodular Functions
Hui Lin
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195, USA
hlin@ee.washington.edu
Jeff Bilmes
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195, USA
bilmes@ee.washington.edu
Abstract
We treat the text summarization problem as
maximizing a submodular function under a
budget constraint. We show, both theoretically
and empirically, a modified greedy algorithm
can efficiently solve the budgeted submodu-
lar maximization problem near-optimally, and
we derive new approximation bounds in do-
ing so. Experiments on DUC?04 task show
that our approach is superior to the best-
performing method from the DUC?04 evalu-
ation on ROUGE-1 scores.
1 Introduction
Automatically generating summaries from large text
corpora has long been studied in both information
retrieval and natural language processing. There
are several types of text summarization tasks. For
example, if an input query is given, the generated
summary can be query-specific, and otherwise it is
generic. Also, the number of documents to be sum-
marized can vary from one to many. The constituent
sentences of a summary, moreover, might be formed
in a variety of different ways ? summarization can
be conducted using either extraction or abstraction,
the former selects only sentences from the origi-
nal document set, whereas the latter involves natu-
ral language generation. In this paper, we address
the problem of generic extractive summaries from
clusters of related documents, commonly known as
multi-document summarization.
In extractive text summarization, textual units
(e.g., sentences) from a document set are extracted
to form a summary, where grammaticality is as-
sured at the local level. Finding the optimal sum-
mary can be viewed as a combinatorial optimiza-
tion problem which is NP-hard to solve (McDon-
ald, 2007). One of the standard methods for
this problem is called Maximum Marginal Rele-
vance (MMR) (Dang, 2005)(Carbonell and Gold-
stein, 1998), where a greedy algorithm selects the
most relevant sentences, and at the same time avoids
redundancy by removing sentences that are too sim-
ilar to already selected ones. One major problem
of MMR is that it is non-optimal because the deci-
sion is made based on the scores at the current it-
eration. McDonald (2007) proposed to replace the
greedy search of MMR with a globally optimal for-
mulation, where the basic MMR framework can be
expressed as a knapsack packing problem, and an
integer linear program (ILP) solver can be used to
maximize the resulting objective function. ILP Al-
gorithms, however, can sometimes either be expen-
sive for large scale problems or themselves might
only be heuristic without associated theoretical ap-
proximation guarantees.
In this paper, we study graph-based approaches
for multi-document summarization. Indeed, several
graph-based methods have been proposed for extrac-
tive summarization in the past. Erkan and Radev
(2004) introduced a stochastic graph-based method,
LexRank, for computing the relative importance of
textual units for multi-document summarization. In
LexRank the importance of sentences is computed
based on the concept of eigenvector centrality in
the graph representation of sentences. Mihalcea and
Tarau also proposed an eigenvector centrality algo-
rithm on weighted graphs for document summariza-
tion (Mihalcea and Tarau, 2004). Mihalcea et al
later applied Google?s PageRank (Brin and Page,
1998) to natural language processing tasks ranging
912
from automatic keyphrase extraction and word sense
disambiguation, to extractive summarization (Mi-
halcea et al, 2004; Mihalcea, 2004). Recent work
in (Lin et al, 2009) presents a graph-based approach
where an undirected weighted graph is built for the
document to be summarized, and vertices represent
the candidate sentences and edge weights represent
the similarity between sentences. The summary ex-
traction procedure is done by maximizing a submod-
ular set function under a cardinality constraint.
Inspired by (Lin et al, 2009), we perform summa-
rization by maximizing submodular functions under
a budget constraint. A budget constraint is natural
in summarization task as the length of the summary
is often restricted. The length (byte budget) limita-
tion represents the real world scenario where sum-
maries are displayed using only limited computer
screen real estate. In practice, the candidate tex-
tual/linguistic units might not have identical costs
(e.g., sentence lengths vary). Since a cardinality
constraint is a special case (a budget constraint with
unity costs), our approach is more general than (Lin
et al, 2009). Moreover, we propose a modified
greedy algorithm (Section 4) and both theoretically
(Section 4.1) and empirically (Section 5.1) show that
the algorithm solves the problem near-optimally,
thanks to submodularity. Regarding summarization
performance, experiments on DUC?04 task show
that our approach is superior to the best-performing
method in DUC?04 evaluation on ROUGE-1 scores
(Section 5).
2 Background on Submodularity
Consider a set function f : 2V ? R, which maps
subsets S ? V of a finite ground set V to real num-
bers. f(?) is called normalized if f(?) = 0, and
is monotone if f(S) ? f(T ) whenever S ? T .
f(?) is called submodular (Lovasz, 1983) if for any
S, T ? V , we have
f(S ? T ) + f(S ? T ) ? f(S) + f(T ). (1)
An equivalent definition of submodularity is the
property of diminishing returns, well-known in the
field of economics. That is, f(?) is submodular if for
any R ? S ? V and s ? V \ S,
f(S ? {s})? f(S) ? f(R ? {s})? f(R). (2)
Eqn. 2 states that the ?value? of s never increases
in the contexts of ever larger sets, exactly the prop-
erty of diminishing returns. This phenomenon arises
naturally in many other contexts as well. For ex-
ample, the Shannon entropy function is submodu-
lar in the set of random variables. Submodular-
ity, moreover, is a discrete analog of convexity (Lo-
vasz, 1983). As convexity makes continuous func-
tions more amenable to optimization, submodular-
ity plays an essential role in combinatorial optimiza-
tion.
Many combinatorial optimization problems can
be solved optimally or near-optimally in polynomial
time only when the underlying function is submod-
ular. It has been shown that any submodular func-
tion can be minimized in polynomial time (Schri-
jver, 2000)(Iwata et al, 2001). Maximization of sub-
modular functions, however, is an NP-complete op-
timization problem but fortunately, some submod-
ular maximization problems can be solved near-
optimally. A famous result is that the maximization
of a monotone submodular function under a cardi-
nality constraint can be solved using a greedy al-
gorithm (Nemhauser et al, 1978) within a constant
factor (0.63) of being optimal. A constant-factor ap-
proximation algorithm has also been obtained for
maximizing monotone submodular function with a
knapsack constraint (see Section 4.2). Feige et.al.
(2007) studied unconstrained maximization of a ar-
bitrary submodular functions (not necessarily mono-
tone). Kawahara et.al. (2009) proposed a cutting-
plane method for optimally maximizing a submod-
ular set function under a cardinality constraint, and
Lee et.al. (2009) studied non-monotone submodu-
lar maximization under matroid and knapsack con-
straints.
3 Problem Setup
In this paper, we study the problem of maximizing a
submodular function under budget constraint, stated
formally below:
max
S?V
{
f(S) :
?
i?S
ci ? B
}
(3)
where V is the ground set of all linguistic units (e.g.,
sentences) in the document, S is the extracted sum-
mary (a subset of V ), ci is the non-negative cost of
913
selecting unit i and B is our budget, and submodular
function f(?) scores the summary quality.
The budgeted constraint arises naturally since of-
ten the summary must be length limited as men-
tioned above. In particular, the budget B could be
the maximum number of words allowed in any sum-
mary, or alternatively the maximum number of bytes
of any summary, where ci would then be either num-
ber of words or the number of bytes in sentence i.
To benefit from submodular optimization, the
objective function measuring the summary quality
must be submodular. In general, there are two ways
to apply submodular optimization to any application
domain. One way is to force submodularity on an
application, leading to an artificial and poorly per-
forming objective function even if it can be opti-
mized well. The alternative is to address applica-
tions where submodularity naturally applies. We are
fortunate in that, like convexity in the continuous do-
main, submodularity seems to arise naturally in a va-
riety of discrete domains, and as we will see below,
extractive summarization is one of them. As men-
tioned in Section 1, our approach is graph-based,
not only because a graph is a natural representation
of the relationships and interactions between textual
units, but also because many submodular functions
are well defined on a graph and can naturally be used
in measuring the summary quality.
Suppose certain pairs (i, j) with i, j ? V are sim-
ilar and the similarity of i and j is measured by a
non-negative value wi,j . We can represent the en-
tire document with a weighted graph (V,E), with
non-negative weights wi,j associated with each edge
ei,j , e ? E. One well-known graph-based submod-
ular function that measures the similarity of S to the
remainder V \ S is the graph-cut function:
fcut(S) =
?
i?V \S
?
j?S
wi,j . (4)
In multi-document summarization, redundancy is a
particularly important issue since textual units from
different documents might convey the same infor-
mation. A high quality (small and meaningful) sum-
mary should not only be informative about the re-
mainder but also be compact (non-redundant). Typ-
ically, this goal is expressed as a combination of
maximizing the information coverage and minimiz-
ing the redundancy (as used in MMR (Carbonell and
Goldstein, 1998)). Inspired by this, we use the fol-
lowing objective by combining a ?-weighted penalty
term with the graph cut function:
fMMR(S) =
?
i?V \S
?
j?S
wi,j??
?
i,j?S:i?=j
wi,j , ? ? 0.
Luckily, this function is still submodular as both the
graph cut function and the redundancy term are sub-
modular. Neither objective, however, is monotone,
something we address in Theorem 3. Although sim-
ilar to the MMR objective function, our approach is
different since 1) ours is graph-based and 2) we for-
malize the problem as submodular function maxi-
mization under the budget constraint where a simple
greedy algorithm can solve the problem guaranteed
near-optimally.
4 Algorithms
Algorithm 1 Modified greedy algorithm
1: G? ?
2: U ? V
3: while U ?= ? do
4: k ? argmax??U f(G?{?})?f(G)(c?)r
5: G ? G ? {k} if
?
i?G ci + ck ? B and
f(G ? {k})? f(G) ? 0
6: U ? U \ {k}
7: end while
8: v? ? argmaxv?V,cv?B f({v})
9: return Gf = argmaxS?{{v?},G} f(S)
Inspired by (Khuller et al, 1999), we propose
Algorithm 1 to solve Eqn. (3). The algorithm se-
quentially finds unit k with the largest ratio of ob-
jective function gain to scaled cost, i.e., (f(G ?
{?})? f(G))/cr? , where r > 0 is the scaling factor.
If adding k increases the objective function value
while not violating the budget constraint, it is then
selected and otherwise bypassed. After the sequen-
tial selection, setG is compared to the within-budget
singleton with the largest objective value, and the
larger of the two becomes the final output.
The essential aspect of a greedy algorithm is
the design of the greedy heuristic. As discussed
in (Khuller et al, 1999), a heuristic that greedily se-
lects the k that maximizes (f(G?{k})? f(G))/ck
has an unbounded approximation factor. For ex-
ample, let V = {a, b}, f({a}) = 1, f({b}) = p,
914
ca = 1, cb = p + 1, and B = p + 1. The solution
obtained by the greedy heuristic is {a} with objec-
tive function value 1, while the true optimal objec-
tive function value is p. The approximation factor
for this example is then p and therefore unbounded.
We address this issue by the following two mod-
ifications to the naive greedy algorithms. The first
one is the final step (line 8 and 9) in Algorithm 1
where set G and singletons are compared. This step
ensures that we could obtain a constant approxima-
tion factor for r = 1 (see the proof in the Appendix).
The second modification is that we introduce a
scaling factor r to adjust the scale of the cost. Sup-
pose, in the above example, we scale the cost as
ca = 1r, cb = (p+1)r, then selecting a or b depends
also on the scale r, and we might get the optimal so-
lution using a appropriate r. Indeed, the objective
function values and the costs might be uncalibrated
since they might measure different units. E.g., it is
hard to say if selecting a sentence of 15 words with
an objective function gain of 2 is better than select-
ing sentence of 10 words with gain of 1. Scaling
can potentially alleviate this mismatch (i.e., we can
adjust r on development set). Interestingly, our the-
oretical analysis of the performance guarantee of the
algorithm also gives us guidance about how to scale
the cost for a particular problem (see Section 4.1).
4.1 Analysis of performance guarantee
Although Algorithm 1 is essentially a simple greedy
strategy, we show that it solves Eqn. (3) globally and
near-optimally, by exploiting the structure of sub-
modularity. As far as we know, this is a new result
for submodular optimization, not previously stated
or published before.
Theorem 1. For normalized monotone submodular
function f(?), Algorithm 1 with r = 1 has a constant
approximation factor as follows:
f(Gf ) ?
(
1? e?
1
2
)
f(S?), (5)
where S? is an optimal solution.
Proof. See Appendix.
Note that an ?-approximation algorithm for an
optimization problem is a polynomial-time algo-
rithm that for all instances of the problem produces
a solution whose value is within a factor of ? of the
value of the an optimal solution. So Theorem 1 ba-
sically states that the solution found by Algorithm 1
can be at least as good as (1 ? 1/
?
e)f(S?) ?
0.39f(S?) even in the worst case. A constant ap-
proximation bound is good since it is true for all in-
stances of the problem, and we always know how
good the algorithm is guaranteed to be without any
extra computation. For r ?= 1, we resort to instance-
dependent bound where the approximation can be
easily computed per problem instance.
Theorem 2. With normalized monotone submodu-
lar f(?), for i = 1, . . . , |G|, let vi be the ith unit
added intoG andGi is the set after adding vi. When
0 ? r ? 1,
f(Gi) ?
(
1?
i
?
k=1
(
1?
crvk
Br|S?|1?r
)
)
f(S?)
(6)
?
(
1?
i
?
k=1
(
1?
crvk
Br|V |1?r
)
)
f(S?) (7)
and when r ? 1,
f(Gi) ?
(
1?
i
?
k=1
(
1?
(cvk
B
)r)
)
f(S?). (8)
Proof. See Appendix.
Theorem 2 gives bounds for a specific instance of
the problem. Eqn. (6) requires the size |S?|, which
is unknown, requiring us to estimate an upper bound
of the cardinality of the optimal set S?. Obviously,
|S?| ? |V |, giving us Eqn. (7). A tighter upper
bound is obtained, however, by sorting the costs.
That is, let c[1], c[2], . . . , c[|V |] be the sorted sequence
of costs in nondecreasing order, giving |S?| < m
where
?m?1
k=1 c[i] ? B and
?m
k=1 c[i] > B. In this
case, the computation cost for the bound estimation
is O(|V | log |V |), which is quite feasible.
Note that both Theorem 1 and 2 are for mono-
tone submodular functions while our practical ob-
jective function, i.e. fMMR, is not guaranteed every-
where monotone. However, our theoretical results
still holds for fMMR with high probability in prac-
tice. Intuitively, in summarization tasks, the sum-
mary is usually small compared to the ground set
size (|S| ? |V |). When |S| is small, fMMR is
915
monotone and our theoretical results still hold. Pre-
cisely, assume that all edge weights are bounded:
wi,j ? [0, 1] (which is the case for cosine simi-
larity between non-negative vectors). Also assume
that edges weights are independently identically dis-
tributed with mean ?, i.e. E(wi,j) = ?. Given a
budget B, assume the maximum possible size of a
solution is K. Let ? = 2? + 1, and ? = 2K ? 1.
Notice that ? ? |V | for our summarization task. We
have the following theorem:
Theorem 3. Algorithm 1 solves the summarization
problem near-optimally (i.e. Theorem 1 and Theo-
rem 2 hold) with high probability of at least
1? exp
{
?2(|V | ? (? + 1)?)
2?2
|V |+ (?2 ? 1)?
+ lnK
}
Proof. Omitted due to space limitation.
4.2 Related work
Algorithms for maximizing submodular function
under budget constraint (Eqn. (3)) have been stud-
ied before. Krause (2005) generalized the work by
Khuller et al(1999) on budgeted maximum cover
problem to the submodular framework, and showed
a 12(1 ? 1/e)-approximation algorithm. The algo-
rithm in (Krause and Guestrin, 2005) and (Khuller
et al, 1999) is actually a special case of Algorithm 1
when r = 1, and Theorem 1 gives a better bound
(i.e., (1? 1/
?
e) > 12(1? 1/e)) in this case. There
is also a greedy algorithm with partial enumerations
(Sviridenko, 2004; Krause and Guestrin, 2005) fac-
tor (1? 1/e). This algorithm, however, is too com-
putationally expensive and thus not practical for real
world applications (the computation cost is O(|V |5)
in general). When each unit has identical cost, the
budget constraint reduces to cardinality constraint
where a greedy algorithm is known to be a (1?1/e)-
approximation algorithm (Nemhauser et al, 1978)
which is the best that can be achieved in polyno-
mial time (Feige, 1998) if P ?= NP. Recent work
(Takamura and Okumura, 2009) applied the maxi-
mum coverage problem to text summarization (with-
out apparently being aware that their objective is
submodular) and studied a similar algorithm to ours
when r = 1 and for the non-penalized graph-cut
function. This problem, however, is a special case
of constrained submodular function maximization.
5 Experiments
We evaluated our approach on the data set of
DUC?04 (2004) with the setting of task 2, which
is a multi-document summarization task on English
news articles. In this task, 50 document clusters
are given, each of which consists of 10 documents.
For each document cluster, a short multi-document
summary is to be generated. The summary should
not be longer than 665 bytes including spaces and
punctuation, as required in the DUC?04 evaluation.
We used DUC?03 as our development set. All docu-
ments were segmented into sentences using a script
distributed by DUC. ROUGE version 1.5.5 (Lin,
2004), which is widely used in the study of summa-
rization, was used to evaluate summarization perfor-
mance in our experiments 1. We focus on ROUGE-
1 (unigram) F-measure scores since it has demon-
strated strong correlation with human annotation
(Lin, 2004).
The basic textual/linguistic units we consider in
our experiments are sentences. For each document
cluster, sentences in all the documents of this cluster
forms the ground set V . We built semantic graphs
for each document cluster based on cosine similar-
ity, where cosine similarity is computed based on
the TF-IDF (term frequency, inverse document fre-
quency) vectors for the words in the sentences. The
cosine similarity measures the similarity between
sentences, i.e., wi,j .
Here the IDF values were calculated using all the
document clusters. The weighted graph was built
by connecting vertices (corresponding to sentences)
with weight wi,j > 0. Any unconnected vertex was
removed from the graph, which is equivalent to pre-
excluding certain sentences from the summary.
5.1 Comparison with exact solution
In this section, we empirically show that Algo-
rithm 1 works near-optimally in practice. To deter-
mine how much accuracy is lost due to approxima-
tions, we compared our approximation algorithms
with an exact solution. The exact solutions were ob-
tained by Integer Linear Programming (ILP). Solv-
ing arbitrary ILP is an NP-hard problem. If the size
of the problem is not too large, we can sometimes
find the exact solution within a manageable time
1Options used: -a -c 95 -b 665 -m -n 4 -w 1.2
916
using a branch-and-bound method. In our experi-
ments, MOSEK was used as our ILP solver.
We formalize Eqn. (3) as an ILP by introducing
indicator (binary) variables xi,j , yi,j , i ?= j and zi
for i, j ? V . In particular, zi = 1 indicates that
unit i is selected, i.e., i ? S, xi,j = 1 indicates that
i ? S but j /? S, and yi,j = 1 indicates both i and
j are selected. Adding constraints to ensure a valid
solution, we have the following ILP formulation for
Eqn. (3) with objective function fMMR(S):
max
?
i?=j,i,j?V
wi,jxi,j ? ?
?
i?=j,i,j?V
wi,jyi,j
subject to:
?
i?V
cizi ? B,
xi,j ? zi ? 0, xi,j + zj ? 1, zi ? zj ? xi,j ? 0,
yi,j ? zi ? 0, yi,j ? zj ? 0, zi + zj ? yi,j ? 1,
xi,j , yi,j , zi ? {0, 1},?i ?= j, i, j ? V
Note that the number of variables in the ILP for-
mulation is O(|V |2). For a document cluster with
hundreds of candidate textual units, the scale of the
problem easily grows involving tens of thousands
of variables, making the problem very expensive to
solve. For instance, solving the ILP exactly on a
document cluster with 182 sentences (as used in Fig-
ure 1) took about 17 hours while our Algorithm 1
finished in less than 0.01 seconds.
We tested both approximate and exact algorithms
on DUC?03 data where 60 document clusters were
used (30 TDT document clusters and 30 TREC doc-
ument clusters), each of which contains 10 docu-
ments on average. The true approximation factor
was computed by dividing the objective function
value found by Algorithm 1 over the optimal ob-
jective function value (found by ILP). The average
approximation factors over the 58 document clus-
ters (ILP on 2 of the 60 document clusters failed to
finish) are shown in Table 1, along with other statis-
tics. On average Algorithm 1 finds a solution that is
over 90% as good as the optimal solution for many
different r values, which backs up our claim that
the modified greedy algorithm solves the problem
near-optimally, even occasionally optimally (Figure
1 shows one such example).
The higher objective function value does not al-
ways indicate higher ROUGE-1 score. Indeed,
0
20
40
60
80
100
120
140
0 2 4 6 8 10 12
op mal
r=0
r=0.5
r=1
r=1.5
number of sentences in the summary
Ob
jec
tiv
e f
un
cti
on
 va
lue
exact solution
Figure 1: Application of Algorithm 1 when summariz-
ing document cluster d30001t in the DUC?04 dataset with
summary size limited to 665 bytes. The objective func-
tion was fMMR with ? = 2. The plots show the achieved
objective function as the number of selected sentences
grows. The plots stop when in each case adding more
sentences violates the budget. Algorithm 1 with r = 1
found the optimal solution exactly.
rather than directly optimizing ROUGE, we opti-
mize a surrogate submodular function that indicates
the quality of a summary. Optimality in the submod-
ular function does not necessary indicate optimality
in ROUGE score. Nevertheless, we will show that
our approach outperforms several other approaches
in terms of ROUGE. We note that ROUGE is itself
a surrogate for true human-judged summary quality,
it might possibly be that fMMR is a still better surro-
gate ? we do not consider this possibility further in
this work, however.
5.2 Summarization Results
We used DUC?03 (as above) for our development
set to investigate how r and ? relate to the ROUGE-
1 score. From Figure 2, the best performance is
achieved with r = 0.3, ? = 4. Using these settings,
we applied our approach to the DUC?04 task. The
results, along with the results of other approaches,
are shown in Table 2. All the results in Table 2 are
presented as ROUGE-1 F-measure scores. 2
We compared our approach to two other well-
2When the evaluation was done in 2004, ROUGEwas still in
revision 1.2.1, so we re-evaluated the DUC?04 submissions us-
ing ROUGE v1.5.5 and the numbers are slightly different from
the those reported officially.
917
Table 1: Comparison of Algorithm 1 to exact algorithms
on DUC?03 dataset. All the numbers shown in the ta-
ble are the average statistics (mean/std). The ?true? ap-
proximation factor is the ratio of objective function value
found by Algorithm 1 over the ILP-derived true-optimal
objective value, and the approximation bounds were esti-
mated using Theorem 2.
Approx. factor ROUGE-1
true bound (%)
exact 1.00 - 33.60/5.05
r = 0.0 0.65/0.15 ?0.19/0.08 33.50/5.94
r = 0.1 0.71/0.15 ?0.24/0.08 33.68/6.03
r = 0.3 0.88/0.11 ?0.37/0.06 34.77/5.49
r = 0.5 0.96/0.04 ?0.48/0.05 34.33/5.94
r = 0.7 0.98/0.02 ?0.56/0.05 34.08/5.41
r = 1.0 0.98/0.02 ?0.65/0.04 33.32/5.14
r = 1.2 0.97/0.02 ?0.48/0.05 32.54/4.69
32.0%
32.5%
33.0%
33.5%
34.0%
34.5%
35.0%
0 5 10 15
r=0
r=0.3
r=0.5
r=0.7
r=1
RO
UG
E-1
 F-
me
as
ur
e
Figure 2: Different combinations of r and ? for fMMR
related to ROUGE-1 score on DUC?03 task 1.
known graph-based approaches, LexRank and
PageRank. LexRank was one of the participat-
ing system in DUC?04, with peer code 104. For
PageRank, we implemented the recursive graph-
based ranking algorithm ourselves. The importance
of sentences was estimated in an iterative way as
in (Brin and Page, 1998)(Mihalcea et al, 2004).
Sentences were then selected based on their impor-
tance rankings until the budget constraint was vi-
olated. The graphs used for PageRank were ex-
actly the graphs in our submodular approaches (i.e.,
an undirected graph). In both cases, submodu-
lar summarization achieves better ROUGE-1 scores.
The improvement is statistically significant by the
Wilcoxon signed rank test at level p < 0.05. Our
approach also outperforms the best system (Conroy
et al, 2004), peer code 65 in the DUC?04 evalua-
tion although not as significant (p < 0.08). The rea-
son might be that DUC?03 is a poor representation
of DUC?04 ? indeed, by varying r and ? over the
ranges 0 ? r ? 0.2 and 5 ? ? ? 9 respectively, the
DUC?04 ROUGE-1 scores were all > 38.8% with
the best DUC?04 score being 39.3%.
Table 2: ROUGE-1 F-measure results (%)
Method ROUGE-1 score
peer65 (best system in DUC04) 37.94
peer104 (LexRank) 37.12
PageRank 35.37
Submodular (r = 0.3, ? = 4) 38.39
6 Appendix
We analyze the performance guarantee of Algorithm 1.
We use the following notation: S? is the optimal solu-
tion; Gf is the final solution obtained by Algorithm 1;
G is the solution obtained by the greedy heuristic (line
1 to 7 in Algorithm 1); vi is the ith unit added to G,
i = 1, . . . , |G|;Gi is the set obtained by greedy algorithm
after adding vi (i.e., Gi = ?ik=1{vk}, for i = 1, . . . , |G|,
with G0 = ? and G|G| = G); f(?) : 2V ? R is a
monotone submodular function; and ?k(S) is the gain of
adding k to S, i.e., f(S ? {k})? f(S).
Lemma 1. ?X,Y ? V ,
f(X) ? f(Y ) +
?
k?X\Y
?k(Y ). (9)
Proof. See (Nemhauser et al, 1978)
Lemma 2. For i = 1, . . . , |G|, when 0 ? r ? 1,
f(S?)? f(Gi?1) ?
Br|S?|1?r
crvi
(f(Gi)? f(Gi?1)),
(10)
and when r ? 1,
f(S?)? f(Gi?1) ?
(
B
cvi
)r
(f(Gi)? f(Gi?1))
(11)
Proof. Based on line 4 of Algorithm 1, we have
?u ? S? \Gi?1,
?u(Gi?1)
cru
? ?vi(Gi?1)
crvi
.
918
Thus when 0 ? r ? 1,
?
u?S?\Gi?1
?u(Gi?1) ?
?vi(Gi?1)
crvi
?
u?S?\Gi?1
cru
? ?vi(Gi?1)
crvi
|S? \Gi?1|
(
?
u?S?\Gi?1 cu
|S? \Gi?1|
)r
? ?vi(Gi?1)
crvi
|S?|1?r
?
?
?
u?S?\Gi?1
cu
?
?
r
? ?vi(Gi?1)
crvi
|S?|1?rBr,
where the second inequality is due to the concavity of
g(x) = xr, x > 0, 0 ? r ? 1. The last inequality uses
the fact that
?
u?S? cu ? B. Similarly, when r ? 1,
?
u?S?\Gi?1
?u(Gi?1) ?
?vi(Gi?1)
crvi
?
u?S?\Gi?1
cru
? ?vi(Gi?1)
crvi
?
?
?
u?S?\Gi?1
cu
?
?
r
? ?vi(Gi?1)
crvi
Br.
Applying Lemma 1, i.e., let X = S? and Y = Gi?1, the
lemma immediately follows.
The following is a proof of Theorem 2.
Proof. Obviously, the theorem is true when i = 1 by
applying Lemma 2.
Assume that the theorem is true for i?1, 2 ? i ? |G|,
we show that it also holds for i. When 0 ? r ? 1,
f(Gi) = f(Gi?1) + (f(Gi)? f(Gi?1))
? f(Gi?1) +
crvi
Br|S?|1?r
(f(S?)? f(Gi?1))
=
(
1?
crvi
Br|S?|1?r
)
f(Gi?1) +
crvi
Br|S?|1?r
f(S?)
?
(
1?
crvi
Br|S?|1?r
)
(
1?
i?1
?
k=1
(
1?
crvk
Br|S?|1?r
)
)
f(S?) +
crvi
Br|S?|1?r
f(S?)
=
(
1?
i
?
k=1
(
1?
crvk
Br|S?|1?r
)
)
f(S?).
The case when r ? 1 can be proven similarly.
Now we are ready to prove Theorem 1.
Proof. Consider the following two cases:
Case 1: ?v ? V such that f({v}) > 12f(S
?). Then it
is guaranteed that f(Gf ) ? f({v})) > 12f(S
?) due line
9 of Algorithm 1.
Case 2: ?v ? V , we have f({v}) ? 12f(S
?). We
consider the following two sub-cases, namely Case 2.1
and Case 2.2:
Case 2.1: If
?
v?G cv ?
1
2B, then we know that
?v /? G, cv > 12B since otherwise we can add a v /? G
into G to increase the objective function value without
violating the budget constraint. This implies that there is
at most one unit in S? \ G since otherwise we will have
?
v?S? cv > B. By assumption, we have f(S? \ G) ?
1
2f(S
?). Submodularity of f(?) gives us:
f(S? \G) + f(S? ?G) ? f(S?),
which implies f(S? ?G) ? 12f(S
?). Thus we have
f(Gf ) ? f(G) ? f(S? ?G) ?
1
2
f(S?),
where the second inequality follows from monotonicity.
Case 2.2: If
?
v?G cv >
1
2B, for 0 ? r ? 1, using
Theorem 2, we have
f(G) ?
?
?1?
|G|
?
k=1
(
1?
crvk
Br|S?|1?r
)
?
? f(S?)
?
?
?1?
|G|
?
k=1
?
?1?
crvk |S
?|r?1
2r
(
?|G|
k=1 cvk
)r
?
?
?
? f(S?)
?
(
1?
(
1? |S
?|r?1
2r|G|r
)|G|)
f(S?)
?
(
1? e?
1
2
?
|S?|
2|G|
?r?1)
f(S?)
where the third inequality uses the fact (provable using
Lagrange multipliers) that for a1, . . . , an ? R+ such that
?n
i=1 ai = ?, function
1?
n
?
i=1
(
1? ?a
r
i
?r
)
achieves its minimum of 1 ? (1 ? ?/nr)n when a1 =
? ? ? = an = ?/n for?, ? > 0. The last inequality follows
from e?x ? 1? x.
In all cases, we have
f(Gf ) ? min
{
1
2
, 1? e?
1
2
?
|S?|
2|G|
?r?1}
f(S?)
In particular, when r = 1, we obtain the constant approx-
imation factor, i.e.
f(Gf ) ?
(
1? e? 12
)
f(S?)
919
Acknowledgments
This work is supported by an ONR MURI grant
(No. N000140510388), the Companions project
(IST programme under EC grant IST-FP6-034434),
and the National Science Foundation under grant
IIS-0535100. We also wish to thank the anonymous
reviewers for their comments.
References
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer networks
and ISDN systems, 30(1-7):107?117.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proc. of SIGIR.
J.M. Conroy, J.D. Schlesinger, J. Goldstein, and D.P.
O?leary. 2004. Left-brain/right-brain multi-document
summarization. In Proceedings of the Document Un-
derstanding Conference (DUC 2004).
H.T. Dang. 2005. Overview of DUC 2005. In Proceed-
ings of the Document Understanding Conference.
2004. Document understanding conferences (DUC).
http://www-nlpir.nist.gov/projects/duc/index.html.
G. Erkan and D.R. Radev. 2004. LexRank: Graph-
based Lexical Centrality as Salience in Text Summa-
rization. Journal of Artificial Intelligence Research,
22:457?479.
U. Feige, V. Mirrokni, and J. Vondrak. 2007. Maximiz-
ing non-monotone submodular functions. In Proceed-
ings of 48th Annual IEEE Symposium on Foundations
of Computer Science (FOCS).
U. Feige. 1998. A threshold of ln n for approximating set
cover. Journal of the ACM (JACM), 45(4):634?652.
G. Goel, , C. Karande, P. Tripathi, and L. Wang.
2009. Approximability of Combinatorial Problems
with Multi-agent Submodular Cost Functions. FOCS.
S. Iwata, L. Fleischer, and S. Fujishige. 2001. A
combinatorial strongly polynomial algorithm for min-
imizing submodular functions. Journal of the ACM,
48(4):761?777.
Yoshinobu Kawahara, Kiyohito Nagano, Koji Tsuda, and
Jeff Bilmes. 2009. Submodularity cuts and appli-
cations. In Neural Information Processing Society
(NIPS), Vancouver, Canada, December.
S. Khuller, A. Moss, and J. Naor. 1999. The budgeted
maximum coverage problem. Information Processing
Letters, 70(1):39?45.
A. Krause and C. Guestrin. 2005. A note on the bud-
geted maximization of submodular functions. Techni-
cal Rep. No. CMU-CALD-05, 103.
J. Lee, V.S. Mirrokni, V. Nagarajan, and M. Sviridenko.
2009. Non-monotone submodular maximization un-
der matroid and knapsack constraints. In Proceedings
of the 41st annual ACM symposium on Symposium on
theory of computing, pages 323?332. ACM New York,
NY, USA.
Hui Lin, Jeff Bilmes, and Shasha Xie. 2009. Graph-
based submodular selection for extractive summariza-
tion. In Proc. IEEE Automatic Speech Recognition
and Understanding (ASRU), Merano, Italy, December.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop.
L. Lovasz. 1983. Submodular functions and convexity.
Mathematical programming-The state of the art,(eds.
A. Bachem, M. Grotschel and B. Korte) Springer,
pages 235?257.
R. McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. Lecture
Notes in Computer Science, 4425:557.
R. Mihalcea and P. Tarau. 2004. TextRank: bringing or-
der into texts. In Proceedings of EMNLP, Barcelona,
Spain.
R. Mihalcea, P. Tarau, and E. Figa. 2004. PageRank on
semantic networks, with application to word sense dis-
ambiguation. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING-
04).
R. Mihalcea. 2004. Graph-based ranking algorithms for
sentence extraction, applied to text summarization. In
Proceedings of the ACL 2004 (companion volume).
2006. Mosek.
G.L. Nemhauser, L.A. Wolsey, and M.L. Fisher. 1978.
An analysis of approximations for maximizing sub-
modular set functions I. Mathematical Programming,
14(1):265?294.
A. Schrijver. 2000. A combinatorial algorithm mini-
mizing submodular functions in strongly polynomial
time. Journal of Combinatorial Theory, Series B,
80(2):346?355.
M. Sviridenko. 2004. A note on maximizing a submod-
ular set function subject to a knapsack constraint. Op-
erations Research Letters, 32(1):41?43.
H. Takamura and M. Okumura. 2009. Text summariza-
tion model based on maximum coverage problem and
its variant. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 781?789. Association for
Computational Linguistics.
920
Proceedings of NAACL-HLT 2013, pages 721?726,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Using Document Summarization Techniques for Speech Data Subset
Selection
Kai Wei?, Yuzong Liu?, Katrin Kirchhoff , Jeff Bilmes
Department of Eletrical Engineering
University of Washington
Seattle, WA 98195, USA
{kaiwei,yzliu,katrin,bilmes}@ee.washington.edu
Abstract
In this paper we leverage methods from sub-
modular function optimization developed for
document summarization and apply them to
the problem of subselecting acoustic data. We
evaluate our results on data subset selection
for a phone recognition task. Our framework
shows significant improvements over random
selection and previously proposed methods us-
ing a similar amount of resources.
1 Introduction
Present-day applications in spoken language technol-
ogy (speech recognizers, keyword spotters, etc.) can
draw on an unprecedented amount of training data.
However, larger data sets come with increased de-
mands on computational resources; moreover, they
tend to include redundant information as their size
increases. Therefore, the performance gain curves
of large-scale systems with respect to the amount of
training data often show ?diminishing returns?: new
data is often less valuable (in terms of performance
gain) when added to a larger pre-existing data set than
when added to a smaller pre-existing set (e.g.,(Moore,
2003)). Therefore it is of prime importance to de-
velop methods for data subset selection. We distin-
guish two data subselection scenarios: (a) a priori
selection of a data set before (re-)training a system;
in this case the goal is to subselect the existing data
set as well as possible, eliminating redundant infor-
mation; (b) selection for adaptation, where the goal
?These authors are joint first authors with equal contribu-
tions.
is to tune a system to a known development or test
set. While many studies have addressed the second
scenario, this paper investigates the first: our goal is
to select a smaller subset of the data that fits a given
?budget? (e.g. maximum number of hours of data) but
provides, to the extent possible, as much information
as the complete data set. Additionally, our selection
method should be a low-resource method that does
not require an already-trained complex system such
as an existing word recognizer.
This problem is akin to unsupervised data ?sum-
marization?. In (Lin and Bilmes, 2009) a novel class
of summarization techniques based on submodular
function optimization were proposed for extractive
document summarization. Interestingly, these meth-
ods can also be applied to speech data ?summariza-
tion? with only small modifications. In the following
sections we develop a submodular framework for
speech data summarization and evaluate it on a proof-
of-concept phone recognition task.
2 Related Work
Most approaches to data subset selection in speech
have relied on ?rank-and-select? approaches that de-
termine the utility of each sample in the data set,
rank all samples according to their utility scores, and
then select the top N samples. In weakly supervised
approaches (e.g.,(Kemp and Waibel, 1998; Lamel
et al, 2002; Hakkani-Tur et al, 2002), utility is re-
lated to the confidence of an existing word recognizer
on new data samples: untranscribed training data is
automatically transcribed using an existing baseline
speech recognizer, and individual utterances are se-
lected as additional training data if they have low
721
confidence. These are active learning approaches
suitable for a scenario where a well-trained speech
recognizer is already available and additional data
for retraining needs to be selected. However, we
would like to reduce available training data ahead of
time with a low-resource approach. In (Chen et al,
2009) individual samples are selected for the purpose
of discriminative training by considering phone ac-
curacy and the frame-level entropy of the Gaussian
posteriors. (Itoh et al, 2012) use a utility function
consisting of the entropy of word hypothesis N-best
lists and the representativeness of the sample using a
phone-based TF-IDF measure. The latter is compa-
rable to methods used in this paper, though the first
term in their objective function still requires a word
recognizer. In (Wu et al, 2007) acoustic training data
associated with transcriptions is subselected to max-
imize the entropy of the distribution over linguistic
units (phones or words). Most importantly, all these
methods select samples in a greedy fashion without
optimality guarantees. As we will explain in the next
section, greedy selection is near-optimal only when
applied to monotone submodular functions.
3 Submodular Functions
Submodular functions (Edmonds, 1970) have been
widely studied in mathematics, economics, and op-
erations research and have recently attracted interest
in machine learning (Krause and Guestrin, 2011). A
submodular function is defined as follows: Given a fi-
nite ground set of objects (samples) V = {v1, ..., vn}
and a function f : 2V ? R+ that returns a real value
for any subset S ? V , f is submodular if ?A ? B,
and v /? B, f(A+ v)? f(A) ? f(B + v)? f(B).
That is, the incremental ?value? of v decreases when
the set in which v is considered grows from A to B.
Powerful optimization guarantees exist for certain
subtypes of submodular functions. If, for example,
the function is monotone submodular, i.e. ?A ?
B, f(A) ? f(B), then it can be maximized, under
a cardinality constraint, by a greedy algorithm that
scales to extremely large data sets, and finds a solu-
tion guaranteed to approximate the optimal solution
to within a constant factor 1? 1/e (Nemhauser et al,
1978). Submodular functions can be considered the
discrete analog of convexity.
3.1 Submodular Document Summarization
In (Lin and Bilmes, 2011) submodular functions were
recently applied to extractive document summariza-
tion. The problem was formulated as a monotone
submodular function that had to be maximized sub-
ject to cardinality or knapsack constraints:
argmaxS?V {f(S) : c(S) ? K} (1)
where V is the set of sentences to be summarized, K
is the maximum number of sentences to be selected,
and c(?) ? 0 is sentence cost. f(S) was instantiated
by a form of saturated coverage:
fSC(S) =
?
i?V
min{Ci(S), ?Ci(V )} (2)
where Ci(S) =
?
j?S wij , and where wij ? 0 in-
dicates the similarity between sentences i and j ?
Ci : 2V ? R is itself monotone submodular (modu-
lar in fact) and 0 ? ? ? 1 is a saturation coefficient.
fSC(S) is monotone submodular and therefore has
the previously mentioned performance guarantees.
The weighting function w was implemented as the
cosine similarity between TF-IDF weighted n-gram
count vectors for the sentences in the dataset.
3.2 Submodular Speech Summarization
Similar to the procedure described above we can treat
the task of subselecting an acoustic data set as an
extractive summarization problem. For our a priori
data selection scenario we would like to extract those
training samples that jointly are representative of
the total data set. Initial explorations of submodular
functions for speech data can be found in (Lin and
Bilmes, 2009), where submodular functions were
used in combination with a purely acoustic similarity
measure (Fisher kernel). In addition Equation 2 the
facility location function was used:
ffac(S) =
?
i?V
max
j?S
wij (3)
Here our focus is on utilizing methods that move
beyond purely acoustic similarity measures and con-
sider kernels derived from discrete representations
of the acoustic signal. To this end we first run a to-
kenizer over the acoustic signal that converts it into
a sequence of discrete labels. In our case we use a
722
simple bottom-up monophone recognizer (without
higher-level constraints such as a phone language
model) that produces phone labels. We then use the
hypothesized sequence of phonetic labels to compute
two different sentence similarity measures: (a) co-
sine similarity using TF-IDF weighted phone n-gram
counts, and (b) string kernels. We compare their
performance to that of the Fisher kernel as a purely
acoustic similarity measure.
TF-IDF weighted cosine similarity
The cosine similarity between phone sequences si
and sj is computed as
simij =
?
w?si tfw,si ? tfw,sj ? idf
2
w
??
w?si tf
2
w,si idf
2
w
??
w?sj tf
2
w,sj idf
2
w
(4)
where tfw,si is the count of n-gram w in si and idfw
is the inverse document count of w (each sentence is
a ?document?). We use n = 1, 2, 3.
String kernel
The particular string kernel we use is a gapped,
weighted subsequence kernel of the type described in
(Rousu and Shawe-Taylor, 2005). Formally, we de-
fine a sentence s as a concatenation of symbols from
a finite alphabet ? (here the inventory of phones) and
an embedding function from strings to feature vec-
tors, ? : ?? ? H. The string kernel function K(s, t)
computes the distance between the resulting vectors
for two sentences si and sj . The embedding function
is defined as
?ku(s) :=
?
i:u=s(i)
?|i| u ? ?k (5)
where k is the maximum length of subsequences,
|i| is the length of i, and ? is a penalty parameter
for each gap encountered in the subsequence. K is
defined as
K(si, sj) =
?
u
??u(si), ?u(sj)?wu (6)
where w is a weight dependent on the length of
u, l(u). Finally, the kernel score is normalized by?
K(si, si) ? K(sj , sj) to discourage long sentences
from being favored.
Fisher kernel
The Fisher kernel is based on the vector of derivatives
UX of the log-likelihood of the acoustic data (X)
with respect to the parameters in the phone HMMs
?1, ..., ?m for m models, having similarity score:
simij = (max
i?,j?
di?j?)? dij , where dij = ||U ?i ? U
?
j ||1,
U ?X = 5? logP (X|?), and U
?
X = U
?1
X ? U
?2
x , ..., ?U
?m
X .
4 Data and Systems
We evaluate our approach on subselecting training
data from the TIMIT corpus for training a phone rec-
ognizer. Although this not a large-scale data task, it
is an appropriate proof-of-concept task for rapidly
testing different combinations of submodular func-
tions and similarity measures. Our goal is to focus
on acoustic modeling only; we thus look at phone
recognition performance and do not have to take into
account potential interactions with a language model.
We also chose a simple acoustic model, a monophone
HMM recognizer, rather than a more powerful but
computationally complex model in order to ensure
quick experimental turnaround time. Note that the
goal of this study is not to obtain the highest phone
accuracy possible; what is important is the relative
performance of the different subset selection meth-
ods, especially on small data subsets.
The sizes of the training, development and test data
are 4620, 200 and 192 utterances, respectively. Pre-
processing was done by extracting 39-dimensional
MFCC feature vectors every 10 ms, with a window
of 25.6ms. Speaker mean and variance normaliza-
tion was applied. A 16-component Gaussian mixture
monophone HMM system was trained on the full data
set to generate parameters for the Fisher kernel and
phone sequences for the string kernel and TF-IDF
based similarity measures.
Following the selection of subsets (2.5%, 5%, 10%,
20%, 30%, 40%, 50%, 60%, 70% and 80% of the
data, measured as percentage of non-silence speech
frames), we train a 3-state HMM monophone recog-
nizer for all 48 TIMIT phone classes on the result-
ing sets and evaluate the performance on the core
test set of 192 utterances, collapsing the 48 classes
into 39 in line with standard practice (Lee and Hon,
1989). The HMM state output distributions are mod-
eled by diagonal-covariance Gaussian mixtures with
the number of Gaussians ranging between 4 and 64,
depending on the data size.
As a baseline we perform 100 random draws of
the specified subset sizes and average the results.
723
The second baseline consists of the method in (Wu et
al., 2007), where utterances are selected to maximize
the entropy of the distribution over phones in the
selected subset.
5 Experiments
We tested the three different similarity measures de-
scribed above in combination with the submodular
functions in Equations 2 and 3. The parameters of
the gapped string kernel (i.e. the kernel order (k), the
gap penalty (?), and the contiguous substring length
l) were optimized on the development set. The best
values were ? = 0.1, k = 4, l = 3. We found that
facility location was superior to saturated cover func-
tion across the board.
Comparison of different data subset selection methods 
Phone Accuracy (%)
P
e
r
c
e
n
t
a
g
e
 
o
f
 
S
p
e
e
c
h
 
i
n
 
S
e
l
e
c
t
e
d
 
S
u
b
s
e
t
40# 45# 50# 55# 60# 65#
80#
70#
60#
50#
40#
30#
20#
10#
5#
2.5# string#kernel#TF7IDF#trigram#TF7IDF#bigram#TF7IDF#unigram#Fisher#kernel#entropy#random#
Figure 1: Phone accuracy for different subset sizes; each
block of bars lists, from bottom to top: random baseline,
entropy baseline, Fisher kernel, TF-IDF (unigram), TF-
IDF (bigram), TF-IDF (trigram), string kernel.
Figure 1 shows the performance of the random and
entropy-based baselines as well as the performance
of the facility location function with different sim-
ilarity measures. The entropy-based baseline beats
the random baseline for most percentage cases but
is otherwise the lowest-performing method overall.
Note that this baseline uses the true transcriptions in
line with (Wu et al, 2007) rather than the hypothe-
sized phone labels output by our recognizer. The low
performance and the fact that it is even outperformed
by the random baseline in the 2.5% and 70% cases
P
e
r
c
e
n
t
a
g
e
 
o
f
 
S
p
e
e
c
h
 
i
n
 
S
e
l
e
c
t
e
d
 
S
u
b
s
e
t
Phone Accuracy (%)
Comparison of different submodular functions
45
50
55
60
65
2.5 5 10 20 30 40 60 60 70 80
Figure 2: Phone accuracy obtained by random selection,
facility location function, and saturated coverage function
(string kernel similarity measure).
may be because the selection method encourages
highly diverse but not very representative subsets.
Furthermore, the entropy-based baseline utilizes a
non-submodular objective function with a heuristic
greedy search method. No theoretical guarantee of
optimality can be made for the subset found by this
method.
Among the different similarity measures the Fisher
kernel outperforms the baseline methods but has
lower performance than the TF-IDF kernel and the
string kernel. The best performance is obtained with
the string kernel, especially when using small train-
ing data sets (2.5%-10%). The submodular selection
methods yield significant improvements (p < 0.05)
over both the random baseline and over the entropy-
based method.
We also investigated using different submodular
functions, i.e. the facility location function and the
saturated coverage function. Figure 2 shows the per-
formance of the facility location (ffac) and saturated
coverage (fSC) functions in combination with the
string kernel similarity measure. The reason ffac
outperforms fSC is that fSC primarily controls for
over-coverage of any element not in the subset via the
? saturation hyper-parameter. However, it does not
ensure that every non-selected element has good rep-
resentation in the subset. fSC measures the quality of
the subset by how well each individual element out-
side the subset has a surrogate within the subset (via
724
40
45
50
55
60
65
2.5p 5p 10p 20p 30p
40
45
50
55
60
65
2.5p 5p 10p 20p 30p
40
45
50
55
60
65
2.5p 5p 10p 20p 30p
40
45
50
55
60
65
2.5p 5p 10p 20p 30p
TF-IDF bigramTF-IDF unigram
string kernelTF-IDF trigram
Figure 3: Phone accuracy for true vs. hypothesized phone
labels, for string-based similarity measures.
the max function) and hence tends to model complete
coverage better, leading to better results.
Finally we examined whether using hypothesized
phone sequences vs. the true transcriptions has nega-
tive effects. Figure 3 shows that this is not the case:
interestingly, the hypothesized labels even result in
slightly better results. This may be because the rec-
ognized phone sequences are a function of both the
underlying phonetic sequences that were spoken and
the acoustic signal characteristics, such as the speaker
and channel. The true transcriptions, on the other
hand, are able to provide information only about pho-
netic as opposed to acoustic characteristics.
6 Discussion
We have presented a low-resource framework for
acoustic data subset selection based on submodular
function optimization, which was previously devel-
oped for document summarization. Evaluation on a
proof-of-concept task has shown that the method is
successful at selecting data subsets that outperform
subsets selected randomly or by a previously pro-
posed low-resource method. We note that the best
selection strategies for the experimental conditions
tested here involve similarity measures based on a
discrete tokenization of the speech signal rather than
direct acoustic similarity measures.
Acknowledgments
This material is based on research sponsored by
Intelligence Advanced Research Projects Activity
(IARPA) under agreement number FA8650-12-2-
7263. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental
purposes notwithstanding any copyright notation
thereon. The views and conclusions contained herein
are those of the authors and should not be interpreted
as necessarily representing the official policies or
endorsements, either expressed or implied, of Intelli-
gence Advanced Research Projects Activity (IARPA)
or the U.S. Government.
References
B. Chen, S.H Liu, and F.H. Chu. 2009. Training data se-
lection for improving discriminative training of acoustic
models. Pattern Recognition Letters, 30:1228?1235.
J. Edmonds, 1970. Combinatorial Structures and their Ap-
plications, chapter Submodular functions, matroids and
certain polyhedra, pages 69?87. Gordon and Breach.
G. Hakkani-Tur, G. Riccardi, and A. Gorin. 2002. Active
learning for automatic speech recognition. In Proc. of
ICASSP, pages 3904?3907.
N. Itoh, T.N. Sainath, D.N. Jiang, J. Zhou, and B. Ramab-
hadran. 2012. N-best entropy based data selection for
acoustic modeling. In Proceedings of ICASSP.
Thomas Kemp and Alex Waibel. 1998. Unsupervised
training of a speech recognizer using TV broadcasts.
In in Proceedings of the International Conference on
Spoken Language Processing (ICSLP-98), pages 2207?
2210.
A. Krause and C. Guestrin. 2011. Submodularity and its
applications in optimized information gathering. ACM
Transactions on Intelligent Systems and Technology,
2(4).
L. Lamel, J.L. Gauvain, and G. Adda. 2002. Lightly
supervised and unsupervised acoustic model training.
Computer, Speech and Language, 16:116 ? 125.
K.F. Lee and H.W. Hon. 1989. Speaker-independent
phone recognition using Hidden Markov Models. IEEE
Trans. ASSP, 37:1641?1648.
Hui Lin and Jeff A. Bilmes. 2009. How to select a good
training-data subset for transcription: Submodular ac-
tive selection for sequences. In Proc. Annual Confer-
ence of the International Speech Communication Asso-
ciation (INTERSPEECH), Brighton, UK, September.
H. Lin and J. Bilmes. 2011. A class of submodular
functions for document summarization. In Proceedings
of ACL.
R.K. Moore. 2003. A comparison of the data require-
ments of automatic speech recognition systems and
human listeners. In Proceedings of Eurospeech, pages
2581?2584.
G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. 1978.
An analysis of approximations for maximizing submod-
ular functions-I. Math. Program., 14:265?294.
725
J. Rousu and J. Shawe-Taylor. 2005. Efficien computa-
tion of of gapped substring kernels for large alphabets.
Journal of Machine Leaning Research, 6:13231344.
Y. Wu, R. Zhang, and A. Rudnicky. 2007. Data selection
for speech recognition. In Proceedings of ASRU.
726
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 510?520,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Class of Submodular Functions for Document Summarization
Hui Lin
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195, USA
hlin@ee.washington.edu
Jeff Bilmes
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195, USA
bilmes@ee.washington.edu
Abstract
We design a class of submodular functions
meant for document summarization tasks.
These functions each combine two terms,
one which encourages the summary to be
representative of the corpus, and the other
which positively rewards diversity. Critically,
our functions are monotone nondecreasing
and submodular, which means that an efficient
scalable greedy optimization scheme has
a constant factor guarantee of optimality.
When evaluated on DUC 2004-2007 corpora,
we obtain better than existing state-of-art
results in both generic and query-focused
document summarization. Lastly, we show
that several well-established methods for
document summarization correspond, in fact,
to submodular function optimization, adding
further evidence that submodular functions are
a natural fit for document summarization.
1 Introduction
In this paper, we address the problem of generic and
query-based extractive summarization from collec-
tions of related documents, a task commonly known
as multi-document summarization. We treat this task
as monotone submodular function maximization (to
be defined in Section 2). This has a number of criti-
cal benefits. On the one hand, there exists a simple
greedy algorithm for monotone submodular func-
tion maximization where the summary solution ob-
tained (say S?) is guaranteed to be almost as good
as the best possible solution (say Sopt) according to
an objective F . More precisely, the greedy algo-
rithm is a constant factor approximation to the car-
dinality constrained version of the problem, so that
F(S?) ? (1 ? 1/e)F(Sopt) ? 0.632F(Sopt). This
is particularly attractive since the quality of the so-
lution does not depend on the size of the problem,
so even very large size problems do well. It is also
important to note that this is a worst case bound, and
in most cases the quality of the solution obtained will
be much better than this bound suggests.
Of course, none of this is useful if the objective
function F is inappropriate for the summarization
task. In this paper, we argue that monotone nonde-
creasing submodular functionsF are an ideal class of
functions to investigate for document summarization.
We show, in fact, that many well-established methods
for summarization (Carbonell and Goldstein, 1998;
Filatova and Hatzivassiloglou, 2004; Takamura and
Okumura, 2009; Riedhammer et al, 2010; Shen and
Li, 2010) correspond to submodular function opti-
mization, a property not explicitly mentioned in these
publications. We take this fact, however, as testament
to the value of submodular functions for summariza-
tion: if summarization algorithms are repeatedly de-
veloped that, by chance, happen to be an instance of a
submodular function optimization, this suggests that
submodular functions are a natural fit. On the other
hand, other authors have started realizing explicitly
the value of submodular functions for summarization
(Lin and Bilmes, 2010; Qazvinian et al, 2010).
Submodular functions share many properties in
common with convex functions, one of which is that
they are closed under a number of common combi-
nation operations (summation, certain compositions,
restrictions, and so on). These operations give us the
tools necessary to design a powerful submodular ob-
jective for submodular document summarization that
extends beyond any previous work. We demonstrate
this by carefully crafting a class of submodular func-
510
tions we feel are ideal for extractive summarization
tasks, both generic and query-focused. In doing so,
we demonstrate better than existing state-of-the-art
performance on a number of standard summarization
evaluation tasks, namely DUC-04 through to DUC-
07. We believe our work, moreover, might act as a
springboard for researchers in summarization to con-
sider the problem of ?how to design a submodular
function? for the summarization task.
In Section 2, we provide a brief background on sub-
modular functions and their optimization. Section 3
describes how the task of extractive summarization
can be viewed as a problem of submodular function
maximization. We also in this section show that many
standard methods for summarization are, in fact, al-
ready performing submodular function optimization.
In Section 4, we present our own submodular func-
tions. Section 5 presents results on both generic and
query-focused summarization tasks, showing as far
as we know the best known ROUGE results for DUC-
04 through DUC-06, and the best known precision
results for DUC-07, and the best recall DUC-07 re-
sults among those that do not use a web search engine.
Section 6 discusses implications for future work.
2 Background on Submodularity
We are given a set of objects V = {v1, . . . , vn} and a
functionF : 2V ? R that returns a real value for any
subset S ? V . We are interested in finding the subset
of bounded size |S| ? k that maximizes the function,
e.g., argmaxS?V F(S). In general, this operation
is hopelessly intractable, an unfortunate fact since
the optimization coincides with many important ap-
plications. For example, F might correspond to the
value or coverage of a set of sensor locations in an
environment, and the goal is to find the best locations
for a fixed number of sensors (Krause et al, 2008).
If the function F is monotone submodular then the
maximization is still NP complete, but it was shown
in (Nemhauser et al, 1978) that a greedy algorithm
finds an approximate solution guaranteed to be within
e?1
e ? 0.63 of the optimal solution, as mentioned
in Section 1. A version of this algorithm (Minoux,
1978), moreover, scales to very large data sets. Sub-
modular functions are those that satisfy the property
of diminishing returns: for anyA ? B ? V \v, a sub-
modular functionF must satisfyF(A+v)?F(A) ?
F(B + v)?F(B). That is, the incremental ?value?
of v decreases as the context in which v is considered
grows from A to B. An equivalent definition, useful
mathematically, is that for any A,B ? V , we must
have that F(A) +F(B) ? F(A ?B) +F(A ?B).
If this is satisfied everywhere with equality, then
the function F is called modular, and in such case
F(A) = c +
?
a?A
~fa for a sized |V | vector ~f of
real values and constant c. A set function F is mono-
tone nondecreasing if ?A ? B, F(A) ? F(B). As
shorthand, in this paper, monotone nondecreasing
submodular functions will simply be referred to as
monotone submodular.
Historically, submodular functions have their roots
in economics, game theory, combinatorial optimiza-
tion, and operations research. More recently, submod-
ular functions have started receiving attention in the
machine learning and computer vision community
(Kempe et al, 2003; Narasimhan and Bilmes, 2005;
Krause and Guestrin, 2005; Narasimhan and Bilmes,
2007; Krause et al, 2008; Kolmogorov and Zabin,
2004) and have recently been introduced to natural
language processing for the tasks of document sum-
marization (Lin and Bilmes, 2010) and word align-
ment (Lin and Bilmes, 2011).
Submodular functions share a number of proper-
ties in common with convex and concave functions
(Lova?sz, 1983), including their wide applicability,
their generality, their multiple options for their repre-
sentation, and their closure under a number of com-
mon operators (including mixtures, truncation, com-
plementation, and certain convolutions). For exam-
ple, if a collection of functions {Fi}i is submodular,
then so is their weighted sum F =
?
i ?iFi where
?i are nonnegative weights. It is not hard to show
that submodular functions also have the following
composition property with concave functions:
Theorem 1. Given functions F : 2V ? R and
f : R? R, the composition F ? = f ? F : 2V ? R
(i.e., F ?(S) = f(F(S))) is nondecreasing sub-
modular, if f is non-decreasing concave and F is
nondecreasing submodular.
This property will be quite useful when defining sub-
modular functions for document summarization.
511
3 Submodularity in Summarization
3.1 Summarization with knapsack constraint
Let the ground set V represents all the sentences
(or other linguistic units) in a document (or docu-
ment collection, in the multi-document summariza-
tion case). The task of extractive document sum-
marization is to select a subset S ? V to represent
the entirety (ground set V ). There are typically con-
straints on S, however. Obviously, we should have
|S| < |V | = N as it is a summary and should
be small. In standard summarization tasks (e.g.,
DUC evaluations), the summary is usually required
to be length-limited. Therefore, constraints on S
can naturally be modeled as knapsack constraints:
?
i?S ci ? b, where ci is the non-negative cost of
selecting unit i (e.g., the number of words in the sen-
tence) and b is our budget. If we use a set function
F : 2V ? R to measure the quality of the summary
set S, the summarization problem can then be for-
malized as the following combinatorial optimization
problem:
Problem 1. Find
S? ? argmax
S?V
F(S) subject to:
?
i?S
ci ? b.
Since this is a generalization of the cardinality
constraint (where ci = 1,?i), this also constitutes
a (well-known) NP-hard problem. In this case as
well, however, a modified greedy algorithm with par-
tial enumeration can solve Problem 1 near-optimally
with (1?1/e)-approximation factor ifF is monotone
submodular (Sviridenko, 2004). The partial enumer-
ation, however, is too computationally expensive for
real world applications. In (Lin and Bilmes, 2010),
we generalize the work by Khuller et al (1999) on
the budgeted maximum cover problem to the gen-
eral submodular framework, and show a practical
greedy algorithm with a (1? 1/?e)-approximation
factor, where each greedy step adds the unit with the
largest ratio of objective function gain to scaled cost,
while not violating the budget constraint (see (Lin
and Bilmes, 2010) for details). Note that in all cases,
submodularity and monotonicity are two necessary
ingredients to guarantee that the greedy algorithm
gives near-optimal solutions.
In fact, greedy-like algorithms have been widely
used in summarization. One of the more popular
approaches is maximum marginal relevance (MMR)
(Carbonell and Goldstein, 1998), where a greedy
algorithm selects the most relevant sentences, and
at the same time avoids redundancy by removing
sentences that are too similar to ones already selected.
Interestingly, the gain function defined in the original
MMR paper (Carbonell and Goldstein, 1998) satisfies
diminishing returns, a fact apparently unnoticed until
now. In particular, Carbonell and Goldstein (1998)
define an objective function gain of adding element
k to set S (k /? S) as:
?Sim1(sk, q)? (1? ?) max
i?S
Sim2(si, sk), (1)
where Sim1(sk, q) measures the similarity between
unit sk to a query q, Sim2(si, sk) measures the simi-
larity between unit si and unit sk, and 0 ? ? ? 1 is
a trade-off coefficient. We have:
Theorem 2. Given an expression forFMMR such that
FMMR(S ?{k})?FMMR(S) is equal to Eq. 1, FMMR
is non-monotone submodular.
Obviously, diminishing-returns hold since
max
i?S
Sim2(si, sk) ? max
i?R
Sim2(si, sk)
for all S ? R, and therefore FMMR is submodular.
On the other hand,FMMR, would not be monotone, so
the greedy algorithm?s constant-factor approximation
guarantee does not apply in this case.
When scoring a summary at the sub-sentence
level, submodularity naturally arises. Concept-based
summarization (Filatova and Hatzivassiloglou, 2004;
Takamura and Okumura, 2009; Riedhammer et al,
2010; Qazvinian et al, 2010) usually maximizes the
weighted credit of concepts covered by the summary.
Although the authors may not have noticed, their ob-
jective functions are also submodular, adding more
evidence suggesting that submodularity is natural for
summarization tasks. Indeed, let S be a subset of
sentences in the document and denote ?(S) as the
set of concepts contained in S. The total credit of the
concepts covered by S is then
Fconcept(S) ,
?
i??(S)
ci,
where ci is the credit of concept i. This function is
known to be submodular (Narayanan, 1997).
512
Similar to the MMR approach, in (Lin and Bilmes,
2010), a submodular graph based objective function
is proposed where a graph cut function, measuring
the similarity of the summary to the rest of document,
is combined with a subtracted redundancy penalty
function. The objective function is submodular but
again, non-monotone. We theoretically justify that
the performance guarantee of the greedy algorithm
holds for this objective function with high probability
(Lin and Bilmes, 2010). Our justification, however,
is shown to be applicable only to certain particular
non-monotone submodular functions, under certain
reasonable assumptions about the probability distri-
bution over weights of the graph.
3.2 Summarization with covering constraint
Another perspective is to treat the summarization
problem as finding a low-cost subset of the document
under the constraint that a summary should cover
all (or a sufficient amount of) the information in the
document. Formally, this can be expressed as
Problem 2. Find
S? ? argmin
S?V
?
i?S
ci subject to: F(S) ? ?,
where ci are the element costs, and set function F(S)
measure the information covered by S. When F
is submodular, the constraint F(S) ? ? is called
a submodular cover constraint. When F is mono-
tone submodular, a greedy algorithm that iteratively
selects k with minimum ck/(F(S ? {k}) ? F(S))
has approximation guarantees (Wolsey, 1982). Re-
cent work (Shen and Li, 2010) proposes to model
document summarization as finding a minimum dom-
inating set and a greedy algorithm is used to solve
the problem. The dominating set constraint is also
a submodular cover constraint. Define ?(S) be the
set of elements that is either in S or is adjacent to
some element in S. Then S is a dominating set if
|?(S)| = |V |. Note that
Fdom(S) , |?(S)|
is monotone submodular. The dominating set
constraint is then also a submodular cover constraint,
and therefore the approaches in (Shen and Li, 2010)
are special cases of Problem 2. The solutions found
in this framework, however, do not necessarily
satisfy a summary?s budget constraint. Consequently,
a subset of the solution found by solving Problem 2
has to be constructed as the final summary, and the
near-optimality is no longer guaranteed. Therefore,
solving Problem 1 for document summarization
appears to be a better framework regarding global
optimality. In the present paper, our framework is
that of Problem 1.
3.3 Automatic summarization evaluation
Automatic evaluation of summary quality is impor-
tant for the research of document summarization as
it avoids the labor-intensive and potentially inconsis-
tent human evaluation. ROUGE (Lin, 2004) is widely
used for summarization evaluation and it has been
shown that ROUGE-N scores are highly correlated
with human evaluation (Lin, 2004). Interestingly,
ROUGE-N is monotone submodular, adding further
evidence that monotone submodular functions are
natural for document summarization.
Theorem 3. ROUGE-N is monotone submodular.
Proof. By definition (Lin, 2004), ROUGE-N is the
n-gram recall between a candidate summary and a
set of reference summaries. Precisely, let S be the
candidate summary (a set of sentences extracted from
the ground set V ), ce : 2V ? Z+ be the number of
times n-gram e occurs in summary S, and Ri be the
set of n-grams contained in the reference summary i
(suppose we have K reference summaries, i.e., i =
1, ? ? ? ,K). Then ROUGE-N can be written as the
following set function:
FROUGE-N(S) ,
?K
i=1
?
e?Ri
min(ce(S), re,i)
?K
i=1
?
e?Ri
re,i
,
where re,i is the number of times n-gram e occurs
in reference summary i. Since ce(S) is monotone
modular and min(x, a) is a concave non-decreasing
function of x, min(ce(S), re,i) is monotone sub-
modular by Theorem 1. Since summation preserves
submodularity, and the denominator is constant, we
see that FROUGE-N is monotone submodular.
Since the reference summaries are unknown, it is
of course impossible to optimize FROUGE-N directly.
Therefore, some approaches (Filatova and Hatzivas-
siloglou, 2004; Takamura and Okumura, 2009; Ried-
hammer et al, 2010) instead define ?concepts?. Alter-
513
natively, we herein propose a class of monotone sub-
modular functions that naturally models the quality of
a summary while not depending on an explicit notion
of concepts, as we will see in the following section.
4 Monotone Submodular Objectives
Two properties of a good summary are relevance and
non-redundancy. Objective functions for extractive
summarization usually measure these two separately
and then mix them together trading off encouraging
relevance and penalizing redundancy. The redun-
dancy penalty usually violates the monotonicity of
the objective functions (Carbonell and Goldstein,
1998; Lin and Bilmes, 2010). We therefore propose
to positively reward diversity instead of negatively
penalizing redundancy. In particular, we model the
summary quality as
F(S) = L(S) + ?R(S), (2)
where L(S) measures the coverage, or ?fidelity?,
of summary set S to the document, R(S) rewards
diversity in S, and ? ? 0 is a trade-off coefficient.
Note that the above is analogous to the objectives
widely used in machine learning, where a loss
function that measures the training set error (we
measure the coverage of summary to a document),
is combined with a regularization term encouraging
certain desirable (e.g., sparsity) properties (in
our case, we ?regularize? the solution to be more
diverse). In the following, we discuss how both L(S)
andR(S) are naturally monotone submodular.
4.1 Coverage function
L(S) can be interpreted either as a set function that
measures the similarity of summary set S to the docu-
ment to be summarized, or as a function representing
some form of ?coverage? of V by S. Most naturally,
L(S) should be monotone, as coverage improves
with a larger summary. L(S) should also be submod-
ular: consider adding a new sentence into two sum-
mary sets, one a subset of the other. Intuitively, the
increment when adding a new sentence to the small
summary set should be larger than the increment
when adding it to the larger set, as the information
carried by the new sentence might have already been
covered by those sentences that are in the larger sum-
mary but not in the smaller summary. This is exactly
the property of diminishing returns. Indeed, Shan-
non entropy, as the measurement of information, is
another well-known monotone submodular function.
There are several ways to define L(S) in our
context. For instance, we could use L(S) =
?
i?V,j?S wi,j where wi,j represents the similarity
between i and j. L(S) could also be facility
location objective, i.e., L(S) =
?
i?V maxj?S wi,j ,
as used in (Lin et al, 2009). We could also use
L(S) =
?
i??(S) ci as used in concept-based
summarization, where the definition of ?concept?
and the mechanism to extract these concepts become
important. All of these are monotone submodular.
Alternatively, in this paper we propose the follow-
ing objective that does not reply on concepts. Let
L(S) =
?
i?V
min {Ci(S), ? Ci(V )} , (3)
where Ci : 2V ? R is a monotone submodular func-
tion and 0 ? ? ? 1 is a threshold co-efficient. Firstly,
L(S) as defined in Eqn. 3 is a monotone submodular
function. The monotonicity is immediate. To see that
L(S) is submodular, consider the fact that f(x) =
min(x, a) where a ? 0 is a concave non-decreasing
function, and by Theorem 1, each summand in Eqn. 3
is a submodular function, and as summation pre-
serves submodularity, L(S) is submodular.
Next, we explain the intuition behind Eqn. 3. Basi-
cally, Ci(S) measures how similar S is to element i,
or how much of i is ?covered? by S. Then Ci(V ) is
just the largest value that Ci(S) can achieve. We call
i ?saturated? by S when min{Ci(S), ?Ci(V )} =
?Ci(V ). When i is already saturated in this way,
any new sentence j can not further improve the
coverage of i even if it is very similar to i (i.e.,
Ci(S ? {j}) ? Ci(S) is large). This will give other
sentences that are not yet saturated a higher chance
of being better covered, and therefore the resulting
summary tends to better cover the entire document.
One simple way to define Ci(S) is just to use
Ci(S) =
?
j?S
wi,j (4)
where wi,j ? 0 measures the similarity between i
and j. In this case, when ? = 1, Eqn. 3 reduces
to the case where L(S) =
?
i?V,j?S wi,j . As we
will see in Section 5, having an ? that is less than
514
1 significantly improves the performance compared
to the case when ? = 1, which coincides with our
intuition that using a truncation threshold improves
the final summary?s coverage.
4.2 Diversity reward function
Instead of penalizing redundancy by subtracting from
the objective, we propose to reward diversity by
adding the following to the objective:
R(S) =
K?
i=1
? ?
j?Pi?S
rj . (5)
where Pi, i = 1, ? ? ?K is a partition of the ground
set V (i.e.,
?
i Pi = V and the Pis are disjoint) into
separate clusters, and ri ? 0 indicates the singleton
reward of i (i.e., the reward of adding i into the empty
set). The value ri estimates the importance of i to
the summary. The functionR(S) rewards diversity
in that there is usually more benefit to selecting a
sentence from a cluster not yet having one of its
elements already chosen. As soon as an element
is selected from a cluster, other elements from the
same cluster start having diminishing gain, thanks
to the square root function. For instance, consider
the case where k1, k2 ? P1, k3 ? P2, and rk1 = 4,
rk2 = 9, and rk3 = 4. Assume k1 is already in the
summary set S. Greedily selecting the next element
will choose k3 rather than k2 since
?
13 < 2 + 2. In
other words, adding k3 achieves a greater reward as it
increases the diversity of the summary (by choosing
from a different cluster). Note,R(S) is distinct from
L(S) in that R(S) might wish to include certain
outlier material that L(S) could ignore.
It is easy to show that R(S) is submodular by
using the composition rule from Theorem 1. The
square root is non-decreasing concave function.
Inside each square root lies a modular function
with non-negative weights (and thus is monotone).
Applying the square root to such a monotone sub-
modular function yields a submodular function, and
summing them all together retains submodularity, as
mentioned in Section 2. The monotonicity ofR(S)
is straightforward. Note, the form of Eqn. 5 is similar
to structured group norms (e.g., (Zhao et al, 2009)),
recently shown to be related to submodularity (Bach,
2010; Jegelka and Bilmes, 2011).
Several extensions to Eqn. 5 are discussed
next: First, instead of using a ground set partition,
intersecting clusters can be used. Second, the
square root function in Eqn. 5 can be replaced with
any other non-decreasing concave functions (e.g.,
f(x) = log(1 + x)) while preserving the desired
property ofR(S), and the curvature of the concave
function then determines the rate that the reward
diminishes. Last, multi-resolution clustering (or
partitions) with different sizes (K) can be used, i.e.,
we can use a mixture of components, each of which
has the structure of Eqn. 5. A mixture can better
represent the core structure of the ground set (e.g.,
the hierarchical structure in the documents (Celiky-
ilmaz and Hakkani-tu?r, 2010)). All such extensions
preserve both monotonicity and submodularity.
5 Experiments
The document understanding conference (DUC)
(http://duc.nist.org) was the main forum
providing benchmarks for researchers working
on document summarization. The tasks in DUC
evolved from single-document summarization to
multi-document summarization, and from generic
summarization (2001?2004) to query-focused sum-
marization (2005?2007). As ROUGE (Lin, 2004)
has been officially adopted for DUC evaluations
since 2004, we also take it as our main evaluation
criterion. We evaluated our approaches on DUC
data 2003-2007, and demonstrate results on both
generic and query-focused summarization. In all
experiments, the modified greedy algorithm (Lin and
Bilmes, 2010) was used for summary generation.
5.1 Generic summarization
Summarization tasks in DUC-03 and DUC-04 are
multi-document summarization on English news
articles. In each task, 50 document clusters are
given, each of which consists of 10 documents.
For each document cluster, the system generated
summary may not be longer than 665 bytes including
spaces and punctuation. We used DUC-03 as
our development set, and tested on DUC-04 data.
We show ROUGE-1 scores1 as it was the main
evaluation criterion for DUC-03, 04 evaluations.
1ROUGE version 1.5.5 with options: -a -c 95 -b 665 -m -n 4
-w 1.2
515
Documents were pre-processed by segmenting sen-
tences and stemming words using the Porter Stemmer.
Each sentence was represented using a bag-of-terms
vector, where we used context terms up to bi-grams.
Similarity between sentence i and sentence j, i.e.,
wi,j , was computed using cosine similarity:
wi,j =
?
w?si
tfw,i ? tfw,j ? idf2w
??
w?si
tf2w,si idf
2
w
??
w?sj
tf2w,j idf
2
w
,
where tfw,i and tfw,j are the numbers of times that
w appears in si and sentence sj respectively, and
idfw is the inverse document frequency (IDF) of
term w (up to bigram), which was calculated as the
logarithm of the ratio of the number of articles that
w appears over the total number of all articles in the
document cluster.
Table 1: ROUGE-1 recall (R) and F-measure (F) results
(%) on DUC-04. DUC-03 was used as development set.
DUC-04 R F
P
i?V
P
j?S wi,j 33.59 32.44
L1(S) 39.03 38.65
R1(S) 38.23 37.81
L1(S) + ?R1(S) 39.35 38.90
Takamura and Okumura (2009) 38.50 -
Wang et al (2009) 39.07 -
Lin and Bilmes (2010) - 38.39
Best system in DUC-04 (peer 65) 38.28 37.94
We first tested our coverage and diversity re-
ward objectives separately. For coverage, we use a
modular Ci(S) =
?
j?S wi,j for each sentence i, i.e.,
L1(S) =
?
i?V
min
?
?
?
?
j?S
wi,j , ?
?
k?V
wi,k
?
?
?
. (6)
When ? = 1, L1(S) reduces to
?
i?V,j?S wi,j ,
which measures the overall similarity of summary
set S to ground set V . As mentioned in Section 4.1,
using such similarity measurement could possibly
over-concentrate on a small portion of the document
and result in a poor coverage of the whole document.
As shown in Table 1, optimizing this objective
function gives a ROUGE-1 F-measure score 32.44%.
On the other hand, when using L1(S) with an ? < 1
(the value of ? was determined on DUC-03 using
a grid search), a ROUGE-1 F-measure score 38.65%
36.2
36.4
36.6
36.8
37
37.2
37.4
37.6
0 5 10 15 20
RO
UG
E-1
 F-
me
asu
re 
(%
)
K=0.05N
K=0.1N
K=0.2N
a
Figure 1: ROUGE-1 F-measure scores on DUC-03 when
? and K vary in objective function L1(S) + ?R1(S),
where ? = 6 and ? = aN .
is achieved, which is already better than the best
performing system in DUC-04.
As for the diversity reward objective, we define
the singleton reward as ri = 1N
?
j wi,j , which is
the average similarity of sentence i to the rest of the
document. It basically states that the more similar to
the whole document a sentence is, the more reward
there will be by adding this sentence to an empty
summary set. By using this singleton reward, we
have the following diversity reward function:
R1(S) =
K?
k=1
?
?
j?S?Pk
1
N
?
i?V
wi,j . (7)
In order to generate Pk, k = 1, ? ? ?K, we used
CLUTO2 to cluster the sentences, where the IDF-
weighted term vector was used as feature vector, and
a direct K-mean clustering algorithm was used. In
this experiment, we set K = 0.2N . In other words,
there are 5 sentences in each cluster on average.
And as we can see in Table 1, optimizing the
diversity reward function alone achieves comparable
performance to the DUC-04 best system.
Combining L1(S) and R1(S), our system outper-
forms the best system in DUC-04 significantly, and
it also outperforms several recent systems, including
a concept-based summarization approach (Takamura
and Okumura, 2009), a sentence topic model based
system (Wang et al, 2009), and our MMR-styled
submodular system (Lin and Bilmes, 2010). Figure 1
illustrates how ROUGE-1 scores change when ? and
K vary on the development set (DUC-03).
2http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview
516
Table 2: ROUGE-2 recall (R) and F-measure (F) results
(%) on DUC-05, where DUC-05 was used as training set.
DUC-05 R F
L1(S) + ?RQ(S) 8.38 8.31
Daume? III and Marcu (2006) 7.62 -
Extr, Daume? et al (2009) 7.67 -
Vine, Daume? et al (2009) 8.24 -
Table 3: ROUGE-2 recall (R) and F-measure (F) results
on DUC-05 (%). We used DUC-06 as training set.
DUC-05 R F
L1(S) + ?RQ(S) 7.82 7.72
Daume? III and Marcu (2006) 6.98 -
Best system in DUC-05 (peer 15) 7.44 7.43
5.2 Query-focused summarization
We evaluated our approach on the task of query-
focused summarization using DUC 05-07 data. In
DUC-05 and DUC-06, participants were given 50
document clusters, where each cluster contains 25
news articles related to the same topic. Participants
were asked to generate summaries of at most 250
words for each cluster. For each cluster, a title and
a narrative describing a user?s information need are
provided. The narrative is usually composed of a
set of questions or a multi-sentence task description.
The main task in DUC-07 is the same as in DUC-06.
In DUC 05-07, ROUGE-2 was the primary
criterion for evaluation, and thus we also report
ROUGE-23 (both recall R, and precision F). Docu-
ments were processed as in Section 5.1. We used both
the title and the narrative as query, where stop words,
including some function words (e.g., ?describe?) that
appear frequently in the query, were removed. All
queries were then stemmed using the Porter Stemmer.
Note that there are several ways to incorporate
query-focused information into both the coverage
and diversity reward objectives. For instance, Ci(S)
could be query-dependent in how it measures how
much query-dependent information in i is covered
by S. Also, the coefficient ? could be query and sen-
tence dependent, where it takes larger value when a
sentence is more relevant to query (i.e., a larger value
of ? means later truncation, and therefore more pos-
sible coverage). Similarly, sentence clustering and
singleton rewards in the diversity function can also
3ROUGE version 1.5.5 was used with option -n 2 -x -m -2 4
-u -c 95 -r 1000 -f A -p 0.5 -t 0 -d -l 250
Table 4: ROUGE-2 recall (R) and F-measure (F) results
(%) on DUC-06, where DUC-05 was used as training set.
DUC-06 R F
L1(S) + ?RQ(S) 9.75 9.77
Celikyilmaz and Hakkani-tu?r (2010) 9.10 -
Shen and Li (2010) 9.30 -
Best system in DUC-06 (peer 24) 9.51 9.51
Table 5: ROUGE-2 recall (R) and F-measure (F) re-
sults (%) on DUC-07. DUC-05 was used as training
set for objective L1(S) + ?RQ(S). DUC-05 and DUC-
06 were used as training sets for objective L1(S) +?
? ??RQ,?(S).
DUC-07 R F
L1(S) + ?RQ(S) 12.18 12.13
L1(S) +
P3
?=1 ??RQ,?(S) 12.38 12.33
Toutanova et al (2007) 11.89 11.89
Haghighi and Vanderwende (2009) 11.80 -
Celikyilmaz and Hakkani-tu?r (2010) 11.40 -
Best system in DUC-07 (peer 15) 12.45 12.29
be query-dependent. In this experiment, we explore
an objective with a query-independent coverage func-
tion (R1(S)), indicating prior importance, combined
with a query-dependent diversity reward function,
where the latter is defined as:
RQ(S) =
K?
k=1
?
?
?
?
?
j?S?Pk
(
?
N
?
i?V
wi,j + (1? ?)rj,Q
)
,
where 0 ? ? ? 1, and rj,Q represents the rel-
evance between sentence j to query Q. This
query-dependent reward function is derived by
using a singleton reward that is expressed as a
convex combination of the query-independent score
( 1N
?
i?V wi,j) and the query-dependent score (rj,Q)
of a sentence. We simply used the number of
terms (up to a bi-gram) that sentence j overlaps the
query Q as rj,Q, where the IDF weighting is not
used (i.e., every term in the query, after stop word
removal, was treated as equally important). Both
query-independent and query-dependent scores were
then normalized by their largest value respectively
such that they had roughly the same dynamic range.
To better estimate of the relevance between query
and sentences, we further expanded sentences with
synonyms and hypernyms of its constituent words. In
particular, part-of-speech tags were obtained for each
sentence using the maximum entropy part-of-speech
tagger (Ratnaparkhi, 1996), and all nouns were then
517
expanded with their synonyms and hypernyms using
WordNet (Fellbaum, 1998). Note that these expanded
documents were only used in the estimation rj,Q, and
we plan to further explore whether there is benefit to
use the expanded documents either in sentence sim-
ilarity estimation or in sentence clustering in our fu-
ture work. We also tried to expand the query with syn-
onyms and observed a performance decrease, presum-
ably due to noisy information in a query expression.
While it is possible to use an approach that is
similar to (Toutanova et al, 2007) to learn the
coefficients in our objective function, we trained all
coefficients to maximize ROUGE-2 F-measure score
using the Nelder-Mead (derivative-free) method.
Using L1(S)+?RQ(S) as the objective and with the
same sentence clustering algorithm as in the generic
summarization experiment (K = 0.2N ), our system,
when both trained and tested on DUC-05 (results in
Table 2), outperforms the Bayesian query-focused
summarization approach and the search-based
structured prediction approach, which were also
trained and tested on DUC-05 (Daume? et al, 2009).
Note that the system in (Daume? et al, 2009) that
achieves its best performance (8.24% in ROUGE-2
recall) is a so called ?vine-growth? system, which
can be seen as an abstractive approach, whereas our
system is purely an extractive system. Comparing
to the extractive system in (Daume? et al, 2009), our
system performs much better (8.38% v.s. 7.67%).
More importantly, when trained only on DUC-06 and
tested on DUC-05 (results in Table 3), our approach
outperforms the best system in DUC-05 significantly.
We further tested the system trained on DUC-05
on both DUC-06 and DUC-07. The results on
DUC-06 are shown in Table. 4. Our system outper-
forms the best system in DUC-06, as well as two
recent approaches (Shen and Li, 2010; Celikyilmaz
and Hakkani-tu?r, 2010). On DUC-07, in terms of
ROUGE-2 score, our system outperforms PYTHY
(Toutanova et al, 2007), a state-of-the-art supervised
summarization system, as well as two recent systems
including a generative summarization system based
on topic models (Haghighi and Vanderwende,
2009), and a hybrid hierarchical summarization
system (Celikyilmaz and Hakkani-tu?r, 2010). It
also achieves comparable performance to the best
DUC-07 system. Note that in the best DUC-07
system (Pingali et al, 2007; Jagarlamudi et al, 2006),
an external web search engine (Yahoo!) was used
to estimate a language model for query relevance. In
our system, no such web search expansion was used.
To further improve the performance of our system,
we used both DUC-05 and DUC-06 as a training
set, and introduced three diversity reward terms
into the objective where three different sentence
clusterings with different resolutions were produced
(with sizes 0.3N, 0.15N and 0.05N ). Denoting
a diversity reward corresponding to clustering ?
as RQ,?(S), we model the summary quality as
L1(S) +
?3
?=1 ??RQ,?(S). As shown in Table 5,
using this objective function with multi-resolution
diversity rewards improves our results further, and
outperforms the best system in DUC-07 in terms of
ROUGE-2 F-measure score.
6 Conclusion and discussion
In this paper, we show that submodularity naturally
arises in document summarization. Not only do
many existing automatic summarization methods cor-
respond to submodular function optimization, but
also the widely used ROUGE evaluation is closely
related to submodular functions. As the correspond-
ing submodular optimization problem can be solved
efficiently and effectively, the remaining question
is then how to design a submodular objective that
best models the task. To address this problem, we
introduce a powerful class of monotone submodular
functions that are well suited to document summariza-
tion by modeling two important properties of a sum-
mary, fidelity and diversity. While more advanced
NLP techniques could be easily incorporated into our
functions (e.g., language models could define a better
Ci(S), more advanced relevance estimations for the
singleton rewards ri, and better and/or overlapping
clustering algorithms for our diversity reward), we
already show top results on standard benchmark eval-
uations using fairly basic NLP methods (e.g., term
weighting and WordNet expansion), all, we believe,
thanks to the power and generality of submodular
functions. As information retrieval and web search
are closely related to query-focused summarization,
our approach might be beneficial in those areas as
well.
518
References
F. Bach. 2010. Structured sparsity-inducing norms
through submodular functions. Advances in Neural
Information Processing Systems.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents and
producing summaries. In Proc. of SIGIR.
A. Celikyilmaz and D. Hakkani-tu?r. 2010. A hybrid hier-
archical model for multi-document summarization. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 815?824,
Uppsala, Sweden, July. Association for Computational
Linguistics.
H. Daume?, J. Langford, and D. Marcu. 2009. Search-
based structured prediction. Machine learning,
75(3):297?325.
H. Daume? III and D. Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the 21st
International Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, page 312.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. The MIT press.
E. Filatova and V. Hatzivassiloglou. 2004. Event-based
extractive summarization. In Proceedings of ACL Work-
shop on Summarization, volume 111.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362?370, Boulder, Colorado, June. Association
for Computational Linguistics.
J. Jagarlamudi, P. Pingali, and V. Varma. 2006. Query
independent sentence scoring approach to DUC 2006.
In DUC 2006.
S. Jegelka and J. A. Bilmes. 2011. Submodularity beyond
submodular energies: coupling edges in graph cuts.
In Computer Vision and Pattern Recognition (CVPR),
Colorado Springs, CO, June.
D. Kempe, J. Kleinberg, and E. Tardos. 2003. Maximiz-
ing the spread of influence through a social network.
In Proceedings of the 9th Conference on SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD).
S. Khuller, A. Moss, and J. Naor. 1999. The budgeted
maximum coverage problem. Information Processing
Letters, 70(1):39?45.
V. Kolmogorov and R. Zabin. 2004. What energy func-
tions can be minimized via graph cuts? IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
26(2):147?159.
A. Krause and C. Guestrin. 2005. Near-optimal nonmy-
opic value of information in graphical models. In Proc.
of Uncertainty in AI.
A. Krause, H.B. McMahan, C. Guestrin, and A. Gupta.
2008. Robust submodular observation selection. Jour-
nal of Machine Learning Research, 9:2761?2801.
H. Lin and J. Bilmes. 2010. Multi-document summariza-
tion via budgeted maximization of submodular func-
tions. In North American chapter of the Association
for Computational Linguistics/Human Language Tech-
nology Conference (NAACL/HLT-2010), Los Angeles,
CA, June.
H. Lin and J. Bilmes. 2011. Word alignment via submod-
ular maximization over matroids. In The 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT), Port-
land, OR, June.
H. Lin, J. Bilmes, and S. Xie. 2009. Graph-based submod-
ular selection for extractive summarization. In Proc.
IEEE Automatic Speech Recognition and Understand-
ing (ASRU), Merano, Italy, December.
C.-Y. Lin. 2004. ROUGE: A package for automatic eval-
uation of summaries. In Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop.
L. Lova?sz. 1983. Submodular functions and convexity.
Mathematical programming-The state of the art,(eds. A.
Bachem, M. Grotschel and B. Korte) Springer, pages
235?257.
M. Minoux. 1978. Accelerated greedy algorithms for
maximizing submodular set functions. Optimization
Techniques, pages 234?243.
M. Narasimhan and J. Bilmes. 2005. A submodular-
supermodular procedure with applications to discrimi-
native structure learning. In Proc. Conf. Uncertainty in
Artifical Intelligence, Edinburgh, Scotland, July. Mor-
gan Kaufmann Publishers.
M. Narasimhan and J. Bilmes. 2007. Local search for
balanced submodular clusterings. In Twentieth Inter-
national Joint Conference on Artificial Intelligence (IJ-
CAI07), Hyderabad, India, January.
H. Narayanan. 1997. Submodular functions and electrical
networks. North-Holland.
G.L. Nemhauser, L.A. Wolsey, and M.L. Fisher. 1978. An
analysis of approximations for maximizing submodular
set functions I. Mathematical Programming, 14(1):265?
294.
P. Pingali, K. Rahul, and V. Varma. 2007. IIIT Hyderabad
at DUC 2007. Proceedings of DUC 2007.
V. Qazvinian, D.R. Radev, and A. Ozgu?r. 2010. Cita-
tion Summarization Through Keyphrase Extraction. In
Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010), pages 895?
903.
519
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In EMNLP, volume 1, pages
133?142.
K. Riedhammer, B. Favre, and D. Hakkani-Tu?r. 2010.
Long story short-Global unsupervised models for
keyphrase based meeting summarization. Speech Com-
munication.
C. Shen and T. Li. 2010. Multi-document summarization
via the minimum dominating set. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 984?992, Beijing, China,
August. Coling 2010 Organizing Committee.
M. Sviridenko. 2004. A note on maximizing a submodu-
lar set function subject to a knapsack constraint. Oper-
ations Research Letters, 32(1):41?43.
H. Takamura and M. Okumura. 2009. Text summariza-
tion model based on maximum coverage problem and
its variant. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 781?789. Association for
Computational Linguistics.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The PYTHY
summarization system: Microsoft research at DUC
2007. In the proceedings of Document Understanding
Conference.
D. Wang, S. Zhu, T. Li, and Y. Gong. 2009. Multi-
document summarization using sentence-based topic
models. In Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, pages 297?300, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
L.A. Wolsey. 1982. An analysis of the greedy algorithm
for the submodular set covering problem. Combinator-
ica, 2(4):385?393.
P. Zhao, G. Rocha, and B. Yu. 2009. Grouped and hier-
archical model selection through composite absolute
penalties. Annals of Statistics, 37(6A):3468?3497.
520
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 170?175,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Word Alignment via Submodular Maximization over Matroids
Hui Lin
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195, USA
hlin@ee.washington.edu
Jeff Bilmes
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195, USA
bilmes@ee.washington.edu
Abstract
We cast the word alignment problem as max-
imizing a submodular function under matroid
constraints. Our framework is able to express
complex interactions between alignment com-
ponents while remaining computationally ef-
ficient, thanks to the power and generality of
submodular functions. We show that submod-
ularity naturally arises when modeling word
fertility. Experiments on the English-French
Hansards alignment task show that our ap-
proach achieves lower alignment error rates
compared to conventional matching based ap-
proaches.
1 Introduction
Word alignment is a key component in most statisti-
cal machine translation systems. While classical ap-
proaches for word alignment are based on generative
models (e.g., IBM models (Brown et al, 1993) and
HMM (Vogel et al, 1996)), word alignment can also
be viewed as a matching problem, where each word
pair is associated with a score reflecting the desirabil-
ity of aligning that pair, and the alignment is then the
highest scored matching under some constraints.
Several matching-based approaches have been
proposed in the past. Melamed (2000) introduces
the competitive linking algorithm which greedily
constructs matchings under the one-to-one mapping
assumption. In (Matusov et al, 2004), matchings
are found using an algorithm for constructing
a maximum weighted bipartite graph matching
(Schrijver, 2003), where word pair scores come from
alignment posteriors of generative models. Similarly,
Taskar et al (2005) cast word alignment as a
maximum weighted matching problem and propose a
framework for learning word pair scores as a function
of arbitrary features of that pair. These approaches,
however, have two potentially substantial limitations:
words have fertility of at most one, and interactions
between alignment decisions are not representable.
Lacoste-Julien et al (2006) address this issue by
formulating the alignment problem as a quadratic
assignment problem, and off-the-shelf integer linear
programming (ILP) solvers are used to solve to op-
timization problem. While efficient for some median
scale problems, ILP-based approaches are limited
since when modeling more sophisticated interactions,
the number of variables (and/or constraints) required
grows polynomially, or even exponentially, making
the resultant optimization impractical to solve.
In this paper, we treat the word alignment problem
as maximizing a submodular function subject to
matroid constraints (to be defined in Section 2).
Submodular objective functions can represent
complex interactions among alignment decisions,
and essentially extend the modular (linear) objectives
used in the aforementioned approaches. While our
extensions add expressive power, they do not result
in a heavy computational burden. This is because
maximizing a monotone submodular function under
a matroid constraint can be solved efficiently using
a simple greedy algorithm. The greedy algorithm,
moreover, is a constant factor approximation
algorithm that guarantees a near-optimal solution.
In this paper, we moreover show that submodularity
naturally arises in word alignment problems when
modeling word fertility (see Section 4). Experiment
results on the English-French Hansards alignment
task show that our approach achieves lower align-
ment error rates compared to the maximum weighted
matching approach, while being at least 50 times
170
faster than an ILP-based approach.
2 Background
Matroids and submodularity both play important
roles in combinatorial optimization. We briefly in-
troduce them here, referring the reader to (Schrijver,
2003) for details.
Matroids are combinatorial structures that general-
ize the notion of linear independence in matrices. A
pair (V, I) is called a matroid if V is a finite ground
set and I is a nonempty collection of subsets of V
that are independent. In particular, I must satisfy (i)
if X ? Y and Y ? I then X ? I, (ii) if X,Y ? I
and |X| < |Y | thenX?{e} ? I for some e ? Y \X .
We typically refer to a matroid by listing its ground
set and its family of independent sets:M = (V, I).
A set function f : 2V ? R is called submodu-
lar (Edmonds, 1970) if it satisfies the property of
diminishing returns: for any X ? Y ? V \ v, a sub-
modular function f must satisfy f(X+v)?f(X) ?
f(Y + v)? f(Y ). That is, the incremental ?value?
of v decreases as the context in which v is considered
grows from X to Y . If this is satisfied everywhere
with equality, then the function f is called modu-
lar. A set function f is monotone nondecreasing if
?X ? Y , f(X) ? f(Y ). As shorthand, in this pa-
per, monotone nondecreasing submodular functions
will simply be referred to as monotone submodular.
Historically, submodular functions have their roots
in economics, game theory, combinatorial optimiza-
tion, and operations research. More recently, submod-
ular functions have started receiving attention in the
machine learning and computer vision community
(Kempe et al, 2003; Narasimhan and Bilmes, 2004;
Narasimhan and Bilmes, 2005; Krause and Guestrin,
2005; Narasimhan and Bilmes, 2007; Krause et al,
2008; Kolmogorov and Zabin, 2004; Jegelka and
Bilmes, 2011) and have recently been introduced
to natural language processing for the task of docu-
ment summarization (Lin and Bilmes, 2010; Lin and
Bilmes, 2011).
3 Approach
We are given a source language (English) string eI1 =
e1, ? ? ? , ei, ? ? ? , eI and a target language (French)
string fJ1 = f1, ? ? ? , fj , ? ? ? , fJ that have to be
aligned. Define the word positions in the English
string as set E , {1, ? ? ? , I} and positions in the
French string as set F , {1, ? ? ? , J}. An alignment
A between the two word strings can then be seen as
a subset of the Cartesian product of the word posi-
tions, i.e., A ? {(i, j) : i ? E, j ? F} , V, and
V = E ? F is the ground set. For convenience, we
refer to element (i, j) ? A as an edge that connects i
and j in alignment A.
Restricting the fertility of word fj to be at most kj
is mathematically equivalent to having |A ? PEj | ?
kj , whereA ? V is an alignment and PEj = E?{j}.
Intuitively, PEj is the set of all possible edges in the
ground set that connect to j, and the cardinality of
the intersection between A and PEj indicates how
many edges in A are connected to j. Similarly, we
can impose constraints on the fertility of English
words by constraining the alignment A to satisfy
|A ? PFi | ? ki for i ? E where P
F
i = {i} ? F .
Note that either of {PEj : j ? F} or {P
F
i : i ? E}
constitute a partition of V . Therefore, alignments A
that satisfy |A ? PEj | ? kj ,?j ? F , are independent
in the partition matroidME = (V, IE) with
IE = {A ? V : ?j ? F, |A ? P
E
j | ? kj},
and alignmentsA that satisfy |A?PFi | ? ki, ?i ? E,
are independent in matroidMF = (V, IF ) with
IF = {A ? V : ?i ? E, |A ? P
F
i | ? ki}.
Suppose we have a set function f : 2V ? R+ that
measures quality (or scores) of an alignment A ? V ,
then when also considering fertility constraints, we
can treat the word alignment problem as maximizing
a set function subject to matroid constraint:
Problem 1. maxA?V f(A), subject to: A ? I,
where I is the set of independent sets of a matroid (or
it might be the set of independent sets simultaneously
in two matroids, as we shall see later).
Independence in partition matroids generalizes
the typical matching constraints for word alignment,
where each word aligns to at most one word (kj =
1,?j) in the other sentence (Matusov et al, 2004;
Taskar et al, 2005). Our matroid generalizations pro-
vide flexibility in modeling fertility, and also strate-
gies for solving the word alignment problem effi-
ciently and near-optimally. In particular, when f
is monotone submodular, near-optimal solutions for
Problem 1 can be efficiently guaranteed.
171
For example, in (Fisher et al, 1978), a simple
greedy algorithm for monotone submodular function
maximization with a matroid constraint is shown
to have a constant approximation factor. Precisely,
the greedy algorithm finds a solution A such that
f(A) ? 1m+1f(A
?) whereA? is the optimal solution
and m is number of matroid constraints. When there
is only one matroid constraint, we get an approxima-
tion factor 12 . Constant factor approximation algo-
rithms are particularly attractive since the quality of
the solution does not depend on the size of the prob-
lem, so even very large size problems do well. It is
also important to note that this is a worst case bound,
and in most cases the quality of the solution obtained
will be much better than this bound suggests.
Vondra?k (2008) shows a continuous greedy al-
gorithm followed by pipage rounding with approx-
imation factor 1 ? 1/e (? 0.63) for maximizing
a monotone submodular function subject to a ma-
troid constraint. Lee et al (2009) improve the 1m+1 -
approximation result in (Fisher et al, 1978) by show-
ing a local-search algorithm has approximation guar-
antee of 1m+ for the problem of maximizing a mono-
tone submodular function subject to m matroid con-
straints (m ? 2 and  > 0). In this paper, however,
we use the simple greedy algorithm for the sake of
efficiency. We outline our greedy algorithm for Prob-
lem 1 in Algorithm 1, which is slightly different from
the one in (Fisher et al, 1978) as in line 4 of Al-
gorithm 1, we have an additional requirement on a
such that the increment of adding a is strictly greater
than zero. This additional requirement is to main-
tain a higher precision word alignment solution. The
theoretical guarantee still holds as f is monotone ?
i.e., Algorithm 1 is a 12 -approximation algorithm for
Problem 1 (only one matroid constraint) when f is
monotone submodular.
Algorithm 1: A greedy algorithm for Problem 1.
input : A = ?, N = V .
begin1
while N 6= ? do2
a? argmaxe?N f(A ? {e})? f(A);3
if A ? {a} ? I and f(A ? {a})? f(A) > 04
then
A? A ? {a}5
N ? N \ {a}.6
end7
Algorithm 1 requires O(|V |2) evaluations of f . In
practice, the argmax in Algorithm 1 can be efficient
implemented with priority queue when f is submod-
ular (Minoux, 1978), which brings the complexity
down to O(|V | log |V |) oracle function calls.
4 Submodular Fertility
We begin this section by demonstrating that submod-
ularity arises naturally when modeling word fertility.
To do so, we borrow an example of fertility from
(Melamed, 2000). Suppose a trained model estimates
s(e1, f1) = .05, s(e1, f2) = .02 and s(e2, f2) = .01,
where s(ei, fj) represents the score of aligning ei and
fj . To find the correct alignment (e1, f1) and (e2, f2),
the competitive linking algorithm in (Melamed, 2000)
poses a one-to-one assumption to prevent choosing
(e1, f2) over (e2, f2). The one-to-one assumption,
however, limits the algorithm?s capability of handling
models with fertility larger than one. Alternatively,
we argue that the reason of choosing (e2, f2) rather
than (e1, f2) is that the benefit of aligning e1 and f2
diminishes after e1 is already aligned with f1 ? this
is exactly the property of diminishing returns, and
therefore, it is natural to use submodular functions to
model alignment scores.
To illustrate this further, we use another real
example taken from the trial set of English-French
Hansards data. The scores estimated from the data
for aligning word pairs (the, le), (the, de) and (of,
de) are 0.68, 0.60 and 0.44 respectively. Given
an English-French sentence pair: ?I have stressed
the CDC as an example of creative, aggressive
effective public ownership? and ?je le ai cite? comme
exemple de proprie?te? publique cre?atrice, dynamique
et efficace?, an algorithm that allows word fertility
larger than 1 might choose alignment (the, de) over
(of, de) since 0.68 + 0.60 > 0.68 + 0.44, regardless
the fact that the is already aligned with le. Now if
we use a submodular function to model the score of
aligning an English word to a set of French words,
we might obtain the correct alignments (the, le) and
(of, de) by incorporating the diminishing returns
property (i.e., the score gain of (the, de), which is
0.60 out of context, could diminish to something less
than 0.44 when evaluated in the context of (the, le)).
Formally, for each i in E, we define a mapping
172
?i : 2V ? 2F with
?i(A) = {j ? F |(i, j) ? A}, (1)
i.e., ?i(A) is the set of positions in F that are aligned
with position i in alignment A.
We use function fi : 2F ? R+ to represent the
benefit of aligning position i ? E to a set of positions
in F . Given score si,j of aligning i and j, we could
have, for S ? F ,
fi(S) =
?
?
?
j?S
si,j
?
?
?
, (2)
where 0 < ? ? 1, i.e., we impose a concave function
over a modular function, which produces a submod-
ular function. The value of ? determines the rate
that the marginal benefit diminishes when aligning
a word to more than one words in the other string.
Summing over alignment scores in all positions in
E, we obtain the total score of an alignment A:
f(A) =
?
i?E
fi(?i(A)), (3)
which is again, monotone submodular. By diminish-
ing the marginal benefits of aligning a word to more
than one words in the other string, f(A) encourages
the common case of low fertility while allowing fer-
tility larger than one. For instance in the aforemen-
tioned example, when ? = 12 , the score for aligning
both le and de to the is
?
0.68 + 0.60 ? 1.13, while
the score of aligning the to le and of to de is
?
0.68 +?
0.44 ? 1.49, leading to the correct alignment.
5 Experiments
We evaluated our approaches using the English-
French Hansards data from the 2003 NAACL shared
task (Mihalcea and Pedersen, 2003). This corpus con-
sists of 1.1M automatically aligned sentences, and
comes with a test set of 447 sentences, which have
been hand-aligned and are marked with both ?sure?
and ?possible? alignments (Och and Ney, 2003). Us-
ing these alignments, alignment error rate (AER) is
calculated as:
AER(A,S, P ) = 1?
|A ? S|+ |A ? P |
|A|+ |S|
(4)
where S is the set of sure gold pairs, and P is the
set of possible gold pairs. We followed the work
in (Taskar et al, 2005) and split the original test set
into 347 test examples, and 100 training examples
for parameters tuning.
In general, the score of aligning i to j can be
modeled as a function of arbitrary features. Although
parameter learning in our framework would be
another interesting topic to study, we focus herein on
the inference problem. Therefore, only one feature
(Eq. 5) was used in our experiments in order for no
feature weight learning to be required. In particular,
we estimated the score of aligning i to j as
si,j =
p(fj |ei) ? p(i|j, I)
?
j??F p(fj? |ei) ? p(i|j
?, I)
, (5)
where the translation probability p(fj |ei) and
alignment probability p(i|j, I) were obtained from
IBM model 2 trained on the 1.1M sentences. The
IBM 2 models gives an AER of 21.0% with French
as the target, in line with the numbers reported in
Och and Ney (2003) and Lacoste-Julien et al (2006).
We tested two types of partition matroid con-
straints. The first is a global matroid constraint:
A ? {A? ? V : ?j ? F, |A? ? PEj | ? b}, (6)
which restricts fertility of all words on F side to be at
most b. This constraint is denoted as FertF (A) ? b
in Table 1 for simplicity. The second type, denoted
as FertF (A) ? kj , is word-dependent:
A ? {A? ? V : ?j ? F, |A? ? PEj | ? kj}, (7)
where the fertility of word on j is restricted to be
at most kj . Here kj = max{b : pb(f) ? ?, b ?
{0, 1, . . . , 5}}, where ? is a threshold and pb(f) is
the probability that French word f was aligned to at
most b English words based on the IBM 2 alignment.
As mentioned in Section 3, matroid constraints
generalize the matching constraint. In particular,
when using two matroid constraints, FertE(A) ? 1
and FertF (A) ? 1, we have the matching constraint
where fertility for both English and French words
are restricted to be at most one. Our setup 1 (see Ta-
ble 1) uses these two constraints along with a modular
objective function, which is equivalent to the max-
imum weighted bipartite matching problem. Using
173
Table 1: AER results
ID Objective function Constraint AER(%)
1 FertF (A) ? 1, FertE(A) ? 1 21.0
2 FertF (A) ? 1 23.1
3
modular: f(A) =
P
i?E
P
j??i(A)
si,j
FertF (A) ? kj 22.1
4 FertF (A) ? 1 19.8
5
submodular: f(A) =
P
i?E
?P
j??i(A)
si,j
??
FertF (A) ? kj 18.6
Generative model (IBM 2, E?F) 21.0
Maximum weighted bipartite matching 20.9
Matching with negative penalty on fertility (ILP) 19.3
greedy algorithm to solve this problem, we get AER
21.0% (setup 1 in Table 1) ? no significant difference
compared to the AER (20.9%) achieved by the ex-
act solution (maximum weighted bipartite matching
approach), illustrating that greedy solutions are near-
optimal. Note that the bipartite matching approach
does not improve performance over IBM 2 model,
presumably because only one feature was used here.
When allowing fertility of English words to be
more than one, we see a significant AER reduction
using a submodular objective (setup 4 and 5) instead
of a modular objective (setup 2 and 3), which verifies
our claim that submodularity lends itself to modeling
the marginal benefit of growing fertility. In setup
2 and 4, while allowing larger fertility for English
words, we restrict the fertility of French words to
be most one. To allow higher fertility for French
words, one possible approach is to use constraint
FertF (A) ? 2, in which all French words are
allowed to have fertility up to 2. This approach, how-
ever, results in a significant increase of false positive
alignments since all French words tend to collect
as many matches as permitted. This issue could be
alleviated by introducing a symmetric version of
the objective function in Eq. 3 such that marginal
benefit of higher fertility of French words are also
compressed. Alternatively, we use the second type
of matroid constraint in which fertility upper bounds
of French words are word-dependent instead of
global. With ? = .8, about 10 percent of the French
words have kj equal to 2 or greater. By using the
word-dependent matroid constraint (setup 3 and 5),
AERs are reduced compared to those using global
matroid constraints. In particular, 18.6% AER is
achieved by setup 5, which significantly outperforms
the maximum weighted bipartite matching approach.
We also compare our method with model of
Lacoste-Julien et al (2006) which also allows fer-
tility larger than one by penalizing different levels of
fertility. We used si,j as an edge feature and pb(f) as
a node feature together with two additional features:
a bias feature and the bucketed frequency of the word
type. The same procedures for training and decoding
as in (Lacoste-Julien et al, 2006) were performed
where MOSEK was used as the ILP solver. As shown
in Table 1, performance of setup 5 outperforms this
model and moreover, our approach is at least 50 times
faster: it took our approach only about half a second
to align all the 347 test set sentence pairs whereas
using the ILP-based approach took about 40 seconds.
6 Discussion
We have presented a novel framework where word
alignment is framed as submodular maximization
subject to matroid constraints. Our framework
extends previous matching-based frameworks
in two respects: submodular objective functions
generalize modular (linear) objective functions, and
matroid constraints generalize matching constraints.
Moreover, such generalizations do not incur a
prohibitive computational price since submodular
maximization over matroids can be efficiently solved
with performance guarantees. As it is possible to
leverage richer forms of submodular functions that
model higher order interactions, we believe that the
full potential of our approach has yet to be explored.
Our approach might lead to novel approaches for
machine translation as well.
Acknowledgment
We thank Simon Lacoste-Julien for sharing his code
and features from (Lacoste-Julien et al, 2006), and
the anonymous reviewers for their comments. This
work was supported by NSF award 0905341.
174
References
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L. Mercer.
1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational linguistics,
19(2):263?311.
J. Edmonds, 1970. Combinatorial Structures and their Ap-
plications, chapter Submodular functions, matroids and
certain polyhedra, pages 69?87. Gordon and Breach.
ML Fisher, GL Nemhauser, and LA Wolsey. 1978. An
analysis of approximations for maximizing submodular
set functions?II. Polyhedral combinatorics, pages
73?87.
S. Jegelka and J. A. Bilmes. 2011. Submodularity beyond
submodular energies: coupling edges in graph cuts.
In Computer Vision and Pattern Recognition (CVPR),
Colorado Springs, CO, June.
D. Kempe, J. Kleinberg, and E. Tardos. 2003. Maximiz-
ing the spread of influence through a social network.
In Proceedings of the 9th Conference on SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD).
V. Kolmogorov and R. Zabin. 2004. What energy func-
tions can be minimized via graph cuts? IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
26(2):147?159.
A. Krause and C. Guestrin. 2005. Near-optimal nonmy-
opic value of information in graphical models. In Proc.
of Uncertainty in AI.
A. Krause, H.B. McMahan, C. Guestrin, and A. Gupta.
2008. Robust submodular observation selection. Jour-
nal of Machine Learning Research, 9:2761?2801.
S. Lacoste-Julien, B. Taskar, D. Klein, and M.I. Jordan.
2006. Word alignment via quadratic assignment. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, pages 112?119. Association for Computational
Linguistics.
J. Lee, M. Sviridenko, and J. Vondra?k. 2009. Submodular
maximization over multiple matroids via generalized
exchange properties. Approximation, Randomization,
and Combinatorial Optimization. Algorithms and Tech-
niques, pages 244?257.
H. Lin and J. Bilmes. 2010. Multi-document summariza-
tion via budgeted maximization of submodular func-
tions. In North American chapter of the Association
for Computational Linguistics/Human Language Tech-
nology Conference (NAACL/HLT-2010), Los Angeles,
CA, June.
H. Lin and J. Bilmes. 2011. A class of submodular func-
tions for document summarization. In The 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT), Port-
land, OR, June.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
word alignments for statistical machine translation. In
Proceedings of the 20th international conference on
Computational Linguistics, page 219. Association for
Computational Linguistics.
I.D. Melamed. 2000. Models of translational equivalence
among words. Computational Linguistics, 26(2):221?
249.
R. Mihalcea and T. Pedersen. 2003. An evaluation exer-
cise for word alignment. In Proceedings of the HLT-
NAACL 2003 Workshop on Building and using parallel
texts: data driven machine translation and beyond-
Volume 3, pages 1?10. Association for Computational
Linguistics.
M. Minoux. 1978. Accelerated greedy algorithms for
maximizing submodular set functions. Optimization
Techniques, pages 234?243.
Mukund Narasimhan and Jeff Bilmes. 2004. PAC-
learning bounded tree-width graphical models. In Un-
certainty in Artificial Intelligence: Proceedings of the
Twentieth Conference (UAI-2004). Morgan Kaufmann
Publishers, July.
M. Narasimhan and J. Bilmes. 2005. A submodular-
supermodular procedure with applications to discrimi-
native structure learning. In Proc. Conf. Uncertainty in
Artifical Intelligence, Edinburgh, Scotland, July. Mor-
gan Kaufmann Publishers.
M. Narasimhan and J. Bilmes. 2007. Local search for
balanced submodular clusterings. In Twentieth Inter-
national Joint Conference on Artificial Intelligence (IJ-
CAI07), Hyderabad, India, January.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
linguistics, 29(1):19?51.
A. Schrijver. 2003. Combinatorial optimization: polyhe-
dra and efficiency. Springer Verlag.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 73?80. Association for Com-
putational Linguistics.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
Proceedings of the 16th conference on Computational
linguistics-Volume 2, pages 836?841. Association for
Computational Linguistics.
J. Vondra?k. 2008. Optimal approximation for the sub-
modular welfare problem in the value oracle model. In
Proceedings of the 40th annual ACM symposium on
Theory of computing, pages 67?74. ACM.
175
