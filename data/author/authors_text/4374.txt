Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1121?1128,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Effective Two-Stage Model for Exploiting Non-Local Dependencies in
Named Entity Recognition
Vijay Krishnan
Computer Science Department
Stanford University
Stanford, CA 94305
vijayk@cs.stanford.edu
Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
manning@cs.stanford.edu
Abstract
This paper shows that a simple two-stage
approach to handle non-local dependen-
cies in Named Entity Recognition (NER)
can outperform existing approaches that
handle non-local dependencies, while be-
ing much more computationally efficient.
NER systems typically use sequence mod-
els for tractable inference, but this makes
them unable to capture the long distance
structure present in text. We use a Con-
ditional Random Field (CRF) based NER
system using local features to make pre-
dictions and then train another CRF which
uses both local information and features
extracted from the output of the first CRF.
Using features capturing non-local depen-
dencies from the same document, our ap-
proach yields a 12.6% relative error re-
duction on the F1 score, over state-of-the-
art NER systems using local-information
alone, when compared to the 9.3% relative
error reduction offered by the best systems
that exploit non-local information. Our
approach also makes it easy to incorpo-
rate non-local information from other doc-
uments in the test corpus, and this gives
us a 13.3% error reduction over NER sys-
tems using local-information alone. Ad-
ditionally, our running time for inference
is just the inference time of two sequen-
tial CRFs, which is much less than that
of other more complicated approaches that
directly model the dependencies and do
approximate inference.
1 Introduction
Named entity recognition (NER) seeks to lo-
cate and classify atomic elements in unstructured
text into predefined entities such as the names
of persons, organizations, locations, expressions
of times, quantities, monetary values, percent-
ages, etc. A particular problem for Named En-
tity Recognition(NER) systems is to exploit the
presence of useful information regarding labels as-
signed at a long distance from a given entity. An
example is the label-consistency constraint that if
our text has two occurrences of New York sepa-
rated by other tokens, we would want our learner
to encourage both these entities to get the same la-
bel.
Most statistical models currently used for
Named Entity Recognition, use sequence mod-
els and thereby capture local structure. Hidden
Markov Models (HMMs) (Leek, 1997; Freitag
and McCallum, 1999), Conditional Markov Mod-
els (CMMs) (Borthwick, 1999; McCallum et al,
2000), and Conditional Random Fields (CRFs)
(Lafferty et al, 2001) have been successfully em-
ployed in NER and other information extraction
tasks. All these models encode the Markov prop-
erty i.e. labels directly depend only on the labels
assigned to a small window around them. These
models exploit this property for tractable com-
putation as this allows the Forward-Backward,
Viterbi and Clique Calibration algorithms to be-
come tractable. Although this constraint is essen-
tial to make exact inference tractable, it makes us
unable to exploit the non-local structure present in
natural language.
Label consistency is an example of a non-local
dependency important in NER. Apart from label
consistency between the same token sequences,
we would also like to exploit richer sources of de-
pendencies between similar token sequences. For
example, as shown in Figure 1, we would want
it to encourage Einstein to be labeled ?Person? if
there is strong evidence that Albert Einstein should
be labeled ?Person?. Sequence models unfortu-
1121
told that Albert Einstein proved . . . on seeing Einstein at the
Figure 1: An example of the label consistency problem. Here we would like our model to encourage entities Albert Einstein
and Einstein to get the same label, so as to improve the chance that both are labeled PERSON.
nately cannot model this due to their Markovian
assumption.
Recent approaches attempting to capture non-
local dependencies model the non-local dependen-
cies directly, and use approximate inference al-
gorithms, since exact inference is in general, not
tractable for graphs with non-local structure.
Bunescu and Mooney (2004) define a Rela-
tional Markov Network (RMN) which explicitly
models long-distance dependencies, and use it to
represent relations between entities. Sutton and
McCallum (2004) augment a sequential CRF with
skip-edges i.e. edges between different occur-
rences of a token, in a document. Both these
approaches use loopy belief propagation (Pearl,
1988; Yedidia et al, 2000) for approximate infer-
ence.
Finkel et al (2005) hand-set penalties for incon-
sistency in entity labeling at different occurrences
in the text, based on some statistics from training
data. They then employ Gibbs sampling (Geman
and Geman, 1984) for dealing with their local fea-
ture weights and their non-local penalties to do ap-
proximate inference.
We present a simple two-stage approach where
our second CRF uses features derived from the
output of the first CRF. This gives us the advan-
tage of defining a rich set of features to model
non-local dependencies, and also eliminates the
need to do approximate inference, since we do not
explicitly capture the non-local dependencies in a
single model, like the more complex existing ap-
proaches. This also enables us to do inference ef-
ficiently since our inference time is merely the in-
ference time of two sequential CRF?s; in contrast
Finkel et al (2005) reported an increase in running
time by a factor of 30 over the sequential CRF,
with their Gibbs sampling approximate inference.
In all, our approach is simpler, yields higher
F1 scores, and is also much more computationally
efficient than existing approaches modeling non-
local dependencies.
2 Conditional Random Fields
We use a Conditional Random Field (Lafferty et
al., 2001; Sha and Pereira, 2003) since it rep-
resents the state of the art in sequence model-
ing and has also been very effective at Named
Entity Recognition. It allows us both discrim-
inative training that CMMs offer as well and
the bi-directional flow of probabilistic information
across the sequence that HMMs allow, thereby
giving us the best of both worlds. Due to the
bi-directional flow of information, CRFs guard
against the myopic locally attractive decisions that
CMMs make. It is customary to use the Viterbi al-
gorithm, to find the most probably state sequence
during inference. A large number of possibly re-
dundant and correlated features can be supplied
without fear of further reducing the accuracy of
a high-dimensional distribution. These are well-
documented benefits (Lafferty et al, 2001).
2.1 Our Baseline CRF for Named Entity
Recognition
Our baseline CRF is a sequence model in which la-
bels for tokens directly depend only on the labels
corresponding to the previous and next tokens. We
use features that have been shown to be effective
in NER, namely the current, previous and next
words, character n-grams of the current word, Part
of Speech tag of the current word and surround-
ing words, the shallow parse chunk of the current
word, shape of the current word, the surrounding
word shape sequence, the presence of a word in a
left window of size 5 around the current word and
the presence of a word in a left window of size 5
around the current word. This gives us a compet-
itive baseline CRF using local information alone,
whose performance is close to the best published
local CRF models, for Named Entity Recognition
3 Label Consistency
The intuition for modeling label consistency is
that within a particular document, different occur-
1122
Document Level Statistics Corpus Level Statistics
PER LOC ORG MISC PER LOC ORG MISC
PER 3141 4 5 0 33830 113 153 0
LOC 6436 188 3 346966 6749 60
ORG 2975 0 43892 223
MISC 2030 66286
Table 1: Table showing the number of pairs of different occurrences of the same token sequence, where one occurrence is given
a certain label and the other occurrence is given a certain label. We show these counts both within documents, as well as over
the whole corpus. As we would expect, most pairs of the same entity sequence are labeled the same(i.e. the diagonal has most
of the density) at both the document and corpus levels. These statistics are from the CoNLL 2003 English training set.
Document Level Statistics Corpus Level Statistics
PER LOC ORG MISC PER LOC ORG MISC
PER 1941 5 2 3 9111 401 261 38
LOC 0 167 6 63 68 4560 580 1543
ORG 22 328 819 191 221 19683 5131 4752
MISC 14 224 7 365 50 12713 329 8768
Table 2: Table showing the number of (token sequence, token subsequence) pairs where the token sequence is assigned a certain
entity label, and the token subsequence is assigned a certain entity label. We show these counts both within documents, as well
as over the whole corpus. Rows correspond to sequences, and columns to subsequences. These statistics are from the CoNLL
2003 English training set.
rences of a particular token sequence (or similar
token sequences) are unlikely to have different en-
tity labels. While this constraint holds strongly
at the level of a document, there exists additional
value to be derived by enforcing this constraint
less strongly across different documents. We want
to model label consistency as a soft and not a hard
constraint; while we want to encourage different
occurrences of similar token sequences to get la-
beled as the same entity, we do not want to force
this to always hold, since there do exist exceptions,
as can be seen from the off-diagonal entries of ta-
bles 1 and 2.
A named entity recognition system modeling
this structure would encourage all the occurrences
of the token sequence to the same entity type,
thereby sharing evidence among them. Thus, if
the system has strong evidence about the label of
a given token sequence, but is relatively unsure
about the label to be assigned to another occur-
rence of a similar token sequence, the system can
gain significantly by using the information about
the label assigned to the former occurrence, to la-
bel the relatively ambiguous token sequence, lead-
ing to accuracy improvements.
The strength of the label consistency constraint,
can be seen from statistics extracted from the
CoNLL 2003 English training data. Table 1 shows
the counts of entity labels pairs assigned for each
pair of identical token sequences both within a
document and across the whole corpus. As we
would expect, inconsistent labelings are relatively
rare and most pairs of the same entity sequence
are labeled the same(i.e. the diagonal has most
of the density) at both the document and corpus
levels. A notable exception to this is the labeling
of the same text as both organization and location
within the same document and across documents.
This is a due to the large amount of sports news in
the CoNLL dataset due to which city and country
names are often also team names. We will see that
our approach is capable of exploiting this as well,
i.e. we can learn a model which would not pe-
nalize an Organization-Location inconsistency as
strongly as it penalizes other inconsistencies.
In addition, we also want to model subsequence
constraints: having seen Albert Einstein earlier in
a document as a person is a good indicator that a
subsequent occurrence of Einstein should also be
labeled as a person. Here, we would expect that a
subsequence would gain much more by knowing
the label of a supersequence, than the other way
around.
However, as can be seen from table 2, we
find that the consistency constraint does not hold
nearly so strictly in this case. A very common case
of this in the CoNLL dataset is that of documents
containing references to both The China Daily, a
newspaper, and China, the country (Finkel et al,
2005). The first should be labeled as an organiza-
tion, and second as a location. The counts of sub-
sequence labelings within a document and across
documents listed in Table 2, show that there are
many off-diagonal entries: the China Daily case is
among the most common, occurring 328 times in
the dataset. Just as we can model off-diagonal pat-
1123
terns with exact token sequence matches, we can
also model off-diagonal patterns for the token sub-
sequence case.
In addition, we could also derive some value by
enforcing some label consistency at the level of
an individual token. Obviously, our model would
learn much lower weights for these constraints,
when compared to label consistency at the level
of token sequences.
4 Our Approach to Handling non-local
Dependencies
To handle the non-local dependencies between
same and similar token sequences, we define three
sets of feature pairs where one member of the fea-
ture pair corresponds to a function of aggregate
statistics of the output of the first CRF at the doc-
ument level, and the other member corresponds
to a function of aggregate statistics of the out-
put of the first CRF over the whole test corpus.
Thus this gives us six additional feature types for
the second round CRF, namely Document-level
Token-majority features, Document-level Entity-
majority features, Document-level Superentity-
majority features, Corpus-level Token-majority
features, Corpus-level Entity-majority features
and Corpus-level Superentity-majority features.
These feature types are described in detail below.
All these features are a function of the output
labels of the first CRF, where predictions on the
test set are obtained by training on all the data, and
predictions on the train data are obtained by 10
fold cross-validation (details in the next section).
Our features fired based on document and corpus
level statistics are:
? Token-majority features: These refer to the
majority label assigned to the particular to-
ken in the document/corpus. Eg: Suppose
we have three occurrences of the token Aus-
tralia, such that two are labeled Location
and one is labeled Organization, our token-
majority feature would take value Location
for all three occurrences of the token. This
feature can enable us to capture some depen-
dence between token sequences correspond-
ing to a single entity and having common to-
kens.
? Entity-majority features: These refer to the
majority label assigned to the particular en-
tity in the document/corpus. Eg: Suppose we
have three occurrences of the entity sequence
(we define it as a token sequence labeled as a
single entity by the first stage CRF) Bank of
Australia, such that two are labeled Organi-
zation and one is labeled Location, our entity-
majority feature would take value Organiza-
tion for all tokens in all three occurrences of
the entity sequence. This feature enables us
to capture the dependence between identical
entity sequences. For token labeled as not a
Named Entity by the first CRF, this feature
returns the majority label assigned to that to-
ken when it occurs as a single token named
entity.
? Superentity-majority features: These re-
fer to the majority label assigned to superse-
quences of the particular entity in the docu-
ment/corpus. By entity supersequences, we
refer to entity sequences, that strictly contain
within their span, another entity sequence.
For example, if we have two occurrences of
Bank of Australia labeled Organization and
one occurrence of Australia Cup labeled Mis-
cellaneous, then for all occurrences of the en-
tity Australia, the superentity-majority fea-
ture would take value Organization. This fea-
ture enables us to take into account labels as-
signed to supersequences of a particular en-
tity, while labeling it. For token labeled as not
a Named Entity by the first CRF, this feature
returns the majority label assigned to all enti-
ties containing the token within their span.
The last feature enables entity sequences to
benefit from labels assigned to entities which
are entity supersequences of it. We attempted
to add subentity-majority features, analogous to
the superentity-majority features to model depen-
dence on entity subsequences, but got no bene-
fit from it. This is intuitive, since the basic se-
quence model would usually be much more cer-
tain about labels assigned to the entity superse-
quences, since they are longer and have more con-
textual information. As a result of this, while
there would be several cases in which the basic
sequence model would be uncertain about labels
of entity subsequences but relatively certain about
labels of token supersequences, the converse is
very unlikely. Thus, it is difficult to profit from
labels of entity subsequences while labeling en-
tity sequences. We also attempted using more fine
1124
grained features corresponding to the majority la-
bel of supersequences that takes into account the
position of the entity sequence in the entity su-
persequence(whether the entity sequence occurs in
the start, middle or end of the supersequence), but
could obtain no additional gains from this.
It is to be noted that while deciding if to-
ken sequences are equal or hold a subsequence-
supersequence relation, we ignore case, which
clearly performs better than being sensitive to
case. This is because our dataset contains sev-
eral entities in allCaps such as AUSTRALIA, es-
pecially in news headlines. Ignoring case enables
us to model dependences with other occurrences
with a different case such as Australia.
It may appear at first glance, that our frame-
work can only learn to encourage entities to switch
to the most popular label assigned to other occur-
rences of the entity sequence and similar entity se-
quences. However this framework is capable of
learning interesting off-diagonal patterns as well.
To understand this, let us consider the example of
different occurrences of token sequences being la-
beled Location and Organization. Suppose, the
majority label of the token sequence is Location.
While this majority label would encourage the sec-
ond CRF to switch the labels of all occurrences
of the token sequence to Location, it would not
strongly discourage the CRF from labeling these
as Organization, since there would be several oc-
currences of token sequences in the training data
labeled Organization, with the majority label of
the token sequence being Location. However it
would discourage the other labels strongly. The
reasoning is analogous when the majority label is
Organization.
In case of a tie (when computing the majority
label), if the label assigned to a particular token
sequence is one of the majority labels, we fire the
feature corresponding to that particular label being
the majority label, instead of breaking ties arbi-
trarily. This is done to encourage the second stage
CRF to make its decision based on local informa-
tion, in the absence of compelling non-local infor-
mation to choose a different label.
5 Advantages of our approach
With our two-stage approach, we manage to get
improvements on the F1 measure over existing ap-
proaches that model non-local dependencies. At
the same time, the simplicity of our two-stage ap-
proach keeps inference time down to just the in-
ference time of two sequential CRFs, when com-
pared to approaches such as those of Finkel et al
(2005) who report that their inference time with
Gibbs sampling goes up by a factor of about 30,
compared to the Viterbi algorithm for the sequen-
tial CRF.
Below, we give some intuition about areas for
improvement in existing work and explain how
our approach incorporates the improvements.
? Most existing work to capture label-
consistency, has attempted to create all
(n
2
)
pairwise dependencies between the different
occurrences of an entity, (Finkel et al, 2005;
Sutton and McCallum, 2004), where n is
the number of occurrences of the given
entity. This complicates the dependency
graph making inference harder. It also leads
to the penalty for deviation in labeling to
grow linearly with n, since each entity would
be connected to ?(n) entities. When an
entity occurs several times, these models
would force all occurrences to take the same
value. This is not what we want, since
there exist several instances in real-life data
where different entities like persons and
organizations share the same name. Thus,
our approach makes a certain entity?s label
depend on certain aggregate information of
other labels assigned to the same entity, and
does not enforce pairwise dependencies.
? We also exploit the fact that the predictions
of a learner that takes non-local dependen-
cies into account would have a good amount
of overlap with a sequential CRF, since the
sequence model is already quite competitive.
We use this intuition to approximate the ag-
gregate information about labels assigned to
other occurrences of the entity by the non-
local model, with the aggregate information
about labels assigned to other occurrences of
the entity by the sequence model. This intu-
ition enables us to learn weights for non-local
dependencies in two stages; we first get pre-
dictions from a regular sequential CRF and
in turn use aggregate information about pre-
dictions made by the CRF as extra features to
train a second CRF.
? Most work has looked to model non-local de-
pendencies only within a document (Finkel
1125
et al, 2005; Chieu and Ng, 2002; Sutton
and McCallum, 2004; Bunescu and Mooney,
2004). Our model can capture the weaker but
still important consistency constraints across
the whole document collection, whereas pre-
vious work has not, for reasons of tractabil-
ity. Capturing label-consistency at the level
of the whole test corpus is particularly helpful
for token sequences that appear only once in
their documents, but occur a few times over
the corpus, since they do not have strong non-
local information from within the document.
? For training our second-stage CRF, we need
to get predictions on our train data as well as
test data. Suppose we were to use the same
train data to train the first CRF, we would get
unrealistically good predictions on our train
data, which would not be reflective of its per-
formance on the test data. One option is to
partition the train data. This however, can
lead to a drop in performance, since the sec-
ond CRF would be trained on less data. To
overcome this problem, we make predictions
on our train data by doing a 10-fold cross val-
idation on the train data. For predictions on
the test data, we use all the training data to
train the CRF. Intuitively, we would expect
that the quality of predictions with 90% of
the train data would be similar to the qual-
ity of predictions with all the training data. It
turns out that this is indeed the case, as can
be seen from our improved performance.
6 Experiments
6.1 Dataset and Evaluation
We test the effectiveness of our technique
on the CoNLL 2003 English named en-
tity recognition dataset downloadable from
http://cnts.uia.ac.be/conll2003/ner/. The data
comprises Reuters newswire articles annotated
with four entity types: person (PER), location
(LOC), organization (ORG), and miscellaneous
(MISC). The data is separated into a training set,
a development set (testa), and a test set (testb).
The training set contains 945 documents, and
approximately 203,000 tokens and the test set
has 231 documents and approximately 46,000
tokens. Performance on this task is evaluated by
measuring the precision and recall of annotated
entities (and not tokens), combined into an F1
score. There is no partial credit for labeling part
of an entity sequence correctly; an incorrect entity
boundary is penalized as both a false positive and
as a false negative.
6.2 Results and Discussion
It can be seen from table 3, that we achieve a
12.6% relative error reduction, by restricting our-
selves to features approximating non-local depen-
dency within a document, which is higher than
other approaches modeling non-local dependen-
cies within a document. Additionally, by incorpo-
rating non-local dependencies across documents
in the test corpus, we manage a 13.3% relative er-
ror reduction, over an already competitive base-
line. We can see that all three features approxi-
mating non-local dependencies within a document
yield reasonable gains. As we would expect the
additional gains from features approximating non-
local dependencies across the whole test corpus
are relatively small.
We use the approximate randomization test
(Yeh, 2000) for statistical significance of the dif-
ference between the basic sequential CRF and our
second round CRF, which has additional features
derived from the output of the first CRF. With a
1000 iterations, our improvements were statisti-
cally significant with a p-value of 0.001. Since
this value is less than the cutoff threshold of 0.05,
we reject the null hypothesis.
The simplicity of our approach makes it easy to
incorporate dependencies across the whole corpus,
which would be relatively much harder to incor-
porate in approaches like (Bunescu and Mooney,
2004) and (Finkel et al, 2005). Additionally,
our approach makes it possible to do inference
in just about twice the inference time with a sin-
gle sequential CRF; in contrast, approaches like
Gibbs Sampling that model the dependencies di-
rectly can increase inference time by a factor of
30 (Finkel et al, 2005).
An analysis of errors by the first stage CRF re-
vealed that most errors are that of single token en-
tities being mislabeled or missed altogether fol-
lowed by a much smaller percentage of multi-
ple token entities mislabelled completely. All our
features directly encode information that is use-
ful to reducing these errors. The widely preva-
lent boundary detection error is that of miss-
ing a single-token entity (i.e. labeling it as
Other(O)). Our approach helps correct many such
errors based on occurrences of the token in other
1126
F1 scores on the CoNLL Dataset
Approach LOC ORG MISC PER ALL Relative Error reduction
Bunescu and Mooney (2004) (Relational Markov Networks)
Only Local Templates - - - - 80.09
Global and Local Templates - - - - 82.30 11.1%
Finkel et al (2005)(Gibbs Sampling)
Local+Viterbi 88.16 80.83 78.51 90.36 85.51
Non Local+Gibbs 88.51 81.72 80.43 92.29 86.86 9.3%
Our Approach with the 2-stage CRF
Baseline CRF 88.09 80.88 78.26 89.76 85.29
+ Document token-majority features 89.17 80.15 78.73 91.60 86.50
+ Document entity-majority features 89.50 81.98 79.38 91.74 86.75
+ Document superentity-majority features 89.52 82.27 79.76 92.71 87.15 12.6%
+ Corpus token-majority features 89.48 82.36 79.59 92.65 87.13
+ Corpus entity-majority features 89.72 82.40 79.71 92.65 87.23
+ Corpus superentity-majority features
(All features) 89.80 82.39 79.76 92.57 87.24 13.3%
Table 3: Table showing improvements obtained with our additional features, over the baseline CRF. We also compare our
performance against (Bunescu and Mooney, 2004) and (Finkel et al, 2005) and find that we manage higher relative improvement
than existing work despite starting from a very competitive baseline CRF.
named entities. Other kinds of boundary detec-
tion errors involving multiple tokens are very rare.
Our approach can also handle these errors by en-
couraging certain tokens to take different labels.
This together with the clique features encoding
the markovian dependency among neighbours can
correct some multiple-token boundary detection
errors.
7 Related Work
Recent work looking to directly model non-local
dependencies and do approximate inference are
that of Bunescu and Mooney (2004), who use
a Relational Markov Network (RMN) (Taskar et
al., 2002) to explicitly model long-distance de-
pendencies, Sutton and McCallum (2004), who
introduce skip-chain CRFs, which add additional
non-local edges to the underlying CRF sequence
model (which Bunescu and Mooney (2004) lack)
and Finkel et al (2005) who hand-set penalties
for inconsistency in labels based on the training
data and then use Gibbs Sampling for doing ap-
proximate inference where the goal is to obtain
the label sequence that maximizes the product of
the CRF objective function and their penalty. Un-
fortunately, in the RMN model, the dependencies
must be defined in the model structure before do-
ing any inference, and so the authors use heuristic
part-of-speech patterns, and then add dependen-
cies between these text spans using clique tem-
plates. This generates an extremely large num-
ber of overlapping candidate entities, which ren-
ders necessary additional templates to enforce the
constraint that text subsequences cannot both be
different entities, something that is more naturally
modeled by a CRF. Another disadvantage of this
approach is that it uses loopy belief propagation
and a voted perceptron for approximate learning
and inference, which are inherently unstable algo-
rithms leading to convergence problems, as noted
by the authors. In the skip-chain CRFs model,
the decision of which nodes to connect is also
made heuristically, and because the authors focus
on named entity recognition, they chose to connect
all pairs of identical capitalized words. They also
utilize loopy belief propagation for approximate
learning and inference. It is hard to directly ex-
tend their approach to model dependencies richer
than those at the token level.
The approach of Finkel et al (2005) makes
it possible a to model a broader class of long-
distance dependencies than Sutton and McCallum
(2004), because they do not need to make any ini-
tial assumptions about which nodes should be con-
nected and they too model dependencies between
whole token sequences representing entities and
between entity token sequences and their token su-
persequences that are entities. The disadvantage
of their approach is the relatively ad-hoc selec-
tion of penalties and the high computational cost
of running Gibbs sampling.
Early work in discriminative NER employed
two stage approaches that are broadly similar to
ours, but the effectiveness of this approach appears
to have been overlooked in more recent work.
Mikheev et al (1999) exploit label consistency
information within a document using relatively
ad hoc multi-stage labeling procedures. Borth-
1127
wick (1999) used a two-stage approach similar to
ours with CMM?s where Reference Resolution fea-
tures which encoded the frequency of occurrences
of other entities similar to the current token se-
quence, were derived from the output of the first
stage. Malouf (2002) and Curran and Clark (2003)
condition the label of a token at a particular posi-
tion on the label of the most recent previous in-
stance of that same token in a previous sentence
of the same document. This violates the Markov
property and therefore instead of finding the max-
imum likelihood sequence over the entire docu-
ment (exact inference), they label one sentence at a
time, which allows them to condition on the max-
imum likelihood sequence of previous sentences.
While this approach is quite effective for enforc-
ing label consistency in many NLP tasks, it per-
mits a forward flow of information only, which can
result in loss of valuable information. Chieu and
Ng (2002) propose a solution to this problem: for
each token, they define additional features based
on known information, taken from other occur-
rences of the same token in the document. This ap-
proach has the advantage of allowing the training
procedure to automatically learn good weights for
these ?global? features relative to the local ones.
However, it is hard to extend this to incorporate
other types of non-local structure.
8 Conclusion
We presented a two stage approach to model non-
local dependencies and saw that it outperformed
existing approaches to modeling non-local depen-
dencies. Our approach also made it easy to ex-
ploit various dependencies across documents in
the test corpus, whereas incorporating this infor-
mation in most existing approaches would make
them intractable due to the complexity of the resul-
tant graphical model. Our simple approach is also
very computationally efficient since the inference
time is just twice the inference time of the basic se-
quential CRF, while for approaches doing approx-
imate inference, the inference time is often well
over an order of magnitude over the basic sequen-
tial CRF. The simplicity of our approach makes it
easy to understand, implement, and adapt to new
applications.
Acknowledgments
We wish to Jenny R. Finkel for discussions on
NER and her CRF code. Also, thanks to Trond
Grenager for NER discussions and to William
Morgan for help with statistical significance tests.
Also, thanks to Vignesh Ganapathy for helpful dis-
cussions and Rohini Rajaraman for comments on
the writeup.
This work was supported in part by a Scot-
tish Enterprise Edinburgh-Stanford Link grant
(R37588), as part of the EASIE project.
References
A. Borthwick. 1999. A Maximum Entropy Approach to
Named Entity Recognition. Ph.D. thesis, New York Uni-
versity.
R. Bunescu and R. J. Mooney. 2004. Collective information
extraction with relational Markov networks. In Proceed-
ings of the 42nd ACL, pages 439?446.
H. L. Chieu and H. T. Ng. 2002. Named entity recognition: a
maximum entropy approach using global information. In
Proceedings of the 19th Coling, pages 190?196.
J. R. Curran and S. Clark. 2003. Language independent NER
using a maximum entropy tagger. In Proceedings of the
7th CoNLL, pages 164?167.
J. Finkel, T. Grenager, and C. D. Manning. 2005. Incorporat-
ing non-local information into information extraction sys-
tems by gibbs sampling. In Proceedings of the 42nd ACL.
D. Freitag and A. McCallum. 1999. Information extraction
with HMMs and shrinkage. In Proceedings of the AAAI-
99 Workshop on Machine Learning for Information Ex-
traction.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of im-
ages. IEEE Transitions on Pattern Analysis and Machine
Intelligence, 6:721?741.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
Random Fields: Probabilistic models for segmenting and
labeling sequence data. In Proceedings of the 18th ICML,
pages 282?289. Morgan Kaufmann, San Francisco, CA.
T. R. Leek. 1997. Information extraction using hidden
Markov models. Master?s thesis, U.C. San Diego.
R. Malouf. 2002. Markov models for language-independent
named entity recognition. In Proceedings of the 6th
CoNLL, pages 187?190.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum en-
tropy Markov models for information extraction and seg-
mentation. In Proceedings of the 17th ICML, pages 591?
598. Morgan Kaufmann, San Francisco, CA.
A. Mikheev, M. Moens, and C. Grover. 1999. Named entity
recognition without gazetteers. In Proceedings of the 9th
EACL, pages 1?8.
J. Pearl. 1988. Probabilistic reasoning in intelligent systems:
Networks of plausible inference. In Morgan Kauffmann.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of NAACL-2003,
pages 134?141.
C. Sutton and A. McCallum. 2004. Collective segmentation
and labeling of distant entities in information extraction.
In ICML Workshop on Statistical Relational Learning and
Its connections to Other Fields.
B. Taskar, P. Abbeel, and D. Koller. 2002. Discriminative
probabilistic models for relational data. In Proceedings of
UAI-02.
J. S. Yedidia, W. T. Freeman, and Y. Weiss. 2000. Gener-
alized belief propagation. In Proceedings of NIPS-2000,
pages 689?695.
Alexander Yeh. 2000. More accurate tests for the statisti-
cal significance of result differences. In Proceedings of
COLING 2000.
1128
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 315?322, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Enhanced Answer Type Inference from Questions using Sequential Models
Vijay Krishnan and Sujatha Das and Soumen Chakrabarti?
Computer Science and Engineering Department, IIT Bombay, India
Abstract
Question classification is an important
step in factual question answering (QA)
and other dialog systems. Several at-
tempts have been made to apply statistical
machine learning approaches, including
Support Vector Machines (SVMs) with
sophisticated features and kernels. Curi-
ously, the payoff beyond a simple bag-of-
words representation has been small. We
show that most questions reveal their class
through a short contiguous token subse-
quence, which we call its informer span.
Perfect knowledge of informer spans can
enhance accuracy from 79.4% to 88%
using linear SVMs on standard bench-
marks. In contrast, standard heuristics
based on shallow pattern-matching give
only a 3% improvement, showing that the
notion of an informer is non-trivial. Us-
ing a novel multi-resolution encoding of
the question?s parse tree, we induce a Con-
ditional Random Field (CRF) to identify
informer spans with about 85% accuracy.
Then we build a meta-classifier using a
linear SVM on the CRF output, enhancing
accuracy to 86.2%, which is better than all
published numbers.
1 Introduction
An important step in factual question answering
(QA) and other dialog systems is to classify the
question (e.g., Who painted Olympia?) to the antic-
ipated type of the answer (e.g., person). This step
is called ?question classification? or ?answer type
identification?.
The answer type is picked from a hand-built tax-
onomy having dozens to hundreds of answer types
(Harabagiu et al, 2000; Hovy et al, 2001; Kwok et
al., 2001; Zheng, 2002; Dumais et al, 2002). QA
? soumen@cse.iitb.ac.in
systems can use the answer type to short-list answer
tokens from passages retrieved by an information re-
trieval (IR) subsystem, or use the type together with
other question words to inject IR queries.
Early successful QA systems used manually-
constructed sets of rules to map a question to a
type, exploiting clues such as the wh-word (who,
where, when, how many) and the head of noun
phrases associated with the main verb (what is the
tallest mountain in . . .).
With the increasing popularity of statistical NLP,
Li and Roth (2002), Hacioglu and Ward (2003) and
Zhang and Lee (2003) used supervised learning for
question classification on a data set from UIUC that
is now standard1. It has 6 coarse and 50 fine answer
types in a two-level taxonomy, together with 5500
training and 500 test questions. Webclopedia (Hovy
et al, 2001) has also published its taxonomy with
over 140 types.
The promise of a machine learning approach is
that the QA system builder can now focus on de-
signing features and providing labeled data, rather
than coding and maintaining complex heuristic rule-
bases. The data sets and learning systems quoted
above have made question classification a well-
defined and non-trivial subtask of QA for which al-
gorithms can be evaluated precisely, isolating more
complex factors at work in a complete QA system.
Prior work: Compared to human performance,
the accuracy of question classifiers is not high. In all
studies, surprisingly slim gains have resulted from
sophisticated design of features and kernels.
Li and Roth (2002) used a Sparse Network of
Winnows (SNoW) (Khardon et al, 1999). Their fea-
tures included tokens, parts of speech (POS), chunks
(non-overlapping phrases) and named entity (NE)
tags. They achieved 78.8% accuracy for 50 classes,
which improved to 84.2% on using an (unpublished,
to our knowledge) hand-built dictionary of ?seman-
tically related words?.
1http://l2r.cs.uiuc.edu/?cogcomp/Data/
QA/QC/
315
Hacioglu and Ward (2003) used linear support
vector machines (SVMs) with question word 2-
grams and error-correcting output codes (ECOC)?
but no NE tagger or related word dictionary?to get
80.2?82% accuracy.
Zhang and Lee (2003) used linear SVMs with
all possible question word q-grams, and obtained
79.2% accuracy. They went on to design an inge-
nious kernel on question parse trees, which yielded
visible gains for the 6 coarse labels, but only ?slight?
gains for the 50 fine classes, because ?the syntactic
tree does not normally contain the information re-
quired to distinguish between the various fine cate-
gories within a coarse category?.
Algorithm 6-class 50-class
Li and Roth, SNoW (1) 78.8(2)
Hacioglu et al, SVM+ECOC ? 80.2?82
Zhang & Lee, LinearSVMq 87.4 79.2
Zhang & Lee, TreeSVM 90 ?
SVM, ?perfect? informer 94.2 88
SVM, CRF-informer 93.4 86.2
Table 1: Summary of % accuracy for UIUC data.
(1) SNoW accuracy without the related word dictio-
nary was not reported. With the related-word dic-
tionary, it achieved 91%. (2) SNoW with a related-
word dictionary achieved 84.2% but the other algo-
rithms did not use it. Our results are summarized in
the last two rows, see text for details.
Our contributions: We introduce the notion of
the answer type informer span of the question (in
?2): a short (typically 1?3 word) subsequence of
question tokens that are adequate clues for question
classification; e.g.: How much does an adult ele-
phant weigh?
We show (in ?3.2) that a simple linear SVM us-
ing features derived from human-annotated informer
spans beats all known learning approaches. This
confirms our suspicion that the earlier approaches
suffered from a feature localization problem.
Of course, informers are useful only if we can find
ways to automatically identify informer spans. Sur-
prisingly, syntactic pattern-matching and heuristics
widely used in QA systems are not very good at cap-
turing informer spans (?3.3). Therefore, the notion
of an informer is non-trivial.
Using a parse of the question sentence, we derive
a novel set of multi-resolution features suitable for
training a conditional random field (CRF) (Lafferty
et al, 2001; Sha and Pereira, 2003). Our feature de-
sign paradigm may be of independent interest (?4).
Our informer tagger is about 85?87% accurate.
We use a meta-learning framework (Chan and
Stolfo, 1993) in which a linear SVM predicts the an-
swer type based on features derived from the origi-
nal question as well as the output of the CRF. This
meta-classifier beats all published numbers on stan-
dard question classification benchmarks (?4.4). Ta-
ble 1 (last two rows) summarizes our main results.
2 Informer overview
Our key insight is that a human can classify a ques-
tion based on very few tokens gleaned from skeletal
syntactic information. This is certainly true of the
most trivial classes (Who wrote Hamlet? or How
many dogs pull a sled at Iditarod?) but is also true of
more subtle clues (How much does a rhino weigh?).
In fact, informal experiments revealed the surpris-
ing property that only one contiguous span of tokens
is adequate for a human to classify a question. E.g.,
in the above question, a human does not even need
the how much clue once the word weigh is avail-
able. In fact, ?How much does a rhino cost?? has an
identical syntax but a completely different answer
type, not revealed by how much alone. The only
exceptions to the single-span hypothesis are multi-
function questions like ?What is the name and age
of . . .?, which should be assigned to multiple answer
types. In this paper we consider questions where one
type suffices.
Consider another question with multiple clues:
Who is the CEO of IBM? In isolation, the clue who
merely tells us that the answer might be a person or
country or organization, while CEO is perfectly pre-
cise, rendering who unnecessary. All of the above
applies a forteriori to what and which clues, which
are essentially uninformative on their own, as in
?What is the distance between Pisa and Rome??
Conventional QA systems use mild analysis on
the wh-clues, and need much more sophistication on
the rest of the question (e.g. inferring author from
wrote, and even verb subcategorization). We submit
that a single, minimal, suitably-chosen contiguous
316
span of question token/s, defined as the informer
span of the question, is adequate for question clas-
sification.
The informer span is very sensitive to the struc-
ture of clauses, phrases and possessives in the ques-
tion, as is clear from these examples (informers ital-
icized): ?What is Bill Clinton?s wife?s profession?,
and ?What country?s president was shot at Ford?s
Theater?. The choice of informer spans also de-
pends on the target classification system. Initially
we wished to handle definition questions separately,
and marked no informer tokens in ?What is digi-
talis?. However, what is is an excellent informer
for the UIUC class DESC:def (description, defi-
nition).
3 The meta-learning approach
We propose a meta-learning approach (?3.1) in
which the SVM can use features from the original
question as well as its informer span. We show
(?3.2) that human-annotated informer spans lead to
large improvements in accuracy. However, we show
(?3.3) that simple heuristic extraction rules com-
monly used in QA systems (e.g. head of noun phrase
following wh-word) cannot provide informers that
are nearly as useful. This naturally leads us to de-
signing an informer tagger in ?4.
Figure 1 shows our meta-learning (Chan and
Stolfo, 1993) framework. The combiner is a linear
multi-class one-vs-one SVM2, as in the Zhang and
Lee (2003) baseline. We did not use ECOC (Ha-
cioglu and Ward, 2003) because the reported gain is
less than 1%.
The word feature extractor selects unigrams and
q-grams from the question. In our experience, q =
1 or q = 2 were best; if unspecified, all possible
qgrams were used. Through tuning, we also found
that the SVM ?C? parameter (used to trade between
training data fit and model complexity) must be set
to 300 to achieve their published baseline numbers.
3.1 Adding informer features
We propose two very simple ways to derive features
from informers for use with SVMs. Initially, assume
that perfect informers are known for all questions;
2http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
question CRF Informer
span tagger
Word and qgram
feature extractor
Informer
feature extractor
Combined feature vector
class
SV
M
 
M
et
a 
Le
ar
n
er
Figure 1: The meta-learning approach.
later (?4) we study how to predict informers.
Informer q-grams: This comprises of all word q-
grams within the informer span, for all possible q.
E.g., such features enable effective exploitation of
informers like length or height to classify to the
NUMBER:distance class in the UIUC data.
Informer q-gram hypernyms: For each word or
compound within the informer span that is a Word-
Net noun, we add all hypernyms of all senses. The
intuition is that the informer (e.g. author, crick-
eter, CEO) is often narrower than a broad ques-
tion class (HUMAN:individual). Following hy-
pernym links up to person via WordNet produces a
more reliably correlated feature.
Given informers, other question words might
seem useless to the classifier. However, retaining
regular features from other question words is an ex-
cellent idea for the following reasons.
First, we kept word sense disambiguation (WSD)
outside the scope of this work because WSD en-
tails computation costs, and is unlikely to be reliable
on short single-sentence questions. Questions like
How long . . . or Which bank . . . can thus become
ambiguous and corrupt the informer hypernym fea-
tures. Additional question words can often help nail
the correct class despite the feature corruption.
Second, while our CRF-based approach to in-
former span tagging is better than obvious alterna-
tives, it still has a 15% error rate. For the questions
where the CRF prediction is wrong, features from
non-informer words give the SVM an opportunity to
still pick the correct question class.
Word features: Based on the above discussion,
one boolean SVM feature is created for every word
q-gram over all question tokens. In experiments, we
found bigrams (q = 2) to be most effective, closely
followed by unigrams (q = 1). As with informers,
we can also use hypernyms of regular words as SVM
317
features (marked ?Question bigrams + hypernyms?
in Table 2).
3.2 Benefits from ?perfect? informers
We first wished to test the hypothesis that identi-
fying informer spans to an SVM learner can im-
prove classification accuracy. Over and above the
class labels, we had two volunteers tag the 6000
UIUC questions with informer spans (which we call
?perfect??agreement was near-perfect).
Features Coarse Fine
Question trigrams 91.2 77.6
All question qgrams 87.2 71.8
All question unigrams 88.4 78.2
Question bigrams 91.6 79.4
+informer q-grams 94.0 82.4
+informer hypernyms 94.2 88.0
Question unigrams + all informer 93.4 88.0
Only informer 92.2 85.0
Question bigrams + hypernyms 91.6 79.4
Table 2: Percent accuracy with linear SVMs, ?per-
fect? informer spans, and various feature encodings.
Observe in Table 2 that the unigram baseline is
already quite competitive with the best prior num-
bers, and exploiting perfect informer spans beats all
known numbers. It is clear that both informer q-
grams and informer hypernyms are very valuable
features for question classification. The fact that no
improvement was obtained with over Question bi-
grams using Question hypernyms highlights the im-
portance of choosing a few relevant tokens as in-
formers and designing suitable features on them.
Table 3 (columns b and e) shows the benefits from
perfect informers broken down into broad question
types. Questions with what as the trigger are the
biggest beneficiaries, and they also form by far the
most frequent category.
The remaining question, one that we address in
the rest of the paper, is whether we can effectively
and accurately automate the process of providing in-
former spans to the question classifier.
3.3 Informers provided by heuristics
In ?4 we will propose a non-trivial solution to the
informer-tagging problem. Before that, we must jus-
tify that such machinery is indeed required.
Some leading QA systems extract words very
similar in function to informers from the parse tree
of the question. Some (Singhal et al, 2000) pick
the head of the first noun phrase detected by a shal-
low parser, while others use the head of the noun
phrase adjoining the main verb (Ramakrishnan et al,
2004). Yet others (Harabagiu et al, 2000; Hovy
et al, 2001) use hundreds of (unpublished to our
knowledge) hand-built pattern-matching rules on the
output of a full-scale parser.
A natural baseline is to use these extracted words,
which we call ?heuristic informers?, with an SVM
just like we used ?perfect? informers. All that re-
mains is to make the heuristics precise.
How: For questions starting with how, we use the
bigram starting with how unless the next word
is a verb.
Wh: If the wh-word is not how, what or which, use
the wh-word in the question as a separate fea-
ture.
WhNP: For questions having what and which, use
the WHNP if it encloses a noun. WHNP is the
Noun Phrase corresponding to the Wh-word,
given by a sentence parser (see ?4.2).
NP1: Otherwise, for what and which questions, the
first (leftmost) noun phrase is added to yet an-
other feature subspace.
Table 3 (columns c and f) shows that these
already-messy heuristic informers do not capture the
same signal quality as ?perfect? informers. Our find-
ings corroborate Li and Roth (2002), who report lit-
tle benefit from adding head chunk features for the
fine classification task.
Moreover, observe that using heuristic informer
features without any word features leads to rather
poor performance (column c), unlike using perfect
informers (column b) or even CRF-predicted in-
former (column d, see ?4). These clearly establish
that the notion of an informer is nontrivial.
4 Using CRFs to label informers
Given informers are useful but nontrivial to recog-
nize, the next natural question is, how can we learn
to identify them automatically? From earlier sec-
tions, it is clear (and we give evidence later, see Ta-
ble 5) that sequence and syntax information will be
318
6 coarse classes
B Only Informers B+ B+ B+
Type #Quest. (Bigrams) Perf.Inf H.Inf CRF.Inf Perf.Inf H.Inf CRF.Inf
what 349 88.8 89.4 69.6 79.3 91.7 87.4 91.4
which 11 72.7 100.0 45.4 81.8 100.0 63.6 81.8
when 28 100.0 100.0 100.0 100.0 100.0 100.0 100.0
where 27 100.0 96.3 100.0 96.3 100.0 100.0 100.0
who 47 100.0 100.0 100.0 100.0 100.0 100.0 100.0
how * 32 100.0 96.9 100.0 100.0 100.0 100.0 100.0
rest 6 100.0 100.0 100.0 66.7 100.0 66.7 66.7
Total 500 91.6 92.2 77.2 84.6 94.2 90.0 93.4
50 fine classes
what 349 73.6 82.2 61.9 78.0 85.1 79.1 83.1
which 11 81.8 90.9 45.4 73.1 90.9 54.5 81.8
when 28 100.0 100.0 100.0 100.0 100.0 100.0 100.0
where 27 92.6 85.2 92.6 88.9 88.9 92.5 88.9
who 47 97.9 93.6 93.6 93.6 100.0 100.0 97.9
how * 32 87.5 84.3 81.2 78.1 87.5 90.6 90.6
rest 6 66.7 66.7 66.7 66.7 100.0 66.7 66.7
Total 500 79.4 85.0 69.6 78.0 88.0 82.6 86.2
a b c d e f g
Table 3: Summary of % accuracy broken down by question type (referred from ?3.2, ?3.3 and ?4.4). a:
question bigrams, b: perfect informers only, c: heuristic informers only, d: CRF informers only, e?g:
bigrams plus perfect, heuristic and CRF informers.
important.
We will model informer span identification as a
sequence tagging problem. An automaton makes
probabilistic transitions between hidden states y,
one of which is an ?informer generating state?, and
emits tokens x. We observe the tokens and have to
guess which were produced from the ?informer gen-
erating state?.
Hidden Markov models are extremely popular for
such applications, but recent work has shown that
conditional random fields (CRFs) (Lafferty et al,
2001; Sha and Pereira, 2003) have a consistent ad-
vantage over traditional HMMs in the face of many
redundant features. We refer the reader to the above
references for a detailed treatment of CRFs. Here
we will regard a CRF as largely a black box3.
To train a CRF, we need a set of state nodes, a
transition graph on these nodes, and tokenized text
where each token is assigned a state. Once the CRF
is trained, it can be applied to a token sequence, pro-
3We used http://crf.sourceforge.net/
ducing a predicted state sequence.
4.1 State transition models
We started with the common 2-state ?in/out? model
used in information extraction, shown in the left half
of Figure 2. State ?1? is the informer-generating
state. Either state can be initial and final (double
circle) states.
0 1 0 1 2
What kind of an animal is Winnie the Pooh
What, kind,
of, an, is,
Winnie, the,
Pooh 
animal
What, kind,
of, an 
is, Winnie,
the, Pooh 
animal
start start
Figure 2: 2- and 3-state transition models.
The 2-state model can be myopic. Consider the
question pair
319
A: What country is the largest producer of wheat?
B: Name the largest producer of wheat
The i?1 context of producer is identical in A and
B. In B, for want of a better informer, we would want
producer to be flagged as the informer, although it
might refer to a country, person, animal, company,
etc. But in A, country is far more precise.
Any 2-state model that depends on positions i?1
to define features will fail to distinguish between A
and B, and might select both country and producer
in A. As we have seen with heuristic informers, pol-
luting the informer pool can significantly hurt SVM
accuracy.
Therefore we also use the 3-state ?begin/in/out?
(BIO) model. The initial state cannot be ?2? in the
3-state model; all states can be final. The 3-state
model allows at most one informer span. Once the
3-state model chooses country as the informer, it is
unlikely to stretch state 1 up to producer.
There is no natural significance to using four or
more states. Besides, longer range syntax dependen-
cies are already largely captured by the parser.
What is the capital city of Japan
WP VBZ DT NN NN IN NNP
NP NP
PP
NP
VP
SQ
SBARQ
WHNP
0
1
2
3
4
5
6

Le
v
e
l
Figure 3: Stanford Parser output example.
4.2 Features from a parse of the question
Sentences with similar parse trees are likely to have
the informer in similar positions. This was the in-
tuition behind Zhang et al?s tree kernel, and is also
our starting point. We used the Stanford Lexicalized
Parser (Klein and Manning, 2003) to parse the ques-
tion. (We assume familiarity with parse tree notation
for lack of space.) Figure 3 shows a sample parse
tree organized in levels. Our first step was to trans-
i 1 2 3 4 5 6 7
yi 0 0 0 1 1 2 2
xi What is the capital city of Japan
` ? Features for xis
1 WP,1 VBZ,1 DT,1 NN,1 NN,1 IN,1 NNP,1
2 WHNP,1 VP,1 NP,1 NP,1 NP,1 Null,1 NP,2
3 Null,1 Null,1 Null,1 Null,1 Null,1 PP,1 PP,1
4 Null,1 Null,1 NP,1 NP,1 NP,1 NP,1 NP,1
5 Null,1 SQ,1 SQ,1 SQ,1 SQ,1 SQ,1 SQ,1
6 SBARQ SBARQSBARQSBARQSBARQSBARQSBARQ
Table 4: A multi-resolution tabular view of the ques-
tion parse showing tag and num attributes. capital
city is the informer span with y = 1.
late the parse tree into an equivalent multi-resolution
tabular format shown in Table 4.
Cells and attributes: A labeled question com-
prises the token sequence xi; i = 1, . . . and the label
sequence yi, i = 1, . . . Each xi leads to a column
vector of observations. Therefore we use matrix no-
tation to write down x: A table cell is addressed as
x[i, `] where i is the token position (column index)
and ` is the level or row index, 1?6 in this example.
(Although the parse tree can be arbitrarily deep, we
found that using features from up to level ` = 2 was
adequate.)
Intuitively, much of the information required for
spotting an informer can be obtained from the part
of speech of the tokens and phrase/clause attachment
information. Conversely, specific word information
is generally sparse and misleading; the same word
may or may not be an informer depending on its po-
sition. E.g., ?What birds eat snakes?? and ?What
snakes eat birds?? have the same words but different
informers. Accordingly, we observe two properties
at each cell:
tag: The syntactic class assigned to the cell by
the parser, e.g. x[4, 2].tag = NP. It is well-known
that POS and chunk information are major clues to
informer-tagging, specifically, informers are often
nouns or noun phrases.
num: Many heuristics exploit the fact that the first
NP is known to have a higher chance of containing
informers than subsequent NPs. To capture this po-
sitional information, we define num of a cell at [i, `]
as one plus the number of distinct contiguous chunks
to the left of [i, `] with tags equal to x[4, 2].tag.
E.g., at level 2 in the table above, the capital city
320
forms the first NP, while Japan forms the second NP.
Therefore x[7, 2].num = 2.
In conditional models, it is notationally conve-
nient to express features as functions on (xi, yi). To
one unfamiliar with CRFs, it may seem strange that
yi is passed as an argument to features. At training
time, yi is indeed known, and at testing time, the
CRF algorithm efficiently finds the most probable
sequence of yis using a Viterbi search. True labels
are not revealed to the CRF at testing time.
Cell features IsTag and IsNum: E.g., the ob-
servation ?y4 = 1 and x[4, 2].tag = NP? is cap-
tured by the statement that ?position 4 fires the fea-
ture IsTag1,NP,2? (which has a boolean value).
There is an IsTagy,t,` feature for each (y, t, `)
triplet. Similarly, for every possible state y, ev-
ery possible num value n (up to some maximum
horizon), and every level `, we define boolean fea-
tures IsNumy,n,`. E.g., position 7 fires the feature
IsNum2,2,2 in the 3-state model, capturing the state-
ment ?x[7, 2].num = 2 and y7 = 2?.
Adjacent cell features IsPrevTag and
IsNextTag: Context can be exploited by a
CRF by coupling the state at position i with
observations at positions adjacent to position i
(extending to larger windows did not help). To
capture this, we use more boolean features: posi-
tion 4 fires the feature IsPrevTag1,DT,1 because
x[3, 1].tag = DT and y4 = 1. Position 4 also fires
IsPrevTag1,NP,2 because x[3, 2].tag = NP and
y4 = 1. Similarly we define a IsNextTagy,t,`
feature for each possible (y, t, `) triple.
State transition features IsEdge: Position i
fires feature IsEdgeu,v if yi?1 = u and yi = v.
There is one such feature for each state-pair (u, v)
allowed by the transition graph. In addition we have
sentinel features IsBeginu and IsEndu marking
the beginning and end of the token sequence.
4.3 Informer-tagging accuracy
We study the accuracy of our CRF-based informer
tagger wrt human informer annotations. In the next
section we will see the effect of CRF tagging on
question classification.
There are at least two useful measures of
informer-tagging accuracy. Each question has a
known set Ik of informer tokens, and gets a set
of tokens Ic flagged as informers by the CRF. For
each question, we can grant ourself a reward of 1 if
Ic = Ik, and 0 otherwise. In ?3.1, informers were
regarded as a separate (high-value) bag of words.
Therefore, overlap between Ic and Ik would be a
reasonable predictor of question classification accu-
racy. We use the Jaccard similarity |Ik?Ic|/|Ik?Ic|.
Table 5 shows the effect of using diverse feature sets.
Fraction Jaccard
Features used Ic = Ik overlap
IsTag 0.368 0.396
+IsNum 0.474 0.542
+IsPrevTag+IsNextTag 0.692 0.751
+IsEdge+IsBegin+IsEnd 0.848 0.867
Table 5: Effect of feature choices.
? IsTag features are not adequate.
? IsNum features improve accuracy 10?20%.
? IsPrevTag and IsNextTag (?+Prev
+Next?) add over 20% of accuracy.
? IsEdge transition features help exploit
Markovian dependencies and adds another
10?15% accuracy, showing that sequential
models are indeed required.
Type #Quest. Heuristic 2-state 3-state
Informers CRF CRF
what 349 57.3 68.2 83.4
which 11 77.3 83.3 77.2
when 28 75.0 98.8 100.0
where 27 84.3 100.0 96.3
who 47 55.0 47.2 96.8
how * 32 90.6 88.5 93.8
rest 6 66.7 66.7 77.8
Total 500 62.4 71.2 86.7
Table 6: Effect of number of CRF states, and com-
parison with the heuristic baseline (Jaccard accuracy
expressed as %).
Table 6 shows that the 3-state CRF performs
much better than the 2-state CRF, especially on diffi-
cult questions with what and which. It also compares
the Jaccard accuracy of informers found by the CRF
vs. informers found by the heuristics described in
?3.3. Again we see a clear superiority of the CRF
321
approach.
Unlike the heuristic approach, the CRF approach
is relatively robust to the parser emitting a somewhat
incorrect parse tree, which is not uncommon. The
heuristic approach picks the ?easy? informer, who,
over the better one, CEO, in ?Who is the CEO of
IBM?. Its bias toward the NP-head can also be a
problem, as in ?What country?s president . . .?.
4.4 Question classification accuracy
We have already seen in ?3.2 that perfect knowledge
of informers can be a big help. Because the CRF
can make mistakes, the margin may decrease. In this
section we study this issue.
We used questions with human-tagged informers
(?3.2) to train a CRF. The CRF was applied back
on the training questions to get informer predictions,
which were used to train the 1-vs-1 SVM meta-
learner (?3). Using CRF-tagged and not human-
tagged informers may seem odd, but this lets the
SVM learn and work around systematic errors in
CRF outputs.
Results are shown in columns d and g of Table 3.
Despite the CRF tagger having about 15% error, we
obtained 86.2% SVM accuracy which is rather close
to the the SVM accuracy of 88% with perfect in-
formers.
The CRF-generated tags, being on the training
data, might be more accurate that would be for un-
seen test cases, potentially misleading the SVM.
This turns out not to be a problem: clearly we are
very close to the upper bound of 88%. In fact, anec-
dotal evidence suggests that using CRF-assigned
tags actually helped the SVM.
5 Conclusion
We presented a new approach to inferring the type
of the answer sought by a well-formed natural lan-
guage question. We introduced the notion of a span
of informer tokens and extract it using a sequential
graphical model with a novel feature representation
derived from the parse tree of the question. Our ap-
proach beats the accuracy of recent algorithms, even
ones that used max-margin methods with sophisti-
cated kernels defined on parse trees.
An intriguing feature of our approach is that
when an informer (actor) is narrower than the ques-
tion class (person), we can exploit direct hyper-
nymy connections like actor to Tom Hanks, if avail-
able. Existing knowledge bases like WordNet and
Wikipedia, combined with intense recent work (Et-
zioni et al, 2004) on bootstrapping is-a hierarchies,
can thus lead to potentially large benefits.
Acknowledgments: Thanks to Sunita Sarawagi
for help with CRFs, and the reviewers for improv-
ing the presentation.
References
P. K Chan and S. J Stolfo. 1993. Experiments in mul-
tistrategy learning by meta-learning. In CIKM, pages
314?323, Washington, DC.
S Dumais, M Banko, E Brill, J Lin, and A Ng. 2002.
Web question answering: Is more always better? In
SIGIR, pages 291?298.
O Etzioni, M Cafarella, et al 2004. Web-scale informa-
tion extraction in KnowItAll. In WWW Conference,
New York. ACM.
K Hacioglu and W Ward. 2003. Question classifica-
tion with support vector machines and error correcting
codes. In HLT, pages 28?30.
S Harabagiu, D Moldovan, M Pasca, R Mihalcea, M Sur-
deanu, R Bunescu, R Girju, V Rus, and P Morarescu.
2000. FALCON: Boosting knowledge for answer en-
gines. In TREC 9, pages 479?488. NIST.
E Hovy, L Gerber, U Hermjakob, M Junk, and C.-Y
Lin. 2001. Question answering in Webclopedia. In
TREC 9. NIST.
R Khardon, D Roth, and L. G Valiant. 1999. Relational
learning for NLP using linear threshold elements. In
IJCAI.
D Klein and C. D Manning. 2003. Accurate unlexical-
ized parsing. In ACL, volume 41, pages 423?430.
C Kwok, O Etzioni, and D. S Weld. 2001. Scaling ques-
tion answering to the Web. In WWW Conference, vol-
ume 10, pages 150?161, Hong Kong.
J Lafferty, A McCallum, and F Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML.
X Li and D Roth. 2002. Learning question classifiers. In
COLING, pages 556?562.
G Ramakrishnan, S Chakrabarti, D. A Paranjpe, and
P Bhattacharyya. 2004. Is question answering an ac-
quired skill? In WWW Conference, pages 111?120,
New York.
F Sha and F Pereira. 2003. Shallow parsing with condi-
tional random fields. In HLT-NAACL, pages 134?141.
A Singhal, S Abney, M Bacchiani, M Collins, D Hindle,
and F Pereira. 2000. AT&T at TREC-8. In TREC 8,
pages 317?330. NIST.
D Zhang and W Lee. 2003. Question classification using
support vector machines. In SIGIR, pages 26?32.
Z Zheng. 2002. AnswerBus question answering system.
In HLT.
322
