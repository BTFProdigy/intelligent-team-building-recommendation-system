Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 208?215,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Learning Predictive Structures for Semantic Role Labeling of NomBank
Chang Liu and Hwee Tou Ng
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
{liuchan1, nght}@comp.nus.edu.sg
Abstract
This paper presents a novel application of
Alternating Structure Optimization (ASO)
to the task of Semantic Role Labeling (SRL)
of noun predicates in NomBank. ASO is
a recently proposed linear multi-task learn-
ing algorithm, which extracts the common
structures of multiple tasks to improve accu-
racy, via the use of auxiliary problems. In
this paper, we explore a number of different
auxiliary problems, and we are able to sig-
nificantly improve the accuracy of the Nom-
Bank SRL task using this approach. To our
knowledge, our proposed approach achieves
the highest accuracy published to date on the
English NomBank SRL task.
1 Introduction
The task of Semantic Role Labeling (SRL) is to
identify predicate-argument relationships in natural
language texts in a domain-independent fashion. In
recent years, the availability of large human-labeled
corpora such as PropBank (Palmer et al, 2005) and
FrameNet (Baker et al, 1998) has made possible
a statistical approach of identifying and classifying
the arguments of verbs in natural language texts.
A large number of SRL systems have been evalu-
ated and compared on the standard data set in the
CoNLL shared tasks (Carreras and Marquez, 2004;
Carreras and Marquez, 2005), and many systems
have performed reasonably well. Compared to the
previous CoNLL shared tasks (noun phrase bracket-
ing, chunking, clause identification, and named en-
tity recognition), SRL represents a significant step
towards processing the semantic content of natural
language texts.
Although verbs are probably the most obvious
predicates in a sentence, many nouns are also ca-
pable of having complex argument structures, often
with much more flexibility than its verb counterpart.
For example, compare affect and effect:
[subj Auto prices] [arg?ext greatly] [pred
affect] [obj the PPI].
[subj Auto prices] have a [arg?ext big]
[pred effect] [obj on the PPI].
The [pred effect] [subj of auto prices] [obj
on the PPI] is [arg?ext big].
[subj The auto prices?] [pred effect] [obj on
the PPI] is [arg?ext big].
The arguments of noun predicates can often be
more easily omitted compared to the verb predi-
cates:
The [pred effect] [subj of auto prices] is
[arg?ext big].
The [pred effect] [obj on the PPI] is
[arg?ext big].
The [pred effect] is [arg?ext big].
With the recent release of NomBank (Meyers et
al., 2004), it becomes possible to apply machine
learning techniques to the task. So far we are aware
of only one English NomBank-based SRL system
(Jiang and Ng, 2006), which uses the maximum
entropy classifier, although similar efforts are re-
ported on the Chinese NomBank by (Xue, 2006)
208
and on FrameNet by (Pradhan et al, 2004) us-
ing a small set of hand-selected nominalizations.
Noun predicates also appear in FrameNet semantic
role labeling (Gildea and Jurafsky, 2002), and many
FrameNet SRL systems are evaluated in Senseval-3
(Litkowski, 2004).
Semantic role labeling of NomBank is a multi-
class classification problem by nature. Using the
one-vs-all arrangement, that is, one binary classi-
fier for each possible outcome, the SRL task can
be treated as multiple binary classification problems.
In the latter view, we are presented with the oppor-
tunity to exploit the common structures of these re-
lated problems. This is known as multi-task learning
in the machine learning literature (Caruana, 1997;
Ben-David and Schuller, 2003; Evgeniou and Pon-
til, 2004; Micchelli and Pontil, 2005; Maurer, 2006).
In this paper, we apply Alternating Structure Op-
timization (ASO) (Ando and Zhang, 2005a) to the
semantic role labeling task on NomBank. ASO is
a recently proposed linear multi-task learning algo-
rithm based on empirical risk minimization. The
method requires the use of multiple auxiliary prob-
lems, and its effectiveness may vary depending on
the specific auxiliary problems used. ASO has
been shown to be effective on the following natu-
ral language processing tasks: text categorization,
named entity recognition, part-of-speech tagging,
and word sense disambiguation (Ando and Zhang,
2005a; Ando and Zhang, 2005b; Ando, 2006).
This paper makes two significant contributions.
First, we present a novel application of ASO to the
SRL task on NomBank. We explore the effect of
different auxiliary problems, and show that learn-
ing predictive structures with ASO results in signifi-
cantly improved SRL accuracy. Second, we achieve
accuracy higher than that reported in (Jiang and Ng,
2006) and advance the state of the art in SRL re-
search.
The rest of this paper is organized as follows. We
give an overview of NomBank and ASO in Sec-
tions 2 and 3 respectively. The baseline linear clas-
sifier is described in detail in Section 4, followed
by the description of the ASO classifier in Sec-
tion 5, where we focus on exploring different auxil-
iary problems. We provide discussions in Section 6,
present related work in Section 7, and conclude in
Section 8.
2 NomBank
NomBank annotates the set of arguments of noun
predicates, just as PropBank annotates the argu-
ments of verb predicates. As many noun predicates
are nominalizations (e.g., replacement vs. replace),
the same frames are shared with PropBank as much
as possible, thus achieving some consistency with
the latter regarding the accepted arguments and the
meanings of each label.
Unlike in PropBank, arguments in NomBank can
overlap with each other and with the predicate. For
example:
[location U.S.] [pred,subj,obj steelmakers]
have supplied the steel.
Here the predicate make has subject steelmakers and
object steel, analogous to Steelmakers make steel.
The difference is that here make and steel are both
part of the word steelmaker.
Each argument in NomBank is given one or more
labels, out of the following 20: ARG0, ARG1, ARG2,
ARG3, ARG4, ARG5, ARG8, ARG9, ARGM-ADV,
ARGM-CAU, ARGM-DIR, ARGM-DIS, ARGM-EXT,
ARGM-LOC, ARGM-MNR, ARGM-MOD, ARGM-
NEG, ARGM-PNC, ARGM-PRD, and ARGM-TMP.
Thus, the above sentence is annotated in NomBank
as:
[ARGM-LOC U.S.] [PRED,ARG0,ARG1 steelmak-
ers] have supplied the steel.
3 Alternating structure optimization
This section gives a brief overview of ASO as imple-
mented in this work. For a more complete descrip-
tion, see (Ando and Zhang, 2005a).
3.1 Multi-task linear classifier
Given a set of training samples consisting of n fea-
ture vectors and their corresponding binary labels,
{Xi, Yi} for i ? {1, . . . , n} where each Xi is a
p-dimensional vector, a binary linear classifier at-
tempts to approximate the unknown relation by Yi =
uTXi. The outcome is considered +1 if uTX is pos-
itive, or ?1 otherwise. A well-established way to
find the weight vector u is empirical risk minimiza-
tion with least square regularization:
u? = arg min
u
1
n
n
?
i=1
L
(
uTXi, Yi
)
+ ??u?2 (1)
209
Function L(p, y) is known as the loss function.
It encodes the penalty for a given discrepancy be-
tween the predicted label and the true label. In this
work, we use a modification of Huber?s robust loss
function, similar to that used in (Ando and Zhang,
2005a):
L(p, y) =
?
?
?
?4py if py < ?1
(1 ? py)2 if ?1 ? py < 1
0 if py ? 1
(2)
We fix the regularization parameter ? to 10?4,
similar to that used in (Ando and Zhang, 2005a).
The expression ?u?2 is defined as ?pi=1 u2p.
When m binary classification problems are to be
solved together, a h?p matrix ? may be used to cap-
ture the common structures of the m weight vectors
ul for l ? {1, . . . ,m} (h ? m). We mandate that
the rows of ? be orthonormal, i.e., ??T = Ih?h.
The h rows of ? represent the h most significant
components shared by all the u?s. This relationship
is modeled by
ul = wl + ?Tvl (3)
The parameters [{wl,vl},?] may then be found
by joint empirical risk minimization over all the
m problems, i.e., their values should minimize the
combined empirical risk:
m
?
l=1
(
1
n
n
?
i=1
L
(
(wl + ?Tvl)TXli, Y li
)
+ ??wl?2
)
(4)
3.2 The ASO algorithm
An important observation in (Ando and Zhang,
2005a) is that the binary classification problems
used to derive ? are not necessarily those problems
we are aiming to solve. In fact, new problems can be
invented for the sole purpose of obtaining a better ?.
Thus, we distinguish between two types of problems
in ASO: auxiliary problems, which are used to ob-
tain ?, and target problems, which are the problems
we are aiming to solve1.
For instance, in the argument identification task,
the only target problem is to identify arguments vs.
1Note that this definition deviates slightly from the one in
(Ando and Zhang, 2005a). We find the definition here more
convenient for our subsequent discussion.
non-arguments, whereas in the argument classifica-
tion task, there are 20 binary target problems, one to
identify each of the 20 labels (ARG0, ARG1, . . . ).
The target problems can also be used as an aux-
iliary problem. In addition, we can invent new aux-
iliary problems, e.g., in the argument identification
stage, we can predict whether there are three words
between the constituent and the predicate using the
features of argument identification.
Assuming there are k target problems and m aux-
iliary problems, it is shown in (Ando and Zhang,
2005a) that by performing one round of minimiza-
tion, an approximate solution of ? can be obtained
from (4) by the following algorithm:
1. For each of the m auxiliary problems, learn ul
as described by (1).
2. Find U = [u1,u2, . . . ,um], a p ? m matrix.
This is a simplified version of the definition in
(Ando and Zhang, 2005a), made possible be-
cause the same ? is used for all auxiliary prob-
lems.
3. Perform Singular Value Decomposition (SVD)
on U : U = V1DV T2 , where V1 is a p ? m ma-
trix. The first h columns of V1 are stored as
rows of ?.
4. Given ?, we learn w and v for each of the
k target problems by minimizing the empirical
risk of the associated training samples:
1
n
n
?
i=1
L
(
(w + ?Tv)TXi, Yi
)
+ ??w?2 (5)
5. The weight vector of each target problem can
be found by:
u = w + ?Tv (6)
By choosing a convex loss function, e.g., (2),
steps 1 and 4 above can be formulated as convex op-
timization problems and are efficiently solvable.
The procedure above can be considered as a Prin-
cipal Component Analysis in the predictor space.
Step (3) above extracts the most significant compo-
nents shared by the predictors of the auxiliary prob-
lems and hopefully, by the predictors of the target
210
problems as well. The hint of potential significant
components helps (5) to outperform the simple lin-
ear predictor (1).
4 Baseline classifier
The SRL task is typically separated into two stages:
argument identification and argument classification.
During the identification stage, each constituent in a
sentence?s parse tree is labeled as either argument
or non-argument. During the classification stage,
each argument is given one of the 20 possible labels
(ARG0, ARG1, . . . ). The linear classifier described
by (1) is used as the baseline in both stages. For
comparison, the F1 scores of a maximum entropy
classifier are also reported here.
4.1 Argument identification
Eighteen baseline features and six additional fea-
tures are proposed in (Jiang and Ng, 2006) for Nom-
Bank argument identification. As the improvement
of the F1 score due to the additional features is not
statistically significant, we use the set of eighteen
baseline features for simplicity. These features are
reproduced in Table 1 for easy reference.
Unlike in (Jiang and Ng, 2006), we do not prune
arguments dominated by other arguments or those
that overlap with the predicate in the training data.
Accordingly, we do not maximize the probability of
the entire labeled parse tree as in (Toutanova et al,
2005). After the features of every constituent are
extracted, each constituent is simply classified inde-
pendently as either argument or non-argument.
The linear classifier described above is trained on
sections 2 to 21 and tested on section 23. A max-
imum entropy classifier is trained and tested in the
same manner. The F1 scores are presented in the
first row of Table 3, in columns linear and maxent
respectively. The J&N column presents the result
reported in (Jiang and Ng, 2006) using both base-
line and additional features. The last column aso
presents the best result from this work, to be ex-
plained in Section 5.
4.2 Argument classification
In NomBank, some constituents have more than one
label. For simplicity, we always assign exactly one
label to each identified argument in this step. For the
0.16% arguments with multiple labels in the training
1 pred the stemmed predicate
2 subcat grammar rule that expands the
predicate P?s parent
3 ptype syntactic category (phrase
type) of the constituent C
4 hw syntactic head word of C
5 path syntactic path from C to P
6 position whether C is to the left/right of
or overlaps with P
7 firstword first word spanned by C
8 lastword last word spanned by C
9 lsis.ptype phrase type of left sister
10 rsis.hw right sister?s head word
11 rsis.hw.pos POS of right sister?s head word
12 parent.ptype phrase type of parent
13 parent.hw parent?s head word
14 partialpath path from C to the lowest com-
mon ancestor with P
15 ptype & length of path
16 pred & hw
17 pred & path
18 pred & position
Table 1: Features used in argument identification
data, we pick the first and discard the rest. (Note that
the same is not done on the test data.)
A diverse set of 28 features is used in (Jiang and
Ng, 2006) for argument classification. In this work,
the number of features is pruned to 11, so that we
can work with reasonably many auxiliary problems
in later experiments with ASO.
To find a smaller set of effective features, we start
with all the features considered in (Jiang and Ng,
2006), in (Xue and Palmer, 2004), and various com-
binations of them, for a total of 52 features. These
features are then pruned by the following algorithm:
1. For each feature in the current feature set, do
step (2).
2. Remove the selected feature from the feature
set. Obtain the F1 score of the remaining fea-
tures when applied to the argument classifica-
tion task, on development data section 24 with
gold identification.
3. Select the highest of all the scores obtained in
211
1 position to the left/right of or overlaps
with the predicate
2 ptype syntactic category (phrase
type) of the constituent C
3 firstword first word spanned by C
4 lastword last word spanned by C
5 rsis.ptype phrase type of right sister
6 nomtype NOM-TYPE of predicate sup-
plied by NOMLEX dictionary
7 predicate & ptype
8 predicate & lastword
9 morphed predicate stem & head word
10 morphed predicate stem & position
11 nomtype & position
Table 2: Features used in argument classification
step (2). The corresponding feature is removed
from the current feature set if its F1 score is the
same as or higher than the F1 score of retaining
all features.
4. Repeat steps (1)-(3) until the F1 score starts to
drop.
The 11 features so obtained are presented in Ta-
ble 2. Using these features, a linear classifier and a
maximum entropy classifier are trained on sections 2
to 21, and tested on section 23. The F1 scores are
presented in the second row of Table 3, in columns
linear and maxent respectively. The J&N column
presents the result reported in (Jiang and Ng, 2006).
4.3 Further experiments and discussion
In the combined task, we run the identification task
with gold parse trees, and then the classification task
with the output of the identification task. This way
the combined effect of errors from both stages on
the final classification output can be assessed. The
scores of this complete SRL system are presented in
the third row of Table 3.
To test the performance of the combined task on
automatic parse trees, we employ two different con-
figurations. First, we train the various classifiers
on sections 2 to 21 using gold argument labels and
automatic parse trees produced by Charniak?s re-
ranking parser (Charniak and Johnson, 2005), and
test them on section 23 with automatic parse trees.
This is the same configuration as reported in (Prad-
han et al, 2005; Jiang and Ng, 2006). The scores
are presented in the fourth row auto parse (t&t) in
Table 3.
Next, we train the various classifiers on sections 2
to 21 using gold argument labels and gold parse
trees. To minimize the discrepancy between gold
and automatic parse trees, we remove all the nodes
in the gold trees whose POS are -NONE-, as they
do not span any word and are thus never generated
by the automatic parser. The resulting classifiers are
then tested on section 23 using automatic parse trees.
The scores are presented in the last row auto parse
(test) of Table 3. We note that auto parse (test) con-
sistently outperforms auto parse (t&t).
We believe that auto parse (test) is a more realis-
tic setting in which to test the performance of SRL
on automatic parse trees. When presented with some
previously unseen test data, we are forced to rely on
its automatic parse trees. However, for the best re-
sults we should take advantage of gold parse trees
whenever possible, including those of the labeled
training data.
J&N maxent linear aso
identification 82.50 83.58 81.34 85.32
classification 87.80 88.35 87.86 89.17
combined 72.73 75.35 72.63 77.04
auto parse (t&t) 69.14 69.61 67.38 72.11
auto parse (test) - 71.19 69.05 72.83
Table 3: F1 scores of various classifiers on Nom-
Bank SRL
Our maximum entropy classifier consistently out-
performs (Jiang and Ng, 2006), which also uses a
maximum entropy classifier. The primary difference
is that we use a later version of NomBank (Septem-
ber 2006 release vs. September 2005 release). In ad-
dition, we use somewhat different features and treat
overlapping arguments differently.
5 Applying ASO to SRL
Our ASO classifier uses the same features as the
baseline linear classifier. The defining characteris-
tic, and also the major challenge in successfully ap-
plying the ASO algorithm is to find related auxiliary
problems that can reveal common structures shared
212
with the target problem. To organize our search for
good auxiliary problems for SRL, we separate them
into two categories, unobservable auxiliary prob-
lems and observable auxiliary problems.
5.1 Unobservable auxiliary problems
Unobservable auxiliary problems are problems
whose true outcome cannot be observed from a raw
text corpus but must come from another source,
e.g., human labeling. For instance, predicting the
argument class (i.e., ARG0, ARG1, . . . ) of a con-
stituent is an unobservable auxiliary problem (which
is also the only usable unobservable auxiliary prob-
lem here), because the true outcomes (i.e., the argu-
ment classes) are only available from human labels
annotated in NomBank.
For argument identification, we invent the follow-
ing 20 binary unobservable auxiliary problems to
take advantage of information previously unused at
this stage:
To predict the outcome of argument classi-
fication (i.e., ARG0, ARG1, . . . ) using the
features of argument identification (pred,
subcat, . . . ).
Thus for argument identification, we have 20 auxil-
iary problems (one auxiliary problem for predicting
each of the argument classes ARG0, ARG1, . . . ) and
one target problem (predicting whether a constituent
is an argument) for the ASO algorithm described in
Section 3.2.
In the argument classification task, the 20 binary
target problems are also the unobservable auxiliary
problems (one auxiliary problem for predicting each
of the argument classes ARG0, ARG1, . . . ). Thus,
we use the same 20 problems as both auxiliary prob-
lems and target problems.
We train an ASO classifier on sections 2 to 21 and
test it on section 23. With the 20 unobservable aux-
iliary problems, we obtain the F1 scores reported in
the last column of Table 3. In all the experiments,
we keep h = 20, i.e., all the 20 columns of V1 are
kept.
Comparing the F1 score of ASO against that of
the linear classifier in every task (i.e., identification,
classification, combined, both auto parse configura-
tions), the improvement achieved by ASO is statis-
tically significant (p < 0.05) based on the ?2 test.
Comparing the F1 score of ASO against that of the
maximum entropy classifier, the improvement in all
but one task (argument classification) is statistically
significant (p < 0.05). For argument classifica-
tion, the improvement is not statistically significant
(p = 0.08).
5.2 Observable auxiliary problems
Observable auxiliary problems are problems whose
true outcome can be observed from a raw text cor-
pus without additional externally provided labels.
An example is to predict whether hw=trader from
a constituent?s other features, since the head word
of a constituent can be obtained from the raw text
alone. By definition, an observable auxiliary prob-
lem can always be formulated as predicting a fea-
ture of the training data. Depending on whether the
baseline linear classifier already uses the feature to
be predicted, we face two possibilities:
Predicting a used feature In auxiliary problems
of this type, we must take care to remove the feature
itself from the training data. For example, we must
not use the feature path or pred&path to predict path
itself.
Predicting an unused feature These auxiliary
problems provide information that the classifier was
previously unable to incorporate. The desirable
characteristics of such a feature are:
1. The feature, although unused, should have been
considered for the target problem so it is prob-
ably related to the target problem.
2. The feature should not be highly correlated
with a used feature, e.g., since the lastword fea-
ture is used in argument identification, we will
not consider predicting lastword.pos as an aux-
iliary problem.
Each chosen feature can create thousands of bi-
nary auxiliary problems. E.g., by choosing to pre-
dict hw, we can create auxiliary problems predict-
ing whether hw=to, whether hw=trader, etc. To
have more positive training samples, we only predict
the most frequent features. Thus we will probably
predict whether hw=to, but not whether hw=trader,
since to occurs more frequently than trader as a head
word.
213
5.2.1 Argument identification
In argument identification using gold parse trees,
we experiment with predicting three unused features
as auxiliary problems: distance (distance between
the predicate and the constituent), parent.lsis.hw
(head word of the parent constituent?s left sister) and
parent.rsis.hw (head word of the parent constituent?s
right sister). We then experiment with predicting
four used features: hw, lastword, ptype and path.
The ASO classifier is trained on sections 2 to 21,
and tested on section 23. Due to the large data size,
we are unable to use more than 20 binary auxil-
iary problems or to experiment with combinations
of them. The F1 scores are presented in Table 4.
5.2.2 Argument classification
In argument classification using gold parse trees
and gold identification, we experiment with pre-
dicting three unused features path, partialpath, and
chunkseq (concatenation of the phrase types of text
chunks between the predicate and the constituent).
We then experiment with predicting three used fea-
tures hw, lastword, and ptype.
Combinations of these auxiliary problems are also
tested. In all combined, we use the first 100 prob-
lems from each of the six groups of observable aux-
iliary problems. In selected combined, we use the
first 100 problems from each of path, chunkseq, last-
word and ptype problems.
The ASO classifier is trained on sections 2 to 21,
and tested on section 23. The F1 scores are shown
in Table 5.
feature to be predicted F1
20 most frequent distances 81.48
20 most frequent parent.lsis.hws 81.51
20 most frequent parent.rsis.hws 81.60
20 most frequent hws 81.40
20 most frequent lastwords 81.33
20 most frequent ptypes 81.35
20 most frequent paths 81.47
linear baseline 81.34
Table 4: F1 scores of ASO with observable auxiliary
problems on argument identification. All h = 20.
From Table 4 and 5, we observe that although
the use of observable auxiliary problems consis-
feature to be predicted F1
300 most frequent paths 87.97
300 most frequent partialpaths 87.95
300 most frequent chunkseqs 88.09
300 most frequent hws 87.93
300 most frequent lastwords 88.01
all 63 ptypes 88.05
all combined 87.95
selected combined 88.07
linear baseline 87.86
Table 5: F1 scores of ASO with observable auxiliary
problems on argument classification. All h = 100.
tently improves the performance of the classifier,
the differences are small and not statistically signif-
icant. Further experiments combining unobservable
and observable auxiliary problems fail to outperform
ASO with unobservable auxiliary problems alone.
In summary, our work shows that unobservable
auxiliary problems significantly improve the perfor-
mance of NomBank SRL. In contrast, observable
auxiliary problems are not effective.
6 Discussions
Some of our experiments are limited by the exten-
sive computing resources required for a fuller ex-
ploration. For instance, ?predicting unused features?
type of auxiliary problems might hold some hope for
further improvement in argument identification, if a
larger number of auxiliary problems can be used.
ASO has been demonstrated to be an effec-
tive semi-supervised learning algorithm (Ando and
Zhang, 2005a; Ando and Zhang, 2005b; Ando,
2006). However, we have been unable to use un-
labeled data to improve the accuracy. One possible
reason is the cumulative noise from the many cas-
cading steps involved in automatic SRL of unlabeled
data: syntactic parse, predicate identification (where
we identify nouns with at least one argument), ar-
gument identification, and finally argument classi-
fication, which reduces the effectiveness of adding
unlabeled data using ASO.
7 Related work
Multi-output neural networks learn several tasks si-
multaneously. In addition to the target outputs,
214
(Caruana, 1997) discusses configurations where
both used inputs and unused inputs (due to excessive
noise) are utilized as additional outputs. In contrast,
our work concerns linear predictors using empirical
risk minimization.
A variety of auxiliary problems are tested in
(Ando and Zhang, 2005a; Ando and Zhang, 2005b)
in the semi-supervised settings, i.e., their auxiliary
problems are generated from unlabeled data. This
differs significantly from the supervised setting in
our work, where only labeled data is used. While
(Ando and Zhang, 2005b) uses ?predicting used
features? (previous/current/next word) as auxiliary
problems with good results in named entity recog-
nition, the use of similar observable auxiliary prob-
lems in our work gives no statistically significant im-
provements.
More recently, for the word sense disambiguation
(WSD) task, (Ando, 2006) experimented with both
supervised and semi-supervised auxiliary problems,
although the auxiliary problems she used are differ-
ent from ours.
8 Conclusion
In this paper, we have presented a novel application
of Alternating Structure Optimization (ASO) to the
Semantic Role Labeling (SRL) task on NomBank.
The possible auxiliary problems are categorized and
tested extensively. Our results outperform those re-
ported in (Jiang and Ng, 2006). To the best of our
knowledge, we achieve the highest SRL accuracy
published to date on the English NomBank.
References
R. K. Ando and T. Zhang. 2005a. A framework for learning
predictive structures from multiple tasks and unlabeled data.
Journal of Machine Learning Research.
R. K. Ando and T. Zhang. 2005b. A high-performance semi-
supervised learning method for text chunking. In Proc. of
ACL.
R. K. Ando. 2006. Applying alternating structure optimization
to word sense disambiguation. In Proc. of CoNLL.
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berkeley
FrameNet project. In Proc. of COLING-ACL.
S. Ben-David and R. Schuller. 2003. Exploiting task related-
ness for multiple task learning. In Proc. of COLT.
X. Carreras and L. Marquez. 2004. Introduction to the CoNLL-
2004 shared task: Semantic role labeling. In Proc. of
CoNLL.
X. Carreras and L. Marquez. 2005. Introduction to the CoNLL-
2005 shared task: Semantic role labeling. In Proc. of
CoNLL.
R. Caruana. 1997. Multitask Learning. Ph.D. thesis, School of
Computer Science, CMU.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In Proc. of ACL.
T. Evgeniou and M. Pontil. 2004. Regularized multitask learn-
ing. In Proc. of KDD.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics.
Z. P. Jiang and H. T. Ng. 2006. Semantic role labeling of Nom-
Bank: A maximum entropy approach. In Proc. of EMNLP.
K. C. Litkowski. 2004. Senseval-3 task: automatic labeling of
semantic roles. In Proc. of SENSEVAL-3.
A. Maurer. 2006. Bounds for linear multitask learning. Journal
of Machine Learning Research.
A. Meyers, R. Reeves, C. Macleod, R. Szekeley, V. Zielinska,
B. Young, and R. Grishman. 2004. The NomBank project:
An interim report. In Proc. of HLT/NAACL Workshop on
Frontiers in Corpus Annotation.
C. A. Micchelli and M. Pontil. 2005. Kernels for multitask
learning. In Proc. of NIPS.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposition
Bank: an annotated corpus of semantic roles. Computational
Linguistics.
S. S. Pradhan, H. Sun, W. Ward, J. H. Martin, and D. Jurafsky.
2004. Parsing arguments of nominalizations in English and
Chinese. In Proc. of HLT/NAACL.
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Martin,
and D. Jurafsky. 2005. Support vector learning for semantic
argument classification. Machine Learning.
K. Toutanova, A. Haghighi, and C. D. Manning. 2005. Joint
learning improves semantic role labeling. In Proc. of ACL.
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proc. of EMNLP.
N. Xue. 2006. Semantic role labeling of nominalized predi-
cates in Chinese. In Proc. of HLT/NAACL.
215
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 923?932,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
PEM: A Paraphrase Evaluation Metric Exploiting Parallel Texts
Chang Liu1 and Daniel Dahlmeier2 and Hwee Tou Ng1,2
1Department of Computer Science, National University of Singapore
2NUS Graduate School for Integrative Sciences and Engineering
{liuchan1,danielhe,nght}@comp.nus.edu.sg
Abstract
We present PEM, the first fully automatic met-
ric to evaluate the quality of paraphrases, and
consequently, that of paraphrase generation
systems. Our metric is based on three crite-
ria: adequacy, fluency, and lexical dissimilar-
ity. The key component in our metric is a ro-
bust and shallow semantic similarity measure
based on pivot language N-grams that allows
us to approximate adequacy independently of
lexical similarity. Human evaluation shows
that PEM achieves high correlation with hu-
man judgments.
1 Introduction
In recent years, there has been an increasing inter-
est in the task of paraphrase generation (PG) (Barzi-
lay and Lee, 2003; Pang et al, 2003; Quirk et al,
2004; Bannard and Callison-Burch, 2005; Kauchak
and Barzilay, 2006; Zhao et al, 2008; Zhao et al,
2009). At the same time, the task has seen appli-
cations such as machine translation (MT) (Callison-
Burch et al, 2006; Madnani et al, 2007; Madnani
et al, 2008), MT evaluation (Kauchak and Barzilay,
2006; Zhou et al, 2006a; Owczarzak et al, 2006),
summary evaluation (Zhou et al, 2006b), and ques-
tion answering (Duboue and Chu-Carroll, 2006).
Despite the research activities, we see two major
problems in the field. First, there is currently no con-
sensus on what attributes characterize a good para-
phrase. As a result, works on the application of para-
phrases tend to build their own PG system in view
of the immediate needs instead of using an existing
system.
Second, and as a consequence, no automatic eval-
uation metric exists for paraphrases. Most works in
this area resort to ad hoc manual evaluations, such as
the percentage of ?yes? judgments to the question of
?is the meaning preserved?. This type of evaluation
is incomprehensive, expensive, and non-comparable
between different studies, making progress hard to
judge.
In this work we address both problems. We pro-
pose a set of three criteria for good paraphrases: ad-
equacy, fluency, and lexical dissimilarity. Consid-
ering that paraphrase evaluation is a very subjec-
tive task with no rigid definition, we conduct ex-
periments with human judges to show that humans
generally have a consistent intuition for good para-
phrases, and that the three criteria are good indica-
tors.
Based on these criteria, we construct PEM (Para-
phrase Evaluation Metric), a fully automatic evalua-
tion metric for PG systems. PEM takes as input the
original sentence R and its paraphrase candidate P ,
and outputs a single numeric score b estimating the
quality of P as a paraphrase of R. PG systems can
be compared based on the average scores of their
output paraphrases. To the best of our knowledge,
this is the first automatic metric that gives an objec-
tive and unambiguous ranking of different PG sys-
tems, which serves as a benchmark of progress in
the field of PG.
The main difficulty of deriving PEM is to measure
semantic closeness without relying on lexical level
similarity. To this end, we propose bag of pivot lan-
guage N-grams (BPNG) as a robust, broad-coverage,
and knowledge-lean semantic representation for nat-
ural language sentences. Most importantly, BPNG
does not depend on lexical or syntactic similarity,
allowing us to address the conflicting requirements
of paraphrase evaluation. The only linguistic re-
923
source required to evaluate BPNG is a parallel text
of the target language and an arbitrary other lan-
guage, known as the pivot language.
We highlight that paraphrase evaluation and para-
phrase recognition (Heilman and Smith, 2010; Das
and Smith, 2009; Wan et al, 2006; Qiu et al, 2006)
are related yet distinct tasks. Consider two sentences
S1 and S2 that are the same except for the substitu-
tion of a single synonym. A paraphrase recognition
system should assign them a very high score, but a
paraphrase evaluation system would assign a rela-
tively low one. Indeed, the latter is often a better
indicator of how useful a PG system potentially is
for the applications of PG described earlier.
The rest of the paper is organized as follows. We
survey other automatic evaluation metrics in natural
language processing (NLP) in Section 2. We define
the task of paraphrase evaluation in Section 3 and
develop our metric in Section 4. We conduct a hu-
man evaluation and analyze the results in Section 5.
The correlation of PEM with human judgments is
studied in Section 6. Finally, we discuss our find-
ings and future work in Section 7 and conclude in
Section 8.
2 Related work
Themost well-known automatic evaluation metric in
NLP is BLEU (Papineni et al, 2002) for MT, based
on N-gram matching precisions. The simplicity of
BLEU lends well to MT techniques that directly op-
timize the evaluation metric.
The weakness of BLEU is that it operates purely
at the lexical surface level. Later works attempt to
take more syntactic and semantic features into con-
sideration (see (Callison-Burch et al, 2009) for an
overview). The whole spectrum of NLP resources
has found application in machine translation eval-
uation, including POS tags, constituent and depen-
dency parses, WordNet (Fellbaum, 1998), semantic
roles, textual entailment features, and more. Many
of these metrics have been shown to correlate bet-
ter with human judges than BLEU (Chan and Ng,
2008; Liu et al, 2010). Interestingly, few MT eval-
uation metrics exploit parallel texts as a source of
information, when statistical MT is centered almost
entirely around mining parallel texts.
Compared to these MT evaluation metrics, our
method focuses on addressing the unique require-
ment of paraphrase evaluation: that lexical closeness
does not necessarily entail goodness, contrary to the
basis of MT evaluation.
Inspired by the success of automatic MT evalua-
tion, Lin (2004) and Hovy et al (2006) propose au-
tomatic metrics for summary evaluation. The for-
mer is entirely lexical based, whereas the latter also
exploits constituent and dependency parses, and se-
mantic features derived from WordNet.
The only prior attempt to devise an automatic
evaluation metric for paraphrases that we are aware
of is ParaMetric (Callison-Burch et al, 2008), which
compares the collection of paraphrases discovered
by automatic paraphrasing algorithms against a
manual gold standard collected over the same sen-
tences. The recall and precision of several current
paraphrase generation systems are evaluated. Para-
Metric does not attempt to propose a single metric
to correlate well with human judgments. Rather, it
consists of a few indirect and partial measures of the
quality of PG systems.
3 Task definition
The first step in defining a paraphrase evaluation
metric is to define a good paraphrase. Merriam-
Webster dictionary gives the following definition:
a restatement of a text, passage, or work giving
the meaning in another form. We identify two key
points in this definition: (1) that the meaning is pre-
served, and (2) that the lexical form is different. To
which we add a third, that the paraphrase must be
fluent.
The first and last point are similar to MT evalua-
tion, where adequacy and fluency have been estab-
lished as the standard criteria. In paraphrase evalu-
ation, we have one more: lexical dissimilarity. Al-
though lexical dissimilarity is seemingly the easiest
to judge automatically among the three, it poses an
interesting challenge to automatic evaluation met-
rics, as overlap with the reference has been the basis
of almost all evaluation metrics. That is, while MT
evaluation and paraphrase evaluation are conceptu-
ally closely related, the latter actually highlights the
deficiencies of the former, namely that in most au-
tomatic evaluations, semantic equivalence is under-
represented and substituted by lexical and syntactic
924
equivalence.
The task of paraphrase evaluation is then defined
as follows: Given an original sentence R and a para-
phrase candidate P , output a numeric score b esti-
mating the quality of P as a paraphrase ofR by con-
sidering adequacy, fluency, and lexical dissimilarity.
In this study, we use a scale of 1 to 5 (inclusive) for
b, although that can be transformed linearly into any
range desired.
We observe here that the overall assessment b is
not a linear combination of the three measures. In
particular, a high dissimilarity score is meaningless
by itself. It could simply be that the paraphrase is
unrelated to the source sentence, or is incoherent.
However, when accompanied by high adequacy and
fluency scores, it differentiates the mediocre para-
phrases from the good ones.
4 Paraphrase Evaluation Metric (PEM)
In this section we devise our metric according to the
three proposed evaluation criteria, namely adequacy,
fluency, and dissimilarity. The main challenge is to
measure the adequacy, or semantic similarity, com-
pletely independent of any lexical similarity. We ad-
dress this problem in Sections 4.1 to 4.3. The re-
maining two criteria are addressed in Section 4.4,
and we describe the final combined metric PEM in
Section 4.5.
4.1 Phrase-level semantic representation
Without loss of generality, suppose we are to eval-
uate English paraphrases, and have been supplied
many sentence-aligned parallel texts of French and
English as an additional resource. We can then align
the parallel texts at word level automatically using
well-known algorithms such as GIZA++ (Och and
Ney, 2003) or the Berkeley aligner (Liang et al,
2006; Haghighi et al, 2009).
To measure adequacy without relying on lexical
similarity, we make the key observation that the
aligned French texts can act as a proxy of the se-
mantics to a fragment of an English text. If two En-
glish phrases are often mapped to the same French
phrase, they can be considered similar in mean-
ing. Similar observations have been made by previ-
ous researchers (Wu and Zhou, 2003; Bannard and
Callison-Burch, 2005; Callison-Burch et al, 2006;
Snover et al, 2009). We can treat the distribution
of aligned French phrases as a semantic representa-
tion of the English phrase. The semantic distance
between two English phrases can then be measured
by their degree of overlap in this representation.
In this work, we use the widely-used phrase ex-
traction heuristic in (Koehn et al, 2003) to extract
phrase pairs from parallel texts into a phrase table1.
The phrases extracted do not necessarily correspond
to the speakers? intuition. Rather, they are units
whose boundaries are preserved during translation.
However, the distinction does not affect our work.
4.2 Segmenting a sentence into phrases
Having established a way to measure the similarity
of two English phrases, we now extend the concept
to sentences. Here we discuss how to segment an
English sentence (the original or the paraphrase) into
phrases.
From the phrase table, we know the frequencies of
all the phrases and we approximate the probability
of a phrase p by:
Pr(p) =
N(p)
?
p? N(p?)
(1)
N(?) is the count of a phrase in the phrase table, and
the denominator is a constant for all p. We define
the likelihood of segmenting a sentence S into a se-
quence of phrases (p1, p2, . . . , pn) by:
Pr(p1, p2, . . . , pn|S) =
1
Z(S)
n?
i=1
Pr(pi) (2)
where Z(S) is a normalizing constant. The best seg-
mentation of S according to Equation 2 can be cal-
culated efficiently using a dynamic programming al-
gorithm. Note that Z(S) does not need to be calcu-
lated, as it is the same for all different segmentations
of S. The formula has a strong preference for longer
phrases, since every Pr(pi) has a large denominator.
Many sentences are impossible to segment into
known phrases, including all those containing out-
of-vocabulary words. We therefore allow any sin-
gle word w to be considered as a phrase, and if
N(w) = 0, we use N(w) = 0.5 instead.
1The same heuristic is used in the popular MT package
Moses.
925
Bonjour , / 0.9
Salut , / 0.1
Querrien / 1.0 . / 1.0
Figure 1: A confusion network in the pivot language
Bonjour , /on0.9ur S a lSt
Figure 2: A degenerated confusion network in the pivot
language
4.3 Sentence-level semantic representation
Simply merging the phrase-level semantic represen-
tations is insufficient to produce a sensible sentence-
level semantic representation. For example, assume
the English sentence Morning , sir . is segmented as
a single phrase, because the following phrase pair is
found in the phrase table:
En: Morning , sir .
Fr: Bonjour , monsieur .
However, another English sentence Hello , Quer-
rien . has an out-of-vocabulary word Querrien
and consequently the most probable segmentation is
found to be ?Hello , ||| Querrien ||| .?:
En: Hello ,
Fr: Bonjour , (Pr(Bonjour ,|Hello ,) = 0.9)
Fr: Salut , (Pr(Salut ,|Hello ,) = 0.1)
En: Querrien
Fr: Querrien
En: .
Fr: .
A naive comparison of the bags of French phrases
aligned to Morning , sir . and Hello , Querrien . de-
picted above would conclude that the two sentences
are completely unrelated, as their bags of aligned
French phrases are completely disjoint. We tackle
this problem by constructing a confusion network
representation of the French phrases, as shown in
Figures 1 and 2. The confusion network is formed
by first joining the different French translations of
every English phrase in parallel, and then joining
these segments in series.
The confusion network is a compact representa-
tion of an exponentially large number of (likely mal-
formed) weighted French sentences. We can easily
enumerate the N-grams from the confusion network
representation and collect the statistics for this en-
semble of French sentences efficiently. In this work,
we consider N up to 4. The N-grams for Hello ,
Querrien . are:
1-grams: Bonjour (0.9), Salut (0.1), comma
(1.0), Querrien (1.0), period (1.0).
2-grams: Bonjour comma (0.9), Salut comma
(0.1), comma Querrien (1.0), Querrien period (1.0).
3-grams: Bonjour comma Querrien (0.9), Salut
comma Querrien (0.1), comma Querrien period
(1.0).
4-grams: Bonjour comma Querrien period (0.9),
Salut comma Querrien period (0.1).
We call this representation of an English sentence
a bag of pivot language N-grams (BPNG), where
French is the pivot language in our illustrating ex-
ample. We can extract the BPNG of Morning , sir .
analogously:
1-grams: Bonjour (1.0), comma (1.0), monsieur
(1.0), period (1.0).
2-grams: Bonjour comma (1.0), comma mon-
sieur (1.0), monsieur period (1.0).
3-grams: Bonjour comma monsieur (1.0),
comma monsieur period (1.0).
4-grams: Bonjour comma monsieur period (1.0).
The BPNG of Hello , Querrien. can now be com-
pared sensibly with that of the sentence Morning ,
sir . We use the F1 agreement between the two BP-
NGs as a measure of the semantic similarity. The F1
agreement is defined as
F1 =
2 ? Precision ? Recall
Precision + Recall
The precision and the recall for an original sen-
tence R and a paraphrase P is defined as follows.
Let French N-gram g ? BPNG(R)?BPNG(P ), and
WR(g) and WP (g) be the weights of g in the BPNG
of R and P respectively, then
Precision =
?
g min(WR(g),WP (g))
?
g WP (g)
Recall =
?
g min(WR(g),WP (g))
?
g WR(g)
In our example, the numerators for both the preci-
sion and the recall are 0.9 + 1 + 1 + 0.9, for the N-
grams Bonjour, comma, period, and Bonjour comma
926
respectively. The denominators for both terms are
10.0. Consequently, F1 = Precision = Recall =
0.38, and we conclude that the two sentences are
38% similar. We call the resulting metric the pivot
language F1. Note that since F1 is symmetric with
respect to the precision and the recall, our metric is
unaffected whether we consider Morning, sir. as the
paraphrase of Hello, Querrien . or the other way
round.
An actual example from our corpus is:
Reference sihanouk ||| put forth ||| this proposal |||
in ||| a statement ||| made ||| yesterday ||| .
Paraphrase shihanuk ||| put forward ||| this pro-
posal ||| in his ||| yesterday ||| ?s statement ||| .
The ||| sign denotes phrase segmentation as de-
scribed earlier. Our semantic representation suc-
cessfully recognizes that put forth and put forward
are paraphrases of each other, based on their similar
Chinese translation statistics (ti2 chu1 in Chinese).
4.4 Fluency and dissimilarity
We measure the fluency of a paraphrase by a nor-
malized language model score Pn, defined by
Pn =
logPr(S)
length(S)
where Pr(S) is the sentence probability predicted
by a standard 4-gram language model.
We measure dissimilarity between two English
sentences using the target language F1, where we
collect the bag of all N-grams up to 4-grams from
each English (referred to as the target language) sen-
tence. The target language F1 is then defined as the
F1 agreement of the two bags of N-grams, analogous
to the definition of the pivot language F1. The target
language F1 correlates positively with the similar-
ity of the two sentences, or equivalently, negatively
with the dissimilarity of the two sentences.
4.5 The metric
To produce the final PEM metric, we combine the
three component automatic metrics, pivot language
F1, normalized language model, and target language
F1, which measure adequacy, fluency, and dissimi-
larity respectively.
As discussed previously, a linear combination of
the three component metrics is insufficient. We turn
to support vector machine (SVM) regression with
the radial basis function (RBF) kernel. The RBF is
a simple and expressive function, commonly used to
introduce non-linearity into large margin classifica-
tions and regressions.
RBF(xi, xj) = e
???xi?xj?2
We use the implementation in SVM light
(Joachims, 1999). The SVM is to be trained on a set
of human-judged paraphrase pairs, where the three
component automatic metrics are fit to the human
overall assessment. After training, the model can
then be used to evaluate new paraphrase pairs in a
fully automatic fashion.
5 Human evaluation
To validate our definition of paraphrase evaluation
and the PEM method, we conduct an experiment
to evaluate paraphrase qualities manually, which al-
lows us to judge whether paraphrase evaluation ac-
cording to our definition is an inherently coherent
and well-defined problem. The evaluation also al-
lows us to establish an upper bound for the para-
phrase evaluation task, and to validate the contribu-
tion of the three proposed criteria to the overall para-
phrase score.
5.1 Evaluation setup
We use the Multiple-Translation Chinese Corpus
(MTC)2 as a source of paraphrases. The MTC
corpus consists of Chinese news articles (993 sen-
tences in total) and multiple sentence-aligned En-
glish translations. We select one human transla-
tion as the original text. Two other human transla-
tions and two automatic machine translations serve
as paraphrases of the original sentences. We refer to
the two human translations and the two MT systems
as paraphrase systems human1, human2, machine1,
and machine2.
We employ three human judges to manually as-
sess the quality of 300 original sentences paired
with each of the four paraphrases. Therefore, each
judge assesses 1,200 paraphrase pairs in total. The
2LDC Catalog No.: LDC2002T01
927
judgment for each paraphrase pair consists of four
scores, each given on a five-point scale:
? Adequacy (Is the meaning preserved ade-
quately?)
? Fluency (Is the paraphrase fluent English?)
? Lexical Dissimilarity (How much has the para-
phrase changed the original sentence?)
? Overall score
The instructions given to the judges for the overall
score were as follows.
A good paraphrase should convey the
same meaning as the original sentence,
while being as different as possible on the
surface form and being fluent and gram-
matical English. With respect to this defi-
nition, give an overall score from 5 (per-
fect) to 1 (unacceptable) for this para-
phrase.
The paraphrases are presented to the judges in a ran-
dom order and without any information as to which
paraphrase system produced the paraphrase.
In addition to the four paraphrase systems men-
tioned above, for each original English sentence, we
add three more artificially constructed paraphrases
with pre-determined ?human? judgment scores: (1)
the original sentence itself, with adequacy 5, fluency
5, dissimilarity 1, and overall score 2; (2) a random
sentence drawn from the same domain, with ade-
quacy 1, fluency 5, dissimilarity 5, and overall score
1; and (3) a random sentence generated by a uni-
gram language model, with adequacy 1, fluency 1,
dissimilarity 5, and overall score 1. These artificial
paraphrases serve as controls in our evaluation. Our
final data set therefore consists of 2,100 paraphrase
pairs with judgments on 4 different criteria.
5.2 Inter-judge correlation
The first step in our evaluation is to investigate the
correlation between the human judges. We use Pear-
son?s correlation coefficient, a common measure of
the linear dependence between two random vari-
ables.
We investigate inter-judge correlation at the sen-
tence and at the system level. At the sentence
level, we construct three vectors, each containing
the 1,200 sentence level judgments from one judge
Sentence Level System Level
Judge A Judge B Judge A Judge B
Judge B 0.6406 - 0.9962 -
Judge C 0.6717 0.5993 0.9995 0.9943
Table 1: Inter-judge correlation for overall paraphrase
score
Sentence Level System Level
Adequacy 0.7635 0.7616
Fluency 0.3736 0.3351
Dissimilarity -0.3737 -0.3937
Dissimilarity (A,F?4) 0.8881 0.9956
Table 2: Correlation of paraphrase criteria with overall
score
for the overall score. The pair-wise correlations be-
tween these three vectors are then taken. Note that
we exclude the three artificial control paraphrase
systems from consideration, as that would inflate the
correlation. At the system level, we construct three
vectors each of size four, containing the average
scores given by one judge to each of the four para-
phrase systems human1, human2, machine1, and
machine2. The correlations are then taken in the
same fashion.
The results are listed in Table 1. The inter-judge
correlation is between 0.60 and 0.67 at the sentence
level and above 0.99 at the system level. These cor-
relation scores can be considered very high when
compared to similar results reported in MT evalu-
ations, e.g., Blatz et al (2003). The high correlation
confirms that our evaluation task is well defined.
Having confirmed that human judgments corre-
late strongly, we combine the scores of the three
judges by taking their arithmetic mean. Together
with the three artificial control paraphrase systems,
they form the human reference evaluation which we
use for the remainder of the experiments.
5.3 Adequacy, fluency, and dissimilarity
In this section, we empirically validate the impor-
tance of our three proposed criteria: adequacy, flu-
ency, and lexical dissimilarity. This can be done by
measuring the correlation of each criterion with the
overall score. The system and sentence level corre-
lations are shown in Table 2.
We can see a positive correlation of adequacy and
928
Figure 3: Scatter plot of dissimilarity vs. overall score
for paraphrases with high adequacy and fluency.
fluency with the overall score, and the correlation
with adequacy is particularly strong. Thus, higher
adequacy and to a lesser degree higher fluency indi-
cate higher paraphrase quality to the human judges.
On the other hand, dissimilarity is found to have a
negative correlation with the overall score. This can
be explained by the fact that the two human trans-
lations usually have much higher similarity with the
reference translation, and at the same time are scored
as better paraphrases. This effect dominates a sim-
ple linear fitting of the paraphrase score vs. the dis-
similarity, resulting in the counter intuitive negative
correlation. We note that a high dissimilarity alone
tells us little about the quality of the paraphrase.
Rather, we expect dissimilarity to be a differentia-
tor between the mediocre and good paraphrases.
To test this hypothesis, we select the subset of
paraphrase pairs that receive adequacy and fluency
scores of at least four and again measure the cor-
relation of the dissimilarity and the overall score.
The result is tabulated in the last row of Table 2 and
shows a strong correlation. Figure 3 shows a scatter
plot of the same result3.
The empirical results presented so far confirm that
paraphrase evaluation is a well-defined task permit-
ting consistent subjective judgments, and that ade-
quacy, fluency, and dissimilarity are suitable criteria
for paraphrase quality.
3We automatically add jitter (small amounts of noise) for
ease of presentation.
6 PEM vs. human evaluation
In the last section, we have shown that the three
proposed criteria are good indicators of paraphrase
quality. In this section, we investigate how well
PEM can predict the overall paraphrase quality from
the three automatic metrics (pivot language F1, nor-
malized language model, and target language F1),
designed to match the three evaluation criteria. We
describe the experimental setup in Section 6.1, be-
fore we show the results in Section 6.2.
6.1 Experimental setup
We build the phrase table used to evaluate the pivot
language F1 from the FBIS Chinese-English corpus,
consisting of about 250,000 Chinese sentences, each
with a single English translation. The paraphrases
are taken from the MTC corpus in the same way
as the human experiment described in Section 5.1.
Both FBIS and MTC are in the Chinese newswire
domain.
We stem all English words in both data sets with
the Porter stemmer (Porter, 1980). We use the maxi-
mum entropy segmenter of (Low et al, 2005) to seg-
ment the Chinese part of the FBIS corpus. Subse-
quently, word level Chinese-English alignments are
generated using the Berkeley aligner (Liang et al,
2006; Haghighi et al, 2009) with five iterations of
training. Phrases are then extracted with the widely-
used heuristic in Koehn et al (2003). We extract
phrases of up to four words in length.
Bags of Chinese pivot language N-grams are ex-
tracted for all paraphrase pairs as described in Sec-
tion 4.3. For computational efficiency, we consider
only edges of the confusion network with probabil-
ities higher than 0.1, and only N-grams with proba-
bilities higher than 0.01 in the bag of N-grams. We
collect N-grams up to length four.
The language model used to judge fluency is
trained on the English side of the FBIS parallel text.
We use SRILM (Stolcke, 2002) to build a 4-gram
model with the default parameters.
The PEM SVM regression is trained on the para-
phrase pairs for the first 200 original English sen-
tences and tested on the paraphrase pairs of the re-
maining 100 original English sentences. Thus, there
are 1,400 instances for training and 700 instances for
testing. For each instance, we calculate the values
929
Figure 4: Scatter plot of PEM vs. human judgment (over-
all score) at the sentence level
Figure 5: Scatter plot of PEM vs. human judgment (over-
all score) at the system level
of pivot language F1, normalized language model
score, and target language F1. These values serve
as the input features to the SVM regression and the
target value is the human assessment of the overall
score, on a scale of 1 to 5.
6.2 Results
As in the human evaluation, we investigate the cor-
relation of the PEM scores with the human judg-
ments at the sentence and at the system level. Fig-
ure 4 shows the sentence level PEM scores plotted
against the human overall scores, where each human
overall score is the arithmetic mean of the scores
given by the three judges. The Pearson correlation
between the automatic PEM scores and the human
judgments is 0.8073. This is substantially higher
than the sentence level correlation of MT metrics
Sentence Level System Level
PEM vs. Human Avg. 0.8073 0.9867
PEM vs. Judge A 0.5777 0.9757
PEM vs. Judge B 0.5281 0.9892
PEM vs. Judge C 0.5231 0.9718
Table 3: Correlation of PEMwith human judgment (over-
all score)
like BLEU. For example, the highest sentence level
Pearson correlation by any metric in the Metrics-
MATR 2008 competition (Przybocki et al, 2009)
was 0.6855 by METEOR-v0.6; BLEU achieved a
correlation of 0.4513.
Figure 5 shows the system level PEM scores plot-
ted against the human scores. The Pearson correla-
tion between PEM scores and the human scores at
the system level is 0.9867.
We also calculate the Pearson correlation between
PEM and each individual human judge. Here, we
exclude the three artificial control paraphrase sys-
tems from the data, to make the results compara-
ble to the inter-judge correlation presented in Sec-
tion 5.2. The correlation is between 0.52 and 0.57
at the sentence level and between 0.97 and 0.98 at
the system level. As we would expect, the correla-
tion between PEM and a human judge is not as high
as the correlation between two human judges, but
PEM still shows a strong and consistent correlation
with all three judges. The results are summarized in
Table 3.
7 Discussion and future work
The paraphrases that we use in this study are not
actual machine generated paraphrases. Instead, the
English paraphrases are multiple translations of the
same Chinese source sentence. Our seven ?para-
phrase systems? are two human translators, two ma-
chine translation systems, and three artificially cre-
ated extreme scenarios. The reason for using multi-
ple translations is that we could not find any PG sys-
tem that can paraphrase a whole input sentence and
is publicly available. We intend to obtain and evalu-
ate paraphrases generated from real PG systems and
compare their performances in a follow-up study.
Our method models paraphrasing up to the phrase
level. Unfortunately, it makes no provisions for syn-
930
tactic paraphrasing at the sentence level, which is
probably a much greater challenge, and the literature
offers few successes to draw inspirations from. We
hope to be able to partially address this deficiency in
future work.
The only external linguistic resource required by
PEM is a parallel text of the target language and
another arbitrary language. While we only use
Chinese-English parallel text in this study, other lan-
guage pairs need to be explored too. Another alter-
native is to collect parallel texts against multiple for-
eign languages, e.g., using Europarl (Koehn, 2005).
We leave this for future work.
Our evaluation method does not require human-
generated references like in MT evaluation. There-
fore, we can easily formulate a paraphrase genera-
tor by directly optimizing the PEM metric, although
solving it is not trivial:
paraphrase(R) = argmax
P
PEM(P,R)
where R is the original sentence and P is the para-
phrase.
Finally, the PEM metric, in particular the seman-
tic representation BPNG, can be useful in many
other contexts, such as MT evaluation, summary
evaluation, and paraphrase recognition. To facil-
itate future research, we will package and release
PEM under an open source license at http://
nlp.comp.nus.edu.sg/software.
8 Conclusion
We proposed PEM, a novel automatic metric for
paraphrase evaluation based on adequacy, fluency,
and lexical dissimilarity. The key component in our
metric is a novel technique to measure the seman-
tic similarity of two sentences through their N-gram
overlap in an aligned foreign language text. We
conducted an extensive human evaluation of para-
phrase quality which shows that our proposed met-
ric achieves high correlation with human judgments.
To the best of our knowledge, PEM is the first auto-
matic metric for paraphrase evaluation.
Acknowledgments
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore.
References
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. of ACL.
R. Barzilay and L. Lee. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In Proc. of HLT-NAACL.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2003. Con-
fidence estimation for machine translation. Technical
report, CLSP Workshop Johns Hopkins University.
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proc. of HLT-NAACL.
C. Callison-Burch, T. Cohn, and M. Lapata. 2008. Para-
Metric: An automatic evaluation metric for paraphras-
ing. In Proc. of COLING.
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.
2009. Findings of the 2009 Workshop on Statistical
Machine Translation. In Proceedings of WMT.
Y.S. Chan and H.T. Ng. 2008. MAXSIM: A maximum
similarity metric for machine translation evaluation.
In Proc. of ACL-08: HLT.
D. Das and N.A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition. In
Proc. of ACL-IJCNLP.
P. Duboue and J. Chu-Carroll. 2006. Answering the
question you wish they had asked: The impact of para-
phrasing for question answering. In Proc. of HLT-
NAACL Companion Volume: Short Papers.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press, Cambridge, MA.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Proc. of ACL-IJCNLP.
M. Heilman and N.A. Smith. 2010. Tree edit models
for recognizing textual entailments, paraphrases, and
answers to questions. In Proc. of NAACL.
E. Hovy, C.Y. Lin, L. Zhou, and J. Fukumoto. 2006.
Automated summarization evaluation with basic ele-
ments. In Proc. of LREC.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Sch?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT Press.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proc. of HLT-NAACL.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
931
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit, volume 5.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of HLT-NAACL.
C.Y. Lin. 2004. ROUGE: A package for automatic eval-
uation of summaries. In Proc. of the ACL-04 Work-
shop on Text Summarization Branches Out.
C. Liu, D. Dahlmeier, and H.T. Ng. 2010. TESLA:
translation evaluation of sentences with linear-
programming-based analysis. In Proc. of WMT.
J.K. Low, H.T. Ng, and W. Guo. 2005. A maximum
entropy approach to Chinese word segmentation. In
Proc. of the 4th SIGHAN Workshop.
N. Madnani, N.F. Ayan, P. Resnik, and B.J. Dorr. 2007.
Using paraphrases for parameter tuning in statistical
machine translation. In Proc. of WMT.
N. Madnani, P. Resnik, B.J. Dorr, and R. Schwartz. 2008.
Are multiple reference translations necessary? Investi-
gating the value of paraphrased reference translations
in parameter optimization. In Proc. of AMTA.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
K. Owczarzak, D. Groves, J. Van Genabith, and A. Way.
2006. Contextual bitext-derived paraphrases in auto-
matic MT evaluation. In Proc. of WMT.
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based
alignment of multiple translations: Extracting para-
phrases and generating new sentences. In Proc. of
HLT-NAACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
M. Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 40(3).
M. Przybocki, K. Peterson, S. Bronsart, and G Sanders.
2009. Evaluating machine translation with LFG de-
pendencies. Machine Translation, 23(2).
L. Qiu, M.Y. Kan, and T.S. Chua. 2006. Paraphrase
recognition via dissimilarity significance classifica-
tion. In Proc. of EMNLP.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation. In
Proc. of EMNLP.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009.
Fluency, adequacy, or HTER? Exploring different hu-
man judgments with a tunable MT metric. In Proc. of
WMT.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. of ICSLP.
S. Wan, M. Dras, R. Dale, and C Paris. 2006. Using
dependency-based features to take the ?para-farce? out
of paraphrase. In Proc. of ALTW 2006.
H. Wu and M. Zhou. 2003. Synonymous collocation
extraction using translation information. In Proc. of
ACL.
S.Q. Zhao, C. Niu, M. Zhou, T. Liu, and S. Li. 2008.
Combining multiple resources to improve SMT-based
paraphrasing model. In Proc. of ACL-08: HLT.
S.Q. Zhao, X. Lan, T. Liu, and S. Li. 2009. Application-
driven statistical paraphrase generation. In Proc. of
ACL-IJCNLP.
L. Zhou, C.Y. Lin, and E. Hovy. 2006a. Re-evaluating
machine translation results with paraphrase support.
In Proc. of EMNLP.
L. Zhou, C.Y. Lin, D.S. Munteanu, and E. Hovy. 2006b.
ParaEval: Using paraphrases to evaluate summaries
automatically. In Proc. of HLT-NAACL.
932
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 375?384,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Better Evaluation Metrics Lead to Better Machine Translation
Chang Liu1 and Daniel Dahlmeier2 and Hwee Tou Ng1,2
1Department of Computer Science, National University of Singapore
2NUS Graduate School for Integrative Sciences and Engineering
{liuchan1,danielhe,nght}@comp.nus.edu.sg
Abstract
Many machine translation evaluation met-
rics have been proposed after the seminal
BLEU metric, and many among them have
been found to consistently outperform BLEU,
demonstrated by their better correlations with
human judgment. It has long been the hope
that by tuning machine translation systems
against these new generation metrics, ad-
vances in automatic machine translation eval-
uation can lead directly to advances in auto-
matic machine translation. However, to date
there has been no unambiguous report that
these new metrics can improve a state-of-the-
art machine translation system over its BLEU-
tuned baseline.
In this paper, we demonstrate that tuning
Joshua, a hierarchical phrase-based statistical
machine translation system, with the TESLA
metrics results in significantly better human-
judged translation quality than the BLEU-
tuned baseline. TESLA-M in particular is
simple and performs well in practice on large
datasets. We release all our implementation
under an open source license. It is our hope
that this work will encourage the machine
translation community to finally move away
from BLEU as the unquestioned default and
to consider the new generation metrics when
tuning their systems.
1 Introduction
The dominant framework of machine translation
(MT) today is statistical machine translation (SMT)
(Hutchins, 2007). At the core of the system is the
decoder, which performs the actual translation. The
decoder is parameterized, and estimating the optimal
set of parameter values is of paramount importance
in getting good translations. In SMT, the parame-
ter space is explored by a tuning algorithm, typically
MERT (Minimum Error Rate Training) (Och, 2003),
though the exact method is not important for our
purpose. The tuning algorithm carries out repeated
experiments with different decoder parameter val-
ues over a development data set, for which reference
translations are given. An automatic MT evaluation
metric compares the output of the decoder against
the reference(s), and guides the tuning algorithm to-
wards iteratively better decoder parameters and out-
put translations. The quality of the automatic MT
evaluation metric therefore has an immediate effect
on the whole system.
The first automatic MT evaluation metric to show
a high correlation with human judgment is BLEU
(Papineni et al, 2002). Together with its close vari-
ant the NIST metric, they have quickly become the
standard way of tuning statistical machine transla-
tion systems. While BLEU is an impressively sim-
ple and effective metric, recent evaluations have
shown that many new generation metrics can out-
perform BLEU in terms of correlation with human
judgment (Callison-Burch et al, 2009; Callison-
Burch et al, 2010). Some of these new metrics in-
clude METEOR (Banerjee and Lavie, 2005; Lavie
and Agarwal, 2007), TER (Snover et al, 2006),
MAXSIM (Chan and Ng, 2008; Chan and Ng,
2009), and TESLA (Liu et al, 2010).
Given the close relationship between automatic
MT and automatic MT evaluation, the logical expec-
tation is that a better MT evaluation metric would
375
lead to better MT systems. However, this linkage
has not yet been realized. In the SMT community,
MT tuning still uses BLEU almost exclusively.
Some researchers have investigated the use of bet-
ter metrics for MT tuning, with mixed results. Most
notably, Pado? et al (2009) reported improved human
judgment using their entailment-based metric. How-
ever, the metric is heavy weight and slow in practice,
with an estimated runtime of 40 days on the NIST
MT 2002/2006/2008 dataset, and the authors had to
resort to a two-phase MERT process with a reduced
n-best list. As we shall see, our experiments use the
similarly sized WMT 2010 dataset, and most of our
runs take less than one day.
Cer et al (2010) compared tuning a phrase-based
SMT system with BLEU, NIST, METEOR, and
TER, and concluded that BLEU and NIST are still
the best choices for MT tuning, despite the proven
higher correlation of METEOR and TER with hu-
man judgment.
In this work, we investigate the effect of MERT
using BLEU, TER, and two variants of TESLA,
TESLA-M and TESLA-F, on Joshua (Li et al,
2009), a state-of-the-art hierarchical phrase-based
SMT system (Chiang, 2005; Chiang, 2007). Our
empirical study is carried out in the context of WMT
2010, for the French-English, Spanish-English, and
German-English machine translation tasks. We
show that Joshua responds well to the change of
evaluation metric, in that a system trained on met-
ric M typically does well when judged by the same
metric M. We further evaluate the different systems
with manual judgments and show that the TESLA
family of metrics (both TESLA-M and TESLA-F)
significantly outperforms BLEU when used to guide
the MERT search.
The rest of this paper is organized as follows. In
Section 2, we describe the four evaluation metrics
used. Section 3 outlines our experimental set up us-
ing the WMT 2010 machine translation tasks. Sec-
tion 4 presents the evaluation results, both automatic
and manual. Finally, we discuss our findings in Sec-
tion 5, future work in Section 6, and conclude in
Section 7.
2 Evaluation metrics
This section describes the metrics used in our exper-
iments. We do not seek to explain all their variants
and intricate details, but rather to outline their core
characteristics and to highlight their similarities and
differences. In particular, since all our experiments
are based on single references, we omit the com-
plications due to multiple references and refer our
readers instead to the respective original papers for
the details.
2.1 BLEU
BLEU is fundamentally based on n-gram match pre-
cisions. Given a reference text R and a translation
candidate T , we generate the bag of all n-grams con-
tained inR and T for n = 1, 2, 3, 4, and denote them
as BNGnR and BNGnT respectively. The n-gram pre-
cision is thus defined as
Pn =
|BNGnR ? BNGnT|
|BNGnT|
To compensate for the lack of the recall measure,
and hence the tendency to produce short translations,
BLEU introduces a brevity penalty, defined as
BP =
{
1 if|T| > |R|
e1?|R|/|T | if|T| ? |R|
where the | ? | operator denotes the size of a bag or
the number of words in a text. The metric is finally
defined as
BLEU(R,T) = BP? 4
?
P1P2P3P4
BLEU is a very simple metric requiring neither
training nor language-specific resources. Its use of
the brevity penalty is however questionable, as sub-
sequent research on n-gram-based metrics has con-
sistently found that recall is in fact a more potent
indicator than precision (Banerjee and Lavie, 2005;
Zhou et al, 2006; Chan and Ng, 2009). As we
shall see, despite the BP term, BLEU still exhibits
a strong tendency to produce short translations.
2.2 TER
TER is based on counting transformations rather
than n-gram matches. The metric is defined as the
376
minimum number of edits needed to change a can-
didate translation T to the reference R, normalized
by the length of the reference, i.e.,
TER(R,T) = number of edits|R|
One edit is defined as one insertion, deletion, or
substitution of a single word, or the shift of a con-
tiguous sequence of words, regardless of size and
distance. Minimizing the edit distance so defined
has been shown to be NP-complete, so the evalua-
tion is carried out in practice by a heuristic greedy
search algorithm.
TER is a strong contender as the leading new gen-
eration automatic metric and has been used in major
evaluation campaigns such as GALE. Like BLEU,
it is simple and requires no language specific re-
sources. TER also corresponds well to the human
intuition of an evaluation metric.
2.3 TESLA-M
TESLA1 is a family of linear programming-based
metrics proposed by Liu et al (2010) that incor-
porates many newer ideas. The simplest varia-
tion is TESLA-M2, based on matching bags of n-
grams (BNG) like BLEU. However, unlike BLEU,
TESLA-M formulates the matching process as a
real-valued linear programming problem, thereby
allowing the use of weights. An example weighted
BNG matching problem is shown in Figure 1.
Two kinds of weights are used in TESLA-M.
First, the metric emphasizes the content words by
discounting the weight of an n-gram by 0.1 for ev-
ery function word it contains. Second, the similarity
between two n-grams is a function dependent on the
lemmas, the WordNet synsets (Fellbaum, 1998), and
the POS tag of every word in the n-grams.
Each node in Figure 1 represents one weighted n-
gram. The four in the top row represent one BNG,
and the three at the bottom represent the other BNG.
The goal of the linear programming problem is to
assign weights to the links between the two BNGs,
so as to maximize the sum of the products of the link
weights and their corresponding similarity scores.
1The source code of TESLA is available at
nlp.comp.nus.edu.sg/software/
2M stands for minimal.
w=1.0 w=0.1 w=0.1 w=0.1
w=0.01 w=0.1 w=0.1
s=0.1 s=0.8s=0.5 s=0.8
Good morning morning , , sir sir .
Hello , , Querrien Querrien .
s=0.4
(a) The matching problem
w=1.0 w=0.1 w=0.1 w=0.1
w=0.01 w=0.1 w=0.1
w=0.1w=0.01 w=0.1
s885Go8d m r o8d m r Gn nGimd imdG.
g,HH8Gn nGel,ddm, el,ddm, G.
(b) The solution
Figure 1: Matching two weighted bags of n-grams.
w denotes the weight and s denotes the similarity.
The constraints of the linear programming prob-
lem are: (1) all assigned weights must be non-
negative, and (2) the sum of weights assigned to all
links connecting a node cannot exceed the node?s
weight. Intuitively, we allow splitting n-grams into
fractional counts, and match them giving priority to
the pairs with the highest similarities.
The linear programming formulation ensures that
the matching can be solved uniquely and efficiently.
Once the solution is found and let the maximized
objective function value be S, the precision is com-
puted as S over the sum of weights of the translation
candidate n-grams. Similarly, the recall is S over the
sum of weights of the reference n-grams. The pre-
cision and the recall are then combined to form the
F-0.8 measure:
Fn =
Precision? Recall
0.8? Precision + 0.2? Recall
This F-measure gives more importance to the re-
call, reflecting its closer correlation with human
judgment. Fn for n = 1, 2, 3 are calculated and av-
eraged to produce the final score.
TESLA-M gains an edge over the previous two
metrics by the use of lightweight linguistic features
such as lemmas, synonym dictionaries, and POS
377
Metric Spearman?s rho
TESLA-F .94
TESLA-M .93
meteor-next-* .92
1-TERp .90
BLEU-4-v13a-c .89
Table 1: Selected system-level Spearman?s rho cor-
relation with the human judgment for the into-
English task, as reported in WMT 2010.
Metric Spearman?s rho
TESLA-M .93
meteor-next-rank .82
1-TERp .81
BLEU-4-v13a-c .80
TESLA-F .76
Table 2: Selected system-level Spearman?s rho cor-
relation with the human judgment for the out-of-
English task, as reported in WMT 2010.
tags. While such tools are usually available even for
languages other than English, it does make TESLA-
M more troublesome to port to non-English lan-
guages.
TESLA-M did well in the WMT 2010 evaluation
campaign. According to the system-level correla-
tion with human judgments (Tables 1 and 2), it ranks
top for the out-of-English task and very close to the
top for the into-English task (Callison-Burch et al,
2010).
2.4 TESLA-F3
TESLA-F builds on top of TESLA-M. While word-
level synonyms are handled in TESLA-M by exam-
ining WordNet synsets, no modeling of phrase-level
synonyms is possible. TESLA-F attempts to rem-
edy this shortcoming by exploiting a phrase table
between the target language and another language,
known as the pivot language.
Assume the target language is English and the
pivot language is French, i.e., we are provided with
an English-French phrase table. Let R and T be the
3TESLA-F refers to the metric called TESLA in (Liu et al,
2010). To minimize confusion, in this work we call the metric
TESLA-F and refer to the whole family of metrics as TESLA.
F stands for full.
w=1.=0s858G=1od 0s8m8r8nmi
Figure 2: A degenerate confusion network in
French. The phrase table maps Good morning , sir .
to Bonjour , monsieur .
w=1.=0s858G8od 
mrn0i858G8odg
,0HsseH18G8gdo d8G8gdo
Figure 3: A confusion network in French. The
phrase table maps Hello , to Bonjour , with P = 0.9
and to Salut , with P = 0.1.
reference and the translation candidate respectively,
both in English. As an example,
R: Good morning , sir .
T: Hello , Querrien .
TESLA-F first segments both R and T into
phrases to maximize the probability of the sen-
tences. For example, suppose both Good morning ,
sir . and Hello , can be found in the English-French
phrase table, and proper name Querrien is out-of-
vocabulary, then a likely segmentation is:
R: ||| Good morning , sir . |||
T: ||| Hello , ||| Querrien ||| . |||
Each English phrase is then mapped to a bag
of weighted French phrases using the phrase table,
transforming the English sentences into confusion
networks resembling Figures 2 and 3. French n-
grams are extracted from these confusion network
representations, known as pivot language n-grams.
The bag of pivot language n-grams generated by R
is then matched against that generated by T with
the same linear programming formulation used in
TESLA-M.
TESLA-F incorporates all the F-measures used in
TESLA-M, with the addition of (1) the F-measures
generated over the pivot language n-grams described
above, and (2) the normalized language model score,
defined as 1n logP , where n is the length of thetranslation, and P the language model probability.
Unlike BLEU and TESLA-M which rely on simple
averages (geometric and arithmetic average respec-
tively) to combine the component scores, TESLA-
378
F trains the weights over a set of human judg-
ments using a linear ranking support vector machine
(RSVM). This allows TESLA-F to exploit its com-
ponents more effectively, but also makes it more te-
dious to work with and introduces potential domain
mismatch problems.
TESLA-F makes use of even more linguistic in-
formation than TESLA-M, and has the capability
of recognizing some forms of phrase synonyms.
TESLA-F ranked top for the into-English evalua-
tion task in WMT 2010 (Table 1). However, the
added complexity, in particular the use of the lan-
guage model score and the tuning of the component
weights appear to make it less stable than TESLA-M
in practice. For example, it did not perform as well
in the out-of-English task.
3 Experimental setup
We run our experiments in the setting of the WMT
2010 news commentary machine translation cam-
paign, for three language pairs:
1. French-English (fr-en): the training text con-
sists of 84624 sentences of French-English bi-
text. The average French sentence length is 25
words.
2. Spanish-English (es-en): the training text con-
sists of 98598 sentences of Spanish-English bi-
text. The average Spanish sentence length is 25
words.
3. German-English (de-en): the training text con-
sists of 100269 sentences of German-English
bitext. The average German sentence length is
22 words.
The average English sentence length is 21 words
for all three language pairs. The text domain is
newswire report, and the English sides of the train-
ing texts for the three language pairs overlap sub-
stantially. The development data are 2525 four-way
translated sentences, in English, French, Spanish,
and German respectively. Similarly, the test data
are 2489 four-way translated sentences. As a conse-
quence, all MT evaluations involve only single ref-
erences.
We follow the standard approach for training hi-
erarchical phrase-based SMT systems. First, we to-
kenize and lowercase the training texts and create
fr-en es-en de-en
BLEU 3:49 (4) 5:09 (6) 2:41 (4)
TER 4:03 (4) 3:59 (4) 3:59 (5)
TESLA-M 13:00 (3) 17:34 (5) 13:40 (4)
TESLA-F 35:07 (4) 40:54 (4) 40:28 (5)
Table 3: Z-MERT training times in hours:minutes
and number of iterations in parenthesis
word alignments using the Berkeley aligner (Liang
et al, 2006; Haghighi et al, 2009) with five iter-
ations of training. Then, we create suffix arrays
and extract translation grammars for the develop-
ment and test set with Joshua in its default setting.
The maximum phrase length is 10. For the language
model, we use SRILM (Stolcke, 2002) to build a
trigram model with modified Kneser-Ney smooth-
ing from the monolingual training data supplied in
WMT 2010.
Parameter tuning is carried out using Z-
MERT (Zaidan, 2009). TER and BLEU are al-
ready implemented in the publicly released version
of Z-MERT, and Z-MERT?s modular design makes
it easy to integrate TESLA-M and TESLA-F into the
package. The maximum number of MERT iterations
is set to 100, although we observe that in practice,
the algorithm converges after 3 to 6 iterations. The
number of intermediate initial points per iteration is
set to 20 and the n-best list is capped to 300 trans-
lations. Table 3 shows the training times and the
number of MERT iterations for each of the language
pairs and evaluation metrics.
We use the publicly available version of TESLA-
F, which comes with phrase tables and a ranking
SVM model trained on the WMT 2010 development
data.
4 Automatic and manual evaluations
The results of the automatic evaluations are pre-
sented in Table 4. The best score according to each
metric is shown in bold. Note that smaller TER
scores are better, as are larger BLEU, TESLA-M,
and TESLA-F scores.4
We note that Joshua generally responds well to
the change of tuning metric. A system tuned on met-
4The TESLA-F scores shown here have been monotonically
scaled.
379
tune\test BLEU TER TESLA-M TESLA-F
BLEU 0.5237 0.6029 0.3922 0.4114
TER 0.5239 0.6028 0.3880 0.4095
TESLA-M 0.5005 0.6359 0.4170 0.4223
TESLA-F 0.4992 0.6377 0.4164 0.4224
(a) The French-English task
tune\test BLEU TER TESLA-M TESLA-F
BLEU 0.5641 0.5764 0.4315 0.4328
TER 0.5667 0.5725 0.4204 0.4282
TESLA-M 0.5253 0.6246 0.4511 0.4398
TESLA-F 0.5331 0.6111 0.4498 0.4409
(b) The Spanish-English task
tune\test BLEU TER TESLA-M TESLA-F
BLEU 0.4963 0.6329 0.3369 0.3927
TER 0.4963 0.6355 0.3191 0.3851
TESLA-M 0.4557 0.7055 0.3784 0.4070
TESLA-F 0.4642 0.6888 0.3753 0.4068
(c) The German-English task
Table 4: Automatic evaluation scores
P(A) Kappa
French-English 0.6846 0.5269
Spanish-English 0.6124 0.4185
German-English 0.3973 0.0960
Table 5: Inter-annotator agreement
ric M usually does the best or very close to the best
when evaluated by M. On the other hand, the dif-
ferences between different systems can be substan-
tial, especially between BLEU/TER and TESLA-
M/TESLA-F.
In addition to the automatic evaluation, we en-
listed twelve judges to manually evaluate the first
200 test sentences. Four judges are assigned to
each of the three language pairs. For each test sen-
tence, the judges are presented with the source sen-
tence, the reference English translation, and the out-
put from the four competing Joshua systems. The
order of the translation candidates is randomized so
that the judges will not see any patterns. The judges
are instructed to rank the four candidates, and ties
are allowed.
The inter-annotator agreement is reported in Ta-
ble 5. We consider the judgment for a pair of system
outputs as one data point. Let P (A) be the propor-
tion of times that the annotators agree, and P (E)
fr-en es-en de-en
BLEU 44.1% 33.8% 49.6%
TER 41.4% 34.4% 47.8%
TESLA-M 65.8% 49.5% 57.8%
TESLA-F 66.4% 53.8% 55.1%
Table 6: Percentage of times each system produces
the best translation
be the proportion of times that they would agree by
chance. The Kappa coefficient is defined as
Kappa = P(A)? P(E)1? P(E)
In our experiments, each data point has three pos-
sible values: A is preferred, B is preferred, and no
preference, hence P (E) = 1/3. Our Kappa is cal-
culated in the same way as the WMT workshops
(Callison-Burch et al, 2009; Callison-Burch et al,
2010).
Kappa coefficients between 0.4 and 0.6 are con-
sidered moderate, and our values are in line with
those reported in the WMT 2010 translation cam-
paign. The exception is the German-English pair,
where the annotators only reach slight agreement.
This might be caused by the lower quality of Ger-
man to English translations compared to the other
two language pairs.
Table 6 shows the proportion of times each sys-
tem produces the best translation among the four.
We observe that the rankings are largely consis-
tent across different language pairs: Both TESLA-
F and TESLA-M strongly outperform BLEU and
TER. Note that the values in each column do not
add up to 100%, since the candidate translations are
often identical, and even a different translation can
receive the same human judgment.
Table 7 shows our main result, the pairwise com-
parison between the four systems for each of the lan-
guage pairs. Again the rankings consistently show
that both TESLA-F and TESLA-M strongly out-
perform BLEU and TER. All differences are sta-
tistically significant under the Sign Test at p =
0.01, with the exception of TESLA-M vs TESLA-
F in the French-English task, BLEU vs TER in the
Spanish-English task, and TESLA-M vs TESLA-F
and BLEU vs TER in the German-English task. The
results provide strong evidence that tuning machine
380
A\B BLEU TER TESLA-M TESLA-F
BLEU - 11.4% / 6.5% 29.1% / 52.1% 28.0% / 52.3%
TER 6.5% / 11.4% - 28.6% / 54.5% 27.5% / 55.0%
TESLA-M 52.1% / 29.1% 54.5% / 28.6% - 7.6% / 8.8%
TESLA-F 52.3% / 28.0% 55.0% / 27.5% 8.8% / 7.6% -
(a) The French-English task. All differences are significant under the Sign Test at p = 0.01, except the
strikeout TESLA-M vs TESLA-F.
A\B BLEU TER TESLA-M TESLA-F
BLEU - 25.8% / 22.3% 31.0% / 50.6% 24.4% / 50.8%
TER 22.3% / 25.8% - 31.9% / 51.0% 26.4% / 52.4%
TESLA-M 50.6% / 31.0% 51.0% / 31.9% - 25.9% / 33.4%
TESLA-F 50.8% / 24.4% 52.4% / 26.4% 33.4% / 25.9% -
(b) The Spanish-English task. All differences are significant under the Sign Test at p = 0.01, except
the strikeout BLEU vs TER.
A\B BLEU TER TESLA-M TESLA-F
BLEU - 21.8% / 18.4% 28.1% / 36.9% 27.3% / 35.3%
TER 18.4% / 21.8% - 26.9% / 39.5% 27.3% / 37.5%
TESLA-M 36.9% / 28.1% 39.5% / 26.9% - 24.3% / 21.3%
TESLA-F 35.3% / 27.3% 37.5% / 27.3% 21.3% / 24.3% -
(c) The German-English task. All differences are significant under the Sign Test at p = 0.01, except
the strikeout BLEU vs TER, and TESLA-M vs TESLA-F.
Table 7: Pairwise system comparisons. Each cell shows the proportion of time the system tuned on A is
preferred over the system tuned on B, and the proportion of time the opposite happens. Notice that the upper
right half of each table is the mirror image of the lower left half.
381
translation systems using the TESLA metrics leads
to significantly better translation output.
5 Discussion
We examined the results manually, and found that
the relationship between the types of mistakes each
system makes and the characteristics of the corre-
sponding metric to be intricate. We discuss our find-
ings in this section.
First we observe that BLEU and TER tend to pro-
duce very similar translations, and so do TESLA-
F and TESLA-M. Of the 2489 test sentences in the
French-English task, BLEU and TER produced dif-
ferent translations for only 760 sentences, or 31%.
Similarly, TESLA-F and TESLA-M gave different
outputs for only 857 sentences, or 34%. In contrast,
BLEU and TESLA-M gave different translations for
2248 sentences, or 90%. It is interesting to find that
BLEU and TER should be so similar, considering
that they are based on very different principles. As a
metric, TESLA-M is certainly much more similar to
BLEU than TER is, yet they behave very differently
when used as a tuning metric.
We also observe that TESLA-F and TESLA-M
tend to produce much longer sentences than do
BLEU and TER. The average sentence lengths of the
TESLA-F- and TESLA-M-tuned systems across all
three language pairs are 26.5 and 26.6 words respec-
tively, whereas those for BLEU and TER are only
22.4 and 21.7 words. Comparing the translations
from the two groups, the tendency of BLEU and
TER to pick shorter paraphrases and to drop func-
tion words is unmistakable, often to the detriment of
the translation quality. Some typical examples from
the French-English task are shown in Figure 4.
Interestingly, the human translations average only
22 words, so BLEU and TER translations are in fact
much closer on average to the reference lengths, yet
their translations often feel too short. In contrast,
manual inspections reveal no tendency for TESLA-F
and TESLA-M to produce overly long translations.
These observations suggest that the brevity
penalty of BLEU is not aggressive enough. Nei-
ther is TER, which penalizes insertions and dele-
tions equally. Interestingly, by placing much more
emphasis on the recall, TESLA-M and TESLA-F
produce translations that are statistically too long,
but feel much more ?correct? lengthwise.
Another major difference between TESLA-
M/TESLA-F and BLEU/TER is that the TESLAs
heavily discount n-grams with function words. One
might thus expect the TESLA-tuned systems to be
less adept at function words; yet they translate them
surprisingly well, as shown in Figure 4. One ex-
planation is of course the sentence length effect we
have discussed. Another reason may be that since
the metric does not care much about function words,
the language model is given more freedom to pick
function words as it sees fit, without the fear of large
penalties. Paradoxically, by reducing the weights
of function words, we end up making better trans-
lations for them.
TER is the only metric that allows cheap block
movements, regardless of size or distance. One
might reasonably speculate that a TER-tuned system
should be more prone to reordering phrases. How-
ever, we find no evidence that this is so.
The relative performance of TESLA-M vs
TESLA-F is unsurprising. TESLA-F, being heav-
ier and slower, produces somewhat better results
than its minimalist counterpart, though the margin
is far less pronounced than that between TESLA-
M and the conventional BLEU and TER. Since ex-
tra resources including bitexts are needed in using
TESLA-F, TESLA-M emerges as the MT evaluation
metric of choice for tuning SMT systems.
6 Future work
We have presented empirical evidence that the
TESLA metrics outperform BLEU for MT tuning
in a hierarchical phrase-based SMT system. At
the same time, some open questions remain unan-
swered. We intend to investigate them in our future
work.
The work of (Cer et al, 2010) investigated the ef-
fect of tuning a phrase-based SMT system and found
that of the MT evaluation metrics that they tried,
none of them could outperform BLEU. We would
like to verify whether TESLA tuning is still pre-
ferred over BLEU tuning in a phrase-based SMT
system.
Based on our observations, it may be possible to
improve the performance of BLEU-based tuning by
(1) increasing the brevity penalty; (2) introducing
382
BLEU in the future , americans want a phone that allow the user to . . .
TER in the future , americans want a phone that allow the user to . . .
TESLA-M in the future , the americans want a cell phone , which allow the user to . . .
TESLA-F in the future , the americans want a phone that allow the user to . . .
BLEU . . . also for interest on debt of the state . . .
TER . . . also for interest on debt of the state . . .
TESLA-M . . . also for the interest on debt of the state . . .
TESLA-F . . . also for the interest on debt of the state . . .
BLEU and it is hardly the end of carnival-like transfers .
TER and it is hardly the end of carnival-like transfers .
TESLA-M and it is far from being the end of the carnival-like transfers .
TESLA-F and it is far from being the end of the carnival-like transfers .
BLEU it is not certain that the state can act without money .
TER it is not certain that the state can act without money .
TESLA-M it is not certain that the state can act without this money .
TESLA-F it is not certain that the state can act without this money .
BLEU but the expense of a debt of the state . . .
TER but the expense of a debt of the state . . .
TESLA-M but at the expense of a greater debt of the state . . .
TESLA-F but at the expense of a great debt of the state . . .
Figure 4: Comparison of selected translations from the French-English task
a recall measure and emphasizing it over precision;
and/or (3) introducing function word discounting. In
the ideal case, such a modified BLEU metric would
deliver results similar to that of TESLA-M, yet with
a runtime cost closer to BLEU. It would also make
porting existing tuning code easier.
7 Conclusion
We demonstrate for the first time that a practical
new generation MT evaluation metric can signifi-
cantly improve the quality of automatic MT com-
pared to BLEU, as measured by human judgment.
We hope this work will encourage the MT research
community to finally move away from BLEU and to
consider tuning their systems with a new generation
metric.
All the data, source code, and results reported in
this work can be downloaded from our website at
http://nlp.comp.nus.edu.sg/software.
Acknowledgments
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. Proceedings of the
ACL 2005 Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR.
Daniel Cer, Christopher D. Manning, and Daniel Juraf-
sky. 2010. The best lexical metric for phrase-based
statistical MT system optimization. In Human Lan-
guage Technologies: The 2010 Annual Conference of
383
the North American Chapter of the Association for
Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2008. MaxSim:
A maximum similarity metric for machine translation
evaluation. In Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2009. MaxSim: per-
formance and effects of translation fluency. Machine
Translation, 23(2):157?168, September.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT press.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proceedings of 47th Annual Meeting
of the Association for Computational Linguistics and
the 4th IJCNLP of the AFNLP.
John W. Hutchins. 2007. Machine translation: A con-
cise history. Computer Aided Translation: Theory and
Practice.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels of
correlation with human judgments. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N.G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit for
parsing-based machine translation. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010.
Tesla: Translation evaluation of sentences with linear-
programming-based analysis. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics.
Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23(2):181?193, August.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Seventh Conference of the Asso-
ciation for Machine Translation in the Americas.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing.
Omar Zaidan. 2009. Z-MERT: A fully configurable open
source tool for minimum error rate training of machine
translation systems. The Prague Bulletin of Mathe-
matical Linguistics, 91:79?88.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006. Re-
evaluating machine translation results with paraphrase
support. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing.
384
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 921?929,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Character-Level Machine Translation Evaluation
for Languages with Ambiguous Word Boundaries
Chang Liu and Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive, Singapore 117417
{liuchan1,nght}@comp.nus.edu.sg
Abstract
In this work, we introduce the TESLA-
CELAB metric (Translation Evaluation of
Sentences with Linear-programming-based
Analysis ? Character-level Evaluation for
Languages with Ambiguous word Bound-
aries) for automatic machine translation eval-
uation. For languages such as Chinese where
words usually have meaningful internal struc-
ture and word boundaries are often fuzzy,
TESLA-CELAB acknowledges the advantage
of character-level evaluation over word-level
evaluation. By reformulating the problem in
the linear programming framework, TESLA-
CELAB addresses several drawbacks of the
character-level metrics, in particular the mod-
eling of synonyms spanning multiple char-
acters. We show empirically that TESLA-
CELAB significantly outperforms character-
level BLEU in the English-Chinese translation
evaluation tasks.
1 Introduction
Since the introduction of BLEU (Papineni et al,
2002), automatic machine translation (MT) eval-
uation has received a lot of research interest.
The Workshop on Statistical Machine Transla-
tion (WMT) hosts regular campaigns comparing
different machine translation evaluation metrics
(Callison-Burch et al, 2009; Callison-Burch et al,
2010; Callison-Burch et al, 2011). In the WMT
shared tasks, many new generation metrics, such as
METEOR (Banerjee and Lavie, 2005), TER (Snover
et al, 2006), and TESLA (Liu et al, 2010) have con-
sistently outperformed BLEU as judged by the cor-
relations with human judgments.
The research on automatic machine translation
evaluation is important for a number of reasons. Au-
tomatic translation evaluation gives machine trans-
lation researchers a cheap and reproducible way to
guide their research and makes it possible to com-
pare machine translation methods across different
studies. In addition, machine translation system
parameters are tuned by maximizing the automatic
scores. Some recent research (Liu et al, 2011) has
shown evidence that replacing BLEU by a newer
metric, TESLA, can improve the human judged
translation quality.
Despite the importance and the research inter-
est on automatic MT evaluation, almost all existing
work has focused on European languages, in partic-
ular on English. Although many methods aim to
be language neutral, languages with very different
characteristics such as Chinese do present additional
challenges. The most obvious challenge for Chinese
is that of word segmentation.
Unlike European languages, written Chinese is
not split into words. Segmenting Chinese sentences
into words is a natural language processing task
in its own right (Zhao and Liu, 2010; Low et al,
2005). However, many different segmentation stan-
dards exist for different purposes, such as Microsoft
Research Asia (MSRA) for Named Entity Recog-
nition (NER), Chinese Treebank (CTB) for parsing
and part-of-speech (POS) tagging, and City Univer-
sity of Hong Kong (CITYU) and Academia Sinica
(AS) for general word segmentation and POS tag-
ging. It is not clear which standard is the best in a
given scenario.
The only prior work attempting to address the
problem of word segmentation in automatic MT
evaluation for Chinese that we are aware of is Li et
921
? ?
buy umbrella
? ??
buy umbrella
? ? ?
buy rain umbrella
Figure 1: Three forms of the same expression buy um-
brella in Chinese
al. (2011). The work compared various MT eval-
uation metrics (BLEU, NIST, METEOR, GTM, 1
? TER) with different segmentation schemes, and
found that treating every single character as a token
(character-level MT evaluation) gives the best corre-
lation with human judgments.
2 Motivation
Li et al (2011) identify two reasons that character-
based metrics outperform word-based metrics. For
illustrative purposes, we use Figure 1 as a running
example in this paper. All three expressions are se-
mantically identical (buy umbrella). The first two
forms are identical because ??1 and ? are syn-
onyms. The last form is simply an (arguably wrong)
alternative segmented form of the second expres-
sion.
1. Word-based metrics do not award partial
matches, e.g., ?_?? and ?_? would be
penalized for the mismatch between ?? and
?. Character-based metrics award the match
between characters? and?.
2. Character-based metrics do not suffer from er-
rors and differences in word segmentation, so
?_?? and ?_?_? would be judged ex-
actly equal.
Li et al (2011) conduct empirical experiments to
show that character-based metrics consistently out-
perform their word-based counterparts. Despite
that, we observe two important problems for the
character-based metrics:
1. Although partial matches are partially awarded,
the mechanism breaks down for n-grams where
1Literally, rain umbrella.
n > 1. For example, between ?_?_? and
?_?, higher-order n-grams such as?_? and
?_? still have no match, and will be penal-
ized accordingly, even though ?_?_? and
?_? should match exactly. N-grams such
as ?_? which cross natural word boundaries
and are meaningless by themselves can be par-
ticularly tricky.
2. Character-level metrics can utilize only a small
part of the Chinese synonym dictionary, such as
? and ? (you). The majority of Chinese syn-
onyms involve more than one character, such
as ?? and? (umbrella), and ?? and ??
(child).
In this work, we attempt to address both of these
issues by introducing TESLA-CELAB, a character-
level metric that also models word-level linguistic
phenomenon. We formulate the n-gram matching
process as a real-valued linear programming prob-
lem, which can be solved efficiently. The metric
is based on the TESLA automatic MT evaluation
framework (Liu et al, 2010; Dahlmeier et al, 2011).
3 The Algorithm
3.1 Basic Matching
We illustrate our matching algorithm using the ex-
amples in Figure 1. Let ??? be the reference,
and?? be the candidate translation.
We use Cilin (?????)2 as our synonym
dictionary. The basic n-gram matching problem is
shown in Figure 2. Two n-grams are connected if
they are identical, or if they are identified as syn-
onyms by Cilin. Notice that all n-grams are put in
the same matching problem regardless of n, unlike
in translation evaluation metrics designed for Eu-
ropean languages. This enables us to designate n-
grams with different values of n as synonyms, such
as?? (n = 2) and? (n = 1).
In this example, we are able to make a total of two
successful matches. The recall is therefore 2/6 and
the precision is 2/3.
2http://ir.hit.edu.cn/phpwebsite/index.php?module=pagemaster
&PAGE_user_op=view_page&PAGE_id=162
922
? ? ? ?? ?? ???
? ? ??
Figure 2: The basic n-gram matching problem
? ? ? ?? ?? ???
? ? ??
Figure 3: The n-gram matching problem after phrase
matching
3.2 Phrase Matching
We note in Figure 2 that the trigram??? and the
bigram ?? are still unmatched, even though the
match between?? and? should imply the match
between??? and??.
We infer the matching of such phrases using a
dynamic programming algorithm. Two n-grams are
considered synonyms if they can be segmented into
synonyms that are aligned. With this extension,
we are able to match ??? and ?? (since ?
matches ? and ?? matches ?). The matching
problem is now depicted by Figure 3.
The linear programming problem is mathemati-
cally described as follows. The variables w(?, ?) are
the weights assigned to the edges,
w(?,?) ? [0, 1]
w(?,?) ? [0, 1]
w(??,?) ? [0, 1]
w(???,??) ? [0, 1]
We require that for any node N , the sum of
weights assigned to edges linking N must not ex-
ceed one.
wref(?) = w(?,?)
wref(?) = w(?,?)
wref(??) = w(??,?)
wref(???) = w(???,??)
?
? ? ??
Figure 4: A covered n-gram matching problem
wcand(?) = w(?,?)
wcand(?) = w(?,?) + w(??,?)
wcand(??) = w(???,??)
where
wref(X) ? [0, 1] ?X
wcand(X) ? [0, 1] ?X
Now we maximize the total match,
w(?,?)+w(?,?)+w(??,?)+w(???,??)
In this example, the best match is 3, resulting in a
recall of 3/6 and a precision of 3/3.
3.3 Covered Matching
In Figure 3, n-grams? and?? in the reference re-
main impossible to match, which implies misguided
penalty for the candidate translation. We observe
that since ??? has been matched, all its sub-n-
grams should be considered matched as well, includ-
ing ? and ??. We call this the covered n-gram
matching rule. This relationship is implicit in the
matching problem for English translation evaluation
metrics where words are well delimited. But with
phrase matching in Chinese, it must be modeled ex-
plicitly.
However, we cannot simply perform covered n-
gram matching as a post processing step. As an ex-
ample, suppose we are matching phrases ?? and
?, as shown in Figure 4. The linear programming
solver may come up with any of the solutions where
w(?,?) + w(??,?) = 1, w(?,?) ? [0, 1],
and w(??,?) ? [0, 1].
To give the maximum coverage for the node ?,
only the solution w(?,?) = 0, w(??,?) = 1 is
accepted. This indicates the need to model covered
923
n-gram matching in the linear programming prob-
lem itself.
We return to the matching of the reference ??
? and the candidate?? in Figure 3. On top of the
w(?) variables already introduced, we add the vari-
ables maximum covering weights c(?). Each c(X)
represents the maximum w(Y ) variable where n-
gram Y completely covers n-gram X .
cref(?) ? max(wref(?), wref(??),
wref(???))
cref(?) ? max(wref(?), wref(??),
wref(??), wref(???))
cref(?) ? max(wref(?), wref(??),
wref(???))
cref(??) ? max(wref(??), wref(???))
cref(??) ? max(wref(??), wref(???))
cref(???) ? wref(???)
ccand(?) ? max(wcand(?), wcand(??))
ccand(?) ? max(wcand(?), wcand(??))
ccand(??) ? wcand(??)
where
cref(X) ? [0, 1] ?X
ccand(X) ? [0, 1] ?X
However, the max(?) operator is not allowed in
the linear programming formulation. We get around
this by approximating max(?) with the sum instead.
Hence,
cref(?) ? wref(?) + wref(??)+
wref(???)
cref(?) ? wref(?) + wref(??)+
wref(??) + wref(???)
. . .
We justify this approximation by the following
observation. Consider the sub-problem consisting
of just the w(?, ?), wref(?), wcand(?) variables and
their associated constraints. This sub-problem can
be seen as a maximum flow problem where all con-
stants are integers, hence there exists an optimal so-
lution where each of the w variables is assigned a
value of either 0 or 1. For such a solution, the
max and the sum forms are equivalent, since the
cref(?) and ccand(?) variables are also constrained to
the range [0, 1].
The maximum flow equivalence breaks down
when the c(?) variables are introduced, so in the gen-
eral case, replacing max with sum is only an approx-
imation.
Returning to our sample problem, the linear pro-
gramming solver simply needs to assign:
w(???,??) = 1
wref(???) = 1
wcand(??) = 1
Consequently, due to the maximum covering
weights constraint, we can give the following value
assignment, implying that all n-grams have been
matched.
cref(X) = 1 ?X
ccand(X) = 1 ?X
3.4 The Objective Function
We now define our objective function in terms of
the c(?) variables. The recall is a function of
?
X cref(X), and the precision is a function of?
Y ccand(Y ), where X is the set of all n-grams of
the reference, and Y is the set of all n-grams of the
candidate translation.
Many prior translation evaluation metrics such as
MAXSIM (Chan and Ng, 2008) and TESLA (Liu
et al, 2010; Dahlmeier et al, 2011) use the F-0.8
measure as the final score:
F0.8 =
Precision? Recall
0.8? Precision + 0.2? Recall
Under some simplifying assumptions ? specifi-
cally, that precision = recall ? basic calculus shows
that F0.8 is four times as sensitive to recall than to
precision. Following the same reasoning, we want
to place more emphasis on recall than on precision.
We are also constrained by the linear programming
framework, hence we set the objective function as
1
Z
(
?
X
cref(X) + f
?
Y
ccand(Y )
)
0 < f < 1
924
We set f = 0.25 so that our objective function
is also four times as sensitive to recall than to pre-
cision.3 The value of this objective function is our
TESLA-CELAB score. Similar to the other TESLA
metrics, when there are N multiple references, we
match the candidate translation against each of them
and use the average of the N objective function val-
ues as the segment level score. System level score is
the average of all the segment level scores.
Z is a normalizing constant to scale the metric to
the range [0, 1], chosen so that when all the c(?) vari-
ables have the value of one, our metric score attains
the value of one.
4 Experiments
In this section, we test the effectiveness of TESLA-
CELAB on some real-world English-Chinese trans-
lation tasks.
4.1 IWSLT 2008 English-Chinese CT
The test set of the IWSLT 2008 (Paul, 2008)
English-Chinese ASR challenge task (CT) consists
of 300 sentences of spoken language text. The av-
erage English source sentence is 5.8 words long and
the average Chinese reference translation is 9.2 char-
acters long. The domain is travel expressions.
The test set was translated by seven MT systems,
and each translation has been manually judged for
adequacy and fluency. Adequacy measures whether
the translation conveys the correct meaning, even if
the translation is not fully fluent, whereas fluency
measures whether a translation is fluent, regardless
of whether the meaning is correct. Due to high
evaluation costs, adequacy and fluency assessments
were limited to the translation outputs of four sys-
tems. In addition, the translation outputs of the MT
systems are also manually ranked according to their
translation quality.
Inter-judge agreement is measured by the Kappa
coefficient, defined as:
Kappa =
P (A)? P (E)
1? P (E)
where P (A) is the percentage of agreement, and
P (E) is the percentage of agreement by pure
3Our empirical experiments suggest that the correlations do
plateau near this value. For simplicity, we choose not to tune f
on the training data.
Judgment Set 2 3
1 0.4406 0.4355
2 - 0.4134
Table 1: Inter-judge Kappa for the NIST 2008 English-
Chinese task
chance. The inter-judge Kappa is 0.41 for fluency,
0.40 for adequacy, and 0.57 for ranking. Kappa val-
ues between 0.4 and 0.6 are considered moderate,
and the numbers are in line with other comparable
experiments.
4.2 NIST 2008 English-Chinese MT Task
The NIST 2008 English-Chinese MT task consists
of 127 documents with 1,830 segments, each with
four reference translations and eleven automatic
MT system translations. The data is available as
LDC2010T01 from the Linguistic Data Consortiuim
(LDC). The domain is newswire texts. The average
English source sentence is 21.5 words long and the
average Chinese reference translation is 43.2 char-
acters long.
Since no manual evaluation is given for the data
set, we recruited twelve bilingual judges to evalu-
ate the first thirty documents for adequacy and flu-
ency (355 segments for a total of 355? 11 = 3, 905
translated segments). The final score of a sentence
is the average of its adequacy and fluency scores.
Each judge works on one quarter of the sentences so
that each translation is judged by three judges. The
judgments are concatenated to form three full sets of
judgments.
We ignore judgments where two sentences are
equal in quality, so that there are only two possible
outcomes (X is better than Y; or Y is better than X),
and P (E) = 1/2. The Kappa values are shown in
Table 1. The values indicate moderate agreement,
and are in line with other comparable experiments.
4.3 Baseline Metrics
4.3.1 BLEU
Although word-level BLEU has often been found
inferior to the new-generation metrics when the
target language is English or other European lan-
guages, prior research has shown that character-level
BLEU is highly competitive when the target lan-
guage is Chinese (Li et al, 2011). Therefore, we
925
Segment Pearson Spearman rank
Metric Type consistency correlation correlation
BLEU character-level 0.7004 0.9130 0.9643
TESLA-M word-level 0.6771 0.9167 0.8929
TESLA-CELAB? character-level 0.7018 0.9229 0.9643
TESLA-CELAB hybrid 0.7281? 0.9490?? 0.9643
Table 2: Correlation with human judgment on the IWSLT 2008 English-Chinese challenge task. * denotes better than
the BLEU baseline at 5% significance level. ** denotes better than the BLEU baseline at 1% significance level.
Segment Pearson Spearman rank
Metric Type consistency correlation correlation
BLEU character-level 0.7091 0.8429 0.7818
TESLA-M word-level 0.6969 0.8301 0.8091
TESLA-CELAB? character-level 0.7158 0.8514 0.8227
TESLA-CELAB hybrid 0.7162 0.8923?? 0.8909??
Table 3: Correlation with human judgment on the NIST 2008 English-Chinese MT task. ** denotes better than the
BLEU baseline at 1% significance level.
use character-level BLEU as our main baseline.
The correlations of character-level BLEU and the
average human judgments are shown in the first row
of Tables 2 and 3 for the IWSLT and the NIST
data set, respectively. Segment-level consistency is
defined as the number of correctly predicted pair-
wise rankings divided by the total number of pair-
wise rankings. Ties are excluded from the calcu-
lation. We also report the Pearson correlation and
the Spearman rank correlation of the system-level
scores. Note that in the IWSLT data set, the Spear-
man rank correlation is highly unstable due to the
small number of participating systems.
4.3.2 TESLA-M
In addition to character-level BLEU, we also
present the correlations for the word-level metric
TESLA. Compared to BLEU, TESLA allows more
sophisticated weighting of n-grams and measures of
word similarity including synonym relations. It has
been shown to give better correlations than BLEU
for many European languages including English
(Callison-Burch et al, 2011). However, its use of
POS tags and synonym dictionaries prevents its use
at the character-level. We use TESLA as a represen-
tative of a competitive word-level metric.
We use the Stanford Chinese word segmenter
(Tseng et al, 2005) and POS tagger (Toutanova et
al., 2003) for preprocessing and Cilin for synonym
definition during matching. TESLA has several vari-
ants, and the simplest and often the most robust,
TESLA-M, is used in this work. The various cor-
relations are reported in the second row of Tables 2
and 3.
The scores show that word-level TESLA-M has
no clear advantage over character-level BLEU, de-
spite its use of linguistic features. We consider this
conclusion to be in line with that of Li et al (2011).
4.4 TESLA-CELAB
In all our experiments here we use TESLA-CELAB
with n-grams for n up to four, since the vast majority
of Chinese words, and therefore synonyms, are at
most four characters long.
The correlations between the TESLA-CELAB
scores and human judgments are shown in the last
row of Tables 2 and 3. We conducted significance
testing using the resampling method of (Koehn,
2004). Entries that outperform the BLEU base-
line at 5% significance level are marked with ?*?,
and those that outperform at the 1% significance
level are marked with ?**?. The results indicate that
TESLA-CELAB significantly outperforms BLEU.
For comparison, we also run TESLA-CELAB
without the use of the Cilin dictionary, reported
in the third row of Tables 2 and 3 and de-
noted as TESLA-CELAB?. This disables TESLA-
926
CELAB?s ability to detect word-level synonyms and
turns TESLA-CELAB into a linear programming
based character-level metric. The performance of
TESLA-CELAB? is comparable to the character-
level BLEU baseline.
Note that
? TESLA-M can process word-level synonyms,
but does not award character-level matches.
? TESLA-CELAB? and character-level BLEU
award character-level matches, but do not con-
sider word-level synonyms.
? TESLA-CELAB can process word-level syn-
onyms and can award character-level matches.
Therefore, the difference between TESLA-M
and TESLA-CELAB highlights the contribution
of character-level matching, and the difference
between TESLA-CELAB? and TESLA-CELAB
highlights the contribution of word-level synonyms.
4.5 Sample Sentences
Some sample sentences taken from the IWSLT test
set are shown in Table 4 (some are simplified from
the original). The Cilin dictionary correctly identi-
fied the following as synonyms:
? = ?? week
?? = ?? daughter
? = ? you
?? = ?? work
The dictionary fails to recognize the following
synonyms:
?? = ? a
?? = ?? here
However, partial awards are still given for the
matching characters? and?.
Based on these synonyms, TESLA-CELAB is
able to award less trivial n-gram matches, such as?
?=???, ???=???, and ???=???,
as these pairs can all be segmented into aligned syn-
onyms. The covered n-gram matching rule is then
able to award tricky n-grams such as??,??,?
?, ?? and ??, which are covered by ???,
???,???,??? and??? respectively.
Note also that the word segmentations shown in
these examples are for clarity only. The TESLA-
CELAB algorithm does not need pre-segmented
Reference: ? ? ?
next week .
Candidate: ? ?? ?
next week .
Reference: ? ? ?? ?? ?
I have a daughter .
Candidate: ? ? ? ?? ?
I have a daughter .
Reference: ? ? ?? ?? ? ?
you at here work qn ?
Candidate: ? ? ?? ?? ? ?
you at here work qn ?
Table 4: Sample sentences from the IWSLT 2008 test set
Schirm kaufen
umbrella buy
Regenschirm kaufen
umbrella buy
Regen schirm kaufen
rain umbrella buy
Figure 5: Three forms of buy umbrella in German
sentences, and essentially finds multi-character syn-
onyms opportunistically.
5 Discussion and Future Work
5.1 Other Languages with Ambiguous Word
Boundaries
Although our experiments here are limited to Chi-
nese, many other languages have similarly ambigu-
ous word boundaries. For example, in German, the
exact counterpart to our example exists, as depicted
in Figure 5.
Regenschirm, literally rain-umbrella, is a syn-
onym of Schirm. The first two forms in Figure 5
appear in natural text, and in standard BLEU, they
would be penalized for the non-matching words
Schirm and Regenschirm. Since compound nouns
such as Regenschirm are very common in German
and generate many out-of-vocabulary words, a com-
mon preprocessing step in German translation (and
translation evaluation to a lesser extent) is to split
compound words, and we end up with the last form
Regen schirm kaufen. This process is analogous to
927
Chinese word segmentation.
We plan to conduct experiments on German and
other Asian languages with the same linguistic phe-
nomenon in future work.
5.2 Fractional Similarity Measures
In the current formulation of TESLA-CELAB, two
n-grams X and Y are either synonyms which com-
pletely match each other, or are completely unre-
lated. In contrast, the linear-programming based
TESLA metric allows fractional similarity measures
between 0 (completely unrelated) and 1 (exact syn-
onyms). We can then award partial scores for related
words, such as those identified as such by WordNet
or those with the same POS tags.
Supporting fractional similarity measures is non-
trivial in the TESLA-CELAB framework. We plan
to address this in future work.
5.3 Fractional Weights for N-grams
The TESLA-M metric allows each n-gram to have
a weight, which is primarily used to discount func-
tion words. TESLA-CELAB can support fractional
weights for n-grams as well by the following exten-
sion. We introduce a function m(X) that assigns a
weight in [0, 1] for each n-gram X. Accordingly, our
objective function is replaced by:
1
Z
(
?
X
m(X)cref(X) + f
?
Y
m(Y )ccand(Y )
)
where Z is a normalizing constant so that the metric
has a range of [0, 1].
Z =
?
X
m(X) + f
?
Y
m(Y )
However, experiments with different weight func-
tions m(?) on the test data set failed to find a better
weight function than the currently implied m(?) =
1. This is probably due to the linguistic character-
istics of Chinese, where human judges apparently
give equal importance to function words and con-
tent words. In contrast, TESLA-M found discount-
ing function words very effective for English and
other European languages such as German. We plan
to investigate this in the context of non-Chinese lan-
guages.
6 Conclusion
In this work, we devise a new MT evaluation met-
ric in the family of TESLA (Translation Evaluation
of Sentences with Linear-programming-based Anal-
ysis), called TESLA-CELAB (Character-level Eval-
uation for Languages with Ambiguous word Bound-
aries), to address the problem of fuzzy word bound-
aries in the Chinese language, although neither the
phenomenon nor the method is unique to Chinese.
Our metric combines the advantages of character-
level and word-level metrics:
1. TESLA-CELAB is able to award scores for
partial word-level matches.
2. TESLA-CELAB does not have a segmentation
step, hence it will not introduce word segmen-
tation errors.
3. TESLA-CELAB is able to take full advantage
of the synonym dictionary, even when the syn-
onyms differ in the number of characters.
We show empirically that TESLA-CELAB
significantly outperforms the strong baseline
of character-level BLEU in two well known
English-Chinese MT evaluation data sets. The
source code of TESLA-CELAB is available from
http://nlp.comp.nus.edu.sg/software/.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation.
928
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM:
A maximum similarity metric for machine translation
evaluation. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies.
Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng. 2011.
TESLA at WMT2011: Translation evaluation and tun-
able metric. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing.
Maoxi Li, Chengqing Zong, and Hwee Tou Ng. 2011.
Automatic evaluation of Chinese translation output:
word-level or character-level? In Proceedings of the
49th Annual Meeting of the Association for Computa-
tional Linguistics: Short Papers.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. TESLA: Translation evaluation of sentences
with linear-programming-based analysis. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A
maximum entropy approach to Chinese word segmen-
tation. In Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics.
Michael Paul. 2008. Overview of the iwslt 2008 eval-
uation campaign. In Proceedings of the International
Workshop on Spoken Language Translation.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Association for Machine Trans-
lation in the Americas.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter for SIGHAN
bakeoff 2005. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing.
Hongmei Zhao and Qun Liu. 2010. The CIPS-SIGHAN
CLP 2010 Chinese word segmentation bakeoff. In
Proceedings of the Joint Conference on Chinese Lan-
guage Processing.
929
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1006?1014,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Combining Coherence Models and Machine Translation Evaluation Metrics
for Summarization Evaluation
Ziheng Lin?, Chang Liu?, Hwee Tou Ng? and Min-Yen Kan?
? SAP Research, SAP Asia Pte Ltd
30 Pasir Panjang Road, Singapore 117440
ziheng.lin@sap.com
? Department of Computer Science, National University of Singapore
13 Computing Drive, Singapore 117417
{liuchan1,nght,kanmy}@comp.nus.edu.sg
Abstract
An ideal summarization system should pro-
duce summaries that have high content cov-
erage and linguistic quality. Many state-of-
the-art summarization systems focus on con-
tent coverage by extracting content-dense sen-
tences from source articles. A current research
focus is to process these sentences so that they
read fluently as a whole. The current AE-
SOP task encourages research on evaluating
summaries on content, readability, and over-
all responsiveness. In this work, we adapt
a machine translation metric to measure con-
tent coverage, apply an enhanced discourse
coherence model to evaluate summary read-
ability, and combine both in a trained regres-
sion model to evaluate overall responsiveness.
The results show significantly improved per-
formance over AESOP 2011 submitted met-
rics.
1 Introduction
Research and development on automatic and man-
ual evaluation of summarization systems have been
mainly focused on content coverage (Lin and Hovy,
2003; Nenkova and Passonneau, 2004; Hovy et al,
2006; Zhou et al, 2006). However, users may still
find it difficult to read such high-content coverage
summaries as they lack fluency. To promote research
on automatic evaluation of summary readability, the
Text Analysis Conference (TAC) (Owczarzak and
Dang, 2011) introduced a new subtask on readability
to its Automatically Evaluating Summaries of Peers
(AESOP) task.
Most of the state-of-the-art summarization sys-
tems (Ng et al, 2011; Zhang et al, 2011; Conroy
et al, 2011) are extraction-based. They extract the
most content-dense sentences from source articles.
If no post-processing is performed to the generated
summaries, the presentation of the extracted sen-
tences may confuse readers. Knott (1996) argued
that when the sentences of a text are randomly or-
dered, the text becomes difficult to understand, as its
discourse structure is disturbed. Lin et al (2011)
validated this argument by using a trained model
to differentiate an original text from a randomly-
ordered permutation of its sentences by looking at
their discourse structures. This prior work leads us
to believe that we can apply such discourse mod-
els to evaluate the readability of extract-based sum-
maries. We will discuss the application of Lin et
al.?s discourse coherence model to evaluate read-
ability of machine generated summaries. We also
introduce two new feature sources to enhance the
model with hierarchical and Explicit/Non-Explicit
information, and demonstrate that they improve the
original model.
There are parallels between evaluations of ma-
chine translation (MT) and summarization with re-
spect to textual content. For instance, the widely
used ROUGE (Lin and Hovy, 2003) metrics are in-
fluenced by BLEU (Papineni et al, 2002): both
look at surface n-gram overlap for content cover-
age. Motivated by this, we will adapt a state-of-the-
art, linear programming-based MT evaluation met-
ric, TESLA (Liu et al, 2010), to evaluate the content
coverage of summaries.
TAC?s overall responsiveness metric evaluates the
1006
quality of a summary with regard to both its con-
tent and readability. Given this, we combine our
two component coherence and content models into
an SVM-trained regression model as our surrogate
to overall responsiveness. Our experiments show
that the coherence model significantly outperforms
all AESOP 2011 submissions on both initial and up-
date tasks, while the adapted MT evaluation metric
and the combined model significantly outperform all
submissions on the initial task. To the best of our
knowledge, this is the first work that applies a dis-
course coherence model to measure the readability
of summaries in the AESOP task.
2 Related Work
Nenkova and Passonneau (2004) proposed a manual
evaluation method that was based on the idea that
there is no single best model summary for a collec-
tion of documents. Human annotators construct a
pyramid to capture important Summarization Con-
tent Units (SCUs) and their weights, which is used
to evaluate machine generated summaries.
Lin and Hovy (2003) introduced an automatic
summarization evaluation metric, called ROUGE,
which was motivated by the MT evaluation met-
ric, BLEU (Papineni et al, 2002). It automati-
cally determines the content quality of a summary
by comparing it to the model summaries and count-
ing the overlapping n-gram units. Two configura-
tions ? ROUGE-2, which counts bigram overlaps,
and ROUGE-SU4, which counts unigram and bi-
gram overlaps in a word window of four ? have been
found to correlate well with human evaluations.
Hovy et al (2006) pointed out that automated
methods such as ROUGE, which match fixed length
n-grams, face two problems of tuning the appropri-
ate fragment lengths and matching them properly.
They introduced an evaluation method that makes
use of small units of content, called Basic Elements
(BEs). Their method automatically segments a text
into BEs, matches similar BEs, and finally scores
them.
Both ROUGE and BE have been implemented
and included in the ROUGE/BE evaluation toolkit1,
which has been used as the default evaluation tool
in the summarization track in the Document Un-
1http://berouge.com/default.aspx
derstanding Conference (DUC) and Text Analysis
Conference (TAC). DUC and TAC also manually
evaluated machine generated summaries by adopt-
ing the Pyramid method. Besides evaluating with
ROUGE/BE and Pyramid, DUC and TAC also asked
human judges to score every candidate summary
with regard to its content, readability, and overall re-
sponsiveness.
DUC and TAC defined linguistic quality to cover
several aspects: grammaticality, non-redundancy,
referential clarity, focus, and structure/coherence.
Recently, Pitler et al (2010) conducted experiments
on various metrics designed to capture these as-
pects. Their experimental results on DUC 2006 and
2007 show that grammaticality can be measured by
a set of syntactic features, while the last three as-
pects are best evaluated by local coherence. Con-
roy and Dang (2008) combined two manual linguis-
tic scores ? grammaticality and focus ? with various
ROUGE/BE metrics, and showed this helps better
predict the responsiveness of the summarizers.
Since 2009, TAC introduced the task of Auto-
matically Evaluating Summaries of Peers (AESOP).
AESOP 2009 and 2010 focused on two summary
qualities: content and overall responsiveness. Sum-
mary content is measured by comparing the output
of an automatic metric with the manual Pyramid
score. Overall responsiveness measures a combi-
nation of content and linguistic quality. In AESOP
2011 (Owczarzak and Dang, 2011), automatic met-
rics are also evaluated for their ability to assess sum-
mary readability, i.e., to measure how linguistically
readable a machine generated summary is. Sub-
mitted metrics that perform consistently well on the
three aspects include Giannakopoulos and Karkalet-
sis (2011), Conroy et al (2011), and de Oliveira
(2011). Giannakopoulos and Karkaletsis (2011) cre-
ated two character-based n-gram graph representa-
tions for both the model and candidate summaries,
and applied graph matching algorithm to assess their
similarity. Conroy et al (2011) extended the model
in (Conroy and Dang, 2008) to include shallow lin-
guistic features such as term overlap, redundancy,
and term and sentence entropy. de Oliveira (2011)
modeled the similarity between the model and can-
didate summaries as a maximum bipartite matching
problem, where the two summaries are represented
as two sets of nodes and precision and recall are cal-
1007
w=1.0 w=0.8 w=0.2 w=0.1
w=1.0 w=0.8 w=0.1
.2
s=0.5 s=1.0s=0.5 s=1.0
(a) The matching problem
w=1.0 w=0.8 w=0.2 w=0.1
w=1.0 w=0.8 w=0.1
.2
w=1.0 w=0.2w=0.s w=0.1
(b) The matching solution
Figure 1: A BNG matching problem. Top and
bottom rows of each figure represent BNG from
the model and candidate summaries, respectively.
Links are similarities. Both n-grams and links are
weighted.
culated from the matched edges. However, none of
the AESOP metrics currently apply deep linguistic
analysis, which includes discourse analysis.
Motivated by the parallels between summariza-
tion and MT evaluation, we will adapt a state-of-
the-art MT evaluation metric to measure summary
content quality. To apply deep linguistic analysis,
we also enhance an existing discourse coherence
model to evaluate summary readability. We focus
on metrics that measure the average quality of ma-
chine summarizers, i.e., metrics that can rank a set
of machine summarizers correctly (human summa-
rizers are not included in the list).
3 TESLA-S: Evaluating Summary
Content
TESLA (Liu et al, 2010) is an MT evaluation
metric which extends BLEU by introducing a lin-
ear programming-based framework for improved
matching. It also makes use of linguistic resources
and considers both precision and recall.
3.1 The Linear Programming Matching
Framework
Figure 1 shows the matching of bags of n-grams
(BNGs) that forms the core of the TESLA metric.
The top row in Figure 1a represents the bag of n-
grams (BNG) from the model summary, and the
bottom row represents the BNG from the candidate
summary. Each n-gram has a weight. The links
between the n-grams represent the similarity score,
which are constrained to be between 0 and 1. Math-
ematically, TESLA takes as input the following:
1. The BNG of the model summary, X , and the
BNG of the candidate summary, Y . The ith en-
try in X is xi and has weight xWi (analogously
for yi and yWi ).
2. A similarity score s(xi, yj) between all n-
grams xi and yj .
The goal of the matching process is to align the
two BNGs so as to maximize the overall similar-
ity. The variables of the problem are the allocated
weights for the edges,
w(xi, yj) ?i, j
TESLA maximizes
?
i,j
s(xi, yj)w(xi, yj)
subject to
w(xi, yj) ? 0 ?i, j
?
j
w(xi, yj) ? x
W
i ?i
?
i
w(xi, yj) ? y
W
j ?j
This real-valued linear programming problem can
be solved efficiently. The overall similarity S is the
value of the objective function. Thus,
Precision =
S
?
j y
W
j
Recall =
S
?
i x
W
i
The final TESLA score is given by the F-measure:
F =
Precision? Recall
?? Precision + (1? ?)? Recall
In this work, we set ? = 0.8, following (Liu et al,
2010). The score places more importance on recall
than precision. When multiple model summaries are
provided, TESLA matches the candidate BNG with
each of the model BNGs. The maximum score is
taken as the combined score.
1008
3.2 TESLA-S: TESLA for Summarization
We adapted TESLA for the nuances of summariza-
tion. Mimicking ROUGE-SU4, we construct one
matching problem between the unigrams and one
between skip bigrams with a window size of four.
The two F scores are averaged to give the final score.
The similarity score s(xi, yj) is 1 if the word sur-
face forms of xi and yj are identical, and 0 other-
wise. TESLA has a more sophisticated similarity
measure that focuses on awarding partial scores for
synonyms and parts of speech (POS) matches. How-
ever, the majority of current state-of-the-art sum-
marization systems are extraction-based systems,
which do not generate new words. Although our
simplistic similarity score may be problematic when
evaluating abstract-based systems, the experimen-
tal results support our choice of the similarity func-
tion. This reflects a major difference between MT
and summarization evaluation: while MT systems
always generate new sentences, most summarization
systems focus on locating existing salient sentences.
Like in TESLA, function words (words in closed
POS categories, such as prepositions and articles)
have their weights reduced by a factor of 0.1, thus
placing more emphasis on the content words. We
found this useful empirically.
3.3 Significance Test
Koehn (2004) introduced a bootstrap resampling
method to compute statistical significance of the dif-
ference between two machine translation systems
with regard to the BLEU score. We adapt this
method to compute the difference between two eval-
uation metrics in summarization:
1. Randomly choose n topics from the n given
topics with replacement.
2. Summarize the topics with the list of machine
summarizers.
3. Evaluate the list of summaries from Step 2 with
the two evaluation metrics under comparison.
4. Determine which metric gives a higher correla-
tion score.
5. Repeat Step 1 ? 4 for 1,000 times.
As we have 44 topics in TAC 2011 summarization
track, n = 44. The percentage of times metric a
gives higher correlation than metric b is said to be
the significance level at which a outperforms b.
Initial Update
P S K P S K
R-2 0.9606 0.8943 0.7450 0.9029 0.8024 0.6323
R-SU4 0.9806 0.8935 0.7371 0.8847 0.8382 0.6654
BE 0.9388 0.9030 0.7456 0.9057 0.8385 0.6843
4 0.9672 0.9017 0.7351 0.8249 0.8035 0.6070
6 0.9678 0.8816 0.7229 0.9107 0.8370 0.6606
8 0.9555 0.8686 0.7024 0.8981 0.8251 0.6606
10 0.9501 0.8973 0.7550 0.7680 0.7149 0.5504
11 0.9617 0.8937 0.7450 0.9037 0.8018 0.6291
12 0.9739 0.8972 0.7466 0.8559 0.8249 0.6402
13 0.9648 0.9033 0.7582 0.8842 0.7961 0.6276
24 0.9509 0.8997 0.7535 0.8115 0.8199 0.6386
TESLA-S 0.9807 0.9173 0.7734 0.9072 0.8457 0.6811
Table 1: Content correlation with human judgment
on summarizer level. Top three scores among AE-
SOP metrics are underlined. The TESLA-S score is
bolded when it outperforms all others. ROUGE-2 is
shortened to R-2 and ROUGE-SU4 to R-SU4.
3.4 Experiments
We test TESLA-S on the AESOP 2011 content eval-
uation task, judging the metric fitness by compar-
ing its correlations with human judgments for con-
tent. The results for the initial and update tasks are
reported in Table 1. We show the three baselines
(ROUGE-2, ROUGE-SU4, and BE) and submitted
metrics with correlations among the top three scores,
which are underlined. This setting remains the same
for the rest of the experiments. We use three cor-
relation measures: Pearson?s r, Spearman?s ?, and
Kendall?s ? , represented by P, S, and K, respectively.
The ROUGE scores are the recall scores, as per con-
vention. On the initial task, TESLA-S outperforms
all metrics on all three correlation measures. On the
update task, TESLA-S ranks second, first, and sec-
ond on Pearson?s r, Spearman?s ?, and Kendall?s ? ,
respectively.
To test how significant the differences are, we per-
form significance testing using Koehn?s resampling
method between TESLA-S and ROUGE-2/ROUGE-
SU4, on which TESLA-S is based. The findings are:
? Initial task: TESLA-S is better than ROUGE-2
at 99% significance level as measured by Pear-
son?s r.
? Update task: TESLA-S is better than ROUGE-
SU4 at 95% significance level as measured by
Pearson?s r.
? All other differences are statistically insignifi-
cant, including all correlations on Spearman?s
1009
? and Kendall?s ? .
The last point can be explained by the fact that
Spearman?s ? and Kendall?s ? are sensitive to only
the system rankings, whereas Pearson?s r is sensitive
to the magnitude of the differences as well, hence
Pearson?s r is in general a more sensitive measure.
4 DICOMER: Evaluating Summary
Readability
Intuitively, a readable text should also be coherent,
and an incoherent text will result in low readabil-
ity. Both readability and coherence indicate how
fluent a text is. We thus hypothesize that a model
that measures how coherent a text is can also mea-
sure its readability. Lin et al (2011) introduced dis-
course role matrix to represent discourse coherence
of a text. W first illustrate their model with an exam-
ple, and then introduce two new feature sources. We
then apply the models and evaluate summary read-
ability.
4.1 Lin et al?s Discourse Coherence Model
First, a free text in Figure 2 is parsed by a dis-
course parser to derive its discourse relations, which
are shown in Figure 3. Lin et al observed that
coherent texts preferentially follow certain relation
patterns. However, simply using such patterns to
measure the coherence of a text can result in fea-
ture sparseness. To solve this problem, they expand
the relation sequence into a discourse role matrix,
as shown in Table 2. The matrix essentially cap-
tures term occurrences in the sentence-to-sentence
relation sequences. This model is motivated by
the entity-based model (Barzilay and Lapata, 2008)
which captures sentence-to-sentence entity transi-
tions. Next, the discourse role transition probabili-
ties of lengths 2 and 3 (e.g., Temp.Arg1?Exp.Arg2
and Comp.Arg1?nil?Temp.Arg1) are calculated
with respect to the matrix. For example, the prob-
ability of Comp.Arg2?Exp.Arg2 is 2/25 = 0.08 in
Table 2.
Lin et al applied their model on the task of dis-
cerning an original text from a permuted ordering of
its sentences. They modeled it as a pairwise rank-
ing model (i.e., original vs. permuted), and trained a
SVM preference ranking model with discourse role
S1 Japan normally depends heavily on the High-
land Valley and Cananea mines as well as the
Bougainville mine in Papua New Guinea.
S2 Recently, Japan has been buying copper elsewhere.
S3.1 But as Highland Valley and Cananea begin operat-
ing,
S3.2 they are expected to resume their roles as Japan?s
suppliers.
S4.1 According to Fred Demler, metals economist for
Drexel Burnham Lambert, New York,
S4.2 ?Highland Valley has already started operating
S4.3 and Cananea is expected to do so soon.?
Figure 2: A text with four sentences. Si.j means the
jth clause in the ith sentence.
S1           S2          S3.1          S3.2          S4.1          S4.2          S4.3 
Implicit Comparison 
Explicit Comparison 
Explicit Temporal 
Implicit Expansion 
Explicit Expansion 
Figure 3: The discourse relations for Figure 2. Ar-
rows are pointing from Arg2 to Arg1.
S#
Terms
copper cananea operat depend . . .
S1 nil Comp.Arg1 nil Comp.Arg1
S2
Comp.Arg2
nil nil nil
Comp.Arg1
S3 nil
Comp.Arg2 Comp.Arg2
nilTemp.Arg1 Temp.Arg1
Exp.Arg1 Exp.Arg1
S4 nil Exp.Arg2
Exp.Arg1
nil
Exp.Arg2
Table 2: Discourse role matrix fragment extracted
from Figure 2 and 3. Rows correspond to sen-
tences, columns to stemmed terms, and cells contain
extracted discourse roles. Temporal, Contingency,
Comparison, and Expansion are shortened to Temp,
Cont, Comp, and Exp, respectively.
transitions as features and their probabilities as val-
ues.
4.2 Two New Feature Sources
We observe that there are two kinds of informa-
tion in Figure 3 that are not captured by Lin et al?s
1010
model. The first one is whether a relation is Ex-
plicit or Non-Explicit (Lin et al (2010) termed Non-
Explicit to include Implicit, AltLex, EntRel, and
NoRel). Explicit relation and Non-Explicit relation
have different distributions on each discourse rela-
tion (PDTB-Group, 2007). Thus, adding this in-
formation may further improve the model. In ad-
dition to the set of the discourse roles of ?Rela-
tion type . Argument tag?, we introduce another
set of ?Explicit/Non-Explicit . Relation type . Ar-
gument tag?. The cell Ccananea,S3 now contains
Comp.Arg2, Temp.Arg1, Exp.Arg1, E.Comp.Arg2,
E.Temp.Arg1, and N.Exp.Arg1 (E for Explicit and
N for Non-Explicit).
The other information that is not in the discourse
role matrix is the discourse hierarchy structure,
i.e., whether one relation is embedded within
another relation. In Figure 3, S3.1 is Arg1 of
Explicit Temporal, which is Arg2 of the higher
relation Explicit Comparison as well as Arg1 of
another higher relation Implicit Expansion. These
dependencies are important for us to know how
well-structured a summary is. It is represented
by the multiple discourse roles in each cell of the
matrix. For example, the multiple discourse roles in
the cell Ccananea,S3 capture the three dependencies
just mentioned. We introduce intra-cell bigrams
as a new set of features to the original model: for
a cell with multiple discourse roles, we sort them
by their surface strings and multiply to obtain
the bigrams. For instance, Ccananea,S3 will pro-
duce bigrams such as Comp.Arg2?Exp.Arg1
and Comp.Arg2?Temp.Arg1. When both
the Explicit/Non-Explicit feature source and
the intra-cell feature source are joined to-
gether, it also produces bigram features such
as E.Comp.Arg2?Temp.Arg1.
4.3 Predicting Readability Scores
Lin et al (2011) used the SVMlight (Joachims,
1999) package with the preference ranking config-
uration. To train the model, each source text and
one of its permutations form a training pair, where
the source text is given a rank of 1 and the permuta-
tion is given 0. In testing, the trained model predicts
a real number score for each instance, and the in-
stance with the higher score in a pair is said to be
the source text.
In the TAC summarization track, human judges
scored each model and candidate summary with a
readability score from 1 to 5 (5 means most read-
able). Thus in our setting, instead of a pair of texts,
the training input consists of a list of model and can-
didate summaries from each topic, with their anno-
tated scores as the rankings. Given an unseen test
summary, the trained model predicts a real number
score. This score essentially is the readability rank-
ing of the test summary. Such ranking can be eval-
uated by the ranking-based correlations of Spear-
man?s ? and Kendall?s ? . As Pearson?s r measures
linear correlation and we do not know whether the
real number score follows a linear function, we take
the logarithm of this score as the readability score
for this instance.
We use the data from AESOP 2009 and 2010 as
the training data, and test our metrics on AESOP
2011 data. To obtain the discourse relations of a
summary, we use the discourse parser2 developed in
Lin et al (2010).
4.4 Experiments
Table 3 shows the resulting readability correlations.
The last four rows show the correlation scores for
our coherence model: LIN is the default model
by (Lin et al, 2011), LIN+C is LIN with the
intra-cell feature class, LIN+E is enhanced with
the Explicit/Non-Explicit feature class. We name
the LIN model with both new feature sources (i.e.,
LIN+C+E) DICOMER ? a DIscourse COherence
Model for Evaluating Readability.
LIN outperforms all metrics on all correlations on
both tasks. On the initial task, it outperforms the
best scores by 3.62%, 16.20%, and 12.95% on Pear-
son, Spearman, and Kendall, respectively. Similar
gaps (4.27%, 18.52%, and 13.96%) are observed
on the update task. The results are much better
on Spearman and Kendall. This is because LIN is
trained with a ranking model, and both Spearman
and Kendall are ranking-based correlations.
Adding either intra-cell or Explicit/Non-Explicit
features improves all correlation scores, with
Explicit/Non-Explicit giving more pronounced im-
provements. When both new feature sources are in-
2http://wing.comp.nus.edu.sg/?linzihen/
parser/
1011
Initial Update
P S K P S K
R-2 0.7524 0.3975 0.2925 0.6580 0.3732 0.2635
R-SU4 0.7840 0.3953 0.2925 0.6716 0.3627 0.2540
BE 0.7171 0.4091 0.2911 0.5455 0.2445 0.1622
4 0.8194 0.4937 0.3658 0.7423 0.4819 0.3612
6 0.7840 0.4070 0.3036 0.6830 0.4263 0.3141
12 0.7944 0.4973 0.3589 0.6443 0.3991 0.3062
18 0.7914 0.4746 0.3510 0.6698 0.3941 0.2856
23 0.7677 0.4341 0.3162 0.7054 0.4223 0.3014
LIN 0.8556 0.6593 0.4953 0.7850 0.6671 0.5008
LIN+C 0.8612 0.6703 0.4984 0.7879 0.6828 0.5135
LIN+E 0.8619 0.6855 0.5079 0.7928 0.6990 0.5309
DICOMER 0.8666 0.7122 0.5348 0.8100 0.7145 0.5435
Table 3: Readability correlation with human judg-
ment on summarizer level. Top three scores among
AESOP metrics are underlined. Our score is bolded
when it outperforms all AESOP metrics.
Initial Update
vs. P S K P S K
LIN
4
? ?? ?? ?? ?? ??
LIN+C ?? ?? ?? ?? ?? ??
LIN+E ?? ?? ?? ? ?? ??
DICOMER ?? ?? ?? ?? ?? ??
DICOMER LIN ? ? ? ? ? ?
Table 4: Koehn?s significance test for readability.
??, ?, and ? indicate significance level >=99%,
>=95%, and <95%, respectively.
corporated into the metric, we obtain the best results
for all correlation scores: DICOMER outperforms
LIN by 1.10%, 5.29%, and 3.95% on the initial task,
and 2.50%, 4.74%, and 4.27% on the update task.
Table 3 shows that summarization evaluation
Metric 4 tops all other AESOP metrics, except in
the case of Spearman?s ? on the initial task. We
compare our four models to this metric. The results
of Koehn?s significance test are reported in Table 4,
which demonstrates that all four models outperform
Metric 4 significantly. In the last row, we see that
when comparing DICOMER to LIN, DICOMER is
significantly better on three correlation measures.
5 CREMER: Evaluating Overall
Responsiveness
With TESLA-S measuring content coverage and DI-
COMER measuring readability, it is feasible to com-
bine them to predict the overall responsiveness of a
summary. There exist many ways to combine two
variables mathematically: we can combine them in
a linear function or polynomial function, or in a way
Initial Update
P S K P S K
R-2 0.9416 0.7897 0.6096 0.9169 0.8401 0.6778
R-SU4 0.9545 0.7902 0.6017 0.9123 0.8758 0.7065
BE 0.9155 0.7683 0.5673 0.8755 0.7964 0.6254
4 0.9498 0.8372 0.6662 0.8706 0.8674 0.7033
6 0.9512 0.7955 0.6112 0.9271 0.8769 0.7160
11 0.9427 0.7873 0.6064 0.9194 0.8432 0.6794
12 0.9469 0.8450 0.6746 0.8728 0.8611 0.6858
18 0.9480 0.8447 0.6715 0.8912 0.8377 0.6683
23 0.9317 0.7952 0.6080 0.9192 0.8664 0.6953
25 0.9512 0.7899 0.6033 0.9033 0.8139 0.6349
CREMERLF 0.9381 0.8346 0.6635 0.8280 0.6860 0.5173
CREMERPF 0.9621 0.8567 0.6921 0.8852 0.7863 0.6159
CREMERRBF 0.9716 0.8836 0.7206 0.9018 0.8285 0.6588
Table 5: Responsiveness correlation with human
judgment on summarizer level. Top three scores
among AESOP metrics are underlined. CREMER
score is bolded when it outperforms all AESOP met-
rics.
similar to how precision and recall are combined
in F measure. We applied a machine learning ap-
proach to train a regression model for measuring
responsiveness. The scores predicted by TESLA-
S and DICOMER are used as two features. We
use SVMlight with the regression configuration, test-
ing three kernels: linear function, polynomial func-
tion, and radial basis function. We called this model
CREMER ? a Combined REgression Model for
Evaluating Responsiveness.
We train the regression model on AESOP 2009
and 2010 data sets, and test it on AESOP 2011. The
DICOMER model that is trained in Section 4 is used
to predict the readability scores on all AESOP 2009,
2010, and 2011 summaries. We apply TESLA-S to
predict content scores on all AESOP 2009, 2010,
and 2011 summaries.
5.1 Experiments
The last three rows in Table 5 show the correlation
scores of our regression model trained with SVM
linear function (LF), polynomial function (PF), and
radial basis function (RBF). PF performs better than
LF, suggesting that content and readability scores
should not be linearly combined. RBF gives bet-
ter performances than both LF and PF, suggesting
that RBF better models the way humans combine
content and readability. On the initial task, the
model trained with RBF outperforms all submitted
metrics. It outperforms the best correlation scores
1012
by 1.71%, 3.86%, and 4.60% on Pearson, Spear-
man, and Kendall, respectively. All three regression
models do not perform as well on the update task.
Koehn?s significance test shows that when trained
with RBF, CREMER outperforms ROUGE-2 and
ROUGE-SU4 on the initial task at a significance
level of 99% for all three correlation measures.
6 Discussion
The intuition behind the combined regression model
is that combining the readability and content scores
will give an overall good responsiveness score. The
function to combine them and their weights can be
obtained by training. While the results showed that
SVM radial basis kernel gave the best performances,
this function may not truly mimic how human evalu-
ates responsiveness. Human judges were told to rate
summaries by their overall qualities. They may take
into account other aspects besides content and read-
ability. Given CREMER did not perform well on the
update task, we hypothesize that human judgment
of update summaries may involve more complicated
rankings or factor in additional input that CREMER
currently does not model. We plan to devise a bet-
ter responsiveness metric in our future work, beyond
using a simple combination.
Figure 4 shows a complete picture of Pearson?s r
for all AESOP 2011 metrics and our three met-
rics on both initial and update tasks. We highlight
our metrics with a circle on these curves. On the
initial task, correlation scores for content are con-
sistently higher than those for responsiveness with
small gaps, whereas on the update task, they are al-
most overlapping. On the other hand, correlation
scores for readability are much lower than those for
content and responsiveness, with a gap of about 0.2.
Comparing Figure 4a and 4b, evaluation metrics al-
ways correlate better on the initial task than on the
update task. This suggests that there is much room
for improvement for readability metrics, and metrics
need to consider update information when evaluat-
ing update summarizers.
7 Conclusion
We proposed TESLA-S by adapting an MT eval-
uation metric to measure summary content cover-
age, and introduced DICOMER by applying a dis-
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
P
e
a
r
s
o
n
?
s
 
r
ContentResponsivenessReadability
(a) Evaluation metric values on the initial task.
 0.3 0.4
 0.5 0.6
 0.7 0.8
 0.9 1
P
e
a
r
s
o
n
?
s
 
r
ContentResponsivenessReadability
(b) Evaluation metric values on the update
task.
Figure 4: Pearson?s r for all AESOP 2011 submitted
metrics and our proposed metrics. Our metrics are
circled. Higher r value is better.
course coherence model with newly introduced fea-
tures to evaluate summary readability. We com-
bined these two metrics in the CREMER metric
? an SVM-trained regression model ? for auto-
matic summarization overall responsiveness evalu-
ation. Experimental results on AESOP 2011 show
that DICOMER significantly outperforms all sub-
mitted metrics on both initial and update tasks with
large gaps, while TESLA-S and CREMER signifi-
cantly outperform all metrics on the initial task. 3
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative and
administered by the IDM Programme Office.
3Our metrics are publicly available at http://wing.
comp.nus.edu.sg/?linzihen/summeval/.
1013
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34:1?34, March.
John M. Conroy and Hoa Trang Dang. 2008. Mind
the gap: Dangers of divorcing evaluations of summary
content from linguistic quality. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (Coling 2008), Manchester, UK, August.
John M. Conroy, Judith D. Schlesinger, Jeff Kubina,
Peter A. Rankel, and Dianne P. O?Leary. 2011.
CLASSY 2011 at TAC: Guided and multi-lingual sum-
maries and evaluation metrics. In Proceedings of the
Text Analysis Conference 2011 (TAC 2011), Gaithers-
burg, Maryland, USA, November.
Paulo C. F. de Oliveira. 2011. CatolicaSC at TAC 2011.
In Proceedings of the Text Analysis Conference (TAC
2011), Gaithersburg, Maryland, USA, November.
George Giannakopoulos and Vangelis Karkaletsis. 2011.
AutoSummENG and MeMoG in evaluating guided
summaries. In Proceedings of the Text Analysis Con-
ference (TAC 2011), Gaithersburg, Maryland, USA,
November.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi
Fukumoto. 2006. Automated summarization evalua-
tion with basic elements. In Proceedings of the Fifth
Conference on Language Resources and Evaluation
(LREC 2006).
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Bernhard
Schlkopf, Christopher J. C. Burges, and Alexander J.
Smola, editors, Advances in Kernel Methods ? Support
Vector Learning. MIT Press, Cambridge, MA, USA.
Alistair Knott. 1996. A Data-Driven Methodology for
Motivating a Set of Coherence Relations. Ph.D. the-
sis, Department of Artificial Intelligence, University
of Edinburgh.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2004).
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy (NAACL 2003), Morristown, NJ, USA.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A
PDTB-styled end-to-end discourse parser. Technical
Report TRB8/10, School of Computing, National Uni-
versity of Singapore, August.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Au-
tomatically evaluating text coherence using discourse
relations. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies (ACL-HLT 2011), Port-
land, Oregon, USA, June.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. TESLA: Translation evaluation of sentences
with linear-programming-based analysis. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, Uppsala, Sweden. As-
sociation for Computational Linguistics.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proceedings of the 2004 Human Language
Technology Conference / North American Chapter of
the Association for Computational Linguistics Annual
Meeting (HLT-NAACL 2004), Boston, Massachusetts,
USA, May.
Jun Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen
Kan, and Chew Lim Tan. 2011. SWING: Exploit-
ing category-specific information for guided summa-
rization. In Proceedings of the Text Analysis Confer-
ence 2011 (TAC 2011), Gaithersburg, Maryland, USA,
November.
Karolina Owczarzak and Hoa Trang Dang. 2011.
Overview of the TAC 2011 summarization track:
Guided task and AESOP task. In Proceedings of the
Text Analysis Conference (TAC 2011), Gaithersburg,
Maryland, USA, November.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2002), Stroudsburg, PA, USA.
PDTB-Group, 2007. The Penn Discourse Treebank 2.0
Annotation Manual. The PDTB Research Group.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL 2010), Stroudsburg, PA, USA.
Renxian Zhang, You Ouyang, and Wenjie Li. 2011.
Guided summarization with aspect recognition. In
Proceedings of the Text Analysis Conference 2011
(TAC 2011), Gaithersburg, Maryland, USA, Novem-
ber.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL 2006), Strouds-
burg, PA, USA.
1014
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 354?359,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
TESLA: Translation Evaluation of Sentences with
Linear-programming-based Analysis
Chang Liu1 and Daniel Dahlmeier2 and Hwee Tou Ng1,2
1Department of Computer Science, National University of Singapore
2NUS Graduate School for Integrative Sciences and Engineering
{liuchan1,danielhe,nght}@comp.nus.edu.sg
Abstract
We present TESLA-M and TESLA, two
novel automatic machine translation eval-
uation metrics with state-of-the-art perfor-
mances. TESLA-M builds on the suc-
cess of METEOR and MaxSim, but em-
ploys a more expressive linear program-
ming framework. TESLA further exploits
parallel texts to build a shallow seman-
tic representation. We evaluate both on
the WMT 2009 shared evaluation task and
show that they outperform all participating
systems in most tasks.
1 Introduction
In recent years, many machine translation (MT)
evaluation metrics have been proposed, exploiting
varying amounts of linguistic resources.
Heavyweight linguistic approaches including
RTE (Pado et al, 2009) and ULC (Gim?nez and
M?rquez, 2008) performed the best in the WMT
2009 shared evaluation task. They exploit an ex-
tensive array of linguistic features such as parsing,
semantic role labeling, textual entailment, and dis-
course representation, which may also limit their
practical applications.
Lightweight linguistic approaches such as ME-
TEOR (Banerjee and Lavie, 2005), MaxSim
(Chan and Ng, 2008), wpF and wpBleu (Popovic?
and Ney, 2009) exploit a limited range of linguis-
tic information that is relatively cheap to acquire
and to compute, including lemmatization, part-of-
speech (POS) tagging, and synonym dictionaries.
Non-linguistic approaches include BLEU (Pap-
ineni et al, 2002) and its variants, TER (Snover et
al., 2006), among others. They operate purely at
the surface word level and no linguistic resources
are required. Although still very popular with MT
researchers, they have generally shown inferior
performances than the linguistic approaches.
We believe that the lightweight linguistic ap-
proaches are a good compromise given the current
state of computational linguistics research and re-
sources. In this paper, we devise TESLA-M and
TESLA, two lightweight approaches to MT eval-
uation. Specifically: (1) the core features are F-
measures derived by matching bags of N-grams;
(2) both recall and precision are considered, with
more emphasis on recall; and (3) WordNet syn-
onyms feature prominently.
The main novelty of TESLA-M compared to
METEOR and MaxSim is that we match the N-
grams under a very expressive linear programming
framework, which allows us to assign weights to
the N-grams. This is in contrast to the greedy ap-
proach ofMETEOR, and the more restrictive max-
imum bipartite matching formulation of MaxSim.
In addition, we present a heavier version
TESLA, which combines the features using a lin-
ear model trained on development data, making
it easy to exploit features not on the same scale,
and leaving open the possibility of domain adapta-
tion. It also exploits parallel texts of the target lan-
guage with other languages as a shallow semantic
representation, which allows us to model phrase
synonyms and idioms. In contrast, METEOR and
MaxSim are capable of processing only word syn-
onyms from WordNet.
The rest of this paper is organized as follows.
Section 2 gives a high level overview of the eval-
uation task. Sections 3 and 4 describe TESLA-M
and TESLA, respectively. Section 5 presents ex-
perimental results in the setting of the WMT 2009
shared evaluation task. Finally, Section 6 con-
cludes the paper.
2 Overview
We consider the task of evaluating machine trans-
lation systems in the direction of translating the
source language to the target language. Given a
reference translation and a system translation, the
354
goal of an automatic machine translation evalua-
tion algorithm such as TESLA(-M) is to output a
score predicting the quality of the system transla-
tion. Neither TESLA-M nor TESLA requires the
source text, but as additional linguistic resources,
TESLAmakes use of phrase tables generated from
parallel texts of the target language and other lan-
guages, which we refer to as pivot languages. The
source language may or may not be one of the
pivot languages.
3 TESLA-M
This section describes TESLA-M, the lighter
one among the two metrics. At the highest
level, TESLA-M is the arithmetic average of F-
measures between bags of N-grams (BNGs). A
BNG is a multiset of weighted N-grams. Math-
ematically, a BNG B consists of tuples (bi, bWi ),
where each bi is an N-gram and bWi is a posi-
tive real number representing its weight. In the
simplest case, a BNG contains every N-gram in a
translated sentence, and the weights are just the
counts of the respective N-grams. However, to
emphasize the content words over the function
words, we discount the weight of an N-gram by
a factor of 0.1 for every function word in the N-
gram. We decide whether a word is a function
word based on its POS tag.
In TESLA-M, the BNGs are extracted in the tar-
get language, so we call them bags of target lan-
guage N-grams (BTNGs).
3.1 Similarity functions
To match two BNGs, we first need a similarity
measure between N-grams. In this section, we
define the similarity measures used in our exper-
iments.
We adopt the similarity measure from MaxSim
as sms. For unigrams x and y,
? If lemma(x) = lemma(y), then sms = 1.
? Otherwise, let
a = I(synsets(x) overlap with synsets(y))
b = I(POS(x) = POS(y))
where I(?) is the indicator function, then
sms = (a + b)/2.
The synsets are obtained by querying WordNet
(Fellbaum, 1998). For languages other than En-
glish, a synonym dictionary is used instead.
We define two other similarity functions be-
tween unigrams:
slem(x, y) = I(lemma(x) = lemma(y))
spos(x, y) = I(POS(x) = POS(y))
All the three unigram similarity functions general-
ize to N-grams in the same way. For two N-grams
x = x1,2,...,n and y = y1,2,...,n,
s(x, y) =
{
0 if ?i, s(xi, yi) = 0
1
n
?n
i=1 s(x
i, yi) otherwise
3.2 Matching two BNGs
Now we describe the procedure of matching two
BNGs. We take as input the following:
1. Two BNGs, X and Y . The ith entry in X
is xi and has weight xWi (analogously for yj
and yWj ).
2. A similarity measure, s, that gives a similar-
ity score between any two entries in the range
of 0 to 1.
Intuitively, we wish to align the entries of the two
BNGs in a way that maximizes the overall simi-
larity. As translations often contain one-to-many
or many-to-many alignments, we allow one entry
to split its weight among multiple alignments. An
example matching problem is shown in Figure 1a,
where the weight of each node is shown, along
with the similarity for each edge. Edges with a
similarity of zero are not shown. The solution to
the matching problem is shown in Figure 1b, and
the overall similarity is 0.5 ? 1.0 + 0.5 ? 0.6 +
1.0 ? 0.2 + 1.0 ? 0.1 = 1.1.
Mathematically, we formulate this as a (real-
valued) linear programming problem1. The vari-
ables are the allocated weights for the edges
w(xi, yj) ?i, j
We maximize
?
i,j
s(xi, yj)w(xi, yj)
subject to
w(xi, yj) ? 0 ?i, j
?
j
w(xi, yj) ? x
W
i ?i
?
i
w(xi, yj) ? y
W
j ?j
1While integer linear programming is NP-complete, real-
valued linear programming can be solved efficiently.
355
w=1.0 w=0.8 w=0.2 w=0.1
w=1.0 w=0.8 w=0.1
.2
s=0.5 s=1.0s=0.5 s=1.0
(a) The matching problem
w=1.0 w=0.8 w=0.2 w=0.1
w=1.0 w=0.8 w=0.1
.2
w=1.0 w=0.2w=0.s w=0.1
(b) The solution
Figure 1: A BNG matching problem
The value of the objective function is the overall
similarity S. Assuming X is the reference and Y
is the system translation, we have
Precision =
S
?
j y
W
j
Recall =
S
?
i x
W
i
The F-measure is derived from the precision and
the recall:
F =
Precision ? Recall
?? Precision + (1 ? ?) ? Recall
In this work, we set ? = 0.8, following MaxSim.
The value gives more importance to the recall than
the precision.
3.3 Scoring
The TESLA-M sentence-level score for a refer-
ence and a system translation is the arithmetic av-
erage of the BTNG F-measures for unigrams, bi-
grams, and trigrams based on similarity functions
sms and spos. We thus have 3? 2 = 6 features for
TESLA-M.
We can compute a system-level score for a ma-
chine translation system by averaging its sentence-
level scores over the complete test set.
3.4 Reduction
When every xWi and y
W
j is 1, the linear program-
ming problem proposed above reduces toweighted
bipartite matching. This is a well known result;
see for example, Cormen et al (2001) for details.
This is the formalism of MaxSim, which precludes
the use of fractional weights.
If the similarity function is binary-valued
and transitive, such as slem and spos, then
we can use a much simpler and faster greedy
matching procedure: the best match is simply
?
g min(
?
xi=g
xWi ,
?
yi=g
yWi ).
4 TESLA
Unlike the simple arithmetic average used in
TESLA-M, TESLA uses a general linear com-
bination of three types of features: BTNG F-
measures as in TESLA-M, F-measures between
bags of N-grams in each of the pivot languages,
called bags of pivot language N-grams (BPNGs),
and normalized language model scores of the sys-
tem translation, defined as 1n logP , where n is
the length of the translation, and P the language
model probability. The method of training the lin-
ear model depends on the development data. In
the case of WMT, the development data is in the
form of manual rankings, so we train SVM rank
(Joachims, 2006) on these instances to build the
linear model. In other scenarios, some form of re-
gression can be more appropriate.
The rest of this section focuses on the genera-
tion of the BPNGs. Their matching is done in the
same way as described for BTNGs in the previous
section.
4.1 Phrase level semantic representation
Given a sentence-aligned bitext between the target
language and a pivot language, we can align the
text at the word level using well known tools such
as GIZA++ (Och and Ney, 2003) or the Berkeley
aligner (Liang et al, 2006; Haghighi et al, 2009).
We observe that the distribution of aligned
phrases in a pivot language can serve as a se-
mantic representation of a target language phrase.
That is, if two target language phrases are often
aligned to the same pivot language phrase, then
they can be inferred to be similar in meaning.
Similar observations have been made by previous
researchers (Bannard and Callison-Burch, 2005;
Callison-Burch et al, 2006; Snover et al, 2009).
We note here two differences from WordNet
synonyms: (1) the relationship is not restricted to
the word level only, and (2) the relationship is not
binary. The degree of similarity can be measured
by the percentage of overlap between the seman-
tic representations. For example, at the word level,
356
the phrases good morning and hello are unrelated
even with a synonym dictionary, but they both very
often align to the same French phrase bonjour, and
we conclude they are semantically related to a high
degree.
4.2 Segmenting a sentence into phrases
To extend the concept of this semantic represen-
tation of phrases to sentences, we segment a sen-
tence in the target language into phrases. Given a
phrase table, we can approximate the probability
of a phrase p by:
Pr(p) =
N(p)
?
p? N(p
?)
(1)
where N(?) is the count of a phrase in the phrase
table. We then define the likelihood of seg-
menting a sentence S into a sequence of phrases
(p1, p2, . . . , pn) by:
Pr(p1, p2, . . . , pn|S) =
1
Z(S)
n?
i=1
Pr(pi) (2)
where Z(S) is a normalizing constant. The seg-
mentation of S that maximizes the probability can
be determined efficiently using a dynamic pro-
gramming algorithm. The formula has a strong
preference for longer phrases, as every Pr(p) is
a small fraction. To deal with out-of-vocabulary
(OOV) words, we allow any single word w to be
considered a phrase, and if N(w) = 0, we set
N(w) = 0.5 instead.
4.3 BPNGs as sentence level semantic
representation
Simply merging the phrase-level semantic rep-
resentation is insufficient to produce a sensible
sentence-level semantic representation. As an ex-
ample, we consider two target language (English)
sentences segmented as follows:
1. ||| Hello , ||| Querrien ||| . |||
2. ||| Morning , sir . |||
A naive comparison of the bags of aligned pivot
language (French) phrases would likely conclude
that the two sentences are completely unrelated,
as the bags of aligned phrases are likely to be
completely disjoint. We tackle this problem by
constructing a confusion network representation
of the aligned phrases, as shown in Figures 2 and
w=1.=082s252

02s252

0881252
252

Figure 2: A confusion network as a semantic rep-
resentation
w=1.=082s25=1
08222
Figure 3: A degenerate confusion network as a se-
mantic representation
3. A confusion network is a compact representa-
tion of a potentially exponentially large number of
weighted and likely malformed French sentences.
We can collect the N-gram statistics of this ensem-
ble of French sentences efficiently from the confu-
sion network representation. For example, the tri-
gram Bonjour , Querrien 2 would receive a weight
of 0.9 ? 1.0 = 0.9 in Figure 2. As with BTNGs,
we discount the weight of an N-gram by a factor
of 0.1 for every function word in the N-gram, so
as to place more emphasis on the content words.
The collection of all such N-grams and their
corresponding weights forms the BPNG of a sen-
tence. The reference and system BPNGs are then
matched using the algorithm outlined in Section
3.2.
4.4 Scoring
The TESLA sentence-level score is a linear com-
bination of (1) BTNG F-measures for unigrams,
bigrams, and trigrams based on similarity func-
tions sms and spos, (2) BPNG F-measures for un-
igrams, bigrams, and trigrams based on similar-
ity functions slem and spos for each pivot lan-
guage, and (3) normalized language model scores.
In this work, we use two language models. We
thus have 3 ? 2 features from the BTNGs, 3 ?
2 ? #pivot languages features from the BPNGs, and
2 features from the language models. Again, we
can compute system-level scores by averaging the
sentence-level scores.
5 Experiments
5.1 Setup
We test our metrics in the setting of the WMT
2009 evaluation task (Callison-Burch et al, 2009).
The manual judgments from WMT 2008 are used
2Note that the N-gram can span more than one segment.
357
as the development data and the metric is evalu-
ated on WMT 2009 manual judgments with re-
spect to two criteria: sentence level consistency
and system level correlation.
The sentence level consistency is defined as the
percentage of correctly predicted pairs among all
the manually judged pairs. Pairs judged as ties
by humans are excluded from the evaluation. The
system level correlation is defined as the average
Spearman?s rank correlation coefficient across all
translation tracks.
5.2 Pre-processing
We POS tag and lemmatize the texts using the fol-
lowing tools: for English, OpenNLP POS-tagger3
and WordNet lemmatizer; for French and German,
TreeTagger4; for Spanish, the FreeLing toolkit
(Atserias et al, 2006); and for Czech, the Morce
morphological tagger5.
For German, we additionally perform noun
compound splitting. For each noun, we choose the
split that maximizes the geometric mean of the fre-
quency counts of its parts, following the method in
(Koehn and Knight, 2003):
max
n,p1,p2,...,pn
[
n?
i=1
N(pi)
] 1
n
The resulting compound split sentence is then POS
tagged and lemmatized.
Finally, we remove all non-alphanumeric tokens
from the text in all languages. To generate the lan-
guage model features, we train SRILM (Stolcke,
2002) trigram models with modified Kneser-Ney
discounting on the supplied monolingual Europarl
and news commentary texts.
We build phrase tables from the supplied news
commentary bitexts. Word alignments are pro-
duced by the Berkeley aligner. The widely used
phrase extraction heuristic in (Koehn et al, 2003)
is used to extract phrase pairs and phrases of up to
4 words are collected.
5.3 Into-English task
For each of the BNG features, we generate three
scores, for unigrams, bigrams, and trigrams re-
spectively. For BPNGs, we generate one such
triple for each of the four pivot languages supplied,
namely Czech, French, German, and Spanish.
3opennlp.sourceforge.net
4www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
5ufal.mff.cuni.cz/morce/index.php
System
correlation
Sentence
consistency
TESLA 0.8993 0.6324
TESLA-M 0.8718 0.6097
ulc 0.83 0.63
maxsim 0.80 0.62
meteor-0.6 0.72 0.50
Table 1: Into-English task on WMT 2009 data
Table 1 compares the scores of TESLA and
TESLA-M against three participants in WMT
2009 under identical settings6: ULC (a heavy-
weight linguistic approach with the best per-
formance in WMT 2009), MaxSim, and ME-
TEOR. The results show that TESLA outperforms
all these systems by a substantial margin, and
TESLA-M is very competitive too.
5.4 Out-of-English task
A synonym dictionary is required for target lan-
guages other than English. We use the freely avail-
able Wiktionary dictionary7 for each language.
For Spanish, we additionally use the Spanish
WordNet, a component of FreeLing.
Only one pivot language (English) is used for
the BPNG. For the English-Czech task, we only
have one language model instead of two, as the
Europarl language model is not available.
Tables 2 and 3 show the sentence-level consis-
tency and system-level correlation respectively of
TESLA and TESLA-M against the best reported
results in WMT 2009 under identical setting. The
results show that both TESLA and TESLA-M
give very competitive performances. Interestingly,
TESLA and TESLA-M obtain similar scores in the
out-of-English task. This could be because we use
only one pivot language (English), compared to
four in the into-English task. We plan to inves-
tigate this phenomenon in our future work.
6 Conclusion
This paper describes TESLA-M and TESLA. Our
main contributions are: (1) we generalize the
bipartite matching formalism of MaxSim into a
more expressive linear programming framework;
6The original WMT09 report contained erroneous results.
The scores here are the corrected results released after publi-
cation.
7www.wiktionary.org
358
en-fr en-de en-es en-cz Overall
TESLA 0.6828 0.5734 0.5940 0.5519 0.5796
TESLA-M 0.6390 0.5890 0.5927 0.5656 0.5847
wcd6p4er 0.67 0.58 0.61 0.59 0.60
wpF 0.66 0.60 0.61 n/a 0.61
terp 0.62 0.50 0.54 0.31 0.43
Table 2: Out-of-English task sentence-level con-
sistency on WMT 2009 data
en-fr en-de en-es en-cz Overall
TESLA 0.8529 0.7857 0.7272 0.3141 0.6700
TESLA-M 0.9294 0.8571 0.7909 0.0857 0.6657
wcd6p4er -0.89 0.54 -0.45 -0.1 -0.22
wpF 0.90 -0.06 0.58 n/a n/a
terp -0.89 0.03 -0.58 -0.40 -0.46
Table 3: Out-of-English task system-level correla-
tion on WMT 2009 data
(2) we exploit parallel texts to create a shallow se-
mantic representation of the sentences; and (3) we
show that they outperform all participants in most
WMT 2009 shared evaluation tasks.
Acknowledgments
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) ad-
ministered by the Media Development Authority
(MDA) of Singapore.
References
J. Atserias, B. Casas, E. Comelles, M. Gonz?lez,
L. Padr?, and M. Padr?. 2006. Freeling 1.3: Syn-
tactic and semantic services in an open-source NLP
library. In Proceedings of LREC.
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved cor-
relation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for Machine Translation and/or
Summarization.
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings
of ACL.
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proceedings of HLT-NAACL.
C. Callison-Burch, P. Koehn, C. Monz, and
J. Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of WMT.
Y.S. Chan and H.T. Ng. 2008. MAXSIM: A maximum
similarity metric for machine translation evaluation.
In Proceedings of ACL.
T. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein,
2001. Introduction to Algorithms. MIT Press, Cam-
bridge, MA.
C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. MIT Press, Cambridge, MA.
J. Gim?nez and L. M?rquez. 2008. A smorgasbord of
features for automatic MT evaluation. In Proceed-
ings of the Third WMT.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG mod-
els. In Proceedings of ACL-IJCNLP.
T. Joachims. 2006. Training linear svms in linear time.
In Proceedings of KDD.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In Proceedings of EACL.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of HLT-
NAACL.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proceedings of HLT-NAACL.
F.J. Och and N. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1).
S. Pado, M. Galley, D. Jurafsky, and C.D. Man-
ning. 2009. Robust machine translation evaluation
with entailment features. In Proceedings of ACL-
IJCNLP.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proceedings of ACL.
M. Popovic? and H. Ney. 2009. Syntax-oriented eval-
uation measures for machine translation output. In
Proceedings of WMT.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
AMTA.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009. Fluency, adequacy, or HTER? Exploring dif-
ferent human judgments with a tunable MT metric.
In Proceedings of WMT.
A. Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of ICSLP.
359
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 78?84,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
TESLA at WMT 2011: Translation Evaluation and Tunable Metric
Daniel Dahlmeier1 and Chang Liu2 and Hwee Tou Ng1,2
1NUS Graduate School for Integrative Sciences and Engineering
2Department of Computer Science, National University of Singapore
{danielhe,liuchan1,nght}@comp.nus.edu.sg
Abstract
This paper describes the submission from the
National University of Singapore to the WMT
2011 Shared Evaluation Task and the Tunable
Metric Task. Our entry is TESLA in three dif-
ferent configurations: TESLA-M, TESLA-F,
and the new TESLA-B.
1 Introduction
TESLA (Translation Evaluation of Sentences with
Linear-programming-based Analysis) was first pro-
posed in Liu et al (2010). The simplest variant,
TESLA-M (M stands for minimal), is based on N-
gram matching, and utilizes light-weight linguis-
tic analysis including lemmatization, part-of-speech
tagging, and WordNet synonym relations. TESLA-
B (B stands for basic) additionally takes advan-
tage of bilingual phrase tables to model phrase syn-
onyms. It is a new configuration proposed in this pa-
per. The most sophisticated configuration TESLA-F
(F stands for full) additionally uses language mod-
els and a ranking support vector machine instead of
simple averaging. TESLA-F was called TESLA in
Liu et al (2010). In this paper, we rationalize the
naming convention by using TESLA to refer to the
whole family of metrics.
The rest of this paper is organized as follows. Sec-
tions 2 to 4 describe the TESLA variants TESLA-M,
TESLA-B, and TESLA-F, respectively. Section 5
describes MT tuning with TESLA. Section 6 shows
experimental results for the evaluation and the tun-
able metric task. The last section concludes the pa-
per.
2 TESLA-M
The version of TESLA-M used in WMT 2011 is ex-
actly the same as in Liu et al (2010). The descrip-
tion is reproduced here for completeness.
We consider the task of evaluating machine trans-
lation systems in the direction of translating a source
language to a target language. We are given a refer-
ence translation produced by a professional human
translator and a machine-produced system transla-
tion. At the highest level, TESLA-M is the arith-
metic average of F-measures between bags of N-
grams (BNGs). A BNG is a multiset of weighted
N-grams. Mathematically, a BNG B consists of tu-
ples (bi, bWi ), where each bi is an N-gram and b
W
i is
a positive real number representing the weight of bi.
In the simplest case, a BNG contains every N-gram
in a translated sentence, and the weights are just the
counts of the respective N-grams. However, to em-
phasize the content words over the function words,
we discount the weight of an N-gram by a factor of
0.1 for every function word in the N-gram. We de-
cide whether a word is a function word based on its
POS tag.
In TESLA-M, the BNGs are extracted in the target
language, so we call them bags of target language
N-grams (BTNGs).
2.1 Similarity functions
To match two BNGs, we first need a similarity mea-
sure between N-grams. In this section, we define
the similarity measures used in our experiments. We
adopt the similarity measure from MaxSim (Chan
and Ng, 2008; Chan and Ng, 2009) as sms. For uni-
grams x and y,
78
? If lemma(x) = lemma(y), then sms = 1.
? Otherwise, let
a = I(synsets(x) overlap with synsets(y))
b = I(POS(x) = POS(y))
where I(?) is the indicator function, then sms =
(a + b)/2.
The synsets are obtained by querying WordNet
(Fellbaum, 1998). For languages other than English,
a synonym dictionary is used instead.
We define two other similarity functions between
unigrams:
slem(x, y) = I(lemma(x) = lemma(y))
spos(x, y) = I(POS(x) = POS(y))
All the three unigram similarity functions generalize
to N-grams in the same way. For two N-grams x =
x1,2,...,n and y = y1,2,...,n,
s(x, y) =
{
0 if ?i, s(xi, yi) = 0
1
n
?n
i=1 s(x
i, yi) otherwise
2.2 Matching two BNGs
Now we describe the procedure of matching two
BNGs. We take as input BNGs X and Y and a sim-
ilarity measure s. The i-th entry in X is xi and has
weight xWi (analogously for yj and y
W
j ).
Intuitively, we wish to align the entries of the two
BNGs in a way that maximizes the overall similar-
ity. An example matching problem for bigrams is
shown in Figure 1a, where the weight of each node
is shown, along with the hypothetical similarity for
each edge. Edges with a similarity of zero are not
shown. Note that for each function word, we dis-
count the weight by a factor of ten. The solution to
the matching problem is shown in Figure 1b, and the
overall similarity is 0.5 ? 0.01 + 0.8 ? 0.1 + 0.8 ?
0.1 = 0.165.
Mathematically, we formulate this as a (real-
valued) linear programming problem1. The vari-
ables are the allocated weights for the edges
w(xi, yj) ?i, j
1While integer linear programming is NP-complete, real-
valued linear programming can be solved efficiently.
w=1.0 w=0.1 w=0.1 w=0.1
w=0.01 w=0.1 w=0.1
s=0.1 s=0.8s=0.5 s=0.8
Good morning morning , , sir sir .
Hello , , Querrien Querrien .
s=0.4
(a) The matching problem
w=1.0 w=0.1 w=0.1 w=0.1
w=0.01 w=0.1 w=0.1
w=0.1w=0.01 w=0.1
s885Go8d m r o8d m r Gn nGimd imdG.
g,HH8Gn nGel,ddm, el,ddm, G.
(b) The solution
Figure 1: A BNG matching problem
We maximize
?
i,j
s(xi, yj)w(xi, yj)
subject to
w(xi, yj) ? 0 ?i, j
?
j
w(xi, yj) ? x
W
i ?i
?
i
w(xi, yj) ? y
W
j ?j
The value of the objective function is the overall
similarity S. Assuming X is the reference and Y
is the system translation, we have
Precision =
S
?
j y
W
j
Recall =
S
?
i x
W
i
The F-measure is derived from the precision and the
recall:
F =
Precision ? Recall
? ? Precision + (1 ? ?) ? Recall
In this work, we set ? = 0.8, following MaxSim.
The value gives more importance to the recall than
the precision.
79
If the similarity function is binary-valued and
transitive, such as slem and spos, then we
can use a much simpler and faster greedy
matching procedure: the best match is simply
?
g min(
?
xi=g
xWi ,
?
yi=g
yWi ).
2.3 Scoring
The TESLA-M sentence-level score for a reference
and a system translation is the arithmetic average of
the BTNG F-measures for unigrams, bigrams, and
trigrams based on similarity functions sms and spos.
We thus have 3 ? 2 = 6 BTNG F-measures for
TESLA-M.
We can compute a system-level score for a ma-
chine translation system by averaging its sentence-
level scores over the complete test set.
3 TESLA-B
TESLA-B uses the average of two types of F-
measures: (1) BTNG F-measures as in TESLA-M
and (2) F-measures between bags of N-grams in one
or more pivot languages, called bags of pivot lan-
guage N-grams (BPNGs), The rest of this section fo-
cuses on the generation of the BPNGs. Their match-
ing is done in the same way as described for BTNGs
in the previous section.
3.1 Phrase level semantic representation
Given a sentence-aligned bitext between the target
language and a pivot language, we can align the
text at the word level using well known tools such
as GIZA++ (Och and Ney, 2003) or the Berkeley
aligner (Liang et al, 2006; Haghighi et al, 2009).
We observe that the distribution of aligned
phrases in a pivot language can serve as a seman-
tic representation of a target language phrase. That
is, if two target language phrases are often aligned
to the same pivot language phrase, then they can be
inferred to be similar in meaning. Similar observa-
tions have been made by previous researchers (Ban-
nard and Callison-Burch, 2005; Callison-Burch et
al., 2006; Snover et al, 2009).
We note here two differences from WordNet syn-
onyms: (1) the relationship is not restricted to the
word level only, and (2) the relationship is not bi-
nary. The degree of similarity can be measured by
the percentage of overlap between the semantic rep-
resentations.
3.2 Segmenting a sentence into phrases
To extend the concept of this semantic representa-
tion of phrases to sentences, we segment a sentence
in the target language into phrases. Given a phrase
table, we can approximate the probability of a phrase
p by:
Pr(p) =
N(p)
?
p? N(p
?)
(1)
where N(?) is the count of a phrase in the phrase
table. We then define the likelihood of seg-
menting a sentence S into a sequence of phrases
(p1, p2, . . . , pn) by:
Pr(p1, p2, . . . , pn|S) =
1
Z(S)
n?
i=1
Pr(pi) (2)
where Z(S) is a normalizing constant. The segmen-
tation of S that maximizes the probability can be de-
termined efficiently using a dynamic programming
algorithm. The formula has a strong preference for
longer phrases, as every Pr(p) is a small fraction.
To deal with out-of-vocabulary (OOV) words, we
allow any single word w to be considered a phrase,
and if N(w) = 0, we set N(w) = 0.5 instead.
3.3 BPNGs as sentence level semantic
representation
Simply merging the phrase-level semantic represen-
tation is insufficient to produce a sensible sentence-
level semantic representation. As an example, we
consider two target language (English) sentences
segmented as follows:
1. ||| Hello , ||| Querrien ||| . |||
2. ||| Good morning , sir . |||
A naive comparison of the bags of aligned pivot lan-
guage (French) phrases would likely conclude that
the two sentences are completely unrelated, as the
bags of aligned phrases are likely to be completely
disjoint. We tackle this problem by constructing
a confusion network representation of the aligned
phrases, as shown in Figures 2 and 3. A confusion
network is a compact representation of a potentially
exponentially large number of weighted and likely
malformed French sentences. We can collect the N-
gram statistics of this ensemble of French sentences
80
w=1.=0s858G8od 
mrn0i858G8odg
,0HsseH18G8gdo d8G8gdo
Figure 2: A confusion network as a semantic repre-
sentation
w=1.=0s858G=1od 0s8m8r8nmi
Figure 3: A degenerate confusion network as a se-
mantic representation
efficiently from the confusion network representa-
tion. For example, the trigram Bonjour , Querrien 2
would receive a weight of 0.9 ? 1.0 = 0.9 in Fig-
ure 2. As with BTNGs, we discount the weight of an
N-gram by a factor of 0.1 for every function word in
the N-gram, so as to place more emphasis on the
content words.
The collection of all such N-grams and their cor-
responding weights forms the BPNG of a sentence.
The reference and system BPNGs are then matched
using the algorithm outlined in Section 2.2.
3.4 Scoring
The TESLA-B sentence-level score is a linear com-
bination of (1) BTNG F-measures for unigrams,
bigrams, and trigrams based on similarity func-
tions sms and spos, and (2) BPNG F-measures for
unigrams, bigrams, and trigrams based on sim-
ilarity functions slem and spos. We thus have
3 ? 2 F-measures from the BTNGs and 3 ? 2 ?
#pivot languages F-measures from the BPNGs. We
average the BTNG and BPNG scores to obtain
sBTNG and sBPNG, respectively. The sentence-
level TESLA-B score is then defined as 12(sBTNG +
sBPNG). The two-step averaging process prevents
the BPNG scores from overwhelming the BTNG
scores, especially when we have many pivot lan-
guages. The system-level TESLA-B score is the
arithmetic average of the sentence-level TESLA-B
scores.
2Note that an N-gram can span more than one segment.
4 TESLA-F
Unlike the simple arithmetic averages used in
TESLA-M and TESLA-B, TESLA-F uses a gen-
eral linear combination of three types of scores: (1)
BTNG F-measures as in TESLA-M and TESLA-B,
(2) BPNG F-measures as in TESLA-B, and (3) nor-
malized language model scores of the system trans-
lation, defined as 1n logP , where n is the length of
the translation, and P the language model probabil-
ity. The method of training the linear model depends
on the development data. In the case of WMT, the
development data is in the form of manual rankings,
so we train SVM rank (Joachims, 2006) on these in-
stances to build the linear model. In other scenarios,
some form of regression can be more appropriate.
The BTNG and BPNG scores are the same as
used in TESLA-B. In the WMT campaigns, we use
two language models, one generated from the Eu-
roparl dataset and one from the news-train dataset.
We thus have 3 ? 2 features from the BTNGs,
3 ? 2 ? #pivot languages features from the BPNGs,
and 2 features from the language models. Again, we
can compute system-level scores by averaging the
sentence-level scores.
4.1 Scaling of TESLA-F Scores
While machine translation evaluation is concerned
only with the relative order of the different trans-
lations but not with the absolute scores, there are
practical advantages in normalizing the evaluation
scores to a range between 0 and 1. For TESLA-M
and TESLA-B, this is already the case, since every
F-measure has a range of [0, 1] and so do their av-
erages. In contrast, the SVM rank -produced model
typically gives scores very close to zero.
To remedy that, we note that we have the free-
dom to scale and shift the linear SVM model with-
out changing the metric. We observe that the F-
measures have a range of [0, 1], and studying the
data reveals that [?15, 0] is a good approximation of
the range for normalized language model scores, for
all languages involved in the WMT campaign. Since
we know the range of values of an F-measure feature
(between 0 and 1) and assuming that the range of
the normalized LM score is between ?15 and 0, we
can find the maximum and minimum possible score
given the weights. Then we linearly scale the range
81
of scores from [min, max] to [0, 1]. We provide an
option of scaling TESLA-F scores in the new release
of TESLA.
5 MT tuning with TESLA
All variants of TESLA can be used for automatic
MT tuning using Z-MERT (Zaidan, 2009). Z-
MERT?s modular design makes it easy to integrate a
new metric. As TESLA already computes scores at
the sentence level, integrating TESLA into Z-MERT
was straightforward. First, we created a ?streaming?
version of each TESLA metric which reads trans-
lation candidates from standard input and prints the
sentence-level scores to standard output. This allows
Z-MERT to easily query the metric for sentence-
level scores during MT tuning. Second, we wrote
a Java wrapper that calls the TESLA code from Z-
MERT. The resulting metric can be used for MERT
tuning in the standard fashion. All that a user has
to do is to change the metric in the Z-MERT config-
uration file to TESLA. All the necessary code for
Z-MERT tuning is included in the new release of
TESLA.
6 Experiments
6.1 Evaluation Task
We evaluate TESLA using the publicly available
data from WMT 2009 for into-English and out-
of-English translation. The pivot language phrase
tables and language models are built using the
WMT 2009 training data. The SVM rank model for
TESLA-F is trained on manual rankings from WMT
2008. The results for TESLA-M and TESLA-F have
previously been reported in Liu et al (2010)3. We
add results for the new variant TESLA-B here.
Tables 1 and 2 show the sentence-level consis-
tency and system-level Spearman?s rank correlation,
respectively for into-English translation. For com-
parison, we include results for some of the best per-
forming metrics in WMT 2009. Tables 3 and 4 show
the same results for out-of-English translation. We
do not include the English-Czech language pair in
our experiments, as we unfortunately do not have
good linguistic resources for the Czech language.
3The English-Spanish system correlation differs from our
previous result after fixing a minor mistake in the language
model.
cz-en fr-en de-en es-en hu-en Overall
TESLA-M 0.60 0.61 0.61 0.59 0.63 0.61
TESLA-B 0.63 0.64 0.63 0.62 0.63 0.63
TESLA-F 0.63 0.65 0.64 0.62 0.66 0.63
ulc 0.63 0.64 0.64 0.61 0.60 0.63
maxsim 0.60 0.63 0.63 0.61 0.62 0.62
meteor-0.6 0.47 0.51 0.52 0.49 0.48 0.50
Table 1: Into-English sentence-level consistency on
WMT 2009 data
cz-en fr-en de-en es-en hu-en Avg
TESLA-M 1.00 0.86 0.85 0.99 0.66 0.87
TESLA-B 1.00 0.92 0.67 0.95 0.83 0.87
TESLA-F 1.00 0.92 0.68 0.94 0.94 0.90
ulc 1.00 0.92 0.78 0.86 0.60 0.83
maxsim 0.70 0.91 0.76 0.98 0.66 0.80
meteor-0.6 0.70 0.93 0.56 0.87 0.54 0.72
Table 2: Into-English system-level Spearman?s rank
correlation on WMT 2009 data
The new TESLA-B metric proves to be competi-
tive to its siblings and is often on par with the more
sophisticated TESLA-F metric. The exception is
the English-German language pair, where TESLA-
B has very low system-level correlation. We have
two possible explanations for this. First, the system-
level correlation is computed on a very small sample
size (the ranked list of MT systems). This makes the
system-level correlation score more volatile com-
pared to the sentence-level consistency score which
is computed on thousands of sentence pairs. Sec-
ond, German has a relatively free word order which
potentially makes word alignment and phrase table
extraction more noisy. Interestingly, all participating
metrics in WMT 2009 had low system-level correla-
tion for the English-German language pair.
en-fr en-de en-es Overall
TESLA-M 0.64 0.59 0.59 0.60
TESLA-B 0.65 0.59 0.60 0.61
TESLA-F 0.68 0.57 0.60 0.61
wpF 0.66 0.60 0.61 0.61
wpbleu 0.60 0.47 0.49 0.51
Table 3: Out-of-English sentence-level consistency
on WMT 2009 data
82
en-fr en-de en-es Avg
TESLA-M 0.93 0.86 0.79 0.86
TESLA-B 0.91 0.05 0.63 0.53
TESLA-F 0.85 0.78 0.67 0.77
wpF 0.90 -0.06 0.58 0.47
wpbleu 0.92 0.07 0.63 0.54
Table 4: Out-of-English system-level Spearman?s
rank correlation on WMT 2009 data
6.2 Tunable Metric Task
The goal of the new tunable metric task is to explore
MT tuning with metrics other than BLEU (Papineni
et al, 2002). To allow for a fair comparison, the
WMT organizers provided participants with a com-
plete Joshua MT system for an Urdu-English trans-
lation task. We tuned models for each variant of
TESLA, using Z-MERT in the default configuration
provided by the organizers. There are four reference
translations for each Urdu source sentence. The size
of the N-best list is set to 300.
For our own experiments, we randomly split the
development set into a development portion (781
sentences) and a held-out test portion (200 sen-
tences). We run the same Z-MERT tuning process
for each TESLA variant on this reduced develop-
ment set and evaluate the resulting models on the
held out test set. We include a model trained with
BLEU as an additional reference point. The results
are shown in Table 5. We observe that the model
trained with TESLA-F achieves the best results
when evaluated with any of the TESLA metrics, al-
though the differences between the scores are small.
We found that TESLA produces slightly longer
translations than BLEU: 22.4 words (TESLA-M),
21.7 words (TESLA-B), and 22.5 words (TESLA-
F), versus 18.7 words (BLEU). The average refer-
ence length is 19.8 words.
The official evaluation for the tunable metric task
is performed using manual rankings. The score of
a system is calculated as the percentage of times
the system is judged to be either better or equal
(score1) or strictly better (score2) compared to each
other system in pairwise comparisons. Although
we submit results for all TESLA variants, only our
primary submission TESLA-F is included in the
manual evaluation. The results for TESLA-F are
mixed. When evaluated with score1, TESLA-F is
Tune\Test BLEU TESLA-M TESLA-B TESLA-F
BLEU 0.2715 0.3756 0.3129 0.3920
TESLA-M 0.2279 0.4056 0.3279 0.3981
TESLA-B 0.2370 0.4001 0.3257 0.3977
TESLA-F 0.2432 0.4076 0.3299 0.4007
Table 5: Automatic evaluation scores on held out
test portion for the tunable metric task. The best re-
sult in each column is printed in bold.
ranked 7th out of 8 participating systems, but when
evaluated with score2, TESLA-F is ranked second
best. These findings differ from previous results
that we reported in Liu et al (2011) where MT
systems tuned with TESLA-M and TESLA-F con-
sistently outperform two other systems tuned with
BLEU and TER for translations from French, Ger-
man, and Spanish into English on the WMT 2010
news data set. A manual inspection of the references
in the tunable metric task shows that the translations
are of lower quality compared to the news data sets
used in WMT. As the SVM model in TESLA-F is
trained with rankings from WMT 2008, it is possible
that the model is less robust when applied to Urdu-
English translations. This could explain the mixed
performance of TESLA-F in the tunable metric task.
7 Conclusion
We introduce TESLA-B, a new variant of the
TESLA machine translation metric and present ex-
perimental results for all TESLA variants in the set-
ting of the WMT evaluation task and tunable met-
ric task. All TESLA variants are integrated into Z-
MERT for automatic machine translation tuning.
Acknowledgments
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics.
83
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics.
Yee Seng Chan and Hwee Tou Ng. 2008. MaxSim:
A maximum similarity metric for machine translation
evaluation. In Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2009. MaxSim: Per-
formance and effects of translation fluency. Machine
Translation, 23(2?3):157?168.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proceedings of 47th Annual Meeting
of the Association for Computational Linguistics and
the 4th IJCNLP of the AFNLP.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. TESLA: Translation evaluation of sentences
with linear-programming-based analysis. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with a
tunable MT metric. In Proceedings of of the Fourth
Workshop on Statistical Machine Translation.
Omar Zaidan. 2009. Z-MERT: A fully configurable open
source tool for minimum error rate training of machine
translation systems. The Prague Bulletin of Mathe-
matical Linguistics, 91:79?88.
84
