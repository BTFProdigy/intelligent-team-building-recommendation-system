Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 194?197,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
UvT: The UvT Term Extraction System in the Keyphrase Extraction task
Kalliopi Zervanou
ILK / TiCC - Tilburg centre for Cognition and Communication
University of Tilburg, P.O. Box 90153, 5000 LE Tilburg, The Netherlands
K.Zervanou@uvt.nl
Abstract
The UvT system is based on a hybrid, lin-
guistic and statistical approach, originally
proposed for the recognition of multi-
word terminological phrases, the C-value
method (Frantzi et al, 2000). In the UvT
implementation, we use an extended noun
phrase rule set and take into consideration
orthographic and morphological variation,
term abbreviations and acronyms, and ba-
sic document structure information.
1 Introduction
The increasing amount of documents in elec-
tronic form makes imperative the need for docu-
ment content classification and semantic labelling.
Keyphrase extraction contributes to this goal by
the identification of important and discriminative
concepts expressed as keyphrases. Keyphrases
as reduced document content representations may
find applications in document retrieval, classifica-
tion and summarisation (D?Avanzo and Magnini,
2005). The literature distinguishes between two
principal processes: keyphrase extraction and
keyphrase assignment. In the case of keyphrase
assignment, suitable keyphrases from an exist-
ing knowledge resource, such as a controlled vo-
cabulary, or a thesaurus are assigned to docu-
ments based on classification of their content. In
keyphrase extraction, the phrases are mined from
the document itself. Supervised approaches to
the problem of keyphrase extraction include the
Naive Bayes-based KEA algorithms (Gordon et
al., 1999) (Medelyan and Witten, 2006), deci-
sion tree-based and the genetic algorithm-based
GenEx (Turney, 1999), and the probabilistic KL
divergence-based language model (Tomokiyo and
Hurst, 2003). Research in keyphrase extrac-
tion proposes the detection of keyphrases based
on various statistics-based, or pattern-based fea-
tures. Statistical measures investigated focus pri-
marily on keyphrase frequency measures, whereas
pattern-features include noun phrase pattern filter-
ing, identification of keyphrase head and respec-
tive frequencies (Barker and Cornacchia, 2000),
document section position of the keyphrase (e.g.,
(Medelyan and Witten, 2006)) and keyphrase
coherence (Turney, 2003). In this paper, we
present an unsupervised approach which combines
pattern-based morphosyntactic rules with a statis-
tical measure, the C-value measure (Frantzi et al,
2000) which originates from research in the field
of automatic term recognition and was initially de-
signed for specialised domain terminology acqui-
sition.
2 System description
The input documents in the Keyphrase Extrac-
tion task were scientific articles converted from
their originally published form to plain text.
Due to this process, some compound hyphen-
ated words are erroneously converted into a single
word (e.g., ?resourcemanagement? vs. ?resource-
management?). Moreover, document sections
such as tables, figures, footnotes, headers and foot-
ers, often intercept sentence and paragraph text.
Finally, due to the particularity of the scientific ar-
ticles domain, input documents often contain ir-
regular text, such as URLs, inline bibliographic
references, mathematical formulas and symbols.
In our approach, we attempted to address some
of these issues by document structuring, treatment
of orthographic variation and filtering of irregular
text.
The approach adopted first applies part-of-
speech tagging and basic document structuring
(sec. 2.1 and 2.2). Subsequently, keyphrase can-
didates conforming to pre-defined morphosyntac-
tic rule patterns are identified (sec. 2.3). In
the next stage, orthographic, morphological and
abbreviation variation phenomena are addressed
194
(sec. 2.4) and, finally, candidate keyphrases are
selected based on C-value statistical measure (sec.
2.5).
2.1 Linguistic pre-processing
For morphosyntactic analysis, we used the Maxent
(Ratnaparkhi, 1996) POS tagger implementation
of the openNLP toolsuite
1
. In order to improve
tagging accuracy, irregular text, such as URLs,
inline references, and recurrent patterns indicat-
ing footers and mathematical formulas are filtered
prior to tagging.
2.2 Basic document structuring
Document structuring is based on identified re-
current patterns, such as common section titles
and legend indicators (e.g., ?Abstract?, ?Table...?),
section headers numbering and preserved format-
ting, such as newline characters. Thus, the doc-
ument sections that the system may recognise
are: Title, Abstract, Introduction, Conclusion,
Acknowledgements, References, Header (for any
other section headers and legends) and Main (for
any other document section text).
2.3 Rule pattern filtering
The UvT system considers as candidate
keyphrases, those multi-word noun phrases
conforming to pre-defined morphosyntactic rule
patterns. In particular, the patterns considered are:
M
+
N
M C M N
M
+
N C N
N P M
?
N
N P M
?
N C N
N C N P M
?
N
M C M N
M
+
N C N
where M is a modifier, such as an adjective, a
noun, a present or past participle, or a proper noun
including a possessive ending, N is a noun, P a
preposition and C a conjunction. For every sen-
tence input, the matching process is exhaustive:
after the longest valid match is identified, the rules
1
http://opennlp.sourceforge.net/
are re-applied, so as to identify all possible shorter
valid matches for nested noun phrases. At this
stage, the rules also allow for inclusion of poten-
tial abbreviations and acronyms in the identified
noun phrase of the form:
M
+
(A) N
M
+
N (A)
where (A) is a potential acronym appearing as a
single token in uppercase, enclosed by parentheses
and tagged as a proper noun.
2.4 Text normalisation
In this processing stage, the objective is the
recognition and reduction of variation phenom-
ena which, if left untreated, will affect the C-
value statistical measures at the keyphrase selec-
tion stage. Variation is a pervasive phenomenon
in terminology and is generally defined as the al-
teration of the surface form of a terminological
concept (Jacquemin, 2001). In our approach, we
attempt to address morphological variation, i.e.,
variation due to morphological affixes and ortho-
graphic variation, such as hyphenated vs. non-
hyphenated compound phrases and abbreviated
phrase forms vs. full noun phrase forms.
In order to reduce morphological variation, UvT
system uses the J.Renie interface
2
to WordNet lex-
icon
3
to acquire lemmas for the respective can-
didate phrases. Orthographic variation phenom-
ena are treated by rule matching techniques. In
this process, for every candidate keyphrase match-
ing a rule, the respective string alternations are
generated and added as variant phrases. For ex-
ample, for patterns including acronyms and the
respective full form, alternative variant phrases
generated may contain either the full form only,
or the acronym replacing its respective full form.
Similarly, for hyphenated words, non-hyphenated
forms are generated.
2.5 C-value measure
The statistical measure used for keyphrase ranking
and selection is the C-value measure (Frantzi et al,
2000). C-value was originally proposed for defin-
ing potential terminological phrases and is based
on normalising frequency of occurrence measures
2
http://www.ai.mit.edu/ jrennie/WordNet/
3
http://wordnet.princeton.edu/
195
Performance over Reader-Assigned Keywords
System top 5 candidates top 10 candidates top 15 candidates
P R F P R F P R F
TF?IDF 17.80% 7.39% 10.44% 13.90% 11.54% 12.61% 11.60% 14.45% 12.87%
NB & ME 16.80% 6.98% 9.86% 13.30% 11.05% 12.07% 11.40% 14.20% 12.65%
UvT 20.40% 8.47% 11.97% 15.60% 12.96% 14.16% 11.93% 14.87% 13.24%
UvT - A 23.60% 9.80% 13.85% 16.10% 13.37% 14.61% 12.00% 14.95% 13.31%
UvT - I 21.20% 8.80% 12.44% 14.50% 12.04% 13.16% 12.00% 14.95% 13.31%
UvT - M 20.40% 8.47% 11.97% 15.10% 12.54% 13.70% 11.40% 14.20% 12.65%
UvT - IC 23.20% 9.63% 13.61% 16.00% 13.29% 14.52% 13.07% 16.28% 14.50%
Performance over Combined Keywords
System top 5 candidates top 10 candidates top 15 candidates
P R F P R F P R F
TF?IDF 22.00% 7.50% 11.19% 17.70% 12.07% 14.35% 14.93% 15.28% 15.10%
NB & ME 21.40% 7.30% 10.89% 17.30% 11.80% 14.03% 14.53% 14.87% 14.70%
UvT 24.80% 8.46% 12.62% 18.60% 12.69% 15.09% 14.60% 14.94% 14.77%
UvT - A 28.80% 9.82% 14.65% 19.60% 13.37% 15.90% 14.67% 15.01% 14.84%
UvT - I 26.40% 9.00% 13.42% 17.80% 12.14% 14.44% 14.73% 15.08% 14.90%
UvT - M 24.80% 8.46% 12.62% 17.90% 12.21% 14.52% 14.07% 14.39% 14.23%
UvT - IC 28.60% 9.75% 14.54% 19.70% 13.44% 15.98% 16.13% 16.51% 16.32%
Table 1: UvT, UvT variants and baseline systems performance on the Keyphrase Extraction Task
by taking into consideration the candidate multi-
word phrase constituent length and terms appear-
ing as nested within longer terms. In particu-
lar, depending on whether a candidate multi-word
phrase is nested or not, C-value is defined as:
C-value =
?
?
?
log
2
|a|f(a)
log
2
|a|(f(a)?
1
P (T
a
)
?
b?T
a
f(b))
In the above, the first C-value measurement is
for non-nested terms and the second for nested
terms, where a denotes the word sequence that is
proposed as a term, |a| is the length of this term
in words, f(a) is the frequency of occurrence of
this term in the corpus, both as an independent
term and as a nested term within larger terms, and
P (T
a
) denotes the probability of a term string oc-
curring as nested term.
In this processing stage of keyphrase selection,
we start by measuring frequency of occurrence for
all our candidate phrases, taking into considera-
tion phrase variants, as identified in the Text nor-
malisation stage. Then, we proceed by calculating
nested phrases frequences and, finally, we estimate
C-value.
The result of this process is a list of proposed
keyphrases, ranked by decreasing C-value mea-
sure, wherefrom the top 15 were selected for the
evaluation of the system results.
3 Results
The overall official results of the UvT system are
shown in Table 1, where P , R and F correspond
to micro-averaged precision, recall and F-score
for the respective sets of candidate keyphrases,
based on reader-assigned and combined author-
and reader-assigned gold standards. Table 1 also
illustrates the reported performance of the task
baseline systems (i.e., TF?IDF, Naive Bayes (NB)
and maximum entropy (ME)
4
) and the UvT sys-
tem performance variance based on document sec-
tion candidates (-A: Abstract, -I: Introduction, -M:
Main, -IC: Introduction and Conclusion combina-
tion). In these system variants, rather than select-
ing the top 15 C-value candidates from the sys-
tem output, we also apply restrictions based on
the candidate keyphrase document section infor-
mation, thus skipping candidates which do not ap-
pear in the respective document section.
Overall, the UvT system performance is close
to the baseline systems results. We observe that
the system exhibits higher performance for its top
4
The reported performance of both NB and ME for the re-
spective gold-standard sets in the Keyphrase Extraction Task
is identical.
196
5 candidate set and this performance drops rapidly
as we include more terms in the answer set. One
possible reason for its average performance could
be attributed to increased ?noise? in the results set.
In particular, our text filtering method failed to ac-
curately remove a large amount of irregular text
in form of mathematical formulas and symbols
which were erroneously tagged as proper nouns.
As indicated in Table 1, the improved results of
system variants based on document sections, such
as Abstract, Introduction and Conclusion, where
these symbols and formulas are rather uncommon,
could be partly attributed to ?noise? reduction.
Interestingly, the best system performance
in these document section results is demon-
strated by the Introduction-Conclusion com-
bination (UvT-IC). Other tested combinations
(not illustrated in Table 1), such as abstract-
intro, abstract-intro-conclusions, abstract-intro-
conclusions-references, display similar results on
the reader-assigned set and a performance rang-
ing between 15,6-16% for the 15 candidates on
the combined set, while the inclusion of the Main
section candidates reduces the performance to the
overall system output (i.e., UvT results). Further
experiments are required for refining the criteria
for document section information, when the text
filtering process for ?noise? is improved.
Finally, another reason that contributes to the
system?s average performance lies in its inherent
limitation for the detection of multi-word phrases,
rather than both single and multi-word. In partic-
ular, single word keyphrases account for approx.
20% of the correct keyphrases in the gold standard
sets.
4 Conclusion
We have presented an approach to keyphrase ex-
traction mainly based on adaptation and imple-
mentation of the C-value method. This method
was originally proposed for the detection of ter-
minological phrases and although domain terms
may express the principal informational content of
a scientific article document, a method designed
for their exhaustive identification (including both
nested and longer multi-word terms) has not been
proven more effective than baseline methods in
the keyphrase detection task. Potential improve-
ments in performance could be investigated by
(1) improving document structure detection, so as
to reduce irregular text, (2) refinement of docu-
ment section information in keyphrase selection,
(3) adaptation of the C-value measure, so as to
possibly combine keyphrase frequency with a dis-
criminative measure, such as idf .
References
Ken Barker and Nadia Cornacchia. 2000. Using noun
phrase heads to extract document keyphrases. In
Proceedings of the 13th Biennial Conference of the
Canadian Society on Computational Studies of In-
telligence: Advances in Artificial Intelligence, pages
40?52, Montreal, Canada, May.
Ernesto D?Avanzo and Bernado Magnini. 2005. A
keyphrase-based approach to summarization: the
LAKE system. In Proceedings of Document Under-
standing Conferences, pages 6?8, Vancouver, BC,
Canada, October 9-10.
Katerina Frantzi, Sophia Ananiadou, and Hideki
Mima. 2000. Automatic recognition of multi-
word terms: The C-Value/NC-value Method. Intern.
Journal of Digital Libraries, 3(2):117?132.
Ian Witten Gordon, Gordon W. Paynter, Eibe Frank,
Carl Gutwin, and Craig G. Nevill-manning. 1999.
Kea: Practical automatic keyphrase extraction. In
Proceedings of the Fourth ACM conference on Dig-
ital Libraries, pages 254?256, Berkeley, CA, USA,
August 11-14. ACM Press.
Christian Jacquemin. 2001. Spotting and Discovering
Terms through Natural Language Processing. MIT
Press, Cambridge, MA, USA.
Olena Medelyan and Ian H. Witten. 2006. Thesaurus
based automatic keyphrase indexing. In JCDL ?06:
Proceedings of the 6th ACM/IEEE-CS joint confer-
ence on Digital libraries, pages 296?297, New York,
NY, USA. ACM.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Eric Brill and
Kenneth Church, editors, Proceedings of the Empiri-
cal Methods in Natural Language Processing, pages
133?142.
Takashi Tomokiyo and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL 2003 workshop on Mul-
tiword expressions, pages 33?40, Morristown, NJ,
USA. Association for Computational Linguistics.
Peter Turney. 1999. Learning to extract keyphrases
from text. Technical Report ERB-1057, National
Research Council, Institute for Information Technol-
ogy, February 17.
Peter Turney. 2003. Coherent keyphrase extraction via
web mining. In IJCAI?03: Proceedings of the 18th
international joint conference on Artificial intelli-
gence, pages 434?439, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
197
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 508?511,
Dublin, Ireland, August 23-24, 2014.
SAIL-GRS: Grammar Induction for Spoken Dialogue Systems using
CF-IRF Rule Similarity
Kalliopi Zervanou, Nikolaos Malandrakis and Shrikanth Narayanan
Signal Analysis and Interpretation Laboratory (SAIL),
University of Southern California, Los Angeles, CA 90089, USA
kzervanou@gmail.com, malandra@usc.edu , shri@sipi.usc.edu
Abstract
The SAIL-GRS system is based on a
widely used approach originating from in-
formation retrieval and document index-
ing, the TF -IDF measure. In this im-
plementation for spoken dialogue system
grammar induction, rule constituent fre-
quency and inverse rule frequency mea-
sures are used for estimating lexical and
semantic similarity of candidate grammar
rules to a seed set of rule pattern instances.
The performance of the system is evalu-
ated for the English language in three dif-
ferent domains, travel, tourism and finance
and in the travel domain, for Greek. The
simplicity of our approach makes it quite
easy and fast to implement irrespective of
language and domain. The results show
that the SAIL-GRS system performs quite
well in all three domains and in both lan-
guages.
1 Introduction
Spoken dialogue systems typically rely on gram-
mars which define the semantic frames and re-
spective fillers in dialogue scenarios (Chen et al.,
2013). Such systems are tailored for specific
domains for which the respective grammars are
mostly manually developed (Ward, 1990; Seneff,
1992). In order to address this issue, numerous
current approaches attempt to infer these grammar
rules automatically (Pargellis et al., 2001; Meng
and Siu, 2002; Yoshino et al., 2011; Chen et al.,
2013).
The acquisition of grammar rules for spoken
language systems is defined as a task comprising
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
of two subtasks (Meng and Siu, 2002; Iosif and
Potamianos, 2007), the acquisition of:
(i) Low-level rules These are rules defining
domain-specific entities, such as names of lo-
cations, hotels, airports, e.g. CountryName:
?USA?, Date: ?July 15th, 2014?, CardType:
?VISA? and other common domain multi-word ex-
pressions, e.g. DoYouKnowQ: ?do you know?.
(ii) High-level rules These are larger,
frame-like rule patterns which contain as
semantic slot fillers multi-word entities
identified by low-level rules. For exam-
ple: DirectionsQ: ?<DoYouKnowQ>
<where> the <MuseumName> is lo-
cated?, ExpressionCardProblem: ?my
<CardType> has expired?.
The shared task of Grammar Induction for Spo-
ken Dialogue Systems, where our system partic-
ipated, focused on the induction of high-level
grammar rules and in particular on the identifica-
tion and semantic classification of new rule pat-
terns based on their semantic similarity to known
rule instances.
Within this research framework, the work de-
scribed in this paper proposes a methodology for
estimating rule semantic similarity using a varia-
tion of the well-known measure of TF -IDF as
rule constituent frequency vs. inverse rule fre-
quency, henceforth CF -IRF .
In the remainder of this paper, we start in Sec-
tion 2 by a detailed description of our system. Sub-
sequently, in Section 3, we present the datasets
used and the evaluation process, and in Section 4
we discuss our results. We conclude in Section 5
with a summary of our observations and directions
for future work.
2 System Description
The SAIL-GRS system is based on a widely used
approach in information retrieval and document
indexing, the TF -IDF measure. TF -IDF is
508
an approach that has found numerous applications
in information management applications, such as
document keyword extraction, (e.g., Dillon and
Gray (1983)), document clustering, summarisa-
tion, (e.g., Gong and Liu (2001)), event cluster-
ing, (e.g., De Smet and Moens (2013)). In dia-
logue systems, TF -IDF has been used, among
other applications, for discovering local coher-
ence (Gandhe and Traum, 2007) and for acquir-
ing predicate-argument rule fragments in an open
domain, information extraction-based spoken dia-
logue system (Yoshino et al., 2011). In their ap-
proach, Yoshino et al. (2011) use the TF -IDF
measure to determine the importance of a given
word for a given domain or topic, so as to select
the most salient predicate-argument structure rule
patterns from their corpus.
In our implementation for spoken dialogue
system grammar induction, rule constituent fre-
quency (CF ) and inverse rule frequency (IRF )
measures are used for estimating lexical and se-
mantic similarity of candidate grammar rules to a
seed set of rule pattern instances. As illustrated in
Table 1, the SAIL-GRS algorithm has two main
steps, the training stage and the rule induction
stage.
Input: known rule pattern instances
Output: new candidate rule patterns
Training stage:
1. Known rule instance parsing
2. Rule constituent extraction (uni-/bigrams)
3. Rule constituent frequency count (CF )
4. Inverse rule frequency count (IRF )
5. CF -IRF rule instance vector creation
Rule induction stage:
1. Unknown text fragment parsing
2. Unigram & bigram extraction
3. Uni-/bigram CF -IRF value lookup
4. Creation of CF -IRF vector for
unknown text fragment
5. Estimation of cosine similarity of
unknown fragment to rule instances
6. New candidate rule selection & rule
semantic category classification using
maximum cosine similarity
Table 1: The SAIL-GRS system algorithm.
In the first, the Training stage, known rule in-
stances are parsed and, for each rule semantic cat-
egory, the respective high-level rule pattern in-
stances are acquired. These patterns are subse-
quently split into unigram and bigram constituents
and the respective constituent frequencies and in-
verse rule frequencies are estimated. Finally, for
each rule category, a vector representation is cre-
ated for the respective rule pattern instance, based
on the CF -IRF value of its unigram and bigram
constituents.
In the second step, the Rule induction stage, the
unknown text fragments are parsed and split into
unigrams and bigrams. Subsequently, we lookup
the known rule instance unigram and bigram rep-
resentations for potential lexical matches to these
new unigrams and bigrams. If these are found,
then the new n-grams acquire the respective CF -
IRF values found in the training instances and the
respective CF -IRF vector for the unknown text
fragments is created. Finally, we estimate the co-
sine similarity of this unknown text vector to each
known rule vector. The unknown text fragments
that are most similar to a given rule category are
selected as candidate rule patterns and are classi-
fied in the known rule semantic category. An un-
known text fragment that is selected as candidate
rule pattern is assigned only to one, the most sim-
ilar, rule category.
3 Experimental Setup
The overall objective in spoken dialogue system
grammar induction is the fast and efficient devel-
opment and portability of grammar resources. In
the Grammar Induction for Spoken Dialogue Sys-
tems task, this challenge was addressed by pro-
viding datasets in three different domains, travel,
tourism and finance, and by attempting to cover
more than one language for the travel domain,
namely English and Greek.
As illustrated in Table 2, the travel domain data
for the two languages are comparable, with 32 and
35 number of known rule categories, for English
and Greek, comprising of 982 and 956 high-level
rule pattern instances respectively. The smallest
dataset is the finance dataset, with 9 rule categories
and 136 rule pattern instances, while the tourism
dataset has a relatively low number of rule cate-
gories comprising of the highest number of rule
pattern instances. Interestingly, as indicated in the
column depicting the percent of unknown n-grams
in the test-set, i.e. the unigrams and the bigrams
without a CF -IRF value in the training data, the
tourism domain test-set appears also to be the one
509
with the greatest overlap with the training data,
with a mere 0.72% and 4.84% of unknown uni-
grams and bigrams respectively.
For the evaluation, the system performance is
estimated in terms of precision (P ), recall (R) and
F -score measures, for the correct classification of
an unknown text fragment to a given rule cate-
gory cluster of pattern instances. In addition to
these measures, the weighted average of the per
rule scores is computed as follows:
P
w
=
?
N?1
i=1
P
i
c
i
?
N?1
i=1
c
i
, R
w
=
?
N?1
i=1
R
i
n
i
?
N?1
i=1
n
i
(1)
F
w
=
2 ? P
w
?R
w
P
w
+ R
w
(2)
where N ? 1 is the total number of rule cate-
gories, P
i
and R
i
are the per rule i scores for preci-
sion and recall, c
i
the unknown patterns correctly
assigned to rule i, and n
i
the total number of cor-
rect rule instance patterns for rule i indicated in
the ground truth data.
4 Results
The results of the SAIL-GRS system outperform
the Baseline in all dataset categories, except the
Tourism domain, as illustrated in Table 3. In this
domain, both systems present the highest scores
compared to the other domains. The high results
in the travel domain are probably due to the high
data overlap between the train and the test data, as
discussed in the previous section and illustrated in
Table 2. However, this domain was also the one
with the highest average number of rule instances
per rule category, compared to the other domains,
thus presenting an additional challenge in the cor-
rect classification of unknown rule fragments.
We observe that the overall higher F measures
of the SAIL-GRS system in the travel and fi-
nance domains are due to higher precision scores,
whereas Baseline system displays higher recall but
lower precision scores and lower F-measure in
these domains.
The overall lowest scores for both systems are
reached in the Travel domain for Greek, which
is also the dataset with the lowest overlap with
the training data. However, the performance of
the SAIL-GRS system does not deteriorate to the
same extent as the Baseline, the precision of which
falls to a mere 0.16-0.17, compared to 0.49-0.46
for the SAIL-GRS system.
5 Conclusion
In this work, we have presented the SAIL-GRS
system used for the Grammar Induction for Spo-
ken Dialogue Systems task. Our approach uses
a fairly simple, language independent method for
measuring lexical and semantic similarity of rule
pattern instances. Our rule constituent frequency
vs. inverse rule frequency measure, CF -IRF is a
modification the TF -IDF measure for estimating
rule similarity in the induction process of new rule
instances.
The performance of our system in rule induc-
tion and rule pattern semantic classification was
tested in three different domains, travel, tourism
and finance in four datasets, three for English
and an additional dataset for the travel domain
in Greek. SAIL-GRS outperforms the Baseline
in all datasets, except the travel domain for En-
glish. Moreover, our results showed that our sys-
tem achieved an overall better score in precision
and respective F-measure, in the travel and finance
domains, even when applied to a language other
than English. Finally, in cases of a larger percent-
age of unknown data in the test set, as in the Greek
travel dataset, the smooth degradation of SAIL-
GRS results compared to the Baseline indicates
the robustness of our method.
A limitation of our system in its current version
lies in the requirement for absolute lexical match
with unknown rule unigrams and bigrams. Fu-
ture extensions of the system could include rule
constituent expansion using synonyms, variants or
semantically or lexically similar words, so as to
improve recall and the overall F-measure perfor-
mance.
References
Yun-Nung Chen, William Yang Wang, and Alexan-
der I. Rudnicky. 2013. Unsupervised induction and
filling of semantic slots for spoken dialogue systems
using frame-semantic parsing. In Proceedings of the
2013 IEEEWorkshop on Automatic Speech Recogni-
tion and Understanding, pages 120?125.
Wim De Smet and Marie-Francine Moens. 2013. Rep-
resentations for multi-document event clustering.
Data Mining and Knowledge Discovery, 26(3):533?
558.
Martin Dillon and Ann S. Gray. 1983. FASIT: A
fully automatic syntactically based indexing system.
Journal of the American Society for Information Sci-
ence, 34(2):99?108.
510
High-Level Rule Rule Patterns # Test-set: Unknown n-grams %
Domain Categories #
Training-set Test-set Unigrams Bigrams
Travel EN 32 982 284 5.13% 20.71%
Travel GR 35 956 324 17.26% 33.09%
Tourism EN 24 1004 285 0.72% 4.84%
Finance EN 9 136 37 12.35% 36.74%
Table 2: Characteristics of training and test datasets.
Domain SAIL-GRS Baseline
P P
w
R R
w
F F
w
P P
w
R R
w
F F
w
Travel EN 0.57 0.54 0.66 0.62 0.61 0.58 0.38 0.40 0.67 0.69 0.48 0.51
Travel GR 0.49 0.46 0.62 0.51 0.55 0.49 0.16 0.17 0.73 0.65 0.26 0.26
Tourism EN 0.75 0.75 0.90 0.90 0.82 0.82 0.82 0.80 0.94 0.94 0.87 0.87
Finance EN 0.67 0.78 0.62 0.78 0.65 0.78 0.40 0.48 0.63 0.78 0.49 0.60
Table 3: Evaluation results for SAIL-GRS system compared to the baseline in all four datasets in terms
of per rule Precision P , Recall R, and F-score F . In the grey column, P
w
, R
w
, and F
w
stand for the
weighted average of the per rule precision, recall and F-score respectively, as defined in Equ. 1 and 2.
Sudeep Gandhe and David Traum. 2007. First steps
towards dialogue modelling from an un-annotated
human-human corpus. In Proceedings of the Fifth
IJCAI Workshop on Knowledge and Reasoning in
Practical Dialogue Systems, pages 22?27.
Yihong Gong and Xin Liu. 2001. Generic text summa-
rization using relevance measure and latent semantic
analysis. In Proceedings of the 24th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?01,
pages 19?25, New York, NY, USA. ACM.
Elias Iosif and Alexandros Potamianos. 2007. A soft-
clustering algorithm for automatic induction of se-
mantic classes. In Proceedings of the 8th Annual
Conference of the International Speech Communi-
cation Association, pages 1609?1612. ISCA.
Helen M. Meng and Kai-Chung Siu. 2002. Semi-
automatic acquisition of semantic structures for
understanding domain-specific natural language
queries. IEEE Transactions on Knowledge and Data
Engineering, 14(1):172?181.
Andrew N. Pargellis, Eric Fosler-Lussier, Alexandros
Potamianos, and Chin-Hui Lee. 2001. Metrics
for measuring domain independence of semantic
classes. In Proceedings of the 7th European Con-
ference on Speech Communication and Technology,
pages 447?450. ISCA.
Stephanie Seneff. 1992. TINA: A natural language
system for spoken language applications. Computa-
tional Linguistics, 18(1):61?86, March.
Wayne Ward. 1990. The CMU air travel informa-
tion service: Understanding spontaneous speech.
In Speech and Natural Language: Proceedings of
a Workshop Held at Hidden Valley, Pennsylvania,
pages 127?129.
Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawa-
hara. 2011. Spoken dialogue system based on in-
formation extraction using similarity of predicate ar-
gument structures. In Proceedings of the SIGDIAL
2011 Conference, pages 59?66.
511
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 44?53,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Enrichment and Structuring of Archival Description Metadata
Kalliopi Zervanou?, Ioannis Korkontzelos?, Antal van den Bosch? and Sophia Ananiadou?
? Tilburg centre for Cognition and Communication (TiCC), University of Tilburg
Warandelaan 2 - PO Box 90153, 5000 LE Tilburg, The Netherlands
{K.Zervanou, Antal.vdnBosch}@uvt.nl
? National Centre for Text Mining, University of Manchester
131 Princess Street, Manchester M1 7DN, UK
{Ioannis.Korkontzelos, Sophia.Ananiadou}@manchester.ac.uk
Abstract
Cultural heritage institutions are making their
digital content available and searchable on-
line. Digital metadata descriptions play an im-
portant role in this endeavour. This metadata
is mostly manually created and often lacks de-
tailed annotation, consistency and, most im-
portantly, explicit semantic content descrip-
tors which would facilitate online browsing
and exploration of available information. This
paper proposes the enrichment of existing
cultural heritage metadata with automatically
generated semantic content descriptors. In
particular, it is concerned with metadata en-
coding archival descriptions (EAD) and pro-
poses to use automatic term recognition and
term clustering techniques for knowledge ac-
quisition and content-based document classi-
fication purposes.
1 Introduction
The advent of the digital age has long changed the
processes and the media which cultural heritage in-
stitutions (such as libraries, archives and museums)
apply for describing and cataloguing their objects:
electronic cataloguing systems support classification
and search, while cultural heritage objects are asso-
ciated to digital metadata content descriptions. The
expansion of the web and the increasing engagement
of web users throughout the world has brought about
the need for cultural heritage institutions to make
their content available and accessible to a wider au-
dience online.
In this endeavour, cultural heritage institutions
face numerous challenges. In terms of metadata,
different metadata standards currently exist for de-
scribing various types of objects, both within the
same institution and across different institutions.
Moreover, metadata object descriptions have been
typically both created by and addressed to librar-
ian and archivist experts who have been expected
to assist visitors in their search. For this reason,
they primarily refer to bibliographic descriptions
(e.g. author/creator, title, etc.), or physical descrip-
tions (e.g. size, shape, material, etc.), and location.
The lack of semantic descriptors in this type of meta-
data makes it difficult for potential online visitors to
browse and explore available information based on
more intuitive content criteria.
Work on metadata in cultural heritage institu-
tions has been largely focused on the issue of meta-
data heterogeneity. There have been efforts towards
the development and adoption of collection-specific
metadata standards, such as MARC 21 (Library of
Congress, 2010) and EAD (Library of Congress,
2002), for library and archival material respectively,
which are intended to standardise metadata descrip-
tions across different institutions. To address the is-
sue of heterogeneity across different types of object
collections, generic metadata schemas have been
proposed, such as the Dublin Core Metadata Initia-
tive (DCMI, 2011). Moreover, current research has
attempted to integrate diverse metadata schemas by
mappings across existing schemas (Bountouri and
Gergatsoulis, 2009), or mappings of existing meta-
data to ontologies, either based on ad-hoc manually
developed ontologies (Liao et al, 2010), or on ex-
isting standard ontologies for cultural heritage pur-
poses (Lourdi et al, 2009), such as the CIDOC Con-
44
ceptual Reference Model (CIDOC, 2006). Other
approaches attempt to address the issue of meta-
data heterogeneity from a pure information retrieval
perspective and discard the diverse metadata struc-
tures in favour of the respective text content descrip-
tions for full text indexing (Koolen et al, 2007).
Zhang and Kamps (2009) attempt to exploit the ex-
isting metadata XML structure for XML-based re-
trieval, thus targeting individual document compo-
nents. Similarly to our approach, they investigate
metadata describing archive collections.
The work presented in this paper focuses on meta-
data for textual objects, such as archive documents,
and on the issue of explicit, semantic, content de-
scriptors in this metadata, rather than heterogene-
ity. In particular, we are concerned with the lack
of explicit content descriptors which would support
exploratory information search. For this purpose,
we attempt to automatically enrich manually cre-
ated metadata with content information. We view
the problem from an unsupervised, text mining per-
spective, whereby multi-word terms recognised in
free text are assumed to indicate content. In turn,
the respective inter-relationships among the recog-
nised terms in the hierarchy are assumed to reveal
the knowledge structure of the document collection.
In this paper, we start with a description of our
EAD dataset and the challenges which our dataset
poses in text processing. Subsequently, we discuss
our approach to the enrichment and structuring of
these archival descriptions and present our experi-
ments. We conclude with a discussion on our results
and our considerations for future work.
2 EAD and Challenges in Text Processing
The Encoded Archival Description (EAD) was con-
ceived as ?a nonproprietary encoding standard for
machine-readable finding aids such as inventories,
registers, indexes, and other documents created by
archives, libraries, museums, and manuscript repos-
itories to support the use of their holdings? (Li-
brary of Congress, 2002). It is intended to be a data
communication format based on SGML/XML syn-
tax, aiming at supporting the accessibility to archival
resources across different institutions and focusing
on the structural content of the archival descrip-
tion, rather than its presentation. For this reason,
the EAD schema is characterised by a hierarchi-
cal informational structure, where the deepest lev-
els in the schema may inherit descriptive informa-
tion defined in the upper levels. The schema de-
fines a total of 146 elements. The three highest level
elements are <eadheader>, <frontmatter>,
and <archdesc>. <eadheader> is an ele-
ment containing bibliographic and descriptive in-
formation about the metadata document, while
<frontmatter> is an optional element describ-
ing the creation, publication, or use of the metadata
document (Library of Congress, 2002). Both these
two upper level elements do not contain information
about the archival material itself. The designated el-
ement for this purpose is <archdesc> which de-
scribes ?the content, context, and extent of a body
of archival materials, including administrative and
supplemental information that facilitates use of the
materials? (Library of Congress, 2002).
EAD metadata files can be lengthy and com-
plex in structure, with deep nesting of the XML
hierarchy elements. As Zhang and Kamps (2009)
also observe, the EAD elements may be of three
types:
i. atomic units (or text content elements) which
contain only text and no XML elements;
ii. composite units (or nested elements) which
contain as nested other XML elements;
iii. mixed elements which contain both atomic and
composite units.
The EAD documents used in this study describe
archival collections of the International Institute of
Social History (IISH). They are of varying length
and are often characterised by long spans of non-
annotated, free text. The degree of annotation, es-
pecially within mixed element types is inconsistent.
For example, some names may be annotated in one
element and others not, while quite often repeated
mentions of the same name may not be annotated.
Moreover, the text within an annotated element may
include annotator comments (e.g., translations, alter-
nate names, questions, notes, etc.), either in square
brackets or parentheses, again in an inconsistent
manner. The multilingual text content poses another
challenge. In particular, the languages used in the
description text vary, not only within a single EAD
document, but often also within an element (mixed
or atomic). In our approach, the former is addressed
45
by identifying the language at element level (cf. Sec-
tion 3.2). However, the issue of mixed languages
within an element is not addressed. This introduces
errors, especially for multilingual elements of short
text length.
3 Enrichment and Structuring Method
The overall rationale behind our method for the en-
richment of EAD metadata with semantic content in-
formation is based on two hypotheses:
i. multi-word terms recognised in free text are
valid indicators of content, and
ii. the respective term inter-relationships reflect
the knowledge structure of the collection.
Thus, automatic term recognition and subsequent
term clustering constitute the two core components
of our EAD processing. In particular, as illustrated
in Figure 1, we start with a pre-processing phase,
where the EAD input SGML/XML files are first
parsed, in order to retrieve the respective text con-
tent snippets, and then classified, based on language.
Subsequently, terms are recognised automatically.
The resulting terms are clustered as a hierarchy and,
finally, the documents are classified according to the
term hierarchy, based on the terms that they contain.
To evaluate our term recognition process, we exploit
knowledge from two sources: existing annotations
in the EAD files, such as entity annotation residing
in mixed elements (cf. Section 2) and entity and sub-
ject term information originating from the respective
cultural heritage institution Authority files, namely
the library files providing standard references for en-
tities and terms that curators should use in their ob-
ject descriptions. In this section, we discuss in more
detail the methodology for each of the components
of our approach.
3.1 EAD Text Element Extraction
In our processing of the EAD metadata XML, we
focused on the free text content structured below
the <archdesc> root element. As discussed in
Section 2, it is the only top element which con-
tains information about the archival material itself.
In the text element extraction process, we parse
the EAD XML and, from the hierarchically struc-
tured elements below <archdesc>, we select the
text contained in <abstract>, <bioghist>,
<scopecontent>, <odd>, <note> , <dsc>
and <descgrp> and their nested elements.
Among these elements, the <dsc> (Description
of Subordinate Components) provides information
about the hierarchical groupings of the materials be-
ing described, whereas <descgrp> (DSC Group)
defines nested encoded finding aids. They were se-
lected because they may contain nested information
of interest. The rest of the elements were selected
because they contain important free text information
related to the archive content:
- <bioghist>: describing the archive creator
e.g. the life of the individual or family, or
the administrative history of the organisation
which created the archive;
- <scopecontent>: referring to the range
and topical coverage of the described materials,
often naming significant organisations, individ-
uals, events, places, and subjects represented;
- <odd>: other descriptive data;
- <note>: referring to archivist comments and
explanations;
- <abstract>: brief summaries of all the
above information.
All other elements not referring to the archive se-
mantic content, such as administrative information,
storage arrangement, physical location, etc. were ig-
nored. Moreover, atomic or composite elements
without free text descriptions were not selected, be-
cause the descriptive information therein is assumed
to be already fully structured.
3.2 Language Identification
As mentioned in Section 2, the languages used in
the description text of the EAD documents vary, not
only within a single EAD document, but often also
within an EAD element. In our approach, the objec-
tive of the language identification process is to de-
tect the language of the text content snippets, i.e. the
output of the text element extraction process, and
classify these snippets accordingly (cf. Figure 1).
Language identification is a text categorisation
task, whereby identifiers attempt to learn the mor-
phology of a language based on training text and,
subsequently, use this information to classify un-
known text accordingly. For this reason, training a
language identification component requires a train-
ing corpus for each language of interest.
46
Figure 1: Block diagram of EAD metadata enrichment and structuring process
Computational approaches to language identifi-
cation can be coarsely classified into information-
theoretic, word-based, and N-gram-based.
Information-theoretic approaches compare the
compressibility of the input text to the compress-
ibility of text in the known languages. Measuring
compressibility employs mutual information mea-
sures (Poutsma, 2002). Word-based approaches
consider the amount of common words or special
characters between the input text and a known
language. Finally, N-gram-based approaches con-
struct language models beyond word boundaries,
based on the occurrence statistics of N-grams up
to some predefined length N (Dunning, 1994).
The subsequent language identification in unknown
text is based on the similarity of the unknown text
N-gram model to each training language model.
As evidenced by these approaches, language iden-
tification relies on some form of comparison of the
unknown text to known languages. For this reason,
the respective text categorisation into a given lan-
guage suffers when the input text is not long enough:
the shorter the input text is, the fewer the available
features for comparison against known language
models. Moreover, errors in the categorisation pro-
cess are also introduced, when the language models
under comparison share the same word forms.
In our approach, we have opted for the most pop-
ular language identification method, the one based
on N-grams. Nevertheless, any other language iden-
tification method could have been applied.
3.3 Term Recognition
The objective of term recognition is the identifica-
tion of linguistic expressions denoting specialised
concepts, namely domain or scientific terms. For in-
formation management and retrieval purposes, the
automatic identification of terms is of particular im-
portance because these specialised concept expres-
sions reflect the respective document content.
Term recognition approaches largely rely on the
identification of term formation patterns. Linguistic
approaches use either syntactic (Justeson and Katz,
1995; Hearst, 1998), or morphological (Heid, 1998)
rule patterns, often in combination with termino-
logical or other lexical resources (Gaizauskas et al,
2000) and are typically language and domain spe-
cific.
Statistical approaches typically combine linguis-
tic information with statistical measures. These
measures can be coarsely classified into two
categories: unithood-based and termhood-based.
Unithood-based approaches measure the attachment
strength among the constituents of a candidate
term. For example, some unithood-based mea-
sures are frequency of co-occurrence, hypothesis
testing statistics, log-likelihood ratios test (Dunning,
1993) and pointwise mutual information (Church
and Hanks, 1990). Termhood-based approaches at-
tempt to measure the degree up to which a candidate
expression is a valid term, i.e. refers to a specialised
concept. They attempt to measure this degree by
considering nestedness information, namely the fre-
47
quencies of candidate terms and their subsequences.
Examples of such approaches are C-Value and NC-
Value (Frantzi et al, 2000) and the statistical barrier
method (Nakagawa, 2000).
It has been experimentally shown that termhood-
based approaches to automatic term extraction out-
perform unithood-based ones and that C-Value
(Frantzi et al, 2000) is among the best perform-
ing termhood-based approaches (Korkontzelos et
al., 2008). For this reason, we choose to employ
the C-Value measure in our pipeline. C-Value ex-
ploits nestedness and comes together with a com-
putationally efficient algorithm, which scores can-
didate multi-word terms according to the measure,
considering:
- the total frequency of occurrence of the candi-
date term;
- the frequency of the candidate term as part of
longer candidate terms;
- the number of these distinct longer candidates;
- the length of the candidate term (in tokens).
These arguments are expressed in the following
nestedness formula:
N(?) =
?
?
?
f(?), if ? is not nested
f(?)?
1
|T?|
?
b?T?
f(b), otherwise (1)
where ? is the candidate term, f(?) is its frequency,
T? is the set of candidate terms that contain ? and
|T?| is the cardinality of T?. In simple terms, the
more frequently a candidate term appears as a sub-
string of other candidates, the less likely it is to be a
valid term. However, the greater the number of dis-
tinct term candidates in which the target term can-
didate occurs as nested, the more likely it is to be
a valid term. The final C-Value score considers the
length (|?|) of each candidate term (?) as well:
C-value(?) = log2 |?| ?N(?) (2)
The C-Value method requires linguistic pre-
processing in order to detect syntactic term for-
mation patterns. In our approach, we used Lex-
Tagger (Vasilakopoulos, 2003), which combines
transformation-based learning with decision trees
and we adapted its respective lexicon to our domain.
We also included WordNet lemma information in
our processing, for text normalisation purposes. Lin-
guistic pre-processing is followed by the computa-
tion of C-Value on the candidate terms, in length or-
der, longest first. Candidates that satisfy a C-Value
threshold are sorted in decreasing C-Value order.
3.4 Hierarchical Agglomerative Clustering
In our approach, term recognition provides content
indicators. In order to make explicit the knowl-
edge structure of the EAD, our method requires
some form of concept classification and structuring.
The process of hierarchical agglomerative cluster-
ing serves this objective.
Agglomerative algorithms are very popular in
the field of unsupervised concept hierarchy induc-
tion and are typically employed to produce unla-
belled taxonomies (King, 1967; Sneath and Sokal,
1973). Hierarchical clustering algorithms are based
on measuring the distance (dissimilarity) between
pairs of objects. Given an object distance metric D,
the similarity of two clusters, A and B, can be de-
fined as a function of the distance D between the
objects that the clusters contain. According to this
similarity, also called linkage criterion, the choice
of which clusters to merge or split is made. In our
approach, we have experimented with the three most
popular criteria, namely:
Complete linkage (CL): The similarity of two clus-
ters is the maximum distance between their elements
simCL(A,B) = max
x?A,y?B
D(x, y) (3)
Single linkage (SL): The similarity of two clusters
is the minimum distance between their elements
simSL(A,B) = min
x?A,y?B
D(x, y) (4)
Average linkage (AL): The similarity of two clusters
is the average distance between their elements
simAL(A,B) =
1
|A| ? |B|
?
x?A
?
y?B
D(x, y) (5)
To estimate the distance metric D we use either
the document co-occurrence or the lexical similar-
ity metric. The chosen distance metric D and link-
age criterion are employed to derive a hierarchy of
terms by agglomerative clustering.
Our document co-occurrence (DC) metric is de-
fined as the number of documents (d) in the collec-
tion (R) in which both terms (t1 and t2) co-occur:
DC =
1
|R|
|{d : (d ? R) ? (t1 ? d) ? (t2 ? d)}| (6)
48
The above metric accepts that the distance between
two terms is inversely proportional to the number of
documents in which they co-occur.
Lexical Similarity (LS), as defined in Nenadic?
and Ananiadou (2006), is based on shared term con-
stituents:
LS =
|P (h1) ? P (h2)|
|P (h1)|+ |P (h2)|
+
|P (t1) ? P (t2)|
|P (t1)|+ |P (t2)|
(7)
where t1 and t2 are two terms, h1 and h2 their heads,
P (h1) and P (h2) their set of head words, and P (t1)
and P (t2) their set of constituent words, respec-
tively.
3.5 Document Classification
The term hierarchy is used in our approach for se-
mantic classification of documents. In this process,
we start by assigning to each leaf node of the term
hierarchy the set of EAD documents in which the
corresponding term occurs. Higher level nodes are
assigned the union of the document sets of their
daughters. The process is bottom-up and applied it-
eratively, until all hierarchy nodes are assigned a set
of documents.
Document classification, i.e. the assignment of
document sets to term hierarchy nodes, is use-
ful, among others, for structured search and index-
ing purposes. Moreover, it provides a direct soft-
clustering of documents based on semantics, given
the number of desired clusters, C. C corresponds
to a certain horizontal cut of the term hierarchy, so
that C top nodes appear, instead of one. The doc-
ument sets assigned to these C top nodes represent
the C desired clusters. This document clustering ap-
proach is soft, since each document can occur in one
or more clusters.
3.6 Evaluation Process
The automatic evaluation process, illustrated in Fig-
ure 1, serves the purpose of evaluating the term
recognition accuracy. Since the objective of term
recognition tools is the detection of linguistic ex-
pressions denoting specialised concepts, i.e. terms,
the results evaluation would ideally require input
from the respective domain experts. This is a la-
borious and time consuming process which also en-
tails finding the experts willing to dedicate effort
and time for this task. In response to this issue,
we decided to exploit the available domain-specific
knowledge resources and automate part of the eval-
uation process by comparing our results to this ex-
isting information. Thus, the automatic evaluation
process is intended to give us an initial estimate
of our performance and reduce the amount of re-
sults requiring manual evaluation. The available re-
sources used are of two types:
i. entity annotations in the EAD documents (i.e.
names of persons, organisations and geograph-
ical locations);
ii. entity and subject terms originating from the
cultural heritage institution Authority files.
The entity annotations in the EAD documents
were not considered during our term recognition.
The entity and subject terms of the respective Au-
thority file records are encoded in MARC21/XML
format (Library of Congress, 2010). MARC
(MAchine-Readable Cataloging) is a standard initi-
ated by the US Library of Congress and concerns
the representation of bibliographic information and
related data elements used in library catalogues. The
MARC21 Authority files resource used in our eval-
uation provides, among other information, the stan-
dard references for entities and the respective pos-
sible entity reference variations, such as alternate
names or acronyms, etc., that curators should use
in their object descriptions. The subject term Au-
thority records provide mappings between a legacy
subject term thesaurus which is no longer used for
classification, and current library records.
In the evaluation process the EAD SGML/XML
and the MARC21/XML Authority files are first
parsed by the respective parsers in order to extract
the XML elements of interest. Subsequently, the
text-content of the elements is processed for nor-
malisation and variant generation purposes. In this
process, normalisation involves cleaning up the text
from intercepted comments and various types of
inconsistent notes, such as dates, aliases and al-
ternate names, translations, clarifications, assump-
tions, questions, lists, etc. Variant generation in-
volves detecting the acronyms, abbreviated names
and aliases mentioned in the element text and cre-
ating the reversed variants for, e.g., [Last Name,
First Name] sequences. The results of this pro-
cess, from both EAD and Authority files, are merged
into a single list for every respective category (or-
49
language snippets language snippets
Dutch 50,363 Spanish 3,430
German 41,334 Danish 2,478
English 19,767 Italian 1,100
French 6,182 Swedish 699
Table 1: Number of snippets per identified language.
ganisations, persons, geographic locations and sub-
ject terms) and are compared to our term results list.
4 Experimental Setting
For training the language identification component,
we used the European Parliament Proceedings Par-
allel Corpus (Europarl) which covers the proceed-
ings of the European Parliament from 1996 to 2006
(Koehn, 2005). The corpus size is 40 million words
per language and is translated in Danish, German,
Greek, English, Spanish, Finnish, French, Italian,
Dutch, Portuguese and Swedish. In our experiments,
we take as input for subsequent term recognition
only the snippets identified as English text.
In the experiments reported in this work, we ac-
cept as term candidates morpho-syntactic pattern se-
quences which consist of adjectives and nouns, and
end with a noun. The C-Value algorithm (cf. Sec-
tion 3.3) was implemented under two different set-
tings:
i. one only considering as term candidates adjec-
tive and noun sequences that appear at least
once as non-nested in other candidate terms;
and
ii. one that considers all adjective and noun se-
quences, even if they never occur as non-
nested.
Considering that part-of-speech taggers usually suf-
fer high error rates when applied on specialty do-
mains, the former setting is expected to increase pre-
cision, whereas the latter to increase recall (cf. Sec-
tion 5).
We accepted as valid terms all term candidates
whose C-Value score exceeds a threshold, which
was set to 3.0 after experimentation. In the subse-
quent hierarchical agglomerative clustering process,
we experimented with all six combinations of the
three linkage criteria (i.e. complete, single and aver-
age) with the two distance metrics (i.e. document
co-occurrence and lexical similarity) described in
Figure 2: Length of snippets per identified language.
Section 3.4.
5 Results
The EAD document collection used for this study
consisted of 3, 093 SGML/XML files. As shown on
Table 1, according to our language identifier, the ma-
jority of the text snippets of the selected EAD XML
elements were in Dutch, followed by German and
English. We selected for later processing 19, 767
snippets classified as English text, corresponding to
419, 857 tokens. A quantitative evaluation of the
language identifier results has not been performed.
However, our observation of the term recognition re-
sults showed that there were some phrases, mostly
Dutch and German entity names (organisations and
persons mostly) classified as English. This might be
due to these entities appearing in their original lan-
guage within English text, as it is often the case in
our EAD files. Moreover, manual inspection of our
results showed that other languages classified as En-
glish, e.g. Turkish and Czech, were not covered by
Europarl.
As mentioned in Section 3.2, short text snip-
pets may affect language identification performance.
Figure 2 illustrates the snippet length per identified
language. We observe that the majority of text snip-
pets is below 10 tokens, few fall within an average
length of 20 to 50 tokens approximately, and very
few are above 100 tokens.
Figure 3 shows the results of our automatic evalu-
ation for the term recognition process. In this graph,
the upper, red curve shows the percentage of cor-
rect terms for the C-Value setting considering as
term candidates adjective and noun sequences that
appear at least once as non-nested in other candi-
date terms. The lower, blue curve shows the per-
50
Figure 3: Term coverage for each C-Value setting based
on EAD & Authority entity and subject term evaluation.
centage of correct terms for the C-Value setting con-
sidering all adjective and noun sequences, even if
they never occur as non-nested. In this automatic
evaluation, correct terms are, as presented in Sec-
tion 3.6, those candidate terms matching the com-
bined lists of entity and subject terms acquired by
the respective EAD and MARC21 Authority files.
We observe that the C-Value setting which considers
only noun phrase patterns occurring at least once as
non-nested, displays precision up to approximately
70% for the top terms in the ranked list, whereas the
other setting considering all noun phrase sequences,
reaches a maximum of 49%. The entire result set
above the 3.0 C-Value threshold amounts to 1, 345
and 2, 297 terms for each setting, and reaches pre-
cision of 42.01% and 28.91% respectively. Thus,
regarding precision, the selective setting clearly out-
performs the one considering all noun phrases, but it
also reaches a lower recall, as indicated by the ac-
tual terms within the threshold. We also observe
that precision drops gradually below the threshold,
an indication that the ranking of the C-Value mea-
sure is effective in promoting valid terms towards
the top. This automatic evaluation considers as erro-
neous unknown terms which may be valid. Further
manual evaluation by domain experts is required for
a more complete picture of the results.
Figure 4 shows six dendrograms, each represent-
ing the term hierarchy produced by the respective
combination of linkage criterion to distance metric.
The input for these experiments consists of all terms
exceeding the C-Value threshold, and by considering
only noun phrase sequences appearing at least once
as non-nested. Since the hierarchies contain 1, 345
terms, the dendrograms are very dense and difficult
to inspect thoroughly. However, we include them
based on the fact that the overall shape of the den-
drogram can indicate how much narrow or broad the
corresponding hierarchy is and indirectly its quality.
Narrow here characterises hierarchies whose most
non-terminal nodes are parents of one terminal and
one non-terminal node. Narrow hierarchies are deep
while broader hierarchies are shallower.
Broad and shallow hierarchies are, in our case, of
higher quality, since terms are expected to be related
to each other and form distinct groups. In this view,
average linkage leads to richer hierarchies (Figures
4(c), 4(f)), followed by single linkage (Figures 4(b),
4(e)) and, finally, complete linkage (Figures 4(a),
4(d)). The hierarchy of higher quality seems to
be the result of average linkage and document co-
occurrence combination (Figure 4(c)), followed by
the combination of average linkage and lexical sim-
ilarity (Figure 4(f)). Clearly, these two hierarchies
need to be investigated manually and closely to ex-
tract further conclusions. Moreover, an application-
based evaluation could investigate whether different
clustering settings suit different tasks.
6 Conclusion and Future Work
In this paper, we have presented a methodology for
semantically enriching archival description meta-
data and structuring the metadata collection. We
consider that terms are indicators of content seman-
tics. In our approach, we perform term recogni-
tion and then hierarchically structure the recognised
terms. Finally, we use the term hierarchy to classify
the metadata documents. We also propose an auto-
matic evaluation of the recognised terms, by com-
paring them to domain knowledge resources.
For term recognition, we used the C-Value al-
gorithm and found that considering noun phrases
which appear at least once independently, outper-
forms considering all noun phrases. Regarding hier-
archical clustering, we observe that the average link-
age criterion combined with a distance metric based
on document co-occurrence produces a rich broad
hierarchy. A more thorough evaluation of these re-
sults is required. This should include a manual eval-
uation of recognised terms by domain experts and
an application-based evaluation of the resulting doc-
ument classification.
51
(a) Complete linkage - DC (b) Single linkage - DC (c) Average linkage - DC
(d) Complete linkage - LS (e) Single linkage - LS (f) Average linkage - LS
Figure 4: Dendrograms showing the results of agglomerative clustering for all linkage criteria and distance metrics,
document co-occurrence (DC) and Lexical Similarity (LS).
References
Lina Bountouri and Manolis Gergatsoulis. 2009. Inter-
operability between archival and bibliographic meta-
data: An EAD to MODS crosswalk. Journal of Li-
brary Metadata, 9(1-2):98?133.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
CIDOC. 2006. The CIDOC Conceptual Reference
Model. CIDOC Documentation Standards Working
Group, International Documentation Committee, In-
ternational Council of Museums. http://www.
cidoc-crm.org/.
DCMI. 2011. The Dublin Core Metadata Initiative.
http://dublincore.org/.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Ted Dunning. 1994. Statistical identification of lan-
guage. MCCS 94-273. Technical report, Computing
Research Laboratory, New Mexico State University.
Katerina Frantzi, Sophia Ananiadou, and Hideki Mima.
2000. Automatic recognition of multi-word terms: the
C-value/NC-value method. International Journal on
Digital Libraries, 3(2):115?130.
Robert Gaizauskas, George Demetriou, and Kevin
Humphreys. 2000. Term recognition in biological sci-
ence journal articles. In Proc. of the NLP 2000 Work-
shop on Computational Terminology for Medical and
Biological Applications, pages 37?44, Patras, Greece.
Marti Hearst. 1998. Automated discovery of WordNet
relations. In Christiane Fellbaum, editor, WordNet:
An Electronic Lexical Database, pages 131?153. MIT
Press.
Ulrich Heid. 1998. A linguistic bootstrapping approach
to the extraction of term candidates from german text.
Terminology, 5(2):161?181.
John Justeson and Slava Katz. 1995. Technical terminol-
ogy: some linguistic properties and an algorithm for
identification in text. Natural Language Engineering,
1(1):9?27.
Benjamin King. 1967. Step-Wise clustering proce-
dures. Journal of the American Statistical Association,
62(317):86?101.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Machine Transla-
tion Summit X, pages 79?86, Phuket, Thailand.
Marijn Koolen, Avi Arampatzis, Jaap Kamps, Vincent
de Keijzer, and Nir Nussbaum. 2007. Unified access
to heterogeneous data in cultural heritage. In Proc. of
RIAO ?07, pages 108?122, Pittsburgh, PA, USA.
Ioannis Korkontzelos, Ioannis Klapaftis, and Suresh
Manandhar. 2008. Reviewing and evaluating auto-
matic term recognition techniques. In Bengt Nord-
stro?m and Aarne Ranta, editors, Proc. of GoTAL ?08,
volume 5221 of LNCS, pages 248?259, Gothenburg,
Sweden. Springer.
Shu-Hsien Liao, Hong-Chu Huang, and Ya-Ning Chen.
2010. A semantic web approach to heterogeneous
metadata integration. In Jeng-Shyang Pan, Shyi-Ming
Chen, and Ngoc Thanh Nguyen, editors, Proc. of
ICCCI ?10, volume 6421 of LNCS, pages 205?214,
Kaohsiung, Taiwan. Springer.
Library of Congress. 2002. Encoded archival descrip-
tion (EAD), version 2002. Encoded Archival Descrip-
tion Working Group: Society of American Archivists,
52
Network Development and MARC Standards Office,
Library of Congress. http://www.loc.gov/
ead/.
Library of Congress. 2010. MARC standards. Network
Development and MARC Standards Office, Library of
Congress, USA. http://www.loc.gov/marc/
index.html.
Irene Lourdi, Christos Papatheodorou, and Martin Doerr.
2009. Semantic integration of collection description:
Combining CIDOC/CRM and Dublin Core collections
application profile. D-Lib Magazine, 15(7/8).
Hiroshi Nakagawa. 2000. Automatic term recognition
based on statistics of compound nouns. Terminology,
6(2):195?210.
Goran Nenadic? and Sophia Ananiadou. 2006. Min-
ing semantically related terms from biomedical liter-
ature. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 5(1):22?43.
Arjen Poutsma. 2002. Applying monte carlo techniques
to language identification. Language and Computers,
45:179?189.
Peter Sneath and Robert Sokal. 1973. Numerical taxon-
omy: the principles and practice of numerical classifi-
cation. Freeman, San Francisco, USA.
Argyris Vasilakopoulos. 2003. Improved unknown word
guessing by decision tree induction for POS tagging
with tbl. In Proc. of CLUK ?03, Edinburgh, UK.
Junte Zhang and Jaap Kamps. 2009. Focused search
in digital archives. In Gottfried Vossen, Darrell D. E.
Long, and Jeffrey Xu Yu, editors, Proc. of WISE
?09, volume 5802 of LNCS, pages 463?471, Poznan,
Poland. Springer.
53
